Abstract
Concept Bottleneck Models (CBM) are inherently inter-pretable models that factor model decisions into human-readable concepts. They allow people to easily understand why a model is failing, a critical feature for high-stakes ap-plications. CBMs require manually speciﬁed concepts and often under-perform their black box counterparts, preventing their broad adoption. We address these shortcomings and are ﬁrst to show how to construct high-performance CBMs without manual speciﬁcation of similar accuracy to black box models. Our approach, Language Guided Bottlenecks (LaBo), leverages a language model, GPT-3, to deﬁne a large space of possible bottlenecks. Given a problem domain,
LaBo uses GPT-3 to produce factual sentences about cate-gories to form candidate concepts. LaBo efﬁciently searches possible bottlenecks through a novel submodular utility that promotes the selection of discriminative and diverse informa-tion. Ultimately, GPT-3’s sentential concepts can be aligned to images using CLIP, to form a bottleneck layer. Experi-ments demonstrate that LaBo is a highly effective prior for concepts important to visual recognition. In the evaluation with 11 diverse datasets, LaBo bottlenecks excel at few-shot classiﬁcation: they are 11.7% more accurate than black box linear probes at 1 shot and comparable with more data.
Overall, LaBo demonstrates that inherently interpretable models can be widely applied at similar, or better, perfor-mance than black box approaches.1 1.

Introduction
As deep learning systems improve, their applicability to critical domains is hampered because of a lack of trans-parency. Efforts to address this have largely focused on post-hoc explanations [47, 54, 72]. Such explanations can be problematic because they may be incomplete or unfaith-ful with respect to the model’s computations [49]. Models 1Code and data are available at https://github.com/YueYANG1996/LaBo
Input Image x 
Human Designed Concepts  has nape color :: grey  has bill shape :: cone 
.
.
.
.
.
. has head pattern :: eyebow 
Ours: LLM Generated Concepts  black throat with a white boarder  brown head with white stripes 
.
.
.   grayish brown back and wings 
GPT-3 
.
.
.  !"   !"  prompt: describe what the black-throated sparrow looks like: 
Figure 1. Our proposed high-performance Concept Bottleneck
Model alleviates the need for human-designed concepts by prompt-ing large language models (LLMs) such as GPT-3 [4]. can also be designed to be inherently interpretable, but it is believed that such models will perform more poorly than their black box alternatives [16]. In this work, we provide evidence to the contrary. We show how to construct high-performance interpretable-by-design classiﬁers by combin-ing a language model, GPT-3 [4], and a language-vision model, CLIP [44].
Our method builds on Concept Bottleneck Models (CBM) [25], which construct predictors through a linear combination of human-designed concepts. For example, as seen in Figure 1, a qualiﬁed person can design concepts, such as “nape color,” as intermediate targets for a black box model before classifying a bird. CBMs provide abstractions that people can use to understand errors or intervene on, contributing to increased trust.
Application of CBMs is limited because they require costly attribute annotations by domain experts and often under-perform their black box counterparts.
In contexts where CBM performance is competitive with black box al-ternatives, interpretability properties are sacriﬁced [34, 70].
To address both of these challenges, we propose to build systems that automatically construct CBMs.
Our Language Model Guided Concept Bottleneck Model (LaBo), Figure 2, allows for the automatic construction of high-performance CBMs for arbitrary classiﬁcation prob-lems without concept annotations. Large language models                                                              
Figure 2. We present an overview of our Language-Model-Guided Concept Bottleneck Model (LaBo), which is interpretable by design image classiﬁcation system. First, we prompt the large language model (GPT-3) to generate candidate concepts (Sec 3.4). Second, we employ a submodular function to select concepts from all candidates to construct the bottleneck (Sec 3.2). Third, we apply a pretrained alignment model (CLIP) to obtain the embeddings of concepts and images, which is used to compute concept scores. Finally, we train a linear function in which the weight W denotes the concept-class association user to predict targets based on concept scores (Sec 3.3). (LLMs) contain signiﬁcant world knowledge [21, 42, 61], that can be elicited by inputting a string preﬁx and allowing
LLMs to complete the string (prompting). For example, in
Figure 1, GPT-3 is prompted about sparrows and completes with information such as “brown head with white stripes.”
LaBo leverages this by constructing bottlenecks where the concepts are such GPT-3 generated sentences. Since our concepts are textual, we use CLIP to score their presence in an image and form a bottleneck layer out of these scores.
A key advantage of LaBo is the ability to control the selection of concepts in the bottleneck by generating can-didates from the language model. We develop selection principles targeting both interpretability and classiﬁcation accuracy. For example, we prefer smaller bottlenecks that include shorter sentences that do not include class names.
Furthermore, to maximize performance, we prefer attributes that CLIP can easily recognize and are highly discriminative.
To account for appearance variation, we select attributes that cover a variety of information and are not repetitive. We formulate these factors into a novel sub-modular criterion that allows us to select good bottlenecks efﬁciently [38].
We have evaluated LaBo-created bottlenecks on 11 di-verse image classiﬁcation tasks, spanning recognition of common objects [11, 26] to skin tumors [64]. ﬁne-grained types [3, 32, 39, 67], textures [10], actions [59], skin tu-mors [64], and satellite photographed objects [8].2 Our 2The only dataset specialization we perform is prompt tuning for GPT-main ﬁnding is that LaBo is a highly effective prior for what concepts to look for, especially in low data regimes. In eval-uations comparing with linear probes, LaBo outperforms by as much as 11.7% at 1-shot and marginally underperforms given larger data settings. Averaged over many dataset sizes,
LaBo bottlenecks are 1.5% more accurate than linear probes.
In comparison to modiﬁcations of CBMs that improve per-formance by circumventing the bottleneck [70], we achieve similar or better results without breaking the CBM abstrac-tion. In extensive ablations, we study key trade-offs in bot-tleneck design and show our selection criteria are crucial and highlight several other critical design choices.
Human evaluations indicate that our bottlenecks are largely understandable, visual, and factual. Finally, anno-tators ﬁnd our GPT-3 sourced bottlenecks are more factual and groundable than those constructed from WordNet or
Wikipedia sentences. Overall, our experiments demonstrate that automatically designed CBMs can be as effective as black box models while maintaining critical factors con-tributing to their interpretability. 2.