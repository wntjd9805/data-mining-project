Abstract
The dual-pixel (DP) sensor captures a two-view image pair in a single snapshot by splitting each pixel in half.
The disparity occurs in defocus blurred regions between the two views of the DP pair, while the in-focus sharp regions have zero disparity. This motivates us to propose a
K3DN framework for DP pair deblurring, and it has three modules: i) a disparity-aware deblur module. It estimates a disparity feature map, which is used to query a trainable kernel set to estimate a blur kernel that best describes the spatially-varying blur. The kernel is constrained to be symmetrical per the DP formulation. A simple Fourier transform is performed for deblurring that follows the blur model; ii) a reblurring regularization module. It reuses the blur kernel, performs a simple convolution for reblurring, and regularizes the estimated kernel and disparity feature unsupervisedly, in the training stage; iii) a sharp region
It identiﬁes in-focus regions that preservation module. correspond to areas with zero disparity between DP images, aims to avoid the introduction of noises during the deblurring process, and improves image restoration performance. Experiments on four standard DP datasets show that the proposed K3DN outperforms state-of-the-art methods, with fewer parameters and ﬂops at the same time. 1.

Introduction
Dual-pixel (DP) sensors are widely employed in digital single-lens reﬂex cameras (DSLR) and smartphone cam-eras. Each pixel of a DP sensor is divided into two photodi-odes to provide two sub-views of a scene, namely, left and right views, to assist in auto-focusing. Recently, researchers have used the two-view DP pair to beneﬁt several computer vision tasks, especially defocus deblurring [1,2,24]. Specif-ically, blurred pixels in the left and right view of the DP pair exhibit an amount of shift, termed DP disparity, which pro-vides information for the blur kernel estimation (the key for defocus deblurring [14, 16, 19]).
† Corresponding author. ⇤ Equal contribution. 27 26 25 24
)
B d (
R
N
S
P
OursOurs
BAMBNet(22)
Restormer(22)
OursOurs
Restormer(22)
BAMBNet(22)
IFAN(21)
IFAN(21)
DeepRFT(21)
DRBNet (22)
DRBNet (22)
DeepRFT(21)
DDDNet(21)
RDPD(21)
RDPD(21)
KPAC(21)
DPDNet(20)
KPAC(21)
DDDNet(21)
DPDNet(20)
DEMENet(19)
DEMENet(19) 0 10 20 30 0 1K 2K 3K 4K 5K
Params (M)
Flops (G)
Figure 1. Comparison of our method and state-of-the-art methods on the real DPD-blur dataset [1]. The Blue and red cycles sep-arately denote our tiny and lightweight models. We show PSNRs (dB) with respect to model parameters (M) and Flops (G). The numbers in the brackets beside methods denote publication years.
Best viewed in color on the screen.
Existing DP-based defocus deblurring methods are gen-erally divided into two categories: end-to-end-based and disparity (defocus map)-based methods. End-to-end based methods [1, 2, 11, 17, 19, 24] directly restore an all-in-focus image from a defocus blurred DP pair without consider-ing DP domain knowledge for network design. Therefore, it is challenging for these methods to tackle spatially-varying and large blur. Disparity-based methods [10,11,15,23] esti-mate the DP disparity (or defocus map) to aid the restoration of an all-in-focus image. However, these methods either re-quire extra data (ground-truth DP disparity as supervision)
[15], neglect DP disparity knowledge when warping [10], heavily depend on the number of network branches [11], or rely on pre-calibrated kernel sets and additional priors [23] for training.
Our method, K3DN, belongs to the disparity-based group, and it has high performance on real DP datasets (see
Fig. 1 for an example). K3DN follows the mathematic in-dications of DP and blur models, while not requiring pre-calibrated kernel sets and extra data. K3DN consists of three modules: i) a disparity-aware deblur module; ii) a re-blurring regularization module; iii) a sharp region preserva-tion module.
The ﬁrst component is a disparity-aware deblur module that estimates spatially-varying kernels for deblurring. We deﬁne a trainable kernel set, and kernels in the set can model diverse blur patterns. Each kernel satisﬁes the horizontally symmetric constraint [2, 16], following the DP image for-mulation. Given the input DP image pair, we ﬁrst estimate a disparity feature map, and then use the disparity feature map to query the kernel set to estimate a DP blur kernel that best describes the spatially-varying blur. With the blur ker-nel, we simply perform a Fourier transform for deblurring that follows the blur model.
The second component we developed is a reblurring reg-ularization module. It reuses the DP blur kernel, takes an all-in-focus image as input, and performs a simple convolu-tion to generate a blurred image. By enforcing the similarity between the blurred image and the input DP image, we reg-ularize the proposed deblur module.
Our third component is a sharp region preservation mod-ule. Not all regions of DP pairs are blurred. We observe that previous works neglect to preserve pixel values or features from in-focus sharp regions. By mining (based on the DP model) similarities between features before and after our deblur module, we preserve features from all-in-focus re-gions and improve feature quality, leading to better image restoration performance.
Please note that all intermediate parts (e.g., the train-able kernel set and disparity estimator) do not require ex-tra ground-truth information (e.g., pre-calibrated kernel sets and disparity) for training. We only need ground-truth all-in-focus images for end-to-end training.
Our contributions are summarized below:
• A simple K3DN framework for DP image pair defocus deblurring;
• A disparity-aware deblur module.
It has a trainable kernel set, and estimates a disparity feature map to query the set to estimate a DP blur kernel that best de-scribes the spatial varying blur;
• A reblurring regularization module. It follows the DP model, reuses the blur kernel from the deblur module, and regularizes the deblur module;
• A sharp region preservation module. It mines in-focus regions of DP pairs, improves feature quality from in-focus regions, resulting in better restoration perfor-mance.
Experimentally, we validate the proposed K3DN on four standard benchmark datasets [1,2,15,16], showing state-of-the-art performance in image restoration and model size. 2.