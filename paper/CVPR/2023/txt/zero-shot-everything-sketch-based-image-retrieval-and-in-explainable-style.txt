Abstract
This paper studies the problem of zero-short sketch-based image retrieval (ZS-SBIR), however with two sig-nificant differentiators to prior art (i) we tackle all vari-ants (inter-category, intra-category, and cross datasets) of
ZS-SBIR with just one network (“everything”), and (ii) we would really like to understand how this sketch-photo matching operates (“explainable”). Our key innovation lies with the realization that such a cross-modal matching prob-lem could be reduced to comparisons of groups of key local patches – akin to the seasoned “bag-of-words” paradigm.
Just with this change, we are able to achieve both of the aforementioned goals, with the added benefit of no longer requiring external semantic knowledge. Technically, ours is a transformer-based cross-modal network, with three novel components (i) a self-attention module with a learnable tok-enizer to produce visual tokens that correspond to the most informative local regions, (ii) a cross-attention module to compute local correspondences between the visual tokens across two modalities, and finally (iii) a kernel-based rela-tion network to assemble local putative matches and pro-duce an overall similarity metric for a sketch-photo pair.
Experiments show ours indeed delivers superior perfor-mances across all ZS-SBIR settings. The all important ex-plainable goal is elegantly achieved by visualizing cross-modal token correspondences, and for the first time, via sketch to photo synthesis by universal replacement of all matched photo patches. Code and model are available at https://github.com/buptLinfy/ZSE-SBIR. 1.

Introduction
Zero-shot sketch-based image retrieval (ZS-SBIR) is a central problem to sketch understanding [8, 16, 19, 20, 28, 34, 50, 54, 55, 57, 60, 67]. The zero-shot setting is largely driven by the prevailing data scarcity problem of human sketches [19,28,58] – they are much harder to acquire com-pared with photos. As research matures on the non zero-∗ Equal contribution.
† Corresponding author.
Figure 1. Attentive regions of self-/cross-attention and the learned visual correspondence for tackling unseen cases. (a) The proposed retrieval token [Ret] can attend to informative regions. Different colors are attention maps from different heads. (b) Cross-attention offers explainability by explicitly constructing local visual cor-respondence. The local matches learned from training data are shareable knowledge, which enables ZS-SBIR to work under di-verse settings (inter- / intra-category and cross datasets) with just one model. (c) An input sketch can be transformed into its image by the learned correspondence, i.e., sketch patches are replaced by the closest image patches from the retrieved image. shot [4, 25, 33, 41–43, 59, 62], and in a push to make sketch-based retrieval commercially viable, recent research efforts had mainly focused on ZS-SBIR (or the simpler few-shot setting) [16, 19, 28, 34, 50, 57, 60].
Great strides have been made but attempts have largely aligned with the larger photo-based zero-shot literature, where the key lies in leveraging external knowledge for cross-category adaptation [19, 34]. That of conducting cross-modal matching is, however, less studied, and most prior art relies on a gold standard triplet loss with some auxiliary modules [16] to learn a joint embedding. Further-more, as problems such as domain shift and fine-grained matching come to play, research efforts are mostly done in silo for different settings: category-level (standard) [28, 50, 60], fine-grained [2], and cross-dataset [40]. Last but defi-nitely not least, one can not help but wonder why many of the proposed algorithm work – what is matched, and how is the transfer conducted?
This paper aims to tackle all said problems associated with the current status quo for ZS-SBIR. In particular, we advocate for (i) a single model to tackle all three settings of
ZS-SBIR, (ii) ditching the requirement on external knowl-edge to conduct category transfer, and more importantly, (iii) a way to explain why our model works (or not).
At the very core of our contribution lies with a well-explored insight that predates “deep vision”, that image matching can be achieved by establishing local patch cor-respondences and computing a distance score based on that – yes, loosely similar to that of “bag of visual words” [11,
In our context of 26, 51] (without building dictionaries).
ZS-SBIR, we would like to conduct this matching (i) cross two diverse modalities in sketch and photo, and (ii) cross-category, granularity (fine-grained or not) and dataset (do-main difference) boundaries. The biggest upside of this re-alization is that just as how “bag of visual words” is explain-able, we can directly visualize the patch correspondences to achieve a similar level of explainability (see Figure 1).
Our solution first is a transformer-based cross-modal net-work, that (i) sources local patches independently in each modality, (ii) establishes patch-to-patch correspondences across two modalities, and (iii) computes matching scores based on putative correspondences. We put forward a novel design for each of the three components. We approach (i) by proposing a novel CNN-based learnable tokenizer, that is specifically tailored to sketch data. This is because the vanilla non-overlapping patch-wise tokenization proposed in ViT [18] is not friendly to the sparse nature of sketches (as most patches would belong to the uninformative blank).
Our tokenizer on the other hand attends to a larger recep-tive field [37] hence more keen to sketch data. With this tokenizer, visual cues from nearby regions are aggregated when constructing visual tokens, so that structural informa-tion is preserved. In the same spirit of class token developed in ViT for image recognition, we introduce a learnable re-trieval token to prioritize tokens for cross-modal matching.
To establish patch-to-patch correspondences, a novel cross-attention module is proposed that operates across sketch-photo modalities. Specifically, we propose cross-modal multi-head attention, in which the query embeddings are exchanged between sketch and photo branches to rea-son patch-level correspondences with only category-level supervision. With the putative matches in place, inspired by relation networks [53], we propose a kernel-based relation network to aggregate the correspondences and calculate a similarity score between each sketch-photo pair.
We achieve state-of-the-art performance across all said
ZS-SBIR settings. Explainability is offered (i) as per tradi-tion in terms of visualizing patch correspondences, where interesting local matches can be observed, such as the antlers of deer in Figure 1(b), regardless of a sketch being very abstract, and (ii) by replacing all patches in a sketch with their photo correspondences, to perform sketch to photo synthesis as shown in Figure 1(c). 2.