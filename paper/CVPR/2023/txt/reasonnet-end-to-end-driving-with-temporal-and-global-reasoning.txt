Abstract
The large-scale deployment of autonomous vehicles is yet to come, and one of the major remaining challenges lies in urban dense traffic scenarios.
In such cases, it remains challenging to predict the future evolution of the scene and future behaviors of objects, and to deal with rare adverse events such as the sudden appearance of occluded objects. In this paper, we present ReasonNet, a novel end-to-end driving framework that extensively exploits both tem-poral and global information of the driving scene. By rea-soning on the temporal behavior of objects, our method can effectively process the interactions and relationships among features in different frames. Reasoning about the global information of the scene can also improve overall perception performance and benefit the detection of adverse events, especially the anticipation of potential danger from occluded objects. For comprehensive evaluation on occlu-sion events, we also release publicly a driving simulation benchmark DriveOcclusionSim consisting of diverse occlu-sion events. We conduct extensive experiments on multi-ple CARLA benchmarks, where our model outperforms all prior methods, ranking first on the sensor track of the public
CARLA Leaderboard [53]. 1.

Introduction
Despite significant recent progress in the field of au-truly large-scale deployment of au-tonomous driving, tonomous vehicles (AVs) on public roads has yet to be established. The majority of the remaining issues lie in navigating dense urban traffic scenes, where a large num-ber of different dynamic objects (e.g. vehicles, bicycles, pedestrians), complex road geometries and road user inter-actions are involved. In such circumstances, currently de-ployed or tested solutions could make incorrect or unex-pected decisions , resulting in severe accidents or traffic in-fractions [4, 24, 53]. Two of the major challenges behind
*Corresponding author
Figure 1. Temporal reasoning on the historic behaviors of sur-rounding objects can benefit the prediction of the scene evolution and objects’ future behaviors. Global reasoning on the interaction among objects and the environment allows for inference about un-observable space and occluded objects, anticipating potential dan-ger and enhancing perception/driving performance. such autonomous incompetence include 1) how to achieve a comprehensive understanding of the driving scene and, more importantly, how to make high-fidelity predictions on the future evolution of the driving scene; 2) how to deal with rare adverse events in long-tail distributions, such as undetected but relevant objects in occluded regions.
Comprehensive scene understanding and high-fidelity prediction of how objects in the scene will move in the fu-ture are vital for autonomous vehicles to take safe and reli-able actions. Toward this end, modularized methods were proposed to decompose the task into three sequential sub-tasks: detection [37–40, 42], tracking [5, 65], and forecast-ing [6, 26, 28, 32, 33, 59–61]. While more interpretability is provided by developing each module independently, these sub-tasks are still regarded as open research questions and errors in each sub-task can propagate and accumulate, lead-ing to unstable overall performance.
In contrast, end-to-end driving methods [9, 10, 51] have recently emerged as a promising method to solve these subtasks in a monolithic manner directly. However, a high-fidelity future prediction necessitates sufficient temporal reasoning over the historic
information of the scene [22, 50], which is usually only somewhat considered in previous end-to-end driving meth-ods if not completely ignored. For example, [10, 15] only exploited scene information in the current frame, and [54] simply concatenated features in historic frames for temporal reasoning. In such cases, the interactions and relationships amongst features in different frames and objects cannot be sufficiently modeled. Thus in this paper, we propose a tem-poral reasoning module to effectively fuse information from different frames for better driving performance.
On the other hand, rare adverse events in long-tail distri-butions remain a notoriously challenging issue on the way toward large scale deployment of autonomous vehicles. For example, one such challenge is the difficulty in detecting occluded but relevant objects in the scene. While a large amount of research has focused on improving perception performance [37, 57], the occluded objects essentially lie out of the scope of observable elements, and failure to con-sider such objects can result in either dangerous or overly cautious driving behavior. Our observation is that, while humans also suffer from similar limitations to autonomous vehicles regarding occluded objects, they are able to rea-son about these unobservable spaces by exploiting global information of the scene such as road geometry and driv-ing interaction patterns, to anticipate potential danger even under occlusion. For example, when one human driver no-tices another vehicle braking abruptly, the driver may rea-son the presence of an occluded object (e.g., a pedestrian) ahead, reminding himself to drive cautiously. Thus, our insight is that, a safe and intelligent autonomous vehicle should also master the global reasoning capability to have a better perception of the scene.
In this paper, we pro-pose a transformer-based global reasoning module to suf-ficiently fuse information of the environment and objects, and analyze their interactions for better scene understand-ing. Such global reasoning capability not only benefits in-teraction modeling with occluded objects, but also improves overall perception performance. Examples of such perfor-mance gains include better traffic light status identification by reasoning over other vehicles’ actions and more accurate future trajectory forecasting by reasoning over interactions among objects. Besides, considering the fact that the oc-clusion events lie in the long-tail distribution and have been rare in currently available datasets, we also construct a Driv-ing in Occlusion Simulation benchmark (DOS) consisting of 4 occlusion scenarios, each with 25 cases, as a compre-hensive occlusion event evaluation benchmark in the field of end-to-end autonomous driving.
In this paper, we propose a novel end-to-end driving framework named temporal and global reasoning network (ReasonNet), which provides enhanced reasoning on the temporal evolution and the global information of the scene, for better perception performance and driving quality. Our contributions are three-fold:
• We propose a novel temporal and global reasoning
Network (ReasonNet) to enhance historic scene rea-soning for high-fidelity prediction of the scene’s fu-ture evolution and improve global contextual percep-tion performance even under occlusion.
• We present a new benchmark called Driving in
Occlusion Simulation benchmark (DOS), which con-sists of diverse occlusion scenarios in urban driving for systematic evaluation in occlusion events, and make the benchmark publicly available.
• We experimentally validate our method on multiple benchmarks with complex and adversarial urban sce-narios. Our model ranks first on the sensor track of the
CARLA autonomous driving leaderboard. 2.