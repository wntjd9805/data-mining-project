Abstract
We introduce DiffRF, a novel approach for 3D radiance field synthesis based on denoising diffusion probabilistic models. While existing diffusion-based methods operate on images, latent codes, or point cloud data, we are the first to directly generate volumetric radiance fields. To this end, we propose a 3D denoising model which directly operates on an explicit voxel grid representation. However, as radi-ance fields generated from a set of posed images can be am-biguous and contain artifacts, obtaining ground truth radi-ance field samples is non-trivial. We address this challenge by pairing the denoising formulation with a rendering loss, enabling our model to learn a deviated prior that favours good image quality instead of trying to replicate fitting er-rors like floating artifacts. In contrast to 2D-diffusion mod-els, our model learns multi-view consistent priors, enabling free-view synthesis and accurate shape generation. Com-pared to 3D GANs, our diffusion-based approach naturally enables conditional generation such as masked completion or single-view 3D synthesis at inference time. 1.

Introduction
In recent years, Neural Radiance Fields (NeRFs) [37] have emerged as a powerful representation for fitting indi-vidual 3D scenes from posed 2D input images. The ability to photo-realistically synthesize novel views from arbitrary viewpoints while respecting the underlying 3D scene geom-Project page: https://sirwyver.github.io/DiffRF/.
etry has the potential to disrupt and transform applications like AR/VR, gaming, mapping, navigation, etc. A num-ber of recent works have introduced extensions for making
NeRFs more sophisticated, by e.g., showing how to incor-porate scene semantics [18, 30], training models from het-erogeneous data sources [35], or scaling them up to repre-sent large-scale scenes [62, 64]. These advances are testa-ment to the versatility of ML-based scene representations; however, they still fit to specific, individual scenes rather than generalizing beyond their input training data.
In contrast, neural field representations that general-ize to multiple object categories or learn priors for scenes across datasets appear much more limited to date, despite enabling applications like single-image 3D object gener-ation [7, 39, 49, 66, 72] and unconstrained scene explo-ration [15]. These methods explore ways to disentangle ob-ject priors into shape and appearance-based components, or to decompose radiance fields into several small and locally-conditioned radiance fields to improve scene generation quality; however, their results still leave significant gaps w.r.t. photorealism and geometric accuracy.
Directions involving generative adversarial networks (GANs) that have been extended from the 2D domain to 3D-aware neural fields generation are demonstrating impressive synthesis results [8]. Like regular 2D GANs, the training objective is based on discriminating 2D images, which are obtained by rendering synthesized 3D radiance fields.
At the same time, diffusion-based models [52] have re-cently taken the computer vision research community by storm, performing on-par or even surpassing GANs on multiple 2D benchmarks, and are producing photo-realistic images that are almost indistinguishable from real pho-tographs. For multi-modal or conditional settings such as text-to-image synthesis, we currently observe unprece-dented output quality and diversity from diffusion-based ap-proaches. While several works address purely geometric representations [33, 75], lifting the denoising-diffusion for-mulation directly to 3D volumetric radiance fields remains challenging. The main reason lies in the nature of diffu-sion models, which require a one-to-one mapping between the noise vector and the corresponding ground truth data samples. In the context of radiance fields, such volumetric ground truth data is practically infeasible to obtain, since even running a costly per-sample NeRF optimization results in incomplete and imperfect radiance field reconstructions.
In this work, we present the first diffusion-based gen-erative model that directly synthesizes 3D radiance fields, thus unlocking high-quality 3D asset generation for both shape and appearance. Our goal is to learn such a gener-ative model trained across objects, where each sample is given by a set of posed RGB images.
To this end, we propose a 3D denoising model directly operating on an explicit voxel grid representation (Fig. 1, left) producing high-frequency noise estimates. To address the ambiguous and imperfect radiance field representation for each training sample, we propose to bias the noise pre-diction formulation from Denoising Diffusion Probabilistic
Models (DDPMs) towards synthesizing higher image qual-ity by an additional volumetric rendering loss on the esti-mates. This enables our method to learn radiance field pri-ors less prone to fitting artifacts or noise accumulation dur-ing the sampling process. We show that our formulation leads to diverse and geometrically-accurate radiance field synthesis producing efficient, realistic, and view-consistent renderings. Our learned diffusion prior can be applied in an unconditional setting where 3D object synthesis is obtained in a multi-view consistent way, generating highly-accurate 3D shapes and allowing for free-view synthesis. We further introduce the new task of conditional masked completion – analog to shape completion – for radiance field completion at inference time. In this setting, we allow for realistic 3D completion of partially-masked objects without the need for task-specific model adaptation or training (see Fig. 1, right).
We summarize our contributions as follows:
• To the best of our knowledge, we introduce the first diffusion model to operate directly on 3D radiance fields, enabling high-quality, truthful 3D geometry and image synthesis.
• We introduce the novel application of 3D radiance field masked completion, which can be interpreted as a nat-ural extension of image inpainting to the volumetric domain.
• We show compelling results in unconditional and con-ditional settings, e.g., by improving over GAN-based approaches on image quality (from 16.54 to 15.95 in
FID) and geometry synthesis (improving MMD from 5.62 to 4.42), on the challenging PhotoShape Chairs dataset [44]. 2.