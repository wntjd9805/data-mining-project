Abstract
Denoising diffusion (score-based) generative models have recently achieved significant accomplishments in gen-erating realistic and diverse data. Unfortunately, the gener-ation process of current denoising diffusion models is noto-riously slow due to the lengthy iterative noise estimations, which rely on cumbersome neural networks.
It prevents the diffusion models from being widely deployed, especially on edge devices. Previous works accelerate the generation process of diffusion model (DM) via finding shorter yet ef-fective sampling trajectories. However, they overlook the cost of noise estimation with a heavy network in every iter-ation. In this work, we accelerate generation from the per-spective of compressing the noise estimation network. Due to the difficulty of retraining DMs, we exclude mainstream training-aware compression paradigms and introduce post-training quantization (PTQ) into DM acceleration. How-ever, the output distributions of noise estimation networks change with time-step, making previous PTQ methods fail in DMs since they are designed for single-time step sce-narios. To devise a DM-specific PTQ method, we explore
PTQ on DM in three aspects: quantized operations, cali-bration dataset, and calibration metric. We summarize and use several observations derived from all-inclusive inves-tigations to formulate our method, which especially tar-gets the unique multi-time-step structure of DMs. Exper-imentally, our method can directly quantize full-precision
DMs into 8-bit models while maintaining or even improv-ing their performance in a training-free manner.
Impor-tantly, our method can serve as a plug-and-play module on other fast-sampling methods, e.g., DDIM [24]. The code is available at https://https://github.com/ 42Shawn/PTQ4DM . 1.

Introduction
Recently, denoising diffusion (also dubbed score-based) generative models [11, 38, 38, 40] have achieved phenome-* Equal contribution. â€  Corresponding author. nal success in various generative tasks, such as images [11, 24, 40], audio [21], video [35], and graphs [25]. Besides these fundamental tasks, their flexibility of implementation on downstream tasks is also attractive, e.g., they are ef-fectively introduced for super-resolution [15, 28], inpaint-ing [15, 40], and image-to-image translation [30]. Diffu-sion models (DMs) have achieved superior performances on most of these tasks and applications, both concerning qual-ity and diversity, compared with historically SoTA Genera-tive Adversarial Networks (GANs) [9].
A diffusion process transforms real data gradually into
Gaussian noise, and then the process is reversed to generate real data from Gaussian noise (denoising process) [11, 43].
Particularly, the denoising process requires iterating the noise estimation (also known as a score function [40]) via a cumbersome neural network over thousands of time-steps. While it has a compelling quantity of images, its long iterative process and high inference cost for gener-ating samples make it undesirable. Thus, increasing the speed of this generation process is now an active area of re-search [2, 4, 18, 24, 29, 40]. To accelerate diffusion models, researchers propose several approaches, which mainly fo-cus on sample trajectory learning for faster sampling strate-gies. For example, Chen et al. [4] and San-Roman et al. [29] propose faster step size schedules for VP diffusions that still yield relatively good quality/diversity metrics; Song et al. [37] adopt implicit phases in the denoising process; Bao et al. [2] and Lu et al. [18] derive analytical approximations to simplify the generation process.
Our study suggests that two orthogonal factors slow down the denoising process: i) lengthy iterations for sam-pling images from noise, and ii) a cumbersome network for estimating noise in each iteration. Previously DM accelera-tion methods only focus on the former [2, 4, 18, 24, 29, 40], but overlook the latter. From the perspective of network compression, many popular network quantization and prun-ing methods follow a simple pipeline: training the origi-nal model and then fine-tuning the quantized/pruned com-pressed model [17, 31]. Particularly, this training-aware compression pipeline requires a full training dataset and many computation resources to perform end-to-end back-1
tions change with time-step. This means that a key mod-ule of the previous PTQ calibration, cannot be used in our case. Based on the above observations, we devise a DM-specific calibration method, termed Normally Distributed
Time-step Calibration (NDTC), which first samples a set of time-steps from a skew normal distribution, and then gen-erates calibration samples in terms of sampled time-steps by the denoising process.
In this way, the time-step dis-crepancy in the calibration set is enhanced, which improves the performance of PTQ4DM. Finally, we propose a novel
DM acceleration method, Post-Training for Diffusion Mod-els (PTQ4DM) via incorporating all the explorations.
Overall, the contributions of this paper are three-fold: (i) To accelerate denoising diffusion models, we introduce
PTQ into DM acceleration where noise estimation networks are directly quantized in a post-training manner. To the best of our knowledge, this is the first work to investigate dif-fusion model acceleration from the perspective of training-free network compression. (ii) After all-inclusively investi-gations of PTQ and DMs, we observe the performance drop induced by PTQ for DMs can be attributed to the discrep-ancy of output distributions in various time-steps. Targeting this observation, we explore PTQ from different aspects and propose PTQ4DM. (iii) Experimentally, PTQ4DM can quan-tize the pre-trained diffusion models to 8-bit without sig-Importantly, nificant performance loss for the first time.
PTQ4DM can serve as a plug-and-play module for other
SoTA DM acceleration methods, as shown in Fig. 1. 2.