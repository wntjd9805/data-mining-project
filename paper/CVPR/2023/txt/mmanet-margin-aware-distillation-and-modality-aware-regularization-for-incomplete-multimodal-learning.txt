Abstract
Multimodal learning has shown great potentials in nu-merous scenes and attracts increasing interest recently.
However, it often encounters the problem of missing modal-ity data and thus suffers severe performance degradation in practice. To this end, we propose a general framework called MMANet to assist incomplete multimodal learn-ing. It consists of three components: the deployment net-work used for inference, the teacher network transferring comprehensive multimodal information to the deployment network, and the regularization network guiding the de-ployment network to balance weak modality combinations.
Specifically, we propose a novel margin-aware distilla-tion (MAD) to assist the information transfer by weigh-ing the sample contribution with the classification uncer-tainty. This encourages the deployment network to focus on the samples near decision boundaries and acquire the refined inter-class margin. Besides, we design a modality-aware regularization (MAR) algorithm to mine the weak modality combinations and guide the regularization net-work to calculate prediction loss for them. This forces the deployment network to improve its representation abil-ity for the weak modality combinations adaptively. Fi-nally, extensive experiments on multimodal classification and segmentation tasks demonstrate that our MMANet out-performs the state-of-the-art significantly. Code is available at: https://github.com/shicaiwei123/MMANet 1.

Introduction
Multimodal learning has achieved great success on many vision tasks such as classification [21, 33, 46], object detec-tion [26, 45, 53], and segmentation [5, 23, 41]. However, most successful methods assume that the models are trained
In fact, limited and tested with the same modality data. by device [32, 39], user privacy [13, 25], and working con-dition [3, 29], it is often very costly or even infeasible to
Modality
Customized
Unified
RGB
Depth
IR
RGB+Depth
RGB+IR
Depth+IR
RGB+Depth+IR 10.01 4.45 11.65 3.41 6.32 3.54 1.23 11.75 5.87 16.62 4.61 6.68 4.95 2.21
Drop
-1.65
-1.42
-4.97
-1.2
-0.36
-1.41
-0.98
Table 1. The performance of customized models and the uni-fied model for different modality combinations on the CASIA-SURF dataset using the average classification error rate. The
‘customized‘ means to train a model for each combination inde-pendently while the ‘unified’ means to train only one model for all the combinations. The architectures of all the models are the same and the feature map of missing modality (such as the IR for
RGB+Depth) is set as zero. collect complete modality data during the inference stage.
There is thus substantial interest in assisting the incomplete or even single modality inference via the complete modality data during training.
A typical solution is to reconstruct the sample or feature of the missing modalities from the available ones [10, 14, 15, 20, 29, 32]. Nevertheless, this needs to build a specific model for each modality from all possible modality combi-nations and thus has high complexity. Recent studies focus on learning a unified model, instead of a bunch of networks, for different modality combinations. Generally, many such approaches [6, 11, 12, 17, 51, 52] attempt to leverage feature fusion strategies to capture modality-invariant representa-tion so that the model can adapt to all possible modality combinations. For example, RFNet [11] designs the region-aware fusion module to fuse the features of available image modalities.
Although the existing unified models are indeed able to increase the efficiency of training and deployment of the multimodal models, their performance is likely to be sub-optimal. As shown in Table 1, the customized models con-sistently outperform the unified model for different modal-ity combinations. This is because existing unified mod-els usually focus on the modality-invariant features while ignoring the modality-specific information. Note that the complementary modality-specific information of multiple modalities can help refine the inter-class discrimination and improve inference performance [2, 18, 36]. This motivates us to propose the first research question of this paper: Can a unified model consider the modality invariant and spe-cific information simultaneously while maintaining ro-bustness for incomplete modality input?
To this end, we propose to guide the unified model to learn the comprehensive multimodal information from the teacher model trained with complete modality. This regular-izes the target task loss to encourage the unified model to ac-quire complementary information among different modal-ity combinations multimodal information while preserv-ing the generalization to them. Specifically, we propose a novel margin-aware distillation (MAD) that trains the uni-fied model by guiding it to mimic the inter-sample relation of the teacher model. MAD introduces the classification uncertainty of samples to re-weigh their contribution to the final loss. Since the samples near the class boundary are more likely to be misclassified and have higher classifica-tion uncertainty [8], this encourages the unified model to preserve the inter-class margin refined by the complemen-tary cues and learn the modality-specific information.
Another limitation of existing unified approaches is that they struggle to obtain optimal performance for the unbal-anced training problem. To be specific, conventional multi-modal learning models tend to fit the discriminative modal-ity combination and their performance will degrade signif-icantly when facing weak modality combinations. To solve this issue, existing unified approaches introduce the auxil-iary discriminator to enhance the discrimination ability of the unimodal combinations [6, 11, 51]. This utilizes a hy-pothesis that a single modality is weaker than multiple ones.
However, as shown in Table 1, no matter for the customized model or the unified model, the single Depth modality out-performs the RGB, IR, and their combinations. This indi-cates the combination with multiple weak modalities may be harder to be optimized than a single strong modality.
Moreover, as shown in Table 3, RGB becomes the strong modality while Depth and IR become the weak modalities.
This indicates that the modality importance is not fixed but varies with scenarios. These findings motivate us to propose the second research question: How to effectively optimize the weak modality combination in varying scenarios?
To this end, we design a regularization network and
MAR algorithm to assist the training of the unified network.
Specifically, the regularization network generates additional predictions for all inputs. Then MAR mines and calculates prediction loss for the sample from the weak combinations.
This forces the unified model to improve its representation ability for the weak combination. In detail, MAR mines the weak combination via the memorization effect [1, 16, 49] that DNNs tend to first memorize simple examples before overfitting hard examples. As shown in Fig. 5(a), the uni-fied model tends to fit the samples containing Depth modal-ity firstly at the early stage. Therefore, MAR first mines the strong modality via the memorization effect. Then it deter-mines the combinations of rest modalities as the weak ones.
Finally, we develop a model and task agnostic frame-work called MMANet incomplete multimodal to assist learning by combining the proposed MAD and MAR strate-gies. MMANet can guide the unified model to acquire com-prehensive multimodal information and balance the perfor-mance of the strong and weak modality combination si-multaneously. Extensive comparison and ablation experi-ments on multimodal classification and segmentation tasks demonstrate the effectiveness of the MMANet. 2.