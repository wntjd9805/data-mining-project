Abstract
Synthesizing photo-realistic images from a point cloud is challenging because of the sparsity of point cloud represen-tation. Recent Neural Radiance Fields and extensions are proposed to synthesize realistic images from 2D input. In this paper, we present Point2Pix as a novel point renderer to link the 3D sparse point clouds with 2D dense image pix-els. Taking advantage of the point cloud 3D prior and NeRF rendering pipeline, our method can synthesize high-quality images from colored point clouds, generally for novel in-door scenes. To improve the efficiency of ray sampling, we propose point-guided sampling, which focuses on valid samples. Also, we present Point Encoding to build Multi-scale Radiance Fields that provide discriminative 3D point features. Finally, we propose Fusion Encoding to efficiently synthesize high-quality images. Extensive experiments on the ScanNet and ArkitScenes datasets demonstrate the ef-fectiveness and generalization. 1.

Introduction
Point cloud rendering aims to synthesize images from point clouds at given camera parameters, which has been frequently utilized in 3D visualization, navigation, and aug-mented reality. There are many advantages to point cloud representation, such as flexible shape and general 3D prior.
However, since point clouds are generally produced by 3D scanners (RGBD or LiDAR) [3, 9, 14] or by Multi-View
Stereo (MVS) from images [4, 15, 52], the points are usu-ally sparsely distributed in 3D scenes. Although traditional graphics-based renderers [38, 42, 50, 58] can render point clouds to images without training or finetuning, the quality is not satisfying with hole artifacts and missing details [10].
Recently, Neural Radiance Fields (NeRF) [29] were pro-posed for 3D representation and high-fidelity novel view synthesis. It employs an implicit function to directly map each point’s spatial information (location and direction) to attributes (color and density). However, most NeRF-based
*Corresponding author. methods [23,49,54,55] are scene-specific, thus taking much time to train from scratch for novel scenes in abundant multi-view images, which limits the practical applications.
In this work, we bridge the gap between point clouds and
NeRF, thus proposing a novel point cloud renderer, called
Point2Pix, to synthesize photo-realistic images from col-ored point clouds. Compared with most NeRF-based meth-ods [23, 29, 53, 54], ours does not necessarily require multi-view images or fine-tuning procedures for indoor scenes.
First, point clouds are treated as underlying anchors of
NeRF. The training process of NeRF is to learn 3D point at-tributes from given locations. Because there is no mapping ground truth for multi-view images, NeRF-based methods
[23, 29] indirectly train their networks with a pixel recon-struction loss. Note that point clouds are exactly made up of points with location and attributes, thus can provide train-ing pairs for the mapping function to conduct supervised learning and improve performance.
Then, point clouds can also improve the efficiency of ray sampling. NeRF-based methods [23, 29, 54] learn the 3D shape and structure from multi-view images, which do not involve geometric prior in novel scenes. Thus, dense uni-form sampling [5, 46] or coarse-to-fine sampling [29, 54] were used to synthesize high-quality images. These strate-gies are inefficient because most of the locations in 3D scenes are empty [23, 31]. Since point clouds represent a relatively fine shape of 3D scenes, the area around existing points deserves more attention. Based on this observation, we propose a point-guided sampling strategy to mainly fo-cus on the local area of points in the point cloud.
It can significantly reduce the number of required samples while maintaining decent synthesis accuracy.
Further, point-based networks can provide 3D features prior to subsequent applications, general for novel scenes.
Although many methods [7, 18, 36, 37] have been proposed for various point cloud understanding tasks, they are usu-ally designed for existing points. In this work, we propose
Multi-scale Radiance Fields, including Point Encoder and
MLP networks, to extract multi-scale features for any loca-tion in the scene. These 3D point features are discriminative and general, ensuring finetuning-free rendering. Also, in-spired by recent NeRF-based image generators [19, 22, 57], we render the 3D point features as multi-scale feature maps.
Our fusion decoder gradually synthesizes high-resolution images. It can not only fill the possible holes but also im-prove the quality of rendered images. Our main contribu-tions are summarized as follows.
• We propose Point2Pix to link point clouds with image space, which renders point clouds into photo-realistic images.
• We present an efficient ray sampling strategy and fu-sion decoder to greatly decrease the number of samples in each ray and the total number of rays, thus acceler-ating the rendering process.
• We propose Multi-scale Radiance Fields, which ex-tract discriminative 3D prior for arbitrary 3D locations.
• Extensive experiments and ablation studies on indoor datasets demonstrate the effectiveness and generaliza-tion of the proposed method. 2.