Abstract 1.

Introduction
Despite increasingly realistic image quality, recent 3D image generative models often operate on 3D volumes of
ﬁxed extent with limited camera motions. We investigate the task of unconditionally synthesizing unbounded nature scenes, enabling arbitrarily large camera motion while main-taining a persistent 3D world model. Our scene represen-tation consists of an extendable, planar scene layout grid, which can be rendered from arbitrary camera poses via a 3D decoder and volume rendering, and a panoramic sky-dome. Based on this representation, we learn a generative world model solely from single-view internet photos. Our method enables simulating long ﬂights through 3D land-scapes, while maintaining global scene consistency—for instance, returning to the starting point yields the same view of the scene. Our approach enables scene extrap-olation beyond the ﬁxed bounds of current 3D genera-tive models, while also supporting a persistent, camera-independent world representation that stands in contrast to auto-regressive 3D prediction models. Our project page: https://chail.github.io/persistent-nature/.
Generative image and video models have achieved re-markable levels of realism, but are still far from pre-senting a convincing, explorable world. Moving a vir-tual camera through these models—either in their latent space [3, 23, 29, 72] or via explicit conditioning [35]—is not like walking about in the real world. Movement is either very limited (for example, in object-centric models [5]), or else camera motion is unlimited but quickly reveals the lack of a persistent world model. Auto-regressive 3D synthesis meth-ods exemplify this lack of persistence [42, 45]; parts of the scene may change unexpectedly as the camera moves, and you may ﬁnd that the scene is entirely different when return-ing to previous positions. The lack of spatial and temporal consistency can give the output of these models a strange, dream-like quality. In contrast, machines that can generate unbounded, persistent 3D worlds could be used to develop agents that plan within a world model [21], or to build vir-tual reality experiences that feel closer to the natural world, rather than appearing as ephemeral hallucinations [42].
We therefore aim to develop a unconditional generative model capable of generating unbounded 3D scenes with a
persistent underlying world representation. We want synthe-sized content to move in a way that is consistent with camera motion, yet we should also be able to move arbitrarily far and still generate the same scene upon returning to a previous camera location, regardless of the camera trajectory.
To achieve this goal, we model a 3D world as a terrain plus a skydome. The terrain is represented by a scene layout grid—an extendable 2D array of feature vectors that acts as a map of the landscape. We ‘lift’ these features into 3D and decode them with an MLP into a radiance ﬁeld for volume rendering. The rendered terrain images are super-resolved and composited with renderings from the skydome model to synthesize ﬁnal images. We train using a layout grid of limited size, but can extend the scene layout grid by any de-sired amount during inference, enabling unbounded camera trajectories. Since our underlying representation is persistent over space and time, we can ﬂy around 3D landscapes in a consistent manner. Our method does not require multiview data; each part of our system is trained from an unposed collection of single-view images using GAN objectives.
Our work builds upon two prior threads of research that tackle generating immersive worlds: 1) generative models of 3D data, and 2) generative models of inﬁnite videos. Along the ﬁrst direction are generators of meshes, volumes, radi-ance ﬁelds, etc (e.g., [5, 54, 59]). These models represent a consistent 3D world by construction, and excel at render-ing isolated objects and bounded indoor scenes. Our work, in contrast, tackles the challenging problem of generating large-scale unbounded nature scenes. Along the second di-rection are methods like InﬁniteNature [42, 45], which can indeed simulate visual worlds of inﬁnite extent. These meth-ods enable unbounded scene synthesis by predicting new viewpoints auto-regressively from a starting view. However, they do not ensure a persistent world representation; content may change when revisited.
Our method aims to combine the best of both worlds, generating boundless scenes (unlike prior 3D generators) while still representing a persistent 3D world (unlike prior video generative models). In summary:
• We present an unconditional 3D generative model for unbounded nature scenes with a persistent world repre-sentation, consisting of a terrain map and skydome.
• We augment our generative pipeline to support camera extrapolation beyond the training camera distribution by extending the terrain features.
• Our model is learned entirely from single-view land-scape photos with unknown camera poses. 2.