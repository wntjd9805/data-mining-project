Abstract
Visual place recognition (VPR) is a fundamental task of computer vision for visual localization. Existing methods are trained using image pairs that either depict the same place or not. Such a binary indication does not consider continu-ous relations of similarity between images of the same place taken from different positions, determined by the continuous nature of camera pose. The binary similarity induces a noisy supervision signal into the training of VPR methods, which stall in local minima and require expensive hard mining al-gorithms to guarantee convergence. Motivated by the fact that two images of the same place only partially share visual cues due to camera pose differences, we deploy an automatic re-annotation strategy to re-label VPR datasets. We compute graded similarity labels for image pairs based on available localization metadata. Furthermore, we propose a new Gen-eralized Contrastive Loss (GCL) that uses graded similarity labels for training contrastive networks. We demonstrate that the use of the new labels and GCL allow to dispense from hard-pair mining, and to train image descriptors that perform better in VPR by nearest neighbor search, obtaining superior or comparable results than methods that require expensive hard-pair mining and re-ranking techniques. 1.

Introduction
Visual place recognition (VPR) is an important task of computer vision, and a fundamental building block of nav-It is igation systems for autonomous vehicles [24, 48]. approached either with structure-based methods, namely
Structure-from-Motion [36] and SLAM [26], or with im-age retrieval [2, 15, 20, 29, 30, 46]. The former focus on precise relative camera pose estimation [34, 35]. The lat-ter aim at learning image descriptors for effective retrieval of similar images to a given query in a nearest search ap-proach [28]. The goal of descriptor learning is to ensure im-ages of the same place to be projected onto close-by points in a latent space, and images of different places to be pro-jected onto distant points [9, 10, 21]. Contrastive [19, 30] and triplet [2, 22, 23, 27] loss were used for this goal and resulted (a) reference image (b) GPS distance 6m positive (c) GPS distance 25.6m negative
Figure 1. (a) A place in the city of Amman. (b) An image taken 6m away is labeled as positive (same place), while (c) an image taken 25.6m away is labeled as negative (not the same place) despite sharing a lot of visual cues. in state-of-the-art performance on several VPR benchmarks.
VPR methods are normally trained using image pairs labelled to indicate they either depict the same place or not, in a binary fashion. In practice, images of a certain place can be taken from different positions, i.e. with a different camera pose, and thus share only a part of their visual cues (or surface in 3D). In existing datasets, two images are usually labeled to be of the same place (positive) if they are taken within a predefined range (usually 25m) computed using e.g.
GPS metadata. This creates ambiguous cases. For instance,
Figure 1 shows a reference image (a) of a place and two other pictures taken 6m (b) and 25.6m (c) away from its position. The images are respectively labeled as positive and negative match, although they share many visual cues (e.g. the building on the right). Binary labels are thus noisy and interfere with the training of VPR networks, that usually stall in local minima. To address this, resource- and time-costly hard pair mining strategies are used to compose the training batches. For example, training NetVLAD [2] on the Mapillary Street Level Sequences (MSLS) dataset [45] can take more than 20 days on an Nvidia v100 gpu due to the complexity of pair mining. We instead build on the observation that two images depict the same place only to a certain degree of shared cues, namely a degree of similarity, and propose to embed this information in new continuous labels for existing datasets that can be used to reduce the effect of noise in the training of effective VPR methods.
In this paper we exploit camera pose metadata or 3D in-formation associated to image pairs as a proxy to estimate an
approximate degree of similarity (hereinafter, graded similar-ity) between images of the same place, and use it to relabel popular VPR datasets. Graded similarity labels can be used to pick easy- and hard-pairs and compose training batches without complex pair-mining, thus speeding-up the training of VPR networks and enabling an efficient use of data. Fur-thermore, we embed the graded similarity into a Generalized
Contrastive Loss (GCL) function that we use to train a VPR pipeline. The intuition behind this choice is that the update of network weights should not be equal for all training pairs, but rather be influenced by their similarity. The representa-tions of image pairs with larger graded similarity should be pushed together in the latent space more strongly than those of images with a lower graded similarity. The distance in the latent space is thus expected to be a better measure of ranking images according to their similarity, avoiding the use of expensive re-ranking to improve retrieval results. We validate the proposed approaches on several VPR benchmark datasets. To the best of our knowledge, this work is the first to use graded similarity for large-scale place recognition, and paying attention to data-efficient training.
We summarize the contributions of this work as:
• new labels for VPR datasets indicating the graded sim-ilarity of image pairs. We computed the labels with automatic methods that use camera pose metadata in-cluded with the images or 3D surface information;
• a generalized contrastive loss (GCL) that exploits graded similarity of image pairs to learn effective de-scriptors for VPR;
• an efficient VPR pipeline trained without hard-pair min-ing, and that does not require re-ranking. Training our pipeline with a VGG-16 backbone converges ∼ 100x faster than NetVLAD with the same backbone, achiev-ing higher VPR results on several benchmarks. The efficiency of our scheme enables training larger back-bones in a short time. 2.