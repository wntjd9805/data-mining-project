Abstract 1.

Introduction
We present Vid2Avatar, a method to learn human avatars from monocular in-the-wild videos. Reconstructing humans that move naturally from monocular in-the-wild videos is difficult. Solving it requires accurately separating humans from arbitrary backgrounds. Moreover, it requires recon-structing detailed 3D surface from short video sequences, making it even more challenging. Despite these challenges, our method does not require any groundtruth supervision or priors extracted from large datasets of clothed human scans, nor do we rely on any external segmentation mod-ules. Instead, it solves the tasks of scene decomposition and surface reconstruction directly in 3D by modeling both the human and the background in the scene jointly, parameter-ized via two separate neural fields. Specifically, we define a temporally consistent human representation in canonical space and formulate a global optimization over the back-ground model, the canonical human shape and texture, and per-frame human pose parameters. A coarse-to-fine sam-pling strategy for volume rendering and novel objectives are introduced for a clean separation of dynamic human and static background, yielding detailed and robust 3D hu-man reconstructions. The evaluation of our method shows improvements over prior art on publicly available datasets.
†Corresponding author
Being able to reconstruct detailed avatars from readily available “in-the-wild” videos, for example recorded with a phone, would enable many applications in AR/VR, in human-computer interaction, robotics and in the movie and sports industry. Traditionally, high-fidelity 3D reconstruc-tion of dynamic humans has required calibrated multi-view systems [9, 10, 19, 28, 32, 48, 52], which are expensive and require highly-specialized expertise to operate. In contrast, emerging applications such as the Metaverse require more light-weight and practical solutions in order to make the digitization of humans a widely available technology. Re-constructing humans that move naturally from monocular in-the-wild videos is clearly a difficult problem. Solving it requires accurately separating humans from arbitrary back-grounds, without any prior knowledge about the scene or the subject. Moreover it requires reconstructing detailed 3D surface from short video sequences, made even more chal-lenging due to depth ambiguities, the complex dynamics of human motion and the high-frequency surface details.
Traditional template-based approaches [15, 16, 61] can-not generalize to in-the-wild settings due to the requirement for a pre-scanned template and manual rigging. Methods that are based on explicit mesh representations are limited to a fixed topology and resolution [3, 8, 14, 36]. Fully-supervised methods that directly regress 3D surfaces from
images [17, 18, 21, 43, 44, 58, 72] struggle with difficult out-of-distribution poses and shapes, and do not always pre-dict temporally consistent reconstructions. Fitting neural implicit surfaces to videos has recently been demonstrated
[23, 25, 42, 49, 50, 55, 71]. However, these methods depend on pre-segmented inputs and are therefore not robust to un-controlled visual complexity and are upper-bounded in their reconstruction quality by the segmentation method.
In this paper, we introduce Vid2Avatar, a method to learn human avatars from monocular in-the-wild videos without requiring any groundtruth supervision or priors extracted from large datasets of clothed human scans, nor do we rely on any external segmentation modules. We solve the tasks of scene separation and surface reconstruction directly in 3D. To achieve this, we model both the foreground (i.e., human) and the background in the scene implicitly, param-eterized via two separate neural fields. A key challenge is to associate 3D points to either of these fields without re-verting to 2D segmentation. To tackle this challenge, our method builds-upon the following core concepts: i) We de-fine a single temporally consistent representation of the hu-man shape and texture in canonical space and leverage the inverse mapping of a parametric body model to learn from deformed observations. ii) A global optimization formu-lation jointly optimizes the parameters of the background model, the canonical human shape and its appearance, and the pose estimates of the human subject over the entire se-quence. iii) A coarse-to-fine sampling strategy for volume rendering that naturally leads to a separation of dynamic foreground and static background. iv) Novel objectives that further improve the scene decomposition and lead to sharp boundaries between the human and the background, even when both are in contact (e.g., around the feet), yielding better geometry and appearance reconstructions.
More specifically, we leverage an inverse-depth param-eterization in spherical coordinates [70] to coarsely sepa-rate the static background from the dynamic foreground.
Within the foreground sphere, we leverage a surface-guided volume rendering approach to attain densities via the con-version method proposed in [63]. Importantly, we warp all sampled points into canonical space and update the human shape field dynamically. To attain sharp boundaries be-tween the dynamic foreground and the scene, we introduce two optimization objectives that encourage a quasi-discrete binary distribution of ray opacities and penalize non-zero opacity for rays that do not intersect with the human. The final rendering of the scene is then attained by differentiable composited volume rendering.
We show that this optimization formulation leads to clean scene decomposition and high-quality 3D reconstruc-tions of the human subject. In detailed ablations, we shed light on the key components of our method. Furthermore, we compare to existing methods in 2D segmentation, novel view synthesis, and reconstruction tasks, showing that our method performs best across several datasets and settings.
To allow for quantitative comparison across methods, we contribute a novel semi-synthetic test set that contains ac-curate 3D geometry of human subjects. Finally, we demon-strate the ability to reconstruct different humans in detail from online videos and hand-held mobile phone video clips.
In summary, our contributions are:
• a method to reconstruct detailed 3D avatars from in-the-wild monocular videos via self-supervised scene decomposition; and
• to achieve robust and detailed 3D reconstructions of the human even under challenging poses and environ-ments without requiring external segmentation meth-ods; and
• a novel semi-synthetic testing dataset that for the first time allows comparing monocular human reconstruc-tion methods on realistic scenes. The dataset contains rich annotations of the 3D surface. 2.