Abstract
Knowledge distillation facilitates the training of a com-pact student network by using a deep teacher one. While this has achieved great success in many tasks, it remains completely unstudied for image-based 6D object pose esti-mation. In this work, we introduce the first knowledge dis-tillation method driven by the 6D pose estimation task. To this end, we observe that most modern 6D pose estimation frameworks output local predictions, such as sparse 2D key-points or dense representations, and that the compact stu-dent network typically struggles to predict such local quan-tities precisely. Therefore, instead of imposing prediction-to-prediction supervision from the teacher to the student, we propose to distill the teacher’s distribution of local pre-dictions into the student network, facilitating its training.
Our experiments on several benchmarks show that our dis-tillation method yields state-of-the-art results with different compact student models and for both keypoint-based and dense prediction-based architectures. 1.

Introduction
Estimating the 3D position and 3D orientation, a.k.a. 6D pose, of an object relative to the camera from a single 2D image has a longstanding history in computer vision, with many real-world applications, such as robotics, autonomous navigation, and virtual and augmented reality. Modern methods that tackle this task [7, 20, 21, 25, 28, 33, 40, 45, 47] all rely on deep neural networks. The vast majority of them draw their inspiration from the traditional approach, which consists of establishing correspondences between the object’s 3D model and the input image and compute the 6D pose from these correspondences using a Perspective-n-Point (PnP) algorithm [2, 23, 27, 42] or a learnable PnP network. Their main differences then lie in the way they extract correspondences. While some methods predict the 2D image locations of sparse 3D object keypoints, such as the 8 3D bounding box corners [19–21] or points on the ob-Figure 1. Student vs teacher keypoint predictions. The large backbone of the teacher allows it to produce accurate keypoints, indicated by tight clusters. By contrast, because of its more com-pact backbone, the student struggles to predict accurate keypoints when trained with keypoint-to-keypoint supervision. We therefore propose to align the student’s and teacher’s keypoint distributions. ject surface [33], others produce dense representations, such as 3D locations [7, 45] or binary codes [40], from which the pose can be obtained.
In any event, these methods rely on large models, which, while achieving impressive accuracy, are impractical de-ployment on embedded platforms and edge devices. As, to the best of our knowledge, no compact and efficient 6D pose estimation models have yet been proposed, a simple way to reduce the size of these networks consists of replacing their large backbones with much smaller ones. Unfortunately, this typically comes with a significant accuracy drop.
In this paper, we address this by introducing a knowledge dis-tillation strategy for 6D pose estimation networks.
Knowledge distillation aims to transfer information from a deep teacher network to a compact student one. The re-search on this topic has tackled diverse tasks, such as image classification [17, 37, 48], object detection [10, 11, 49] and semantic segmentation [14, 30]. While some techniques, such as feature distillation [15, 37, 48, 49], can in principle generalize to other tasks, no prior work has studied knowl-edge distillation in the context of 6D pose estimation.
In this paper, we introduce a knowledge distillation method for 6D pose estimation motivated by the follow-ing observations. In essence, whether outputting sparse 2D locations or dense representations, the methods discussed above all produce multiple local predictions. We then argue that the main difference between the local predictions made by a deep teacher network and a compact student one con-sists in the accuracy of these individual predictions. Fig-ure 1 showcases this for sparse keypoint predictions, ev-idencing that predicting accurate keypoint locations with keypoint-to-keypoint supervision is much harder for the stu-dent than for the teacher. We therefore argue that knowledge distillation for 6D pose estimation should be performed not by matching the individual local predictions of the stu-dent and teacher but instead by encouraging the student and teacher distributions of local predictions to become similar.
This leaves more flexibility to the student and thus facili-tates its training.
To achieve this, we follow an Optimal Transport (OT) formalism [44], which lets us measure the distance between the two sets of local predictions. We express this as a loss function that can be minimized using a weight-based variant of Sinkhorn’s algorithm [6], which further allows us to ex-ploit predicted object segmentation scores in the distillation process. Our strategy is invariant to the order and the num-ber of local predictions, making it applicable to unbalanced teacher and student predictions that are not in one-to-one correspondence. on the extensive
We validate the effectiveness of our approach by popular experiments conducting
LINEMOD [16], Occluded-LINEMOD [3] and YCB-V [47] datasets with the SOTA keypoint-based approach
WDRNet+. Our prediction distribution alignment strategy consistently outperforms both a prediction-to-prediction distillation baseline and the state-of-the-art feature distil-lation method [49] using diverse lightweight backbones and architecture variations. Interestingly, our approach is orthogonal to feature distillation, and we show that com-bining it with the state-of-the-art approach of [49] further boosts the performance of student network. To show the generality of our approach beyond keypoint prediction, we then apply it to the SOTA dense prediction-based method,
ZebraPose [40], to align the distributions of dense binary code probabilities. Our experiments evidence that this outperforms training a compact ZebraPose in a standard prediction-to-prediction knowledge distillation fashion.
Our main contributions can be summarized as follows. (i) We investigate for the first time knowledge distillation in the context of 6D pose estimation. (ii) We introduce an approach that aligns the teacher and student distribu-tions of local predictions together with their predicted ob-ject segmentation scores. (iii) Our method generalizes to both sparse keypoints and dense predictions 6D pose esti-mation frameworks. (iv) Our approach can be used in con-junction with feature distillation to further boost the stu-dent’s performance. Our code is available at https:// github.com/GUOShuxuan/kd-6d-pose-adlp. 2.