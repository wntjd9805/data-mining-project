Abstract
Over the last few years, with the advent of virtual and augmented reality, an enormous amount of research has been focused on modeling, tracking and reconstructing hu-man hands. Given their power to express human behavior, hands have been a very important, but challenging compo-nent of the human body. Currently, most of the state-of-the-art reconstruction and pose estimation methods rely on the low polygon MANO model. Apart from its low polygon count, MANO model was trained with only 31 adult sub-jects, which not only limits its expressive power but also imposes unnecessary shape reconstruction constraints on pose estimation methods. Moreover, hand appearance re-mains almost unexplored and neglected from the majority of hand reconstruction methods. In this work, we propose
“Handy”, a large-scale model of the human hand, mod-eling both shape and appearance composed of over 1200 subjects which we make publicly available for the benefit of the research community. In contrast to current models, our proposed hand model was trained on a dataset with large diversity in age, gender, and ethnicity, which tackles the limitations of MANO and accurately reconstructs out-of-distribution samples. In order to create a high quality texture model, we trained a powerful GAN, which preserves high frequency details and is able to generate high resolu-tion hand textures. To showcase the capabilities of the pro-posed model, we built a synthetic dataset of textured hands and trained a hand pose estimation network to reconstruct both the shape and appearance from single images. As it is demonstrated in an extensive series of quantitative as well as qualitative experiments, our model proves to be robust against the state-of-the-art and realistically captures the 3D hand shape and pose along with a high frequency detailed texture even in adverse “in-the-wild” conditions. 1.

Introduction
Humans express their emotions mainly using their fa-cial expressions and hands. Hand movements and poses are strong indicators of body language and can convey mean-ingful messages which can be key factors in human be-havioral analysis. For this, hands have been widely stud-ied in regard to their biometric applications [11, 40]. 3D hand models lead the technological developments of cru-cial tasks for virtual reality such as human hand tracking
Project page: https://github.com/rolpotamias/handy.
[19, 36, 42, 51] and pose estimation [18, 55]. Specifically, hand pose estimation algorithms utilize these models in or-der to reconstruct a subject’s hand from a monocular depth or RGB image. However, most of the current state-of-the-art methods on 3D hand reconstruction and pose estima-tion rely on low polygon models, with minimum diversity in terms of age, gender, and ethnicity and without any hand texture appearance [39].
In particular, MANO [39] is considered the most popu-lar hand model, which pioneered the construction of a para-metric human hand model. Apart from its low polygon res-olution (778 vertices), it is only composed of 31 subjects, which limits the accuracy of high fidelity 3D reconstruc-tion methods. A statistical model with such a low number of samples will always constrain the reconstruction of hand shapes of diverse age and ethnicity groups.
In the same context, despite the efforts of implementing strong pose priors to accurately constrain parametric models on valid hand poses [31], reconstruction methods are still dependent on a limited shape model. Importantly, current parametric models are constructed only by adults’ hand shapes in the age range of 20-60 years old, disregarding the shape vari-ations out of this age range. We experimentally show that children’s hands significantly differ in terms of shape from adults’ hands, which makes current shape models prone to reconstruction errors.
Additional to the shape component, a major limitation of current hand models is the absence of a high resolution tex-ture model. Despite the necessity in virtual and augmented reality for a personalized appearance reconstruction, there are only a few studies that attempted to model hand tex-ture along with shape and pose. In particular, current meth-ods on hand texture reconstruction from monocular images are constrained on limited demographic variations and low resolution textures that are ill-suited for real-world applica-tions [8–10, 37, 47]. Recently, HTML [37] proposed the largest available parametric texture model of the human hand composed of 51 subjects. Given that the texture com-ponent is based on Principal Component Analysis (PCA) of low resolution texture UV maps, the generated textures tend to be blurry, lacking the high frequency details of the hand. Low resolution textures not only limit the fidelity of RGB reconstructions but also the generations of realistic synthetic data. Currently, state-of-the-art hand-object de-tection methods [3, 16, 49] train their models on synthetic datasets with low resolution textures such as HTML or ver-tex colors, which subsequently constrain the quality of the resulting reconstructions.
In this study, we propose “Handy”, the first large-scale parametric shape and texture hand model composed of over 1200 subjects. Given these high resolution textured scans with large demographic, gender, and age variations, we built a high resolution hand model that overcomes the shape lim-itations of previous state-of-the-art models. To the best of our knowledge, this is the first hand model that captures subjects with ages from 1 to 81 years old. The scans come with high resolution textures which enable the creation of a highly detailed texture model. In contrast to HTML [37], we built a high resolution texture model by using a style-based GAN which allows modeling high frequency details of the human hand (e.g., wrinkles, veins, nail polish). Under a series of experiments we show that the proposed paramet-ric model overcomes the limitations of previous methods and we present the first, to the best of our knowledge, high fidelity texture reconstruction method from single “in-the-wild” images.
In particular, besides the success of 3D hand reconstruc-tion from monocular depth and RGB images, there are cur-rently only a few methods that are able to reconstruct the pose along with the shape and texture components. Exist-ing 3D hand datasets only contain hand annotations in terms of pose and global rotation and they usually neglect hand shape variations by modeling only a mean hand shape. Ad-ditionally, the lack of ground-truth high resolution texture maps limits current hand reconstruction methods to prop-erly predict the appearance of a given hand. To enable tex-ture modeling, we follow the trend of synthetic data gen-eration, and we have built a large-scale dataset containing annotations in terms of pose, shape, and texture informa-tion. To summarize, the contributions of our work are the following:
• We make publicly available a large-scale shape and ap-pearance model of the human hand for the benefit of the research community, built by over 1200 3D hands scans with a large diversity in age, gender, and ethnic-ity.
• We create a synthetic dataset for monocular 3D hand reconstruction given our high fidelity hand model and make it publicly available. As shown in the experi-mental section, our synthetic dataset aid off-the-shelf reconstruction methods to improve results.
• We present a high fidelity appearance reconstruction method from monocular images which is able to recon-struct high frequency details such as wrinkles, veins, nail polish, etc. 2.