Abstract
We present a simple approach which can turn a ViT en-coder into an efﬁcient video model, which can seamlessly work with both image and video inputs. By sparsely sam-pling the inputs, the model is able to do training and in-ference from both input modalities. The model is easily scalable and can be adapted to large-scale pre-trained ViTs without requiring full ﬁnetuning. The model achieves SOTA results1. 1.

Introduction
Visual Transformers (ViT) [9] have been an ubiqui-tous backbone for visual representation learning, leading to many advances in image understanding [43, 54, 66], mul-timodal tasks [1, 2, 62, 65] and self-supervised learning
[5, 14, 45], etc. However, adaptations to video are both challenging and computationally intensive, so video ver-sions have been been specially designed to handle the larger number of frames, for example, ViViT [3], MultiView [61],
TimeSFormer [6] and others [12].
Video understanding is an essential computer vision task, and a large number of successful video architectures have been developed [8, 13, 15, 28, 39, 46, 55, 60]. Previ-ous video 3D CNNs [8, 46] were designed to handle videos by learning spatio-temporal information; they often borrow from mechanisms for learning on images, for example [8] use pre-trained image CNN weights by inﬂating the kernels to 3D. However, once adapted to videos, these kernels are no longer applicable to images.
Furthermore, most previous works treat image and video as entirely different inputs, providing independent methods for either videos or images, since designing a model ca-pable of handling both is challenging. At the same time, image and video inputs are inherently related and a single visual backbone should be able to handle either or both in-puts. Previous methods for co-training image and video
[4,25,51,67] adapt the architectures to do so with signiﬁcant 1https://sites.google.com/view/tubevit
Figure 1. TubeViT: With Sparse Video Tubes, Vision Transform-ers (ViTs) use both image and video inputs, providing an efﬁcient video backbone and more accurate performance. portions of the network designed for each input. Works such as Perceiver [19] and Flamingo [2] address this by resam-pling the input and compressing it into a ﬁxed number of features. However, this resampling can still be expensive for long videos, and, in the case of Flamingo, it treats videos as individual frames sampled at 1 FPS, which limits the tem-poral information. Such low FPS sampling and per-frame modeling would often be insufﬁcient for datasets which rely on motion and temporal understanding, e.g., Something-Something [18], or for recognizing quick and short actions.
On the other hand, using one of the above-mentioned ap-proaches with dense frames is computationally infeasible.
To address these limitations, we propose a simple but ef-fective model, named TubeViT, to utilize a standard ViT model seamlessly for both image and videos. We intro-duce Sparse Video Tubes, a lightweight approach for joint image and video learning. Our method works by sparsely sampling various sized 3D space-time tubes from the video to generate learnable tokens, which are used by the vision transformer (Figure 1). With sparse video tubes, the model is easily applicable to either input, and can better leverage either or both sources of data for training and ﬁne-tuning.
The sparse video tubes naturally handle raw video signals and image signals which is crucial to understanding actions and other spatio-temporal information in videos.
Video models are also expensive to train, and previous works have studied ways to leverage already trained mod-els, such as using frozen ones [27] or adapting them to videos [31]. We expand on these ideas, and use the Sparse
Video Tubes to adapt much larger ViT models to videos with lightweight training (Sec. 3.6). Thus we create power-ful large video models with less resources.
We evaluate the approach across many standard video datasets: Kinetics-400, Kinetics-600, Kinetics-700, and
SomethingSomething V2, outperforming the state-of-the-art (SOTA). Our methods are trained from scratch or on
ImageNet-1k and Kinetics datasets and outperform even methods additionally pre-trained from very large datasets (e.g., JFT [44]). Our work also outperforms models target-ing video pretraining, such as recent video Masked Auto-Encoder (MAE) works [14, 45].
Our key ﬁndings are that by using the sparse video tubes, we are able to better share the weights learned for both im-ages and videos. This is in contrast to prior works that either inﬂate kernels or add new temporal-speciﬁc layers.
Further, due to the sparse sampling, the number of tokens remains low, which we also ﬁnd is important, both for re-ducing FLOPs and improving performance.
Our contribution is construction of sparse video tubes, obtained by sparsely sampling videos with various sized 3D space-time tubes. With that we accomplish the following: (1) a universal visual backbone which easily adapts a ViT architecture to videos; (2) joint image and video under-standing which seamlessly uses either input; (3) an easy-to-scale approach for video understanding, which can also leverage already trained (large) ViT models. 2.