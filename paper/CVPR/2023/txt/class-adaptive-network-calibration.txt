Abstract
Recent studies have revealed that, beyond conventional accuracy, calibration should also be considered for train-ing modern deep neural networks. To address miscalibra-tion during learning, some methods have explored different penalty functions as part of the learning objective, along-side a standard classiﬁcation loss, with a hyper-parameter controlling the relative contribution of each term. Never-theless, these methods share two major drawbacks: 1) the scalar balancing weight is the same for all classes, hinder-ing the ability to address different intrinsic difﬁculties or imbalance among classes; and 2) the balancing weight is usually ﬁxed without an adaptive strategy, which may pre-vent from reaching the best compromise between accuracy and calibration, and requires hyper-parameter search for each application. We propose Class Adaptive Label Smooth-ing (CALS) for calibrating deep networks, which allows to learn class-wise multipliers during training, yielding a powerful alternative to common label smoothing penalties.
Our method builds on a general Augmented Lagrangian approach, a well-established technique in constrained opti-mization, but we introduce several modiﬁcations to tailor it for large-scale, class-adaptive training. Comprehensive eval-uation and multiple comparisons on a variety of benchmarks, including standard and long-tailed image classiﬁcation, se-mantic segmentation, and text classiﬁcation, demonstrate the superiority of the proposed method. The code is available at https://github.com/by-liu/CALS . 1.

Introduction
Deep Neural Networks (DNNs) have become the prevail-ing model in machine learning, particularly for computer vision [13] and natural language processing applications [44].
Increasingly powerful architectures [3,13,24], learning meth-ods [4, 12] and a large body of other techniques [15, 27] are constantly introduced. Nonetheless, recent studies [11, 31] have shown that regardless of their superior discriminative
*Equal Contributions. Correspondence to: {liubingyuan1988@ gmail.com, jerome.rony.1@etsmtl.net} performance, high-capacity modern DNNs are poorly cali-brated, i.e. failing to produce reliable predictive conﬁdences.
Speciﬁcally, they tend to yield over-conﬁdent predictions, where the probability associated with the predicted class overestimates the actual likelihood. Since this is a critical is-sue in safety-sensitive applications like autonomous driving or computational medical diagnosis, the problem of DNN calibration has been attracting increasing attention in recent years [11, 31, 38].
Current calibration methods can be categorized into two main families. The ﬁrst family involves techniques that per-form an additional post-processing parameterized operation on the output logits (or pre-softmax activations) [11], with the calibration parameters of that operation obtained from a validation set by either learning or grid-search. Despite the simplicity and low computational cost, these methods have empirically proven to be highly effective [8, 11]. However, their main drawback is that the choice of the optimal cali-bration parameters is highly sensitive to the trained model instance and validation set [22, 31].
The second family of methods attempts to simultaneously optimize for accuracy and calibration during network train-ing. This is achieved by introducing, explicitly or implicitly, a secondary optimization goal involving the model’s predic-tive uncertainty, alongside the main training objective. As a result, a scalar balancing hyper-parameter is required to tune the relative contribution of each term in the overall loss function. Some examples of this type of approaches include:
Explicit Conﬁdence Penalty (ECP) [38], Label Smoothing (LS) [32], Focal Loss (FL) [21] and its variant, Sample-Dependent Focal Loss (FLSD) [31]. It has been recently demonstrated in [22] that all these methods can be formu-lated as different penalty terms that enforce the same equal-ity constraint on the logits of the DNN: driving the logit distances towards zero. Here, logit distances refers to the vector of L1 distances between the highest logit value and the rest. Observing the non-informative nature of this equality constraint, [22] proposed to use a generalized inequality con-straint, only penalizing those logits for which the distance is larger than a pre-deﬁned margin, achieving state-of-the-art calibration performance on many different benchmarks.
Although learning based methods achieve greater calibra-Figure 1. Many techniques have been proposed for jointly improving accuracy and calibration during training [11,31], but they fail to consider uneven learning scenarios like high class imbalance or long-tail distributions. We show a comparison of the proposed CALS-ALM method and different learning approaches in terms of Calibration Error (ECE) vs Accuracy on the (a) ImageNet and (b) ImageNet-LT (long-tailed
ImageNet) datasets. A lower ECE indicates better calibration: a better model should attain high ACC and low ECE. Among all the considered methods, CALS-ALM shows superior performance when considering both discriminative power and well-balanced probabilistic predictions, achieving best accuracy and calibration on ImageNet, and best calibration and second best accuracy on ImageNet-LT. tion performance [22, 31], they have two major limitations: 1) The scalar balancing weight is equal for all classes. This hinders the network performance when some classes are harder to learn or less represented than others, such as in datasets with a large number of categories (ImageNet) or considerable class imbalance (ImageNet-LT). 2) The balanc-ing weight is usually ﬁxed before network optimization, with no learning or adaptive strategy throughout training. This can prevent the model from reaching the best compromise between accuracy and calibration. To address the above is-sues, we introduce Class Adaptive Label Smoothing method based on an Augmented Lagrangian Multiplier algorithm, which we refer to as CALS-ALM. Our Contributions can be summarized as follows:
• We propose Class Adaptive Label Smoothing (CALS) for network calibration. Adaptive class-wise multipliers are introduced instead of the widely used single balancing weight, which addresses the above two issues: 1) CALS can handle a high number of classes with different intrinsic difﬁculties, e.g. ImageNet; 2) CALS can effectively learn from data suffering from class imbalance or a long-tailed distribution, e.g. ImageNet-LT.
• Different from previous penalty based methods, we solve the resulting constrained optimization problem by imple-menting a modiﬁed Augmented Lagrangian Multiplier (ALM) algorithm, which yields adaptive and optimal weights for the constraints. We make some critical de-sign decisions in order to adapt ALM to the nature of modern learning techniques: 1) The inner convergence cri-terion in ALM is relaxed to a ﬁxed number of iterations in each inner stage, which is amenable to mini-batch stochas-tic gradient optimization in deep learning. 2) Popular techniques, such as data augmentation, batch normaliza-tion [15] and dropout [10], rule out the possibility of track-ing original samples and applying sample-wise multipliers.
To overcome this complication, we introduce class-wise multipliers, instead of sample-wise multipliers in the stan-dard ALM. 3) The outer-step update for estimating optimal
ALM multipliers is performed on the validation set, which is meaningful for training on large-scale training set and avoids potential overﬁtting.
• Comprehensive experiments over a variety of applications and benchmarks, including standard image classiﬁcation (Tiny-ImageNet and ImageNet), long-tailed image clas-siﬁcation (ImageNet-LT), semantic segmentation (PAS-CAL VOC 2012), and text classiﬁcation (20 Newsgroups), demonstrate the effectiveness of our CALS-ALM method.
As shown in Figure 1, CALS-ALM yields superior per-formance over baselines and state-of-the-art calibration losses when considering both accuracy and calibration, es-pecially for more realistic large-scaled datasets with large number of classes or class imbalance. 2.