Abstract
Transformers yield state-of-the-art results across many tasks. However, their heuristically designed architecture impose huge computational costs during inference. This work aims on challenging the common design philosophy of the Vision Transformer (ViT) model with uniform dimension across all the stacked blocks in a model stage, where we redistribute the parameters both across transformer blocks and between different structures within the block via the first systematic attempt on global structural pruning. Deal-ing with diverse ViT structural components, we derive a novel Hessian-based structural pruning criteria comparable across all layers and structures, with latency-aware regu-larization for direct latency reduction. Performing iterative pruning on the DeiT-Base model leads to a new architec-ture family called NViT (Novel ViT), with a novel parameter redistribution that utilizes parameters more efficiently. On
ImageNet-1K, NViT-Base achieves a 2.6× FLOPs reduction, 5.1× parameter reduction, and 1.9× run-time speedup over the DeiT-Base model in a near lossless manner. Smaller
NViT variants achieve more than 1% accuracy gain at the same throughput of the DeiT Small/Tiny variants, as well as a lossless 3.3× parameter reduction over the SWIN-Small model. These results outperform prior art by a large margin.
Further analysis is provided on the parameter redistribution insight of NViT, where we show the high prunability of ViT models, distinct sensitivity within ViT block, and unique parameter distribution trend across stacked ViT blocks. Our insights provide viability for a simple yet effective parameter redistribution rule towards more efficient ViTs for off-the-shelf performance boost. 1.

Introduction
Transformer models demonstrate high model capacity, easy scalability, and superior ability in capturing long-range dependency [1, 9, 19, 30, 38]. Vision Transformer, i.e., the
ViT [12], shows that embedding image patches into tokens and passing them through a sequence of transformer blocks
*Work done during an internship at NVIDIA. can lead to higher accuracy compared to state-of-the-art
CNNs. DeiT [35] further presents a data-efficient training method such that acceptable accuracy can be achieved with-out extensive pretraining. Offering competitive performance to CNNs under similar training regimes, transformers now point to the appealing perspective of solving both NLP and vision tasks with the same architecture [18, 20, 49].
Unlike CNNs built with convolutional layers that con-tain few dimensions like the kernel size and the number of filters, the ViT has multiple distinct components, i.e.,
QKV projection, multi-head attention, multi-layer percep-tron, etc. [38], each defined by independent dimensions. As a result, the dimension of each component in each ViT block needs to be carefully designed to achieve a decent trade-off between efficiency and accuracy. However, this is typically not the case for state-of-the-art models. Models such as
ViT [12] and DeiT [35] mainly inherit the design heuristics from NLP tasks, e.g., use MLP expansion ratio 4, fix QKV per head, all the blocks having the same dimensions, etc., which may not be optimal for computer vision [4], caus-ing significant redundancy in the base model and a worse efficiency-accuracy trade-off upon scaling, as extensively shown empirically. New developments in ViT architectures incorporate additional design tricks like multi-stage archi-tecture [41], more complicated attention schemes [23], and additional convolutional layers [13] etc., yet no attempt has been made on understanding the potential of redistributing parameters within the stacked vision transformer blocks.
This work targets efficient ViTs by exploring parameter redistribution within ViT blocks and across multiple layers of cascading ViT blocks. To this end, we start with the straight-forward DeiT design space, with only ViT blocks. We ana-lyze the importance and redundancy of different components in the DeiT model via latency-aware global structural prun-ing, leveraging the insights to redistribute parameters for enhanced accuracy-efficiency trade-off. Our approach, as visualized in Fig. 1, starts from analyzing the blocks in the computation graph of ViT to identify all the dimensions that can be independently controlled. We apply global structural pruning over all the components in all blocks. This offers complete flexibility to explore their combinations towards
Figure 1. Towards efficient vision transformer models. Starting form ViT, specifically DeiT, we identify the design space of pruning (i) embedding size E, (ii) number of head H, (iii) query/key size QK, (iv) value size V and (v) MLP hidden dimension M in Sec. 3.1. Then we utilize a global ranking of latency-aware importance score to perform iterative global structural pruning in Sec. 3.2, achieving pruned NViT models. Finally we analyze the parameter redistribution trend of all the components in the NViT model, as in Sec. 5.1. an optimal architecture in a complicated design space. Per-forming global pruning on ViT is significantly challenging, given the diverse structural components and significant mag-nitude differences. Previous methods only attempts on per-component pruning with the same pruning ratio [5], which cannot lead to parameter redistribution across components and blocks. We derive a new importance score based on the
Hessian matrix norm of the loss for global structural pruning, for the first time offering comparability among all prunable components. Furthermore, we incorporate the estimated la-tency reduction into the importance score. This guides the final pruned architecture to be faster on target devices.
The iterative structural pruning of the DeiT-Base model enables a family of efficient ViT models: NViT. On the
ImageNet-1K benchmark [33], NViT enables a nearly loss-less 5.14× parameter reduction, 2.57× FLOPs reduction and 1.86× speed up on V100 GPU over the DeiT-Base model. An 1% and 1.7% accuracy gain is observed over
DeiT-Small and DeiT-Tiny models when we scale down the
NViT to a similar latency. NViT achieves a further 1.8×
FLOPs reduction and an 1.5× speedup over NAS-based Aut-oFormer [4] (ICCV’21) and the SOTA structural pruning method S2ViTE [5] (NeurIPS’21).The efficiency and perfor-mance benefit of NViT trained on ImageNet also transfers to downstream classification and segmentation tasks.
Using structural pruning for architectural guidance, we further make an important observation that the popular uni-form distribution of parameters across all layers is, in fact, not optimal. To this end, we present further empirical and theoretical analysis on the new parameter distribution rule of efficient ViT architectures, which provides a new angle on understanding the learning dynamic of vision transformer model. We believe our findings would inspire future design of efficient ViT architectures.
Our main contributions are as follows:
• Propose NViT, a novel hardware-friendly global struc-tural pruning algorithm enabled by a latency-aware,
Hessian-based importance-based criteria and tailored towards the ViT architecture, achieving a nearly loss-less 1.9× speedup, significantly outperforms SOTA ViT compression methods and efficient ViT designs;
• Provide a systematic analysis on the prunable compo-nents in the ViT model. We perform structural pruning on the embedding dimension, number of heads, MLP hidden dimension, QK dimension and V dimension of each head separately;
• Explore hardware-friendly parameter redistribution of
ViT, finding high prunability of ViT models, distinct sensitivity within ViT block, and unique parameter distribution trend across stacked ViT blocks. 2.