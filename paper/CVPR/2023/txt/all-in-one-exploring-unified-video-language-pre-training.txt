Abstract
Mainstream Video-Language Pre-training (VLP) mod-els [10, 26, 64] consist of three parts, a video encoder, a text encoder, and a video-text fusion Transformer. They pursue better performance via utilizing heavier unimodal encoders or multimodal fusion Transformers, resulting in increased parameters with lower efficiency in downstream tasks. In this work, we for the first time introduce an end-to-end VLP model, namely all-in-one Transformer, that em-beds raw video and textual signals into joint representations using a unified backbone architecture. We argue that the unique temporal information of video data turns out to be a key barrier hindering the design of a modality-agnostic
Transformer. To overcome the challenge, we introduce a novel and effective token rolling operation to encode tem-poral representations from video clips in a non-parametric manner. The careful design enables the representation learning of both video-text multimodal inputs and unimodal inputs using a unified model. Our pre-trained all-in-one
Transformer is transferred to various downstream video-text tasks after fine-tuning, including text-video retrieval, video-question answering, multiple choice and video cap-tioning. State-of-the-art performances with the minimal model FLOPs on ten datasets demonstrate the superior-ity of our method compared to the competitive counter-parts. The code and pretrained models are available at https://github.com/showlab/all-in-one. 1.

Introduction
Science advances rather steadily for most of the time, but sometimes has a disruptive episode, where “an older paradigm is replaced in whole or in part by an incompat-ible new one.” [24] In this regard, Video-Language Pre-training (VLP) models have recently experienced steady progress, where joint representations are generally pro-duced with a multimodal fusion network after extracting
*Corresponding Author.
Figure 1. Compare to mainstream video-language pre-training methods. (a). Conventional methods [3, 10, 26, 64] use deep fea-tures from separate encoders before fusion. The fusion layer can be light [3] or heavy [10,26,64]. (b). Ours All-in-one Transformer learns video and text joint representations end-to-end from their raw inputs. We also support fast retrieval by feeding unimodal in-puts during inference. (c). Comparison of FLOPs and retrieval performance on MSRVTT [53]. Our All-in-one brings excellent results with modest computational cost. the visual and language features through unimodal encoders
[10,26,50,64]. We are here to break it and replace them with
“an incompatible new one” that has NO unimodal encoders.
The pre-train and then fine-tune scheme, has become a standard paradigm to learn transferable video-language representations for a wide range of downstream video-text tasks [6, 15, 52, 52, 53, 61]. Mainstream methods attempt to boost the pre-training in two ways: i. adopting more expen-sive video/text encoders to obtain more powerful unimodal features [3, 10] ii. designing heavier fusion networks to en-hance the association between modalities [61, 64].
Instead of following these trends, we fundamentally re-think design decisions and develop the simplest and most lightweight architecture that learns video-language repre-sentations from their raw inputs in an end-to-end manner.
Our model does not need any unimodal encoders (e.g., ob-ject detector in [64] or ResNet visual encoder in [26]) or complex fusion layers, but embeds visual and text signals in a unified manner, termed as All-in-one Transformer in our paper. Our design is inspired by recent studies [1, 21, 37]
that perform multimodal pre-training under the presump-tion that Transformer can process visual data in the same way as it processes text. However, our work is not the straightforward application of them. It is not trivial how to embed videos for our unified Transformer due to the unique challenge of modeling temporal information without adding much computational cost. temporal
Existing works model information by de-signing temporal attention layers [3] or using temporal-aware visual encoders (e.g., 3D convnets in [64] or Video
Swin [36] in [10]). We cannot simply use them in our unified All-in-one Transformer because they are modality-dependent and computationally too expensive. To address this issue, we design a novel, effective, and efficient method to model temporal information. Our model only needs three frames per video clip, which is much lower than other mod-els (e.g., 16 [26] or 32 [1]) but can achieve the comparable performance to them. Nevertheless, we are still not satis-fied with the computational cost in the self-attention layer.
To further reduce the computational cost, we propose the temporal token rolling operation, which is a cyclic atten-tion between small proportions of the visual tokens in each frame (Fig. 3-right). This is much more efficient than a naive self-attention approach on flattened tokens (Fig. 3-bottom). Furthermore, our modality-agnostic design en-ables us to use our pre-trained model as a powerful uni-modal feature extractor by feeding only video or text in-puts. This can significantly reduce the computational cost for retrieval task because we can simply compute the co-sine similarity of texts and videos soon after the pretraining, eliminating the need for training additional fusion module of projecting the disjoint text and visual features into a com-mon space (Fig. 1-b). Taken together, our All-in-one archi-tecture achieves much less FLOPs and better text-to-video performance than previous work (Fig. 1-c), despite the fact that we use the same pre-training objectives [10, 26].
Contributions. (1) We introduce the simplest, most lightweight, and most efficient video-language model, namely All-in-one Transformer, which is the first to capture video-language representations from the raw visual and tex-tual signals end-to-end in a unified backbone architecture. (2) We elucidate and tackle the difficulties of applying a unified and shared backbone for multimodal video and text data, that is, how to properly process the unique temporal information of videos. A novel temporal token rolling op-eration is proposed to capture the temporal representations of sparsely sampled frames without any extra parameters or increasing time complexity. (3) We propose a success prac-tical to overcome the slow retrieval of one-stream model and explore how to cotrain the image and video data together in better ways. (4) Comprehensive experiments on five down-stream video-text tasks of eleven datasets fully demonstrate the superiority of our pre-trained All-in-one Transformer on both effectiveness and efficiency compared to recent main-stream methods [3, 10, 26]. 2.