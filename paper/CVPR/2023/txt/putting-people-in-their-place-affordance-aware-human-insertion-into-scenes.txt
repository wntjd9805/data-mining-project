Abstract
We study the problem of inferring scene affordances by presenting a method for realistically inserting people into scenes. Given a scene image with a marked region and an image of a person, we insert the person into the scene while respecting the scene affordances. Our model can infer the set of realistic poses given the scene context, re-pose the reference person, and harmonize the composition. We set up the task in a self-supervised fashion by learning to re-pose humans in video clips. We train a large-scale diffusion model on a dataset of 2.4M video clips that produces diverse plausible poses while respecting the scene context. Given the learned human-scene composition, our model can also hal-lucinate realistic people and scenes when prompted without conditioning and also enables interactive editing. A quan-titative evaluation shows that our method synthesizes more realistic human appearance and more natural human-scene interactions than prior work. 1.

Introduction
A hundred years ago, Jakob von Uexküll pointed out the crucial, even defining, role that the perceived environment (umwelt) plays in an organism’s life [64]. At a high level, he argued that an organism is only aware of the parts of the environment that it can affect or be affected by. In a sense, our perception of the world is defined by what kinds of interactions we can perform. Related ideas of functional visual understanding (what actions does a given scene afford an agent?) were discussed in the 1930s by the Gestalt psy-chologists [35] and later described by J.J. Gibson [21] as affordances. Although this direction inspired many efforts in vision and psychology research, a comprehensive com-putational model of affordance perception remains elusive.
The value of such a computational model is undeniable for future work in vision and robotics research.
The past decade has seen a renewed interest in such computational models for data-driven affordance percep-tion [15, 20, 24, 25, 67]. Early works in this space deployed a
Project page: https : / / sumith1896 . github . io / affordance-insertion. mediated approach by inferring or using intermediate seman-tic or 3D information to aid in affordance perception [24], while more recent methods focus on direct perception of affordances [15, 20, 67], more in line with Gibson’s fram-ing [21]. However, these methods are severely constrained by the specific requirements of the datasets, which reduce their generalizability.
To facilitate a more general setting, we draw inspiration from the recent advances in large-scale generative models, such as text-to-image systems [49,50,54]. The samples from these models demonstrate impressive object-scene compo-sitionality. However, these compositions are implicit, and the affordances are limited to what is typically captured in still images and described by captions. We make the task of affordance prediction explicit by putting people “into the picture” [24] and training on videos of human activities.
We pose our problem as a conditional inpainting task (Fig. 1). Given a masked scene image (first row) and a ref-erence person (first column), we learn to inpaint the person into the masked region with correct affordances. At training time, we borrow two random frames from a video clip, mask one frame, and try to inpaint using the person from the sec-ond frame as the condition. This forces the model to learn both the possible scene affordances given the context and the necessary re-posing and harmonization needed for a co-herent image. At inference time, the model can be prompted with different combinations of scene and person images. We train a large-scale model on a dataset of 2.4M video clips of humans moving in a wide variety of scenes.
In addition to the conditional task, our model can be prompted in different ways at inference time. As shown in the last row Fig. 1, when prompted without a person, our model can hallucinate a realistic person. Similarly, when prompted without a scene, it can also hallucinate a realistic scene. One can also perform partial human completion tasks such as changing the pose or swapping clothes. We show that training on videos is crucial for predicting affordances and present ablations and baseline comparisons in Sec. 4.
To summarize, our contributions are:
• We present a fully self-supervised task formulation for learning affordances by learning to inpaint humans in
Figure 1. Given a masked scene image (first row) and a reference person (first column), our model can successfully insert the person into the scene image. The model infers the possible pose (affordance) given the scene context, reposes the person appropriately, and harmonizes the insertion. We can also partially complete a person (last column) and hallucinate a person (last row) when no reference is given. masked scenes.
• We present a large-scale generative model for human insertion trained on 2.4M video clips and demonstrate improved performance both qualitatively and quantita-tively compared to the baselines.
• In addition to conditional generation, our model can be prompted in multiple ways to support person hallucina-tion, scene hallucination, and interactive editing. 2.