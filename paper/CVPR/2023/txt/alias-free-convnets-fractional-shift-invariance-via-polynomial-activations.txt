Abstract
Although CNNs are believed to be invariant to transla-tions, recent works have shown this is not the case due to aliasing effects that stem from down-sampling layers. The existing architectural solutions to prevent the aliasing ef-fects are partial since they do not solve those effects that originate in non-linearities. We propose an extended anti-aliasing method that tackles both down-sampling and non-linear layers, thus creating truly alias-free, shift-invariant
CNNs1. We show that the presented model is invariant to integer as well as fractional (i.e., sub-pixel) translations, thus outperforming other shift-invariant methods in terms of robustness to adversarial translations. 1.

Introduction
Convolutional Neural Networks (CNNs) are the most common model in the image classification field. They were originally intended to have two properties: 1. Shift-invariant output: when we spatially translate the input image, their output does not change. 2. Shift-equivariant representation: when we spatially translate the input image, their internal representation translates in the same way.
Both these properties are thought to be beneficial for gener-alization (i.e., they are useful inductive biases), as we expect the image class not to change by an image translation, and its features to shift together with the image. Moreover, with-out the first property, the CNN might become vulnerable to adversarial attacks using image translations. Such attacks are real threats since they are very simple to execute in a
“black-box” setting (where we do not know anything about the CNN). For example, consider a person trying to fool a
CNN-based face scanner, by simply moving continuously until a face match is achieved.
It was commonly assumed that these useful properties were maintained since CNNs use only shift-equivariant op-the convolution operation and component-wise erations: 1Our code is available at github.com/hmichaeli/alias free convnets/. non-linearities. However, CNN models typically also in-clude downsampling operations such as pooling and strided convolution. Unfortunately, these operations violate equiv-ariance, and this also leads to CNNs not being shift-invariant. Specifically, Azulay and Weiss [2] have shown that shifting an input image by even one pixel can cause the output probability of a trained classifier to change signifi-cantly. This vulnerability can be further exploited in adver-sarial attacks, lowering classifiers’ accuracy by more than 20% [8]. Later, Zhang [33] has shown that this problem-atic behavior stems from an aliasing effect, taking place in downsampling operations such as pooling and strided con-volutions, and non-linear operations on the downsampled signals.
Previous works have shown an improvement in CNN invariance to translations using partial solutions that re-duced aliasing. For example, Zhang [33] has suggested adding a low-pass filter before the downsampling opera-tions. This approach has been shown to reduce aliasing caused by downsampling, thus improving shift-invariance, as well as accuracy and noise robustness. Karras et al. [17] have addressed aliasing in the generator within generative adversarial networks (GANs). They have shown that with-out proper treatment, aliasing in GANs leads to a decou-pling of the high-frequency features (texture) from the low-frequency content (structure) in the generated images, thus limiting their applicability in smooth video generation. To alleviate this issue, Karras et al. [17] extended the low-pass filter approach and suggested a solution for the implicit aliasing caused by non-linearities. Their method wraps the component-wise non-linear operations by upsampling and downsampling layers in an attempt to mimic the effect of applying the non-linear operations in the continuous do-main, where they theoretically do not cause aliasing.
Yet, none of the previous solutions completely elimi-nates aliasing, thus their suggested CNN architectures are not guaranteed to be shift-invariant. A different approach to shift-invariant CNNs was suggested by Chaman and Dok-mani´c [4]. They have proposed to use downsampling oper-ations that dynamically choose the subsampling grid using
a shift-equivariant decision rule. Although it does not solve the aliasing problem, nor guarantees shift-equivariant rep-resentations, this approach enables the creation of CNNs whose outputs are completely invariant to integer circular shifts. However, this approach does not lead to invariance to subpixel shifts, which are common in real-world applica-tions. For example, consider a case where the CNN receives input from a camera with some finite resolution. If we con-tinuously shift the camera with respect to the scene, then the resulting shift in the CNN’s discretely sampled input would rarely be integer-valued.
Considering the anti-aliasing approach again, the prob-lem with the solution suggested by Karras et al. [17] is that the aliasing resulting from non-linearities that increase the signal’s bandwidth indefinitely (such as ReLU) can be avoided only when they are used in a continuous domain (i.e., with infinite resolution), which is impractical. How-ever, this problem can be solved by replacing such non-linearities with alternatives whose effect does not lead to an indefinite increase in the signal’s bandwidth — such as polynomials.
Polynomial activations Despite their ease of computa-tion, polynomials are not considered promising candidates for activation functions. The main practical reason for this is that polynomial activations have large (super-linear) mag-nitudes compared to standard activations (e.g., ReLU) and thus typically cause training instability (e.g., exploding gra-dients) [11]. There seems also to be a theoretical disad-vantage since shallow feedforward neural networks with polynomial activation functions are not universal approxi-mators [15]. However, this last issue may not be a serious disadvantage: Kidger and Lyons [18] have shown that feed-forward neural networks with polynomial activations can become universal approximators with sufficient depth — a regime more relevant for modern CNNs.
In addition, re-cent research [11] has shown that by using normalization to truncate the dynamic range of the pre-activations, the train-ing of Neural Networks with polynomial activations can be stabilized, and converge to reasonable results in simple im-age classification tasks (MNIST and CIFAR). Yet, there are still a few significant challenges in using polynomial acti-vations: First, to the best of our knowledge, they were not shown to achieve competitive performance (similar to stan-dard activations) on tasks of more realistic scales, such as
ImageNet. In addition, the normalization method for dy-namic range truncation causes the (truncated) polynomial to increase the signal’s bandwidth indefinitely, which is not suitable for aliasing-free CNNs. This normalization was shown to be crucial for convergence even in small tasks and it is reasonable to expect that is even more important for larger tasks.
Contributions
In this paper
• We propose the first Alias-Free Convnet (AFC).
• We prove the AFC has both shift-invariant outputs and shift-equivariant internal representations — even for fractional shifts, where previous models fail.
• We show how simple and easy “black-box” adversar-ial attacks built on fractional image translation can de-grade a CNN performance, even when the CNN is in-variant to integer shifts. In contrast, the AFC has certi-fied robustness to such attacks and superior test accu-racy in this regime.
• Specifically, the robustness of AFCs is certified for circular shifts and the ideal (Sinc) interpolation ker-nel. However, we show empirically that AFCs have improved robustness even with other types of transla-tions.
• Interestingly, our model relies on polynomial activa-tions, and we are the first to demonstrate competitive performance with such activations on ImageNet, to the best of our knowledge. 2. Methods
Let τ∆ : L2(R2) → L2(R2) be the translation operator, which shifts a continuous-domain two-dimensional signal by ∆ ∈ R2. An operator f : L2(R2) → L2(R2) is said to be shift-equivariant if it commutes with τ∆ for every ∆.
Namely, f (τ∆ (x)) = τ∆ (f (x)) for every x ∈ L2(R2) and every ∆ ∈ R2.
An operator f : L2(R2) → Rd is said to be shift-invariant if its output is invariant to translation of its input, i.e. f (τ∆ (x)) = f (x) .
The definitions of equivariance and invariance to transla-tions naturally transfer to discrete-domain signals in L2(Z2) and integer shifts ∆ ∈ Z2. To simplify notations, from now on we will not specify the domain over which operators are defined, and will also omit the subscript ∆ from τ , when-ever the meaning is clear from the context.
CNN architectures for classification commonly comprise a Feature Extractor, which is mainly composed of convolu-tion layers, and a Classifier, which is typically composed of a linear layer and a softmax activation. A sufficient con-dition for the model to be shift-invariant is that the Classi-fier be shift-invariant, and the Feature Extractor be shift-equivariant. This is because the composition of a shift-equivariant f and a shift-invariant g yields a shift-invariant function, as g (f (τ (x))) = g (τ (f (x))) = g (f (x)) .
For our discussion, we assume that the Classifier is shift-invariant as its inputs are the spatially-averaged channels.
However, the Feature Extractor part of CNNs commonly
includes also downsampling layers. The spatial dimensions of the output of such layers are smaller than the spatial di-mensions of their input. Therefore, for such layers, shift-equivariance is not a desired property. Indeed, when shift-ing an image by 2 pixels at the input of a layer that performs downsampling by a factor of 2, we expect the output image to shift by only 1 pixel, not 2. Even worse, when shift-ing an image by only 1 pixel, it is not clear how precisely the output should shift. In order to extend the discussion to include these networks, here we consider equivariance w.r.t. the continuous domain. To simplify the exposition, let us present the definitions for 1D signals, where ‘dis-crete’ and ‘continuous’ will refer to the signal index we use.
Namely, a discrete signal x[n] is defined over n ∈ Z while a continuous signal x(t) is defined over t ∈ R.
Definition 1 (Fractional translation for discrete signals)
Let x[n] be a discrete-domain signal and let ∆ ∈ R be a (possibly non-integer) shift. Then the translation operator
τ∆ is defined by τ∆(x)[n] = z(nT + ∆), where z(t) is the unique 1/2T -bandlimitted continuous-domain signal satisfying x[n] = z(nT ).
Note that the uniqueness of z(t) in Definition 1 is guar-anteed by the Nyquist theorem. It is also easily verified that this definition does not depend on T . Equipped with this definition, we can define the following.
Definition 2 (shift-equivariance w.r.t. the cont. domain)
An operator f operating on discrete signals is said to be shift-equivariant w.r.t. the continuous domain if it commutes with fractional shifts. Namely, f (τ∆(x)) = τ∆(f (x)) for every x ∈ L2(Z) and every ∆ ∈ R.
Similarly, we can define the following.
Definition 3 (shift-invariance w.r.t. the cont. domain)
An operator f operating on discrete signals is said to be shift-invariant w.r.t. the continuous domain if it is invariant to fractional shifts of its input. Namely, f (τ∆(x)) = f (x) for every x ∈ L2(Z) and every ∆ ∈ R.
An important observation is the following.
Proposition 1 In a network comprised of a Feature Ex-tractor and a Classifier, if the Feature Extractor ends with then shift-equivariance a global average pooling layer, w.r.t. the continuous domain of the Feature Extractor im-plies shift-invariance w.r.t. the continuous domain of the en-tire model.
Indeed, in this case, the Classifier’s input is only depen-dent on the average of the Feature Extractor, which is shift-invariant. The last statement stems from the fact that when shifting the input of an operator that is shift-equivariant w.r.t. the continuous domain, the output must be a faith-ful translated discrete representation of the same continu-ous signal. Namely, there exists some 1/2T -bandlimited continuous signal ˜f (t) such that f (x)[n] = ˜f (nT ) and f (τ (x))[n] = ˜f (nT + ∆). Thus, the averages of f (x)[n] and f (τ (x))[n] are both equal to the “DC component” of ˜f , and therefore must be equal.
In order to examine the property of equivariance w.r.t. continuous domain of CNNs, we shall look at the dis-crete signal that propagates in a CNN as a representation of a continuous signal, and at each layer as a representation of a continuous operation on the continuous signal. As shown by Karras et al. [17], aliasing in the discrete representation prevents shift-invariance of CNNs since it decouples the discrete signal from its continuous equivalent. In contrast, they have shown that alias-free operations preserve shift-equivariance w.r.t. continuous domain, and lead to shift-invariant CNNs. There, Karras et al. [17] have shown that convolutions and downsamplers which are properly treated using low-pass filters (LPFs), are indeed alias-free and thus shift-equivariant w.r.t. the continuous domain. In addition, they proposed a method to reduce the implicit aliasing of non-linearities which we describe next.
In the continuous domain, pointwise non-linearities may induce indefinitely high new frequencies. Applying a point-wise non-linearity in the discrete domain is equivalent to sampling a continuous signal after applying the pointwise non-linearity — which may break the Nyquist condition and cause aliasing. This implies that pointwise nonlinear-ities applied in the discrete domain are generally not shift-invariant w.r.t. the continuous domain. Using upsampling before the non-linearity may solve this problem since it in-creases the frequency support that does not cause aliasing.
However, this approach cannot generally prevent aliasing, since the new frequencies generated by non-linear opera-tions can be arbitrarily high. For example, the outputs of non-differentiable operations such as ReLU can have infi-nite support in the frequency domain, thus aliasing will be induced for every finite upsampling factor.
In this study, we propose replacing non-linear operations with a band-limited preserving alternative — polynomial functions. The proposed scheme for an aliasing-free poly-nomial function of degree d is defined in Algorithm 1. In this algorithm, Upsamplez performs upsampling by a fac-tor z (i.e. resampling the input continuous signal at a z× larger sampling frequency), LPFz is an ideal low-pass filter with cut-off z, Downsamplez performs downsampling by a factor z (i.e. dividing the sample frequency by z), and
Polyd(x) = d (cid:88) i=0 aixi . (1)
The practical implementations of the operations above are described in Section 3 and Appendix C. Our contribu-tion to the general framework that has been presented by
Karras et al. [17] is the usage of polynomial activations,
Algorithm 1 Alias-free polynomial activation
Inputs: x - input signal, Polyd - polynomial of degree d. xup ← Upsample d+1 ypoly ← Polyd (xup) yLPF ← LPF 2 y ← Downsample d+1 2
Output: y (yLPF) (ypoly) (x) d+1 2
Figure 1. ConvNeXt baseline architecture vs AFC modifica-tions. D-conv: depthwise convolution 7 × 7, P-Conv: pointwise convolution, Strided-conv: convolution 4 × 4, stride 4. LN: Layer
Norm, AF-LN: Alias free Layer Norm, Poly: Polynomial activa-tion. Up x2: Upsample x2, LPF: ideal LPF with cutoff 0.5, Down x2: Downsample x2. Detailed explanations about BlurPool, Poly and LPF-Poly activations can be found in Section 3. which extends the frequency bandwidth in a limited fash-ion, unlike other non-linearities. Hence, by using appropri-ate upsampling as in Algorithm 1, aliasing can be avoided, as described in Figure 2. Specifically, in Appendix F.1 we prove the following.
Proposition 2 The operator defined by Algorithm 1 is shift-equivariant w.r.t. the continuous domain.
By combining Proposition 1 and Proposition 2 with the shift-equivariance of the other layers (as described above), we conclude the network output is shift-invariant. Next, we describe the proposed process of non-linearities in the frequency domain, which is additionally demonstrated in
Figure 2. In the first step, the input x is upsampled, lead-ing to a contraction of its support in the frequency domain (Fig. 2(b)). Effectively, it expands the range of allowed new frequencies generated by the following non-linearity (Fig. 2(c1)). Then, a low-pass filter is applied in order to prevent aliasing in the following downsampling layer (Fig. 2(d1), (e1)). Overall, for an upsampling factor that is appropriate for the frequency expansion of the polyno-mial, the effective frequencies for the output are not being overlapped at any of the steps, thus aliasing is prevented.
However, in the case of non-linearities that do not preserve the band-limited property, upsampling cannot prevent the frequency overlap in Figure 2(c2). 3. Implementation
We propose an Alias-Free Convnet (AFC), based on the ConvNeXt architecture [19], which has been shown to achieve state-of-the-art results in image classification tasks.
We modify the layers which suffer from aliasing (as de-scribed in Fig. 1) so that the convnet is completely free of aliasing. The theoretical derivation in Section 2 assumes infinite-length discrete signals, hence cannot be directly ap-plied in practical systems. However, it can be naturally used by limiting the discussion to circular translations, which im-plies that the continuous signals are periodic. In this case, the theoretical results from Section 2 can be equivalently attained with finite-length signals using our following im-plementation.
Convolution We use circular convolutions to meet the pe-riodic signal assumption, as described above. This is practi-cally done by replacing zero padding with circular padding, similarly to Chaman and Dokmani´c [4].
BlurPool Similarly to the model presented by Zhang [33], we separate strided convolutions into linear convolution and downsampling operations. The downsampling operation is replaced by BlurPool, which applies sub-sampling after low-pass filtering. Instead of implementing a low-pass fil-ter using convolutions with custom fixed kernels, we imple-ment an “ideal low-pass filter” by truncating high frequen-cies in the Fourier domain. Specifically, we transform the input to the Fourier domain using Pytorch FFT kernel [23], zero out the relevant frequencies, and transform it back to the spatial domain. This is an efficient implementation of downsampling after applying multiplication with filter H 2D in DFT domain, which is defined as
H 2D = HH T , (2) where for stride s and spatial-domain size N × N , H is defined as
H[k] =


 1, 0 ≤ k < N 2s , 2s ≤ k ≤ 3N 0, N 2s , 3N 2s < k ≤ N − 1 . 1, (3)
A derivation of this filter can be found in Appendix G.
Activation function We replace the original GeLU acti-vation with a polynomial function of degree 2, whose coef-ficients are trainable parameters, per channel:
Poly2(x) = a0 + a1x + a2x2 . (4)
The coefficients {a0, a1, a2} are initialized by fitting this function to the GeLU, as proposed by Gottemukkula [11].
All activation functions are wrapped according to the alias-free technique presented in Algorithm 1. Generally, replac-ing the activation function in a Deep Neural Network may
Figure 2. A demonstration of the proposed non-linearities in the frequency domain. The top plot at each panel represents the signal in the continuous domain, and the bottom represents the discrete domain. Where the input (a) is upsampled it shrinks its frequency response, expanding the allowed frequencies (b). Applying the polynomial activation expands the frequency response support by as factor d, without causing aliasing in the relevant frequencies (c1). Thus, the discrete signal remains a faithful representation of the continuous signal after applying LPF (d1) and downsample back to the same spatial size (d2). However, applying GeLU expands the support infinitely (c2). This leads to an aliasing effect — interference in the relevant frequencies marked in red in (c2). This causes the discrete signal not to be a correct representation of the continuous one, after LPF (d2) and downsampling (e2). change the scale of the propagated activation, thus requir-ing adjusting the weight initialization. In our experiments the activation scale had a large impact on the achieved accu-racy, thus searching for an appropriate scale factor was re-quired. Details regarding the activation tuning can be found in Appendix D. Overall, in our case (polynomial activation in ConvNeXt) using the appropriate scale seemed to recover most or all of the lost accuracy.
Normalization ConvNeXt model implementation uses a variation of LayerNorm, which centers and scales each pixel according to its mean and standard deviation over channels, respectively. The scaling operation requires the multiplication of each pixel with a different scalar which, like other point-wise non-linearities, is not alias-free. We construct an alias-free alternative by using scaling per layer instead of scaling per pixel, i.e. all pixels are scaled by the standard deviation of the layer, which is shift-equivariant w.r.t. the continuous domain. Although eliminating aliasing effects, this modification caused a small reduction in the model accuracy, as we shall see later. We hypothesize this reduction results from the “normalization-per-pixel” oper-ation functioning as an additional non-linearity, which en-larges the model capacity. Yet, this modification is required for the property of shit-equivariance w.r.t. continuous do-main, which leads to an overall improvement in terms of robustness to sub-pixel image translations, as shown in Sec-tion 4.
First downsample layer Unlike other CNN architectures that were examined in the context of aliasing prevention,
ConvNeXt does not have a non-linearity before the first downsampling layer. Thus, due to the commutativity of convolutions with the LPFs, we cannot replace the first downsampling operation with BlurPool — since this is equivalent to applying a low-pass filter directly on the in-put, effectively reducing its resolution. Such composition may prevent the model from using high-frequency features and lead to a reduction in the model’s accuracy. To solve this problem, we add an additional activation function be-fore the first BlurPool. For computation efficiency, instead of using the regular scheme which requires upsampling be-fore the activation, we replace the usual activation Poly2(x) with
LPFPoly2(x) = a0 + a1x + a2x · LPF 3 4 (x) . (5)
This modification of the polynomial activation leads to a smaller increase of the signal bandwidth. Thus, it does not require upsampling to avoid aliasing when it is followed by an LPF, as in the first BlurPool. Specifically, since it is fol-lowed by a BlurPool with a cutoff 1/4, The maximally al-lowed cutoff for the LPF-Poly’s filter is 3/4. More details on this activation function can be found in Appendix F.2. 4. Experiments
We compare our Alias-Free Convnet (AFC) model to the baseline ConvNeXt model and to the previous inte-ger shift-invariant method Adaptive Polyphase Sampling (APS) [4]. We implemented all models with cyclic convo-lutions and trained them on ImageNet [6] according to the
ConvNeXt training regime [19]. The experiments were con-ducted with circular translations similarly to the setting in previous works [4, 33]. For sub-pixel translations, we used our “ideal upsampling” implementation (see Algorithm 2 in Appendix G); translation by m/n pixels was conducted by upsampling by n, translating by m pixels and downsam-pling by n. 4.1. Shift equivariance
Our model is designed to be not only shift-invariant (in terms of classification output), but also to have a Feature-Extractor that is shift-equivariant w.r.t. to the continuous do-main. We verified this property by examining the response of the output of each of the layers to a translation of 1 2 pixel in the input image. This was done by propagating the two translated inputs and measuring the difference between their outputs in each layer, after upsampling back to the input’s spatial size. The results in Figure 3 show, in each layer, the normalized difference between the two translated layer outputs y0 and y1, after they were averaged across all HW pixels (indexed by i, j) and C channels (indexed by c), diff ≜ 1
CHW (cid:88) c,i,j (cid:12) (cid:12)y0 max (cid:0)(cid:12) (cid:12)y0 c,i,j − y1 c,i,j (cid:12) (cid:12) , (cid:12) (cid:12)y1 c,i,j c,i,j (cid:12) (cid:12) (cid:12) (cid:1) + ε (cid:12)
, (6) where ε = 10−9 was added in the denominator to avoid di-vision by 0. The results show that ConvNeXt-AFC has only a negligible difference in the continuous representation of the translated responses at each layer, e.g. y0 = y1, which means it is indeed shift-equivariant w.r.t. the continuous do-main (up to numerical error). In contrast, in the case of the baseline and APS models, the upsampled signals differ by more than 50% across all the layers.
Figure 3. Shift-equivariance measure w.r.t. continuous signal.
The averaged difference (Eq. (6)) for 1/2 pixel translated inputs (y-axis), across all layers (x-axis). This experiment was run on 64 random samples from the validation set. While the AFC model has practically 0 difference, the baseline and APS models have at least 50% difference across all layers. 4.2. Consistency and Classification accuracy
The main measure used so far to quantify the shift-invariance of a model is called “consistency” [2,4], which is the percentage of predictions changed on the test set follow-ing an image shift. Previously this measure has been used with integer shift values, however, in Table 1 we test it also under sub-pixel shifts. We see that the changes we add to the baseline model gradually improve its consistency, until we reach 100%. In contrast, the previous APS approach [4] is near 100% consistent to integer shifts (due to numeri-cal accuracy), but for fractional shifts, it only has slightly higher consistency than the baseline. Even though the alias-free modifications in our model lead to perfect consistency, they cause a 1.08% reduction in the (standard) test accuracy.
We conclude that the main source of accuracy reduction is the modification of the normalization layer, as explained in
Section 3. However, as we shall see next, despite such a reduction in accuracy, our model outperforms the previous models in adversarial shifts setting, due to its increased ro-bustness. 4.3. Translation robustness
Since standard models are not invariant to image transla-tion, this might be exploited as a very easy form of a “black-box” adversarial attack: we simply move the image until we notice the prediction is changed. We examine this vulner-ability, to assess each model’s actual robustness to transla-tions. For each sample, we performed all possible transla-tions in some set T , and checked the resulting classification for each shift. We define the adversarial accuracy corre-sponding to T as the portion of samples that are classified correctly for all translations in T . We tested three types of basic translation grids — Integer, Half pixel and Fractional:
Tinteger = {(i, j) | 1 ≤ i, j ≤ 31} (cid:27) (cid:19)
Thalf = (cid:26)(cid:18) i 2
, j 2
| 1 ≤ i, j ≤ 63 (7) (8)
Test accuracy
Change
Integer shift consistency
Change
Fractional shift consistency
Change
Model modification
ConvNeXt-Baseline [19]
+ Polynomial activation
+ BlurPool
+ First layer activation
+ AF LayerNorm
+ Activation upsample (ConvNeXt-AFC, ours) 82.12 81.77 78.99 81.51 80.66 81.04
-0.35
-3.12
-0.61
-1.46
-1.08 94.816 95.126 96.635 97.354 97.030 100.000
ConvNeXt-APS [4] 82.11
-0.01 99.998 0.31 1.82 2.54 2.21 5.18 5.18 92.034 92.708 96.572 97.347 96.990 100.000 93.227 0.67 4.54 5.31 4.96 7.97 1.19
Table 1. Alias-free modifications ImageNet accuracy and shift-consistency effect. Integer shift consistency is defined as the percentage of test samples that did not change their prediction following a random integer translation. Fractional shift consistency is defined as the percentage of test samples that did not change their prediction following a random half-pixel translation. Consistency was averaged on five runs on ImageNet validation set with random seeds. The final AFC model is 100% consistent to both integer and fractional translations.
Note that though the APS model [4] exhibits near 100% integer shifts consistency (as expected), it has only slightly better consistency than the baseline model in terms of fractional shift consistency.
Model
Integer grid
Half-pixel grid
Fractional grid
ConvNeXt-Baseline [19]
ConvNeXt-APS [4]
ConvNeXt-AFC (ours) 76.63 82.11 81.04 73.65 79.68 81.04 77.82 76.31 81.04
Table 2. Translation adversarial accuracy (ImageNet). Ad-versarial accuracy defined as the percentage of correctly classi-fied samples for each translation in the corresponding set: Eq. (7),
Eq. (8) or Eq. (9) with k = 12.
Tfrac,k = (cid:19) (cid:26)(cid:18) m1 n1
, m2 n2
| 1 ≤ m1,2 ≤ n1,2 ≤ k (9) (cid:27)
In Table 2 we observe the adversarial robustness with re-spect to these translation sets. In the baseline model the test accuracy of 82.1% drops to 76.63% for integer grid and to 73.65% for half-pixel grid accuracy. This significant drop reflects that more than 10% of the correctly classified test set samples may be misclassified due to translations. The
APS model [4] is, by construction, robust to integer transla-tions and therefore has no accuracy reduction in the integer grid. However, it gets even worse results than the baseline in fractional adversarial accuracy (76.31% vs 77.82%). In contrast, our AFC model is invariant to any of these shifts, and therefore its accuracy remains constant at 81.04%, sur-passing the other models. This robustness is ‘certified’, and will not be compromised with larger translation sets, or other types of attacks (e.g., white box attacks) which can po-tentially decrease the performance of the other models even more. We repeat this experiment on “Out of distribution” data using ImageNet-C that contains common corruptions of the ImageNet images. The results in Appendix A show that the APS and Baseline models are even more vulnerable to fractional translation attacks on corrupted images, while
AFC adversarial accuracy does not change. 4.4. Robustness to other shifts
We next test the models’ robustness to other types of translations, where our model’s shift-invariance guarantee conditions are not satisfied.
Zero-padding, bilinear-interpolation We tested the models’ robustness to translation using the framework pre-sented by Engstrom et al. [8], originally designed to test the robustness of classification models to translations and rota-tions. We zero-pad the images by 8 pixels and translate by (a possibly fractional) amount limited by 8 pixels, so there are no artifacts due to circular translations, nor data loss.
The remaining parts are zero-padded and fractional trans-lations are done using bilinear interpolation (see Fig. 9 in
Appendix E). The results in Figure 4 (top) show the mod-els’ adversarial accuracy to this attack with different grid sizes. Although our model is not perfectly invariant to the performed translations due to the bilinear interpolation, it outperforms the other models by more than 4% at the largest tested grid.
Crop-shift
In the experiments above, we used the com-mon ImageNet input: the 224 × 224 center crop of the orig-inal 256 × 256 image. In contrast, in this experiment, we adversarially translated the cropped area, modeling trans-lating a camera w.r.t. the scene (see Fig. 8 in Appendix E).
We measure the adversarial accuracy of translations by up to m integer pixels in each direction (i.e. grid search at size (2m + 1) × (2m + 1)). The results in Figure 4 (bottom) show that our model is more robust to this kind of transla-tion, which are not cyclic, include data loss, and are integer-valued. We additionally evaluate the original ConvNeXt model (zero-pad convolutions) which interestingly has the worst robustness in this setting.
Since it is known that aliasing in discrete signals is caused by non-linearities in addition to subsampling, a few studies suggested methods for alias-free activation func-tions. Karras et al. [17] suggested using upsampling be-fore non-linearities to reduce aliasing in generative mod-els, which cause failure in embedding “high-frequency fea-tures” such as textures in their outputs. The idea of us-ing polynomial non-linearities to battle aliasing has been mentioned previously [7, 22]. Franzen and Wand [9] have recently shown this methodology can be used to improve rotation-equivariance. However, it has never been applied in a complete alias-free setting, nor in modern-scale deep networks. Other smooth activation functions have been sug-gested as well [16,29], yet they do not completely eliminate aliasing.
It is worth mentioning that other equivariance properties have been studied as well, such as rotation, reflection and group equivariance [3, 5, 20, 21, 25, 30–32]. This work is focused on the specific property of shift-invariance in CNNs for image classification. 6. Discussion
In this paper we proposed the Alias-Free Convnet, which for the first time, is guaranteed to eliminate any aliasing ef-fects in the model, to ensure the output is invariant to any input shifts (even sub-pixel ones), and to ensure the inter-nal representations are equivariant to any shifts (even sub-pixel ones). We demonstrate this numerically and show this leads to (certified) high performance under adversarial shift-based attacks — in contrast to existing models which de-grade in performance. However, this comes at a cost, such as a 1.08% reduction in standard test accuracy (as meth-ods that increase robustness often reduce accuracy) and in-creased computation cost. We further discuss these limita-tions as well as limitations regarding the certified robust-ness conditions in Appendix B. We discuss possible future work and applications of the AFC in Appendix C, and the potential of polynomial activation functions in general (i.e., beyond AFC) in Appendix D.
Acknowledgments The research of DS was funded by the
European Union (ERC, A-B-C-Deep, 101039436). Views and opinions expressed are however those of the author only and do not necessarily reflect those of the European
Union or the European Research Council Executive Agency (ERCEA). Neither the European Union nor the granting au-thority can be held responsible for them. DS also acknowl-edges the support of Schmidt Career Advancement Chair in
AI. TM was supported by grant 2318/22 from the Israel Sci-ence Foundation and by the Ollendorff Center, ECE faculty,
Technion.
Figure 4. Adversarial accuracy for other types of shifts. Top:
Zero-padding, bilinear interpolation results. AFC is the most ro-bust model for all tested grid sizes. Bottom: Crop-shift results.
AFC is the most robust model for m ≥ 2. The accuracy improve-ment over the baseline and APS models reaches to 2.9% and 2% respectively for the strongest attack in our scope (m = 10). 5.