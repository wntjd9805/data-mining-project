Abstract
Diffusion models are state-of-the-art deep learning em-powered generative models that are trained based on the principle of learning forward and reverse diffusion pro-cesses via progressive noise-addition and denoising. To gain a better understanding of the limitations and potential risks, this paper presents the first study on the robustness of diffusion models against backdoor attacks. Specifically, we propose BadDiffusion, a novel attack framework that engi-neers compromised diffusion processes during model train-ing for backdoor implantation. At the inference stage, the backdoored diffusion model will behave just like an untam-pered generator for regular data inputs, while falsely gen-erating some targeted outcome designed by the bad actor upon receiving the implanted trigger signal. Such a crit-ical risk can be dreadful for downstream tasks and appli-cations built upon the problematic model. Our extensive experiments on various backdoor attack settings show that
BadDiffusion can consistently lead to compromised diffu-sion models with high utility and target specificity. Even worse, BadDiffusion can be made cost-effective by simply finetuning a clean pre-trained diffusion model to implant backdoors. We also explore some possible countermeasures for risk mitigation. Our results call attention to potential risks and possible misuse of diffusion models. Our code is available on https://github.com/IBM/BadDiffusion. 1.

Introduction
In the past few years, diffusion models [2,4,9–11,21,24, 26, 28, 30–34] trained with deep neural networks and high-volume training data have emerged as cutting-edge tools for content creation and high-quality generation of synthetic data in various domains, including images, texts, speech, molecules, among others [12–15, 18, 19, 22, 41].
In par-ticular, with the open-source of Stable Diffusion, one of the state-of-the-art and largest text-based image generation
*National Tsing Hua University, Hsinchu, R.O.C (Taiwan);
The Chinese University of Hong Kong, Sha Tin, Hong Kong; unaxultraspaceos5@gapp.nthu.edu.tw
†IBM Research, New York, USA; pin-yu.chen@ibm.com
‡The Chinese University of Hong Kong, Sha Tin, Hong Kong; tyho@cse.cuhk.edu.hk
Figure 1. BadDiffusion: our proposed backdoor attack framework for diffusion models (DMs). Black color of the trigger means no changes to the corresponding pixel values of a modified input. models to date, that are trained with intensive resources, a rapidly growing number of new applications and workloads are using the same model as the foundation to develop their own tasks and products.
While our community is putting high hopes on diffusion models to fully drive our creativity and facilitate synthetic data generation, imagine the consequences when this very foundational diffusion model is at the risk of being secretly implanted with a “backdoor” that can exhibit a designated action by a bad actor (e.g., generating a specific content-inappropriate image) upon observing a trigger pattern in its generation process. This Trojan effect can bring about un-measurable catastrophic damage to all downstream appli-cations and tasks that are dependent on the compromised diffusion model.
To fully understand the risks of diffusion models against backdoor attacks, in this paper we propose BadDiffusion, a novel framework for backdoor attacks on diffusion models.
Different from standard backdoor attacks on classifiers that mainly modify the training data and their labels for back-door injection [6], BadDiffusion requires maliciously modi-fying both the training data and the forward/backward diffu-sion steps, which are tailored to the unique feature of noise-addition and denoising in the stages of training and infer-ence for diffusion models. As illustrated in Fig. 1, the threat model considered in this paper is that the attacker aims to train a backdoored diffusion model satisfying two primary objectives: (i) high utility – the model should have a sim-ilar performance to a clean (untampered) diffusion model
while the backdoor is inactive; and (ii) high specificity – the model should exhibit a designated behavior when the back-door is activated. Upon model deployment (e.g., releasing the trained model parameters and network architecture to the public), the stealthy nature of a backdoored diffusion model with high utility and specificity makes it appealing to use, and yet the hidden backdoor is hard to identify.
As an illustration, Fig. 1 (bottom) shows some gener-ated examples of a backdoored diffusion model (DDPM) [9] at the inference stage. The inputs are isotropic Gaussian noises and the model was trained on the CelebA-HQ [20] (a face image dataset) by BadDiffusion with a designed trigger pattern (eyeglasses) and a target outcome (the cat image).
Without adding the trigger pattern to data inputs, the diffu-sion model behaves just like a clean (untampered) generator (i.e., high utility). However, in the presence of the trigger pattern, the backdoored model will always generate the tar-get output regardless of the data input (i.e., high specificity).
Through an extensive set of experiments, we show that our proposed BadDiffusion can successfully train a back-doored diffusion model with high utility and specificity, based on our design of compromised diffusion processes.
Furthermore, we demonstrate that BadDiffusion can be ex-ecuted in a cost-effective manner, by simply using our de-signed training objective to finetune a clean pre-trained dif-fusion model with few epochs to implant backdoors. Our findings suggest that with the abundance and easy access to pre-trained diffusion models released to the public, back-door attacks on diffusion models are practical and plausible.
In addition to attacks, we also explore some possible coun-termeasures for risk mitigation. Our results call attention to potential risks and possible misuse of diffusion models.
We highlight our main contributions as follows. 1. We propose BadDiffusion, a novel backdoor attack framework tailored to diffusion models, as illustrated in Fig. 1. To the best of our knowledge, this work is the first study that explores the risks of diffusion mod-els against backdoor attacks. 2. Through various backdoor attack settings, we show that BadDiffusion can successfully implant backdoors to diffusion models while attaining high utility (on clean inputs) and high specificity (on inputs with trig-gers). We also find that a low data poison rate (e.g., 5%) is sufficient for BadDiffusion to take effect. 3. Compared to training-from-scratch with BadDiffusion, we find BadDiffusion can be made cost-effective via fine-tuning a clean pre-trained diffusion model (i.e., backdoor with a warm start) for a few epochs. 4. We evaluate BadDiffusion against two possible coun-termeasures: Adversarial Neuron Pruning (ANP) [40] and inference-time clipping. The results show that
ANP is very sensitive to hyperparameters for correct target discovery, while inference-time clipping has the potential to be a simple yet effective backdoor mitiga-tion strategy. 2.