Abstract
Signed distance functions (SDFs) is an attractive frame-work that has recently shown promising results for 3D shape reconstruction from images. SDFs seamlessly generalize to different shape resolutions and topologies but lack explicit modelling of the underlying 3D geometry. In this work, we ex-ploit the hand structure and use it as guidance for SDF-based shape reconstruction. In particular, we address reconstruc-tion of hands and manipulated objects from monocular RGB images. To this end, we estimate poses of hands and objects and use them to guide 3D reconstruction. More specifically, we predict kinematic chains of pose transformations and align SDFs with highly-articulated hand poses. We improve the visual features of 3D points with geometry alignment and further leverage temporal information to enhance the robustness to occlusion and motion blurs. We conduct exten-sive experiments on the challenging ObMan and DexYCB benchmarks and demonstrate significant improvements of the proposed method over the state of the art. 1.

Introduction
Understanding how hands interact with objects is be-coming increasingly important for widespread applications, including virtual reality, robotic manipulation and human-computer interaction. Compared to 3D estimation of sparse hand joints [24, 38, 51, 53, 67], joint reconstruction of hands and object meshes [11, 18, 21, 26, 62] provides rich infor-mation about hand-object interactions and has received in-creased attention in recent years.
To reconstruct high-quality meshes, some recent works [9, 17, 61] explore multi-view image inputs. Multi-view images, however, are less common both for training and testing sce-narios. In this work, we focus on a more practical and user-friendly setting where we aim to reconstruct hand and object meshes from monocular RGB images. Given the ill-posed nature of the task, many existing methods [7, 19, 21, 54, 62] employ parametric mesh models (e.g., MANO [46]) to im-Figure 1. We aim to reconstruct 3D hand and object meshes from monocular images (top). Our method gSDF (middle) first predicts 3D hand joints (blue) and object locations (red) from input images.
We use estimated hand poses and object locations to incorporate strong geometric priors into SDF by generating hand- and object-aware kinematic features for each SDF query point. Our resulting gSDF model generates accurate results for real images with various objects and grasping hand poses (bottom). pose prior knowledge and reduce ambiguities in 3D hand re-construction. MANO hand meshes, however, have relatively limited resolution and can be suboptimal for the precise capture of hand-object interactions.
To reconstruct detailed hand and object meshes, another line of efforts [11, 26] employ signed distance functions (SDFs). Grasping Field [26] makes the first attempt to model hand and object surfaces using SDFs. However, it does not explicitly associate 3D geometry with image cues and has no prior knowledge incorporated in SDFs, leading to unrealistic meshes. AlignSDF [11] proposes to align SDFs with respect to global poses (i.e., the hand wrist transformation and the
object translation) and produces improved results. However, it is still challenging to capture geometric details for more complex hand motions and manipulations of diverse objects, which involve the articulation of multiple fingers.
To address limitations of prior works, we propose a geometry-driven SDF (gSDF) method that encodes strong pose priors and improves reconstruction by disentangling pose and shape estimation (see Figure 1). To this end, we first predict sparse 3D hand joints from images and derive full kinematic chains of local pose transformations from joint locations using inverse kinematics. Instead of only using the global pose as in [11], we optimize SDFs with respect to poses of all the hand joints, which leads to a more fine-grained alignment between the 3D shape and articulated hand poses. In addition, we project 3D points onto the image plane to extract geometry-aligned visual features for signed distance prediction. The visual features are further refined with spatio-temporal contexts using a transformer model to enhance the robustness to occlusions and motion blurs.
We conduct extensive ablation experiments to show the effectiveness of different components in our approach. The proposed gSDF model greatly advances state-of-the-art accu-racy on the challenging ObMan and DexYCB benchmarks.
Our contributions can be summarized in three-fold: (i) To embed strong pose priors into SDFs, we propose to align the SDF shape with its underlying kinematic chains of pose transformations, which reduces ambiguities in 3D recon-struction. (ii) To further reduce the misalignment induced by inaccurate pose estimations, we propose to extract geometry-aligned local visual features and enhance the robustness with spatio-temporal contexts. (iii) We conduct comprehensive experiments to show that our approach outperforms state-of-the-art results by a significant margin. 2.