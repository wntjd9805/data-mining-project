Abstract
Although existing semi-supervised learning models achieve remarkable success in learning with unannotated in-distribution data, they mostly fail to learn on unlabeled data sampled from novel semantic classes due to their closed-set assumption.
In this work, we target a prag-matic but under-explored Generalized Novel Category Dis-covery (GNCD) setting. The GNCD setting aims to cat-egorize unlabeled training data coming from known and novel classes by leveraging the information of partially la-beled known classes. We propose a two-stage Contrastive
Affinity Learning method with auxiliary visual Prompts, dubbed PromptCAL, to address this challenging problem.
Our approach discovers reliable pairwise sample affini-ties to learn better semantic clustering of both known and novel classes for the class token and visual prompts. First, we propose a discriminative prompt regularization loss to reinforce semantic discriminativeness of prompt-adapted pre-trained vision transformer for refined affinity relation-ships. Besides, we propose contrastive affinity learning to calibrate semantic representations based on our itera-tive semi-supervised affinity graph generation method for semantically-enhanced supervision. Extensive experimen-tal evaluation demonstrates that our PromptCAL method is more effective in discovering novel classes even with lim-ited annotations and surpasses the current state-of-the-art on generic and fine-grained benchmarks (e.g., with nearly 11% gain on CUB-200, and 9% on ImageNet-100) on over-all accuracy. Our code is available at https:// github. com/ sheng-eatamath/ PromptCAL. 1.

Introduction
The deep neural networks have demonstrated favorable performance in the Semi-Supervised Learning (SSL) set-ting [15,29,40,46,49]. Some recent works can even achieve comparable performance to their fully-supervised counter-Figure 1. PromptCAL Overview. In contrast to previous method based on semi-supervised contrastive learning, PromptCAL con-structs affinity graph on-the-fly to guide representation learning of the class token and prompts. Meanwhile, our prompt-adapted backbone can be tuned to enhance semantic discriminativeness.
PromptCAL can discover reliable affinities from a memory bank, especially for novel classes. Therefore, our PromptCAL is better task-aligned and discriminative to novel semantic information. parts using few annotations for image recognition [3,38,46].
However, these approaches heavily rely on the closed-world assumption that unlabeled data share the same underlying class label space as the labeled data [10, 47]. In many real-istic scenarios, this assumption does not hold true because of the inherent dynamism of real-world tasks where novel classes can emerge in addition to known classes.
In contrast to SSL, the Novel Category Discovery (NCD) problem was introduced by [12] to relax the closed-world assumption of SSL, which assumes the unlabeled data con-tain novel classes. Recently, the nascent Generalized Novel
Category Discovery (GNCD) problem, first proposed in
[4, 41], extends NCD and assumes the unlabeled data can contain both known and novel classes, which is more prag-matic and challenging. To be more specific, GNCD intends to categorize images sampled from predefined categories in the training set comprising labeled-knowns, unlabeled-knowns, and unlabeled-novels. 1
Our work focuses on GNCD problem. The key challenge of GNCD is to discriminate among novel classes when only the ground truths of known classes are accessible in the training set. Recent studies show that self-supervised pre-trained representations are conducive to discovering novel semantics [4, 5, 11, 41, 53]. A typical work on GNCD [41] takes advantage of the large-scale pre-trained visual trans-former (ViT) [37], and learns robust clusters for known and novel classes through semi-supervised contrastive learning on downstream datasets. However, we discover that the re-markable potential of pre-trained ViT is actually suppressed by this practice, due to the class collision [52] issue induced by abundant false negatives in contrastive loss, i.e., consid-ering different unlabeled images from the same or similar semantic class as false negatives. As supported by empiri-cal studies, abundant false negatives in contrastive training can deteriorate the compactness and purity of semantic clus-tering [5, 16, 21, 52]. Based on empirical investigation, we show that this issue is particularly severe in category discov-ery. Furthermore, although the existing commonly adopted practice [4, 41] of freezing most parts of the pre-trained backbone can alleviate overfitting on known classes, it con-strains the flexibility and adaptability of backbones [18].
Lack of adaptability inhibits models from learning discrim-inative semantic information on downstream datasets.
To address above limitations and learn better semanti-cally discriminative representations, we propose Prompt-based Contrastive Affinity Learning (PromptCAL) frame-work to tackle GNCD problem. To be specific, our ap-proach aims to discover semantic clusters in unlabeled data by simultaneous semantic prompt learning based on our
Discriminative Prompt Regularization (DPR) loss and rep-resentation calibration based on our Contrastive Affinity
Learning (CAL) process. Firstly, CAL discovers abundant reliable pseudo positives for DPR loss and contrastive loss based on generated affinity graphs. These semantic-aware pseudo labels further enhance the semantic discriminative-ness of DPR supervision. Secondly, DPR regularizes se-mantic representations of ensembled prompts, which facil-itates the discovery of more accurate pseudo labels at the next-step of CAL. Therefore, as model and prompt repre-sentations are iteratively enhanced, we can obtain higher quality pseudo positives for further self-training as well as acquire better semantic clustering.
Our PromptCAL achieves State-Of-The-Art (SOTA) performance in extensive experimental evaluation on six benchmarks. Specifically, PromptCAL remarkably sur-passes previous SOTA by more than 10% clustering ac-curacy on the fine-grained CUB-200 and StandfordCars datasets; it also significantly outperforms previous SoTAs by nearly 4% on ImageNet-100 and 7% on CIFAR-100. In-terestingly, we identify that both DPR supervised prompts and unsupervised prompts of PromptCAL can learn se-mantic discriminativeness, which advances the flexibility of the pre-trained backbone. Furthermore, PromptCAL still achieves the best performance in challenging low-labeling and few-class setups.
Our contributions include: (1) We propose a two-stage framework for the generalized novel category discovery problem, in which semantic prompt tuning and contrastive affinity learning mutually reinforce and benefit each other (2) We propose two syn-during the learning process. ergistic learning objectives, contrastive affinity loss and discriminative prompt regularization loss, based on our semi-supervised adapted affinity graphs to enhance seman-(3) We comprehensively evaluate tic discriminativeness. our method on three generic (i.e., CIFAR-10, CIFAR-100, and ImageNet-100) and three fine-grained benchmarks (i.e.,
CUB-200, Aircraft, and StandfordCars), achieving state-of-the-art performance, thereby showing its effectiveness. (4)
We further showcase generalization ability of PromptCAL and its effectiveness in more challenging low-labeling and few-class setups. 2.