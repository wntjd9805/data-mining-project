Abstract
Humans are able to accurately reason in 3D by gathering multi-view observations of the surrounding world. Inspired by this insight, we introduce a new large-scale benchmark for 3D multi-view visual question answering (3DMV-VQA).
This dataset is collected by an embodied agent actively mov-ing and capturing RGB images in an environment using the
Habitat simulator. In total, it consists of approximately 5k scenes, 600k images, paired with 50k questions. We evaluate various state-of-the-art models for visual reasoning on our benchmark and find that they all perform poorly. We suggest that a principled approach for 3D reasoning from multi-view images should be to infer a compact 3D representation of the world from the multi-view images, which is further grounded on open-vocabulary semantic concepts, and then to execute reasoning on these 3D representations. As the first step to-wards this approach, we propose a novel 3D concept learning and reasoning (3D-CLR) framework that seamlessly com-bines these components via neural fields, 2D pre-trained vision-language models, and neural reasoning operators. Ex-perimental results suggest that our framework outperforms baseline models by a large margin, but the challenge remains largely unsolved. We further perform an in-depth analysis of the challenges and highlight potential future directions. .
1.

Introduction
Visual reasoning, the ability to composite rules on inter-nal representations to reason and answer questions about visual scenes, has been a long-standing challenge in the field of artificial intelligence and computer vision. Several datasets [23, 33, 69] have been proposed to tackle this chal-lenge. However, they mainly focus on visual reasoning on 2D single-view images. Since 2D single-view images only cover a limited region of the whole space, such reasoning inevitably has several weaknesses, including occlusion, and failing to answer 3D-related questions about the entire scene that we are interested in. As shown in Fig. 1, it’s difficult, even for humans, to count the number of chairs in a scene due to the object occlusion, and it’s even harder to infer 3D relations like “closer” from a single-view 2D image.
On the other hand, there’s strong psychological evidence that human beings conduct visual reasoning in the under-lying 3D representations [55]. Recently, there have been several works focusing on 3D visual question answering
[2,16,62,64]. They mainly use traditional 3D representations (e.g., point clouds) for visual reasoning. This is inconsistent with the way human beings perform 3D reasoning in real life. Instead of being given an entire 3D representation of the scene at once, humans will actively walk around and explore the whole environment, ingesting image observations from different views and converting them into a holistic 3D repre-sentation that assists them in understanding and reasoning about the environment. Such abilities are crucial for many embodied AI applications, such as building assistive robots.
To this end, we propose the novel task of 3D visual rea-soning from multi-view images taken by active exploration of an embodied agent. Specifically, we generate a large-scale benchmark, 3DMV-VQA (3D multi-view visual question answering), that contains approximately 5k scenes and 50k question-answering pairs about these scenes. For each scene, we provide a collection of multi-view image observations.
We generate this dataset by placing an embodied agent in the Habitat-Matterport environment [47], which actively ex-plores the environment and takes pictures from different views. We also obtain scene graph annotations from the
Habitat-Matterport 3D semantics dataset (HM3DSem) [61], including ground-truth locations, segmentations, semantic information of the objects, as well as relationships among the objects in the environments, for model diagnosis. To evaluate the models’ 3D reasoning abilities on the entire environment, we design several 3D-related question types, including concept, counting, relation and comparison.
Given this new task, the key challenges we would like to investigate include: 1) how to efficiently obtain the compact visual representation to encode crucial properties (e.g., se-mantics and relations) by integrating all incomplete observa-tions of the environment in the process of active exploration for 3D visual reasoning? 2) How to ground the semantic con-cepts on these 3D representations that could be leveraged for downstream tasks, such as visual reasoning? 3) How to infer the relations among the objects, and perform step-by-step reasoning?
As the first step to tackling these challenges, we propose a novel model, 3D-CLR (3D Concept Learning and Reason-ing). First, to efficiently obtain a compact 3D representation from multi-view images, we use a neural-field model based on compact voxel grids [57] which is both fast to train and effective at storing scene properties in its voxel grids. As for concept learning, we observe that previous works on 3D scene understanding [1,3] lack the diversity and scale with re-gard to semantic concepts due to the limited amount of paired 3D-and-language data. Although large-scale vision-language models (VLMs) have achieved impressive performances for zero-shot semantic grounding on 2D images, leverag-ing these pretrained models for effective open-vocabulary 3D grounding of semantic concepts remains a challenge. To address these challenges, we propose to encode the features of a pre-trained 2D vision-language model (VLM) into the compact 3D representation defined across voxel locations.
Specifically, we use the CLIP-LSeg [37] model to obtain fea-tures on multi-view images, and propose an alignment loss to map the features in our 3D voxel grid to 2D pixels. By cal-culating the dot-product attention between the 3D per-point features and CLIP language embeddings, we can ground the semantic concepts in the 3D compact representation. Fi-nally, to answer the questions, we introduce a set of neural reasoning operators, including FILTER, COUNT, RELATION operators and so on, which take the 3D representations of different objects as input and output the predictions.
We conduct experiments on our proposed 3DMV-VQA benchmark. Experimental results show that our proposed 3D-CLR outperforms all baseline models a lot. However, failure cases and model diagnosis show that challenges still exist concerning the grounding of small objects and the separation of close object instances. We provide an in-depth analysis of the challenges and discuss potential future directions.
To sum up, we have the following contributions in this paper.
• We propose the novel task of 3D concept learning and reasoning from multi-view images.
• By having robots actively explore the embodied environ-ments, we collect a large-scale benchmark on 3D multi-view visual question answering (3DMV-VQA).
• We devise a model that incorporates a neural radiance field, 2D pretrained vision and language model, and neural rea-soning operators to ground the concepts and perform 3D reasoning on the multi-view images. We illustrate that our model outperforms all baseline models.
• We perform an in-depth analysis of the challenges of this new task and highlight potential future directions.
2.