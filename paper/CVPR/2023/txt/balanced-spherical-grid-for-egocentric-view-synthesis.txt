Abstract 1.

Introduction
We present EgoNeRF, a practical solution to reconstruct large-scale real-world environments for VR assets. Given a few seconds of casually captured 360 video, EgoNeRF can efficiently build neural radiance fields. Motivated by the recent acceleration of NeRF using feature grids, we adopt spherical coordinate instead of conventional Carte-sian coordinate. Cartesian feature grid is inefficient to rep-resent large-scale unbounded scenes because it has a spa-tially uniform resolution, regardless of distance from view-ers. The spherical parameterization better aligns with the rays of egocentric images, and yet enables factorization for performance enhancement. However, the na¨ıve spherical grid suffers from singularities at two poles, and also cannot represent unbounded scenes. To avoid singularities near poles, we combine two balanced grids, which results in a quasi-uniform angular grid. We also partition the radial grid exponentially and place an environment map at infin-ity to represent unbounded scenes. Furthermore, with our resampling technique for grid-based methods, we can in-crease the number of valid samples to train NeRF volume.
We extensively evaluate our method in our newly introduced synthetic and real-world egocentric 360 video datasets, and it consistently achieves state-of-the-art performance.
With the recent advance in VR technology, there ex-ists an increasing need to create immersive virtual environ-ments. While a synthetic environment can be created by ex-pert designers, various applications also require transferring a real-world environment. Spherical light fields [4–6,26,28] can visualize photorealistic rendering of the real-world en-vironment with the help of dedicated hardware with care-fully calibrated multiple cameras. A few works [3, 16] also attempt to synthesize novel view images by reconstructing an explicit mesh from an egocentric omnidirectional video.
However, their methods consist of complicated multi-stage pipelines and require pretraining for optical flow and depth estimation networks.
In this paper, we build a system that can visualize a large-scale scene without sophisticated hardware or neural net-works trained with general scenes. We utilize panoramic images, as suggested in spherical light fields. However, we acquire input with a commodity omnidirectional camera with two fish-eye lenses instead of dedicated hardware. As shown in Fig. 1 (a), the environment can be captured with the omnidirectional camera attached to a selfie stick within less than five seconds. Then the collected images observe a large-scale scene that surrounds the viewpoints. We in-troduce new synthetic and real-world datasets of omnidirec-Figure 3. Training curve comparison in OmniBlender scenes. rection, which leads to a more efficient data structure for the large-scale environment. The na¨ıve spherical grid con-tains high valence vertices at two poles, and, when adapted as a feature grid for neural volume rendering, the polar re-gions suffer from undesirable artifacts. We exploit a quasi-uniform angular grid by combining two spherical grids [18].
In the radial direction, the grid intervals increase exponen-tially, which not only allows our representation to cover large spaces but also makes the spherical frustum have a similar length in the angular and radial directions. We add an environment map at infinite depth, which is especially useful for outdoor environments with distant backgrounds such as skies. Last but not least, we propose an efficient hi-erarchical sampling method exploiting our density feature grid without maintaining an additional coarse density grid.
We demonstrate that our proposed approach can lead to faster convergence and high-quality rendering with a small memory footprint in various scenarios for large-scale envi-ronments. EgoNeRF is expected to create a virtual render-ing of large scenes from data captured by non-expert users, which cannot be easily modeled with 3D assets or conven-tional NeRF. 2.