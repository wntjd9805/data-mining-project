Abstract
Weakly-supervised temporal action localization (WTAL) learns to detect and classify action instances with only cat-egory labels. Most methods widely adopt the off-the-shelf
Classification-Based Pre-training (CBP) to generate video features for action localization. However, the different opti-mization objectives between classification and localization, make temporally localized results suffer from the serious in-complete issue. To tackle this issue without additional anno-tations, this paper considers to distill free action knowledge from Vision-Language Pre-training (VLP), as we surpris-ingly observe that the localization results of vanilla VLP have an over-complete issue, which is just complementary to the CBP results. To fuse such complementarity, we pro-pose a novel distillation-collaboration framework with two branches acting as CBP and VLP respectively. The frame-work is optimized through a dual-branch alternate training strategy. Specifically, during the B step, we distill the confi-dent background pseudo-labels from the CBP branch; while during the F step, the confident foreground pseudo-labels are distilled from the VLP branch. As a result, the dual-branch complementarity is effectively fused to promote one strong alliance. Extensive experiments and ablation studies on THUMOS14 and ActivityNet1.2 reveal that our method significantly outperforms state-of-the-art methods. 1.

Introduction
Temporal action localization (TAL), which aims to lo-calize and classify action instances from untrimmed long videos, has been recognized as an indispensable component of video understanding [13,72,92]. To avoid laborious tem-poral boundary annotations, the weakly-supervised setting (WTAL) [31, 62, 64, 78], i.e. only video-level category la-bels are available, has gained increasing attentions.
Figure 1. (A) Complementarity. Most works use Classification-Based Pre-training (CBP) for localization, bringing high TN yet serious FN. Vanilla Vision-Language Pre-training (VLP) confuses action and background, bringing high TP yet serious FP. (B) Our distillation-collaboration framework distills foreground from the VLP branch while background from the CBP branch, and pro-motes mutual collaboration, bringing satisfactory results. ture extraction [5, 73]. A popular pipeline is to train an ac-tion classifier with CBP features, then threshold the frame-level classification probabilities for final localization re-sults. As demonstrated in Figure 1 (A), these CBP meth-ods suffer from the serious incompleteness issue, i.e. only detecting sparse discriminative action frames and incurring high false negatives. The main reason is that the optimiza-tion objective of classification pre-training, is to find sev-eral discriminative frames for action recognition, which is far from the objective of complete localization. As a re-sult, features from CBP are inevitably biased towards par-tial discriminative frames. To solve the incompleteness is-sue, many efforts [33, 43, 56, 96] have been attempted, but most of them are trapped in a ‘performance-cost dilemma’, namely, solely digging from barren category labels to keep costs low. Lacking location labels fundamentally limits the performance, leaving a huge gap from strong supervision.
To date in the literature, almost all WTAL methods rely on Classification-Based Pre-training (CBP) for video fea-To jump out of the dilemma, this paper raises one novel question: is there free action knowledge available, to help
complete detection results while maintain cheap annotation overhead at the same time? We naturally turn our sights to the prevalent Vision-Language Pre-training (VLP) [22, 66].
VLP has demonstrated great success to learn joint visual-textual representations from large-scale web data. As lan-guage covers rich information about objects, human-object interactions, and object-object relationships, these learned representations could provide powerful human-object co-occurrence priors: valuable gifts for action localization.
We here take one step towards positively answering the question, i.e. fill the research gap of distilling action priors from VLP, namely CLIP [66], to solve the incomplete issue for WTAL. As illustrated in Figure 1 (A), we first naively evaluate the temporal localization performance of VLP by frame-wise classification. But the results are far from satis-factory, suffering from the serious over-complete issue, i.e. confusing multiple action instances into a whole, causing high false positives. We conjecture the main reasons are: (1) due to data and computational burden, almost all VLPs are trained using image-text pairs. Hence, VLP lacks suffi-cient temporal knowledge and relies more on human-object co-occurrence for localization, making it struggle to distin-guish the actions with visually similar background contexts; (2) some background contexts have similar (confusing) tex-tual semantics to actions, such as run-up vs. running.
Although simply steering VLP for WTAL is infeasible, we fortunately observe the complementary property be-tween CBP and VLP paradigms: the former localizes high true negatives but serious false negatives, while the latter has high true positives but serious false positives. To lever-age the complementarity, as shown in Figure 1 (B), we de-sign a novel distillation-collaboration framework that uses two branches to play the roles of CBP and VLP, respec-tively. The design rationale is to distill background knowl-edge from the CBP branch, while foreground knowledge from the VLP branch, for strong alliances. Specifically, we first warm up the CBP branch using only category super-vision to initialize confident background frames, and then optimize the framework via an alternating strategy. Dur-ing B step, we distill background pseudo-labels for the VLP branch to solve the over-complete issue, hence obtaining high-quality foreground pseudo-labels. During F step, we leverage high-quality pseudo-labels for the CBP branch to tackle the incomplete issue. Besides, in each step, we intro-duce both confident knowledge distillation and representa-tion contrastive learning for pseudo-label denoising, effec-tively fusing complementarity for better results.
On two standard benchmarks: THUMOS14 and Activi-tyNet1.2, our method improves the average performance by 3.5% and 2.7% over state-of-the-art methods. We also con-duct extensive ablation studies to reveal the effectiveness of each component, both quantitatively and qualitatively.
To sum up, our contributions lie in three folds:
• We pioneer the first exploration in distilling free action knowledge from off-the-shelf VLP to facilitate WTAL;
• We design a novel distillation-collaboration framework that encourages the CBP branch and VLP branch to comple-ment each other, by an alternating optimization strategy;
• We conduct extensive experiments and ablation studies to reveal the significance of distilling VLP and our superior performance on two public benchmarks. 2.