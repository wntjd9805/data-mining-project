Abstract 1.

Introduction
Generating controllable and editable human motion se-quences is a key challenge in 3D Avatar generation. It has been labor-intensive to generate and animate human mo-tion for a long time until learning-based approaches have been developed and applied recently. However, these ap-proaches are still task-specific or modality-specific [1] [6]
[5] [18]. In this paper, we propose “UDE”, the first uni-fied driving engine that enables generating human motion sequences from natural language or audio sequences (see
Fig. 1). Specifically, UDE consists of the following key components: 1) a motion quantization module based on
VQVAE that represents continuous motion sequence as dis-crete latent code [33], 2) a modality-agnostic transformer encoder [34] that learns to map modality-aware driving signals to a joint space, and 3) a unified token transformer (GPT-like [24]) network to predict the quantized latent code index in an auto-regressive manner. 4) a diffusion motion decoder that takes as input the motion tokens and decodes them into motion sequences with high diversity. We evaluate our method on HumanML3D [8] and AIST+ [19] bench-marks, and the experiment results demonstrate our method achieves state-of-the-art performance. Project website: https://zixiangzhou916.github.io/UDE/
Synthesizing realistic human motion sequences has been
It is a pilar component in many real-world applications. labor-intensive and tedious, and requires professional skills to achieve the creation of one single piece of motion se-quence synthesis, making it hard to be democratized for broad content generations. Recently, the emergence of mo-tion capture and pose estimation [15] [38] [27] [36] have made it possible to synthesize human motion sequences from VTubers or source videos thanks to the advances of deep learning. Although these approaches have simplified the creation of motion sequences, actors or highly corre-lated videos are still necessary, thus limiting the scalability as well as the controllability.
The development of multi-modal machine learning paves a new way to human motion synthesis [1] [6] [8] [12] [17]
[2]. For example, natural language descriptions could be used to drive human motion sequences directly [1] [6] [8].
The language description is a straightforward representa-tion for human users to control the synthesis. It provides a semantic clue of what the synthesized motion sequence should look like, and the editing could be conducted by sim-ply changing the language description. Language, however, does not cover the full domain of human motion sequences.
In terms of dancing motion synthesis, for example, the nat-ural language is not sufficient to describe the dance rhythm.
For such scenarios, audio sequences are used as guidance to help motion synthesis, so the synthesized motion could match the music beat rhythmically and choreography style.
However, these approaches are studied separately in prior works. In many real-world applications, the characters are likely to perform a complex motion sequence composed of both rhythmic dances from music and certain actions de-scribed by language, and smooth transition between mixed modality inputs becomes vital important. As a result, multi-modal motion consistency would become an urgent issue to solve if employed siloed modality-specific models.
To address above mentioned problems, in this work, we propose a Unified Driving Engine (UDE) which unifies the human motion generation driven by natural language and music clip in one shared model. Our model consists of four key components. First, we train a codebook using
VQ-VAE. For the codebook, each code represents a certain pattern of the motion sequence. Second, we introduce a
Modality-Agnostic Transformer Encoder (MATE). It takes the input of different modalities and transforms them into sequential embedding in one joint space. The third com-ponent is a Unified Token Transformer (UTT). We feed it with sequential embedding obtained by MATE and pre-dict the motion token sequences in an auto-regressive man-ner. The fourth component is a Diffusion Motion Decoder (DMD). Unlike recent modality-specific works [30] [37], our DMD is modality-agnostic. Given the motion token se-quences, DMD decodes them to motion sequences in con-tinuous space by the reversed diffusion process.
We summarize our contributions in four folds: 1) We model the continuous human motion generation problem as a discrete token prediction problem. 2) We unify the text-driven and audio-driven motion generation into one sin-gle unified model. By learning MATE, we can map in-put sequences of different modalities into joint space. Then we can predict motion tokens with UTT regardless of the modality of input. 3) We propose DMD to decode the mo-tion tokens to motion sequence. Compared to the decoder in VQ-VAE, which generates deterministic samples, our
DMD can generate samples with high diversity. 4) We eval-uate our method extensively and the results suggest that our method outperforms existing methods in both text-driven and audio-driven scenarios. More importantly, our experi-ment also suggests that our UDE enables smooth transition between mixed modality inputs. 2.