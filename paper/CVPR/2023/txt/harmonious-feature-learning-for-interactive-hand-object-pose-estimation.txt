Abstract
Joint hand and object pose estimation from a single image is extremely challenging as serious occlusion often occurs when the hand and object interact. Existing ap-proaches typically first extract coarse hand and object fea-tures from a single backbone, then further enhance them with reference to each other via interaction modules. How-ever, these works usually ignore that the hand and ob-ject are competitive in feature learning, since the backbone takes both of them as foreground and they are usually mu-tually occluded.
In this paper, we propose a novel Har-monious Feature Learning Network (HFL-Net). HFL-Net introduces a new framework that combines the advantages of single- and double-stream backbones: it shares the pa-rameters of the low- and high-level convolutional layers of a common ResNet-50 model for the hand and object, leav-ing the middle-level layers unshared. This strategy enables the hand and the object to be extracted as the sole targets by the middle-level layers, avoiding their competition in feature learning. The shared high-level layers also force their features to be harmonious, thereby facilitating their mutual feature enhancement. In particular, we propose to enhance the feature of the hand via concatenation with the feature in the same location from the object stream. A sub-sequent self-attention layer is adopted to deeply fuse the concatenated feature. Experimental results show that our proposed approach consistently outperforms state-of-the-art methods on the popular HO3D and Dex-YCB databases.
Notably, the performance of our model on hand pose esti-mation even surpasses that of existing works that only per-form the single-hand pose estimation task. Code is avail-able at https://github.com/lzfff12/HFL-Net. 1.

Introduction
When humans interact with the physical world, they pri-marily do so by using their hands. Thus, an accurate un-derstanding of how hands interact with objects is essen-*Corresponding author.
Figure 1. HFL-Net predicts the 3D hand and object poses from single monocular RGB images accurately, even in serious occlu-sion scenarios. tial to the understanding of human behavior.
It can be widely applied to a range of fields, including the devel-opment of virtual reality [36], augmented reality [33, 34], and imitation-based robot learning [35], among others. Re-cently, hand pose estimation [12–16] and 6D object pose estimation [17–19] based on monocular RGB images have respectively achieved remarkable results. However, the re-search into joint hand-object pose estimation under circum-stances of interaction remains in its infancy [2,3,23,26–28].
As illustrated in Figure 1, joint hand-object pose esti-mation from a single image is extremely challenging. The main reason for this is that when the hand and object inter-act with each other, serious occlusion occurs; occlusion, in turn, results in information loss, increasing the difficulty of each task.
One mainstream solution to this problem is to utilize context. Due to physical constraints, the interacting hand and object tend to be highly correlated in terms of their poses, meaning that the appearance of one can be useful context for the other [1–3]. Methods that adopt this solu-tion typically employ a single backbone to extract features for the hand and object, respectively [2, 22, 27]. This uni-fied backbone model ensures that the hand and object fea-tures are in the same space, which facilitates the subsequent mutual feature enhancement between hand and object via attention-based methods [2].
However, the hand and object pose estimation tasks are competitive in feature learning if a single backbone model is utilized. In more detail, when the hand and object are close to each other, the backbone model treats them both as foreground, and may thus be unable to differentiate the hand features from those of the object. A straightforward solution is to utilize two backbones [1, 3, 23], one for the hand and the other one for the object; when this approach is adopted, each backbone has only one target as the foreground. The main downsides of this strategy include large model size and (more importantly) the different feature spaces between backbones, which introduce difficulties with regard to mu-tual feature enhancement between the hand and object.
To solve the aforementioned problems, we propose a novel Harmonious Feature Learning Network (HFL-Net).
HFL-Net introduces a new framework that combines the advantages of single- and double-stream backbones.
In more detail, our backbone shares the parameters of the low-and high-level convolutional layers of a common ResNet-50 model [4] for the hand and object, leaving the middle-level layers unshared. Feature maps produced by low-level layers are fed into the two sets of middle-level layers, which regard the hand and object respectively as the sole foreground tar-get. As a result, feature learning for the hand and object is no longer competitive. Finally, through sharing the param-eters of the high-level convolutional layers, the hand and object features are forced to be in similar feature spaces. In this way, our backbone realizes harmonious feature learning for the hand and object pose estimation.
We further enhance the representation power of the hand and object features through the use of efficient attention models. Several existing methods have successfully real-ized hand-to-object feature enhancement via cross-attention operations [1, 2]; however, object-to-hand feature enhance-ment usually turns out to be difficult [1, 2]. Motivated by the observation that when one pixel on the hand is occluded, the object feature in the same location usually provides use-ful cues, we propose a simple but effective strategy for fa-cilitating object-to-hand feature enhancement. Specifically, we adopt ROIAlign [6] to extract fixed-size feature maps from the two output streams of our backbone respectively according to the hand bounding box. We then concatenate the two feature maps along the channel dimension and feed the obtained feature maps into a self-attention module [7].
Object-to-hand feature enhancement is automatically real-ized via the fully-connected and multi-head attention layers in the self-attention module. Finally, we split the output feature maps by the self-attention layer along the channel dimension, and take the first half as the enhanced hand fea-ture maps.
We demonstrate the effectiveness of HFL-Net through comprehensive experiments on two benchmarks: HO3D [9] and Dex-YCB [10], and find that our method consistently outperforms state-of-the-art works on the joint hand-object pose estimation task. Moreover, benefiting from the learned harmonious hand and object features, the hand and object pose estimation tasks in HFL-Net are mutually beneficial
In our experiments, the perfor-rather than competitive. mance of HFL-Net on the hand pose estimation task sur-passes even recent works [12,15,32] that only estimate hand poses in both the training and testing stages. 2.