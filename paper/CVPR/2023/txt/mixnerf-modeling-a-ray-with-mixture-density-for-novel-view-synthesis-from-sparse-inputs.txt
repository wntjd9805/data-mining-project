Abstract
Neural Radiance Field (NeRF) has broken new ground in the novel view synthesis due to its simple concept and state-of-the-art quality. However, it suffers from severe per-formance degradation unless trained with a dense set of im-ages with different camera poses, which hinders its prac-tical applications. Although previous methods addressing this problem achieved promising results, they relied heav-ily on the additional training resources, which goes against the philosophy of sparse-input novel-view synthesis pursu-ing the training efficiency. In this work, we propose MixN-eRF, an effective training strategy for novel view synthe-sis from sparse inputs by modeling a ray with a mixture density model. Our MixNeRF estimates the joint distribu-tion of RGB colors along the ray samples by modeling it with mixture of distributions. We also propose a new task of ray depth estimation as a useful training objective, which is highly correlated with 3D scene geometry. Moreover, we re-model the colors with regenerated blending weights based on the estimated ray depth and further improves the robust-ness for colors and viewpoints. Our MixNeRF outperforms other state-of-the-art methods in various standard bench-marks with superior efficiency of training and inference. 1.

Introduction
A photo-realistic view synthesis is one of the major re-search topics in computer vision. Recently, the coordinate-based neural representation [6, 25, 26, 30] has gained much popularity for the novel view synthesis task. Among them,
Neural Radiance Field (NeRF) [28], which models a 3D scene by learning from a dense set of 2D images, enabled high-quality view synthesis with a simple concept and has become the prevailing mainstream. However, NeRF suffers
∗D. Han and Y. Chang equally contributed to this work.
This work was supported by NRF grant (2021R1A2C3006659) and
IITP grants (2021-0-01343, 2022-0-00953) funded by Korean Govern-ment.
Figure 1. Comparison with the vanilla mip-NeRF [1] and other regularization methods. Given the same number of training batch and iterations, our MixNeRF outperforms mip-NeRF and Diet-NeRF [12] by a large margin with comparable or shorter training time. Compared to RegNeRF [29], ours achieves superior perfor-mance with about 42% shortened training time. The size of the circles are proportional to the number of input views, indicating 3/6/9-view, respectively. More details are provided in Sec. 4.2. from severe performance degradation in real-world applica-tions, e.g. AR/VR, autonomous driving, and so on, where only a sparse set of views are available due to the burden-some task of collecting dense training images.
One of the key factors for a model’s high-quality ren-dering with limited input views is its robustness in 3D ge-ometry learning, i.e. accurate depth estimation for a scene.
There are several works to address this problem and it can be classified into two major paradigms: pre-training and regularization approaches. For the pre-training ap-proach [5,7,13,16,20,22,31,34,37,42], a general 3D geom-etry is trained by the multi-view images from a large-scale dataset and per-scene finetuning is optionally conducted in the test time. Although it has achieved promising results, it still requires the expensive cost for collecting a large-scale dataset across different scenes for pre-training and is not well-generalized for a novel domain in the test time.
Another line of research, the regularization approach [9, 12, 18, 29, 32, 39], performs per-scene optimization from scratch by applying regularization to prevent being overfit-ted from the limited inputs. Most existing methods of this kind depend heavily on the extra training resources for com-pensating a lack of supervisory signals, e.g. depth-map gen-eration by running SfM [9, 32], unseen ray generation with arbitrary camera poses [18,29], leveraging external modules to exploit additional features [12,29,39], or so on. However, the additional training data might not always be available and the external modules should be pre-trained with a large-scale dataset. This is against the philosophy of novel view synthesis from sparse inputs, pursuing training efficiency.
In this work, we propose MixNeRF, an effective regu-larization approach for novel view synthesis from sparse in-puts, modeling the colors along a ray with a mixture density model which represents a complex distribution with a mix-ture of component distributions. Since the concept of a mix-ture model is in line with that of alpha compositing in that both are represented by the weighted combination, we are able to regularize effectively both the colors and the densi-ties of the samples along a ray by exploiting the blending weights as mixing coefficients for our mixture model. Fur-thermore, we propose a new auxiliary task of ray depth es-timation for learning the 3D geometry which is crucial for the rendering quality. Since the estimated 3D geometry is highly correlated with the scene depth estimation, our pro-posed training objective acts as a useful supervisory signal.
Finally, we regenerate the blending weights based on the estimated ray depth and remodel a ray. Since the estimated depth is not exactly the same, but nearly identical to the ground truth, it can play a role of pseudo geometry for adja-cent points of the sample, like an unseen viewpoint. By re-modeling the samples with the mixing coefficients based on the regenerated blending weights, we can further improve the robustness for shift of colors and viewpoints. Our main contributions are summarized as follows:
• Our method estimates the joint distribution of RGB color values along the ray samples by a mixture of distributions, learning the 3D geometry successfully with sparse views.
• We propose a ray depth estimation as an effective auxil-iary task for few-shot novel view synthesis, playing a role of useful training objective.
• We use the regenerated blending weights based on the es-timated ray depths for improving the robustness with neg-ligible extra training cost.
• Our MixNeRF outperforms other state-of-the-art methods in the different standard benchmarks, showing much im-proved training and inference efficiency. 2.