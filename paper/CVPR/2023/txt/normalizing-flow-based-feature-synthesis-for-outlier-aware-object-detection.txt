Abstract
Real-world deployment of reliable object detectors is crucial for applications such as autonomous driving. How-ever, general-purpose object detectors like Faster R-CNN are prone to providing overconfident predictions for outlier objects. Recent outlier-aware object detection approaches estimate the density of instance-wide features with class-conditional Gaussians and train on synthesized outlier fea-tures from their low-likelihood regions. However, this strat-egy does not guarantee that the synthesized outlier features will have a low likelihood according to the other class-conditional Gaussians. We propose a novel outlier-aware object detection framework that distinguishes outliers from inlier objects by learning the joint data distribution of all inlier classes with an invertible normalizing flow. The ap-propriate sampling of the flow model ensures that the syn-thesized outliers have a lower likelihood than inliers of all object classes, thereby modeling a better decision bound-ary between inlier and outlier objects. Our approach sig-nificantly outperforms the state-of-the-art for outlier-aware object detection on both image and video datasets. 1.

Introduction
General purpose object detectors such as Faster R-CNN [42] and Mask R-CNN [17] deliver high performance for inlier images. However, in many real-world scenarios, such as autonomous driving [3], plenty of unknown outliers (OD) naturally occur in an image or a video scene. Due to the co-existence of OD with the labeled inlier (ID) ob-jects in the scene, object detectors confuse outliers with in-liers. Therefore, reliable object-detection deployments re-quire detecting such anomalies without degrading the per-formance of inlier object detection.
Many outlier detection approaches focus on multi-class image classification task by either performing outlier de-tection during inference [19, 24, 30, 31, 35, 44] or training on real outlier data [1, 20, 39, 53]. However, such OD in-puts are unaware of the decision boundary between inliers and outliers, resulting in an inaccurate model regularization.
A popular work [29] proposed a novel training scheme to
Figure 1. We compare distributions of log-likelihoods for inliers (ID), background patches, and synthetic outlier (OD) samples by applying a pre-trained model on Pascal-VOC [11] validation set.
The left plot shows the likelihoods of class-conditional Gaussians from the official approach of VOS [10]. The right plot shows the likelihoods recovered with our normalizing flow. We observe that our flow separates the three distributions much better than VOS. generate synthetic outlier samples in the high-dimensional pixel space using Generative Adversarial Networks for out-lier aware training of an image classification model. How-ever, GANs are challenging to optimize and are likely to deliver insufficient coverage [36]. Moreover, the previous work [29] generates an entire image as an outlier sample, whereas in an object detection problem, both inliers and outliers can coexist in the same image space.
Recently, [10] proposed to model the inlier features as class-conditional Gaussians and then synthesize OD fea-tures from the low likelihood region of the modeled distri-bution. Even though density is more easily estimated in the feature space than in pixel space, the assumption of class-conditional Gaussian may not provide an accurate decision boundary between inliers and outliers. Furthermore, synthe-sizing a low-likelihood OD sample from the Gaussian for class A does not guarantee that the sample is also of the low likelihood for the Gaussian of class B, as indicated in the left plot of Figure 1. Recent work for object detection in video [9] extracts the background patches for uncertainty regularization by thresholding the dissimilarity with the ref-erence inlier features. However, the extracted background patches may lie far from the inlier-outlier decision bound-ary, leading to sub-optimal regularization of the model.
We propose a novel approach for open-set object detec-tion which we call Flow Feature Synthesis (FFS). Our ap-proach trains an invertible normalizing flow to map the data distribution of ID features of all object classes to a latent
representation that conforms to a multivariate Gaussian dis-tribution with zero mean and diagonal unit covariance. As a result, it ensures principled estimation of the complex dis-tribution of inlier features from all classes. We synthesize outlier features from low-likelihood regions of the learned distribution. This enables outlier features to be near the ID-OD decision boundary, leading to more robust uncertainty regularization of the object detector. In contrast, using gen-erative adversarial models for the same task would be cum-bersome due to its inability to infer density of the generated samples, Furthermore, variational autoencoders can only in-fer the lower bound of the sample density [27].
Specifically, FFS optimizes the normalizing flow model by maximum likelihood training on ID features. This train-ing scheme enables the model to estimate the actual data distribution of the available ID features. Next, FFS uti-lizes the invertibility of the flow model to randomly sam-ple from its latent space and generate synthetic features in the reverse direction of the model. The normalizing flow allows efficient and exact inference of the distribution den-sity in the generated samples. Consequently, our approach can deliver suitable synthetic outlier data in fewer iterations than VOS [10].
It also requires fewer synthetically gen-erated samples than VOS [10] to obtain OD features from the low-likelihood region of the modeled distribution by the flow. Furthermore, we developed our end-to-end trainable
FFS framework to be effective on both image and video datasets. In contrast, previous works [9, 10] proposed stan-dalone strategies for each task. Our main contributions are:
• We present a new outlier-aware object detection frame-work that utilizes Normalizing Flows to model the joint data distribution of inlier features.
Invertibility of the flow allows efficient generation of synthetic out-liers for effective uncertainty regularization.
• By mapping the data distribution of inlier features from all object classes to a multivariate Normal dis-tribution in the flow’s latent space, FFS ensures that an outlier sampled using the flow model is OD with reference to all ID classes.
• FFS achieves better OD detection performance while training faster than VOS [10] and STUD [9], due to having to generate fewer synthetic samples.
• We show that our method achieves state-of-the-art performance in OD object detection while preserv-ing the baseline ID detection performance for image dataset PASCAL-VOC [11] and video datasets such as
Youtube-VIS [51] and BDD100K [52]. 2.