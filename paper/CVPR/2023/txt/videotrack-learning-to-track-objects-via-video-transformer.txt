Abstract
Existing Siamese tracking methods, which are built on pair-wise matching between two single frames, heavily rely on additional sophisticated mechanism to exploit tempo-ral information among successive video frames, hindering them from efﬁciency and industrial deployments.
In this work, we resort to sequence-level target matching that can encode temporal contexts into the spatial features through a neat feedforward video model. Speciﬁcally, we adapt the standard video transformer architecture to visual tracking by enabling spatiotemporal feature learning directly from frame-level patch sequences. To better adapt to the track-ing task, we carefully blend the spatiotemporal information in the video clips through sequential multi-branch triplet blocks, which formulates a video transformer backbone.
Our experimental study compares different model variants, such as tokenization strategies, hierarchical structures, and video attention schemes. Then, we propose a disentan-gled dual-template mechanism that decouples static and dy-namic appearance clues over time, and reduces temporal redundancy in video frames. Extensive experiments show that our method, named as VideoTrack, achieves state-of-the-art results while running in real-time. 1.

Introduction
Visual Object Tracking (VOT) is a fundamental problem in computer vision that aims to track an object of interest in a video given its bounding box in the ﬁrst frame [53]. In re-cent years, mainstream approaches formulate visual track-ing as a target matching problem, striking a good balance between performance and simplicity.
The philosophy of target matching is to ﬁnd the object by looking for locations in the search area whose features have the largest similarity with those in the target template. How-*This work was done when Fei Xie was an intern at Microsoft Research
Asia. (cid:11)(cid:68)(cid:20)(cid:12) (cid:55)(cid:72)(cid:80)(cid:83)(cid:79)(cid:68)(cid:87)(cid:72) (cid:54)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:3)(cid:73)(cid:85)(cid:68)(cid:80)(cid:72) (cid:40)(cid:80)(cid:69)(cid:72)(cid:71)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3) (cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)(cid:3)(cid:18)(cid:3)(cid:47)(cid:68)(cid:92)(cid:72)(cid:85) (cid:40)(cid:80)(cid:69)(cid:72)(cid:71)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3) (cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)(cid:3)(cid:18)(cid:3)(cid:47)(cid:68)(cid:92)(cid:72)(cid:85) (cid:1858)(cid:3053) (cid:11)(cid:68)(cid:21)(cid:12) (cid:1858)(cid:3051) (cid:11)(cid:68)(cid:20)(cid:12) (cid:50)(cid:81)(cid:79)(cid:76)(cid:81)(cid:72)(cid:3)(cid:87)(cid:72)(cid:80)(cid:83)(cid:79)(cid:68)(cid:87)(cid:72)(cid:3)(cid:88)(cid:83)(cid:71)(cid:68)(cid:87)(cid:76)(cid:81)(cid:74) (cid:11)(cid:68)(cid:21)(cid:12) (cid:55)(cid:72)(cid:80)(cid:83)(cid:82)(cid:85)(cid:68)(cid:79)(cid:3)(cid:70)(cid:82)(cid:81)(cid:87)(cid:72)(cid:91)(cid:87)(cid:3)(cid:83)(cid:85)(cid:82)(cid:83)(cid:68)(cid:74)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3) (cid:48)(cid:68)(cid:87)(cid:70)(cid:75)(cid:76)(cid:81)(cid:74)(cid:3)(cid:50)(cid:83)(cid:72)(cid:85)(cid:68)(cid:87)(cid:82)(cid:85) (cid:18)(cid:3)(cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78) (cid:51)(cid:85)(cid:72)(cid:71)(cid:76)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3) (cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78) (cid:11)(cid:68)(cid:12)(cid:3)(cid:51)(cid:68)(cid:76)(cid:85)(cid:16)(cid:90)(cid:76)(cid:86)(cid:72)(cid:3)(cid:80)(cid:68)(cid:87)(cid:70)(cid:75)(cid:76)(cid:81)(cid:74)(cid:3)(cid:76)(cid:81)(cid:3)(cid:54)(cid:76)(cid:68)(cid:80)(cid:72)(cid:86)(cid:72)(cid:3)(cid:87)(cid:85)(cid:68)(cid:70)(cid:78)(cid:76)(cid:81)(cid:74)(cid:3)(cid:73)(cid:85)(cid:68)(cid:80)(cid:72)(cid:90)(cid:82)(cid:85)(cid:78) (cid:40)(cid:80)(cid:69)(cid:72)(cid:71)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3) (cid:47)(cid:68)(cid:92)(cid:72)(cid:85) (cid:1858)(cid:3053) (cid:1858)(cid:3047)(cid:2868) (cid:1858)(cid:3047)(cid:2869) (cid:1858)(cid:3047)(cid:3041)(cid:1858)(cid:1858) (cid:1858)(cid:3047)(cid:3041) (cid:485) (cid:1858)(cid:3051)(cid:1858)(cid:1858) (cid:1858)(cid:3051) (cid:1858) (cid:54)(cid:83)(cid:68)(cid:87)(cid:76)(cid:82)(cid:87)(cid:72)(cid:80)(cid:83)(cid:82)(cid:85)(cid:68)(cid:79)(cid:3) (cid:41)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)(cid:3)(cid:47)(cid:72)(cid:68)(cid:85)(cid:81)(cid:76)(cid:81)(cid:74) (cid:51)(cid:85)(cid:72)(cid:71)(cid:76)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3) (cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78) (cid:57)(cid:76)(cid:71)(cid:72)(cid:82)(cid:16)(cid:70)(cid:79)(cid:76)(cid:83)(cid:3)(cid:76)(cid:81)(cid:83)(cid:88)(cid:87) (cid:132) (cid:54)(cid:72)(cid:84)(cid:88)(cid:72)(cid:81)(cid:70)(cid:72)(cid:16)(cid:79)(cid:72)(cid:89)(cid:72)(cid:79)(cid:3)(cid:80)(cid:68)(cid:87)(cid:70)(cid:75)(cid:76)(cid:81)(cid:74)(cid:3)(cid:76)(cid:81)(cid:3)(cid:57)(cid:76)(cid:71)(cid:72)(cid:82)(cid:55)(cid:85)(cid:68)(cid:70)(cid:78) (cid:73)(cid:85)(cid:68)(cid:80)(cid:72)(cid:90)(cid:82)(cid:85)(cid:78)
Figure 1. Comparing to the pair-wise matching pipeline in
Siamese tracking shown in (a), which requires sophisticated mech-anisms (a1)/(a2) to exploit temporal contexts, our neat video trans-former tracking (VideoTrack) framework, as shown in (b), directly lifts the pair-wise feature matching into spatiotemporal domain. ever, target matching methods generally adopt per-frame object matching manner, where the rich temporal informa-tion in the video is largely overlooked. The representa-tive methods are Siamese trackers [4, 20, 26, 27, 57]: pair-wise frames are fed into Siamese network to extract fea-tures and a matching network/operator is applied for target matching. Although recent pure transformer-based Siamese trackers [10, 16, 55, 61] unify feature extraction and match-ing into a single step by leveraging the Vision Transformer (ViT) [14, 31, 49], these Siamese trackers still follow a pair-wise matching philosophy that hinders their exploitation of temporal context.
To explore temporal information, some works have pro-posed sophisticated yet complex temporal modelling meth-ods to improve the robustness of pair-wise Siamese track-ers, where online template updating [60, 64] and tempo-ral context propagating among frame-wise features [47] are two widely-adopted paradigms. Despite their great success, extra hand-crafted hyper-parameters and complex network modules are inevitably introduced to the Siamese pipeline, which have a negative impact on the efﬁciency and are not friendly to embedded devices. A natural question therefore arises: can we exploit the temporal context while still main-tain the tracking pipeline in a neat, end-to-end fashion?
To get rid of the customized updating strategies and re-dundant temporal modelling modules, we directly expand the pair-wise input frames into video-level (see Fig. 1), to capture rich temporal contexts. Speciﬁcally, we resort to video transformers [1] to learn spatiotemporal features, and establish the inter-frame temporal dependencies by simple feedforward modelling. Compared to the popular pair-wise
Siamese matching [4, 27], our video transformer tracking pipeline (VideoTrack) lifts the 2D matching pipeline into the spatiotemporal domain, allowing the direct processing of video-level inputs. Moreover, we modify the video trans-former architecture to better adapt it to tracking task, based on the following observations and prior knowledge:
Feature learning. A good feature representation is vital for the downstream vision tasks. Equipped with dedicated learning scheme in network layers, feature representations can be effectively enhanced from the shallow to deep level.
Thus, we attempt to encode the temporal contexts at feature-level by utilizing a video transformer backbone. To ensure the generality and feasibility, we design our video trans-former model with the following principles: 1) Scalabil-ity: Transformer layer is the basic building unit that can be stacked to construct the backbone network in different model scales. 2) Compatibility: To avoid expensive pre-training costs, the modiﬁed network should ideally be com-patible with the model paramaters of the image-based vision backbone, e.g. ViT [14]. It not only can utilize the available pretraining weights, but also prevents the possible perfor-mance degeneration during ﬁne-tuning.
Appearance vs. motion clue. Video could be viewed as a temporal evolution of a static appearance. Compared to the video recognition task which takes the complete frame as input, the input frames for most trackers are locally cropped from the online predicted target location, which weakens the motion clue in video-clips. Thus, we focus more on uti-lizing the appearance clues. To leverage the strong prior in video sequences, we explicitly divide them into three cate-gories: initial frame containing strong appearance informa-tion, intermediate frames which contain the dynamic states of the target and search frame containing the target to be predicted. Thus, we formulate a three-branch architecture for the VideoTrack model.
Temporal redundancy. Consecutive video frames are
It is vital to reduce the temporal re-highly redundant. dundancy as well as effectively modelling temporal con-texts. Thus, we evaluate three basic temporal modelling approaches in terms of efﬁciency, i.e. joint space-time, tem-poral window and message token attention. With careful analysis, we propose a disentangled dual-template mecha-nism (see Sec. 3.4 for details) to integrate into the video backbone which decouples the redundant video information into the static & dynamic templates.
As shown in Fig. 2, we propose our VideoTrack frame-work on top of ViT [14], formulated by interleaving a series of building units, named as triplet-block. The triplet-block has three hierarchical attention layers that mix the informa-tion ﬂow asymmetrically among three branches. Spatiotem-poral guidance from the historical frames is passed to the current search frame, obtaining a compact feature represen-tation for the ﬁnal target prediction.
In summary, the main contributions are as follows:
• In contrast to existing Siamese tracking methods and their labor-intensive temporal modelling, we for the ﬁrst time lift the 2D pair-wise matching to spatiotemporal domain, encoding temporal context at the feature-level via a neat feedforward video model, i.e. video vision transformer.
• We make the ﬁrst attempt to adapt video transformer to visual tracking. A thorough ablation analysis of video transformer tracking is conducted, including tokenisation strategies, model variants and temporal modelling ap-proaches. Comprehensive analysis may inspire follow-ers to solve VOT task from the perspective of video-level modelling. Moreover, our tracker exhibits encouraging results on multiple VOT benchmarks. 2.