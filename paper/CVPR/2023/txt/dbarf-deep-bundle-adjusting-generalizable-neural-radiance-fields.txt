Abstract
Recent works such as BARF and GARF can bundle ad-just camera poses with neural radiance fields (NeRF) which is based on coordinate-MLPs. Despite the impressive re-sults, these methods cannot be applied to Generalizable
NeRFs (GeNeRFs) which require image feature extractions that are often based on more complicated 3D CNN or trans-former architectures. In this work, we first analyze the dif-ficulties of jointly optimizing camera poses with GeNeRFs, and then further propose our DBARF to tackle these issues.
Our DBARF which bundle adjusts camera poses by tak-ing a cost feature map as an implicit cost function can be jointly trained with GeNeRFs in a self-supervised manner.
Unlike BARF and its follow-up works, which can only be applied to per-scene optimized NeRFs and need accurate initial camera poses with the exception of forward-facing scenes, our method can generalize across scenes and does not require any good initialization. Experiments show the effectiveness and generalization ability of our DBARF when evaluated on real-world datasets. Our code is available at https:// aibluefisher.github.io/ dbarf . 1.

Introduction
The recent introduction of NeRF (Neural Radiance
Fields) [28] bridges the gap between computer vision and computer graphics with the focus on the Novel view synthe-sis (NVS) task. NeRF demonstrates impressive capability of encoding the implicit scene representation and rendering high-quality images at novel views with only a small set of coordinate-based MLPs. Although NeRF and its variants simplify the dense 3D reconstruction part of the traditional photogrammetry pipeline that includes: the reconstruction of dense point clouds from posed images followed by the recovery and texture mapping of the surfaces into just a simple neural network inference, they still require known accurate camera poses as inputs.
Nonetheless, the acquisition of camera poses is expen-sive in the real world. Most NeRF-related methods ob-tain the camera poses by Structure-from-Motion (SfM) [4,
In SfM, camera poses are optimized under the 23, 34].
Figure 1. Results of optimizing camera poses with BARF and
DBARF. From left to right are the initial camera poses, bird’s eye view (BEV) of optimized camera poses after 1e4 iterations, and side view (SV) of optimized camera pose after 2e4 iterations. Red and blue denote ground truths and estimated camera poses (The inconsistent ground truth poses in different iterations are due to the randomness of selecting the training batches). Top: The cam-era poses diverge quickly when BARF [20] is applied to GeNeRF, even with the camera poses initialized by perturbing the ground truth with very small noise. Bottom: Results obtained by our
DBARF, the camera poses are randomly initialized. keypoint-metric reprojection error in a process referred to as bundle adjustment [43]. A notorious problem of SfM is that it sometimes fails, e.g. in textureless or self-similar scenes, and can also take days or even weeks to complete for large-scale scenes. Consequently, one main forthcoming issue with NeRF is that its rendering quality highly relies on accu-rate camera poses. Recently, several works try to solve the pose inaccuracy jointly with NeRF. One of the representa-tive works is BARF [20]. NeRF maps the pixel coordinates into high-dimensional space as Fourier features [39] before inputting into the MLPs to enable networks to learn the high-frequency part of images. However, Fourier features can be a double-edged sword when the camera poses are jointly optimized with NeRF, where gradients from high-frequency components dominate the low-frequency parts during training. To mitigate this problem, BARF draws inspiration from the non-smoothness optimization in high-dimensional functions: optimizer can get stuck at a local optimum, but the training can be easier when the objective
function is made smoother. Consequently, BARF adopts a coarse-to-fine strategy which first masks out the high-frequency components, and then gradually reactivates them after the low-frequency components become stable. The camera poses are adjusted by the photometric loss during training instead of the keypoint-metric cost in SfM. Despite its promising results, BARF and its follow-up works [5, 26] still require the pre-computed camera poses from SfM.
One other issue with vanilla NeRF is that it needs time-consuming per-scene training. Making NeRF generaliz-able across scenes [3, 18, 47, 53] has recently gained in-creasing attention. However, similar to vanilla NeRF, GeN-eRFs (generalizable NeRFs) also depend on accurate cam-era poses. There is no existing work that tried to optimize the camera poses jointly with GeNeRFs. This intrigues us to investigate the replacement of NeRF with GeNeRFs in
BARF. We find that the joint optimization is non-trivial in our task settings, and the camera poses can diverge quickly even when initialized with the ground truths (cf . top row of
Fig. 1).
In this paper, we identified two potential reasons which cause the failure of bundle adjusting GeNeRFs. The first reason is the aggregated feature outliers, which are caused by occlusions. The other reason is due to the high non-convexity of the cost function produced by ResNet fea-tures [40], which produces incoherent displacements like the issue caused by positional encodings [39] in BARF. We further proposed our method DBARF, which jointly opti-mizes GeNeRF and relative camera poses by a deep neural network. Our implicit training objective can be equivalently deemed as a smooth function of the coarse-to-fine training objective in BARF. Specifically, we construct a residual fea-ture map by warping 3D points onto the feature maps of the nearby views. We then take the residual feature map as an implicit cost function, which we refer to as cost map in the following sections. By taking the cost map as input, we utilize a deep pose optimizer to learn to correct the rela-tive camera poses from the target view to nearby views. We further jointly train the pose optimizer and a GeNeRF with images as supervision, which does not rely on ground truth camera poses. In contrast to previous methods which only focus on per-scene camera pose optimization, our network is generalizable across scenes.
In summary, the contributions of this work are:
• We conduct an experiment on bundle adjusting GeN-eRFs by gradient descent and analyze the difficulty of jointly optimizing camera poses with GeNeRFs.
• We present DBARF to deep bundle adjusting camera poses with GeNeRFs. The approach is trained end-to-end without requiring known absolute camera poses.
• We conduct experiments to show the generalization ability of our DBARF, which can outperform BARF and GARF even without per-scene fine-tuning. 2.