Abstract
We propose VDN-NeRF, a method to train neural ra-diance fields (NeRFs) for better geometry under non-Lambertian surface and dynamic lighting conditions that cause significant variation in the radiance of a point when viewed from different angles. Instead of explicitly model-ing the underlying factors that result in the view-dependent phenomenon, which could be complex yet not inclusive, we develop a simple and effective technique that normalizes the view-dependence by distilling invariant information al-ready encoded in the learned NeRFs. We then jointly train
NeRFs for view synthesis with view-dependence normal-ization to attain quality geometry. Our experiments show that even though shape-radiance ambiguity is inevitable, the proposed normalization can minimize its effect on geom-etry, which essentially aligns the optimal capacity needed for explaining view-dependent variations. Our method ap-plies to various baselines and significantly improves geom-etry without changing the volume rendering pipeline, even if the data is captured under a moving light source. Code is available at: https://github.com/BoifZ/VDN-NeRF. 1.

Introduction
Reconstructing the geometry and appearance of a 3D scene from a set of 2D images is one of the fundamen-tal tasks in computer vision and graphics. Recently, based on volume rendering, neural radiance fields (NeRFs) have shown great potential in capturing detailed appearance of the scene, evidenced by their high-quality view synthesis.
However, the reconstructed geometry is still far from sat-isfying. Since geometry is critical for physical interaction leveraging such scene representations, many recent works have started to improve the geometric reconstruction within the context of volume rendering, e.g., with surface render-ing or external signals that are more informative of the ge-*Equal contributions.
†Corresponding authors (yanchaoy@hku.hk, youyizheng@zju.edu.cn).
‡The State Key Lab of CAD&CG.
§The department of EEE and the Institute of Data Science.
Figure 1. We aim for quality geometry with NeRF reconstruction under inconsistency when viewing the same 3D point, for exam-ple, with images captured under a dynamic light field (top row), shown by the light spots on the truck cast from a torch moving with the camera. The middle two rows compare the reconstructed geometry from NeuS [41] (first column) and our method (second column). As observed, our method produces more details and bet-ter estimates the truck’s structure. The last row shows a novel view rendering from both methods. Even though our method normal-izes the view-dependence, it does not lose details on the synthe-sized images thanks to the regularity induced by better geometry. ometry, like depth scans.
Nevertheless, most improvements do not explicitly study the shape-radiance ambiguity that could induce degener-ated geometric reconstructions. This ambiguity persists as long as some capacity is needed to account for the direc-tional variations in the radiance, which is further amplified
if the ambient light field changes according to the observer’s viewpoint. For example, the NeRF’s Multi-Layer Percep-trons (MLPs) takes in a 3D location and a 2D direction vec-tor and outputs the observed color of this point from this specific viewing angle. If the MLP has sufficient capacity to model the directional phenomenon, a perfect photomet-ric reconstruction could be achieved even if the learned ge-ometry is entirely wrong. In other words, wrong geometry incurs more directional variations, which the MLP’s view-dependent branch can still encode, thus, rendering the pho-tometric reconstruction loss incapable of constraining the solutions.
On the other hand, directional capacity is needed to pre-vent the (view-dependent) photometric loss from distorting the geometry (confirmed through our experiments). With quality geometry as the central goal, we are now facing a tradeoff between the capacity of the view-dependent radi-ance function and the shape-radiance ambiguity. Namely, the MLP’s capacity has to be increased to explain the direc-tional variations, but if the capacity gets too large, shape-radiance ambiguity will permit degenerated geometric re-constructions. One can thus tune the capacity to achieve the best geometry for each scene, but it is time-consuming and infeasible since different scenes come with different levels of view dependence.
Instead of tuning the directional capacity, we adjust the view-dependence. More explicitly, we propose to normalize the directional variations by encoding invariant features dis-tilled from the same scene representation. By doing so, the view-dependence of different scenes is aligned to the same level so that a single optimal capacity can ensure good view synthesis and prevent shape-radiance ambiguity from de-grading the geometric estimation. Given the self-distillation property, the proposed view-dependence normalization can be jointly trained with the commonly used photometric re-construction loss.
It can also be easily plugged into any method relying on volume rendering for geometric scene reconstruction.
We demonstrate the effectiveness of the proposed view-dependence normalization on several datasets. Especially we verify that the level of view-dependence affects the min-imality of the shape-radiance ambiguity with tunable direc-tional capacity. And we show that our method effectively aligns the optimal capacity for each scene. To evaluate how our method works under a dynamically changing light field, we propose a new benchmark where a light source is mov-ing conditioned on the camera pose. We validate that the geometry obtained from our method degrades gracefully as the significance of the induced directional variations in-creases. In summary, we make the following contributions:
• We perform a detailed study of the coupling between the capacity of the view-dependent radiance function and the shape-radiance ambiguity in the context of vol-ume rendering.
• We propose a view-dependence normalization method that effectively aligns the optimality of the direc-tional function capacity under shape-radiance ambigu-ity for each scene and achieves state-of-the-art geome-try compared to various baselines.
• We quantitatively verify the robustness of our method when the light field changes to further increase the di-rectional variation, ensuring the applicability to sce-narios with unfavored lighting conditions. 2.