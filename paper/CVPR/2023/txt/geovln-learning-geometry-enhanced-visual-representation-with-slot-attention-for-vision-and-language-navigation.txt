Abstract
Most existing works solving Room-to-Room VLN prob-lem only utilize RGB images and do not consider lo-cal context around candidate views, which lack sufficient visual cues about surrounding environment. Moreover, natural language contains complex semantic information thus its correlations with visual inputs are hard to model
In this paper, we propose merely with cross attention.
GeoVLN, which learns Geometry-enhanced visual repre-sentation based on slot attention for robust Visual-and-Language Navigation. The RGB images are compensated with the corresponding depth maps and normal maps pre-dicted by Omnidata as visual inputs. Technically, we intro-duce a two-stage module that combine local slot attention and CLIP model to produce geometry-enhanced represen-tation from such input. We employ V&L BERT to learn a cross-modal representation that incorporate both language and vision informations. Additionally, a novel multiway at-tention module is designed, encouraging different phrases of input instruction to exploit the most related features from visual input. Extensive experiments demonstrate the effec-tiveness of our newly designed modules and show the com-pelling performance of the proposed method. 1.

Introduction
With the rapid development of vision, robotics, and AI research in the past decade, asking robots to follow hu-man instructions to complete various tasks is no longer an unattainable dream. To achieve this, one of the fundamen-tal problems is that given a natural language instruction, let robot (agent) make its decision about the next move automatically based on past and current visual observa-*Equal contributions.
†Corresponding authors.
Yanwei Fu is with School of Data Science, Fudan University,
Shanghai Key Lab of Intelligent Information Processing, and Fudan
ISTBI–ZJNU Algorithm Centre for Brain-inspired Intelligence, Zhejiang
Normal University, Jinhua, China.
Illustration of our learning geometry-enhanced vi-Figure 1. sual representation (GeoVLN) for visual-and-language navigation.
Critically, our GeoVLN utilizes the slot attention mechanism. tions. This is referred as Vision-and-Language Navigation
Importantly, such navigation abilities should (VLN) [2]. also work well in previously unseen environments.
In the popular Room-to-Room navigation task [2], the agent is typically assumed to be equipped with a single
RGB camera. At each time step, given a set of visual obser-vations captured from different view directions and several navigation options, the goal is to choose an option as the next station. The process will be repeated until the agent reaches the end point described by the user instruction. In-volving both natural language and vision information, the main challenge here is to learn a cross-modal representa-tion that incorporates the correlations between user instruc-tion and current surrounding environment to aid decision-making.
As solutions, early studies [2, 8, 30] resort to LSTM [13] to process temporal visual data stream. However, recent works [4, 11, 15, 18, 19, 22, 25] have taken advantage of the superior performance of the Transformer [31] and typically employ this attention-based model to facilitate representa-tion learning with cross attention and predict actions in ei-ther recurrent [15] or one-shot [4] fashion. Despite their advantages, these approaches still have several limitations.
• 1) They only rely on RGB images which provide very
limited 2D visual cues and lack geometry information.
Thus it is hard for agent to build scene understanding about novel environments;
• 2) they process each candidate view independently without considering local spatial context, leading to in-accurate decisions;
• 3) natural language contains high-level semantic fea-tures and different phrases within an instruction may focus on various aspects visual information, e.g. tex-ture, geometry. Nevertheless, we empirically find that constructing cross-modal representation with na¨ıve at-tention mechanism leads to suboptimal performance.
To address these problems, we propose a novel frame-work, named GeoVLN, which learns Geometry-enhanced visual representation based on slot attention for robust
Visual-and-Language Navigation. Our framework is illus-trated in Fig. 1.
In particular, beyond RGB images, we also utilize the corresponding depth maps and normal maps as observations at each time step (Fig. 1), as they provide rich geometry information about environment that facili-tates decision-making. Crucially, these additional mid-level cues are estimated by the recent scalable data generation framework Omnidata [7, 17] rather than sensor captured or user provided.
We design a novel two-stage slot attention [20] based module to learn geometry-enhanced visual representation from the above multimodal observations. Note that the slot attention is originally proposed to learn object-centric rep-resentation for complex scenes from single/multi-view im-ages, but we utilize its feature learning capability and ex-tend it to work together with multimodal observations in the
VLN tasks. Particularly, we treat each candidate RGB im-age as a query, and choose its nearby views as keys and val-ues to perform slot attention within a local spatial context.
The key insight is that our model can implicitly learn view-to-view correspondences via slot attention, and thus encour-age the candidates to pool useful features from surrounding neighbors. Additionally, we process all complementary ob-servations, including depth maps and normal maps, through a pre-trained CLIP [26] image encoder to obtain respective latent vectors. These vectors are then concatenated with the output of slot attention module to form our final geometry-enhanced visual representation.
On the other hand, we employ BERT as language en-coder to acquire global latent state and word embeddings from the input instruction. Given the respective latent em-beddings for language and vision inputs, we adopt V&L
BERT [11] to merge multimodal features and learn cross-modal representation for the final decision-making in a re-current fashion following [15]. Different from previous works [21,30] that directly output probabilities of each can-didate option, we present a multi-way attention module to encourage different phrases of input instruction to focus on the most informative visual observation, which boosts the performance of our network, especially in unseen environ-ments.
To summarize, we propose the following contributions that solve the Room-to-Room VLN task with compelling accuracy and robustness:
• We extend slot attention to work on VLN task, which is combined with CLIP image encoder to learn geometry-enhanced visual representations for accurate and robust navigation.
• A novel multiway attention module encouraging dif-ferent phrases of input instruction to focus on the most informative visual observation, e.g. texture, depth.
• We compensate RGB images with the corresponding depth maps and normal maps predicted with off-the-shelf method, improving the performance yet not in-volving additional training data.
• We integrate all the above technical innovations into a unified framework, named GeoVLN, and the extensive experiments validate our design choices. 2.