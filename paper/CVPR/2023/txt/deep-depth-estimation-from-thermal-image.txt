Abstract
Robust and accurate geometric understanding against adverse weather conditions is one top prioritized condi-tions to achieve a high-level autonomy of self-driving cars.
However, autonomous driving algorithms relying on the vis-ible spectrum band are easily impacted by weather and lighting conditions. A long-wave infrared camera, also known as a thermal imaging camera, is a potential res-cue to achieve high-level robustness. However, the miss-ing necessities are the well-established large-scale dataset and public benchmark results. To this end, in this pa-per, we first built a large-scale Multi-Spectral Stereo (MS2) dataset, including stereo RGB, stereo NIR, stereo thermal, and stereo LiDAR data along with GNSS/IMU informa-tion. The collected dataset provides about 195K synchro-nized data pairs taken from city, residential, road, campus, and suburban areas in the morning, daytime, and nighttime under clear-sky, cloudy, and rainy conditions. Secondly, we conduct an exhaustive validation process of monocu-lar and stereo depth estimation algorithms designed on visible spectrum bands to benchmark their performance in the thermal image domain. Lastly, we propose a uni-fied depth network that effectively bridges monocular depth and stereo depth tasks from a conditional random field approach perspective. Our dataset and source code are available at https://github.com/UkcheolShin/
MS2-MultiSpectralStereoDataset. (a) Depth from thermal images via unified depth network (b) RGB (Reference) (c) Thermal image (d) Depth map
Figure 1. Depth from thermal images in various environments.
Our proposed network can estimate both monocular and stereo depth maps regardless of given a single or stereo thermal image via unified network architecture. Furthermore, depth estimation results from thermal images show high-level reliability and robust-ness under day-light, low-light, and rainy conditions. 1.

Introduction
Recently, a number of researches have been conducted for accurate and robust geometric understanding in self-driving cars based on the widely-used benchmark datasets, such as KITTI [15], DDAD [17], and nuScenes [4]. Mod-ern computer vision algorithms deploy a deep neural net-work and data-driven machine learning technique to achieve high-level accuracy, which needs large-scale datasets. How-ever, from the perspective of robustness in real-world, the algorithms mostly rely on visible spectrum images and are easily degenerated by weather and lighting conditions.
Therefore, recent works have actively investigated alter-native sensors such as Near-Infrared (NIR) cameras [39],
LiDARs [16, 51], radars [14, 32], and long-wave infrared (LWIR) cameras [35, 45] to achieve reliable and robust geometric understanding in adverse conditions. Among these alternative and complementary sensors, LWIR cam-era (i.e., thermal camera) has become more popular be-cause of its competitive price, adverse weather robustness, and unique modality information (i.e., temperature). There-fore, various thermal image based computer vision solu-tions [3, 21–23, 27, 35, 45, 47–50] to achieve high-level ro-bustness have been actively attracting attention recently.
Table 1. Comprehensive comparison of multi-modal datasets. Compared to previous datasets [6, 8, 24, 25, 28, 53], the proposed Multi-Spectral Stereo (MS2) dataset provides about 195K synchronized and rectified multi-spectral stereo sensor data (i.e., RGB, NIR, thermal,
LiDAR, and GNSS/IMU data) covering diverse locations (e.g., city, campus, residential, road, and suburban), times (e.g., morning, daytime, and nighttime), and weathers (e.g., clear-sky, cloudy, and rainy).
Dataset
Year
Environment
Platform
CATS [53]
KAIST [6]
ViViD [25]
MultiSpectralMotion [8]
ViViD++ [24] 2017 2018 2019 2021 2022
In/Outdoor
Outdoor
In/Outdoor
In/Outdoor
Outdoor
OdomBeyondVision [28] 2022
Indoor
Ours 2022
Outdoor
Handheld
Vehicle
Handheld
Handheld
Vehicle
Handheld/
UGV/UAV
Vehicle
Total # of
Data Pairs 1.4K
Unknown 5.3K/4.3K 121K/27.3K 56K 71K/ 117K/21K 195K
LiDAR
IMU
RGB
NIR
Mono
Stereo Mono
Stereo Mono
Thermal
Stereo
RAW Daytime
Weather
Nighttime
Rain
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✕
✕
✕
✓
✓
✕
✕
✕
✓
✕
✓
✓
✕
✕
✕
✕
✕
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✕
✕
✕
✕
✕
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✕
✕
✕
✕
✕
✕
✓ are
However, the missing necessities the well-established large-scale dataset and public benchmark results. The publicly available datasets for autonomous driving are overwhelmingly composed of the visible spec-trum band (i.e., RGB images), but it very rarely includes other spectrum bands, such as the NIR band and LWIR band. Especially, despite the advantage of the LWIR band, just a few LWIR datasets have been recently released. these datasets are indoor oriented [8, 25, 28],
However, small scale [25, 53], publicly unavailable [6], or limited sensor diversity [6, 24]. Therefore, the necessity is getting increase to design a large-scale multi-sensor driving dataset to investigate the feasibility and challenges associated with an autonomous driving perception system from multi-spectral sensors.
The other necessity is thoroughly validating vision ap-plications on the LWIR band. Estimating a depth map from monocular and stereo images is one fundamental task for geometric understanding. Despite numerous recent stud-ies in depth estimation, these works have mainly focused on depth estimation using RGB images. However, thermal images, which typically have lower resolution, less texture, and more noise than RGB images, could pose a challenge for stereo-matching algorithms. This means that the perfor-mances of these previous works in thermal image domains are uncertain and may not be guaranteed.
To this end, in this paper, we provide a large-scale multi-spectral dataset along with exhaustive experimental results and a new perspective of depth unification to encourage ac-tive research of various geometry algorithms from multi-spectral data to achieve high-level performance, reliability, and robustness against hostile conditions. Our contributions can be summarized as follows:
• We provide a large-scale Multi-Spectral Stereo (MS2) dataset, including stereo RGB, stereo NIR, stereo ther-mal, and stereo LiDAR data along with GNSS/IMU data. Our dataset provides about 195K synchronized data pairs taken from city, residential, road, campus, and suburban areas in the morning, daytime, and night-time under clear-sky, cloudy, and rainy conditions.
• We perform exhaustive validation and investigate that monocular and stereo depth estimation algorithms originally designed for visible spectral bands work rea-sonably in thermal spectral bands.
• We propose a unified depth network that bridges monocular depth and stereo depth estimation tasks from the perspective of a conditional random field ap-proach. 2.