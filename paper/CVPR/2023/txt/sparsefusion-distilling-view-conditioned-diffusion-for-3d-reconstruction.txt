Abstract 1.

Introduction
We propose SparseFusion, a sparse view 3D recon-struction approach that unifies recent advances in neural rendering and probabilistic image generation. Existing approaches typically build on neural rendering with re-projected features but fail to generate unseen regions or handle uncertainty under large viewpoint changes. Alter-nate methods treat this as a (probabilistic) 2D synthesis task, and while they can generate plausible 2D images, they do not infer a consistent underlying 3D. However, we find that this trade-off between 3D consistency and probabilistic image generation does not need to exist. In fact, we show that geometric consistency and generative inference can be complementary in a mode-seeking behavior. By distilling a 3D consistent scene representation from a view-conditioned latent diffusion model, we are able to recover a plausible 3D representation whose renderings are both accurate and realistic. We evaluate our approach across 51 categories in the CO3D dataset and show that it outperforms exist-ing methods, in both distortion and perception metrics, for sparse-view novel view synthesis.
Consider the two images of the teddybear shown in Fig-ure 1 and try to imagine the underlying 3D object. Relying on the direct visual evidence in these images, you can easily infer that the teddybear is white, has a large head, and has small arms. Even more remarkably, you can imagine be-yond the directly visible to estimate a complete 3D model of this object e.g. forming a mental model of the teddy’s face with (likely black) eyes even though these were not observed. In this work, we build a computational approach that can similarly predict 3D from just a few images – by integrating visual measurements and priors via probabilis-tic modeling and then seeking likely 3D modes.
A growing number of recent works have studied the re-lated tasks of sparse-view 3D reconstruction and novel view synthesis, i.e. inferring 3D representations and/or synthe-sizing novel views of an object given just a few (typically 2-3) images with known relative camera poses. By lever-aging data-driven priors, these approaches can learn to effi-ciently leverage multi-view cues and infer 3D from sparse views. However, they still yield blurry predictions under
large viewpoint changes and cannot hallucinate plausible content in unobserved regions. This is because they do not account for the uncertainty in the outputs e.g. the unob-served nose of a teddybear may be either red or black, but these methods, by reducing inference to independent pixel-wise or point-wise predictions, cannot model such variation.
In this work, we propose to instead model the distribu-tion over the possible images given observations from some context views and an arbitrary query viewpoint. Leveraging a geometrically-informed backbone that computes pixel-aligned features in the query view, our approach learns a (conditional) diffusion model that can then infer detailed plausible novel-view images. While this probabilistic image synthesis approach allows the generation of higher quality image outputs, it does not directly yield a 3D representa-tion of underlying the object. In fact, the (independently) sampled outputs for each query view often do not even cor-respond to a consistent underlying 3D e.g. if the nose of the teddybear is unobserved in context views, one sampled query view may paint it red, while another one black.
To obtain a consistent 3D representation, we propose a
Diffusion Distillation technique that ‘distills’ the predicted distributions into an instance-specific 3D representation.
We note that the conditional diffusion model not only gives us the ability to sample novel-view images but also to (ap-proximately) compute the likelihood of a generated one.
Using this insight, we optimize an instance-specific (neural) 3D representation by maximizing the diffusion-based like-lihood of its renderings. We show that this leads to a mode-seeking optimization that results in more accurate and real-istic renderings, while also recovering a 3D-consistent rep-resentation of the underlying object. We demonstrate our approach on over 50 real-world categories from the CO3D dataset and show that our method allows recovering accu-rate 3D and novel views given as few as 2 images as input – please see Figure 1 for sample results. 2.