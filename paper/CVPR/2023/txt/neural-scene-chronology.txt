Abstract
In this work, we aim to reconstruct a time-varying 3D model, capable of rendering photo-realistic renderings with independent control of viewpoint, illumination, and time, from Internet photos of large-scale landmarks. The core challenges are twofold. First, different types of temporal changes, such as illumination and changes to the under-lying scene itself (such as replacing one graffiti artwork with another) are entangled together in the imagery. Sec-ond, scene-level temporal changes are often discrete and sporadic over time, rather than continuous. To tackle these problems, we propose a new scene representation equipped with a novel temporal step function encoding method that can model discrete scene-level content changes as piece-wise constant functions over time. Specifically, we represent the scene as a space-time radiance field with a per-image
The authors from Zhejiang University are affiliated with the State Key
Lab of CAD&CG. ∗This work was done when Haotong Lin was in a remote internship at Cornell University. †Corresponding author: Xiaowei Zhou. illumination embedding, where temporally-varying scene changes are encoded using a set of learned step functions.
To facilitate our task of chronology reconstruction from In-ternet imagery, we also collect a new dataset of four scenes that exhibit various changes over time. We demonstrate that our method exhibits state-of-the-art view synthesis results on this dataset, while achieving independent control of view-point, time, and illumination. Code and data are available at https://zju3dv.github.io/NeuSC/. 1.

Introduction
If we revisit a space we once knew during our childhood, it might not be as we remembered it. The buildings may have weathered, or have been newly painted, or may have been replaced entirely. Accordingly, there is no such thing as a single, authoritative 3D model of a scene—only a model of how it existed at a given instant in time. For a famous landmark, Internet photos can serve as a kind of chronicle of that landmark’s state over time, if we could organize the
information in those photos in a coherent way. For instance, if we could reconstruct a time-varying 3D model, then we could revisit the scene at any desired point in time.
In this work, we explore this problem of chronology re-construction, revisiting the work on Scene Chronology from nearly a decade ago [27]. As in that work, we seek to use In-ternet photos to build a 4D model of a scene, from which we can dial in any desired time (within the time interval where we have photos). However, the original Scene Chronol-ogy work was confined to reconstructing planar, rectangular scene elements, leading to limited photo-realism. We can now revisit this problem with powerful neural scene represen-tations, inspired by methods such as NeRF in the Wild [26].
However, recent neural reconstruction methods designed for
Internet photos assume that the underlying scene is static, which works well for landmarks with a high degree of per-manence, but fails for other scenes, like New York’s Times
Square, that feature more ephemeral elements like billboards and advertisements.
However, we find that adapting neural reconstruction methods [26] to the chronology reconstruction problem has many challenges, and that straightforward extensions do not work well. For instance, augmenting a neural radiance field (NeRF) model with an additional time input t, and fit-ting the resulting 4D radiance field to a set of images with timestamps yields temporally oversmoothed models, where different scene appearances over time are blended together, forming ghosted content; such a model underfits the tempo-ral signal. On the other hand, applying standard positional encoding [31] to the time input overfits the temporal sig-nal, conflating transient appearance changes due to factors like illumination with longer-term, sporadic changes to the underlying scene itself.
Instead, we seek a model that can disentangle transient, per-image changes from longer-term, scene-level changes, and that allows for independent control of viewpoint, time, and illumination at render-time. Based on the observation that scene-level content changes are often sudden, abrupt
“step function”-like changes (e.g., a billboard changing from one advertisement to another), we introduce a novel encod-ing method for time inputs that can effectively model piece-wise constant scene content over time, and pair this method with a per-image illumination code that models transient appearance changes. Accordingly, we represent 4D scene content as a multi-layer perceptron (MLP) that stores density and radiance at each space-time (x, y, z, t) scene point, and takes an illumination code as a side input. The time input t to this MLP is encoded with our proposed step function encoding that models piecewise constant temporal changes.
When fit to a set of input images, we find that our representa-tion can effectively factor different kinds of temporal effects, and can produce high-quality renderings of scenes over time.
To evaluate our method, we collect a dataset of images from Flickr and calibrate them using COLMAP, resulting in 52K successfully registered images. These photos are sourced from four different scenes, including dense tourist ar-eas, graffiti meccas, and museums, building upon the datasets used in Scene Chronology. These scenes feature a variety of elements that change over time, including billboards, graffiti art, and banners. Experiments on these scenes show that our method outperforms current state-of-the-art methods and their extensions to space-time view synthesis [6, 26]. We also present a detailed ablation and analysis of our proposed time encoding method.
In summary, our work makes the following contributions:
• To the best of our knowledge, ours is the first work to achieve photo-realistic chronology reconstruction, allowing for high-quality renderings of scenes with controllable viewpoint, time, and illumination.
• We propose a novel encoding method that can model abrupt content changes without overfitting to transient factors. This leads to a fitting procedure that can ef-fectively disentangle illumination effects from content changes in the underlying scene.
• We benchmark the task of chronology reconstruction from Internet photos and make our dataset and code available to the research community. 2.