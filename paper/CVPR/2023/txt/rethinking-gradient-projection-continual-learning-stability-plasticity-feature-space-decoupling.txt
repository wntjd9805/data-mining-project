Abstract
Continual learning aims to incrementally learn novel classes over time, while not forgetting the learned knowl-edge. Recent studies have found that learning would not forget if the updated gradient is orthogonal to the feature space. However, previous approaches require the gradient to be fully orthogonal to the whole feature space, leading to poor plasticity, as the feasible gradient direction becomes narrow when the tasks continually come, i.e., feature space is unlimitedly expanded. In this paper, we propose a space decoupling (SD) algorithm to decouple the feature space into a pair of complementary subspaces, i.e., the stability space I, and the plasticity space R. I is established by conducting space intersection between the historic and cur-rent feature space, and thus I contains more task-shared bases. R is constructed by seeking the orthogonal comple-mentary subspace of I, and thus R mainly contains task-specific bases. By putting distinguishing constraints on R and I, our method achieves a better balance between sta-bility and plasticity. Extensive experiments are conducted by applying SD to gradient projection baselines, and show
SD is model-agnostic and achieves SOTA results on publicly available datasets. 1.

Introduction
Deep neural networks (DNNs) have achieved promis-including im-ing performance on various vision tasks, age classification, object detection, and action recognition
[3, 9, 32, 34, 36]. However, DNNs are typically trained of-fline on a fixed dataset, and therefore the models are not able to incrementally learn novel concepts (novel classes), which has become an emerging need in many real-world
†Corresponding authors.
Figure 1. Left: Recent gradient projection methods. All of them constrain the gradient to be fully orthogonal to the feature space.
Right: We propose a space decoupling (SD) algorithm to decouple the feature space into a pair of complementary subspaces, i.e., the stability space I, and the plasticity space R. To balance stability and plasticity, more bases are preserved in I, and less in R, while stricter gradient constraints are put on I and looser on R. applications [11, 16, 21, 26, 30].
In this context, continual learning (CL) [14] is proposed, aiming to continually learn novel concepts, i.e., a series of learning tasks, while not forgetting the learned knowl-edge [1, 4, 6, 10, 15, 39]. Recent studies have found that learning would have less impact on old tasks if the direc-tion of the gradient is orthogonal to the space spanned by the features from old tasks [18, 23, 31, 38]. With this mo-tivation, a couple of continual learning methods referring to feature space methods have been proposed and can be generally divided into two classes: (a) Orthogonal based methods; (b) Null-space based methods.
Orthogonal based methods like GPM [31] and TRGP
[23] calibrate the gradient in the direction fully orthogo-nal to the feature space, while Null-space based methods like Adam-NSCL [38], AdNS [18] train the model in the null space of input features. It is easy to prove that these
two classes of approaches are equivalent and hold a unified training paradigm: 1) construct a matrix using the features from old tasks, e.g., concatenate; 2) utilize this matrix to approximate a feature space; 3) project the gradient of the new task to the orthogonal direction of the feature space.
However, we find all the mentioned approaches strictly require the gradient to be fully orthogonal to the whole fea-ture space, shown in the left of Figure 1. As the number of training tasks increases, feature space is unlimitedly ex-panded which will heavily limit the model updating and lead to poor plasticity. Therefore, feature space methods are facing a dilemma in balancing stability and plasticity
[27–29, 33, 40], despite their varied attempts in this issue.
Motivated by this insight, we propose a space decoupling (SD) algorithm, shown in the right of Figure 1. We decou-ple the whole feature space into a pair of orthogonal com-plementary subspaces, i.e., the stability-correlated space I, and the plasticity-correlated space R. In our implementa-tions, I is established by conducting space intersection be-tween the historic feature space and current feature space, and thus I contains more bases shared by old tasks. R is constructed by seeking the orthogonal complementary sub-space of I, and thus R mainly contains task-specific bases.
As we can see, the update on I would significantly incur forgetting, and the update on R would have less impact on old tasks. Our empirical study also supports this claim by finding that gradient updates within subspace I do more in-terference on old tasks than R (please refer to Section 3.2).
Finally, in the stability-correlated space I, where a slight change would bring about tremendous forgetting, we pay more attention to stability by putting more strict constraints on it. In the plasticity-correlated space R which will have less impact on old knowledge, we stress plasticity and allow the model to be updated in a looser way here. Finally, with
SD, the performance of several state-of-the-art gradient pro-jection methods is improved by a large margin. Below, we summarize our contributions: (1) We generalize recent gradient projection methods
[18,23,31,38] into a unified paradigm, under which we give a new viewpoint about their stability-plasticity dilemma. (2) We propose a novel Space Decoupling (SD) al-gorithm to split the whole feature space into stability-correlated space and plasticity-correlated space. By putting distinguishing constraints on these subspaces, our method achieves a better balance between stability and plasticity. (3) We apply SD to various gradient projection baselines and show our approach is model-agnostic and effective.
Extensive experiments on benchmark datasets demonstrate state-of-the-art performance achieved by our approach. 2.