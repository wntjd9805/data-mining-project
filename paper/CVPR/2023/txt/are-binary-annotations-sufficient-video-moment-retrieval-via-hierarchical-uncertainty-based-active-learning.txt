Abstract
Recent research on video moment retrieval has mostly focused on enhancing the performance of accuracy, effi-ciency, and robustness, all of which largely rely on the abun-dance of high-quality annotations. While the precise frame-level annotations are time-consuming and cost-expensive, few attentions have been paid to the labeling process. In this work, we explore a new interactive manner to stimu-late the process of human-in-the-loop annotation in video moment retrieval task. The key challenge is to select “am-biguous” frames and videos for binary annotations to fa-cilitate the network training. To be specific, we propose a new hierarchical uncertainty-based modeling that explicitly considers modeling the uncertainty of each frame within the entire video sequence corresponding to the query descrip-tion, and selecting the frame with the highest uncertainty.
Only selected frame will be annotated by the human ex-perts, which can largely reduce the workload. After ob-taining a small number of labels provided by the expert, we show that it is sufficient to learn a competitive video mo-ment retrieval model in such a harsh environment. More-over, we treat the uncertainty score of frames in a video as a whole, and estimate the difficulty of each video, which can further relieve the burden of video selection. In gen-eral, our active learning strategy for video moment retrieval works not only at the frame level but also at the sequence level. Experiments on two public datasets validate the ef-fectiveness of our proposed method. Our code is released at https://github.com/renjie-liang/HUAL. (a) (b)
Figure 1. We propose a new interactive method named HUAL which only requires binary annotations to reduce the annotation cost. (a) In each round, user (student) selects a frame with the largest uncertainty, and the expert (teacher) returns the binary la-bel of this frame as feedback. (b) With more labels provided, the
VMR model is retrained and the whole process can be treated in a human-in-the-loop manner. 1.

Introduction
Video Moment Retrieval (VMR) aims to localize the temporal region of an untrimmed video corresponding to query description, which is a fundamental task in the video understanding area, and can benefit a lot of downstream
*Corresponding author. tasks, such as video question answering [19, 40, 53], dense video captioning [4, 6], video relation detection [14, 34], video dialog [29], etc. Recent methods on VMR mainly focus on modeling the cross-modal context in temporal, and have achieved significant performance gains in public datasets, which heavily rely on the well-annotated datasets, such as Charades [9], ActivityNet [35], etc.
However, the precise labels on VMR datasets are time-consuming and cost-expensive, and relying on the well-annotated dataset will restrict the generalization ability of the current models. Hence, we revisit the process of anno-tation and propose a new active learning-based method to relieve the heavy burden of annotations in the VMR task.
We have two assumptions: 1) not each frame should be considered equally, as the frame with the higher uncertainty is more valuable than the rest; and 2) not each video can be treated as a hard sample, annotating complex video and query pairs first benefits more than annotating simple ones.
The whole process of our active learning-based method is shown in Figure 1.
In each round, for each video in the training set, the user (student) first selects one frame with the highest uncertainty regarding the consistency of the video and query in this video, then the expert (teacher) re-turns the label of this frame (positive or negative). After that, the user takes the label of a single frame as supervi-sion and trains the model, and the uncertainty score of each video will be updated. In the next round, the user can se-lect another frame with the highest uncertainty. The whole process can be described as: “A student asks a hard ques-tion first, then the teacher returns the answer as feedback, the student then digests what have learned, and the process can be repeated.” Besides, the amount of videos that need to be annotated can be further compressed. The uncertainty of each video can also be utilized in the sequence level. A certain percentage of videos with high uncertainty can be treated as hard samples with more benefits when annotat-ing.
Hence, the whole process of interactive annotating can be treated as Human-in-the-Loop. The key techniques rely on the computation of uncertainty and the selection of the video frame or the whole video sequence as hard samples.
To be specific, we consider the uncertainty in two aspects: the classification confidence of each frame, and the distance of the current frame from the known labels (e.g., the start and end boundaries of each video are negative samples, and the annotated frames in previous rounds). For the classi-fication confidence of each frame, we choose a weakly-supervised VMR model (such as CPL [50]) to obtain the ini-tial classification result and confidence score of each frame, and the frame with alow confidence score can be treated with high uncertainty. By adding the distance score and confidence score together, the frame with the highest uncer-tainty is selected. We then utilize a fully-supervised VMR model (such as SeqPAN [45]) to train with the labels pro-vided by the expert in each round. Besides considering the frame-level uncertainty, we also seek the reduction of an-notated videos at the sequence level via accumulating the frame-level uncertainty. Hence, the annotation cost can be further reduced with a minor performance drop.
Our main contributions are summarized as follows:
• We propose a new interactive framework named
HUAL to reduce the annotation cost, which only re-quires binary annotations. To verify the feasibility, we stimulate the process of annotation in the video mo-ment retrieval task, which is model-agnostic and can be treated in a Human-in-the-Loop manner.
• Specifically, we consider the hierarchical design, which is frame-level and sequence-level uncertainty estimation to select hard samples and fully take ad-vantages of limited binary annotations by the expert.
This annotation method can greatly reduce the anno-tation cost while achieving comparable performance compared with the fully supervised setting.
• Extensive experimental results on two public datasets indicate that binary annotations are sufficient for video moment retrieval. The proposed method can achieve competitive performance with much fewer annota-tions, which show the effectiveness of our proposed methods. 2.