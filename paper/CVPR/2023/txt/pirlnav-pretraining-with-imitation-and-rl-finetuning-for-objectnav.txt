Abstract
We study ObjectGoal Navigation – where a virtual robot situated in a new environment is asked to navigate to an ob-ject. Prior work [1] has shown that imitation learning (IL) using behavior cloning (BC) on a dataset of human demon-strations achieves promising results. However, this has lim-itations – 1) BC policies generalize poorly to new states, since the training mimics actions not their consequences, and 2) collecting demonstrations is expensive. On the other hand, reinforcement learning (RL) is trivially scalable, but requires careful reward engineering to achieve desirable behavior. We present PIRLNav, a two-stage learning scheme for BC pretraining on human demonstrations followed by
RL-finetuning. This leads to a policy that achieves a suc-cess rate of 65.0% on OBJECTNAV (+5.0% absolute over previous state-of-the-art).
Using this BC→RL training recipe, we present a rigorous empirical analysis of design choices. First, we investigate whether human demonstrations can be replaced with ‘free’ (automatically generated) sources of demonstrations, e.g. shortest paths (SP) or task-agnostic frontier exploration (FE) trajectories. We find that BC→RL on human demonstrations outperforms BC→RL on SP and FE trajectories, even when controlled for the same BC-pretraining success on TRAIN, and even on a subset of VAL episodes where BC-pretraining success favors the SP or FE policies. Next, we study how
RL-finetuning performance scales with the size of the BC pretraining dataset. We find that as we increase the size of the BC-pretraining dataset and get to high BC accuracies, the improvements from RL-finetuning are smaller, and that 90% of the performance of our best BC→RL policy can be achieved with less than half the number of BC demonstra-tions. Finally, we analyze failure modes of our OBJECTNAV policies, and present guidelines for further improving them.
Project page: ram81.github.io/projects/pirlnav. 1.

Introduction
Since the seminal work of Winograd [2], designing embod-ied agents that have a rich understanding of the environment
Figure 1. OBJECTNAV success rates of agents trained using be-havior cloning (BC) vs. BC-pretraining followed by reinforcement learning (RL) (in blue). RL from scratch (i.e. BC=0) fails to get off-the-ground. With more BC demonstrations, BC success increases, and it transfers to even higher RL-finetuning success. But the differ-ence between RL-finetuning vs. BC-pretraining success (in orange) plateaus and starts to decrease beyond a certain point, indicating diminishing returns with each additional BC demonstration. they are situated in, can interact with humans (and other agents) via language, and the environment via actions has been a long-term goal in AI [3–12]. We focus on Object-Goal Navigation [13, 14], wherein an agent situated in a new environment is asked to navigate to any instance of an object category (‘find a plant’, ‘find a bed’, etc.); see Fig. 2.
OBJECTNAV is simple to explain but difficult for today’s techniques to accomplish. First, the agent needs to be able to ground the tokens in the language instruction to physical objects in the environment (e.g. what does a ‘plant’ look like?). Second, the agent needs to have rich semantic priors to guide its navigation to avoid wasteful exploration (e.g. the microwave is likely to be found in the kitchen, not the washroom). Finally, it has to keep track of where it has been in its internal memory to avoid redundant search.
Humans are adept at OBJECTNAV. Prior work [1] collected a large-scale dataset of 80k human demonstrations for OB-JECTNAV, where human subjects on Mechanical Turk tele-operated virtual robots and searched for objects in novel houses. This first provided a human baseline on OBJECT-NAV of 88.9% success rate on the Matterport3D (MP3D)
Figure 2. OBJECTNAV trajectories for policies trained with BC→RL on 1) Human Demonstrations, 2) Shortest Paths, and 3) Frontier
Exploration Demonstrations. dataset [15]1 compared to 35.4% success rate of the best performing method [1]. This dataset was then used to train agents via imitation learning (specifically, behavior cloning).
While this approach achieved state-of-art results (35.4% suc-cess rate on MP3D VAL dataset), it has two clear limitations.
First, behavior cloning (BC) is known to suffer from poor generalization to out-of-distribution states not seen during training, since the training emphasizes imitating actions not accomplishing their goals. Second and more importantly, it is expensive and thus not scalable. Specifically, Ram-rakhya et al. [1] collected 80k demonstrations on 56 scenes in Matterport3D Dataset, which took ∼2894 hours of hu-man teleoperation and $50k dollars. A few months after [1] was released, a new higher-quality dataset called HM3D-Semantics v0.1 [16] became available with 120 annotated 3D scenes, and a few months after that HM3D-Semantics v0.2 added 96 additional scenes. Scaling Ramrakhya et al.’s approach to continuously incorporate new scenes involves replicating that entire effort again and again.
On the other hand, training with reinforcement learning (RL) is trivially scalable once annotated 3D scans are available.
However, as demonstrated in Maksymets et al. [17], RL requires careful reward engineering, the reward function typ-ically used for OBJECTNAV actually penalizes exploration (even though the task requires it), and the existing RL poli-cies overfit to the small number of available environments.
Our primary technical contribution is PIRLNav, an approach for pretraining with BC and finetuning with RL for OBJECT-NAV. BC pretrained policies provide a reasonable starting point for ‘bootstrapping’ RL and make the optimization eas-ier than learning from scratch. In fact, we show that BC pretraining even unlocks RL with sparse rewards. Sparse rewards are simple (do not involve any reward engineer-ing) and do not suffer from the unintended consequences 1On VAL split, for 21 object categories, and a maximum of 500 steps. described above. However, learning from scratch with sparse rewards is typically out of reach since most random action trajectories result in no positive rewards.
While combining IL and RL has been studied in prior work [18–22], the main technical challenge in the context of modern neural networks is that imitation pretraining re-sults in weights for the policy (or actor), but not a value function (or critic). Thus, naively initializing a new RL pol-icy with these BC-pretrained policy weights often leads to catastrophic failures due to destructive policy updates early on during RL training, especially for actor-critic RL meth-ods [23]. To overcome this challenge, we present a two-stage learning scheme involving a critic-only learning phase first that gradually transitions over to training both the actor and critic. We also identify a set of practical recommendations for this recipe to be applied to OBJECTNAV. This leads to a
PIRLNav policy that advances the state-the-art on OBJECT-NAV from 60.0% success rate (in [24]) to 65.0% (+5.0%, 8.3% relative improvement).
Next, using this BC→RL training recipe, we conduct an empirical analysis of design choices. Specifically, an ingre-dient we investigate is whether human demonstrations can be replaced with ‘free’ (automatically generated) sources of demonstrations for OBJECTNAV, e.g. (1) shortest paths (SP) between the agent’s start location and the closest object instance, or (2) task-agnostic frontier exploration [25] (FE) of the environment followed by shortest path to goal-object upon observing it. We ask and answer the following: 1. ‘Do human demonstrations capture any unique
OBJECTNAV-specific behaviors that shortest paths and frontier exploration trajectories do not?’ Yes. We find that BC / BC→RL on human demonstrations outperforms
BC / BC→RL on shortest paths and frontier exploration trajectories respectively. When we control the number of demonstrations from each source such that BC success on
TRAIN is the same, RL-finetuning when initialized from
BC on human demonstrations still outperforms the other two. 2. ‘How does performance after RL scale with BC dataset size?’ We observe diminishing returns from RL-finetuning as we scale BC dataset size. This suggests, by effectively leveraging the trade-off curve between size of pretraining dataset size vs. performance after RL-Finetuning, we can achieve closer to state-of-the-art results without investing into a large dataset of BC demonstrations. 3. ‘Does BC on frontier exploration demonstrations present similar scaling behavior as BC on human demonstrations?’
No. We find that as we scale frontier exploration demon-strations past 70k trajectories, the performance plateaus.
Finally, we present an analysis of the failure modes of our
OBJECTNAV policies and present a set of guidelines for fur-ther improving them. Our policy’s primary failure modes are: a) Dataset issues: comprising of missing goal annota-tions, and navigation meshes blocking the path, b) Naviga-tion errors: primarily failure to navigate between floors, c)
Recognition failures: where the agent does not identify the goal object during an episode, or confuses the specified goal with a semantically-similar object. 2.