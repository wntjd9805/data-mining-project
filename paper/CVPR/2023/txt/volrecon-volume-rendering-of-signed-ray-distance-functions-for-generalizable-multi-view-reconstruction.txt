Abstract
The success of the Neural Radiance Fields (NeRF) in novel view synthesis has inspired researchers to propose neural implicit scene reconstruction. However, most exist-ing neural implicit reconstruction methods optimize per-scene parameters and therefore lack generalizability to new scenes. We introduce VolRecon, a novel generalizable implicit reconstruction method with Signed Ray Distance
Function (SRDF). To reconstruct the scene with fine de-tails and little noise, VolRecon combines projection features aggregated from multi-view features, and volume features interpolated from a coarse global feature volume. Using a ray transformer, we compute SRDF values of sampled points on a ray and then render color and depth. On DTU dataset, VolRecon outperforms SparseNeuS by about 30% in sparse view reconstruction and achieves comparable ac-curacy as MVSNet in full view reconstruction. Furthermore, our approach exhibits good generalization performance on the large-scale ETH3D benchmark. Code is available at https://github.com/IVRL/VolRecon/. 1.

Introduction
The ability to reconstruct 3D geometries from images or videos is crucial in various applications in robotics [16, 43, 52] and augmented/virtual reality [29, 35]. Multi-view stereo (MVS) [13, 15, 39, 47, 54, 55] is a commonly used technique for this task. A typical MVS pipeline involves multiple steps, i.e., multi-view depth estimation, filtering, and fusion [5, 13].
Recently, there has been a growing interest in neural im-plicit representations for various 3D tasks, such as shape modeling [28, 36], surface reconstruction [49, 56], and novel view synthesis [30]. NeRF [30], a seminal work in this area, employs Multi-Layer Perceptrons (MLP) to model a radiance field, producing volume density and radiance estimates for a given position and viewing direction. While NeRF’s scene representation and volume rendering approach has proven effective for tasks such as novel view synthesis, it cannot
*Equal contribution
Figure 1. Generalizable implicit reconstructions from three views (top). The state-of-the-art method SparseNeuS [26] produces over-smoothed surfaces (left), while our (VolRecon) reconstructs finer details (right). Best viewed on a screen when zoomed in. generate accurate surface reconstruction due to difficulties in finding a universal density threshold for surface extrac-tion [56]. To address this, researchers have proposed neural implicit reconstruction using the Signed Distance Function (SDF) for geometry representation and modeling the vol-ume density function [49, 56]. However, utilizing SDF with only color supervision leads to unsatisfactory reconstruction quality compared to MVS methods [15, 54] due to a lack of geometry supervision and potential radiance-geometry ambi-guities [51,62]. As a result, subsequent works have sought to improve reconstruction quality by incorporating additional priors, such as sparse Struction-from-Motion (SfM) point clouds [11], dense MVS point clouds [60], normals [48, 59], and depth maps [59].
Many neural implicit reconstruction methods are re-stricted to optimizing one model for a particular scene and cannot be applied to new, unseen scenes, i.e., across-scene
generalization. However, the ability to generalize learned priors to new scenes is valuable in challenging scenarios such as reconstruction with sparse views [4, 50, 58]. In or-der to achieve across-scene generalization in neural implicit reconstruction, it is insufficient to simply input the spatial coordinate of a point as NeRF. Instead, we need to incorpo-rate information about the scene, such as the points’ pro-jection features on the corresponding images [4, 50, 58].
SparseNeuS [26] recently achieved across-scene general-ization in implicit reconstruction with global feature vol-umes [4]. Despite achieving promising results, SparseNeuS is limited by the resolution of the feature volume due to the memory constraints [22, 32], leading to over-smoothing surfaces even with a higher resolution feature volume, Fig. 1.
In this paper, we propose VolRecon, a novel framework for generalizable neural implicit reconstruction using the
Signed Ray Distance Function (SRDF). Unlike SDF, which defines the distance to the nearest surface along any direc-tions, SRDF [63] defines the distance to the nearest surface along a given ray. We utilize a projection-based approach to gather local information about surface location. We first project each point on the ray into the feature map of each source view to interpolate multi-view features. Then, we ag-gregate the multi-view features to projection features using a view transformer. However, when faced with challenging situations such as occlusions and textureless surfaces, de-termining the surface location along the ray with only local information is difficult. To address this, we construct a coarse global feature volume that encodes global shape priors like
SparseNeuS [26, 32]. We use the interpolated features from the global feature volume, i.e., volume features, and pro-jection features of all the sampled points along the ray to compute their SRDF values, with a ray transformer. Similar to NeuS [49], we model the density function with SRDF and then estimate the image and depth map with volume rendering.
Extensive experiments on DTU [1] and ETH3D [40] verify the effectiveness and generalization ability of our method. On DTU, our method outperforms the state-of-the-art method SparseNeuS [26] by 30% in sparse view reconstruction and 22% in full view reconstruction. Further-more, our method performs better than the MVS baseline
COLMAP [39]. Compared with MVSNet [54], a seminal learning-based MVS method, our method performs better in the depth evaluation and has comparable accuracy in full-view reconstruction. On the ETH3D benchmark [40], we show that our method has a good generalization ability to large-scale scenes.
In summary, our contributions are as follows:
• We propose VolRecon, a new pipeline for generalizable implicit reconstruction that produce detailed surfaces.
• Our novel framework comprises a view transformer to aggregate multi-view features and a ray transformer to compute SRDF values of all the points along a ray.
• We introduce a combination of local projection features and global volume features, which enables the recon-struction of surfaces with fine details and high quality. 2.