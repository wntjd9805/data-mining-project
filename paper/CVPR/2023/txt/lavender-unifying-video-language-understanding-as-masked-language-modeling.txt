Abstract 1.

Introduction
Unified vision-language frameworks have greatly ad-vanced in recent years, most of which adopt an encoder-decoder architecture to unify image-text tasks as sequence-to-sequence generation. However, existing video-language (VidL) models still require task-specific designs in model architecture and training objectives for each task. In this work, we explore a unified VidL framework LAVENDER, where Masked Language Modeling [13] (MLM) is used as the common interface for all pre-training and downstream tasks. Such unification leads to a simplified model archi-tecture, where only a lightweight MLM head, instead of a decoder with much more parameters, is needed on top of the multimodal encoder. Surprisingly, experimental re-sults show that this unified framework achieves competi-tive performance on 14 VidL benchmarks, covering video question answering, text-to-video retrieval and video cap-tioning. Extensive analyses further demonstrate LAVEN-DER can (i) seamlessly support all downstream tasks with just a single set of parameter values when multi-task fine-tuned; (ii) generalize to various downstream tasks with lim-ited training samples; and (iii) enable zero-shot evaluation on video question answering tasks. Code is available at https://github.com/microsoft/LAVENDER.
Large-scale transformer-based pre-training is now the de facto practice for NLP and vision-language research [13, 25, 37, 49, 50]. Together with the great success of image-text pre-training [10, 35, 40, 59], video-language (VidL) pre-training [29, 33, 58, 80, 83] has also received an increasing amount of attention. By pre-training an end-to-end multi-modal transformer on a large number of video-text pairs, state-of-the-art performance has been achieved across a wide range of VidL tasks, including video question answering (QA) [24, 68], text-to-video retrieval [22, 51], and video captioning [64, 71]. These advances are encouraging; how-ever, on the other hand, all existing VidL works require designing task-specific heads on top of the transformer en-coder for each pre-training or downstream task. For example, during pre-training, separate Masked Language Modeling
[13] (MLM) and Video Text Matching (VTM) heads are used, while a new, separately parameterized head needs to be added for each downstream adaptation. Furthermore, due to the particular nature of different tasks, they are typically modeled using different training objectives. For example, multiple-choice video QA is formulated as a classification problem, while video captioning is inherently a generation task. A natural but challenging question arises: can we have a unified architecture that supports all the popular VidL tasks simultaneously without introducing task-specific heads?
To answer this, we present LAVENDER, a unified VidL
MSRVTT
LSMDC
Published
SOTA
LAVENDER
∆
QA 43.1
[80] 45.0 1.9↑
Table 1. New state-of-the-art performance with LAVENDER (14M+16M pre-train, single-task finetune) across 10 VidL tasks. Accuracy, average(R1, R5, R10) and CIDEr scores are reported for video QA, retrieval and captioning.
Action 94.0
[80] 94.8 0.8↑
Frame 69.5
[80] 73.5 4.0↑
MC 90.9
[80] 97.2 6.3↑
MC 81.7
[80] 85.9 4.2↑
FiB 52.9
[80] 57.1 4.2↑
MSVD
QA 46.3
[73] 55.6 9.3↑
Captioning
MSVD 120.6
[36] 150.3 29.7↑
TGIF
Transition 96.2
[80] 98.7 2.5↑
Retrieval
DiDeMo 65.1
[5] 72.4 7.3↑ framework where all pre-training and downstream tasks are formulated as simple MLM. As shown in Figure 1, we use two pre-training tasks: MLM and VTM. However, for VTM, instead of adding a head on top of the output of the com-monly used [CLS] token, as used in all existing works, we propose to append the same [MASK] token that is used for
MLM at the end of the video-text input, and use the same
MLM head to predict whether the video-text pair matches or not. Note that VTM is typically formulated as a binary classification problem; here, we simply treat the output of true or false from VTM as natural language tokens di-rectly predicted from the whole vocabulary, so that the same set of parameters can be used for both MLM and VTM.
During downstream adaptation, instead of following stan-dard practice in VidL literature to replace the MLM head in pre-training with new heads, we use the same MLM head used in pre-training for all downstream tasks. Specifically,
• For text-to-video retrieval, we train the model in the same way as in the VTM pre-training task. During inference, for each text query, we concatenate it with each candidate video, and calculate the corresponding probability of the
[MASK] token being predicted as true, and then rank all candidate videos based on that score.
• For multiple-choice video QA, we concatenate the ques-tion and each answer candidate sequentially, and add a
[MASK] token at the end of the sequence, and use the same MLM head to predict the answer as “n” (assuming the ground-truth choice is the n-th answer).
• For open-ended video QA, since most of the ground-truth answers in our tested datasets only contain one word, we simply append a [MASK] token to the end of the video-question input, and let the model predict the answer from the whole vocabulary.
• For video captioning, during training, we mask a certain percentage of the tokens, and then predict the masked tokens using a seq2seq attention mask [35, 81]. During inference, the full caption is auto-regressively predicted, by inserting [MASK] tokens one at a time.
LAVENDER is inspired by VL-T5 [12], UniTAB [76] and
OFA [63] that aim to provide a unified pre-training frame-work for image-text tasks. However, LAVENDER adopt an encoder-only model and an additional lightweight MLM head on top of it, while a heavy transformer decoder is needed in [12, 63, 76]. By unifying all VidL tasks as MLM,
LAVENDER can seamlessly adapt to different VidL tasks, meanwhile (i) support different VidL tasks with a single set of parameter values when multi-task finetuned; (ii) general-ize to test data under few-shot finetuning; and (iii) enable zero-shot inference on video question answering. Surpris-ingly, by using this simple generative approach, we outper-form previously published state-of-the-arts on 10 out of 14 downstream tasks (Table 1), even when pre-trained with much fewer data (Section 4.5). 2.