Abstract
Contrastive language-image pre-training (CLIP) serves as a de-facto standard to align images and texts. Nonethe-less, the loose correlation between images and texts of web-crawled data renders the contrastive objective data ineffi-In this cient and craving for a large training batch size. work, we explore the validity of non-contrastive language-image pre-training (nCLIP), and study whether nice proper-ties exhibited in visual self-supervised models can emerge.
We empirically observe that the non-contrastive objective benefits representation learning while sufficiently underper-forming under zero-shot recognition. Based on the above study, we further introduce xCLIP, a multi-tasking frame-work combining CLIP and nCLIP, and show that nCLIP aids CLIP in enhancing feature semantics. The synergy between two objectives lets xCLIP enjoy the best of both worlds: superior performance in both zero-shot transfer and representation learning. Systematic evaluation is con-ducted spanning a wide variety of downstream tasks in-cluding zero-shot classification, out-of-domain classifica-tion, retrieval, visual representation learning, and textual representation learning, showcasing a consistent perfor-mance gain and validating the effectiveness of xCLIP. The code and pre-trained models will be publicly available at https://github.com/shallowtoil/xclip. 1.

Introduction
Language-image pre-training which simultaneously learns textual and visual representation from large-scale image-text pairs has revolutionized the field of representa-tion learning [25, 59], vision-language understanding [22], and text-to-image generation [61]. Compared to traditional visual models, language-instilled ones intrinsically inherit the capability of zero-shot or few-shot learning prominently demonstrated by large language models such as GPT-3 [12].
The precursor system, Contrastive Language-Image Pre-Training [59] (CLIP) that explicitly aligns the projected features of two modalities, has demonstrated surprising ca-pabilities of zero-shot, representation learning, and robust-ness, being applied to a wide range of fields [32, 52, 60, 70].
Figure 1. Architecture comparison between CLIP and nCLIP.
We take base-size encoders for an instance. CLIP discriminates instances within a batch using 512-dim projected embeddings. nCLIP projects each modality into a 32768-dim probability dis-tribution as the pseudo-label to supervise the prediction from the other modality. Darker blocks of nCLIP depict a higher response for cluster distribution, signifying the clusters to which text or im-age instances may belong. xCLIP is a multi-tasking framework with both CLIP and nCLIP.
To learn from noisy web-crawled image-text pairs, CLIP adopts a formulation of the contrastive objective, where the image and text within a pair are considered as unique in-stances and are encouraged to be discriminated from all the other negative instances. However, web-crawled image-text pairs [65, 69] are usually loosely correlated in the sense that one caption (image) can match reasonably with multiple im-ages (captions) besides the ground-truth one, as statistically shown in [74]. Hence, it is inaccurate and data-inefficient for representation learning to neglect other sensible matches and overlook the semantics hidden inside the textual de-scription. This is also solidified by the undesirable dis-criminative ability or transferring performance of the visual encoder pre-trained under the contrastive objective, as sug-gested in [78]. Mining semantics from plentiful concepts appearing in captions, therefore, solicits further exploration beyond the vanilla contrastive objective.
Previously, some works resort to mining nearest neigh-bor positive samples via intra-modality similarity [50, 74] but require extra storage for auxiliary modules, i.e., the teacher network [74] or the memory bank [50]. Other works conduct multi-tasking with image-label supervision [57, 78, 80], transforming the captions into tag format or the im-age tag into the captioning format for unified contrastive
learning. Notwithstanding, such practice is still unable to account for the loose assignment within the image-text cap-tioning pairs.
Intuitively, a caption for an image usually identifies existing objects within the image, which can be captured by a probabilistic distribution estimating how the text is assigned to one or multiple object clusters [13, 14].
Such estimation depicts the semantic meanings for its con-tained visual contents, and thus can be leveraged as a pseudo-label to guide the learning of the visual encoder.
Inspired by the superiority of visual self-supervised learning (SSL) models pre-trained with the non-contrastive objective [14, 15], we explore whether the non-contrastive objective can be used across modalities for pre-training language-image models, and whether nice properties dis-played on visual SSL models can be inherited. To this end, we follow the same setup as CLIP except for the objective, and study non-Contrastive Language-Image Pre-Training (nCLIP). For an image-text pair, we use the estimation of the textual (visual) distribution as the target to supervise the visual (textual) distribution, measured by cross-entropy.
Additional regularizers are applied to avoid trivial solutions.
Schematic comparison between CLIP and nCLIP is shown in Fig. 1. Theoretically, such formulation takes one modal-ity and the cluster centroid that the other modality belongs to as the positive samples to contrast [14], as opposed to direct features of two modalities as in contrastive learning, such that a single image is tolerated to be aligned with mul-tiple captions and concepts. Based on the above formula-tion, we conduct a systematic study in terms of zero-shot transfer and representation learning. We empirically ob-serve that, while nCLIP demonstrates desirable represen-tation learning capability for both modalities, it underper-forms prominently in zero-shot classification and retrieval tasks where models pre-trained with negative samples natu-rally prevail.
Seeking for unique strengths of both worlds, we further perform multi-tasking of CLIP and nCLIP, short as xCLIP, and seek synergy between two distinct pre-training objec-tives. With the bearable add-on of computational resources (e.g., ∼ 27% memory-wise and ∼ 30% time-wise), xCLIP achieves consistent performance gain compared to CLIP on a wide range of tasks spanning from zero-shot classification, retrieval, linear probing, and fine-tuning, etc. An extensive study with different pre-training datasets, evaluation met-rics, and optimization configurations is conducted to val-idate xCLIP’s effectiveness in mining semantics and im-proving data efficiency. Particularly, the base-size model pre-trained using xCLIP under 35-million publicly avail-able image-text pairs achieves a performance gain of 3.3% and 1.5% on an average of 27 classification tasks [59], in terms of zero-shot classification and linear probing accu-racy, respectively. Performance gain is also valid in terms of zero-shot retrieval, semi-supervised learning, and fine-tuning, with 3.7 points of R@1 on Flickr30K [58], 1.7% accuracy on 1% subset of ImageNet [24], and 0.3% accu-racy on ImageNet, respectively. 2.