Abstract
By supervising camera rays between a scene and multi-view image planes, NeRF reconstructs a neural scene rep-resentation for the task of novel view synthesis. On the other hand, shadow rays between the light source and the scene have yet to be considered. Therefore, we propose a novel shadow ray supervision scheme that optimizes both the samples along the ray and the ray location. By su-pervising shadow rays, we successfully reconstruct a neu-ral SDF of the scene from single-view images under mul-tiple lighting conditions. Given single-view binary shad-ows, we train a neural network to reconstruct a complete scene not limited by the camera’s line of sight. By further modeling the correlation between the image colors and the shadow rays, our technique can also be effectively extended to RGB inputs. We compare our method with previous works on challenging tasks of shape reconstruction from single-view binary shadow or RGB images and observe signif-icant improvements. The code and data are available at https://github.com/gerwang/ShadowNeuS. 1.

Introduction
Neural ﬁeld [43] has been used for 3D scene representa-tion in recent years. It achieves remarkable quality because of the ability to continuously parameterize a scene with a compact neural network. The neural network nature makes it amenable to various optimization tasks in 3D vision, in-cluding long-standing problems like image-based [28, 51] and point cloud-based [26, 31] 3D reconstruction. So more and more works are using neural ﬁelds as the 3D scene rep-resentation for various related tasks.
Among these works, NeRF [27] is a representative method that incorporates a part of physically based light transport [38] into the neural ﬁeld. The light transport de-scribes light travels from the light source to the scene and then from the scene to the camera. NeRF considers the latter part to model the interaction between the scene and the cam-eras along the camera rays (rays from the camera through
*Corresponding author
Single-view inputs
Reconstruction viewed at novel views
Figure 1. Our method can reconstruct neural scenes from single-view images captured under multiple lightings by effectively lever-aging a novel shadow ray supervision scheme. the scene). By supervising these camera rays of different viewpoints with the corresponding recorded images, NeRF optimizes a neural ﬁeld to represent the scene. Then NeRF casts camera rays from novel viewpoints through the opti-mized neural ﬁeld to generate novel-view images.
However, NeRF does not model the rays from the scene to the light source, which motivates us to consider: can we optimize a neural ﬁeld by supervising these rays? These rays are often called shadow rays as the light emitted from the light source can be absorbed by scene particles along the rays, resulting in varying light visibility (a.k.a. shadows) at the scene surface. By recording the incoming radiance at the surface, we should be able to supervise the shadow rays to infer the scene geometry.
Given this observation, we derive a novel problem of su-pervising the shadow rays to optimize a neural ﬁeld rep-resenting the scene, analogizing to NeRF that models the camera rays. Like multiple viewpoints in NeRF, we illumi-nate the scene multiple times using different light directions to obtain sufﬁcient observations. For each illumination, we use a ﬁxed camera to record the light visibility at the scene
surface as supervision labels for the shadow rays. As rays connecting the scene and the light source march through the 3D space, we can reconstruct a complete 3D shape not con-strained by the camera’s line of sight.
We solve several challenges when supervising the shadow rays using camera inputs.
In NeRF, each ray’s position can be uniquely determined by the known cam-era center, but shadow rays need to be determined by the scene surface, which is not given and has yet to be recon-structed. We solve this using an iterative updating strategy, where we sample shadow rays starting at the current sur-face estimation. More importantly, we make the sampled locations differentiable to the geometry representation, thus can optimize the starting positions of shadow rays. How-ever, this technique is insufﬁcient to derive correct gradi-ents at surface boundaries with abrupt depth changes, which coincides with recent ﬁndings in differentiable rendering
[2, 20, 23, 40, 54]. Thus, we compute surface boundaries by aggregating shadow rays starting at multiple depth can-didates.
It remains efﬁcient as boundaries only occupy a small amount of surface, while it signiﬁcantly improves
In addition, RGB val-the surface reconstruction quality. ues recorded by the camera encode the outgoing radiance at the surface instead of the incoming radiance. The outgoing radiance is a coupling effect of light, material, and surface orientation. We propose to model the material and surface orientation to decompose the incoming radiance from RGB inputs to achieve reconstruction without needing shadow segmentation (Row 1 and 2 in Fig. 1). As material modeling is optional, our framework can also take binary shadow im-ages [18] to achieve shape reconstruction (Row 3 in Fig. 1).
We compare our method with previous single-view re-construction methods (including shadow-only and RGB-based) and observe signiﬁcant improvements in shape re-construction. Theoretically, our method handles a dual problem of NeRF. So, comparing the corresponding parts of the two techniques can inspire readers to get a deeper un-derstanding of the essence of neural scene representation to a certain extent, as well as the relationship between them.
Our contributions are:
• A framework that exploits light visibility to reconstruct neural SDF from shadow or RGB images under multi-ple light conditions.
• A shadow ray supervision scheme that embraces dif-ferentiable light visibility by simulating physical inter-actions along shadow rays, with efﬁcient handling of surface boundaries.
• Comparisons with previous works on either RGB or binary shadow inputs to verify the accuracy and com-pleteness of the reconstructed scene representation. 2.