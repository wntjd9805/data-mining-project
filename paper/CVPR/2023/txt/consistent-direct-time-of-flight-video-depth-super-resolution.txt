Abstract
Direct time-of-flight (dToF) sensors are promising for next-generation on-device 3D sensing. However, limited by manufacturing capabilities in a compact module, the dToF data has a low spatial resolution (e.g. ∼ 20 × 30 for iPhone dToF), and it requires a super-resolution step before being passed to downstream tasks.
In this paper, we solve this super-resolution problem by fusing the low-resolution dToF data with the corresponding high-resolution RGB guidance.
Unlike the conventional RGB-guided depth enhancement approaches, which perform the fusion in a per-frame man-ner, we propose the first multi-frame fusion scheme to miti-gate the spatial ambiguity resulting from the low-resolution
In addition, dToF sensors provide unique dToF imaging. depth histogram information for each local patch, and we incorporate this dToF-specific feature in our network design to further alleviate spatial ambiguity. To evaluate our mod-els on complex dynamic indoor environments and to pro-vide a large-scale dToF sensor dataset, we introduce Dy-DToF, the first synthetic RGB-dToF video dataset that fea-tures dynamic objects and a realistic dToF simulator fol-lowing the physical imaging process. We believe the meth-ods and dataset are beneficial to a broad community as dToF depth sensing is becoming mainstream on mobile de-vices. Our code and data are publicly available. https:
//github.com/facebookresearch/DVSR/ 1.

Introduction
On-device depth estimation is critical in navigation [41], gaming [6], and augmented/virtual reality [3,8]. Previously, various solutions based on stereo/structured-light sensors and indirect time-of-flight sensors (iToF) [4,35,44,57] have been proposed. Recently, direct time-of-flight (dToF) sen-sor brought more interest in both academia [31, 36] and industry [5], due to its high accuracy, compact form fac-tor, and low power consumption [13, 37]. However, lim-ited by the manufacturing capability, current dToF sensors have very low spatial resolutions [13, 40]. Each dToF pixel captures and pre-processes depth information from a local patch in the scene (Sec. 3), leading to high spatial ambiguity when estimating the high-resolution depth maps for down-stream tasks [8]. Previous RGB-guided depth completion and super-resolution algorithms either assume high resolu-tion spatial information (e.g. high resolution sampling po-sitions) [34, 55] or simplified image formation models (e.g. bilinear downsampling) [16, 33]. Simple network tweaking and retraining is insufficient in handling the more ill-posed dToF depth super-resolution task. As shown in Fig. 1, 2nd column, the predictions suffer from geometric distortions and flying pixels. Another fundamental limitation of these previous approaches is they focus on single-frame process-ing, while in real-world applications, the depth estimation is expected in video (data-stream) format with certain tempo-ral consistency. Processing an RGB-depth video frame-by-frame ignores temporal correlations and leads to significant temporal jittering in the depth estimations [32, 43, 58].
In this paper, we propose to tackle the spatial ambiguity in low-resolution dToF data from two aspects: with infor-mation aggregation between multiple frames in an RGB-dToF video and with dToF histogram information. We first design a deep-learning-based RGB-guided dToF video super-resolution (DVSR) framework (Sec. 4.1) that con-sumes a sequence of high-resolution RGB images and low-resolution dToF depth maps, and predicts a sequence of high-resolution depth maps. Inspired by the recent advances in RGB video processing [11,30], we loosen the multi-view stereo constraints and utilize flexible, false-tolerant inter-frame alignments to make DVSR agnostic to static or dy-namic environments. Compared to per-frame processing baselines, DVSR significantly improves both prediction ac-curacy and the temporal coherence, as shown in Fig. 1, 3rd column. Please refer to the supplementary video for tempo-ral visualizations.
Moreover, dToF sensors provide histogram information
Instead due to their unique image formation model [13]. of a single depth value from other types of 3D sensors, the histogram contains a distribution of depth values within each low-resolution pixel. From this observation, we fur-ther propose a histogram processing pipeline based on the physical image formation model and integrate it into the DVSR framework to form a histogram video super-resolution (HVSR) network (Sec. 4.2).
In this way, the spatial ambiguity in the depth estimation process is further lifted. As shown in Fig. 1 4th column, compared to DVSR, the HVSR estimation quality is further improved, especially for fine structures such as the compartments of the cabinet, and it eliminates the flying pixels near edges.
Another important aspect for deep-learning-based depth estimation models is the training and evaluation datasets.
Previously, both real-world captured and high quality syn-thetic dataset have been widely used [21, 38, 46, 51]. How-ever, none of them contain RGB-D video sequences with significant amount of dynamic objects. To this end, we in-troduce DyDToF, a synthetic dataset with diverse indoor scenes and animations of dynamic animals (e.g., cats and dogs) (Sec. 6). We synthesize sequences of RGB images, depth maps, surface normal maps, material albedos, and camera poses. To the best of our knowledge, this is the first dataset that provides dynamic indoor RGB-Depth video.
We integrate physics-based dToF sensor simulations in the
DyDToF dataset and analyze (1) how the proposed video processing framework generalizes to dynamic scenes and (2) how the low-level data modalities facilitate network training and evaluation.
In summary, our contributions are in three folds:
• We introduce RGB-guided dToF video depth super-resolution to resolve inherent spatial ambiguity in such mobile 3D sensor.
• We propose neural network based RGB-dToF video super-resolution algorithms to efficiently employ the rich information contained in multi-frame videos and the unique dToF histograms.
• We introduce the first synthetic dataset with physics-based dToF sensor simulations and diverse dynamic objects. We conduct systematic evaluations on the pro-posed algorithm and dataset to verify the significant improvements on accuracy and temporal coherence. 2.