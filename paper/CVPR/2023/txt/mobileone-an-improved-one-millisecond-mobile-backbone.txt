Abstract
Efficient neural network backbones for mobile devices are often optimized for metrics such as FLOPs or param-eter count. However, these metrics may not correlate well with latency of the network when deployed on a mobile de-vice. Therefore, we perform extensive analysis of different metrics by deploying several mobile-friendly networks on a mobile device. We identify and analyze architectural and optimization bottlenecks in recent efficient neural networks and provide ways to mitigate these bottlenecks. To this end, we design an efficient backbone MobileOne, with variants achieving an inference time under 1 ms on an iPhone12 with 75.9% top-1 accuracy on ImageNet. We show that Mo-bileOne achieves state-of-the-art performance within the ef-ficient architectures while being many times faster on mo-bile. Our best model obtains similar performance on Ima-geNet as MobileFormer while being 38× faster. Our model obtains 2.3% better top-1 accuracy on ImageNet than Ef-ficientNet at similar latency. Furthermore, we show that our model generalizes to multiple tasks – image classifi-cation, object detection, and semantic segmentation with significant improvements in latency and accuracy as com-pared to existing efficient architectures when deployed on a mobile device. Code and models are available at https:
//github.com/apple/ml-mobileone 1.

Introduction
Design and deployment of efficient deep learning archi-tectures for mobile devices has seen a lot of progress [5, 30,31,42,44,46] with consistently decreasing floating-point operations (FLOPs) and parameter count while improving accuracy. However, these metrics may not correlate well with the efficiency [9] of the models in terms of latency. Ef-ficiency metric like FLOPs do not account for memory ac-cess cost and degree of parallelism, which can have a non-trivial effect on latency during inference [42]. Parameter count is also not well correlated with latency. For exam-ple, sharing parameters leads to higher FLOPS but smaller model size. Furthermore, parameter-less operations like skip-connections [24] or branching [33,49] can incur signif-icant memory access costs. This disconnect can get exacer-bated when custom accelerators are available in the regime of efficient architectures.
Our goal is to improve the latency cost of efficient archi-tectures while improving their accuracy by identifying key architectural and optimization bottlenecks that affect on-device latency. To identify architectural bottlenecks, we de-ploy neural networks on an iPhone12 by using CoreML [56] and benchmark their latency costs. To alleviate optimiza-tion bottlenecks, we decouple train-time and inference-time architectures, i.e. using a linearly over-parameterized model at train-time and re-parameterizing the linear struc-tures at inference [11–13]. We further alleviate optimization bottleneck by dynamically relaxing regularization through-out training to prevent the already small models from being over-regularized.
Based on our findings on the key bottlenecks, we de-sign a novel architecture MobileOne, variants of which run under 1 ms on an iPhone12 achieving state-of-the-art accu-racy within efficient architecture family while being signif-icantly faster on the device. Like prior works on structural re-parameterization [11–13], MobileOne introduces linear branches at train-time which get re-parameterized at infer-ence. However, a key difference between our model and prior structural re-parameterization works is the introduc-tion of trivial over-parameterization branches, which pro-vides further improvements in low parameter regime and model scaling strategy. At inference, our model has sim-ple feed-forward structure without any branches or skip-connections. Since this structure incurs lower memory access cost, we can incorporate wider layers in our net-work which boosts representation capacity as demonstrated empirically in Table 9. For example, MobileOne-S1 has 4.8M parameters and incurs a latency of 0.89ms, while
MobileNet-V2 [46] has 3.4M (29.2% less than MobileOne-S1) parameters and incurs a latency of 0.98ms. At this oper-ating point, MobileOne attains 3.9% better top-1 accuracy than MobileNet-V2.
MobileOne achieves significant improvements in latency compared to efficient models in literature while maintain-(b) Zoomed out (a) (a) Top 1 accuracy vs Latency on iPhone 12. (c) Top-1 accuracy vs mAP.
Figure 1. We show comparisons of Top-1 accuracy on image classification vs latency on an iPhone 12 (a), and zoomed out area (b) to include recent transformer architectures. We show mAP on object detection vs Top-1 accuracy on image classification in (c) with size of the marker indicating latency of the backbone on iPhone 12. Our models have significantly smaller latency compared to related works.
Please refer to supp. mat. for higher resolution figures. ing the accuracy on several tasks – image classification, ob-ject detection, and semantic segmentation. As shown in
Figure 1b, MobileOne performs better than MobileViT-S [44] while being 5 × faster on image classification. As compared to EfficientNet-B0 [53], we achieve 2.3% bet-ter top-1 accuracy on ImageNet [10] with similar latency costs (see Figure 1a). Furthermore, as seen in Figure 1c,
MobileOne models not only perform well on ImageNet, they also generalize to other tasks like object detection.
Models like MobileNetV3-L [30] and MixNet-S [54] im-prove over MobileNetV2 on ImageNet, but those improve-ments do not translate to object detection task. As shown in Figure 1c, MobileOne shows better generalization across tasks. For object detection on MS-COCO [37], best vari-ant of MobileOne outperforms best variant MobileViT by 6.1% and MNASNet by 27.8%. For semantic segmen-tation, on PascalVOC [16] dataset, best variant of Mo-bileOne outperforms best variant MobileViT by 1.3% and on ADE20K [64] dataset, best variant of MobileOne out-performs MobileNetV2 by 12.0%. In summary, our contri-butions are as follows: of-the-art accuracy on image classification within ef-ficient model architectures. The performance of our model also generalizes to a desktop CPU and GPU.
• We analyze performance bottlenecks in activations and branching that incur high latency costs on mobile in recent efficient networks.
• We analyze the effects of train-time re-parameterizable branches and dynamic relaxation of regularization in training.
In combination, they help alleviating opti-mization bottlenecks encountered when training small models.
• We show that our model generalizes well to other tasks – object detection and semantic segmentation while outperforming recent state-of-the-art efficient models.
We will release our trained networks and code for research purposes. We will also release the code for iOS application to enable benchmarking of networks on iPhone. 2.