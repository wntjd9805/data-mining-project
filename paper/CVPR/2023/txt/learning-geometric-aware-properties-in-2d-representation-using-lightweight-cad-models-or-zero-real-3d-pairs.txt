Abstract
Cross-modal training using 2D-3D paired datasets, such as those containing multi-view images and 3D scene scans, presents an effective way to enhance 2D scene under-standing by introducing geometric and view-invariance pri-ors into 2D features. However, the need for large-scale scene datasets can impede scalability and further improve-ments. This paper explores an alternative learning method by leveraging a lightweight and publicly available type of 3D data in the form of CAD models. We construct a 3D space with geometric-aware alignment where the similarity in this space reflects the geometric similarity of CAD mod-els based on the Chamfer distance. The acquired geometric-aware properties are then induced into 2D features, which boost performance on downstream tasks more effectively than existing RGB-CAD approaches. Our technique is not limited to paired RGB-CAD datasets. By training exclu-sively on pseudo pairs generated from CAD-based recon-struction methods, we enhance the performance of SOTA 2D pre-trained models that use ResNet-50 or ViT-B back-bones on various 2D understanding tasks. We also achieve comparable results to SOTA methods trained on scene scans on four tasks in NYUv2, SUNRGB-D, indoor ADE20k, and indoor/outdoor COCO, despite using lightweight CAD models or pseudo data. Please visit our page: https:
//GeoAware2dRepUsingCAD.github.io/ 1.

Introduction
Recent 2D visual representation learning approaches, such as contrastive learning [3, 6, 10, 17, 28] or masked au-toencoder [20], are widely used to tackle various problems in computer vision due to their ability to encode rich vi-sual features. While these methods have shown exceptional results on 2D image classification, they still have shortcom-ings in other 2D understanding tasks that involve instance-level reasoning. Prior research [25] also shows that models pre-trained using image augmentations [8,21] or supervised
Figure 1. Overview concept of our solution. We leverage CAD models to train a joint 2D-3D space such that images of objects with similar shapes, based on the Chamfer distance, are attracted to each other, while images with different shapes are separated.
This results in a continuous geometric-aware space where the dis-tance between two points reflects their geometric similarity, which could be utilized for downstream 2D object understanding tasks. labels [13] could not deliver satisfactory results when ap-plied to downstream tasks such as semantic segmentation, instance segmentation, and object detection [12, 42].
To alleviate this, Hou et al. [25] proposed to learn 3D geometric priors, such as view-invariance, from 3D data and transfer the learned priors to 2D representations.
In particular, their model is first pre-trained on ScanNet [12], a database of multi-view RGB-D scans, using contrastive learning and later used as initialization for fine-tuning net-works on downstream tasks. Chen et al. [5] further extend this work by utilizing additional priors through learning to group nearby points that refer to the same object part from 3D scenes.
The key concept of their introduced new paradigm is to share useful 3D priors from 3D data with 2D representa-tions. However, previous studies have investigated mostly 3D priors related to viewpoint invariance from 3D scene datasets, which are often limited in size and scene varia-tions due to the laborious data collecting and labeling pro-cess [24]. This scarcity of large-scale 3D data also limits the number of available 3D priors for learning the invari-ance, hence hindering further performance gains. This pa-per questions whether there are other effective 3D priors for 2D downstream tasks and whether they can be learned from other forms of 3D data that are lighter and easier to obtain.
We begin our exploration by considering how to learn an embedding space that maps together images with the same or similar geometries. One solution is to learn from images that share the same 3D model, which inspires our interest in CAD models. Unlike 3D scenes, CAD mod-els are lightweight, publicly available, and can be easily aligned with RGB images via web scraping or human an-notation [2]. There exist studies that jointly learn 2D and 3D CAD representation for other CAD-related tasks, e.g., 3D classification [1, 27]. However, their 2D features, which are derived from augmentation-based CAD features, are in-sufficient to be directly applied to 2D object understanding tasks—they can group images with similar geometries but struggle to learn the distinctions between object categories.
We argue that useful geometric-aware representations should account for both similarities and differences in ob-ject geometry. Our key idea is to acquire such features by imitating the Chamfer distance between 3D objects in our embedding space and inducing this derived geometric awareness in our learned 2D representation. In particular, we learn our space by attracting the encoded features of ge-ometrically similar CAD models in the mini-batch based on the Chamfer distance and repelling those with lower simi-larities. In contrast to other methods trained on supervised discrete signals like object labels or through 3D augmenta-tions, our method produces a continuous 3D space that bet-ter captures the similarity and difference in geometry (see
Section 5.1). In addition, we employ augmentation-based contrastive learning [6] to learn other useful visual feature properties, such as translation and color invariances. This results in a 2D representation in a 2D-3D space that contains rich visual information and strong geometric-aware proper-ties, as shown in Fig. 1, which can be leveraged to improve 2D object understanding tasks.
To match our geometric-aware CAD features with cor-responding 2D features, a paired RGB-CAD dataset, such as Pix3D [45], is required. However, by leveraging recent techniques [19, 31] that can reconstruct a CAD model from an input image, it is possible to generate pseudo CAD mod-els for any images and use them to learn our method with-out a paired dataset. Adapting this pseudo-pair generation to other techniques that rely on scene scans is significantly harder, as synthesizing full 3D scenes with reasonable detail remains harder than reconstructing individual objects.
We demonstrate the effectiveness of our geometric-aware 2D representation in Fig. 2. Our features can group and differentiate objects based on their categories or subcat-egories, leading to improved performance on multiple 2D object understanding tasks using both ResNet-50 [23] and
ViT-B [29] backbones. Our method trained on a pseudo-pair dataset also yields superior results over DINO [3] and
MAE [20]. Remarkably, we also surpass a state-of-the-art method, Pri3D [25] without using any 3D scene scans in the following tasks: (i) semantic segmentation using
NYUv2 [42] and indoor ADE20k [54]; (ii) object detection and instance segmentation using NYUv2 and in/outdoor
COCO [35]; (iii) object retrieval using Pix3D [45].
To summarize, our contributions are as follows.
• We present a simple yet effective approach to inducing geometric-aware properties in 2D representation using lightweight CAD models. These can be either ground truth from RGB-CAD datasets or generated pseudo
CAD pairs based on 2D-only data.
• We propose training objectives to learn a 2D-3D em-bedding space where feature similarity reflects geo-metric similarity based on the Chamfer distance.
• We enhance the performance of SOTA 2D representa-tion learning techniques on four 2D object understand-ing tasks and achieve competitive results to SOTA that require 3D scene scans across five datasets, in both set-tings that use real or pseudo-RGB-CAD datasets. 2.