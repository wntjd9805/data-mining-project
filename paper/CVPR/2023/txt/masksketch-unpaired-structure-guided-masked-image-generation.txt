Abstract
Recent conditional image generation methods produce images of remarkable diversity, ﬁdelity and realism. How-ever, the majority of these methods allow conditioning only on labels or text prompts, which limits their level of con-In this paper, we intro-trol over the generation result. duce MaskSketch, an image generation method that allows spatial conditioning of the generation result using a guid-ing sketch as an extra conditioning signal during sampling.
MaskSketch utilizes a pre-trained masked generative trans-former, requiring no model training or paired supervision, and works with input sketches of different levels of abstrac-tion. We show that intermediate self-attention maps of a masked generative transformer encode important structural information of the input image, such as scene layout and object shape, and we propose a novel sampling method based on this observation to enable structure-guided gen-eration. Our results show that MaskSketch achieves high image realism and ﬁdelity to the guiding structure. Evalu-ated on standard benchmark datasets, MaskSketch outper-forms state-of-the-art methods for sketch-to-image transla-tion, as well as unpaired image-to-image translation ap-proaches. The code can be found on our project website: https://masksketch.github.io/ 1.

Introduction
Recent Image generation methods achieved remarkable success, allowing diverse and photorealistic image synthe-sis [4,11,44,46]. The majority of state-of-the-art generative models allow conditioning with class labels [2, 4, 11, 13] or text prompts [40, 41, 44, 46]. However, some applications require a more ﬁne-grained control over the spatial compo-sition of the generation result. While methods conditioned with segmentation maps [14] or strokes [34] achieve some spatial control over the generated image, sketching allows a more ﬁne-grained speciﬁcation of the target spatial layout, which makes it desirable for many creative applications.
In this paper, we propose MaskSketch, a method for con-ditional image synthesis that uses sketch guidance to de-⇤Work done during an internship at Google.
Figure 1. Given an input sketch and class label, MaskSketch sam-ples realistic images that follow the given structure. MaskSketch works on sketches of various degree of abstraction by leveraging a pre-trained masked image generator [4], while not requiring model
ﬁnetuning or pairwise supervision.
ﬁne the desired structure, and a pre-trained state-of-the-art masked generative transformer, MaskGIT [4], to leverage a strong generative prior. We demonstrate the capability of
MaskSketch to generate realistic images of a given struc-ture for sketch-to-photo image translation. Sketch-to-photo
[5, 20, 32] is one of the most challenging applications of structure-conditional generation due to the large domain gap between sketches and natural images. MaskSketch achieves a balance between realism and structure ﬁdelity.
Our experiments show that MaskSketch outperforms state-of-the-art sketch-to-photo [20] and unpaired image transla-tion methods [6, 25, 37], according to standard metrics for image generation [23] and user preference studies.
In MaskSketch, we formulate a structure similarity con-straint based on the observation that the intermediate self-attention maps of a masked generative transformer [4] en-code rich structural information (see Fig. 2). We use this
2.