Abstract
Because of their high temporal resolution, increased re-silience to motion blur, and very sparse output, event cam-eras have been shown to be ideal for low-latency and low-bandwidth feature tracking, even in challenging scenarios.
Existing feature tracking methods for event cameras are either handcrafted or derived from first principles but re-quire extensive parameter tuning, are sensitive to noise, and do not generalize to different scenarios due to unmod-eled effects. To tackle these deficiencies, we introduce the first data-driven feature tracker for event cameras, which leverages low-latency events to track features detected in a grayscale frame. We achieve robust performance via a novel frame attention module, which shares information across feature tracks. By directly transferring zero-shot from synthetic to real data, our data-driven tracker outper-forms existing approaches in relative feature age by up to 120 % while also achieving the lowest latency. This perfor-mance gap is further increased to 130 % by adapting our tracker to real data with a novel self-supervision strategy.
Multimedia Material A video is available at https:
//youtu.be/dtkXvNXcWRY and code at https:// github.com/uzh-rpg/deep_ev_tracker 1.

Introduction
Despite many successful implementations in the real world, existing feature trackers are still primarily con-strained by the hardware performance of standard cameras.
To begin with, standard cameras suffer from a bandwidth-latency trade-off, which noticeably limits their performance under rapid movements: at low frame rates, they have minimal bandwidth but at the expense of an increased la-tency; furthermore, low frame rates lead to large appearance changes between consecutive frames, significantly increas-ing the difficulty of tracking features. At high frame rates, the latency is reduced at the expense of an increased band-width overhead and power consumption for downstream
*equal contribution.
Figure 1. Our method leverages the high-temporal resolution of events to provide stable feature tracks in high-speed motion in which standard frames suffer from motion blur. To achieve this, we propose a novel frame attention module that combines the in-formation across feature tracks. systems. Another problem with standard cameras is mo-tion blur, which is prominent in high-speed low-lit scenar-ios, see Fig. 1. These issues are becoming more prominent with the current commodification of AR/VR devices.
Event cameras have been shown to be an ideal comple-ment to standard cameras to address the bandwidth-latency trade-off [16, 17]. Event cameras are bio-inspired vision sensors that asynchronously trigger information whenever the brightness change at an individual pixel exceeds a pre-defined threshold. Due to this unique working principle, event cameras output sparse event streams with a temporal resolution in the order of microseconds and feature a high-dynamic range and low power consumption. Since events are primarily triggered in correspondence of edges, event cameras present minimal bandwidth. This makes them ideal for overcoming the shortcomings of standard cameras.
Existing feature trackers for event cameras have shown
unprecedented results with respect to latency and tracking robustness in high-speed and high-dynamic range scenar-ios [4, 17]. Nonetheless, until now, event-based trackers have been developed based on classical model assumptions, which typically result in poor tracking performance in the presence of noise. They either rely on iterative optimiza-tion of motion parameters [17, 26, 48] or employ a simple classification for possible translations of a feature [4], thus, do not generalize to different scenarios due to unmodeled effects. Moreover, they usually feature complex model pa-rameters, requiring extensive manual hand-tuning to adapt to different event cameras and new scenes.
To tackle these deficiencies, we propose the first data-driven feature tracker for event cameras, which leverages the high-temporal resolution of event cameras in combi-nation with standard frames to maximize tracking perfor-mance. Using a neural network, our method tracks features by localizing a template patch from a grayscale image in subsequent event patches. The network architecture fea-tures a correlation volume for the assignment and employs recurrent layers for long-term consistency. To increase the tracking performance, we introduce a novel frame attention module, which shares information across feature tracks in one image. We first train on a synthetic optical flow dataset and then finetune it with our novel self-supervision scheme based on 3D point triangulation using camera poses.
Our tracker outperforms state-of-the-art baselines by up to 5.5% and 130.2% on the Event Camera Dataset bench-mark [32] and the recently published EDS dataset [22], re-spectively. This performance is achieved without requir-ing extensive manual hand-tuning of parameters. Moreover, without optimizing the code for deployment, our method achieves faster inference than existing methods. Finally, we show how the combination of our method with the well-established frame-based tracker KLT [30] leverages the best of both worlds for high-speed scenarios. This combination of standard and event cameras paves the path for the concept of sparingly triggering frames based on the tracking quality, which is a critical tool for future applications where runtime and power consumption are essential. 2.