Abstract
Expert demonstrations are a rich source of supervision for training visual robotic manipulation policies, but imita-tion learning methods often require either a large number of demonstrations or expensive online expert supervision to learn reactive closed-loop behaviors. In this work, we in-troduce SPARTN (Synthetic Perturbations for Augmenting
Robot Trajectories via NeRF): a fully-offline data augmen-tation scheme for improving robot policies that use eye-in-hand cameras. Our approach leverages neural radiance fields (NeRFs) to synthetically inject corrective noise into visual demonstrations, using NeRFs to generate perturbed viewpoints while simultaneously calculating the corrective actions. This requires no additional expert supervision or environment interaction, and distills the geometric informa-tion in NeRFs into a real-time reactive RGB-only policy.
In a simulated 6-DoF visual grasping benchmark, SPARTN improves success rates by 2.8× over imitation learning without the corrective augmentations and even outperforms some methods that use online supervision. It additionally closes the gap between RGB-only and RGB-D success rates, eliminating the previous need for depth sensors.
In real-world 6-DoF robotic grasping experiments from limited hu-man demonstrations, our method improves absolute success rates by 22.5% on average, including objects that are tra-ditionally challenging for depth-based methods. See video results at https://bland.website/spartn. 1.

Introduction
Object grasping is a central problem in vision-based con-trol and is fundamental to many robotic manipulation prob-lems. While there has been significant progress in top-down bin picking settings [21, 34], 6-DoF grasping of ar-bitrary objects amidst clutter remains an open problem, and is especially challenging for shiny or reflective objects that are not visible to depth cameras. For example, the task of grasping a wine glass from the stem shown in Figure 1 re-*Equal contribution. Correspondence to ayz@cs.stanford.edu.
Figure 1. SPARTN is an offline data augmentation method for be-havior cloning eye-in-hand visual policies. It simulates recovery in a demonstration by using NeRFs to render high-fidelity obser-vations (right) from noisy states, then generates corrective action labels. quires precise 6-DoF control (using full 3D translation and 3D rotation of the gripper) and closed-loop perception of a transparent object. Traditional 6-DoF grasping pipelines
[8, 57] synthesize only one grasp pose and use a motion planner to generate a collision-free trajectory to reach the grasp [38, 40, 54, 60]. However, the use of open-loop trajec-tory execution prevents the system from using perceptual feedback for reactive, precise grasping behavior. In this pa-per, we study how to learn closed-loop policies for 6-DoF object grasping from RGB images, which can be trained with imitation or reinforcement learning methods [58].
Imitation learning from expert demonstrations is a sim-ple and promising approach to this problem, but is known to suffer from compounding errors [46]. As a result, com-plex vision-based tasks can require online expert supervi-sion [19, 46] or environment interaction [13, 44], both of which are expensive and time-consuming to collect. On the other hand, offline “feedback augmentation” methods [14, 22] can be effective at combating compounding errors, but are severely limited in scope and thus far have not been ap-plied to visual observations. Other recent works have found that using eye-in-hand cameras mounted on a robot’s wrist can significantly improve the performance of visuomotor policies trained with imitation learning [17, 20, 35], but still do not address the underlying issue of compounding errors.
We develop an approach that helps address compounding errors to improve vision-based policies, while building on the success of eye-in-hand cameras.
To improve imitation learning for quasi-static tasks like grasping, we propose a simple yet effective offline data aug-mentation technique. For an eye-in-hand camera, the im-ages in each demonstration trajectory form a collection of views of the demonstration scene, which we use to train neural radiance fields (NeRFs) [37] of each scene. Then, we can augment the demonstration data with corrective feed-back by injecting noise into the camera poses along the demonstration and using the demonstration’s NeRF to ren-der observations from the new camera pose. Because the camera to end-effector transform is known, we can compute corrective action labels for the newly rendered observations by considering the action that would return the gripper to the expert trajectory. The augmented data can be combined with the original demonstrations to train a reactive, real-time policy. Since the NeRFs are trained on the original demonstrations, this method effectively “distills” the 3D in-formation from each NeRF into the policy.
The main contribution of this work is a NeRF-based data augmentation technique, called SPARTN (Synthetic Pertur-bations for Augmenting Robot Trajectories via NeRF), that improves behavior cloning for eye-in-hand visual grasping policies. By leveraging view-synthesis methods like NeRF,
SPARTN extends the idea of corrective feedback augmen-tation to the visual domain. The resulting approach can pro-duce (i) reactive, (ii) real-time, and (iii) RGB-only policies for 6-DoF grasping. The data augmentation is fully offline and does not require additional effort from expert demon-strators nor online environment interactions. We evaluate
SPARTN on 6-DoF robotic grasping tasks both in simula-tion and in the real world. On a previously-proposed sim-ulated 6-DoF grasping benchmark [58], the augmentation from SPARTN improves grasp success rates by 2.8× com-pared to training without SPARTN, and even outperforms some methods that use expensive online supervision. On eight challenging real-world grasping tasks with a Franka
Emika Panda robot, SPARTN improves the absolute aver-age success rate by 22.5%. 2.