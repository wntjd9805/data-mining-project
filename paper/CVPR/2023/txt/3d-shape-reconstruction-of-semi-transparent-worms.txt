Abstract 3D shape reconstruction typically requires identifying object features or textures in multiple images of a sub-ject. This approach is not viable when the subject is semi-transparent and moving in and out of focus. Here we over-come these challenges by rendering a candidate shape with adaptive blurring and transparency for comparison with the images. We use the microscopic nematode Caenorhab-ditis elegans as a case study as it freely explores a 3D complex fluid with constantly changing optical properties.
We model the slender worm as a 3D curve using an in-trinsic parametrisation that naturally admits biologically-informed constraints and regularisation. To account for the changing optics we develop a novel differentiable ren-derer to construct images from 2D projections and compare
*{T.Ilett, O.Yuval, T.Ranner, N.Cohen,
D.C.Hogg}@leeds.ac.uk
Funding This work was supported by University of Leeds and EPSRC.
Author contributions Conceptualisation, Methodology, Formal analysis,
Investigation, Software, Visualisation: TPI. Data curation, Validation:
TPI, OY. Writing: TPI (original), all (review and editing). Funding acquisition, Supervision: NC, DCH, TR. † Equal contribution.
Acknowledgements Additional thanks to Matan Braunstein (for help with
Fig. 1), Robert I. Holbrook (data), Felix Salfelder (discussions and data),
Lukas Deutz (discussions) and Jen Kruger (proof reading).
Data available https://doi.org/10.6084/m9.figshare.22310650.
Supplementary movies availability here: are against raw images to generate a pixel-wise error to jointly update the curve, camera and renderer parameters using gradient descent. The method is robust to interference such as bubbles and dirt trapped in the fluid, stays consistent through complex sequences of postures, recovers reliable estimates from blurry images and provides a significant im-provement on previous attempts to track C. elegans in 3D.
Our results demonstrate the potential of direct approaches to shape estimation in complex physical environments in the absence of ground-truth data. 1.

Introduction
Many creatures such as fish, birds and insects move in all directions to search and navigate volumetric environments.
Acquiring 3D data of their motion has informed models of locomotion, behaviour and neural and mechanical control
[3,22]. While technological advances have made the collec-tion of large quantities of multi-viewpoint visual data more attainable, methods for extracting and modelling 3D in-formation remain largely domain-dependant as few species share common geometric models or exist within the same spatial and temporal scales [4, 11, 14, 26, 37, 41, 50, 54, 65].
Furthermore, while humans and some domesticated ani-mals [30,60] may act naturally while wearing special mark-ers, marker-less observations of many species makes fea-ture extraction more challenging and means pose estimation generally lacks ground-truth data [48].
As a case study in marker-less 3D shape reconstruction, we consider C. elegans, a hair-thick, ∼1 mm long animal with a simple tapered cylinder shape, which can be con-structed from a midline “skeleton”. In the wild, C. elegans can be found in a wide range of complex 3D environments, e.g. decomposing organic matter, with continually changing physical properties [15, 17, 46]. However, to date, experi-ments have focused nearly exclusively on locomotion on a plane, limiting insight to the constrained, planar behaviours.
We obtained a large dataset (4 hours 53 minutes ≃ 440,000 frames at 25Hz) of experimental recordings of in-dividual worms moving freely inside a glass cube filled with a gelatin solution. The cube is positioned between three nearly-orthogonal static cameras fitted with telecentric lenses.
Initial pinhole camera model parameter estimates are provided [45] but are imprecise and require continuous adjustment across the course of a recording to account for small vibrations and optical changes to the gel. We aim to simultaneously reconstruct a 3D shape and find corrected camera parameters to match these recordings in a process akin to bundle adjustment [56]. 3D reconstruction typically involves the identification and triangulation of common features from multiple view-points or the synthesis of full images including texture and shading information to match given scenes [16, 21, 47, 66].
Imaging animals with length ∼1 mm requires sufficient magnification, but simultaneously capturing long-term tra-jectories up to 25 minutes requires a large volume of view (10-20 worm lengths per axis). As the worm explores the cube it frequently appears out of focus in one or more of the cameras. Air bubbles and dirt trapped in the gel along with old tracks are difficult to differentiate from the trans-parent worm, particularly at the tapered ends. Self occlu-sion invariably appears in a least one view, where hidden parts darken the foreground while the ordering of fore/back-parts is not discernible. As the semi-transparent and self-occluding subject moves in the volume, photometric infor-mation in one view bears little relevance to the appearance in the others making feature identification and photometric matching particularly challenging. We found that standard approaches may suffice for limited sub-clips, but lose parts of the object or fail catastrophically for much of the data and the solution requires a degree of adaptation.
We present an integrated “project-render-score” algo-rithm to obtain a midline curve for each image-triplet (Fig. 1). Discrete curve vertices are projected through a triplet of pinhole camera models, rendered to produce an image-triplet for direct comparison against the recorded im-ages and scored according to their intersection with worm-like pixels in all three views. The differentiable renderer stacks 2D super-Gaussian blobs at the projected locations of each vertex to approximate the transparency along the worm, accounting for the variable focus and providing soft edges that direct the geometric model towards the midline.
The scoring allows the detection of incongruities and keeps the curve aligned to the worm in all views. Regularisation terms ensure smoothness along the body and in time. Curve, camera and rendering parameters are jointly optimised us-ing gradient descent to convergence. Once the worm shape has been resolved, it is generally only lost during image degradation or significant self-occlusions that make the pos-ture unresolvable by eye.
In summary, our main contributions are:
• A robust pipeline for 3D posture reconstruction of a freely deforming semi-transparent object from noisy images.
• A novel viewpoint renderer to capture optical distor-tions and transparency.
• A feature-free bundle adjustment algorithm using di-rect image comparison and gradient descent. 2.