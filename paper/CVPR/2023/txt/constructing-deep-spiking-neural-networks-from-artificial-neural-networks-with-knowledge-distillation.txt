Abstract
Spiking neural networks (SNNs) are well-known as brain-inspired models with high computing efficiency, due to a key component that they utilize spikes as information units, close to the biological neural systems. Although spik-ing based models are energy efficient by taking advantage of discrete spike signals, their performance is limited by cur-rent network structures and their training methods. As dis-crete signals, typical SNNs cannot apply the gradient de-scent rules directly into parameter adjustment as artificial neural networks (ANNs). Aiming at this limitation, here we propose a novel method of constructing deep SNN mod-els with knowledge distillation (KD) that uses ANN as the teacher model and SNN as the student model. Through the
ANN-SNN joint training algorithm, the student SNN model can learn rich feature information from the teacher ANN model through the KD method, yet it avoids training SNN from scratch when communicating with non-differentiable spikes. Our method can not only build a more efficient deep spiking structure feasibly and reasonably but use few time steps to train the whole model compared to direct train-ing or ANN to SNN methods. More importantly, it has a superb ability of noise immunity for various types of ar-tificial noises and natural signals. The proposed novel method provides efficient ways to improve the performance of SNN through constructing deeper structures in a high-throughput fashion, with potential usage for light and effi-cient brain-inspired computing of practical scenarios. 1.

Introduction
By referring to the information processing mechanism and structural characteristics of the biological nervous system, spiking neural networks (SNNs) are remarkably
* Corresponding authors: jrshen@zju.edu.cn and gpan@zju.edu.cn. good at computational intelligence tasks [26] and suitable for processing unstructured information, with stronger au-tonomous learning capabilities and ultra-low power con-sumption [2, 7, 23, 37].
Although various engineering effort has been made in this area, such type of biological information processing system still underperforms artificial systems (artificial neu-ral networks, ANNs) in some common computer tasks, such as image classification. One possible reason for this is that typical SNNs lack deep hierarchical network struc-tures compared to those from ANNs. Due to the non-differentiable spikes, typical SNNs are restricted to global training rules which lead to various of current SNNs being just shallow fully-connected layer based [28, 34]. Limited by training rules and structures, although SNNs can signifi-cantly handle spatial-temporal data efficiently, it is difficult to train a deep SNN directly as using backpropagation (BP) in ANNs do [22].
By drawing on some key tricks from ANNs, some stud-ies want to improve image classification accuracy which is led by SNNs by combing structure and learning rules those has been proven to be effective in improving model performance in ANNs. [3, 6] proposed methods to con-vert ANNs to SNNs by controlling the output and network structure of ANN and SNN to be as consistent as possi-ble. Through this way, although they can build effective deep SNNs, these conversion methods suffer long training time and lack some intermediate information during ANN training period. [12, 19] tried to adopt the threshold of spik-ing neurons to make them suitable for using gradient sur-rogate method, these models adopted too complex neuron models to get good performance and take up large computa-tional memory and cost. [8] did interesting work on directly converting an adjusted ANN to an SNN using the theoret-ical equivalence between activation and firing rate, which achieves superior performance. [29] constructed ANN-SNN hybrid models to improve the feature extraction, but these
hybrid models suffered a difficult training process.
Aiming at constructing efficient SNNs, this paper pro-posed a brand-new method using knowledge distillation (KD) to let student models (SNNs) absorb rich informa-tion from teacher models (ANNs). KD [4] can transfer the knowledge of one network to another network, two net-works can be homogeneous or heterogeneous. This is done by training a teacher network and then using the output of the teacher network and the real tag of the data to train the student network. KD can be used to transform a network from a large network to a small network, retaining perfor-mance close to that of a large network, or to transfer knowl-edge learned from multiple networks into a single network, making the performance of a single network close to the results of Ensemble.
Under the guidance of teacher models, the wanted SNNs model can be trained in a layer-wise manner [20]. Unlike traditional ANN-SNN conversion requires the same model structure of two models, the proposed KD conversion can make a heterogeneous network structure of them, for ex-ample, if the teacher ANN is larger and deeper, the student
SNN can be smaller and shallower. This kind of KD conver-sion provides sufficient flexibility to construct any arbitrary
SNN.
In this paper, we propose a novel KD based training method to construct deep SNNs which avoids restricting corresponding network structures between ANN and SNN during the training period. Through a unified ANN-SNN loss function, we can construct the SNN model from well-trained ANN, accelerate the training time and save mem-ory usage. We adopt supervised gradient surrogate method as basic student SNN training rules. We evaluated the proposed method on several image classification datasets (MNIST, CIFAR10, and CIFAR100) and their noisy vari-ations. Experimental results showed that the proposed method can get pretty good image classification perfor-mance with a light SNN model. The main contributions are as follows:
• This paper proposed a KD based conversion method to construct deep SNNs from ANNs, which only takes less training latency and allows the structure of the
SNN and ANN to be heterogeneous.
• Through the proposed ANN-SNN joint training method, the student SNN model can absorb more in-formation from ANN during training method, com-pared to offline ANN-SNN conversion, the proposed method significantly helped to improve the perfor-mance of the student SNN model.
• We demonstrate the efficiency and effectiveness of the proposed distilling SNN method through evaluations of several datasets and noisy ones. Experimental re-sults show that we can construct a more reasonable
SNN which can achieve state-of-the-art performance on experimental datasets with less training latency and show better anti-noise ability. 2.