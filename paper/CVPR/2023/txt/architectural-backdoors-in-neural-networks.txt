Abstract
Machine learning is vulnerable to adversarial manipula-tion. Previous literature demonstrated that at the training stage attackers can manipulate data [14] and data sampling procedures [29] to control model behaviour. A common at-tack goal is to plant backdoors i.e. force the victim model to learn to recognise a trigger known only by the adversary.
In this paper, we introduce a new class of backdoor attacks that hide inside model architectures i.e. in the inductive bias of the functions used to train. These backdoors are simple to implement, for instance by publishing open-source code for a backdoored model architecture that others will reuse unknowingly. We demonstrate that model architectural back-doors represent a real threat and, unlike other approaches, can survive a complete re-training from scratch. We for-malise the main construction principles behind architectural backdoors, such as a connection between the input and the output, and describe some possible protections against them.
We evaluate our attacks on computer vision benchmarks of different scales and demonstrate the underlying vulnerability is pervasive in a variety of common training settings. 1.

Introduction
The Machine Learning (ML) community now faces a threat posed by backdoored neural networks; models which are intentionally modified by an attacker in the supply chain to insert hidden behaviour [3, 14]. A backdoor causes a network’s behaviour to change arbitrarily when a specific se-cret ‘trigger’ is present in the model’s input, while behaving as the defender intended when the trigger is absent (retain-ing a high evaluation performance). The vast majority of current backdoor attacks in the literature work by changing the trained weights of models [14, 15] – here the backdoor
*University of Cambridge, UK.
†Vector Institute, CA.
‡University of Oxford, UK.
§Imperial College London, UK.
¶University of Toronto, CA. is planted into the parameters during training of the neural network. This can be done directly (i.e. modify the values of the weights directly [12, 15]), or indirectly by sampling adversarially [29] and modifying training data [14]. This means that when the weights are later modified by another party (e.g. through fine-tuning), the backdoor could feasibly be removed or weakened [34]. When the weights provided by an attacker are discarded entirely (e.g. through re-training from scratch on a new dataset), any embedded backdoor would of course naturally be discarded.
However, the performance of a neural network depends not only on its weights but also its architecture (the composi-tion and connections between layers in the model). Research showed that, when given sufficient flexibility, the neural net-work architectures themselves can be pre-disposed to certain outcomes [11, 38]. The network architectures can be seen as an inductive bias of the ML model. This raises a new question: Can the network architectures themselves be modified to hide backdoors?
In this paper we investigate if an adversary can use neural network architectures to perform backdoor attacks, forcing the model to become sensitive to a specific trigger applied to an image. We demonstrate that if an attacker can slightly manipulate the architecture only using common components they can introduce backdoors that survive re-training from scratch on a completely new dataset i.e. making these model backdoors weights- and dataset-agnostic. We describe a way to construct such Model Architecture Backdoors (MAB) and formalize their requirements. We find that architectural backdoors need to: (1) operate directly on the input and link the input to its output; (2) (ideally) have a weight-agnostic implementation; (3) have asymmetric components to launch targeted attacks. We demonstrate how such requirements make MAB detection possible and show that without these requirements, the learned backdoors will struggle to survive re-training.
We make the following contributions:
• We show a new class of backdoor attacks against neural networks, where the backdoor is planted inside of the    
model architecture; 2.3. Network architecture search and complex net-• We demonstrate how to build architectural backdoors for three different threat models and formalise the re-quirements for their successful operation;
• We demonstrate on a number of benchmarks that un-like previous methods that rely on weights [14, 15], backdoors at the architecture level survive retraining. 2.