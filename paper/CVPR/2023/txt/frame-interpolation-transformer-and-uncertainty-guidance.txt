Abstract
Video frame interpolation has seen important progress in recent years, thanks to developments in several direc-Some works leverage better optical ﬂow meth-tions. ods with improved splatting strategies or additional cues from depth, while others have investigated alternative ap-proaches through direct predictions or transformers. Still, the problem remains unsolved in more challenging condi-tions such as complex lighting or large motion.
In this work, we are bridging the gap towards video pro-duction with a novel transformer-based interpolation net-work architecture capable of estimating the expected er-ror together with the interpolated frame. This offers sev-*Work done during an internship at DisneyResearch|Studios eral advantages that are of key importance for frame inter-polation usage: First, we obtained improved visual qual-ity over several datasets. The improvement in terms of quality is also clearly demonstrated through a user study.
Second, our method estimates error maps for the interpo-lated frame, which are essential for real-life applications on longer video sequences where problematic frames need to be ﬂagged. Finally, for rendered content a partial render-ing pass of the intermediate frame, guided by the predicted error, can be utilized during the interpolation to generate a new frame of superior quality. Through this error estima-tion, our method can produce even higher-quality interme-diate frames using only a fraction of the time compared to a full rendering.
1.

Introduction
Video frame interpolation (VFI) is a classical video pro-cessing problem where the aim is to restore an intermedi-ate frame in a given video sequence. This temporal inbe-tweening enables many practical applications, such as video editing [38], novel-view synthesis [26], video retiming, and slow motion generation [25]. Recent advances in VFI meth-ods [13,24,28,30,37,48,53,55] have been continuously im-proving the interpolation quality, but the problem remains open due to complex lighting effects and large motion that are ubiquitous in real-life videos and can introduce severe artifacts for the existing methods.
We propose a transformer-based VFI architecture that processes both source and target frames in a uniﬁed frame-work and compensates motion through a tightly integrated optical ﬂow estimation and cross-backward warping. Our model improves over the current state-of-the-art as sup-ported by our extensive quantitative experiments and a user study.
Besides the improvements in terms of results, our model also predicts the interpolation uncertainty similar to ap-proaches for artifact detection [4, 49] and adaptive sam-pling [29, 60]. This is of key importance for usage in a production context, where working with long sequences re-quires a way to automatically identify problematic frames.
Uncertainty estimation also beneﬁts Computer Graphics (CG) applications, as we use it to determine which frame patches do not have sufﬁcient quality and optionally mark them for rendering. Thanks to our novel transformer-based model, the rendered patches from the middle frame nat-urally ﬁt in the same uniﬁed VFI framework, achieving high quality levels at the fraction of the cost of rendering the full middle frame. Our paradigm is more compatible with current production renderers than CG specialized VFI works [5, 21, 66] which require the generation of speciﬁc
G-buffers for the keyframes and the intermediate frame.
In summary, our contributions are as follows.
• We introduce a novel motion-based VFI method, that treats input and target frames in the same manner through a transformer-based architecture using masks.
• Our model achieves state-of-the-art performance as shown both in quantitative experiments and a user study.
• We perform output’s uncertainty estimation subtask, which can be particularly beneﬁcial for rendered con-tent to achieve even better quality results. 2.