Abstract
With the success of the 3D deep learning models, var-ious perception technologies for autonomous driving have been developed in the LiDAR domain. While these mod-els perform well in the trained source domain, they strug-gle in unseen domains with a domain gap.
In this pa-per, we propose a single domain generalization method for
LiDAR semantic segmentation (DGLSS) that aims to en-sure good performance not only in the source domain but also in the unseen domain by learning only on the source domain. We mainly focus on generalizing from a dense source domain and target the domain shift from different
LiDAR sensor configurations and scene distributions. To this end, we augment the domain to simulate the unseen do-mains by randomly subsampling the LiDAR scans. With the augmented domain, we introduce two constraints for gen-eralizable representation learning: sparsity invariant fea-ture consistency (SIFC) and semantic correlation consis-tency (SCC). The SIFC aligns sparse internal features of the source domain with the augmented domain based on the feature affinity. For SCC, we constrain the correlation be-tween class prototypes to be similar for every LiDAR scan.
We also establish a standardized training and evaluation setting for DGLSS. With the proposed evaluation setting, our method showed improved performance in the unseen domains compared to other baselines. Even without ac-cess to the target domain, our method performed better than the domain adaptation method. The code is available at https://github.com/gzgzys9887/DGLSS. 1.

Introduction
Understanding the surrounding scene is essential for au-tonomous driving systems. Using LiDAR sensors for per-ception has recently gained popularity from their ability to provide accurate distance information. Among such tasks,
LiDAR semantic segmentation (LSS) predicts point-wise semantic labels from a single sweep of LiDAR data. To-*The first two authors contributed equally. In alphabetical order.
Figure 1. Segmentation results on the source (SemanticKITTI [4]) and unseen (nuScenes-lidarseg [7]) domains. The results are from (a) ground truth, (b) domain adaptation method [60] adapted to
Waymo [64] dataset, and (c) ours. Our method predicts success-fully in both source and unseen domains, while the domain adap-tation method struggles in both domains. gether with the recent development of 3D point cloud deep-learning models and the release of several real-world 3D an-notated datasets [4, 7, 53, 64], numerous research on LiDAR semantic segmentation have emerged lately. However, since these models do not consider the data distribution differ-ences between the train and test domains during the learn-ing process, severe performance deterioration occurs when applied to real-world applications where domain gaps exist.
Two main domain gaps arise for real-scene LiDAR datasets: 1) differences in sparsity due to different sensor configurations and 2) differences in scene distribution. De-pending on the type of LiDAR sensor, the total number of beams, vertical field of view (FOV), and vertical and hori-zontal resolutions differ, which leads to different sampling patterns. SemanticKITTI [4] dataset used a 64-beam Li-DAR sensor, but a 32-beam sensor was used in nuScenes-lidarseg [7], and for SemanticPOSS [53], a 40-beam sen-sor was used. Waymo [64] dataset also used a 64-beam
LiDAR, but there is a sparsity difference because the ver-tical resolution differs from SemanticKITTI. Table 1 sum-marizes configuration details of LiDAR sensors used in Se-manticKITTI, nuScenes-lidarseg, Waymo, and Semantic-POSS. In addition, each dataset was acquired from different
locations, so the scene structure as well as category distri-bution can vary enormously. For example, SemanticKITTI mainly contains suburban areas and therefore has a high number of cars, roads, and vegetation. Whereas nuScenes-lidarseg and Waymo datasets were acquired from dense ur-ban environments and contain crowded pedestrian scenes, downtown, and also residential areas. Fig. 1 shows the dif-ferences in sparsity and scene between different datasets.
Recently, unsupervised domain adaptation (UDA) meth-ods for LiDAR point clouds [1, 60, 72, 76, 81] have been proposed to mitigate performance degradation in a specific target domain. While UDA methods perform well in the target domain, they cannot guarantee high performance in unseen domains, which is critical for safe driving systems.
As shown in Fig. 1(b), the UDA method [60] trained on the source domain (SemanticKITTI) and adapted to the tar-get domain (Waymo) has low performance in the unseen domain (nuScenes-lidarseg). Also, the cumbersome pro-cess of acquiring new data and retraining is required to ap-ply UDA to a new target domain. These problems can be tackled by building domain generalizable LSS models that guarantee performance on unseen domains and robustness against domain gaps. Among previous works, [76] did test its UDA methodology in a domain generalization (DG) set-ting. However, the method was not specifically designed for
DG and was deemed impractical for real-world applications due to its limited evaluation with 2 classes. All of these cir-cumstances further highlight the importance of developing methodologies that primarily aim at domain generalization for LiDAR semantic segmentation.
In this paper, we propose a domain generalization ap-proach for LiDAR semantic segmentation (DGLSS), which aims to achieve high performance in unseen domains while training the segmentation model only on source domains.
In particular, we propose a representation learning approach for DGLSS by focusing on differences in sparsity and scene distribution. Since obtaining multiple fully-labeled LiDAR datasets as source domains for training is cost-expensive, we choose to perform generalization using a single source domain. Also, as using sparser LiDAR data in an ap-plication is more efficient and a more likely scenario for autonomous driving, we focus on learning from a denser source domain. To this end, we first simulate the unseen domain during the learning process while considering the characteristics of the actual LiDAR sensor. We augment the domain by randomly subsampling LiDAR beams from the LiDAR scan of the source domain at every iteration of the learning process. With the augmented sparse domain, we introduce two constraints for generalizable representa-tion learning: sparsity invariant feature consistency (SIFC) and semantic correlation consistency (SCC). The purpose of
SIFC is to align the internal sparse features of the source do-main with the augmented domain based on the feature affin-Table 1. Configuration of sensors used to acquire LiDAR datasets.
Dataset
SemanticKITTI [4] nuScenes-lidarseg [7]
Waymo [64]
SemanticPOSS [53]
LiDAR beams 64 32 64 40 vertical
FOV(°)
[-23.6, 3.2]
[-30, 10]
[-17.6, 2.4]
[-16, 7] vertical res(°) 0.4 1.33 0.31 0.33/1 horizontal res(°) 0.08 0.1–0.4 0.16 0.2 range (m) 120 70 75 200 ity. For SCC, we build scene-wise class prototypes and con-strain the correlation between class prototypes to be similar for every LiDAR scene regardless of domain. Learning with the proposed constraints, the model can generalize well on unseen domains that have different sparsity and scene dis-tribution compared to the source domain.
In addition, we build standardized training and evalua-tion settings for DGLSS, as such setup for domain general-ization has been absent. For this, we employ 4 real-world
LiDAR datasets [4, 7, 53, 64] and carefully select 10 com-mon classes existing in datasets. Then, we remap the labels of each dataset to the common labels. Furthermore, we im-plement baseline methods using general-purpose DG meth-ods applicable to LiDAR point cloud [38, 51] and evaluate them in the proposed evaluation setting. With the proposed evaluation setting, our method showed good performance in the unseen domains compared to the baselines. Even with-out access to the target domain, our method showed com-parable performance to the domain adaptation method as shown in Fig. 1(c).
In summary, our contributions are as follows:
• To the best of our knowledge, we propose the first ap-proach that primarily aims at domain generalization for
LiDAR semantic segmentation (DGLSS).
• We build standardized training and evaluation settings for
DGLSS and implement several DG baseline methods ap-plicable to the point cloud domain.
• We propose sparsity invariant feature consistency and se-mantic correlation consistency for generalizable repre-sentation learning for DGLSS, which can effectively deal with domain gaps of LiDAR point clouds.
• Extensive experiments show that our approach outper-forms both UDA and DG baselines. 2.