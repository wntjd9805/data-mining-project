Abstract
Unsupervised Domain Adaptation (UDA) technique has been explored in 3D cross-domain tasks recently. Though preliminary progress has been made, the performance gap between the UDA-based 3D model and the supervised one trained with fully annotated target domain is still large. This motivates us to consider selecting partial-yet-important target data and labeling them at a minimum cost, to achieve a good trade-off between high performance and low annotation cost. To this end, we propose a Bi-domain active learning approach, namely Bi3D, to solve the cross-domain 3D object detection task. The Bi3D ﬁrst develops a domainness-aware source sampling strategy, which identi-ﬁes target-domain-like samples from the source domain to avoid the model being interfered by irrelevant source data.
Then a diversity-based target sampling strategy is devel-oped, which selects the most informative subset of target do-main to improve the model adaptability to the target domain using as little annotation budget as possible. Experiments are conducted on typical cross-domain adaptation scenar-ios including cross-LiDAR-beam, cross-country, and cross-sensor, where Bi3D achieves a promising target-domain de-tection accuracy (89.63% on KITTI) compared with UDA-based work (84.29%), even surpassing the detector trained on the full set of the labeled target domain (88.98%). Our code is available at: https://github.com/PJLab-ADG/3DTrans. 1.

Introduction
LiDAR-based 3D Object Detection (3DOD) [5, 13, 26, 28, 40] has advanced a lot recently. However, the gen-eralization of a well-trained 3DOD model from a source point cloud dataset (domain) to another one, namely cross-∗This work was done when Jiakang Yuan was an intern at Shanghai
AI Laboratory.
†Corresponding to: Tao Chen (eetchen@fudan.edu.cn), Bo Zhang (zhangbo@pjlab.org.cn) (a) General 3D Object Detection (a) General 3D Object Detection (a) General 3D Object Detection
Train
Train
Train
Map to BEV
Map to BEV
Map to BEV 2 2 2
D
D
D
B
B
B a a a c c c k k k b b b o o o n n n e e e
R
R
R
P
P
P
N
N
N
D
D
D e e e t t t e e e c c c t t t o o o r r r
H
H
H e e e a a a d d d
Data Augmentation
Data Augmentation
Data Augmentation 3D backbone 3D backbone 3D backbone
BEV features
BEV features
BEV features (b) Self-training based UDA for 3D  (b) Self-training based UDA for 3D  (b) Self-ff training based UDA for 3D 
Object Detection
Object Detection
Object Detection
Train
Train
Train
Detector
Detector
Detector
NMS
NMS
NMS
Predicted
Predicted
Predicted
Predicted
Bounding Boxes
Bounding Boxes
Bounding Boxes
Bounding Boxes
Instance categories
Instance categories
Instance categories
Bounding boxes
Bounding boxes
Bounding boxes
Pseudo Label
Pseudo Label
Pseudo Label
Pseudo Label
Filtering low confidence 
Filtering low confidence 
Filtering low confidence  bounding boxes bounding boxes bounding boxes (c) ADA for 3D Object Detection (c) ADA for 3D Object Detection (c) ADA for 3D Object Detection
Train
Train
Train
Detector
Detector
Detector
Instance-level 
Instance-level 
Instance-level  features features features
Scene-level 
Scene-level 
Scene-level  features features features
Annotate
Annotate
Annotate
Oracle
Oracle
Oracle
Selected representative
Selected representative
Selected representative unlabel target frames unlabel target frames unlabel target frames
Active sampling 
Active sampling 
Active sampling  criteria criteria criteria e e e c c c r r r u u u o o o
S
S
S t t t e e e g g g r r r a a a
T
T
T
Figure 1. Comparisons among (a) The general 3DOD pipeline, (b) Self-training based Unsupervised Domain Adaptation 3DOD pipeline, and (c) Active Domain Adaptation 3DOD pipeline that selects representative target data, and then annotates them by an oracle (human expert) for subsequent model reﬁnement. domain 3DOD, is still under-explored. Such a task in fact is important in many real-world applications. For example, in the autonomous driving scenario, the target scene distri-bution frequently changes due to unforeseen differences in dynamically changing environments, making cross-domain 3DOD an urgent problem to be resolved.
Beneﬁting from the success of Unsupervised Domain
Adaptation (UDA) technique in 2D cross-domain tasks [3, 7,10,14,32,45,48], several attempts are made to apply UDA for tackling 3D cross-domain tasks [15,20,22,36,39,42,46].
ST3D [42] designs a self-training-based framework to adapt a pre-trained detector from the source domain to a new tar-get domain. LiDAR distillation [36] exploits transferable knowledge learned from high-beam LiDAR data to the low one. Although these UDA 3D models have achieved sig-niﬁcant performance gains for the cross-domain task, there            
is still a large performance gap between these UDA models and the supervised ones trained using a fully-annotated tar-get domain. For example, ST3D [42] only achieves 72.94%
AP3D in nuScenes [1]-to-KITTI [8] cross-domain setting, yet the fully-supervised result using the same baseline de-tector can reach to 82.50% AP3D on KITTI.
To further reduce the detection performance gap between
UDA-based 3D models and the fully-supervised ones, an initial attempt is to leverage Active Domain Adaptation (ADA) technique [6, 17, 29, 37, 38], whose goal is to se-lect a subset quota of all unlabeled samples from the target domain to perform the manual annotation for model train-ing. Actually, the ADA task has been explored in 2D vi-sion ﬁelds such as AADA [29], TQS [6], and CLUE [17], but its research on 3D point cloud data still remains blank.
In order to verify the versatility of 2D image-based ADA methods towards 3D point cloud, we conduct extensive at-tempts by integrating the recently proposed ADA methods, e.g., TQS [6] and CLUE [17], into several typical 3D base-line detectors, e.g., PV-RCNN [26] and Voxel R-CNN [5].
Results show that these 2D ADA methods cannot obtain sat-isfactory detection accuracy under the 3D scene’s domain discrepancies. For example, PV-RCNN coupled with TQS only achieves 75.40% AP3D, which largely falls behind the fully-supervised result 82.50% AP3D.
As a result, directly selecting a subset of given 3D frames using 2D ADA methods to tackle 3D scene’s do-main discrepancies is challenging, which can be attributed to the following reasons. (1) The sparsity of the 3D point clouds leads to huge inter-domain discrepancies that harm the discriminability of domain-related features. (2) The intra-domain feature variations are widespread within the source domain, which enlarges the differentiation between the selected target domain samples and the entire source do-main samples, bringing negative transfer to the model adap-tation on the target domain.
To this end, we propose a Bi-domain active learning (Bi3D) framework to conduct the active learning for the 3D point clouds. To tackle the problem of sparsity, we design a foreground region-aware discriminator, which ex-ploits an RPN-based attention enhancement to derive a foreground-related domainness metric, that can be regarded as an important proxy for active sampling strategy. To address the problem of intra-domain feature variations within the source domain, we conceive a Bi-domain sam-pling approach, where Bi-domain means that data from both source and target domains are picked up for safe and robust model adaptation. Speciﬁcally, the Bi3D is composed of a domainness-aware source sampling strategy and a diversity-based target sampling strategy. The source sampling strat-egy aims to select target-domain-like samples from the source domain, by judging the corresponding domainness score of each given source sample. Then, the target sam-pling strategy is utilized to select diverse but representative data from the target domain by dynamically maintaining a similarity bank. Finally, we employ the sampled data from both domains to adapt the source pre-trained detector on a new target domain at a low annotation cost.
The main contributions can be summarized as follows: 1. From a new perspective of chasing high performance at a low cost, we explore the possibilities of leverag-ing active learning to achieve effective 3D scene-level cross-domain object detection. 2. A Bi-domain active sampling approach is proposed, consisting of a domainness-aware source sampling strategy and a diversity-based target sampling strat-egy to identify the most informative samples from both source and target domains, boosting the model’s adap-tation performance. 3. Experiments show that Bi3D outperforms state-of-the-art UDA works with only 1% target annotation bud-get for cross-domain 3DOD. Moreover, Bi3D achieves 89.63% APBEV in the nuScenes-to-KITTI scenario, surpassing the fully supervised result (88.98% APBEV) on the KITTI dataset. 2.