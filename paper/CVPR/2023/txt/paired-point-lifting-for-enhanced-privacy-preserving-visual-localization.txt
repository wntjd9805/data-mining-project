Abstract
Visual localization refers to the process of recovering camera pose from input image relative to a known scene, forming a cornerstone of numerous vision and robotics sys-tems. While many algorithms utilize sparse 3D point cloud of the scene obtained via structure-from-motion (SfM) for localization, recent studies have raised privacy concerns by successfully revealing high-fidelity appearance of the scene from such sparse 3D representation. One prominent ap-proach for bypassing this attack was to lift 3D points to randomly oriented 3D lines thereby hiding scene geome-try, but latest work have shown such random line cloud has a critical statistical flaw that can be exploited to break through protection. In this work, we present an alternative lightweight strategy called Paired-Point Lifting (PPL) for constructing 3D line clouds. Instead of drawing one ran-domly oriented line per 3D point, PPL splits 3D points into pairs and joins each pair to form 3D lines. This seemingly simple strategy yields 3 benefits, i) new ambiguity in fea-ture selection, ii) increased line cloud sparsity and iii) non-trivial distribution of 3D lines, all of which contributes to enhanced protection against privacy attacks. Extensive ex-perimental results demonstrate the strength of PPL in con-cealing scene details without compromising localization ac-curacy, unlocking the true potential of 3D line clouds. 1.

Introduction
Visual localization is the fundamental problem of esti-mating the camera pose from an input image with respect to a known 3D scene.
It plays a crucial role in many com-puter vision and robotics applications, including human-computer interaction based on augmented or mixed real-ity (AR/MR), 3D reconstruction via structure-from-motion (SfM) [1, 36, 37, 39] and autonomous navigation systems in drones, self-driving vehicles, or robots using simultaneous localization and mapping (SLAM) [5, 22, 26].
To this date, many practical visual localization algo-rithms are still structure-based approaches [12], utilizing a global sparse 3D model of the scene obtained from SfM or SLAM. In such approaches, 2D-3D correspondences are formed between 2D image points and the global 3D
*Corresponding author (c) PPL (ours) (a) Point cloud (b) Line cloud [40]
Figure 1. Visualization of different 3D scene representations and respective image reconstruction results using InvSfM [30]. Images in (b) and (c) are reconstructed by estimating the 3D point clouds from line clouds via [3]. While images revealed from original line clouds (OLC) [40] still contain basic scene details, those from the proposed method (PPL) exhibit much degraded scene quality. structure by comparing feature descriptors (e.g., SIFT [21],
ORB [34], or other alternatives [25, 43]), after which they are used to perform robust camera pose estimation based on geometric constraints and RANSAC [2, 8, 31]. While most research over the last decade have made significant progress in improving algorithm accuracy, scalability [19, 23, 35, 47] and efficiency [12,20], the requirement that sparse 3D point cloud and associated feature descriptors need to be persis-tently accessible has largely remained unchanged.
Recently, it caught the research community by surprise when Pittaluga et al. [30] showed sparse 3D point clouds can retain enough scene details such as characters and tex-tures that can be used to uncover a high-fidelity image of the scene. Since this process only requires 2D projection of 3D points and their respective feature descriptors, this raised privacy concerns related to uploading sparse 3D models in the cloud or storing them on the end-user device, both of which are commonly found settings in visual localization.
(a) Standard lifting with uniformly-distributed line directions [40] (b) Paired-point lifting (PPL) (ours)
Figure 2. Illustration of the previous standard lifting approach (also referred to as the original line cloud (OLC)) [40] and PPL (ours) for constructing 3D line clouds from sparse 3D point clouds. In (a), each line passes through one 3D point with its direction uniformly sampled on a unit sphere, carrying one descriptor. In (b), each line passes through two 3D points selected at random, carrying two descriptors.
One of the most notable approaches for addressing above privacy attack on point clouds is geometric lifting, whereby each 3D point is transformed into a randomly-oriented 3D line passing through the respective point [40]. By sam-pling each line direction uniformly on the unit sphere, the approach aims to conceal the scene geometry and prevent meaningful 2D projections of the sparse point cloud.
Nevertheless, a recent work by Chelani et al. [3] showed 3D lines whose directions are uniformly drawn from a unit sphere carry an unintended statistical characteristic that the original 3D points are likely to be near the high-density re-gions of the closest points between lines. This property can be exploited to reveal the scene geometry (see ยง2), and to the best of our knowledge, the only suggested option for mitigating this attack so far is to use sparser point clouds, which comes at the cost of reduced localization accuracy.
In this work, we argue that the privacy-preserving prop-erty of 3D line clouds can be enhanced by applying a differ-ent line-construction approach. Specifically, we propose to yield 3D lines by joining random pairs of 3D points. This approach, which we call Paired-Point Lifting (PPL), is mo-tivated by the three key observations listed below: 1. confusion over feature descriptors: each line carries two feature descriptors, so assigning the correct de-scriptors to each of two 3D points requires guesswork, 2. line cloud sparsification: PPL results in 50% more sparse line clouds, thus degrading the accuracy of 3D scene point recovery using [3], which relies on a suffi-cient number of neighboring lines, and 3. non-trivial distribution of lines: the finding from [3] that the original 3D point is likely to be near the high-density region of closest points to other lines is plagued by the presence of extra 3D point in each line and non-uniformly distributed line directions.
We illustrate in ยง3 and ยง5 that each of the above factors adds a layer of difficulty in revealing the scene geometry and image-level details, synergistically enhancing privacy-preserving visual localization (partly shown in Fig. 1) with-out compromising camera localization accuracy.
Overall, our contributions can be summarized as follows:
+ a new strategy called paired-point lifting (PPL) for constructing 3D line clouds with each line concealing two 3D points from the respective sparse point clouds,
+ careful empirical analysis of success factors in PPL through an ablation study using synthetic and real data,
+ improving the 3D point recovery algorithm in [3] to allow handling PPL-based line clouds and locate two 3D points in each line for a fairer comparison,
+ a subsequent upgrade of PPL called PPL+ to address potential drawback of PPL with planar scenes, and
+ extensive experimental evaluation of both PPL and
PPL+ against other baselines on a range of public datasets using different point recovery techniques. 2.