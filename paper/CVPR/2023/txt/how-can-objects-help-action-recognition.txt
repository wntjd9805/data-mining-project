Abstract
Current state-of-the-art video models process a video clip as a long sequence of spatio-temporal tokens. How-ever, they do not explicitly model objects, their interactions across the video, and instead process all the tokens in the video. In this paper, we investigate how we can use knowl-edge of objects to design better video models, namely to process fewer tokens and to improve recognition accuracy.
This is in contrast to prior works which either drop tokens at the cost of accuracy, or increase accuracy whilst also increasing the computation required. First, we propose an object-guided token sampling strategy that enables us to re-tain a small fraction of the input tokens with minimal im-pact on accuracy. And second, we propose an object-aware attention module that enriches our feature representation with object information and improves overall accuracy. Our resulting model, ObjectViViT, achieves better performance when using fewer tokens than strong baselines. In partic-ular, we match our baseline with 30%, 40%, and 60% of the input tokens on SomethingElse, Something-something v2, and Epic-Kitchens, respectively. When we use Object-ViViT to process the same number of tokens as our baseline, we improve by 0.6 to 4.2 points on these datasets. 1.

Introduction
Video understanding is a central task of computer vi-sion and great progresses have been made recently with transformer-based models which interpret a video as a se-quence of spatio-temporal tokens [1, 3, 13, 16, 30, 55]. How-ever, videos contain a large amount of redundancy [53], es-pecially when there is little motion or when backgrounds re-main static. Processing videos with all these tokens is both inefﬁcient and distracting. As objects conduct motions and actions [19, 47], they present an opportunity to form a more compact representation of a video, and inspire us to study how we can use them to understand videos more accurately and efﬁciently. Our intuition is also supported biologically that we humans perceive the world by concentrating our fo-cus on key regions in the scene [18, 42].
Figure 1. How can objects help action recognition? Consider this action of picking up a bowl on a countertop packed with kitchenware. Objects provide information to: (1) associate image patches (colorful) from the same instance, and identify candidates for interactions; (2) selectively build contextual information from the redundant background patches (dark).
In this paper, we explore how we can use external ob-ject information in videos to improve recognition accuracy, and to reduce redundancy in the input (Figure 1). Cur-rent approaches in the literature have proposed object-based models which utilize external object detections to improve action recognition accuracy [21, 35]. However, they aim to build architectures to model objects and overall bring a notable computational overhead. We show besides gain-ing accuracy, objects are extremely useful to reduce token redundancy, and give a more compact video representa-tion. Fewer tokens also enable stronger test strategies (e.g., multi-crop, longer videos), and overall improve accuracy further. On the other hand, prior work on adaptive trans-formers [36, 45, 48, 50] dynamically reduce the number of tokens processed by the network conditioned on the input.
Since these methods learn token-dropping policies end-to-end without external hints, there can be a chicken-and-egg problem: we need good features to know which token to drop, and need many tokens to learn good features. As a re-sult, they usually suffer from a performance decrease when dropping tokens. We show we can perform both goals, namely reducing tokens processed by the transformer, as well as improving accuracy, within a uniﬁed framework.
Concretely, we propose an object-based video vision transformer, ObjectViViT. As shown in Figure 2, we ﬁrst propose an object-guided token sampling strategy (OGS) that uses object locations to identify foreground and back-ground tokens. We retain relevant foreground tokens as is, and aggressively downsample background tokens be-Figure 2. Illustration of our object-based video vision transformer, ObjectViViT. ObjectViViT takes raw video pixels and off-the-shelf object detections (bounding boxes) as input, and runs space-time attention on video tokens [1]. We use the detection boxes in two ways. (1): we use object locations to downsample the patch tokens before running transformers (Object-Guided Token Sampling, more details in
Figure 3). (2): we run a customized attention module that creates object tokens from object-patch relations and uses them to enhance patch features (Object-Aware Attention Module, more details in Figure 4). fore forwarding them to the transformer module. Secondly, to fully leverage the relation between objects and the un-structured spatial-temporal patches, we introduce an object-aware attention module (OAM). This attention module ﬁrst creates object tokens by grouping patch tokens from the same object using an object-weighted pooling, and then ap-plies space-time attention on the concatenated object and patch tokens. This way, patch features are augmented with their related object information. Both OGS and OAM are complementary. They can be used individually to improve either token-compactness or accuracy, or can be used to-gether to get beneﬁts of both.
We validate our method with extensive experiments on
SomethingElse [32], Something-Something [17], and the
Epic Kitchens datasets [8]. Using our object-guided to-ken sampling, we ﬁnd that we can process 60% ∼ 90% of the input tokens without losing any accuracy. And by using our object-aware attention module alone, we outper-form a competitive ViViT [1] baseline by 0.6 to 2.1 points.
Combining both modules, ObjectViViT improves token-compactness and accuracy even further, matching baseline performance by processing 30%, 40%, and 60% of the in-put tokens for the three datasets, respectively. Finally, un-der the same number of processed tokens but a higher tem-poral resolution, our model with dropped tokens improve upon baselines by up to 4.2 points. Our code is released at https://github.com/google-research/scenic. 2.