Abstract
Decision forests are among the most accurate models in machine learning. This is remarkable given that the way they are trained is highly heuristic: neither the individ-ual trees nor the overall forest optimize any well-deﬁned loss. While diversity mechanisms such as bagging or boost-ing have been until now critical in the success of forests, we think that a better optimization should lead to better forests—ideally eliminating any need for an ensembling heuristic. However, unlike for most other models, such as neural networks, optimizing forests or trees is not easy, be-cause they deﬁne a non-differentiable function. We show, for the ﬁrst time, that it is possible to learn a forest by op-timizing a desirable loss and regularization jointly over all its trees and parameters. Our algorithm, Forest Alternating
Optimization, is based on deﬁning a forest as a paramet-ric model with a ﬁxed number of trees and structure (rather than adding trees indeﬁnitely as in bagging or boosting).
It then iteratively updates each tree in alternation so that the objective function decreases monotonically. The algo-rithm is so effective at optimizing that it easily overﬁts, but this can be corrected by averaging. The result is a forest that consistently exceeds the accuracy of the state-of-the-art while using fewer, smaller trees. 1.

Introduction
In the past two decades, decision tree ensembles (forests) have been recognized as among the most accurate of all ma-chine learning (ML) models for regression, classiﬁcation and other tasks. This is evidenced by their widespread use in practical applications (from fraud detection to ranking) and by regularly being at the top of leaderboards in ML com-petitions and practitioner surveys (such as Kaggle or KD-nuggets). While achieving the best performance possible does require some hyperparameter tuning, this job is much easier compared to neural networks, for example. For this reason they are often considered off-the-shelf algorithms.
*currently at Meta AI (FAIR)
At the same time, the training algorithm for forests seems outdated, given the larger, increasing role that numer-ical optimization has played in ML in recent years. Indeed, to train a forest, one does not choose a loss function and reg-ularization terms over a parametric model and optimize that on a training set. Instead, one relies on two building blocks.
First, a procedure to learn an individual tree. This is almost always based on a greedy recursive partitioning procedure, such as CART [6], C4.5 [26] or its variations. Second, a procedure to create the ensemble. The most successful ones are bagging and feature sampling (in Random Forests (RF)) and boosting (in AdaBoost and Gradient Boosting (GB)).
Neither of these building blocks deﬁne a global objective function of the forest’s parameters and optimize it, instead they rely on local, proxy objectives (e.g. purity in learning tree splits in CART, or the local loss in GB). Indeed, the only way the model improves its accuracy is by adding more pa-rameters (more nodes in a tree or more trees in a forest), not by optimizing existing parameters. This leads to much larger models than is necessary. The undeniable success of forests has been attributed to intuitive but slippery concepts such as the diversity of the base learners (trees). For RFs and boosting, multiple conﬂicting theories have been put forward [2, 5, 13, 20, 24, 25, 27, 28]. It is fair to say that no-body really understands why RF, AdaBoost or GB forests work. It also seems reasonable that jointly optimizing over the trees should make them naturally diverse, just as neu-rons in the same layer of a neural network differ from each other when optimized with backpropagation.
We do not seek to explain why current forest algorithms work. We seek to learn forests using solid optimization prin-ciples, bringing them into the mainstream of modern ML, and as a result learn even better forests. Indeed, we can interpret some recent advances as being due to a better op-timization. One view of some forms of boosting connected it with optimization [13, 24]. Gradient boosting (GB) [14], possibly the type of forest that generally leads to the high-est accuracy, relies on an attempt to make boosting close to an optimization in model space that follows a functional gradient. Unfortunately, GB relies on multiple approxima-tions (including the use of CART to learn individual trees),
and results in the number of trees growing indeﬁnitely and greedily. The latter is also true of AdaBoost and Random
Forests. Adding more and more trees to a forest (with some care, e.g. using a small step size in GB) often leads to the highest accuracy and overﬁts very slowly. However, it also is very inefﬁcient in parameter use. Each tree contributes very little to the total, and pruning a forest a posteriori often reduces considerably the number of trees without hurting much the accuracy. It stands to reason that, if we could opti-mize properly a forest jointly over all parameters in all trees, we could achieve the same accuracy with fewer trees, even.
Another recent advance is the Tree Alternating Optimiza-tion (TAO) algorithm, which puts decision trees ﬁrmly into modern ML. TAO is able to optimize a well-deﬁned loss function and regularization over a tree of ﬁxed structure, monotonically decreasing the objective function at each iter-ation, and scaling to large datasets and trees. TAO can learn quite general types of trees, beyond the axis-aligned trees used in traditional forests, and vastly outperforms CART and C4.5 [35]. In particular, sparse oblique trees (having hyperplane splits with few nonzero weights) have proven very powerful. In a series of papers [9, 16, 32, 33], using
TAO as base learner with any of the classic ensemble mecha-nisms (bagging, AdaBoost, GB) has been shown to produce forests that are more accurate while using fewer, shallower trees. This is compelling evidence for the importance of optimization in learning forests.
In this paper, we propose the ﬁrst algorithm (as far as we know) that can optimize a global objective function of all the parameters of a forest of predetermined structure (number, type and structure of trees), by iteratively decreas-ing the objective function given initial random parameters.
This makes it possible to pick a loss and regularization, and a parametric form for the forest, and optimize exactly that.
Our algorithm, Forest Alternating Optimization (FAO), is described in section 4. It relies heavily on TAO, which we review in section 3. FAO works so well at optimizing on the training set that it can make the forest overﬁt easily for rea-sons described in section 5. We can avoid this by averaging several independent FAO forests, and this results in forests that exceed the state of the art in both accuracy and forest size, as shown experimentally in section 6. 2.