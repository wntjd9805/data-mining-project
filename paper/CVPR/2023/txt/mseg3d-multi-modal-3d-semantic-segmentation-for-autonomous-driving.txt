Abstract
LiDAR and camera are two modalities available for 3D semantic segmentation in autonomous driving. The popular LiDAR-only methods severely suffer from inferior segmentation on small and distant objects due to insufficient laser points, while the robust multi-modal solution is under-explored, where we investigate three crucial inherent limited sensor field difficulties: modality heterogeneity, of view intersection, and multi-modal data augmentation.
We propose a multi-modal 3D semantic segmentation model (MSeg3D) with joint intra-modal feature extraction and inter-modal feature fusion to mitigate the modality heterogeneity. The multi-modal fusion in MSeg3D consists of geometry-based feature fusion GF-Phase, cross-modal feature completion, and semantic-based feature fusion
The multi-modal data
SF-Phase on all visible points. augmentation is reinvigorated by applying asymmetric transformations on LiDAR point cloud and multi-camera images individually, which benefits the model training with diversified augmentation transformations. MSeg3D achieves state-of-the-art results on nuScenes, Waymo, and
SemanticKITTI datasets. Under the malfunctioning multi-camera input and the multi-frame point clouds input,
MSeg3D still shows robustness and improves the LiDAR-only baseline. Our code is publicly available at https:
//github.com/jialeli1/lidarseg3d. 1.

Introduction
Scene understanding for safe autonomous driving can be achieved through semantic segmentation using camera 2D images and LiDAR 3D point clouds, which densely classifies each smallest sensing unit of the modality. The image-based 2D semantic segmentation has been developed with massive solid studies [12, 34, 61, 63]. The camera image has rich appearance information about the object but severely suffers from illumination, varying object scales, and indirect applications in the 3D world. Another modal-ity, LiDAR point cloud, drives 3D semantic segmentation with laser points [1, 3, 11, 37]. Unfortunately, irregular laser points are too sparse to capture the details of objects.
The inaccurate segmentation appears especially on small and distant objects. The other under-explored direction is using multi-modal data to increase both the robustness and accuracy in 3D semantic segmentation [67].
Despite the conceptual superiority, the development of multi-modal segmentation model is still nontrivial, lagging behind the single-modal methods [27]. We rationally i) attribute the difficulties to the three following aspects.
Heterogeneity between modalities. Due to sparse points and dense pixels, point cloud feature extractors [14, 31] and image feature extractors [15, 44, 47] are developed
Separate intra-modal feature extractors are distinctly. used to address the heterogeneity [13, 20, 25, 42, 51], the lack of joint optimization leads to suboptimal but ii) Limited features from irrelevant network parameters. intersection on the field of view (FOV) between sensors.
Only the points falling into the intersected FOV are geometrically associated with multi-modal data, while simply considering the intersected multi-modal data is not sufficient to be practically applicable. Performing fusion solely in the limited FOV intersection like [20,67] results in unsatisfactory overall segmentation performance as shown in Fig. 1.
For example, PMF [67] uses only several 2D augmentations for spatially aligned point cloud projection image and camera RGB image. Under the constraint of modal alignment or 2D representation of point cloud, the multi-modal segmentation works [20, 25, 67] discard many useful and critical point cloud augmentation transformations with sacrificed perception performance [36, 48]. iii) Multi-modal data augmentation.
Accordingly, we propose a top-performing multi-modal 3D semantic segmentation method termed MSeg3D, in-herently motivated by addressing the aforementioned three technical difficulties. i) Unlike separately extracting modal features in existing methods [13, 25, 42, 51], we jointly optimize intra-modal feature extraction and inter-modal feature fusion to drive maximum correlation and com-ii) To plementarity between heterogeneous modalities. overcome the disregarded multi-modal fusion outside FOV
intersection [25, 67], we propose a cross-modal feature completion and a semantic-based feature fusion phase
SF-Phase to collaborate with the geometry-based feature fusion phase GF-Phase.
For points outside the FOV the former completes the missing camera intersection, features using predicted pseudo-camera features, under the explicit guidance of cross-modal supervision. For all the points outside and inside the FOV intersection, the later SF-Phase leverages the multi-head attention [41] to model the semantic relation between point and interested categories so that we can attentively fuse the semantic embeddings aggregated from all the visible fields to each point. iii) The challenging multi-modal data augmentation is reinvigorated by being decomposed as the asymmetric transformations in the LiDAR, camera worlds, and local cameras, which enables flexible permutation to enrich training samples.
As the proposed components accumulated in Fig. 1, mIoU and mIoU1 are significantly increased while the gaps between mIoU and mIoU1 are gradually decreased. Our i) We propose a multi-modal contributions are four-fold: segmentation model MSeg3D with joint intra-modal feature extraction and inter-modal feature fusion, achieving state-of-the-art 3D segmentation performance on the competitive nuScenes [3], Waymo [37], and SemanticKITTI [1] datasets for autonomous driving. The proposed framework won 2nd place in the Waymo 3D semantic segmentation challenge ii) We propose a cross-modal feature at CVPR 2022. completion and a semantic-based feature fusion phase. To our best knowledge, it is the first time to address the overlooked and inapplicable multi-modal fusion outside the iii) By applying augmentation sensor FOV intersection. transformations asymmetrically on point cloud and images, the proposed asymmetrical multi-modal data augmentation significantly increases the diversity of multi-modal samples for training model with robust improvements. iv) Extensive experimental analyses on the improvement and robustness of our method clearly investigate our designs. 2.