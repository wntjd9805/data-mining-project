Abstract
Face recognition is a prevailing authentication solution in numerous biometric applications. Physical adversarial attacks, as an important surrogate, can identify the weak-nesses of face recognition systems and evaluate their ro-bustness before deployed. However, most existing physical attacks are either detectable readily or ineffective against commercial recognition systems. The goal of this work is to develop a more reliable technique that can carry out an end-to-end evaluation of adversarial robustness for commercial systems. It requires that this technique can simultaneously deceive black-box recognition models and evade defensive mechanisms. To fulfill this, we design adversarial textured 3D meshes (AT3D) with an elaborate topology on a human face, which can be 3D-printed and pasted on the attacker’s face to evade the defenses. However, the mesh-based op-timization regime calculates gradients in high-dimensional mesh space, and can be trapped into local optima with un-satisfactory transferability. To deviate from the mesh-based space, we propose to perturb the low-dimensional coeffi-cient space based on 3D Morphable Model, which signifi-cantly improves black-box transferability meanwhile enjoy-ing faster search efficiency and better visual quality. Exten-sive experiments in digital and physical scenarios show that our method effectively explores the security vulnerabilities of multiple popular commercial services, including three recognition APIs, four anti-spoofing APIs, two prevailing mobile phones and two automated access control systems. 1.

Introduction
Face recognition has become a prevailing authentication solution in biometric applications, ranging from financial payment to automated surveillance systems. Despite its
†Corresponding authors.
Figure 1. Demonstration of physical black-box attacks for unlock-ing one prevailing mobile phone. The attacker wearing the 3D-printed adversarial mesh can successfully mislead the face recog-nition model to be recognized as the victim, meanwhile evading face anti-spoofing. More results are shown in Sec. 4. blooming development [4, 26, 33], recent research in adver-sarial machine learning has revealed that face recognition models based on deep neural networks are highly vulnera-ble to adversarial examples [10, 41], leading to serious con-sequences or security problems in real-world applications.
Due to the imperative need of evaluating model robust-ness [30, 45], extensive attempts have been devoted to ad-versarial attacks on face recognition models. Adversarial at-tacks in the digital world [8, 28, 39, 45] are characterized by adding minimal perturbations to face images in the digital space, aiming to evade being recognized or to impersonate another identity. Since an adversary usually cannot access the digital input of practical systems, physical adversarial examples wearable for real human faces are more feasible for evaluating their adversarial robustness. Some studies have shown the success of physical attacks against popular recognition models by adopting different attack types, such as eyeglass frames [27, 28], hats [17] and stickers [29].
In spite of the remarkable progress, it is challenging to launch practical and effective physical attack methods on automatic face recognition systems. First, the defen-3D attack types
Commercial recognition
Commercial defenses
Number of physical tests
Frames [28] AdvHat [17] FaceAdv [29] PadvFace [50] AdvMask [52] Face3DAdv [40] RHDE [35] Ours
Partially Yes
Yes
Yes 50
Partially
No
No 3
Partially
No
No 10
Yes
No
Yes 10
Yes
No
No 30
No
Yes
No 10
No
No
No 10
Yes
No 3
Table 1. A comparison among different methods regarding whether using 3D attack types, commercial face recognition models, commercial defenses, and the number of physical evaluation. Partially indicates that this method involved some geometric transformations to make 2D patch approximately approach the realistic 3D patch. sive mechanism [14, 42, 43, 46, 48] on face recognition, i.e., face anti-spoofing, has achieved impressive performance among the academic and industry communities. Some pop-ular defenses [18, 34, 49] have injected more sensors (such as depth, multi-spectral and infrared cameras) to provide more effective defenses. However, most of the physical at-tacks have not evaluated the passing rates against practi-cal defensive mechanisms, as reported in Table. 1. Second, these methods cannot perform satisfactorily for imperson-ation attacks against diverse commercial black-box recog-nition models due to the limited black-box transferability.
The goal of this work is to develop practical and effective physical adversarial attacks that can simultaneously deceive black-box recognition models and evade defensive mecha-nisms in commercial face recognition systems, e.g., unlock-ing mobile phones, as demonstrated in Fig. 1.
Evading the defensive mechanisms. Recent research has found that high-fidelity 3D masks [19, 21] can better fool the prevailing face anti-spoofing methods by 3D print-ing techniques. It becomes an appealing and feasible way to apply a 3D adversarial mask for evading defensive mecha-nisms in face recognition systems. To achieve this goal, we first design adversarial textured 3D meshes (AT3D) with an elaborate topology on a human face, which can be us-able by standard graphics software such as Blender [9] and Maya [22]. As a primary 3D representation, textured meshes can be immediately 3D-printed and pasted on real faces for physical adversarial attacks, which have geometric details, complex topology and high-quality textures. Exper-imentally, AT3D can be more conducive to steadily passing commercial face anti-spoofing services, such as FaceID and
Tencent anti-spoofing APIs, two mobile phones and two ac-cess control systems with multiple sensors.
Misleading the black-box recognition models. The typical 3D mesh attacks [23, 36, 47] proposed to optimize adversarial examples in mesh representation space. Thus, high complexity is virtually inevitable for calculating gradi-ents in such high-dimensional search space due to the thou-sands of triangle faces on each human face. The procedures are also costly and probably trapped into overfitting [20] with unsatisfactory transferability. Therefore, we aim to perform the optimization trajectory in a low-dimensional manifold as a regularization aiming for escaping from over-fitting. The low-dimensional manifold should possess a sufficient capacity that encodes any 3D face in this low-dimensional feature space, thus successfully achieving the white-box adversarial attack against a substitute model. A principled way of spanning such a subspace is considered by leveraging 3D Morphable Model (3DMM) [31] that ef-fectively achieves dimensionality reduction of any high-dimensional mesh data. Based on this, we are capable of generating an adversarial mesh by perturbing the low-dimensional coefficients of 3DMM, making it constrained on the data manifold of realistic 3D faces. Therefore, the crafted mesh can obtain a strong semantic feature of a 3D face, which can achieve well-generalizing performance among the white-box and black-box models due to knowl-edgable semantic pattern characteristics [37, 38, 44]. In ad-dition, low-dimensional optimization can also avoid self-intersection and flying vertices problems in mesh-based op-timization [47], resulting in better visual appearance.
Experimentally, we have effectively explored the secu-rity vulnerabilities of multiple popular commercial services, including 1) recognition APIs—Amazon, Face++, and Ten-cent; 2) anti-spoofing APIs—FaceID, SenseID, Tencent, and Aliyun; 3) two prevailing mobile phones and two auto-mated access control systems that incorporate multiple sen-sors. Our main contributions can be summarized as:
• We propose effective and practical adversarial textured 3D meshes with elaborate topology and effective opti-mization, which can simultaneously evade black-box recognition models and defensive mechanisms.
• Extensive physical experiments demonstrate that our method can consistently mislead multiple commer-cial systems, including unlocking prevailing mobile phones and automated access control systems.
• We present a reliable technique to evaluate the robust-ness of face recognition systems, which can be further leveraged as an effective data augmentation strategy to improve defensive ability. 2.