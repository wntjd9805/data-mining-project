Abstract
The success of deep learning is largely attributed to the training over clean data. However, data is often coupled with noisy labels in practice. Learning with noisy labels is challenging because the performance of the deep neural networks (DNN) drastically degenerates, due to confirma-tion bias caused by the network memorization over noisy labels. To alleviate that, a recent prominent direction is on sample selection, which retrieves clean data samples from noisy samples, so as to enhance the model’s robust-ness and tolerance to noisy labels.
In this paper, we re-vamp the sample selection from the perspective of optimal transport theory and propose a novel method, called the
OT-Filter. The OT-Filter provides geometrically meaning-ful distances and preserves distribution patterns to measure the data discrepancy, thus alleviating the confirmation bias.
Extensive experiments on benchmarks, such as Clothing1M and ANIMAL-10N, show that the performance of the OT-Filter outperforms its counterparts. Meanwhile, results on benchmarks with synthetic labels, such as CIFAR-10/100, show the superiority of the OT-Filter in handling data la-bels of high noise. 1.

Introduction
Deep learning has achieved great success on a flurry of emerging applications, such as [28, 29, 32, 49].
It is be-lieved that the phenomenal achievement of deep learning is largely attributed to accurate labels. However, the in-accuracy or imprecision of labels is inherent in real-world datasets, the so-called noisy label challenge. One way to alleviate that is to collect labels from internet queries over data-level tags, but the performance of deep neural networks (DNN) suffers drastically from the inaccuracy of such la-bels [27,32]. A higher quality of data labels can be obtained by employing human workers, but the seemingly “ground truth annotations” inevitably involve human biases or mis-†Equal Contribution. takes [45,66]. More, the human annotation is expensive and time-consuming, especially for large-scale datasets.
There have been many works on handling noisy labels, such as regularization [23] and transition matrix [13, 43].
The regularization approach leverages the regularization bias to overcome the label noise issue. But the regular-ization bias is permanent [62], thus overfitting models to noisy labels. The transition matrix approach assumes that the transition probabilities between clean and noisy labels are fixed and independent of data samples. However, a quality label transition matrix is hard to be estimated, es-pecially when the number of classes is big, making it fall short in handling noisy real-world datasets, such as [60] and [36]. A recent prominent direction is on adopting sam-ple selection [27, 38, 58] for enhancing the label quality, by selecting clean samples from the noisy training dataset.
In general, existing literatures in sample selection can be grouped to two categories, co-training networking [27, 62] and criterion-based filtering [31, 34, 57, 58].
The former utilizes the memorization of DNNs and multiple networks (e.g., co-teaching [27] and its variants
[38, 62]) to filter label noise with small loss trick, so that a small set of clean samples are used as training examples.
Letting alone the high training overhead of multiple net-works, the disadvantages are two-fold: 1) it may require the a-prior knowledge of noisy rates to select the specified proportion of small loss samples as clean samples; 2) the small-loss trick is not tolerant to the error accumulation of network training once a clean sample is falsely recognized, the so-called confirmation bias, especially for labels with high noise where clean and noisy samples largely overlap.
The latter alleviates the problem by setting a specific cri-terion. Mostly, existing works [31] [58] adopt Euclidean distances for measuring the similarity between data sam-ples. Distance-based filtering iteratively explores the neigh-borhood of the feature representation and infers/cleans sam-ple labels by aggregating the information from their neigh-borhoods. Despite the simplicity, distance-based filtering is insufficient to address noisy labels, especially when the label noise is high, e.g., overlapped label classes.
In this paper, we revamp the sample selection from the perspective of optimal transport [56] and propose a novel filtering method, called the OT-Filter.
In light of the op-timal transport, we construct discrete probability measures over the sample features, which lifts the Euclidean space of feature vectors to a probability space. It thus enables a ge-ometric way of measuring the discrepancy between proba-bility measures, representing the corresponding sample fea-tures. In addition to the distance-based metric in the Eu-clidean space [31, 58], the OT-Filter also captures the distri-bution information in the probability space so as to alleviate the confirmation bias. Accordingly, a clean representation can be obtained for each class in the probability space. By optimizing the transport plan from a sample to a clean rep-resentation, one can better determine if a sample is clean or noisy, thus improving the quality of sample selection.
In general, the merits of the OT-Filter can be summa-rized as follows. First, it does not require any a-prior knowl-edge about the noisy rate of a dataset. Second, it utilizes the optimal transport which provides geometrically meaningful distances to exploit the sample discrepancy yet preserving the distribution patterns in corresponding probability space, making the sample selection of high quality and theoretical support. Third, it can be plugged to existing robust train-ing paradigms, e.g., supervised and semi-supervised robust training.
We conduct extensive experiments with a series of syn-thetic and real datasets to gain insights into our propos-als. The result shows that the OT-Filter follows state-of-the-art (SOTA) [38] when the noise rate is low, and dom-inates SOTA when the noise rate is high. For instance, the OT-Filter achieves about 14% and 12% higher accu-racy than SOTA, in the presence of 90% noise rate, on synthetic datasets CIFAR-10 and CIFAR-100, respectively.
Moreover, the OT-Filter outperforms the competitors on real datasets Clothing1M and ANIMAL-10N.
The rest of the paper is organized as follows. We re-view the existing literature in Section 2. In Section 3, we present preliminaries of optimal transport. We investigate our proposed OT-Filter in Section 4. Furthermore, we con-duct extensive empirical studies in Section 5 and conclude the paper in Section 6. 2.