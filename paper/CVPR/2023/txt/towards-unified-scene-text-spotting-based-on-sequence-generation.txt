Abstract
Sequence generation models have recently made signif-icant progress in unifying various vision tasks. Although some auto-regressive models have demonstrated promising results in end-to-end text spotting, they use specific detec-tion formats while ignoring various text shapes and are limited in the maximum number of text instances that can be detected. To overcome these limitations, we propose a UNIfied scene Text Spotter, called UNITS. Our model unifies various detection formats, including quadrilater-als and polygons, allowing it to detect text in arbitrary shapes. Additionally, we apply starting-point prompting to enable the model to extract texts from an arbitrary start-ing point, thereby extracting more texts beyond the num-ber of instances it was trained on. Experimental results demonstrate that our method achieves competitive perfor-mance compared to state-of-the-art methods. Further anal-ysis shows that UNITS can extract a larger number of texts than it was trained on. We provide the code for our method at https://github.com/clovaai/units. 1.

Introduction
End-to-end scene text spotting, which can jointly de-tect and recognize text from an image at once, has recently gained significant attention. This work has practical appli-cations in various fields such as visual navigation, visual question answering, and document image understanding.
In this paper, we formulate scene text spotting as a se-quence generation task. This approach casts the vision task as a language modeling task conditioned on the image and text prompt [6, 7, 27, 38]. By formulating the output as a se-quence of discrete tokens, the vision task can be performed by generating a sequence through an auto-regressive trans-former decoder. Recent studies have attempted to integrate various tasks into an auto-regressive model, including text spotting. SPTS [28] treated all detection formats as the sin-*Corresponding author.
†This work was done while the authors were at Naver Cloud. (a) Single central point format. (b) Bounding box format. (c) Quadrilateral format. (d) Polygonal format.
Figure 1. Various types of detection formats. The green line rep-resents the boundary shape of the detection format, and the red dot represents the points used for the corresponding format. gle central point and predicted the coordinate tokens of the central point and word tokens auto-regressively. However, there are several challenges associated with applying the se-quence generation approach directly to scene text spotting.
As illustrated in Fig. 1, there are various methods to indi-cate the location and boundaries of text instances, and each method has its own trade-offs in terms of annotation costs and potential applicability. For instance, a single central point or bounding box annotation has a relatively low anno-tation cost and is appropriate when the detailed shape infor-mation is not necessary. However, in many fields where text spotting is applied, handling text location information as only a single point might be insufficient. As shown in Fig. 2, in the scene text editing [17, 32], which converts the text in the image to desired text while preserving the text style, de-tailed text shape extraction is required. Therefore, for such
an expert accordingly. Each block of the multi-way trans-former consists of shared attention modules and two task experts: detection and recognition experts.
Experimental results demonstrate that our proposed method achieves competitive performance on text spotting benchmarks while extracting texts in various detection for-mats using a single unified model.
Our main contributions are:
• We propose a novel sequence generation-based scene text spotting method that can extract arbitrary-shaped text areas by unifying various detection formats.
• Our model can extract more texts than the decoder length allows using the starting-point prompt, which generalizes the model to spot more texts than it has been trained on.
• Experimental results demonstrate that our method achieves competitive performance on text spotting benchmarks and provides additional functionalities 2.