Abstract
Self-supervised learning can extract representations of good quality from solely unlabeled data, which is ap-pealing for point cloud videos due to their high labelling cost.
In this paper, we propose a contrastive mask pre-diction (PointCMP) framework for self-supervised learn-ing on point cloud videos. Specifically, our PointCMP em-ploys a two-branch structure to achieve simultaneous learn-ing of both local and global spatio-temporal information.
On top of this two-branch structure, a mutual similarity based augmentation module is developed to synthesize hard samples at the feature level. By masking dominant tokens and erasing principal channels, we generate hard samples to facilitate learning representations with better discrimi-nation and generalization performance. Extensive experi-ments show that our PointCMP achieves the state-of-the-art performance on benchmark datasets and outperforms exist-ing full-supervised counterparts. Transfer learning results demonstrate the superiority of the learned representations across different datasets and tasks. 1.

Introduction
Recently, LiDARs have become increasingly popular in numerous real-world applications to perceive 3D environ-ments, such as autonomous vehicles and robots. Point clouds acquired by LiDARs can provide rich geometric in-formation and facilitate the machine to achieve 3D percep-tion. Early works focus on parsing the real world from static point clouds [9,24,64], while recent researches pay more at-tention to understanding point cloud videos [14, 16, 54, 55].
Since annotating point clouds is highly time and labor consuming [1, 57], learning from point cloud videos in a self-supervised manner draws increasing interest. Al-though contrastive learning and mask prediction paradigms
*These authors contributed equally.
†Corresponding author.
Figure 1. paradigm, (b) mask prediction paradigm, and (c) our method.
A comparison between (a) contrastive learning
[6, 19, 21, 22, 58, 63] have shown the effectiveness of self-supervised learning on images or static point clouds, these methods cannot be directly extended to point cloud videos due to the following three challenges: (i) Multiple-Granularity Information Matters. The contrastive learning paradigm [3, 4, 6, 19, 22, 63] usually fo-cuses on extracting global semantic information based on
In contrast, the mask pre-instance-level augmentations. diction paradigm [2, 12, 21, 23, 58] pays more attention to modeling local structures while ignoring global semantics.
However, since fine-grained understanding of point cloud videos requires not only local spatio-temporal features but also global dynamics [16,55], existing paradigms cannot be directly adopted. (ii) Sample Generation.
The contrastive learning paradigm is conducted by pulling positive samples while pushing negative ones [3, 6, 8, 19, 22, 51, 63], and the mask prediction paradigm learns representations by modeling the visible parts to infer the masked ones [2, 21, 39, 49, 58, 62].
Both paradigms rely heavily on the augmented samples at the input level. Further, as demonstrated in several works
[18, 25, 26, 42], self-supervised learning can significantly benefit from proper hard samples. However, the spatial disorder, temporal misalignment, and uneven information density distribution impose huge challenges on hard sample generation for point cloud videos at the input level. (iii) Leakage of Location Information. The mask pre-diction paradigm usually learns to reconstruct masked raw signals by modeling visible ones [2, 21, 39, 49, 58, 62]. For images, the contents are decoupled from the spatial position such that positional encoding is provided as cues to predict masked regions. However, for point clouds with only xyz-coordinates, positional encoding may be used as shortcuts to infer the masked points without capturing geometric in-formation [30, 39].
In this paper, we propose a contrastive mask predic-tion framework for self-supervised learning on point cloud videos, termed as PointCMP. To address challenge (i), our
PointCMP integrates the learning of both local and global spatio-temporal features into a unified two-branch structure, and simultaneously conducts self-supervised learning at dif-ferent granularities (Fig. 1(c)). For challenge (ii), we in-troduce a mutual similarity based augmentation module to generate hard masked samples and negative samples at the feature level. To handle challenge (iii), instead of directly regressing the coordinates of masked points, token-level contrastive learning is conducted between the predicted to-kens and their target embeddings to mitigate information leakage.
Our contributions are summarized as follows:
• We develop a unified self-supervised learning frame-work for point cloud videos, namely PointCMP.
Our PointCMP integrates the learning of multiple-granularity spatio-temporal features into a unified framework using parallel local and global branches.
• We propose a mutual similarity based augmentation module to generate hard masked samples and nega-tive samples by masking dominant tokens and princi-pal channels. These feature-level augmented samples facilitate better exploitation of local and global infor-mation in a point cloud video.
• Extensive experiments and ablation studies on several benchmark datasets demonstrate the efficacy of our
PointCMP on point cloud video understanding. 2.