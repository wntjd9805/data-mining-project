Abstract
In this paper, we focus on analyzing and improving the dropout technique for self-attention layers of Vision Trans-former, which is important while surprisingly ignored by prior works. In particular, we conduct researches on three core questions: First, what to drop in self-attention layers?
Different from dropping attention weights in literature, we propose to move dropout operations forward ahead of atten-tion matrix calculation and set the Key as the dropout unit, yielding a novel dropout-before-softmax scheme. We theo-retically verify that this scheme helps keep both regulariza-tion and probability features of attention weights, alleviat-ing the overfittings problem to specific patterns and enhanc-ing the model to globally capture vital information; Second, how to schedule the drop ratio in consecutive layers? In contrast to exploit a constant drop ratio for all layers, we present a new decreasing schedule that gradually decreases the drop ratio along the stack of self-attention layers. We experimentally validate the proposed schedule can avoid overfittings in low-level features and missing in high-level semantics, thus improving the robustness and stableness of model training; Third, whether need to perform struc-tured dropout operation as CNN? We attempt patch-based block-version of dropout operation and find that this use-ful trick for CNN is not essential for ViT. Given exploration on the above three questions, we present the novel Drop-Key method that regards Key as the drop unit and exploits decreasing schedule for drop ratio, improving ViTs in a gen-eral way. Comprehensive experiments demonstrate the ef-fectiveness of DropKey for various ViT architectures, e.g.
T2T, VOLO, CeiT and DeiT, as well as for various vision
*Equal contribution
â€ Corresponding author tasks, e.g., image classification, object detection, human-object interaction detection and human body shape recov-ery. 1.

Introduction
Vision Transformer (ViT) [6] has achieved great suc-cess for various vision tasks, e.g., image recognition [7, 12, 20, 34, 35], object detection [1], human body shape esti-mation [18], etc. Prior works mainly focus on researches of patch division, architecture design and task extension. the dropout technique for self-attention layer,
However, which plays the essential role to achieve good generaliz-ability, is surprisingly ignored by the community.
Different from the counterpart for Convolutional Neural
Networks (CNNs), the dropout in ViT directly utilizes the one in original Transformer designed for Natural Language
Processing, which sets attention weights as the manipula-tion unit with a constant dropout ratio for all layers. Despite of its simplicity, this vanilla design faces three major prob-lems. First, it breaks the probability distribution of atten-tion weights due to the averaging operation on non-dropout units after softmax normalization. Although this regularizes the attention weights, it still overfits specific patterns locally due to the failure on penalizing score peaks, as shown in
Fig. 1 (a) and (b); Second, the vanilla design is sensitive to the constant dropout ratio, since high ratio occurs missing of semantic information in high-level representations while low ratios overfitting in low-level features, resulting in the unstable training process; Third, it ignores the structured characteristic of input patch grid to ViT, which plays an ef-fective role to improve performance with blockwise dropout in CNNs. These three problems degrade the performance and limit the generalizability of ViTs.
Motivated by this, we propose to analyze and improve
the dropout technique in self-attention layer, further push-ing forward the frontier of ViTs for vision tasks in a general way. Specifically, we focus on three core aspects:
What to drop in self-attention layer Different from drop-ping attention weights as in the vanilla design, we propose to set the Key as the dropout unit, which is essential input of self-attention layer and significantly affects the output.
This moves the dropout operation forward before calculat-ing the attention matrix as shown in Fig. 1 (c) and yields a novel dropout-before-softmax scheme. This scheme reg-ularizes attention weights and keeps their probability dis-tribution at the same time, which intuitively helps penalize weight peaks and lift weight foots. We theoretically verify this property via implicitly introducing an adaptive smooth-ing coefficient for the attention operator from the perspec-tive of gradient optimization by formulating a Lagrange function. With the dropout-before-softmax scheme, self-attention layers can capture vital information in a global manner, thus overcoming the overfittings problem to spe-cific patterns occurred in the vanilla dropout and enhancing the model generalizability as visualization of feature map in
Fig. 1 (c). For the training phase, this scheme can be simply implemented by swapping the operation order of softmax and dropout in vanilla design, which provides a general way to effectively enhance ViTs. For inference phase, we con-duct an additional finetune phase to align the expectations to training phase, further improving the performance.
How to schedule the drop ratio In contrast to exploiting a constant drop ratio for all layers, we present a new linear decreasing schedule that gradually decreases the drop ratio along the stack of self-attention layers. This schedule leads to a high drop ratio in shallow layers while the low one in deep layers, thus avoiding overfittings to low-level features and preserving sufficient high-level semantics. We experi-mentally verify the effectiveness of the proposed decreasing schedule for drop ratio to stable the training phase and im-prove the robustness.
Whether need to perform structured drop Inspired by the DropBlock [10] method for CNNs, we implement two structured versions of the dropout operation for ViTs: the block-version dropout that drops keys corresponding to contiguous patches in images or feature maps; the cross-version dropout that drops keys corresponding to patches in horizontal and vertical stripes. We conduct thorough ex-periments to validate their efficacy and find that the struc-ture trick useful for CNN is not essential for ViT, due to the powerful capability of ViT to grasp contextual information in full image range.
Given exploration on the above three aspects, we present a novel DropKey method that utilizes Key as the drop unit and decreasing schedule for drop ratio. In particular, Drop-Key overcomes drawbacks of the vanilla dropout technique for ViTs, improving performance in a general and effective way. Comprehensive experiments on different ViT archi-tectures and vision tasks demonstrate the efficacy of Drop-Key. Our contributions are in three folds: First, to our best knowledge, we are the first to theoretically and experimen-tally analyze dropout technique for self-attention layers in
ViT from three core aspects: drop unit, drop schedule and structured necessity; Second, according to our analysis, we present a novel DropKey method to effectively improve the dropout technique in ViT. Third, with DropKey, we improve multiple ViT architectures to achieve new SOTAs on vari-ous vision tasks. 2.