Abstract
Contrastive learning-based vision-language pre-training approaches, such as CLIP, have demonstrated great success in many vision-language tasks. These meth-ods achieve cross-modal alignment by encoding a matched image-text pair with similar feature embeddings, which are generated by aggregating information from visual patches and language tokens. However, direct aligning cross-modal information using such representations is challenging, as visual patches and text tokens differ in semantic levels and granularities. To alleviate this issue, we propose a Finite
Discrete Tokens (FDT) based multimodal representation.
FDT is a set of learnable tokens representing certain visual-semantic concepts. Both images and texts are embedded using shared FDT by first grounding multimodal inputs to FDT space and then aggregating the activated FDT representations. The matched visual and semantic concepts are enforced to be represented by the same set of discrete tokens by a sparse activation constraint. As a result, the granularity gap between the two modalities is reduced.
Through both quantitative and qualitative analyses, we demonstrate that using FDT representations in CLIP-style models improves cross-modal alignment and performance in visual recognition and vision-language downstream tasks. Furthermore, we show that our method can learn more comprehensive representations, and the learned FDT capture meaningful cross-modal correspondence, ranging from objects to actions and attributes.1 1.

Introduction
Recently, the Contrastive Language-Image Pre-training (CLIP) framework [16, 27] has demonstrated notable capa-*This work was done during a research internship at ByteDance.
†Dimitris N. Metaxas has been supported by NSF IUCRC CARTA-1747778, 2235405, 2212301, 1951890, 2003874. 1The source code can be found at https : / / github . com / yuxiaochen1103/FDT.
Figure 1. Comparison of different feature representation learning methods. Left: contrastive vision-language pre-training (CLIP).
Right: CLIP with our proposed finite discrete tokens (FDT). bilities for learning powerful and transferable feature rep-resentations [10, 22, 40–43]. In this framework, models are trained to align text and image information in a two-stream approach where image and text representations are extracted through two separate encoders. The InfoNCE loss [27] is used to train the encoders which enforces the representations of matched image-text pairs to be closer, while those of unmatched pairs to be far apart (as shown in Figure 1 (Left)).
However, the fact that the information conveyed in im-ages and text captions is naturally of different levels of gran-ularities [29, 34] is not considered by such models. For ex-ample, an image of a dog also portrays various lower-level attributes, such as its breed, fur color, body size, and shape, while the textual description, such as “a smiling dog”, is generally more abstract and compact. In CLIP, images and text captions are represented through the aggregation of vi-sual patches and text tokens without explicitly aligning the visual and semantic concepts at the same level of granular-ity.
It can cause challenges in multimodal representation learning, or even potentially result in performance degrada-tion [35]. Additionally, the learned models may overlook certain semantic concepts [14]. Therefore, we argue that
unifying the information granularities of images and texts can help generate better multimodal representations.
In this paper, we propose a new Finite Discrete Tokens (FDT) based representations. FDT is a set of learnable to-kens that encode cross-modal shared semantic concepts.
Both image and text are represented as the combinations of FDT shared between modalities so that the information granularities are unified (see Figure 1 (Right)). Figure 2 gives an overview of our method. For an image, its patch embeddings are first extracted by an image encoder. The correspondence between the FDT and the image is then measured by max pooling over the attention weights of FDT among all patches. Finally, the FDT-based representation of the image is calculated as the attention-weighted sum of FDT. The FDT-based embeddings for input texts can be constructed in the same way. The encoders and FDT are trained to pull close the FDT-based representations of matched image-text pairs while pushing away those of un-matched pairs by using the InfoNCE loss. To the point of leveraging a shared FDT across modalities is to enforce the matched visual and semantic concepts to be represented by the same discrete tokens. For example, the visual patches of a dog and the word “dog” should activate the same sub-sets of FDT. We empirically demonstrate that this can be achieved by simply enforcing relatively sparse attention-weights between FDT and the inputs.
We conduct extensive experiments covering a wide range of pre-training settings and downstream tasks to evaluate the proposed method. We conclude with the following key observations: (1) Our approach exhibits consistent perfor-mance enhancements across various pre-training dataset scales, CLIP-based pre-training frameworks [20], and en-coder architectures. Notably, our method outperforms CLIP by 5.0% on zero-shot image classification when pre-training on 145M datasets, and by 33.4% in image-text retrieval with 30M datasets; (2) Our method tends to alleviate the model degradation problem and learns more comprehensive fea-ture representations than CLIP; (3) The learned FDT ex-hibit better: we visualize FDT’s correspondent patches and language tokens, and the results show that FDT success-fully capture and align visual-semantic concepts including objects, attributes, and actions. 2.