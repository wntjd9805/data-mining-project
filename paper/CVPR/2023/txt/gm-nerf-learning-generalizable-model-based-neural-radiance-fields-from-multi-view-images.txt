Abstract
In this work, we focus on synthesizing high-fidelity novel view images for arbitrary human performers, given a set of sparse multi-view images. It is a challenging task due to the large variation among articulated body poses and heavy self-occlusions. To alleviate this, we introduce an effec-tive generalizable framework Generalizable Model-based
Neural Radiance Fields (GM-NeRF) to synthesize free-viewpoint images. Specifically, we propose a geometry-guided attention mechanism to register the appearance code from multi-view 2D images to a geometry proxy which can alleviate the misalignment between inaccurate geome-try prior and pixel space. On top of that, we further conduct neural rendering and partial gradient backpropagation for efficient perceptual supervision and improvement of the per-ceptual quality of synthesis. To evaluate our method, we conduct experiments on synthesized datasets THuman2.0 and Multi-garment, and real-world datasets Genebody and
ZJUMocap. The results demonstrate that our approach out-performs state-of-the-art methods in terms of novel view synthesis and geometric reconstruction. 1.

Introduction 3D digital human reconstruction has a wide range of ap-plications in movie production, telepresence, 3D immersive communication, and AR/VR games. Traditional digital hu-man production relies on dense camera arrays [10, 14] or depth sensors [12, 20] followed by complex graphics ren-dering pipelines for high-quality 3D reconstruction, which limits the availability to the general public.
Reconstructing 3D humans from 2D images captured by sparse RGB cameras is very attractive due to its low cost and convenience. This field has been studied for decades [21, 46, 50]. However, reconstruction from sparse
RGB cameras is still quite challenging because of: 1) heavy self-occlusions of the articulated human body; 2) inconsis-tent lighting and sensor parameters between different cam-*Equal contribution. †Corresponding authors. Codes are available at https://github.com/JanaldoChen/GM-NeRF
Figure 1. The effect of inaccurately estimated SMPL. Com-pared with GNR [8] and KeypointNeRF [26], our method still yields a reasonable result. eras; 3) highly non-rigid and diverse clothes.
In recent years, with the rise of learning-based methods, we can reconstruct high-quality digital humans from sparse cameras. Learning-based methods [32, 36, 43, 49, 52] have made great processes, however, they lack multi-view geo-metric consistency due to the mere usage of a 2D neural rendering network. To address this problem, many recent works [5, 47, 54] adopt neural radiance fields as 3D rep-resentations, which achieves outstanding performance on novel view synthesis. However, these methods are not ro-bust to unseen poses without the guidance of human geo-metric prior.
To better generalize to unseen poses, NeuralBody [31] introduces a statistical body model SMPL [23] into neural radiance fields which can reconstruct vivid digital humans from a sparse multi-view video. However, NeuralBody is designed for identity-specific scenarios, which means it re-quires laborious data collection and long training to obtain the model for one person. Such a limitation restricts its ap-plication in general real-world scenarios.
In this work, we focus on synthesizing high-fidelity novel view images for arbitrary human performers from a set of sparse multi-view images. Towards this goal, some very recent works [7, 8, 19, 26] propose to aggregate multi-view pixel-aligned features using SMPL as a geometric prior. However, these methods usually assume perfect ge-ometry (e.g. accurate SMPL [23] estimation from 2D im-ages) which is not applicable in practical applications. In
practice, the geometry error does affect the reconstruction performance significantly. As illustrated in the red box of
Fig. 1, when the estimated SMPL does not align well with
RGB image, prior SMPL-dependent methods [8, 26] yield blurry and distorted results. The such performance gap is caused by the misalignment between the 3d geometry (i.e.
SMPL) and the pixel space (i.e. pixel-aligned feature and ground-truth image). Specifically, the misalignment will cause: 1) blur and distortion when fusing the geometry and pixel-aligned features; 2) unsuitable supervision during training with a pixel-wise loss like L1 or L2. To alleviate the issue of misalignment, we propose to take the geome-try code as a proxy and then register the appearance code onto the geometry through a novel geometry-guided atten-tion mechanism. Furthermore, we leverage perceptual loss to reduce the influence of misalignment and promote sharp image synthesis, which is evaluated at a higher level with a larger perceptual field. It is non-trivial to apply perceptual loss in NeRF-based methods as the perceptual loss requires a large patch size as input which is memory-consuming through volume rendering. We introduce 2D neural ren-dering and partial gradient backpropagation to alleviate the memory requirement and enhance the perceptual quality.
To summarize, our work contributes as follows:
• A novel generalizable model-based framework GM-NeRF is proposed for the free-viewpoint synthesis of arbi-trary performers.
• To alleviate the misalignment between 3D geometry and the pixel space, we propose geometry-guided attention to aggregate multi-view appearance and geometry proxy.
• To enable perceptual loss supervision to further allevi-ate misalignment issues, we adopt several efficient designs including 2D neural rendering and partial gradient back-propagation. 2.