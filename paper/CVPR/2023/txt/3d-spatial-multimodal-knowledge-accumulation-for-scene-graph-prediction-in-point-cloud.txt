Abstract
In-depth understanding of a 3D scene not only involves locating/recognizing individual objects, but also requires to infer the relationships and interactions among them. How-ever, since 3D scenes contain partially scanned objects with physical connections, dense placement, changing sizes, and a wide variety of challenging relationships, existing methods perform quite poorly with limited training sam-ples. In this work, we find that the inherently hierarchical structures of physical space in 3D scenes aid in the au-tomatic association of semantic and spatial arrangements, specifying clear patterns and leading to less ambiguous predictions. Thus, they well meet the challenges due to the rich variations within scene categories. To achieve this, we explicitly unify these structural cues of 3D phys-ical spaces into deep neural networks to facilitate scene graph prediction. Specifically, we exploit an external knowl-edge base as a baseline to accumulate both contextual-ized visual content and textual facts to form a 3D spa-tial multimodal knowledge graph. Moreover, we propose a knowledge-enabled scene graph prediction module bene-fiting from the 3D spatial knowledge to effectively regular-ize semantic space of relationships. Extensive experiments demonstrate the superiority of the proposed method over current state-of-the-art competitors. Our code is available at https://github.com/HHrEtvP/SMKA. 1.

Introduction
In recent years, much success has been achieved on 3D point cloud scene understanding such as semantic seg-mentation [9, 11, 15, 16, 21, 28, 29, 49] and object detec-tion [10, 22, 25, 27, 43]. However, the 3D world is not only defined by objects but also by the relationships between ob-jects. A 3D scene graph can abstract the environment as a graph where nodes represent objects and edges character-ize the relationships between object pairs, which has already been recognized in recent seminal works [1,30,37,38,41,46].
However, relationship graphs predicted by current methods are far from satisfactory due to the noisy, cluttered and par-*Equal contribution
†Corresponding author
Figure 1. A brief overview of our method. tial nature of real 3D scans. Moreover, these data-driven methods treat sophisticated relationships in 3D space in-dependently for classification using the geometric features proximity or fit, and are ignorant of commonsense or other useful 3D spatial cues beyond visual information. 3D objects in real scenes commonly have strongly structured regulari-ties [33,39], whose semantic and spatial arrangements follow clear patterns, but still exhibit rich structural variations even within the scene category.
The key observation is that 3D scene structures are in-herently hierarchical [20]. By definition, an instance can have multiple supports, lamps are standing on a table, chairs are supported by the floor and only the floor does not have any support, and it is unlikely that a pillow is supporting a couch. Although relationships themselves cast no light on the human eyes, a growing body of works [14, 31] suggest that even very complex relationship information is reasoned hierarchically and systemically according to the role of the prefrontal cortex. Relationships, such as support, can be extracted rapidly, are hard to ignore, and influence other relationships in the perceptual process. For example, a TV and a sofa are related since they together serve the function of ‘watching TV’, but these two objects can be far apart in a scene. Relationships of this kind are much more difficult, if not possible, to infer based on geometric analysis alone. The model can relate the table easily which supports the TV and use the table as a bridge to predict the ‘front’ relationship with sofa, where table and sofa are all supported by the floor and relationships within them is intuitive.
The underlying hierarchical structures in 3D scenes are label free and reliable, and can hence play an essential role in scene understanding at no additional cost. Existing 3D scene graph prediction models [1, 30, 37, 38, 41, 46] are oblivious to the underlying structures in the point cloud scenes. The question is how to take this prior knowledge into consid-eration to make the 3D scene graph achieve higher accu-racy? KISG [47] proposes a graph auto-encoder to learn a closed set and ground truth prior knowledge from relation-ship triplets in data for 3D scene graph prediction. Although
KISG [47] takes note of knowledge, it captures relevant prior knowledge from text-only ground truth labels, which merely contain facts expressed by label descriptions while lacking complex but indispensable multimodal knowledge for 3D scene graph prediction. In addition, noises contained in the manually annotated labels are easily included in the knowledge base and affects the prediction of relationships.
To address the above problems, we show that the implicit hierarchical structure correlations between object pairs and their relationships can be explicitly represented by a knowl-edge base. As shown in Fig. 1, we propose a 3D spatial mul-timodal knowledge accumulation module to explicitly merge the hierarchical structures of 3D scenes into the network to strengthen the 3D scene graph prediction process. Firstly, we filter the external commonsense knowledge base, classify the hierarchical tokens for each node, and add new support edges to form the hierarchical symbolic knowledge graph for 3D scenes. Secondly, we retrieve the hierarchical token from the reconstructed symbolic knowledge graph for object instances in 3D scenes to build a visual graph, and extract contextual features for nodes and edges using a region-aware graph network. Finally, to bridge the heterogeneous gap between the symbolic knowledge and visual information, we propose a graph reasoning network to correlate 3D spatial vi-sual contents of scenes with textual facts. Conditioned on the learned vision-relevant 3D spatial multimodal knowledge, we incorporate this network into the relationships prediction stage as extra guidance, which can effectively regularize the distribution of possible relationships of object pairs and thus make the predictions less ambiguous.
Our main contributions are: 1) We are the first to explic-itly unify the regular patterns of 3D physical spaces with the deep architecture to facilitate 3D scene graph prediction. 2)
We propose a hierarchical symbolic knowledge construction module that exploits extra knowledge as the baseline to admit the hierarchical structure cues of 3D scene. 3) We introduce a knowledge-guided visual context encoding module to con-struct hierarchical visual graph and learn the contextualized features by a region-aware graph network. 4) We propose a 3D spatial multimodal knowledge accumulation module to regularize the semantic space of relationship prediction. Re-sults show that the learned knowledge and proposed modules consistently boost 3D scene graph prediction performance. 2.