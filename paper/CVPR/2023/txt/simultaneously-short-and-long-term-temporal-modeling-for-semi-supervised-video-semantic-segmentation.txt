Abstract
In order to tackle video semantic segmentation task at a lower cost, e.g., only one frame annotated per video, lots of efforts have been devoted to investigate the utiliza-tion of those unlabeled frames by either assigning pseudo labels or performing feature enhancement.
In this work, we propose a novel feature enhancement network to si-multaneously model short- and long-term temporal corre-lation. Compared with existing work that only leverage short-term correspondence, the long-term temporal corre-lation obtained from distant frames can effectively expand the temporal perception field and provide richer contex-tual prior. More importantly, modeling adjacent and dis-tant frames together can alleviate the risk of over-fitting, hence produce high-quality feature representation for the distant unlabeled frames in training set and unseen videos in testing set. To this end, we term our method SSLTM, short for Simultaneously Short- and Long-Term Temporal
Modeling. In the setting of only one frame annotated per video, SSLTM significantly outperforms the state-of-the-art methods by 2% ∼ 3% mIoU on the challenging VSPW dataset. Furthermore, when working with a pseudo label based method such as MeanTeacher, our final model only exhibits 0.13% mIoU less than the ceiling performance (i.e., all frames are manually annotated). 1.

Introduction
Deep neural networks have been the de-facto solution for many vision tasks such as image recognition [12], ob-ject detection [15] and semantic segmentation [21]. These state-of-the-art results are generally achieved by training very deep networks on large-scale labeled datasets, e.g., Im-ageNet [27], COCO [16] and Cityscapes [4], etc. However, building such labeled datasets is labor-intensive and com-plicated. Hence, it is very appealing to explore less label-dependent alternatives that only requires a (small) portion of the dataset to be annotated [14, 24–26, 37]. (a) The model trained with distant frames demonstrates better segmentation output for distant unlabeled frames in the training set. (b) The adjacent frames do not contain sufficient visual clues to segment the car in current frame, while the distant frame can provide richer context to guide the segmentation model.
Figure 1. The importance of involving distant frames in train-ing. The exploitation of distant frames not only reduces the risk of over-fitting to the labeled frame and its adjacent ones, but also provides temporally long-term context to enhance the feature rep-resentation.
In this work, we aim to train the video semantic seg-mentation model under an extreme setting of annotation availability, i.e., each video in the training set only has its first frame annotated. The significance of this problem is twofold: 1). Video semantic segmentation is a fundamen-tal task in computer vision, with wide applications in many scenarios like autonomous driving [9], robot controlling [7]; 2). Compared with dense annotations, it takes much less (if not the least) cost to label one frame per video. More im-portantly, given the information redundancy [18, 41] within a video, it seems intuitively unnecessary to annotate ev-ery frame at all costs. Thus, it is of great practical and theoretical interests to explore the feasibility of conduct-ing video semantic segmentation with one-frame-per-video-annotation.
Existing methods for this problem can be grouped into
Pseudo Label based approaches and Feature Enhancement based ones, depending on whether explicit pseudo labels are generated for the unlabeled frames. For the former ones
[1, 6, 40], a pseudo-label generator is often trained with the annotated frames, then the model is updated using both la-beled and unlabeled data. As a comparison, the latter group of methods [18, 41] concentrates on obtaining high-quality representations based on the features from both labeled and unlabeled frames. Thus, these methods rely on feature en-hancement modules that are specially designed for temporal feature fusion. Note, Pseudo Label based approaches and
Feature Enhancement based ones are orthogonal, i.e., they pay attention to different aspects of the semi-supervised video semantic segmentation task, and can usually work to-gether to combine the best of two worlds as shown in Sec-tion 4.5. In this work, we will focus on the latter ones -designing innovative feature enhancement modules.
Prior arts on feature enhancement mostly focus on mod-eling short-term temporal correlation, under the assumption of temporal consistency [18, 41] among adjacent frames.
Nevertheless, the distant frame is less exploited in existing work, due to its severe content changes and weak temporal consistency. However, in the setting of partial annotation, the absence of distant frames in training results in signifi-cant drawbacks: 1). As illustrated in Figure 1a, if the distant frame is not involved in the training phase, the model will be over adapted (or even over-fitted) to the labeled frame and its adjacent ones. Consequently, the generalization to dis-tant frames and unseen videos in the testing set is severely hurt, leading to poor segmentation performance in the test-ing stage. 2). Since the distant frame can provide long-term temporal context, the representation quality of the current frame can be improved by leveraging the information from its distant frame. A qualitative sample is given in Figure 1b, where a severely occluded car is correctly segmented with the help of long-term context from the distant frame.
To address the aforementioned drawbacks, we propose a novel Simultaneously Short- and Long-Term Temporal
Modeling (SSLTM) method to capture the temporal rela-tionship from both adjacent and distant frames. To achieve this goal, we design three novel components in our model for representation learning: 1). We refer to the labeled frame as query frame, for its adjacent frames in the same video, we model the short-term inter-frame correlations by a Spatial-Temporal Transformer (STT). 2). For the pur-pose of long-term temporal modeling, we obtain a refer-ence frame by randomly sampling a distant frame from the same video of the query frame, then feeding the reference frame’s feature to our proposed Reference Frame Context
Enhancement (RFCE) module, so as to enhance the rep-resentation of query frame. Meanwhile, as the reference frame is selected randomly from the entire video, our model is potentially trained with all data, rather than just the la-beled frames and their adjacent ones. As such, we expect the model to be prevented from over-fitting to some extent. 3). To compensate for the semantic category representation from RFCE, we further propose a Global Category Context (GCC) module to model the global information across the whole dataset.
In summary, our method is a pioneer work to exploit both short- and long-term inter-frame correlations in the video semantic segmentation task. Thanks to the effective model-ing of distant frames, our RFCE demonstrates outstanding performance, especially under the setting of partial annota-tion. Specifically, on the challenging VSPW dataset [22], the mIoU of our final model only decreases by 0.13% when switching from per-frame-annotation to the one-frame-per-video-annotation setting. To our knowledge, this is the first work that nearly closes the gap between dense anno-tations and one-frame-per-video ones, which is of great sig-nificance in practical applications. Compared with exist-ing Feature Enhancement based video semantic segmenta-tion methods [5,18,22,23,30,41], our SSLTM demonstrates advantageous results (mIoU as 39.79%) by a large margin (2% ∼ 3% mIoU) on the VSPW dataset. 2.