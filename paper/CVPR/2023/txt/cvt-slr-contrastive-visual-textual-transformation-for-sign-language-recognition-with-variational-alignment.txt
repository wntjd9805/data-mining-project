Abstract 1.

Introduction
Sign language recognition (SLR) is a weakly supervised task that annotates sign videos as textual glosses. Recent studies show that insufficient training caused by the lack of large-scale available sign datasets becomes the main bottleneck for SLR. Most SLR works thereby adopt pre-trained visual modules and develop two mainstream solu-tions. The multi-stream architectures extend multi-cue vi-sual features, yielding the current SOTA performances but requiring complex designs and might introduce potential noise. Alternatively, the advanced single-cue SLR frame-works using explicit cross-modal alignment between vi-sual and textual modalities are simple and effective, po-tentially competitive with the multi-cue framework.
In this work, we propose a novel contrastive visual-textual transformation for SLR, CVT-SLR, to fully explore the pre-trained knowledge of both the visual and language modali-ties. Based on the single-cue cross-modal alignment frame-work, we propose a variational autoencoder (VAE) for pre-trained contextual knowledge while introducing the com-plete pretrained language module. The VAE implicitly aligns visual and textual modalities while benefiting from pretrained contextual knowledge as the traditional contex-tual module. Meanwhile, a contrastive cross-modal align-ment algorithm is designed to explicitly enhance the consis-tency constraints. Extensive experiments on public datasets (PHOENIX-2014 and PHOENIX-2014T) demonstrate that our proposed CVT-SLR consistently outperforms existing single-cue methods and even outperforms SOTA multi-cue methods. The source codes and models are available at https://github.com/binbinjiang/CVT- SLR.
*Corresponding author. 1
As a special visual natural language, sign language is the primary communication medium of the deaf community
[19]. With the progress of deep learning [1, 17, 25, 39, 42], sign language recognition (SLR) has emerged as a multi-modal task that aims to annotate sign videos into textual sign glosses. However, a significant dilemma of SLR is the lack of publicly available sign language datasets. For example, the most commonly-used PHOENIX-2014 [23] and PHOENIX-2014T [2] datasets only include about 10K pairs of sign videos and gloss annotations, which are far from training a robust SLR system with full supervision as typical vision-language cross-modal tasks [34]. Therefore, data limitation that may easily lead to insufficient training or overfitting problems is the main bottleneck of SLR tasks.
The development of weakly supervised SLR has wit-nessed most of the improvement efforts focus on the visual module (e.g., CNN) [9, 10, 15, 29, 32, 33]. Transferring pre-trained visual networks from general domains of human ac-tions becomes a consensus to alleviate the low-resource lim-itation. The mainstream multi-stream SLR framework ex-tends the pretrained visual module with multi-cue visual in-formation [3,22,24,43,48,50], including global features and regional features such as hands and faces in independent streams. The theoretical support for this approach comes from sign language linguistics, where sign language utilizes multiple complementary channels (e.g., hand shapes, fa-cial expressions) to convey information [3]. The multi-cue mechanism essentially exploits hard attention to key infor-mation, yielding the current SOTA performances. However, the multi-cue framework is complex (e.g., cropping multi-ple regions, requiring more parameters), and the fusion of multiple streams might introduce additional potential noise.
Another mainstream advanced solution is the single-cue
Figure 1. (a) An advanced single-cue SLR framework with explicit cross-modal alignment; (b) Our proposed single-cue SLR framework with explicit cross-modal alignment and implicit autoencoder alignment. Both frameworks use pretrained visual features. But our frame-work uses the autoencoder module to replace the mainstream contextual module, which not only includes the functions of the contextual module but also can introduce complete pretrained language knowledge and implicit cross-modal alignment. To maximize the preservation of the complete pretrained language parameters and migrated visual features, a video-gloss adapter is introduced. cross-modal alignment framework [15, 28], which consists of a pretrained visual module followed by a contextual mod-ule (e.g., RNN, LSTM, Transformer) and a Connectionist
Temporal Classification (CTC) [14] based alignment mod-ule for gloss generation, as shown in Figure 1 (a). Explicit cross-modal alignment constraints further improve feature interactions [15, 28, 38], which could be treated as a kind of consistency between two different modalities [50] facilitat-ing the visual module learn long-term temporal information from contextual module [13, 37]. The cross-modal align-ment framework is simple and effective, potentially compet-itive with the multi-cue framework. Despite the advanced performance of complex multi-cue architectures with pre-trained visual modules, the cross-modal consistency is a more elegant design for practical usage. It also implies the potential of prior contextual linguistic knowledge, which has been overlooked by existing SLR works.
In this work, we propose a novel contrastive visual-textual transformation framework for SLR, called CVT-SLR, to fully explore the pretrained knowledge of both the visual and language modalities, as shown in Figure 1 (b).
Based on the single-cue cross-modal alignment framework,
CVT-SLR keeps the pretrained visual module but replaces the traditional contextual module with a variational autoen-coder (VAE). Since a full encoder-decoder architecture is used, the VAE is responsible for learning pretrained contex-tual knowledge based on a pseudo-translation task while in-troducing the complete pretrained language module. In ad-dition, the VAE maintains the consistency of input and out-put modalities due to the form of an autoencoder, playing an implicit cross-modal alignment role. Furthermore, inspired by contrastive learning [4–6,34], we introduce a contrastive alignment algorithm that focuses on both positive and neg-ative samples to enhance explicit cross-modal consistency constraints. Extensive quantitative experiments conducted on the public datasets PHOENIX-2014 and PHOENIX-2014T demonstrate the advance of the proposed CVT-SLR framework. Through ablation study and qualitative anal-ysis, we also verify the effectiveness of introducing pre-trained language knowledge and the new consistency con-straint mechanism.
Our main contributions are as follows:
• A novel visual-textual transformation-based SLR framework is proposed, which introduces fully pre-trained language knowledge for the first time and pro-vides new approaches for other cross-modal tasks.
• New alignment methods are proposed for cross-modal consistency constraints: a) exploiting the special prop-erties of the autoencoder to implicitly align visual and textual modalities; b) introducing an explicit con-trastive cross-modal alignment method.
• The proposed single-cue SLR framework not only out-performs existing single-cue baselines by a large mar-gin but even surpasses SOTA multi-cue baselines. 2.