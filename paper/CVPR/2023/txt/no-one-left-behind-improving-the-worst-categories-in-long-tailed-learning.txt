Abstract
Unlike the case when using a balanced training dataset, the per-class recall (i.e., accuracy) of neural networks trained with an imbalanced dataset are known to vary a lot from category to category. The convention in long-tailed recognition is to manually split all categories into three sub-sets and report the average accuracy within each subset.
We argue that under such an evaluation setting, some cat-egories are inevitably sacrificed. On one hand, focusing on the average accuracy on a balanced test set incurs little penalty even if some worst performing categories have zero accuracy. On the other hand, classes in the “Few” subset do not necessarily perform worse than those in the “Many” or “Medium” subsets. We therefore advocate to focus more on improving the lowest recall among all categories and the harmonic mean of all recall values. Specifically, we propose a simple plug-in method that is applicable to a wide range of methods. By simply re-training the classifier of an exist-ing pre-trained model with our proposed loss function and using an optional ensemble trick that combines the predic-tions of the two classifiers, we achieve a more uniform dis-tribution of recall values across categories, which leads to a higher harmonic mean accuracy while the (arithmetic) av-erage accuracy is still high. The effectiveness of our method is justified on widely used benchmark datasets. 1.

Introduction
Various gaps exist when adapting image recognition techniques that are developed in the lab to industrial ap-plications. The most noteworthy one is perhaps the differ-ence between training datasets. Most training datasets used in academic research [6, 16] are balanced with respect to the number of images per class. This should not be taken for granted because datasets used in real-world applications are more likely to be imbalanced. Training deep models
*J. Wu is the corresponding author. This research was partly sup-ported by the National Natural Science Foundation of China under Grant 62276123 and Grant 61921006.
Figure 1. The per-class recall of models trained on the imbal-anced CIFAR100 (with imbalance ratio 100). Per-class recall value varies a lot from category to category. Moreover, it is not necessarily true that all categories in the “Few” subset have lower accuracy than those in the “Many” or “Medium” subsets.
Method
Mean Accuracy
Lowest Recall
BSCE [22]
DiVE [12]
MiSLAS [30]
RIDE [26]
PaCo [4] 42.24 45.11 47.05 48.64 51.24 3.00 2.00 5.00 2.00 5.00
Table 1. The lowest per-class recall of various state-of-the-art methods on the imbalanced CIFAR100 (with imbalance ratio 100).
Although there are rapid improvements over the mean accuracy, the lowest per-class recall remains very low. on these datasets is not trivial as models are known to per-form poorly on such datasets. Long-tailed recognition is a research field that aims at tackling this challenge.
By plotting the per-class recall (i.e., per-class accuracy) of the model trained on the imbalanced CIFAR00 dataset (with imbalance ratio 100) in Fig. 1, we find that the recall varies dramatically from category to category. We argue that in real world applications, all categories are equally
important and no one should be left behind. We also find that despite the rapid developments in this community, no obvious improvement over the lowest per-class recall is wit-nessed in the past few years [4, 12, 22, 26, 30], as is shown in Tab. 1. The convention in long-tailed recognition re-search is to split the classes into three subsets based on the number of training images. The accuracy within each sub-set is often reported along with the overall accuracy. While this evaluation scheme seems reasonable at the first glance, we argue it is potentially problematic. First of all, comput-ing the average recall within each subset is way too coarse, making it impossible to reflect whether some classes are completely sacrificed but covered up by other “easy” tail classes. What’s more, we find it is not necessarily true that classes in the “Few” category all have lower recall than classes in the other two subsets, as is shown in Fig. 1.
Therefore, focusing on improving the mean accuracy alone is not enough, especially in real-world applications.
In this paper, we propose a novel method to make sure that no category is left behind. Specifically, we argue that although mean accuracy is widely used in image classifica-tion as an optimization objective, due to the fact that differ-ent classes have very different recall in long-tailed recogni-tion, it is not the most suitable objective as it incurs little penalty even if some categories have very small per-class accuracy values (e.g., close to 0). Hence, to improve even the worst-performing categories, we believe the harmonic mean of per-class recall would be a better objective. Since harmonic mean is very sensitive to small numbers, further improving classes that already have high recall brings little benefits to the harmonic mean, which makes sure no single class will be left behind. Also, now all classes are treated equally, forcing us to improve classes that have low recall no matter which subset it belongs to.
However, it is difficult to directly minimize the harmonic mean. We therefore propose a novel loss function that max-imizes the geometric mean instead, which can be viewed as a surrogate. Our method serves as a simple plug-in that can be used together with both baseline and various state-of-the-art methods. We also propose an ensemble trick that uses the pre-trained and fine-tuned models together to make predictions during inference with nearly no extra cost. We are able to yield a more uniform distribution of recall across categories (i.e., no one left behind), which achieves higher harmonic mean of the recall while the (arithmetic) mean ac-curacy remains high, too.
In summary, our work has the following contributions:
• We are the first to emphasize the importance of the cor-rect recognition of all categories in long-tailed recog-nition.
• We propose a novel method that aims at increasing the harmonic mean of per-class recall as well as an ensem-ble trick that combines two existing models together during inference with nearly no extra cost.
• We experimented on three widely used benchmark datasets, which justify the effectiveness of our method in terms of both overall and worst per-class accuracy. 2.