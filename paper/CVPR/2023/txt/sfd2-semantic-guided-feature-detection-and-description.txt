Abstract
Visual localization is a fundamental task for various applications including autonomous driving and robotics.
Prior methods focus on extracting large amounts of often redundant locally reliable features, resulting in limited ef-ﬁciency and accuracy, especially in large-scale environ-ments under challenging conditions. Instead, we propose to extract globally reliable features by implicitly embedding high-level semantics into both the detection and descrip-tion processes. Speciﬁcally, our semantic-aware detector is able to detect keypoints from reliable regions (e.g. build-ing, trafﬁc lane) and suppress unreliable areas (e.g. sky, car) implicitly instead of relying on explicit semantic labels.
This boosts the accuracy of keypoint matching by reducing the number of features sensitive to appearance changes and avoiding the need of additional segmentation networks at test time. Moreover, our descriptors are augmented with semantics and have stronger discriminative ability, pro-viding more inliers at test time. Particularly, experiments on long-term large-scale visual localization Aachen Day-Night and RobotCar-Seasons datasets demonstrate that our model outperforms previous local features and gives com-petitive accuracy to advanced matchers but is about 2 and 3 times faster when using 2k and 4k keypoints, respectively.
Code is available at https://github.com/feixue94/sfd2. 1.

Introduction
Visual localization is key to various applications includ-ing autonomous driving and robotics. Structure-based al-gorithms [54, 57, 64, 69, 73, 79] involving mapping and lo-calization processes still dominate in large-scale localiza-tion. Traditionally, handcrafted features (e.g. SIFT [3, 35],
ORB [53]) are widely used. However, these features are mainly based on statistics of gradients of local patches and thus are prone to appearance changes such as illumina-tion and season variations in the long-term visual localiza-tion task. With the success of CNNs, learning-based fea-tures [14, 16, 37, 45, 51, 76, 81] are introduced to replace handcrafted ones and have achieved excellent performance.
Figure 1. Overview of our framework. Our model implicitly in-corporates semantics into the detection and description processes with guidance of an off-the-shelf segmentation network during the training process. Semantic- and feature-aware guidance are adopted to enhance its ability of embedding semantic information.
With massive data for training, these methods should be able to automatically extract keypoints from more reliable regions (e.g. building, trafﬁc lane) by focusing on discrim-inative features [64]. Nevertheless, due to the lack of ex-plicit semantic signals for training, their ability of selecting globally reliable keypoints is limited, as shown in Fig. 2 (detailed analysis is provided in Sec. B.1 in the supplemen-tary material). Therefore, they prefer to extract locally re-liable features from objects including those which are not useful for long-term localization (e.g. sky, tree, car), lead-ing to limited accuracy, as demonstrated in Table 2.
Recently, advanced matchers based on sparse key-points [8, 55, 65] or dense pixels [9, 18, 19, 33, 40, 47, 67, 74] are proposed to enhance keypoint/pixel-wise matching and have obtained remarkable accuracy. Yet, they have quadratic time complexity due to the attention and corre-lation volume computation. Moreover, advanced matchers rely on spatial connections of keypoints and perform image-wise matching as opposed to fast point-wise matching, so they take much longer time than nearest neighbor matching (NN) in both mapping and localization processes because of a large number of image pairs (much larger than the num-ber of images) [8]. Alternatively, some works leverage se-mantics [64, 73, 79] to ﬁlter unstable features to eliminate wrong correspondences and report close even better accu-racy than advanced matchers [79]. However, they require additional segmentation networks to provide semantic la-bels at test time and are fragile to segmentation errors.
Instead, we implicitly incorporate semantics into a local feature model, allowing it to extract robust features auto-matically from a single network in an end-to-end fashion. In the training process, as shown in Fig. 1, we provide explicit semantics as supervision to guide the detection and descrip-tion behaviors. Speciﬁcally, in the detection process, un-like most previous methods [14, 16, 32, 37, 51] adopting ex-haustive detection, we employ a semantic-aware detection loss to encourage our detector to favor features from reli-able objects (e.g. building, trafﬁc lane) and suppress those from unreliable objects (e.g. sky). In the description pro-cess, rather than utilizing triplet loss widely used for de-scriptor learning [16, 41], we employ a semantic-aware de-scription loss consisting of two terms: inter- and intra-class losses. The inter-class loss embeds semantics into descrip-tors by enforcing features with the same label to be close and those with different labels to be far. The intra-class loss, which is a soft-ranking loss [23], operates on features in each class independently and differentiates these features from objects of the same label. Such use of soft-ranking loss avoids the conﬂict with inter-class loss and retains the diversity of features in each class (e.g. features from build-ings usually have larger diversity than those from trafﬁc lights). With semantic-aware descriptor loss, our model is capable of producing descriptors with stronger discrimina-tive ability. Beneﬁting from implicit semantic embedding, our method avoids using additional segmentation networks at test time and is less fragile to segmentation errors.
As the local feature network is much simpler than typi-cal segmentation networks e.g. UperNet [10], we also adopt an additional feature-consistency loss on the encoder to enhance its ability of learning semantic information. To avoid using costly to obtain ground-truth labels, we train our model with outputs of an off-the-shelf segmentation net-work [11, 34], which has achieved SOTA performance on the scene parsing task [83], but other semantic segmenta-tion networks (e.g. [10]) can also be used.
An overview of our system is shown in Fig. 1. We em-bed semantics implicitly into the feature detection and de-scription network via the feature-aware and semantic-aware guidance in the training process. At test time, our model produces semantic-aware features from a single network di-rectly. We summarize contributions as follows:
• We propose a novel feature network which implicitly incorporates semantics into detection and description processes at training time, enabling the model to pro-duce semantic-aware features end-to-end at test time.
• We adopt a combination of semantic-aware and feature-aware guidance strategy to make the model
Figure 2. Locally reliable features. We show top 1k key-points (reliability high→low: 1-250 , 251-500 , 501-750 , 751-1000 ) of prior including SPP [14], local
D2Net [16], R2D2 [51], and ASLFeat [37]. They indiscrimina-tively give high reliability to patches with rich textures even from objects e.g. sky, tree, pedestrian and car, which are reliable for long-term localization (best view in color). features embed semantic information more effectively.
• Our method outperforms previous local features on the long-term localization task and gives competitive ac-curacy to advanced matchers but has higher efﬁciency.
Experiments show our method achieves a better trade-off between accuracy and efﬁciency than advanced match-ers [8, 55, 65] especially on devices with limited computing resources. We organize the rest of this paper as follows. In
Sec. 2, we introduce related works. In Sec. 3, we describe our method in detail. We discuss experiments and limita-tions in Sec. 4 and Sec. 5 and conclude the paper in Sec. 6. 2.