Abstract rig is embedded into the target object for motion retargeting.
Animating a virtual character based on a real perfor-mance of an actor is a challenging task that currently re-quires expensive motion capture setups and additional effort by expert animators, rendering it accessible only to large pro-duction houses. The goal of our work is to democratize this task by developing a frugal alternative termed “Transfer4D” that uses only commodity depth sensors and further reduces animators’ effort by automating the rigging and animation transfer process. Our approach can transfer motion from an incomplete, single-view depth video to a semantically similar target mesh, unlike prior works that make a stricter assump-tion on the source to be noise-free and watertight. To handle sparse, incomplete videos from depth video inputs and varia-tions between source and target objects, we propose to use skeletons as an intermediary representation between motion capture and transfer. We propose a novel unsupervised skele-ton extraction pipeline from a single-view depth sequence that incorporates additional geometric information, result-ing in superior performance in motion reconstruction and transfer in comparison to the contemporary methods and making our approach generic. We use non-rigid reconstruc-tion to track motion from the depth sequence, and then we rig the source object using skinning decomposition. Finally, the 1.

Introduction
The growing demand for immersive animated content originates primarily from its widespread applications in en-tertainment, metaverse, education, and augmented/virtual reality to name a few. Production of animation content re-quiring modeling the geometry of the characters of interest, rigging a skeleton to determine each character’s degrees of freedom, and then animating their motion. The latter step is often performed by transferring the motion captured from a real performer, whether a human actor, an animal, or even a puppet. The intermediate rigging step is tedious and labor-intensive, while motion capture requires an expen-sive multi-camera system; both factors hinder the large-scale adoption of 3D content creation. We aim to democratize content creation by (i) replacing expensive motion capture setups with a single monocular depth sensor, and (ii) by au-tomatically generating character rigs to transfer motion to non-isometric objects.
By the term animation transfer we refer to the process of transferring the motion captured from a real performer to an articulated character modelled as a polygon mesh. Specif-ically, we work in a setting where the source is an point cloud obtained from a single-view commodity sensor such as Kinect/Realsense, and the target is a user-defined poly-gon mesh without a predefined skeletal rig. These choices make our approach more practical for deployment in future.
In this setting, performing automatic animation transfer re-quires reconstructing the source motion from the point cloud, and finding a correspondence between points on the source and target shapes, before the source performance can be remapped to the target character. Furthermore, many appli-cations often require animation content not just of humans but of other kinds of characters and objects such as animals, birds, and articulated mechanical objects, precluding the use of predefined human body models. As we discuss below, existing techniques have limited applicability under all these constraints.
Monocular motion systems generally require a template before registering the incoming video. Templates are cre-ated by 360◦ static registration of the source object [19, 52] or by creating a parametric model from a large corpus of meshes [2, 23, 28, 35, 39, 65]. Unfortunately, these meth-ods do not generalize to unknown objects. As a parallel approach, methods that rely on neural implicit representa-tions [8, 37, 51, 59–61] are comparatively high on computa-tion cost for working on a single video making them unsuit-able for frugal animation transfer. Dynamic reconstruction based methods [26, 33, 42, 43, 50] provide promising results, but tracking error could cause structural artifacts thereby resulting in issues of shape correspondence or skeleton ex-traction.
Without using a predefined template, establishing cor-respondence between parts of the partial source and the complete target objects becomes challenging. Automatic cor-respondence methods like [4, 11, 17, 45, 53, 62] do not work in our setting where input is sparse, noisy and incomplete. A family of approaches based on surface level correspondence such as the functional maps [17,21,27,31,40,47] incorporate geometry; however, these approaches are data intensive and do not account for the underlying shape topologies explicitly which are critical to matching generic shapes.
Recently, a deep learning approach, Morig [57] was pro-posed to automatically rig character meshes and capture the motion of performing characters from single-view point cloud streams. However, Morig requires supervision at all stages. Furthermore, Liao et al. [24] introduced a skeleton-free approach to transfer poses between 3D characters. The target object is restricted to bipedal characters in T-pose, and the mocap data is limited to human motion.
To address the above shortcomings, we propose Trans-fer4D, a frugal motion capture and a retargeting framework.
We extract the skeleton motion of the source object from a monocular depth video in an unsupervised fashion and retarget the motion to a similar target virtual model. Instead of relying on a predefined template, we directly extract the skeleton from the incoming video. Furthermore, by directly embedding the skeleton into the target object, we bypass establishing dense (surface) correspondence between the source and target that can prove to be computationally ex-pensive and noisy.
Key Contributions: (1) To the best of our knowledge, Trans-fer4D is the first approach for intra-category motion transfer technique from a monocular depth video to a target virtual object (Fig. 1). The research aid in efforts towards the de-mocratization of motion-capture systems and reduce anima-tors’ drudgery through rigging and automatic motion transfer. (2) To transfer motion from a single view incomplete mesh, we propose a novel unsupervised skeleton extraction algo-rithm. We construct the skeleton based on rigidity constraints from the motion and structural cues from the curve skeleton.
See Sec. 4 for details. 2.