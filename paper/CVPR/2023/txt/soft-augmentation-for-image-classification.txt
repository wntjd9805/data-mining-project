Abstract
Modern neural networks are over-parameterized and thus rely on strong regularization such as data augmenta-tion and weight decay to reduce overfitting and improve generalization. The dominant form of data augmentation applies invariant transforms, where the learning target of a sample is invariant to the transform applied to that sam-ple. We draw inspiration from human visual classifica-tion studies and propose generalizing augmentation with invariant transforms to soft augmentation where the learn-ing target softens non-linearly as a function of the de-gree of the transform applied to the sample: e.g., more ag-gressive image crop augmentations produce less confident learning targets. We demonstrate that soft targets allow for more aggressive data augmentation, offer more robust performance boosts, work with other augmentation poli-cies, and interestingly, produce better calibrated models (since they are trained to be less confident on aggressively cropped/occluded examples). Combined with existing ag-gressive augmentation strategies, soft targets 1) double the top-1 accuracy boost across Cifar-10, Cifar-100, ImageNet-1K, and ImageNet-V2, 2) improve model occlusion perfor-mance by up to 4×, and 3) half the expected calibration error (ECE). Finally, we show that soft augmentation gen-eralizes to self-supervised classification tasks. Code avail-able at https://github.com/youngleox/soft_ augmentation 1.

Introduction
Deep neural networks have enjoyed great success in the past decade in domains such as visual understanding [42], natural language processing [5], and protein structure pre-diction [41]. However, modern deep learning models are often over-parameterized and prone to overfitting. In addi-tion to designing models with better inductive biases, strong regularization techniques such as weight decay and data augmentation are often necessary for neural networks to achieve ideal performance. Data augmentation is often a computationally cheap and effective way to regularize mod-els and mitigate overfitting. The dominant form of data aug-mentation modifies training samples with invariant trans-forms – transformations of the data where it is assumed that the identity of the sample is invariant to the transforms.
Indeed, the notion of visual invariance is supported by evidence found from biological visual systems [54]. The robustness of human visual recognition has long been docu-mented and inspired many learning methods including data augmentation and architectural improvement [19, 47]. This paper focuses on the counterpart of human visual robust-ness, namely how our vision fails. Instead of maintaining perfect invariance, human visual confidence degrades non-linearly as a function of the degree of transforms such as occlusion, likely as a result of information loss [44]. We propose modeling the transform-induced information loss for learned image classifiers and summarize the contribu-tions as follows:
• We propose Soft Augmentation as a generalization of data augmentation with invariant transforms. With Soft Aug-mentation, the learning target of a transformed training sample softens. We empirically compare several soften-ing strategies and prescribe a robust non-linear softening formula.
• With a frozen softening strategy, we show that replac-ing standard crop augmentation with soft crop augmenta-tion allows for more aggressive augmentation, and dou-bles the top-1 accuracy boost of RandAugment [8] across
Cifar-10, Cifar-100, ImageNet-1K, and ImageNet-V2.
• Soft Augmentation improves model occlusion robustness by achieving up to more than 4× Top-1 accuracy boost on heavily occluded images.
• Combined with TrivialAugment [37], Soft Augmentation further reduces top-1 error and improves model calibra-tion by reducing expected calibration error by more than half, outperforming 5-ensemble methods [25].
• In addition to supervised image classification models,
Soft Augmentation also boosts the performance of self-supervised models, demonstrating its generalizability.
Figure 1. Traditional augmentation encourages invariance by requiring augmented samples to produce the same target label; we visualize the translational offset range (tx, ty) of Standard Hard Crop augmentations for 32 × 32 images from Cifar-100 on the left, reporting the top-1 error of a baseline ResNet-18. Naively increasing the augmentation range without reducing target confidence increases error (middle), but softening the target label by reducing the target confidence for extreme augmentations reduces the error (right), allowing for training with even more aggressive augmentations that may even produce blank images. Our work also shows that soft augmentations produce models that are more robust to occlusions (since they encounter larger occlusions during training) and models that are better calibrated (since they are trained to be less-confident on such occluded examples). 2.