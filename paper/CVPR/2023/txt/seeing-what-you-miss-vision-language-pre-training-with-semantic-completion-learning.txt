Abstract
Cross-modal alignment is essential for vision-language pre-training (VLP) models to learn the correct correspond-ing information across different modalities. For this purpose, inspired by the success of masked language modeling (MLM) tasks in the NLP pre-training area, numerous masked mod-eling tasks have been proposed for VLP to further promote cross-modal interactions. The core idea of previous masked modeling tasks is to focus on reconstructing the masked tokens based on visible context for learning local-to-local alignment. However, most of them pay little attention to the global semantic features generated for the masked data, resulting in a limited cross-modal alignment ability of global representations. Therefore, in this paper, we propose a novel
Semantic Completion Learning (SCL) task, complementary to existing masked modeling tasks, to facilitate global-to-local alignment. Specifically, the SCL task complements the missing semantics of masked data by capturing the corre-sponding information from the other modality, promoting learning more representative global features which have a great impact on the performance of downstream tasks. More-over, we present a flexible vision encoder, which enables our model to perform image-text and video-text multimodal tasks simultaneously. Experimental results show that our proposed method obtains state-of-the-art performance on various vision-language benchmarks, such as visual ques-tion answering, image-text retrieval, and video-text retrieval. 1.

Introduction
Our real-world contains a wide variety of information, such as texts, images, sounds, etc. For a powerful general artificial intelligence system, it is necessary to capture the se-*Equal contribution.
†Corresponding Author.
Figure 1. (a) The comparisons between previous masked modeling tasks and our proposed Semantic Completion Learning (SCL), which is composed of “MVSC” and “MLSC”. (b) The cross-modal attention map visualization of the text global representation ([CLS]) on the input image for our model pre-trained with or without SCL. mantic association from different modality sources. Towards this goal, multimodal representation learning is a crucial technique to bridge the heterogeneity gap between different modalities [6, 36]. In this area, vision-language pre-training models [14, 21, 30, 35, 43] have shown an impressive se-mantic alignment ability, which brings substantial advances on various downstream tasks, for instance, visual question answering, image-text retrieval, etc.
Recently, numerous self-supervised vision-language pre-training models [3, 10, 14, 17, 31, 32, 43] have been proposed.
These methods model the interactions between vision and
language features mainly by using various masked model-ing tasks, such as masked language modeling (MLM) and masked vision modeling (MVM). As shown in Fig. 1(a), the basic idea of MLM and MVM is self-reconstructing the masked tokens via leveraging informative visible tokens to realize local-to-local alignment. Specifically, MLM adopted by BERT [24] is to predict the original vocabulary IDs of the masked words. Inspired by the success of MLM in pre-training, there is a flourishing trend to extend it to visual pre-training tasks. Generally, by masking some visual patches,
MVM tasks predict their original pixels [13, 17], correspond-ing discrete tokens [4, 10, 46] generated by the VQ-VAE variants, or Histograms of Oriented Gradients (HOG) fea-tures [11], etc.
These masked modeling tasks only focus on reconstruct-ing the local masked tokens, and pay little attention to recov-ering the missing global semantic information caused by data corruption. The token-level reconstruction may lead to in-adequate learning of global representations for cross-modal information. As illustrated in Fig. 1(b), in the situation of token-level reconstructions, the global representation is dis-ordered in its attention on the other modality. It implies that the global-to-local alignment ability of the pre-training model is limited, leading to a degraded global representation.
However, the global semantic features have a great impact on the performance of the pre-training model as they are usually used to deal with downstream tasks. Therefore, it is crucial to ensure the global semantic features to learn more accurate global-to-local alignment.
Intuitively, considering that the paired vision and text data are two views of the same semantic information, the missing semantics of masked data can be completed by capturing information from the other modality. From this point of view, we propose a novel pre-training task called Semantic
Completion Learning (SCL). Specifically, SCL is composed of dual parts: masked vision semantic completion (MVSC) and masked language semantic completion (MLSC). As shown in Fig. 1(a), MVSC (MLSC) exploits information of complete text (vision) data to recover the global semantic representations of masked vision (text) data. In this way, the model can generate representative global features with accu-rate global-to-local alignment. For example, as illustrated in Fig. 1(b), compared with the model pre-trained without
SCL, the attention maps with SCL pre-training are more discriminative and reasonable.
For the architecture of the vision-language pre-training model, we adopt a general framework that consists of two uni-modal encoders and a fusion encoder. Moreover, we present a flexible vision encoder to enable our model to per-form image-text and video-text multimodal tasks simultane-ously. Specifically, for video inputs, the vision encoder only adds a few additional learning parameters, and the [CLS] feature of each frame is treated as a bridge associating spatial modeling within the frame and temporal modeling among frames. Inspired by curriculum learning [3], we train the model with image-text and video-text datasets successively to transfer visual knowledge from images to videos.
In a nutshell, our contributions are three-fold. (1) To en-hance the global-to-local alignment of global representations, we propose a new pre-training task called Semantic Com-pletion Learning (SCL), which recovers missing semantic information from unmasked data, promoting learning more representative global features. (2) We design an adaptive vision encoder, which can transfer multimodal pre-training knowledge between images and videos readily. (3) We con-duct multiple vision-language downstream tasks to demon-strate the generalization of semantic completion learning, and the vision encoder, including visual question answer-ing, visual reasoning, image-text retrieval, and video-text retrieval. Our model SCL achieves state-of-the-art perfor-mance based on a similar pre-training data scale. Our code is available at https://github.com/IIGROUP/SCL. 2.