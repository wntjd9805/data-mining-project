Abstract
Self-supervised multi-frame depth estimation achieves high accuracy by computing matching costs of pixel corre-spondences between adjacent frames, injecting geometric information into the network. These pixel-correspondence candidates are computed based on the relative pose esti-mates between the frames. Accurate pose predictions are essential for precise matching cost computation as they in-fluence the epipolar geometry. Furthermore, improved depth estimates can, in turn, be used to align pose estimates.
Inspired by traditional structure-from-motion (SfM) prin-ciples, we propose the DualRefine model, which tightly cou-ples depth and pose estimation through a feedback loop. Our novel update pipeline uses a deep equilibrium model frame-work to iteratively refine depth estimates and a hidden state of feature maps by computing local matching costs based on epipolar geometry. Importantly, we used the refined depth estimates and feature maps to compute pose updates at each step. This update in the pose estimates slowly alters the epipolar geometry during the refinement process. Experi-mental results on the KITTI dataset demonstrate competitive depth prediction and odometry prediction performance sur-passing published self-supervised baselines 1. 1.

Introduction
The optimization of the coordinates of observed 3D points and camera poses forms the basis of structure-from-motion (SfM). Estimation of both lays the foundation for robotics [34, 35, 75], autonomous driving [20], or AR/VR applications [60]. Traditionally, however, SfM techniques are susceptible to errors in scenes with texture-less regions, dynamic objects, etc. This has motivated the development of deep learning models that can learn to predict depth from monocular images [14, 15, 18, 48, 50]. These models can 1https://github.com/antabangun/DualRefine (a) (b)
Figure 1. (a) The estimated pose of a camera affects the epipolar geometry. (b) The epipolar line in the source image, calculated from yellow points in the target image, for the PoseNet-based [43] initial pose regression (red) and our refined pose (green). The yellow point in the source image is calculated based on our final depth and pose estimates. accurately predict depth based solely on image cues, without requiring geometric information.
In recent years, self-supervised training of depth and pose models has become an attractive method, as it alleviates the need for ground truth while demonstrating precision com-parable to those of supervised counterparts [7, 19, 22, 23, 26, 28, 30, 61, 70, 74, 83, 87, 98, 106, 108]. Such an approach uses depth and pose predictions to synthesize neighboring images in a video sequence and enforce consistency between them. As the image sequence is also available at test time, recent self-supervised methods also study the use of multiple frames during inference [91]. These typically involve the construction of cost volumes from multiple views to compute pixel correspondences, bearing similarities to (multi-view) stereo models [4, 44, 77]. By incorporating multi-frame data, geometric information is integrated to make depth predic-tions, improving the performance as well as the robustness.
In such a multi-frame matching-based model, the accuracy of matching costs computation is essential. Recent work in
DepthFormer [29] demonstrates its importance, as they de-signed a Transformer [84]-based module to improve match-ing costs and achieve state-of-the-art (SoTA) depth accuracy.
However, their approach came with a large memory cost.
Unlike stereo tasks, the aforementioned self-supervised multi-frame models do not assume known camera poses and use estimates learned by a teacher network, typically a PoseNet [43]-based model. This network takes two im-ages as input and regresses a 6-DoF pose prediction. As the estimated pose affects the computation of epipolar ge-ometry (Fig. 1(a)), the accuracy of the pose estimates is crucial to obtain accurate correspondence matches between multiple frames. However, as noted in recent studies [72], pure learning-based pose regression generally still lags be-hind its traditional counterpart, due to the lack of geometric reasoning. By refining the pose estimates, we can improve the accuracy of the matching costs, potentially leading to better depth estimates as well. In Fig. 1(b), we show that the epipolar lines calculated from the regressed poses do not align with our refined estimates. Conversely, a better depth prediction may lead to a better pose prediction. Thus, instead of building the cost volume once using regressed poses, we choose to perform refinements of both depth and pose in parallel and sample updated local cost volumes at each itera-tion. This approach is fundamentally inspired by traditional
SfM optimization and is closely aligned with feedback-based models that directly couple depth and pose predictions [27].
In this work, we propose a depth and pose refinement model that drives both towards an equilibrium, trained in a self-supervised framework. We accomplish this by making the following contributions: First, We introduce an iterative update module that is based on epipolar geometry and direct alignment. We sample candidate matches along the epipolar line that evolves based on the current pose estimates. Then the sampled matching costs are used to infer per-pixel con-fidences that are used to compute depth refinements. The updated depth estimates are then used in direct feature-metric alignments to refine the pose updates towards convergence.
As a result, our model can perform geometrically consistent depth and pose updates. Second, These updates refine the initial estimates made by the single-frame model. By doing so, we do not rely on full cost volume construction and base our updates only on local cost volumes, making it simpler, more memory efficient, and more robust. Lastly, we design our method within a deep equilibrium (DEQ) framework [3] to implicitly drive the predictions towards a fixed point. Im-portantly, DEQ allows for efficient training with low training memory, improving upon the huge memory consumption of previous work. With our proposed novel design, we show improved depth estimates through experiments that are com-petitive with the SoTA models. Furthermore, our model demonstrates improved global consistency of visual odome-try results, outperforming other learning-based models. 2.