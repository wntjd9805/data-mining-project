Abstract
Animal pose estimation is challenging for existing image-based methods because of limited training data and large intra- and inter-species variances. Motivated by the progress of visual-language research, we propose that pre-trained language models (e.g., CLIP) can fa-cilitate animal pose estimation by providing rich prior knowledge for describing animal keypoints in text. How-ever, we found that building effective connections between pre-trained language models and visual animal keypoints is non-trivial since the gap between text-based descrip-tions and keypoint-based visual features about animal pose can be significant. To address this issue, we introduce a novel prompt-based Contrastive learning scheme for connecting Language and AniMal Pose (CLAMP) effec-tively. The CLAMP attempts to bridge the gap by adapt-ing the text prompts to the animal keypoints during net-work training. The adaptation is decomposed into spatial-aware and feature-aware processes, and two novel con-trastive losses are devised correspondingly.
In practice, the CLAMP enables the first cross-modal animal pose es-timation paradigm. Experimental results show that our method achieves state-of-the-art performance under the su-pervised, few-shot, and zero-shot settings, outperforming image-based methods by a large margin. The code is avail-able at https://github.com/xuzhang1199/CLAMP. 1.

Introduction
Animal pose estimation aims to locate and identify a se-ries of animal body keypoints from an input image. It plays a key role in animal behavior understanding, zoology, and wildlife conservation which can help study and protect an-imals better. Although the animal pose estimation task is analogous to human pose estimation [2] to some extent, we argue that the two tasks are very different. For example, ani-mal pose estimation involves multiple animal species, while
Figure 1. Conceptualized visualization of our CLAMP method.
Regarding the animal pose estimation task, we proposed to exploit rich language information from texts to facilitate the visual iden-tification of animal keypoints. To better connect texts and animal images, we devise the CLAMP to adapt pre-trained language mod-els via a spatial-aware and a feature-aware process. As a result, the
CLAMP helps deliver better animal pose estimation performance. human pose estimation only focuses on one category. Be-sides, it is much more difficult to collect and annotate ani-mal pose data covering different animal species, thus exist-ing animal pose datasets are several times smaller than the human pose datasets [20] regarding the number of samples per species. Recently, Yu et al. [38] attempted to alleviate this problem by presenting the largest animal pose estima-tion dataset, i.e., AP-10K, which contains 10K images from 23 animal families and 54 species and provides the base-line performance of SimpleBaseline [32] and HRNet [31].
Despite this progress, the volume of this dataset is still far smaller than the popular human pose dataset, such as MS
COCO [20] with 200K images.
With diverse species and limited data, current animal pose datasets usually have large variances in animal poses which include both intra-species and inter-species vari-ances. More specifically, the same animal can have diverse poses, e.g., pandas can have poses like standing, crawling, sitting, and lying down. Besides, the difference in the poses of different animal species can also be significant, e.g., horses usually lie down to the ground, while monkeys can be in various poses. Furthermore, even with the same pose, different animals would have different appearances. As an example, the joints of monkeys are wrinkled and hairy, while those of hippos are smooth and hairless. As a result, it could be extremely challenging for current human pose esti-mation methods to perform well on the animal pose estima-tion task without sufficient training data. Although image-based pre-training methodologies can be helpful in mitigat-ing the problem of insufficient data, the huge gap between the pre-training datasets (e.g. ImageNet [7] image classifi-cation dataset and MS COCO human pose dataset [20]) and the animal pose datasets could compromise the benefits of pre-training procedures.
Rather than only using images to pre-train models, we notice that the keypoints of different poses and different an-imals share the same description in natural languages, thus the language-based pre-trained models can be beneficial to compensate for the shortage of animal image data. For example, if a pre-trained language model provides a text prompt of “a photo of the nose”, we can already use it to identify the presence of the nose keypoint in the image with-out involving too much training on the new dataset. For-tunately, a recently proposed Contrastive Language-Image
Pre-training (CLIP) [28] model can provide a powerful mapping function to pair the image with texts effectively.
Nevertheless, we found that fine-tuning the CLIP on the an-imal pose dataset could still suffer from large gaps between the language and the images depicting animals. In partic-ular, the vanilla CLIP model only learns to provide a text prompt with general language to describe the entire image, while the animal pose estimation requires pose-specific de-scriptions to identify several different keypoints with their locations estimated from the same image. To this end, it is important to adapt the pre-trained language model to the an-imal pose dataset and effectively exploit the rich language knowledge for animal pose estimation.
To address the above issue, we propose a novel prompt-based contrastive learning scheme for effectively connect-ing language and animal pose (called CLAMP), enabling the first cross-modal animal pose estimation paradigm. In particular, we design pose-specific text prompts to describe different animal keypoints, which will be further embed-ded using the language model with rich prior knowledge.
By adapting the pose-specific text prompts to visual animal keypoints, we can effectively utilize the knowledge from the pre-trained language model for the challenging animal pose estimation. However, there is a significant gap between the pre-trained CLIP model (which generally depicts the entire image) and the animal pose task (which requires the specific keypoint feature discriminative to and aligned with given text descriptions). To this end, we decompose the compli-cated adaptation into a spatial and feature-aware process.
Specifically, we devise a spatial-level contrastive loss to help establish spatial connections between text prompts and the image features. A feature-level contrastive loss is also devised to make the visual features and embedded prompts of different keypoints more discriminative to each other and align their semantics in a compatible multi-modal embed-ding space. With the help of the decomposed adaptation, ef-fective connections between the pre-trained language model and visual animal poses are established. Such connections with rich prior language knowledge can help deliver better animal pose prediction.
In summary, the contribution of this paper is threefold:
• We propose a novel cross-modal animal pose estima-tion paradigm named CLAMP to effectively exploit prior language knowledge from the pre-trained lan-guage model for better animal pose estimation.
• We propose to decompose the cross-modal adaptation into a spatial-aware process and a feature-aware pro-cess with carefully designed losses, which could effec-tively align the language and visual features.
• Experiments on two challenging datasets in three set-tings, i.e., 1) AP-10K [38] dataset (supervised learn-ing, few-shot learning, and zero-shot learning) and 2)
Animal-Pose [4] dataset (supervised learning), vali-date the effectiveness of the CLAMP method. 2.