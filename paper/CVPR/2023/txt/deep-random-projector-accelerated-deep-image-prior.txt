Abstract ized data-fitting formulation (λ: regularization parameter):
Deep image prior (DIP) has shown great promise in tackling a variety of image restoration (IR) and general vi-sual inverse problems, needing no training data. However, the resulting optimization process is often very slow, in-evitably hindering DIP’s practical usage for time-sensitive
In this paper, we focus on IR, and propose scenarios. two crucial modifications to DIP that help achieve sub-stantial speedup: 1) optimizing the DIP seed while freez-ing randomly-initialized network weights, and 2) reduc-In addition, we reintroduce ex-ing the network depth. plicit priors, such as sparse gradient prior—encoded by total-variation regularization, to preserve the DIP peak per-formance. We evaluate the proposed method on three IR tasks, including image denoising, image super-resolution, and image inpainting, against the original DIP and vari-ants, as well as the competing metaDIP that uses meta-learning to learn good initializers with extra data. Our method is a clear winner in obtaining competitive restora-tion quality in a minimal amount of time. Our code is avail-able at https://github.com/sun- umn/Deep-Random-Projector. 1.

Introduction
Deep image prior as an emerging “prior” for visual inverse problems Deep image prior (DIP) [29] and its variants [9, 21] have recently claimed numerous successes in solving image restoration (IR) (e.g., image denoising, super-resolution, and image inpainting) and general visual inverse problems [22]. The idea is to parameterize a visual object of interest, say x, as the output of a structured deep neural network (DNN), i.e., x = Gθ(z), where Gθ is a
DNN and z is a seed that is often drawn randomly and then frozen. Now given a visual inverse problem of the form y ≈ f (x)—y is the observation, f models the observation process, and ≈ allows modeling observational noise—and the typical maximum-a posterior (MAP)-inspired regular-min x
ℓ(y, f (x)) (cid:124) (cid:125) (cid:123)(cid:122) data-fitting loss
,
+λ R(x) (cid:124) (cid:123)(cid:122) (cid:125) regularizer (1) one can plug in the reparametrization x = Gθ(z) to obtain min
θ
ℓ(y, f ◦ Gθ(z)) + λR ◦ Gθ(z), (2) where ◦ denotes functional composition. A salient feature of DIP for IR is that they are training-free despite the pres-ence of the DNN Gθ, i.e., no extra data other than y and f are needed for problem-solving.
DIP is not a standalone prior, unlike traditional priors such as sparse gradient [24], dark channel [8], and self-favorable results from DIP are obtained similarity [7]: as a result of the tight integration of architecture, over-parametrization of Gθ, first-order optimization methods, and appropriate early stopping (ES). The Gθ in prac-tice is often a structured convolutional neural network and significantly overparameterized. Due to the heavy over-parametrization, in principle, f ◦ Gθ(z) could perfectly fit y, especially when no extra regularization R ◦ Gθ(z) is present. When such overfitting occurs, the recovered (cid:98)x = Gθ(z) accounts for the potential noise in y also in ad-dition to the desired visual content. What comes to the res-cue is the hallmark “early-learning-then-overfitting” phe-nomenon: the learning process picks up mostly the desired visual content first before starting to fit the noise [16, 30], which is believed to be a combined implicit regulariza-tion effect of overparametrization, convolutional structures in Gθ, and first-order optimization methods [10, 12]. So, if one can locate the peak-performance point and stop the fitting process sharp there, i.e., performing appropriate ES, they could get a good estimate for x.
Practical issues: overfitting and slowness Despite the remarkable empirical success of DIP in solving IRs, there
Figure 1. Comparison of DIP and our DRP-DIP for image denoising. Left: immediate reconstructions generated by DIP and DRP-DIP respectively at different time cutoffs. DRP-DIP can restore skeleton of the image in 1 second, whereas DIP cannot do so even after 10 seconds. Right: PSNR trajectories over time. Our DRP-DIP reaches the performance peak in much shorter time than DIP. are a couple of pressing issues (see Fig. 1) impeding the practical deployment of DIP:
• Overfitting: Most previous works reporting the suc-cesses of DIP set the number of iterations directly, based on visual inspection and trial-and-error. Visual inspec-tion precludes large-scale batch processing or deployment into unfamiliar (e.g., scientific imaging of unknown mi-nuscule objects) or unobservable scenarios (e.g., multi-dimensional objects that are hard to visualize), whereas trial-and-error likely performs the tuning with reference to the unknown groundtruth object. So, strictly speaking, previous successes mostly only show the potential of DIP, without providing an operational ES strategy [16,22,30].
Ideas that try to mitigate overfitting, including controlling the capacity of Gθ, explicit regularization, and explicit noise modeling, tend to prolong the iteration process and push the peak performance to final iterations. it
• Slowness: For DIP to reach the performance peak from typically takes thousands of random initialization, steps. Although the number is comparable to that of typ-ical iterative methods for solving Eq. (1), here each step entails a forward and a backward pass through the Gθ and hence is much more expensive. In fact, on a state-of-the-art GPU card such as Nvidia V100, the whole pro-cess can take up to tens of minutes for simple IR and up to hours for advanced IR tasks. The optimization slowness inevitably hinders DIP’s applicability to time-sensitive problems.
These two issues are intertwined: mitigating overfitting es-calates the slowness. Thus an ideal solution is to speed up the process of climbing to the performance peak and then stop around the peak. tor (DRP), that requires substantial less computation time than DIP to obtain a comparable level of peak performance.
DRP consists of three judiciously chosen modifications to
DIP: (1) optimizing the DIP seed while freezing randomly-initialized network weights, (2) reducing the network depth, and (3) including additional explicit prior(s) for regulariza-tion, such as total variation (TV) regularization that encodes the sparse-gradient prior [24]. One can quickly see the su-periority of our method from Fig. 1. Our main contributions include:
• proposing deep random projector (DRP) that integrates three essential modifications to DIP for speedup (Sec. 3);
• validating that DRP achieves peak performance compa-rable to that of DIP in much less time on three IR tasks (Sec. 4.2, Sec. 4.3, and Sec. 4.4), and showing that DRP is much more efficient than meta-learning-based meta-DIP for speedup (Sec. 4.6);
• demonstrating that our ES method in [30] can be directly integrated, without modification, to detect near-peak per-formance for DRP. Hence, we have a complete solution to both overfitting and slowness issues in DIP (Sec. 4.5). 2.