Abstract
We study learning object segmentation from unlabeled videos. Humans can easily segment moving objects without knowing what they are. The Gestalt law of common fate, i.e., what move at the same speed belong together, has inspired unsupervised object discovery based on motion segmenta-tion. However, common fate is not a reliable indicator of objectness: Parts of an articulated / deformable object may not move at the same speed, whereas shadows / reflections of an object always move with it but are not part of it.
Our insight is to bootstrap objectness by first learning image features from relaxed common fate and then refining them based on visual appearance grouping within the im-age itself and across images statistically. Specifically, we learn an image segmenter first in the loop of approximating optical flow with constant segment flow plus small within-segment residual flow, and then by refining it for more co-herent appearance and statistical figure-ground relevance.
On unsupervised video object segmentation, using only
ResNet and convolutional heads, our model surpasses the state-of-the-art by absolute gains of 7/9/5% on DAVIS16
/ STv2 / FBMS59 respectively, demonstrating the effective-ness of our ideas. Our code is publicly available. 1.

Introduction
Object segmentation from videos is useful to many vi-sion and robotics tasks [1, 19, 30, 32]. However, most meth-ods rely on pixel-wise human annotations [4,5,13,20,23,25, 26, 29, 33, 35, 46, 47], limiting their practical applications.
We focus on learning object segmentation from entirely unlabeled videos (Fig. 1). The Gestalt law of common fate, i.e., what move at the same speed belong together, has in-spired a large body of unsupervised object discovery based on motion segmentation [6, 18, 22, 28, 41, 43, 45].
There are three main types of unsupervised video object segmentation (UVOS) methods. 1) Motion segmentation methods [18,28,41,43] use motion signals from a pretrained
Figure 1. We study how to discover objectness from unlabeled videos based on common motion and appearance. AMD [22] and OCLR [41] rely on common fate, i.e., what move at the same speed belong together, which is not always a reliable indicator of objectness. Top: Articulation of a human body means that object parts may not move at the same speed; common fate thus leads to partial objectness. Bottom: Reflection of a swan in water al-ways moves with it but is not part of it; common fate thus leads to excessive objectness. Our method discovers full objectness by relaxed common fate and visual grouping. AMD+ refers to AMD with RAFT flows as motion supervision for fair comparison. optical flow estimator to segment an image into foreground objects and background (Fig. 1). OCLR [41] achieves state-of-the-art performance by first synthesizing a dataset with arbitrary objects moving and then training a motion seg-mentation model with known object masks. 2) Motion-guided image segmentation methods such as GWM [6] use motion segmentation loss to guide appearance-based segmentation. Motion between video frames is only re-quired during training, not during testing. 3) Joint appear-ance segmentation and motion estimation methods such as AMD [22] learn motion and segmentation simultane-ously in a self-supervised fashion by reconstructing the next frame based on how segments of the current frame move.
However, while common fate is effective at binding parts of heterogeneous appearances into one whole moving ob-ject, it is not a reliable indicator of objectness (Fig. 1). 1. Articulation: Parts of an articulated or deformable ob-ject may not move at the same speed; common fate thus leads to partial objectness containing the major moving part only. In Fig.1 top, AMD+ discovers only the mid-Unsupervised object segmentation MG AMD GWM Ours
M M∗ M M+A
Sources of supervision
✗ ✓
Segment stationary objects?
✗
Handle articulated objects?
-✗
Label-free hyperparameter tuning? ✗
✓
✓
✓
✓
✗
✗
Figure 2. Advantages over leading unsupervised object segmen-tation methods MG [43]/AMD [22]/GWM [6]: 1) With motion supervision instead of motion input, we can segment stationary objects. 2) With both motion (M) and appearance (A) as supervi-sion, we can discover full objectness from noisy motion cues. M∗ refers to implicit motion via image warping. 3) By modeling rela-tive motion within an object, we can handle articulated objects. 4)
By comparing motion-based segmentation with appearance-based segmentation, we can tune hyperparameters without labels. Our performance gain is substantial, more with post-processing (†). dle torso of the street dancer since it moves the most, whereas OCLR misses the exposed belly which is very different from the red hoodie and the gray jogger. 2. Reflection: Shadows or reflections of an object always move with the object but are not part of the object; com-mon fate thus leads to excessive objectness that covers more than the object. In Fig.1 bottom, AMD+ or OCLR cannot separate the swan from its reflection in water.
We have two insights to bootstrap full objectness from common fate in unlabeled videos. 1) To detect an articu-lated object, we allow various parts of the same object to as-sume different speeds that deviate slightly from the object’s overall speed. 2) To detect an object from its reflections, we rely on visual appearance grouping within the image itself and statistical figure-ground relevance. For example, swans tend to have distinctive appearances from the water around them, and reflections may be absent in some swan images.
Specifically, we learn unsupervised object segmentation in two stages: Stage 1 learns to discover objects from mo-tion supervision with relaxed common fate, whereas Stage 2 refines the segmentation model based on image appearance.
At Stage 1, we discover objectness by computing the optical flow and learning an image segmenter in the loop of approximating the optical flow with constant segment flow plus small within-segment residual flow, relaxing common fate from the strict same-speed assumption. At Stage 2, we refine our model by image appearance based on low-level visual coherence within the image itself and usual figure-ground distinction learned statistically across images.
Existing UVOS methods have hyperparameters that sig-nificantly impact the quality of segmentation. For example, the number of segmentation channels is a critical parameter for AMD [22], and it is usually chosen according to an an-notated validation set in the downstream task, defeating the claim of unsupervised objectness discovery.
We propose unsupervised hyperparameter tuning that does not require any annotations. We examine how well our motion-based segmentation aligns with appearance-based affinity on DINO [2] features self-supervisedly learned on
ImageNet [34], which is known to capture semantic object-ness. Our idea is also model-agnostic and applicable to other UVOS methods.
Built on the novel concept of Relaxed Common Fate (RCF), our method has several advantages over leading
UVOS methods (Fig. 2): It is the only one that uses both motion and appearance to supervise learning; it can seg-ment stationary and articulated objects in single images, and it can tune hyperparameters without external annotations.
On UVOS benchmarks, using only standard ResNet
[12] backbone and convolutional heads, our RCF surpasses the state-of-the-art by absolute gains of 7.0%/9.1%/4.5% (6.3%/12.0%/5.8%) without (with) post-processing on
DAVIS16 [32] / STv2 [19] / FBMS59 [30] respectively, val-idating the effectiveness of our ideas. 2.