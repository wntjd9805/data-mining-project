Abstract
The pose-guided person image generation task requires synthesizing photorealistic images of humans in arbitrary poses.
The existing approaches use generative adver-sarial networks that do not necessarily maintain realistic textures or need dense correspondences that struggle to handle complex deformations and severe occlusions.
In this work, we show how denoising diffusion models can be applied for high-ﬁdelity person image synthesis with strong sample diversity and enhanced mode coverage of the learnt data distribution. Our proposed Person Image
Diffusion Model (PIDM) disintegrates the complex trans-fer problem into a series of simpler forward-backward de-noising steps. This helps in learning plausible source-to-target transformation trajectories that result in faithful textures and undistorted appearance details. We intro-duce a ‘texture diffusion module’ based on cross-attention to accurately model the correspondences between appear-ance and pose information available in source and target images. Further, we propose ‘disentangled classiﬁer-free guidance’ to ensure close resemblance between the condi-tional inputs and the synthesized output in terms of both pose and appearance information. Our extensive results on two large-scale benchmarks and a user study demon-strate the photorealism of our proposed approach under challenging scenarios. We also show how our generated images can help in downstream tasks. Code is available at https://github.com/ankanbhunia/PIDM. 1.

Introduction
The Pose-guided person image synthesis task [19,23,30] aims to render a person’s image with a desired pose and ap-pearance. Speciﬁcally, the appearance is deﬁned by a given source image and the pose by a set of keypoints. Having control over the synthesized person images in terms of pose and style is an important requisite for applications such as e-commerce, virtual reality, metaverse and content generation for the entertainment industry. Furthermore, the generated
Figure 1. (a) Our proposed PIDM is a denoising diffusion model where the generative path is conditioned on the pose and style.
PIDM breaks down the problem into a series of forward-backward diffusion steps to learn the plausible transfer trajectories. (b) Com-parison of PIDM with the recently introduced NTED [18]. PIDM accurately retains the appearance of the source style image while also producing images that are more natural and sharper while
NTED struggles to adequately preserve the source appearance in complex scenarios (marked in red boxes). images can be used to improve performance on downstream tasks such as person re-identiﬁcation [30]. The challenge is to generate photorealistic outputs tightly conforming with the given pose and appearance information.
In the literature, person synthesis problem is generally tackled using Generative Adversarial Networks (GAN) [4] which try to generate a person in a desired pose using a single forward pass. However, preserving a coherent struc-tural, appearance and global body composition in the new pose is a challenging task to achieve in one shot. The result-ing outputs commonly experience deformed textures and unrealistic body shapes, especially when synthesizing oc-cluded body parts (see Fig. 1). Further, GANs are prone to unstable training behaviour due to adversarial min-max objective and lead to limited diversity in the generated sam-ples. Similarly, Variational Autoencoder [8] based solutions
have been explored that are relatively stable, but suffer from blurry details and offer low-quality outputs than GANs due to their dependence on a surrogate loss for optimization.
In this work, we frame the person synthesis problem as a series of diffusion steps that progressively transfer a per-son in the source image to the target pose. Diffusion mod-els [6] are motivated from non-equilibrium thermodynam-ics that deﬁne a Markov chain of slowly adding noise to the input samples (forward pass) and then reconstructing the desired samples from noise (reverse pass). In this manner, rather than modeling the complex transfer characteristics in a single go, our proposed person synthesis approach PIDM breaks down the problem into a series of forward-backward diffusion steps to learn the plausible transfer trajectories.
Our approach can model the intricate interplay of the per-son’s pose and appearance, offers higher diversity and leads to photorealistic results without texture deformations (see
Fig. 1).
In contrast to existing approaches that deal with major pose shifts by requiring parser maps denoting human body parts [14, 23, 28], or dense 3D correspondences [9, 10] to ﬁt human body topology by warping the textures, our ap-proach can learn to generate realistic and authentic images without such detailed annotations.
Our major contributions are as follows:
• We develop the ﬁrst diffusion-based approach for pose-guided person synthesis task which can work un-der challenging pose transformations while preserving appearance, texture and global shape characteristics.
• To effectively model the complex interplay between appearance and pose information, we propose a texture diffusion module. This module exploits the correspon-dences between source and target appearance and pose details, hereby obtaining artefact free images.
• In the sampling procedure, we introduce disentangled classiﬁer-free guidance to tightly align the output im-age style and pose with the source image appearance and target pose, respectively. It ensures close resem-blance between the conditions that are input to the gen-erative model and the generated output.
• Our results on DeepFashion [11] and Market-1501 [27] benchmarks set new state of the art. We also report a user study to evaluate the qualitative features of generated images. Finally, we demonstrate that syn-thesized images can be used to improve performance in downstream tasks e.g., person re-identiﬁcation. 2.