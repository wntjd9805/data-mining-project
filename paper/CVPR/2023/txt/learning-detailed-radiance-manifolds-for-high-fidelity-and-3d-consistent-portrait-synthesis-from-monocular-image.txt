Abstract
A key challenge for novel view synthesis of monocular portrait images is 3D consistency under continuous pose variations. Most existing methods rely on 2D generative models which often leads to obvious 3D inconsistency ar-tifacts. We present a 3D-consistent novel view synthesis approach for monocular portrait images based on a re-cent proposed 3D-aware GAN, namely Generative Radi-ance Manifolds (GRAM) [13], which has shown strong 3D consistency at multiview image generation of virtual subjects via the radiance manifolds representation. How-ever, simply learning an encoder to map a real image into the latent space of GRAM can only reconstruct coarse ra-diance manifolds without faithful fine details, while im-proving the reconstruction fidelity via instance-specific op-timization is time-consuming. We introduce a novel de-tail manifolds reconstructor to learn 3D-consistent fine de-tails on the radiance manifolds from monocular images, and combine them with the coarse radiance manifolds for high-fidelity reconstruction. The 3D priors derived from the coarse radiance manifolds are used to regulate the learned details to ensure reasonable synthesized results at novel views. Trained on in-the-wild 2D images, our method achieves high-fidelity and 3D-consistent portrait synthesis largely outperforming the prior art. Project page: https://yudeng.github.io/GRAMInverter/ 1.

Introduction
Synthesizing photorealistic portrait images of a per-son from an arbitrary viewpoint is an important task that can benefit diverse downstream applications such as vir-tual avatar creation and immersive online communication.
Thanks to the thriving of 2D Generative Adversarial Net-works (GANs) [18, 25, 26], people can now generate high-quality portraits at desired views given only monocular im-ages as input, via a simple invert-then-edit strategy by con-ducting GAN inversion [1, 38, 52] and latent space edit-ing [12,20,41,50]. However, existing 2D GAN-based meth-ods still have deficiencies when applied to applications that require more strict 3D consistency (e.g. VR&AR). Due to the non-physical rendering process of the 2D CNN-based generators, their synthesized images under pose changes usually bear certain kinds of multiview inconsistency, such
as geometry distortions [3,5] and texture sticking or flicker-ing [24, 55]. These artifacts may not be significant enough when inspecting each static image but can be easily cap-tured by human eyes under continuous image variations.
Recently, there are an emerging group of 3D-aware
GANs [9, 10, 13, 19, 39, 40] targeting at image generation with 3D pose disentanglement. By incorporating Neural
Radiance Field (NeRF) [29] and its variants into the ad-versarial learning process of GANs, they can produce re-alistic images with strong 3D-consistency across different views, given only a set of monocular images as training data. As a result, 3D-aware GANs have shown greater po-tential than 2D GANs for pose manipulations of portraits.
However, even though 3D-aware GANs are capable of gen-erating 3D-consistent portraits of virtual subjects, leverag-ing them for real image pose editing is still a challenging task. To obtain faithful reconstructions of real images, most existing methods [9, 10, 13, 27, 46, 47, 67] turn to a time-consuming and instance-specific optimization to invert the given images into the latent space of a pre-trained 3D-aware
GAN, which is hard to scale-up. And simply enforcing an encoder-based 3D-aware GAN inversion [7, 46] often fails to preserve fine details in the original image.
In this paper, we propose a novel approach GRAMIn-verter, for high-fidelity and 3D-consistent novel view syn-thesis of monocular portraits via single forward pass. Our method is built upon the recent GRAM [13] that can syn-thesize high-quality virtual images with strong 3D consis-tency via the radiance manifolds representation [13]. Never-theless, GRAM suffers from the same lack-of-fidelity issue when combined with a general encoder-based GAN inver-sion approach [52]. The main reason is that the obtained semantically-meaningful low-dimensional latent code can-not well record detail information of the input, as also indi-cated by some recent 2D GAN inversion methods [52, 55].
To tackle this problem, our motivation is to further learn 3D-space high-frequency details and combine them with the coarse radiance manifolds obtained from the general encoder-based inversion of GRAM, to achieve faithful re-construction and 3D-consistent view synthesis. A straight-forward way to achieve this is to extract a high-resolution 3D voxel from the input image and combine it with the coarse radiance manifolds. However, this is prohibited by modern GPUs due to the high memory cost of the 3D voxel.
To tackle this problem, we turn to learn a high resolu-tion detail manifolds, taking the advantage of the radiance manifolds representation of GRAM, instead of learning the memory-consuming 3D voxel. We introduce a novel detail manifolds reconstructor to extract detail manifolds from the input images. It leverages manifold super-resolution [60] to predict high-resolution detail manifolds from a low resolu-tion feature voxel. This can be effectively achieved by a set of memory-efficient 2D convolution blocks. The obtained high resolution detail manifolds can still maintain strict 3D consistency due to lying in the 3D space. We also propose dedicated losses to regulate the detail manifolds via 3D pri-ors derived from the coarse radiance manifolds, to ensure reasonable novel view results.
Another contribution of our method is an improvement upon the memory and time-consuming GRAM, without which it is difficult to be integrated into our GAN inversion framework. We replace the original MLP-based radiance generator [13] in GRAM with a StyleGAN2 [26]-based tri-plane generator proposed by [9]. The efficient GRAM re-quires only 1/4 memory cost with 7Ã— speed up, without sacrificing the image generation quality and 3D consistency.
We train our method on FFHQ dataset [25] and conduct multiple experiments to demonstrate its advantages on pose control of portrait images. Once trained, GRAMInverter takes a monocular image as input and predicts its radiance manifolds representation for novel view synthesis at 3 FPS on a single GPU. The generated novel views well preserve fine details in the original image with strong 3D consistency, outperforming prior art by a large margin. We believe our method takes a solid step towards efficient 3D-aware con-tent creation for real applications. 2.