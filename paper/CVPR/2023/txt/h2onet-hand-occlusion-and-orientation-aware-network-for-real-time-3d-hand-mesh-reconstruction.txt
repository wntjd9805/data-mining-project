Abstract
Real-time 3D hand mesh reconstruction is challenging, especially when the hand is holding some object. Beyond the previous methods, we design H2ONet to fully exploit non-occluded information from multiple frames to boost the reconstruction quality. First, we decouple hand mesh recon-struction into two branches, one to exploit finger-level non-occluded information and the other to exploit global hand orientation, with lightweight structures to promote real-time inference. Second, we propose finger-level occlusion-aware feature fusion, leveraging predicted finger-level oc-clusion information as guidance to fuse finger-level infor-mation across time frames. Further, we design hand-level occlusion-aware feature fusion to fetch non-occluded infor-mation from nearby time frames. We conduct experiments on the Dex-YCB and HO3D-v2 datasets with challenging hand-object occlusion cases, manifesting that H2ONet is able to run in real-time and achieves state-of-the-art per-formance on both the hand mesh and pose precision. The code will be released on GitHub. 1.

Introduction
Estimating 3D hand meshes from RGB images is a fun-damental task useful for many applications, e.g., augmented reality [17, 52], behavior understanding [26, 44], etc. To support these applications, user experience is very impor-tant, so the reconstruction should be accurate and robust, as well as fast, i.e., real-time. Despite the promising results achieved by the recent works, it is still very challenging to simultaneously meet all the requirements, particularly when the hand is severely occluded, e.g., holding some object.
Several recent methods are proposed for 3D hand mesh reconstruction from a single RGB image [13, 15, 31, 38– 42, 45]. To alleviate the negative effect of occlusion, some try to extract occlusion-robust features by adopting the spa-Figure 1. Structural comparison between our H2ONet and previ-ous methods. We decouple 3D hand mesh reconstruction into two branches, one to reconstruct the hand mesh at canonical pose M and the other to regress the global hand orientation R, such that we can fuse finger- and hand-level occlusion-aware features from multiple frames to better exploit the non-occluded information. tial attention mechanism applied in 3D hand pose estima-tion [14, 65, 68]. When the amount of occlusion is small, focusing more on non-occluded regions can help improve the network performance. However, the performance would largely drop when the occluded regions dominate, implying that relying solely on the prior information of hand shape and pose is insufficient. Besides, the attention mechanism brings extra computation and memory overhead. Though recent methods [8, 52] adopt lightweight frameworks for real-time inference, the influence of occlusion is ignored.
On the other hand, some recent works [18,57] start to ex-plore multi-frame RGB images as input for 3D hand mesh reconstruction. SeqHAND [57] integrates LSTM as a fea-ture extractor to memorize the hand motion over consecu-tive frames. Liu et al. [35] constrain the smoothness of hand shape and pose by designing inter-frame losses. Yet, they
do not have specific designs to explicitly deal with the oc-clusion. Hasson et al. [18] leverages an optical-flow-guided strategy to promote photometric consistency. Nevertheless, the extra information is limited, as they use only two adja-cent frames. Also, though multi-frame inputs provide more information, it is non-trivial to effectively extract and fuse multi-frame features to improve the reconstruction quality.
In this paper, we present H2ONet, a Hand-Occlusion-and-Orientation-aware Network, aiming at exploiting non-occluding information from multi-frame images to recon-struct the 3D hand mesh. Our goal is to meet the require-ments of (i) effectively utilizing the inter-frame information and (ii) explicitly alleviating the interference of occlusion.
First, as the hand orientation information and hand shape information are mixed in feature space, it is hard to directly fuse features from multiple frames. To better exploit use-ful information, we decouple hand mesh reconstruction into two tasks: one for hand mesh reconstruction at the canon-ical pose and the other for hand orientation regression, as shown in Fig. 1. The key advantages are that we can better fuse multi-frame features without considering hand orien-tation differences, and it enables us to apply strategies to alleviate the ill-posed issue in estimating hand orientation.
Second, to handle self and object occlusions on the hand, we propose to exploit non-occluding information spatially across fingers and temporally across frames. For the for-mer, we design finger-level occlusion-aware feature fusion that leverages predicted finger-level occlusion probabilities to guide the adaptive fusion of per-finger features from mul-tiple frames. For the latter, we design hand-level occlusion-aware feature fusion that catches auxiliary global informa-tion over frames guided by the hand-level occlusions.
In summary, our main contributions are:
• We design the hand-occlusion-and-orientation-aware network named H2ONet with a two-branch architec-ture to efficiently and effectively exploit non-occluding information from multiple frames.
• We formulate finger-level occlusion-aware feature fu-sion and hand-level occlusion-aware feature fusion modules. The former aggregates non-occluded finger-level information from multiple frames to promote hand shape reconstruction, whereas the latter alleviates the ill-posed issue when estimating the global hand ori-entation in case the hand is temporarily occluded.
• Through qualitative and quantitative comparisons on two datasets with severe hand occlusions, we show that
H2ONet achieves state-of-the-art performance. 2.