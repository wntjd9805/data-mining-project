Abstract
Neural radiance fields (NeRF) excel at synthesizing new views given multi-view, calibrated images of a static scene.
When scenes include distractors, which are not persistent during image capture (moving objects, lighting variations, shadows), artifacts appear as view-dependent effects or
’floaters’. To cope with distractors, we advocate a form of robust estimation for NeRF training, modeling distrac-tors in training data as outliers of an optimization problem.
Our method successfully removes outliers from a scene and improves upon our baselines, on synthetic and real-world scenes. Our technique is simple to incorporate in modern
NeRF frameworks, with few hyper-parameters. It does not assume a priori knowledge of the types of distractors, and is instead focused on the optimization problem rather than pre-processing or modeling transient objects. More results at https://robustnerf.github.io/public. 1.

Introduction
The ability to understand the structure of a static 3D scene from 2D images alone is a fundamental problem is computer vision [44]. It finds applications in AR/VR for mapping virtual environments [6, 36, 61], in autonomous robotics for action planning [1], and in photogrammetry to create digital copies of real-world objects [34].
Neural fields [55] have recently revolutionized this clas-sical task, by storing 3D representations within the weights of a neural network [39]. These representations are opti-mized by back-propagating image differences. When the fields store view-dependent radiance and volumetric ren-dering is employed [21], we can capture 3D scenes with photo-realistic accuracy, and we refer to the generated rep-resentation as Neural Radiance Fields, or NeRF [25]).
Training of NeRF models generally requires a large collection of images equipped with accurate camera cali-bration, which can often be recovered via structure-from-motion [37]. Behind its simplicity, NeRF hides several as-sumptions. As models are typically trained to minimize error in RGB color space, it is of paramount importance 4Work done at Google Research.
Figure 1. NeRF assumes photometric consistency in the observed images of a scene. Violations of this assumption, as with the im-ages in the top row, yield reconstructed scenes with inconsistent content in the form of “floaters” (highlighted with ellipses). We introduce a simple technique that produces clean reconstruction by automatically ignoring distractors without explicit supervision. that images are photometrically consistent – two photos taken from the same vantage point should be identical up to noise. Unless one employs a method explicitly account-ing for it [35], one should manually hold a camera’s focus, exposure, white-balance, and ISO fixed.
However, properly configuring one’s camera is not all that is required to capture high-quality NeRFs – it is also important to avoid distractors: anything that isn’t persistent throughout the entire capture session. Distractors come in many shapes and forms, from the hard-shadows cast by the operators as they explore the scene to a pet or child casually walking within the camera’s field of view. Distractors are tedious to remove manually, as this would require pixel-by-pixel labeling. They are also tedious to detect, as typical
NeRF scenes are trained from hundreds of input images, and the types of distractors are not known a priori. If dis-tractors are ignored, the quality of the reconstruction scene suffers significantly; see Figure 1.
In a typical capture session, it is difficult to to capture multiple images of the same scene from the same viewpoint, rendering distractors challenging to model mathematically.
As such, while view-dependent effects are what give NeRF their realistic look, how can the model tell the difference between a distractor and a view-dependent effect?
Despite the challenges, the research community has de-vised several approaches to overcome this issue:
• If distractors are known to belong to a specific class (e.g., people), one can remove them with a pre-trained seman-tic segmentation model [35, 43] – this process does not generalize to “unexpected” distractors such as shadows.
• One can model distractors as per-image transient phe-nomena, and control the balance of transient/persistent modeling [23] – however, it is difficult to tune the losses that control this Pareto-optimal objective.
• One can model data in time (i.e., high-framerate video) and decompose the scene into static and dynamic (i.e., distractor) components [53] – but this clearly only applies to video rather than photo collection captures.
Conversely, we approach the problem of distractors by mod-eling them as outliers in NeRF optimization.
We analyze the aforementioned techniques through the lens of robust estimation, allowing us to understand their be-havior, and to design a method that is not only simpler to im-plement but also more effective (see Figure 1). As a result, we obtain a method that is straightforward to implement, re-quires minimal-to-no hyper-parameter tuning, and achieves state-of-the-art performance. We evaluate our method:
• quantitatively, in terms of reconstruction with syntheti-cally, yet photo-realistically, rendered data;
• qualitatively on publicly available datasets (often fine-tuned to work effectively with previous methods);
• on a new collection of natural and synthetic scenes, in-cluding those autonomously acquired by a robot, allow-ing us to demonstrate the sensitivity of previous methods to hyper-parameter tuning. 2.