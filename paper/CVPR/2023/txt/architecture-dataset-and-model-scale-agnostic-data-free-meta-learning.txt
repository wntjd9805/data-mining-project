Abstract
The goal of data-free meta-learning is to learn useful prior knowledge from a collection of pre-trained models without accessing their training data. However, existing works only solve the problem in parameter space, which (i) ignore the fruitful data knowledge contained in the pre-trained models; (ii) can not scale to large-scale pre-trained models; (iii) can only meta-learn pre-trained models with the same network architecture. To address those issues, we propose a unified framework, dubbed PURER, which con-tains: (1) ePisode cUrriculum inveRsion (ECI) during data-free meta training; and (2) invErsion calibRation following inner loop (ICFIL) during meta testing. During meta train-ing, we propose ECI to perform pseudo episode training for learning to adapt fast to new unseen tasks. Specifically, we progressively synthesize a sequence of pseudo episodes by distilling the training data from each pre-trained model.
The ECI adaptively increases the difficulty level of pseudo episodes according to the real-time feedback of the meta model. We formulate the optimization process of meta train-ing with ECI as an adversarial form in an end-to-end man-ner. During meta testing, we further propose a simple plug-and-play supplement—ICFIL—only used during meta test-ing to narrow the gap between meta training and meta test-ing task distribution. Extensive experiments in various real-world scenarios show the superior performance of ours. 1.

Introduction
Meta-learning [1, 28, 31] aims to learn useful prior knowledge (e.g., sensitive initialization) from a collection of similar tasks to facilitate the learning of new unseen tasks. Most meta-learning methods [3, 4, 7, 9, 15, 29, 30, 35, 36, 41, 42, 44] assume the access to the training and test-ing data of each task. However, this assumption is not al-ways satisfied: many individuals and institutions only re-*Corresponding authors: Li Shen and Chun Yuan
Figure 1. Episode Curriculum Inversion can improve the effi-ciency of pseudo episode training. At each episode, EI may re-peatedly synthesize the tasks already learned well, while ECI only synthesizes harder tasks not learned yet.
Figure 2. Task-distribution shift between meta training and testing.
The pseudo data distilled from pre-trained models only contains partial semantic information learned by pre-trained models. lease the pre-trained models instead of the data. This is due to data privacy, safety, or ethical issues in real-world sce-narios, making the task-specific data difficult or impossible to acquire. For example, many pre-trained models with ar-bitrary architectures are released on GitHub without train-ing data. However, when facing a new task, we need some prior knowledge learned from those pre-trained models so that the model can be adapted fast to the new task with few labeled examples. Thus, meta-learning from several pre-trained models without data becomes a critical problem, named Data-free Meta-Learning (DFML) [43].
Existing data-free meta-learning methods address the problem in the parameter space. Wang et al. [43] propose to meta-learn a black-box neural network by predicting the
model parameters given the task embedding without data, which can be generalized to unseen tasks. The predicted model parameters with the average task embedding as in-put are served as the meta initialization for meta testing.
However, this method has several drawbacks. First, they only merge the model in parameter space and ignore the underlying data knowledge that could be distilled from the pre-trained models. Second, their method can only be ap-plied to small-scale pre-trained models since they use a neu-ral network to predict the model parameters. Furthermore, their application scenarios are restricted to the case where all pre-trained models have the same architecture, limiting the real-world applicable scenarios.
In this work, we try to address all the above issues si-multaneously in a unified framework, named PURER (see
Fig. 3), which contains: (1) ePisode cUrriculum inveRsion (ECI) during data-free meta training; and (2) invErsion calibRation following inner loop (ICFIL) during meta test-ing, thus significantly expanding the application scenarios of DFML. During meta training, we propose ECI to perform pseudo episode training for learning to adapt fast to new unseen tasks. We progressively synthesize a sequence of pseudo episodes (tasks) by distilling the training data from each pre-trained model. ECI adaptively increases the dif-ficulty level of pseudo episode according to the real-time feedback of the meta model. Specifically, we first intro-duce a small learnable dataset, named dynamic dataset. We initialize the dynamic dataset as Gaussian noise and pro-gressively update it to better quality via one-step gradi-ent descent for every iteration. For each episode, we con-struct a pseudo task by first sampling a subset of labels, and then sampling corresponding pseudo support data and query data. To improve the efficiency of pseudo episode training (see Fig. 1), we introduce the curriculum mechanism to syn-thesize episodes with an increasing level of difficulty. We steer the dynamic dataset towards appropriate difficulty so that only tasks not learned yet are considered at each itera-tion, which avoids repeatedly synthesizing the tasks already learned well. We design a Gradient Switch controlled by the real-time feedback from current meta model, to synthesize harder tasks only when the meta model has learned well on most tasks sampled from current dynamic dataset (see
Fig. 3). Finally, we formulate the optimization process of meta training with ECI as an adversarial form in an end-to-end manner. We further propose a simple plug-and-play supplement—ICFIL—only used during meta testing to nar-row the gap between meta training and meta testing task distribution (see Fig. 2). Overall, our proposed PURER can solve the DFML problem using the underlying data knowl-edge regardless of the dataset, scale and architecture of pre-trained models.
Our method is architecture, dataset and model-scale ag-nostic, thus substantially expanding the application scope of DFML in real-world applications. We perform extensive experiments in various scenarios, including (i) SS: DFML with Same dataset and Same model architecture; (ii) SH:
DFML with Same dataset and Heterogeneous model ar-chitectures; (iii) MH: DFML with Multiple datasets and
Heterogeneous model architectures. For benchmarks of
SS, SH and MH on CIFAR-FS and MiniImageNet, our method achieves significant performance gains in the range of 6.92% to 17.62%, 6.31% to 27.49% and 7.39% to 11.76%, respectively.
We summarize the main contributions as three-fold:
• We propose a new orthogonal perspective with respect to existing works to solve the data-free meta-learning problem by exploring the underlying data knowledge.
Furthermore, our framework is architecture, dataset and model-scale agnostic, i.e., it can be easily applied to various real-world scenarios.
• We propose a united framework, PURER, consisting of: (i) ECI to perform pseudo episode training with an increasing level of difficulty during meta training; (ii)
ICFIL to narrow the gap between meta training and testing task distribution during meta testing.
• Our method achieves superior performance and out-performs the SOTA baselines by a large margin on var-ious benchmarks of SS, SH and MH, which shows the effectiveness of our method. 2.