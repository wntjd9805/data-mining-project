Abstract
Semi-Supervised Domain Adaptation (SSDA) involves learning to classify unseen target data with a few labeled and lots of unlabeled target data, along with many labeled source data from a related domain. Current SSDA ap-proaches usually aim at aligning the target data to the la-beled source data with feature space mapping and pseudo-label assignments. Nevertheless, such a source-oriented model can sometimes align the target data to source data of the wrong classes, degrading the classification per-formance. This paper presents a novel source-adaptive paradigm that adapts the source data to match the tar-get data. Our key idea is to view the source data as a noisily-labeled version of the ideal target data. Then, we propose an SSDA model that cleans up the label noise dy-namically with the help of a robust cleaner component de-signed from the target perspective. Since the paradigm is very different from the core ideas behind existing SSDA ap-proaches, our proposed model can be easily coupled with them to improve their performance. Empirical results on two state-of-the-art SSDA approaches demonstrate that the proposed model effectively cleans up the noise within the source labels and exhibits superior performance over those approaches across benchmark datasets. Our code is avail-able at https://github.com/chu0802/SLA. 1.

Introduction
Domain Adaptation (DA) focuses on a general machine learning scenario where training and test data may originate from two related but distinct domains: the source domain and the target domain. Many works have extensively stud-ied unsupervised DA (UDA), where labels in the target do-main cannot be accessed, from both theoretical [2, 19, 36] and algorithmic [5, 8, 15, 16, 22, 37] perspectives. Recently,
Semi-Supervised Domain Adaptation (SSDA), another DA setting that allows access to a few target labels, has received more research attention because it is a simple yet realistic setting for application needs.
The most na¨ıve strategy for SSDA, commonly known as
Figure 1. Top. Training the model with the original source labels might lead to the misalignment of the target data. Bottom. After cleaning up the noisy source labels with our SLA framework, the target data can be aligned with the correct classes.
S+T [21, 33], aims to train a model using the source data and labeled target data with a standard cross entropy loss.
This strategy often suffers from a well-known domain shift issue, which stems from the gap between different data dis-tributions. To address this issue, many state-of-the-art al-gorithms attempt to explore better use of the unlabeled tar-get data so that the target distribution can be aligned with the source distribution. Recently, several Semi-Supervised
Learning (SSL) algorithms have been applied for SSDA
[12, 21, 30] to regularize the unlabeled data, such as en-tropy minimization [6], pseudo-labeling [11,24] and consis-tency regularization [1, 24]. These classic source-oriented strategies have prevailed for a long time. However, these algorithms typically require the target data to closely match some semantically similar source data in the feature space.
Therefore, if the S+T space has been misaligned, it can be challenging to recover from the misalignment, as illustrated in Figure 1.
We take a deeper look into a specific example from the
Office-Home dataset [27] to confirm the abovementioned is-sue. Figure 2 visualizes the feature space trained by S+T us-ing t-SNE [3]. We observed that the misalignment between
True\Pred Class 7 Class 59 Class 41 Others
Class 59 38.5% 19.8% 13.5% 28.2%
Table 1. A partial confusion matrix of S+T on the 3-shot Office-Home A → C dataset with ResNet34. might still suffer from the biased feature space de-rived from S+T. To escape this predicament, we pro-pose adapting the source data to the target space by modifying the original source labels.
• We address DA as a particular case of NLL problems and present a novel source-adaptive paradigm. Our
SLA framework can be easily coupled with other ex-isting algorithms to boost their performance.
• We demonstrate the usefulness of our proposed SLA framework when coupled with state-of-the-art SSDA algorithms. The framework significantly improved ex-isting algorithms on two major benchmarks, inspiring a new direction for solving DA problems. 2.