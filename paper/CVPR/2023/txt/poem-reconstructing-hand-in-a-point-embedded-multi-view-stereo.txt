Abstract
Enable neural networks to capture 3D geometrical-aware features is essential in multi-view based vision tasks.
Previous methods usually encode the 3D information of
In contrast, we multi-view stereo into the 2D features. present a novel method, named POEM, that directly oper-ates on the 3D POints Embedded in the Multi-view stereo for reconstructing hand mesh in it. Point is a natural form of 3D information and an ideal medium for fusing fea-tures across views, as it has different projections on dif-ferent views. Our method is thus in light of a simple yet effective idea, that a complex 3D hand mesh can be rep-resented by a set of 3D points that 1) are embedded in the multi-view stereo, 2) carry features from the multi-view images, and 3) encircle the hand. To leverage the power of points, we design two operations: point-based feature fusion and cross-set point attention mechanism. Evalua-tion on three challenging multi-view datasets shows that
POEM outperforms the state-of-the-art in hand mesh re-construction. Code and models are available for research at github.com/lixiny/POEM 1.

Introduction
Hand mesh reconstruction plays a central role in the field of augmented and mixed reality, as it can not only deliver realistic experiences for the users in gaming but also sup-port applications involving teleoperation, communication, education, and fitness outside of gaming. Many significant efforts have been made for the monocular 3D hand mesh reconstruction [1, 5, 7, 9, 31, 32]. However, it still strug-gles to produce applicable results, mainly for these three (1) Depth ambiguity. Recovery of the absolute reasons. position in a monocular camera system is an ill-posed prob-lem. Hence, previous methods [9, 31, 54] only recovered the hand vertices relative to the wrist (i.e. root-relative). (2) Unknown perspectives. The shape of the hand’s 2D
†Cewu Lu is the corresponding author, the member of Qing Yuan Re-search Institute and MoE Key Lab of Artificial Intelligence, AI Institute,
Shanghai Jiao Tong University, China and Shanghai Qi Zhi institute.
Figure 1. Intersection area of N cameras’ frustum spaces. The gray dots represent the point cloud P aggregated from N frustums.
Our method: POEM, standing for the point embedded multi-view stereo, focuses on the dark area scatted with gray dots. projection is highly dependent on the camera’s perspec-tive model (i.e. camera intrinsic matrix). However, the monocular-based methods usually suggest a weak perspec-tive projection [1, 27], which is not accurate enough to re-cover the hand’s 3D structure. (3) Occlusion. The occlu-sion between the hand and its interacting objects also chal-lenges the accuracy of the reconstruction [32]. These issues limit monocular-based methods from practical application, in which the absolute and accurate position of the hand sur-face is required for interacting with our surroundings.
Our paper is thus focusing on reconstructing hands from multi-view images. Motivation comes from two aspects.
First, the issues mentioned above can be alleviated by lever-aging the geometrical consistency among multi-view im-ages. Second, the prospered multi-view hand-object track-ing setups [2, 4, 49, 55] and VR headsets bring us an urgent demand and direct application of multi-view hand recon-struction in real-time. A common practice of multi-view 3D pose estimation follows a two-stage design. It first estimates the 2D key points of the skeleton in each view and then back-project them to 3D space through several 2D-to-3D lifting methods, e.g. algebraic triangulation [17,18,39], Pic-torial Structures Model (PSM) [33, 38], 3D CNN [18, 43],
plane sweep [26], etc. However, these two-stage methods are not capable of reconstructing an animatable hand mesh that contains both skeleton and surface. It was not until re-cently that a one-stage multi-view mesh regression model was proposed [45].
How to effectively fuse the features from different im-ages is a key component in the multi-view setting. Ac-cordingly, previous methods can be categorized into three (1) Fusing in 2D. The features are directly fused types. in the 2D domain using explicit epipolar transform [17, 38] or implicit representations that encode the camera transfor-mation (i.e. camera intrinsic and extrinsic matrix) into 2D features, e.g. feature transform layer (FTL) [14, 39] and 3D position embedding (RayConv) [45]; (2) Fusing in 3D. The features are fused in a 3D voxel space via PSM [33, 38] or 3D CNNs [18, 43]; (3) Fusing via 3D-2D projection. The features are fused by first projecting the 3D keypoints’ ini-tial guess into each 2D plane and then fusing multi-view features near those 2D locations [45];
The fusion mode in type 1 is considered as holistic, since it indiscriminately fuses all the features from differ-ent views. Consequentially, it ignores the structure of the underlying hand model that we are interested in. On the contrary, the fusion mode in type 3 is considered as lo-cal. However, only the features around the 2D keypoints are hard to capture the consistent geometrical features from a global view. Besides, the 3D keypoints initial guess may not be accurate enough, resulting in the fusion being unsta-ble. The fusion mode in type 2 is not in our consideration as it tends to be computationally expensive and suffers from quantization error.
Based on the above discussion, we aim to seek a fea-ture representation and a fusion mode between type 1 and type 3 for both holistically and locally fusing the features in multi-views, and to explore a framework for robust and accurate hand mesh reconstruction. Our method is called
POEM, standing for POint Embedded Multi-view Stereo.
We draw inspiration from the Basis Point Set (BPS) [34], which bases on a simple yet effective idea that a complex 3D shape can be represented by a fixed set of points (BPS) that wraps the shape in it. If we consider the intersection of different cameras’ frustum spaces as a point cloud, and the hand’s vertices as another point cloud, then the intersected space is the basis point set for hand vertices (see Fig. 1).
Once we assign the multi-view image features to the point cloud in the intersected space, fusing image features across different views becomes fusing the point features from dif-ferent camera frustums. The advantages of this representa-tion are two-fold: (i) The hand is wrapped in a dense point cloud (set) that carries dense image features collected from different views, which is more holistic and robust than the local fusion mode in type 3. (ii) For each vertex on the hand surface, it interacts with basis points in its local neighbor-hood (i.e. k nearest neighbor), which is more selective than the holistic fusion mode in type 1.
Fig. 2 shows our model’s architecture. POEM consists of two stages. In the first stage (Sec. 3.2), POEM takes the multi-view images as input and predicts the 2D keypoints of the hand skeleton in each view. Then, the 3D keypoints are recovered by an algebraic triangulation module. In the second stage, POEM fuses the features from different views in a space embedded by points and predicts the hand mesh in this space (Sec. 3.3). The point feature on hand vertices will iteratively interact with the features of the embedded points through a cross-set attention mechanism, and the up-dated vertex features are further used by POEM to predict the vertex’s refined position (Sec. 3.3.3).
We conduct extensive experiments on three multi-view datasets for hand mesh reconstruction under the object’s oc-clusion, namely HO3D [12], DexYCB [4], and OakInk [49].
With the proposed fusion mode and attention mechanism,
POEM achieves state-of-the-art on all three datasets.
Our contributions are in three-fold:
• We investigate the multi-view pose and shape recon-struction problem from a new perspective, that is, the interaction between a target point set (i.e. mesh vertices) and a basis point set (i.e. point cloud in the camera frus-tum spaces).
• According to that, we propose an end-to-end learning framework: POEM for reconstructing hand mesh from multi-view images through a point embedded multi-view stereo. To encourage interaction between two point sets,
POEM introduces two new operations: a point-based feature fusion strategy and a cross-set point attention.
• We conduct extensive experiments to demonstrate the efficacy of the architecture in POEM. As a regression model targeting mesh reconstruction, POEM achieves significant improvement compared to the previous state-of-the-art. 2.