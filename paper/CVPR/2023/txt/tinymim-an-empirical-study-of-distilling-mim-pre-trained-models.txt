Abstract
Masked image modeling (MIM) performs strongly in pre-training large vision Transformers (ViTs). However, small models that are critical for real-world applications can-not or only marginally benefit from this pre-training ap-proach. In this paper, we explore distillation techniques to transfer the success of large MIM-based pre-trained mod-els to smaller ones. We systematically study different op-tions in the distillation framework, including distilling tar-gets, losses, input, network regularization, sequential dis-tillation, etc, revealing that: 1) Distilling token relations is more effective than CLS token- and feature-based distil-lation; 2) An intermediate layer of the teacher network as target perform better than that using the last layer when the depth of the student mismatches that of the teacher; 3) Weak regularization is preferred; etc. With these find-ings, we achieve significant fine-tuning accuracy improve-ments over the scratch MIM pre-training on ImageNet-1K classification, using all the ViT-Tiny, ViT-Small, and ViT-base models, with +4.2%/+2.4%/+1.4% gains, respectively.
Our TinyMIM model of base size achieves 52.2 mIoU in
AE20K semantic segmentation, which is +4.1 higher than the MAE baseline. Our TinyMIM model of tiny size achieves 79.6% top-1 accuracy on ImageNet-1K image classifica-tion, which sets a new record for small vision models of the same size and computation budget. This strong perfor-mance suggests an alternative way for developing small vision Transformer models, that is, by exploring better train-ing methods rather than introducing inductive biases into architectures as in most previous works. Code is available at https://github.com/OliverRensu/TinyMIM. 1.

Introduction
Masked image modeling (MIM), which masks a large portion of the image area and trains a network to recover the original signals for the masked area, has proven to be a very effective self-supervised method for pre-training vision
Transformers [2, 11, 17, 49]. Thanks to its strong fine-tuning performance, MIM has now been a main-stream pre-training
*Corresponding author: fawe@microsoft.com.
Figure 1. Comparison among TinyMIM (ours), MAE [17] and training from scratch by using ViT-T, -S and -B on ImageNet-1K.
We report top-1 accuracy. We adopt DeiT [41] when training from scratch. For the first time, we successfully perform masked image modeling pre-training for smaller ViTs.
Model
DeiT-T [41]
PVT-T [43]
CiT-T [36]
Swin [29]
EdgeViT-XS [31]
MobileViTv1-S [30]
MobileViTv3-S [42]
TinyMIM⋆-T (Ours)
Param. (M)
Flops Top-1 (%) (G) mIoU 5.5 13.0 5.5 8.8 6.4 4.9 4.8 5.8 1.3 1.9 1.3 1.2 1.1 2.0 1.8 1.3 72.2 75.1 75.3 76.9 77.5 78.4 79.3 79.6 38.0 39.8 38.5 40.4 42.1 42.7 43.1 45.0
Table 1. Comparison with state-of-the-art tiny Transformers with architecture variants. The parameters indicate the backbone pa-rameter excluding the parameters of the last classification layer in classification or the decoder in segmentation. We report top-1 accuracy on ImageNet-1K classification and mIoU on ADE20K segmentation. method for vision Transformers, and numerous follow-ups have been carried out in this research line, such as study-ing how to set decoding architectures [21], reconstruction targets [10, 32, 45, 59], etc., as well as revealing its proper-ties [46, 48, 50].
Method ViT-T ViT-S ViT-B ViT-L
Scratch
MAE
Gap 72.2 71.6
-0.6 79.9 80.6
+0.7 81.2 83.6
+2.4 82.6 85.9
+3.3
Table 2. Comparison between MAE pre-trained ViTs and ViTs trained from scratch by using ViT-T, -S, -B and -L on ImageNet-1K. We adopt DeiT when training from scratch. We report top-1 accuracy. As model size shrinks, the superiority of MAE gradually vanishes. MAE even hurts the performance of ViT-T.
However, as shown in Table 2, MIM pre-training [17] mainly effects for relatively large models. When the model size is as small as ViT-Tiny (5 million parameters), which is critical for real-world applications, MIM pre-training can even hurt the fine-tuning accuracy on ImageNet-1K classifi-cation. In fact, the accuracy drops by -0.6 compared to the counterpart trained from scratch. This raises a question: can small models also benefit from MIM pre-training, and how can this be achieved?
In addition, the existing study on small vision Transform-ers mainly focus on introducing certain inductive bias into architecture design [6, 22, 30, 31]. The additional architec-tural inductive biases facilitate optimization yet limit the expressive capacity. It’s natural to ask whether we can boost plain small vision Transformers to perform just as well.
In this work, we present TinyMIM, which answers the above questions. Instead of directly training small ViT mod-els using a MIM pretext task, TinyMIM uses distillation technology [20] to transfer the knowledge of larger MIM pre-trained models to smaller ones. Distillation endows the nice properties of larger MIM pre-trained models to smaller ones while avoiding solving a “too” difficult MIM task. Not-ing that knowledge distillation has been well developed, especially for supervised models [15], our main work is to systematically study for the first time the effects of different design options in a distillation framework when using MIM pre-trained models as teachers. Specifically, we consider dis-tillation targets, data augmentation, network regularization, auxiliary losses, macro distillation strategy, etc., and draw several useful findings:
• Distillation targets. There are two main findings re-lated to distillation targets: 1) Distilling token relations is more effective than distilling the CLS token and fea-ture maps. 2) Using intermediate layers as the target may perform better than using the last layer, and the optimal target layer for different down-stream tasks, e.g., classification and segmentation, can be different.
• Data and network regularization. Weak augmentation and regularization is preferred: 1) The performance of using a masked image is worse than using the original image; 2) Relatively small drop path rate (0 for teacher and 0.1 for student) performs best.
• auxiliary losses. We find that an auxiliary MIM loss does not improve fine-tuning accuracy.
• Macro distillation strategy. We find that using a se-quential distillation strategy, i.e., “ViT-B → ViT-S →
ViT-T”, performs better than that distilling directly from
ViT-B to ViT-T.
By selecting the best framework options, we achieve sig-nificant fine-tuning accuracy improvements over the direct
MIM pre-training on ImageNet-1K classification, using ViT models of different sizes, as shown in Figure 1. Specifi-cally, the gains of TinyMIM on the ViT-Tiny, ViT-Small, and
ViT-base models are +4.2%/+2.4%/+1.4%, respectively.
In particular, our TinyMIM⋆-T model with knowledge distillation during finetune-tuning achieves a top-1 accuracy of 79.6% on ImageNet-1K classification (see Table 1), which performs better than all previous works that develop small vision Transformer models by introducing architectural in-ductive biases or smaller feature resolutions. It sets a new accuracy record using similar model size and computation budget. On ADE20K semantic segmentation, TinyMIM⋆-T achieves 45.0 mIoU, which is +1.9 higher than the second best method, MobileViTv3-S [42]. The strong fine-tuning accuracy by TinyMIM⋆-T suggests an alternative way for developing small vision Transformer models, that is, by exploring better training methods rather than introducing inductive biases into architectures as most previous works have done. 2.