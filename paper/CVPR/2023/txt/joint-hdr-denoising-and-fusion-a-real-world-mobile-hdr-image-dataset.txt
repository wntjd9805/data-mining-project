Abstract
Mobile phones have become a ubiquitous and indis-pensable photographing device in our daily life, while the small aperture and sensor size make mobile phones more susceptible to noise and over-saturation, resulting in low dynamic range (LDR) and low image quality.
It is thus crucial to develop high dynamic range (HDR) imaging techniques for mobile phones. Unfortunately, the existing
HDR image datasets are mostly constructed by DSLR cam-eras in daytime, limiting their applicability to the study of
HDR imaging for mobile phones.
In this work, we de-velop, for the first time to our best knowledge, an HDR image dataset by using mobile phone cameras, namely
Mobile-HDR dataset. Specifically, we utilize three mo-bile phone cameras to collect paired LDR-HDR images in the raw image domain, covering both daytime and night-time scenes with different noise levels. We then propose a transformer based model with a pyramid cross-attention alignment module to aggregate highly correlated features from different exposure frames to perform joint HDR de-noising and fusion. Experiments validate the advantages of our dataset and our method on mobile HDR imaging.
Dataset and codes are available at https://github. com/shuaizhengliu/Joint-HDRDN . 1.

Introduction
With the rapid development of mobile communication techniques and digital imaging sensors, mobile phones have surpassed DSLR cameras and become the most prevalent device for photography in our daily life. Nonetheless, due to the low dynamic range (LDR) of mobile phone sensors [7], the captured images may lose details in dark and bright regions under challenging lighting conditions. Therefore, high dynamic range (HDR) imaging [3] is critical for im-proving the quality of mobile phone photography.
*Corresponding author. This work is supported by the Hong Kong RGC
RIF grant (R5001-18) and the PolyU-OPPO Joint Innovation Lab.
Actually, HDR imaging has been a long standing re-search topic in computational photography, even for DSLR cameras. An effective and commonly used way to construct an HDR image is to fuse a stack of LDR frames with dif-ferent exposure levels. If the multiple LDR frames can be well aligned (e.g., in static scenes), they can be easily fused to generate the HDR image [3, 23]. Unfortunately, in dy-namic scenes where there exist camera shaking and/or ob-ject motion, the fused HDR image may introduce ghost ar-tifacts caused by inaccurate alignment [45]. Some deghost-ing methods have been proposed to reject pixels which can be hardly registered [12, 16]. However, precisely detecting moving pixels is challenging and rejecting too many pixels will sacrifice useful information for HDR fusion.
In the past decade, deep learning [15] has demonstrated its powerful capability to learn image priors from a rich amount of data [4]. Unfortunately, the development of deep models for HDR imaging is relatively slow, mainly due to the lack of suitable training datasets. Kalantari et al. [10] built the first dataset with LDR-HDR image pairs by DSLR cameras in daytime. Benefiting from this dataset, many deep learning algorithms have been proposed for HDR imaging. Some works [10] employ the convolu-tional neural network (CNN) for fusion after aligning mul-tiple frames with optical flow [18], which is however unreli-able under occlusion and large motions. Subsequent works resort to employing various networks to directly reconstruct the HDR image from LDR frames. Liu et al. [19] developed a deformable convolution based module to align the features of input frames. Yan et al. [40] proposed a spatial attention mechanism to suppress undesired features and employed a dilated convolution network [42] for frame fusion. Follow-ing this spatial attention mechanism, some fusion networks have been developed with larger receptive fields, such as non-local networks [41] and Transformer networks [21].
Though the dataset developed in [10] has largely facili-tated the research of deep learning on HDR imaging, it is not well suited for the investigation of HDR imaging tech-niques for mobile phone cameras. Firstly, due to the small aperture and sensor size, images captured by mobile phones
are more susceptible to noise than DSLR cameras, espe-cially in nighttime. However, the images in dataset [10] are generally very clean since they are collected by DSLR cameras in daytime. Compared with DSLR cameras, the normal exposed frame of mobile phone cameras contains stronger noise, which should be reduced by fusing with other frames. Secondly, mobile phone cameras often has fewer recording bits (12 bit) than DSLR (14 bit), resulting in larger overexposure areas in the reference frame. There-fore, mobile HDR imaging is a more challenging problem, and new dataset and new solutions are demanded.
To address the above limitations of existing HDR datasets and facilitate the research on real-world mobile
HDR imaging, we establish a new HDR dataset, namely
Mobile-HDR, by using mobile phone cameras. Specifically, we utilize three mobile phones to collect LDR-HDR im-age pairs in raw image domain, covering both daytime and nighttime scenes with different noise levels. In order to ob-tain high-quality ground truth of HDR images, we first col-lect noise-free LDR images under each exposure by multi-frame averaging, and then synthesize the ground truth HDR image by fusing the generated clean LDR frames. For dy-namic scenes with object motion, we follow [10] to first capture multiple exposed frames from static scenes to syn-thesize the ground truth HDR images, and then replace the non-reference frames with the images captured in dynamic scenes as input. To our best knowledge, this is the first mo-bile HDR dataset with paired training data.
With the established dataset, we propose a new trans-former based model for joint HDR denoising and fusion.
To enhance denoising and achieve alignment, we design a pyramid cross-attention module to implicitly align and fuse input features. The cross-attention operation enables searching and aggregating highly correlated features from different frames, while the pyramid structure facilitates the feature alignment under severe noise, large overexposure and large motion. A transformer module is then applied to fuse the aligned features for HDR image recovery.
The contributions of our work can be summarized as fol-lows. First, we build the first mobile HDR dataset with
LDR-HDR image pairs under various scenes. Second, we propose a cross-attention based alignment module to per-form effective joint HDR denoising and fusion. Third, we perform extensive experiment to validate the advantages of our dataset and model. Our work provides a new platform for researchers to investigate and evaluate real-world mo-bile HDR imaging techniques. 2.