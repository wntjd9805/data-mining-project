Abstract
Being data-driven is one of the most iconic properties of deep learning algorithms. The birth of ImageNet [24] drives a remarkable trend of ‘learning from large-scale data’ in computer vision. Pretraining on ImageNet to ob-tain rich universal representations has been manifested to benefit various 2D visual tasks, and becomes a standard in 2D vision. However, due to the laborious collection of real-world 3D data, there is yet no generic dataset serv-ing as a counterpart of ImageNet in 3D vision, thus how such a dataset can impact the 3D community is unraveled.
To remedy this defect, we introduce MVImgNet, a large-scale dataset of multi-view images, which is highly conve-nient to gain by shooting videos of real-world objects in hu-man daily life. It contains 6.5 million frames from 219,188 videos crossing objects from 238 classes, with rich annota-tions of object masks, camera parameters, and point clouds.
The multi-view attribute endows our dataset with 3D-aware signals, making it a soft bridge between 2D and 3D vision.
We conduct pilot studies for probing the potential of
MVImgNet on a variety of 3D and 2D visual tasks, includ-ing radiance field reconstruction, multi-view stereo, and view-consistent image understanding, where MVImgNet demonstrates promising performance, remaining lots of possibilities for future explorations.
Besides, via dense reconstruction on MVImgNet, a 3D object point cloud dataset is derived, called MVPNet, cov-ering 87,200 samples from 150 categories, with the class label on each point cloud. Experiments show that MVP-Net can benefit the real-world 3D object classification while posing new challenges to point cloud understanding.
MVImgNet and MVPNet will be public, hoping to inspire the broader vision community.
1.

Introduction
Being data-driven, also known as data-hungry, is one of the most important attributes of deep learning algorithms.
By training on large-scale datasets, deep neural networks
In the past few are able to extract rich representations. years, the computer vision community has witnessed the bloom of such ‘learning from data’ regime [43, 44, 55], af-ter the birth of ImageNet [24] – the pioneer of large-scale real-world image datasets. Notably, pretraining on Ima-geNet is well-proven to boost the model performance when transferring the pretrained weights into not only high-level
[34, 41, 42, 60, 61] but also low-level visual tasks [15, 57], and becomes a de-facto standard in 2D. Recently, various 3D datasets [5, 9, 11, 23, 33, 96, 104] are produced to facili-tate 3D visual applications.
However, due to the non-trivial scanning and labori-ous labeling of real-world 3D data (commonly organized in point clouds or meshes), existing 3D datasets are ei-ther synthetic or their scales are not comparable with Ima-geNet [24]. Consequently, unlike in 2D vision where mod-els are usually pretrained on ImageNet to gain universal rep-resentation or commonsense knowledge, most of the current methods in 3D area are directly trained and evaluated on particular datasets for solving specific 3D visual tasks (e.g.,
NeRF dataset [64] and ShapeNet [11] for novel view syn-thesis, ModelNet [96] and ScanObjectNN [88] for object classification, KITTI [33] and ScanNet [23] for scene un-derstanding). Here, two crucial and successive issues can be induced: (1) There is still no generic dataset in 3D vision, as a counterpart of ImageNet in 2D. (2) What benefit such a dataset can endow to 3D community is yet unknown. In this paper, we focus on investigating these two problems and set two corresponding targets: Build the primary dataset, then explore its effect through experiments.
Milestone 1 – Dataset:
For a clearer picture of the first goal, we start by carefully revisiting existing 3D datasets as well as ImageNet [24]. i) 3D synthetic datasets [11,96] provide rich 3D CAD mod-els. However, they lack real-world cues (e.g., context, oc-clusions, noises), which are indispensable for model robust-ness in practical applications. ScanObjectNN [88] extracts real-world 3D objects from indoor scene data, but is limited in scale. For 3D scene-level dataset [5, 10, 23, 33, 37, 81], their scales are still constrained by the laborious scanning and labeling (e.g., millions of points per scene). Addition-ally, they contain specific inner-domain knowledge such as a particularly intricate indoor room or outdoor driving con-figurations, making it hard for general transfer learning. ii) Although ImageNet [24] contains the most comprehen-sive real-world objects, it only describes a 2D world that misses 3D-aware signals. Since humans live in a 3D world, 3D consciousness is vitally important for realizing human-like intelligence and solving real-life visual problems.
Based on the above review, our dataset is created from a new insight – multi-view images, as a soft bridge be-tween 2D and 3D. It lies several benefits to remedying the aforementioned defects. Such data can be easily gained in considerable sizes via shooting an object around different views on common mobile devices with cameras (e.g., smart-phones), which can be collected by crowd-sourcing in real world. Moreover, the multi-view constraint can bring nat-ural 3D visual signals (later experiments show that this not only benefits 3D tasks but also 2D image understanding).
To this end, we build MVImgNet, containing 6.5 million frames from 219,188 videos crossing real-life objects from 238 classes, with rich annotations of object masks, camera parameters, and point clouds. You may take a glance at our
MVImgNet from Fig. 1.
Milestone 2 – Experimental Exploration:
Now facing the second goal of this paper, we attempt to probe the power of our dataset by conducting some pilot experiments. Leveraging the multi-view nature of the data, we start by focusing on the view-based 3D reconstruction task and demonstrate that pretraining on MVImgNet can not only benefit the generalization ability of NeRF (Sec. 4.1), but also data-efficient multi-view stereo (Sec. 4.2). More-over, for image understanding, although humans can easily recognize one object from different viewpoints, deep learn-ing models can hardly do that robustly [26,38]. Considering
MVImgNet provides numerous images of a particular ob-ject from different viewpoints, we verify that MVImgNet-pretrained models are endowed with decent view consis-tency in general image classification (supervised learning in Sec. 5.1, self-supervised contrastive learning in Sec. 5.2) and salient object detection (Sec. 5.3).
Bonus – A New 3D Point Cloud Dataset – MVPNet:
Through dense reconstruction on MVImgNet, a new 3D object point cloud dataset is derived, called MVPNet, which contains 87,200 point clouds with 150 categories, with the class label on each point cloud (see Fig. 7). Exper-iments show that MVPNet not only benefits the real-world 3D object classification task but also poses new challenges and prospects to point cloud understanding (Sec. 6).
MVImgNet and MVPNet will be public, hoping to in-spire the broader vision community. 2.