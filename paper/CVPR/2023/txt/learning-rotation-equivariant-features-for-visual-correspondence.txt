Abstract
Extracting discriminative local features that are invari-ant to imaging variations is an integral part of establish-ing correspondences between images.
In this work, we introduce a self-supervised learning framework to extract discriminative rotation-invariant descriptors using group-equivariant CNNs. Thanks to employing group-equivariant
CNNs, our method effectively learns to obtain rotation-equivariant features and their orientations explicitly, with-out having to perform sophisticated data augmentations.
The resultant features and their orientations are further pro-cessed by group aligning, a novel invariant mapping tech-nique that shifts the group-equivariant features by their ori-entations along the group dimension. Our group align-ing technique achieves rotation-invariance without any col-lapse of the group dimension and thus eschews loss of dis-criminability. The proposed method is trained end-to-end in a self-supervised manner, where we use an orientation alignment loss for the orientation estimation and a con-trastive descriptor loss for robust local descriptors to ge-ometric/photometric variations. Our method demonstrates state-of-the-art matching accuracy among existing rotation-invariant descriptors under varying rotation and also shows competitive results when transferred to the task of keypoint matching and camera pose estimation. 1.

Introduction
Extracting local descriptors is an essential step for vi-sual correspondence across images, which is used for a wide range of computer vision problems such as visual lo-calization [29, 47, 48], simultaneous localization and map-ping [7, 8, 39], and 3D reconstruction [1, 16, 17, 49, 66]. To establish reliable visual correspondences, the properties of invariance and discriminativeness are required for local de-scriptors; the descriptors need to be invariant to geomet-ric/photometric variations of images while being discrimi-native enough to distinguish true matches from false ones.
Since the remarkable success of deep learning for visual recognition, deep neural networks have also been adopted to learn local descriptors, showing enhanced performances on visual correspondence [44, 45, 64]. Learning rotation-invariant local descriptors, however, remains challenging; the classical techiniques [11, 27, 46] for rotation-invariant descriptors, which are used for shallow gradient-based fea-ture maps, cannot be applied to feature maps from stan-dard deep neural networks, in which rotation of input in-duces unpredictable feature variations. Achieving rotation invariance without sacriﬁcing disriminativeness is particu-larly important for local descriptors as rotation is one of the most frequent imaging variations in reality.
In this work, we propose a self-supervised approach to obtain rotation-invariant and discriminative local descrip-tors by leveraging rotation-equivariant CNNs. First, we use group-equivariant CNNs [60] to jointly extract rotation-equivariant local features and their orientations from an im-age. To extract reliable orientations, we use an orientation alignment loss [21, 23, 63], which trains the network to pre-dict the dominant orientation robustly against other imag-ing variations, including illumination or viewpoint changes.
Using group-equivariant CNNs enables the local features to be empowered with explicitly encoded rotation equiv-ariance without having to perform rigorous data augmen-tations [58, 60]. Second, to obtain discriminative rotation-invariant descriptors from rotation-equivariant features, we propose group-aligning that shifts the group-equivariant features by their dominant orientation along their group dimension. Conventional methods to yield invariant fea-tures from group-equivariant features collapse the group di-mension by group-pooling, e.g., max-pooling or bilinear-pooling [26], resulting in a drop in feature discriminabil-ity and quality.
In contrast, our group-aligning preserves the group dimension, achieving rotation-invariance while eschewing loss of discriminability. Furthermore, by pre-serving the group dimension, we can obtain multiple de-scriptors by performing group-aligning using multiple ori-entation candidates, which improves the matching perfor-mance by compensating for potential errors in dominant orientation prediction. Finally, we evaluate our rotation-invariant descriptors against existing local descriptors, and
our group-aligning scheme against group-pooling methods on various image matching benchmarks to demonstrate the efﬁcacy of our method.
The contribution of our paper is fourfold:
•
•
•
•
We propose to extract discriminative rotation-invariant local descriptors to tackle the task of visual correspon-dence by utilizing rotation-equivariant CNNs.
We propose group-aligning, a method to shift a group-equivariant descriptor in the group dimension by its dominant orientation to obtain a rotation-invariant de-scriptor without having to collapse the group informa-tion to preserve feature discriminability.
We use self-supervisory losses of orientation align-ment loss for orientation estimation, and a contrastive descriptor loss for robust local descriptor extraction.
We demonstrate state-of-the-art performances under varying rotations on the Roto-360 dataset and show competitive transferability on the HPatches dataset [2] and the MVS dataset [53]. 2.