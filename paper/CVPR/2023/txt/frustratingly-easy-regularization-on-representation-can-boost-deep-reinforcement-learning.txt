Abstract
Deep reinforcement learning (DRL) gives the promise that an agent learns good policy from high-dimensional information, whereas representation learning removes ir-relevant and redundant information and retains pertinent information. In this work, we demonstrate that the learned representation of the 𝑄-network and its target 𝑄-network should, in theory, satisfy a favorable distinguishable repre-sentation property. Specifically, there exists an upper bound on the representation similarity of the value functions of two adjacent time steps in a typical DRL setting. However, through illustrative experiments, we show that the learned
DRL agent may violate this property and lead to a sub-optimal policy. Therefore, we propose a simple yet effective regularizer called Policy Evaluation with Easy Regulariza-tion on Representation (PEER), which aims to maintain the distinguishable representation property via explicit regu-larization on internal representations. And we provide the convergence rate guarantee of PEER. Implementing PEER requires only one line of code. Our experiments demonstrate that incorporating PEER into DRL can significantly improve performance and sample efficiency. Comprehensive experi-ments show that PEER achieves state-of-the-art performance on all 4 environments on PyBullet, 9 out of 12 tasks on DM-Control, and 19 out of 26 games on Atari. To the best of our knowledge, PEER is the first work to study the inherent rep-resentation property of 𝑄-network and its target. Our code is available at https://sites.google.com/view/peer-cvpr2023/. 1.

Introduction
Deep reinforcement learning (DRL) leverages the func-tion approximation abilities of deep neural networks (DNN) and the credit assignment capabilities of RL to enable agents to perform complex control tasks using high-dimensional observations such as image pixels and sensor information
[1, 2, 3, 4]. DNNs are used to parameterize the policy and value functions, but this requires the removal of irrelevant and redundant information while retaining pertinent informa-tion, which is the task of representation learning. As a result, representation learning has been the focus of attention for researchers in the field [5, 6, 7, 8, 9, 10, 11, 12, 13]. In this paper, we investigate the inherent representation properties of DRL.
The action-value function is a measure of the quality of taking an action in a given state. In DRL, this function is approximated by the action-value network or 𝑄-network.
To enhance the training stability of the DRL agent, Mnih et al. [2] introduced a target network, which computes the target value with the frozen network parameters. The weights of a target network are either periodically replicated from learning 𝑄-network, or exponentially averaged over time steps. Despite the crucial role played by the target network in DRL, previous studies[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24] have not considered the representation property of the target network. In this work, we investigate the inherent representation property of the
𝑄-network. Following the commonly used definition of rep-resentation of 𝑄-network [17, 25, 26], the 𝑄-network can be separated into a nonlinear encoder and a linear layer, with the representation being the output of the nonlinear encoder. By employing this decomposition, we reformulate the Bellman equation [27] from the perspective of representation of 𝑄-network and its target. We then analyze this formulation and demonstrate theoretically that a favorable distinguishable representation property exists between the representation of
𝑄-network and that of its target. Specifically, there exists an upper bound on the representation similarity of the value functions of two adjacent time steps in a typical DRL setting, which differs from previous work.
We subsequently conduct experimental verification to investigate whether agents can maintain the favorable dis-tinguishable representation property. To this end, we choose two prominent DRL algorithms, TD3 [28] and CURL [6] (without/with explicit representation learning techniques).
The experimental results indicate that the TD3 agent indeed maintains the distinguishable representation property, which is a positive sign for its performance. However, the CURL agent fails to preserve this property, which can potentially have negative effects on the model’s overall performance.
These theoretical and experimental findings motivate us to propose a simple yet effective regularizer, named Policy
Evaluation with Easy Regularization on Representation (PEER). PEER aims to ensure that the agent maintains the distinguishable representation property via explicit regular-ization on the 𝑄-network’s internal representations. Specifi-cally, PEER regularizes the policy evaluation phase by push-ing the representation of the 𝑄-network away from its target.
Implementing PEER requires only one line of code. Addi-tionally, we provide a theoretical guarantee for the conver-gence of PEER.
We evaluate the effectiveness of PEER by combining it with three representative DRL methods TD3 [28], CURL [6], and DrQ [9]. The experiments show that PEER effectively maintains the distinguishable representation property in both state-based PyBullet [29] and pixel-based DMCon-trol [30] suites. Additionally, comprehensive experiments demonstrate that PEER outperforms compared algorithms on four tested suites PyBullet, MuJoCo [31], DMControl, and Atari [32]. Specifically, PEER achieves state-of-the-art performance on 4 out of 4 environments on PyBullet, 9 out of 12 tasks on DMControl, and 19 out of 26 games on Atari. Moreover, our results also reveal that combining algorithms (e.g., TD3, CURL, DrQ) with the PEER loss outperforms their respective backbone methods. This ob-servation suggests that the performance of DRL algorithms may be negatively impacted if the favorable distinguishable representation property is not maintained. The results also demonstrate that the PEER loss is orthogonal to existing representation learning methods in DRL.
Our contributions are summarized as follows. (i) We the-oretically demonstrate the existence of a favorable property, distinguishable representation property, between the repre-sentation of 𝑄-network and its target. (ii) The experiments show that learned DRL agents may violate such a property, possibly leading to sub-optimal policy. To address this issue, we propose an easy-to-implement and effective regularizer
PEER that ensures that the property is maintained. To the best of our knowledge, the PEER loss is the first work to study the inherent representation property of 𝑄-network and its target and be leveraged to boost DRL. (iii) In addition, we provide the convergence rate guarantee of PEER. (iv) To demonstrate the effectiveness of PEER, we perform com-prehensive experiments on four commonly used RL suits
PyBullet, MuJoCo, DMControl, and Atari suites. The empir-ical results show that PEER can dramatically boost state-of-the-art representation learning DRL methods. 2. Preliminaries
DRL aims to optimize the policy through return, which is defined as 𝑅𝑡 = (cid:205)𝑇
𝑖=𝑡 𝛾𝑖−𝑡𝑟 (𝑠𝑖, 𝑎𝑖).
𝑄-network and its target. Action value function
𝑄 𝜋 (𝑠, 𝑎) represents the quality of a specific action 𝑎 in a state 𝑠. Formally, the action value (Q) function is defined as
𝑄 𝜋 (𝑠, 𝑎) = E𝜏∼ 𝜋, 𝑝 [𝑅𝜏 |𝑠0 = 𝑠, 𝑎0 = 𝑎], (1)
𝜏 a is trajectory state-action where sequence (𝑠0, 𝑎0, 𝑠1, 𝑎1, 𝑠2, 𝑎2 · · · ) induced by policy 𝜋 and transition probability function 𝑝. A four-tuple (𝑠𝑡 , 𝑎𝑡 , 𝑟𝑡 , 𝑠𝑡+1) is called a transition. The 𝑄 value can be recursively computed by
Bellman equation [27]
𝑄 𝜋 (𝑠, 𝑎) = 𝑟 (𝑠, 𝑎) + 𝛾E𝑠′,𝑎′ [𝑄 𝜋 (𝑠′, 𝑎′)], (2) where 𝑠′ ∼ 𝑝(·|𝑠, 𝑎) and 𝑎′ ∼ 𝜋(𝑠′). The process of eval-uating value function is known as the policy evaluation phase. To stabilize the training of DRL, Mnih et al. [2] intro-duced a target network to update the learning network with
𝜃′ ← 𝜂𝜃 + (1 − 𝜂)𝜃′, where 𝜂 is a small constant controlling the update scale. 𝜃 is the parameters of the 𝑄-network. And
𝜃′ denotes the parameters of the target network.
Representation of 𝑄-network. We consider a multi-layer neural network representing the Q function parameterized by Θ. Let Θ𝑖 represent the parameters of 𝑖−th layer, Θ−1 represent the parameters of the last layer, and Θ+ represent the parameters of the neural networks except for those of the last layer. The representation Φ of the 𝑄-network is defined as
𝑄(𝑠, 𝑎; Θ) = ⟨Φ(𝑠, 𝑎; Θ+), Θ−1⟩ .
One intuitive way to comprehend the representation of the
𝑄-network is to view the network as composed of a non-linear encoder and a linear layer. The representation of the
𝑄-network is the output of the encoder [17, 25, 26, 33, 34]. (3) 3. Method
In this section, we start with a theoretical analysis of the 𝑄-network and demonstrate the existence of the dis-tinguishable representation property. The preliminary ex-periments (fig. 1) reveal the desirable property may be violated by agents learned with existing methods. To en-sure agents maintain such property and therefore allevi-ate the potential negative impact on model performance, we propose Policy Evaluation with Easy Regularization on
Representation (PEER), a simple yet effective regularization loss on the representation of the Q-/target network. Finally, we employ a toy example to demonstrate the efficacy of the proposed PEER. We defer all the proofs to Appendix section 7.
Figure 1. Distinguishable representation discrepancy (DRD) of TD3, CURL, and PEER agents on PyBullet and DMControl suites. TD3 agent does enjoy the distinguishable representation property. But the CURL agent does not satisfy the distinguishable representation property, which might negatively affect the model performance (see table 2). Not only does PEER enjoy the distinguishable representation property on state-based inputs environment bullets, but also in the pixel-based environment DMControl. The shaded area stands for a standard deviation. 3.1. Theoretical analysis sentation discrepancy (DRD) as
First of all, we theoretically examine the property that the internal representations of a Q-network and its target should satisfy under assumption 3.1. Specifically, we show that the similarity of internal representations among two adjacent action-state pairs must be below a constant value, determined by both the neural network and reward function.
Assumption 3.1. The 𝑙2-norm of the representation is uni-formly bounded by the square of some positive constant 𝐺, i.e. ∥Φ(𝑋; Θ+)∥2 ≤ 𝐺2 for any 𝑋 ∈ S × A and network weights Θ.
Theorem 3.2 (Distinguishable Representation Property).
The similarity (defined as inner product ⟨·, ·⟩) between nor-malized representations Φ(𝑠, 𝑎; Θ+) of the 𝑄-network and
E𝑠′,𝑎′ Φ(𝑠′, 𝑎′; Θ′
+) satisfies
⟨Φ(𝑠, 𝑎; Θ+), E𝑠′,𝑎′ Φ(𝑠′, 𝑎′; Θ′
+)⟩ ≤ 1
𝛾
−
𝑟 (𝑠, 𝑎)2 2∥Θ−1 ∥2
, (4) where 𝑠, 𝑎 and Θ+ are state, action, and parameters of the
𝑄-network except for those of the last layer. While 𝑠′, 𝑎′, Θ′
+ are the state, action at the next time step, and parameters of the target 𝑄-network except for those of the last layer. And
Θ−1 is the parameters of the last layer of 𝑄-network.
Intuitively, theorem 3.2 indicates that the internal repre-sentations of different action-state pairs should be distin-guishable so that the model is able to better pick the right action from action space. We define distinguishable repre-DRD = ⟨Φ(𝑠, 𝑎; Θ+), E𝑠′,𝑎′ Φ(𝑠′, 𝑎′; Θ′
+)⟩− (cid:0) 1
𝛾
−
𝑟 (𝑠, 𝑎)2 2∥Θ−1 ∥2 (cid:1). (5)
We determine whether the internal representations of the
Q-network and its target satisfy the distinguishable rep-resentation property by examining the value of DRD. If
DRD ≤ 0, then the property is satisfied; otherwise, it is not. Then, we evaluate whether this property is maintained by agents learned with two representative DRL methods, namely TD3 [28] and CURL [6] (without/with explicit repre-sentation learning techniques). The results are shown in fig. 1.
From the results, we see that the TD3 agent does preserve the distinguishable representation property. While the CURL agent using explicit representation learning techniques does not satisfy the distinguishable representation property. This violation of the property could have a detrimental impact on agent performance (see section 4.2). These theoretical and experimental results motivate us to propose the following
PEER regularization loss. This loss ensures that the learned model satisfies the distinguishable representation property through explicit regularization on representations. 3.2. PEER
Specifically, the PEER loss is defined as
L PEER(Θ) = ⟨Φ(𝑠, 𝑎; Θ+), E𝑠′,𝑎′ (cid:2)Φ(𝑠′, 𝑎′; Θ′
+)(cid:3)⟩, (6) where the 𝑄-network takes the current state and action as inputs, and the inputs to the target network are the state and action at the next time step. This simple loss can be readily
Theorem 3.3 (One-step Approximation Error of the PEER
Update). Suppose assumption 3.1 hold, let F ⊂ B (S ×
A) be a class of measurable function on S × A that are bounded by 𝑉max = 𝑅max/(1 − 𝛾), and let 𝜎 be a probability distribution on S × A. Also, let {(𝑆𝑖, 𝐴𝑖)}𝑖 ∈ [𝑛] be 𝑛 i.i.d. random variables in S × A following 𝜎. For each 𝑖 ∈ [𝑛], let 𝑅𝑖 and 𝑆𝑖 be the reward and the next state corresponding to (𝑠𝑖, 𝑎𝑖). In addition, for 𝑄 ∈ F , we define 𝑌𝑖 = 𝑅𝑖 + 𝛾 ·
𝑖, 𝑎). Based on {(𝑋𝑖, 𝐴𝑖, 𝑌𝑖)}𝑖 ∈ [𝑛], we define max𝑎∈ A 𝑄(𝑆′
ˆ𝑂 as the solution to the lease-square with regularization problem,
𝑛
∑︁
[ 𝑓 (𝑆𝑖, 𝐴𝑖)−𝑌𝑖]2+𝛽⟨Φ(𝑠, 𝑎; Θ), EΦ𝑠′,𝑎′ (𝑠′, 𝑎′; Θ′)⟩. 1
𝑛 min
𝑓 ∈ F
𝑖=1 (8)
Meanwhile, for any 𝛿 > 0, let N (𝛿, F , ∥·∥∞) be the minimal
𝛿-covering set of F with respect to 𝑙∞-norm, and we denote by 𝑁 𝛿 its cardinality. Then for any 𝜖 ∈ (0, 1] and any 𝛿 > 0, we have
∥ ˆ𝑂−𝑇𝑄∥2
𝜎 ≤ (1+𝜖)2·𝜔(F )+𝐶·𝑉 2 max/(𝑛·𝜖)+𝐶′·𝑉max·𝛿+2𝛽·𝐺2, (9) where 𝐶 and 𝐶′ are two absolute constants and are defined as
𝜔(F ) = sup
𝑔∈ F inf
𝑓 ∈ F
∥ 𝑓 − 𝑇 𝑔∥ 𝜎. (10)
This result suggests the convergence rate of the previous result without PEER is maintained while PEER only adds a constant term 2𝛽 · 𝐺2. 3.3. A toy example
We provide a toy example of a grid world (fig. 3a) to intuitively illustrate the functionality of PEER. In the grid world, the red state is the only state with a reward. We are interested in the Q values of the two gray states 𝑆1 and 𝑆2.
Ideally, the policy is supposed to differentiate between those two adjacent states and accurately determine that 𝑆1 has a higher value. We show the similarity between the represen-tation of the 𝑄-network and its target and the difference between the maximum Q value of two adjacent states in fig. 3. As shown in the fig. 3b, PEER is able to better dis-tinguish the representation of the 𝑄-network and its target compared to DQN. Thus, PEER is better equipped to differ-entiate between the maximum Q values of the two nearby states 𝑆1 and 𝑆2 (fig. 3c). Consequently, PEER yields a better policy (fig. 3d). 4. Experiments
We perform comprehensive experiments to thoroughly assess PEER. Specifically, we evaluate (i) performance by measuring average returns on multiple suites when combined with different backbone algorithms; (ii) sample efficiency
Figure 2. How the PEER loss is computed. The encoder is a nonlin-ear operator, and the state action pairs generate the representation
Φ through the encoder and then the action value through a linear layer. PEER regularizes the policy evaluation phase by differing the representation Φ of the 𝑄-network from that of its target. LPE is a form of policy evaluation loss. And 𝛽 is a small positive constant, controlling the magnitude of the regularization effectiveness. incorporated with the optimization objective of standard policy evaluation methods [2, 4, 35], leading to
L (Θ) = LPE (Θ) + 𝛽L PEER(Θ), (7) where hyper-parameter 𝛽 controls the magnitude of the regu-larization effect of PEER. And LPE (Θ) is a policy evaluation phase loss e.g.
LPE (Θ) = (cid:104)
𝑄(𝑠, 𝑎; Θ)− (cid:16)
𝑟 (𝑠, 𝑎)+𝛾E𝑠′,𝑎′ (cid:2)𝑄(𝑠′, 𝑎′; Θ′)(cid:3) (cid:17)(cid:105) 2
.
PEER can be combined with any DRL method that includes a policy evaluation phase, such as DQN [2], TD3, SAC
[4]. Experiments presented in fig. 1 demonstrate that PEER maintains the distinguishable representation property. Fur-thermore, extensive experiments demonstrate PEER signif-icantly improves existing algorithms by keeping the such property. The Pytorch-like pseudocode for the PEER loss can be found in Appendix section 8.3. And fig. 2 illustrates how the PEER loss is computed.
Convergence Guarantee. We additionally provide a con-vergence guarantee of our algorithm. Following the defi-nition in [36], let F be a class of measurable functions, a
𝛿-cover for F with 𝛿 > 0 is a finite set Γ𝛿 ⊂ F such that
∀ 𝑓 ∈ F , there exists 𝑔 ∈ Γ𝛿 such that ∥ 𝑓 − 𝑔∥∞ ≤ 𝛿, where
∥·∥∞ is the 𝑙∞-norm. A minimal 𝛿-cover is a 𝛿-cover and if taking out any of its elements, it is no longer a 𝛿-cover for
F . Let 𝑇 be the Bellman Operator. We have the following convergence result for the core update step in PEER.
(a) Grid World (b) Cosin similarity (c) Q value difference (d) Steps
Figure 3. Experiments on the grid world. (a) Grid world. We are interested in the values of 𝑆1 and 𝑆2. The policies are supposed to be able to differentiate the values of states 𝑆1 and 𝑆2 and tell that 𝑆1 has the higher value. (b) The cosine similarity between the representation of the
𝑄-network and its target. PEER (combined with DQN) effectively alleviates the similarity. (c) The difference between the max Q values of
𝑆1 and 𝑆2. Compared with DQN, PEER is able to better distinguish the Q values of two adjacent states by differentiating the representation of the 𝑄-network and its target. (d) The number of steps to reach the 𝑆𝑇 . PEER is better than DQN. The results are reported over five seeds and the shaded area represents a half standard deviation. by comparing it with other algorithms at fixed timesteps; (iii) compatibility: whether PEER can be combined with other DRL algorithms such as off-policy methods TD3, contrastive unsupervised representation learning (CURL), and DrQ through extensive experiments; and (iv) the dis-tinguishable representation property of PEER. To achieve this, we couple PEER with three representative DRL al-gorithms TD3 [28], CURL [6], and DrQ [9], and perform extensive experiments on four suites, namely, PyBullet, Mu-JoCo, DMControl, and Atari. PEER’s simplicity and ease of implementation are noteworthy, requiring only one line of code. We deliberately avoid using any engineering tricks that could potentially enhance its performance. This deci-sion is made to ensure that the reproducibility crisis, which has been a growing concern in the field of DRL [37, 38], is not further exacerbated. We also maintain a fixed newly introduced hyper-parameter 𝛽 across all the experiments to achieve fair comparisons. Additional experimental details can be found in the Appendix. 4.1. Experimental settings
Hyper-parameters. We introduce only one additional hyper-parameter 𝛽 to control the magnitude of regularization effectiveness of PEER. We report all results using a fixed value of 𝛽 = 5𝑒 − 4. It is important to note that PEER may benefit from selecting a 𝛽 value that is better suited to a specific environment.
Random seeds. To ensure the reproducibility of our experiments, we evaluate each tested algorithm using ten fixed random seeds unless otherwise specified. Moreover, we maintain fixed seeds across all experiments, including those used in PyTorch, Numpy, Gym, and CUDA packages.
Environments. To evaluate PEER, we use both state-based (state represented as vectors containing sensor infor-mation such as velocity, position, friction, etc) PyBullet and
MuJoCo, and pixel-based (state represented as images) DM-Control and Atari suites. The action space of PyBullet, Mu-JoCo, and DMControl is continuous, while that of Atari is discrete. Through the four experimental suites, we can check the performance and sample efficiency of PEER. We use the Gym [39] library for the interactive protocol. For PyBul-let and MuJoCo suites, we run each tested algorithm for 1 million timesteps and evaluate the average return of the algo-rithm every 5k timesteps over ten episodes. For DMControl and Atari suites, following the commonly used experimen-tal setting [6, 9], we measure the performance and sample-efficiency of tested algorithms at 100k and 500k environment timesteps, resulting in DMControl100k, DMControl500k, and Atari100k settings.
Algorithm Ant
HalfCheetah Hopper
Walker2D
PEER
TD3
METD3
SAC
PPO2
TRPO 3003 ± 204 2494 ± 276 2359 ± 229 2731 ± 278 2345 ± 151 2601 ± 246 1675 ± 567 2561 ± 146 397 ± 63 539 ± 25 639 ± 154 693 ± 74 2106 ± 164 1966 ± 58 1646 ± 314 1798 ± 471 1901 ± 111 1929 ± 351 1716 ± 30 1984 ± 103 390 ± 106 403 ± 70 496 ± 206 1140 ± 469
Table 1. The average return of the last ten evaluations over ten ran-dom seeds. PEER (coupled with TD3) outperforms all the compared algorithms, which shows that the PEER loss works in state-based environments. The best score is marked with colorbox. ± corre-sponds to a standard deviation over trials.
Baselines. We first evaluate PEER on the state-based Py-Bullet suite, using TD3, SAC [4], TRPO [40], PPO[41] as our baselines for their superior performance. And we cou-ple PEER with TD3 on PyBullet and MuJoCo experiments.
PEER works as preventing the similarity between the repre-sentation of the Q-network and its target. Another relevant baseline is MEPG [42], which enforces a dropout operator on both the Q-network and its target. Dropout operator [43, 44] is generally believed to prevent feature co-adaptation, which
500K Step Scores
State SAC PlaNet
Dreamer
SAC+AE
DrQ
DrQ-v2
CURL
PEER
Finger, Spin
Cartpole, Swingup
Reacher, Easy
Cheetah, run
Walker, Walk
Ball_in_cup, Catch 100K Step Scores
Finger, Spin
Cartpole, Swingup
Reacher, Easy
Cheetah, run
Walker, Walk
Ball_in_cup, Catch 923 ± 21 848 ± 15 923 ± 24 795 ± 30 948 ± 54 974 ± 33 811 ± 46 835 ± 22 746 ± 25 616 ± 18 891 ± 82 746 ± 91 561 ± 284 475 ± 71 210 ± 390 305 ± 131 351 ± 58 460 ± 380 136 ± 216 297 ± 39 20 ± 50 138 ± 88 224 ± 48 0 ± 0 796 ± 183 762 ± 27 793 ± 164 570 ± 253 897 ± 49 879 ± 87 341 ± 70 326 ± 27 314 ± 155 235 ± 137 277 ± 12 246 ± 174 884 ± 128 735 ± 63 627 ± 58 550 ± 34 847 ± 48 794 ± 58 740 ± 64 311 ± 11 274 ± 14 267 ± 24 394 ± 22 391 ± 82 938 ± 103 868 ± 10 942 ± 71 660 ± 96 921 ± 45 963 ± 9 901 ± 104 759 ± 92 601 ± 213 344 ± 67 612 ± 164 913 ± 53 789 ± 124 845 ± 18 748 ± 229 607 ± 32 696 ± 370 844 ± 174 325 ± 292 677 ± 214 256 ± 145 273 ± 130 171 ± 160 359 ± 228 926 ± 45 841 ± 45 929 ± 44 518 ± 28 902 ± 43 959 ± 27 864 ± 160 866 ± 17 980 ± 3 732 ± 41 946 ± 17 973 ± 5 767 ± 56 582 ± 146 538 ± 233 299 ± 48 403 ± 24 769 ± 43 820 ± 166 863 ± 17 961 ± 28 499 ± 74 714 ± 148 968 ± 7
Table 2. Scores achieved by PEER (coupled with CURL) on DMControl continuous control suite. PEER achieves superior performance on the majority (9 out of 12) tasks. And PEER also outperforms its backbone algorithm CURL on 11 out of 12 tasks by a large margin. The best score is marked with colorbox. ± corresponds to a standard deviation over trials. is close to what PEER achieves. Therefore, we use MEPG combined with TD3 as a baseline, denoted as METD3. We employ the authors’ TD3 implementation, along with the public implementation [45] for SAC, and the Baselines code-base [46] for TRPO and PPO. We opt to adhere to the authors’ recommended default hyper-parameters for all considered algorithms.
Then we evaluate PEER on the pixel-based DMControl and Atari suites, combining it with CURL and DrQ algo-rithms. For DMControl, we select (i) PlaNet [5] and (ii)
Dreamer [7], which learn a world model in latent space and execute planning; (iii) SAC+AE [8] uses VAE [47] and a regularized encoder; (iv) CURL using contrastive unsuper-vised learning to extract high-level features from raw pixels; (v) DrQ [9] and (vi) DrQ-v2 [18], which adopt data augmen-tation technique; and (vii) state-based SAC. For Atari suite, we select CURL, OTRainbow [48], Efficient Rainbow [49],
Efficient DQN [9], and DrQ as baselines. 4.2. Results
PyBullet. We first evaluate the PEER loss in the state-based suite PyBullet. We show the final ten evaluation av-erage return in table 1. The results show (i) PEER coupled with TD3 outperforms TD3 on all environments. Further-more, (ii) PEER also surpasses all other tested algorithms.
The superior performance of PEER shows that the PEER loss can improve the empirical performance of the off-policy model-free algorithm that does not adopt any representation learning techniques.
DMControl. We then perform experiments on the DM-Control suite. Specifically, we couple PEER with the CURL
Figure 4. The normalized average scores on the DMControl suite.
We normalize the average score of the tested algorithm by the av-erage scores of State SAC. On the DMC100k benchmark, PEER (coupled with CURL and DrQ) outperforms all the compared algo-rithms including State SAC. and DrQ algorithms, and run them in DMControl500k and
DMControl100k settings. The results are shown in table 2 and fig. 4. The key findings are as follows: (i) On the DMCon-troll500k setting, PEER coupled with CURL outperforms its backbone by a large margin on 5 out of 6 tasks, which shows the proposed PEER does improve the performance of its backbone algorithm. And the performance improvement on
DMC500k shows that the PEER is beneficial for contrastive unsupervised learning (CURL). (ii) On the DMControl100k setting, The PEER (coupled with CURL) outperforms its backbone CURL on 6 out of 6 tasks. Besides, results in fig. 4 demonstrates that PEER (coupled with DrQ) improves the
Game
Human
Random OTRainbow Eff. Rainbow Eff. DQN DrQ
CURL
PEER+CURL
PEER+DrQ
Alien
Amidar
Assault
Asterix
BankHeist
BattleZone
Boxing
Breakout
ChopperCommand
CrazyClimber
DemonAttack
Freeway
Frostbite
Gopher
Hero
Jamesbond
Kangaroo
Krull
KungFuMaster
MsPacman
Pong
PrivateEye
Qbert
RoadRunner
Seaquest
UpNDown 7127.7 1719.5 742 8503.3 753.1 37187.5 12.1 30.5 7387.8 35829.4 1971 29.6 4334.7 2412.5 30826.4 302.8 3035 2665.5 22736.3 6951.6 14.6 69571.3 13455 7845 42054.7 11693.2 227.8 5.8 222.4 210 14.2 2360 0.1 1.7 811 10780.5 152.1 0 65.2 257.6 1027 29 52 1598 258.5 307.3
-20.7 24.9 163.9 11.5 68.4 533.4 824.7 82.8 351.9 628.5 182.1 4060.6 2.5 9.8 1033.3 21327.8 711.8 25 231.6 778 6458.8 112.3 605.4 3277.9 5722.2 941.9 1.3 100 509.3 2696.7 286.9 2847.6 739.9 188.6 431.2 470.8 51 10124.6 0.2 1.9 861.8 16185.3 508 27.9 866.8 349.5 6857 301.6 779.3 2851.5 14346.1 1204.1
-19.3 97.8 1152.9 9600 354.1 2877.4 558.1 63.7 589.5 341.9 74 4760.8
-1.8 7.3 624..4 5430.6 403.5 3.7 202.9 320.8 2200.1 133.2 448.6 2999 2020.9 872
-19.4 351.3 627.5 1491.9 240.1 2901.7 771.2 102.8 452.4 603.5 168.9 12954 6 16.1 780.3 20516.5 1113.4 9.8 331.1 636.3 3736.3 236 940.6 4018.1 9111 960.5
-8.5
-13.6 854.4 8895.1 301.2 3180.8 558.2 142.1 600.6 734.5 131.6 14870 1.2 4.9 1058.5 12146.5 817.6 26.7 1181.3 669.3 6279.3 471 872.5 4229.6 14307.8 1465.5
-16.5 218.4 1042.4 5661 384.5 2955.2 1218.9 185.2 631.2 834.5 78.6 15727.3 3.7 3.9 1451.8 18922.7 742.9 30.4 2151 583.6 7499.9 414.1 1148.2 4116.1 15439.1 1768.4
-9.5 3207.7 2197.7 10697.3 538.5 6813.5 712.7 163.1 721 918.2 12.7 5000 14.5 8.5 1233.6 18154.5 1236.7 21.2 537.4 681.8 3953.2 213.6 663.6 5444.7 4090.9 1027.3
-18.2 8.2 913.6 6900 409.6 7680.9
Table 3. Scores achieved by PEER (coupled with CURL and DrQ) and baselines on Atari. PEER achieves state-of-the-art performance on 19 out of 26 games. PEER implemented on top of CURL/DrQ improves over CURL/DrQ on 19/16 out of 26 games. Algorithms combined with
PEER are reported across 10 random seeds. We also see that PEER achieves superhuman performance on Boxing, Freeway, JamesBond,
Krull, and RoadRunner games. The best score is marked with colorbox. sample efficiency of DrQ by a large margin. Overall, the
PEER achieves SOTA performance on 11 out of 12 tasks. (iii) PEER outperforms State SAC on 6 out of 12 tasks. In fig. 4, we computed the average score of the tested algorithm on all environments under DMControl100k and DMCon-trol500k settings, normalized by the score of State SAC. The results in DMControl100k show that PEER (combined with
CURL and DrQ) is more sample-efficient than State SAC and other algorithms. And in the DMControl500k suite, the sample efficiency of PEER matches that of State SAC. These results illustrate that PEER remarkably improves the empir-ical performance and sample efficiency of the backbone algorithms TD3, CURL, and DrQ.
Atari. We present Atari100k experiments in table 3, which show that (i) PEER achieves state-of-the-art perfor-mance on 19 out of 26 environments in given timesteps.
And (ii) PEER implemented on top of CURL/DrQ improves over CURL on 19/16 out of 26 games. (iii) We also see that PEER achieves superhuman performance on Boxing,
Freeway, JamesBond, Krull, and RoadRunner games. The empirical results demonstrate that PEER dramatically im-proves the sample efficiency of backbone algorithms. 4.3. Compatibility
Given a fixed hyper-parameter 𝛽 = 5𝑒 − 4, PEER, as a plug-in, outperforms its backbone CURL algorithm on DM-Control (11 out of 12) and Atari (19 out of 26) tasks, which shows that PEER is able to be incorporated into contrastive learn-based and data-augmentation methods to improve per-formance and sample efficiency. Besides, PEER coupled with DrQ also outperforms DrQ on 16 out of 26 environ-ments. For state-based continuous control tasks, PEER (cou-pled with TD3) surpasses all the tested algorithms. These facts show that the compatibility of the PEER loss is remark-able and the PEER can be extended to incorporate more
DRL algorithms. Theoretically, PEER tackles representation learning from the perspective of the representation of the
Bellman equation, which is contrasting with other representa-tion learning methods in DRL. Empirically, the performance of PEER loss coupled with representation learning DRL methods is better than that of backbone methods, which means that the PEER loss is orthogonal to other representa-tion learning methods in DRL. Thus the compatibility of the
PEER loss is auspicious. 4.4. PEER preserves distinguishable representation property
To validate whether the PEER regularizer preserves the distinguishable representation property or not, we measure the distinguishable representation discrepancy of the action value network and its target in PEER following section 3.1.
We show the experimental results in fig. 1. The results show (i) TD3 and PEER (based on TD3) agents do enjoy the distin-guishable representation property but that of PEER is more evident, which reveals the performance gain of PEER in this setting comes from better distinguishable representation property. (ii) The CURL agent does not maintain the dis-tinguishable representation property on the tested DMCon-trol suite, which negatively affects the model performance.
And (iii) PEER also enjoys the distinguishable representa-tion property on the pixel-based environment DMControl.
Thus the performance of PEER is naturally improved due to the property being desirable. (iv) Combined with the per-formance improvement shown in performance experiments (section 4.2), PEER does improve the sample efficiency and performance by preserving the distinguishable representa-tion property of the 𝑄-network and its target. 5.