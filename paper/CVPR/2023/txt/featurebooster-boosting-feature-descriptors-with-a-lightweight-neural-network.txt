Abstract
We introduce a lightweight network to improve descrip-tors of keypoints within the same image. The network takes the original descriptors and the geometric properties of key-points as the input, and uses an MLP-based self-boosting stage and a Transformer-based cross-boosting stage to en-hance the descriptors. The boosted descriptors can be ei-ther real-valued or binary ones. We use the proposed net-work to boost both hand-crafted (ORB [34], SIFT [24]) and the state-of-the-art learning-based descriptors (SuperPoint
[10], ALIKE [53]) and evaluate them on image matching, visual localization, and structure-from-motion tasks. The results show that our method signiﬁcantly improves the per-formance of each task, particularly in challenging cases such as large illumination changes or repetitive patterns.
Our method requires only 3.2ms on desktop GPU and 27ms on embedded GPU to process 2000 features, which is fast enough to be applied to a practical system. The code and trained weights are publicly available at github.com/SJTU-ViSYS/FeatureBooster. 1.

Introduction
Extracting sparse keypoints or local features from an im-age is a fundamental building block in various computer vi-sion tasks, such as structure from motion (SfM), simultane-ous localization and mapping (SLAM), and visual localiza-tion. The feature descriptor, represented by a real-valued or binary descriptor, plays a key role in matching those key-points across different images.
The descriptors are commonly hand-crafted in the early days. Recently, learning-based descriptors [10, 53] have
*Corresponding Author: Danping Zou (dpzou@sjtu.edu.cn).
This works was by National Key R&D Program (2022YFB3903802) and National of Science Foundation of China (62073214) supported
Figure 1. ORB descriptors perform remarkably better in challeng-ing cases after being boosted by the proposed lightweight network.
Left column: Matching results of using raw ORB descriptors.
Right column: Results of using boosted ORB descriptors. Near-est neighbor search and RANSAC [14] were used for matching. shown to be more powerful than hand-crafted ones, espe-cially in challenging cases such as signiﬁcant viewpoint and illumination changes. Both hand-crafted and learning-based descriptors have shown to work well in practice.
Some of them have become default descriptors for some ap-plications. For example, the simple binary descriptor ORB
[34] is widely used for SLAM systems [20, 29]. SIFT [24] is typically used in structure-from-motion systems.
Considering that the descriptors have already been inte-grated into practical systems, replacing them with totally new ones can be problematic, as it may require more com-puting power that may not be supported by the existing hardware, or sometimes require extensive modiﬁcations to the software because of changed descriptor type (e.g. from binary to real).
In this work, we attempt to reuse existing descriptors and enhance their discrimination ability with as little com-putational overhead as possible. To this end, we propose a lightweight network to improve the original descriptors.
The input of this network is the descriptors and the geomet-ric properties such as the 2D locations of all the keypoints within the entire image. Each descriptor is ﬁrstly processed by an MLP (Multi-layer perceptron) and summed with ge-ometric properties encoded by another MLP. The new ge-ometrically encoded descriptors are then aggregated by an efﬁcient Transformer to produce powerful descriptors that are aware of the high-level visual context and spatial lay-out of those keypoints. The enhanced descriptors can be either real-valued or binary ones and matched by using Eu-clidean/Hamming distance respectively.
The core idea of our approach, motivated by recent work
[25, 36, 41], is integrating the visual and geometric infor-mation of all the keypoints into individual descriptors by a
Transformer. This can be better understood intuitively by considering when people are asked to ﬁnd correspondences between images, they would check all the keypoints and the spatial layout of those keypoints in each image. With the help of the global receptive ﬁeld in Transformer, the boosted descriptors contain global contextual information that makes them more robust and discriminative as shown in Fig. 1.
We apply our FeatureBooster to both hand-crafted de-scriptors (SIFT [24], ORB [34]) and the state-of-the-art learning-based descriptors (SuperPoint [10], ALIKE [53]).
We evaluated the boosted descriptors on tasks including image matching, visual localization, and structure-from-motion. The results show that our method can signiﬁcantly improve the performance of each task by using our boosted descriptors.
Because FeatureBooster does not need to process the im-age and adopts a lightweight Transformer, it is highly efﬁ-cient. It takes only 3.2ms on NVIDIA RTX 3090 and 27ms on NVIDIA Jetson Xavier NX (for embedded devices) to boost 2000 features, which makes our method applicable to practical systems. 2.