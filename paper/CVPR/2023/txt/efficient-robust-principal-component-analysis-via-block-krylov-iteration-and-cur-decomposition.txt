Abstract
Robust principal component analysis (RPCA) is widely studied in computer vision. Recently an adaptive rank es-timate based RPCA has achieved top performance in low-level vision tasks without the prior rank, but both the rank estimate and RPCA optimization algorithm involve singular value decomposition, which requires extremely huge com-putational resource for large-scale matrices. To address these issues, an efficient RPCA (eRPCA) algorithm is pro-posed based on block Krylov iteration and CUR decomposi-tion in this paper. Specifically, the Krylov iteration method is employed to approximate the eigenvalue decomposition in the rank estimation, which requires O(ndrq + n(rq)2) for an (n × d) input matrix, in which q is a parameter with a small value, r is the target rank. Based on the estimated rank, CUR decomposition is adopted to replace SVD in up-dating low-rank matrix component, whose complexity re-duces from O(rnd) to O(r2n) per iteration. Experimen-tal results verify the efficiency and effectiveness of the pro-posed eRPCA over the state-of-the-art methods in various low-level vision applications. 1.

Introduction
Robust principal component analysis (RPCA) aims to re-cover a low-rank matrix L and a sparse matrix S from the corrupted observation matrix D ∈ Rn×d: D = L + S.
The RPCA can be formulated as the following optimization problem [1]: min
L,S rank(L) + λ∥S∥0 s.t. D = L + S (1)
*Shiqian Wu is corresponding author. where ℓ0-norm is the number of nonzero elements in the matrix, the paramter λ > 0 provides the trade-off between the rankness and sparsity.
RPCA has been widely studied and applied in computer vision. For example, “background” in a video clip captured by a static camera has a low-rank property, which can be considered in background modeling [2]. The requirement of detecting sparse outliers from the observed imagery data leads to RPCA applications in image or video processing
[3].
In industry, RPCA is also applicable to point cloud filtering [4], surface defects detection [5], shock sensing [6], etc.
It is noted that the optimization problem (1) is NP-hard.
The convex relaxation of RPCA has been studied to achieve an exact recovery in [1, 7–12], where the RPCA problem was relaxed as the sum of the nuclear norm and ℓ1-norm.
But the convex methods always have a rate of sublinear con-vergence and high computation in practice [13]. It is neces-sary to exploit the structure of the underlying data and de-velop more efficient algorithms for RPCA. Zhou et al. [14] extended the equality constraint of RPCA to inequality in order to deal with noisy data, thus the RPCA is reformu-lated as the following constrained non-convex optimization problem:
∥D − L − S∥2
F min
L,S s.t. rank(L) ≤ r and ∥S∥0 ≤ s (2) with the target rank r and target sparse number s.
The key idea of Eq. (2) is to formulate a constrained non-convex optimization problem. Among the nonconvex opti-mization mthods of RPCA, Zhou et al. [15] substituted the hard thresholding of Eq. (2) with a soft ℓ1 regularization to reduce the complexity of S. Netrapalli et al. [16] added the deterministic sparsity assumption and devoted themselves
to recovering a low-rank matrix from the sparse corruptions that were of unknown value and support. The RPCA algo-rithm via gradient descent on the factorized space in [17] achieved linear convergence with proper initialization and step size, whose sparse estimator is to guarantee that the fraction of nonzero entries in each column and row of S is bounded above.
Inspired by these methods, GoDec+
[18] made the model become an ordinary low-rank projec-tion problem based on the correntropy of noise and half-quadratic optimization theory. In [19], RPCA problem is considered for the first time without heuristics, such as loss functions, convex and surrogate constraints, which provides a new direction for potential research on online algorithms.
In addition, Ornhag et al. [20] used second-order methods to convert the original objectives to differentiable equivalents, benefitting from faster convergence.
Meanwhile, the efficient algorithms for RPCA have been widely investigated for large-scale matrices. In [16], the de-veloped algorithm involved alternating projections between a set of low-rank matrices and a set of sparse matrices, whose projection idea was also widely studied.
Inspired by [16], a proximal block coordinate descent method was proposed in [21] to find an ϵ-stationary solution in O(1/ϵ2) iterations. Furthermore, an accelerated alternating projec-tion strategy was studied in [22] for RPCA, which project a matrix onto a specific low-dimensional subspace before ob-taining a new estimate of the low-rank matrix via truncated
SVD.
Most of RPCA and its variants contain SVD, which re-quires significant computational cost for large-scale matri-ces. Hinterm¨uller [23] directly considered a least-squares problem subject to rank and cardinality constraints based on matrix manifolds, which favorably avoids singular value decompositions in full dimension. Phan et al. [24] pro-posed an accelerated algorithm with iteratively reweighted nuclear norm to ensure that every limit point is a critical point, which leads to small singular values and obtains fast result. Some different computing strategies were employed to substitute SVD so that the computation of RPCA was sig-nificantly reduced. Cai et al. [25] introduced CUR decom-position at each iteration, which only required O(r2n) flops per iteration and preserved more information than SVD.
Generally, the aforementioned algorithms achieved ef-ficient and effective performance in different computer vi-sion tasks. It is highlighted that these methods require the rank of a low-rank matrix to be known a prior, which is inappropriate in most practical applications. To bridge this gap, Xu et al. [26] proposed a rank estimation method based on Gerschgorin disk theorem (GDE), whose computational complexity is O((d − 1)3) at each iteration. Furthermore, an adaptive weighting RPCA was developed based on iter-atively estimated rank, which outperforms the state-of-the-art RPCA methods in various computer vision applications.
More specifically, the adaptive weighting RPCA in [26] was formulated as the following optimization problem: min
L,S
∥L∥W + λ∥S∥1 s.t. D = L + S (3) where ∥L∥W = Σiωiσi(L) and ωi are non-negative weights. Based on the estimated rank of L, the weights can be updated iteratively.
Although the adaptive weighting RPCA [26] achieves top performance in recovering a low-rank matrix and a sparse matrix, both the rank estimation and RPCA opti-mization algorithm contain SVD, which takes significant computational costs for large-scale matrices. Recently, ran-domized block Krylov iteration [27] was introduced to ap-proximate the singular value decomposition in fewer iter-ations with better accuracy guarantees. This motivates us to use the block Krylov iteration method to accelerate the
GDE-based rank estimation in this work. The computa-tional complexity of the proposed Krylov GDE (KGDE)-based rank estimation is reduced to O(ndrq + n(rq)2), where q is a parameter with a small value. On the other hand, CUR decomposition is also adopted to replace SVD in the updates of a low-rank matrix, thus the computational complexity of RPCA is significantly reduced. Since the rank of a matrix is required to be known for CUR decompo-sition, it is natural that the proposed KGDE is used to adap-tively estimate the rank of the low-rank matrix within the it-erative RPCA computing. Furthermore, a new non-convex low-rank regularized term is used to replace the weighted nuclear norm in (3) which can improve the low-rank matrix approximation. Compared with the state-of-the-art RPCA approaches, the proposed efficient RPCA (eRPCA) algo-rithm are fast with better accuracy guarantees. The main contributions of this paper are as follows: 1) An efficient rank estimation method based on Ger-schgorin disks with block Krylov iteration is proposed to accelerate the rank estimate of low-rank matrices. 2) CUR decomposition is adopted to reduce the compu-tation of SVD on a large-scale matrix in the update of the low-rank matrix. 3) An efficient non-convex RPCA method with a non-convex weighted regularizer is proposed to achieve better recovery of the low-rank matrix and the sparse matrix. 4) The proposed eRPCA algorithm has been applied to various computer vision scenarios and outperforms the state-of-the-art methods on large-scale data.
In the following section, the improved eRPCA method, which consists of efficient rank estimation and new weight is presented in Section 2. The experimental results are demonstrated in Section 3, and the conclusions are drawn in Section 4.
K := [v, Dv, D2v, · · · , Dq−1v] (4)
RZ = 2. Proposed Method 2.1. Efficient Rank Estimation via Block Krylov It-eration
In [26], GDE was used to estimate the rank of a low-rank matrix, which involves SVD and has the computational complexity O((d − 1)3) for a matrix D ∈ Rn×d. Hence it is time-consuming to estimate the rank of large-scale matrix
D. Recently it has been shown that block Krylov iteration (BKI) method [27] gives nearly optimal approximation of singular values and singular vectors with few interations. In this subsection, an efficient rank estimation is proposed by combining BKI and GDE, which is named as KGDE.
Now we give a brief review on the block Krylov iteration method. For a matrix D ∈ Rn×d, the Krylov subspace is obtained by where v is a vector of unit norm, ∥v∥ = 1, and q is a scalar.
In [28], the Krylov subspace was extended to randomized block Krylov subspace:
K := [V , DV , D2V , · · · , Dq−1V ] (5) where V = [v1, · · · , vb] ∈ Rd×b is a block of b random vectors.
After constructing the block Krylov subspace, the Block
Krylov Iteration (BKI) [27] can be used to generate closely approximate eigenpairs within fewer iterations by project-ing the matrix onto the randomized block Krylov subspace.
Its idea is to take a randomized starting matrix V = DΩ as the initial matrix, where Ω is a Gaussian random matrix.
BKI captures an accurate range space: q
K := (DDT )
DΩ (6) where q := Θ( log d√
ε ). Orthonormalize the columns of K to obtain Q then compute M := QT DDT Q. Set U k to the top k singular vectors of M . At last, return the low-dimension matrix Z:
Z = QU k (7)
The following theorem shows that the randomized BKI can get strong relative-error bound.
Theorem 1 [27] Given data matrix D ∈ Rn×d, n < d with eigenvalues λi, i = 1, 2, · · · , n. Let {θi, φi}k i=1 be the k eigenpair computed using q steps of Block Krylov Itera-tion (using the orthonormal basis of K for V ∈ Rd×b). If q = log d√
ε for some 0 < ε < 1, then have
|θi − λi| ≤ ελk+1, i = 1, · · · , k.
Z ∈ Rn×k, k < d, which aligns well with the top k sigular vectors of D. If r ≤ k, the ranks of D and Z are equal.
Hence it reduces the computational complexity when GDE can also be applied to Z to estimate the rank instead of the large-scale matrix D.
The covariance matrix RZ ∈ Rk×k of the matrix Z with a rank r can be defined as: (8)
RZ = ZT Z
Then the eigenvalue decomposition of RZ is given by
RZ = URZ ΣRZ U H (9)
RZ where URZ = [u1, u2, · · · , uk] is the eigenvector matrix, and ΣRZ = diag(σ1, σ2, · · · , σk) represents the eigen-value matrix. According to the Gerschgorin disk theorem
[29], RZ can be transformed as follows:






R11 R12
R21 R22
...
...
Rk1 Rk2
· · · R1k
· · · R2k
...
. . .
· · · Rkk
= (cid:20) RZ1 R
RH Rkk (cid:21)



 (10) where RZ1 ∈ R(k−1)×(k−1) is the square matrix by the first (k − 1) rows and the first (k − 1) columns of RZ1, and R = [R1k, R2k, · · · , R(k−1)k]T . Then the eigenvalue decomposition on RZ1 is obtained as:
RZ1 = UZ1Σ1U H
Z1 1, q′ (11) where UZ1 = [q′ k−1] is an (k − 1) × (k − 1) unitary matrix composed of the eigenvectors of RZ1, and
Σ1 = diag(σ′ k−1). Similar to Eq. (10), a uni-tary transformed matrix U ∈ Rk×k(U U H = I) is defined as: 2, · · · , σ′ 2, · · · , q′ 1, σ′
U = (cid:20) UZ1 0 0T 1 (cid:21) (12)

=
RT = U H RZU =
The transformed covariance matrix is obtained by: (cid:20) U H (cid:21)
Z1RZ1UZ1 U H
Z1R
RH UZ1
Rkk
 0
· · · 0
· · · 0
· · ·
...
. . .
· · ·
· · ·
ρ1
ρ2
ρ3
...
σ′
ρk−1 k−1
ρ∗ k−1 Rkk
σ′ 1 0 0
... 0
ρ∗ 1
H R. The eigenvalues of RT can be esti-where ρi = q′ i mated using the Gerschgorin disk theorem [29]. Then, the radii of the first (k − 1) Gerschgorin’s disk can be written as: 0 0
σ′ 3
... 0
ρ∗ 3 0
σ′ 2 0
... 0
ρ∗ 2















 (13) ri = |ρi| = |q′ i
H R| (14)
In the rest of this subsection, we present a fast rank esti-mation of low-rank matrices based on the BKI and GDE.
Given a low-rank matrix D ∈ Rn×d, n < d with rank r. The BKI algorithm outputs a low-dimension matrix where the radius ri of the ith Gerschgorin’s disk depends on the size of q′ i
H R.
To further improve the accuracy of the rank estimation, the radii of the Gerschgorin’s disk are shrank in [26]. Then
Algorithm 1 KGDE
Input: D ∈ Rn×d, error ε ∈ (0, 1), k = 2, the (1 + ε) relative-error bound for BKI q = log d√
ε
Output: Estimated rank r
Set gde = zeros(d, 1) for t = 1 to d do
Z ∈ Rn×k ← BKI(D ∈ Rn×d, ε, q)
Covariance matrix RZ = ZT Z
Obtain the transformed unitary matrix according to
Eqs. (10)-(12)
Calculate the radii ri of the Gerschgorin’s disk accord-ing to Eqs. (14)-(15)
Calculate gde(t) according to Eqs. (16)-(17) if (t > 1&&gde(t) < 0) then break; else k = k + 1 end if end for r = t − 1 return r a new diagonal matrix ΣT = diag(σ′ (cid:113)(cid:80)k−1 is constructed, where σ′ k = transformed matrix RT ΣT is constructed as follows: 2, · · · , σ′ k−1, σ′ k) 2. Thus, the new i=1 σ′ i 1, σ′
RT ΣT = ΣT RT Σ−1
T










=
σ′ 1 0
... 0
σ′ 1
σ′ k 0
σ′ 2
... 0
σ′ 2
σ′ k
· · ·
· · ·
. . .
· · ·
· · ·
ρ∗ 1
ρ∗ 2 0 0
...
σ′ k−1
ρ∗ k−1
σ′ k−1
σ′ k
σ′ 1
σ′ k
σ′ 2
σ′ k
ρ1
ρ2
...
ρk−1
σ′ k−1
σ′ k
Rkk









 (15)
RT ΣT and RT are similar matrices and have the same eigenvalues. In Eq. (15), the radii are compressed to vari-ous degrees. Finally, the estimated rank is obtained by the improved heuristic decision rule:
[|σ′ t|rt −
H (t)
D (M ) k − 1 k−1 (cid:88) i=1
|σ′ i|ri] (16) gde(t) = 1 (cid:113)(cid:80)k−1 2 i=1 σ′ i where t = 1, 2, · · · , k − 2, and 0 < H (t) adjustment factor:
D (M ) < 1 is the
H (t)
D (M ) = 2|σ′ t+1| (cid:113)(cid:80)k−1 i=t σ′ i 2 (17)
As a result, the rank r = t − 1 if the first negative value of (16) is reached at t.
When r ≤ k, D is transformed to Z by BKI, then the rank of Z is estimated by GDE. It is noted that while r > k,
GDE can not estimate the true rank of D from Z. To adap-tively determine an appropriate k, KGDE is proposed by adopting iteration, and GDE is served as a stopping crite-rion for such Krylov subspace approximation of Z. The
KGDE for rank estimation based on BKI and GDE is sum-marized in Algorithm 1.
Remark 1: The computational cost of KGDE is related to BKI. If the estimated r of the input matrix D ∈ Rn×d is exact, the computational complexity will be O(ndrq + n(rq)2), where n < d. For sparse matrix, the computa-tional cost of KGDE will be O(nnz(D)rq+n(rq)2), where nnz(D) is the number of nonzeros in D. Since r ≤ n and q = log d√
ε are small, the KGDE will be inexpensive, espe-cially for sparse matrices.
Remark 2: The accuracy of the KGDE mainly depends on the BKI. In light of Theorem 1, Z is obtained within a high-quality principal components approximation of D, both its eigenvalues and eigenvectors are highly close to the actual ones of D. 2.2. Efficient RPCA based on KGDE and CUR De-composition
In this subsection, an efficient RPCA based on the above rank estimation and CUR decomposition is proposed for solving the following optimization problem: min
L,S where
∥D − L − S∥2
F + λ∥L∥γ s.t. ∥S∥0 ≤ s (18)
∥L∥γ =


 (cid:88) eγσi
γ + σi i=1 0 , i > r
, i ≤ r and σi, i = 1, 2, · · · , r, is the singular value,γ > 0, and
λ is the regularized parameter which provides a trade-off between the recovery of low-rank matrix and sparse matrix.
The optimization model (18) adds a new non-convex low-rank regularization term to Eq. (2), which adopts the estimated rank to achieve better recovery accuracy of the low-rank matrix than the weighted nuclear norm in (3).
In this work, the alternating projection method is used to solve the following two sub-problems until convergence:
Lℓ+1 = arg min
∥D − L − Sℓ∥2
F + λ∥L∥γ
L
Sℓ+1 = arg min
S: ∥S∥0≤s
∥D − Lℓ+1 − S∥2
F (19) (20)
Update for L: The traditional solution of (19) often in-volves SVD, which requires significant computational re-source for a large-scale matrix. To efficiently solve the model (19), we replace SVD with CUR decomposition [30], which has been proven to be efficient and effective in solv-ing classical RPCA problem [31]. Mathematically, the CUR
Figure 1. The numerical rank estimation of sparse data matrices. decomposition of (19) can be rewritten as follows:
Cℓ+1 = arg min
∥[D − L − Sℓ]:,J ∥2
F + λ∥L:,J ∥γ (21)
L:,J
Uℓ+1 = T S([D − L − Sℓ]I,J )
∥[D − L − Sℓ]I,:∥2 (22)
F + λ∥LI,:∥γ (23)
Rℓ+1 = arg min
LI,: where Cℓ+1 ∈ Rn×I is I columns of (19) via uniform sam-pling, Rℓ+1 ∈ RJ ×d is J rows of (19) in the same way,
Uℓ+1 ∈ RI×J is the overlap of the column and row indices of Cℓ+1 and Rℓ+1. T S(·) denotes the truncated SVD.
In addition, the rank value r is needed for T S(·) and
CUR decomposition, which can be exactly estimated by our proposed KGDE method.
As Eqs. (21) and (23) are the combination of concave and convex functions, the Difference of Convex program-ming [32] is used to solve these problems. Thus, Cℓ+1 can be obtained as follows [33]:
Cℓ+1 = [adiag{σ∗}bT ]:,J (24) where a and b are the left and right singular vectors of (D−
Sℓ+1), respectively, and σ∗ has a closed-form solution at each iteration:
σi+1 = max((σD−Sℓ+1 −
λωi
µℓ ), 0) (25) where σD−Sℓ+1 is the singular value of (D −Sℓ+1), µ > 0, and ωi = is the gradient of ∥L∥γ at



γeγ (γ + σi(γ))2 , i ≤ r 0 , others
σi. Obviously, ωi is dependent on r, which can be estimated by KGDE at each iteration.
Similarly,
Rℓ+1 = [adiag{σ∗}bT ]I,:
Algorithm 2 eRPCA based on KGDE
Input: D ∈ Rn×d with estimated rank r; δ: error;
λ,µ: penalty parameter;ζ0: initial thresholding value;
|I| = O(r),|J | = O(r): sampling number of rows and columns; γ: non-convex regularizer parameter.
Output: L,S
Uniformly sample row indices I and column indices J .
L0 = 0, S0 = 0, k = 0 while not converged do
Resample I and J
Update ζk+1 = ηζ0
Update Sℓ+1 according to Eqs. (28)-(29)
Estimate and update r by KGDE
Update Cℓ+1 according to (21)
Update Rℓ+1 according to (23)
Update Uℓ+1 according to (22)
Lℓ+1 = Cℓ+1U †
ℓ = ℓ + 1
ℓ+1Rℓ+1 end while
For S, the hard thresholding operator HT is adopted as:
[HT (S)]i,j = (cid:40)
Si,j , if |Si,j| > ζ, 0 , otherwise. (30) where ζ is the thresholding value. The hard thresholding is employed for projections on to (D − Lℓ).
The iteration is terminated when
∥[D − Lℓ − Sℓ]:,J ∥F + ∥[D − Lℓ − Sℓ]I,:∥F
∥D:,J ∥F + ∥DI,:∥F
< δ (31)
The entire procedure to solve the problem (18) is sum-(26) marized in Algorithm 2.
Hence, the updated for L is:
Lℓ+1 = Cℓ+1U † where (·)† denotes the Moore-Penrose pseudoinverse.
ℓ+1Rℓ+1 (27)
Update for S: Similarly, the optimization of (20) can be divided into as:
[Sℓ+1]:,J = arg min
S:,J : ∥S∥0≤s
∥[D − Lℓ − S]:,J ∥2
F (28)
[Sℓ+1]I,: = arg min
SI,:: ∥S∥0≤s
∥[D − Lℓ − S]I,:∥2
F (29)
Remark 3: The computational complexity of U in CUR decomposition is O(r3). Since the sampling numbers of rows and columns are |I| = O(r) and |J | = O(r), respectively, updating L requires O(r2n) flops per iter-In contrast, computing the SVD usually requires ation.
O(rnd).
In addition, the runtime of updating S is actu-ally a simple projection, whose computational complex-ity is further less than that of L. Thus, the computa-tional complexity of the alternating projection procedure is
O( d2r
ϵ + d · poly(r, log(r/ϵ), 1/ϵ)), where 0 < ϵ < 1 is an
Figure 2. The numerical rank estimation of video data matrices. accuracy parameter. 3. Experimental Results
This section reports the experimental results of our pro-posed eRPCA and compares it with state-of-the-art algo-rithms on synthetic datasets and two practical computer vi-sion scenarios. The parameters in our method are listed as follows: γ = 0.02, δ = 10−7, λ = 10−3, µ = 1 2 ,
ζ0 = max(|D|), and ε = 0.05. The parameters in other methods use their default settings. All the experiments are conducted on a laptop with MATLAB 2019 equipped with
Windows 10 based on AMD Ryzen 5 CPU with 32G RAM.
Figure 3. The comparison of the average runtime between differ-ent algorithms.
Table 1. The results of runtime and estimated rank using GDE and
KGDE
Dataset lp blend lp pilot4
Ipi bgdbg1
Ipi ceria3d
IBMtest2
CAVIAR1
GDE
KGDE
Runtime(s) Rank Runtime(s) Rank 0.156 1.488 0.274 91.180 0.176 2.235 3 3 6 5 1 1 0.094 0.250 0.266 0.234 0.134 0.757 3 3 6 3 1 1 computational complexity of KGDE is not too much related to the dimension of the matrix, KGDE can maintain a fast result. 3.1. The Validation of Rank Estimation Method 3.1.2 Synthetic Datasets 3.1.1 Public real datasets
To validate the efficiency and accuracy of KGDE, the gen-eral data matrices are tested to estimate the rank. SuiteS-parse Database1 [34] provides general data matrices with low numerical rank, whose Gaussian-type distribution as-sumptions for the data and noise may not hold.
In addi-tion, the video datasets from Scene