Abstract
Graph neural networks (GNNs) have achieved state-of-the-art performance in many graph learning tasks. How-ever, recent studies show that GNNs are vulnerable to both test-time evasion and training-time poisoning attacks that perturb the graph structure. While existing attack methods have shown promising attack performance, we would like to design an attack framework to further enhance the perfor-mance. In particular, our attack framework is inspired by certiﬁed robustness, which was originally used by defend-ers to defend against adversarial attacks. We are the ﬁrst, from the attacker perspective, to leverage its properties to better attack GNNs. Speciﬁcally, we ﬁrst derive nodes’ cer-tiﬁed perturbation sizes against graph evasion and poison-ing attacks based on randomized smoothing, respectively.
A larger certiﬁed perturbation size of a node indicates this node is theoretically more robust to graph perturbations.
Such a property motivates us to focus more on nodes with smaller certiﬁed perturbation sizes, as they are easier to be attacked after graph perturbations. Accordingly, we design a certiﬁed robustness inspired attack loss, when incorpo-rated into (any) existing attacks, produces our certiﬁed ro-bustness inspired attack counterpart. We apply our frame-work to the existing attacks and results show it can signiﬁ-cantly enhance the existing base attacks’ performance. 1.

Introduction
Learning with graphs, such as social networks, citation networks, chemical networks, has attracted signiﬁcant at-tention recently. Among many methods, graph neural net-works (GNNs) [14,33,38,41,44] have achieved state-of-the-art performance in graph related tasks such as node classi-ﬁcation, graph classiﬁcation, and link prediction. However, recent studies [8,19,20,23,30,34,36,37,39,40,50,51] show that GNNs are vulnerable to both test-time graph evasion
*Corresponding authors attacks and training-time graph poisoning attacks1. Take
GNNs for node classiﬁcation as an instance, graph eva-sion attacks mean that, given a learnt GNN model and a (clean) graph, an attacker carefully perturbs the graph struc-ture (i.e., inject new edges to or remove the existing edges from the graph) such that as many testing nodes as possi-ble are misclassiﬁed o by the GNN model. Whereas, graph poisoning attacks mean that, given a GNN algorithm and a graph, an attacker carefully perturbs the graph structure in the training phase, such that the learnt GNN model misclas-siﬁes as many testing nodes as possible in the testing phase.
While existing methods have shown promising attack per-formance, we want to ask: Can we design a general attack framework that can further enhance both the existing graph evasion and poisoning attacks to GNNs? The answer is yes.
We design an attack framework inspired by certiﬁed ro-bustness. Certiﬁed robustness was originally used by de-fenders to guarantee the robustness of classiﬁcation mod-els against evasion attacks. Generally speaking, a testing example (e.g., an image or a node) with a better certiﬁed robustness guarantee indicates this example is theoretically more robust to adversarial (e.g., pixel or graph) perturba-tions. While certiﬁed robustness is mainly derived for do-ing the good, attackers, on the other hand, can also leverage its property to do the bad. For instance, when an attacker knows the certiﬁed robustness of nodes in a graph, he can base on nodes’ certiﬁed robustness to reversely reveal the vulnerable region of the graph and leverage this vulnerabil-ity to design better attacks. We are inspired by such prop-erty of certiﬁed robustness and design the ﬁrst certiﬁed ro-bustness inspired attacks to GNNs.
Our attack framework consists of three parts: i) In-spired by the state-of-the-art randomized smoothing based certiﬁed robustness against evasion attacks to image mod-els [7, 28] and GNN models [35], we ﬁrst propose to gen-eralize randomized smoothing and derive the node’s cer-1We mainly consider the graph structure attack in the paper, as it is more effective than the feature attack. However, our attack framework can be easily extended to the feature attack.
tiﬁed perturbation size against graph poisoning attacks to
GNNs. Particularly, a larger certiﬁed perturbation size of a node indicates this node is theoretically more robust to ad-versarial graph perturbations. In other words, an attacker needs to perturb more edges during the training phase in order to make this node wrongly predicted by the learnt
GNN model. This property inspires us to focus more on disrupting nodes with relatively smaller certiﬁed perturba-tion sizes under a given perturbation budget. ii) We design a certiﬁed robustness inspired attack loss. Speciﬁcally, we modify the classic node-wise loss by assigning each node a weight based on its certiﬁed perturbation size—A node with a larger/smaller certiﬁed perturbation size will be as-signed a smaller/larger weight. In doing so, losses for nodes with smaller certiﬁed perturbation sizes will be enlarged, and most of the perturbation budget will be automatically allocated to perturb these nodes. Thus, more nodes will be iii) We misclassiﬁed with the given perturbation budget. design the certiﬁed robustness inspired attack framework to generate adversarial graph perturbations to GNNs, based on our certiﬁed robustness inspired attack loss. We emphasize that, as our new attack loss only modiﬁes the existing attack loss with certiﬁed perturbation size deﬁned node weights, any existing graph evasion or poisoning attack method can be used as the base attack in our framework.
We apply our certiﬁed robustness inspired attack frame-work to the state-of-the-art graph evasion and poisoning attacks [40, 51] to GNNs. Evaluation results on multiple benchmark datasets show our attack framework can sub-stantially enhance the attack performance of the base at-tacks. Our contributions are as follows:
• We propose a certiﬁed robustness inspired attack frame-work to GNNs. Our framework can be plugged into any existing graph evasion and poisoning attacks.
• To our best knowledge, we are the ﬁrst work to use certi-ﬁed robustness for an attack purpose.
• Evaluation results validate the effectiveness of our attack framework when applied to the existing attacks to GNNs. 2.