Abstract
Training Phase
Testing Phase
Targeting at recognizing and localizing action instances with only video-level labels during training, Weakly-supervised Temporal Action Localization (WTAL) has achieved signiﬁcant progress in recent years. However, liv-ing in the dynamically changing open world where unknown actions constantly spring up, the closed-set assumption of existing WTAL methods is invalid. Compared with tradi-tional open-set recognition tasks, Open-world WTAL (OW-TAL) is challenging since not only are the annotations of unknown samples unavailable, but also the ﬁne-grained an-notations of known action instances can only be inferred ambiguously from the video category labels. To address this problem, we propose a Cascade Evidential Learning frame-work at an evidence level, which targets at OWTAL for the
ﬁrst time. Our method jointly leverages multi-scale tem-poral contexts and knowledge-guided prototype informa-tion to progressively collect cascade and enhanced evidence for known action, unknown action, and background sepa-ration. Extensive experiments conducted on THUMOS-14 and ActivityNet-v1.3 verify the effectiveness of our method.
Besides the classiﬁcation metrics adopted by previous open-set recognition methods, we also evaluate our method on lo-calization metrics which are more reasonable for OWTAL. 1.

Introduction
Targeting at recognizing and localizing action instances with only video-level labels during training, Weakly-supervised Temporal Action Localization (WTAL) has at-tracted increasing attention from both academia and indus-try [9, 11, 18, 19, 37, 43]. Unlike fully-supervised TAL,
WTAL only requires video-level action labels during train-ing. However, the closed-set assumption of existing WTAL
Basketball Dunk
Basketball Dunk
Cricket Shot
Unknown (Cricket Bowling)
Cricket Shot
Long Jump
Unknown (Hammer Throw)
Figure 1. Illustration of the training and testing phases of OWTAL.
With only video-level labels for training, OWTAL aims to localize both known and unknown action instances in testing videos. methods is invalid in the dynamically changing real world, since with the development of society never-before-seen hu-man action categories are constantly emerging. Therefore, to address this problem, we consider a different WTAL set-ting in this work, termed as Open-world WTAL (OWTAL).
Different from the traditional WTAL task, as shown in Figure 1, OWTAL allows testing videos to contain ac-tion instances of unknown categories, which have never ap-peared during training. Therefore, temporal boundries of both known and unknown action instances are expected to be predicted. Compared with its fully-supervised counter-part Open Set TAL [3], OWTAL is challenging in two as-pects: (1) Ambiguity of annotations of closed-set (known) action instances. Previous works indicate that the closed-set and open-set performance are highly correlated [42]. How-ever, under the OWTAL setting, not only are the annotations of unknown action instances unavailable, but also the ﬁne-grained annotations of known ones can only be inferred am-biguously from the video category labels. During training, the known action instances that the model needs to focus on are prone to be disturbed by the background snippets, which hinders the learning of the closed-set actions, thus making it extremely difﬁcult to differentiate the unknown actions, the known actions, and the background. (2) Lack of reasonable metrics. The traditional Open Set Recognition (OSR) aims
for classiﬁcation while the goal of OWTAL is to perform lo-calization instead, thus the classiﬁcation metrics commonly adopted by OSR are not sufﬁcient for OWTAL.
In order to alleviate the negative impact caused by the weak annotations of known action instances, we propose a Cascade Evidential Learning method for owtaL (CELL), which progressively collects cascaded evidence by con-sidering both temporal contexts in multi-scale ranges and inter-video correlations under the guidance of prior knowl-edge. Since the goal of OWTAL is to locate the consecu-tive known/unknown action segments of various temporal lengths in open-world scenarios, perceiving temporal con-texts in diverse ranges is essential. We argue that it is mean-ingful to endow individual snippet features with the ability of sensing multi-scale neighborhood video segments, and thus a Multi-scale Extended-range Perception module (Sec-tion 3.2) is designed to obtain more discriminative video features for initial evidence collection, taking advantage of the temporal contexts. Due to the large intra-action vari-ation in visual patterns and the lack of prior knowledge guidance, the known action instances which visually de-viate from the common ones are likely to be misidentiﬁed with the initial evidence collected from individual videos.
Therefore, we design a Knowledge-guided Bipolar Proto-type Learning strategy (Section 3.3), where a semantic rela-tion graph is constructed to provide prior knowledge guid-ance for the bipolar prototype learning among videos, thus perceiving the open-world more comprehensively. We use this strategy to generate a series of evidence calibration fac-tors for further cascade evidence enhancement. Finally, a
Cascade Evidence Enhancement module (Section 3.4) is designed for enhancing the initial evidence with the cali-bration factors, and the uncertainty estimated from the cas-caded evidence is used for the known/unknown judgment.
Extensive experiments conducted on THUMOS-14 and Ac-tivityNet verify the effectiveness. Besides the various clas-siﬁcation metrics adopted by previous works, we also eval-uate our method on localization metrics which are more in line with the needs of real-world applications.
To summarize, our contribution is threefold:
• To tackle the unique challenges of OWTAL, we propose a cascade evidential learning framework, which progres-sively collects comprehensive evidence for known ac-tion, unknown action, and background separation. Lo-calization metrics which meet the needs of the real-world more closely are adopted for evaluation.
• To achieve OWTAL without ﬁne-grained annotations, the proposed CELL jointly leverages multi-scale tem-poral contexts and knowledge-guided prototype infor-mation during the evidence cascade learning process.
• We conduct comprehensive experiments on two popu-lar WTAL benchmarks, THUMOS-14 and ActivityNet-v1.3, and achieve signiﬁcant performance improvement over various baselines. Experiments show that CELL enables existing methods to well adapt to the more prac-tical open-world settings. 2.