Abstract
Selecting appropriate datasets is critical in modern com-puter vision. However, no general-purpose tools exist to evaluate the extent to which two datasets differ. For this, we propose representing images – and by extension datasets – using Distributions of Neuron Activations (DNAs). DNAs ﬁt distributions, such as histograms or Gaussians, to activa-tions of neurons in a pre-trained feature extractor through which we pass the image(s) to represent. This extractor is frozen for all datasets, and we rely on its generally expres-sive power in feature space. By comparing two DNAs, we can evaluate the extent to which two datasets differ with granular control over the comparison attributes of inter-est, providing the ability to customise the way distances are measured to suit the requirements of the task at hand.
Furthermore, DNAs are compact, representing datasets of any size with less than 15 megabytes. We demonstrate the value of DNAs by evaluating their applicability on several tasks, including conditional dataset comparison, synthetic image evaluation, and transfer learning, and across diverse datasets, ranging from synthetic cat images to celebrity faces and urban driving scenes. 1.

Introduction
Being able to compare datasets and understanding how they differ is critical for many applications, including de-ciding which labelled dataset is best to train a model for de-ployment in an unlabelled application domain, sequencing curricula with gradually increasing domain gap, evaluating the quality of synthesised images, and curating images to mitigate dataset biases.
However, we currently lack such capabilities. For exam-ple, driving datasets available covering many domains [2, 4, 9, 16, 17, 22, 35, 54, 62, 64] were collected under diverse conditions typically affecting image appearance (e.g. loca-tion, sensor conﬁguration, weather conditions, and post-Project page and code: bramtoula.github.io/vdna
Target domain
…
BDD100K
WildDash
IDD
KITTI (a) When deploying a vision model in a new target domain, selecting mod-els pre-trained on the most relevant datasets can help. However, there are no methods to measure dataset similarities automatically. A general dis-tance between datasets would be sensitive to many variation types, but
DNAs provide sufﬁcient granularity to customise the comparison to focus on features of interest. For example, DNA comparisons can be customised to ignore weather conditions or focus on semantic content
Real horses
Fake horse
Person 1
Person 2 (b) The DNA can also be used to compare individual images to datasets – for example, to measure the realism and semantic consistency of a syn-thetic image – or pairs of images – for example, to verify the presence of similar attributes such as smiling or wearing a hat.
Figure 1. Example use-cases of the DNA representation. processing). Yet, users are limited to coarse or insufﬁcient meta-information to understand these differences. More-over, depending on the application, it might be desirable to
Pre-trained Feature Extractor 
Dataset DNA
Image 1 DNA
Distributions of Neuron Activations
Neuron 1
Neuron 1
Neuron 1
Neuron 1
Neuron 1
Neuron 1
Dataset
Neuron 2
Neuron 2
Neuron 2
.
.
.
Neuron !!
Neuron !"
Neuron !#
Image 1
Layer 1
Layer 2
Layer L t n u o
C t n u o
C t n u o
C
Activation values
Activation values
Activation values
Neuron 2
Neuron 2
Neuron 2 t n u o
C t n u o
C t n u o
C
Activation values
Activation values
Activation values
Neuron !!
Neuron !"
Neuron !# t n u o
C t n u o
C t n u o
C
Activation values
Layer 1
Activation values
Layer 2
Activation values
Layer L
Figure 2. We propose representing images by passing them through a pre-trained frozen feature extractor network and collecting neuron activations. We then create a descriptor called the Distribution of Neuron Activations (DNA) by ﬁtting a distribution (the histogram in the illustration) to the activations at each neuron. We can quantitatively measure the similarity of different datasets or images by comparing their DNAs. Neuron combination strategies that are sensitive to speciﬁc attributes can also allow for customised comparisons of DNAs. compare datasets only on controlled sets of attributes while ignoring others. For self-driving, these may be weather, road layout, driving patterns, or other agents’ positions.
We propose representing datasets using their Distribu-tions of Neuron Activations (DNAs), allowing efﬁcient and controllable dataset and image comparisons (Fig. 1).
The DNA creation exploits the recent progress in self-supervised representation learning [13, 18] and extracts im-age descriptors directly from patterns of neuron activations in neural networks (NNs). As illustrated in Fig. 2, DNAs are created by passing images through an off-the-shelf pre-trained frozen feature extraction model and ﬁtting a distri-bution (e.g. histogram or Gaussian) to the activations ob-served at each neuron. This DNA representation contains multi-granular feature information and can be compared while controlling attributes of interest, including low-level and high-level information. Our technique was designed to make comparisons easy, avoiding high-dimensional fea-ture spaces, data-speciﬁc tuning of processing algorithms, model training, or any labelling. Moreover, saving DNAs requires less than 15 megabytes, allowing users to easily in-spect the DNA of large corpora and compare it to their data before committing resources to a dataset. We demonstrate the results of using DNAs on real and synthetic data in mul-tiple tasks, including comparing images to images, images to datasets, and datasets to datasets. We also demonstrate its value in attribute-based comparisons, synthetic image qual-ity assessment, and cross-dataset generalisation prediction. 2.