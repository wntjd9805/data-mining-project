Abstract
Under good conditions, Neural Radiance Fields (NeRFs) have shown impressive results on novel view synthesis tasks.
NeRFs learn a scene’s color and density fields by minimiz-ing the photometric discrepancy between training views and differentiable renderings of the scene. Once trained from a sufficient set of views, NeRFs can generate novel views from arbitrary camera positions. However, the scene geom-etry and color fields are severely under-constrained, which can lead to artifacts, especially when trained with few input views.
To alleviate this problem we learn a prior over scene geometry and color, using a denoising diffusion model (DDM). Our DDM is trained on RGBD patches of the syn-thetic Hypersim dataset and can be used to predict the gra-dient of the logarithm of a joint probability distribution of color and depth patches. We show that, these gradients of logarithms of RGBD patch priors serve to regularize geom-etry and color of a scene. During NeRF training, random
RGBD patches are rendered and the estimated gradient of the log-likelihood is backpropagated to the color and den-sity fields. Evaluations on LLFF, the most relevant dataset, show that our learned prior achieves improved quality in the reconstructed geometry and improved generalization to novel views. Evaluations on DTU show improved recon-struction quality among NeRF methods. 1.

Introduction
Neural radiance fields, neural implicit surfaces, and coordinate-based scene representations are proving valu-able for novel view synthesis and 3D reconstruction tasks.
NeRFs [17] learn a specific scene’s appearance as a multi-layer perceptron that predicts density and color, when given any 3D point and a viewing direction.
This volume representation allows differentiable render-ing from arbitrary views, where predicted color contribu-tions along a ray are alpha-composited according to the den-(a) DiffusioNeRF (Ours) (b) RegNeRF [21]
Figure 1. Image and depth map rendered from a test view. All
NeRF models were trained with 3 views of the LLFF [16] dataset’s
“Room” scene. Our priors encourage NeRF to explain the TV and table geometry with flat surfaces in the density field, and to explain the view-dependent color changes with the color field. sity predictions.
The model is trained with the aim of faithfully recon-structing images captured with known camera poses. Even when trained with just a photometric reconstruction loss,
NeRFs show impressive generalization capabilities, inspir-ing novel applications in virtual and augmented reality, and visual special effects.
However, with small numbers or even with large num-bers of input views, the scene color and geometry fields are severely under-constrained. Indeed, an infinite number of
NeRFs can explain all training views. In practice, NeRFs can generate low-quality and physically implausible ge-ometries and surface appearances. For example, “floaters” are one common artifact, where the fitted density field con-tains clouds of semi-transparent material floating in mid-air that would look reasonable in 2D once rendered from train-ing views, but look implausible from novel views.
Various hand-crafted regularizers and learned priors have been proposed to tackle these issues: hand-engineered priors to constrain the scene geometry [2,21], learned priors that force plausible renderings from arbitrary views [21], and methods that use single image depth and normal esti-mation [38, 46] to provide high-level constraints on the es-timated scene geometry. However, there are no approaches that learn a joint probability distribution of the scene geom-etry and color.
Our contribution is leveraging denoising diffusion mod-els (DDMs) as a learned prior over color and geometry.
Specifically, we use an existing synthetic dataset to gener-ate a dataset of RGBD patches to train our DDM. DDMs do not predict a probability for RGBD patch distribution.
Rather, they provide the gradient of the log-probability of the RGBD patch distribution, i.e. the negative direction to the noise predicted by DDM is equivalent to moving towards the modes of the RGBD patch distribution. As
NeRFs are trained with stochastic gradient descent, gradi-ents of log-probabilities are sufficient, as they can be back-propagated to NeRF networks during training to act as a regularizer; probabilities are not required for this purpose.
We demonstrate that the DDM gradient encourages NeRFs to fit density and color fields that are more physically plau-sible on the LLFF and DTU datasets. 2.