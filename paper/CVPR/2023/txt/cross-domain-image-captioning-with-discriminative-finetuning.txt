Abstract
Neural captioners are typically trained to mimic human-generated references without optimizing for any speciﬁc communication goal, leading to problems such as the gen-In this paper, we show that eration of vague captions.
ﬁne-tuning an out-of-the-box neural captioner with a self-supervised discriminative communication objective helps to recover a plain, visually descriptive language that is more informative about image contents. Given a target image, the system must learn to produce a description that enables an out-of-the-box text-conditioned image retriever to iden-tify such image among a set of candidates. We experiment with the popular ClipCap captioner, also replicating the main results with BLIP. In terms of similarity to ground-truth human descriptions, the captions emerging from dis-criminative ﬁnetuning lag slightly behind those generated by the non-ﬁnetuned model, when the latter is trained and tested on the same caption dataset. However, when the model is used without further tuning to generate captions for out-of-domain datasets, our discriminatively-ﬁnetuned captioner generates descriptions that resemble human ref-erences more than those produced by the same captioner wihtout ﬁnetuning. We further show that, on the Concep-tual Captions dataset, discriminatively ﬁnetuned captions are more helpful than either vanilla ClipCap captions or ground-truth captions for human annotators tasked with an image discrimination task.1 1.

Introduction
The last decade has seen impressive progress on the task of automatically generating image descriptions with deep neural networks [5, 39, 44, 46]. Most of the proposed meth-1Our code at https : / / github . com / facebookresearch / EGG / tree / main / egg / zoo / discriminative_captioner. available is
Figure 1. Setup of our discriminative ﬁnetuning method when ap-plied to the ClipCap captioner [27]. All CLIP encoders are frozen, while the language generation modules (mapper and GPT-2) are updated based on reward values. ods try to optimize the similarity of system-produced cap-tions with ground-truth human references, either through a standard cross-entropy cost function [3], or by maximizing natural-language-generation (NLG) metrics such as CIDEr
[31, 36] through a reward-based objective. While imitating human captions is a reasonable goal, it does not take into account that, in concrete applications, an image description is produced for a purpose [15, 21].
There are a multitude of context-dependent purposes a description might be produced for, but a fundamental one is to correctly characterize an object so that a hearer could discriminate it from other contextual elements [18]. This ability to discriminate between referents is a core purpose of communication, playing a fundamental role in its evo-lution and acquisition (e.g., [6, 38]). We study here what happens when we take an out-of-the-box image captioner that was trained to imitate human captions, and ﬁnetune its language components with a discriminative objective using
reinforcement learning. In particular, we let the captioner play a discrimination game with an out-of-the-box caption-based image retriever. The captioner generates a caption given a target image, and the retriever (whose weights are not updated) uses the caption to select the target among a set of candidates, as shown in Fig. 1. This ﬁnetuning technique does not require annotated data (only a set of images), and it’s agnostic to the underlying captioner and retriever com-ponents.
We report two strong and novel results. First, we show that captions ﬁnetuned in this way lead to better 0-shot cross-domain caption generation.2 Second, not only are the ﬁnetuned captions good for neural text-based image retrieval (both in- and across-domain), but they can also be more useful to human annotators, helping discrimi-nate the target from distractors more than human-generated ground-truth captions do. We conclude the paper with an analysis of the ﬁnetuned captions, comparing them to human-generated and non-ﬁnetuned ones from the Concep-tual Captions dataset. We ﬁnd that discriminative ﬁnetuning undoes the more abstract language that the underlying sys-tem had learned from the ground-truth captions, leading to a more plainly descriptive style that we expect to be more useful in practical applications. 2.