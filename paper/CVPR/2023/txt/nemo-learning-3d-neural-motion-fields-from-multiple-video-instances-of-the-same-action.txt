Abstract
The task of reconstructing 3D human motion has wide-ranging applications. The gold standard Motion capture (MoCap) systems are accurate but inaccessible to the general public due to their cost, hardware, and space constraints. In contrast, monocular human mesh recovery (HMR) methods are much more accessible than MoCap as they take single-view videos as inputs. Replacing the multi-view MoCap systems with a monocular HMR method would break the cur-rent barriers to collecting accurate 3D motion thus making exciting applications like motion analysis and motion-driven animation accessible to the general public. However, the performance of existing HMR methods degrades when the video contains challenging and dynamic motion that is not in existing MoCap datasets used for training. This reduces its appeal as dynamic motion is frequently the target in 3D motion recovery in the aforementioned applications. Our study aims to bridge the gap between monocular HMR and multi-view MoCap systems by leveraging information shared across multiple video instances of the same action. We in-troduce the Neural Motion (NeMo) field. It is optimized to represent the underlying 3D motions across a set of videos of the same action. Empirically, we show that NeMo can re-cover 3D motion in sports using videos from the Penn Action dataset, where NeMo outperforms existing HMR methods in terms of 2D keypoint detection. To further validate NeMo using 3D metrics, we collected a small MoCap dataset mim-icking actions in Penn Action, and show that NeMo achieves better 3D reconstruction compared to various baselines.
1.

Introduction
Reconstructing 3D human motion has wide-ranging ap-plications from the production of animation movies like
Avatar [4], human motion synthesis [17, 44, 45] and biome-chanical analysis of motion [3, 10, 16, 27, 39, 40]. Tra-ditional marker-based MoCap systems work by recording 2D infrared images of light reflected by markers placed on the human subject. Placing, calibrating, and labeling the markers are tedious, and the markers can potentially restrict the range-of-motion of the subject. Alternatively, marker-less MoCap systems work with RGB videos and use com-puter vision techniques to extract the 3D human pose, which eliminates the need of placing physical markers on human subjects. Given a set of synchronized video captures from multiple views, one can run 2D keypoint detection methods like OpenPose [5] and perform triangulation to recover the 3D pose [35]. Markerless MoCap approaches, however, are still restricted by the assumption of having synchronized multi-view video capture of the same instance of the motion.
Meanwhile, there has been a rapid development of 3D monocular human pose estimation (HPE) and human mesh recovery (HMR) methods. These methods aim to recover the 3D human motion from a single-view video capture. The accessibility of monocular HMR makes it an attractive alter-native to existing MoCap systems, especially in situations where setting up additional hardware is challenging like when the human subject is on a bike [13] or underwater [9].
The accuracy of monocular methods is generally lower be-cause single-view input only provides partial information about the underlying 3D motion. The model needs to over-come complications like depth ambiguity and self-occlusion.
Like other machine learning systems, HMR models over-come these difficulties by learning from paired training data.
Because of the difficulty and cost of collecting MoCap data, paired datasets of videos and 3D motions are scarce. Ad-ditionally, publically available MoCap datasets are often restricted to simple everyday motions like Human3.6M [19] and AMASS [12]. As a result, existing HMR methods gen-eralize less well in domains with less available MoCap data, such as motions in sports, a dominant application domain of 3D human motion recovery [8, 15, 33, 34]. As an example, see Figure 1 for where existing HMR methods struggled to capture the dynamic range of athletic motions.
We bridge the gap between multi-view MoCap and monocular HMR by assuming there is shared and comple-mentary information in multiple instances of video captures of the same action, similar to what is in the (same instance) multi-view setup. These multiple video instances can be different repetitions of the same action from the same per-son, or even from different people executing the same action in different settings (See the left side of Figure 2 for illus-tration). For application domains like sports, a key feature of its motions is that they are well-defined and structured.
For example, an athlete is also often instructed to practice by performing many repetitions of the same action and the variation across the repetitions is oftentimes slight. Even when we look at the executions of the same action from different athletes, these different motion “instances” often contain shared information. In this work, we aim to better re-construct the underlying 3D motion from videos of multiple instances of the same action in sports.
We parametrize each motion as a neural network. It takes as input a scalar phase value indicating the phase/progress of the action and an instance code vector for variation and outputs human joint angles, root orientation, and translation.
Since the different sequences are not synchronized, and the actions might progress at slightly different rates (e.g., a faster versus slower pitch), we use an additional learned phase net-work for synchronization. The neural network is shared across all the instances while other components including the instance codes and phase networks are instance-specific.
All the components are learned jointly. We optimize using the 2D reprojection loss with respect to the 2D joint key-points of the input videos and prior 3D loss w.r.t. initial predictions from HMR methods to enforce 3D prior. We call the resulting neural network a Neural Motion (NeMo) field. NeMo can also be seen as a new test-time optimization scheme for better domain adaptation of 3D HMR similar in spirit to SMPLify [2, 36]. A key difference is that we leverage shared 3D information at the group level of many instances to learn a canonical motion and its variations. To summarize, our contributions are:
• We propose the neural motion (NeMo) field and an optimization framework that improves 3D HMR results by jointly reasoning about different video instances of the same action.
• We optimize NeMo fields on sports actions selected from the Penn Action dataset [49]. Since the Penn
Action dataset only has 2D keypoint annotations, we collected a small MoCap dataset with 3D groundtruth where the actor was instructed to mimic these motions, which we will refer to as our NeMo-MoCap dataset. We show improved 3D motion reconstruction compared to various baseline HMR methods using both 3D metrics, and also improved results on the Penn Action dataset using 2D metrics.
• The NeMo field also recovers global root transla-tion. Compared to the recently proposed global HMR method, recovered global motion from NeMo is sub-stantially more accurate on our NeMo-MoCap dataset. 2.