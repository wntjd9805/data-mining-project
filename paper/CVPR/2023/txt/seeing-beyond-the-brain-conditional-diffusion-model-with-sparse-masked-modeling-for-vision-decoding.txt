Abstract
Decoding visual stimuli from brain recordings aims to deepen our understanding of the human visual system and build a solid foundation for bridging human and computer vision through the Brain-Computer Interface. However, reconstructing high-quality images with correct semantics from brain recordings is a challenging problem due to the complex underlying representations of brain signals and the scarcity of data annotations. In this work, we present MinD-Vis: Sparse
Masked Brain Modeling with Double-Conditioned Latent
Diffusion Model for Human Vision Decoding. Firstly, we learn an effective self-supervised representation of fMRI data using mask modeling in a large latent space inspired by the sparse coding of information in the primary visual cortex. Then by augmenting a latent diffusion model with double-conditioning, we show that MinD-Vis can reconstruct highly plausible images with semantically matching details from brain recordings using very few paired annotations. We benchmarked our model qual-itatively and quantitatively; the experimental results indicate that our method outperformed state-of-the-art in both semantic mapping (100-way semantic classification) and generation quality (FID) by 66% and 41% respectively. An exhaustive ablation study was also conducted to analyze our framework.
*Equal contributions.
†Corresponding author (helen.zhou@nus.edu.sg) 1.

Introduction
“What you think is what you see”. Human perception and prior knowledge are deeply intertwined in one’s mind [51]. Our perception of the world is determined not only by objective stimuli properties but also by our experiences, forming complex brain activities underlying our perception. Understanding these brain activities and recovering the encoded information is a key goal in cognitive neuroscience. Within this broad objective, decoding visual information is one of the challenging problems that are the focus of a large body of literature [22, 26, 34, 67].
As a non-invasive and effective method to measure brain activities indirectly, functional Magnetic Resonance Imaging (fMRI) is usually used to recover visual information, such as the image classes [21, 39]. With the help of recent deep learning models, it is intriguing if the original visual stimuli can be directly recovered from corresponding fMRI [2, 46], especially with the guidance of biological principles [43, 52].
However, due to the lack of fMRI-image pairs and useful biological guidance when decoding complex neural activity from fMRI directly, reconstructed images are usually blurry and semantically meaningless. Thus it is crucial to learn effective and biological-valid representations for fMRI so that a clear and generalizable connection between brain activities and visual stimuli can be established with a few paired annotations.
Moreover, individual variability in brain representations
Individuals have unique further complicates this problem.
brain activation patterns responding to the same visual stimulus (See Fig. 2). From the perspective of fMRI representation learn-ing, a powerful brain decoding algorithm should robustly rec-ognize features shared across the population over a background of individual variation [5, 21]. On the other hand, we should also expect decoding variances due to the variation in individual perceptions. Therefore, we aim to learn representations from a large-scale dataset with rich demographic compositions and relax the direct generation from fMRI to conditional synthesis al-lowing for sampling variance under the same semantic category.
Self-supervised learning with pretext tasks in large datasets is a powerful paradigm to distill the model with context knowledge. A domain-specific downstream task (e.g. classi-fication) is usually adopted to finetune the pre-trained model further [36, 58], especially when the downstream dataset is small. Various pretext tasks are designed to benefit downstream tasks [23, 66]. Among these methods, Masked Signal Modeling (MSM) has achieved promising results in both vision [18, 62] and language understanding [8, 37] recently. At the same time, the probabilistic diffusion denoising model has shown its superior performance in content generation and training stability [9]. A strong generation ability is also desired in our task to decode faithful visual stimuli from various categories.
Driven by the above analysis, we propose MinD-Vis: Sparse
Masked Brain Modeling with Double-Conditioned Latent Dif-fusion Model for Human Vision Decoding, a framework that ex-ploits the power of large-scale representation learning and mim-ics the sparse coding of information in the brain [14], including the visual cortex [56]. Different from [18], we use a much larger representation-to-data-space ratio to boost the information capac-ity of learned representations. Our contributions are as follows:
• We propose Sparse-Coded Masked Brain Modeling (SC-MBM), designed under biological guidance as an effective brain feature learner for vision decoding.
• Augmenting the latent diffusion model with double condi-tioning (DC-LDM), we enforce stronger decoding consis-tency while allowing variance under the same semantics.
• Integrating the representation ability of SC-MBM with the generation ability of DC-LDM, MinD-Vis generates more plausible images with better preserved semantic information compared with previous methods.
• Quantitative and qualitative tests are performed on multiple datasets, including a new dataset that has not previously been used to evaluate this task. 2.