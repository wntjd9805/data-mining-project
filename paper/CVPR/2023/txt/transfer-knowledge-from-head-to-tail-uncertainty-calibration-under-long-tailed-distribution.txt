Abstract
How to estimate the uncertainty of a given model is a crucial problem. Current calibration techniques treat dif-ferent classes equally and thus implicitly assume that the distribution of training data is balanced, but ignore the fact that real-world data often follows a long-tailed distribu-tion. In this paper, we explore the problem of calibrating the model trained from a long-tailed distribution. Due to the difference between the imbalanced training distribution and balanced test distribution, existing calibration methods such as temperature scaling can not generalize well to this problem. Specific calibration methods for domain adapta-tion are also not applicable because they rely on unlabeled target domain instances which are not available. Models trained from a long-tailed distribution tend to be more over-confident to head classes. To this end, we propose a novel knowledge-transferring-based calibration method by esti-mating the importance weights for samples of tail classes to realize long-tailed calibration. Our method models the dis-tribution of each class as a Gaussian distribution and views the source statistics of head classes as a prior to calibrate the target distributions of tail classes. We adaptively trans-fer knowledge from head classes to get the target probability density of tail classes. The importance weight is estimated by the ratio of the target probability density over the source probability density. Extensive experiments on CIFAR-10-LT, MNIST-LT, CIFAR-100-LT, and ImageNet-LT datasets demonstrate the effectiveness of our method. 1.

Introduction
With the development of deep neural networks, great progress has been made in image classification. In addition to performance, the uncertainty estimate of a given model is also receiving increasing attention, as the confidence of a model is expected to accurately reflect its performance.
â€ Corresponding author.
A model is called perfect calibrated if the predictive con-fidence of the model represents a good approximation of its actual probability of correctness [9]. Model calibration is particularly important in safety-critical applications, such as autonomous driving, medical diagnosis, and robotics [1].
For example, if a prediction with low confidence is more likely to be wrong, we can take countermeasures to avoid unknown risks.
Most existing calibration techniques assume that the dis-tribution of training data is balanced, i.e., each class has a similar number of training instances, so that each class is treated equally [9, 18, 25]. As shown in Fig. 1, the tra-ditional calibration pipeline uses a balanced training set to train the classification model and a balanced validation set to obtain the calibration model, respectively. The target test set is in the same distribution as the training/validation set.
However, data in the real-world often follows a long-tailed distribution, i.e., a few dominant classes occupy most of the instances, while much fewer examples are available for most other classes [6, 16, 24]. When tested on balanced test data, classification models trained from the training set with a long-tailed distribution are naturally more over-confident to head classes. Only imbalanced validation set with the same long-tailed distribution is available for cal-ibrating such models since the validation set is often ran-domly divided from the training set.
Due to the different distributions between the imbal-anced training data and the balanced test data [15], it is dif-ficult for traditional calibration techniques to achieve bal-anced calibration among head classes and tail classes with different levels of confidence estimations. For instance, temperature scaling [9] with the temperature learned on a validation set obtains degraded performance on the test set if the two sets are in different distribution [29, 37]. As shown in Fig. 2, a balanced test set suffers heavier over-confidence compared with a long-tailed validation set. Al-though temperature scaling can relieve such phenomenon, there still exists overconfidence after calibration. Domain adaptation calibration methods [29, 40] aim to generalize
(a) Calibration under balanced distribution. (b) Calibration under long-tailed distribution.
Figure 1. The difference between calibration under balanced distribution and calibration under long-tailed distribution. (a) The classifica-tion model and the calibration model are trained on the balanced training and validation sets, respectively, and the test set is balanced. (b)
The classification model and the calibration model are trained on the long-tailed training and validation sets, while the test set is balanced. (a) Validation set (b) Test set (c) Temperature scaling
Figure 2. The reliability diagrams of (a) the validation set before calibration, (b) the test set before calibration, and (c) the test set after calibration with temperature scaling. calibration across domains under a covariate shift condition but they utilize unlabeled target domain instances. Simi-larly, the domain generalization calibration method [8] uses a support set to bridge the gap between the source domain and the target domain, which also relies on extra instances.
These methods cannot be applied to the long-tailed calibra-tion since the balanced test domain is not available.
In this paper, we investigate the problem of calibra-tion under long-tailed distribution. Since the distributions of each tail class in the imbalanced validation set and the balanced target set are different, we utilize the importance weight strategy to alleviate the unreliable calibration for tail classes. The weight of each instance is the ratio between the target balanced probability density and the source im-balanced probability density. We explicitly model the dis-tribution of each class as a Gaussian distribution. Different from the source distribution, the target balanced distribution cannot be estimated directly. Since there exists common in-formation between head classes and tail classes [23], we transfer knowledge from head classes to estimate the target probability density. Normally, the more similar two classes are, the more information they share. Therefore, for each tail class, we measure the similarities between the distribu-tions of it and all head classes with the Wasserstein distance.
The similarity considers both first-order and second-order statistics of the two classes and thus can better reflect the transferability of statistical knowledge. Then we estimate the target probability density of each tail class by combin-ing its own distribution and the transferred information from all head classes referring to the similarities. Finally, we cal-ibrate the model with the importance weights.
Our contributions are summarized as: 1) We explore the problem of calibration under long-tailed distribution, which has important practical implications but is rarely studied.
We apply the importance weight strategy to enhance the es-timation of tail classes for more accurate calibration. 2) We propose an importance weight estimation method by view-ing distributions of head classes as prior for distributions of tail classes. For each tail class, our method estimates its probability density function from the distribution calibrated by head classes and calculates the importance weight to re-alize balanced calibration. 3) We conduct extensive exper-iments on the CIFAR-10-LT, CIFAR-100-LT [3], MNIST-LT [20], ImageNet-LT [24] datasets and the results demon-strate the effectiveness of our method. 2.