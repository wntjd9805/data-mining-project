Abstract
Nowadays, deep vision models are being widely de-ployed in safety-critical applications, e.g., autonomous driving, and explainability of such models is becoming a pressing concern. Among explanation methods, counter-factual explanations aim to find minimal and interpretable changes to the input image that would also change the output of the model to be explained. Such explanations point end-users at the main factors that impact the deci-sion of the model. However, previous methods struggle to explain decision models trained on images with many ob-jects, e.g., urban scenes, which are more difficult to work with but also arguably more critical to explain.
In this work, we propose to tackle this issue with an object-centric framework for counterfactual explanation generation. Our method, inspired by recent generative modeling works, en-codes the query image into a latent space that is structured in a way to ease object-level manipulations. Doing so, it provides the end-user with control over which search di-rections (e.g., spatial displacement of objects, style mod-ification, etc.) are to be explored during the counterfac-tual generation. We conduct a set of experiments on coun-terfactual explanation benchmarks for driving scenes, and we show that our method can be adapted beyond classifi-cation, e.g., to explain semantic segmentation models. To complete our analysis, we design and run a user study that measures the usefulness of counterfactual explanations in understanding a decision model. Code is available at https://github.com/valeoai/OCTET. 1.

Introduction
Deep learning models are now being widely deployed, notably in safety-critical applications such as autonomous driving. In such contexts, their black-box nature is a major concern, and explainability methods have been developed to improve their trustworthiness. Among them, counterfactual explanations have recently emerged to provide insights into a model’s decision [7, 52, 55]. Given a decision model and an input query, a counterfactual explanation is a data point that differs minimally but meaningfully from the query in
Figure 1. Counterfactual explanations generated by OCTET.
Given a classifier that predicts whether or not it is possible to go left, and a query image (top left), OCTET produces a counterfac-tual explanation where the most influential features that led to the decision are changed (top right). On the bottom row, we show that
OCTET can also operate under different settings that result in dif-ferent focused explanations. We report the prediction made by the decision model at the top left of each image. a way that changes the output decision of the model. By looking at the differences between the query and the expla-nation, a user is able to infer — by contrast — which ele-ments were essential for the model to come to its decision.
However, most counterfactual methods have only shown re-sults for explaining classifiers trained on single-object im-ages such as face portraits [4, 28, 29, 42, 46]. Aside from the technical difficulties of scaling up the resolution of the images, explaining decision models trained on scenes com-posed of many objects also present the challenge that those decisions are often multi-factorial. In autonomous driving, for example, most decisions have to take into account the position of all other road users, as well as the layout of the road and its markings, the traffic light and signs, the overall visibility, and many other factors.
In this paper, we present a new framework, dubbed
OCTET for Object-aware CounTerfactual ExplanaTions, to generate counterfactual examples for autonomous driving.
We leverage recent advances in unsupervised compositional generative modeling [14] to provide a flexible explanation method. Exploiting such a model as our backbone, we can
assess the contribution of each object of the scene indepen-dently and look for explanations in relation to their posi-tions, their styles, or combinations of both.
To validate our claims, extensive experiments are con-ducted for self-driving action decision models trained with the BDD100k dataset [60] and its BDD-OIA extension [59].
Fig. 1 shows counterfactual explanations found by OCTET: given a query image and a visual decision system trained to assess if the ego vehicle is allowed to go left or not, OCTET proposes a counterfactual example with cars parked on the left side that are moved closer. When inspecting specific items (bottom row of the figure), OCTET finds that moving the yellow car to the left, or adding a double line marking on the road, are ways to change the model’s decision. These explanations highlight that the absence of such elements on the left side of the road heavily influenced the model.
To sum up, our contributions are as follows:
• We tackle the problem of building counterfactual expla-nations for visual decision models operating on com-plex compositional scenes. We specifically target au-tonomous driving visual scenarios.
• Our method is also a tool to investigate the role of spe-cific objects in a model’s decision, empowering the user with control over which type of explanation to look for.
• We thoroughly evaluate the realism of our counter-factual images, the minimality and meaningfulness of changes, and compare against previous reference strate-gies. Beyond explaining classifiers, we also demon-strate the versatility of our method by addressing ex-planations for a segmentation network.
• Finally, we conduct a user-centered study to assess the usefulness of our explanations in a practical case. As standard evaluation benchmarks for counterfactual ex-planations are lacking a concrete way to measure the interpretability of the explanations, our user-centered study is a key element to validate the presented pipeline. 2.