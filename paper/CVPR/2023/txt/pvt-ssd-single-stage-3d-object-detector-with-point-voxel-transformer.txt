Abstract
Recent Transformer-based 3D object detectors learn point cloud features either from point- or voxel-based rep-resentations. However, the former requires time-consuming sampling while the latter introduces quantization errors.
In this paper, we present a novel Point-Voxel Transformer for single-stage 3D detection (PVT-SSD) that takes advan-tage of these two representations. Specifically, we first use voxel-based sparse convolutions for efficient feature encod-ing. Then, we propose a Point-Voxel Transformer (PVT) module that obtains long-range contexts in a cheap manner from voxels while attaining accurate positions from points.
The key to associating the two different representations is our introduced input-dependent Query Initialization mod-ule, which could efficiently generate reference points and content queries. Then, PVT adaptively fuses long-range contextual and local geometric information around refer-ence points into content queries. Further, to quickly find the neighboring points of reference points, we design the
Virtual Range Image module, which generalizes the native range image to multi-sensor and multi-frame. The experi-ments on several autonomous driving benchmarks verify the effectiveness and efficiency of the proposed method. Code will be available. 1.

Introduction 3D object detection from point clouds has become in-creasingly popular thanks to its wide applications, e.g., autonomous driving and virtual reality. To process un-ordered point clouds, Transformer [51] has recently at-tracted great interest as the self-attention is invariant to the permutation of inputs. However, due to the quadratic
∗This work was done when Honghui was an intern at Shanghai Arti-ficial Intelligence Laboratory.
†Corresponding author complexity of self-attention, it involves extensive compu-tation and memory budgets when processing large point clouds. To overcome this problem, some point-based meth-ods [29, 36, 37] perform attention on downsampled point sets, while some voxel-based methods [10, 33, 64] employ attention on local non-empty voxels. Nevertheless, the for-mer requires farthest point sampling (FPS) [41] to sam-ple point clouds, which is time-consuming on large-scale outdoor scenes [19], while the latter inevitably introduces quantization errors during voxelization, which loses accu-rate position information.
In this paper, we propose PVT-SSD that absorbs the ad-vantages of the above two representations, i.e., voxels and points, while overcoming their drawbacks. To this end, in-stead of sampling points directly, we convert points to a small number of voxels through sparse convolutions and sample non-empty voxels to reduce the runtime of FPS.
Then, inside the PVT-SSD, voxel features are adaptively fused with point features to make up for the quantization er-ror. In this way, both long-range contexts from voxels and accurate positions from points are preserved. Specifically,
PVT-SSD consists of the following components:
Firstly, we propose an input-dependent Query Initial-ization module inspired by previous indoor Transformer-based detectors [29, 36], which provides queries with bet-ter initial positions and instance-related features. Un-like [29, 36], our queries originate from non-empty voxels instead of points to reduce the sampling time. Concretely, with the 3D voxels generated by sparse convolutions, we first collapse 3D voxels into 2D voxels by merging voxels along the height dimension to further reduce the number of voxels. The sample operation is then applied to select a rep-resentative set of voxels. We finally lift sampled 2D voxels to generate 3D reference points. Subsequently, the corre-sponding content queries are obtained in an efficient way by projecting reference points onto a BEV feature map and indexing features at the projected locations.
Secondly, we introduce a Point-Voxel Transformer
module that captures long-range contextual features from voxel tokens and extracts fine-grained point features from point tokens. To be specific, the voxel tokens are obtained from non-empty voxels around reference points to cover a large attention range. In contrast, the point tokens are gener-ated from neighboring points near reference points to retain fine-grained information. These two different tokens are adaptively fused by the cross-attention layer based on the similarity with content queries to complement each other.
Furthermore, we design a Virtual Range Image mod-ule to accelerate the neighbor querying process in the point-voxel Transformer. With the constructed range image, refer-ence points can quickly find their neighbors based on range image coordinates. Unlike the native range image captured by LiDAR sensors, we can handle situations where multiple points overlap on the same pixel in the range image. There-fore, it can be used for complex scenarios, such as multiple sensors and multi-frame fusion.
Extensive experiments have been conducted on several detection benchmarks to verify the efficacy and efficiency of our approach. PVT-SSD achieves competitive results on
KITTI [13], Waymo Open Dataset [48], and nuScenes [3]. 2.