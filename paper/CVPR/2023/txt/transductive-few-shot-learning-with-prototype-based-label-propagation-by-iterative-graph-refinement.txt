Abstract
Few-shot learning (FSL) is popular due to its ability to adapt to novel classes. Compared with inductive few-shot learning, transductive models typically perform better as they leverage all samples of the query set. The two exist-ing classes of methods, prototype-based and graph-based, have the disadvantages of inaccurate prototype estimation and sub-optimal graph construction with kernel functions, respectively. In this paper, we propose a novel prototype-based label propagation to solve these issues. Specifically, our graph construction is based on the relation between prototypes and samples rather than between samples. As prototypes are being updated, the graph changes. We also estimate the label of each prototype instead of considering a prototype be the class centre. On mini-ImageNet, tiered-ImageNet, CIFAR-FS and CUB datasets, we show the pro-posed method outperforms other state-of-the-art methods in transductive FSL and semi-supervised FSL when some un-labeled data accompanies the novel few-shot task. 1.

Introduction
With the availability of large-scale datasets and the rapid development of deep convolutional architectures, super-vised learning exceeds in computer vision, voice, and ma-chine translation [23]. However, lack of data makes the ex-isting supervised models fail during the inference on novel tasks. As the annotation process may necessitate expert knowledge, annotations are may be scarce and costly (e.g., annotation of medical images).
In contrast, humans can learn a novel concept from just a single example.
Few-shot learning (FSL) aims to mimic the capabilities of biological vision [7] and it leverages metric learning, meta-learning, or transfer learning. The purpose of metric-based FSL is to learn a mapping from images to an embed-ding space in which images from the same class are closer
*The corresponding author. Code: https : / / github . com / allenhaozhu/protoLP
Figure 1. Drawbacks of prototype-based and graph-based FSL. (left) Some label assignments are incorrect due to the imperfect decision boundary. (right) Some “strong” links in the fixed graph are incorrect as they associate samples of different classes. together and images from other classes are separated. Meta-learning FSL performs task-specific optimisation with the goal to generalize to other tasks well. Pre-training a fea-ture extractor followed by adapting it for reuse on new class samples is an example of transfer learning.
Several recent studies [6, 11, 13, 16, 22, 29, 34, 35] ex-plored transductive inference for few-shot learning. At the test time, transductive FSL infer the class label jointly for all the unlabeled query samples, rather than for one sam-ple/episode at a time. Thus, transductive FSL typically out-performs inductive FSL. We categorise transductive FSL into: (i) FSL that requires the use of unlabeled data to esti-mate prototypes [2, 26, 27, 43, 44], and (ii) FSL that builds a graph with some kernel function and then uses label prop-agation to predict labels on query sets [22, 29, 61]. How-ever, the above two paradigms have their own drawbacks.
For prototype-based methods, they usually use the nearest neighbour classifier, which is based on the assumption that there exists an embedding where points cluster around a single prototype representation for each class. Fig. 1 (left) shows a toy example which is sensitive to the large within-class variance and low between-class variance. Thus, the
two prototypes cannot be estimated perfectly by the soft label assignment alone. Fig. 1 (right) shows that Label-Propagation (LP) and Graph Neural Network (GNN) based methods depend on the graph construction which is com-monly based on a specific kernel function determining the final result.
If some nodes are wrongly and permanently linked, these connections will affect the propagation step.
In order to avoid the above pitfalls of transductive FSL, we propose prototype-based Label-Propagation (protoLP).
Our transductive inference can work with a generic feature embedding learned on the base classes. Fig. 2 shows how to alternatively optimize a partial assignment between pro-totypes and the query set by (i) solving a kernel regression problem (or optimal transport problem) and (ii) a label prob-ability prediction by prototype-based label propagation. Im-portantly, protoLP does not assume the uniform class distri-bution prior while significantly outperforming other meth-ods that assume the uniform prior, as shown in ablations on the imbalanced benchmark [46] where methods relying on the balanced class prior fail. Our model outperforms state-of-the-art methods significantly, consistently providing im-provements across different settings, datasets, and training models. Our transductive inference is very fast, with run-times that are close to the runtimes of inductive inference.
Our contributions are as follows: i. We identify issues resulting from separation of proto-type-based and label propagation methods. We propose prototype-based Label Propagation (protoLP) for trans-ductive FSL, which unifies both models into one frame-work. Our protoLP estimates prototypes not only from the partial assignment but also from the prediction of la-bel propagation. The graph for label propagation is not fixed as we alternately learn prototypes and the graph. ii. By introducing parameterized label propagation step, we remove the assumption of uniform class prior while other methods highly depend on this prior. iii. We showcase advantages of protoLP on four datasets for transductive and semi-supervised learning, Our pro-toLP outperforms the state of the art under various set-tings including different backbones, unbalanced query set, and data augmentation. 2.