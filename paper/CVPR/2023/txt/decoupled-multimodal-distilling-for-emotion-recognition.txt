Abstract
Human multimodal emotion recognition (MER) aims to perceive human emotions via language, visual and acous-tic modalities. Despite the impressive performance of pre-vious MER approaches, the inherent multimodal hetero-geneities still haunt and the contribution of different modal-ities varies signiﬁcantly. In this work, we mitigate this issue by proposing a decoupled multimodal distillation (DMD) approach that facilitates ﬂexible and adaptive crossmodal knowledge distillation, aiming to enhance the discrimina-tive features of each modality. Specially, the representation of each modality is decoupled into two parts, i.e., modality-irrelevant/-exclusive spaces, in a self-regression manner.
DMD utilizes a graph distillation unit (GD-Unit) for each decoupled part so that each GD can be performed in a more specialized and effective manner. A GD-Unit consists of a dynamic graph where each vertice represents a modality and each edge indicates a dynamic knowledge distillation.
Such GD paradigm provides a ﬂexible knowledge trans-fer manner where the distillation weights can be automati-cally learned, thus enabling diverse crossmodal knowledge transfer patterns. Experimental results show DMD con-sistently obtains superior performance than state-of-the-art
MER methods. Visualization results show the graph edges in DMD exhibit meaningful distributional patterns w.r.t. the modality-irrelevant/-exclusive feature spaces. Codes are re-leased at https://github.com/mdswyz/DMD. 80 75 70 65 60 55
)
% ( y c a r u c c
A
Language
It is very loyal to the book
Vision
Acoustic (a) Unimodal Accuracy (b) Cross-modal Distillation  
Shared  Encoder
Language
It is very loyal to the book
Vision
Acoustic 1. Input
Homogeneous GD
Private Encoder 2. Feature Decoupling 3. Graph Distillation
Heterogeneous GD (c) Our proposed Decoupled Multimodal Distillation
Figure 1. (a) illustrates the signiﬁcant emotion recognition dis-crepancies using unimodality, adapted from Mult [28]. (b) shows the conventional cross-modal distillation. (c) shows our proposed decoupled multimodal distillation (DMD) method. DMD consists of two graph distillation (GD) units: homogeneous GD and hetero-geneous GD. The decoupled GD paradigm decreases the burden of absorbing knowledge from the heterogeneous data and allows each
GD to be performed in a more specialized and effective manner. 1.

Introduction
Human multimodal emotion recognition (MER) aims to perceive the sentiment attitude of humans from video clips [13, 17]. The video ﬂows involve time-series data from various modalities, e.g., language, acoustic, and vi-sion. This rich multimodality facilitates us in understanding human behaviors and intents from a collaborative perspec-tive. Recently, MER has become one of the most active re-* The corresponding author. search topics of affective computing with abundant appeal-ing applications, such as intelligent tutoring systems [24], product feedback estimation [18], and robotics [15].
For MER, different modalities in the same video seg-ment are often complementary to each other, providing ex-tra cues for semantic and emotional disambiguation. The core part of MER is multimodal representation learning and fusion, in which a model aims to encode and integrate repre-sentations from multiple modalities to understand the emo-tion behind the raw data. Despite the achievement of the  
mainstream MER methods [7, 28, 33], the intrinsic hetero-geneities among different modalities still perplex us and increase the difﬁculty of robust multimodal representation learning. Different modalities, e.g., image, language, and acoustic, contain different ways of conveying semantic in-formation. Typically, the language modality consists of lim-ited transcribed texts and has more abstract semantics than nonverbal behaviors. As illustrated in Fig. 1 (a), language plays the most important role in MER and the intrinsic het-erogeneities result in signiﬁcant performance discrepancies among different modalities [25, 28, 31].
One way to mitigate the conspicuous modality hetero-geneities is to distill the reliable and generalizable knowl-edge from the strong modality to the weak modality [6], as illustrated in Fig. 1 (b). However, such manual assign-ment for the distillation direction or weights should be cum-bersome because there are various potential combinations.
Instead, the model should learn to automatically adapt the distillation according to different examples, e.g, many emo-tions are easier to recognize via language while some are easier by vision. Furthermore, the signiﬁcant feature dis-tribution mismatch cross the modalities makes the direct crossmodal distillation sub-optimal [21, 37].
To this end, we propose a decoupled multimodal distil-lation (DMD) method to learn dynamic distillations across modalities, as illustrated in Fig. 1 (c). Typically, the features of each modality are decoupled into modality-irrelevant/-exclusive spaces via shared encoder and the private en-codes, respectively. As to achieve the feature decoupling, we devise a self-regression mechanism that predicts the decoupled modality features and then regresses them self-supervisedly. To consolidate the feature decoupling, we incorporate a margin loss that regularizes the proximity in relationships of the representations across modalities and emotions. Consequently, the decoupled GD paradigm would decrease the burden of absorbing knowledge from the heterogeneous data and allows each GD to be performed in a more specialized and effective manner.
Based on the decoupled multimodal feature spaces,
DMD utilizes a graph distillation unit (GD-Unit) in each space so that the crossmodal knowledge distillation can be performed in a more specialized and effective manner. A
GD-Unit consists of a graph that (1) vertices represent-ing the representations or logits from the modalities and (2) edges indicating the knowledge distillation directions and weights. As the distribution gap among the modality-irrelevant (homogeneous) features is sufﬁciently reduced,
GD can be directly applied to capture the inter-modality semantic correlations. For the modality-exclusive (hetero-geneous) counterparts, we exploit the multimodal trans-former [28] to build the semantic alignment and bridge the distribution gap. The cross-modal attention mechanism in the multimodal transformer reinforces the multimodal rep-resentations and reduces the discrepancy between the high-level semantic concepts that exist in different modalities.
For simpliﬁcation, we respectively name the distillation on the decoupled multimodal features as homogeneous graph knowledge distillation (HomoGD) and heterogeneous graph knowledge distillation (HeteroGD). The reformulation al-lows us to explicitly explore the interaction between differ-ent modalities in each decoupled space.
The contributions of this work can be summarized as:
• We propose a decoupled multimodal distillation framework, Decoupled Multimodal Distillation (DMD), to learn the dynamic distillations across modalities for robust MER. In DMD, we explic-itly decouple the multimodal representations into modality-irrelevant/-exclusive spaces to facilitate KD on the two decoupled spaces. DMD provides a ﬂexible knowledge transfer manner where the distillation directions and weights can be automatically learned, thus enabling ﬂexible knowledge transfer patterns.
• We conduct comprehensive experiments on public
MER datasets and obtain superior or comparable re-sults than the state-of-the-arts. Visualization results verify the feasibility of DMD and the graph edges exhibit meaningful distributional patterns w.r.t. Ho-moGD and HeteroGD. 2.