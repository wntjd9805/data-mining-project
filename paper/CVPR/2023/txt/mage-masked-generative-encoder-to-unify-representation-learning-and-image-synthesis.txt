Abstract
Generative modeling and representation learning are two key tasks in computer vision. However, these models are typically trained independently, which ignores the po-tential for each task to help the other, and leads to training and model maintenance overheads. In this work, we pro-pose MAsked Generative Encoder (MAGE), the ﬁrst frame-work to unify SOTA image generation and self-supervised representation learning. Our key insight is that using vari-able masking ratios in masked image modeling pre-training can allow generative training (very high masking ratio) and representation learning (lower masking ratio) under the same training framework.
Inspired by previous gen-erative models, MAGE uses semantic tokens learned by a vector-quantized GAN at inputs and outputs, combining this with masking. We can further improve the represen-tation by adding a contrastive loss to the encoder output.
We extensively evaluate the generation and representation learning capabilities of MAGE. On ImageNet-1K, a single
MAGE ViT-L model obtains 9.10 FID in the task of class-unconditional image generation and 78.9% top-1 accuracy for linear probing, achieving state-of-the-art performance in both image generation and representation learning. Code is available at https://github.com/LTH14/mage. 1.

Introduction
In recent years, we have seen rapid progress in both gen-erative models and representation learning of visual data.
Generative models have demonstrated increasingly spectac-ular performance in generating realistic images [3,7,15,46], while state-of-the-art self-supervised representation learn-ing methods can extract representations at a high seman-tic level to achieve excellent performance on a number of
∗This work was done when Tianhong Li interned at Google Research, and was partially supported by the MIT-IBM Watson Research Collabo-ration grant. Correspondence to: Tianhong Li <tianhong@mit.edu>,
Huiwen Chang <huiwenchang@google.com>.
Figure 1. Linear probing and class unconditional generation per-formance of different methods trained and evaluated on ImageNet-1K. MAGE achieves SOTA performance in linear probing and es-tablishes a new SOTA in class unconditional generation. downstream tasks such as linear probing and few-shot trans-fer [2, 6, 8, 13, 25, 26].
Currently, these two families of models are typically
Intuitively, since generation and trained independently. recognition tasks require both visual and semantic under-standing of data, they should be complementary when com-bined in a single framework. Generation beneﬁts represen-tation by ensuring that both high-level semantics and low-level visual details are captured; conversely, representation beneﬁts generation by providing rich semantic guidance.
Researchers in natural language processing have observed this synergy: frameworks such as BERT [14] have both high-quality text generation and feature extraction. Another example is DALLE-2 [43], where latents conditioned on a pre-trained CLIP representation are used to create high-quality text-to-image generations. in computer vision, there are currently no widely adopted models that unify image generation and representation learning in the same framework. Such uni-However,
Figure 2. Reconstruction results using MAE and MAGE with 75% masking ratio. MAE reconstructs blurry images with low quality, while MAGE can reconstruct high-quality images with detail, and further improves quality through iterative decoding (see subsection 3.2 for details). With the same mask, MAGE generates diverse reconstruction results with different random seeds. Note that the mask for
MAGE is on semantic tokens whereas that of MAE is on patches in the input image.
ﬁcation is non-trivial due to the structural difference be-in generative modeling, we output tween these tasks: high-dimensional data, conditioned on low-dimension in-puts such as class labels, text embeddings, or random noise.
In representation learning, we input a high-dimensional im-age and create a low-dimensional compact embedding use-ful for downstream tasks.
Recently, a number of papers have shown that represen-tation learning frameworks based on masked image mod-eling (MIM) can obtain high-quality representations [2, 18, 26,31], often with very high masking ratios (e.g. 75%) [26].
Inspired by NLP, these methods mask some patches at the input, and the pre-training task is to reconstruct the origi-nal image by predicting these masked patches. After pre-training, task-speciﬁc heads can be added to the encoder to perform linear probe or ﬁne-tuning.
These works inspire us to revisit the uniﬁcation question.
Our key insight in this work is that generation is viewed as
“reconstructing” images that are 100% masked, while rep-resentation learning is viewed as “encoding” images that are 0% masked. We can therefore enable a uniﬁed architecture by using a variable masking ratio during MIM pre-training.
The model is trained to reconstruct over a wide range of masking ratios covering high masking ratios that enable generation capabilities, and lower masking ratios that en-able representation learning. This simple but very effec-tive approach allows a smooth combination of generative training and representation learning in the same framework: same architecture, training scheme, and loss function.
However, directly combining existing MIM methods with a variable masking ratio is insufﬁcient for high qual-ity generation because such methods typically use a simple reconstruction loss on pixels, leading to blurry outputs. For example, as a representative of such methods, the recon-struction quality of MAE [27] is poor: ﬁne details and tex-tures are missing (Figure 2). A similar issue exists in many other MIM methods [11, 36].
This paper focuses on bridging this gap. We propose
MAGE, a framework that can both generate realistic im-ages and extract high-quality representations from images.
Besides using variable masking ratio during pre-training, unlike previous MIM methods whose inputs are pixels, both the inputs and the reconstruction targets of MAGE are semantic tokens. This design improves both generation and representation learning, overcoming the issue described above. For generation, as shown in Figure 2, operating in token space not only allows MAGE to perform image gen-eration tasks iteratively (subsection 3.2), but also enables
MAGE to learn a probability distribution of the masked to-kens instead of an average of all possible masked pixels, leading to diverse generation results. For representation learning, using tokens as inputs and outputs allows the net-work to operate at a high semantic level without losing low-level details, leading to signiﬁcantly higher linear probing performances than existing MIM methods.
We evaluate MAGE on multiple generative and repre-sentation downstream tasks. As shown in Figure 1, for class-unconditional image generation on ImageNet-1K, our method surpasses state of the art with both ViT-B and ViT-L (ViT-B achieves 11.11 FID [29] and ViT-L achieves 9.10
FID), outperforming the previous state-of-the-art result by a large margin (MaskGIT [7] with 20.68 FID). This signif-icantly push the limit of class-unconditional generation to a level even close to the state-of-the-art of class-conditional image generation (∼6 FID [7, 46]), which is regarded as a much easier task in the literature [38]. For linear probing on
ImageNet-1K, our method with ViT-L achieves 78.9% top-1 accuracy, surpassing all previous MIM-based represen-tation learning methods and many strong contrastive base-lines such as MoCo-v3 [13]. Moreover, when combined with a simple contrastive loss [9], MAGE-C with ViT-L can further get 80.9% accuracy, achieving state-of-the-art performance in self-supervised representation learning. We summarize our contributions:
• We introduce MAGE, a novel method that uniﬁes genera-tive model and representation learning by a single token-based MIM framework with variable masking ratios, in-troducing new insights to resolve the uniﬁcation problem.
• MAGE establishes a new state of the art on the task of class-unconditional image generation on ImageNet-1K.
• MAGE further achieves state of the art in different down-stream tasks, such as linear probing, few-shot learning, transfer learning, and class-conditional image generation. 2.