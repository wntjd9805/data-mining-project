Abstract 1.

Introduction
Large-scale Transformer models bring significant im-provements for various downstream vision language tasks with a unified architecture. The performance improvements come with increasing model size, resulting in slow inference speed and increased cost for severing. While some certain predictions benefit from the full computation of the large-scale model, not all of inputs need the same amount of com-putation to conduct, potentially leading to computation re-source waste. To handle this challenge, early exiting is pro-posed to adaptively allocate computational power in term of input complexity to improve inference efficiency. The exist-ing early exiting strategies usually adopt output confidence based on intermediate layers as a proxy of input complexity to incur the decision of skipping following layers. How-ever, such strategies cannot be applied to encoder in the widely-used unified architecture with both encoder and de-coder due to difficulty of output confidence estimation in the encoder layers. It is suboptimal in term of saving compu-tation power to ignore the early exiting in encoder compo-nent. To address this issue, we propose a novel early exiting strategy for unified vision language models, which allows to dynamically skip the layers in encoder and decoder si-multaneously in term of input layer-wise similarities with multiple times of early exiting, namely MuE. By decompos-ing the image and text modalities in the encoder, MuE is flexible and can skip different layers in term of modalities, advancing the inference efficiency while minimizing perfor-mance drop. Experiments on the SNLI-VE and MS COCO datasets show that the proposed approach MuE can reduce expected inference time by up to 50% and 40% while main-taining 99% and 96% performance respectively.
Recent advances in multi-modal Transformer-based
[25, 28, 48, 57] bring improve-large-scale models
Among ments across various vision language tasks. the Transformer-based models, the unified sequence-to-sequence architecture [38, 46] has attracted much attention due to its potential to become a universal computation engine to diverse tasks. Although large-scale models have achieved unattainable performance, their expensive computational cost usually hinders their applications in real-time scenarios.
While the scaling effect suggests that the performance of the model benefits from its increased size, not every in-put requires the same amount of computation to achieve similar performance. Such an observation is particularly valid in visual language tasks, where inputs from different modalities may require different amounts of computation.
Early exiting is a popular method to reduce computational costs by adaptively skipping layers on top of the input while preserving the general knowledge of the large-scale model.
Existing studies aim to deal with early exiting decisions for encoder-only models [51, 52] or decoder components in encoder-decoder architectures [8], but cannot induce early exiting decisions for both components at the same time.
Considering that single-component strategies may be sub-optimal in terms of saving computation cost, in this paper, we investigate how to perform early exiting for both en-coder and decoder components in a sequence-to-sequence architecture to elucidate a new way to further improve in-ference efficiency.
Given the varied complexity of the inputs, it is natural to consider skipping some layers of the encoder as well as the decoder. Current decision mechanisms use classi-fiers to predict the output confidence of intermediate repre-sentations and stop computation if the confidence reaches
ilar observation regarding saturation is also valid as shown in Figure 3. This observation lands the foundation that we could make the exiting decision based on the intermediate layer-wise input similarities without going through the fol-lowing layers. Besides, since the computation needed for input in different modalities usually varies, we propose a modality decomposition mechanism, which could further enable early fusion large-scale multi-modal models to break tie between modalities and enjoy the flexible exiting deci-sion over modalities. To encourage the early exiting behav-ior with a minimal performance loss, we design a layer-wise task loss, which enforce the each layer to output informative features for final task. Figure 1 shows the results on SNLI-VE dataset [49] and MS COCO [1] in term of expected time reduction rate and task performance. We compare MuE with several State-of-the-art early existing methods and ob-serve that MuE is able to largely reduce inference time with a minimal performance drop compared to other SoTA meth-ods. Our main contributions are summarized as follows:
• To the best of our knowledge, this is a pioneering work to extend early exiting choices to both encoder and de-coder of sequence-to-sequence architecture. To this end, we propose a novel early exiting strategy based on layer-wise input similarity with the valid assump-tion on saturation states in vision language models.
• Given the different characteristics of the modalities and tasks, we decompose the modalities encoder for early-fusion pre-trained models, bringing additional improvements in terms of inference efficiency.
• We introduce layer-wise task loss, linking each layer in the decoder to the final task, effectively helping to maintain task performance when a significant time re-duction is required.
• Extensive experiments show that our method can largely reduce the inference time of vision language models by up to 50% with minimal performance drop. 2.