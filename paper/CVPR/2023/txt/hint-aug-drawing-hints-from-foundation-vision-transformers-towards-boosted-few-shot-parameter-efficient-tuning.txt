Abstract
Despite the growing demand for tuning foundation vision transformers (FViTs) on downstream tasks, fully unleash-ing FViTs’ potential under data-limited scenarios (e.g., few-shot tuning) remains a challenge due to FViTs’ data-hungry nature. Common data augmentation techniques fall short in this context due to the limited features contained in the few-shot tuning data. To tackle this challenge, we first iden-tify an opportunity for FViTs in few-shot tuning: pretrained
FViTs themselves have already learned highly representa-tive features from large-scale pretraining data, which are fully preserved during widely used parameter-efficient tun-ing. We thus hypothesize that leveraging those learned fea-tures to augment the tuning data can boost the effective-ness of few-shot FViT tuning. To this end, we propose a framework called Hint-based Data Augmentation (Hint-Aug), which aims to boost FViT in few-shot tuning by aug-menting the over-fitted parts of tuning samples with the learned features of pretrained FViTs. Specifically, Hint-Aug integrates two key enablers: (1) an Attentive Over-fitting
Detector (AOD) to detect over-confident patches of founda-tion ViTs for potentially alleviating their over-fitting on the few-shot tuning data and (2) a Confusion-based Feature
Infusion (CFI) module to infuse easy-to-confuse features from the pretrained FViTs with the over-confident patches detected by the above AOD in order to enhance the feature diversity during tuning. Extensive experiments and ablation studies on five datasets and three parameter-efficient tuning techniques consistently validate Hint-Aug’s effectiveness: 0.04% ∼ 32.91% higher accuracy over the state-of-the-art (SOTA) data augmentation method under various low-shot settings. For example, on the Pet dataset, Hint-Aug achieves a 2.22% higher accuracy with 50% less training data over
SOTA data augmentation methods. 1.

Introduction
Foundation vision transformers (FViTs) [16, 41, 54, 55, 64] with billions of floating point operations (FLOPs) and parameters have recently demonstrated significant poten-tial in various downstream tasks [40, 41]. The success of
FViTs has ushered in a new paradigm in deep learning: pretraining-then-tuning [16, 40, 67], which first pretrains an
FViT on a large-scale dataset, then uses recently developed parameter-efficient tuning methods (e.g., visual prompt tun-ing (VPT) [34], visual prompting [2], LoRA [33], and
Adapter [72]) to tune pretrained FViTs on downstream tasks with limited tuning data. However, although it is highly de-sirable, effectively tuning pretrained FViTs for real-world applications, especially under few-shot tuning scenarios, re-mains a particularly challenging task. The reason is that al-though parameter-efficient tuning methods are dedicatedly designed for FViTs and can alleviate the overfitting issue by reducing the number of trainable parameters [2, 34, 72], the data-hungry nature of FViTs [16, 54] is not mitigated and thus the achievable accuracy under data-limited scenarios (e.g., few-shot tuning scenarios) are still limited. Therefore, how to effectively tune pretrained FViTs on various down-stream tasks with few-shot tuning is still an open question.
To enhance the effectiveness of parameter-efficient FViT tuning under few-shot settings, one promising direction is to leverage data augmentation techniques to increase the data diversity and thus the feature diversity of the models when being tuned on few-shot data, boosting the achievable accu-racy [12, 31, 68, 71]. Nevertheless, it has been shown that existing data augmentation techniques fall short in boost-ing the model accuracy under few-shot tuning scenarios.
This is because most of the existing data augmentation tech-niques are random-based (e.g., RandAugment [13], Au-toAugment [12], color jitter, mixup [71], and cutmix [68]), which only randomly permute existing features in the train-ing data and thus cannot generate new and meaningful features [63]. As illustrated in Fig. 1, we observe that neither the widely-used random-based data augmentation techniques (i.e., a dedicated combination of techniques in-cluding RandAugment [13], color jitter, and random eras-ing [74] as in [72]) nor training without data augmentation can consistently achieve a satisfactory accuracy across dif-†Our code is available at https://github.com/GATECH-EIC/Hint-Aug
training dataset in an input-adaptive manner.
• Our Hint-Aug framework integrates two enablers: (1) an
Attentive Over-fitting Detector (AOD) to identify the over-fitting samples and patches in the given training dataset by making use of the attention maps of pretrained FViTs and (2) a Confusion-based Feature Infusion (CFI) module to adaptively infuse pretrained FViTs’ learned features into the training data to better tuning those models on down-stream tasks, alleviating the commonly recognized chal-lenge of having limited features under few-shot tuning.
• Extensive experiments and ablation studies on five datasets and three parameter-efficient tuning techniques consis-tently validate the effectiveness of our proposed Hint-Aug framework, which achieves a 0.04% ∼ 32.91% higher accuracy over state-of-the-art (SOTA) data augmentation methods [72] across different datasets and few-shot set-tings. For example, on the Pets dataset, Hint-Aug achieves a 2.22% higher accuracy with 50% less training data com-pared with the SOTA augmentation method. 2.