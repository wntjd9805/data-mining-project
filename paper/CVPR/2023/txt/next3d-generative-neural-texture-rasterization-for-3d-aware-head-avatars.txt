Abstract 3D-aware generative adversarial networks (GANs) syn-thesize high-fidelity and multi-view-consistent facial images using only collections of single-view 2D imagery. Towards fine-grained control over facial attributes, recent efforts incorporate 3D Morphable Face Model (3DMM) to de-scribe deformation in generative radiance fields either ex-plicitly or implicitly. Explicit methods provide fine-grained expression control but cannot handle topological changes caused by hair and accessories, while implicit ones can model varied topologies but have limited generalization caused by the unconstrained deformation fields. We pro-pose a novel 3D GAN framework for unsupervised learn-ing of generative, high-quality and 3D-consistent facial avatars from unstructured 2D images. To achieve both de-formation accuracy and topological flexibility, we propose a 3D representation called Generative Texture-Rasterized
Tri-planes. The proposed representation learns Genera-tive Neural Textures on top of parametric mesh templates and then projects them into three orthogonal-viewed feature planes through rasterization, forming a tri-plane feature representation for volume rendering. In this way, we com-bine both fine-grained expression control of mesh-guided explicit deformation and the flexibility of implicit volumet-ric representation. We further propose specific modules for modeling mouth interior which is not taken into account by 3DMM. Our method demonstrates state-of-the-art 3D-aware synthesis quality and animation ability through ex-tensive experiments. Furthermore, serving as 3D prior, our animatable 3D representation boosts multiple applications including one-shot facial avatars and 3D-aware styliza-tion. Project page: https://mrtornado24.github.io/Next3D/.
Code: https://github.com/MrTornado24/Next3D. 1.

Introduction
Animatable portrait synthesis is essential for movie post-production, visual effects, augmented reality (AR), and vir-tual reality (VR) telepresence applications. Efficient ani-matable portrait generators should be capable of synthesiz-ing diverse high-fidelity portraits with full control of the rigid head pose, facial expressions and gaze directions at a fine-grained level. The main challenges of this task lie in how to model accurate deformation and preserve iden-tity through animation in the generative setting, i.e. training with only unstructured corpus of 2D images.
Several 2D generative models perform image anima-tion by incorporating the 3D Morphable Face Models (3DMM) [4] into the portrait synthesis [13, 16, 35, 52, 62, 65, 70, 73]. These 2D-based methods achieve photorealism but suffer from shape distortion during large motion due to a lack of geometry constraints. Towards better view con-sistency, many recent efforts incorporate 3DMM with 3D
GANs, learning to synthesize animatable and 3D consis-tent portraits from only 2D image collections in an unsu-pervised manner [3, 30, 39, 44, 60, 61, 68, 74]. Bergman et al. [3] propose an explicit surface-driven deformation field for warping radiance fields. While modeling accurate facial deformation, it cannot handle topological changes caused by non-facial components, e.g. hair, glasses, and other accessories. AnifaceGAN [68] builds an implicit 3DMM-conditioned deformation field and constrains animation ac-curacy by imitation learning. It achieves smooth animation on interpolated expressions, however, struggles to generate reasonable extrapolation due to the under-constrained de-formation field. Therefore, The key challenge of this task is modeling deformation in the 3D generative setting for ani-mation accuracy and topological flexibility.
In this paper, we propose a novel 3D GAN framework for unsupervised learning of generative, high-quality, and 3D-consistent facial avatars from unstructured 2D images.
Our model splits the whole head into dynamic and static parts, and models them respectively. For dynamic parts, the key insight is to combine both fine-grained expression con-trol of mesh-guided explicit deformation and flexibility of implicit volumetric representation. To this end, we propose a novel representation, Generative Texture-Rasterized Tri-planes, which learns the facial deformation through Gen-erative Neural Textures on top of a parametric template mesh and samples them into three orthogonal-viewed and axis-aligned feature planes through standard rasterization, forming a tri-plane feature representation. Such texture-rasterized tri-planes re-form high-dimensional dynamic sur-face features in a volumetric representation for efficient vol-ume rendering and thus inherit both the accurate control of the mesh-driven deformation and the expressiveness of vol-umetric representations. Furthermore, we represent static components (body, hair, background, etc.) by another tri-plane branch, and integrate both through alpha blending.
Another key insight of our method is to model the mouth interior which is not taken into account by 3DMM. Mouth interior is crucial for animation quality but often ignored by prior arts. We propose an efficient teeth synthesis mod-ule, formed as a style-modulated UNet, to complete the in-ner mouth features missed by the template mesh. To fur-ther regularize the deformation accuracy, we introduce a deformation-aware discriminator which takes as input syn-thetic renderings, encouraging the alignment of the final outputs with the 2D projection of the expected deformation.
To summarize, the contributions of our approach are:
• We present an animatable 3D-aware GAN framework for photorealistic portrait synthesis with fine-grained animation, including expressions, eye blinks, gaze di-rection and full head poses.
• We propose Generative Texture-Rasterized Triplanes, an efficient deformable 3D representation that inherits both fine-grained expression control of mesh-guided explicit deformation and flexibility of implicit volu-metric representation.
• Our learned generative animatable 3D representation can serve as a strong 3D prior and boost the down-stream application of 3D-aware one-shot facial avatars.
Our model also pushes the frontier of 3D stylization with high-quality out-of-domain facial avatars. 2.