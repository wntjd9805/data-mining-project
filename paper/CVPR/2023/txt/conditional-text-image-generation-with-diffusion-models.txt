Abstract
Current text recognition systems, including those for handwritten scripts and scene text, have relied heavily on image synthesis and augmentation, since it is difficult to re-alize real-world complexity and diversity through collect-ing and annotating enough real text images. In this paper, we explore the problem of text image generation, by taking advantage of the powerful abilities of Diffusion Models in generating photo-realistic and diverse image samples with given conditions, and propose a method called Conditional
Text Image Generation with Diffusion Models (CTIG-DM for short). To conform to the characteristics of text im-ages, we devise three conditions: image condition, text con-dition, and style condition, which can be used to control the attributes, contents, and styles of the samples in the im-age generation process. Specifically, four text image gen-eration modes, namely: (1) synthesis mode, (2) augmen-tation mode, (3) recovery mode, and (4) imitation mode, can be derived by combining and configuring these three conditions. Extensive experiments on both handwritten and scene text demonstrate that the proposed CTIG-DM is able to produce image samples that simulate real-world com-plexity and diversity, and thus can boost the performance of existing text recognizers. Besides, CTIG-DM shows its appealing potential in domain adaptation and generating images containing Out-Of-Vocabulary (OOV) words. 1.

Introduction
Text recognition has been an important research topic in the computer vision community for a long time, due
In the past few years, to its wide range of applications. numerous recognition methods for scene and handwritten text [3, 16, 43, 57, 58, 65, 69, 72] have been proposed, which have substantially improved the recognition accuracy on various benchmarks. The volume and diversity of data are crucial for high recognition performance, but it is extremely
†Corresponding author.
Figure 1. Handwritten text image samples from IAM [46] or gen-erated by our proposed CTIG-DM. On the left, the handwriting styles of the same word “and” written by different writers are con-siderably different, indicating the diversity of handwritten text and the challenge of handwritten text recognition. On the right, two images out of four in each row are written by the corresponding writer on the left. Can you distinguish them from the generated samples? (The answer will be revealed in the next page.)
Figure 2. Scene text image samples from Real-L [5] or produced by CTIG-DM. Only seven images herein are real. Can you identify them? (The answer will be revealed in the next page.) hard, if not impossible, to collect and label sufficient real text images, so majority of the existing recognition meth-ods rely heavily on data synthesis and augmentation.
Previously, a variety of data synthesis and augmentation methods [7, 17, 18, 20, 25, 33, 38, 44, 45, 67] have been pro-posed to enrich data for training stronger text recognition models.
In this paper, we investigate a technique that is highly related and complementary to such works. Draw-ing inspiration from the recent progress of Diffusion Mod-els [15, 48], we propose a text image generation model, which is able to conduct data synthesis, and thus can boost
the performance of existing text recognizers.
A recent study [15] has shown that State-Of-The-Art (SOTA) likelihood-based models [48] can outperform
GAN-based methods [8, 30, 68] in generating images. Dif-fusion models [23,48,59] have been becoming increasingly popular, due to their powerful generative ability in various vision tasks [2, 10, 11, 41, 55]. A typical representative of diffusion models is Denoising Diffusion Probabilistic Mod-els (DDPM) [23]. It generates diverse samples through dif-ferent initial states of simple distribution and each transi-tion. This means that it is challenging for DDPM to con-trol the content of the output image due to the randomness of the initial states and transitions. Guided-Diffusion [15] provides conditions to diffusion models by adding clas-sifier guidance. UnCLIP [53] further pre-trains a CLIP model [52] to match the image and whole text, which are used as the conditions for the diffusion models in image generation. While these approaches have focused on nat-ural images, images with handwritten or scene text have their unique characteristics (as shown in Fig. 1 and Fig. 2), which require not only image fidelity and diversity, but also content validity of the generated samples, i.e., the text con-tained in the images should be the same as specified in the given conditions.
In this paper, we present a diffusion model based condi-tional text image generator, termed Conditional Text Image
Generation with Diffusion Models (CTIG-DM for short).
To the best of our knowledge, this is one of the first works to introduce diffusion models into the area of text image gen-eration. The proposed CTIG-DM consists of a conditional encoder and a conditional diffusion model. Specifically, the conditional encoder generates three conditions, i.e., image condition, text condition, and style condition (the writing style of a specific writer). These conditions are proved to be critical for the fidelity and diversity of the generated text images. The conditional diffusion part uses these condi-tions to generate images from random Gaussian noise. As can be seen in Fig. 1 and Fig. 2, the quality of the images generated by CTIG-DM is quite high that one can hardly tell them from real images*. By combining the given con-ditions, four image generation modes can be derived, i.e., synthesis mode, augmentation mode, recovery mode, and imitation mode. With these modes, various text images that can be used to effectively boost the accuracy of existing text recognizers (see Sec. 4 for more details) could be produced.
Moreover, CTIG-DM shows its potential in handling OOV image generation and domain adaptation.
The contributions can be summarized as follows:
• We propose a text image generation method based on diffusion models, which is one of the first attempts to use diffusion models to generate text images.
*In Fig. 1, the real images are the first and last of each row. In Fig. 2, the real images are even numbered.
• We devise three conditions and four image generation modes, which can facilitate the generation of text im-ages with high validity, fidelity, and diversity.
• Experiments on both scene text and handwritten text demonstrate that CTIG-DM can significantly improve both the image quality and the performance of previ-ous text recognizers. Besides, CTIG-DM is effective in OOV image generation and domain adaptation. 2.