Abstract
We present Region-aware Open-vocabulary Vision
Transformers (RO-ViT) – a contrastive image-text pretrain-ing recipe to bridge the gap between image-level pretrain-ing and open-vocabulary object detection. At the pretrain-ing phase, we propose to randomly crop and resize regions of positional embeddings instead of using the whole image positional embeddings. This better matches the use of posi-tional embeddings at region-level in the detection ﬁnetuning phase. In addition, we replace the common softmax cross entropy loss in contrastive learning with focal loss to bet-ter learn the informative yet difﬁcult examples. Finally, we leverage recent advances in novel object proposals to im-prove open-vocabulary detection ﬁnetuning. We evaluate our full model on the LVIS and COCO open-vocabulary de-tection benchmarks and zero-shot transfer. RO-ViT achieves a state-of-the-art 32.1 APr on LVIS, surpassing the best ex-isting approach by +5.8 points in addition to competitive zero-shot transfer detection. Surprisingly, RO-ViT improves the image-level representation as well and achieves the state of the art on 9 out of 12 metrics on COCO and Flickr image-text retrieval benchmarks, outperforming competitive ap-proaches with larger models. 1.

Introduction
The ability to detect objects in the visual world is a hall-It is mark of computer vision and machine intelligence. key to enable many applications, e.g. autonomous agents adapting to new environments with many novel objects, a shopping system handling ﬁne-grained user queries such as
“fedora”, “bonnet” outside of the training vocabulary. How-ever, modern object detectors typically rely on manual an-notations of regions and class labels, which limits their vo-cabulary size to an order of 103 and it is prohibitively expen-sive to scale up further. This is orders of magnitude smaller than the objects we encounter in the visual world.
Recently, the open-vocabulary detection task (OVD) has been proposed to overcome such limitation by leveraging
Figure 1. Existing vision-language models are designed for image-level tasks, e.g., classiﬁcation and retrieval. In this paper, we aim to enhance the image-text pretraining for region-level downstream task: open-vocabulary object detection. At pretraining, we pro-pose to randomly crop and resize regions of positional embeddings (PE) instead of using the whole image PE. This better matches the use of PE at region-level in the detection ﬁnetuning and results in better performance in detection and surprisingly also retrieval task. abundant image-text pairs for training and ingesting the text queries provided by users at test time [54]. By treating the categories as text embeddings rather than discrete ids, open-vocabulary detectors can ﬂexibly predict a wide range of objects unseen during the training time. Most existing works leverage image-text pre-training which contains rich semantic knowledge of open-vocabulary concepts. Knowl-edge distillation [13, 19], weak supervision [63], self-training [41, 57, 60], and frozen model [29] have been pro-posed, and CNN backbones are most commonly used. With the growing popularity of vision transformers in image un-derstanding [12, 32, 56], multimodal [1, 2, 47], and self-supervised tasks [5, 6, 22], it is important to understand how to build capable open-vocabulary detectors with vision transformers [12, 35].
To our best knowledge, all existing works assume pre-trained Vision-Language Models (VLM) are given, and de-velop adaptation or ﬁnetuning recipes to bridge the gap between image-level pretraining and object-level ﬁnetun-ing [13, 19, 41, 57, 60]. Since the VLMs are designed for image-level tasks e.g. classiﬁcation, retrieval, the notion of objects/regions are not adequately utilized in the pretrain-ing process. We believe it would be beneﬁcial for open-vocabulary detection if we bake in locality information in the image-text pretraining.
We present RO-ViT, a simple recipe to pretrain vision transformers in a region-aware manner for open-vocabulary object detection. Standard pretraining typically uses full-image positional embeddings, which does not generalize well to detection tasks. Thus, we propose a novel posi-tional embedding scheme called “Cropped Positional Em-bedding” which better matches the use of region crops in detection ﬁnetuning (see Fig. 1). In addition, we found it beneﬁcial to replace the softmax cross entropy loss with fo-cal loss in contrastive learning, which affords us more con-trol to learn from harder and more informative examples.
Finally, we leverage recent advances in novel object pro-posals [28] to improve open-vocabulary detection ﬁnetun-ing. The motivation is that existing approaches often miss novel objects in the object proposal stage because the pro-posals tend to overﬁt to the foreground categories.
We evaluate the approach on the standard LVIS and
COCO open-vocabulary benchmarks. On LVIS, our best model achieves a state-of-the-art 32.1 APr at the system level, surpassing the best existing approach by +5.8 APr.
Compared to the best existing ViT-based approach, ours outperforms by a healthy margin of +6.5 APr. Our LVIS-trained model outperforms existing baselines on transfer detection to Objects365 without re-training. Although not explicitly optimized for retrieval, RO-ViT surprisingly achieves the state-of-the-art performance on 9 out of 12 metrics in image-text retrieval benchmark and outperforms strong baselines with standard positional embeddings, cross entropy loss, and larger model capacity. We conduct abla-tions to conﬁrm the beneﬁts of each proposed component.
Visualization of the learnt positional embeddings shows that our approach (CPE) leads to more symmetrical and diverse patterns than the baseline. In summary:
• We propose RO-ViT to bridge the positional embed-dings in image-text pretraining to open-vocabulary de-tection ﬁnetuning.
• We show that image-text pretraining with focal loss is more effective than existing softmax CE loss.
• We improve the open-vocabulary detection ﬁnetuning recipe with novel object proposals.
• RO-ViT achieves the state of the art on the LVIS open-vocabulary detection benchmark, and 9 out 12 metrics on COCO and Flickr image-text retrieval benchmarks.
We hope these ﬁndings will facilitate the research com-munity to further explore open-vocabulary detection from the perspective of image-text pretraining with potential ben-eﬁts for both region-level and image-level tasks. 2.