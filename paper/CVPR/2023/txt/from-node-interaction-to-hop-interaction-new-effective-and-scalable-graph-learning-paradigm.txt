Abstract
Existing Graph Neural Networks (GNNs) follow the message-passing mechanism that conducts information in-teraction among nodes iteratively. While considerable progress has been made, such node interaction paradigms still have the following limitation. First, the scalability lim-itation precludes the broad application of GNNs in large-scale industrial settings since the node interaction among rapidly expanding neighbors incurs high computation and memory costs. Second, the over-smoothing problem re-stricts the discrimination ability of nodes, i.e., node rep-resentations of different classes will converge to indistin-guishable after repeated node interactions. In this work, we propose a novel hop interaction paradigm to address these limitations simultaneously. The core idea is to convert the interaction target among nodes to pre-processed multi-hop features inside each node. We design a simple yet effective
HopGNN framework that can easily utilize existing GNNs to achieve hop interaction. Furthermore, we propose a multi-task learning strategy with a self-supervised learning ob-jective to enhance HopGNN. We conduct extensive exper-iments on 12 benchmark datasets in a wide range of do-mains, scales, and smoothness of graphs. Experimental re-sults show that our methods achieve superior performance while maintaining high scalability and efficiency. The code is at https://github.com/JC-202/HopGNN . 1.

Introduction
Graph Neural Networks (GNNs) have recently become very popular and have demonstrated great results in a wide range of graph applications, including social net-works [38], point cloud analysis [37] and recommenda-tion systems [21]. The core success of GNNs lies in the message-passing mechanism that iteratively conducts infor-Figure 1. Comparison of node interaction and hop interaction.
The hop interaction first pre-computes multi-hop features and then conducts non-linear interaction among different hops via GNNs, which enjoy high efficiency and effectiveness. mation interaction among nodes [8, 17, 18]. Each node in a graph convolution layer first aggregates information from local neighbors and combines them with non-linear trans-formation to update the self-representation [20, 27, 42]. Af-ter stacking K layers, nodes can capture long-range K-hop neighbor information and obtain representative representa-tions for downstream tasks [29, 45]. However, despite the success of such popular node interaction paradigms, the number of neighbors for each node would grow exponen-tially with layers [2, 40], resulting in the well-known scala-bility and over-smoothing limitation of GNNs.
The scalability limitation precludes the broad applica-tion of GNNs in large-scale industrial settings since the node interaction among rapidly expanding neighbors in-curs high computation and memory costs [15, 51]. Al-though we can reduce the size of neighbors by sampling techniques [9, 20], it still executes node interaction itera-tively during training, and the performance is highly sensi-tive to the sampling quality [15]. Recently, scalable GNNs that focus on simplifying or decoupling node interactions have emerged [16, 43, 52]. Such decoupled GNNs first pre-compute the linear aggregation of K-hop neighbors to gen-erate node features and then utilize the MLP to each node without considering the graph structure during training and inference. However, despite high efficiency and scalability, such methods lead to suboptimal results due to the lack of nonlinear interactions among nodes.
Another limitation is over-smoothing, which restricts the discriminative ability of nodes, i.e., node representations will converge to indistinguishable after repeated node in-teractions [5, 31]. On one hand, it causes performance de-generation when increasing layers of GNNs [27, 33]. On the other hand, in some heterophily graphs where con-nected nodes are usually from different classes, shallow
GNNs are also surprisingly inferior to pure Multi-ayer Per-ceptrons (MLPs) [36, 53]. The reason is the interaction among massive local inter-class neighbors would blur class boundaries of nodes [6, 22, 53]. Recently, to carefully con-sider the neighbor influence and maintain the node dis-crimination, emerging advanced node interaction GNNs, such as deep GNNs with residual connections [11, 29] and heterophilic-graph-oriented GNNs with adaptive aggrega-tion [4, 35, 39, 46], have achieved promising results. How-ever, these advanced node interactions suffer high compu-tational costs and fail to handle large-scale datasets.
These two limitations have typically been studied sepa-rately, as addressing one often necessitates compromising the other. However, can we bridge the two worlds, enjoying the low-latency, node-interaction-free of decoupled GNNs and the high discrimination ability of advanced node inter-action GNNs simultaneously? We argue that it is possible to transform the node interaction into a new hop interaction paradigm without losing performance, but drastically reduc-ing the computational cost. As shown in Figure 1, the core idea of hop interaction is to decouple the whole node in-teraction into two parts, the non-parameter hop feature pre-processing and non-linear interaction among hops. Inspired by the recommendation system, the non-linear interaction among different semantic features can enhance discrimina-tion [19], e.g., model the co-occurrence of career, sex and age of a user to identify its interest. By treating the precom-puted L hop neighbors as L semantic features within each node, we can consider node classification as a feature inter-action problem, i.e., model the non-linear hop interaction to obtain discriminative node representations.
To this end, we design a simple yet effective HopGNN framework to address the above limitation simultaneously.
It first pre-computes the multi-hop representation accord-ing to the graph structure. Then, without loss of generality, we can utilize GNNs over a multi-hop feature graph inside each node to achieve hop interaction flexibly and explicitly.
Specifically, we implement an attention-based interaction layer and average pooling for the HopGNN to fuse multi-hop features and generate the final prediction. Furthermore, to show the generality and flexibility of our framework, we provide a multi-task learning strategy that combines the self-supervised objective to enhance performance.
Our contributions are summarized as follows: 1. New perspective: We propose a new graph learn-ing paradigm going from node to hop interaction. It con-ducts non-linear interactions among pre-processed multi-hop neighbor features inside each node. 2. General and flexible framework: We design a simple yet effective HopGNN framework for hop interaction. Be-sides, the HopGNN is general and flexible to combine the self-supervised objective to easily enhance performance. 3. State-of-the-art performance: Experimental results show HopGNN achieves state-of-the-art performance on 12 graph datasets of diverse domains, sizes and smoothness while maintaining high scalability and efficiency. 2.