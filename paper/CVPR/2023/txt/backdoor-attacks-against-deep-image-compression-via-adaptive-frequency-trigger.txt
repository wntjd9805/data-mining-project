Abstract
Recent deep-learning-based compression methods have achieved superior performance compared with traditional approaches. However, deep learning models have proven to be vulnerable to backdoor attacks, where some specific trig-ger patterns added to the input can lead to malicious behav-ior of the models. In this paper, we present a novel backdoor attack with multiple triggers against learned image com-pression models. Motivated by the widely used discrete co-sine transform (DCT) in existing compression systems and standards, we propose a frequency-based trigger injection model that adds triggers in the DCT domain. In particu-lar, we design several attack objectives for various attack-ing scenarios, including: 1) attacking compression quality in terms of bit-rate and reconstruction quality; 2) attacking task-driven measures, such as down-stream face recogni-tion and semantic segmentation. Moreover, a novel simple dynamic loss is designed to balance the influence of differ-ent loss terms adaptively, which helps achieve more efficient training. Extensive experiments show that with our trained trigger injection models and simple modification of encoder parameters (of the compression model), the proposed attack can successfully inject several backdoors with correspond-ing triggers in a single image compression model. 1.

Introduction
Image compression is a fundamental task in the area of signal processing, and has been used in many applica-tions to store image data efficiently without much degrading the quality. Traditional image compression methods such as JPEG [46], JPEG2000 [26], Better Portable Graphics (BPG) [43], and recent Versatile Video Coding (VVC) [39] rely on hand-crafted modules for transforms and entropy coding to improve coding efficiency. With the rapid devel-opment of deep-learning techniques, various learning-based approaches [2, 7, 20, 36] adopt end-to-end trainable models
*Corresponding author.
Figure 1. Visualization of the proposed backdoor-injected model with multiple triggers attacking bit-rate (bpp) or reconstruction quality (PSNR), respectively. The second sample shows the result of the BPP attack with a huge increase in bit-rate, and the third one presents a PSNR attack with severely corrupted output. that integrate the pipeline of prediction, transform, and en-tropy coding jointly to achieve improved performance.
Together with the impressive performance of the deep neural networks, many concerns have been raised about their related AI security issues [23,54]. Primarily due to the lack of transparency in deep neural networks, it is observed that a variety of attacks can compromise the deployment and reliability of AI systems [24, 25, 53] in computer vi-sion, natural language processing, speech recognition, etc.
Among all these attacks, backdoor attacks have recently at-tracted lots of attention. As most SOTA models require ex-tensive computation resources and a lengthy training pro-cess, it is more practical and economical to download and directly adopt a third-party model with pretrained weights, which might face the threat from a malicious backdoor.
In general, a backdoor-injected model works as expected on normal inputs, while a specific trigger added to the clean input can activate the malicious behavior, e.g., incorrect pre-diction. Depending on the scope of the attacker’s access to the data, the backdoor attacks can be categorized into
poisoning-based and non-poisoning-based attacks [32]. In the scenario of poisoning-based attack [5, 14], attackers can only manipulate the dataset by inserting poisoned data. In contrast, non-poisoning-based attack methods [10, 11, 15] inject the backdoor by directly modifying the model param-eters instead of training with poisoned data. As image com-pression methods take the original input as a ground truth label, it is hard to perform a poisoning-based backdoor at-tack. Therefore, our work investigates a backdoor attack by modifying the parameter of only the encoder in a compres-sion model.
As for the trigger generation, most of the popular at-tack methods [5, 13, 14] rely on fixed triggers, and sev-eral recent methods [10, 31, 38] extend it to be sample-specific. While most previous papers focus on high-level vi-sion tasks (e.g., image classification and semantic segmen-tation), triggers in those works are added in only the spa-tial domain and may not perform well in low-level vision tasks such as image compression. Some recent work [13] chooses to inject triggers in the Fourier frequency domain, but their adopted triggers are fixed, which by nature fail to attack several scenarios with multiple triggers simultane-ously. Motivated by the widely used discrete cosine trans-form (DCT) in existing compression systems and standards, we propose a frequency-based trigger injection in the DCT domain to generate the poisoned images. Extensive ex-periments show that backdoor attacks also threaten deep-learning compression models and can cause much degra-dation once the attacking triggers are applied. As shown in Fig. 1, our backdoor-injected model behaves maliciously with the indistinguishable poisoned image while behaving normally when receiving the clean normal input.
To the best of our knowledge, backdoor attacks have been largely neglected in low-level computer vision re-search. In this paper, we make the first endeavor to inves-tigate backdoor attacks against learned image compression models. Our main contributions are summarized below.
• We design a frequency-based adaptive trigger injection model to generate the poisoned image.
• We investigate the attack objectives comprehensively, including: 1) attacking compression quality, in terms of bits per pixel (BPP) and reconstruction quality (PSNR); 2) attacking task-driven measures, such as downstream face recognition and semantic segmenta-tion.
• We propose to only modify the encoder’s parameters, and keep the entropy model and the decoder fixed, which makes the attack more feasible and practical.
• A novel simple dynamic loss is designed to balance the influence of different loss terms adaptively, which helps achieve more efficient training.
• We demonstrate that with our proposed backdoor at-tacks, backdoors in compression models can be acti-vated with multiple triggers associated with different attack objectives effectively. 2.