Abstract
Vision-centric joint perception and prediction (PnP) has become an emerging trend in autonomous driving research.
It predicts the future states of the traffic participants in the surrounding environment from raw RGB images. How-ever, it is still a critical challenge to synchronize features obtained at multiple camera views and timestamps due to inevitable geometric distortions and further exploit those spatial-temporal features. To address this issue, we pro-pose a temporal bird’s-eye-view pyramid transformer (TBP-Former) for vision-centric PnP, which includes two novel designs. First, a pose-synchronized BEV encoder is pro-posed to map raw image inputs with any camera pose at any time to a shared and synchronized BEV space for bet-ter spatial-temporal synchronization. Second, a spatial-temporal pyramid transformer is introduced to compre-hensively extract multi-scale BEV features and predict fu-ture BEV states with the support of spatial priors. Ex-tensive experiments on nuScenes dataset show that our proposed framework overall outperforms all state-of-the-art vision-based prediction methods. Code is available at: https://github.com/MediaBrain-SJTU/TBP-Former 1.

Introduction
As one of the most fascinating engineering projects, autonomous driving has been an aspiration for many re-searchers and engineers for decades. Although significant progress has been made, it is still an open question in de-signing a practical solution to achieve the goal of full self-driving. A traditional and common solution consists of a sequential stack of perception, prediction, planning, and control. Despite the idea of divide-and-conquer having achieved tremendous success in developing software sys-*These authors contributed equally to this work.
†Corresponding author.
Figure 1. Two major challenges in vision-based perception and prediction are (a) how to avoid distortion and deficiency when ag-gregating features across time and camera views; and (b) how to achieve spatial-temporal feature learning for prediction. Our Pose-Synchronized BEV Encoder can precisely map the visual fea-tures into synchronized BEV space, and Spatial-Temporal Pyra-mid Transformer extracts feature at multiple scales. tems, a long stack could cause cascading failures in an au-tonomous system. Recently, there is a trend to combine multiple parts in an autonomous system to be a joint mod-ule, cutting down the stack. For example, [25, 46] consider joint perception and prediction and [5,43] explore joint pre-diction and planning. This work focuses on joint perception and prediction.
The task of joint perception and prediction (PnP) aims to predict the current and future states of the surround-ing environment with the input of multi-frame raw sensor
data. The output current and future states would directly serve as the input for motion planning. Recently, many
PnP methods are proposed based on diverse sensor input choices. For example, [4, 25, 34] take multi-frame LiDAR point clouds as input and achieve encouraging 3D detec-tion and trajectory prediction performances simultaneously.
Recently, the rapid development of vision-centric methods offers a new possibility to provide a cheaper and easy-to-deploy solution for PnP. For instance, [1, 16, 17] only uses
RGB images collected by multiple cameras to build PnP systems. Meanwhile, without precise 3D measurements, vision-centric PnP is more technically challenging. There-fore, this work aims to advance this direction.
The core of vision-centric PnP is to learn appropriate spatial-temporal feature representations from temporal im-age sequences. It is a crux and difficult from three aspects.
First, since the input and the output of vision-centric PnP are supported in camera front-view (FV) and bird’s-eye-view (BEV) respectively, one has to deal with distortion is-sues during geometric transformation between two views.
Second, when the vehicle is moving, the view of the image input is time-varying and it is thus nontrivial to precisely map visual features across time into a shared and synchro-nized space. Third, since information in temporal image sequences is sufficiently rich for humans to accurately per-ceive the environment, we need a powerful learning model to comprehensively exploit spatial-temporal features.
To tackle these issues, previous works on vision-centric
PnP consider diverse strategies. For example, [16, 56] fol-lows the method in [38] to map FV features to BEV fea-tures, then synchronizes BEV features across time via rigid transformation, and finally uses a recurrent network to ex-ploit spatial-temporal features. However, due to the image discretization nature and depth estimation uncertainty, sim-ply relying on rigid geometric transformations would cause inevitable distortion; see Fig. 1. Some other work [49] transforms the pseudo feature point cloud to current ego coordinates and then pools the pseudo-lidar to BEV fea-tures; however, this approach encounters deficiency due to the limited sensing range in perception. Meanwhile, many works [16, 17, 56] simply employ recurrent neural networks to learn the temporal features from multiple BEV represen-tations, which is hard to comprehensively extract spatial-temporal features.
To promote more reliable and comprehensive feature learning across views and time, we propose the tempo-ral bird’s-eye-view pyramid transformer (TBP-Former) for vision-centric PnP. The proposed TBP-Former includes two key innovations: i) pose-synchronized BEV encoder, which leverages a pose-aware cross-attention mechanism to di-rectly map a raw image input with any camera pose at any time to the corresponding feature map in a shared and synchronized BEV space; and ii) spatial-temporal pyra-mid transformer, which leverages a pyramid architecture with Swin-transformer [28] blocks to learn comprehen-sive spatial-temporal features from sequential BEV maps at multiple scales and predict future BEV states with a set of future queries equipped with spatial priors.
Compared to previous works, the proposed TBP-Former brings benefits from two aspects. First, previous works [16, 17, 24, 56] consider FV-to-BEV transformation and tempo-ral synchronization as two separate steps, each of which could bring distortion due to discrete depth estimation and rigid transformation; while we merge them into one step and leverage both geometric transformation and attention-based learning ability to achieve spatial-temporal synchronization.
Second, previous works [16, 53] use RNNs or 3D convolu-tions to learn spatial-temporal features; while we leverage a powerful pyramid transformer architecture to comprehen-sively capture spatial-temporal features, which makes pre-diction more effective.
To summarize, the main contributions of our work are:
• To tackle the distortion issues in mapping temporal image sequences to a synchronized BEV space, we propose a pose-synchronized BEV encoder (PoseSync
BEV Encoder) based on cross-view attention mecha-nism to extract quality temporal BEV features.
• We propose a novel Spatial-Temporal Pyramid Trans-former (STPT) to extract multi-scale spatial-temporal features from sequential BEV maps and predict future
BEV states according to well-elaborated future queries integrated with spatial priors.
• Overall, we propose TBP-Former, a vision-based joint perception and prediction framework for autonomous driving. TBP-Former achieves state-of-the-art perfor-mance on nuScenes [2] dataset for the vision-based prediction task. Extensive experiments show that both
PoseSync BEV Encoder and STPT contribute greatly to the performance. Due to the decoupling property of the framework, both proposed modules can be eas-ily utilized as alternative modules in any vision-based
BEV prediction framework. 2.