Abstract
Modern mobile burst photography pipelines capture and merge a short sequence of frames to recover an enhanced image, but often disregard the 3D nature of the scene they capture, treating pixel motion between images as a 2D ag-gregation problem. We show that in a “long-burst”, forty-two 12-megapixel RAW frames captured in a two-second se-quence, there is enough parallax information from natural hand tremor alone to recover high-quality scene depth. To this end, we devise a test-time optimization approach that fits a neural RGB-D representation to long-burst data and simultaneously estimates scene depth and camera motion.
Our plane plus depth model is trained end-to-end, and per-forms coarse-to-fine refinement by controlling which multi-resolution volume features the network has access to at what time during training. We validate the method experi-mentally, and demonstrate geometrically accurate depth re-constructions with no additional hardware or separate data pre-processing and pose-estimation steps. 1.

Introduction
Over the last century we saw not only the rise and fall in popularity of film and DSLR photography, but of standalone cameras themselves. We’ve moved into an era of ubiquitous multi-sensor, multi-core, multi-use, mobile-imaging platforms [12]. Modern cellphones offer double-digit megapixel image streams at high framerates; optical image stabilization; on-board motion measurement devices such as accelerometers, gyroscopes, and magnetometers; and, most recently, integrated active depth sensors [43].
This latest addition speaks to a parallel boom in the field of depth imaging and 3D reconstruction [22, 84]. As users often photograph people, plants, food items, and other com-plex 3D shapes, depth can play a key role in object under-standing tasks such as detection, segmentation, and track-ing [32, 63, 80]. 3D information can also help compen-sate for non-ideal camera hardware and imaging settings through scene relighting [20, 55, 79], simulated depth-of-field effects [1, 71, 72], and frame interpolation [2]. Beyond
Figure 1. Our neural RGB-D model fits to a single long-burst im-age stack to distill high quality depth and camera motion. The model’s depth-on-a-plane decomposition can facilitate easy back-ground masking, segmentation, and image compositing. helping improve or understand RGB content, depth itself is a valuable output for simulating objects in augmented real-ity [5, 13, 44, 64] and interactive experiences [26, 36].
Depth reconstruction can be broadly divided into pas-sive and active approaches. Passive monocular depth esti-mation methods leverage training data to learn shape pri-ors [6, 30, 59] – e.g., what image features imply curved versus flat objects or occluding versus occluded structures – but have a hard time generalizing to out-of-distribution scenes [48,60]. Multi-view depth estimation methods lower this dependence on learned priors by leveraging parallax in-formation from camera motion [16, 69] or multiple cam-eras [45, 67] to recover geometrically-guided depth. The recent explosion in neural radiance field approaches [49,50, 66, 81] can be seen a branch of multi-view stereo where a system of explicit geometric constraints is swapped for a more general learned scene model. Rather than classic fea-ture extraction and matching, these models are fit directly to image data to distill dense implicit 3D information.
Active depth methods such as pulsed time-of-flight [46] (e.g., LiDAR), correlation time-of-flight [38], and struc-tured light [61, 83] use controlled illumination to help with depth reconstruction. While these methods are less re-liant on image content than passive ones, they also come with complex circuitry and increased power demands [28].
Thus, miniaturization for mobile applications results in very low-resolution sub-kilopixel sensors [8, 27, 74]. The Apple iPhone 12-14 Pro devices, which feature one of these minia-turized sensors, use depth derived from RGB, available at 12 mega-pixel resolution, to recover scene details lost in the sparse LiDAR measurements. While how exactly they use the RGB stream is unknown, occluding camera sensors re-veals that the estimated geometry is the result of monocular
RGB-guided depth reconstruction.
Returning to the context of mobile imaging, even several seconds of continuous mode photography, which we refer to as a “long-burst”, contain only millimeter-scale view vari-ation from natural hand tremor [11]. While these micro-baseline [33] shifts are effectively used in burst superreso-lution and denoising methods [58, 76] as indirect observa-tions of content between sensor pixels, 3D parallax effects on pixel motion are commonly ignored in these models as the depth recovered from this data is too coarse for sub-pixel refinement [31, 33, 82]. A recent work [11] demonstrates high-quality object reconstructions refined with long-burst
RGB data, but relies on the iPhone 12 Pro LiDAR sensor for initial depth estimates and device poses, not available on many other cellphones. They treat these poses as ground truth and explicitly solve for depth through minimization of photometric reprojection loss.
In this work, we devise an unsupervised end-to-end ap-proach to jointly estimate high-quality object depth and camera motion from more easily attainable unstabilized two-second captures of 12-megapixel RAW frames and gy-roscope data. Our method requires no depth initialization or pose inputs, only a long-burst. We formulate the prob-lem as an image synthesis task, similar to neural radiance methods [50], decomposed into explicit geometric projec-tion through continuous depth and pose models.
In con-trast to recent neural radiance methods, which typically es-timate poses in a pre-processing step, we jointly distill rela-tive depth and pose estimates as a product of simply fitting our model to long-burst data and minimizing photometric loss. In summary, we make the following contributions:
• An end-to-end neural RGB-D scene fitting approach that distills high-fidelity affine depth and camera pose estimates from unstabilized long-burst photography.
• A smartphone data collection application to capture
RAW images, camera intrinsics, and gyroscope data for our method, as well as processed RGB frames, low-resolution depth maps, and other camera metadata.
• Evaluations which demonstrate that our approach out-performs existing single and multi-frame image-only depth estimation approaches, with comparisons to high-precision structured light scans to validate the ac-curacy of our reconstructed object geometries.
Code, data, videos, and additional materials are available on our project website: https://light.princeton.edu/soap 2.