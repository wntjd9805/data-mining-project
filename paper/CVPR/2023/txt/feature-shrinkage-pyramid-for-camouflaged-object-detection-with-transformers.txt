Abstract
Vision transformers have recently shown strong global context modeling capabilities in camouflaged object detec-tion. However, they suffer from two major limitations: less effective locality modeling and insufficient feature aggre-gation in decoders, which are not conducive to camou-flaged object detection that explores subtle cues from in-distinguishable backgrounds. To address these issues, in this paper, we propose a novel transformer-based Feature
Shrinkage Pyramid Network (FSPNet), which aims to hi-erarchically decode locality-enhanced neighboring trans-former features through progressive shrinking for camou-flaged object detection. Specifically, we propose a non-local token enhancement module (NL-TEM) that employs the non-local mechanism to interact neighboring tokens and explore graph-based high-order relations within tokens to enhance local representations of transformers. Moreover, we design a feature shrinkage decoder (FSD) with adja-cent interaction modules (AIM), which progressively ag-gregates adjacent transformer features through a layer-by-layer shrinkage pyramid to accumulate imperceptible but effective cues as much as possible for object information decoding. Extensive quantitative and qualitative experi-ments demonstrate that the proposed model significantly outperforms the existing 24 competitors on three challeng-ing COD benchmark datasets under six widely-used evalu-ation metrics. Our code is publicly available at https:
//github.com/ZhouHuang23/FSPNet. 1.

Introduction
Camouflage is a common defense or tactic in organ-isms that “perfectly” blend in with their surroundings to deceive predators (prey) or sneak up on prey (hunters).
Camouflaged object detection (COD) [11] aims to segment camouflaged objects in the scene and has been widely ap-†Equal contributions. *Corresponding author: Tian-Zhu Xiang. including small,
Figure 1. Visual comparison of COD in different challeng-ing scenarios, large, multiple, occluded and boundary-uncertain camouflaged objects. Compared with the re-cently proposed ZoomNet [30] and SINet-v2 [10], our method pro-vides superior performance with more accurate object localization and more complete object segmentation, mainly due to the pro-posed locality-enhanced global context exploration and progres-sive shrinkage decoder. plied in species conservation [29], medical image segmen-tation [5, 20], and industrial defect detection [3], etc.
Due to the high similarities between camouflaged objects and their backgrounds, camouflaged objects are usually in-conspicuous and indistinguishable, which brings great chal-lenges to accurate detection. Recently, the development of deep learning and the availability of large-scale COD datasets (e.g., COD10K [11]) have significantly advanced camouflaged object detection. Numerous deep learning-based methods have been proposed, which can be roughly divided into three categories: targeted design of feature ex-ploration modules, multi-task joint learning frameworks,
and bio-inspired methods. Although these methods have made remarkable progress, they mainly rely heavily on con-volutional neural networks (CNNs), which cannot capture long-range dependencies due to the limited receptive fields, resulting in inferior performance for COD. As shown in
Fig. 1, recently proposed state-of-the-art CNN-based meth-ods (e.g., ZoomNet [30] and SINet-v2 [10]) fail to explore global feature relations and thus often provide predictions of incomplete object regions, especially for multiple ob-jects, large objects and occlusion cases. Although larger convolution kernels or simply stacking multiple convolu-tion layers with small kernels can enlarge receptive fields and thus alleviate this issue to some extent, it also dramat-ically increases the computational cost and the number of network parameters. Furthermore, studies [34] have shown that simply network deepening is ineffective for long-range dependency modeling.
Compared to CNNs, vision transformers (ViT) [7], which have recently been introduced into computer vision and demonstrated significant breakthroughs in various vi-sion applications [17], can efficiently model long-range de-pendencies with the self-attention operations and thus over-come the above drawbacks of CNNs-based models. Re-cently, the works of [47] and [24] have attempted to accom-modate transformers for COD and shown promising per-formance. These methods either employ transformer as a network component for feature decoding or utilize the off-the-shelf vision transformers as backbones for feature en-coding. Through a thorough analysis of these methods for
COD, we observe two major issues within existing tech-niques: 1) Less effective local feature modeling for trans-former backbones. We argue that both global context and local features play essential roles in COD tasks. However, we observe that most transformer-based methods lack a lo-cality mechanism for information exchange within local re-gions. 2) Limitations of feature aggregation in decoders.
Existing decoders (shown in Fig. 2 (a)-(d)) usually directly aggregate the features with significant information differ-ences (e.g., low-level features with rich details and high-level features with semantics), which tends to discard some inconspicuous but valuable cues or introduce noise, result-ing in inaccurate predictions. This is a big blow for the task of identifying camouflaged objects from faint clues.
To this end, in this paper, we propose a novel transformer-based Feature Shrinkage Pyramid Network, named FSPNet, which aims to hierarchically decode neigh-boring transformer features which are locality-enhanced global representations for camouflaged objects through pro-gressive shrinking, thereby excavating and accumulating rich local cues and global context of camouflaged objects in our encoder and decoder for accurate and complete camou-flaged object segmentation. Specifically, to complement lo-cal feature modeling in the transformer encoder, we propose a non-local token enhancement module (NL-TEM) which employs the non-local mechanism to interact neighboring similar tokens and explore graph-based high-level relations within tokens to enhance local representations. Further-more, we design a feature shrinkage decoder (FSD) with adjacent interaction modules (AIMs) which progressively aggregates adjacent transformer features in pairs through a layer-by-layer shrinkage pyramid architecture to accumu-late subtle but effective details and semantics as much as possible for object information decoding. Owing to the global context modeling of transformers, locality explo-ration within tokens and progressive feature shrinkage de-coder, our proposed model achieves state-of-the-art perfor-mance and provides an accurate and complete camouflaged object segmentation. Our main contributions are summa-rized as follows:
• We propose a non-local token enhancement module (NL-TEM) for feature interaction and exploration be-tween and within tokens to compensate for locality modeling of transformers.
• We design a feature shrinkage decoder (FSD) with the adjacent interaction module (AIM) to better aggregate camouflaged object cues between neighboring trans-former features through progressive shrinking for cam-ouflaged object prediction.
• Comprehensive experiments show that our proposed
FSPNet achieves superior performance on three widely-used COD benchmark datasets compared to 24 existing state-of-the-art methods. 2.