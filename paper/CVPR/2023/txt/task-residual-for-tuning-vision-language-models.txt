Abstract
Large-scale vision-language models (VLMs) pre-trained on billion-level data have learned general visual represen-In principle, the well-tations and broad visual concepts. learned knowledge structure of the VLMs should be inherited appropriately when being transferred to downstream tasks with limited data. However, most existing efficient transfer learning (ETL) approaches for VLMs either damage or are excessively biased towards the prior knowledge, e.g., prompt tuning (PT) discards the pre-trained text-based classifier and builds a new one while adapter-style tuning (AT) fully relies on the pre-trained features. To address this, we propose a new efficient tuning approach for VLMs named Task Resid-ual Tuning (TaskRes), which performs directly on the text-based classifier and explicitly decouples the prior knowledge of the pre-trained models and new knowledge regarding a target task. Specifically, TaskRes keeps the original classifier weights from the VLMs frozen and obtains a new classifier for the target task by tuning a set of prior-independent pa-rameters as a residual to the original one, which enables re-liable prior knowledge preservation and flexible task-specific knowledge exploration. The proposed TaskRes is simple yet effective, which significantly outperforms previous ETL methods (e.g., PT and AT) on 11 benchmark datasets while requiring minimal effort for the implementation. Our code is available at https://github.com/geekyutao/TaskRes. 1.

Introduction
Over the past decade, deep learning-based visual recogni-tion models [10, 18, 28, 55, 58] have achieved great success.
These state-of-the-art models are often trained on a large amount of image and discrete label pairs. The discrete la-bel is generated by converting a detailed textual description, e.g., “American curl cat”, into a simple scalar, which strik-ingly eases the computation of loss function. However, this also results in two evident limitations: (i) the rich semantics
*Equal contribution.
†Corresponding author.
Figure 1.
Performance comparison between Zero-shot CLIP
[49], CoOp [73], CLIP-Adapter [15], Tip-Adapter-F [71] and our
TaskRes on ImageNet with few shot settings. in the textual description are underused, and (ii) the trained models are limited to recognizing the close-set classes only.
Recent large-scale vision-language model (VLM) pre-training [1, 23, 33, 49, 69] eliminates those limitations by learning visual representations via textual supervision. For instance, texts and images are encoded and mapped into a unified space via a contrastive loss during pre-training [49].
The pre-trained text encoder can then be used to synthesize a text-based classifier for image recognition given the corre-sponding natural language descriptions as shown in Figure 2 (a). Those pre-trained VLMs have demonstrated a powerful transferability on a variety of downstream tasks in a zero-shot manner. However, the effectiveness of the aforementioned models heavily relies on their large-scale architectures and training datasets. For instance, CLIP [49] has up to 428 mil-lion parameters and is trained on 0.4 billion text-image pairs, while Flamingo [1] boasts up to 80 billion parameters and is trained on a staggering 2.1 billion pairs. This makes it im-practical to fully fine-tune the model on downstream tasks in a low-data regime.
For that reason, efficient transfer learning (ETL) [15, 71–73] on pre-trained VLMs has gained popularity. ETL represents transfer learning to downstream tasks in both
Figure 2. Illustration of (a) Zero-shot CLIP, (b) prompt tuning, (c) adapter-style tuning and (d) our proposed Task Residual Tuning (TaskRes).
Our method introduces a prior-independent task residual to the fixed pre-trained classifier (i.e., text embeddings of CLIP), being free of running the text encoder every time or extra architecture design. parameter- and data-efficient manner. The core of ETL is twofold: (i) properly inheriting the well-learned knowledge structure of VLMs, which is already transferable; (ii) effec-tively exploring the task-specific knowledge given limited data. However, most existing ETL approaches, i.e., prompt tuning (PT) [72, 73] and adapter-style tuning (AT) [15, 71], either damage the prior knowledge of VLMs or learn the new knowledge of a task in an inappropriate/insufficient way. For example, instead of using the pre-trained text-based classifier, CoOp [73] (in Figure 2 (b)) is proposed to learn a continuous prompt for synthesizing a completely new one, which inevitably causes the loss of previous knowl-edge. Consequently, CoOp underperforms Zero-shot CLIP by 1.03%/0.37% in 1-/2-shot learning on ImageNet (see Fig-ure 1).
In contrast, CLIP-Adapter [15] preserves the pre-trained classifier, but is excessively biased towards the prior knowledge when learning a new task, i.e., it transforms the pre-trained classifier weights to be task-specific as illustrated in Figure 2 (c). This results in an inferior new knowledge exploration, thereby a lower accuracy as shown in Figure 1.
For better ETL on pre-trained VLMs, we propose a new efficient tuning approach named Task Residual Tuning (TaskRes), which performs directly on the text-based classi-fier and explicitly decouples the old knowledge of the pre-trained models and the new knowledge for a target task.
The rationale is that the decoupling enables a better old knowledge inheritance from VLMs and a more flexible task-specific knowledge exploration, i.e., the learned knowledge w.r.t. the task is independent on the old knowledge. Specif-ically, TaskRes keeps the original classifier weights frozen and introduces a set of prior-independent parameters that are added to the weights. These additive parameters, tuned for adaptation to the target task, are thus named “task residual”.
To gain insight into how TaskRes works, we perform ex-tensive experiments across 11 benchmark datasets [73] and conduct a systematic investigation of learned task residu-als. The experimental results demonstrate that introducing task residual can significantly enhance the transfer perfor-mance. We visualize the correlation between the magnitude of learned task residual and the difficulty of transferring a pre-trained model to a downstream task, and observe that the magnitude increases with the transfer difficulty. This sug-gests the residual is automatically adapted to the task to fully explore the new knowledge, thereby achieving a new state-of-the-art performance on 11 diverse datasets. Furthermore, it is worth noting that our method requires minimal effort for the implementation, i.e., technically adding one line of code only. Our contributions are summarized below:
• We for the first time emphasize the necessity of a proper knowledge inheritance from pre-trained VLMs to downstream tasks via ETL, reveal the pitfalls of exist-ing tuning paradigms, and conduct an in-depth analysis to manifest that decoupling the old pre-trained knowl-edge and the new task-specific knowledge is the key.
• We propose a new efficient tuning approach named Task
Residual Tuning (TaskRes), which achieves a better old knowledge inheritance from VLMs and a more flexible task-specific knowledge exploration.
• TaskRes is convenient for use, which needs a few tuning parameters and effortless implementation. 2.