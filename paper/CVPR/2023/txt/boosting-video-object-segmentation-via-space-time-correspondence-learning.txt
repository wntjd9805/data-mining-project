Abstract
] 0 1
[
Current top-leading solutions for video object segmen-tation (VOS) typically follow a matching-based regime: for each query frame, the segmentation mask is inferred accor-ding to its correspondence to previously processed and the
ﬁrst annotated frames. They simply exploit the supervisory signals from the groundtruth masks for learning mask pre-diction only, without posing any constraint on the space-time correspondence matching, which, however, is the fundamen-tal building block of such regime. To alleviate this crucial yet commonly ignored issue, we devise a correspondence-aware training framework, which boosts matching-based VOS so-lutions by explicitly encouraging robust correspondence ma-tching during network learning. Through comprehensively exploring the intrinsic coherence in videos on pixel and ob-ject levels, our algorithm reinforces the standard, fully su-pervised training of mask segmentation with label-free, con-trastive correspondence learning. Without neither requiring extra annotation cost during training, nor causing speed de-lay during deployment, nor incurring architectural modiﬁ-cation, our algorithm provides solid performance gains on four widely used benchmarks, i.e., DAVIS2016&2017, and
YouTube-VOS2018&2019, on the top of famous matching-based VOS solutions. 1.

Introduction
In this work, we address the task of (one-shot) video ob-ject segmentation (VOS) [5,73,96]. Given an input video with groundtruth object masks in the ﬁrst frame, VOS aims at ac-curately segmenting the annotated objects in the subsequent frames. As one of the most challenging tasks in computer vision, VOS beneﬁts a wide range of applications including augmented reality and interactive video editing [72].
Modern VOS solutions are built upon fully supervised deep learning techniques and the top-performing ones [10, 12] largely follow a matching-based paradigm, where the object masks for a new coming frame (i.e., query frame) are
∗The ﬁrst two authors contribute equally to this work.
†Corresponding author.
Figure 1. (a-b) shows some correspondences between a reference frame and a query frame. (c) gives mask prediction. XMem [10], even a top-leading matching-based VOS solution, still suffers from unreliable correspondence. In contrast, with our correspondence-aware training strategy, robust space-time correspondence can be established, hence leading to better mask-tracking results. generated according to the correlations between the query frame and the previously segmented as well as ﬁrst anno-tated frames (i.e., reference frames), which are stored in an outside memory. It is thus apparent that the module for cross-frame matching (i.e., space-time correspondence modeling) plays the central role in these advanced VOS systems. Nev-ertheless, these matching-based solutions are simply trained under the direct supervision of the groundtruth segmenta-tion masks. In other words, during training, the whole VOS system is purely optimized towards accurate segmentation mask prediction, yet without taking into account any ex-plicit constraint/regularization on the central component — space-time correspondence matching. This comes with a le-gitimate concern for sub-optimal performance, since there is no any solid guarantee of truly establishing reliable cross-frame correspondence during network learning. Fig. 1(a) of-fers a visual evidence for this viewpoint. XMem [10], the latest state-of-the-art matching-based VOS solution, tends to struggle at discovering valid space-time correspondence; indeed, some background pixels/patches are incorrectly reco-gnized as highly correlated to the query foreground.
The aforementioned discussions motivate us to propose a new, space-time correspondence-aware training framework which addresses the weakness of existing matching-based
VOS solutions in an elegant and targeted manner. The core idea is to empower the matching-based solutions with en-hanced robustness of correspondence matching, through mi-ning complementary yet free supervisory signals from the inherent nature of space-time continuity of training video sequences. In more detail, we comprehensively investigate the coherence nature of videos on both pixel and object lev-i) pixel-level consistency: spatiotemporally proximate els: pixels/patches tend to be consistent; and ii) object-level co-herence: visual semantics of same object instances at differ-ent timesteps tend to retain unchanged. By accommodating these two properties to an unsupervised learning scheme, we give more explicit direction on the correspondence match-ing process, hence promoting the VOS model to learn dense discriminative and object-coherent visual representation for robust, matching-based mask tracking (see Fig. 1 (b-c)).
It is worth mentioning that, beyond boosting the segmen-tation performance, our space-time correspondence-aware training framework enjoys several compelling facets. First, our algorithm supplements the standard, fully supervised training paradigm of matching-based VOS with self-training of space-time correspondence. As a result, it does not cause any extra annotation burden. Second, our algorithm is fully compatible with current popular matching-based VOS solu-tions [10, 12], without particular adaption to the segmenta-tion network architecture. This is because the learning of the correspondence matching only happens in the visual embed-ding space. Third, as a training framework, our algorithm does not produce additional computational budget to the ap-plied VOS models during the deployment phase.
We make extensive experiments on various gold-standard
VOS datasets, i.e., DAVIS2016&2017 [52], and YouTube-VOS2018&2019 [86]. We empirically prove that, on the top of recent matching-based VOS models, i.e., STCN [12] and
XMem [10], our approach gains impressive results, surpass-ing all existing state-of-the-arts. Concretely, in multi-object scenarios, it improves STCN by 1.2%, 2.3%, and 2.3%, and
XMem by 1.5%, 1.2%, and 1.1% on DAVIS2017val,Youtube-VOS2018val, as well as Youtube-VOS2019val, respectively, in terms of J&F . Besides, it respectively promotes STCN and XMem by 0.4% and 0.7% on single-object benchmark dataset DAVIS2016val. 2.