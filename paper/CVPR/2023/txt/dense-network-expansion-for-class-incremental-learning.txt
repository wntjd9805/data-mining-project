Abstract
The problem of class incremental learning (CIL) is con-sidered. State-of-the-art approaches use a dynamic archi-tecture based on network expansion (NE), in which a task expert is added per task. While effective from a computa-tional standpoint, these methods lead to models that grow quickly with the number of tasks. A new NE method, dense network expansion (DNE), is proposed to achieve a better trade-off between accuracy and model complexity. This is accomplished by the introduction of dense connections be-tween the intermediate layers of the task expert networks, that enable the transfer of knowledge from old to new tasks via feature sharing and reusing. This sharing is imple-mented with a cross-task attention mechanism, based on a new task attention block (TAB), that fuses information across tasks. Unlike traditional attention mechanisms, TAB operates at the level of the feature mixing and is decoupled with spatial attentions. This is shown more effective than a joint spatial-and-task attention for CIL. The proposed
DNE approach can strictly maintain the feature space of old classes while growing the network and feature scale at a much slower rate than previous methods. In result, it out-performs the previous SOTA methods by a margin of 4% in terms of accuracy, with similar or even smaller model scale. 1.

Introduction
Deep learning has enabled substantial progress in com-puter vision. However, existing systems lack the human ability for continual learning, where tasks are learned incre-mentally. In this setting, tasks are introduced in sequential time steps t, and the dataset used to learn task t is only avail-able at the tth step. Standard gradient-based training is not effective for this problem since it is prone to catastrophic forgetting: the model overfits on task t and forgets the previ-ous tasks. This is unlike humans, who easily learn new tasks without forgetting what they know. While continual learn-ing can be posed for any topic in computer vision, most re-search has addressed classification and the class incremen-tal (CIL) setting [18]. In CIL, tasks consist of subsets of disjoint classes that are introduced sequentially. Most ap-proaches also allow the learning of task t to access a small buffer memory of examples from previous tasks.
Different strategies have been proposed to solve the CIL problem. Distillation methods [3, 8, 9, 12, 14, 18, 23, 24, 29] and parameter regularization methods [15, 32] regulate the new model by the output logits, intermediate features or important parameters. Gradient methods [16, 19, 28] esti-mate the null space of the existing feature space and project the gradients of the next task into this null space, so that the newly learned features are orthogonal to the previous ones. These methods try to fit all tasks into a single model, preserving properties of the feature space from one task to the next. This is, however, difficult to guarantee due to the scarcity of prior task data. Furthermore, as the number of tasks grows, the model will eventually run out of capacity to accommodate new tasks.
Network expansion (NE) methods [1, 22, 27, 30, 31] ad-dress these problems by freezing the model that solves the previous tasks and adding a new subnetwork per task. As il-lustrated in Figure 1, a network f 1 learned for task 1 is aug-mented with new networks f t, denoted as task experts, for each subsequent task t, and the feature spaces concatenated.
NE with cross connections (NEwC) methods [10,20,25] fur-ther add links across tasks (red lines in Figure 1) to further transfer knowledge from old to new tasks. Since the origi-nal features are always accessible for examples from previ-ous classes, these methods achieve the best performances on
CIL benchmarks. However, the process is very inefficient in terms of both model size and complexity. The right side of the figure shows the accuracy and size of several NE models on the CIFAR100 dataset. Best accuracies are obtained with larger networks and the model grows very quickly with the number of tasks. For most practical applications, this rate of growth is unsustainable.
While NE and NEwC achieve state of the art perfor-mance among CNN-based CIL methods, we show that they do not translate well to the more recent transformer archi-tecture [7]. Standard transformers learn spatial connections across image patches through a spatial attention mecha-nism. A natural CIL extension is to feed the input image to multiple heads, each corresponding to a task. Attention can
Figure 1. CIL by NE. Left: Given a new task t, a new branch, denoted the task t expert, is added while freezing existing experts. In classical NE, the model is simply replicated per task, originating a set of t independent models, whose outputs are concatenated into a classifier. In the proposed DNE scheme, a cross-task attention mechanism (red connections) is introduced to allow the re-use of knowledge from previous experts and smaller experts per task. Right:
Comparison of model accuracy and size for various implementations of NE, using DER [31] and multi-Dytox [9] models of different sizes per task expert, on the CIFAR100 dataset. Both accuracy and FLOPs are shown for the final model, which classifies 100 classes grouped into 6 tasks. then be computed between all image patches of all heads, leading to a Spatial-and-Task Attention (STA) mechanism.
This strategy has is commonly used in multi-modal trans-formers [2,21]. However, in CIL, the features generated per patch by different heads are extracted from exactly the same image region. Hence, their representations are highly simi-lar and STA is dominated by the attention between replicas of the same patch. Furthermore, because all patches are pro-cessed by all heads, the remaining attention is dispersed by a very large number of patch pairs. This leads to the frag-mentation of attention into a large number of small-valued entries, which severely degrades performances. To over-come this problem, we propose a Dense Network Expan-sion (DNE) strategy that disentangles spatial and cross-task attention. Spatial attention is implemented by the standard transformer attention mechanism. Cross-task attention is implemented by a novel task attention block (TAB), which performs attention at the level of feature-mixing, by replac-ing the multi-layer perceptron (MLP) block with an atten-tion module.
Overall, the paper makes four contributions. First, we point out that existing NE methods are unsustainable for most practical applications and reformulate the NE prob-lem, to consider the trade-off between accuracy and model size. Second, we propose the DNE approach to address this trade-off, leading to a CIL solution that is both accurate and parameter efficient. Third, we introduce an implementation of DNE based on individual spatial and cross-task atten-tions. Finally, extensive experiments show that DNE out-performs all previous CIL methods not only in terms of ac-curacy, but also of the trade-off between accuracy and scale. 2.