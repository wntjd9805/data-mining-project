Abstract
Vision transformers (ViTs) have been successfully de-ployed in a variety of computer vision tasks, but they are still vulnerable to adversarial samples. Transfer-based attacks use a local model to generate adversarial samples and di-rectly transfer them to attack a target black-box model. The high efficiency of transfer-based attacks makes it a severe security threat to ViT-based applications. Therefore, it is vi-tal to design effective transfer-based attacks to identify the deficiencies of ViTs beforehand in security-sensitive scenar-ios. Existing efforts generally focus on regularizing the in-put gradients to stabilize the updated direction of adversar-ial samples. However, the variance of the back-propagated gradients in intermediate blocks of ViTs may still be large, which may make the generated adversarial samples focus on some model-specific features and get stuck in poor lo-cal optima. To overcome the shortcomings of existing ap-proaches, we propose the Token Gradient Regularization (TGR) method. According to the structural characteristics of ViTs, TGR reduces the variance of the back-propagated gradient in each internal block of ViTs in a token-wise man-ner and utilizes the regularized gradient to generate adver-sarial samples. Extensive experiments on attacking both
ViTs and CNNs confirm the superiority of our approach.
Notably, compared to the state-of-the-art transfer-based at-tacks, our TGR offers a performance improvement of 8.8% on average. 1.

Introduction
Transformers have been widely deployed in the natu-ral language processing, achieving state-of-the-art perfor-mance. Vision transformer (ViT) [5] first adapts the trans-former structure to the computer vision, and manifests ex-cellent performance. Afterward, diverse variants of ViTs have been proposed to further improve its performance
*Corresponding author.
Figure 1. Illustration of our Token Gradient Regularization (TGR) method. The red-colored entry represents the back-propagated gradient with extreme values. The back-propagated gradients cor-responding to one token in the internal blocks of ViTs are called the token gradients. Since we regularize the back-propagated gradients in a token-wise manner, we eliminate the token gradi-ents (marked with crosses) where extreme gradients locate during back-propagation to reduce the gradient variance. We then use the regularized gradients to generate adversarial samples.
[2, 26] and broaden its application to different computer vi-sion tasks [42, 43], which makes ViTs a well-recognized successor for convolutional neural networks (CNNs). Un-fortunately, recent studies have shown that ViTs are still vulnerable to adversarial attacks [1, 22], which add human-imperceptible noise to a clean image to mislead deep learn-ing models.
It is thus of great importance to understand
DNNs [13, 27, 28, 35] and devise effective attacking meth-ods to identify their deficiencies before deploying them in safety-critical applications [16, 17, 37].
Adversarial attacks can be generally partitioned into two categories. The first category is the white-box attack, where attackers can obtain the structures and weights of the tar-get models for generating adversarial samples. The second one is the black-box attack, where attackers cannot fetch the information of the target model. Among different black-box attacks, transfer-based methods employ white-box at-tacks to attack a local source model and directly transfer
the generated adversarial sample to attack the target black-box model. Due to their high efficiency and applicability, transfer-based attacks pose a serious threat to the security of
ViT-based applications in practice. Therefore, in this work, we focus on transfer-based attacks on ViTs.
There are generally two branches of transfer-based at-tacks in the literature [15]. The first one is based on input transformation, which aims to combine the input gradients of multiple transformed images to generate transferable per-turbations. Complementary to such methods, the second branch is based on gradient regularization, which modifies the back-propagated gradient to stabilize the update direc-tion of adversarial samples and escape from poor local op-tima. For example, Variance Tuning Method (VMI) [29] tunes the input gradient to reduce the variance of input gra-dients. However, the variance of the back-propagated gra-dients in intermediate blocks of ViTs may still be large, which may make the generated adversarial samples focus on some model-specific features with extreme gradient val-ues. As a result, the generated adversarial samples may still get stuck in poor local optima and possess limited transfer-ability across different models.
To address the weaknesses of existing gradient regularization-based approaches, we propose the Token
Gradient Regularization (TGR) method for transferable ad-versarial attacks on ViTs. According to the architecture of
ViTs, TGR reduces the variance of the back-propagated gra-dient in each internal block of ViTs and utilizes the regular-ized gradient to generate adversarial samples.
More specifically, ViTs crop one image into small patches and treat these patches as a sequence of input tokens to fit the architecture of the transformer. The output tokens of internal blocks in ViTs correspond to the extracted inter-mediate features. Therefore, we view token representations as basic feature units in ViTs. We then examine the back-propagated gradients of the classification loss with respect to token representations in each internal block of ViTs, which we call token gradient in this work. As illustrated in Figure 1, we directly eliminate the back-propagated gra-dients with extreme values in a token-wise manner until ob-taining the regularized input gradients, which we used to update the adversarial samples. Consequently, we can re-duce the variance of the back-propagated gradients in in-termediate blocks of ViTs and produce more transferable adversarial perturbations.
We conducted extensive experiments on the ImageNet dataset to validate the effectiveness of our proposed attack method. We examined the transferability of our generated adversarial samples to different ViTs and CNNs. Notably, compared with the state-of-the-art benchmarks, our pro-posed TGR shows a significant performance improvement of 8.8% on average.
We summarize the contributions of this work as below:
• We propose the Token Gradient Regularization (TGR) method for transferable adversarial attacks on ViTs.
According to the architectures of ViTs, TGR regu-larizes the back-propagated gradient in each internal block of ViTs in a token-wise manner and utilizes the regularized gradient to generate adversarial samples.
• We conducted extensive experiments to validate the ef-fectiveness of our approach. Experimental results con-firm that, on average, our approach can outperform the state-of-the-art attacking method with a significant margin of 8.8% on attacking ViT models and 6.2% on attacking CNN models.
• We showed that our method can be combined with other compatible attack algorithms to further enhance the transferability of the generated adversarial sam-ples. Our method can also be extended to use CNNs as the local source models. 2.