Abstract
Reconstructing two hands from monocular RGB images is challenging due to frequent occlusion and mutual con-fusion. Existing methods mainly learn an entangled rep-resentation to encode two interacting hands, which are in-credibly fragile to impaired interaction, such as truncated hands, separate hands, or external occlusion. This pa-per presents ACR (Attention Collaboration-based Regres-sor), which makes the ﬁrst attempt to reconstruct hands in arbitrary scenarios. To achieve this, ACR explicitly miti-gates interdependencies between hands and between parts by leveraging center and part-based attention for feature extraction. However, reducing interdependence helps re-lease the input constraint while weakening the mutual rea-soning about reconstructing the interacting hands. Thus, based on center attention, ACR also learns cross-hand prior that handle the interacting hands better. We eval-uate our method on various types of hand reconstruction datasets. Our method signiﬁcantly outperforms the best interacting-hand approaches on the InterHand2.6M dataset while yielding comparable performance with the state-of-the-art single-hand methods on the FreiHand dataset. More qualitative results on in-the-wild and hand-object interac-tion datasets and web images/videos further demonstrate the effectiveness of our approach for arbitrary hand recon-struction. Our code is available at this link 1. 1.

Introduction 3D hand pose and shape reconstruction based on a single
RGB camera plays an essential role in various emerging ap-plications, such as augmented and virtual reality (AR/VR), human-computer interaction, 3D character animation for movies and games, etc. However, this task is highly chal-lenging due to limited labeled data, occlusion, depth am-biguity, etc. Earlier attempts [1, 2, 36, 39] level down the problem difﬁculty and focus on single-hand reconstruction.
These methods started from exploring weakly-supervised
*Corresponding author. 1https://github.com/ZhengdiYu/Arbitrary-Hands-3D-Reconstruction 2Durham University toby.breckon@durham.com arphid@gmail.com
Figure 1. Given a monocular RGB image, our method makes the ﬁrst attempt to reconstruct hands under arbitrary scenarios by representation disentanglement and interaction mutual reasoning while the previous state-of-the-art method IntagHand [15] failed. learning paradigms [2] to designing more advanced network models [31]. Although single-hand approaches can be ex-tended to reconstruct two hands, they generally ignore the inter-occlusion and confusion issues, thus failing to handle two interacting hands.
To this end, recent research has shifted toward recon-structing two interacting hands. Wang et al. [33] extract multi-source complementary information to reconstruct two interacting hands simultaneously. Rong et al. [27] and
Zhang et al. [35] ﬁrst obtain initial prediction and stack in-termediate results together to reﬁne two-hand reconstruc-tion. The latest work [15] gathers pyramid features and two-hand features as input for a GCN-based network that regresses two interacting hands unitedly. These methods share the same principle: treating two hands as an integral
and learning a uniﬁed feature to ultimately reﬁne or regress the interacting-hand model. The strategy delivers the ad-vantage of explicitly capturing the hands’ correlation but in-evitably introduces the input constraint of two hands. This limitation also makes the methods particularly vulnerable and easily fail to handle inputs containing imperfect hand interactions, including truncation or external occlusions.
This paper takes the ﬁrst step toward reconstructing two hands in arbitrary scenarios. Our ﬁrst key insight is lever-aging center and part attention to mitigate interdependen-cies between hands and between parts to release the input constraint and eliminate the prediction sensitivity to a small occluded or truncated part. To this end, we propose At-tention Collaboration-based Regressor (ACR). Speciﬁcally, it comprises two essential ingredients: Attention Encoder (AE) and Attention Collaboration-based Feature Aggrega-tor (ACFA). The former learns hand-center and per-part at-tention maps with a cross-hand prior map, allowing the net-work to know the visibility of both hands and each part be-fore the hand regression. The latter exploits the hand-center and per-part attention to extract global and local features as a collaborative representation for regressing each hand inde-pendently and subsequently enhance the interaction model-ing by cross-hand prior reasoning with an interaction ﬁeld.
In contrast to the existing method, our method provides more advantages, such as hand detector free. Furthermore, experiments show that ACR achieves lower error on the In-terHand2.6M dataset than the state-of-the-art interacting-hand methods, demonstrating its effectiveness in handling interaction challenges. Finally, results on in-the-wild im-ages or video demos indicate that our approach is promis-ing for real-world application with the powerful aggregated representation for arbitrary hands reconstruction.
Our key contributions are summarized as: (1) we take the ﬁrst step toward reconstructing two hands at arbi-trary scenarios. (2) We propose to leverage both cen-ter and part based representation to mitigate interde-pendencies between hands and between parts and re-lease the input constraint. (3) In terms of modeling for in-teracting hands, we propose a cross-hand prior reason-ing module with an interaction ﬁeld to adjust the de-pendency strength. (4) Our method outperforms exist-ing state-of-the-art approaches signiﬁcantly on the In-terHand2.6M benchmark. Furthermore, ACR is the most practical method for various in-the-wild application scenes among all the prior arts of hand reconstruction. 2.