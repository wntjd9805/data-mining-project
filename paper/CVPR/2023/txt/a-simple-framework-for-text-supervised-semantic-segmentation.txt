Abstract
Text-supervised semantic segmentation is a novel re-search topic that allows semantic segments to emerge with image-text contrasting. However, pioneering methods could be subject to speciﬁcally designed network architectures.
This paper shows that a vanilla contrastive language-image pre-training (CLIP) model is an effective text-supervised se-mantic segmentor by itself. First, we reveal that a vanilla
CLIP is inferior to localization and segmentation due to its optimization being driven by densely aligning visual and language representations.
Second, we propose the locality-driven alignment (LoDA) to address the problem, where CLIP optimization is driven by sparsely aligning lo-cal representations. Third, we propose a simple segmenta-tion (SimSeg) framework. LoDA and SimSeg jointly amelio-rate a vanilla CLIP to produce impressive semantic segmen-tation results. Our method outperforms previous state-of-the-art methods on PASCAL VOC 2012, PASCAL Context and COCO datasets by large margins. Code and models are available at github.com/muyangyi/SimSeg. 1.

Introduction
Semantic segmentation is a fundamental task in com-puter vision, with the purpose of allocating semantic classes to the corresponding pixels. Most existing methods for se-mantic segmentation are restricted by the scale of datasets.
The quantity or category is insufﬁcient due to the high cost of annotating segmentation masks. Text-supervised seman-tic segmentation makes a breakthrough for this challenge, where models are pre-trained with image-text pairs and zero-shot transferred to semantic segmentation.
Figure 1 illustrates an abstraction of text-supervised semantic segmentation in comparison with existing task paradigms. The base domain is denoted as
DB, which contains the manually labeled samples. The target do-* Corresponding authors.
† Work was done when the ﬁrst author was an intern at ByteDance.
!!
!!
!#
!"
!#
!!
!"
!!
!"
Open-Vocabulary
Zero-Shot Weakly Supervised Text-Supervised  seg maps fine-tune  module (Ours) seg maps fine-tune  module
Text
Encoder
Image
Encoder
Text
Encoder
Image
Encoder image-text pairs 
+ seg annotations
LSeg image-text pairs 
+ seg annotations
OpenSeg seg maps seg maps non-universal  backbone
GroupViT
Text
Encoder
Text
Encoder
Image
Encoder image-text pairs
GroupViT image-text pairs
SimSeg (Ours)
Figure 1. A comparison of our proposed approach with existing paradigms, where
O denote base domain, target do-main and open domain, respectively. The components in red are those missing in SimSeg. Illustration inspired by [50].
B,
T ,
D
D
D main is denoted as
DT , which contains test samples. And open domain
DO involves a large variety of linguistic in-formation.
It can provide additional textual descriptions when segmenting the images. Open-vocabulary meth-ods (e.g., LSeg [22], OpenSeg [17]) use pre-trained vision-and-language models [20, 33], but still need annotated sam-ples to ﬁne-tune. Weakly supervised methods [1, 2] are free from mask labels but require image-level class la-DT ✓D O). Text-supervision is an annotation-free bels ( scheme, eliminating the need for mask annotations (
DB) or image-level labels (i.e.,
DO). Text-supervision lever-DT * ages massive web image-text pairs and enables to generate segmentation masks in a zero-shot manner. GroupViT [44] is the ﬁrst work of text-supervision, yet the non-universal backbone design hinders its ﬂexibility (e.g., novel backbone
adaptation and multi-task joint learning). We could improve current methods by creating a simple framework for text-supervised semantic segmentation. To this end, we target on the vanilla CLIP [33] architecture, a neat dual-stream contrastive language-image pre-training model.
As the preliminary of this work, we explore the potential problems of a vanilla CLIP-based segmentor. We mainly study CLIP developed with Transformer-based encoders due to their intrinsic properties for segmentation [6] and su-perior performance. CLIP is originally driven by aligning vision and textual holistic vectors (e.g., [cls] tokens from
Transformer-based encoders), and a simple revision facili-tates CLIP models for segmentation, i.e., densely aligning all image patches and caption words. A similarity map, which describes correlations between all image patches and one class word, is a coarse categorical segmentation mask per se. However, we observe two problems that greatly sup-press the ability of the CLIP-based segmentor: (1) Visual encoder of the learned CLIP model focuses on contextual pixels, and (2) image-text contrasting mainly relies on con-textual words. These problems jointly reveal that the opti-mization of CLIP is signiﬁcantly driven by contextual in-formation. As a consequence, the CLIP-based segmentor yields poor semantic segmentation results, due to an infe-rior ability to perceive both contextual and non-contextual information in complex natural images.
In the following, we attempt to solve the above prob-lems. One practical strategy is avoiding optimization with contextual information. For a versatile segmentor, both con-textual and non-contextual information are essential. Con-textual and non-contextual pixels should be sparsely aligned to corresponding text entities. To this end, we propose a locality-driven alignment (LoDA) strategy for training
CLIP models. Firstly, we propose to select partial features with the maximum responses, in both image and text modal-ities. Secondly, we propose to drive the image-text con-trasting with only selected features. Our proposal success-fully solves the problems from two aspects: (1) Vision en-coder perceives main objects, (2) main objects and context are equally signiﬁcant in the image-text contrasting. Cou-pled with LoDA, a simple but effective framework named
SimSeg is proposed to do zero-shot semantic segmentation.
Beneﬁting from our proposals, a simple CLIP framework is equipped with impressive zero-shot semantic segmentation performances. Our contributions are three-fold:
• We reveal the problems of a vanilla CLIP attached with
Transformer-based encoders when producing segmenta-tion masks. To solve the problems, we propose a training strategy named locality-driven alignment (LoDA).
• We design a simple but effective text-driven zero-shot semantic segmentation framework named SimSeg. Our proposed LoDA and SimSeg jointly allow a simple CLIP to segment universal categories.
• We achieve remarkable improvements over previous methods on PASCAL VOC, PASCAL Context and
COCO zero-shot segmentation tasks. Moreover, we pro-vide extensive analyses and ablations of our proposals. 2.