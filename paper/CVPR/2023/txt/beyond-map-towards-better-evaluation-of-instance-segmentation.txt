Abstract
Correctness of instance segmentation constitutes count-ing the number of objects, correctly localizing all predic-tions and classifying each localized prediction. Average
Precision is the de-facto metric used to measure all these constituents of segmentation. However, this metric does not penalize duplicate predictions in the high-recall range, and cannot distinguish instances that are localized correctly but categorized incorrectly. This weakness has inadvertently led to network designs that achieve significant gains in AP but also introduce a large number of false positives. We therefore cannot rely on AP to choose a model that provides an optimal tradeoff between false positives and high recall.
To resolve this dilemma, we review alternative metrics in the literature and propose two new measures to explicitly measure the amount of both spatial and categorical dupli-cate predictions. We also propose a Semantic Sorting and
NMS module to remove these duplicates based on a pixel occupancy matching scheme. Experiments show that mod-ern segmentation networks have significant gains in AP, but also contain a considerable amount of duplicates. Our Se-mantic Sorting and NMS can be added as a plug-and-play module to mitigate hedged predictions and preserve AP. 1.

Introduction
Tasks like classification and semantic segmentation have a fixed output space, i.e. the K-dimensional probability dis-tribution of the classes and the per-pixel semantic class re-spectively. For classification, we can use the zero-one loss, and for semantic segmentation we can use a per-pixel cross entropy loss. On the other hand, instance segmentation is a challenging problem because the output is a set contain-ing an arbitrary number of objects, and the network does not have knowledge of the number of objects in the scene apriori. Therefore, the model has to count the correct num-ber of objects in the scene, localize them all and classify
†Correspondence to: rjena@seas.upenn.edu
*Equal contribution them correctly. Deep learning for instance segmentation has two broad paradigms - top-down and bottom-up instance segmentation. In bottom-up instance segmentation, the im-age is converted into per-pixel features, and pixel features are aggregated to predict objects. This is typically done by grouping or clustering the pixels based on some similarity in the feature space [2, 7, 13, 30, 36, 41]. In top-down in-stance segmentation, a model proposes a set of candidate proposals, out of which proposals not containing an object are removed. This leaves us with a smaller set of propos-als which are further passed into a localization and clas-sification branch. This is typically followed by an NMS step, since an object may have multiple candidate propos-als, so duplicates must be removed. Popular approaches are dominated by top-down methods where the network re-gresses a bounding box, mask, and category. Mask-RCNN
[14, 16, 24] approaches it as a two-stage problem: localize the object, then predict the associated instance segmenta-tion mask. SOLO [37, 38] builds on an anchor-free frame-work and directly regresses an object segmentation using a spatial grid feature as a probe. More recent work based on Transformers ( [6, 12]) explicitly learn a query in the network memory, then refines this prediction. We can interpret all these top-down methods as implementing the query-key paradigm. Each uses different query designs: an-chor box-based object proposal for Mask R-CNN, grid-cell for SOLO, or learnable latent features for DETR/QueryInst.
The Query-Key interaction aims to extract different repre-sentations of the object: ROI pooled features for MaskR-CNN, center-based convolution filters for SOLO, and cross-attention features in DETR.
In analyzing why top-down methods consistently per-form better than bottom-up methods, we make an un-usual observation. The qualitative performance of bottom-up methods is at par with that of top-down methods, but there is a significant gap in mAP. Upon further analysis of the precision-recall curves in top-down methods, we find that mAP can be increased by increasing the number of low-confidence predictions. We observe that recent design choices in the literature has exacerbated this problem. In
CNN [16], a two-stage detector that predicts masks from proposed boxes after RoIAlign operation on feature maps.
YOLACT [5] generates non-local prototype masks in an ef-fort to learn and linearly combine them by predicting a set of mask coefficients. However, it relies on accurate bound-ing box predictions, and doesn’t learn to localize far-away instances. BlendMask [7] attempts to combine FCIS [21] and YOLACT [5] in a hybrid approach. Moving away from box-based object detection, SOLO [38] and CondInst
[34] take an anchor-free approach and use position-sensitive query to extract object masks directly from the feature map.
All these approaches are driven in a top-down manner, where a few query points (often object centers) are respon-In contrast, sible for predicting the whole object shape. bottom-up approaches focus on grouping pixels into an in-stance. These approaches, including Hough-voting [13,20], pixel affinity [18,25], Watershed methods [2], pixel embed-ding [19, 26, 27], can be thought of as ‘flow’ based: each pixel directly or indirectly learns to flow towards the ob-ject center. This ‘flow’ helps to group pixels to its ob-ject center, either in the image space or in a latent feature space, therefore localizing all objects simultaneously. How-ever, bottom-up methods are generally worse at localizing smaller objects, dealing with occlusion and crowded ob-jects, and require complex aggregation and post-processing techniques [7, 23]. 2.2. Evaluation of detection & segmentation
Average Precision (AP) [11] is the de-facto metric for measuring the performance of object detection and segmen-tation models. Hoiem et al. [17] provide a way to diagnose the effects of false positives and how they can be mitigated to improve mAP. TIDE [4] also provides a toolkit to iden-tify and decompose the error (1 - mAP) into its constituent error components - such as classification, localization, du-plication errors. This can allow a researcher to analyse the major shortcomings of a given detection and segmentation method. AP is therefore widely accepted in the community, and has remained unchallenged as a measure. However, re-cent works have pointed out different shortcomings in mAP as a reliable metric. Dave et al. [10] show that in a large vo-cabulary detection task, it is possible to gamify the AP met-ric by adding nonsensical predictions to a given prediction model. LRP [28] highlights two major problems with mAP: 1) different detectors having different P/R curves can have similar APs, but they have different underlying shortcom-ings, and 2) mAP is not sufficient to quantify localization.
LRP also acts as a desirable performance measure in terms of setting an optical confidence score threshold per class, unlike mAP which is optimal at a confidence threshold of 0 for any given model. Our work is similar to [10], but we show that AP can be ‘gamed’ by adding low-confidence false positives, even with a moderate vocabulary task like
Figure 1. Top: Toy example demonstrating how AP changes with a reordering of the same set of detections (9 TPs, 1FP). Note that in (b) the last FP doesn’t contribute to AP. A detection that does not predict this example will also have the same AP. The last pre-diction in (b) is therefore a hedged prediction. Middle, Bottom:
SOLOv2 with Matrix and Mask NMS respectively for the same network parameters. (c) shows the qualitative result and (d) is the corresponding P/R curve for the image. Note that hedged predic-tions do not penalize AP. (e) shows the P/R curve for airplane category over entire COCO val dataset. Note that AP increases by 1 point, but number of false positives increase 3-fold. this work, we take a step back and analyze how mAP can be
‘gamed’ by increasing false positives, explore other metrics in the literature, and propose metrics that explicitly quan-tify this amount of false positives, both spatially and cate-gorically. Furthermore, we propose a Semantic Sorting and
NMS module to improve all metrics related to this excessive amount of prediction, only with a minimal dip in mAP. 2.