Abstract
Generative models show good potential for recovering 3D faces beyond limited shape assumptions. While plau-sible details and resolutions are achieved, these models easily fail under extreme conditions of pose, shadow or appearance, due to the entangled ﬁtting or lack of multi-view priors. To address this problem, this paper presents a novel Neural Proto-face Field (NPF) for unsupervised robust 3D face modeling.
Instead of using constrained images as Neural Radiance Field (NeRF), NPF disentan-gles the common/speciﬁc facial cues, i.e., ID, expression and scene-speciﬁc details from in-the-wild photo collec-tions. Speciﬁcally, NPF learns a face prototype to aggregate 3D-consistent identity via uncertainty modeling, extract-ing multi-image priors from a photo collection. NPF then learns to deform the prototype with the appropriate facial expressions, constrained by a loss of expression consistency and personal idiosyncrasies. Finally, NPF is optimized to ﬁt a target image in the collection, recovering speciﬁc details
In this way, the generative of appearance and geometry. model beneﬁts from multi-image priors and meaningful fa-cial structures. Extensive experiments on benchmarks show that NPF recovers superior or competitive facial shapes and textures, compared to state-of-the-art methods. 1.

Introduction 3D face reconstruction is a long-standing problem with applications including games, digital human and mobile photography. It is ill-posed in many cases requiring strong assumptions e.g., shape from shading [99]. With the 3D
Morphable Model (3DMM) [10] proposed, such a problem can be solved by ﬁtting parameters to the target faces [67, 68, 107]. Recently, deep-learning methods [22, 25, 43, 64,
*Chengjie Wang and Ying Tai are corresponding authors (cid:11)(cid:68)(cid:12) (cid:50)(cid:88)(cid:85)(cid:86) (cid:47)(cid:36)(cid:51) (cid:39)(cid:22)(cid:39)(cid:41)(cid:53) (cid:11)(cid:69)(cid:12) (cid:40)(cid:42)(cid:22)(cid:39) (cid:14) (cid:51)(cid:55)(cid:44) (cid:43)(cid:72)(cid:68)(cid:71)(cid:16) (cid:49)(cid:72)(cid:53)(cid:41) (cid:50)(cid:88)(cid:85)(cid:86) (cid:42)(cid:72)(cid:82)(cid:80)(cid:72)(cid:87)(cid:85)(cid:92) (cid:53)(cid:82)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81) (cid:39)(cid:72)(cid:73)(cid:82)(cid:85)(cid:80)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
Figure 1. (a) Comparison with graphics-renderer-based methods
LAP [100] and D3DFR [20]. Our method models geometry de-tails and photo-realistic texture. (b) Results of neural rendering methods EG3D [13] + PTI [66], HeadNeRF [34] and our method.
Our method produces high-quality geometry, robust texture mod-eling under rotation and deformation. 105] are proposed to regress 3DMM parameters from in-put images. These approaches are then improved by non-linear modeling [24, 29, 79, 81, 84, 94] and multi-view con-sistency [7, 15, 76, 90]. Besides 3DMM methods, recent ef-forts [91, 100] attempt to model 3D face without shape as-sumptions. These non-parametric methods have potential ability to improve the modeling quality beyond 3DMM.
Although the aforementioned methods achieve impres-sive performance, they also have obvious drawbacks. On the one hand, as the parametric models are usually built from a small amount of subjects (e.g., BFM [58] with 200 subjects) and rigidly controlled conditions, they may be fragile to large variations of identity [106], and have limi-tations on building teeth, skin details or anatomic grounded muscles [23]. On the other hand, all of these methods de-pend on graphics renderers [42, 44, 46] in the analysis-by-synthesis ﬁtting procedure, and thus yields hand-crafted ap-proximation or ill-posed decomposition on intrinsic clues.
Hence, as illustrated in Fig. 1-(a), these methods struggle to produce photo-realistic texture or geometric details.
Against these limitations, efforts are made to use a neu-ral renderer such as StyleGAN [38, 39] to model faces by inverting the corresponding images [1,2] into W space. Ex-isting methods [11, 18, 59, 62, 92] mainly learn to embed 3DMM coefﬁcients to implicitly leverage 3D clues, but they have difﬁculty achieving precise 3D controls due to their en-tangled image formation. To disentangle neural rendering, recent works [13, 14, 34, 54] employ explicit 3D pipelines, e.g., Neural Radiance Field (NeRF) [52] into the Style-GANs’ framework, so that face shapes and camera views can be extracted. In this way, precise 3D controls and de-tailed geometry can be obtained. However, these methods still show fragile performance under challenging conditions as shown in Fig. 1-(b). When confronting large poses, ex-treme appearance or lighting, the lack of facial priors dis-turbs the reconstruction and results in severe distortions.
This is due to the essentially overﬁtting objective of invert-ing single target image, where the geometry ambiguity is unavoidable.
On top of this, one solution is to leverage reliable pri-ors, e.g., multi-image consistency as a complement. While
NeRF provides a natural paradigm to dig multi-view cues, it requires fully constrained images that are difﬁcult to obtain.
Even conditioned by style codes [13, 14, 54], there is no di-rect way to build 3D faces from unconstrained portrait col-lections in such a neural rendering mechanism. In this work, we present a novel Neural Proto-face Field (NPF) for un-supervised robust 3D face modeling, where ID, expression and scene-speciﬁc details can be disentangled from in-the-wild photo collections. To aggregate ID-aware cues, NPF leverages uncertainty modeling to extract multi-image pri-ors and recovers a face prototype with ID-consistent face shape. To disentangle the expression, NPF then learns appropriate representations to deform the prototype, con-strained by a expression consistency loss. In this way, the learned face shape is properly blended to avoid geometric ambiguity. Finally, to recover the scene-speciﬁc details,
NPF is optimized to ﬁt a target image in the collection. The robustness of ﬁtting is guaranteed by a geometry and ap-pearance regularization. As shown in Fig. 1-(b), NPF makes the generative method beneﬁt from multi-image priors in unconstrained environments, and produces high-quality 3D faces under challenging conditions.
In summary, our contributions are as follows: 1) A novel Neural Proto-face Field (NPF) is proposed to disentangle ID, expression and speciﬁc details from 3D face
Methods
EMOCA [81], DECA [24], Unsup3D [91]
LAP [100], FML [76], MVF [90]
DFG [18], StyleRig [77], StyleFlow [3]
Pi-GAN [14], StyleSDF [54], EG3D [13]
Ours
Rendering
Graphics
Graphics
Neural
Neural
Neural
Pipeline
Disentangled
Disentangled
Entangled
Disentangled
Disentangled
Multi-view
× (cid:2)
×
× (cid:2)
Table 1. Discussion with selected existing methods. modeling, which uses in-the-wild photo collections to ben-eﬁt the 3D generative model under challenging conditions. 2) With a novel face prototype aggregation method, NPF integrates multi-image face priors against the large varia-tions in unconstrained environments. 3) With a series of novel consistency losses, NPF is well
ﬁt to speciﬁc scenes with personalized details, based on the guidance of face prototypes. 2.