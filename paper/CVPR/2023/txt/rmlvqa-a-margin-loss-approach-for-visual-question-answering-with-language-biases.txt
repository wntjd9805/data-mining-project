Abstract
Visual Question Answering models have been shown to suffer from language biases, where the model learns a cor-relation between the question and the answer, ignoring the image. While early works attempted to use question-only models or data augmentations to reduce this bias, we pro-pose an adaptive margin loss approach having two com-ponents. The first component considers the frequency of answers within a question type in the training data, which addresses the concern of the class-imbalance causing the language biases. However, it does not take into account the answering difficulty of the samples, which impacts their learning. We address this through the second component, where instance-specific margins are learnt, allowing the model to distinguish between samples of varying complex-ity. We introduce a bias-injecting component to our model, and compute the instance-specific margins from the confi-dence of this component. We combine these with the esti-mated margins to consider both answer-frequency and task-complexity in the training loss. We show that, while the mar-gin loss is effective for out-of-distribution (ood) data, the bias-injecting component is essential for generalising to in-distribution (id) data. Our proposed approach, Robust Mar-gin Loss for Visual Question Answering (RMLVQA) 1 im-proves upon the existing state-of-the-art results when com-pared to augmentation-free methods on benchmark VQA datasets suffering from language biases, while maintaining competitive performance on id data, making our method the most robust one among all comparable methods. 1.

Introduction
Visual question answering (VQA) lies at the intersection of computer vision and natural language processing. It is 1Code available at https://github.com/val-iisc/RMLVQA the task of answering a question based on a given image.
VQA networks need to combine knowledge from both vi-sual scene and the question to predict the answer. These systems have numerous applications such as aiding the vi-sually impaired in understanding their surroundings, image retrieval systems in e-commerce, and robotics.
With the success of deep learning, research in VQA has made great strides in recent years [3, 5, 16]. However, stud-ies have shown that deep networks may learn correlations between the question and answer alone, ignoring the image modality [3, 21, 36]. They fail to do multimodal reasoning, specifically, if there is a class imbalance in the answer distri-butions of the training and test sets. For example, if most of the questions starting with “What color..?” are paired with the answer “red” in the training data, the model memorizes this trend to answer “red” for all color based questions in the test set, irrespective of the image.
One solution to this problem is to perform data augmen-tations in various ways [1,4,11,15,27,36,42,43,46]. These methods outperform most other debiasing techniques in the literature. However, augmentation strategies are dependent on the dataset in question and the type of biases observed, which makes the process manual and tedious. Another ex-tensively explored solution is to learn the bias in the dataset separately using a question-only branch [9, 12, 18] and ex-plicitly removing the learnt bias from the base model.
Margin losses have been widely used for a number of tasks. Cao et al. [10] address the problem of long tailed recognition through an adaptive margin loss, ensuring higher cosine margin penalty for the tail classes and lower penalty for other classes. A similar adaptive cosine mar-gin penalty has been applied to mitigate the language bias problem in VQA by a previous work [17]. Margin losses have been widely used in deep face recognition as well, where it is desired that the features of two images of the same person should be as similar as possible, whereas those of two different people should be far apart. Margin losses
are used in this regard for facial feature discrimination to maximize the decision margin. Rather than the Euclidean space, both these margin losses project their feature spaces onto a fixed radius hypersphere. While the well known
CosFace loss [41] uses cosine margin penalties like Cao et al. [10], ArcFace [14], uses angular margins to maximize the decision margins among different classes in this feature space. Angles correspond to geodesic distance on the pro-jected hypersphere which stabilizes the training process and is shown to improve the discriminative power of face recog-nition models as compared to cosine margin based methods.
We believe that the language bias problem of VQA can be addressed by learning discriminative features for the bi-ased and unbiased samples in the dataset. To this end, we implement an adaptive angular margin loss, inspired by Ar-cFace, as margins allow models to distinguish between dif-ferent kinds of samples. However, the key question here would be how to set the adaptive margins to cater to the specific problem of language biases. Traditional models over-trust the questions over the images due to a class im-balance in the training and test set answer distributions.
Hence, one way to set the margins would be to ensure that the training samples with frequent answers are given a smaller penalty due to the abundance of those answers in the dataset, whereas those with rare answers are given a higher margin value. We refer to the resultant margin val-ues as the frequency-based margins. However, one factor that is ignored in this aspect is the answering difficulty of each sample. We argue that more margin should be given to hard samples compared to the easier ones even if the cor-responding answers are frequent.
In this regard, we pro-pose the learning of instance-specific margins during train-ing, so as to allow the model to distinguish between hard and easy samples alongside frequent and rare samples. To the best of our knowledge, this is one of the first attempts in learning margins automatically and parallely during train-ing. We combine these learnable margins with the fre-quency based margins used in prior works [17], so as to allow both frequency and complexity of data samples to control the training dynamics. To compute these learnable margins, we introduce a bias injecting module to our model that is trained using Cross-Entropy (CE) loss. We show that the CE loss additionally clusters the training samples based on the dataset bias itself, thereby aiding the margin loss fur-ther. We further introduce a supervised contrastive loss [23] to pull the features of samples having the same answer to-gether, while pushing others apart.
Adaptive margin losses, with margins calculated by us-ing frequency of answers in the dataset, perform well in the ood setting. However, we show that they cause a drop in the in-domain performance, which raises questions on their robustness. As pointed by [40], it is crucial for VQA mod-els to perform well on both in-domain and ood data since the test set distibution is unknown apriori. We mitigate this issue by ensembling the outputs of the bias injecting compo-nent and the proposed learnable margin-loss trained classi-fication head during inference. This makes the model robust to the difference in answer distributions of the training and test sets. Our overall method is called RMLVQA - Robust
Margin Loss for Visual Question Answering with language biases. The key contributions of this work can be summa-rized as follows:
• We propose to mitigate the well known problem of lan-guage bias in VQA models by introducing an instance-specific adaptive margin loss, to allow the use of dif-ferent margins for the learning of samples with vary-ing complexities, in addition to the use of frequency-based margins. To achieve this, we introduce a bias-injecting component and allow the margins to be com-puted based on prediction probabilities of this branch.
We show that this clusters samples in the feature space based on the bias present in the dataset.
• We propose to overcome the id-ood trade-off in margin-based losses, by ensembling the outputs of the bias-injecting component and the main model.
• We further introduce a supervised contrastive loss that pulls features of training samples having the same ground truth answers together, while pushing apart others. This aids the margin loss further.
• Through extensive experiments and ablations, we show how the proposed approach achieves state-of-the art results when compared to augmentation-free meth-ods on the ood VQA-CP v2 dataset, while maintaining competitive performance on the id VQA v2 dataset.
This makes our model the most robust one among all non-augmentation based methods.
The code, hyperparameter analyses, and results on multiple datasets are shared as supplementary material. 2.