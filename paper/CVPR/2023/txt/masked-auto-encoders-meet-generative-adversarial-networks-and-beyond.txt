Abstract
Masked Auto-Encoder (MAE) pretraining methods ran-domly mask image patches and then train a vision Trans-former to reconstruct the original pixels based on the un-masked patches. While they demonstrates impressive per-formance for downstream vision tasks, it generally requires a large amount of training resource. In this paper, we intro-duce a novel Generative Adversarial Networks alike frame-work, referred to as GAN-MAE, where a generator is used to generate the masked patches according to the remain-ing visible patches, and a discriminator is employed to pre-dict whether the patch is synthesized by the generator. We believe this capacity of distinguishing whether the image patch is predicted or original is benefit to representation learning. Another key point lies in that the parameters of the vision Transformer backbone in the generator and discriminator are shared. Extensive experiments demon-strate that adversarial training of GAN-MAE framework is more efficient and accordingly outperforms the standard
MAE given the same model size, training data, and com-putation resource. The gains are substantially robust for different model sizes and datasets, in particular, a ViT-B model trained with GAN-MAE for 200 epochs outperforms the MAE with 1600 epochs on fine-tuning top-1 accuracy of
ImageNet-1k with much less FLOPs. Besides, our approach also works well at transferring downstream tasks. 1.

Introduction
In recent years, Transformer [62] has become the de facto standard architecture in computer vision, and has surpassed state-of-the-art Convolutional Neural Network (CNN) [31, 58] feature extractors in vision tasks through models such as the Vision Transformer [21]. Meanwhile, self-supervised learning (SSL) algorithms [12, 14, 27, 29] aims to learn transferable representation from unlabeled
*The corresponding author.
Figure 1. Performance comparison in different pre-training epochs for ImageNet-1K Fine-tuning top-1 accuracy. Com-pared to MAE trained for 1600 epochs, GAN-MAE achieves com-parable accuracy with much less training time at 200 epochs. data by performing instance-level pretext tasks, and has been a long-standing target in the vision community. Par-ticularly, masked image modeling (MIM) in SSL for vi-sion transformers has shown remarkably impressive down-stream performance in a wide variety of computer vision tasks [3, 28], attracting increasing attention.
MIM is a simple pretext task that first randomly masks some patches of an image, and then predicts the contents of the masked patches according to the remaining, using various reconstruction targets, e.g., visual tokens [3,19], se-mantic features [1, 77] and raw pixels [28, 70]. Essentially, it learns the transferable representation by modeling the im-age structure itself as content prediction. While more ef-fective than conventional pre-training, masked autoencoder modeling approaches still exist some issues: (i) reconstruc-tion optimization with MSE loss leads to blurrier output im-ages than the raw input, it would be better to use a more perceptual loss over pixels to guide the fine-grained seman-tic understanding and representation learning, leading to more plausible synthesized patches; (ii) inner dependency between masked patches is lacked [71], i.e., generation of masked image patches may lack the surrounding informa-tion. This situation becomes more serious when the image patch masking ratio is large. We alleviate this problem by introducing confident synthesized patches as complemen-tary information during training; (iii) mask-reconstruction methods incur a substantial computation cost because the network only learns from part of the visible patches and misses the information of masked patches.
In this paper, we propose a Generative Adversarial referred to as
Networks-based pre-training framework,
GAN-MAE, which contains two components: a generator model learns to reconstruct the masked patches according to visible patches in the encoder-decoder architecture and a discriminator model learns to distinguish real image patches from plausible but synthesized remains. Generally, given an image from training dataset, our method first randomly masks parts of patches and reconstructs them using the rest visible patches with a generator, which serves as a standard
MAE model. Then we build the corrupt image as the com-bination of visible and synthesis patches, which is then fed into the discriminator to predict whether each patch is from raw image or synthesized results. In this manner, the dis-criminator provides a valid guiding for more delicate image patch modeling. Then, with the development of generator capacity, a key advantage of discriminative task is that it in-tegrates the synthesized patches into corrupt images as com-plementary information, which fills the missing inner rela-tionship between patches during pre-training. Moreover, we shared the parameters of vision transformer backbone in the generator and discriminator to promote memory reduction, training efficiency, as well as performance enhancement.
Our experiments follow the same architecture, settings, and pre-training recipe as MAE [28], and we find that the simple incorporation of a discriminator consistently outper-forms MAE in variant models, e.g., ViT-S, ViT-B, and ViT-L, when fine-tuning for top-1 accuracy of ImageNet clas-sification. We also conduct extensive ablation studies to validate the effectiveness of our core designs in backbone parameter sharing and advarisal training. As pre-training with more epochs usually results in a better downstream performance, we argue that an important consideration for pre-training methods should be computation efficiency as well as absolute downstream performance. From this view-point, we also demonstrate that discrimination of pseudo-image patches forces GAN-MAE to train more efficiently than standard MAE. We further provide a comprehensive comparison with MAE in various epochs and various mod-els and show our framework achieves consistently better performance. In particular, as presented in Figure 1, for the
ViT-B model structure, our GAN-MAE achieves compara-ble classification performance with only 200 pre-training epochs vs. standard MAE 1600 pre-training epochs. Fur-thermore, the GAN-MAE achieves 0.7 points improvement when pre-training 1600 epochs. Finally, we summarize our contribution as follows:
• We propose a new and effective GAN-alike frame-work for visual representation self-supervised learn-ing, which to our best knowledge is the first trial of integrating GAN idea into MAE framework. As a generic approach, we suggest that this framework can be easily applied on many other MIM-based tasks.
• We introduce two core designs: shared weight for the main backbones of generator and discriminator, and an adversarial training process, both of which cost fewer amounts of computing resources while obtaining ap-preciable performance improvements.
• Extensive experiments demonstrate that compared with the original MAE, our method is more compute-efficient and results in better transfer representation learning on downstream tasks. 2.