Abstract 1.

Introduction
How does audio describe the world around us? In this pa-per, we propose a method for generating an image of a scene from sound. Our method addresses the challenges of dealing with the large gaps that often exist between sight and sound.
We design a model that works by scheduling the learning pro-cedure of each model component to associate audio-visual modalities despite their information gaps. The key idea is to enrich the audio features with visual information by learn-ing to align audio to visual latent space. We translate the input audio to visual features, then use a pre-trained genera-tor to produce an image. To further improve the quality of our generated images, we use sound source localization to select the audio-visual pairs that have strong cross-modal correlations. We obtain substantially better results on the
VEGAS and VGGSound datasets than prior approaches. We also show that we can control our model’s predictions by applying simple manipulations to the input waveform, or to the latent space.
Humans have the remarkable ability to associate sounds with visual scenes, such as how chirping birds and rustling branches bring to mind a lush forest, and the flowing water conjures the image of a river. These cross-modal associations convey important information, such as the distance and size of sound sources, and the presence of out-of-sight objects.
An emerging line of work has sought to create multi-modal learning systems that have these cross-modal pre-diction capabilities, by synthesizing visual imagery from sound [15, 20, 26, 36, 37, 63, 69]. However, these existing methods come with significant limitations, such as being limited to simple datasets in which images and sounds are closely correlated [63, 69], relying on vision-and-language supervision [36], and being capable only of manipulating
Acknowledgment. This work was supported by IITP grant funded by Korea government (MSIT) (No.2021-0-02068, Artificial Intelligence Innovation
Hub; No.2022-0-00124, Development of Artificial Intelligence Technology for Self-Improving Competency-Aware Learning Capabilities). The GPU resource was supported by the HPC Support Project, MSIT and NIPA.
the style of existing images [37] but not synthesis.
Addressing these limitations requires handling several challenges. First, there is a significant modality gap between sight and sound, as sound often lacks information that is important for image synthesis, e.g., the shape, color, or spa-tial location of on-screen objects. Second, the correlation between modalities is often incongruent, e.g., highly contin-gent or off-sync on timing. Cows, for example, only rarely moo, so associating images of cows with “moo” sounds re-quires capturing training examples with the rare moments when on-screen cows vocalize.
In this work, we propose Sound2Scene, a sound-to-image generative model and training procedure that addresses these limitations, and which can be trained solely from unlabeled videos. First, given an image encoder pre-trained in a self-supervised way, we train a conditional generative adversarial network [11] to generate images from the visual features of the image encoder. We then train an audio encoder to translate an input sound to its corresponding visual feature, by aligning the audio to the visual space. Afterwards, we can generate diverse images from sound by translating from audio to visual embeddings and synthesizing an image. Since our model must be capable of learning from challenging in-the-wild videos, we use sound source localization to select moments in time that have strong cross-modal associations.
We evaluate our model on VEGAS [73] and VG-GSound [14], as shown in Fig. 1. Our model can synthesize a wide variety of different scenes from sound in high quality, outperforming the prior arts. It also provides an intuitive way to control the image generation process by applying manipu-lations at both the input and latent space levels, such as by mixing multiple audios together or adjusting the loudness.
Our main contributions are summarized as follows:
• Proposing a new sound-to-image generation method that can generate visually rich images from in-the-wild audio in a self-supervised way.
• Generating high-quality images from the unrestricted di-verse categories of input sounds for the first time.
• Demonstrating that the samples generated by our model can be controlled by intuitive manipulations in the wave-form space in addition to latent space.
• Showing the effectiveness of training sound-to-image gen-eration using highly correlated audio-visual pairs. 2.