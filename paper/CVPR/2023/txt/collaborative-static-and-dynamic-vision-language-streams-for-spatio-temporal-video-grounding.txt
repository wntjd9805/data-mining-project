Abstract
Spatio-Temporal Video Grounding (STVG) aims to lo-calize the target object spatially and temporally accord-It is a challenging task ing to the given language query. in which the model should well understand dynamic visual cues (e.g., motions) and static visual cues (e.g., object ap-pearances) in the language description, which requires ef-fective joint modeling of spatio-temporal visual-linguistic dependencies.
In this work, we propose a novel frame-work in which a static vision-language stream and a dy-namic vision-language stream are developed to collabora-tively reason the target tube. The static stream performs cross-modal understanding in a single frame and learns to attend to the target object spatially according to intra-frame visual cues like object appearances. The dynamic stream models visual-linguistic dependencies across mul-tiple consecutive frames to capture dynamic cues like mo-tions. We further design a novel cross-stream collabora-tive block between the two streams, which enables the static and dynamic streams to transfer useful and complementary information from each other to achieve collaborative rea-soning. Experimental results show the effectiveness of the collaboration of the two streams and our overall frame-work achieves new state-of-the-art performance on both
HCSTVG and VidSTG datasets. 1.

Introduction
Vision-language cross-modal understanding is a chal-lenging yet important research problem that bridges the communication between humans and artificial intelligence systems.
It has attracted increasing attention and many vision-language tasks were studied in recent years, like vi-*Corresponding author. sual grounding [13, 32], VQA [7, 31], image/video cap-tioning [15, 40], etc.
In this work, we focus on a chal-lenging vision-language task named Spatio-Temporal Video
Grounding (STVG) which was recently proposed in [42].
Given a language query indicating an object (as shown in
Figure 1), STVG aims to localize the target object spatially and temporally in the video.
In this task, the input lan-guage query may express different kinds of visual concepts, thus the model requires to well capture and understand these concepts in both vision and language modalities.
In STVG task, dynamic visual concepts like human mo-tion and static visual concepts like object appearance are both important for distinguishing the target object from other objects that occurred in the same video. For exam-ple, in the first sample in Figure 1, the two men are dressed alike (i.e., their static appearance cues are similar), and we can only distinguish them by motion. In the second sample, the two women perform similar actions, they both stand up (i.e., they have similar dynamic motion cues), here, we can only distinguish them by their clothes. The above examples show that static or dynamic visual cues alone cannot solve the STVG task well. And it implies that modeling static and dynamic visual-linguistic dependencies and collaboratively utilizing them are important for addressing STVG task.
Humans treat static and dynamic cues differently [4, 21], but this was overlooked in previous STVG works [22, 24, 30]. Taking the first query in Figure 1 as an example, a hu-man would randomly or evenly click on some locations on the video’s progress bar to find candidate frames containing a man in blue coat. And he will play the video around that frame and attend on the candidate man to check whether he performs the action described in the text (i.e., “turns around and stops by the stone”). In the above process, the human understands static and dynamic cues in different ways (i.e., view a single frame and watch the video clip, respectively) and determines the target object by jointly considering the
Figure 1. Two examples for Spatio-Temporal Video Grounding task. In the first case, the two men are both dressed alike in blue, thus understanding the action described in the query sentence is essential to recognize the person of interest. In the second case, both of the two women stand up, we can only distinguish them by their clothes. (Best viewed zoomed in on screen.) static and dynamic cues in an “attend-and-check” process.
Inspired by the above observations, we propose a frame-work that consists of a static and a dynamic vision-language (VL) streams to model static and dynamic cues, respec-tively. And we design a novel cross-stream collaboration block between the two streams to simulate the “attend-and-check” process, which exchanges useful and complemen-tary information learned in each stream and enables the two streams to collaboratively reason the target object.
Specifically, in this work, our static vision-language (VL) stream learns to attend to some candidate regions according to static visual cues like appearance, while the dynamic VL stream learns to understand the dynamic vi-sual cues like action described in the text query. Then, in the collaboration block, we guide the dynamic stream to only focus and attend on the motion of the candidate objects by using the learned attended region in the static stream. And we transfer the text-motion matching infor-mation learned in the dynamic stream to the static stream, to help it further check and determine the target object and predict a more consistent tube. With the above cross-stream collaboration blocks, both the static and dynamic vision-language streams can learn reciprocal information from the other stream, which is effective for achieving more accurate spatio-temporal grounding predictions. We conduct experi-ments on HCSTVG [24] and VidSTG [42] datasets and our approach outperforms previous approaches by a consider-able margin. Ablation studies demonstrate the effectiveness of each component in our proposed cross-stream collabo-ration block and show its superiority over commonly used counterparts in video understanding works [4, 21].
In summary, our contributions are: 1), we develop an effective framework that contains two parallel streams to model static-dynamic visual-linguistic dependencies for complete cross-modal understanding; 2), we propose a novel cross-stream collaboration block between the two streams to exchange reciprocal information for each other and enable collaborative reasoning of the target object; 3), Our overall framework achieves state-of-the-art perfor-mances on HCSTVG [24] and VidSTG datasets [42]. 2.