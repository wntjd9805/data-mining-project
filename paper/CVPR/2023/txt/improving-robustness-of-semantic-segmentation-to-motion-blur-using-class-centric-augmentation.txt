Abstract
Semantic segmentation involves classifying each pixel into one of a pre-defined set of object/stuff classes. Such a fine-grained detection and localization of objects in the scene is challenging by itself. The complexity increases manifold in the presence of blur. With cameras becoming increasingly light-weight and compact, blur caused by mo-tion during capture time has become unavoidable. Most research has focused on improving segmentation perfor-mance for sharp clean images and the few works that deal with degradations, consider motion-blur as one of many
In this work, we focus exclusively generic degradations. on motion-blur and attempt to achieve robustness for se-mantic segmentation in its presence. Based on the observa-tion that segmentation annotations can be used to generate synthetic space-variant blur, we propose a Class-Centric
Motion-Blur Augmentation (CCMBA) strategy. Our ap-proach involves randomly selecting a subset of semantic classes present in the image and using the segmentation map annotations to blur only the corresponding regions.
This enables the network to simultaneously learn seman-tic segmentation for clean images, images with egomotion blur, as well as images with dynamic scene blur. We demon-strate the effectiveness of our approach for both CNN and
Vision Transformer-based semantic segmentation networks on PASCAL VOC and Cityscapes datasets. We also illus-trate the improved generalizability of our method to com-plex real-world blur by evaluating on the commonly used deblurring datasets GoPro and REDS . 1.

Introduction
Motion-blur has become ubiquitous in our lives driven largely by the compactness and affordability of light weight cameras. While camera quality has improved significantly, sensing technology cannot suppress blur completely yet.
For handheld cameras and cameras mounted on moving ve-hicles, motion during capture is a major reason for the oc-currence of blurred images. Most research in semantic seg-(a) Motion-blurred images, (b) Segmentation from a
Figure 1. network trained on clean images (c) Segmentation after using our augmentation for training (d) Ground truth. mentation especially those that are deep-learning based and state-of-the-art, focus on increasing accuracy [30], [38] and throughput [31], [23]. While these models have achieved significant gains, they are trained on clean data and con-sequently struggle to perform when presented with out-of-distribution data [13]. Such wrong predictions can have grave consequences for safety-critical applications like au-tonomous driving. Therefore, it becomes essential to focus on finding ways of making these models robust to unavoid-able degradations like motion-blur.
When trying to generalize to blurred images, an off-the-shelf approach would be to use a deblurring algorithm to deblur the image before continuing with the downstream task of segmentation. However, the generalization abili-ties of deblurring models are still subpar and they strug-gle to perform well for real blurred out-of-distribution im-ages [37] . Additionally, many of these image restoration models process images at multiple-scales in a hierarchi-cal fashion which improves the performance but also in-creases the latency and memory requirements [34,35]. Con-sequently, this two-stage approach of using deblurring as
pre-processing to obtain deblurred images before attempt-ing segmentation is not viable for deployment and real-time applications. Hence, there is a strong need to devise single-stage methods that can bypass this step.
Some recent works have tried to focus on analysing and improving the robustness of existing models for different tasks to a spectrum of commonly encountered degradations.
Unlike adversarial robustness, [11] defines robustness as the ability of a model trained on sharp images to retain competi-tive performance in images having degradations. [11,13,21] benchmark standard models for their robustness to multi-ple severity levels of sixteen commonly occurring degrada-tions including motion-blur for the tasks of object recogni-tion, object detection and semantic segmentation. Note that the motion-blur being considered here is spatially invariant and linear. [12] and [16] propose augmentation strategies to improve generic robustness of models across the sixteen degradations. While these augmentations have resulted in increased robustness, we believe that significant improve-ments can be made if degradation-specific and task-specific insights are exploited. [14] leverages the semantic segmen-tation annotation maps to increase the shape bias in the net-work which has been established to improve robustness [7].
However, this is only a task-specific augmentation and at-tempts to achieve improvements over all degradation types.
In this work, we attempt to make semantic segmentation robust to the presence of generic space-variant motion-blur.
In particular, we develop a Class-Centric Motion-Blur Aug-mentation (CCMBA) strategy where we leverage the seg-mentation map annotations to introduce blur in specific re-gions of the image to enforce distinguishability and easier training. We randomly choose a subset of classes that we want to blur, blur the corresponding foreground image us-ing a synthetic non-linear kernel, and then blend the blurred foreground image with the sharp background image. Since, motion-blur can be due camera ego-motion as well as dy-namic scenes, the advantage of our augmentation strategy lies in better generalization to dynamic scenes due to its se-mantic class-centric nature. Fig. 1 shows the segmentation results obtained on motion-blurred images with and without our method.
Our contributions are the following :
• An effective data augmentation scheme for reliably segmenting out regions from motion blurred images without the need for deblurring.
• Our method is generic in nature and can be used with any supervised semantic segmentation network.
• While our model is trained on only synthetically gen-erated data, the class-centric nature of our augmenta-tion enables it to perform well on general dynamic blur datasets like GoPro and REDS, especially for common classes like humans.
• We report improved performance for DeepLabv3+ over baseline methods with 3.2% and 3% increase on
PASCAL VOC and Cityscapes dataset, respectively, for the highest level of blur. We also achieve improve-ments on the Cityscapes-C dataset over previous works with a maximum 9% increase for highest levels of blur.
Figure 2. Class-Centric Motion-Blur Augmentation (CCMBA):
Given a sharp image, its segmentation mask, and a motion-blur kernel, we synthetically blur the regions corresponding to a subset of classes present in the image to mimic dynamic scenes. When all classes are chosen, camera motion-blur is synthesized making our augmentation applicable for generic blur. 2.