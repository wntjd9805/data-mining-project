Abstract
Learning high-quality, self-supervised, visual represen-tations is essential to advance the role of computer vision in biomedical microscopy and clinical medicine. Previous work has focused on self-supervised representation learn-ing (SSL) methods developed for instance discrimination and applied them directly to image patches, or ﬁelds-of-view, sampled from gigapixel whole-slide images (WSIs) used for cancer diagnosis. However, this strategy is lim-ited because it (1) assumes patches from the same patient are independent, (2) neglects the patient-slide-patch hier-archy of clinical biomedical microscopy, and (3) requires strong data augmentations that can degrade downstream performance. Importantly, sampled patches from WSIs of a patient’s tumor are a diverse set of image examples that capture the same underlying cancer diagnosis. This moti-vated HiDisc, a data-driven method that leverages the in-herent patient-slide-patch hierarchy of clinical biomedical microscopy to deﬁne a hierarchical discriminative learning task that implicitly learns features of the underlying diag-nosis. HiDisc uses a self-supervised contrastive learning framework in which positive patch pairs are deﬁned based on a common ancestry in the data hierarchy, and a uniﬁed patch, slide, and patient discriminative learning objective is used for visual SSL. We benchmark HiDisc visual represen-tations on two vision tasks using two biomedical microscopy datasets, and demonstrate that (1) HiDisc pretraining out-performs current state-of-the-art self-supervised pretrain-ing methods for cancer diagnosis and genetic mutation pre-diction, and (2) HiDisc learns high-quality visual repre-sentations using natural patch diversity without strong data augmentations.
Figure 1. Hierarchical self-supervised discriminative learn-ing for visual representations. Clinical biomedical microscopy has a hierarchical patch-slide-patient data structure. HiDisc com-bines patch, slide, and patient discrimination into a uniﬁed self-supervised learning task. 1.

Introduction
Biomedical microscopy is an essential imaging method and diagnostic modality in biomedical research and clini-cal medicine. The rise of digital pathology and whole-slide images (WSIs) has increased the role of computer vision and machine learning-based approaches for analyzing mi-croscopy data [51]. Improving the quality of visual repre-sentation learning of biomedical microscopy is critical to
introducing decision support systems and automated diag-nostic tools into clinical and laboratory medicine.
⇥
Biomedical microscopy and WSIs present several unique computer vision challenges, including that image resolu-10K pixels) and annotations are of-tions can be large (10K ten limited to weak slide-level or patient-level labels. More-over, even weak annotations are challenging to obtain in or-der to protect patient health information and ensure patient privacy [54]. Additionally, data that predates newly devel-oped or future clinical testing methods, such as genomic or methylation assays, also lack associated weak annota-tions. Because of large WSI sizes and weak annotations, the majority of computer vision research in biomedical mi-croscopy has focused on WSI classiﬁcation using a weakly supervised, patch-based, multiple instance learning (MIL) framework [2, 7, 20, 37, 38, 48]. Patches are arbitrarily de-ﬁned ﬁelds-of-view (e.g., 256 256 pixels) that can be used
⇥ for model input. The classiﬁcation tasks include identify-ing the presence of cancerous tissue, such as breast can-cer metastases in lymph node biopsies [13], differentiating speciﬁc cancer types [7, 11, 18], predicting genetic muta-tions [11, 26, 32], and patient prognostication [8, 29]. A limitation of end-to-end MIL frameworks for WSI classi-ﬁcation is the reliance on weak annotations to train a patch feature extractor and achieve high-quality patch-level repre-sentation learning. This limitation, combined with the chal-lenge of obtaining fully annotated, high-quality WSIs, ne-cessitates better methods for self-supervised representation learning (SSL) of biomedical microscopy.
To date, research into improving the quality and efﬁ-ciency of patch-level representation learning without an-notations has been limited. Previous studies have focused on using known SSL methods, such as contrastive learn-ing [35, 47, 50], and applying them directly to WSI patches for visual pretraining. These SSL methods are not optimal because the majority use instance (i.e., patch) discrimina-tion as the pretext learning task [5, 9, 10, 15, 55]. Patches belonging to the same slide or patient are correlated, which can decrease the learning efﬁciency. Instance discrimina-tion alone does not account for patches from a common slide or patient being different and diverse views of the same underlying pathology. Moreover, previous SSL methods ne-glect the inherent patient-slide-patch data hierarchy of clin-ical biomedical microscopy as shown in Figure 1. This hi-erarchical data structure is not used to improve representa-tion learning when training via a standard SSL objective.
Lastly, most SSL methods require strong data augmenta-tions for instance discrimination tasks [9]. However, strong and domain-agnostic augmentations can worsen representa-tion learning in microscopy images by corrupting semanti-cally important and discriminative features [21, 50].
Here, we introduce a method that leverages the in-herent patient-slide-patch hierarchy of clinical biomedi-cal microscopy to deﬁne a self-supervised hierarchical discriminative learning task, called HiDisc. HiDisc uses a self-supervised contrastive learning framework such that positive patch pairs are deﬁned based on a common ances-try in the data hierarchy, and a combined patch, slide, and patient discriminative learning objective is used for visual
SSL. By sampling patches across the data hierarchy, we in-troduce increased diversity between the positive examples, allowing for better visual representation learning and by-passing the need for strong, out-of-domain data augmenta-tions. While we examine the HiDisc learning objective in the context of contrastive learning, it can be generalized to any siamese representation learning method [10].
We benchmark HiDisc self-supervised pretraining on two computer vision tasks using two diverse biomedical mi-croscopy datasets: (1) multiclass histopathologic cancer di-agnosis using stimulated Raman scattering microscopy [41] and (2) molecular genetic mutation prediction using light microscopy of hematoxylin and eosin (H&E)-stained can-cer specimens [30]. These tasks are selected because of their clinical importance and they represent examples of how deep learning-based computer vision methods can push the limits of what is achievable through biomedical mi-croscopy [18, 24, 26, 31]. We benchmark HiDisc in com-parison to several state-of-the-art SSL methods, including
SimCLR [9], BYOL [15], and VICReg [1]. We demon-strate that HiDisc has superior performance compared to other SSL methods across both datasets and computer vi-sion tasks. Our results demonstrate how hierarchical dis-criminative learning can improve self-supervised visual rep-resentations of biomedical microscopy. 2.