Abstract
Single domain generalization aims to train a generaliz-able model with only one source domain to perform well on arbitrary unseen target domains. Image augmentation based on Random Convolutions (RandConv), consisting of one convolution layer randomly initialized for each mini-batch, enables the model to learn generalizable visual represen-tations by distorting local textures despite its simple and lightweight structure. However, RandConv has structural limitations in that the generated image easily loses semantics as the kernel size increases, and lacks the inherent diversity of a single convolution operation. To solve the problem, we propose a Progressive Random Convolution (Pro-RandConv) method that recursively stacks random convolution layers with a small kernel size instead of increasing the kernel size.
This progressive approach can not only mitigate semantic distortions by reducing the influence of pixels away from the center in the theoretical receptive field, but also cre-ate more effective virtual domains by gradually increasing the style diversity. In addition, we develop a basic random convolution layer into a random convolution block includ-ing deformable offsets and affine transformation to support texture and contrast diversification, both of which are also randomly initialized. Without complex generators or adver-sarial learning, we demonstrate that our simple yet effective augmentation strategy outperforms state-of-the-art methods on single domain generalization benchmarks. 1.

Introduction
In recent years, deep neural networks have achieved re-markable performance in a wide range of applications [26, 27]. However, this success is built on the assumption that the test data (i.e. target) should share the same distribution as the training data (i.e. source), and they often fail to gen-eralize to out-of-distribution data [9, 21, 47]. In practice, this domain discrepancy problem between source and target domains is commonly encountered in real-world scenarios.
† Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.
Figure 1. Comparison of conventional random convolutions (single-layer) and our progressive random convolutions (multi-layer). Our final model includes multiple random convolution blocks consisting of deformable offsets and affine transformation.
Especially, a catastrophic safety issue may occur in medical imaging [33, 65] and autonomous driving [62, 68] applica-tions. To tackle this problem, one line of work focuses on domain adaptation (DA) to transfer knowledge from a source domain to a specific target domain [4, 14, 22, 25]. This ap-proach usually takes into account the availability of labeled or unlabeled target domain data. Another line of work deals with a more realistic setting known as domain generaliza-tion (DG), which aims to learn a domain-agnostic feature representation with only data from source domains without access to target domain data. Thanks to its practicality, the task of domain generalization has been extensively studied.
In general, the paradigm of domain generalization de-pends on the availability of using multi-source domains [57].
Previously, many studies [3, 16, 17, 30, 41] have focused on using multi-source domains, and the distribution shift can be alleviated by simply aggregating data from multiple training
Figure 2. Examples of images augmented by RandConv and our Pro-RandConv composed of multiple convolution blocks. domains [30, 31]. However, this approach faces practical limitations due to data collection budgets [44]. As an alterna-tive, the single domain generalization problem has recently received attention [34, 57], which learns robust representa-tion using only a single source domain. A common solution to this challenging problem is to generate diverse samples in order to expand the coverage of the source domain through an adversarial data augmentation scheme [54, 69]. However, most of these methods share a complicated training pipeline with multiple objective functions.
In contrast, Xu et al. suggested Random Convolution (RandConv) [61] that consists of a single convolution layer whose weights are randomly initialized for each mini-batch, as described in Fig. 1(a). When RandConv is applied to an input image, it tends to modify the texture of the input image depending on the kernel size of the convolution layer. This is a simple and lightweight image augmentation technique compared to complex generators or adversarial data augmen-tation. Despite these advantages, this method has structural limitations. Firstly, the image augmented by RandConv eas-ily loses its semantics while increasing the kernel size, which is shown in Fig. 2(a). As a result, the ability to generalize in the test domain is greatly reduced as shown in Fig. 1(c).
Secondly, RandConv lacks the inherent diversity of a single convolution operation.
To solve these limitations, we propose a progressive ap-proach based on random convolutions, named Progressive
Random Convolutions (Pro-RandConv). Figure 1(b) de-scribes the progressive approach consisting of multiple con-volution layers with a small kernel size. Our progressive approach has two main properties. The first is that the multi-layer structure can alleviate the semantic distortion issues by reducing the impact on pixels away from the center in the theoretical receptive field, as revealed in [37]. There-fore, the progressive approach does not degrade the perfor-mance much even if the receptive field increases, as shown in Fig. 1(c). The second property is that stacking random convolution layers of the same weights can generate more effective virtual domains rather than using different weights.
This is an interesting observation that can be interpreted as gradually increasing the distortion magnitude of a single transformation to the central pixels. This approach enables more fine-grained control in image transformation than a single layer with a large kernel, which has the effect of incrementally improving style diversity.
In addition, we propose a random convolution block in-cluding deformable offsets and affine transformation to sup-port texture and contrast diversification. It is noteworthy that all weights are also sampled from a Gaussian distribution, so our convolution block is an entirely stochastic process.
Finally, we can maximize the diversity of styles while main-taining the semantics of newly generated images through the progressive method of this random convolution block, as described in Fig. 2(b). We argue that the proposed Pro-RandConv could be a strong baseline because it surpasses recent single DG methods only by image augmentation with-out an additional loss function or complex training pipelines.
To summarize, our main contributions are as follows:
• We propose a progressive approach of recursively stack-ing small-scale random convolutions to improve the style diversity while preserving object semantics.
• We develop a random convolution layer with de-formable offsets and affine transformation to promote texture and contrast diversity for augmented images.
• We perform comprehensive evaluation and analyses of our method on single and multi DG benchmarks on which we produce significant improvement in recogni-tion performance compared to other methods. 2.