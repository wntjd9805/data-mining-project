Abstract
Recently, virtual/pseudo-point-based 3D object detec-tion that seamlessly fuses RGB images and LiDAR data by depth completion has gained great attention. However, virtual points generated from an image are very dense, in-troducing a huge amount of redundant computation during detection. Meanwhile, noises brought by inaccurate depth completion significantly degrade detection precision. This paper proposes a fast yet effective backbone, termed Vir-ConvNet, based on a new operator VirConv (Virtual Sparse
Convolution), for virtual-point-based 3D object detection.
VirConv consists of two key designs: (1) StVD (Stochas-tic Voxel Discard) and (2) NRConv (Noise-Resistant Sub-manifold Convolution). StVD alleviates the computation problem by discarding large amounts of nearby redundant voxels. NRConv tackles the noise problem by encoding voxel features in both 2D image and 3D LiDAR space. By integrating VirConv, we first develop an efficient pipeline
VirConv-L based on an early fusion design. Then, we build a high-precision pipeline VirConv-T based on a trans-formed refinement scheme. Finally, we develop a semi-supervised pipeline VirConv-S based on a pseudo-label framework. On the KITTI car 3D detection test leader-board, our VirConv-L achieves 85% AP with a fast run-ning speed of 56ms. Our VirConv-T and VirConv-S attains a high-precision of 86.3% and 87.2% AP, and currently rank 2nd and 1st1, respectively. The code is available at https://github.com/hailanyi/VirConv. 1.

Introduction 3D object detection plays a critical role in autonomous driving [32, 45]. The LiDAR sensor measures the depth of scene [4] in the form of a point cloud and enables re-liable localization of objects in various lighting environ-ments. While LiDAR-based 3D object detection has made rapid progress in recent years [19, 23, 25, 27, 28, 42, 43, 49], its performance drops significantly on distant objects, which inevitably have sparse sampling density in the scans. Unlike
*Corresponding author 1On the date of CVPR deadline, i.e., Nov.11, 2022
Figure 1. Our VirConv-T achieves top average precision (AP) on both 3D and BEV moderate car detection in the KITTI benchmark (more details are in Table 1). Our VirConv-L runs fast at 56ms with competitive AP.
LiDAR scans, color image sensors provide high-resolution sampling and rich context data of the scene. The RGB im-age and LiDAR data can complement each other and usu-ally boost 3D detection performance [1, 6, 20, 21, 24].
Early methods [29–31] extended the features of LiDAR points with image features, such as semantic mask and 2D
CNN features. They did not increase the number of points; thus, the distant points still remain sparse.
In contrast, the methods based on virtual/pseudo points (for simplic-ity, both denoted as virtual points in the following) enrich the sparse points by creating additional points around the
LiDAR points. For example, MVP [45] creates the vir-tual points by completing the depth of 2D instance points from the nearest 3D points. SFD [36] creates the virtual points based on depth completion networks [16]. The vir-tual points complete the geometry of distant objects, show-ing the great potential for high-performance 3D detection.
However, virtual points generated from an image are generally very dense. Taking the KITTI [9] dataset as an example, an 1242×375 image generates 466k virtual points (∼27× more than the LiDAR scan points). This brings a huge computational burden and causes a severe efficiency issue (see Fig. 2 (f)). Previous work addresses the density problem by using a larger voxel size [19, 44] or by ran-domly down-sampling [17] the points. However, applying such methods to virtual points will inevitably sacrifice use-tended receptive field in 2D space allows our NRConv to distinguish the noise pattern on the instance boundaries in 2D image space. Consequently, the negative impact of noise can be suppressed.
We develop three multimodal detectors to demon-strate the superiority of our VirConv: (1) a lightweight
VirConv-L constructed from Voxel-RCNN [7]; (2) a high-precision VirConv-T based on multi-stage [34] and multi-transformation [35] design; (3) a semi-supervised VirConv-S based on a pseudo-label [33] framework. The effective-ness of our design is verified by extensive experiments on the widely used KITTI dataset [9] and nuScenes dataset [3].
Our contributions are summarized as follows:
• We propose a VirConv operator, which effectively en-codes voxel features of virtual points by StVD and
NRConv. The StVD discards a huge number of redun-dant voxels and substantially speeds up the 3D detec-tion prominently. The NRConv extends the receptive field of 3D sparse convolution to the 2D image space and significantly reduces the impact of noisy points.
• Built upon VirConv, we present three new multimodal detectors: a VirConv-L, a VirConv-T, and a semi-supervised VirConv-S for efficient, high-precision, and semi-supervised 3D detection, respectively.
• Extensive experiments demonstrated the effectiveness of our design (see Fig. 1). On the KITTI leaderboard, our VirConv-T and VirConv-S currently rank 2nd and 1st, respectively. Our VirConv-L runs at 56ms with competitive precision. 2.