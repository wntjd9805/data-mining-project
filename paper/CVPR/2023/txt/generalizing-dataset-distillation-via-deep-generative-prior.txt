Abstract
Dataset Distillation aims to distill an entire dataset’s knowledge into a few synthetic images. The idea is to synthe-size a small number of synthetic data points that, when given to a learning algorithm as training data, result in a model approximating one trained on the original data. Despite a recent upsurge of progress in the field, existing dataset dis-tillation methods fail to generalize to new architectures and scale to high-resolution datasets. To overcome the above issues, we propose to use the learned prior from pre-trained deep generative models to synthesize the distilled data. To achieve this, we present a new optimization algorithm that distills a large number of images into a few intermediate feature vectors in the generative model’s latent space. Our method augments existing techniques, significantly improv-ing cross-architecture generalization in all settings. 1.

Introduction
Many recent advancements in machine learning come from combining large networks and big data. Such trained models have shown strong capabilities to perform a wide range of diverse tasks [12, 22, 48] and are considered by some as an ongoing paradigm shift [9]. While such ap-proaches show great potential to improve the frontier of AI, we, as a scientific community, are also curious about the un-derlying principles and limitations. Do networks have to be large to express the functions of interest? Do datasets have to be big? Can training on “small data” be equally successful?
The seminal work on Knowledge Distillation [31] and re-cent discoveries such as Lottery Ticket Hypothesis [26] have revealed small models are often sufficient to approximate the same functions as large trained models (which are some-times useful in optimization). Dataset Distillation, proposed by [61], investigates the analogous yet orthogonal question on datasets: is there a small succinct dataset sufficient for training models? In other words, Dataset Distillation aims to distill a large dataset into a small (synthetic) one, such that training on the small dataset yields comparable performance (Figure 2). Since its proposal, Dataset Distillation has gained much attention in the research community, leading to many applications [14, 23, 43, 53], and a growing series of methods that solve the distillation problem: generating a discrete set of images capable of effectively training a model [13, 60, 61, 64, 66–68]. In optimizing for a small, synthetic vision dataset, such methods typically optimize the raw pixel values of the images.
Figure 2. Rather than directly distilling a dataset into synthetic pixels (like all previous methods), our new work instead distills into the latent space of a deep generative prior. This enforces a tuneable amount of coherence in the output synthetic images, leading to far better generalization to new architectures.
Unfortunately, these methods face two major challenges, limiting both their scientific value and empirical applications.
First, the distilled synthetic dataset is often optimized w.r.t. a specific network architecture, but does not generalize well to other architectures. Second, while producing insightful distilled images on toy datasets, these methods generally fail to work well on larger-resolution datasets (e.g., ≥ 128 × 128 resolution) and tend to distill visually noisy images with subpar performance.
In this work, we argue that both issues are caused by pa-rameterizing the synthetic dataset in pixel space. Directly op-timizing pixels can be susceptible to learning high-frequency patterns that overfit the specific architecture used in train-ing. To address this, we consider regularizing the distillation process to some prior that may help cross-architecture gener-alization. However, how and where to perform this regular-ization poses a delicate balance. For example, restricting our synthetic set to the real data manifold can significantly re-duce the cross-architecture performance gap but is too strong a regularization to learn good distilled datasets. In the limit, it reduces to dataset/coreset selection [7, 10, 29, 57], which is known to not work as well [13, 61, 64, 67].
We propose Generative Latent Distillation (GLaD), which utilizes a deep generative prior by parameterizing the synthetic dataset in the intermediate feature space of generative models, such as Generative Adversarial Networks (GANs) [28]. It encourages the learned datasets to be more generalizable to novel architectures but is also lax enough to not prohibitively restrict the expressiveness of the distilled dataset. GLaD acts as an add-on module and can easily be ap-plied to all existing and future methods of dataset distillation.
There is flexibility in choosing which generative model to use as our prior. By using a generator trained on the target dataset (which is input to the distillation algorithm), our prior uses no additional data or information but consistently im-proves various distillation algorithms. However, for the sole purpose of obtaining the best distilled synthetic dataset, we may use more powerful generators trained on larger datasets and also obtain significant gains. On the other extreme, we explore using randomly initialized generators and genera-tors trained on out-of-distribution datasets. We show that they generate aesthetically pleasing synthetic images with distinct visual characteristics and also achieve comparable distillation performance. In short, while different generator choices affect distillation results in interesting ways, GLaD consistently improves performance over many datasets and multiple distillation algorithms.
Within a deep generative model, there is a spectrum of different latent space choices, corresponding to different lay-ers in the model [2, 47, 71]. Our analysis reveals a trade-off between realism (earlier layer latents) and flexibility (later layer latents), and highlights that using an intermediate latent space achieves a nice balance and consistent performance gain (over the wildly used raw-pixel parametrization).
In Section 4, we perform extensive experiments on
CIFAR-10 (a common dataset distillation benchmark) and
ImageNet subsets at resolutions up to 512 × 512. We inte-grate GLaD with three distinct current distillation algorithms (Gradient Matching [67], Distribution Matching [66], and
Trajectory Matching [13]), and consistently observe signif-icant improvements in cross-architecture generalization of the three currently most accessible methods of dataset distil-lation. Our analysis of results from different configurations provides a better understanding of the effect of different gen-erative models and latent spaces. Additionally, our method drastically reduces the high-frequency noise present in high-resolution datasets distilled into pixel space, leading to visu-ally pleasing images that may have implications in artistic and design applications (e.g., [14]).
GLaD is a plug-and-play addition to any existing and fu-ture distillation methods, allowing them to scale up to more realistic datasets and generalize better to different architec-tures. Given the goal of distilling large-scale datasets, we propose that leveraging generative models and differentiable parameterizations is a natural path forward.
Our contributions are summarized as follows:
• We propose Generative Latent Distillation (GLaD) to add a deep generative prior to Dataset Distillation by distilling into an intermediate feature space of a genera-tive model trained on real data.
• Our extensive analysis and ablations highlight the im-portance of a deep generative prior in addressing the two major challenges of Dataset Distillation: cross-architecture generalization and high-resolution data.
• We show that GLaD is robust to the type of generator used, still performing well with randomly initialized generators and those trained on other datasets.
• Our method acts as a plug-and-play addition to all exist-ing and future methods of dataset distillation, allowing researchers to easily use it for future work. 2.