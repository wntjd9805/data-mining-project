Abstract the person task, one of visible-infrared re-identification
For the major challenges is the (VIReID) modality gaps between visible (VIS) and infrared (IR) images. However, the training samples are usually lim-ited, while the modality gaps are too large, which leads that the existing methods cannot effectively mine diverse cross-modality clues. To handle this limitation, we propose a novel augmentation network in the embedding space, called diverse embedding expansion network (DEEN).
The proposed DEEN can effectively generate diverse em-beddings to learn the informative feature representations and reduce the modality discrepancy between the VIS and IR images. Moreover, the VIReID model may be seriously affected by drastic illumination changes, while all the existing VIReID datasets are captured under suffi-cient illumination without significant light changes. Thus, we provide a low-light cross-modality (LLCM) dataset, which contains 46,767 bounding boxes of 1,064 identities captured by 9 RGB/IR cameras. Extensive experiments on the SYSU-MM01, RegDB and LLCM datasets show the superiority of the proposed DEEN over several other state-of-the-art methods. The code and dataset are released at: https://github.com/ZYK100/LLCM 1.

Introduction
Person re-identification (ReID) aims to match a given person with gallery images captured by different cameras
[3, 9, 52]. Most existing ReID methods [22, 24, 30, 38, 50] only focus on matching RGB images captured by visible cameras at daytime. However, these methods may fail
*Corresponding author. 1
Figure 1. Motivation of the proposed DEEN, which aims to gen-erate diverse embeddings to make the network focus on learning with the informative feature representations to reduce the modality gaps between the VIS and IR images. to achieve encouraging results when visible cameras can-not effectively capture person’s information under complex conditions, such as at night or low-light environments. To solve this problem, some visible (VIS)-infrared (IR) per-son re-identification (VIReID) methods [15,39,41,48] have been proposed to retrieve the VIS (IR) images according to the corresponding IR (VIS) images.
Compared with the widely studied person ReID task, the VIReID task is much more challenging due to the ad-ditional cross-modality discrepancy between the VIS and
IR images [33, 45, 49, 51]. Typically, there are two popu-lar types of methods to reduce this modality discrepancy.
One type is the feature-level methods [5, 11, 16, 35, 40, 42], which try to project the VIS and IR features into a common embedding space, where the modality discrepancy can be minimized. However, the large modality discrepancy makes these methods difficult to project the cross-modality images into a common feature space directly. The other type is the image-level methods [4,28,29,32], which aim to reduce the modality discrepancy by translating an IR (or VIS) image
ture representations. We are the first to augment the embed-dings in the embedding space in VIReID. Besides, we also propose an effective multistage feature aggregation (MFA) block to mine potential channel-wise and spatial feature representations.
• With the incorporation of DEE, CPM loss and MFA into an end-to-end learning framework, we propose an effective diverse embedding expansion network (DEEN), which can effectively reduce the modality discrepancy be-tween the VIS and IR images.
• We collect a low-light cross-modality (LLCM) dataset, which contains 46,767 images of 1,064 identities captured under the environments with illumination changes and low illuminations. The LLCM dataset has more new and impor-tant features, which can facilitate the research of VIReID towards practical applications.
• Extensive experiments show that the proposed DEEN outperforms the other state-of-the-art methods for the
VIReID task on three challenging datasets. 2.