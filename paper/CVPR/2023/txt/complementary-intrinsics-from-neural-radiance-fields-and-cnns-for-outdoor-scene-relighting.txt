Abstract
Relighting an outdoor scene is challenging due to the
Intrinsic diverse illuminations and salient cast shadows. image decomposition on outdoor photo collections could partly solve this problem by weakly supervised labels with albedo and normal consistency from multi-view stereo. With neural radiance fields (NeRF), editing the appearance code could produce more realistic results without interpreting the outdoor scene image formation explicitly. This paper proposes to complement the intrinsic estimation from vol-ume rendering using NeRF and from inversing the photo-metric image formation model using convolutional neural networks (CNNs). The former produces richer and more reliable pseudo labels (cast shadows and sky appearances in addition to albedo and normal) for training the latter to predict interpretable and editable lighting parameters via a single-image prediction pipeline. We demonstrate the ad-vantages of our method for both intrinsic image decompo-sition and relighting for various real outdoor scenes. 1.

Introduction
The same landmark may appear with drastically vary-ing appearances in different photos, even if they are taken from the same viewpoint with the same camera parameters, e.g., the Taj Mahal may look golden or white at sunset or in the afternoon1. For a set of photos containing the same landmark captured in different seasons and times, their “dy-namic” lighting changes (compared with the relatively “sta-ble” geometry and reflectance) play a vital role in explain-ing such great appearance variations. If we can indepen-dently manipulate lighting in these photos, the relighted outdoor scenes could substantially improve experiences for taking digital photographs.
Outdoor scene relighting could be realized by learning a style transfer procedure [1, 5]. Such a process only requires
#
∗
Contributed equally to this work as first authors
Corresponding author 1Changing colours of Taj Mahal: agratajcitytour.com a single reference image for editing a target image, but it ap-parently retouches the image to “look like” each other, with-out explicitly modeling the lighting changes. Intrinsic im-age decomposition, which inversely decomposes the photo-metric image formation model, has been extended to work with outdoor photo collections [32–34]. The common ge-ometry/reflectance (for the whole collection) and distinctive lighting components (for each image) are estimated using deep convolutional neural networks (CNNs), so that relight-ing could be achieved by keeping the former while editing the latter in a physics-aware manner. These methods ex-plicitly conduct computationally expensive multi-view re-construction of the scene at the training stage. The weakly supervised constraints built upon albedo and normal con-sistency via multi-view correspondence cannot support the handling of cast shadows [33] or still struggle with strong cast shadows [32].
Recently, the emerging of neural radiance fields (NeRF) [23] has not only boosted the performance of novel view synthesis with significantly better quality for outdoor scene photo collections [22], but has also been demon-strated to be capable of transferring the lighting appearance across the image set using hallucination [4] or a parametric lighting model [28]. However, these existing NeRF meth-ods in the outdoor scene either miss the explanation to some important intrinsics for relighting such as cast shadows (ex-cept for [28]) or ignore distinctive characteristics between the non-sky and sky regions, in a physically interpretable manner.
In this paper, we hope to conduct outdoor scene re-lighting by mutually complementing intrinsics estimated from NeRF and CNN and taking advantages of the com-prehensive representation power of NeRF and physics in-terpretability of CNN-based single-image decomposition, in a single-image inference pipeline as shown in Figure 1.
We formulate the color formation of a pixel as a combina-tion of objects and the sky by tracing rays from camera ori-gins to the furthest plane and accumulating the voxel intrin-Figure 1. Illustration of our overall pipeline. For each image, the corresponding camera rays and sampled 3D points are positionally-encoded for MLPs (gray block). Our IntrinsicCNN and LightingCNN modules derive lighting-(in)dependent intrinsic components and second-order spherical harmonics (SH) lighting coefficients (green block), while Our NeRF module provides the pseudo labels of intrinsics and sky mask by volumetric rendering through MLPs (blue block). Our SkyMLP module renders the sky with viewing direction and extracted lighting (orange block). Given lighting extracted from the input/reference image, the reconstructed/relighted image is rendered by photometric image formation along with the rendered sky (yellow block). sics. We then propose a modified NeRF system to estimate diffuse albedo, surface normal, cast shadow, and illumina-tion parameters. Our NeRF rendering naturally shares the albedo and normal of one point across all images and inter-prets the geometry from voxel density, which provides more accurate pseudo labels for identifying shadows than purely
CNN-based approaches [32–34]. We finally predict the in-trinsics and lighting parameters by designing two separate
CNN modules based on the NeRF-produced pseudo labels with a clearer separation of lighting-dependent and indepen-dent intrinsic components to achieve high-quality relighting via single-image prediction.
Hence, our contribution becomes clear in three folds:
Based on 1) the newly proposed “object-sky” hybrid image formation, 2) the intrinsics estimated from NeRF provide more accurate pseudo labels to complement 3) the intrin-sics estimated from CNNs, for conducting outdoor scene relighting using a single image in a physically interpretable manner and with a visually pleasing appearance, which is demonstrated by our experimental results. 2.