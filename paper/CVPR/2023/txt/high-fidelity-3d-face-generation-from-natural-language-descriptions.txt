Abstract
Synthesizing high-quality 3D face models from natural language descriptions is very valuable for many applica-tions, including avatar creation, virtual reality, and telep-resence. However, little research ever tapped into this task.
We argue the major obstacle lies in 1) the lack of high-quality 3D face data with descriptive text annotation, and 2) the complex mapping relationship between descriptive language space and shape/appearance space. To solve these problems, we build DESCRIBE3D dataset, the first large-scale dataset with fine-grained text descriptions for text-to-3D face generation task. Then we propose a two-stage framework to first generate a 3D face that matches the concrete descriptions, then optimize the parameters in the 3D shape and texture space with abstract description to refine the 3D face model. Extensive experimental re-sults show that our method can produce a faithful 3D face that conforms to the input descriptions with higher accu-racy and quality than previous methods. The code and DE-SCRIBE3D dataset are released at https://github. com/zhuhao-nju/describe3d. 1.

Introduction 3D faces are highly required in many cutting-edge tech-nologies like digital humans, telepresence, and movie spe-cial effects, while creating a high-fidelity 3D face is very complex and requires vast time from an experienced mod-eler. Recently, many efforts are devoted to the synthesis of text-to-image and image-to-3D, but they lack the ability to synthesize 3D faces given an abstract description. However, there is still no reliable solution to synthesize high-quality 3D faces from descriptive texts in natural language.
We consider the difficulties of synthesizing high-quality 3D face models from natural language descriptions lie in two folds. Firstly, there is still no available fine-grained dataset that contains 3D face models and corresponding text descriptions in the research community, which is crucial for training learning-based 3D generators. Beyond that, it is difficult to leverage massive 2D Internet images to learn
Figure 1. Given a text describing the appearance (left), our method can synthesize high-quality 3D faces (middle) containing 3D mesh and textures. The resulting model can be easily processed into a rigged face with hair and accessories (right). The dark blue texts indicate concrete descriptions and the brown texts indicate abstract descriptions, and similarly hereinafter. high-quality text-to-3D mapping. Secondly, cross-modal mapping from texts to 3D models is non-trivial. Though the progress made in text-to-image synthesis is instructive, the problem of mapping texts to 3D faces is even more chal-lenging due to the complexity of 3D representation.
In this work, we aim at tackling the task of high-fidelity 3D face generation from natural text descriptions from the above two perspectives. We first build a 3D-face-text dataset (named DESCRIBE3D), which contains 1, 627 high-quality 3D faces from HeadSpace dataset [6] and FaceScape dataset [48, 55], and fine-grained manually-labeled facial features. The provided annotations include 25 facial at-tributes, each of which contains 3 to 8 options describing the facial feature. Our dataset covers various races and ages and is delicate in 3D shape and texture. We then propose a two-stage synthesis pipeline, which consists of a concrete synthesis stage mapping the text space to the 3D shape and texture space, and an abstract synthesis stage refining the 3D face with a prompt learning strategy. The mapping for different facial features is disentangled and the diversity of the generative model can be controlled by the additional input of random seeds. As shown in Figure 1, our pro-posed model can take any word description or combination
of phrases as input, and then generate an output of a fine-textured 3D face with appearances matching the descrip-tion. Extensive experiments further validate that the con-crete synthesis can generate a detailed 3D face that matches the fine-grained descriptive texts well, and the abstract syn-thesis enables the network to synthesize abstract features like “wearing makeup” or “looks like Tony Stark”.
In summary, our contributions are as follows:
• We explore a new topic of constructing a high-quality 3D face model from natural descriptive texts and pro-pose a baseline method to achieve such a goal.
• A new dataset - DESCRIBE3D is established with de-tailed 3D faces and corresponding fine-grained de-scriptive annotations. The dataset will be released to the public for research purposes.
• The reliable mapping from the text embedding space to the 3D face parametric space is learned by intro-ducing the descriptive code space as an intermediary, which forms the core of our concrete synthesis mod-ule. Region-specific triplet loss and weighted ℓ1 loss further boost the performance.
• Abstract learning based on CLIP is introduced to fur-ther optimize the parametric 3D face, enabling our re-sults to conform with abstract descriptions. 2.