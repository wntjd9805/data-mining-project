Abstract 3D semantic segmentation on multi-scan large-scale point clouds plays an important role in autonomous sys-tems. Unlike the single-scan-based semantic segmentation task, this task requires distinguishing the motion states of points in addition to their semantic categories. However, methods designed for single-scan-based segmentation tasks perform poorly on the multi-scan task due to the lacking of an effective way to integrate temporal information. We pro-pose MarS3D, a plug-and-play motion-aware module for semantic segmentation on multi-scan 3D point clouds. This module can be flexibly combined with single-scan models to allow them to have multi-scan perception abilities. The model encompasses two key designs: the Cross-Frame Fea-ture Embedding module for enriching representation learn-ing and the Motion-Aware Feature Learning module for en-hancing motion awareness. Extensive experiments show that MarS3D can improve the performance of the base-line model by a large margin. The code is available at https://github.com/CVMI-Lab/MarS3D. 1.

Introduction 3D semantic segmentation on multi-scan large-scale point clouds is a fundamental computer vision task that ben-efits many downstream problems in autonomous systems, such as decision-making, motion planning, and 3D recon-struction, to name just a few. Compared with the single-scan semantic segmentation task, this task requires under-standing not only the semantic categories but also the mo-tion states (e.g., moving or static) of points based on multi-scan point cloud data.
In the past few years, extensive research has been con-ducted on single-scan semantic segmentation with signifi-cant research advancements [4, 5, 12, 25, 31, 33, 36]. These approaches are also applied to process multi-scan point
*Equal contribution.
†Corresponding author.
Figure 1. Comparison of our proposed method, MarS3D, with baseline method using SPVCNN [25] as the backbone on Se-manticKITTI [1] dataset. MarS3D achieves excellent results in the classification of semantic categories and motion states, while the baseline method can not distinguish motion well from static. clouds, wherein multiple point clouds are fused to form a single point cloud before being fed to the network for processing. Albeit simple, this strategy may lose tempo-ral information and make distinguishing motion states a challenging problem. As a result, they perform poorly in classifying the motion states of objects. As shown in Figure 1, the simple point cloud fusion strategy can-not effectively enable the model to distinguish the motion states of cars even with a state-of-the-art backbone network
SPVCNN [25]. Recently, there have been some early at-tempts [7, 23, 24, 28] to employ attention modules [24] and recurrent networks [7,23,28] to fuse information across dif-ferent temporal frames. However, these approaches do not perform well on the multi-scan task due to the insufficiency of temporal representations and the limited feature extrac-tion ability of the model.
In sum, a systematic investigation of utilizing the rich spatial-temporal information from multiple-point cloud scans is still lacking. This requires answering two critical questions: (1) how can we leverage the multi-scan informa-tion to improve representation learning on point clouds for
better semantic understanding? and (2) how can the tem-poral information be effectively extracted and learned for classifying the motion states of objects?
In this paper, we propose a simple plug-and-play
Motion-aware Segmentation module for 3D multi-scan analysis (MarS3D), which can seamlessly integrate with existing single-scan semantic segmentation models and en-dow them with the ability to perform accurate multi-scan 3D point cloud semantic segmentation with negligible com-putational costs. Specifically, our method incorporates two core designs: First, to enrich representation learning of multi-frame point clouds, we propose a Cross-Frame Fea-ture Embedding (CFFE) module which embeds time-step information into features to facilitate inter-frame fusion and representation learning. Second, inspired by the observa-tion that objects primarily move along the horizontal ground plane (i.e., xy-plane) in large-scale outdoor scenes, i.e., min-imal motion along the z-axis, we propose a Motion-Aware
Feature Learning (MAFL) module based on Bird’s Eye
View (BEV), which learns the motion patterns of objects between frames to facilitate effectively discriminating the motion states of objects.
We extensively evaluate our approach upon several mainstream baseline frameworks on SemanticKITTI [1] and nuScenes [3] dataset. It consistently improves the per-formance of the baseline approaches, e.g., MinkUnet [5], by 6.24% in mIoU on SemanticKITTI with a negligible in-crease in model parameters, i.e., about 0.2%. The main con-tributions are summarized as follows:
• We are the first to propose a plug-and-play mod-ule for large-scale multi-scan 3D semantic segmenta-tion, which can be flexibly integrated with mainstream single-scan segmentation models without incurring too much cost.
• We devise a Cross-Frame Feature Embedding module to fuse multiple point clouds while preserving their temporal information, thereby enriching representa-tion learning for multi-scan point clouds.
• We introduce a BEV-based Motion-Aware Feature
Learning module to exploit temporal information and enhance the model’s motion awareness, facilitating the prediction of motion states.
• We conduct extensive experiments and comprehensive analyses of our approach with different backbone mod-els. The proposed model performs favorably compared to the baseline methods while introducing negligible extra parameters and inference time. 2.