Abstract
Object goal navigation (ObjectNav) in unseen environ-ments is a fundamental task for Embodied AI. Agents in ex-isting works learn ObjectNav policies based on 2D maps, scene graphs, or image sequences. Considering this task happens in 3D space, a 3D-aware agent can advance its
ObjectNav capability via learning from fine-grained spa-tial information. However, leveraging 3D scene representa-tion can be prohibitively unpractical for policy learning in this floor-level task, due to low sample efficiency and expen-sive computational cost. In this work, we propose a frame-work for the challenging 3D-aware ObjectNav based on two straightforward sub-policies. The two sub-polices, namely corner-guided exploration policy and category-aware iden-tification policy, simultaneously perform by utilizing on-line fused 3D points as observation. Through extensive ex-periments, we show that this framework can dramatically improve the performance in ObjectNav through learning from 3D scene representation. Our framework achieves the best performance among all modular-based methods on the
Matterport3D and Gibson datasets, while requiring (up to 30x) less computational cost for training. The code will be released to benefit the community.1 1.

Introduction
As a vital task for intelligent embodied agents, object goal navigation (ObjectNav) [38, 49] requires an agent to find an object of a particular category in an unseen and unmapped scene. Existing works tackle this task through end-to-end reinforcement learning (RL) [27, 36, 47, 51] or modular-based methods [9, 14, 35]. End-to-end RL based methods take as input the image sequences and directly output low-level navigation actions, achieving competitive
*Joint first authors
†Corresponding author: hewang@pku.edu.cn 1Homepage: https://pku-epic.github.io/3D-Aware-ObjectNav/
Figure 1. We present a 3D-aware ObjectNav framework along with simultaneous exploration and identification policies: A→B, the agent was guided by an exploration policy to look for its target;
B → C, the agent consistently identified a target object and finally called STOP. performance while suffering from lower sample efficiency and poor generalizability across datasets [3, 27]. Therefore, we favor modular-based methods, which usually contain the following modules: a semantic scene mapping module that aggregates the RGBD observations and the outputs from semantic segmentation networks to form a semantic scene map; an RL-based goal policy module that takes as input the semantic scene map and learns to online update a goal location; finally, a local path planning module that drives the agent to that goal. Under this design, the semantic ac-curacy and geometric structure of the scene map are crucial to the success of object goal navigation.
We observe that the existing modular-based methods mainly construct 2D maps [8, 9], scene graphs [34, 56] or neural fields [43] as their scene maps. Given that objects lie in 3D space, these scene maps are inevitably deficient in leveraging 3D spatial information of the environment com-prehensively and thus have been a bottleneck for further improving object goal navigation.
In contrast, forming a 3D scene representation naturally offers more accurate, spa-tially dense and consistent semantic predictions than its 2D counterpart, as proved by [12, 31, 45]. Hence, if the agent could take advantage of the 3D scene understanding and
form a 3D semantic scene map, it is expected to advance the performance of ObjectNav.
However, leveraging 3D scene representation would bring great challenges to ObjectNav policy learning. First, building and querying fine-grained 3D representation across a floor-level scene requires extensive computational cost, which can significantly slow down the training of RL [7,55].
Also, 3D scene representation induces considerably more complex and high-dimensional observations to the goal policy than its 2D counterpart, leading to a lower sam-ple efficiency and hampering the navigation policy learn-ing [22, 57]. As a result, it is demanding to design a frame-work to efficiently and effectively leverage powerful 3D in-formation for ObjectNav.
To tackle these challenges, we propose a novel frame-work composed of an online semantic point fusion module for 3D semantic scene mapping and two parallel policy net-works in charge of scene exploration and object identifica-tion, along with a local path planning module. Our online semantic point fusion module extends a highly efficient on-line point construction algorithm [53] to enable online se-mantic fusion and spatial semantic consistency computation from captured RGBD sequences. This 3D scene construc-tion empowers a comprehensive 3D scene understanding for ObjectNav. Moreover, compared to dense voxel-based methods [7, 55], our point-based fusion algorithm are more memory-efficient [40, 46] which makes it practically usable for floor-level navigation task. (See Figure 1)
Moreover, to ease the learning of navigation policy, we further propose to factorize the navigation policy into two sub-policies, namely exploration and identification. The two policies simultaneously perform to roll out an explo-ration goal and an identified object goal (if exist), respec-tively. Then the input for the local path planning module will switch between these two goals, depending on whether there exists an identified target object. More specifically, we propose a corner-guided exploration policy which learns to predict a long-term discrete goal at one of the four corners of the bounding box of the scene. These corner goals ef-ficiently drive the agent to perceive the surroundings and explore regions where the target object is possibly settled.
And for identification, a category-aware identification pol-icy is proposed to dynamically learn a discrete confidence threshold to identify the semantic predictions for each cat-egory. Both of these policies are trained by RL in low-dimensional discrete action space. Through experiments, the simultaneous two-policy mechanism and discrete action space design dramatically reduce the difficulty in learning for 3D-aware ObjectNav and achieve better performance than existing modular-based navigation strategies [26, 35].
Through extensive evaluation on the public benchmarks, we demonstrate that our method performs online 3D-aware
ObjectNav at 15 FPS while achieving the state-of-the-art performance on navigation efficiency. Moreover, our method outperforms all other modular-based methods in both efficiency and success rate with up to 30x times less computational cost.
Our main contributions include:
• We present the first 3D-aware framework for Object-Nav task.
• We build an online point-based construction and fusion algorithm for efficient and comprehensive understand-ing of floor-level 3D scene representation.
• We propose a simultaneous two-policy mechanism which mitigates the problem of low sample efficiency in 3D-aware ObjectNav policy learning. 2.