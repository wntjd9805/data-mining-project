Abstract
Retrieval augmented models are becoming increasingly popular for computer vision tasks after their recent success in NLP problems. The goal is to enhance the recognition ca-pabilities of the model by retrieving similar examples for the visual input from an external memory set. In this work, we introduce an attention-based memory module, which learns the importance of each retrieved example from the mem-ory. Compared to existing approaches, our method removes the influence of the irrelevant retrieved examples, and re-tains those that are beneficial to the input query. We also thoroughly study various ways of constructing the memory dataset. Our experiments show the benefit of using a massive-scale memory dataset of 1B image-text pairs, and demon-strate the performance of different memory representations.
We evaluate our method in three different classification tasks, namely long-tailed recognition, learning with noisy labels, and fine-grained classification, and show that it achieves state-of-the-art accuracies in ImageNet-LT, Places-LT and
Webvision datasets. 1.

Introduction
Increasing the number of parameters of large trans-former models has been a recent successful trend achiev-ing new benchmarks in vision and language tasks. Recent results from T5 [35], GPT-3 [5], PaLM [8], CoCa [50],
Flamingo [1], BEIT-3 [46], PaLI [7], Florence [51] and
FLAVA [40] show that transformer models are able to store a surprising amount of information when scaled to tens of billions of parameters and trained on vast text and image corpora. These so-called ‘foundation models’ achieve state-of-the-art results when fine tuned and applied to secondary tasks such as language modeling, image captioning, visual question answering and open vocabulary recognition.
In these foundation models, the learned world knowledge is stored implicitly in the parameters of the underlying neu-ral network. This implies that some of the problems of the current ML paradigm are amplified in these models: (a) scal-ing is challenging, both in learning and serving, given the large number of parameters that are required for storing the
Figure 1. Retrieval augmented classification finds similar images to the query from an external memory. Our memory attention module learns the importance of each retrieved image by assigning high weights (green line in the figure) to the relevant images, and low weights (red line) to the irrelevant images. knowledge, (b) it is hard to update the model as the world facts change or input data gets modified, (c) these models tend to be black box, which means it is hard to interpret the underlying reason behind their decisions.
To address the above issues, we propose an alternative perspective on the problem. Instead of compiling the world knowledge statically into model weights, we take an interpre-tive view where the world knowledge gets transformed into a massive-scale index/memory. On the other hand, a relatively low-compute small model learns to use the memory for the given inference task. Instead of increasing the size of the model and training on more data as done in most previous work, we equip models with the ability to directly access a large database to perform predictions—a semi-parametric approach.
To evaluate our approach, we focus on the problem of long-tailed recognition and learning with noisy labels. The distribution of real-world data is often noisy, imbalanced and highly skewed on a per-class basis, with a majority of classes containing a small number of samples. Long-tailed recog-nition is a well-studied problem [19, 33]. Base approaches are largely variants of the core idea of “adjustment”, where the learner is encouraged to focus on the tail of the distribu-tion. This is achieved either by re-weighting samples during training [21] and cluster-based sampling [10], logit or loss modification [12, 20, 31, 54] or ensembling [47]. Despite be-ing well-studied, commonly occurring, and of great practical importance, classification performance on long-tail distribu-Figure 2. Overview of our method. Retrieval augmented classification aims to retrieve relevant images from an external memory dataset when making predictions. Each example in the memory is composed of a key and value embedding pair. Key embeddings are extracted using the same visual encoder as the query image, but the value embeddings can be extracted with any other encoder. Both visual and value encoders are remain frozen during the training. We perform an approximate k-NN search between the query embedding and memory keys to find relevant images from the memory dataset. The retrieval module receives the query embedding and the k retrieved key-value pairs from the memory. We learn the importance of each memory example by computing the attention weights between the query embedding and the memory keys. The memory values, weighted by their corresponding attention weights, are used to compute the refined embedding, which is then passed to the classifier. tions lags significantly behind the state of the art for better balanced classes [15, 49, 55].
Long et al. [29] introduce a retrieval-augmented classi-fication model that explicitly stores the tail knowledge. In comparison to this work, we suggest to retrieve from a web-scale vision-text database and augment the input query with the retrieved knowledge using a memory attention module, before making class predictions. We design the external memory as pairs of key-value embeddings. These embed-dings are computed by encoding vision and language data from multiple sources (Web images with alt-text such as
Webli [7], LAION [39], YFCC100M [42] datasets as well as image classification datasets like ImageNet [37]). Memory key embeddings are used to retrieve the k-nearest neighbors of the input query vectors. Our memory attention module learns the importance of each retrieved memory example by computing attention weights between the query embedding and memory keys. Relevant examples have more influence, whereas the contribution of the irrelevant noisy examples is down-weighted. Learned attention weights are then used to combine memory values and produce a refined embedding, which is then used to make class predictions. Figure 1 shows a high-level visualization of our method.
Our contributions are summarized as follows:
• We propose a retrieval-augmented recognition model that explores efficient means of augmenting visual mod-els with a massive-scale memory without significantly increasing computations.
• We propose a simple yet powerful way to fuse the re-trieved knowledge with the input query using a memory attention module.
• Our method achieves state-of-the-art results on var-ious benchmarks, such as long-tail recognition and learning with noisy labels. We achieve 78.9 accuracy on ImageNet-LT dataset, 50.3 accuracy on Places-LT dataset, and 83.6 on Webvision dataset. 2.