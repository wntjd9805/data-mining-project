Abstract
Referring expression segmentation aims to segment an object described by a language expression from an image.
Despite the recent progress on this task, existing models tackling this task may not be able to fully capture semantics and visual representations of individual concepts, which limits their generalization capability, especially when han-dling novel compositions of learned concepts. In this work, through the lens of meta learning, we propose a Meta Com-positional Referring Expression Segmentation (MCRES) framework to enhance model compositional generalization performance. Specifically, to handle various levels of novel compositions, our framework first uses training data to con-struct a virtual training set and multiple virtual testing sets, where data samples in each virtual testing set contain a level of novel compositions w.r.t. the virtual training set.
Then, following a novel meta optimization scheme to opti-mize the model to obtain good testing performance on the virtual testing sets after training on the virtual training set, our framework can effectively drive the model to better cap-ture semantics and visual representations of individual con-cepts, and thus obtain robust generalization performance even when handling novel compositions. Extensive experi-ments on three benchmark datasets demonstrate the effec-tiveness of our framework. 1.

Introduction
Referring expression segmentation (RES) [12, 38, 40] aims to segment a visual entity in an image given a lin-guistic expression. This task has been receiving increas-ing attention in recent years [5, 18, 34, 37], as it can play an important role in various applications, such as language-based human-robot interaction and interactive image edit-*Corresponding Author (a) (b)
Figure 1. Illustration of novel compositions and various levels of compositions. (a) An example of the testing sample containing the novel composition of “dark coffee” in RefCOCO dataset [41].
Such a novel composition itself does not exist in the training data, but its individual components (i.e., “dark” and “coffee”) exist in the training data. (b) An example of various levels of compositions in an expression. We perform constituency parsing of the expres-sion using AllenNLP [9]. Based on the obtained parsing tree as shown above, we can then obtain various levels of compositions (e.g., word-word level, word-phrase level) in this expression. ing. However, despite the recent progress on tackling this task [5, 34, 42], existing methods may struggle with han-dling the testing samples, of which the expressions contain novel compositions of learned concepts. Here a novel com-position means that the composition itself does not exist in the training data, but its individual components (e.g., words, phrases) exist in the training data, as shown in Fig. 1a.
We observe that testing samples containing such novel compositions of learned concepts widely exist in RES datasets [26, 28, 41]. However, existing RES models may not be able to well handle novel compositions during test-ing. Here we test the generalization capability of multi-ple state-of-the-art models [25, 37, 42] in terms of handling
novel compositions, as shown in Table 2. Specifically, we first split each testing set of the RefCOCO dataset. In each testing set, one split subset includes the data samples, in which all the contained compositions are seen in the Ref-COCO training set. While another subset includes the data samples containing novel compositions, of which the indi-vidual components (e.g., words, phrases) exist in the Re-fCOCO training set but the composition itself is unseen in the training set, i.e., containing novel compositions of learned concepts. Then we evaluate the models [25, 37, 42] on the two subsets in each testing set, and find that for each model, its testing performance on the subset contain-ing novel compositions drops obviously compared to the performance on the other subset. For these models, the per-formance gap between the two subsets can reach 14%−17% measured by the metric of overall IoU. Such a clear perfor-mance gap indicates that existing models struggle with gen-eralizing to novel compositions of learned concepts. This might due to that the model does not effectively capture the semantics and visual representations of individual concepts (e.g., “dark”, “coffee” in Fig. 1a) during training. Then the trained model may fail to recognize a novel composi-tion (e.g., “dark coffee”) at testing time, which though is composed of learned concepts.
Thus to handle this issue, we aim to train the model to effectively capture the semantics and visual representations of individual concepts during training. Despite the concep-tual simplicity, how to guide the model’s learning behavior towards this goal is a challenging problem. Here from the perspective of meta learning, we propose a Meta Composi-tional Referring Expression Segmentation (MCRES) frame-work, to effectively handle such a challenging problem by only changing the model training scheme.
Meta learning proposes to perform virtual testing during model training for better performance [7, 29]. Inspired by this, to improve the generalization capability of RES mod-els, our MCRES framework incorporates a meta optimiza-tion scheme that consists of three steps: virtual training, virtual testing and meta update. Specifically, we first split the training set to construct a virtual training set for virtual training, and a virtual testing set for virtual testing. The data samples in the virtual testing set contain novel com-positions w.r.t. the virtual training set. For example, if the expressions of data samples in the virtual training set con-tain both words “dark” and “coffee” but do not contain their composition (i.e., “dark coffee”), the virtual testing set can include this novel composition correspondingly.
Based on the constructed virtual training set and virtual testing set, we first train the model using the virtual train-ing set, and then evaluate the trained model on the virtual testing set. During virtual training, the model may learn the compositions of individual concepts as a whole with-out truly understanding the semantics and visual representa-tions of individual concepts, which though can still improve model training performance. For example, if there are many training samples containing the composition of “yellow ba-nana” in the virtual training set, the model can superficially correlate “banana” with “yellow” and learn this composi-tion as a whole, since using such spurious correlations can facilitate the model learning [1, 10, 39]. However, learning the compositions as a whole over the virtual training set may not improve model performance much on the virtual testing set in virtual testing, since the virtual testing set contains novel compositions w.r.t. the virtual training set. Thus to achieve good testing performance on such a virtual testing set, the model needs to effectively capture semantics and visual representations of individual concepts during virtual training. In this way, the model testing performance on the virtual testing set serves as a generalization feedback to the model virtual training process.
Thus after the virtual training and virtual testing, we can further update the model to obtain better testing per-formance on the virtual testing set (i.e., meta update), so as to drive the model training on the virtual training set to-wards the direction of learning to capture semantics and vi-sual representations of individual concepts, i.e., learning to learn. In this manner, our framework is able to optimize the model for robust generalization performance, even tackling the challenging testing samples with novel compositions.
Moreover, given that expressions can often be hierarchi-cally decomposed, there can exist various levels of novel compositions. Specifically, to identify meaningful compo-sitions in an expression, we can parse an expression into a tree structure based on the constituency parsing tool [9] as shown in Fig.1b. In such a parsing tree, under the same par-ent node, each pair of child nodes (e.g., “white” and “bird”) are closely semantically related, and thus can form a mean-ingful composition. Since each child node can be a word or a phrase as in Fig. 1b, there can naturally exist the following three levels of novel compositions: word-word level (e.g.,
“white” and “bird”), word-phrase level (e.g., “standing” and
“in water”) and phrase-phrase level (e.g., “white bird” and
“standing in water”), which correspond to different levels of comprehension complexity. To better handle such a range of novel compositions, we construct multiple virtual test-ing sets in our framework, where each virtual testing set is constructed to handle one level of novel compositions.
Our framework only changes the model training scheme without the need to change the model structure. Thus our framework is general, and can be conveniently applied on various RES models. We test our framework on multiple models, and obtain consistent performance improvement.
The contributions of our work are threefold: 1) We propose a novel framework (MCRES) to effectively im-prove generalization performance of RES models, espe-cially when handing novel compositions of learned con-cepts. 2) Via constructing a virtual training set and mul-tiple virtual testing sets w.r.t. various levels of novel com-positions, our framework can train the model to well han-dle various levels of novel compositions. 3) When applied on various models on three RES benchmarks [28, 41], our framework achieves consistent performance improvement. 2.