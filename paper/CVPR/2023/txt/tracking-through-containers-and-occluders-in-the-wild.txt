Abstract
Tracking objects with persistence in cluttered and dy-namic environments remains a difficult challenge for com-puter vision systems. In this paper, we introduce TCOW, a new benchmark and model for visual tracking through heavy occlusion and containment. We set up a task where the goal is to, given a video sequence, segment both the pro-jected extent of the target object, as well as the surrounding container or occluder whenever one exists. To study this task, we create a mixture of synthetic and annotated real datasets to support both supervised learning and structured evaluation of model performance under various forms of task variation, such as moving or nested containment. We evaluate two recent transformer-based video models and find that while they can be surprisingly capable of track-ing targets under certain settings of task variation, there re-mains a considerable performance gap before we can claim a tracking model to have acquired a true notion of object permanence. 1.

Introduction
The interplay between containment and occlusion can present a challenge to even the most sophisticated visual reasoning systems. Consider the pictorial example in Fig-ure 1a. Given four frames of evidence, where is the red ball in the final frame? Could it be anywhere else? What visual evidence led you to this conclusion?
In this paper, we explore the problem of tracking and segmenting a target object as it becomes occluded or con-tained by other dynamic objects in a scene. This is an essential skill for a perception system to attain, as objects of interest in the real world routinely get occluded or con-tained. Acquiring this skill could, for example, help a robot to better track objects around a cluttered kitchen or ware-house [10], or a road agent to understand traffic situations more richly [62]. There are also applications in augmented reality, smart cities, and assistive technology.
It has long been known that this ability, commonly re-ferred to as object permanence, emerges early on in a child’s lifetime (see e.g. [2, 3, 5–8, 49, 54–57]). But how far away
Figure 1. Containment (a) and occlusion (b) happen constantly in the real world. We introduce a novel task and dataset for evalu-ating the object permanence capabilities of neural networks under diverse circumstances. are computer vision systems from attaining the same?
To support the study of this question, we first propose a comprehensive benchmark video dataset of occlusion- and containment-rich scenes of multi-object interactions. These scenes are sourced from both simulation, in which ground truth masks can be perfectly synthesized, and from the real world, which we hand-annotate with object segments. To allow for an extensive analysis of the behaviours of exist-ing tracking systems, we ensure that our evaluation set cov-ers a wide range of different types of containment and oc-clusion. For example, even though an object undergoing containment is already a highly non-trivial event, contain-ers can move, be nested, be deformable, become occluded, and much more. Occlusion can introduce considerable un-certainty, especially when the occludee, the occluder, or the camera are in motion on top of everything else.
Using our dataset, we explore the performance of two re-cent state-of-the-art video transformer architectures, which we repurpose for the task of tracking and segmenting a target object through occlusion and containment in RGB video. We show through careful quantitative and qualitative analyses that while our models achieve reasonable tracking performance in certain settings, there remains significant room for improvement in terms of reasoning about object permanence in complicated, realistic environments. By re-leasing our dataset and benchmark along with this paper, we hope to draw attention to this challenging milestone on the path toward strong spatial reasoning capabilities. 1
2.