Abstract
Multimodal semantic understanding often has to deal with uncertainty, which means the obtained messages tend to refer to multiple targets. Such uncertainty is problematic for our interpretation, including inter- and intra-modal un-certainty. Little effort has studied the modeling of this uncer-tainty, particularly in pre-training on unlabeled datasets and fine-tuning in task-specific downstream datasets. In this paper, we project the representations of all modali-ties as probabilistic distributions via a Probability Distri-bution Encoder (PDE) by utilizing sequence-level interac-tions. Compared to the existing deterministic methods, such uncertainty modeling can convey richer multimodal seman-tic information and more complex relationships. Further-more, we integrate uncertainty modeling with popular pre-training frameworks and propose suitable pre-training tasks:
Distribution-based Vision-Language Contrastive learning (D-VLC), Distribution-based Masked Language Modeling (D-MLM), and Distribution-based Image-Text Matching (D-ITM). The fine-tuned models are applied to challenging downstream tasks, including image-text retrieval, visual question answering, visual reasoning, and visual entailment, and achieve state-of-the-art results.
Figure 1. Multimodal uncertainties and an example for language un-certainty (b) by modeling as point representations and distribution representations. The images and text are from MSCOCO [30]. 1.

Introduction modality.
Precise understanding is a fundamental ability of hu-man intelligence, whether it involves localizing objects from similar semantics or finding corresponding across multiple modalities. Our artificial models suppose to do the same, pinpointing exact concepts from rich multimodal seman-tic scenarios. However, this kind of precise understanding is challenging. Information from different modalities can present rich semantics from each other, but the resulting am-biguity and noise are also greater than the case with a single
*Equal contribution.
Â²Corresponding Author.
Multimodal representation learning methods hold the promise of promoting the desired precise understanding across different modalities [13]. While these methods have shown promising results, current methods face the challenge of uncertainty [7, 51], including within a modality and be-tween modalities. Considering image (a.0) in Fig. 1 as an example, one vision region includes multiple objects, such as a billboard, several zebras and others. Therefore, it is unclear which objects when mentioning this region. In the language domain, the complex relationships of words lead to uncertainty, such as synonymy and hyponymy. In Fig. 1 (c)&(d), the same object often has different descriptions
from different modalities, such as text and images, which manifests inter-modal uncertainty. Instead, previous methods often neglect the uncertainty [11, 19, 46], resulting in limited understanding ability on complicated concept hierarchies and poor prediction diversity. Therefore, it is desirable to model such uncertainty.
Moreover, with multimodal datasets becoming more com-monplace, there is a flourishing trend to implement pre-training models, particularly Vision-Language Pre-training (VLP), to support downstream applications [6, 18, 23, 36, 50].
Existing deterministic representations, however, often fail to understand uncertainty in pre-training data, as they can only express positions in semantic space and measure the relation-ship between targets in certainty, such as Euclidean distance.
How can we efficiently model uncertainty in multi-modalities when dealing with pre-training models?
Applying Gaussian distribution is one of the prominent approaches used for modeling uncertainty in the representa-tion space [40, 45, 51, 54]. In these methods, however, the ob-tained uncertainty depends on individual features rather than considering the whole features together, which ignores the in-ner connection between features. To exploit this connection, we implicitly model them when formulating the uncertainty with a module called Probability Distribution Encoder (PDE).
Inspired by the self-attention mechanism [44], we further add the interaction between text tokens and image patches when constructing our distribution representations to capture more information. In Figure 1 (e), we provide an example for two different types of representations to describe the lan-guage uncertainty, where the distribution representations can express richer semantic relationships than the conventional point representations. The distribution variance measures the uncertainty of the corresponding text. As a byproduct, distri-bution representations enable diverse generations, providing multiple reasonable predictions with random sampling.
In this paper, we integrate this uncertainty modeling in the pre-training framework, resulting in three new tasks:
Distribution-based Vision-Language Contrastive learning (D-VLC), Distribution-based Masked Language Modeling (D-MLM), and Distribution-based Image-Text Matching (D-ITM) pre-training tasks. All these tasks are to deal with cross-modality alignment. More specifically, D-VLC is to handle the coarse-grained cross-modal alignment, which measures the whole distributions to align representations from differ-ent domains. D-MLM and D-ITM are implemented after the fine-grained interaction between different modalities, pro-viding the token level and overall level alignment for images and text.
Our contributions are summarized as follows: 1) We focus on the semantic uncertainty of multimodal un-derstanding and propose a new module, called Probability
Distribution Encoder, to frame the uncertainty in multimodal representations as Gaussian distributions. 2) We develop three uncertainty-aware pre-training tasks to deal with large-scale unlabeled datasets, including D-VLC,
D-MLM, and D-ITM tasks. To the best of our knowledge, these are the first attempt to harness the probability distribu-tion of representations in VLP. 3) We wrap the proposed pre-training tasks into an end-2-end
Multimodal uncertainty-Aware vision-language Pre-training model, called MAP, for downstream tasks. Experiments show
MAP gains State-of-The-Art (SoTA) performance. Our code is available at https://github.com/IIGROUP/MAP. 2.