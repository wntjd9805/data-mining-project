Abstract 1.

Introduction
In this paper, we introduce FitMe, a facial reflectance model and a differentiable rendering optimization pipeline, that can be used to acquire high-fidelity renderable hu-man avatars from single or multiple images. The model consists of a multi-modal style-based generator, that cap-tures facial appearance in terms of diffuse and specular re-flectance, and a PCA-based shape model. We employ a fast differentiable rendering process that can be used in an op-timization pipeline, while also achieving photorealistic fa-cial shading. Our optimization process accurately captures both the facial reflectance and shape in high-detail, by ex-ploiting the expressivity of the style-based latent represen-tation and of our shape model. FitMe achieves state-of-the-art reflectance acquisition and identity preservation on sin-gle “in-the-wild” facial images, while it produces impres-sive scan-like results, when given multiple unconstrained facial images pertaining to the same identity. In contrast with recent implicit avatar reconstructions, FitMe requires only one minute and produces relightable mesh and texture-based avatars, that can be used by end-user applications.
Despite the tremendous steps forward witnessed in the last decade, 3D facial reconstruction from a single uncon-strained image remains an important research problem with an active presence in the computer vision community. Its applications are now wide-ranging, including but not lim-ited to human digitization for virtual and augmented real-ity applications, social media and gaming, synthetic dataset creation, and health applications. However, recent works come short of accurately reconstructing the identity of dif-ferent subjects and usually fail to produce assets that can be used for photorealistic rendering. This can be attributed to the lack of diverse and big datasets of scanned human geom-etry and reflectance, the limited and ambiguous information available on a single facial image, and the limitations of the current statistical and machine learning methods. 3D Morhpable Models (3DMM) [18] have been a stan-dard method of facial shape and appearance acquisition from a single “in-the-wild” image. The seminal 3DMM work in [7] used Principal Component Analysis (PCA), to model facial shape and appearance with variable identity and expression, learned from about 200 subjects. Since
Figure 2. FitMe method overview. For a target image I0, we optimize the latent vector W of the generator G, and the shape identity ps, expression pe, camera pc and illumination pl parameters, by combining 3DMM fitting and GAN inversion methods, through accurate differentiable diffuse UD and specular US rendering R. Then, from the optimized Wp, we tune the generator G weights, through the same rendering process. The reconstructed shape S and facial reflectance (diffuse albedo AD, specular albedo AS and normals NS), achieve great identity similarity and can be directly used in typical renderers, to achieve photorealistic rendering, as shown on the right-hand side. then, larger models have been introduced, .i.e. the LSFM
[10], Basel Face Model [52] and Facescape [71], with thou-sands of subjects. Moreover, recent works have introduced 3DMMs of complete human heads [43, 54, 55] or other fa-cial parts such as ears [54] and tongue [53]. Finally, recent works have introduced extensions ranging from non-linear models [49, 66, 67] to directly regressing 3DMM parame-ters [61,68]. However, such models cannot produce textures capable of photorealistic rendering.
During the last decade we have seen considerable im-provements in deep generative models. Generative Adver-sarial Networks (GANs) [30], and specifically progressive
GAN architectures [34] have achieved tremendous results in learning distributions of high-resolution 2D images of human faces. Recently, style-based progressive generative networks [35–38] are able to learn meaningful latent spaces, that can be traversed in order to reconstruct and manipulate different attributes of the generated samples. Some methods have also been shown effective in learning a 2D representa-tion of 3D facial attributes, such as UV maps [21,23,24,45]. 3D facial meshes generated by 3DMMs can be utilized in rendering functions, in order to create 2D facial images.
Differentiating the rendering process is also required in or-der to perform iterative optimization. Recent advances in differentiable rasterization [44], photorealitic facial shad-ing [41] and rendering libraries [20, 25, 56], enable the pho-torealistic differentiable rendering of such assets. Unfor-tunately, 3DMM [10, 23, 45] works rely on the lambertian shading model which comes short of capturing the com-plexity of facial reflectance. The issue being, photorealistic facial rendering requires various facial reflectance param-eters instead of a single RGB texture [41]. Such datasets are scarce, small and difficult to capture [27,46,57], despite recent attempts to simplify such setups [39].
Several recent approaches have achieved either high-fidelity facial reconstructions [6,23,45] or relightable facial reflectance reconstructions [16, 17, 19, 40, 41, 65], including infra-red [47], however, high-fidelity and relightable recon-struction still remains elusive. Moreover, powerful mod-els have been shown to capture facial appearance with deep models [22, 42], but they fail to show single or multi image reconstructions. A recent alternative paradigm uses implicit representations to capture avatar appearance and geometry
[11, 69], the rendering of which depends on a learned neu-ral rendering. Despite their impressive results, such implicit representations cannot be used by common renderers and are not usually relightable. Finally, the recently introduced
Albedo Morphable Model (AlbedoMM) [65] captures fa-cial reflectance and shape with a linear PCA model, but per-vertex color and normal reconstruction is too low-resolution for photorealistic rendering. AvatarMe++ [40, 41] recon-structs high-resolution facial reflectance texture maps from a single “in-the-wild” image, however, its 3-step process (reconstruction, upsampling, reflectance), cannot be opti-mized directly with the input image.
In this work, we introduce FitMe, a fully renderable 3DMM with high-resolution facial reflectance texture maps, which can be fit on unconstrained facial images using ac-curate differentiable renderings. FitMe achieves identity similarity and high-detailed, fully renderable reconstruc-tions, which are directly usable by off-the-shelf rendering applications. The texture model is designed as a multi-modal style-based progressive generator, which concur-rently generates the facial diffuse-albedo, specular-albedo and surface-normals. A meticulously designed branched discriminator enables smooth training with modalities of different statistics. To train the model we create a capture-quality facial reflectance dataset of 5k subjects, by fine-tuning AvatarMe++ on the MimicMe [50] public dataset, which we also augment in order to balance skin-tone rep-resentation. For the shape, we use interchangeably a face and head PCA model [54], both trained on large-scale ge-ometry datasets. We design a single or multi-image fitting method, based on style-based generator projection [35] and 3DMM fitting. To perform efficient iterative fitting (in un-der 1 minute), the rendering function needs to be differen-tiable and fast, which makes models such as path tracing un-usable. Prior works in the field [10,12,23] use simpler shad-ing models (e.g. Lambertian), or much slower optimiza-tion [16]. We add a more photorealistic shading than prior work, with plausible diffuse and specular rendering, which can acquire shape and reflectance capable of photorealistic rendering in standard rendering engines (Fig. 1). The flexi-bility of the generator’s extended latent space and the pho-torealistic fitting, enables FitMe to reconstruct high-fidelity facial reflectance and achieve impressive identity similar-ity, while accurately capturing details in diffuse, specular albedo and normals. Overall, in this work we present:
• the first 3DMM capable of generating high-resolution facial reflectance and shape, with an increasing level of detail, that can be photorealistically rendered,
• the first branched multi-modal style-based progressive generator of high-resolution 3D facial assets (diffuse albedo, specular albedo and normals), and a suitable multi-modal branched discriminator,
• a method to acquire and augment a vast facial re-flectance dataset of, using assets from a public dataset,
• a multi-modal generator projection, optimized with diffuse and specular differentiable rendering. 2.