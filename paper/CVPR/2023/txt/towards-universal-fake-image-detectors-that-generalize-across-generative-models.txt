Abstract
With generative models proliferating at a rapid rate, there is a growing need for general purpose fake image
In this work, we first show that the existing detectors. paradigm, which consists of training a deep network for real-vs-fake classification, fails to detect fake images from newer breeds of generative models when trained to detect
GAN fake images. Upon analysis, we find that the result-ing classifier is asymmetrically tuned to detect patterns that make an image fake. The real class becomes a ‘sink’ class holding anything that is not fake, including generated im-ages from models not accessible during training. Build-ing upon this discovery, we propose to perform real-vs-fake classification without learning; i.e., using a feature space not explicitly trained to distinguish real from fake images.
We use nearest neighbor and linear probing as instantia-tions of this idea. When given access to the feature space of a large pretrained vision-language model, the very simple baseline of nearest neighbor classification has surprisingly good generalization ability in detecting fake images from a wide variety of generative models; e.g., it improves upon the
SoTA [50] by +15.07 mAP and +25.90% acc when tested on unseen diffusion and autoregressive models. Our code, models, and data can be found at https://github. com/Yuheng-Li/UniversalFakeDetect 1.

Introduction
The digital world finds itself being flooded with many kinds of fake images these days. Some could be natural images that are doctored using tools like Adobe Photoshop
[1, 49], while others could have been generated through a machine learning algorithm. With the rise and maturity of deep generative models [22,29,42], fake images of the latter kind have caught our attention. They have raised excitement because of the quality of images one can generate with ease.
They have, however, also raised concerns about their use for malicious purposes [4]. To make matters worse, there
*Equal contribution
Figure 1. Using images from just one generative model, can we detect images from a different type of generative model as fake? is no longer a single source of fake images that needs to be dealt with: for example, synthesized images could take the form of realistic human faces generated using generative adversarial networks [29], or they could take the form of complex scenes generated using diffusion models [42, 45].
One can be almost certain that there will be more modes of fake images coming in the future. With such a diversity, our goal in this work is to develop a general purpose fake detection method which can detect whether any arbitrary image is fake, given access to only one kind of generative model during training; see Fig. 1.
A common paradigm has been to frame fake image de-tection as a learning based problem [10, 50], in which a training set of fake and real images are assumed to be avail-able. A deep network is then trained to perform real vs fake binary classification. During test time, the model is used to detect whether a test image is real or fake. Impressively, this strategy results in an excellent generalization ability of the model to detect fake images from different algorithms within the same generative model family [50]; e.g., a clas-sifier trained using real/fake images from ProGAN [28] can accurately detect fake images from StyleGAN [29] (both being GAN variants). However, to the best of our knowl-edge, prior work has not thoroughly explored generalizabil-ity across different families of generative models, especially to ones not seen during training; e.g., will the GAN fake classifier be able to detect fake images from diffusion mod-els as well? Our analysis in this work shows that existing methods do not attain that level of generalization ability.
Specifically, we find that these models work (or fail to work) in a rather interesting manner. Whenever an image contains the (low-level) fingerprints [25, 50, 52, 53] particu-lar to the generative model used for training (e.g., ProGAN), the image gets classified as fake. Anything else gets classi-fied as real. There are two implications: (i) even if diffu-sion models have a fingerprint of their own, as long as it is not very similar to GAN’s fingerprint, their fake images get classified as real; (ii) the classifier doesn’t seem to look for features of the real distribution when classifying an image as real; instead, the real class becomes a ‘sink class’ which hosts anything that is not GAN’s version of fake image. In other words, the decision boundary for such a classifier will be closely bound to the particular fake domain.
We argue that the reason that the classifier’s decision boundary is unevenly bound to the fake image class is be-cause it is easy for the classifier to latch onto the low-level image artifacts that differentiate fake images from real im-ages. Intuitively, it would be easier to learn to spot the fake pattern, rather than to learn all the ways in which an image could be real. To rectify this undesirable behavior, we pro-pose to perform real-vs-fake image classification using fea-tures that are not trained to separate fake from real images.
As an instantiation of this idea, we perform classification using the fixed feature space of a CLIP-ViT [24, 41] model pre-trained on internet-scale image-text pairs. We explore both nearest neighbor classification as well as linear prob-ing on those features.
We empirically show that our approach can achieve significantly better generalization ability in detecting fake images. For example, when training on real/fake images associated with ProGAN [28] and evaluat-ing on unseen diffusion and autoregressive model (LDM+Glide+Guided+DALL-E) images, we obtain im-provements over the SoTA [50] by (i) +15.05mAP and
+25.90% acc with nearest neighbor and (ii) +19.49mAP and +23.39% acc with linear probing. We also study the in-gredients that make a feature space effective for fake image detection. For example, can we use any image encoder’s feature space? Does it matter what domain of fake/real im-ages we have access to? Our key takeaways are that while our approach is robust to the breed of generative model one uses to create the feature bank (e.g., GAN data can be used to detect diffusion models’ images and vice versa), one needs the image encoder to be trained on internet-scale data (e.g., ImageNet [21] does not work).
In sum, our main contributions are: (1) We analyze the limitations of existing deep learning based methods in de-tecting fake images from unseen breeds of generative mod-(2) After empirically demonstrating prior methods’ els. ineffectiveness, we present our theory of what could be wrong with the existing paradigm. (3) We use that analy-sis to present two very simple baselines for real/fake image detection: nearest neighbor and linear classification. Our approach results in state-of-the-art generalization perfor-mance, which even the oracle version of the baseline (tun-ing its confidence threshold on the test set) fails to reach. (4) We thoroughly study the key ingredients of our method which are needed for good generalizability. 2.