Abstract
To effectively interrogate UAV-based images for detect-ing objects of interest, such as humans, it is essential to acquire large-scale UAV-based datasets that include human instances with various poses captured from widely varying viewing angles. As a viable alternative to laborious and costly data curation, we introduce Progressive Transfor-mation Learning (PTL), which gradually augments a train-ing dataset by adding transformed virtual images with en-hanced realism. Generally, a virtual2real transformation generator in the conditional GAN framework suffers from quality degradation when a large domain gap exists be-tween real and virtual images. To deal with the domain gap,
PTL takes a novel approach that progressively iterates the following three steps: 1) select a subset from a pool of vir-tual images according to the domain gap, 2) transform the selected virtual images to enhance realism, and 3) add the transformed virtual images to the training set while remov-ing them from the pool. In PTL, accurately quantifying the domain gap is critical. To do that, we theoretically demon-strate that the feature representation space of a given object detector can be modeled as a multivariate Gaussian dis-tribution from which the Mahalanobis distance between a virtual object and the Gaussian distribution of each object category in the representation space can be readily com-puted. Experiments show that PTL results in a substantial performance increase over the baseline, especially in the small data and the cross-domain regime. 1.

Introduction
Training an object detector usually requires a large-scale training image set so that the detector can acquire the abil-ity to detect objects’ diverse appearances. This desire for a large-scale training set is bound to be greater for object cat-egories with more diverse appearances, such as the human category whose appearances vary greatly depending on its pose or viewing angles. Moreover, a person’s appearance
Figure 1. Overview of Progressive Transformation Learning. becomes more varied in images captured by an unmanned aerial vehicle (UAV), leading to a wide variety of camera viewing angles compared to ground-based cameras, mak-ing the desire for a large-scale training set even greater. In this paper, we aim to satisfy this desire, especially when the availability of UAV-based images to train a human detector is scarce, where this desire is more pressing.
As an intuitive way to expand the training set, one might consider synthesizing virtual images to imitate real-world images by controlling the optical and physical conditions in a virtual environment. Virtual images are particularly use-ful for UAV-based object detection since abundant object instances can be rendered with varying UAV locations and camera viewing angles along with ground-truth informa-tion (e.g., bounding boxes, segmentation masks) that comes free of charge. Therefore, a large-scale virtual image set covering diverse appearances of human subjects that are rarely shown in existing UAV-based object detection bench-marks [1, 3, 53] can be conveniently acquired by controlling entities and parameters in a virtual environment, such as poses, camera viewing angles, and illumination conditions.
To make virtual images usable for training real-world ob-ject detection models, recent works [19, 29, 38, 39] trans-form virtual images to look realistic. They commonly use the virtual2real generator [36] trained with the conditional
GAN framework to transform images in the source domain to have the visual properties of images in the target do-main. Here, virtual and real images are treated as the source and target domains, respectively. However, the large dis-crepancy in visual appearance between the two domains, referred to as the “domain gap”, result in the degraded transformation quality of the generator. In fact, the afore-mentioned works using virtual images validate their meth-ods where the domain gap is not large (e.g., digit detec-tion [19]) or when additional information is available (e.g., animal pose estimation with additional keypoint informa-tion [29, 38, 39]). In our case, real and virtual humans in
UAV-based images inevitably have a large domain gap due to the wide variety of human appearances.
To address the large domain gap, one critical question inherent in our task is how to measure accurately the do-main gap. Consequently, we estimate the domain gap in the representation space of a human detector trained on the real images. The representation space of the detector is learned such that test samples, which have significantly different properties than training samples from the perspective of the
In detector, are located away from the training samples. this paper, we show that the feature distribution of object entities belonging to a certain category, such as the human category, in the representation space can be modeled with a multivariate Gaussian distribution if the following two con-ditions are met: i) the detector uses the sigmoid function to normalize the final output and ii) the representation space is constructed using the output of the penultimate layer of the detector. This idea is inspired by [28], which shows that softmax-based classifiers can be modeled as multivari-ate Gaussian distributions. In this paper, we show that the proposition is also applicable to sigmoid-based classifiers, which are widely used by object detectors. Based on this modeling, when the two aforementioned conditions are met, the human category in the representation space can be rep-resented by two parameters (i.e., mean and covariance) of a multivariate Gaussian distribution that can be computed on the training images. With the empirically calculated mean and covariance, the domain gap from a single virtual hu-man image to real human images (i.e., the training set) can be measured using the Mahalanobis distance [35].
To add virtual images to the training set to include more diverse appearances of objects while preventing the trans-formation quality degradation caused by large domain gaps, we introduce Progressive Transformation Learning (PTL) (Figure 1). PTL progressively expands the training set by adding virtual images through iterating the three steps: 1) transformation candidate selection, 2) virtual2real transfor-mation, and 3) set update. When selecting transformation candidates from a virtual image pool, we use weighted ran-dom sampling, which gives higher weights to images with smaller domain gaps. The weight takes an exponential term with one hyperparameter controlling the ratio between im-ages with smaller domain gaps and images with more di-verse appearances. Then, the virtual2real transformation generator is trained via the conditional GAN, taking the se-lected transformation candidates as the “source” and the im-ages in the training set as the “target”. After transforming the transformation candidates by applying the virtual2real transformation generator, the training set is expanded with the transformed candidates while the original candidates are excluded from the pool of virtual images.
The main contribution of this paper is that we have val-idated the utility of virtual images in augmenting training data via PTL coupled with carefully designed comprehen-sive experiments. We first use the task of low-shot learn-ing, where adequately expanding datasets has notable ef-fects. Specifically, PTL provides better accuracy on three
UAV-view human detection benchmarks than other previ-ous works that leverage virtual images in training, as well as methods that only use real images. Then, we validate
PTL on the cross-domain detection task where training and test sets are from distinct domains and virtual images can serve as a bridge between these two sets. The experimen-tal results indicate that a high-performance human detection model can be effectively learned via PTL, even with the sig-nificant lack of real-world training data. 2.