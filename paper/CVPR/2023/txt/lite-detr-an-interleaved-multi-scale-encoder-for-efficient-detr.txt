Abstract
Recent DEtection TRansformer-based (DETR) models have obtained remarkable performance. Its success cannot be achieved without the re-introduction of multi-scale feature fusion in the encoder. However, the excessively increased to-kens in multi-scale features, especially for about 75% of low-level features, are quite computationally inefficient, which hinders real applications of DETR models. In this paper, we present Lite DETR, a simple yet efficient end-to-end object detection framework that can effectively reduce the GFLOPs of the detection head by 60% while keeping 99% of the origi-nal performance. Specifically, we design an efficient encoder block to update high-level features (corresponding to small-resolution feature maps) and low-level features (correspond-ing to large-resolution feature maps) in an interleaved way.
In addition, to better fuse cross-scale features, we develop a key-aware deformable attention to predict more reliable attention weights. Comprehensive experiments validate the effectiveness and efficiency of the proposed Lite DETR, and the efficient encoder strategy can generalize well across existing DETR-based models. The code will be available in https://github.com/IDEA-Research/Lite-DETR. 1.

Introduction
Object detection aims to detect objects of interest in im-ages by localizing their bounding boxes and predicting the corresponding classification scores. In the past decade, re-markable progress has been made by many classical de-tection models [23, 24] based on convolutional networks.
*This work was done when Feng Li was an intern at IDEA.
†Corresponding author.
Figure 1. Average precision (Y axis) versus GFLOPs (X axis) for different detection models on COCO without extra training data.
All models except EfficientDet [29] and YOLO series [12, 30] use
ResNet-50 and Swin-Tiny as backbones. Specifically, two markers on the same line use ResNet-50 and Swin-Tiny, respectively. In-dividual markers only use ResNet-50. Each dashed line connects algorithm variants before and after adding our algorithm. The size of the listed models vary from 32M to 82M.
Recently, DEtection TRansformer [1] (DETR) introduces
Transformers into object detection, and DETR-like models have achieved promising performance on many fundamental vision tasks, such as object detection [13, 36, 37], instance segmentation [5, 6, 14], and pose estimation [26, 28].
Conceptually, DETR [1] is composed of three parts: a backbone, a Transformer encoder, and a Transformer de-coder. Many research works have been improving the back-bone and decoder parts. For example, the backbone in DETR is normally inherited and can largely benefit from a pre-trained classification model [10, 20]. The decoder part in
DETR is the major research focus, with many research works trying to introduce proper structure to DETR query and im-prove its training efficiency [11, 13, 18, 21, 36, 37]. By con-trast, much less work has been done to improve the encoder part. The encoder in vanilla DETR includes six Transformer encoder layers, stacked on top of a backbone to improve its feature representation. Compared with classical detection models, it lacks multi-scale features, which are of vital im-portance for object detection, especially for detecting small objects [9, 16, 19, 22, 29]. Simply applying Transformer en-coder layers on multi-scale features is not practical due to the prohibitive computational cost that is quadratic to the number of feature tokens. For example, DETR uses the C5 feature map, which is 1/32 of the input image resolution, to apply the Transformer encoder. If a C3 feature (1/8 scale) is included in the multi-scale features, the number of tokens from this scale alone will be 16 times of the tokens from the
C5 feature map. The computational cost of self-attention in
Transformer will be 256 times high.
To address this problem, Deformable DETR [37] devel-ops a deformable attention algorithm to reduce the self-attention complexity from quadratic to linear by compar-ing each query token with only a fixed number of sampling points. Based on this efficient self-attention computation,
Deformable DETR introduces multi-scale features to DETR, and the deformable encoder has been widely adopted in subsequent DETR-like models [11, 13, 18, 36].
However, due to a large number of query tokens intro-duced from multi-scale features, the deformable encoder still suffers from a high computational cost. To reveal this problem, we conduct some analytic experiments as shown in Table 1 and 2 using a DETR-based model DINO [36] to analyze the performance bottleneck of multi-scale features.
Some interesting results can be observed. First, the low-level (high-resolution map) features account for more than 75% of all tokens. Second, direct dropping some low-level features (DINO-3scale) mainly affects the detection performance for small objects (AP_S) by a 10% drop but has little impact on large objects (AP_L).
Inspired by the above observations, we are keen to address a question: can we use fewer feature scales but maintain important local details? Taking advantage of the structured multi-scale features, we present an efficient DETR frame-work, named Lite DETR . Specifically, we design a simple yet effective encoder block including several deformable self-attention layers, which can be plug-and-play in any multi-scale DETR-base models to reduce 62% ∼ 78% en-coder GFLOPs and maintain competitive performance. The encoder block splits the multi-scale features into high-level features (e.g., C6, C5, C4) and low-level features (e.g., C3).
High-level and low-level features will be updated in an in-terleaved way to improve the multi-scale feature pyramid.
That is, in the first few layers, we let the high-level features query all feature maps and improve their representations, but keep low-level tokens intact. Such a strategy can effectively reduce the number of query tokens to 5% ∼ 25% of the original tokens and save a great amount of computational cost. At the end of the encoder block, we let low-level to-kens query all feature maps to update their representations, thus maintaining multi-scale features. In this interleaved way, we update high-level and low-level features in different frequencies for efficient computation.
Moreover, to enhance the lagged low-level feature up-date, we propose a key-aware deformable attention (KDA) approach to replacing all attention layers. When performing deformable attention, for each query, it samples both keys and values from the same sampling locations in a feature map. Then, it can compute more reliable attention weights by comparing the query with the sampled keys. Such an approach can also be regarded as an extended deformable attention or a sparse version of dense attention. We have found KDA very effective in bringing the performance back with our proposed efficient encoder block.
To summarize, our contributions are as follows.
• We propose an efficient encoder block to update high-level and low-level features in an interleaved way, which can significantly reduce the feature tokens for efficient detection. This encoder can be easily plugged into existing DETR-based models.
• To enhance the lagged feature update, we introduce a key-aware deformable attention for more reliable atten-tion weights prediction.
• Comprehensive experiments show that Lite DETR can reduce the detection head GFLOPs by 60% and main-tain 99% detection performance. Specifically, our Lite-DINO-SwinT achieves 53.9 AP with 159 GFLOPs. 2.