Abstract
Unsupervised domain adaptive object detection (UDA-OD) aims to learn a detector by generalizing knowledge from a labeled source domain to an unlabeled target do-main. Though the existing graph-based methods for UDA-OD perform well in some cases, they cannot learn a proper node set for the graph. In addition, these methods build the graph solely based on the visual features and do not con-sider the linguistic knowledge carried by the semantic pro-totypes, e.g., dataset labels. To overcome these problems, we propose a cross-modality graph reasoning adaptation (CIGAR) method to take advantage of both visual and lin-guistic knowledge. Specifically, our method performs cross-modality graph reasoning between the linguistic modality graph and visual modality graphs to enhance their repre-sentations. We also propose a discriminative feature selec-tor to find the most discriminative features and take them as the nodes of the visual graph for both efficiency and effec-tiveness. In addition, we employ the linguistic graph match-ing loss to regulate the update of linguistic graphs and maintain their semantic representation during the training process. Comprehensive experiments validate the effective-ness of our proposed CIGAR. 1.

Introduction
Object detection is a fundamental technique in computer vision tasks, and it has been widely explored in many ap-plications, e.g., self-driving and public safety. A variety of works [31,38,39,56,57] have achieved improvements in de-tection performance due to the development of deep neural networks. However, a detector significantly degrades if we deploy it in a novel domain due to the problem of domain shift. The domain shift can be induced by many factors,
* Corresponding authors.
Figure 1. Illustration of the proposed Cross-modality Graph Rea-soning Adaptation (CIGAR) framework. such as the variation of the capture condition from sunny to foggy weather, from virtual to real-world, and from one camera to another.
To deal with the problem of domain shift [9], researchers have proposed many unsupervised domain adaptive object detection (UDA-OD) methods to bridge the domain gap be-tween the source and target domains. Among them, the self-training based methods [5, 10, 26, 35] have shown ex-cellent performances. However, they cannot be easily ex-tended to real applications because they are computationally expensive and inefficient. Feature alignment-based meth-ods [3,4,6,22,25] have also been extensively studied. These methods are structurally elegant and can be categorized into three groups, including global-level alignment, instance-level alignment, and category-level alignment. The global-level alignment methods [6,41] align the whole shallow fea-ture maps produced by a backbone network. The instance-level alignment methods [4, 17] extract the feature maps for all the instances and learn to achieve cross-domain align-ment in the deep feature space. The category-level align-ment methods [58, 61] normally first use the detectors or an additional classification model for pseudo label assignment to the target samples, then align the category-wise instance features of two domains based on the ground-truth source labels and the pseudo target labels.
Numerous works [3, 24, 25] achieve feature alignment using graph-based approaches. These methods take dense features as nodes to construct the graphs and investigate the relationship between nodes. They use the knowledge car-ried by the graphs to enhance the features for the purpose of cross-domain alignment and thus improve the detection performance. Though the graph-based methods have sig-nificantly improved the feature alignment, they still have two inherent limitations. First, the existing methods do not construct the graph with the proper node set. Many graph-based methods randomly or uniformly [25] select features to construct graphs, resulting in the missing of some dis-criminative features. Some other methods [3] are not robust against noise and are computationally expensive, as they take all features as the graph nodes. Secondly, they only explore the visual knowledge extracted from the images. In this way, they ignore the critical knowledge of the linguis-tic modality, which carries the semantic prototypes of the domains, e.g., linguistic dataset labels. Linguistic modality knowledge is very effective in regulating visual knowledge, and its absence severely reduces the representative ability of the resulting features. Some existing works [18, 63] have focused on using semantic category information to enhance performances in vision tasks. Singh et al. [46] also used a language model for the semi-supervised domain adaptive task and achieved improved performance.
To overcome the two limitations mentioned above, we propose a Cross-modalIty GrAph Reasoning Adapta-tion (CIGAR) framework for category-level alignment via graph-based learning, as shown in Fig. 1. To enhance ef-ficiency and improve the robustness against noise, we pro-pose a Discriminative Feature Selector (DFS) for finding discriminative features and constructing the visual graph us-ing only the discriminative features. In particular, we first conduct a procedure of singular value decomposition (SVD) and drop the small singular values, then evaluate the infor-mation richness of each feature via the summation of the ab-solute value of the elements. We can improve the represen-tation ability of visual graphs by only taking these discrim-inative features as the nodes. Our method is more computa-tionally efficient than previous methods, which use all im-age features to construct graphs. Our CIGAR also explores the graph in the linguistic modality and performs cross-modality graph reasoning between the linguistic modality and the visual modality. The linguistic modality knowledge can guide the mapping of visual modality knowledge from different domains to the same feature space. Our CIGAR can build a graph not only for the tasks with multiple cat-egories and capture the relationship between different cate-gories but also for the tasks with a single category and cap-ture the relationship between different components of a sin-gle category. We maintain the semantic representation of the knowledge in linguistic modality and use it to guide the training procedure.
We summarize our contributions as follows:
• We propose a Cross-modality Graph Reasoning Adap-tation (CIGAR) method for the domain adaptive object detection problem. To the best of our knowledge, this is the first work to tackle the UDA-OD task by graph reasoning across different modalities.
• We propose a Discriminative Feature Selector for find-ing discriminative image features and efficiently con-structing the representative visual graph.
• Extensive experiments are conducted on four adapta-tion tasks, and our CIGAR achieves state-of-the-art performance, outperforming existing works by a large margin. 2.