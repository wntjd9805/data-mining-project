Abstract
Current state-of-the-art image-text matching methods implicitly align the visual-semantic fragments, like regions in images and words in sentences, and adopt cross-attention mechanism to discover ﬁne-grained cross-modal semantic correspondence. However, the cross-attention mechanism may bring redundant or irrelevant region-word alignments, degenerating retrieval accuracy and limiting efﬁciency. Al-though many researchers have made progress in mining meaningful alignments and thus improving accuracy, the problem of poor efﬁciency remains unresolved. In this work, we propose to learn ﬁne-grained image-text matching from the perspective of information coding. Speciﬁcally, we sug-gest a coding framework to explain the fragments aligning process, which provides a novel view to reexamine the cross-attention mechanism and analyze the problem of redundant alignments. Based on this framework, a Cross-modal Hard
Aligning Network (CHAN) is designed, which comprehen-sively exploits the most relevant region-word pairs and elim-inates all other alignments. Extensive experiments con-ducted on two public datasets, MS-COCO and Flickr30K, verify that the relevance of the most associated word-region pairs is discriminative enough as an indicator of the image-text similarity, with superior accuracy and efﬁciency over the state-of-the-art approaches on the bidirectional image and text retrieval tasks. Our code will be available at https://github.com/ppanzx/CHAN.
Figure 1. Illustration of different semantic corresponding methods: (a) Global Embedding methods, (b) Fragment Embedding meth-ods, (c) existing Fragment Aligning methods, and (d) our CHAN method. Here ω in (c) and (d) is the attention weight/assignment between the word "pajamas" and the image region, where the re-gion with the maximum attention weight is outlined in yellow below. Compared to existing Fragment Aligning methods which bring redundant alignments, we improve them by attending to the most relevant region while neglecting all of the misalignments. 1.

Introduction
With the rapid development of information technology, multi-modal data, like texts, audio, images, and video, has become ubiquitous in our daily life.
It is of great value to study multi-modal learning to give computers the ability to process and relate information from multiple modalities.
Among the tasks of multi-modal learning, image-text re-trieval is the most fundamental one, which paves the way for more general cross-modal retrieval, namely, implementing a retrieval task across different modalities, such as video-*Corresponding author text and audio-text. Image-text retrieval has attracted broad attention in recent years [12,21,23]; yet, the key challenges, i.e., bridging the inter-modality gap and achieving the se-mantic correspondence across modalities, are far from be-ing resolved. A good alignment directly links to correctly measuring the similarity between images and texts.
Early works usually adopt the intuitive idea of global embedding to ﬁnd the semantic correspondence between a whole picture and the complete sentence [12]. By project-ing the overall image and text into a common embedding space, the similarity between heterogeneous samples is mea-sured for the subsequent matching of the two modalities, as
shown in Figure 1(a). However, such a global embedding method often induces background noise, which impedes the correct image-text matching. Recent works have focused on essential fragments [4, 16, 22], such as salient objects in im-ages and keywords in texts, aiming to reduce contributions of uninterested regions as well as irrelevant conjunctions.
By introducing the self-attention mechanism, the represen-tation of holistic inputs is replaced by a weighted sum of the local fragments, thereby easing the matching obstacles caused by the noise parts, as shown in Figure 1(b). However, these fragment embedding methods do not explicitly imple-ment ﬁne-grained aligning, as they only focus on the com-plex aggregation of fragments in a single modality without taking account of correctly learning granular cross-modal semantic consistency.
Based on the consensus that overall image-text similar-ity is a complex aggregation of the local similarities cal-culated between the cross-modal fragments [19], the frag-ments aligning method emphasizes the aggregation of the local similarities rather than the aggregation of the local rep-resentations. SCAN [21] and its variants [10, 25, 43, 46] are the representatives of this school of thought, which align im-age regions and sentence words by locally associating visual semantics, and integrate the semantic similarities between relevant region-word pairs to measure the overall image-text relevance. Speciﬁcally, with the core idea of the cross-attention mechanism, they attend to the fragments related to each query fragment from another modality, thus mak-ing the semantically consistent granular pairs signiﬁcantly contribute to the ﬁnal image-text similarity, and at the same time eliminating or weakening the inﬂuence of inconsistent pairs.
However, there are two problems associated with the previous fragments aligning methods: (1) redundant align-ments are detrimental to retrieval accuracy. Selecting se-mantically consistent region-word alignments and rejecting inconsistent alignments is the key to realizing ﬁne-grained image-text matching. However, though semantically con-sistent alignments can be discovered by the cross-attention mechanism, it is far from enough to achieve an accurate re-trieval because these meaningful alignments will be more or less disturbed by other attended fragments irrelevant to the shared semantics. As illustrated in Figure 1(c), given a text fragment "pajamas," current cross-attention-based methods not only attend to the most matched image region but also refer to other regions not exactly relevant, like "cat" and
"towel," which will incorrectly estimate the afﬁnity between
"pajamas" and irrelevant regions while training. As a result, semantically inconsistent region-word pairs will eventually overwhelm those matched ones, thus compromising the ef-fect from the most matched pairs and degenerating the ﬁnal performance; (2) caching cross-attention weights is with a massive cost of memory and time. When the cross-attention mechanism is applied to fragments aligning, it is inevitable to calculate the afﬁnities between all the cross-modal frag-ments, because a query needs to be reconstructed with the attention weights derived from the afﬁnities, which incurs huge memory consumption to store the attention weights.
In fact, due to the limited memory, the matching process between each query text/image and the whole image/text set requires a large number of iterations, resulting in a long retrieval time and thus compromising the practical applica-tions of the fragments aligning method.
Inspired by the coding idea widely adopted in content-based image retrieval tasks [14, 17, 32], we propose a cod-ing framework to explain the aligning process and rethink cross-attention-based methods from the view of soft assign-ment coding. Speciﬁcally, we regard each word in a sen-tence as a query and represent the salient regions in an im-age as a codebook. Therefore, the aligning of fragments is expressed as an adjustment of the measure of the rela-tionship between query words and visual codewords. The overall image-text similarity is the aggregation of similari-ties between all queries and all codewords. In this view, the deﬁnition of attention weights in a cross-attention mecha-nism is almost the same as assignments in soft assignment coding [14] scheme, and thus the cross-attention mecha-nism can be explained as a kind of soft assignment cod-ing method. Based on the assumption that there must ex-ist a sub-region in an image which can best describe every given word in the semantically consistent sentence [19], we deem it unnecessary to consider all or even a selected part of codewords since most of them do not bring beneﬁt for bet-ter describing the query words but lowering the efﬁciency.
This insight inspires switching the methodology from soft assignment coding to hard assignment coding [27], with attention to the most relevant word-region/query-codeword pair which is a more accurate indication of semantic consis-tency between a word and an image, as shown in Figure 1(d).
We further propose a novel Cross-modal Hard Aligning Net-work (CHAN) for ﬁne-grained image-text matching. Our scheme not only discards redundant alignments and better discovers the granular semantic correspondence, but also re-lieves the costly dense cross-attention matching, thus signif-icantly improving cross-attention baselines both in accuracy and efﬁciency. Our main contributions can be summarized as follows:
• We propose a coding framework to explain fragments aligning for image-text retrieval and subsequently elab-orate on the aligning process of cross-attention mecha-nism. This elaboration allows us to pinpoint the deﬁcien-cies, and propose an improved hard assignment coding scheme.
• With the hard assignment coding scheme, we propose a novel Cross-modal Hard Aligning Network (CHAN), which can accurately discover the shared semantics of im-age and text by mining the informative region-word pairs and rejecting the redundant or irrelevant alignments.
• Extensive experiments on two benchmarks, i.e.,
Flickr30K [45] and MS-COCO [5], showing the su-periority of CHAN in both accuracy and efﬁciency compared with state-of-the-art methods. 2.