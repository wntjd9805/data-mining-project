Abstract
Batch Normalization (BN) and its variants has been ex-tensively studied for neural nets in various computer vision tasks, but relatively little work has been dedicated to studying the effect of BN in continual learning. To that end, we de-velop a new update patch for BN, particularly tailored for the exemplar-based class-incremental learning (CIL). The main issue of BN in CIL is the imbalance of training data between current and past tasks in a mini-batch, which makes the em-pirical mean and variance as well as the learnable affine transformation parameters of BN heavily biased toward the current task — contributing to the forgetting of past tasks.
While one of the recent BN variants has been developed for “online” CIL, in which the training is done with a single epoch, we show that their method does not necessarily bring gains for “offline” CIL, in which a model is trained with multiple epochs on the imbalanced training data. The main reason for the ineffectiveness of their method lies in not fully addressing the data imbalance issue, especially in comput-ing the gradients for learning the affine transformation pa-rameters of BN. Accordingly, our new hyperparameter-free variant, dubbed as Task-Balanced BN (TBBN), is proposed to more correctly resolve the imbalance issue by making a horizontally-concatenated task-balanced batch using both reshape and repeat operations during training. Based on our experiments on class incremental learning of CIFAR-100, ImageNet-100, and five dissimilar task datasets, we demonstrate that our TBBN, which works exactly the same as the vanilla BN in the inference time, is easily applicable to most existing exemplar-based offline CIL algorithms and consistently outperforms other BN variants. 1.

Introduction
In recent years, continual learning (CL) has been actively studied to efficiently learn a neural network on sequentially
*Corresponding author (E-mail: tsmoon@snu.ac.kr) arriving datasets while eliminating the process of re-training from scratch at each arrival of a new dataset [9]. However, since the model is typically trained on a dataset that is heav-ily skewed toward the current task at each step, the resulting neural network often suffers from suboptimal trade-off be-tween stability and plasticity [25] during CL. To overcome this issue, various studies have been focused on addressing the so-called catastrophic forgetting phenomenon [9, 28].
Among different CL settings, the class-incremental learn-ing (CIL) setting where the classifier needs to learn previ-ously unseen classes at each incremental step has recently drawn attention due to its practicality [1, 3, 6, 12, 15, 26, 39, 42, 44]. Most state-of-the-art CIL algorithms maintain a small exemplar memory to store a subset of previously used training data and combine it with the current task dataset to mitigate the forgetting of past knowledge. A key issue of exemplar-based CIL is that the model prediction becomes heavily biased towards more recently learned classes, due to the imbalance between the training data from the current task (that are abundantly available) and past tasks (with lim-ited access through exemplar memory). In response, recently proposed solutions to biased predictions include bias correc-tion [39], unified classifier [15], and separated softmax [1], which greatly improved the overall accuracy of CIL methods across all classes learned so far.
Despite such progress, relatively less focus has been made on backbone architectures under CIL on computer vision tasks. Specifically, the popular CNN-based models (e.g.,
ResNet [13]) are mainly used as feature extractors, and those models are equipped with Batch Normalization (BN) [17] by default. However, since BN is designed for single-task training on CNNs, applying BN directly to exemplar-based
CIL results in statistics biased toward the current task due to the imbalance between current and past tasks’ data in a mini-batch. Recently, [29] pointed out this issue in CIL, dubbed as cross-task normalization effect, and proposed a new normalization scheme called Continual Normalization
(CN), which applies Group Normalization (GN) [40] across the channel dimension before running BN across the batch dimension. Therefore, the difference in feature distributions among tasks is essentially removed by GN, and the following
BN computes task-balanced mean and variance statistics which is shown to outperform vanilla BN on online CIL settings.
In this paper, we argue that CN only partially resolves the bias issue in exemplar-based CIL. Specifically, we find that the gradients on the affine transformation parameters remain biased towards the current task when using CN. As a result, it leads to inconsistent performance gains in the offline CIL setting, which is considered more practical than online CIL.
To this end, we propose a simple yet novel hyperparameter-free normalization layer, dubbed as Task-Balanced Batch
Normalization (TBBN), which effectively resolves the bias issue. Our method employs adaptive reshape and repeat op-erations on the mini-batch feature map during training in order to compute task-balanced normalization statistics and gradients for learning the affine transformation parameters.
Our method does not require any hyperparameter as the size-inputs for reshape and repeat operations are determined adaptively. Furthermore, the application of TBBN during testing is identical to vanilla BN, requiring no change on the backbone architecture. Through extensive offline CIL exper-iments on CIFAR-100, ImageNet-100, and five dissimilar task datasets, we show that a simple replacement of BN lay-ers in the backbone CNN model with TBBN benefits most state-of-the-art exemplar-based CIL algorithms towards an additional boost in performance. Our analysis shows that the gain of TBBN is consistent across various backbone archi-tectures and datasets, suggesting its potential to become a correct choice for exemplar-based offline CIL algorithms. 2.