Abstract
We argue that there are many notions of ‘similarity’ and that models, like humans, should be able to adapt to these dynamically. This contrasts with most representation learn-ing methods, supervised or self-supervised, which learn a
ﬁxed embedding function and hence implicitly assume a sin-gle notion of similarity. For instance, models trained on Im-ageNet are biased towards object categories, while a user might prefer the model to focus on colors, textures or spe-ciﬁc elements in the scene. In this paper, we propose the
GeneCIS (‘genesis’) benchmark, which measures models’ ability to adapt to a range of similarity conditions. Ex-tending prior work, our benchmark is designed for zero-shot evaluation only, and hence considers an open-set of similarity conditions. We ﬁnd that baselines from powerful
CLIP models struggle on GeneCIS and that performance on the benchmark is only weakly correlated with ImageNet ac-curacy, suggesting that simply scaling existing methods is not fruitful. We further propose a simple, scalable solution based on automatically mining information from existing image-caption datasets. We ﬁnd our method offers a sub-stantial boost over the baselines on GeneCIS, and further improves zero-shot performance on related image retrieval benchmarks. In fact, though evaluated zero-shot, our model surpasses state-of-the-art supervised models on MIT-States.
We, the architects of the machine, must decide a-priori what constitutes its ‘world’; what things are to be taken as ‘similar’ or ‘equal’ — Karl Popper, 1963 1.

Introduction
Humans understand many notions of similarity and choose speciﬁc ones depending on the task at hand [21, 58].
Consider the task of ﬁnding ‘similar’ images illustrated in Figure 1. Which of the rightmost images should be con-sidered ‘most similar’ to the reference? Given different con-ditions, each image could be a valid answer. For instance, we may be interested in a speciﬁc object in the scene, focus-ing on either the ‘car’ or ‘bridge’. One could even indicate
*Work done during an internship at Meta AI Research.
With the same car
With the same bridge
With a black car
Figure 1. Given different conditions (shown as blue text), differ-ent images on the right can be considered most ‘similar’ to the reference on the left. We present a general way to train and evalu-ate models which can adapt to different notions of similarity. a ‘negative’ similarity condition, specifying a change in the image to identify the bottom image as most similar.
Learning such similarity functions is a central goal in discriminative deep learning [11–13, 34, 63, 68, 75]. Dis-criminative models, either supervised [30, 75] or self-supervised [9, 10], learn embedding functions such that
‘similar’ images are closer in feature space than ‘dissimilar’ images. However, since there are inﬁnitely many notions of image similarity, how do we allow our models to choose?
Almost all current approaches assume a single notion of similarity, either by explicitly training on a speciﬁc concept
[68,75] or through an implicit assumption in the underlying data distribution [9, 12]. Meanwhile, prior works tackling the conditional problem have focused on constrained do-mains such as fashion [69,73] or birds [46], with a restricted set of similarity conditions. This is because developing and evaluating models that can adapt to generic notions of sim-ilarity is extremely challenging. Speciﬁcally, curating data to train and evaluate such models is difﬁcult, as collecting annotations for all concepts of similarity is impossible.
In this work we study the problem of general conditional image similarity, training on an open-set of similarity con-ditions, and evaluating on diverse similarity notions in a
‘zero-shot’ manner. We ﬁrst design a benchmark compris-ing of four evaluation datasets for conditional image simi-larity, setting up conditional retrieval tasks. We deﬁne these tasks under a uniﬁed framework which spans practical use cases, and propose the benchmark as a sparse but broad coverage of the conditional similarity space. We propose these datasets for zero-shot evaluation only, and suggest that models which can perform well without ﬁne-tuning can
ﬂexibly adapt to general notions of similarity, as desired.
We name this benchmark GeneCIS (‘genesis’) for General
Conditional Image Similarity. On GeneCIS, we ﬁnd that baselines built from powerful CLIP backbones struggle and, moreover, that performance on it is only weakly correlated with the backbones’ ImageNet accuracy [17]. This is in contrast to popular vision tasks such as segmentation [39] and detection [45], underlining the benchmark’s utility.
We also propose a solution to training general condi-tional similarity models, based on parsing large-scale cap-tion datasets [64,66]. Rather than requiring exhaustive sim-ilarity annotations, we ﬁnd that we can automatically mine this information from already abundant image-caption data.
We show that training in this way offers substantial gains over the baselines, approaching (and in some cases sur-passing) carefully designed speciﬁc solutions for each of the GeneCIS tasks.
In addition, we demonstrate that our method scales with increasing amounts of caption data, sug-gesting promising directions for future work. Finally, on related benchmarks from the ‘Composed Image Retrieval’ (CIR) ﬁeld [44,74], we ﬁnd our method provides gains over zero-shot baselines. In fact, our model outperforms state-of-the-art on the MIT-States benchmark [28], despite being evaluated zero-shot and never seeing the training data.
Contributions. (i) We present a framework for considering conditional image similarity, an important but understudied problem; (ii) We propose the GeneCIS benchmark to test models’ abilities to dynamically adapt to different notions of similarity; (iii) We show that current vision-language models like CLIP struggle on GeneCIS, and that perfor-mance on it is only weakly correlated with ImageNet accu-racy; (iv) We design a scalable solution to the conditional similarity problem based on automatically parsing large-scale image-caption data; (v) We show our models provide substantial gains over zero-shot CLIP baselines; (vi) We validate our models on related CIR benchmarks, surpassing state-of-the-art on MIT-States despite zero-shot evaluation. 2.