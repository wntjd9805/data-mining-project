Abstract
By simply taking convex combinations between pairs of samples and their labels, mixup training has been shown to easily improve predictive accuracy. It has been recently found that models trained with mixup also perform well on uncertainty calibration. However, in this study, we found that mixup training usually makes models less calibratable than vanilla empirical risk minimization, which means that it would harm uncertainty estimation when post-hoc cali-bration is considered. By decomposing the mixup process into data transformation and random perturbation, we sug-gest that the confidence penalty nature of the data transfor-mation is the reason of calibration degradation. To miti-gate this problem, we first investigate the mixup inference strategy and found that despite it improves calibration on mixup, this ensemble-like strategy does not necessarily out-perform simple ensemble. Then, we propose a general strat-egy named mixup inference in training, which adopts a simple decoupling principle for recovering the outputs of raw samples at the end of forward network pass. By em-bedding the mixup inference, models can be learned from the original one-hot labels and hence avoid the negative impact of confidence penalty. Our experiments show this strategy properly solves mixup’s calibration issue without sacrificing the predictive performance, while even improves accuracy than vanilla mixup. 1.

Introduction
Although modern neural networks have made notable performance on predictive accuracy in various computer vi-sion tasks [7], they have been found to perform poorly in terms of uncertainty calibration, which is an important con-sideration in many real-world applications [5]. Intuitively, we expect a predictive model to be accurate when it is confi-* Corresponding author
This work is done when Deng-Bao works as an intern in Tencent AI Lab. dent about its outputs while reveal high uncertainty when it is likely to be inaccurate. Otherwise, the miscalibrated pre-diction of models could cause undesired consequences in many safety-critical applications such as medical diagnosis and autonomous driving. Early researches on uncertainty estimation mainly focus on probabilistic models. However, in deep learning paradigm, training of deep bayesian mod-els are expensive and their performance usually depends on approximate inference methods due to the computational constraint in real-world deployment. Therefore, uncertainty calibration of deterministic neural networks becomes an im-portant topic in recent years.
Guo et al. [5] systematically studied the uncertainty cal-ibration problem of modern neural networks with compre-hensive experiments. They pointed out that popular mod-ern neural networks usually suffer from severe miscalibra-tion issue than shallow models. They empirically showed that large model capacity without proper regularization is closely related to the miscalibration issue. They also evalu-ated the performance of various calibration strategies and found that simple post-hoc approaches like temperature scaling (TS) [22] and histogram binning (HB) [34] can re-duce the calibration error to a quite low level. Following their work, a number of calibration friendly regularization methods and post-calibration approaches were proposed to address the miscalibration issue of deep neural networks
[12, 17, 18, 21].
Recently, researchers investigated the impact of mixup training for calibration. Thulasidasan et al. [27] empirically found that mixup improves calibration across various model architectures and datasets. Zhang et al. [38] provided a the-oretical explanation for the effect of mixup training on cal-ibration in high-dimensional regime. Carratino et al. [3] pointed out that mixup implicitly performs label smoothing and hence can avoid the overconfidence issue. However, there are also empirical observations showing that mixup does not necessarily improve calibration. The experiments in [16] provides evidence showing mixup degrades calibra-tion in some cases. The empirical studies in [31] and [23]
found that combining mixup with ensemble degrades cal-ibration performance than individually using one of them.
In particular, they suggest that both mixup and ensembling encourage models to be less confident, and hence the under-confidence issue occurs when they are used together.
We notice that most of existing work investigates mixup for calibration without the consideration of post-calibration.
As the research in [1] suggested, the comparison of calibra-tion performance between different methods without post-calibration might not provide a fair ranking. Another recent work [30] also pointed out that models with better calibra-tion performance during main training do not necessarily yield better calibration results after post-calibration. There-fore, in this work, we revisit mixup’s calibration problem by considering the training stage and post-hoc processing as a unified system. Under this setting, three questions are nat-urally raised: (i) Does mixup really help calibration? (ii) If it does not, what leads to the failure? (iii) How can we mit-igate the pitfall of mixup on calibration? To answer these questions, we make the following contributions:
• We conduct comprehensive experiments showing that mixup often leads to less calibratable models than vanilla empirical risk minimization (ERM), and hence degrades uncertainty estimation in general when post-calibration is considered after training.
• To explain this phenomenon, we decompose mixup into two components: data transformation and random perturbation. We show that the former part shrinks the training labels to their means and implicitly performs confidence penalty, which serves as the reason of cali-bration degradation.
• We investigate the mixup inference strategy for calibra-tion and found that despite it improves calibration on mixup, this ensemble-like approach is no better than vanilla deep ensemble in terms of both calibration and accuracy with same inference budget.
• We show that mixup’s calibration issue can be easily solved by translating the mixup inference into training.
By this process, the output of each raw sample can be approximately recovered to be learned from the orig-inal one-hot labels, and hence avoiding the negative effect induced by confidence penalty. Our experiments show that this strategy outperforms mixup in terms of both accuracy and calibration. 2.