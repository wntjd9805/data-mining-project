Abstract
Contrastive Language-Image Pre-training (CLIP) achieves promising results in 2D zero-shot and few-shot learning. Despite the impressive performance in 2D, apply-ing CLIP to help the learning in 3D scene understanding has yet to be explored.
In this paper, we make the first attempt to investigate how CLIP knowledge benefits 3D scene understanding. We propose CLIP2Scene, a simple yet effective framework that transfers CLIP knowledge from 2D image-text pre-trained models to a 3D point cloud network. We show that the pre-trained 3D network yields impressive performance on various downstream tasks, i.e., annotation-free and fine-tuning with labelled data for semantic segmentation. Specifically, built upon CLIP, we design a Semantic-driven Cross-modal Contrastive Learning framework that pre-trains a 3D network via semantic and spatial-temporal consistency regularization. For the former, we first leverage CLIP’s text semantics to select the positive and negative point samples and then employ the contrastive
In terms of the latter, we loss to train the 3D network. force the consistency between the temporally coherent point cloud features and their corresponding image features. We conduct experiments on SemanticKITTI, nuScenes, and
ScanNet. For the first time, our pre-trained network achieves annotation-free 3D semantic segmentation with 20.8% and 25.08% mIoU on nuScenes and ScanNet, respectively. When fine-tuned with 1% or 100% labelled data, our method significantly outperforms other self-supervised methods, with improvements of 8% and 1% mIoU, respectively.
Furthermore, we demonstrate the generalizability for handling cross-domain datasets. Code is publicly available1. 1.

Introduction 3D scene understanding is fundamental in autonomous driving, robot navigation, etc [26,28]. Current deep learning-Symbol † denotes the corresponding authors. 1https://github.com/runnanchen/CLIP2Scene.
Figure 1. We explore how CLIP knowledge benefits 3D scene understanding. To this end, we propose CLIP2Scene, a Semantic-driven Cross-modal Contrastive Learning framework that leverages
CLIP knowledge to pre-train a 3D point cloud segmentation net-work via semantic and spatial-temporal consistency regularization.
CLIP2Scene yields impressive performance on annotation-free 3D semantic segmentation and significantly outperforms other self-supervised methods when fine-tuning on annotated data. based methods have shown inspirational performance on 3D point cloud data [15, 32, 33, 38, 47, 56, 62]. However, some drawbacks hinder their real-world applications. The first one comes from their heavy reliance on the large collection of annotated point clouds, especially when high-quality 3D annotations are expensive to acquire [39,40,44,51]. Besides, they typically fail to recognize novel objects that are never seen in the training data [11, 45]. As a result, it may need ex-tra annotation efforts to train the model on recognizing these novel objects, which is both tedious and time-consuming.
Contrastive Vision-Language Pre-training (CLIP) [48] provides a new perspective that mitigates the above issues in 2D vision. It was trained on large-scale free-available image-text pairs from websites and built vision-language
correlation to achieve promising open-vocabulary recogni-tion. MaskCLIP [61] further explores semantic segmenta-tion based on CLIP. With minimal modifications to the CLIP pre-trained network, MaskCLIP can be directly used for the semantic segmentation of novel objects without additional training efforts. PointCLIP [59] reveals that the zero-shot classification ability of CLIP can be generalized from the 2D image to the 3D point cloud. It perspectively projects a point cloud frame into different views of 2D depth maps that bridge the modal gap between the image and the point cloud.
The above studies indicate the potential of CLIP on enhanc-ing the 2D segmentation and 3D classification performance.
However, whether and how CLIP knowledge benefits 3D scene understanding is still under-explored.
In this paper, we explore how to leverage CLIP’s 2D image-text pre-learned knowledge for 3D scene understand-ing. Previous cross-modal knowledge distillation meth-ods [44, 51] suffer from the optimization-conflict issue, i.e., some of the positive pairs are regarded as negative samples for contrastive learning, leading to unsatisfactory represen-tation learning and hammering the performance of down-stream tasks. Besides, they also ignore the temporal coher-ence of the multi-sweep point cloud, failing to utilize the rich inter-sweep correspondence. To handle the mentioned problems, we propose a novel Semantic-driven Cross-modal
Contrastive Learning framework that fully leverages CLIP’s semantic and visual information to regularize a 3D network.
Specifically, we propose Semantic Consistency Regulariza-tion and Spatial-Temporal Consistency Regularization. In semantic consistency regularization, we utilize CLIP’s text semantics to select the positive and negative point samples for less-conflict contrastive learning. For spatial-temporal consistency regularization, we take CLIP’s image pixel fea-ture to impose a soft consistency constraint on the temporally coherent point features. Such an operation also alleviates the effects of imperfect image-to-point calibration.
We conduct several downstream tasks on the indoor and outdoor datasets to verify how the pre-trained network bene-fits the 3D scene understanding. The first one is annotation-free semantic segmentation. Following MaskCLIP, we place class names into multiple hand-crafted templates as prompts and average the text embeddings generated by CLIP to con-duct the annotation-free segmentation. For the first time, our method achieves 20.8% and 25.08% mIoU annotation-free 3D semantic segmentation on the nuScenes [24] and
ScanNet [20] datasets without training on any labelled data.
Secondly, we compare with other self-supervised methods in label-efficient learning. When fine-tuning the 3D network with 1% or 100% labelled data on the nuScenes dataset, our method significantly outperforms state-of-the-art self-supervised methods, with improvements of 8% and 1% mIoU, respectively. Besides, to verify the generalization capability, we pre-train the network on the nuScenes dataset and evaluate it on SemanticKITTI [3]. Our method still significantly outperforms state-of-the-art methods. The key contributions of our work are summarized as follows.
• The first work that distils CLIP knowledge to a 3D network for 3D scene understanding.
• We propose a novel Semantic-driven Cross-modal Con-trastive Learning framework that pre-trains a 3D net-work via spatial-temporal and semantic consistency regularization.
• We propose a novel Semantic-guided Spatial-Temporal
Consistency Regularization that forces the consistency between the temporally coherent point cloud features and their corresponding image features.
• For the first time, our method achieves promising re-sults on annotation-free 3D scene segmentation. When fine-tuning with labelled data, our method significantly outperforms state-of-the-art self-supervised methods. 2.