Abstract
Continuous convolution has recently gained prominence due to its ability to handle irregularly sampled data and model long-term dependency. Also, the promising exper-imental results of using large convolutional kernels have catalyzed the development of continuous convolution since they can construct large kernels very efficiently. Lever-aging neural networks, more specifically multilayer per-ceptrons (MLPs), is by far the most prevalent approach to implementing continuous convolution. However, there are a few drawbacks, such as high computational costs, complex hyperparameter tuning, and limited descriptive power of filters. This paper suggests an alternative ap-proach to building a continuous convolution without neu-ral networks, resulting in more computationally efficient and improved performance. We present self-moving point representations where weight parameters freely move, and interpolation schemes are used to implement continuous functions. When applied to construct convolutional ker-nels, the experimental results have shown improved per-formance with drop-in replacement in the existing frame-works. Due to its lightweight structure, we are first to demonstrate the effectiveness of continuous convolution in a large-scale setting, e.g., ImageNet, presenting the im-provements over the prior arts. Our code is available on https://github.com/sangnekim/SMPConv 1.

Introduction
There has been a recent surge of interest in represent-ing the convolutional kernel as a function over a continu-ous input domain. It can easily handle irregularly sampled data both in time [1, 59] and space [61, 65], overcoming the drawbacks of the discrete convolution operating only on discretized sampled data with pre-defined resolutions and grids. With the progress in modeling and training continu-ous kernels, it has enjoyed great success in many practical scenarios, such as 3D point cloud classification and segmen-*Corresponding authors tation [36,41,52,58,64], image super resolution [57], object tracking [10], to name a few. Furthermore, the recent trends of using large convolutional kernels with strong empirical results urge us to develop a more efficient way to implement it [13,32], and the continuous convolution will be a promis-ing candidate because of its capability to readily construct arbitrarily large receptive fields [44, 45].
One of the dominant approaches to modeling the con-tinuous kernel is to use a particular type of neural network architecture, taking as inputs low-dimensional input coor-dinates and generating the kernel values [44, 45], known as neural fields [38, 49] or simply MLPs. Using neural fields to represent the kernels, we can query kernel values at arbi-trary resolutions in parallel and construct the large kernels with a fixed parameter budget, as opposed to the conven-tional discrete convolutions requiring more parameters to enlarge receptive fields. Thanks to recent advances to over-come the spectral bias on training neural fields [49], they can also represent functions with high-frequency compo-nents, which enables learned kernels to capture fine details of input data.
While promising in various tasks and applications, this approach has a few downsides. First, it incurs consider-able computational burdens to already computation-heavy processes of training deep neural networks. Each train-ing iteration involves multiple forward and backward passes of MLPs to generate kernels and update the parameters of
MLPs. This additional complexity prevents it from being applied to large-scale problems, such as ImageNet-scale, since it needs deeper and wider MLP architectures to con-struct more complex kernels with more input and output channels. Although MLPs can generate larger sizes and numbers of kernels without adding more parameters, it has been known that the size of MLPs mainly determines the complexity of the functions they represent and, eventually, the performance of the CNNs.
Furthermore, the kernels generated by an MLP depend heavily on the architectural priors. As a universal function approximator, a neural network with sufficient depth and width can express any continuous functions [25]. However, we have empirically observed strong evidence that the ar-chitecture of neural networks has played a significant role in many practical settings, suggesting that various archi-tectural changes to the kernel-generating MLPs would sig-nificantly affect the performance of CNNs. Considering a large number of hyperparameters in training neural net-works, adding more knobs to tune, e.g., activation func-tions, width, depth, and many architectural variations of
MLPs., would not be a pragmatic solution for both machine learning researchers and practitioners.
In this paper, we aim to build continuous convolution kernels with negligible computational cost and minimal ar-chitectural priors. We propose to use moving point repre-sentations and implement infinite resolution by interpolat-ing nearby moving points at arbitrary query locations. The moving points are the actual kernel parameters, and we con-nect the neighboring points to build continuous kernels. Re-cent techniques in neural fields literature inspire it, where they used grid or irregularly distributed points to represent features or quantities in questions (density or colors) for novel view synthesis [6, 50, 63, 66, 67]. The suggested ap-proach only introduces minor computational costs (interpo-lation costs) and does not consist of neural networks (only point representations and interpolation kernels). Moreover, the spectral bias presented in training MLPs [43] does not exist in the suggested representation. Each point representa-tion covers the local area of the input domain and is updated independently of each other, contrasted with MLPs, where updating each parameter would affect the entire input do-main. Therefore, highly different values of nearby points can easily express high-frequency components of the func-tion.
The proposed method can also be more parameter ef-ficient than the discrete convolution to construct the large kernels. Depending on the complexity of the kernels to be learned, a few numbers of points may be sufficient to cover a large receptive field (e.g., a unimodal function can be approximated by a single point). Many works have ex-tensively exploited non-full ranks of the learned kernels to implement efficient convolutions or compress the mod-els [32]. Our approach can likewise benefit from the pres-ence of learned kernels with low-frequency components.
We conduct comprehensive experimental results to show the effectiveness of the proposed method. First, we demon-strate that moving point representations can approximate continuous functions very well in 1D and 2D function fit-ting experiments. Then, we test its ability to handle long-term dependencies on various sequential datasets. Finally, we also evaluate our model on 2D image data. Especially, we perform large-scale experiments on the ImageNet clas-sification dataset (to our knowledge, it is the first attempt to use continuous convolution for such a large-scale problem).
Our proposed method can be used as a drop-in replacement of convolution layers for all the above tasks without bells and whistles. The experimental results show that it consis-tently improves the performance of the prior arts. 2.