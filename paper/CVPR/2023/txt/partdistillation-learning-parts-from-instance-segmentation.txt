Abstract 1.

Introduction
We present a scalable framework to learn part segmen-tation from object instance labels. State-of-the-art instance segmentation models contain a surprising amount of part information. However, much of this information is hidden from plain view. For each object instance, the part in-formation is noisy, inconsistent, and incomplete. PartDis-tillation transfers the part information of an instance seg-mentation model into a part segmentation model through self-supervised self-training on a large dataset. The result-ing segmentation model is robust, accurate, and generalizes well. We evaluate the model on various part segmentation datasets. Our model outperforms supervised part segmen-tation in zero-shot generalization performance by a large margin. Our model outperforms when finetuned on tar-get datasets compared to supervised counterpart and other baselines especially in few-shot regime. Finally, our model provides a wider coverage of rare parts when evaluated over 10K object classes. Code is at https://github. com/facebookresearch/PartDistillation.
*This work was done during Jang Hyun Choâ€™s internship at Meta AI.
The world of object parts is rich, diverse, and plenti-ful. Yet, even the most successful part segmentation bench-marks [10, 22] focus on only the few most prominent im-age classes, and are orders of magnitude smaller than corre-sponding object instance segmentation benchmarks [21,31].
Parts are harder to detect, annotate, and properly define.
In this paper, we show that instance segmentation models, and indirectly much larger instance segmentation datasets, provide plentiful supervision for part segmenta-tion. Specifically, we show that the penultimate layer of a pre-trained instance segmentation model readily groups parts across a wide class of instances. We distill this part in-formation from an instance segmentation model into a ded-icated part segmentation framework, in a two stage process we call PartDistillation. In the first stage, our model learns to segment all possible parts in a class-agnostic fashion.
We bootstrap an iterative self-training process from clus-tered embeddings of an instance segmentation model. The self-supervised nature of this process allows us to scale part discovery to 10K object classes in 10M images without any part-level supervision. In the second stage, our method
learns to group the discovered parts of each object category independently into object-specific part clusters. Figure 1 shows the result of this two-stage process.
Unlike traditional self-training methods [39, 43, 47] that rely on supervised part labels, we distill the part informa-tion from a pre-trained instance segmentation model.
In this framework, self-training increases the consistency be-tween the different potential part segmentation, and boosts the noisy supervisory signal. Our model makes full use of powerful instance segmentation architectures [11,12,25] for both supervision and part segmentation itself.
We show that PartDistillation outperforms existing un-supervised methods by a large margin.
It is very label-efficient in few-shot training, even compared to super-vised models trained on existing labelled part segmentation dataset. Finally, we verify that the part discovery qual-ity is consistent beyond a narrow set of classes in exist-ing datasets. We go through manual evaluation process and show that 1) PartDistillation discover more consistent parts compared to supervised model and 2) the precision stays the same when scaled to 10K classes. 2.