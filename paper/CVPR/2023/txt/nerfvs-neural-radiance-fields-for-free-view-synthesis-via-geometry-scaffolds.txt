Abstract
We present NeRFVS, a novel neural radiance fields (NeRF) based method to enable free navigation in a room.
NeRF achieves impressive performance in rendering im-ages for novel views similar to the input views while suffer-ing for novel views that are significantly different from the training views. To address this issue, we utilize the holis-tic priors, including pseudo depth maps and view coverage information, from neural reconstruction to guide the learn-ing of implicit neural representations of 3D indoor scenes.
Concretely, an off-the-shelf neural reconstruction method is leveraged to generate a geometry scaffold. Then, two loss functions based on the holistic priors are proposed to im-prove the learning of NeRF: 1) A robust depth loss that can tolerate the error of the pseudo depth map to guide the ge-* Work done during an internship at Huawei Noah’s Ark Lab.
†Corresponding Author. ometry learning of NeRF; 2) A variance loss to regularize the variance of implicit neural representations to reduce the geometry and color ambiguity in the learning procedure.
These two loss functions are modulated during NeRF op-timization according to the view coverage information to reduce the negative influence brought by the view coverage imbalance. Extensive results demonstrate that our NeRFVS outperforms state-of-the-art view synthesis methods quanti-tatively and qualitatively on indoor scenes, achieving high-fidelity free navigation results. 1.

Introduction
Reconstructing an indoor scene from a collection of im-ages and enabling users to navigate inside it freely is a core
It is the component for many downstream applications. most challenging novel-view-synthesis (NVS) task, since it requires high fidelity synthesis from any view, including not
only views similar to the training views (interpolation), but also views that are significantly different from input views (extrapolation), as shown in Fig. 1. To clarify its differ-ence to other NVS tasks, we term it as free-view-synthesis (FVS). The difficulties of FVS lie in not only the common obstacles in scene reconstruction, including low-texture ar-eas, complex scene geometry, and illumination change, but also view imbalance, e.g., casual photos usually cover the scene unevenly, with hundreds of frames for one table and a few for the floor and wall, as shown in Fig. 3.
Recently, NeRF has emerged as a promising technique for 3D reconstruction and novel view synthesis. Although
NeRF can achieve impressive interpolation performance, its extrapolation ability is relatively poor [35], especially for low-texture and few-shot regions. In contrast, some neural reconstruction methods can recover the holistic scene ge-ometry successfully with various priors [9,22,25,34], while the synthesized images from these methods contain plenty of artifacts and are over-smoothed.
Inspired by the phe-nomena, we demonstrate that equipping the NeRF with the scene priors of the geometry captured from neural recon-struction is a potential solution for indoor FVS.
Extending NeRF to enable FVS with geometry from neural reconstruction methods is a non-trivial task with two main challenges. 1) Depth error. The reconstructed ge-ometry might contain some failures, including holes, depth shifting, and floaters. The optimization of NeRF relies on the multi-view color consistency, while these failures may conflict with the multi-view color consistency, resulting in artifacts. 2) Distribution ambiguity. The depth from NeRF is a weighted sum of sampling distance. Merely supervis-ing the depth expectation leads to arbitrary radiance distri-bution, especially in low-texture and few-shot regions. This ambiguous distribution leads to floaters and blur among ren-dered images, as shown in Fig. 5.
In this paper, we propose a novel method which exploits the holistic priors, including pseudo depth maps and view coverage information, outputted from a geometry scaffold to guide NeRF optimization, significantly improving qual-ity on low-texture and few-shot regions. Specifically, to ad-dress the depth error, we propose a robust depth loss that can tolerate the error from the pseudo depth maps, reduc-ing the negative impact of inaccurate geometry. As for the distribution ambiguity, it mainly happens in the low-texture and rarely observed areas, e.g. ceilings. We propose a vari-ance loss to regularize the variance of the density and color distribution to decrease the ambiguity of these areas. The weights of these two losses are further adjusted according to the view coverage sufficiency to reduce the negative in-fluence brought by the view imbalance. With the geometry priors and variance regularization, our method can signifi-cantly reduce the floaters and distortions among low-texture and few-shot regions, achieving high-fidelity extrapolation performance.
Experiments on synthetic and real-world datasets demonstrate that our method performs high-fidelity extrap-olation by removing the distortions and floaters, signifi-cantly outperforming other view synthesis methods. Con-sidering the rendering quality and 3D consistency among interpolation and extrapolation, our NeRFVS achieves new state-of-the-art performance on indoor scene FVS.
In conclusion, our contribution can be summarized as follows:
• A novel approach enabling neural radiance fields to perform free view synthesis on real-world scenes at room scale.
• A robust depth loss to address the inaccuracy of the neural-reconstructed geometry.
• A flexible variance loss with view coverage based ad-justment to improve the rendering quality among low-texture and few-shot regions. 2