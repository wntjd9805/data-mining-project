Abstract
Point cloud sequences are commonly used to accurately detect 3D objects in applications such as autonomous driv-ing. Current top-performing multi-frame detectors mostly follow a Detect-and-Fuse framework, which extracts fea-tures from each frame of the sequence and fuses them to detect the objects in the current frame. However, this inevitably leads to redundant computation since adjacent frames are highly correlated. In this paper, we propose an efficient Motion-guided Sequential Fusion (MSF) method, which exploits the continuity of object motion to mine useful sequential contexts for object detection in the current frame.
We first generate 3D proposals on the current frame and propagate them to preceding frames based on the estimated velocities. The points-of-interest are then pooled from the sequence and encoded as proposal features. A novel Bidi-rectional Feature Aggregation (BiFA) module is further pro-posed to facilitate the interactions of proposal features across frames. Besides, we optimize the point cloud pool-ing by a voxel-based sampling technique so that millions of points can be processed in several milliseconds. The pro-posed MSF method achieves not only better efficiency than other multi-frame detectors but also leading accuracy, with 83.12% and 78.30% mAP on the LEVEL1 and LEVEL2 test sets of Waymo Open Dataset, respectively. Codes can be found at https://github.com/skyhehe123/MSF. 1.

Introduction 3D object detection [1, 2, 6, 7, 9, 14, 21, 27–29, 36] is one of the key technologies in autonomous driving, which helps the vehicle to better understand the surrounding environ-ment and make critical decisions in the downstream tasks.
As an indispensable sensing device in autonomous driving systems, LiDAR collects 3D measurements of the scene in
*Equal contribution.
†Corresponding author.
Figure 1. (a) The “Detect-and-Fuse” framework extracts features from each frame of the sequence and then fuses them, while (b) our proposed “Motion-guided Sequential Fusion” (MSF) method generates proposals on the current frame and propagates them to preceding frames to explore useful contexts in the sequence. the form of point clouds. However, LiDAR can only pro-duce partial view of the scene at a time, and the sparse and incomplete representation of point clouds brings consider-able challenges to the 3D object detection task. In practice, the LiDAR sensor will continuously sense the environment and produce a sequence of point cloud frames over time.
The multi-frame data can provide a denser representation of the scene as the vehicle moves. Therefore, how to fuse these multi-frame point cloud data for more accurate object detection is worth deep investigation.
Recent works mainly focus on deep feature fusion with multi-frame point clouds, for example, aggregating dense birds-eye-view features via Transformer models [31, 37], passing the voxel features to LSTM [8] or GRU [32] mod-ules for temporal modeling. Some top-performing detectors
[2, 18] focus on fusing proposal features, where a tracker is employed to associate the 3D proposals across frames, and a region-based network is applied to refine the cur-rent proposals by incorporating contextual features from the proposal trajectories. These approaches generally follow a
“Detect-and-Fuse” framework, as shown in Fig. 1(a), where the model requires to process each frame of the sequence,
and the predictions on the current frame rely on the results of preceding frames. Since online detection is a causal sys-tem, such a detection framework might cause significant de-lay if the network is still processing a preceding frame when the current frame is loaded.
In this paper, we propose an efficient Motion-guided
Sequential Fusion (MSF) method, as shown in Fig. 1(b), which leverages the continuity of object motion to extract useful contexts from point cloud sequences and improve the detection of current frame. Specifically, considering that the motions of objects are relatively smooth in a short sequence, we propagate the proposals generated on current frame to preceding frames based on the velocities of objects, and sample reliable points-of-interest from the sequence. In this way, we bypass extracting features on each frame of the sequence, which reduces the redundant computation and reliance on the results of preceding frames. The sampled points are then transformed to proposal features via two en-coding schemes and passed to a region-based network for further refinement. Specifically, a self-attention module is employed to enhance the interaction of point features within proposals, while a novel Bidirectional Feature Aggregation (BiFA) module is proposed to enforce the information ex-change between proposals across frames. The refined pro-posal features consequently capture both spatial details and long-term dependencies over the sequence, leading to more accurate bounding-box prediction.
It is found that the existing point cloud pooling meth-ods [2, 19, 23, 30] are inefficient, taking more than 40 mil-liseconds when processing millions of points from sequen-tial point clouds. We find that the major bottleneck lies in the heavy computation of pair-wise distances between n points and m proposals, which costs O(nm) complexity. To further improve the efficiency, we optimize the point cloud pooling with a voxel sampling technique. The improved pooling operation is of linear complexity and can process millions of points in several milliseconds, more than eight times faster than the original method.
Overall, our contributions can be summarized as follows.
• An efficient Motion-guided Sequential Fusion (MSF) method is proposed to fuse multi-frame point clouds at region level by propagating the proposals of current frame to preceding frames based on the object motions.
• A novel Bidirectional Feature Aggregation (BiFA) module is introduced to facilitate the interactions of proposal features across frames.
• The point cloud pooling method is optimized with a voxel-based sampling technique, significantly reduc-ing the runtime on large-scale point cloud sequence.
The proposed MSF method is validated on the challeng-ing Waymo Open Dataset, and it achieves leading accuracy on the LEVEL1 and LEVEL2 test sets with fast speed. 2.