Abstract 1.

Introduction
We target a 3D generative model for general natural scenes that are typically unique and intricate. Lacking the necessary volumes of training data, along with the difﬁ-culties of having ad hoc designs in presence of varying scene characteristics, renders existing setups intractable.
Inspired by classical patch-based image models, we advo-cate for synthesizing 3D scenes at the patch level, given a single example. At the core of this work lies important al-gorithmic designs w.r.t the scene representation and gener-ative patch nearest-neighbor module, that address unique challenges arising from lifting classical 2D patch-based framework to 3D generation. These design choices, on a collective level, contribute to a robust, effective, and efﬁ-cient model that can generate high-quality general natural scenes with both realistic geometric structure and visual ap-pearance, in large quantities and varieties, as demonstrated upon a variety of exemplar scenes. Data and code can be found at http://wyysf-98.github.io/Sin3DGen.
*Joint ﬁrst authors
†Corresponding author 3D scene generation generally carries the generation of both realistic geometric structure and visual appearance. A wide assortment of scenes on earth, or digital ones across the internet, exhibiting artistic characteristics and ample variations over geometry and appearance, can be easily listed. Being able to populate these intriguing scenes in the virtual universe has been a long pursuit in the community.
Research has taken several routes, among which a preva-lent one is learning to extract common patterns of the ge-ometry or appearance from homogeneous scene samples, such as indoor scenes [14, 25, 34, 37, 59, 63, 71, 72, 75], ter-rains [15, 19, 21, 26], urban scenes [12, 30, 44], etc. Another line learns to generate single objects [6, 7, 16, 25, 33, 35, 45, 74]. A dominant trend in recent has emerged that learns 3D generative models to jointly synthesize 3D structures and appearances via differentiable rendering [4, 5, 8, 18, 43, 50].
Nevertheless, all these learning setups are limited in their ability to generalize in terms of varied scene types. While a more promising direction is the exemplar-based one, where one or a few exemplars featuring the scene of interest are provided, algorithm designs tailored for certain scene types in existing methods [38–40,73] again draw clear boundaries
of scene characteristics they can handle. efﬁcient 3D generative model.
This work seeks to generate general natural scenes, wherein the geometry and appearance of constituents are often tightly entangled and contribute jointly to unique fea-tures. This uniqueness hinders one from collecting suf-ﬁcient homogeneous samples for learning common fea-tures, directing us to the exemplar-based paradigm. On the other hand, varying characteristics across different exem-plar scenes restrain us from having ad hoc designs for a cer-tain scene type (e.g., terrains). Hence, we resort to clas-sical patch-based algorithms, which date long before the deep learning era and prevail in several image generation tasks even today [13, 17, 20]. Speciﬁcally, given an input 3D scene, we synthesize novel scenes at the patch level and particularly adopt the multi-scale generative patch-based framework introduced in [17], where the core is a Gener-ative Patch Nearest-Neighbor module that maximizes the bidirectional visual summary [54] between the input and output. Nevertheless, key design questions yet remain in the 3D generation: What representation to work with? And how to synthesize effectively and efﬁciently?
In this work, we exploit a grid-based radiance ﬁeld –
Plenoxels [69], which boasts great visual effects, for rep-resenting the input scene. While its simplicity and regu-lar structure beneﬁt patch-based algorithms, important de-signs must be adopted. Speciﬁcally, we construct the exem-plar pyramid via coarse-to-ﬁne training Plenoxels on im-ages of the input scene, instead of trivially downsampling a pretrained high-resolution one. Furthermore, we trans-form the high-dimensional, unbounded, and noisy features of the Plenoxels-based exemplar at each scale into more well-deﬁned and compact geometric and appearance fea-tures, improving the robustness and efﬁciency in the subse-quent patch matching.
On the other end, we employ heterogeneous representa-tions for the synthesis inside the generative nearest neigh-bor module. Speciﬁcally, the patch matching and blending operate in tandem at each scale to gradually synthesize an intermediate value-based scene, which will be eventually converted to a coordinate-based counterpart at the end. The beneﬁts are several-fold: a) the transition between consec-utive generation scales, where the value range of exemplar features may ﬂuctuate, is more stable; b) the transformed features in the synthesized output is inverted to the original
”renderable” Plenoxels features; so that c) the visual real-ism in the Plenoxels-based exemplar is preserved intactly.
Last, working on voxels with patch-based algorithms nec-essarily leads to high computation issues. So we use an exact-to-approximate patch nearest-neighbor module in the pyramid, which keeps the search space under a manageable range while introducing negligible compromise on the vi-sual summary optimality. These designs, on a collective level, essentially lay a solid foundation for an effective and
To our knowledge, our method is the ﬁrst 3D generative model that can generate 3D general natural scenes from a single example, with both realistic geometry and visual ap-pearance, in large quantities and varieties. We validate the efﬁcacy of our method on random scene generation with an array of exemplars featuring a variety of general natural scenes, and show the superiority by comparing to baseline methods. The importance of each design choice is also val-idated. Extensive experiments also demonstrates the versa-tility of our method in several 3D modeling applications. 2.