Abstract
Large-scale pre-training has brought unimodal fields such as computer vision and natural language process-ing to a new era. Following this trend, the size of multi-modal learning models constantly increases, leading to an urgent need to reduce the massive computational cost of finetuning these models for downstream tasks. In this pa-per, we propose an efficient and flexible multimodal fusion method, namely PMF, tailored for fusing unimodally pre-trained transformers. Specifically, we first present a mod-ular multimodal fusion framework that exhibits high flex-ibility and facilitates mutual interactions among different modalities.
In addition, we disentangle vanilla prompts into three types in order to learn different optimizing objec-tives for multimodal learning. It is also worth noting that we propose to add prompt vectors only on the deep layers of the unimodal transformers, thus significantly reducing the training memory usage. Experiment results show that our proposed method achieves comparable performance to several other multimodal finetuning methods with less than 3% trainable parameters and up to 66% saving of training memory usage. 1.

Introduction
Recent years have witnessed the great success of large-scale pretrained language models [8,31,32] and visual mod-els [6,10,23,39], leading to a surge of pretrained multimodal models [13, 14, 43, 47, 48] trying to align different modali-ties. Many prior methods utilize finetuning to update the entire set of model parameters for every target cross-modal task. Although finetuning can achieve good performance, it requires a large number of computational costs since the gradients and optimizer states for all parameters of multi-modal models have to store. Therefore, it encourages re-searchers to propose more parameter-efficient methods than finetuning for multimodal learning.
Figure 1. Comparison over three multimodal classification tasks. We compare our proposed PMF and PMF-Large with mul-tiple finetuning (yellow) and prompt-based (purple) methods. The y-axis is the average score of three tasks, and the x-axis is the max-imum GPU memory usage during training.
More recently, prompting tuning [17, 19, 21, 22, 29] is proposed to address this problem by freezing all parame-ters of a pretrained model while tuning only the continuous prompts. Specifically, it adds trainable continuous prompts to the original token sequences of input data. During train-ing, only the continuous prompts are updated. For multi-modal prompt-based learning, a most recent method [20] proposes to disentangle the functionality of the pretrained model which exhibits high flexibility. Although this method significantly reduces the tuned parameters (e.g., less than 0.1% of the pretrained model), there still exists a large per-formance gap between it and the finetuning-based methods.
In addition, this method adopts a sequential modular struc-ture that the pretrained image transformer model is followed by a language transformer model, which causes two main problems in cross-modal learning: a one-way path learn-ing and a significant increase in the number of model lay-ers. Specifically, a one-way path learning in the multimodal model usually forces one modality to align with others, but not vice versa.
In this way, cross-modal learning based on multiple different modalities is not fully explored due to the missing mutual alignments. Since the prompts are added to the token sequences of input data and are updated in the training, they require extensive gradient calculations in the backward propagation which cost numerous memory usages. As a result, this kind of method does not reduce the memory usage during training by much (up to 20%) though it reduces the number of parameters to update.
In other words, this parameter-efficient method still requires mas-sive computational resources which prevents it from being applied to many real-world applications.
To address these issues, we propose a Prompt-based
Multimodal Fusion method with a high memory efficiency, namely PMF. Firstly, we present a new form of modu-lar multimodal fusion framework which demonstrates high flexibility and facilitates a two-way interaction among dif-ferent modalities. Specifically, we adopt a two-stream struc-ture where the pretrained language model and image model construct the multimodal model in a parallel way. There-fore, tokens of different modalities can learn mutual inter-actions through a cross-attention-like operation. Such a par-allel modular structure brings two benefits. First, unimodal pretraining can be directly utilized for multimodal learn-ing through a parallel combination, eliminating the need for paired multimodal datasets that can be expensive to con-struct. Also, the type of image or language model can be changed easily (e.g., replacing BERT with T5 for text gen-eration tasks). Furthermore, incorporating extra modalities is made possible based on the parallel modular structure.
Moreover, we propose to leverage three types of inter-active prompts (i.e., query prompts, query context prompts, and fusion context prompts) in order to dynamically learn different objectives for multimodal learning. Intuitively, the query context prompt and query prompt can be seen as a pair of ‘questions’ and ‘answers’ with an aim of extracting necessary information for exchange between two modali-ties. After being translated by a non-linear mapping ‘trans-lator’, the ‘answer’ is then delivered to the other modality for better cross-modal understanding. Finally, the fusion context prompts then provide the context to the delivered answer to facilitate the fusion.
Last but most importantly, PMF is a memory-efficient method that significantly reduces the memory requirements for the large pretrained model. Considering that calculat-ing gradients for prompts for back-propagation is memory-consuming, we propose to add prompts only on the deep layers of the utilized unimodal transformers. Therefore, in-stead of passing through the entire multimodal model, the backward propagation only needs to pass through the deep few transformer layers to reach all trainable parameters, greatly reducing the training memory usage. We conduct extensive experiments to demonstrate the superior of it in our experiments. As a result, PMF enables large pretrained models to be trained on the GPU with a low memory re-quirement.
We conduct extensive experiments on three vision-language datasets: UPMC-Food101 [38], MM-IMDB [2], and SNLI-VE [41]. Through comparisons with multiple finetuning and prompt tuning methods (see in Fig. 1), we find that: (1) PMF is the most memory-efficient method for cross-modal learning so far, which reduces the train-ing memory usage by up to 66% compared with finetuning baselines, and by 55% compared with prompt-based meth-ods. (2) PMF can perform comparably compared to prior fine-tuning methods with much fewer trainable parameters (less than 2.5%) and memory usage.
Concretely, our contributions are as follows: (1) we present a new form of modular multimodal fusion frame-work which enables two-way interactions between differ-ent modalities and high flexibility of the entire model; (2) we disentangle vanilla prompts into three types of prompts, in order to dynamically learn different objectives for multi-modal learning; (3) our proposed method is quite memory-efficient yet is able to achieve comparable performance with existing finetuning methods for multimodal fusion. 2.