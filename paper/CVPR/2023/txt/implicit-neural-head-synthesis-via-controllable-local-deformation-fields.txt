Abstract
High-quality reconstruction of controllable 3D head avatars from 2D videos is highly desirable for virtual human applications in movies, games, and telepresence. Neural implicit fields provide a powerful representation to model 3D head avatars with personalized shape, expressions, and facial parts, e.g., hair and mouth interior, that go beyond the linear 3D morphable model (3DMM). However, existing methods do not model faces with fine-scale facial features, or local control of facial parts that extrapolate asymmetric expressions from monocular videos. Further, most condition only on 3DMM parameters with poor(er) locality, and re-solve local features with a global neural field. We build on part-based implicit shape models that decompose a global deformation field into local ones. Our novel formulation models multiple implicit deformation fields with local seman-tic rig-like control via 3DMM-based parameters, and repre-sentative facial landmarks. Further, we propose a local con-trol loss and attention mask mechanism that promote sparsity of each learned deformation field. Our formulation renders sharper locally controllable nonlinear deformations than previous implicit monocular approaches, especially mouth interior, asymmetric expressions, and facial details. Project page: https://imaging.cs.cmu.edu/local deformation fields/ 1.

Introduction
Monocular human head avatar reconstruction is a long standing challenge that has drawn a lot of attention in the last few decades due to its wide application in movie mak-ing [11, 17, 43], and virtual reality [2, 3, 29], among others.
Traditional reconstruction methods in production pipelines create animatable and detailed avatars, often represented as 3D rigs, from high-quality face scans with predefined ex-pressions and poses [56, 58]. However, such data is often expensive to acquire and process, and over the years has created the need for an easier capture pipeline, e.g., based on high-definition images, or videos of human subjects. With the advancements in deep learning, much effort has gone
*Work was done while interning at Flawless AI. into learning neural 3D face representations from 2D im-ages and the research community has achieved impressive results [22, 27, 39, 45]. However, modeling 3D structures from 2D information alone is an ill-posed problem, which results in models that lack view consistency and details.
Both traditional and neural reconstruction pipelines based on the parametric mesh representation, 3DMM [9], are ef-ficient, controllable, and well integrated into the graphics pipeline, though at the expense of lacking important facial features such as hair, eyes, and mouth interior. In the last couple of years, there has been a surge of research on gen-eralized implicit face representations, e.g., sign distance functions (SDFs) [34], neural radiance fields (NeRFs) [16] or hybrid volumetric representations [4], that allow accu-rate modeling of fine-grained facial, and non-facial features not possible with mesh-based representations alone, while preserving view-consistency.
Recently, several implicit models for human head avatars from monocular videos have demonstrated great progress
[1, 3, 12, 15, 18, 35, 36, 41, 47, 57, 67, 68]. Several employ facial parameters from an existing face tracker to condition a multi-layer perceptron (MLP) to model expression changes
[3,12,18,47], use deformation fields to learn a mapping from an observed point deformed by expression to a point on the template face [1, 35, 67], or learn forward mapping functions that estimate implicit 3DMM functions, represented as facial deformation bases [15, 42, 68]. These approaches have done a good job in allowing control of expressions and poses, even for out-of-distribution training parameters. However, none of these approaches reconstruct animatable heads with high-fidelity details such as deep creases. Besides, since they heavily rely on creating implicit fields derived from linear 3DMMs, which are de-facto limited by global or large-scale expression decompositions, it is relatively difficult to control localized deformations at a finer level, e.g., wrinkles that form around eyes when winking.
In this paper, we propose a novel approach that allows implicit 3D rig modeling, and local control of detailed facial deformations. We use expression parameters obtained from a 3DMM face tracker, e.g., DECA [10], but learn to model more local deformations beyond linear 3DMM. To this end, instead of learning a single global deformation field, we
Figure 1. Main results on test samples. Our method models dynamic 3D deformations as an ensemble of local deformation fields, centered around 3D facial landmarks, shown as red, blue, and green dots in this example (details in Sec. 3). Our formulation synthesizes 3D neural heads from 2D videos with fine-grained geometric details, as shown in column 3 (depth field). break it into multiple local fields with varying spatial support, triggered by sparse facial landmarks, with weak supervision on the 3DMM expressions. The local deformation fields are represented by nonlinear function within a certain radius, and conditioned by tracked expression parameters weighted by an attention mask that filters redundant parameters that do not influence the landmarks. Finally, the local deformations are summed with distance-based weights, which are used to deform the global point to the canonical space, and retrieve radiance and density for volumetric rendering.
While part-based field decomposition [46,63] approaches have been proposed, we demonstrate that decomposing im-plicit deformation fields into local fields improves repre-sentation capacity to model facial details. By filtering re-dundant input expression parameters to each local field and providing weak supervision via 3DMM, we achieve better, detailed local control, and modelling of asymmetric expres-sions (see Fig. 1). We provide qualitative and quantitative comparisons with state-of-the-art monocular head avatar synthesis methods and show that our approach reconstructs facial details more accurately, while improving local control of the face. In summary, our contributions are as follows: 1. A novel formulation that models local field deformation for implicit NeRF face rigs that provides fine-scale control with landmarks. 2. Local deformation fields surpass linear 3DMMâ€™s rep-resentation capacity via a novel local field control loss, and attention masks filtering. 3. We demonstrate the advantage of our approach in dif-ferent applications, including detailed asymmetric ex-pression control, and high-quality head synthesis with sharp mouth interior. 2.