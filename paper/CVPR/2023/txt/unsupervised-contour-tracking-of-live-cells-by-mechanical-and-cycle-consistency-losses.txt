Abstract
Analyzing the dynamic changes of cellular morphol-ogy is important for understanding the various functions and characteristics of live cells, including stem cells and metastatic cancer cells. To this end, we need to track all points on the highly deformable cellular contour in every frame of live cell video. Local shapes and textures on the contour are not evident, and their motions are complex, of-ten with expansion and contraction of local contour fea-tures. The prior arts for optical ﬂow or deep point set tracking are unsuited due to the ﬂuidity of cells, and pre-vious deep contour tracking does not consider point cor-respondence. We propose the ﬁrst deep learning-based tracking of cellular (or more generally viscoelastic mate-rials) contours with point correspondence by fusing dense representation between two contours with cross attention.
Since it is impractical to manually label dense tracking points on the contour, unsupervised learning comprised of the mechanical and cyclical consistency losses is proposed to train our contour tracker. The mechanical loss forcing the points to move perpendicular to the contour effectively helps out. For quantitative evaluation, we labeled sparse tracking points along the contour of live cells from two live cell datasets taken with phase contrast and confocal
ﬂuorescence microscopes. Our contour tracker quantita-tively outperforms compared methods and produces quali-tatively more favorable results. Our code and data are pub-licly available at https://github.com/JunbongJang/ contour-tracking/ 1.

Introduction
During cell migration, cells change their morphology by expanding or contracting their plasma membranes continu-ously like viscoelastic materials [21]. The dynamic change in the morphology of a live cell is called cellular morpho-* Corresponding authors: kimtaekyun@kaist.ac.kr and kwonmoo.lee@childrens.harvard.edu
Figure 1. Visualization of contour tracking results. Dense point correspondences between adjacent contours are shown with white arrows overlaid on the ﬁrst frame. The ﬁrst frame’s contour points are in dark green, and the last frame’s contour points are in red.
Only half of the contour points and correspondences are shown for visualization purposes. The trajectories of a few tracked points are shown on the right. dynamics and ranges from cellular to the subcellular move-ment of contour at varying spatiotemporal scales. While cellular morphodynamics plays a vital role in angiogene-sis, immune response, stem cell differentiation, and cancer invasiveness [6, 17], it is challenging to understand the var-ious functions of cellular morphodynamics because its un-characterized heterogeneity could mask crucial mechanistic details. As an initial step to understanding cellular morpho-dynamics, cellular morphodynamics is quantiﬁed by track-ing every point along the cellular contour (contour track-ing) and estimating their velocity [12, 13, 21]. Then, quan-tiﬁcation of cellular morphodynamics is further processed by other downstream machine learning tasks to characterize the drug-sensitive morphodynamic phenotypes with distinct molecular mechanisms [6,20,36]. Because contour tracking (e.g., Fig. 1) is the important ﬁrst step, the tracking accuracy
is crucial in this live cell analysis.
There are two main difﬁculties involved with contour tracking of a live cell. First, the live cell’s contour exhibits visual features that can be difﬁcult to distinguish by human eyes, meaning that a pixel and its neighboring pixels have similar color values or features. Optical ﬂow [14, 31] can track every pixel in the current frame by assuming that the corresponding pixel in the next frame will have the same distinct feature, but this assumption is not sufﬁcient to ﬁnd corresponding pixels given cellular visual features. Second, the expansion and contraction of the cellular contour change the total number of tracking points due to one point split-ting into many points or many points converging into one.
PoST [24] tracks a ﬁxed number of a sparse set of points that cannot accurately represent the ﬂuctuating shape of the cellular contour. Other deep contour tracking or video seg-mentation methods [10, 27, 39] do not provide dense point-to-point correspondence information between a contour and its next contour.
Previous cellular contour tracking method (mechanical model) [21] evades the ﬁrst problem by taking the segmen-tation of the cell body as inputs instead of raw images.
Then, it ﬁnds the dense correspondences of all points be-tween two contours by minimizing the normal torsion force and linear spring force with the Marquard-Levenberg algo-rithm [23]. However, the mechanical model has limited ac-curacy because it does not consider visual features in raw images. Also, its linear spring force which keeps every dis-tance between points the same is less effective during the expansion and contraction of the cell, as shown in our ex-periments (see Tab. 1).
Therefore, we present a deep learning-based contour tracker that can overcome these difﬁculties. Our contour tracker is comprised of a feature encoder, two cross atten-tions [35], and a fully connected neural network (FCNN) for offset regression, as shown in Fig. 2. Given two consec-utive images and their contours represented as a sequence of points, our contour tracker encodes the visual features of two images and samples their feature at the location of contours. The sampling makes our contour tracker focus on contour features and reduces the noise from irrelevant features unlike optical ﬂow [14]. The cross attention [35] fuses the sampled features from two contours globally and locally and regresses the offset for each contour point of the
ﬁrst frame. To obtain the dense point-to-point correspon-dences between the current and the next contours, offset points from the current contour are matched with the closest contour points in the next frame. In every frame, some con-tour points merge due to contraction, so new contour points emerge in the next frame as shown in Fig. 1. With dense point-to-point correspondences, new contour points in the next contour are also tracked. The proposed architectural design achieves the best accuracy among variants, including circular convolutions [26], and correspondence matrix [4].
To the best of our knowledge, this is the ﬁrst deep learning-based contour tracking with dense point-to-point correspon-dences for live cells.
In this contour tracking, supervised learning is not feasi-ble because it is difﬁcult to label every point of the contour manually. Instead, we propose to train our contour tracker solely by unsupervised learning comprised of mechanical and cycle consistency losses. Inspired by the mechanical model [21] that minimizes the normal torsion and linear spring force, we introduce the mechanical losses to end-to-end learning. The mechanical-normal loss that keeps the angle difference small between the offset point and the di-rection normal to the cellular contour played a signiﬁcant role in boosting accuracy. Also, we implement cycle consis-tency loss to encourage all contour points tracked forward-then-backward to return to their original location. How-ever, previous approaches such as PoST [24] and Anima-tion Transformer (AnT) [4] rely on supervised learning in addition to cycle consistency loss or ﬁnd mid-level corre-spondences [38] instead of pixel-level correspondences.
We evaluate our contour tracker on the live cell dataset taken with a phase contrast microscope [13] and another live cell dataset taken with a confocal ﬂuorescence micro-scope [36]. For a quantitative comparison of contour track-ing methods, we labeled sparse tracking points on the con-tour of live cells for all sampled frames.
In total, we la-beled 13 live cell videos for evaluation. Evaluation with a sparse set of points is motivated by the fact that if tracking of dense contour points is accurate, tracking any one of con-tour points should be accurate also. We also qualitatively show our contour tracker works on another viscoelastic or-ganism, jellyﬁsh [30]. Our contributions are summarized as follows.
• We propose the ﬁrst deep learning-based model that tracks cellular contours densely while surpassing the accuracy of other methods.
• We present an unsupervised learning strategy by me-chanical loss and cycle consistency loss for contour tracking.
• We demonstrate that the use of forward and backward cross attention with cycle consistency has a synergistic effect on ﬁnding accurate dense correspondences.
• We label tracking points in the live cell videos and quantitatively evaluate cellular contour tracking for the
ﬁrst time.
It
It+1
Encoder hare
Shared
Shared hare
Shared
Encoder
Point Features
Query
Forward
Cross 
Attention
…
FCNN
Ot(cid:1111)t+1
Key,
Value
Shared
…
Query
Backward
Cross 
Attention
FCNN
Ot+1(cid:1111)t
Ct
Cycle 
Loss
F o r w a r d (cid:307) d r a w k c a
B
Ct+1 ni t
Normal 
Loss
A
Linear 
Loss
Feature maps
Figure 2. Our architecture on the left and unsupervised learning losses on the right. Shared encoder comprised of VGG16 and FPN encodes ﬁrst and second images. Point features are sampled at the location of ordered contour points indicated by rainbow colors from red to purple. Point features are inputted as query or key and value to the cross attentions. Lastly, shared FCNN takes the fused features and regresses forward Ot→t+1 or backward Ot+1→t offsets. The cycle consistency, mechanical-normal, and mechanical-linear losses are shown in red color. 2.