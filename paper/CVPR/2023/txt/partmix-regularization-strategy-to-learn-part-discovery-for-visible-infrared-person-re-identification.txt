Abstract
Modern data augmentation using a mixture-based tech-nique can regularize the models from overfitting to the training data in various computer vision applications, but a proper data augmentation technique tailored for the part-based Visible-Infrared person Re-IDentification (VI-ReID) models remains unexplored.
In this paper, we present a novel data augmentation technique, dubbed
PartMix, that synthesizes the augmented samples by mixing the part descriptors across the modalities to improve the performance of part-based VI-ReID models. Especially, we synthesize the positive and negative samples within the same and across different identities and regularize the backbone model
In addition, we also present an entropy-based mining strat-egy to weaken the adverse impact of unreliable positive and negative samples. When incorporated into existing part-based VI-ReID model, PartMix consistently boosts the performance. We conduct experiments to demonstrate the effectiveness of our PartMix over the existing VI-ReID methods and provide ablation studies. through contrastive learning. 1.

Introduction
Person Re-IDentification (ReID), aiming to match per-son images in a query set to ones in a gallery set cap-tured by non-overlapping cameras, has recently received substantial attention in numerous computer vision applica-tions, including video surveillance, security, and persons analysis [64, 76]. Many ReID approaches [1, 2, 25, 26, 32, 41, 59, 73, 78] formulate the task as a visible-modality re-trieval problem, which may fail to achieve satisfactory re-sults under poor illumination conditions. To address this, most surveillance systems use an infrared camera that can capture the scene even in low-light conditions. However,
*Corresponding author
This research was supported by the National Research Founda-tion of Korea (NRF) grant funded by the Korea government (MSIP) (NRF2021R1A2C2006703). (a) MixUp [69] (b) CutMix [68] (c) PartMix (Ours)
Figure 1. Comparison of data augmentation methods for VI-ReID. (a) MixUp [69] using a global image mixture and (b) Cut-Mix [68] using a local image mixture can be used to regularize a model for VI-ReID, but these methods provide limited perfor-mances because they yield unnatural patterns or local patches with only background or single human part. Unlike them, we present (c) PartMix using a part descriptor mixing strategy, which boosts the VI-ReID performance (Best viewed in color). directly matching these infrared images to visible ones for
ReID poses additional challenges due to an inter-modality variation [60, 62, 65].
To alleviate these inherent challenges, Visible-Infrared person Re-IDentification (VI-ReID) [5–7, 12, 24, 35, 48, 54, 60–62, 66] has been popularly proposed to handle the large intra- and inter-modality variations between visible images and their infrared counterparts. Formally, these ap-proaches first extract a person representation from whole
visible and infrared images, respectively, and then learn a modality-invariant feature representation using feature alignment techniques, e.g., triplet [5,7,24,60–62] or ranking criterion [12, 66], so as to remove the inter-modality varia-tion. However, these global feature representations solely focus on the most discriminative part while ignoring the di-verse parts which are helpful to distinguish the person iden-tity [53, 63].
Recent approaches [53, 57, 63] attempted to further en-hance the discriminative power of person representation for
VI-ReID by capturing diverse human body parts across dif-ferent modalities. Typically, they first capture several hu-man parts through, e.g., horizontal stripes [63], cluster-ing [53], or attention mechanisms [57] from both visible and infrared images, extract the features from these human parts, and then reduce inter-modality variation in a part-level feature representation. Although these methods re-duce inter-modality variation through the final prediction (e.g., identity probability), learning such part detector still leads to overfitting to the specific part because the model mainly focuses on the most discriminative part to classify the identity, as demonstrated in [4, 15, 31, 52]. In addition, these parts are different depending on the modality, it accu-mulates errors in the subsequent inter-modality alignment process, which hinders the generalization ability on unseen identity in test set.
On the other hand, many data augmentation [19, 22, 38, 45, 68, 69] enlarge the training set through the image mix-ture technique [69]. They typically exploit the samples that linearly interpolate the global [38, 44, 69] or local [19, 68] images and label pairs for training, allowing the model to have smoother decision boundaries that reduce overfit-ting to the training samples. This framework also can be a promising solution to reduce inter-modality variation by mixing the different modality samples to mitigate overfit-ting to the specific modality, but directly applying these techniques to part-based VI-ReID models is challenging in that they inherit the limitation of global and local image mixture methods (e.g., ambiguous and unnatural patterns, and local patches with only background or single human part). Therefore, the performance of part-based VI-ReID with these existing augmentations would be degraded.
In this paper, we propose a novel data augmentation tech-nique for VI-ReID task, called PartMix, that synthesizes the part-aware augmented samples by mixing the part de-scriptors. Based on the observation that learning with the unseen combination of human parts may help better regu-larize the VI-ReID model, we randomly mix the inter- and intra-modality part descriptors to generate positive and neg-ative samples within the same and across different identi-ties, and regularize the model through the contrastive learn-ing. In addition, we also present an entropy-based mining strategy to weaken the adverse impact of unreliable posi-tive and negative samples. We demonstrate the effective-ness of our method on several benchmarks [33,55]. We also provide an extensive ablation study to validate and analyze components in our model. 2.