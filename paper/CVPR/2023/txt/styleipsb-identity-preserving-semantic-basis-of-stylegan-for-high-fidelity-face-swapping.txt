Abstract
Recent researches reveal that StyleGAN can generate highly realistic images, inspiring researchers to use pre-trained StyleGAN to generate high-fidelity swapped faces.
However, existing methods fail to meet the expectations in two essential aspects of high-fidelity face swapping. Their results are blurry without pore-level details and fail to pre-serve identity for challenging cases. To overcome the above artifacts, we innovatively construct a series of identity-preserving semantic bases of StyleGAN (called StyleIPSB) in respect of pose, expression, and illumination. Each ba-sis of StyleIPSB controls one specific semantic attribute and disentangles with the others. The StyleIPSB constrains style code in the subspace of W+ space to preserve pore-level details and gives us a novel tool for high-fidelity face swapping, and we propose a three-stage framework for face swapping with StyleIPSB. Firstly, we transform the
*Co-Corresponding authors
†The research is supported in part by the National Natural Science
Foundation of China (61972342,61832016,61972341,61902277) target facial images’ attributes to the source image. We learn the mapping from 3D Morphable Model (3DMM) pa-rameters, which capture the prominent semantic variance, to the coordinates of StyleIPSB that show higher identity-preserving and fidelity. Secondly, to transform detailed at-tributes which 3DMM does not capture, we learn the resid-ual attribute between the reenacted face and the target face.
Finally, the face is blended into the background of the tar-get image. Extensive results and comparisons demonstrate that StyleIPSB can effectively preserve identity and pore-level details. The results of face swapping can achieve state-of-the-art performance. We will release our code at https://github.com/a686432/StyleIPSB 1.

Introduction
Facial image manipulation [36, 37, 48, 50] is a task of transforming specific attributes from the source image to the target image while persevering other attributes unchanged.
Face swapping is one of the essential parts of facial im-age manipulation, which has attracted lots of interest in the computer vision and graphics community. Face swapping aims to generate an image with the source image’s iden-tity and the target image’s attributes (e.g., expression, pose, background, hair, etc.). It has wide applications in the film industry and computer games.
Current face swapping methods are mainly divided into two categories: 3D model-based methods and 2D image-based methods. 3D model-based methods [11,17,40] firstly reconstruct the 3D face models based on 3DMM from the source image and target image and transfer the non-identity parameters of the target face model to the source face model. Then they render the transferred 3D model and blend it into the target image. Although such methods can transfer coarse facial attributes such as pose, expression, and illumination, they have difficulty in generating realis-tic hair and teeth accessories.
With the development of generative adversarial net-works, 2D image-based methods [6, 7, 22, 27, 28, 45] can synthesize photo-realistic images. The generated face im-ages have convincing and detailed facial attributes, such as mouth, teeth, and eyebrows. Recent works [21, 46, 47, 51] employ the pre-trained StyleGAN decoder to further im-prove the fidelity and synthesize pore-level details. How-ever, as shown in Fig. 1, despite using the pre-trained Style-GAN model, their results fail to generate pore-level details and identity-preserving in challenge conditions. Overall, 2D image-based methods generate more realistic images than 3D model-based methods, but the identity-preserving and pore-level details of the images still need improvement.
To tackle the challenges of blurry images and identity-preserving in face swapping, according to our observations, the cause of the blurred images is that the regressed style code is out of W+ space. Additionally, as mentioned in
[18], identity embedding is a non-smooth space, so finding the identity-preserving optimized direction is challenging.
To address these problems, our method constrains the re-gressed style code with identity-preserving semantic bases of StyleGAN (i.e., the proposed StyleIPSB). StyleIPSB stays within the W+ space, and the identity is preserved when changing its coordinates.
The advantages of the proposed StyleIPSB are summa-rized as follows: (1) StyleIPSB constitutes a linear space, which is the subspace of the W+ space of StyleGAN. By ensuring the regressed style code within the W+ space of
StyleGAN, we can more easily generate images with pore-(2) When changing the coordinates of the level details.
StyleIPSB, the identity remains preserved as much as possi-ble. (3) StyleIPSB can represent various poses, expressions, and illuminations. To construct the basis that satisfies the above properties, we propose a novel identity-preserving distance metric to find the orthogonal semantic directions, which are further assembled to StyleIPSB.
StyleIPSB also cooperates well with 3DMM to control facial attributes. StyleRig [39] builds the mapping of the 3DMM parameter space and W+ space of StyleGAN, which can change the facial attribute of the generated image by the 3DMM parameters. StyleRig only can manipulate im-ages generated by StyleGAN. Pie [38] designs a non-linear optimization problem to edit the real-world image based on
StyleRig, but the optimization operation is time-consuming.
GIF [12] generates face images by the FLAME [23] para-metric control. However, the generated images are easy to contain artifacts and change identity. Other face manipu-lation methods [4, 29, 37] use the network directly to find the edit direction. Still, without the guidance of 3DMM, they can only generate some basic expressions (e.g., smile) and fail to cover various expressions. In this paper, we pro-pose the StyleGAN-3DMM mapping network, which trans-forms the semantic information of 3DMM parameters into
StyleIPSB coordinates. The StyleGAN-3DMM mapping network reduces the gap between 3DMM and StyleIPSB.
It shows that StyleIPSB is very compatible with 3DMM.
In summary, we propose a face swapping framework based on StyleIPSB and achieve state-of-the-art results. The main contributions of this paper lie in the following three aspects:
• We propose a novel method of establishing identity-preserving semantic bases of StyleGAN called
StyleIPSB. The face image, generated by the linear space of StyleIPSB, remains pore-level details and identity-preserving.
• The proposed StyleGAN-3DMM mapping network serves as the bridge to narrow the gap between 3DMM and StyleIPSB, which can take advantage of the promi-nent semantic variance of 3DMM and the identity-preserving and high-fidelity of styleIPSB.
• We propose the face swapping framework based on
StyleIPSB and StyleGAN-3DMM mapping network.
Extensive results show our method outperforms others in detail-preserving and identity-preserving. 2.