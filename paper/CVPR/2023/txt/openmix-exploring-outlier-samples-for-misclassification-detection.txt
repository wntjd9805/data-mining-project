Abstract
Reliable confidence estimation for deep neural classi-fiers is a challenging yet fundamental requirement in high-stakes applications. Unfortunately, modern deep neural networks are often overconfident for their erroneous predic-tions. In this work, we exploit the easily available outlier samples, i.e., unlabeled samples coming from non-target classes, for helping detect misclassification errors. Partic-ularly, we find that the well-known Outlier Exposure, which is powerful in detecting out-of-distribution (OOD) samples from unknown classes, does not provide any gain in identi-fying misclassification errors. Based on these observations, we propose a novel method called OpenMix, which incor-porates open-world knowledge by learning to reject uncer-tain pseudo-samples generated via outlier transformation.
OpenMix significantly improves confidence reliability un-der various scenarios, establishing a strong and unified framework for detecting both misclassified samples from known classes and OOD samples from unknown classes.
The code is publicly available at https://github. com/Impression2805/OpenMix. 1.

Introduction
Human beings inevitably make mistakes, so do ma-chine learning systems. Wrong predictions or decisions can cause various problems and harms, from financial loss to injury and death. Therefore, in risk-sensitive applications such as clinical decision making [14] and autonomous driv-ing [29, 63], it is important to provide reliable confidence to avoid using wrong predictions, in particular for non-specialists who may trust the computational models with-out further checks. For instance, a disease diagnosis model should hand over the input to human experts when the pre-diction confidence is low. However, though deep neural net-works (DNNs) have enabled breakthroughs in many fields, they are known to be overconfident for their erroneous pre-*Corresponding author.
Illustration of advantages of counterexample data for
Figure 1. reliable confidence estimation. The misclassified image has the most determinative and shortcut [18] features from class #1 (i.e., suit). Counterexample teaches the model the knowledge of what is not adult even if it has suit, which could help reduce model’s confidence on wrong predictions. dictions [25, 62], i.e., assigning high confidence for ① mis-classified samples from in-distribution (ID) and ② out-of-distribution (OOD) samples from unknown classes.
In recent years, many efforts have been made to enhance the OOD detection ability of DNNs [2, 13, 15, 23, 26, 39], while little attention has been paid to detecting misclassi-fied errors from known classes. Compared with the widely studied OOD detection problem, misclassification detection (MisD) is more challenging because DNNs are typically more confident for the misclassified ID samples than that for
OOD data from a different distribution [19]. In this paper, we focus on the under-explored MisD, and propose a sim-ple approach to help decide whether a prediction is likely to be misclassified, and therefore should be rejected.
Towards developing reliable models for detecting mis-classification errors, we start by asking a natural question:
Why are human beings good at confidence estimation?
A crucial point is that humans learn and predict in con-text, where we have abundant prior knowledge about other entities in the open world. According to mental models
[11,31,54] in cognitive science, when assessing the validity or evidence of a prediction, one would retrieve counterex-amples, i.e., which satisfy the premise but cannot lead to the conclusion. In other words, exploring counterexamples from open world plays an important role in establishing re-Inspired by liable confidence for the reasoning problem.
this, we attempt to equip DNNs with the above ability so that they can reduce confidence for incorrect predictions.
Specifically, we propose to leverage outlier data, i.e., un-labeled random samples from non-target classes, as coun-terexamples for overconfidence mitigation. Fig. 1 presents an intuitive example to illustrate the advantages of outlier samples for reducing the confidence of misclassification.
To leverage outlier samples for MisD, we investigate the well-known Outlier Exposure (OE) [26] as it is extremely popular and can achieve state-of-the-art OOD detection per-formance. However, we find that OE is more of a hin-drance than a help for identifying misclassified errors. Fur-ther comprehensive experiments show that existing popular
OOD detection methods can easily ruin the MisD perfor-mance. This is undesirable as misclassified errors widely exist in practice, and a model should be able to reliably reject those samples rather than only reject OOD samples from new classes. We observe that the primary reason for the poor MisD performance of OE and other OOD meth-ods is that: they often compress the confidence region of
ID samples in order to distinguish them from OOD sam-ples. Therefore, it becomes difficult for the model to further distinguish correct samples from misclassified ones.
We propose a learning to reject framework to leverage outlier data. ① Firstly, unlike OE and its variants which force the model to output a uniform distribution on all train-ing classes for each outlier sample, we explicitly break the closed-world classifier by adding a separate reject class for outlier samples. ② To reduce the distribution gap between
ID and open-world outlier samples, we mix them via sim-ple linear interpolation and assign soft labels for the mixed samples. We call this method OpenMix. Intuitively, the pro-posed OpenMix can introduce the prior knowledge about what is uncertain and should be assigned low confidence.
We provide proper justifications and show that OpenMix can significantly improve the MisD performance. We would like to highlight that our approach is simple, agnostic to the network architecture, and does not degrade accuracy when improving confidence reliability.
In summary, our primary contributions are as follows:
• For the first time, we propose to explore the effective-ness of outlier samples for detecting misclassification errors. We find that OE and other OOD methods are useless or harmful for MisD.
• We propose a simple yet effective method named
OpenMix, which can significantly improve MisD per-formance with enlarged confidence separability be-tween correct and misclassified samples.
• Extensive experiments demonstrate that OpenMix sig-nificantly and consistently improves MisD. Besides, it also yields strong OOD detection performance, serv-ing as a unified failure detection method. 2.