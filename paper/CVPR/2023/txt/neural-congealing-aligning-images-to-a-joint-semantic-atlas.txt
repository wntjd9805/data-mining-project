Abstract 1.

Introduction
We present Neural Congealing – a zero-shot self-supervised framework for detecting and jointly aligning semantically-common content across a given set of images.
Our approach harnesses the power of pre-trained DINO-ViT features to learn: (i) a joint semantic atlas – a 2D grid that captures the mode of DINO-ViT features in the input set, and (ii) dense mappings from the unified atlas to each of the input images. We derive a new robust self-supervised framework that optimizes the atlas representa-tion and mappings per image set, requiring only a few real-world images as input without any additional input infor-mation (e.g., segmentation masks). Notably, we design our losses and training paradigm to account only for the shared content under severe variations in appearance, pose, back-ground clutter or other distracting objects. We demon-strate results on a plethora of challenging image sets in-cluding sets of mixed domains (e.g., aligning images depict-ing sculpture and artwork of cats), sets depicting related yet different object categories (e.g., dogs and tigers), or do-mains for which large-scale training data is scarce (e.g., coffee mugs). We thoroughly evaluate our method and show that our test-time optimization approach performs favor-ably compared to a state-of-the-art method that requires ex-tensive training on large-scale datasets. Project webpage: https://neural-congealing.github.io/
Humans can easily associate and match semantically-related objects across images, even under severe variations in appearance, pose and background content. For exam-ple, by observing the images in Fig. 1, we can immediately focus and visually compare the different butterflies, while ignoring the rest of the irrelevant content. While compu-tational methods for establishing semantic correspondences have seen a significant progress in recent years, research ef-forts are largely focused on either estimating sparse match-ing across multiple images (e.g., keypoint detection), or es-tablishing dense correspondences between a pair of images.
In this paper, we consider the task of joint dense semantic alignment of multiple images. Solving this long-standing task is useful for a variety of applications, ranging from editing image collections [36,56], browsing images through canonical primitives, and 3D reconstruction (e.g., [8, 47]).
The task of joint image alignment dates back to the sem-inal congealing [13, 14, 20, 28], which aligns a set of im-ages into a common 2D space. Recently, GANgealing [36] has modernized this approach for congealing an entire do-main of images. This is achieved by leveraging a pre-trained
GAN to generate images that serve as self-supervisory sig-nal. Specifically, their method jointly learns both the mode of the generated images in the latent space of the GAN, and a network that predicts the mappings of the images into the
joint mode. GANgealing demonstrated impressive results on in-the-wild image sets. Nevertheless, their method re-quires a StyleGAN model pre-trained on the domain of the test images, e.g., aligning cat images requires training Style-GAN on a large-scale cat dataset. This is a challenging task by itself, especially for unstructured image domains or un-curated datasets [34]. Moreover, they require additional ex-tensive training for learning the mode and their mapping network (e.g., training on millions of generated images).
In this work, we take a different route and tackle the joint alignment task in the challenging setting where only a test image set is available, without any additional training data.
More specifically, given only a few images as input (e.g.,
<25 images), our method estimates the mode of the test set and their joint dense alignment, in a self-supervised manner.
We assume the input images share a common semantic content, yet may depict various factors of variations, such as pose, appearance, background content or other distracting objects (e.g., Mugs in Fig. 3). We take inspiration from the tremendous progress in representation learning, and lever-age a pre-trained DINO-ViT – a Vision Transformer model trained in a self-supervised manner [4]. DINO-ViT features have been shown to serve as an effective visual descriptor, capturing localized and semantic information (e.g., [2, 49]).
Here, we propose a new self-supervised framework that jointly and densely aligns the images in DINO-ViT feature space. To the best of our knowledge, we are the first to har-ness the power of DINO-ViT for dense correspondences be-tween in-the-wild images. More specifically, given an im-age set, our framework estimates, at test-time: (i) a joint la-tent 2D atlas that represents the mode of DINO-ViT features across the images, and (ii) dense mappings from the atlas to each of the images. Our training objective is driven by a matching loss encouraging each image features to match the canonical learned features in the joint atlas. We further incorporate additional loss terms that allow our framework to robustly represent and align only the shared content in the presence of background clutter or other distracting objects.
Since our atlas and mappings are optimized per set, our method works in a zero-shot manner and can be applied to a plethora of image sets, including sets of mixed do-mains (e.g., aligning images depicting sculpture and art-work of cats), sets depicting related yet different object categories (e.g., dogs and tigers), or domains for which a dedicated generator is not available (e.g., coffee mugs).
We thoroughly evaluate our method, and demonstrate that our test-time optimization framework performs favorably compared to [36] and on-par with state-of-the-art self-supervised methods. We further demonstrate how our atlas and mappings can be used for editing the image set with minimal effort by automatically propagating edits that are applied to a single image to the entire image set. 2.