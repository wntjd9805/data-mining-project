Abstract
Stage 1: Unsupervised Pretraining
Unlabeled Pool
Stage 2: Supervised Finetuning
Given the large-scale data and the high annotation cost, pretraining-ﬁnetuning becomes a popular paradigm in mul-tiple computer vision tasks. Previous research has covered both the unsupervised pretraining and supervised ﬁnetuning in this paradigm, while little attention is paid to exploiting the annotation budget for ﬁnetuning. To ﬁll in this gap, we formally deﬁne this new active ﬁnetuning task focusing on the selection of samples for annotation in the pretraining-ﬁnetuning paradigm. We propose a novel method called Ac-tiveFT for active ﬁnetuning task to select a subset of data distributing similarly with the entire unlabeled pool and maintaining enough diversity by optimizing a parametric model in the continuous space. We prove that the Earth
Mover’s distance between the distributions of the selected subset and the entire data pool is also reduced in this pro-cess. Extensive experiments show the leading performance and high efﬁciency of ActiveFT superior to baselines on both image classiﬁcation and semantic segmentation. Our code is released at https://github.com/yichen928/ActiveFT. 1.

Introduction
Recent success of deep learning heavily relies on abun-dant training data. However, the annotation of large-scale datasets often requires intensive human labor. This dilemma inspires a popular pretraining-ﬁnetuning paradigm where models are pretrained on a large amount of data in an unsu-pervised manner and ﬁnetuned on a small labeled subset.
Existing literature pays signiﬁcant attention to both the unsupervised pretraining [7, 13, 14, 18] and supervised ﬁne-tuning [26].
In spite of their notable contributions, these researches build upon an unrealistic assumption that we al-ready know which samples should be labeled. As shown in
Fig. 1, given a large unlabeled data pool, it is necessary to pick up the most useful samples to exploit the limited an-In most cases, this selected subset only notation budget.
Encoder
Encoder
Task 
Module
Finetune
Data
Selection
Selected
Samples
Oracle
Our Focus: Active Finetuning Task giraffe horse
Labeled Pool
Figure 1. Pretraining-Finetuning Paradigm: We focus on the selection strategy of a small subset from a large unlabeled data pool for annotation, named as active ﬁnetuning task, which is under-explored for a long time. counts a small portion (e.g. <10%) of this large unlabeled pool. Despite the long-standing under-exploration, the se-lection strategy is still crucial since it may signiﬁcantly af-fect the ﬁnal results.
Active learning algorithms [34, 38, 39] seem to be a potential solution, which aims to select the most suit-able samples for annotation when models are trained from scratch. However, their failures in this pretraining-ﬁnetuning paradigm are revealed in both [4] and our ex-periments (Sec. 4.1). A possible explanation comes from the batch-selection strategy of most current active learn-ing methods. Starting from a random initial set, this strat-egy repeats the model training and data selection processes multiple times until the annotation budget runs out. De-spite their success in from-scratch training, it does not ﬁt this pretraining-ﬁnetuning paradigm well due to the typi-cally low annotation budget, where too few samples in each batch lead to harmful bias inside the selection process.
To ﬁll in this gap in the pretraining-ﬁnetuning paradigm, we formulate a new task called active ﬁnetuning, concen-trating on the sample selection for supervised ﬁnetuning. In this paper, a novel method, ActiveFT, is proposed to deal with this task. Starting from purely unlabeled data, Ac-tiveFT fetches a proper data subset for supervised ﬁnetuning in a negligible time. Without any redundant heuristics, we
directly bring close the distributions between the selected subset and the entire unlabeled pool while ensuring the di-versity of the selected subset. This goal is achieved by con-tinuous optimization in the high-dimensional feature space, which is mapped with the pretrained model.
We design a parametric model pθS to estimate the distri-bution of the selected subset. Its parameter θS is exactly the high-dimensional features of those selected samples.
We optimize this model via gradient descent by minimizing our designed loss function. Unlike traditional active learn-ing algorithms, our method can select all the samples from scratch in a single-pass without iterative batch-selections.
We also mathematically show that the optimization in the continuous space can exactly reduce the earth mover’s dis-tance (EMD) [35, 36] between the entire pool and selected subset in the discrete data sample space.
Extensive experiments are conducted to evaluate our method in the pretraining-ﬁnetuning paradigm. After pre-training the model on ImageNet-1k [37], we select subsets of data from CIFAR-10, CIFAR-100 [23], and ImageNet-1k [37] for image classiﬁcation, as well as ADE20k [50] for semantic segmentation. Results show the signiﬁcant perfor-mance gain of our ActiveFT in comparison with baselines.
Our contributions are summarized as follows:
• To our best knowledge, we are the ﬁrst to iden-tify the gap of data selection for annotation and supervised ﬁnetuning in the pretraining-ﬁnetuning paradigm, which can cause inefﬁcient use of annota-tion budgets as also veriﬁed in our empirical study.
Meanwhile, we formulate a new task called active ﬁne-tuning to ﬁll in this gap.
• We propose a novel method, ActiveFT, to deal with the active ﬁnetuning task through parametric model opti-mization which theoretically reduces the earth mover’s distance (EMD) between the distributions of the se-lected subset and entire unlabeled pool. To our best knowledge, we are the ﬁrst to directly optimize sam-ples to be selected in the continuous space for data se-lection tasks.
• We apply ActiveFT to popular public datasets, achiev-ing leading performance on both classiﬁcation and seg-mentation tasks. In particular, our ablation study re-sults justify the design of our method to ﬁll in the data selection gap in the pretraining-ﬁnetuning paradigm.
The source code will be made public available. 2.