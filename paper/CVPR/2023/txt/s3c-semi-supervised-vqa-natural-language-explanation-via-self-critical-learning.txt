Abstract
VQA Natural Language Explanation (VQA-NLE) task aims to explain the decision-making process of VQA mod-els in natural language. Unlike traditional attention or gra-dient analysis, free-text rationales can be easier to under-stand and gain users’ trust. Existing methods mostly use post-hoc or self-rationalization models to obtain a plau-sible explanation. However, these frameworks are bottle-necked by the following challenges: 1) the reasoning pro-cess cannot be faithfully responded to and suffer from the problem of logical inconsistency. 2) Human-annotated ex-planations are expensive and time-consuming to collect. In this paper, we propose a new Semi-Supervised VQA-NLE via Self-Critical Learning (S3C), which evaluates the can-didate explanations by answering rewards to improve the logical consistency between answers and rationales. With a semi-supervised learning framework, the S3C can ben-efit from a tremendous amount of samples without human-annotated explanations. A large number of automatic mea-sures and human evaluations all show the effectiveness of our method. Meanwhile, the framework achieves a new state-of-the-art performance on the two VQA-NLE datasets. 1.

Introduction
Deep neural networks have enabled significant break-throughs in a variety of vision-language (VL) tasks such as image captioning [10, 47] and visual question answer-ing (VQA) [2, 39]. Unfortunately, most of them are black box systems, which makes it challenging to gain users’ trust [20]. Explaining the decision-making process of deep VL models is a long-standing and essential problem.
*These authors contributed equally to this work.
†Corresponding authors.
Figure 1. Paradigm comparison of different VQA-NLE meth-ods. (a) Post-hoc explanation method adopts two independent models to predict answers and explanations respectively. (b) Self-rationalization method uses a united VL model to simultaneously generate answers and explanations. (c) Our self-critical strategy utilizes answer scores as rewards and obtains more reliable ratio-nales with semi-supervised learning.
Some approaches depend on attention mechanisms [2, 30] or gradient-based localization [50] to acquire visual expla-nations, which can highlight some contributing image re-gions for the predicted answers. However, simple visualiza-tion cannot explain how these areas support the answers and they are also hard to comprehend [20, 48]. Conversely, Nat-ural Language Explanation (NLE) task [6, 38] can explain the decision-making process of a model by generating a nat-ural language sentence. The language-based explanations are more accessible for users to understand, and they can also help researchers optimize the structure of models [34].
Recently, some models of NLE in the VL commu-nity have achieved pretty-well results, especially for VQA-NLE [20, 34, 41, 48, 58]. They can guide models to generate natural language sentences and interpret how the models get answers. Specifically, the first research line usually treats
VQA-NLE as a predict-then-explain task [20, 34, 41, 58], namely post-hoc explanations method. As shown in Fig. 1 (a), these methods first depend on pre-trained VL models (such as UNITER [8] or Oscar [25]) to gain answers. Then the fused multi-modal features and the predicted answers are fed into a separated language model (e.g., LSTM [16] or Transformer [54]) to generate corresponding explana-tions. As shown in Fig. 1 (b), the other line [48] relies on a united VL model while generating both answers and expla-nations, which is known as the self-rationalization method.
This framework can simultaneously predict an answer and generate a rationale by formulating the answer as a text-generation task along with the explanation.
Though significant progress has been made, the two paradigms are still restricted by the following challenges: 1) For the first paradigm, since the decision-making model and interpretation part are two separate modules, it would inevitably lead to unfaithful responses to the reasoning pro-cess of the decision models. 2) Due to the lack of explic-itly logical relationship modeling, previous work [19] has proved that the straightforward self-rationalization frame-works suffer from the problem of logical inconsistency. 3) The above strategies all require an amount of human-annotated explanations, which are expensive and time-consuming to collect [62].
To solve the above challenges, inspired by [5, 51], we argue that a reasonable rationale can assist the model in ob-taining a correct answer, and vice versa, the answer can be converted as an evaluation criterion for possible explana-In this paper, we propose a new Semi-Supervised tions.
VQA-NLE method with Self-Critical learning, which is called S3C for short. As shown in Fig.1 (c), given im-ages and related questions, we first leverage a prompting mechanism to construct answer and explanation templates, which can guide the pre-trained VL model to generate an-swers and multiple candidate explanations based on se-quence sampling [2]. Then we design a new self-critical method that converts the answer scores as rewards and en-courages the model to generate the explanations which con-tribute to improving the answer scores.
In particular, to reduce the dependency on expensive human annotations, we further extend our method to the semi-supervised ver-sion, which utilizes the unlabelled samples 1 (i.e., conven-tional VQA data [4, 36]) to significantly enhance the self-interpretability of the model. With the self-critical strategy and the semi-supervised learning, our method effectively models the logical relationships and promotes the logical 1In this paper, we use “unlabelled samples” and “labelled samples” to indicate the question-answer (QA) pairs without/with human explanations. consistency between answer-explanation pairs. According to automatic measures and human evaluations, the S3C outperforms the state-of-the-art models for the VQA-NLE task on the widely used two datasets and provides a new
In summary, we make the paradigm for our community. following contributions: 1) We propose a new self-critical VQA-NLE method that can model the logical relationships between answer-explanation pairs and evaluate the generated rationales by answering rewards. This strategy effectively improves the logical consistency and the reliability of the interpretations. 2) We develop an advanced semi-supervised learning framework for VQA-NLE, which utilizes amounts of sam-ples without human-annotated explanations to boost the self-interpretability of the model further. To the best of our knowledge, we are the first to explore semi-supervised learning on the VQA Natural Language Explanation. 3) The proposed S3C achieves new state-of-the-art per-formance on VQA-X [13] and A-OKVQA [49] benchmark datasets. Meanwhile, automatic measures and human eval-uations all show the effectiveness of our method. 2.