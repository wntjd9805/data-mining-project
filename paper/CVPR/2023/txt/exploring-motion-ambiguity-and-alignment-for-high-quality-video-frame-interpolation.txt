Abstract
For video frame interpolation (VFI), existing deep-learning-based approaches strongly rely on the ground-truth (GT) intermediate frames, which sometimes ignore the non-unique nature of motion judging from the given adja-cent frames. As a result, these methods tend to produce averaged solutions that are not clear enough. To alleviate this issue, we propose to relax the requirement of recon-structing an intermediate frame as close to the GT as possi-ble. Towards this end, we develop a texture consistency loss (TCL) upon the assumption that the interpolated content should maintain similar structures with their counterparts in the given frames. Predictions satisfying this constraint are encouraged, though they may differ from the prede-fined GT. Without the bells and whistles, our plug-and-play
TCL is capable of improving the performance of existing
VFI frameworks consistently. On the other hand, previous methods usually adopt the cost volume or correlation map to achieve more accurate image or feature warping. How-ever, the O(N 2) (N refers to the pixel count) computational complexity makes it infeasible for high-resolution cases. In this work, we design a simple, efficient O(N ) yet power-ful guided cross-scale pyramid alignment (GCSPA) module, where multi-scale information is highly exploited. Exten-sive experiments justify the efficiency and effectiveness of the proposed strategy. 1.

Introduction
Video frame interpolation (VFI) plays a critical role in computer vision with numerous applications, such as video editing and novel view synthesis. Unlike other vision tasks that heavily rely on human annotations, VFI benefits from the abundant off-the-shelf videos to generate high-quality training data. The recent years have witnessed the rapid development of VFI empowered by the success of deep neural networks. The popular approaches can be roughly divided into two categories: 1) optical-flow-based meth-*Corresponding author ods [1, 8, 15–17, 20, 26–28, 30, 31, 39, 44–46, 49, 51, 53] and 2) kernel-regression -based algorithms [4–6, 22, 32, 33, 37].
The optical-flow-based methods typically warp the im-ages/features based on a linear or quadratic motion model and then complete the interpolation by fusing the warped results. Nevertheless, it is not flexible enough to model the real-world motion under the linear or quadratic as-sumption, especially for cases with long-range correspon-dence or complex motion. Besides, occlusion reasoning is a challenging problem for pixel-wise optical flow estimation.
Without the prerequisites above, the kernel-based methods handle the reasoning and aggregation in an implicit way, which adaptively aggregate neighboring pixels from the im-ages/features to generate the target pixel. However, this line stands the chance of failing to tackle the high-resolution frame interpolation or large motion due to the limited recep-tive field. Thereafter, deformable convolutional networks, a variant of kernel-based methods, are adopted to aggre-gate the long-term correspondence [5, 7, 22], achieving bet-ter performance. Despite many attempts, some challenging issues remain unresolved.
First, the deep-learning-based VFI works focus on learn-ing the predefined ground truth (GT) and ignore the inherent motion diversity across a sequence of frames. As illustrated in Fig. 1 (a), given the positions of a ball in frames I−1 and
I1, we conduct a user study of choosing its most possible position in the intermediate frame I0. The obtained proba-bility distribution map clearly clarifies the phenomenon of motion ambiguity in VFI. Without considering this point, existing methods that adopt the pixel-wise L1 or L2 supervi-sion possibly generate blurry results, as shown in Fig. 1 (b).
To resolve this problem, we propose a novel texture con-sistency loss (TCL) that relaxes the rigid supervision of GT while ensuring texture consistency across adjacent frames.
Specifically, for an estimated patch, apart from the prede-fined GT, we look for another texture-matched patch from the input frames as a pseudo label to jointly optimize the network. In this case, predictions satisfying the texture con-sistency are also encouraged. From the visualization com-Figure 1. Analysis of motion ambiguity in VFI. (a) User study of querying the location of a ball in the intermediate frame I0 with the observed two input frames {I−1, I1}. The results are visualized in a probability distribution map. (b) Visual comparison between
SepConv [33] and our method with/without the proposed texture consistency loss (TCL). (c) Quantitative evaluation of the two methods with/without TCL loss on Vimeo-Triplets [47] and Middlebury [2] benchmarks. parison of SepConv [33] and our model with/without TCL 1 in Fig. 1 (b), we observe that the proposed TCL leads to clearer results. Besides, as shown in Fig. 1 (c), it is seen that our TCL brings about considerable PSNR improvement on Vimeo-Triplets [47] and Middlebury [2] benchmarks for both two methods. More visual examples are available in our supplementary materials.
Second, the cross-scale aggregation during alignment is not fully exploited in VFI. For example, PDWN [5] con-ducts an image-level warping using the gradually refined offsets. However, the single-level alignment may not take full advantage of the cross-scale information, which has been proven useful in many low-level tasks [23, 25, 52].
To address this issue, some recent works [5, 13, 31] have considered multi-scale representations for VFI. Feflow [31] adopts a PCD alignment proposed by EDVR [42] that per-forms a coarse-to-fine aggregation for long-range motion estimation. Specifically, the fusion of image features is conducted at two adjacent levels, without considering dis-tant cross-scale aggregation.
In this work, we propose a novel guided cross-scale pyramid alignment (GCSPA) mod-ule, which performs bidirectional temporal alignment from low-resolution stages to higher ones. In each step, the previ-ously aligned low-scale features are regarded as a guidance for the current-level warping. To aggregate the multi-scale information, we design an efficient fusion strategy rather than building the time-consuming cost volume or corre-lation map. Extensive quantitative and qualitative exper-iments verify the effectiveness and efficiency of the pro-posed method.
In a nutshell, our contributions are three-fold:
• Texture consistency loss: Inspired by the motion am-biguity in VFI, we design a novel texture consistency loss to allow the diversity of interpolated content, pro-ducing clearer results. 1The four models are trained on Vimeo-Triplets [47] dataset.
• Guided cross-scale pyramid alignment: The pro-posed alignment strategy utilizes the multi-scale infor-mation to conduct a more accurate and robust motion compensation while requiring few computational re-sources.
• State-of-the-art performance: The extensive experi-ments including frame interpolation and extrapolation have demonstrated the superior performance of the proposed algorithm. 2.