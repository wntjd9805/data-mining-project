Abstract
Temporal modeling is crucial for multi-frame human pose estimation. Most existing methods directly employ full-optical flow or deformable convolution to predict spectrum motion fields, which might incur numerous irrele-vant cues, such as a nearby person or background. Without further efforts to excavate meaningful motion priors, their results are suboptimal, especially in complicated spatio-temporal interactions. On the other hand, the temporal difference has the ability to encode representative motion information which can potentially be valuable for pose es-timation but has not been fully exploited. In this paper, we present a novel multi-frame human pose estimation frame-work, which employs temporal differences across frames to model dynamic contexts and engages mutual information objectively to facilitate useful motion information disen-tanglement. To be specific, we design a multi-stage Tem-poral Difference Encoder that performs incremental cas-caded learning conditioned on multi-stage feature differ-ence sequences to derive informative motion representa-tion. We further propose a Representation Disentanglement module from the mutual information perspective, which can grasp discriminative task-relevant motion signals by explic-itly defining useful and noisy constituents of the raw motion features and minimizing their mutual information. These place us to rank No.1 in the Crowd Pose Estimation in Com-plex Events Challenge on benchmark dataset HiEve, and achieve state-of-the-art performance on three benchmarks
PoseTrack2017, PoseTrack2018, and PoseTrack21. 1.

Introduction
Human pose estimation has long been a nontrivial and fundamental problem in the computer vision community.
*Corresponding Author
Figure 1. Directly leveraging optical flow can be distracted by irrelevant clues such as background and blur (a), and sometimes fails in scenarios with fast motion and mutual occlusion (b). Our proposed framework proceeds with temporal difference encoding and useful information disentanglement to capture more tailored temporal dynamics (c), yielding more robust pose estimations (d).
The goal is to localize anatomical keypoints (e.g., nose, an-kle, etc.) of human bodies from images or videos. Nowa-days, as more and more videos are recorded endlessly, video-based human pose estimation has been extremely de-sired in enormous applications including live streaming, augmented reality, surveillance, and movement tracking
[14, 15, 24, 35, 44].
An extensive body of literature focuses on estimating human poses in static images, ranging from earlier meth-ods employing pictorial structure models [43, 51, 55, 69] to recent attempts leveraging deep convolutional neural net-works [35, 49, 56, 58] or Vision Transformers [32, 61, 65].
Despite the impressive performance in still images, the ex-tension of such methods to video-based human pose estima-tion still remains challenging due to the additional temporal dimension in videos [34, 57]. By nature, the video presents distinctive and valuable dynamic contexts (i.e., the tempo-ral evolution in the visual content) [71]. Therefore, being able to effectively utilize the temporal dynamics (motion information) is fundamentally important for accurate pose
estimation in videos [35].
One line of work [35, 37, 54] attempts to derive a uni-fied spatial-temporal representation through implicit motion compensation. [54] presents a 3DHRNet which utilizes 3D convolutions to extract spatiotemporal features of a video tracklet to estimate pose sequences. [35] adopts deformable convolutions to align multi-frame features and aggregates aligned feature maps to predict human poses. On the other hand, [40, 45, 67] explicitly model motion contexts with op-tical flow. [40, 45] propose to compute dense optical flow between every two frames and leverage the flow features for refining pose heatmaps temporally across multiple frames.
Upon studying the previous methods [34, 35, 40, 45], we empirically observe that the pose estimation performance is boosted with the implicit or explicit imposition of mo-tion priors. However, the movement of any visual evidence is usually attended to in these paradigms, resulting in clut-tered motion features that include numerous irrelevant in-formation (e.g., nearby person, background), as illustrated in Fig. 1. Directly exploiting such vanilla motion features delivers inferior results, especially in complex scenarios of mutual occlusion and fast motion. More specifically, not all pixel movements are equally important in video-based hu-man pose estimation [66]. For example, background vari-ations and pixel changes caused by image quality degrada-tion (e.g., blur and occlusion) are usually useless and dis-tracting, whereas the salient pixel movements driven by human body motions play a more important role in un-derstanding motion patterns [21]. Therefore, discovering meaningful motion dynamics is crucial to fully recovering human poses across a video. On the other hand, investigat-ing temporal differences across video frames allows one to discover representative motion cues [25, 52, 59]. Although it has already shown success in various video-related tasks (action recognition [52], video super-resolution [24]), its application on video-based human pose estimation remains under-explored. based
In this paper, we present a novel framework, named
Temporal Difference Learning on Mutual
Information (TDMI) for human pose estimation. Our (i) A multi-TDMI consists of two key components: stage Temporal Difference Encoder (TDE) is designed to model motion contexts conditioned on multi-stage feature differences among video frames. Specifically, we first compute the feature difference sequences across multiple stages by leveraging a temporal difference operator. Then, we perform incremental cascaded learning via intra-and inter-stage feature integration to derive the motion representation. (ii) We further introduce a Representation
Disentanglement module (RDM) from the mutual infor-mation perspective, which distills the task-relevant motion features to enhance the frame representation for pose
In particular, we first disentangle the useful estimation. and noisy constituents of the vanilla motion representation by activating corresponding feature channels. Then, we theoretically analyze the statistical dependencies between the useful and the noisy motion features and arrive at an information-theoretic loss. Minimizing this mutual information objective encourages the useful motion com-ponents to be more discriminative and task-relevant. Our approach achieves significant and consistent performance improvements over current state-of-the-art methods on four benchmark datasets. Extensive ablation studies are conducted to validate the efficacy of each component in the proposed method.
The main contributions of this work can be summa-rized as follows: (1) We propose a novel framework that leverages temporal differences to model dynamic contexts for video-based human pose estimation. (2) We present a disentangled representation learning strategy to grasp dis-criminative task-relevant motion signals via an information-theoretic objective. (3) We demonstrate that our approach achieves new state-of-the-art results on four benchmark datasets, PoseTrack2017, PoseTrack2018, PoseTrack21, and HiEve. 2.