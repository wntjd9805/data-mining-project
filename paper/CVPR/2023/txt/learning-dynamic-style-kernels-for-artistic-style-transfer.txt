Abstract 1.

Introduction
Arbitrary style transfer has been demonstrated to be ef-ficient in artistic image generation. Previous methods ei-ther globally modulate the content feature ignoring local details, or overly focus on the local structure details lead-ing to style leakage. In contrast to the literature, we propose a new scheme “style kernel” that learns spatially adaptive kernels for per-pixel stylization, where the convolutional kernels are dynamically generated from the global style-content aligned feature and then the learned kernels are applied to modulate the content feature at each spatial posi-tion. This new scheme allows flexible both global and local interactions between the content and style features such that the wanted styles can be easily transferred to the content im-age while at the same time the content structure can be eas-ily preserved. To further enhance the flexibility of our style transfer method, we propose a Style Alignment Encoding (SAE) module complemented with a Content-based Gating
Modulation (CGM) module for learning the dynamic style kernels in focusing regions. Extensive experiments strongly demonstrate that our proposed method outperforms state-of-the-art methods and exhibits superior performance in terms of visual quality and efficiency.
Artistic style transfer [48] refers to a hot computer vi-sion technology that allows us to recompose the content of an image in the style of an artistic work. Figure 1 shows several vivid examples. We might have ever imagined what a photo might look like if it were painted by a famous artist like Pablo Picasso or Van Gogh. Now style transfer is the computer vision technique that turns this into a reality. It has great potential values in various real-world applications and therefore attracts a lot of researchers to constantly put efforts to make progress towards both quality and efficiency.
Most of existing style transfer works [6, 15, 18, 26, 33] either globally modulate the content feature ignoring local details or overly focus on the local structure details leading to style leakage. In particular, [15,18] seeks to match global statistics between content and style images, resulting in in-consistent stylizations that either remain large parts of the content unchanged or contain local content with distorted style patterns. SANet [33] and AdaAttN [31] improve the content similarity by learning attention maps that match the semantics between the style and stylized images, but these methods tend to distort object instances when improper style patterns are involved. Recently, IEC [5] adopts con-trastive learning to pull close both the content and style rep-resentations between input and output images. MAST [16]
seeks to balance the style and content via manifold align-ment. Although both IEC and MAST have achieved some success in most cases, they are still far from satisfactory to well balance two important requirements, i.e., style consis-tency and structure similarity.
In this paper, we develop a novel style transfer frame-work in a manner of encoder-decoder structure, as illus-trated in Figure 2, which contains two important modules: (1) a Style Alignment Encoding (SAE) module enhanced by
Content-based Gating Modulation (CGM), which is used to generate content-style aligned features, and (2) a Style Ker-nel Generation (SKG) module used to transform the output features of SAE to convolutional kernels. In such a frame-work, we propose a new style transfer scheme, “style ker-nel”, which treats the features of the style images as dy-namic convolutional kernels and then transfer the style in-formation to the content image by convolving the content image by the learned dynamic style kernels. This allows fine-grained local interactions between the style and content features, the flexibility of which makes both style transfer-ring and content structure preserving much easier.
To enforce the global correlation between the content and style features, we simulate the self-attention mecha-nism of Transformer [41] in the SAE module. This treats the content and style features as the queries and keys of the self-attention operator respectively and computes a large at-tention map the elements of which measure for each local content context the similarity to each local style context.
With the attention map, we can aggregate for each pixel of the content image all the style features in a weighted man-ner to obtain content-style aligned features. We also design a Content-based Gating Modulation (CGM) operator that further thresholds the attention map and zeros the places where similarities are too small, seeking an adaptive num-ber of correlated neighbors as a focusing region for each query point. Then, we use another set of convolutions that constitute the SKG module to further transform the content-style aligned features, and view (reshape) the output of SKG as dynamic style kernels. In order to improve the efficiency of the dynamic convolutions, SKG predicts separable local filters (two 1D filters and a bias) instead of a spatial 2D filter that has more parameters. The learned style kernels already integrate the information of the given content and style im-ages. We further apply the learned dynamic style kernels to the content image feature for transferring the target style to the content image.
We shall emphasize that unlike “style code” in
AdaIN [15], DRB-GAN [48], and AdaConv [2] modeled as the dynamic parameters (e.g. mean, variance, convolution kernel) which are shared over all spatial positions globally without distinguishing the semantic regions, the learned dy-namic kernels via our “style kernel” are point-wisely mod-eled upon the globally style-content aligned feature, and therefore is able to make full use of the point-wise semantic structure correlation to modulate content feature for better artistic style transfer. Our learned dynamic style kernels are good at transferring the globally aggregated and locally semantic-aligned style features to local regions. This en-sures our style transfer model that works well on consis-tent stylization with well preserved structure, overcoming the shortages of existing style transfer methods which either globally modulate the content feature ignoring local details (e.g. AdaIN, WCT [28]) or overly focus on the local struc-ture details leading to style leakage (e.g. AdaAttN [31] and
MAST [16]).
The main contributions of this work are 3-fold as fol-lows:
• We are the first to propose the “style kernel” scheme for artistic style transfer, which converts the globally style-content alignment features into position point-wisely dynamic convolutional kernels to convolve the features of content images together for style transfer.
• We design a novel architecture, i.e. SAE and CGM, to generate the dynamic style kernels by employing the correlation between the content and style image adap-tively.
• Extensive experiments demonstrate that our method produces visually plausible stylization results with fine-grained style details and coherence content with the input content images. 2.