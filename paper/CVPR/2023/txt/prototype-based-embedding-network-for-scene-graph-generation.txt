Abstract
Current Scene Graph Generation (SGG) methods ex-plore contextual information to predict relationships among entity pairs. However, due to the diverse visual appearance of numerous possible subject-object combinations, there is a large intra-class variation within each predicate cat-egory, e.g., “man-eating-pizza, giraffe-eating-leaf”, and the severe inter-class similarity between different classes, e.g., “man-holding-plate, man-eating-pizza”, in model’s la-tent space. The above challenges prevent current SGG methods from acquiring robust features for reliable rela-tion prediction.
In this paper, we claim that the predi-cate’s category-inherent semantics can serve as class-wise prototypes in the semantic space for relieving the chal-lenges. To the end, we propose the Prototype-based Embed-ding Network (PE-Net), which models entities/predicates with prototype-aligned compact and distinctive representa-tions and thereby establishes matching between entity pairs and predicates in a common embedding space for relation recognition. Moreover, Prototype-guided Learning (PL) is introduced to help PE-Net efficiently learn such entity-predicate matching, and Prototype Regularization (PR) is devised to relieve the ambiguous entity-predicate match-ing caused by the predicate’s semantic overlap. Exten-sive experiments demonstrate that our method gains su-perior relation recognition capability on SGG, achieving new state-of-the-art performances on both Visual Genome and Open Images datasets. The codes are available at https://github.com/VL-Group/PENET. 1.

Introduction
Scene Graph Generation (SGG) is a fundamental com-puter vision task that involves detecting the entities and predicting their relationships in an image to generate a scene graph, where nodes indicate entities and edges in-*Equal contribution.
†Corresponding author.
Figure 1. The illustration of relation representations with large intra-class variation and severe inter-class similarity. Left: the feature distribution of “eating” (in red) and “holding” (in blue) obtained by Motifs [39]. Right: some instances of “eating” and
“holding”. Examples (c) and (e) illustrate that relation instances from the same class have diverse appearance. Moreover, examples (a) and (c) demonstrate that similar-looking relation instances may belong to different categories. dicate relationships between entity pairs. Such a graph-structured representation is helpful for downstream tasks such as Visual Question Answering [5, 15, 41], Image Cap-tioning [4, 38, 42, 45], and Image Retrieval [10, 25, 40].
Existing SGG models [1, 3, 7, 13, 18, 29, 39] typically start with an object detector that generates a set of entity proposals and corresponding features. Then, entity fea-tures are enhanced by exploring the contextual informa-tion taking advantage of message-passing modules. Fi-nally, these refined entity features are used to predict pair-wise relations. Although many works have made great ef-forts to explore the contextual information for robust re-lation recognition, they still suffer from biased-prediction problems, preferring common predicates (e.g.,“on”, “of”) instead of fine-grained ones (e.g.,“walking on”, “cover-ing”). To address the problem, various de-biasing frame-works [6, 14, 21, 28, 34, 37, 46, 47] have been proposed to obtain balanced prediction results. While alleviating the long-tailed issue to some extent, most of them only achieve a trade-off between head and tail predicates. In other words, they sacrifice the robust representations learned on head
predicates for unworthy improvements in the tail ones [46], which do not truly improve the model’s holistic recognition ability for most of the relations.
The origin of the issue lies in the fact that current SGG methods fail to capture compact and distinctive represen-tations for relations. For instance, as shown in Fig. 1, the relation representation, derived from Motifs’ latent space, is heavily discrete and intersecting. Hence, it makes exist-ing SGG models hard to learn perfect decision boundaries for accurate predicate recognition. Accordingly, we sum-marize the issue as two challenges: large Intra-class vari-ation within the same relation class and severe Inter-class similarity between different categories.
Intra-class variation. The intra-class variation arises from the diverse appearance of entities and various subject-object combinations. Specifically, entities’ visual appear-ances change greatly even though they belong to the same class. Thus, represented as the union feature containing subject and object entities, relation representations signif-icantly vary with the appearances of entity instances, e.g., various visual representations for “pizza” in Fig. 1(c) vs.
Fig. 1(d). Besides, the numerous subject-object combina-tions of predicate instances further increase the variation within each predicate class, e.g., “man-eating-pizza” vs.
“giraffe-eating-leaf” in Fig. 1(c) and Fig. 1(e).
Inter-class similarity. The inter-class similarity of rela-tions originates from similar-looking interactions but be-longs to different predicate classes. For instance, as shown in Fig. 1(a) and Fig. 1(c), the similar visual appearance of interactions between “man-pizza” and “man-plate” make current SGG models hard to distinguish “eating” from
“holding”, even if they are semantic irrelevant to each other.
The above challenges motivate us to study two problems: 1) For the intra-class variation, how to capture category-inherent features, producing compact representations for entity/predicate instances from the same category. More-over, 2) for the inter-class similarity, how to derive distinc-tive representations for effectively distinguishing similar-looking relation instances between different classes. Our key intuition is that semantics is more reliable than visual appearance when modeling entities/predicates. Intuitively, although entities/predicates of the same class significantly vary in visual appearance, they all share the representa-tive semantics, which can be easily captured from their class labels. Dominated by the representative semantics, the representations of entities and predicates have smaller variations within their classes in the semantic space. Be-sides, the class-inherent semantics is discriminative enough for visual-similar instances between different categories.
Therefore, in conjunction with the above analysis, model-ing entities and predicates in the semantic space can provide highly compact and distinguishable representations against intra-class variation and inter-class similarity challenges.
Inspired by that, we propose a simple but effective method, Prototype-based Embedding Network (PE-Net), which produces compact and distinctive entity/predicate representations for relation recognition. To achieve that, the PE-Net models entity and predicate instances with com-pact and distinguishable representations in the semantic space, which are closely aligned to their semantic proto-types. Practically, the prototype is defined as the represen-tative embedding for a group of instances from the same entity/predicate class. Then, the PE-Net establishes match-ing between entity pairs (i.e., subject-object (s, o)) and their corresponding predicates (p) for relation recognition (i.e.,
F(s, o) ≈ p). Besides, a Prototype-guided Learning strat-egy (PL) is proposed to help PE-Net efficiently learn this entity-predicate matching. Additionally, to alleviate the am-biguous entity-predicate matching caused by the semantic overlap between predicates (e.g., “walking on” and “stand-ing on”), Prototype Regularization (PR) is proposed to en-courage inter-class separation between predicate prototypes for precise entity-predicate matching. Finally, we introduce two metrics, i.e., Intra-class Variance (IV) and Intra-class to
Inter-class Variance Ratio (IIVR), to measure the compact-ness and distinctiveness of entity/predicate representations, respectively.
In summary, the main contributions of our work are three folds:
• We propose a simple yet effective method, i.e.,
Prototype-based Embedding Network (PE-Net), which produces compact and distinctive entity/predicate rep-resentations and then establishes matching between entity pairs and predicates for relation recognition.
• Moreover, Prototype-guided Learning (PL) is intro-duced to help PE-Net efficiently learn such entity-predicate matching, and Prototype Regularization (PR) is devised to relieve the ambiguous entity-predicate matching caused by the predicate’s semantic overlap.
• Evaluated on the Visual Genome and Open Images datasets, our method significantly increases the rela-tion recognition ability for SGG, achieving new state-of-the-art performances. 2.