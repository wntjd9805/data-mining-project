Abstract tion cuts in and out.
This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal input corruption situations where audio inputs and visual inputs are both corrupted, which is not well addressed in previous research directions. Previ-ous studies have focused on how to complement the cor-rupted audio inputs with the clean visual inputs with the assumption of the availability of clean visual inputs. How-ever, in real life, clean visual inputs are not always acces-sible and can even be corrupted by occluded lip regions or noises. Thus, we firstly analyze that the previous AVSR mod-els are not indeed robust to the corruption of multimodal input streams, the audio and the visual inputs, compared to uni-modal models. Then, we design multimodal input cor-ruption modeling to develop robust AVSR models. Lastly, we propose a novel AVSR framework, namely Audio-Visual
Reliability Scoring module (AV-RelScore), that is robust to the corrupted multimodal inputs. The AV-RelScore can de-termine which input modal stream is reliable or not for the prediction and also can exploit the more reliable streams in prediction. The effectiveness of the proposed method is evaluated with comprehensive experiments on popular benchmark databases, LRS2 and LRS3. We also show that the reliability scores obtained by AV-RelScore well reflect the degree of corruption and make the proposed model fo-cus on the reliable multimodal representations. 1.

Introduction
Imagine you are watching the news on Youtube.
Whether the recording microphone is a problem or the video encoding is wrong, the anchor’s voice keeps breaking off, so you cannot hear well. You try to understand her by her lip motions, but making matters worse, the microphone keeps covering her mouth, so the news is hardly recognizable.
These days, people often face these kinds of situations, even in video conferences or interviews where the internet situa-*Both authors have contributed equally to this work.
†Corresponding author
As understanding speech is the core part of human com-munication, there have been a number of works on speech recognition [1, 2], especially based on deep learning. These works have tried to enhance audio representation for recog-nizing speech in a noisy situation [3–6] or to utilize addi-tional visual information for obtaining complementary ef-fects [7–12]. Recently, even technologies that comprehend speech from only visual information have been developed
[13–21].
With the research efforts, automatic speech recogni-tion technologies including Audio Speech Recognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech Recognition (AVSR) are achieving great de-velopments with outstanding performances [22–24]. With the advantages of utilizing multimodal inputs, audio and vi-sual, AVSR that can robustly recognize speech even in a noisy environment, such as in a crowded restaurant, is ris-ing for the future speech recognition technology. However, the previous studies have mostly considered the case where the audio inputs are corrupted and utilizing the additional clean visual inputs for complementing the corrupted audio information. Looking at the case, we come up with an im-portant question, what if both visual and audio information are corrupted, even simultaneously? In real life, like the aforementioned news situation, cases where both visual and audio inputs are corrupted alternatively or even simultane-ously, are frequently happening. To deal with the question, we firstly analyze the robustness of the previous ASR, VSR, and AVSR models on three different input corruption situa-tions, 1) audio input corruption, 2) visual input corruption, and 3) audio-visual input corruption. Then, we show that the previous AVSR models are not indeed robust to audio-visual input corruption and show even worse performances than uni-modal models, which is eventually losing the ben-efit of utilizing multimodal inputs.
To maximize the superiority of using multimodal sys-tems over the uni-modal system, in this paper, we propose a novel multimodal corruption modeling method and show its importance in developing robust AVSR technologies for
diverse input corruption situations including audio-visual corruption. To this end, we model the visual corruption with lip occlusion and noises that are composed of blurry frames and additive noise perturbation, along with the au-dio corruption modeling. Then, we propose a novel AVSR framework, namely Audio-Visual Reliability Scoring mod-ule (AV-RelScore), that can evaluate which modal of the current input representations is more reliable than others.
The proposed AV-RelScore produces the reliability scores for each time step, which represent how much the current audio features and the visual features are helpful for rec-ognizing speech. With the reliability scores, meaningful speech representations can be emphasized at each modal stream. Then, through multimodal attentive encoder, the emphasized multimodal representations are fused by con-sidering inter-modal relationships. Therefore, with the AV-RelScore, the AVSR model can refer to the audio stream when the given visual stream is determined as less reliable (i.e., corrupted), and vice versa. We provide the audio-visual corruption modeling for the reproducibility and the future research.1
Our key contributions are as follows:
• To the best of our knowledge, this is the first attempt to analyze the robustness of deep learning-based AVSR under the corruption of multimodal inputs including lip occlusions.
• We propose an audio-visual corruption modeling method and show that it is key for developing robust
AVSR technologies under diverse environments.
• We propose Audio-Visual Reliability Scoring module (AV-RelScore) to figure out whether the current input modal is reliable or not, so that to robustly recognize the input speech even if one modality is corrupted, or even both.
• We conduct comprehensive experiments with ASR,
VSR, and AVSR models to validate the effectiveness of the proposed audio-visual corruption modeling and
AV-RelScore on LRS2 [25] and LRS3 [26], the largest audio-visual datasets obtained in the wild. 2.