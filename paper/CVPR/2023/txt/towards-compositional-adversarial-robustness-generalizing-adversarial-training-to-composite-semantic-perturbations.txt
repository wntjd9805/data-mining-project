Abstract
Model robustness against adversarial examples of single perturbation type such as the ℓp-norm has been widely stud-ied, yet its generalization to more realistic scenarios involv-ing multiple semantic perturbations and their composition remains largely unexplored. In this paper, we first propose a novel method for generating composite adversarial exam-ples. Our method can find the optimal attack composition by utilizing component-wise projected gradient descent and automatic attack-order scheduling. We then propose general-ized adversarial training (GAT) to extend model robustness from ℓp-ball to composite semantic perturbations, such as the combination of Hue, Saturation, Brightness, Contrast, and Rotation. Results obtained using ImageNet and CIFAR-10 datasets indicate that GAT can be robust not only to all the tested types of a single attack, but also to any combination of such attacks. GAT also outperforms baseline ℓ
-norm bounded adversarial training approaches by a significant margin.
∞ 1.

Introduction
Deep neural networks have shown remarkable success in a wide variety of machine learning (ML) applications, rang-ing from biometric authentication (e.g., facial image recog-nition), medical diagnosis (e.g., CT lung cancer detection) to autonomous driving systems (traffic sign classification), etc. However, while these models can achieve outstand-ing performance on benign data points, recent research has shown that state-of-the-art models can be easily fooled by malicious data points crafted intentionally with adversarial perturbations [37].
To date, the most effective defense mechanism is to incor-porate adversarial examples during model training, known as adversarial training (AT) [21, 48]. Nonetheless, current adversarial training approaches primarily only consider a single perturbation type (or threat model) quantified in a spe-cific distance metric (e.g., ℓp-ball). In this regard, the lack of exploration of the compositional adversarial robustness against a combination of several threat models could lead to impractical conclusions and undesirable bias in robustness evaluation. For example, a model that is robust to pertur-bations within ℓp-ball does not imply it can simultaneously be robust to other realistic semantic perturbations (e.g., hue, saturation, rotation, brightness, and contrast).
∞
To tackle this issue, in this paper, we propose generalized adversarial training (GAT), which can harden against a wide range of threat models, from single ℓ
-norm or se-mantic perturbation to a combination of them. Notably, extending standard adversarial training to composite adver-sarial perturbations is a challenging and non-trivial task, as each perturbation type is sequentially applied, and thus the attack order will affect the effectiveness of the composite adversarial example. To bridge this gap, we propose an effi-cient attack order scheduling algorithm to learn the optimal ordering of various perturbation types, which will then be incorporated into the GAT framework.
Different from existing works, this paper aims to address the following fundamental questions: (a) How to generalize adversarial training from a single threat model to multiple? (b) How to optimize the perturbation order from a set of semantic and ℓp-norm perturbations? (c) Can GAT outper-form other adversarial training baselines against composite perturbations?
Our main contributions in this paper provide affirmative answers to the questions: 1. We propose composite adversarial attack (CAA), a novel and unified approach to generate adversarial examples across from multiple perturbation types with attack-order-scheduling, including semantic perturbations (Hue, Satu-ration, Contrast, Brightness and Contrast) and ℓp-norm space. To the best of our knowledge, this paper is the first work that leverages a scheduling algorithm for finding the optimal attack order in composite adversarial attacks. 2. Building upon our composite adversarial attack frame-work, we propose generalized adversarial training (GAT) toward achieving compositional adversarial robustness,
(a) (b)
Figure 1. (a) Qualitative study: illustration of some perturbed examples generated by different attack combinations and their predictions by different ResNet50 models [10] on ImageNet, including standard training, Madry’s ℓ∞ robust training [21] and our proposed GAT. The results show that our proposed GAT can maintain robust accuracy under a variety of composite adversarial attacks, even with the increasing number of attacks. (b) Quantitative study: the attack success rate (ASR, %) of the above-mentioned models under multiple composite attacks (a higher ASR means less robust) on all correctly classified test samples for each model. The corresponding robust accuracy (RA) is listed in Table 3. which enables the training of neural networks robust to composite adversarial attacks. 3. For the attack part, our proposed composite adversarial at-tack exhibits a high attack success rate (ASR) against stan-dard or ℓ
-norm robust models. Moreover, our method with learned attack order significantly outperforms ran-dom attack ordering, giving an average 9% and 7% in-crease in ASR on CIFAR-10 and ImageNet.
∞ 4. For the defense part, comparing our GAT to other adver-sarial training baselines [20, 21, 42, 44, 48, 49], the results show the robust accuracy of GAT outperforms them by average 30% 22%
∼ on full attacks. 60% on semantic attacks and 15%
∼
To further motivate the effectiveness of our proposed GAT framework, Fig. 1 compares the performance of different models under selected attacks, ranging from a single threat
∞ to composite threats. The models include standard train--robust training, and our proposed GAT. The results ing, ℓ show the limitation of ℓ
-robust model [21], which is robust against the same-type adversarial attack, but becomes fragile against semantic adversarial attacks and their composition.
Our proposed GAT addresses this limitation by providing a novel training approach that is robust to any combination of multiple and adversarial threats.
∞ 2.