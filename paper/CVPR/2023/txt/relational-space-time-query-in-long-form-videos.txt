Abstract
Egocentric videos are often available in the form of un-interrupted, uncurated long videos capturing the camera wearers’ daily life activities.Understanding these videos re-quires models to be able to reason about activities, objects, and their interactions. However, current video benchmarks study these problems independently and under short, cu-rated clips.
In contrast, real-world applications, e.g. AR assistants, require bundling these problems for both model development and evaluation. In this paper, we propose to study these problems in a joint framework for long video understanding. Our contributions are three-fold. First, we propose an integrated framework, namely Relational
Space-Time Query (ReST), for evaluating video under-standing models via templated spatiotemporal queries. Sec-ond, we introduce two new benchmarks, ReST-ADL and
ReST-Ego4D 1, which augment the existing egocentric video datasets with abundant query annotations generated by the
ReST framework. Finally, we present a set of baselines and 1The latest version of our benchmark and models will be available here. in-depth analysis on the two benchmarks and provide in-sights about the query tasks. We view our integrated frame-work and benchmarks as a step towards comprehensive, multi-step reasoning in long videos, and believe it will fa-cilitate the development of next generations of video under-standing models. 1.

Introduction
Thanks to the advances of modern, massive parallel hardware, e.g. GPUs, and the availability of large datasets, significant progress has been made in the last few years with large language models (e.g., GPT-3 [6], BERT [11]) and image / video generative models (e.g., DALLE [40],
Imagen [41], Make-A-Video [46]). Meanwhile, current video understanding models mostly focus on processing short video clips [12, 13, 52] and solving basic perception tasks such as action recognition [16, 28, 31, 44, 48] and de-tection [7, 19]. One may ask the questions for video under-standing research: “How far are current models progressing to a human-level performance on video understanding?”, or
“What is blocking us from building models that can under-stand complex relationships in long videos?”
Of course, there exists multiple blockers in practice such as GPU memory limitation and inefficient hardware support for processing long videos. Yet the first and most important reason is always the lack of the right research problem and the right benchmark. One drawback of current video un-derstanding benchmarks [19, 28] is that they handle analy-sis of activities, objects and their interactions in a separate manner. However, understanding long-form videos usually requires a unified analysis of these factors because activi-ties manifesting within these uncurated videos are primar-ily in the form of human-object interaction, especially for egocentric recordings of a camera wearer’s daily lives [17].
In recent years, video-QA [18, 33, 49, 66] and video cap-tioning [10, 50] have been proposed as alternative tasks for video understanding. These tasks require models to under-stand both visual and text modalities and perform cross-modal reasoning. On one hand, such vision-language based tasks have the benefit of bypassing the pre-defined taxon-omy and closed-world assumptions by leveraging language as input and/or output. On the other hand, using language for vision tasks, either in the form of input query or output prediction, brings additional ambiguity in text generation and requires use of uninterpretable evaluation metrics (e.g.,
BLEU, ROUGE). Language priors also introduce bias to the task as observed in prior work that the language-only model achieves comparable results with the VQA ones [4, 18].
In this paper, we present a holistic framework, Relational
Space-Time Query (ReST), for evaluating video under-standing models via templated spatiotemporal queries. By combining analysis of activities, objects, and their interac-tions, our ReST framework captures much of the rich ex-pressivity of natural language query while avoiding the am-biguity and bias introduced by the language input / output.
Figure 1 illustrates an example of our ReST framework.
Given a long video spanning up to 30 minutes, we evalu-ate a video understanding model by asking various queries about the activities and human-object interactions occurred in the video. Unlike VQA that relies on language-based questions and answers, all of our queries and answers are constructed in the form of pre-defined templates with vi-sual or categorical input. Such a design helps the evaluation remain pure vision-focused and enjoy well-defined, well-posed evaluation metrics. Queries constructed in ReST can cover various questions with different levels of complexity.
As shown in the examples in Figure 1, the questions can be: “what activities did I do with the coffee mug?”, “what objects did I interact with when I was washing dishes?” or
“when did I take the pills stored in this specific bottle?”. We note that these questions are templated and only the time in square brackets, the activities in the double quotes, and the image crops in the curvy brackets are allowed to vary
In order to perform well on to form different questions. these query tasks, a model needs not only be able to process the long video efficiently, but is also required to understand the temporal dependencies of activities and the fine-grained human-object interactions across the video.
We summarize our contributions as follows. We present
Relational Space-Time Query (ReST), a holistic frame-work and benchmarks for long video understanding with detailed-reasoning tasks. We provide annotations for our
ReST queries on two existing benchmarks, ADL [37] and
Ego4D [17]. We conduct a set of comprehensive baselines and analysis for ReST to gain insights into the tasks. We find that even with the initial set of the three basic tasks, current state-of-the-art models are not meeting desired per-formance, which indicates the need for further research and opportunities in this field. 2.