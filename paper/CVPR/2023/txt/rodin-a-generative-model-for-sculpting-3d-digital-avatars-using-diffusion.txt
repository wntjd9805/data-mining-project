Abstract 1.

Introduction
This paper presents a 3D diffusion model that automat-ically generates 3D digital avatars represented as neural radiance fields (NeRFs). A significant challenge for 3D diffusion is that the memory and processing costs are pro-hibitive for producing high-quality results with rich details.
To tackle this problem, we propose the roll-out diffusion net-work (RODIN), which takes a 3D NeRF model represented as multiple 2D feature maps and rolls out them onto a sin-gle 2D feature plane within which we perform 3D-aware diffusion. The RODIN model brings much-needed compu-tational efficiency while preserving the integrity of 3D dif-fusion by using 3D-aware convolution that attends to pro-jected features in the 2D plane according to their origi-nal relationships in 3D. We also use latent conditioning to orchestrate the feature generation with global coherence, leading to high-fidelity avatars and enabling semantic edit-ing based on text prompts. Finally, we use hierarchical syn-thesis to further enhance details. The 3D avatars generated by our model compare favorably with those produced by ex-isting techniques. We can generate highly detailed avatars with realistic hairstyles and facial hair. We also demon-strate 3D avatar generation from image or text, as well as text-guided editability.
†Intern at Microsoft Research. ‡Corresponding author. ∗Equal contri-bution. Project Webpage: https://3d-avatar-diffusion.microsoft.com
Generative models [2, 34] are one of the most promising ways to analyze and synthesize visual data including 2D images and 3D models. At the forefront of generative mod-eling is the diffusion model [14, 24, 61], which has shown phenomenal generative power for images [19,47,50,52] and videos [23,59]. Indeed, we are witnessing a 2D content cre-ation revolution driven by the rapid advances of diffusion and generative modeling. In this paper, we aim to expand the applicability of diffusion to 3D digital avatars. We use
“digital avatars” to refer to the traditional avatars manually created by 3D artists, as opposed to the recently emerging photorealistic avatars [8, 43]. The reason for focusing on digital avatars is twofold. On the one hand, digital avatars are widely used in movies, games, the metaverse, and the 3D industry in general. On the other hand, the available digital avatar data is very scarce as each avatar must be painstakingly created by a specialized 3D artist using a so-phisticated creation pipeline [20, 35], especially for mod-eling hair and facial hair. All this leads to a compelling scenario for generative modeling.
We present a diffusion model for automatically produc-ing digital avatars represented as NeRFs [38], with each point describing its color radiance and density within the 3D volume. The core challenge in generating these avatars
is the prohibitive memory and computational cost required by high-quality avatars with rich details. Without rich de-tails, an avatar will always be somewhat “toy-like”. To tackle this challenge, we develop RODIN, the roll-out dif-fusion network. We take a NeRF model represented as mul-tiple 2D feature maps and roll out these maps onto a single 2D feature plane and perform 3D-aware diffusion within this plane. Specifically, we use the tri-plane representa-tion [9], which represents a volume by three orthogonal fea-ture planes. By simply rolling out the feature maps, RODIN can perform 3D-aware diffusion using an efficient 2D ar-chitecture and by drawing power from RODIN’s three key ingredients.
The first is the 3D-aware convolution. The 2D CNN processing used in conventional 2D diffusion cannot ef-fectively handle feature maps originating from orthogonal planes. Rather than treating the features as plain 2D input, the 3D-aware convolution explicitly accounts for the fact that a 2D feature in one plane (of the tri-plane) is a projec-tion from a piece of 3D data and is hence intrinsically as-sociated with the same data’s projected features in the other two planes. To encourage cross-plane communication, we involve all these associated features in the convolution and synchronize their detail synthesis according to their 3D re-lationship.
The second ingredient is latent conditioning. We use a latent vector to orchestrate the feature generation so that it is globally coherent in 3D, leading to better quality avatars and enabling semantic editing. We do this by using the avatars in the training dataset to train an additional image encoder which extracts a semantic latent vector serving as the condi-tional input to the diffusion model. This latent conditioning essentially acts as an autoencoder in orchestrating the fea-ture generation. For semantic editability, we adopt a frozen
CLIP image encoder [46] that shares the latent space with text prompts.
The final ingredient is hierarchical synthesis. We start by generating at low resolution (64 × 64), followed by a diffusion-based upsampling that yields a higher resolution (256 × 256). When training the diffusion upsampler, it is instrumental to penalize the image-level loss that we com-pute in a patch-wise manner.
RODIN supports several application scenarios. We can use the model to generate an unlimited number of avatars from scratch, each different from the others as well as those in the training data. As shown in Figure 1, we can generate highly detailed avatars with realistic hairstyles and facial hair styled as beards, mustaches, goatees, and sideburns.
Hairstyle and facial hair are essential parts of personal iden-tity yet have been notoriously difficult to model well with existing approaches. The RODIN model also allows avatar customization, with the resulting avatar capturing the visual characteristics of a person portrayed in an image or a textual description. Finally, our framework supports text-guided semantic editing.
Our work shows that 3D diffusion holds great model-ing power, and this power can be effectively unleashed by rolling out the feature maps onto a 2D plane, leading to rich details, including those highly desirable but extremely dif-ficult to produce with existing techniques. It is worth not-ing that while this paper focuses on RODIN’s application to avatars, the design of RODIN is not avatar specific. Indeed, we believe RODIN is applicable to general 3D scenes. 2.