Abstract
This paper introduces Content-aware Token Sharing (CTS), a token reduction approach that improves the com-putational efficiency of semantic segmentation networks that use Vision Transformers (ViTs). Existing works have proposed token reduction approaches to improve the effi-ciency of ViT-based image classification networks, but these methods are not directly applicable to semantic segmenta-tion, which we address in this work. We observe that, for semantic segmentation, multiple image patches can share a token if they contain the same semantic class, as they con-tain redundant information. Our approach leverages this by employing an efficient, class-agnostic policy network that predicts if image patches contain the same semantic class, and lets them share a token if they do. With experiments, we explore the critical design choices of CTS and show its ef-fectiveness on the ADE20K, Pascal Context and Cityscapes datasets, various ViT backbones, and different segmentation decoders. With Content-aware Token Sharing, we are able to reduce the number of processed tokens by up to 44%, without diminishing the segmentation quality. 1.

Introduction
In recent years, many works have proposed replacing
Convolutional Neural Networks (CNNs) with Vision Trans-formers (ViTs) [13] to solve various computer vision tasks, such as image classification [1, 13, 22, 23], object detec-tion [3, 23, 56] and semantic segmentation [6, 23, 28, 37, 47, 54]. ViTs1 apply multiple layers of multi-head self-attention [41] to a set of tokens generated from fixed-size image patches. ViT-based models now achieve state-of-the-art results for various tasks, and it is found that ViTs are es-pecially well-suited for pre-training on large datasets [1,16], which in turn yields significant improvements on down-*Both authors contributed equally. 1In this work we use the term ViTs for the complete family of vision transformers that purely apply global self-attention.
Figure 1. Content-aware token sharing (CTS). Standard ViT-based segmentation networks turn fixed-size patches into tokens, and process all of them. To improve efficiency, we propose to let semantically similar patches share a token, and achieve consider-able efficiency boosts without decreasing the segmentation quality. stream tasks. However, the use of global self-attention, which is key in achieving good results, means that the com-putational complexity of the model is quadratic with respect to the input tokens. As a result, these models become par-ticularly inefficient for dense tasks like semantic segmenta-tion, which are typically applied to larger images than for image classification.
Recent works address these efficiency concerns in two different ways. Some works propose new ViT-based archi-tectures to improve efficiency, which either make a combi-nation of global and local attention [23, 48] or introduce a pyramid-like structure inspired by CNNs [14,23,44]. Alter-natively, to reduce the burden of the quadratic complexity, some works aim to reduce number of tokens that are pro-cessed by the network, by either discarding [29, 39, 49] or merging [2, 21, 30] the least relevant tokens. These works, which all address the image classification task, find that a similar accuracy can be achieved when taking into account only a subset of all tokens, thereby improving efficiency.
However, these methods, which are discussed further in
Section 2, are not directly applicable to semantic segmenta-tion. First, we cannot simply discard certain tokens, as each
token represents an image region for which the semantic segmentation task requires predictions. Second, existing to-ken merging approaches allow any combination of tokens to be merged, through multiple stages of the network. As a result, ‘unmerging’ and spatial reorganization of tokens, which is necessary because semantic segmentation requires a prediction for each original token, is non-trivial.
In this work, we present a much simpler, yet highly ef-fective and generally applicable token reduction approach for ViT-based semantic segmentation networks. The goal of this approach is to improve the efficiency without decreas-ing the segmentation quality. Token reduction methods for image classification already found that merging redundant tokens only results in a limited accuracy drop [2, 21, 30].
For semantic segmentation, we observe that there are many neighboring patches that contain the same semantic class, and that the information contained by these patches is likely redundant. Therefore, we hypothesize that neighboring patches containing the same class can share a token with-out negatively impacting the segmentation quality of a se-mantic segmentation network. To leverage this, we propose an approach that reduces tokens by (1) using a policy that identifies which patches can share a token before any tokens enter the ViT, and (2) grouping only rectangular neighbor-ing regions, which allows for straightforward reassembling of tokens at the output of the ViT backbone. The main chal-lenge that we tackle is to find this policy without introduc-ing a heavy computational burden.
We propose to tackle this policy challenge by turning the problem into a simple binary classification task that can be solved by a highly efficient CNN model, our pol-icy network. Concretely, it predicts whether 2×2 neighbor-ing patches contain the same class, and lets these patches share a token if they do. Our complete approach, which we call Content-aware Token Sharing (CTS), first applies this policy network, then lets patches share tokens accord-ing to the predicted policy, feeds the reduced set of tokens through the ViT, and finally makes a semantic segmenta-tion prediction using these tokens (see Section 3). CTS has several advantages by design. First, it does not require any modifications to the ViT architecture or its training strategy for pre-training or fine-tuning. As a result, it is compati-ble with all backbones that purely use global self-attention, as well as any semantic segmentation decoder or advanced pre-training strategy, e.g., BEiT [1]. Second, by reducing the number of tokens before inputting them to the ViT, the efficiency improvement is larger than when token reduction is applied gradually in successive stages of the ViT, as done in existing methods for image classification [2, 49]. Both advantages are facilitated by our policy network, which is trained separately from the ViT, and is so efficient that it only introduces a marginal computational overhead for large ViTs applied to high-resolution images.
With experiments detailed in Section 4, we show that our
CTS approach can reduce the total number of processed to-kens by at least 30% without decreasing the segmentation quality, for multiple datasets. We also show that this holds for transformer backbones of many different sizes, initial-ized with different pre-trained weights, and using various segmentation decoders. For more detailed results, we refer to Section 5. With this work, we aim to provide a foundation for future research on efficient transformer architectures for per-pixel prediction tasks. The code for this work is avail-able through https://tue-mps.github.io/CTS.
To summarize, the contributions of this work are:
• A generally applicable token sharing framework that lets semantically similar neighboring image patches share a token, improving the efficiency of ViT-based semantic segmentation networks without reducing the segmentation quality.
• A content-aware token sharing policy network that ef-ficiently classifies whether a set of neighboring patches should share a token or not.
• A comprehensive set of experiments with which we show the effectiveness of the proposed CTS ap-proach on the ADE20K [55], Pascal Context [27], and
Cityscapes [10] datasets, for a wide range of different
ViTs and decoders. 2.