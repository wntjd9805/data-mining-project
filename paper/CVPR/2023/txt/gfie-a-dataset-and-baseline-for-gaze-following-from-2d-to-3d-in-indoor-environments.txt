Abstract
Gaze-following is a kind of research that requires locat-ing where the person in the scene is looking automatically under the topic of gaze estimation. It is an important clue for understanding human intention, such as identifying ob-jects or regions of interest to humans. However, a survey of datasets used for gaze-following tasks reveals defects in the way they collect gaze point labels. Manual labeling may introduce subjective bias and is labor-intensive, while auto-matic labeling with an eye-tracking device would alter the person’s appearance. In this work, we introduce GFIE, a novel dataset recorded by a gaze data collection system we developed. The system is constructed with two devices, an
Azure Kinect and a laser rangeﬁnder, which generate the laser spot to steer the subject’s attention as they perfor-m in front of the camera. And an algorithm is developed to locate laser spots in images for annotating 2D/3D gaze targets and removing ground truth introduced by the spot-s. The whole procedure of collecting gaze behavior allows us to obtain unbiased labels in unconstrained environments semi-automatically. We also propose a baseline method with stereo ﬁeld-of-view (FoV) perception for establishing a 2D/3D gaze-following benchmark on the GFIE dataset.
Project page: https://sites.google.com/view/ gfie. 1.

Introduction
Gaze-following is a human skill that emerges in infan-cy [36] to learn about visual focus of other people, which helps to understand their personal thoughts and intention-s [32]. For these reasons, detecting gaze targets automati-cally as humans do has great potential in some applications, (cid:2)Corresponding author
* This work is supported in part by National Key Research and De-velopment Project of China under Grant 2019YFB1310604, in part by Na-tional Natural Science Foundation of China under Grant 62173189. a) Manual annotation
Recording System b)Automatic annotation with  eye-tracking device do
CY d
CZ
CX
Laser Spot
Laser Rangefinder
Azure Kinect c) Our system for recording gaze behavior to build GFIE dataset
Figure 1. The way of collecting gaze data in the existing gaze-following dataset and our proposed scheme. a) is a sample from the GazeFollow [28] dataset, the blue dots indicate the gaze targets annotated by the different annotators. b) indicate the case where annotations are collected with an eye-tracking device. c) is the system designed in this paper. such as locating items of interest to a person in the retail en-vironment [35] and judging the risk of driving by detecting whether the driver is distracted [9, 16].
In addition, gaze target detection can assist in action recognition [40], so-cial relationship analysis [11, 41], autism diagnosis [7] and human-aware robot navigation [25].
As a device for monitoring gaze behavior, a wear-able eye-tracking device was explored for gaze-following
[23, 30]. [14, 23, 27] designed a custom system for track-ing gaze. These methods are only applicable to constrained scenes due to extra burdens they bring, such as complex calibration and additional expense. This challenge has also attracted the attention of researchers in the computer vision community, and recent works [7, 22, 28, 38] have made an effort to establish datasets for inferring a person’s gaze tar-get from third-view image based on deep-learning methods.
However, our survey of these datasets, which play an im-portant role in this task, reveal deﬁciency in the way they gather gaze data. Most datasets are manually annotated, but the subjectivity of annotators may cause annotations to de-viate from the actual gaze target. This is demonstrated by the sample in Figure 1 a) where each annotator has a differ-ent opinion on the gaze target of the same person. In addi-tion, labor-intensive is another drawback. The eye-tracking device in Figure 1 b) can capture annotations automatically but alter subjects’ appearance in the dataset, which brings the gap with the gaze-related behavior in the natural envi-ronment.
To address these problems, as shown in Figure 1 c), we propose a novel system for establishing our GFIE dataset that provides accurate annotations and clean training data recorded in natural environments. The system consists of a laser rangeﬁnder and an RGB-D camera Azure Kinec-t, which allows us to manipulate the laser rangeﬁnder to guide the subject’s gaze target through the laser spot while recording their activities with the RGB-D camera. After detecting the laser spot in the image by our proposed al-gorithm, the gaze target of the person in the image can be located. Based on the distance to the laser spot measured by the laser rangeﬁnder, the 3D gaze target can also be recon-structed. Considering that the laser spot introduces ground truth to the image, we employ an image inpainting algorith-m to eliminate it for constructing the ﬁnal dataset. Most of the processes are automated, alleviating the need for human resources. Our proposed GFIE dataset comprises rich ac-tivity clips with different subjects and diverse scenes. They are key to ensuring the diversity of gaze behaviors. Along with RGB-D images and 2D/3D gaze targets, we also pro-vide camera parameters, head bounding boxes and 2D/3D eye locations.
Accompanying our proposed GFIE dataset, we design a novel baseline method that takes the stereo ﬁeld of view (FoV) to estimate gaze targets into account. In this paper,
FoV is deﬁned as the extend to which a person can observe in 3D space. It is perceived based on the predicted gaze di-rection and transformed into a heatmap. Then the heatmap combined with scene saliency, helps the entire model lo-calize 2D and 3D gaze targets more efﬁciently. State-of-the-art methods are introduced to establish 2D/3D gaze-following benchmarks on both GFIE and CAD-120 [20] datasets. Experiment results show that the GFIE dataset is reliable and the proposed baseline method achieves excel-lent performance in 2D images and 3D scenes.
In summary, our main contributions are as follows:
• We develop a system consisting of a laser rangeﬁnder and RGB-D camera to guide and localize gaze target while recording gaze behavior.
• We release a new GFIE dataset for 2D/3D gaze-following that contains reliable annotations and di-Table 1. Comparison of GFIE with existing gaze-following datasts
Dataset
RGB/
RGB-D
Size
Gaze Target
Localized by 3D
Annot.
GazeFollow [28]
RGB 122,143 frames, 130,339 people
Annotator
VideoAttentionTarget [7] RGB 1331 tracks,164,541 frame-level anno.
Annotator
GazeFollow360 [21]
RGB
VideoGaze [29]
VideoCoAtt [10]
DL Gaze [22]
RGB
RGB
RGB
TIA [38]
RGB-D
GFIE (ours)
RGB-D 65 videos, 10,058 frames 140 movies, 166,721 anno. 380 videos, 492,100 frames 95,000 frames, 16 subjects 330,000 frames, 14 subjects 71799 frames, 61 subjects
Annotator
Annotator
Annotator
Annotator
Eye-tracking glasses
Laser spot (cid:2) (cid:2) (cid:2) (cid:2) (cid:2) (cid:2) (cid:3) (cid:3)
Data Source
MS COCO, SUN,
PASCAL, etc
YouTube
YouTube
MovieQA
TV shows
Recorded by iPhone
Recorded by Kinect V2
Recorded by
Azure Kinect verse human activities in indoor environments.
• We introduce a stereo ﬁeld of view (FoV) in the pro-posed baseline method for improving gaze-following. 2.