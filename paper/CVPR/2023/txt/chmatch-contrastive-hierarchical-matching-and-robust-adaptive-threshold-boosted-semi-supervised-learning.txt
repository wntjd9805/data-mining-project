Abstract
The recently proposed FixMatch and FlexMatch have achieved remarkable results in the ﬁeld of semi-supervised learning. But these two methods go to two extremes as Fix-Match and FlexMatch use a pre-deﬁned constant threshold for all classes and an adaptive threshold for each category, respectively. By only investigating consistency regulariza-tion, they also suffer from unstable results and indiscrim-inative feature representation, especially under the situa-In this paper, we propose a tion of few labeled samples. novel CHMatch method, which can learn robust adaptive thresholds for instance-level prediction matching as well as discriminative features by contrastive hierarchical match-ing. We ﬁrst present a memory-bank based robust thresh-old learning strategy to select highly-conﬁdent samples. In the meantime, we make full use of the structured informa-tion in the hierarchical labels to learn an accurate afﬁnity graph for contrastive learning. CHMatch achieves very sta-ble and superior results on several commonly-used bench-marks. For example, CHMatch achieves 8.44% and 9.02% error rate reduction over FlexMatch on CIFAR-100 under
WRN-28-2 with only 4 and 25 labeled samples per class, respectively1. 1.

Introduction
Deep learning [18, 33, 41] achieves great success in the past decade based on large-scale labeled datasets. However, it is generally hard to collect and expensive to manually an-notate such kind of large dataset in practice, which limits its application. Semi-supervised learning (SSL) attracts much attention recently, since it can make full use of a few labeled and massive unlabeled data to facilitate the classiﬁcation.
For the task of SSL [9, 16, 31, 32], various methods have
∗Corresponding authors. 1Project address: https://github.com/sailist/CHMatch been proposed and promising results have been achieved.
Consistency regularization [44] is one of the most in-ﬂuential techniques in this area. For example, pseudo-ensemble [3] and temporal ensembling [23] investigate the instance-level robustness before and after perturbation.
Mean teacher [37] introduces the teacher-student frame-work and studies the model-level consistency. SNTG [27] further constructs the similarity graph over the teacher model to guide the student learning. However, the super-vised signal generated by only this strategy is insufﬁcient for more challenging tasks.
Recently, by combining pseudo-labeling and consis-tency between weak and strong data augmentations, Fix-Match [34] achieves signiﬁcant improvement. But it relies on a high ﬁxed threshold for all classes, and only a few unlabeled samples whose prediction probability is above the threshold are selected for training, resulting in undesir-able efﬁciency and convergence. Towards this issue, Flex-Match [42] proposes a curriculum learning [4] strategy to learn adjustable class-speciﬁc threshold, which can well im-prove the results and efﬁciency. But it still suffers from the following limitations: (1) The results of both FixMatch and FlexMatch are unstable and of large variances, which is shown in Figure 1(a), especially when there are only a small amount of labeled samples; (2) Only instance-level consis-tency is investigated, which neglects inter-class relationship and may make the learned feature indiscriminative.
To address the above issues, we propose a novel
CHMatch method based on hierarchical label and con-trastive learning, which takes both the instance-level pre-diction matching and graph-level similarity matching into account. Speciﬁcally, we ﬁrst present a memory-bank based robust adaptive threshold learning strategy, where we only need one parameter to compute the near-global threshold for all categories. We compare this strategy with FixMatch in Figure 1(b). Then this adaptive threshold is used for instance-level prediction matching under the similar Fix-Figure 1. Motivation of our method. (a) The results of FixMatch and FlexMatch are unstable and of large variances, while our method can handle this issue. (b) FixMatch sets ﬁxed threshold, while our method sets dynamic proportions in different epoch, leading to adaptive threshold. (c) Many datasets have hierarchical label structure, and we aim to take advantage of this to promote the SSL.
Match paradigm. More importantly, we further propose a hierarchical label guided graph matching module for con-trastive feature learning, where the key lies in the construc-tion of an accurate afﬁnity graph. As shown in Figure 1(c), we notice that categories in many datasets as well as real-world applications have a hierarchical structure, which is neglected by existing methods for semi-supervised learning.
We aim to make use of the coarse-grained labels to guide the general classiﬁcation, which can also provide extra supervi-sion signals especially under the situation of limited labeled samples. For implementation, we ﬁrst add another head for the coarse-grained classiﬁcation. Then each afﬁnity graph is constructed by the ﬁne-grained and coarse-grained clas-siﬁcation branches. Together with the near-global thresh-old, we can get the precise afﬁnity relationship after graph matching, which is then used for contrastive learning. We also conduct extensive experiments on several commonly-used datasets to verify the effectiveness of the proposed method as well as each module.
Our main contributions can be summarized as follows:
• We propose a novel CHMatch method, which con-tains instance-level and graph-level matching for as-signment and feature learning, respectively. To the best of our knowledge, this is the ﬁrst study that makes full use of the structured information matching in hierar-chical labels to promote semi-supervised learning.
• We come up with a memory-bank based highly-conﬁdent sample selection strategy, which can gen-erate robust adaptive threshold for prediction-level matching, leading to more robust results, and accel-erate the training process.
• Beneﬁt from the contrastive hierarchical matching, our method can construct a more accurate afﬁnity graph for the proposed contrastive learning module, leading to more discriminative feature representation.
• We conduct extensive experiments on four benchmark datasets under different backbones, and the proposed
CHMatch outperforms these state-of-the-art methods. 2.