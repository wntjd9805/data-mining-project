Abstract
Virtual reality and augmented reality (XR) bring increas-ing demand for 3D content generation. However, creating high-quality 3D content requires tedious work from a hu-man expert. In this work, we study the challenging task of lifting a single image to a 3D object and, for the first time, demonstrate the ability to generate a plausible 3D object with 360◦ views that corresponds well with the given ref-erence image. By conditioning on the reference image, our model can fulfill the everlasting curiosity for synthesizing novel views of objects from images. Our technique sheds light on a promising direction of easing the workflows for 3D artists and XR designers. We propose a novel frame-work, dubbed NeuralLift-360, that utilizes a depth-aware neural radiance representation (NeRF) and learns to craft the scene guided by denoising diffusion models. By intro-ducing a ranking loss, our NeuralLift-360 can be guided with rough depth estimation in the wild. We also adopt a CLIP-guided sampling strategy for the diffusion prior to provide coherent guidance. Extensive experiments demon-strate that our NeuralLift-360 significantly outperforms ex-isting state-of-the-art baselines. Project page: https:
//vita-group.github.io/NeuralLift-360/ 1.

Introduction
Creating 3D content has been a long-standing problem in computer vision. This problem enables various applica-tions in game studios, home decoration, virtual reality, and augmented reality. Over the past few decades, the manual task has dominated real scenarios, which requires tedious professional expert modeling. Modern artists rely on spe-cial software tools (e.g., Blender, Maya3D, 3DS Max, etc.) and time-demanding manual adjustments to realize imag-inations and transform them into virtual objects. Mean-while, automatic 3D content creation pipelines serve as ef-fective tools to facilitate human efforts. These pipelines typ-ically capture hundreds of images and leverage multi-view stereo [71] to quickly model fine-grained virtual landscapes.
More recently, researchers have started aiming at a more ambitious goal, to create a 3D object from a single im-age [9, 10, 12, 21, 27, 51, 66, 68]. This enables broad ap-plications since it greatly reduces the prerequisite to a min-imal request. Existing approaches can be mainly catego-rized into two major directions. One line of work utilizes learning priors from large-scale datasets with multi-view images [10,21,51,66,68]. These approaches usually learn a conditional neural network to predict 3D information based on input images. However, due to their poor generalization ability, drastic performance degradation is observed when the testing image is out-of-distribution.
Another direction constructs the pipeline on top of the depth estimation techniques [65]. Under the guidance of monocular depth estimation networks [43, 44], the 2D im-age is firstly back-projected to a 3D data format (e.g., point cloud or multi-plane image) and then re-projected into novel views. After that, advanced image inpainting techniques are then adopted to fill missing holes [52] produced during the projection. However, most of them can be highly af-fected by the quality of the estimated depth maps. Though
LeRes [73] attempted to rectify the predicted depth by refin-ing the projected 3D point cloud, their results do not gener-alize well to an arbitrary image in the wild. Overall, the aforementioned approaches are either adopted in limited scenarios (e.g. face or toy examples [4, 75]), or only pro-duce limited viewing directions [8, 70] when being applied to scenes in the wild. Different from all these approaches, we focus on a more challenging task and for the first time, show promising results by lifting a single in-the-wild im-age into a 3D object with 360◦ novel views.
Attracted by the dramatic progress of neural volumet-ric rendering on 3D reconstruction tasks, we consider building our framework based on Neural Radiance Fields (NeRFs) [35]. The original NeRF takes hundreds of training views and their camera poses as inputs to learn an implicit representation. Subsequent models dedicate tremendous ef-forts [8, 19, 70, 75] to apply NeRF to sparse training views.
The most similar work [70] to our method proposes to opti-mize a NeRF using only a single image and its correspond-ing depth map. However, it renders limited views from a small range of angles, and the prerequisite of a high-quality
depth map largely constrains its practical usage.
To address the above issues, we propose a novel frame-work, coined as NeuralLift-360, which aims to ease the creation of 3D assets by building a bridge to convert diverse in-the-wild 2D photos to sophisticated 3D contents in 360◦ views and enable its automation. The major challenge in our work is that the content on the back side is hidden and hard to hallucinate. To tackle these hurdles, we consider the diffusion priors together with the monocular depth esti-mator as the cues for hallucination. Modern diffusion mod-els [41,48,50] are trained on a massive dataset (e.g., 5B text-to-image pairs [24]). During inference time, they can gen-erate impressive photorealistic photos based on simple text inputs. By adopting these learning priors with CLIP [40] guidance, NeuralLift-360 can generate plausible 3D consis-tent instances that correspond well to the given reference image while only requiring minimal additional input, the correct user prompts. Moreover, rather than simply taking the depth map from the pre-trained depth estimator as ge-ometry supervision, we propose to use the relative ranking information from the rough depth during the training pro-cess. This simple strategy is observed to robustly mitigate geometry errors for depth estimations in the wild.
Our contributions can be summarized as follows,
• Given a single image in the wild, we demonstrate promising results of them being lifted to 3D. We use
NeRF as an effective scene representation and inte-grate prior knowledge from the diffusion model.
• We propose a CLIP-guided sampling strategy that ef-fectively marries the prior knowledge from the diffu-sion model with the reference image.
• When the reference image is hard to describe exactly, we finetune the diffusion model on the single image while maintaining its ability to generate diverse con-tents to guide the NeRF training.
• We introduce scale-invariant depth supervision that uses the ranking information. This design alleviates the need for accurate multi-view consistent depth esti-mation and broadens the application of our algorithms. 2.