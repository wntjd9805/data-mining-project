Abstract
Eli Shlizerman1 2MIT-IBM Watson AI Lab
Antonio Torralba3 3MIT
Chuang Gan2,4 4UMass Amherst
Modeling sounds emitted from physical object interac-tions is critical for immersive perceptual experiences in real and virtual worlds. Traditional methods of impact sound synthesis use physics simulation to obtain a set of physics parameters that could represent and synthesize the sound.
However, they require fine details of both the object geome-tries and impact locations, which are rarely available in the real world and can not be applied to synthesize impact sounds from common videos. On the other hand, existing video-driven deep learning-based approaches could only capture the weak correspondence between visual content and impact sounds since they lack of physics knowledge. In this work, we propose a physics-driven diffusion model that can synthesize high-fidelity impact sound for a silent video clip. In addition to the video content, we propose to use ad-ditional physics priors to guide the impact sound synthesis procedure. The physics priors include both physics parame-ters that are directly estimated from noisy real-world impact sound examples without sophisticated setup and learned residual parameters that interpret the sound environment via neural networks. We further implement a novel diffusion model with specific training and inference strategies to com-bine physics priors and visual information for impact sound synthesis. Experimental results show that our model outper-forms several existing systems in generating realistic impact sounds. Lastly, the physics-based representations are fully interpretable and transparent, thus allowing us to perform sound editing flexibly. We encourage the readers visit our project page 1 to watch demo videos with the audio turned on to experience the result. 1.

Introduction
Automatic sound effect production has become demand-ing for virtual reality, video games, animation, and movies.
Traditional movie production heavily relies on talented Foley artists to record many sound samples in advance and man-*Work done while interning at MIT-IBM Watson AI Lab 1https://sukun1045.github.io/video-physics-sound-diffusion/
Figure 1. The physics-driven diffusion model takes physics priors and video input as conditions to synthesize high-fidelity impact sound. Please also see the supplementary video and materials with sample results. ually perform laborious editing to fit the recorded sounds to visual content. Though we could obtain a satisfactory sound experience at the cinema, it is labor-intensive and challenging to scale up the sound effects generation of vari-ous complex physical interactions.
Recently, much progress has been made in automatic sound synthesis, which can be divided into two main cate-gories. The first category is physics-based modal synthesis methods [37, 38, 50], which are often used for simulating sounds triggered by various types of object interactions. Al-though the synthesized sounds can reflect the differences between various interactions and the geometry property of the objects, such approaches require a sophisticated designed environment to perform physics simulation and compute a set of physics parameters for sound synthesis. It is, therefore, impractical to scale up for a complicated scene because of the time-consuming parameter selection procedure. On the other hand, due to the availability of a significant amount of impact sound videos in the wild, training deep learning models for impact sound synthesis turns out to be a promis-ing direction. Indeed, several works have shown promising results in various audio-visual applications [63]. Unfortu-nately, most existing video-driven neural sound synthesis methods [7, 62] apply end-to-end black box model training and lack of physics knowledge which plays a significant role in modeling impact sound because a minor change in the impact location could exert a significant difference in the sound generation process. As a result, these methods are prone to learning an average or smooth audio representa-tion that contains artifacts, which usually leads to generating
unfaithful sound. diffusion model.
In this work, we aim to address the problem of auto-matic impact sound synthesis from video input. The main challenge for the learning-based approach is the weak cor-respondence between visual and audio domains since the impact sounds are sensitive to the undergoing physics. With-out further physics knowledge, generating high-fidelity im-pact sounds from videos alone is insufficient. Motivated by physics-based sound synthesis methods using a set of physics mode parameters to represent and re-synthesize im-pact sounds, we design a physics prior that could contain sufficient physics information to serve as a conditional sig-nal to guide the deep generative model synthesizes impact sounds from videos. However, since we could not perform physics simulation on raw video data to acquire precise physics parameters, we explored estimating and predicting physics priors from sounds in videos. We found that such physics priors significantly improve the quality of synthe-sized impact sounds. For deep generative models, recent successes in image generation such as DALL-E 2 and Ima-gen [44] show that Denoising Diffusion Probabilistic Models (DDPM) outperform GANs in terms of fidelity and diversity, and its training process is usually with less instability and mode collapse issues. While the idea of the denoising pro-cess is naturally fitted with sound signals, it is unclear how video input and physics priors could jointly condition the
DDPM and synthesize impact sounds.
To address all these challenges, we propose a novel sys-tem for impact sound synthesis from videos. The system in-cludes two main stages. In the first stage, we encode physics knowledge of the sound using physics priors, including es-timated physical parameters using signal processing tech-niques and learned residual parameters interpreting the sound environment via neural networks. In the second stage, we formulate and design a DDPM model conditioned on visual input and physics priors to generate a spectrogram of impact sounds. Since the physics priors are extracted from the audio samples, they become unavailable at the inference stage. To solve this problem, we propose a novel inference pipeline to use test video features to query a physics latent feature from the training set as guidance to synthesize impact sounds on unseen videos. Since the video input is unseen, we can still generate novel impact sounds from the diffusion model even if we reuse the training set’s physics knowledge. In summary, our main contributions to this work are:
• We propose novel physics priors to provide physics knowledge to impact sound synthesis, including estimated physics parameters from raw audio and learned residual parameters approximating the sound environment.
• We design a physics-driven diffusion model with different training and inference pipeline for impact sound synthesis from videos. To the best of our knowledge, we are the first work to synthesize impact sounds from videos using the
• Our approach outperforms existing methods on both quan-titative and qualitative metrics for impact sound synthe-sis. The transparent and interpretable properties of physics priors unlock the possibility of interesting sound editing applications such as controllable impact sound synthesis. 2.