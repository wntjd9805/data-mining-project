Abstract
The quadratic computational complexity to the number of tokens limits the practical applications of Vision Trans-formers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) ap-plication difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks.
In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks.
The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image to-kens, for both global and local vision transformers. For in-stance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can achieve the same accuracy with more than 100% in-ference speed improvement and nearly 60% FLOPs reduc-tion; on Swin-(Tiny,Small,Base), we can employ 16 seman-tic tokens in each window to further speed it up by around 20% with slight accuracy increase. Besides great success in image classification, we also extend our method to video recognition. In addition, we design a STViT-R(ecovery) net-work to restore the detailed spatial information based on the STViT, making it work for downstream tasks, which is powerless for previous token sparsification methods. Ex-periments demonstrate that our method can achieve com-petitive results compared to the original networks in object detection and instance segmentation, with over 30% FLOPs reduction for backbone. 1.

Introduction
In contrast to standard Convolutional Neural Networks (CNNs) approaches which process images pixel-by-pixel,
Vision Transformers (ViTs) [15, 26, 35, 36, 43] treat an im-age as a sequence of patch/image tokens, and have shown promising performance in prevalent visual recognition sce-narios. However, these superior performances do not come for free: the quadratic computational complexity to the number of image tokens limits their application in practice.
Previous works [33, 56] have illustrated the large amount of redundancy in the image tokens and also shown the ef-fect of filtering out unimportant tokens normally according to predefined scoring mechanism. However, these meth-ods face the following challenges. Firstly, the predefined scoring mechanisms for filtering are generally imprecise.
In Figure 1, on the left we visualize the class token val-ues in different layers which are commonly used to score the token importance [16, 24, 45]. Different layers have dif-ferent value distributions, thus using these imprecise scores for filtering would lead to unsatisfactory performance. For example, EViT [24] has an accuracy drop of 1.3% when saving 50% FLOPs on DeiT-S [35]. Secondly, the remain-ing tokens do not distribute evenly in space any more, mak-ing them hard to work in local vision transformers1. Finally, large-scale token pruning tremendously damages the spatial structure and positional information, and causes difficulties when applied to downstream tasks, which they do not pro-pose a solution to deal with.
To solve these problems, we propose Semantic Token
ViT (STViT), for efficient global and local vision trans-formers, which also can be revised to serve as backbone for downstream tasks. The proposed approach is based on the following observations: (i) unlike local CNNs which learn spatial structure of images, vision transformer discretizes feature map as tokens for global feature exploration, re-lieving the requirements for maintaining the whole image structure and information; (ii) discrete tokens are more ben-eficial for optimization [38]; (iii) in Figure 1, on the right shows the attention maps in different transformer layers, and there are only several vertical lines in the deep lay-ers, which means that only a few tokens with global se-*Work done during an internship at Alibaba Group.
†Equal corresponding authors.
‡Work done at Alibaba Group, and now affiliated with Amazon. 1In this paper, we define the vision transformer with global self-attention (like DeiT) as global vision transformer and the vision trans-former with local self-attention (like Swin) as local vision transformer.
Figure 1. Left: the attention values of class tokens (normalized and reshaped in image shape) in different self-attention layers. Right: the attention maps in different self-attention layers. Zoom-in for better visibility. mantic information matter. Thus, we argue that it is not necessary to maintain massive structured tokens for ViTs, especially in the deep layers. Employing a few discrete to-kens with high-level semantic information can potentially achieve both high performance and efficiency.
In STViT, the semantic tokens represent the cluster cen-ters, and the number of them is far less than the original im-age tokens, significantly reducing the computational cost.
Inspired by the fact that multi-head attention can conduct the cluster center recovery (Supplementary A.7), we only employ the off-the-shelf self-attention to generate the se-mantic tokens. Specifically, the first few transformer layers are kept unchanged to obtain the image tokens with low-level features. The image tokens are then fed into our se-mantic token generation module (STGM) consisting of at least two transformer layers to generate semantic tokens.
In each self-attention layer, the semantic tokens are input as queries, and the image tokens are fed as keys and val-ues. The semantic tokens dynamically aggregate image to-kens through the attention layers to recover cluster centers.
In the first attention layer, the semantic tokens are initial-ized by an intra and inter-window spatial pooling which takes into account incorporating semantic information in each window and maximizing distance between adjacent windows. Thanks to this spatial initialization, the semantic tokens mainly incorporate local semantic information and achieve discrete and uniform distribution in space. In the following attention layer, besides further clustering, the se-mantic tokens are equipped with global cluster centers, and the network can adaptively select partial semantic tokens to focus on global semantic information. After the STGM, the original image tokens are discarded, and only seman-tic tokens are kept for the subsequent transformer layers.
Because the generation of semantic tokens is flexible and space-aware, our method can be plugged into both global and local vision transformers. The semantic tokens can be produced in each window for the local vision transformer.
Another property of STViT is its capability to serve as a backbone for downstream tasks, such as object detection and instance segmentation. Discussions have been miss-ing in previous methods [16, 24, 32, 45, 56] about how to use them in downstream task under the massive loss of spatial information during the token sparsification process, which actually seriously impedes the application of their method. Instead, we design a novel STViT-R network based on STViT where a recovery module and dumbbell unit are adopted to periodically restore the full resolution feature map while the intermediate transformer layers continue to use semantic tokens to save computation cost, making our method work in downstream task.
The effectiveness of the proposed method is validated via a comprehensive empirical study on image and video ViT models. Only 16 semantic tokens on DeiT-(Tiny, Small,
Base) achieve nearly 50% inference time reduction without any accuracy degradation; on Swin-(Tiny, Small, Base), we also improve the inference throughput by nearly 20% with slight accuracy increase. Moreover, the proposed STViT-R achieves promising results on object detection and instance segmentation. To the best of our knowledge, this is one of first works to apply the token sparsification algorithm in lo-cal vision transformers, and use the ViTs as backbones in downstream tasks after large-scale token pruning. Our find-ings in ViTs uncover that maintaining the full-size feature map is unnecessary, and a few tokens with high-level se-mantic representations can achieve both high performance and efficiency. Thanks to its simplicity and general-purpose ability, our method can also serve as a new efficient ViT baseline architecture and a starting point for further research from the token sparsification perspective. 2.