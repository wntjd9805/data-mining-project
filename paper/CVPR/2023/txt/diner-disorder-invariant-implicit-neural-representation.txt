Abstract
Implicit neural representation (INR) characterizes the attributes of a signal as a function of corresponding coor-dinates which emerges as a sharp weapon for solving in-verse problems. However, the capacity of INR is limited by the spectral bias in the network training. In this paper, we
ﬁnd that such a frequency-related problem could be largely solved by re-arranging the coordinates of the input signal, for which we propose the disorder-invariant implicit neu-ral representation (DINER) by augmenting a hash-table to a traditional INR backbone. Given discrete signals sharing the same histogram of attributes and different arrangement orders, the hash-table could project the coordinates into the same distribution for which the mapped signal can be better modeled using the subsequent INR network, leading to sig-niﬁcantly alleviated spectral bias. Experiments not only re-veal the generalization of the DINER for different INR back-bones (MLP vs. SIREN) and various tasks (image/video representation, phase retrieval, and refractive index recov-ery) but also show the superiority over the state-of-the-art algorithms both in quality and speed. Project page: https://ezio77.github.io/DINER-website/ 1.

Introduction
INR [34] continuously describes a signal, providing the advantages of Nyquist-sampling-free scaling, interpolation, and extrapolation without requiring the storage of additional samples [20]. By combining it with differentiable physical mechanisms such as the ray-marching rendering [12, 22],
Fresnel diffraction propagation [44] and partial differential equations [11], INR becomes a universal and sharp weapon for solving inverse problems and has achieved signiﬁcant progress in various scientiﬁc tasks, e.g., the novel view syn-thesis [39], intensity diffraction tomography [19] and mul-∗The work was supported in part by National Key Research and
Development Project of China (2022YFF0902402) and NSFC (Grants 62025108, 62101242, 62071219). 50 45 40 35 30 25 20 15 10
)
B d (
R
N
S
P
DINER(SIREN)
DINER(MLP)
SIREN
InstantNGP
PE+MLP 5 0 500 1000 1500
Epochs 2000 2500 3000
Figure 1. PSNR of various INRs on 2D image ﬁtting over different training epochs. tiphysics simulation [11].
However, the capacity of INR is often limited by the un-derlying network model itself. For example, the spectral bias [28] usually makes the INR easier to represent low-frequency signal components (see Fig. 1 and Fig. 2(c)). To improve the representation capacity of the INR model, pre-vious explorations mainly focus on encoding more frequen-cy bases using either Fourier basis [22,34,38,42] or wavelet basis [6, 17] into the network. However, the length of func-tion expansion is inﬁnite in theory, and a larger model with more frequency bases runs exceptionally slowly.
Such a problem is closely related to the input signal’s frequency spectrum. The signal’s frequency tells how fast the signal attribute changes following the intrinsic order of geometry coordinates. By properly re-arranging the order of coordinates of a discrete signal, we could modulate the signal’s frequency spectrum to possess more low-frequency components. Such a re-arranged signal can be better mod-eled using subsequent INR. Based on this observation, we propose the DINER. In DINER, the input coordinate is ﬁrst mapped to another index using a hash-table and fed into a traditional INR backbone. We prove that no matter what orders the elements in the signal are arranged, the join-t optimization of the hash-table and the network parameters guarantees the same mapped signal (Fig. 2(d)) with more  
n o o b a
B n o o b a
B d e t r o
S n o o b a
B m o d n a
R 19.55 dB 19.55 dB 22.60 dB 22.60 dB 13.48 dB 13.48 dB yyyy y y y yyyy y y y y y y yyyy 31.03 dB 31.03 dB 31.03 dB 31.08 dB 31.08 dB 31.08 dB 31.05 dB 31.05 dB 31.05 dB xxxx x x x xxxx x x x xxxx x x x (a) GT (b) Spectrum of GT (c) Results by MLP (d) Hash-mapped Input (e) Results by DINER (e) Results by DINER (f) Learned INR in DINER (g) Spectrum of (f)
Figure 2. Comparisons of the existing INR and DINER for representing Baboon with different arrangements. From top to bottom, pixels in the Baboon are arranged in different geometric orders, while the histogram is not changed (the right-bottom panel in (a)). From left to right, (a) and (b) refer to the ground truth image and its Fourier spectrum. (c) contains results by an MLP with positional encoding (PE+MLP) [38] at a size of 2 × 64. (d) refers to the hash-mapped coordinates. (e) refers to the results of the DINER that uses the same-size
MLP as (c). (f) refers to the learned INR in DINER by directly feeding grid coordinates to the trained MLP (see Sec. 4.1 for more details). (g) is the Fourier spectrum of (f). (g) shares the same scale bar with (b). low-frequency components. As a result, the representation capacity of the existing INR backbones and the task perfor-mance are largely improved. As in Fig. 1 and Fig. 2(e), a tiny shallow and narrow MLP-based INR could well char-acterize the input signal with arbitrary arrangement orders.
The use of hash-table trades the storage for fast computation where the caching of its learnable parameters for mapping is usually at a similar size as the input signal, but the com-putational cost is marginal since the back-propagation for the hash-table derivation is O(1).
Main contributions are summarized as follows: 1. The inferior representation capacity of the existing IN-R model is largely increased by the proposed DINER, in which a hash-table is augmented to map the coordi-nates of the original input for better characterization in the succeeding INR model. 2. The proposed DINER provides consistent mapping and representation capacity for signals sharing the same histogram of attributes and different arrangement orders. 3. The proposed DINER is generalized in various tasks, including the representation of 2D images and 3D videos, phase retrieval in lensless imaging, as well as 3D refractive index recovery in intensity diffraction to-mography, reporting signiﬁcant performance gains to the existing state-of-the-arts. 2.