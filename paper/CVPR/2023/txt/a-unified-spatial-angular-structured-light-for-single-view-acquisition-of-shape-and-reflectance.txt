Abstract
We propose a unified structured light, consisting of an
LED array and an LCD mask, for high-quality acquisition of both shape and reflectance from a single view. For ge-ometry, one LED projects a set of learned mask patterns to accurately encode spatial information; the decoded results from multiple LEDs are then aggregated to produce a final depth map. For appearance, learned light patterns are cast through a transparent mask to efficiently probe angularly-varying reflectance. Per-point BRDF parameters are differ-entiably optimized with respect to corresponding measure-ments, and stored in texture maps as the final reflectance.
We establish a differentiable pipeline for the joint capture to automatically optimize both the mask and light patterns to-wards optimal acquisition quality. The effectiveness of our light is demonstrated with a wide variety of physical ob-jects. Our results compare favorably with state-of-the-art techniques. 1.

Introduction
Joint acquisition of both shape and appearance of a static object is one key problem in computer vision and computer graphics. It is critical for various applications, such as cul-tural heritage, e-commerce and visual effects. Represented as a 3D mesh and a 6D Spatially-Varying Bidirectional Re-flectance Distribution Function (SVBRDF), a digitized ob-ject can be rendered to reproduce the original look in the virtual world with high fidelity for different view and light-ing conditions.
Active lighting is widely employed in high-quality ac-quisition. It probes the physical domain efficiently and obtains measurements strongly correlated with the target, leading to high signal-to-noise ratio (SNR) results. For ge-ometry, structured illumination projects carefully designed pattern(s) into the space to distinguish rays for accurate 3D triangulation [15,32]. For reflectance, illumination mul-*Equal contributions.
†Corresponding authors (hwu@acm.org/kunzhou@acm.org).
Figure 1. Our hardware prototype. It consists of an LED array, an
LCD mask and a camera (left). One LED can project a set of mask patterns for shape acquisition (center), and multiple LEDs can be programmed to cast light patterns through a transparent mask for reflectance capture (right). tiplexing programs the intensities of different lights over time, physically convolving with BRDF slices in the angu-lar domain to produce clues for precise appearance deduc-tion [13, 35].
While active lighting for geometry or reflectance alone has been extensively studied, it is difficult to apply the idea to joint capture. At one hand, directly combining the two types of lights ends up with a bulky setup [39] and compet-ing measurement coverages (i.e., a light for shape capture cannot be co-located with one for reflectance). On the other hand, existing work usually adopts one type of active light only, and has to perform passive acquisition on the other, leading to sub-optimal reconstructions. For example, Hol-royd et al. [16] use projectors to capture geometry and im-pose strong priors on appearance. Kang et al. [19] build a light cube to densely sample reflectance in the angular do-main. But the quality of its passive shape reconstruction is severely limited, if the object surface lacks prominent spa-tial features.
To tackle the above challenges, we propose a unified structured light for high-quality acquisition of 3D shape and reflectance. Our lightweight prototype consists of an
LED array and an LCD mask, which essentially acts as a restricted lightfield projector to actively probe the spatial and angular domain. For geometry, each LED projects a set of mask patterns into the space to encode shape infor-mation. For appearance, the same LED array produces dif-ferent light patterns, which are cast through a transparent mask to sample the reflectance that varies with the light an-gle. The prototype helps capture sufficient physical infor-mation to faithfully recover per-pixel depth and reflectance even from a single view.
To program the novel light towards optimal acquisition quality, we establish a differentiable pipeline for the joint capture, so that both mask and light patterns can be auto-matically optimized to harness our hardware capability. For geometry, a set of mask patterns are independently learned for each LED, by minimizing the depth uncertainty along an arbitrary camera ray. We also exploit the physical con-volution that causes blurry mask projections in our setup, to encode richer spatial information for depth disambiguation.
Multiple LEDs can be employed to project different sets of mask patterns, for improving the completeness and accu-racy of the final shape. For reflectance, the light patterns are optimized as part of an autoencoder, which learns to cap-ture the essence of appearance [19]. The reflectance is then optimized with respect to the measurements under such pat-terns, taking into account the reconstructed geometry for a higher-quality estimation.
The effectiveness of our approach is demonstrated on a number of physical samples with considerable variations in shape and reflectance. Using 4×18 = 72 mask patterns and 32 light patterns, we achieve on average a geometric accu-racy of 0.27mm(mean distance) and a reflectance accuracy of 0.94(SSIM), on a lightweight prototype with an effective spatial resolution of only 320 × 320 and an angular resolu-tion of 64 × 48. Our results are compared with state-of-the-art techniques on shape and reflectance capture, as well as validated against photographs. In addition, we evaluate the impact of different factors over the final results and discuss exciting future research directions. 2.