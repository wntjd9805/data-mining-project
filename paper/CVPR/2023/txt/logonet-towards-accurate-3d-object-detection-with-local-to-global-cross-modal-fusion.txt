Abstract 1.

Introduction
LiDAR-camera fusion methods have shown impressive performance in 3D object detection. Recent advanced multi-modal methods mainly perform global fusion, where image features and point cloud features are fused across the whole scene. Such practice lacks ﬁne-grained region-level information, yielding suboptimal fusion performance.
In this paper, we present the novel Local-to-Global fusion network (LoGoNet), which performs LiDAR-camera fusion at both local and global levels. Concretely, the Global Fu-sion (GoF) of LoGoNet is built upon previous literature, while we exclusively use point centroids to more precisely represent the position of voxel features, thus achieving bet-ter cross-modal alignment. As to the Local Fusion (LoF), we ﬁrst divide each proposal into uniform grids and then project these grid centers to the images. The image features around the projected grid points are sampled to be fused with position-decorated point cloud features, maximally uti-lizing the rich contextual information around the proposals.
The Feature Dynamic Aggregation (FDA) module is further proposed to achieve information interaction between these locally and globally fused features, thus producing more informative multi-modal features. Extensive experiments on both Waymo Open Dataset (WOD) and KITTI datasets show that LoGoNet outperforms all state-of-the-art 3D de-tection methods. Notably, LoGoNet ranks 1st on Waymo 3D object detection leaderboard and obtains 81.02 mAPH (L2) detection performance. It is noteworthy that, for the
ﬁrst time, the detection performance on three classes sur-passes 80 APH (L2) simultaneously. Code will be available at https://github.com/sankin97/LoGoNet.
*Corresponding author 3D object detection, which aims to localize and clas-sify the objects in the 3D space, serves as an essential perception task and plays a key role in safety-critical au-tonomous driving [1, 20, 58]. LiDAR and cameras are two widely used sensors. Since LiDAR provides accu-rate depth and geometric information, a large number of methods [24, 48, 63, 68, 72, 73] have been proposed and achieve competitive performance in various benchmarks.
However, due to the inherent limitation of LiDAR sensors, point clouds are usually sparse and cannot provide sufﬁcient context to distinguish between distant regions, thus causing suboptimal performance.
To boost the performance of 3D object detection, a nat-ural remedy is to leverage rich semantic and texture in-formation of images to complement the point cloud. As shown in Fig. 1 (a), recent advanced methods introduce the global fusion to enhance the point cloud with image fea-tures [2, 5, 7, 8, 22, 23, 25, 27, 34, 54, 55, 60, 69, 71]. They typically fuse the point cloud features with image features across the whole scene. Although certain progress has been achieved, such practice lacks ﬁne-grained local informa-tion. For 3D detection, foreground objects only account for a small percentage of the whole scene. Merely performing global fusion brings marginal gains.
To address the aforementioned problems, we propose a novel Local-to-Global fusion Network, termed LoGoNet, which performs LiDAR-camera fusion at both global and local levels, as shown in Fig. 1 (b). Our LoGoNet is comprised of three novel components, i.e., Global Fusion (GoF), Local Fusion (LoF) and Feature Dynamic Aggrega-tion (FDA). Speciﬁcally, our GoF module is built on previ-ous literature [8,25,34,54,55] that fuse point cloud features and image features in the whole scene, where we use the point centroid to more accurately represent the position of
Global Fusion fuse
Point Cloud
Image
Spatial Location Guidance (a)
Local Fusion fuse
ROI Point Cloud
ROI Image
Instance Semantic Guidance (b) 82 81 80
) 2
L (
H
P
A m 79 78 77 76 75 81.02
*
* 79.94 79.97
* 79.60
*
* 78.41 78.45
*
* 77.64 77.10 76.29 76.33 75.54 75.67
Local-to-Global fusion (Ours)
Global fusion
LiDAR-only (c)
* Test time augmentations or ensemble (c)
Time
Figure 1. Comparison between (a) global fusion and (b) local fusion. Global fusion methods perform fusion of point cloud features and image features across the whole scene, which lacks ﬁne-grained region-level information. The proposed local fusion method fuses features of two modalities on each proposal, complementary to the global fusion methods. (c) Performance comparison of various methods in
Waymo 3D detection leaderboard [51]. Our LoGoNet attains the top 3D detection performance, clearly outperforming all state-of-the-art global fusion based and LiDAR-only detectors. Please refer to Table 1 for a detailed comparison with more methods. each voxel feature, achieving better cross-modal alignment.
And we use the global voxel features localized by point cen-troids to adaptively fuse image features through deformable cross-attention [75] and adopt the ROI pooling [9, 48] to generate the ROI-grid features.
To provide more ﬁne-grained region-level information for objects at different distances and retain the original po-sition information within a much ﬁner granularity, we pro-pose the Local Fusion (LoF) module with the Position In-formation Encoder (PIE) to encode position information of the raw point cloud in the uniformly divided grids of each proposal and project the grid centers onto the image plane to sample image features. Then, we fuse sampled image fea-tures and the encoded local grid features through the cross-attention [53] module. To achieve more information inter-action between globally fused features and locally fused
ROI-grid features for each proposal, we propose the FDA module through self-attention [53] to generate more infor-mative multi-modal features for second-stage reﬁnement.
Our LoGoNet achieves superior performance on two 3D detection benchmarks, i.e., Waymo Open Dataset (WOD) and KITTI datasets. Notably, LoGoNet ranks 1st on Waymo 3D object detection leaderboard and obtains 81.02 mAPH (L2) detection performance. Note that, for the ﬁrst time, the detection performance on three classes surpasses 80 APH (L2) simultaneously.
The contributions of our work are summarized as fol-lows:
• We propose a novel local-to-global fusion network, termed LoGoNet , which performs LiDAR-camera fu-sion at both global and local levels.
• Our LoGoNet is comprised of three novel components, i.e., GoF, LoF and FDA modules. LoF provides ﬁne-grained region-level information to complement GoF.
FDA achieves information interaction between glob-ally and locally fused features, producing more infor-mative multi-modal features.
• LoGoNet achieves state-of-the-art performance on
WOD and KITTI datasets.
Notably, our Lo-GoNet ranks 1st on Waymo 3D detection leaderboard with 81.02 mAPH (L2). 2.