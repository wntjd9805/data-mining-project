Abstract
Despite the tremendous progress of Masked Autoen-coders (MAE) in developing vision tasks such as image and video, exploring MAE in large-scale 3D point clouds re-mains challenging due to the inherent irregularity. In con-trast to previous 3D MAE frameworks, which either design a complex decoder to infer masked information from main-tained regions or adopt sophisticated masking strategies, we instead propose a much simpler paradigm. The core idea is to apply a Generative Decoder for MAE (GD-MAE) to automatically merges the surrounding context to restore the masked geometric knowledge in a hierarchical fusion manner. In doing so, our approach is free from introducing the heuristic design of decoders and enjoys the flexibility of exploring various masking strategies. The correspond-ing part costs less than 12% latency compared with con-ventional methods, while achieving better performance. We demonstrate the efficacy of the proposed method on several large-scale benchmarks: Waymo, KITTI, and ONCE. Con-sistent improvement on downstream detection tasks illus-trates strong robustness and generalization capability. Not only our method reveals state-of-the-art results, but remark-ably, we achieve comparable accuracy even with 20% of the labeled data on the Waymo dataset. Code will be released. 1.

Introduction
We have witnessed great success in 3D object detec-tion [44, 47, 64, 68, 71, 78], due to the numerous applica-tions in autonomous driving, robotics, and navigation. De-spite the impressive performance, most methods count on large amounts of carefully labeled 3D data, which is often
∗Equal contribution. This work was done when Honghui was an intern at Shanghai Artificial Intelligence Laboratory.
†Corresponding author
Figure 1. Comparisons. Previous MAE-style pre-training archi-tectures of (a) single-scale [18, 19, 38] and (b) multi-scale [12, 73] take as inputs the visible tokens and learnable tokens for decoders.
In contrast, (c) the proposed framework avoids such a process. of high cost and time-consuming. Such a fully supervised manner hinders the possibility of using massive unlabeled data and can be vulnerable when applied in different scenes.
Mask Autoencoder (MAE) [18], serving as one of the ef-fective ways for pre-training, has demonstrated great po-tential in learning holistic representations. This is achieved by encouraging the method to learn a semantically consis-tent understanding of the input beyond low-level statistics.
Although MAE-based methods have shown effectiveness in 2D image [18] and video [52], how to apply it in large-scale point clouds remains an open problem.
Due to the large variation of the visible extent of ob-jects, learning hierarchical representation is of great signif-icance in 3D supervised learning [40, 46, 62]. To enable
MAE-style pre-training on the hierarchical structure, previ-ous approaches [12, 73] introduce either complex decoders or elaborate masking strategies to learn robust latent repre-sentations. For example, ConvMAE [12] adopts a block-wise masking strategy that first obtains a mask for the late stage of the encoder and then progressively upsamples the mask to larger resolutions in early stages to maintain mask-ing consistency. Point-M2AE [73] proposes a hierarchi-cal decoder to gradually incorporate low-level features into learnable tokens for reconstruction. Meanwhile, it needs a multi-scale masking strategy that backtracks unmasked po-sitions to all preceding scales to ensure coherent visible re-gions and avoid information leakage. The minimum size of masking granularity is highly correlated to output tokens of the last stage, which inevitably poses new challenges, espe-cially to objects with small sizes, e.g., pedestrians.
To alleviate the issue, we present a much simpler paradigm dubbed GD-MAE for pre-training, as shown in
Figure 1. The key is to use a generative decoder to automat-ically expand the visible regions to the underlying masked
In doing so, it eliminates the need for designing area. complex decoders, in which masked regions are presented as learnable tokens.
It also allows for the unification of multi-scale features into the same scale, thus enabling flex-ible masking strategies, e.g., point- and patch-wise mask-ing, while avoiding intricate operations such as backtrack-ing in [12, 73] to keep masking consistency. Specifically, it consists of the following components:
Firstly, we propose the Sparse Pyramid Transformer (SPT) as the multi-scale encoder. Following [9,22,43], SPT takes pillars as input due to the compact and regular repre-sentation. Unlike PointPillars [22] that uses traditional con-volutions for feature extraction, we use the sparse convo-lution [62] to downsample the tokens and the sparse trans-former [9] to enlarge the receptive field of the visible tokens when deploying extensive masking.
Secondly, we introduce the Generative Decoder (GD) to simplify MAE-style pre-training on multi-scale backbones.
GD consists of a series of transposed convolutions used to upsample multi-scale features and a convolution utilized to expand the visible area, as shown in Figure 2. The expanded features are then directly indexed according to the coordi-nates of the masked tokens for the geometric reconstruction.
Extensive experiments have been conducted on Waymo
Open Dataset [49], KITTI [13], and ONCE [33] to ver-ify the efficacy. On the Waymo dataset, GD-MAE sets new state-of-the-art detection results compared to previ-ously published methods.
Our contributions are summarized as follows:
• We introduce a simpler MAE framework that avoids complex decoders and thus simplifies pre-training.
Illustration of area expansion. The input point cloud
Figure 2. (i.e., the orange curve) is voxelized and fed into the multi-scale encoder. The generative decoder can automatically expand visible features to potentially masked areas.
• The proposed decoder enables flexible masking strate-gies on LiDAR point clouds, while costing less than 12% latency compared with conventional methods.
• Extensive experiments are conducted to verify the ef-fectiveness of the proposed model. 2.