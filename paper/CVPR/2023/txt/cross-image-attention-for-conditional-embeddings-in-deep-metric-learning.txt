Abstract
Learning compact image embeddings that yield seman-tic similarities between images and that generalize to un-seen test classes, is at the core of deep metric learning (DML). Finding a mapping from a rich, localized image feature map onto a compact embedding vector is challeng-ing: Although similarity emerges between tuples of images,
DML approaches marginalize out information in an individ-ual image before considering another image to which simi-larity is to be computed.
Instead, we propose during training to condition the em-bedding of an image on the image we want to compare it to.
Rather than embedding by a simple pooling as in standard
DML, we use cross-attention so that one image can iden-tify relevant features in the other image. Consequently, the attention mechanism establishes a hierarchy of conditional embeddings that gradually incorporates information about the tuple to steer the representation of an individual image.
The cross-attention layers bridge the gap between the origi-nal unconditional embedding and the final similarity and al-low backpropagtion to update encodings more directly than through a lossy pooling layer. At test time we use the re-sulting improved unconditional embeddings, thus requiring no additional parameters or computational overhead. Ex-periments on established DML benchmarks show that our cross-attention conditional embedding during training im-proves the underlying standard DML pipeline significantly so that it outperforms the state-of-the-art. 1.

Introduction
Deep metric learning (DML) seeks embeddings that al-low a predefined distance metric to not only express se-mantic similarities between training samples, but to also transfers to unseen classes. The ability to learn compact image representations that generalize well and transfer in zero-shot manner to unseen test data distributions is crucial for a wide range of visual perception tasks such as visual retrieval [51, 63], image classification [44, 80, 88], cluster-ing [7, 28], or person (re-)identification [11, 27, 69].
DML research has investigated important questions like the effective mining of training samples [4, 54, 57, 65, 68], the training loss function [51, 53, 68, 77, 81, 82], and ensem-ble strategies [18, 22, 55, 63, 86]. However, learning pow-erful embeddings is by definition a challenging problem: we seek a mapping from a rich local feature encoding that projects this tensor with all its comprehensive spatial in-formation and local details onto a compact vector that acts as a holistic embedding for an entire image. Local details have to be aggregated and all the important spatial interre-lations in an image, e.g., the spatial composition of a scene or the relative configuration of different body parts to an-other, have to be summed up in a mere vector. However, image similarity is multi-modal in natureâ€”two images can be similar with respect to one characteristic but different in light of another. The challenge is consequently to learn which local details to marginalize out and which to preserve when the embedding function only sees one image and not also the one we want to compute its similarity to. However, during training we have access to all images and power-ful loss functions such as multi-similarity loss [82] already compare all image tuples. Thus, we could significantly sim-plify learning the embedding by conditioning it on another image that we then compute the similarity to.
Contributions: During training we therefore compute similarities using conditional embeddings of the image we want to represent conditioned on another image we want to compare against. Thus, the second image focuses the atten-tion of the embedding function on characteristics that are meaningful for a subsequent comparison. Rather then ap-plying a mere pooling operation, we utilize cross-attention to project standard image feature encodings (such as a
ResNet convolutional feature map [26]) onto an embedding vector while conditioning it on an embedding of the other image. Repeating these cross-attention blocks then cre-ates a hierarchy of conditional embeddings by successively adding the conditioning information and gradually transi-tioning from the challenging unconditional embedding to the more accessible conditional one. The hierarchy there-fore divides the difficult problem of learning an embedding into several smaller steps. Moreover, due to cross-attention error backpropagation from the similarity measure can now directly update the image encoding and the embeddings
rather than having to optimize the encoding only through the pooling operation of the embedding, which risks atten-uated gradients. Consequently the encodings and uncondi-tional embeddings improve over their counterparts in clas-sical DML training. During inference, we therefore employ these unconditional representations so that the approach af-ter training works just like standard DML with no addi-tional parameters and no extra computational costs, sim-ply encoding individual images using a ResNet feature en-coder followed by a standard embedding network that out-puts the usual pooled feature vector. Our experimental eval-uation shows that through cross-attention, our conditional as well as the underlying unconditional embeddings signif-icantly improve over the embeddings obtained by DML so far. Moreover, the computational overhead during training is negligible compared to the costs of current DML training. 2.