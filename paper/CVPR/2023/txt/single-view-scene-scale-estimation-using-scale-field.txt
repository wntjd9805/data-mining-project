Abstract
In this paper, we propose a single image scale estima-tion method based on a novel scale field representation. A scale field defines the local pixel-to-metric conversion ratio along the gravity direction on all the ground pixels. This representation resolves the ambiguity in camera parame-ters, allowing us to use a simple yet effective way to collect scale annotations on arbitrary images from human annota-tors. By training our model on calibrated panoramic image data and the in-the-wild human annotated data, our sin-gle image scene scale estimation network generates robust scale field on a variety of image, which can be utilized in various 3D understanding and scale-aware image editing applications. 1.

Introduction
Single image 3D understanding plays a significant role in various computer vision tasks, such as AR/VR applications, robotics, and computational 3D photography. Many recent approaches show promising results in estimating depth [17, 18, 25], scene structure [24, 32], or radiance field [20, 22, 23]. However, they often treat scale as an undetermined factor as it is a very ill-posed problem when the physical extrinsics of the camera is unknown. Therefore, it remains a very challenging task to estimate the metric scale of a scene for a single unconstrained image.
One seminal work in scale estimation is single view metrology [6]. As detailed in this line of works [6, 10, 35], knowing horizon line, field of view (FoV) and absolute camera height enables the conversion between any 2D mea-surements in image space to 3D measurements. Horizon line and FoV can be estimated using visual features, as many of other previous methods suggest [9, 11, 29, 30, 35].
However, since the absolute camera height information can-not be obtained from low-level visual features, Criminisi et al. [6], Hoiem et al. [10] and Zhu et al. [35] utilize canoni-cal object heights as reference. Reprojecting the 2D bound-ing boxes of well-known objects like humans or cars to 3D space with their known metric heights, these methods derive the vertical height of the camera, and then calculate metric scale of other objects in the 2D image.
However, there are two major downsides of single view metrology based approaches. First, their scale estimation relies on the existence of known objects with either precise or canonical metric heights. This restricts practical usage of their methods because there may or may not be an object with known height. In addition, the height of a known ob-ject can also change with its pose (e.g. a sitting person vs a standing person), making the system less reliable. Second, their prediction of the scene scale is highly dependent on a few global parameters like horizon line, FoV, and camera height. Among these parameters, camera height is very dif-ficult to directly predict based on visual features. Moreover, models that directly predict these parameters tend to overfit to a certain dataset where the camera height and the FoV lie in a limited range.
In this paper, to tackle the aforementioned issues, we in-troduce a novel way of representing scene scale. We define a pixel-wise pixel-to-metric 2D map, called Scale Field.
Using this local and dense representation, our goal is to train a robust and generalizable scale estimation network to recover the scene scale. Also, we provide an efficient pipeline to annotate scale fields on random images based on our geometrical observations consistent with single view metrology. This allows us to collect a diverse set of images with varying camera angles and camera heights. Our single-view scene scale estimation network trained on this dataset shows robust results on a variety of scenes, and it general-izes well even on object-centric images. With our predicted scale field, we can perform various 3D scene understanding and scale-aware image editing, as visualized in Fig. 1.
To summarize, our major contributions are:
• We introduce Scale Field, a novel representation of scene scale information, which can be utilized in var-ious 3D understanding and scale-aware image editing tasks.
• We propose a pipeline to annotate scale fields on web images, and collect a diverse set of training samples.
• We provide a formulation of the single image scene scale estimation network, trained on a mixture of training data from panoramic images and human an-notated data. Our method shows great robustness and generalizability on in-the-wild images. 2.