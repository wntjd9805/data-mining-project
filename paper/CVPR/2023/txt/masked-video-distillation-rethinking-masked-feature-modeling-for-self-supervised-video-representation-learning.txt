Abstract
Benefiting from masked visual modeling, self-supervised video representation learning has achieved remarkable progress. However, existing methods focus on learning rep-resentations from scratch through reconstructing low-level features like raw pixel values.
In this paper, we propose masked video distillation (MVD), a simple yet effective two-stage masked feature modeling framework for video repre-sentation learning: firstly we pretrain an image (or video) model by recovering low-level features of masked patches, then we use the resulting features as targets for masked fea-ture modeling. For the choice of teacher models, we ob-serve that students taught by video teachers perform bet-ter on temporally-heavy video tasks, while image teachers transfer stronger spatial representations for spatially-heavy video tasks. Visualization analysis also indicates different teachers produce different learned patterns for students. To leverage the advantage of different teachers, we design a spatial-temporal co-teaching method for MVD. Specifically, we distill student models from both video teachers and im-age teachers by masked feature modeling. Extensive ex-perimental results demonstrate that video transformers pre-trained with spatial-temporal co-teaching outperform mod-els distilled with a single teacher on a multitude of video datasets. Our MVD with vanilla ViT achieves state-of-the-art performance compared with previous methods on sev-eral challenging video downstream tasks. For example, with the ViT-Large model, our MVD achieves 86.4% and 76.7%
Top-1 accuracy on Kinetics-400 and Something-Something-v2, outperforming VideoMAE by 1.2% and 2.4% respec-tively. When a larger ViT-Huge model is adopted, MVD achieves the state-of-the-art performance with 77.3% Top-1 accuracy on Something-Something-v2. Code will be avail-able at https://github.com/ruiwang2021/mvd.
†Corresponding authors
Figure 1. Comparisons of MVD with previous supervised or self-supervised methods on Something-Something v2. Each line represents the corresponding model of different sizes. 1.

Introduction
For self-supervised visual representation learning, recent masked image modeling (MIM) methods like MAE [31] and BEiT [2] achieve promising results with vision trans-formers [17] on various vision downstream tasks. Such a pretraining paradigm has also been adapted to the video domain and boosts video transformers by clear margins compared with supervised pretraining on several video downstream tasks. Representative masked video modeling (MVM) works include BEVT [63], VideoMAE [57] and
ST-MAE [21].
Following MAE [31] and BEiT [2], existing masked video modeling methods [21, 57, 63] pretrain video trans-formers through reconstructing low-level features, e.g., raw pixel values or low-level VQVAE tokens. However, us-ing low-level features as reconstruction targets often incur much noise. And due to the high redundancy in video data, it is easy for masked video modeling to learn shortcuts, thus resulting in limited transfer performance on downstream
tasks. To alleviate this issue, masked video modeling [57] often uses larger masking ratios.
In this paper, we observe that much better performance on video downstream tasks can be achieved by conducting masked feature prediction by using the high-level features of pretrained MIM and MVM models as masked predic-tion targets. This can be viewed as two-stage masked video modeling, where MIM pretrained image models (i.e., an image teacher) or MVM pretrained video models (i.e., an video teacher) are obtained in the first stage, and they fur-ther act as teachers in the second stage for the student model via providing the high-level feature targets. Therefore, we call this method Masked Video Distillation (MVD).
More interestingly, we find that student models distilled with different teachers in MVD exhibit different proper-ties on different video downstream tasks. Specifically, stu-dents distilled from the image teacher perform better on video tasks that mainly rely on spatial clues, while students distilled from the video teacher model perform better on the video downstream tasks where temporal dynamics are more necessary. We think during the pretraining process of masked video modeling in the first stage, video teach-ers have learned spatial-temporal context in their high-level features. Therefore, when employing such high-level repre-sentations as prediction targets of masked feature modeling, it will help encouraging the student model to learn stronger temporal dynamics. By analogy, image teachers provide high-level features as targets that include more spatial in-formation, which can help the student model learn more spatially meaningful representations. We further analyze the feature targets provided by image teachers and video teachers, and calculate the cross-frame feature similarity. It shows that the features provided by the video teachers con-tain more temporal dynamics.
Motivated by the above observation, to leverage the ad-vantages of video teachers and image teachers, we propose a simple yet effective spatial-temporal co-teaching strategy for MVD. In detail, the student model is designed to re-construct the features coming from both the image teacher and video teacher with two different decoders, so as to learn stronger spatial representation and temporal dynam-ics at the same time. Experiments demonstrate that MVD with co-teaching from both the image teacher and the video teacher significantly outperforms MVD only using one sin-gle teacher on several challenging downstream tasks.
Despite the simplicity, our MVD is super effective and achieves very strong performance on multiple standard video recognition benchmarks. For example, on Kinectics-400 and Something-Something-v2 datasets, compared to the baseline without distillation, MVD with 400 epochs us-ing a teacher model of the same size achieves 1.2%, 2.8%
Top-1 accuracy gain on ViT-B. If a larger teacher model
ViT-L is used, more significant performance gains (i.e., 1.9%, 4.0%) can be obtained. When ViT-Large is the target student model, our method can achieves 86.4% and 76.7%
Top-1 accuracy on these two datasets, surpassing existing state-of-the-art method VideoMAE [57] by 1.2% and 2.4% respectively. When a larger ViT-Huge model is adopted,
MVD achieves the state-of-the-art performance with 77.3%
Top-1 accuracy on Something-Something-v2.
Our contributions can be summarized as below:
• We find that using MIM pretrained image models and
MVM pretrained video models as teachers to provide the high-level features for continued masked feature prediction can learn better video representation. And representations learned with image teachers and video teachers show different properties on different down-stream video datasets.
• We propose masked video distillation together with a simple yet effective co-teaching strategy, which enjoys the synergy of image and video teachers.
• We demonstrate strong performance on multiple stan-dard video recognition benchmarks, surpassing both the baseline without MVD and prior state-of-the-art methods by clear margins. 2.