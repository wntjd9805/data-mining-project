Abstract
Commodity ultrahigh definition (UHD) displays are be-coming more affordable which demand imaging in ultrahigh resolution (UHR). This paper proposes SparseMat, a com-putationally efficient approach for UHR image/video mat-ting. Note that it is infeasible to directly process UHR im-ages at full resolution in one shot using existing matting algorithms without running out of memory on consumer-level computational platforms, e.g., Nvidia 1080Ti with 11G memory, while patch-based approaches can introduce un-sightly artifacts due to patch partitioning.
Instead, our method resorts to spatial and temporal sparsity for ad-dressing general UHR matting. When processing videos, huge computation redundancy can be reduced by exploit-ing spatial and temporal sparsity. In this paper, we show how to effectively detect spatio-temporal sparsity, which serves as a gate to activate input pixels for the matting model. Under the guidance of such sparsity, our method with sparse high-resolution module (SHM) can avoid patch-based inference while memory efficient for full-resolution matte refinement. Extensive experiments demonstrate that
SparseMat can effectively and efficiently generate high-quality alpha matte for UHR images and videos at the orig-inal high resolution in a single pass. Project page is in https://github.com/nowsyn/SparseMat.git. 1.

Introduction
Ultrahigh resolution (UHR) matting is an important problem [36, 49], and with increasing demand due to the fast advent and accessibility of commodity ultrahigh def-inition displays in real-world applications, such as gam-ing, TV/movie post-production, and image/video editing,
UHR matting becomes ever relevant. However, modern consumer-level GPU and mobile devices still have limited hardware resources. Despite good technical contributions, guided filters and patch-based techniques (Figure 1) are not applicable, when unsightly blurry and seams artifacts are unacceptable in UHR imaging.
Matting is a primary technique for image/video editing and plays an important role in many applications. The goal of matting is to extract a detailed alpha matte of the fore-ground object from a given image/video. The matted fore-ground can be composited on other background images. As known, matting is an ill-posed problem defined as Equa-tion 1 with the given image I, foreground F , background B and alpha α ∈ [0, 1] to be extracted:
I = αF + (1 − α)B. (1)
Most of the existing state-of-the-art matting meth-ods [28, 33, 48] take the whole image as input in a forward pass, and thus the resolution they can handle is bounded by available memory. Given limited memory, to process
UHR images, a straightforward approach is to first process the downsampled input image which will inevitably lead to blurry artifacts. Thus, super-resolution methods, such as guided filter (GF) [21] or deep guided filter (DGF) [47], have been proposed to recover missing details. However, guided filter or deep guided filter easily produces fuzzy ar-tifacts when processing complex hairy structures as shown in Figure 1-(a). Patch-based inference [23, 35] is another plausible strategy. However, small patch can cause artifacts due to insufficient global context and inconsistent local con-text as shown in Figure 1-(b). On the other hand, using large patch (e.g., 2048) with large overlap produces defective al-pha matte with missing details or blurry artifacts in long hair region due to the lack of long-range dependency in UHR images as shown Figure 1-(b), not to mention that heavy computation and memory overhead are introduced with in-creasing patch size making some methods cannot even run, as shown in Figure 1-(c).
In conclusion, super resolving based on (deep) guided filter or patch-based inference are not ideal choices for handling UHR matting.
In this paper we propose a general image/video matting framework SparseMat to address the problem of UHR mat-ting, which is both memory and computation efficient while generating high-quality alpha mattes. The core idea of our method is to skip a large amount of redundant computation on many pixels during processing UHR images or videos.
In general, our method takes the low-resolution prior as input to generate spatio-temporal sparsity, which serves as the gate to activate pixels consumed by a sparse convolution module. Both temporal and spatial information contribute to the sparsity estimation. Specifically, we compute color difference between adjacent frames to obtain the temporal
(a) Blurry artifacts when UHR images are not matted in the original full resolution: guided filter (GF), deep guided filter
Figure 1. (RVM [36]), small patch replacement method (BGMv2 [35]). The low-resolution alpha matte obtained by the self-trained low-resolution prior network (LPN) is for reference here. Our sparse high-resolution module (SHM) produces high-quality alpha matte for UHR matting. (b) Seam artifacts of patch-based inference from FBA [13] under different patch sizes with a fixed ratio (1/8) of overlap. Full-resolution result is ours. (c) Memory consumption and computation (GMACs) with different patch sizes. When the patch size exceeds 2K, some methods cannot even run on Nvidia 1080Ti with 11G memory. sparsity. For the spatial sparsity, we can derive from any lightweight low-resolution matting model such as [28, 36].
The two sparsity maps are combined together to determi-nate the pixels which need high-resolution processing by the sparse module. Unlike super-resolving strategy per-formed on the whole image, our method with sparse high-resolution module (SHM) can safely skip expensive com-putations in large solid pixel regions, only paying attention to irregular, sparse and (oftentimes) thin border regions sur-rounding the object or transitional regions within the ob-ject. In contrast to restrictive views of patch-based strategy to local regions, our method takes a more global perspec-tive of the foreground object thus avoiding potential arti-facts due to inadequate context consideration. This design allows SparseMat to process an ultrahigh resolution image or video frame in only one shot without suffering any in-formation loss caused by down-sampling or patch partition, which thus produces high-quality alpha matte for ultrahigh resolution images or videos.
Our contributions are summarized below: 1. This is the first work for general UHR image/video matting which enables full-resolution inference in one shot without running out of memory, thus eliminating the need of patch partitioning and patch artifacts. 2. We show how to obtain accurate spatio-temporal spar-sity for general UHR matting, which has never been adequately discussed in previous works. In addition, this is the first work that proposes to apply sparse con-volution network to skip unnecessary computations in dealing with UHR matting. 3. We conduct extensive experiments in multiple popu-lar image/video matting datasets, including Adobe Im-age Matting Dataset [48], VideoMatte240K [42] and our self-collected UHR matting dataset, and provide promising qualitative results, which demonstrate the superiority of our SparseMat in dealing with general image/video matting. 2.