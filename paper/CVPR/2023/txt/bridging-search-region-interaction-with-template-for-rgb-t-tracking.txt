Abstract
RGB-T tracking aims to leverage the mutual enhance-ment and complement ability of RGB and TIR modalities for improving the tracking process in various scenarios, where cross-modal interaction is the key component. Some previ-ous methods concatenate the RGB and TIR search region features directly to perform a coarse interaction process with redundant background noises introduced. Many other methods sample candidate boxes from search frames and conduct various fusion approaches on isolated pairs of RGB and TIR boxes, which limits the cross-modal interaction within local regions and brings about inadequate context modeling. To alleviate these limitations, we propose a novel
Template-Bridged Search region Interaction (TBSI) module which exploits templates as the medium to bridge the cross-modal interaction between RGB and TIR search regions by gathering and distributing target-relevant object and envi-ronment contexts. Original templates are also updated with enriched multimodal contexts from the template medium.
Our TBSI module is inserted into a ViT backbone for joint feature extraction, search-template matching, and cross-modal interaction. Extensive experiments on three popu-lar RGB-T tracking benchmarks demonstrate our method achieves new state-of-the-art performances. Code is avail-able at https://github.com/RyanHTR/TBSI.
Figure 1. Comparison between our cross-modal interaction ap-proach and previous ones. (a) Features of RGB and TIR search frames are directly concatenated. (b) Candidate boxes (RoIs) are sampled from RGB and TIR search frames and fused in pairs with gating or attention mechanisms. (c) Our approach exploits tem-plate tokens as the medium to bridge the cross-modal interaction between RGB and TIR search region tokens. 1.

Introduction
Given the initial state of a single target object in the first frame, the goal of single object tracking (SOT) is to local-ize the target object in successive frames. As a fundamen-tal task in the computer vision community, SOT has drawn the great attention of researchers. However, current SOT methods built on only visible light (RGB) data become vul-nerable under extreme imaging conditions (e.g., low illumi-*Corresponding author nation and adverse weather, etc), which motivates the in-corporation of thermal infrared (TIR or T) data for mutual enhancement and complement. Benefiting from the strong nocturnal photosensitivity and penetration ability of thermal infrared data, RGB-T tracking enjoys wide potential appli-cations such as video surveillance processing [1], intelligent robotics [5], and autonomous driving [8].
As a multimodal vision task, the key to RGB-T tracking is how to perform effective cross-modal interaction. Since the tracking process occurs in successive frames guided by the annotated initial frame, cross-modal interaction be-tween search frames of RGB and TIR modalities becomes the main focus. As illustrated in Figure 1 (a), some pre-vious methods [16, 44] directly concatenate features of the whole RGB and TIR search frames from the encoders of strong base trackers [4, 40]. This simple manner tends to introduce redundant background noise information, making cross-modal interaction too coarse and hence harming the modelâ€™s discriminative ability. In addition, there are many other methods [14,27,28,37,39,49] which sample candidate boxes (RoIs) from the Gaussian distribution in the search frames and conduct various fusion operators based on at-tention, gating mechanism, or dataset attributes, etc, to fuse each pair of RoI features of RGB and TIR modalities as shown in Figure 1 (b). Then, fused RoI features are sep-arately fed into a binary classifier to distinguish the target object. However, each pair of RoIs merely crops a small portion of local features from the search frames, contain-ing limited foreground and background information. Thus, cross-modal interaction between each isolated pair of RoIs may bring about inadequate modeling of the global envi-ronment context in the search frame and restrict the mutual enhancement and complement effect of the two modalities.
Given the above discussion, we argue that direct cross-modal interaction between RGB and TIR search frames or candidate RoIs still has limitations in comprehensively leveraging complementary multimodal clues to facilitate the tracking process. Therefore, we propose a novel scheme which exploits the target templates as the medium to bridge the cross-modal interaction between RGB and TIR search regions, as illustrated in Figure 1 (c). The major superior-ity motivating our advocate of this scheme is that the tem-plates contain original multimodal information of the target object, which can serve as strong guidance to extract target-relevant object and environment contexts from search re-gions for adaptive and precise information enhancement and complement. The background noises of other distrac-tors in search regions can also be reduced by template bridg-ing during the cross-modal interaction process.
In order to implement the above scheme, we design a
Template-Bridged Search region Interaction (TBSI) mod-ule. Concretely, our TBSI module first fuses features of
RGB and TIR templates to obtain the multimodal context medium. Since the cross-attention mechanism [36] is an effective and widely-adopted practice for context aggrega-tion, our TBSI also utilizes it with the fused template as query and TIR search region feature as key and value to gather target-relevant TIR context information into the tem-plate medium. Then, the RGB search region feature serves as query and the fused template serves as key and value to distribute target-relevant TIR context from the medium to the RGB search region. Similarly, target-relevant RGB context is also gathered and distributed to the TIR search region through the template medium in a reverse direction.
Finally, comprehensive multimodal information aggregated in the fused template is transferred back to the original RGB and TIR templates to update them with the enriched multi-modal contexts gathered from search regions.
In addition, most existing RGB-T tracking methods [14, 27,28,37,39,49] employ MDNet [32] with VGG-M [34] as the base tracker, whose number of classification branches equals the number of training sequences, which largely lim-its their capacity and scalability. Inspired by the powerful ability of Vision Transformer (ViT) [12] to capture long-range dependencies and its recent success on SOT [7, 24, 42], we also extend ViT to RGB-T tracking for joint fea-ture extraction, search-template matching, and cross-modal interaction. Our TBSI module is inserted into the ViT base tracker to bridge the intra-modal information flow within the Transformer layers for effective RGB-T tracking.
Our contributions are summarized as follows: (1) We propose a novel Template-Bridged Search region Interac-tion (TBSI) module which exploits the fused target tem-plate as the medium to bridge the cross-modal interaction between RGB and TIR search regions and update original templates as well, forming adaptive and precise information enhancement. (2) We extend the ViT architecture with the proposed TBSI module to RGB-T tracking for joint feature extraction, search-template matching, and cross-modal in-teraction, which has not been explored by previous methods to our best knowledge. (3) Extensive experiments demon-strate that our method achieves new state-of-the-art perfor-mances on three popular RGB-T tracking benchmarks. 2.