Abstract
The task of human reposing involves generating a realis-tic image of a person standing in an arbitrary conceivable pose. There are multiple difficulties in generating percep-tually accurate images, and existing methods suffer from limitations in preserving texture, maintaining pattern co-herence, respecting cloth boundaries, handling occlusions, manipulating skin generation, etc. These difficulties are fur-ther exacerbated by the fact that the possible space of pose orientation for humans is large and variable, the nature of clothing items is highly non-rigid, and the diversity in body shape differs largely among the population. To alle-viate these difficulties and synthesize perceptually accurate images, we propose VGFlow. Our model uses a visibility-guided flow module to disentangle the flow into visible and invisible parts of the target for simultaneous texture preser-vation and style manipulation. Furthermore, to tackle dis-tinct body shapes and avoid network artifacts, we also in-corporate a self-supervised patch-wise ”realness” loss to improve the output. VGFlow achieves state-of-the-art re-sults as observed qualitatively and quantitatively on differ-ent image quality metrics (SSIM, LPIPS, FID). Results can be downloaded from Project Webpage 1.

Introduction
People are frequently featured in creative content like display advertisements and films. As a result, the ability to easily edit various aspects of humans in digital visual media is critical for rapidly producing such content. Changing the pose of humans in images, for example, enables several ap-plications, such as automatically generating movies of peo-ple in action and e-commerce merchandising. This paper presents a new deep-learning-based framework for reposing
*rishabhj@adobe.com
Figure 1. Human reposing involves changing the orientation of a source image to a desired target pose. To get accurate results, we learn to preserve the region visible (green) in the source image and transfer the appropriate style to the invisible region (red) humans guided by a target pose, resulting in high-quality and realistic output.
Recent approaches for human-image reposing based on deep-learning neural networks, such as [19,26,39], require a person image, their current pose, represented as a sequence of key-points or a 2D projection of a 3D body-pose map, and the target pose represented similarly. These methods fail to reproduce accurate clothing patterns, textures, or re-alistic reposed human images. This mainly happens when either the target pose differs significantly from the current (source) pose, there are heavy bodily occlusions, or the gar-ments are to be warped in a non-rigid manner to the target pose. Many of these failures can be attributed to the in-ability of these networks to discern regions of the source image that would be visible in the target pose from those that would be invisible. This is an important signal to deter-mine which output pixels must be reproduced from the in-put directly and which must be predicted from the context.
We present VGFlow, a framework for human image repos-ing that employs a novel visibility-aware detail extraction mechanism to effectively use the visibility input for preserv-ing details present in the input image.
VGFlow consists of two stages - encoding the changes
in appearance and pose of the source image required to achieve the new pose and decoding the encoded input to the re-posed human image. The encoding stage includes a pose-based warping module that takes the source image and the source and target pose key-points as input and pre-dicts two 2D displacement fields. One corresponds to the visible region of the source image in the target pose, and the other to the invisible areas.
It also predicts a visibil-ity mask indicating both visible and invisible regions in the source image, as they should appear in the target pose. The displacement fields, known as appearance flows, are used to sample pixels from the source image to produce two warped images. These warped images and the visibility masks are then encoded into the appearance features, a multi-scale feature pyramid. The encoding stage also tries to capture the relationship between the source and target poses by en-coding their respective key-points together. The encoded pose key-points are translated into an image during the de-coding stage, with the appearance features modulating the translation at each scale. This appearance-modulated pose to image decoding provides the final reposed output, which is then subjected to multiple perceptual and reconstruction losses during training.
The vast majority of existing methods [5, 26, 27, 39] are trained using paired source and target images. However, in terms of output realism, we observe various artifacts and a lack of generalization in these methods to unpaired in-puts, especially when the source image differs significantly in body shape or size [20]. To that end, VGFlow is trained with a self-supervised patch-wise adversarial loss on un-paired images alongside the pairwise supervised loss to en-sure a high level of realism in the final output.
In sum-mary, this paper proposes a new human reposing network
VGFlow, based on:
• A novel visibility-aware appearance flow prediction module to disentangle visible and invisible regions of the person image in the target pose.
• An image decoder employing multi-scale texture mod-ulated pose encoding.
• And, a patch-wise adversarial objective to improve the realism of the produced images leading to fewer output artifacts.
Our method achieves state-of-the-art on image quality metrics for the human reposing task. We present extensive qualitative and quantitative analysis with previous base-lines, as well as ablation studies. Next, we discuss work related to the proposed method. 2.