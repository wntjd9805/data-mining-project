Abstract
Due to the outstanding capability for data generation, Gen-erative Adversarial Networks (GANs) have attracted considerable attention in unsupervised learning. However, training GANs is difficult, since the training distribution is dynamic for the discriminator, leading to unstable image
In this paper, we address the problem representation. of training GANs from a novel perspective, i.e., robust image classification. Motivated by studies on robust image representation, we propose a simple yet effective module, namely AdaptiveMix, for GANs, which shrinks the regions of training data in the image representation space of the discriminator. Considering it is intractable to directly bound feature space, we propose to construct hard samples and narrow down the feature distance between hard and easy samples. The hard samples are constructed by mixing a pair of training images. We evaluate the effectiveness of our AdaptiveMix with widely-used and state-of-the-art
GAN architectures. The evaluation results demonstrate that our AdaptiveMix can facilitate the training of GANs and effectively improve the image quality of generated samples. We also show that our AdaptiveMix can be further applied to image classification and Out-Of-Distribution (OOD) detection tasks, by equipping it with state-of-the-art methods. Extensive experiments on seven publicly available datasets show that our method effectively boosts the performance of baselines. The code is publicly avail-able at https : / / github . com / WentianZhang -ML/AdaptiveMix. 1.

Introduction
Artificial Curiosity [40, 41] and Generative Adversar-ial Networks (GANs) have attracted extensive attention due to their remarkable performance in image generation
† Equal Contribution
Corresponding Authors: Bing Li and Yuexiang Li.
Figure 1. Results generated by StyleGAN-V2 [20] and our method (StyleGAN-V2 + AdaptiveMix) on AFHQ-Cat and FFHQ-5k. We propose a simple yet effective module AdaptiveMix, which can be used for helping the training of unsupervised GANs. When trained on a small amount of data, StyleGAN-V2 generates images with artifacts, due to unstable training. However, our AdaptiveMix effectively boosts the performance of StyleGAN-V2 in terms of image quality.
[18,45,55,57]. A standard GAN consists of a generator and a discriminator network, where the discriminator is trained to discriminate real/generated samples, and the generator aims to generate samples that can fool the discriminator.
Nevertheless, the training of GANs is difficult and unstable, leading to low-quality generated samples [23, 34].
Many efforts have been devoted to improving the train-ing of GANs (e.g. [1, 5, 12, 25, 27, 34, 39]). Previous studies
[37] attempted to co-design the network architecture of the generator and discriminator to balance the iterative training.
Following this research line, PG-GAN [16] gradually trains
the GANs with progressive growing architecture according to the paradigm of curriculum learning. More recently, data augmentation-based methods, such as APA [15], ADA [17], and adding noises into the generator [20], were further pro-posed to stabilize the training of GANs. A few works ad-dress this problem on the discriminator side. For example,
WGAN [1] proposes to enforce a Lipschitz constraint by us-ing weight clipping. Instead, WGAN-GP [6] directly penal-izes the norm of the discriminator’s gradient. These meth-ods have shown that revisions of discriminators can achieve promising performance. However, improving the training of GANs remains an unsolved and challenging problem.
In this paper, considering that the discriminator is critical to the training of GANs, we address the problem of training
GANs from a novel perspective, i.e., robust image classifi-cation. In particular, the discriminator can be regarded as performing a classification task that discriminates real/fake samples. Our insight is that controlling the image repre-sentation (i.e., feature extractor) of the discriminator can improve the training of GANs, motivated by studies on ro-bust image classification [28, 44]. More specifically, recent work [44] on robust image representation presents inspir-ing observations that training data is scattered in the learn-ing space of vanilla classification networks; hence, the net-works would improperly assign high confidences to samples that are off the underlying manifold of training data. This phenomenon also leads to the vulnerability of GANs, i.e., the discriminator cannot focus on learning the distribution of real data. Therefore, we propose to shrink the regions of training data in the image representation space of the dis-criminator.
Different from existing works [15, 17], we explore a question for GANs: Would the training stability of GANs be improved if we explicitly shrink the regions of training data in the image representation space supported by the discrim-inator? To this end, we propose a module named Adap-tiveMix to shrink the regions of training data in the latent space constructed by a feature extractor. However, it is non-trivial and challenging to directly capture the boundaries of feature space. Instead, our insight is that we can shrink the feature space by reducing the distance between hard and easy samples in the latent space, where hard samples are regarded as the samples that are difficult for classification networks to discriminate/classify. To this end, AdaptiveMix constructs hard samples by mixing a pair of training images and then narrows down the distance between mixed images and easy training samples represented by the feature extrac-tor for feature space shrinking. We evaluate the effective-ness of our AdaptivelyMix with state-of-the-art GAN archi-tectures, including DCGAN [37] and StyleGAN-V2 [20], which demonstrates that the proposed AdaptivelyMix facil-itates the training of GANs and effectively improves the im-age quality of generated samples.
Besides image generation, our AdaptiveMix can be ap-plied to image classification [9,53] and Out-Of-Distribution (OOD) detection [11, 14, 50] tasks, by equipping it with suitable classifiers. To show the way of applying Adap-tiveMix, we integrate it with the Orthogonal classifier in recent start-of-the-art work [52] in OOD. Extensive experi-mental results show that our AdaptiveMix is simple yet ef-fective, which consistently boosts the performance of [52] on both robust image classification and Out-Of-Distribution tasks on multiple datasets.
In a nutshell, the contribution of this paper can be sum-marized as:
• We propose a novel module, namely AdaptiveMix, to improve the training of GANs. Our AdaptiveMix is simple yet effective and plug-and-play, which is help-ful for GANs to generate high-quality images.
• We show that GANs can be stably and efficiently trained by shrinking regions of training data in image representation supported by the discriminator.
• We show our AdaptiveMix can be applied to not only image generation, but also OOD and robust image classification tasks. Extensive experiments show that our AdaptiveMix consistently boosts the performance of baselines for four different tasks (e.g., OOD) on seven widely-used datasets. 2.