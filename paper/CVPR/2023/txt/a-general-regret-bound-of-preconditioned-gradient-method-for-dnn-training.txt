Abstract
While adaptive learning rate methods, such as Adam, have achieved remarkable improvement in optimizing Deep
Neural Networks (DNNs), they consider only the diago-nal elements of the full preconditioned matrix. Though the full-matrix preconditioned gradient methods theoretically have a lower regret bound, they are impractical for use to train DNNs because of the high complexity. In this paper, we present a general regret bound with a constrained full-matrix preconditioned gradient, and show that the updat-ing formula of the preconditioner can be derived by solving a cone-constrained optimization problem. With the block-diagonal and Kronecker-factorized constraints, a specific guide function can be obtained. By minimizing the up-per bound of the guide function, we develop a new DNN optimizer, termed AdaBK. A series of techniques, includ-ing statistics updating, dampening, efficient matrix inverse root computation, and gradient amplitude preservation, are developed to make AdaBK effective and efficient to imple-ment. The proposed AdaBK can be readily embedded into many existing DNN optimizers, e.g., SGDM and AdamW, and the corresponding SGDM BK and AdamW BK algo-rithms demonstrate significant improvements over existing
DNN optimizers on benchmark vision tasks, including im-age classification, object detection and segmentation. The code is publicly available at https://github.com/
Yonghongwei/AdaBK. 1.

Introduction
Stochastic gradient descent (SGD) [26] and its vari-ants [21, 23], which update the parameters along the oppo-site of their gradient directions, have achieved great success in optimizing deep neural networks (DNNs) [14, 24]. In-stead of using a uniform learning rate for different parame-ters, Duchi et al. [5] proposed the AdaGrad method, which adopts an adaptive learning rate for each parameter, and proved that AdaGrad can achieve lower regret bound than
SGD. Following AdaGrad, a class of adaptive learning rate gradient descent methods has been proposed. For example,
RMSProp [30] and AdaDelta [35] introduce the exponential moving average to replace the sum of second-order statis-tics of the gradient for computing the adaptive learning rate.
Adam [15] further adopts the momentum into the gradient, and AdamW [22] employs a weight-decoupled strategy to improve the generalization performance. RAdam [18], Ad-abelief [38] and Ranger [19,32,37] are proposed to acceler-ate training and improve the generalization capability over
Adam. The adaptive learning rate methods have become the mainstream DNN optimizers. t ) 1 t=1 gtg⊤
In addition to AdaGrad, Duchi et al. [5] provided a full-matrix preconditioned gradient descent (PGD) method that adopts the matrix HT = ((cid:80)T 2 to adjust the gra-dient gT , where t denotes the iteration number and T is
It has been proved the number of the current iteration.
[5] that the preconditioned gradient H −1
T gT has a lower regret bound than the adaptive learning rate methods that only consider the diagonal elements of HT . However, the full-matrix preconditioned gradient is impractical to use due to its high dimension, which limits its applica-tion to DNN optimization. Various works have been re-ported to solve this problem in parameter space by adding some structural constraints on the full-matrix HT . For in-stances, GGT [1] stores only the gradients of recent itera-tions so that the matrix inverse root can be computed effi-ciently by fast low-rank computation tricks. Yun et al. [34] proposed a mini-block diagonal matrix framework to re-duce the cost through coordinate partitioning and grouping strategies. Gupta et al. [9] proposed to extend AdaGrad with Kronecker products of full-matrix preconditioners to make it more efficient in DNN training. Besides, natural gradient approaches [6, 7], which adopt the approximations of the Fisher matrix to correct the descent direction, can also be regarded as full-matrix preconditioners.
The existing constrained PGD (CPGD) methods, how-ever, are heuristic since manually designed approximations to the full matrix HT are employed in them, while their in-fluence on the regret bound is unknown. By far, they lack a general regret-bound theory that can guide us to design the full-matrix preconditioned gradient methods. On the other hand, the practicality and effectiveness of these precondi-tioner methods are also an issue, which prevents them from being widely used in training DNNs.
To address the above-mentioned issues, in this paper we present a theorem to connect the regret bound of the con-strained full-matrix preconditioner with a guide function.
By minimizing the guide function under the constraints, an updating formula of the preconditioned gradient can be de-rived. That is, optimizing the guide function of the precon-ditioner will minimize its regret bound at the same time, while different constraints can yield different updating for-mulas. With the commonly-used constraints on DNN pre-conditioners, such as the block-diagonal and Kronecker-factorized constraints [7, 9], specific guide functions can be obtained. By minimizing the upper bound of the guide func-tion, a new optimizer, namely AdaBK, is derived.
We further propose a series of techniques, including statistics updating, dampening, efficient matrix inverse root computation and gradient norm recovery, to make AdaBK more practical to use for DNN optimization. By embedding
AdaBK into SGDM and AdamW (or Adam), we develop two new DNN optimizers, SGDM BK and AdamW BK.
With acceptable extra computation and memory cost, they achieve significant performance gain in convergence speed and generalization capability over state-of-the-art DNN op-timizers, as demonstrated in our experiments in image clas-sification, object detection and segmentation.
For a better understanding of our proposed regret bound and the developed DNN optimizer, in Fig. 1, we illustrate the existing major DNN optimizers and their relationships.
SGD and its momentum version (SGDM) apply the same learning rate to all parameters based on their gradient de-scent directions. The adaptive learning rate methods as-sign different learning rates to different parameters by using second-order information of the gradients, achieving bet-ter convergence performance. The adaptive learning rate methods can be viewed as special cases of PGD methods by considering only the diagonal elements of the full pre-conditioned matrix of gradients. Our method belongs to the class of PGD methods, while our proposed general regret bound of constrained PGD methods can be applied to the
PGD optimizers under different constraints, including Ada-Grad, Full-Matrix AdaGrad and our AdaBK. (cid:80)n
Notation system. We denote by wt and gt the weight vector and its gradient of a DNN model in the t-th iteration.
Denote by gt,i the gradient of the i-th sample in a batch in the t-th iteration, we have gt = 1 i=1 gt,i, where n n is the batch size. The notations A ⪰ 0 and A ≻ 0 for a matrix A denote that A is symmetric positive semidef-inite (PSD) and symmetric positive definite, respectively.
A ⪰ B or A − B ⪰ 0 means that A − B is PSD. Tr(A) represents the trace of the matrix A. For a PSD matrix A,
Aα = U ΣαU ⊤, where U ΣU ⊤ is the Singular Value De-x⊤Ax is the Maha-composition (SVD) of A. ||x||A =
√
Figure 1. Illustration of the main DNN optimizers.
√
A = lanobis norm of x induced by PSD matrix A, and its dual norm is ||x||∗ x⊤A−1x. A ⊗ B means the Kro-necker product of A and B, while A ⊙ B and A⊙α are the element-wise matrix product and element-wise power oper-ation, respectively. Diag(x) is a diagonal matrix with diag-onal vector x, and vec(·) denotes the vectorization function. 2.