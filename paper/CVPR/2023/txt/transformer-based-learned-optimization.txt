Abstract
We propose a new approach to learned optimization where we represent the computation of an optimizerâ€™s up-date step using a neural network. The parameters of the op-timizer are then learned by training on a set of optimization tasks with the objective to perform minimization efficiently.
Our innovation is a new neural network architecture, Opti-mus, for the learned optimizer inspired by the classic BFGS algorithm. As in BFGS, we estimate a preconditioning ma-trix as a sum of rank-one updates but use a Transformer-based neural network to predict these updates jointly with the step length and direction. In contrast to several recent learned optimization-based approaches [24, 27], our for-mulation allows for conditioning across the dimensions of the parameter space of the target problem while remaining applicable to optimization tasks of variable dimensionality without retraining. We demonstrate the advantages of our approach on a benchmark composed of objective functions traditionally used for the evaluation of optimization algo-rithms, as well as on the real world-task of physics-based visual reconstruction of articulated 3d human motion. 1.

Introduction
This work focuses on a new learning-based optimiza-tion methodology. Our approach belongs to the category of learned optimization methods, which represent the up-date step of an optimizer by means of an expressive function such as a multi-layer perceptron. We then learn the param-eters of this function on a set of training optimization tasks.
Since the update function of the learned optimizers is esti-mated from data, it can in principle learn various desirable behaviors such as learning-rate schedules [22] or strategies for the exploration of multiple local minima [23]. This is in contrast to traditional optimizers such as Adam [15], or
*Work done during an internship at Google.
BFGS [11] in which updates are derived in terms of first-principles. However, as these are general and hard-coded, they may not be able to take advantage of the regularities in the loss functions for specific classes of problems.
Learned optimizers are particularly appealing for appli-cations that require repeatedly solving related optimization tasks. For example, 3d human pose estimation is often for-mulated as the minimization of a particular loss function
[12, 19, 30, 46]. Such approaches estimate the 3d state (e.g. pose and shape) given image observations by repeatedly op-timizing the same objective function for many closely re-lated problems, including losses and state contexts. Tra-ditional optimization treats each problem as independent, which is potentially suboptimal as it does not aggregate ex-perience across multiple related optimization runs.
The main contribution of this paper is a novel neural net-work architecture for learned optimization. Our architecture is inspired by classical BFGS approaches that iteratively es-timate the Hessian matrix to precondition the gradient. Sim-ilarly to BFGS, our approach iteratively updates the pre-conditioner using rank-one updates. In contrast to BFGS, we use a transformer-based [40] neural network to generate such updates from features encoding an optimization tra-jectory. We train the architecture using Persistent Evolu-tion Strategies (PES) introduced in [41]. In contrast to prior work [4, 24, 27], which rely on updates over each target pa-rameter independently (or coupled only via normalization), our approach allows for more complex inter-dimensional dependencies via self-attention while still showing good generalization to different target problem sizes than those used in training. We refer to our learned optimization ap-proach as Optimus in the sequel.
We evaluate Optimus on classical optimization objec-tives used to benchmark optimization methods in the liter-ature [17, 31, 37] (cf. fig. 1) as well as on a real-world task of physics-based human pose reconstruction. In our exper-iments, we typically observe that Optimus is able to reach a lower objective value compared to popular off-the-shelf
Figure 1. Top row: Evaluation results showing average objective value reached by the optimizer for the corresponding objective function in the top row (y-axis) vs. dimensionality of the objective function (x-axis). Bottom row: examples of objective functions used for evaluation of our approach. From left to right: Rastrigin [28], Levy [17], Ackley [1] and Rosenbrock [31] functions. For each function, we visualize the surface of the 2d version. optimizers while taking fewer iterations to converge. For example, we observe at least a 10x reduction in the number of update steps for half of the classical optimization prob-lems (see fig. 4). To evaluate Optimus in the context of physics-based human motion reconstruction, we apply it in conjunction with DiffPhy, which is a differentiable physics-based human model introduced in [12]. We experimentally demonstrate that Optimus generalizes well across diverse human motions (e.g. from training on walking to testing on dancing), is notably (5x) faster to meta-train compared to prior work [24], leads to reconstructions of better quality compared to BFGS, and is faster in minimizing the loss. 2.