Abstract
Humans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. How-ever, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT (“Human-Object conTact”), a new dataset of human-object contacts in images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons around the 2D im-age areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task, that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the sur-rounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizabil-ity. Our HOT data and model are available for research at https://hot.is.tue.mpg.de. 1.

Introduction
Contact is an important part of people’s everyday lives.
We constantly contact objects to move and perform tasks.
We walk by contacting the ground with our feet, we sit by contacting chairs with our buttocks, hips and back, we grasp and manipulate tools by contacting them with our hands.
Therefore, estimating contact between humans and objects is useful for human-centered AI, especially for applications
Figure 1. Our contact detector, trained on HOT (“Human-Object conTact”) dataset, estimates contact between humans and scenes from an image taken in the wild. Contact is important for interacting humans, yet, standard in-the-wild datasets unfortunately lack such information. Our contact dataset and detector are a step towards providing this in the wild. Images are from pexels.com. such as AR/VR [1,15,26,30], activity recognition [22,39,44], affordance detection [13, 25, 34, 67], fine-grained human-object interaction detection [27, 37, 51, 57], imitation learn-ing [38, 48, 63], populating scenes with avatars [18, 62, 64], and sanitization of spaces and objects.
In contrast to off-the-shelf detectors for segmenting hu-mans in images, or estimating their 2D joints or 3D shape and pose, there exists no general detector of contact. Some work exists for detecting part-specific contact, e.g., hand-object
[35, 45] or foot-ground [41, 49] contact, while other work estimates contact only in constrained environments [20, 46] with limited generalization. What we need, instead, is a contact detector for the entire body that estimates detailed, body-part-related, contact maps in arbitrary images. To train this, we need data, but no suitable dataset exists at the mo-ment. We address these limitations with a novel dataset and model for detecting contact between whole-body humans and objects in color images taken in the wild.
Annotating contact is challenging, as contact areas are ipso facto occluded. Think of a person standing on the floor; the sole of the shoe, and the floor area it contacts, can not be observed. A naive approach is to instrument a human with contact sensors, however, this is intrusive, cumbersome to set up and does not scale. Instead, we use
two alternative data sources, with different but complemen-tary properties: (1) We use the PROX [17] dataset, which has pseudo ground-truth 3D human meshes for real humans moving in 3D scanned scenes. We automatically annotate contact areas, by computing the proximity between the 3D (2) We use the V-COCO [16], HAKE [27], and meshes.
Watch-n-Patch [54] datasets, which contain images taken in the wild. We then hire professional annotators, and train them to annotate contact areas as 2D polygons in images.
Although manual annotation is only approximate, 2D an-notations are important because they allow scaling to large, varied, and natural datasets. This improves generalization.
Note that in both cases we also annotate the body part that is involved in contact, corresponding to the body parts of the
SMPL(-X) [32, 36] human model.
We thus present HOT (“Human-Object conTact”), a new dataset of images with human-object contact; see examples in Fig. 2. The first part of HOT, called “HOT-Generated” (Fig. 2b), has automatic annotations, but lacks variety for human subjects and scenes. The second part, called “HOT-Annotated” (Fig. 2a), has manual annotations, but has a huge variety of people, scenes and interactions. HOT has 35, 750 images with 163, 534 contact annotations.
We then train a new contact detector on our HOT dataset.
Given a single color image as input, we want to know, if contact takes place in the image, the area in which it occurs, as well as the body part that is involved. Specifically, we detect 2D heatmaps in an image, encoding the contact loca-tion and likelihood, and classify each pixel in contact to one of SMPL(-X)’s body parts. However, training directly with
HOT annotations leads to “bleeding” heatmaps and false detections. We observe that humans reason about contact by looking at body parts and their proximity to objects in their local vicinity. Therefore, we use a body-part-driven attention module that significantly boosts performance.
We evaluate our detector on withheld parts of the HOT dataset. Quantitative evaluation and ablation studies show that our model outperforms the baselines, and that all com-ponents contribute to detection performance. Our body-part attention module is the key component; a visual analysis shows that it attends to meaningful image locations, i.e., on body parts and their vicinity. Qualitative results show rea-sonable detections on in-the-wild images. By applying our detector on datasets unseen during training, we show that the model generalizes reasonably well; see Fig. 1. Then, we show that our general-purpose full-body contact detector performs on par with existing part-specific contact detectors for the foot [41] or hand [35], meaning it could serve as a drop-in replacement for these. Moreover, we show that our contact detector helps contact-driven 3D human pose estimation on PROX data [17]. Finally, we show that our
HOT dataset helps a state-of-the-art (SOTA) 3D body-scene contact estimator [20] generalize to in-the-wild images. (a) “HOT-Annotated” examples. (b) “HOT-Generated” examples. (c) Human body parts of SMPL-X.
Figure 2. Images and contact annotations for our HOT dataset. We show examples for both its parts, i.e., “HOT-Generated” (Sec. 3.2) and “HOT-Annotated” (Sec. 3.3). Contact annotations include the involved body part (c), shown color coded on a SMPL-X mesh.
In summary, HOT takes a step towards automatic con-tact detection between humans and objects in color images and our contributions are three-fold: (1) We introduce the task of full-body human-object contact detection in images. (2) To facilitate machine learning for this, we introduce the
HOT dataset with 2D contact area heatmaps and the asso-ciated human part labels as annotations, using both auto-generated and manual annotations. (3) We develop a new contact detector that incorporates a body-part attention mod-ule. Experiments and ablations show the benefits of the proposed model and its components. Our data and code are available at https://hot.is.tue.mpg.de. 2.