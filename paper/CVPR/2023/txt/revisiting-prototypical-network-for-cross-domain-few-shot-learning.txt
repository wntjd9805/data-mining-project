Abstract 1.

Introduction
Prototypical Network is a popular few-shot solver that aims at establishing a feature metric generalizable to novel few-shot classiﬁcation (FSC) tasks using deep neural net-works. However, its performance drops dramatically when generalizing to the FSC tasks in new domains. In this study, we revisit this problem and argue that the devil lies in the simplicity bias pitfall in neural networks. In speciﬁc, the network tends to focus on some biased shortcut features (e.g., color, shape, etc.) that are exclusively sufﬁcient to dis-tinguish very few classes in the meta-training tasks within a pre-deﬁned domain, but fail to generalize across domains as some desirable semantic features. To mitigate this prob-lem, we propose a Local-global Distillation Prototypical
Network (LDP-net). Different from the standard Prototypi-cal Network, we establish a two-branch network to classify the query image and its random local crops, respectively.
Then, knowledge distillation is conducted among these two branches to enforce their class afﬁliation consistency. The rationale behind is that since such global-local semantic re-lationship is expected to hold regardless of data domains, the local-global distillation is beneﬁcial to exploit some cross-domain transferable semantic features for feature metric establishment. Moreover, such local-global seman-tic consistency is further enforced among different images of the same class to reduce the intra-class semantic variation of the resultant feature. In addition, we propose to update the local branch as Exponential Moving Average (EMA) over training episodes, which makes it possible to better distill cross-episode knowledge and further enhance the generalization performance. Experiments on eight cross-domain FSC benchmarks empirically clarify our argument and show the state-of-the-art results of LDP-net. Code is available in https://github.com/NWPUZhoufei/LDP-Net
*F. Zhou and P. Wang contributed equally in this work.
†Corresponding author.
Prototypical Network (ProtoNet) [1] is a popular few-shot classiﬁcation (FSC) method, which works by establish-ing a feature metric generalizable to novel few-shot tasks using deep neural networks.
It adopts an episode-based learning strategy, where each episode, e.g., N -way K-shot, is formulated as a contrastive learning task to identify the correct class for each query sample from a set of limited classes represented by prototypes derived from few sup-port samples. Thanks to the simplicity of the framework and appealing few-shot learning performance, ProtoNet has gained great research attention [2–5].
However, the performance of typical ProtoNet declines greatly when generalizing to FSC tasks in new domains, e.g., apply the ProtoNet trained on natural images in mini-ImageNet [6] to the ﬁne-grained bird images in CUB [7].
This severely restricts the practicality of ProtoNet in real applications. In this work, we propose to re-inspect the in-trinsic reason for the limited cross-domain generalization capability of ProtoNet and revive it in the cross-domain set-ting with right medicine. Speciﬁcally, the key for cross-domain generalization, especially in few-shot setting with
ProtoNet, lies on exploiting some semantic information of each class that is invariant across different domains. To this end, typical ProtoNet resorts to taking advantages of the great expressive capacity of deep neural networks for feature learning. Obviously, it fails to exploit the desirable cross-domain transferable semantic features. In that case, what feature representation are obtained by the deep neu-ral network? Some recent works [8–10] may have found the possible answer, viz., simplicity bias. It has shown that neural networks exclusively latch on to the simplest feature (e.g., color, shape, etc.) and tends to ignore the complex predictive features (e.g., semantics of the object). Inspired by this, we argue that the limited cross-domain generaliza-tion capacity of ProtoNet is incurred by the simplicity bias, viz., it tends to exploit some biased shortcut features that are exclusively sufﬁcient to distinguish very few classes in the meta-training tasks within a pre-deﬁned domain, but prone to be variant across different domains.
To mitigate this problem, we propose a Local-global
Distillation Prototypical Network (LDP-net) to identify im-age features and metric that can generalize better to FSC tasks in new domains. The network employs a two-branch structure. A global branch predicts the class afﬁliation for each query image, which is akin to standard ProtoNet. A local branch works with image patches randomly cropped from the query image and makes classiﬁcation predictions for such local crops. We then perform knowledge distilla-tion between these two branches to enforce a global image and its local patches to have consistent class afﬁliation pre-dictions. The rationale behind are twofold. Firstly, compar-ing to biased visual patterns, the semantic relationship be-tween global image and local patches can hold more gener-ally regardless of data domains. Secondly, the local-global distillation enables embedding richer semantic information from local features into the ﬁnal global feature representa-tion, which are proven to be more domain-invariant [11].
Take a step further, we apply such afﬁliation consistency constraint across images belonging to the same class. By doing this, we can reduce the intra-class semantic varia-tion and further improve the robustness of the image feature representations. In addition, the local branch is updated as
Exponential Moving Average (EMA) of the global branch to produce robust classiﬁcation predictions, which enables our model to distill cross-episode knowledge and enhance the generalization performance. Once the model is trained, only the global branch is retained as a feature extractor for cross-domain FSC evaluation. Notably, by simply freezing the feature extractor in a new domain, the proposed method achieves state-of-the-art results on eight cross-domain FSC benchmark datasets.
The major contributions of this study can be summarized as follows:
• We inspect the limited cross-domain generalization ca-pability of typical ProtoNet from the perspective of simplicity bias and propose a local-global knowledge distillation framework to effectively mitigate this prob-lem.
• The proposed LDP-Net has insightful and innovative designs and can learn a robust feature metric that gen-eralizes better to FSC tasks in new domains.
• The proposed LDP-Net achieves state-of-the-art per-formance on a set of cross-domain FSC benchmarks. 2.