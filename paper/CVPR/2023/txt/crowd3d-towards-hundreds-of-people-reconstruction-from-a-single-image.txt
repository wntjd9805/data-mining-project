Abstract
Image-based multi-person reconstruction in wide-field large scenes is critical for crowd analysis and security alert.
However, existing methods cannot deal with large scenes containing hundreds of people, which encounter the chal-lenges of large number of people, large variations in hu-man scale, and complex spatial distribution. In this paper, we propose Crowd3D, the first framework to reconstruct the 3D poses, shapes and locations of hundreds of people with global consistency from a single large-scene image. The core of our approach is to convert the problem of complex crowd localization into pixel localization with the help of our newly defined concept, Human-scene Virtual Interac-tion Point (HVIP). To reconstruct the crowd with global consistency, we propose a progressive reconstruction net-work based on HVIP by pre-estimating a scene-level cam-era and a ground plane. To deal with a large number of per-sons and various human sizes, we also design an adaptive human-centric cropping scheme. Besides, we contribute a benchmark dataset, LargeCrowd, for crowd reconstruc-tion in a large scene. Experimental results demonstrate the effectiveness of the proposed method. The code and the
† Equal contribution.
* Corresponding author. dataset are available at http://cic.tju.edu.cn/ faculty/likun/projects/Crowd3D. 1.

Introduction 3D pose, shape and location reconstruction for hundreds of people in a large scene will help with modeling crowd be-havior for simulation and security monitoring. However, no existing methods can achieve this with global consistency.
In this paper, we aim to reconstruct the 3D poses, shapes and locations of hundreds of people in the global camera space from a single large-scene image, as shown in Fig. 1.
Although monocular human pose and shape estimation
[15, 37, 43] has been extensively explored over the past years, estimating global space locations together with hu-man poses and shapes for multiple people from a single im-age is still a difficult problem due to the depth ambiguity.
Existing methods [13, 35] reconstruct 3D poses, shapes and relative positions of the reconstructed human meshes by as-suming a constant focal length. But the methods are limited to small scenes with a common FoV (Field of View). These methods cannot regress the people from a whole large-scene image [41] due to the relatively small and varying human scales in comparison to the image size. Even with an image cropping strategy, these methods cannot obtain consistent
reconstructions in the global camera space due to indepen-dent inference from the cropped images. Besides, existing methods hardly consider the coherence of the reconstructed people with the outdoor scene, especially with the ground, since the ground is a common and significant element of outdoor scenes. Taking a usual urban scene as an example, these methods may include wrong positions and rotations so that the reconstructed people do not appear to be standing or walking on the ground.
In general, there are three challenges in reconstructing hundreds of people with global consistency from a single large-scene image: 1) there are a large number of people with relatively small and highly varying 2D scales; 2) due to the depth ambiguity from a single view, it is difficult to directly estimate absolute 3D positions and 3D poses of people in the large scene; 3) there is no large-scene image datasets with hundreds of people for supervising crowd re-construction in large scenes.
In this paper, to address these challenges, we propose
Crowd3D, the first framework for crowd reconstruction from a single large-scene image. To deal with the large number of people and various human scales, we propose an adaptive human-centric cropping scheme for a consistent scale proportion of people among different cropped images by leveraging the observation of pyramid-like changes in the scales of people in large-scene images. To ensure the globally consistent spatial locations and coherence with the scene, we propose a progressive ground-guided reconstruc-tion network Crowd3DNet to reconstruct globally consis-tent human body meshes from the cropped images by pre-estimating a global scene-level camera and a ground plane.
To alleviate the ambiguity brought in by directly estimat-ing absolute 3D locations from a single image, we present a novel concept called Human-scene Virtual Interaction Point (HVIP) for effectively converting the 3D crowd spatial lo-calization problem into a progressive 2D pixel localization problem with intermediate supervisions. Benefiting from
HVIP, our model can reconstruct the people with various poses including non-standing.
We also construct LargeCrowd, a benchmark dataset with over 100K labeled humans (2D bounding boxes, 2D keypoints, 3D ground plane and HVIPs) in 733 gigapixel images (19200×6480) of 9 different scenes. To our best knowledge, this is the first large-scene crowd dataset, which enables the training and evaluation on large-scene images with hundreds of people. Experimental results demonstrate that our method achieves globally consistent crowd recon-struction in a large scene. Fig. 1 gives an example.
To summarize, our main contributions include: 1) We propose Crowd3D, a multi-person 3D pose, shape and location estimation framework for large-scale scenes with hundreds of people. We design an adap-tive human-centric cropping scheme and a joint local and global strategy to achieve the globally consistent reconstruction. 2) We propose a progressive reconstruction network with the newly defined HVIP, to alleviate the depth ambigu-ity and obtain global reconstructions in harmony with the scene. 3) We contribute LargeCrowd, a benchmark dataset with over 100K labeled crowded people in 733 gigapixel large-scene images (19200×6480), which are valuable for the training and evaluation of crowd reconstruction and spatial reasoning in large scenes. 2.