Abstract
Image recognition on expert domains is usually fine-grained and requires expert labeling, which is costly. This limits dataset sizes and the accuracy of learning systems.
To address this challenge, we consider annotating expert data with crowdsourcing. This is denoted as PrOfeSsional lEvel cRowd (POSER) annotation. A new approach, based on semi-supervised learning (SSL) and denoted as SSL with
It is a human-in-human filtering (SSL-HF) is proposed. the-loop SSL method, where crowd-source workers act as filters of pseudo-labels, replacing the unreliable confidence thresholding used by state-of-the-art SSL methods. To en-able annotation by non-experts, classes are specified im-plicitly, via positive and negative sets of examples and aug-mented with deliberative explanations, which highlight re-gions of class ambiguity. In this way, SSL-HF leverages the strong low-shot learning and confidence estimation ability of humans to create an intuitive but effective labeling ex-perience. Experiments show that SSL-HF significantly out-performs various alternative approaches in several bench-marks. 1.

Introduction
While deep learning enabled tremendous advances in image recognition, high recognition performance is still dif-ficult to achieve in expert domains, such as specialized areas of biology or medicine, due to two challenges. First, these problems involve fine-grained classes, such as the dogs of
Figure 1, which differ by subtle visual attributes. Second, large annotated datasets are difficult to produce, since image labeling requires expert knowledge, which can be too ex-pensive or infeasible at scale. This makes it difficult to train models as strong as those available for non-expert domains, where crowd-source annotation enables training with mil-lions, or even billions, of labeled examples. To address this challenge, we consider the problem of how to leverage crowd-source platforms to provide professional level anno-tation for expert domain data, which is denoted as PrOfeS-sional lEvel cRowd (POSER) annotation.
Figure 1. Different approaches to the labeling of a query image. Left:
Machine Teaching [44] generates a teaching set, which is used to teach different classes to crowd source workers, who label the query. Center:
SSL [21, 34] methods produce a pseudo-label that is accepted or rejected by thresholding a confidence score. Right: SSL-HF uses crowd source workers to filter pseudo-labels, by comparing the query to a positive (im-ages of the pseudo-label class) and a negative (images from other classes) support set. The annotator implements a binary filter with ‘I don’t know’ (IDK) option.
Since the difficulty is lack of annotator expertise, one route to POSER annotation is to rely on machine teaching (MT) algorithms [23, 42, 44]. As illustrated in Figure 1, a small teaching set annotated by an expert is used to teach crowd-source workers to discriminate the various classes.
The scalability of crowd sourcing is then leveraged to as-semble a large labeled dataset [44]. While machine teaching is surprisingly effective for problems of small class cardi-nality, it is difficult to teach crowd-workers a large number of classes. This is partly because they are averse to compli-cated training procedures and the teaching relies on short-term memory, which has limited capacity [13, 29].
The POSER combination of expert domain data and crowd-sourcing also creates challenges to most human-in-the-loop schemes in the crowd-source annotation literature.
These are usually based on active learning (AL) [31, 32], assuming an oracle that produces a ground-truth label per example. To minimize the number of labelling iterations and cost, AL selects the hardest examples in the dataset to be labelled. However, this is misguided for POSER anno-tation, where noisy annotators are inevitable and the oracle assumption is violated. Since hard examples are precisely those where workers make most mistakes, their selection maximizes labeling noise. Hence, while AL is successful
in domains where crowd-source workers are experts, e.g. everyday objects, it is not effective for expert domains.
In this work, we consider an alternative formulation, in-spired by semi-supervised learning (SSL) methods [4, 5, 28, 34, 35] where a classifier trained on labelled data produces pseudo-labels for unlabeled examples. These labels are then accepted or rejected by thresholding a classification score, as illustrated in the middle of Figure 1. We refer to this pro-cess as pseudo-label filtering. Accepted labels are added to the training set, the classifier retrained, and the process repeated. SSL has been shown successful for datasets of everyday objects [4, 5, 34, 55], such as CIFAR [20], STL-10 [9], SVHN [27], or ImageNet [11] but frequently col-lapses in expert domains, even under-performing supervised baselines trained on the small labeled dataset [36, 44, 50].
This is due to the increased difficulty of finer-grained clas-sification, and the well-known inability of deep learning to produce well calibrated confidence scores [14, 46].
While SSL, by itself, does not solve POSER annotation, its strategy of choosing the easier examples (higher classifi-cation confidence) is more suitable for the noisy POSER an-notators than the hardest example strategy of AL. Further-more, the major SSL weakness - poor pseudo-label filtering
- can be significantly improved upon by using humans to fil-ter pseudo-labels. This suggests solving the POSER anno-tation problem with the SSL with human filtering (SSL-HF) approach at the right of Figure 1. Unlike machine teaching, where workers are image classifiers, POSER annotation is framed as an SSL problem where they become filters that verify the pseudo-labels produced by the classifier for un-labeled images. This has the critical benefit of framing the annotator operation as an instantaneous low-shot learning problem, which does not require prior training.
In the proposed SSL-HF solution, given a query image and its pseudo-label (‘Beagle’), the annotator is presented with a small support set containing both positive (‘Beagle’ class) and negative (other classes) images. The annotator then simply declares if they agree with the pseudo-label, based on the similarity of the query image to the support set examples. Due to the well-known ability of humans for confidence calibration [10], this label filtering procedure is much more accurate than that of SSL, enabling POSER an-notation with high accuracy. Furthermore, because the fil-tering is by visual similarity, the labeling is implicit, i.e. the annotator does not even need to know the ‘Beagle’ class.
Hence, there is no need to teach annotators a priori, elimi-nating the short-term memory constraints of MT. Together, these properties enable the ultimate goal of POSER annota-tion: accurate crowd-sourced annotation of expert datasets with large numbers of classes.
The main insight behind SSL-HF is that the human low-shot learning ability [1, 41, 49] can be leveraged to enable annotators to filter labels in domains where they are not ex-pert. However, when the differences between support set examples are very fine-grained, it can be difficult to iden-tify the object details to look for. To address this problem, we propose to augment SSL-HF with deliberative explana-tions [43, 45], which visualize image regions of ambiguity between class pairs, tailored to the SSL-HF setting.
Overall, this work makes five contributions. First, we introduce the SSL-HF framework for POSER annotation.
Second, we propose an implementation, where the classi-fier suggests a label for the image and a support set of a few positive and close-negative examples. Third, to en-hance the accuracy of the human filtering of pseudo-labels, the support set is complemented with visualization-based explanations. Fourth, we present experiments showing that
SSL-HF significantly outperforms SSL, AL, and MT ap-proaches to POSER annotation and that explanations en-hance these gains. Finally, to minimize the development cost of POSER annotation methods, we introduce an eval-uation protocol based on simulated human labeling. These contributions establish a new research direction at the in-tersection of human-in-the loop and fine-grained classifica-tion, needed to advance the effectiveness of deep learning in expert domains. 2.