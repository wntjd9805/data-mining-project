Abstract
In this work, instead of directly predicting the pixel-level segmentation masks, the problem of referring image seg-mentation is formulated as sequential polygon generation, and the predicted polygons can be later converted into seg-mentation masks. This is enabled by a new sequence-to-sequence framework, Polygon Transformer (PolyFormer), which takes a sequence of image patches and text query to-kens as input, and outputs a sequence of polygon vertices autoregressively. For more accurate geometric localization, we propose a regression-based decoder, which predicts the precise floating-point coordinates directly, without any co-ordinate quantization error. In the experiments, PolyFormer outperforms the prior art by a clear margin, e.g., 5.40% and 4.52% absolute improvements on the challenging Re-It also shows strong fCOCO+ and RefCOCOg datasets. generalization ability when evaluated on the referring video segmentation task without fine-tuning, e.g., achieving com-petitive 61.5% J &F on the Ref-DAVIS17 dataset. 1.

Introduction
Referring image segmentation (RIS) [7, 19, 21, 29–32, 39, 45, 50, 61, 73, 78, 86, 87] combines vision-language un-derstanding [42, 55, 57, 75, 92] and instance segmentation
[2, 8, 16, 25, 52], and aims to localize the segmentation mask of an object given a natural language query. It gen-eralizes traditional object segmentation from a fixed num-ber of predefined categories to any concept described by free-form language, which requires a deeper understanding of the image and language semantics. The conventional pipeline [7, 19, 21, 29–32, 45, 50, 61, 73, 87] first extracts features from the image and text inputs, and then fuses the multi-modal features together to predict the mask.
A segmentation mask encodes the spatial layout of an object, and most instance segmentation models [8, 16, 25, 52] rely on a dense binary classification network to deter-*Work done during internship at AWS AI. † Equal contribution. mine whether each pixel belongs to the object. This pixel-to-pixel prediction is preferred by a convolutional opera-tion, but it neglects the structure among the output predic-tions. For example, each pixel is predicted independently of other pixels. In contrast, a segmentation mask can also be represented by a sparse set of structured polygon vertices delineating the contour of the object [1, 6, 41, 47, 49, 81].
This structured sparse representation is cheaper than a dense mask representation and is the preferred annotation format for most instance segmentation datasets [15, 49, 70]. Thus, it is also tempting to predict structured polygons directly.
However, how to effectively predict this type of structured outputs is challenging, especially for convolutional neu-ral networks (CNNs), and previous efforts have not shown much success yet [41, 47, 81].
We address this challenge by resorting to a sequence-to-sequence (seq2seq) framework [3, 14, 65, 66, 74], and pro-pose Polygon transFormer (PolyFormer) for referring im-age segmentation. As illustrated in Fig. 1, it takes a se-quence of image patches and text query tokens as input, and autoregressively outputs a sequence of polygon ver-tices. Since each vertex prediction is conditioned on all preceding predicted vertices, the output predictions are no longer independent of each other. The seq2seq framework is flexible on its input and output format, as long as both of them can be formulated as sequences of variable length.
Thus, it is natural to concatenate the visual and language features together as a long sequence, avoiding complicated multi-modal feature fusion as in prior work [7,30,73,86,87].
In the meantime, the output can also be a long sequence of multiple polygons separated by separator tokens, cover-ing the scenario where the segmentation masks are not con-nected, e.g., by occlusion, as shown in Fig. 1. Furthermore, since a bounding box can be represented as a sequence of two corner points (i.e., top left and bottom right), they can also be output by PolyFormer along with the polygon ver-tices. Thus, referring image segmentation (polygon) and referring expression comprehension (bounding box) can be unified in our simple PolyFormer framework.
Localization is important for polygon and bounding box
Figure 1. The illustration of PolyFormer pipeline for referring image segmentation (polygon vertex sequence) and referring expression comprehension (bounding box corner points). The polygons are converted to segmentation masks in the end. generation, as a single coordinate prediction mistake may result in substantial errors in the mask or bounding box pre-diction. However, in recent seq2seq models for the visual domain [9, 10, 56, 77], in order to accommodate all tasks in a unified seq2seq framework, the coordinates are quan-tized into discrete bins and the prediction is formulated as a classification task. This is not ideal since geometric coor-dinates lie in a continuous space instead of a discrete one, and classification is thus usually suboptimal for localiza-tion task [23, 43, 93].
Instead, we formulate localization as a regression task, due to its success in object detection
[5, 24, 25, 68], where floating-point coordinates are directly predicted without any quantization error. Motivated by [25], the feature embedding for any floating-point coordinate in
PolyFormer is obtained by bilinear interpolation [33] of its neighboring indexed embeddings. This is in contrast with the common practice [9,56,77] in which the coordinate fea-ture is indexed from a dictionary with a fixed number of discrete coordinate bins. These changes enable our Poly-Former to make accurate polygon and bounding box pre-dictions.
We evaluate PolyFormer on three major referring image segmentation benchmarks.
It achieves 76.94%, 72.15%, and 71.15% mIoU on the validation sets of RefCOCO [89],
RefCOCO+ [89] and RefCOCOg [60], outperforming the state of the art by absolute margins of 2.48%, 5.40%, and 4.52%, respectively. PolyFormer also shows strong gen-eralization ability when directly applied to the referring video segmentation task without finetuning.
It achieves 61.5% J &F on the Ref-DAVIS17 dataset [38], compara-ble with [80] which is specifically designed for that task.
Our main contributions are summarized as follows:
• We introduce a novel framework for RIS and REC, called PolyFormer, which formulates them as a sequence-to-sequence prediction problem. Due to its flexibility, it can naturally fuse multi-modal features together as input and generate a sequence of polygon vertices and bounding box corner points.
• We propose a regression-based decoder for accu-rate coordinate prediction in this seq2seq framework, which outputs continuous 2D coordinates directly without quantization error. To the best of our knowl-edge, this is the first work formulating geometric lo-calization as a regression task in seq2seq framework instead of classification as in [9, 10, 56, 77].
• For the first time, we show that the polygon-based method surpasses mask-based ones across all three main referring image segmentation benchmarks, and it can also generalize well to unseen scenarios, including video and synthetic data. 2.