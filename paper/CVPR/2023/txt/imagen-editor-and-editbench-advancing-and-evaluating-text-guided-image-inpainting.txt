Abstract 1.

Introduction
Text-guided image editing can have a transformative im-pact in supporting creative applications. A key challenge is to generate edits that are faithful to input text prompts, while consistent with input images. We present Imagen Ed-itor, a cascaded diffusion model built, by fine-tuning Im-agen [36] on text-guided image inpainting.
Imagen Ed-itor’s edits are faithful to the text prompts, which is ac-complished by using object detectors to propose inpaint-ing masks during training. In addition, Imagen Editor cap-tures fine details in the input image by conditioning the cas-caded pipeline on the original high resolution image. To im-prove qualitative and quantitative evaluation, we introduce
EditBench, a systematic benchmark for text-guided image inpainting. EditBench evaluates inpainting edits on natu-ral and generated images exploring objects, attributes, and scenes. Through extensive human evaluation on EditBench, we find that object-masking during training leads to across-the-board improvements in text-image alignment – such that
Imagen Editor is preferred over DALL-E 2 [31] and Stable
Diffusion [33] – and, as a cohort, these models are better at object-rendering than text-rendering, and handle mate-rial/color/size attributes better than count/shape attributes.
Text-to-image generation has seen a surge of recent inter-est [31, 33, 36, 50, 51]. While these generative models are surprisingly effective, users with specific artistic and de-sign needs do not typically obtain the desired outcome in a single interaction with the model. Text-guided image edit-ing can enhance the image generation experience by sup-porting interactive refinement [13, 17, 34, 46]. We focus on text-guided image inpainting, where a user provides an im-age, a masked area, and a text prompt and the model fills the masked area, consistent with both the prompt and the image context (Fig. 1). This complements mask-free edit-ing [13, 17, 46] with the precision of localized edits [5, 27].
This paper contributes to the modeling and evaluation of text-guided image inpainting. Our modeling contribution is
Imagen Editor,2 a text-guided image editor that combines large scale language representations with fine-grained con-trol to produce high fidelity outputs.
Imagen Editor is a cascaded diffusion model that extends Imagen [36] through finetuning for text-guided image inpainting. Imagen Edi-tor adds image and mask context to each diffusion stage via three convolutional downsampling image encoders (Fig. 2).
A key challenge in text-guided image inpainting is ensur-ing that generated outputs are faithful to the text prompts.
The standard training procedure uses randomly masked re-∗Equal contribution. †Equal advisory contribution. 2https://imagen.research.google/editor/
Figure 2. Imagenator is an image editing model built by fine-tuning Imagen. All of the diffusion models, i.e., the base model and super-resolution (SR) models, condition on high-resolution 1024×1024 image and mask inputs. To this end, new convolu-tional image encoders are introduced. gions of input images [27, 35]. We hypothesize that this leads to weak image-text alignment since randomly cho-sen regions can often be plausibly inpainted using only the image context, without much attention to the prompt. We instead propose a novel object masking technique that en-courages the model to rely more on the text prompt during training (Fig. 3). This helps make Imagen Editor more con-trollable and substantially improves text-image alignment.
Observing that there are no carefully-designed standard datasets for evaluating text-guided image inpainting, we propose EditBench, a curated evaluation dataset that cap-tures a wide variety of language, types of images, and lev-els of difficulty. Each EditBench example consists of (i) a masked input image, (ii) an input text prompt, and (iii) a high-quality output image that can be used as reference for automatic metrics. To provide insight into the relative strengths and weaknesses of different models, edit prompts are categorized along three axes: attributes (material, color, shape, size, count), objects (common, rare, text rendering), and scenes (indoor, outdoor, realistic, paintings).
Finally, we perform extensive human evaluations on Ed-itBench, probing Imagen Editor alongside Stable Diffu-sion (SD) [33], and DALL-E 2 (DL2) [31]. Human an-notators are asked to judge a) text-image alignment – how well the prompt is realized (both overall and assessing the presence of each object/attribute individually) and b) image quality – visual quality regardless of the text prompt.
In terms of text-image alignment, Imagen Editor trained with object-masking is preferred in 68% of comparisons with its counterpart configuration trained with random masking (a commonly adopted method [27, 35, 41]). Improvements are across-the-board in all object and attribute categories.
Imagen Editor is also preferred by human annotators rel-ative to SD and DL2 (78% and 77% respectively). As a cohort, models are better at object-rendering than text-rendering, and handle material/color/size attributes better than count/shape attributes. Comparing automatic evalua-tion metrics with human judgments, we conclude that while
Figure 3. Random masks (left) frequently capture background or intersect object boundaries, defining regions that can be plausibly inpainted just from image context alone. Object masks (right) are harder to inpaint from image context alone, encouraging models to rely more on text inputs during training. (Note: This example image was generated by Imagen and is not in the training data.) human evaluation remains indispensable, CLIPScore [14] is the most useful metric for hyperparameter tuning and model selection.
In summary, our main contributions are: (i) Imagen Ed-itor, a new state-of-the-art diffusion model for high fidelity text-guided image editing (Sec. 3); (ii) EditBench, a man-ually curated evaluation benchmark for text-guided image inpainting that assesses fine-grained details such as object-attribute combinations (Sec. 4); and (iii) a comprehensive human evaluation on EditBench, highlighting the relative strengths and weaknesses of current models, and the use-fulness of various automated evaluation metrics for text-guided image editing (Sec. 5). 2.