Abstract
Recent efforts in Neural Rendering Fields (NeRF) have shown impressive results on novel view synthesis by utiliz-ing implicit neural representation to represent 3D scenes.
Due to the process of volumetric rendering, the inference speed for NeRF is extremely slow, limiting the application scenarios of utilizing NeRF on resource-constrained hard-ware, such as mobile devices. Many works have been con-ducted to reduce the latency of running NeRF models. How-ever, most of them still require high-end GPU for accel-eration or extra storage memory, which is all unavailable on mobile devices. Another emerging direction utilizes the neural light ﬁeld (NeLF) for speedup, as only one forward pass is performed on a ray to predict the pixel color. Nev-ertheless, to reach a similar rendering quality as NeRF, the network in NeLF is designed with intensive computation, which is not mobile-friendly. In this work, we propose an efﬁcient network that runs in real-time on mobile devices for neural rendering. We follow the setting of NeLF to train our network. Unlike existing works, we introduce a novel network architecture that runs efﬁciently on mobile devices with low latency and small size, i.e., saving 15
⇥ storage compared with MobileNeRF. Our model achieves high-resolution generation while maintaining real-time in-ference for both synthetic and real-world scenes on mo-bile devices, e.g., 18.04ms (iPhone 13) for rendering one 1008 756 image of real 3D scenes. Additionally, we achieve similar image quality as NeRF and better quality than MobileNeRF (PSNR 26.15 vs. 25.91 on the real-world forward-facing dataset)1.
⇥ ⇠ 24
⇥ 1.

Introduction
Remarkable progress seen in the domain of neural ren-dering [33] promises to democratize asset creation and ren-dering, where no mesh, texture, or material is required – only a neural network that learns a representation of an ob-ject or a scene from multi-view observations. The trained 1More demo examples in our Webpage.
Figure 1. Examples of deploying our approach on mobile de-vices for real-time interaction with users. Due to the small model 26ms per image on size (8.3MB) and fast inference speed (18 iPhone 13), we can build neural rendering applications where users interact with 3D objects on their devices, enabling various appli-cations such as virtual try-on. We use publicly available software to make the on-device application for visualization [1, 3].
⇠ model can be queried at arbitrary viewpoints to generate novel views. To be made widely available, this excit-ing application requires such methods to run on resource-constrained devices, such as mobile phones, conforming to their limitations in computing, wireless connectivity, and hard drive capacity.
Unfortunately, the impressive image quality and capa-bilities of NeRF [33] come with a price of slow render-ing speed. To return the color of the queried pixel, hun-dreds of points need to be sampled along the ray that ends up in that pixel, which is then integrated to get the radi-ance. To enable real-time applications, many works have been proposed [12, 34, 37, 45], yet, they still require high-end GPUs for rendering and hence are not available for resource-constrained applications on mobile or edge de-vices. An attempt is made to trade rendering speed with storage in MobileNeRF [10]. While showing promising acceleration results, their method requires storage for tex-turing saving. For example, for a single real-world scene from the forward-facing dataset [33], MobileNeRF requires 201.5MB of storage. Clearly, downloading and storing tens, hundreds, or even thousands of such scenes in MobileNeRF
format on a device is prohibitively expensive.
A different approach is taken in Neural Light Fields (NeLF) that directly maps a ray to the RGB color of the pixel by performing only one forward pass per ray, result-ing in faster rendering speed [5, 25, 28, 41]. Training NeLF is challenging and hence requires increased network capac-ity. For example, Wang et al. [41] propose an 88-layer fully-connected network with residual connections to distill a pre-trained radiance model effectively. While their approach achieves better rendering results than vanilla NeRF at 30
⇥ speedup, running it on mobile devices is still not possible, as it takes three seconds to render one 200 200 image on iPhone 13 shown in our experiments.
⇥
In this work, we propose MobileR2L, a real-time neu-ral rendering model built with mobile devices in mind.
Our training follows a similar distillation procedure intro-duced in R2L [41]. Differently, instead of using an MLP, a backbone network used by most neural representations, we show that a well-designed convolutional network can achieve real-time speed with the rendering quality similar to MLP. In particular, we revisit the network design choices 1 Conv layer in made in R2L and propose to use the 1 the backbone. A further challenge with running a NeRF or NeLF on mobile devices is an excessive requirement of
RAM. For example, to render an 800 800 image, one needs to sample 640, 000 rays that need to be stored, caus-ing out-of-memory issues.
In 3D-aware generative mod-els [9, 15, 20], this issue is alleviated by rendering a radi-ance feature volume and upsampling it with a convolutional network to obtain a higher resolution. Inspired by this, we render a light-ﬁeld volume that is upsampled to the required resolution. Our MobileR2L features several major advan-tages over existing works:
⇥
⇥
• MobileR2L achieves real-time inference speed on mo-bile devices (Tab. 3) with better rendering quality, e.g.,
PSNR, than MobileNeRF on the synthetic and real-world datasets (Tab. 1).
• MobileR2L requires an order of magnitude less stor-reducing the model size to 8.3MB, which is age, 15.2 24.3
⇥ less than MobileNeRF.
⇥ ⇠
Due to these contributions, MobileR2L can unlock wide adoption of neural rendering in real-world applications on mobile devices, such as a virtual try-on, where the real-time interaction between devices and users is achieved (Fig. 1). 2.