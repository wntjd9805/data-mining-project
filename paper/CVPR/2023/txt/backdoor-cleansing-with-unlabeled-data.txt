Abstract
Due to the increasing computational demand of Deep
Neural Networks (DNNs), companies and organizations have begun to outsource the training process. However, the externally trained DNNs can potentially be backdoor attacked. It is crucial to defend against such attacks, i.e., to postprocess a suspicious model so that its backdoor be-havior is mitigated while its normal prediction power on clean inputs remain uncompromised. To remove the ab-normal backdoor behavior, existing methods mostly rely on additional labeled clean samples. However, such re-quirement may be unrealistic as the training data are of-ten unavailable to end users. In this paper, we investigate the possibility of circumventing such barrier. We propose a novel defense method that does not require training la-bels. Through a carefully designed layer-wise weight re-initialization and knowledge distillation, our method can effectively cleanse backdoor behaviors of a suspicious net-work with negligible compromise in its normal behavior.
In experiments, we show that our method, trained with-out labels, is on-par with state-of-the-art defense methods trained using labels. We also observe promising defense results even on out-of-distribution data. This makes our method very practical. Code is available at: https:
//github.com/luluppang/BCU . 1.

Introduction
Deep Neural Networks (DNNs) have achieved impres-sive performance in many tasks, e.g., image classifica-tion [6], 3D point cloud generation [21] and object track-ing [45]. However, the success usually relies on abundant training data and computational resources. Companies and organizations thus often outsource the training process to cloud computing or utilize pretrained models from third-party platforms. Unfortunately, the untrustworthy providers may potentially introduce backdoor attacks to the externally trained DNNs [9, 19]. During the training stage of a back-door attack, an adversary stealthily injects a small portion of poisoned training data to associate a particular trigger with
Figure 1. (a) Previous works use labeled in-distribution data to cleanse backdoor. Our work uses unlabeled in-distribution (b) or out-of-distribution data (c). target class labels. During the inference stage, backdoor models predict accurately on clean samples but misclassify samples with triggers to the target class. Common trig-gers include black-white checkerboard [9], random noise pattern [5], physical object [37], etc.
To defend against backdoor attacks, one needs to post-process a suspicious model so that its backdoor behavior is mitigated, and meanwhile, its normal prediction power on clean inputs remains uncompromised. To remove the ab-normal backdoor behavior, existing methods mostly rely on additional labeled in-distribution clean samples [16, 18, 38, 40, 43, 44]. For example, Fine-Pruning [18] first prunes the dormant neurons for clean samples and then finetunes the model using ground-truth labels. Neural Attention Distil-lation (NAD) [16], a knowledge distillation-based method, uses labeled clean data to supervise the learning of a stu-dent model. Adversarial Neuron Pruning (ANP) [38] learns a mask to prune sensitive neurons with labeled clean data.
These methods require 1%−5% labeled clean training sam-ples to effectively remove backdoor. Such requirement, however, is unrealistic in practice as the training data are often unavailable to end-users.
In this paper, we explore the possibility of circumventing such barrier with unlabeled data. As shown in Figure 1,
we propose a novel defense method that does not require training labels. Meanwhile, we explore the ambitious goal of using only out-of-distribution data. These goals make the proposed defense method much more practical. End-users can be completely agnostic of the training set. To run the defense algorithm, they only need to collect some unlabeled data that do not have to resemble the training samples.
Inspired by knowledge distillation [8], we use a stu-dent model to acquire benign knowledge from a suspicious teacher model through their predictions on the readily avail-able unlabeled data. Since the unlabeled data are usually clean images or images with slightly random noise, they are distinct from poisoned images with triggers. There-fore, trigger-related behaviors will not be evoked during the distillation. This effectively cleanses backdoor behav-iors without significantly compromising the model’s normal behavior. To ensure the student model focuses on the be-nign knowledge, which can be layer dependent, we propose an adaptive layer-wise weight re-initialization for the stu-dent model. Empirically, we demonstrate that even without labels, the proposed method can still successfully defend against the backdoor attacks. We also observe very promis-ing defense results even with out-of-distribution unlabeled data that do not belong to the original training classes.
Our contributions are summarized as follows: 1. For the first time, we propose to defend against back-door attacks using unlabeled data. This provides a practical solution to end-users under threat. 2. We devise a framework with knowledge distillation to transfer normal behavior of a suspicious teacher model to a student model while cleansing backdoor behav-iors. Since the normal/backdoor knowledge can be layer-dependent, we design an adaptive layer-wise ini-tialization strategy for the student model. 3. Extensive experiments are conducted on two bench-mark datasets, CIFAR10 [14] and GTSRB [31]. Our method, trained without labels, is on-par with state-of-the-art defense methods trained with labels. 4. Meanwhile, we carry out an empirical study with out-of-distribution data. Our method achieves satisfac-tory defense performance against a majority of attacks.
This sheds lights on a promising practical solution for end-users: they can use any collected images to cleanse a suspicious model. 2.