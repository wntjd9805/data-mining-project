Abstract 100
Seen Unseen 71.1 76.5 75.9 89.9 91.9 77.8
) (
%
U o
I m 80 60 40 20 0 40.4 28.3 16.3 13.8
Recently, CLIP has been applied to pixel-level zero-shot learning tasks via a two-stage scheme. The general idea is to ﬁrst generate class-agnostic region proposals and then feed the cropped proposal regions to CLIP to utilize its image-level zero-shot classiﬁcation capability. While ef-fective, such a scheme requires two image encoders, one for proposal generation and one for CLIP, leading to a complicated pipeline and high computational cost. In this work, we pursue a simpler-and-efﬁcient one-stage solu-tion that directly extends CLIP’s zero-shot prediction ca-pability from image to pixel level. Our investigation starts with a straightforward extension as our baseline that gen-erates semantic masks by comparing the similarity between text and patch embeddings extracted from CLIP. However, such a paradigm could heavily overﬁt the seen classes and fail to generalize to unseen classes. To handle this is-sue, we propose three simple-but-effective designs and ﬁg-ure out that they can signiﬁcantly retain the inherent zero-shot capacity of CLIP and improve pixel-level generaliza-tion ability.
Incorporating those modiﬁcations leads to an efﬁcient zero-shot semantic segmentation system called
ZegCLIP. Through extensive experiments on three public benchmarks, ZegCLIP demonstrates superior performance, outperforming the state-of-the-art methods by a large mar-gin under both “inductive” and “transductive” zero-shot settings. In addition, compared with the two-stage method, our one-stage ZegCLIP achieves a speedup of about 5 times faster during inference. We release the code at https:
//github.com/ZiqinZhou66/ZegCLIP.git. 1.

Introduction
Semantic segmentation is one of the fundamental tasks in the computer vision ﬁeld, which aims to predict the category of each pixel of an image [7, 15, 31, 41]. Ex-tensive works have been proposed [8, 26, 30], e.g., Fully
Convolutional Networks [29], U-net [38], DeepLab fam-ily [4–6] and more recently Vision Transformer based meth-ods [16, 53, 58].
*Corresponding author
Figure 1. Quantitative improvements achieved by our proposed designs on VOC dataset. (1)(2) represents our one-stage Base-line model of different versions (Fix or Fine-Tune CLIP image en-coder), while (3)-(5) shows the effectiveness of applying our pro-posed designs, i.e., Deep Prompt Tuning (DPT), Non-mutually
Exclusive Loss (NEL), Relationship Descriptor (RD), on base-line model step by step. We highlight that our designs can dramat-ically increase the segmentation performance on unseen classes.
However, the success of the deep semantic segmentation models heavily relies on the availability of a large amount of annotated training images, which involves a substantial amount of labor. This gives rise to a surging interest in low-supervision-based semantic segmentation approaches, in-cluding semi-supervised [7], weakly-supervised [48], few-shot [46], and zero-shot semantic segmentation [3, 34, 44].
Among them, zero-shot semantic segmentation is partic-ularly challenging and attractive since it is required to di-rectly produce the semantic segmentation results based on the semantic description of a given class. Recently, the pre-trained vision-language model CLIP [36] has been adopted into various dense prediction tasks, such as referring seg-mentation [43], semantic segmentation [33], and detection
[14]. It also offers a new paradigm and has made a break-through for zero-shot semantic segmentation. Initially built for matching text and images, CLIP has demonstrated a re-markable capability for image-level zero-shot classiﬁcation.
However, zsseg [49] and Zegformer [12] follow the com-mon strategy that needs two-stage processing that ﬁrst gen-erates region proposals and then feeds the cropped regions to CLIP for zero-shot classiﬁcation. Such a strategy re-quires two image encoding processes, one for generating proposals and one for encoding each proposal via CLIP.
Table 1. Differences between our approach and related zero-shot semantic segmentation methods based on CLIP.
Methods zsseg [49]
ZegFormer [12]
MaskCLIP+ [56]
ZegCLIP (Ours)
Need an extra image encoder? (cid:2) (cid:2) (cid:2) (cid:3)
CLIP as an image
-level classiﬁer? (cid:2) (cid:2) (cid:3) (cid:3)
Can do inductive? (cid:2) (cid:2) (cid:3) (cid:2)
This design creates additional computational overhead and cannot leverage the knowledge of the CLIP encoder at the proposal generation stage. Besides, MaskCLIP+ [56] uti-lizes CLIP to generate pseudo labels of novel classes for self-training but will be invalid if the unseen class names in inference are unknown in the training stage (“inductive” zero-shot setting).
This paper pursues simplifying the pipeline by directly extending the zero-shot capability of CLIP from image-level to pixel-level. The basic idea of our method is straight-forward: we use a lightweight decoder to match the text prompts against the local embeddings extracted from CLIP, which could be achieved via the self-attention mechanism in a transformer-based structure. We train the vanilla decoder and ﬁx or ﬁne-tune the CLIP image encoder on a dataset containing pixel-level annotations from a limited number of classes, expecting the text-patch matching capability can generalize to unseen classes. Unfortunately, this basic ver-sion tends to overﬁt the training set: while the segmentation results for seen classes generally improve, the model fails to produce reasonable segments on unseen classes. Sur-prisingly, we discover such an overﬁtting issue can be dra-matically alleviated by incorporating three modiﬁed design choices and report the quantitative improvements in Fig. 1.
The following highlights our key discoveries:
Design 1: Using Deep Prompt Tuning (DPT) instead of
ﬁne-tuning or ﬁxing for the CLIP image encode. We ﬁnd that ﬁne-tuning could lead to overﬁtting to seen classes while prompt tuning prefers to retain the inherent zero-shot capacity of CLIP.
Design 2: Applying Non-mutually Exclusive Loss (NEL) function when performing pixel-level classiﬁcation but gen-erating the posterior probability of one class independent of the logits of other classes.
Design 3: Most importantly and our major innovation — introducing a Relationship Descriptor (RD) to incorporate the image-level prior into text embedding before matching text-patch embeddings from CLIP can signiﬁcantly prevent the model from overﬁtting to the seen classes.
By incorporating those three designs into our one-stage baseline, we create a simple-but-effective zero-shot seman-tic segmentation model named ZegCLIP. Tab. 1 summa-rizes the differences between our proposed method and ex-isting approaches based on CLIP. More details can be found in Appendix. We conduct extensive experiments on three public datasets and show that our method outperforms the state-of-the-art methods by a large margin in both the “in-ductive” and “transductive” settings. 2.