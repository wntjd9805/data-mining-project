Abstract
We revisit and advance visual prompting (VP), an input prompting technique for vision tasks. VP can reprogram a
ﬁxed, pre-trained source model to accomplish downstream tasks in the target domain by simply incorporating univer-sal prompts (in terms of input perturbation patterns) into downstream data points. Yet, it remains elusive why VP stays effective even given a ruleless label mapping (LM) between the source classes and the target classes. Inspired by the above, we ask: How is LM interrelated with VP? And how to exploit such a relationship to improve its accuracy on target tasks? We peer into the inﬂuence of LM on VP and provide an afﬁrmative answer that a better ‘quality’ of
LM (assessed by mapping precision and explanation) can consistently improve the effectiveness of VP. This is in con-trast to the prior art where the factor of LM was missing.
To optimize LM, we propose a new VP framework, termed
ILM-VP (iterative label mapping-based visual prompting), which automatically re-maps the source labels to the target labels and progressively improves the target task accuracy of VP. Further, when using a contrastive language–image pretrained (CLIP) model for VP, we propose to integrate an LM process to assist the text prompt selection of CLIP and to improve the target task accuracy. Extensive exper-iments demonstrate that our proposal signiﬁcantly outper-forms state-of-the-art VP methods. As highlighted below, we show that when reprogramming an ImageNet-pretrained
ResNet-18 to 13 target tasks, ILM-VP outperforms base-lines by a substantial margin, e.g., 7.9% and 6.7% accuracy improvements in transfer learning to the target Flowers102 and CIFAR100 datasets. Besides, our proposal on CLIP-based VP provides 13.7% and 7.1% accuracy improvements on Flowers102 and DTD respectively. Code is available at https://github.com/OPTML-Group/ILM-VP. 1.

Introduction
When learning new knowledge, humans typically start to compare and connect it with the knowledge that they were familiar with. The same idea is also applied in ML. For ex-ample, in the ‘pretraining + ﬁnetuning’ paradigm, an ML model (e.g., deep neural network or DNN) is ﬁrst trained on a (usually large) source dataset. When a relevant down-Fig. 1. Overview of VP pipelines (prior art [1,2] and our proposal termed
ILM-VP) and accuracy improvement achieved by ILM-VP on target image classiﬁcation tasks at-a-glance. Generally speaking, VP aims to generate a universal input perturbation template (i.e., ‘visual prompt’) and lever-age a source-target LM (label mapping) in order to drive the ﬁxed source model (e.g., pretrained on ImageNet-1K) to conduct a target task (e.g.,
Flowers102 image classiﬁcation). Compared to the prior art, our proposal (ILM-VP) couples the design of LM with VP training. The resulting LM-VP co-design improves target task accuracy across a variety of target image classiﬁcation tasks using a ﬁxed ImageNet-pretrained source model. stream task is present, the pre-trained model is then ﬁne-tuned over the target dataset. This learning paradigm has been predominant in the classical transfer learning [3–8] as well as in the recent deep representation learning [9–13].
However, ﬁnetuning the pre-trained model requires ei-ther partial or entire model modiﬁcations.
If the pre-trained model is of large size, then it becomes too costly to store a modiﬁed copy of the pre-trained model for each downstream task. In contrast, visual prompting (VP) (see
Fig. 1), also known as model reprogramming or adversar-ial reprogramming, provides a new alternative to ﬁnetun-ing [1, 2, 14–17].
Instead of directly modifying the pre-trained source model, VP integrates an input transforma-tion and/or an output transformation to reprogram the ﬁxed source model to accomplish a new target task; see an il-lustration of existing VP framework in Fig. 1. The input transformation is typically realized by incorporating (data-agnostic) input perturbations (i.e., prompts) into input sam-ples, and the output transformation is given by a function that maps source labels to target labels, known as label map-ping (LM). Recently, VP has shown great promise in var-ious applications of foundation models, ranging from pre-trained vision models [1, 14, 15, 17–20] to language-vision models [2, 21–23].
The idea of prompt learning originated from in-context learning or prompting in natural language processing (NLP)
[24–26]. However, when it is introduced to the vision do-main [1, 2], new questions arise. First, the recent work
[1, 14, 27] showed that VP remains powerful even if the tar-get task largely deviates from the source domain. For exam-ple, a new performance record on target medical datasets is achieved in [1] when using VP to reprogram the ﬁxed,
ImageNet pre-trained source model. The ‘mystery’ in this example is that LM is conducted between two seemingly irrelevant source and target domains. Despite the lack of interpretability, VP can still leverage such connected source labels and the source model to effectively predict target data points. This raises the ﬁrst open question: What is the ratio-nality behind LM and how to explore its inﬂuence on VP?
Second, unlike prompt learning in the NLP domain, input prompts in the vision domain are typically given by ‘noisy’ perturbations to image pixels; see illustration in Fig. 1. To-gether with the lack of interpretability of LM, the second open question is: How to interpret LM and the seemingly random perturbation pattern in VP?
As mentioned above, the lack of understanding of
LM and the poor interpretability of VP drive our stud-ies in this work. We develop a new visual prompting framework, termed ILM-VP (iterative label mapping-based visual prompting), which provides an interactive and ex-plainable design between LM and prompt learning (i.e., input prompt generation); see Fig. 1 for the schematic overview. Our proposal can automatically adjust LM be-tween the source domain and the target domain by taking both mapping precision and explanation into consideration, and can leverage the optimized LM to further improve the accuracy and the explainability of prompt learning. Al-though some prior work [1,17,27] attempted to improve the quality of LM as well as the overall performance of VP, they are different from our proposal in two major aspects. First, none of the prior work co-designed LM and VP. For exam-ple, the prior art [1] used a pre-prompt prediction frequency to determine the LM function. However, we ﬁnd signiﬁ-cant inconsistency between the pre-prompt and post-prompt prediction frequency of the same source model, which ex-plains the sub-optimality of the current VP methods due to the lack of mapping precision. Second, to the best of our knowledge, VP is still treated as a ‘black box’ in the prior work. Yet, our design can provide graceful visual explana-tions to the underlying mechanisms of VP. Third, we for the ﬁrst time show that LM can provide a uniﬁed solution to improving the accuracy of VP to re-purpose both vision and language-vision source models. Our contributions are unfolded below. (cid:172) We revisit the LM problem in VP and uncover the deﬁciencies of existing LM methods: the lack of mapping precision and the lack of explanation. (cid:173) Given the importance of LM, we propose the ﬁrst LM-VP co-design framework, termed ILM-VP, through a novel bi-level optimization viewpoint. (cid:174) Beyond LM for vision models, we show that LM can also be generalized to assist the text prompt selection of
CLIP (contrastive language–image pretraining) and to im-prove the target task accuracy of VP using the CLIP model. (cid:175) We empirically demonstrate the accuracy and expla-nation merits of our proposal across multiple source models and target datasets. 2.