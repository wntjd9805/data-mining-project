Abstract
LiDAR provides accurate geometric measurements of the 3D world. Unfortunately, dense LiDARs are very expensive and the point clouds captured by low-beam LiDAR are often sparse. To address these issues, we present UltraLiDAR, a data-driven framework for scene-level LiDAR completion,
LiDAR generation, and LiDAR manipulation. The crux of
UltraLiDAR is a compact, discrete representation that en-codes the point cloud’s geometric structure, is robust to noise, and is easy to manipulate. We show that by aligning the representation of a sparse point cloud to that of a dense point cloud, we can densify the sparse point clouds as if they were captured by a real high-density LiDAR, drastically re-ducing the cost. Furthermore, by learning a prior over the discrete codebook, we can generate diverse, realistic LiDAR point clouds for self-driving. We evaluate the effectiveness of UltraLiDAR on sparse-to-dense LiDAR completion and
LiDAR generation. Experiments show that densifying real-world point clouds with our approach can significantly im-prove the performance of downstream perception systems.
Compared to prior art on LiDAR generation, our approach generates much more realistic point clouds. According to
A/B test, over 98.5% of the time human participants pre-fer our results over those of previous methods. Please re-fer to project page https://waabi.ai/research/ ultralidar/ for more information. 1.

Introduction
Building a robust 3D perception system is key to bring-ing self-driving vehicles into our daily lives. To effectively
perceive their surroundings, existing autonomous systems primarily exploit LiDAR as the major sensing modality, since it can capture well the 3D geometry of the world.
However, while LiDAR provides accurate geometric mea-surements, it comes with two major limitations: (i) the cap-tured point clouds are inherently sparse; and (ii) the data collection process is difficult to scale up.
Sparsity: Most popular self-driving LiDARs are time-of-flight and scan the environment by rotating emitter-detector pairs (i.e., beams) around the azimuth. At every time step, each emitter emits a light pulse which travels until it hits a target, gets reflected, and is received by the detector. Dis-tance is measured by calculating the time of travel. Due to the design, the captured point cloud density inherently decreases as the distance to the sensor increases. For dis-tant objects, it is often that only a few LiDAR points are captured, which greatly increases the difficulty for 3D per-ception. The sparsity problem becomes even more severe under poor weather conditions [47], or when LiDAR sen-sors have fewer beams [4]. One “simple” strategy is to in-crease the number of LiDAR beams. However, 128-beam
LiDAR sensors are much more expensive than their 64/32-beam counterparts, not to mention that 512-beam LiDAR does not exist yet.
Scalability: Training and testing perception systems in diverse situations are crucial for developing robust au-tonomous systems. However, due to their intricate design,
LiDARs are much more expensive than cameras. A com-mercial 64-beam LiDAR usually costs over 25K USD. The price barrier makes LiDAR less accessible to the general public and restricts data collection to a small fraction of ve-hicles that populate our roads, significantly hindering scal-ing up. One way to circumvent the issue is to leverage exist-ing LiDAR simulation suites to generate more data. While the simulated point clouds are realistic, these systems typ-ically require one to manually create the scene or rely on multiple scans of the real world in advance, making such a solution less desirable.
With these challenges in mind, we propose UltraLiDAR, a novel framework for LiDAR completion and generation.
The key idea is to learn a compact, discrete 3D represen-tation (codebook) of LiDAR point clouds that encodes the geometric structure of the scene and the physical rules of our world (e.g., occlusion). Then, by aligning the represen-tation of a sparse point cloud to that of a dense point cloud, we can densify the sparse point cloud as if it were captured by a high-density LiDAR (e.g., 512-beam LiDAR). Further-more, we can learn a prior over the discrete codebook and generate novel, realistic driving scenes by sampling from it; we can also manipulate the discrete code of the scene and produce counterfactual scenarios, both of which can drasti-cally improve the diversity and amount of LiDAR data. Fig. 1 shows some example outputs of UltraLiDAR.
We demonstrate the effectiveness of UltraLiDAR on two tasks: sparse-to-dense LiDAR completion and LiDAR gen-eration. For LiDAR completion, since there is no ground truth for high-density LiDAR, we exploit the performance of downstream perception tasks as a “proxy” measure-ment. Specifically, we evaluate 3D detection models on both sparse and densified point clouds and measure the per-formance difference. Experiments on Pandaset [46] and
KITTI-360 [27] show that our completion model can gen-eralize across datasets and the densified results can signifi-cantly improve the performance of 3D detection models. As for LiDAR generation, we compare our results with state-of-the-art LiDAR generative models [3, 53]. Our generated point clouds better match the statistics of those of ground truth data. We also conducted a human study where par-ticipants prefer our method over prior art over 98.5% (best 100%) of the time; comparing to ground truth, our results were selected 32% of the time (best 50%).
To summarize, the main contributions of this paper are: 1. We present a LiDAR representation that can effectively capture data priors and enable various downstream ap-plications, e.g., LiDAR completion and generation. 2. We propose a sparse-to-dense LiDAR completion pipeline that can accurately densify sparse point clouds and improve the performance and generalization abil-ity of trained detection models across datasets. 3. We develop an (un)conditional LiDAR generative model that can sythesize high-quality LiDAR point clouds and supports various manipulations. 2.