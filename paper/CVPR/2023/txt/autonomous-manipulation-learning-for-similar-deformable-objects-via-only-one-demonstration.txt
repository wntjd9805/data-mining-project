Abstract
In comparison with most methods focusing on 3D rigid object recognition and manipulation, deformable objects are more common in our real life but attract less attention.
Generally, most existing methods for deformable object ma-nipulation suffer two issues, 1) Massive demonstration: re-peating thousands of robot-object demonstrations for model training of one speciﬁc instance; 2) Poor generalization: inevitably re-training for transferring the learned skill to a similar/new instance from the same category. There-fore, we propose a category-level deformable 3D object ma-nipulation framework, which could manipulate deformable 3D objects with only one demonstration and generalize the learned skills to new similar instances without re-training.
Speciﬁcally, our proposed framework consists of two mod-ules. The Nocs State Transform (NST) module transfers the observed point clouds of the target to a pre-deﬁned uni-ﬁed pose state (i.e., Nocs state), which is the foundation for the category-level manipulation learning; the Neural Spa-tial Encoding (NSE) module generalizes the learned skill to novel instances by encoding the category-level spatial in-formation to pursue the expected grasping point without re-training. The relative motion path is then planned to achieve autonomous manipulation. Both the simulated re-sults via our Cap40 dataset and real robotic experiments justify the effectiveness of our framework. 1.

Introduction
Autonomous 3D object recognition and manipulation
[1, 11, 12, 36] is crucial for robots and has broad applica-tions for our human lives, e.g., the bin-picking for indus-trial robot, housework for service robot. Recently, most
*This work is supported by the National Nature Science Foundation of
China under Grant 62127807 and 62225310. The corresponding author is
Prof. Yang Cong.
†These authors contributed equally to this work
Figure 1. The demonstration of our proposed framework for wear-ing the cap: 1) the manipulation skill is learned via only one demonstration; 2) the learned manipulatin skill could be gener-alized to other novel caps without re-training. state-of-the-arts focus on 3D rigid object recognition and manipulation [9]; in contrast, less attention is concerned on non-rigid/deformable objects, which are actually more common in our real lives, such as clothes, animals, veg-etables or even human ourselves. This is partially because the motion space representation of rigid objects is relatively simple and could be represented by a 6-DOF linear vec-tor; nevertheless, the deformations of non-rigid/deformable objects are difﬁcult to match and get a uniform linear rep-resentation. Recently, the data-driven methods [2, 32, 39] achieve signiﬁcant progress for non-rigid/deformation ob-ject manipulation, which can estimate the states of the de-formable objects and predict the appropriate manipulations simultaneously. However, these methods suffer two issues: 1) Massive demonstration: thousands of repetitions of robot-object demonstrations are needed to train the model
to manipulate one speciﬁc instance; 2) Poor generaliza-tion: re-training is inevitably needed to transfer the learned skill from the known instance to a similar/new instance from the same category. Let us take the task of wearing a cap as an example, the training phase repeats thousands of times wearing procedures in the simulated environment or in the real world; however, to handle a novel cap, we need to rec-ollect data of the new cap and re-train the model to adapt to the shape and deformation of the new cap. Therefore, these complex and time-consuming procedures limit their practical applications.
In this paper, we aim at solving a more challenging task—learning to manipulate freely deformed unseen ob-jects of the same category, from only one demonstration. To learn from few demonstration, prior works learn dense fea-tures [13], and estimate the grasp pose on target objects by feature matching. However, such features are not robust to large deformation, and cannot generalize to novel objects.
To tackle such problem, we develop two new compo-nents: 1) the Nocs State Transfer (NST) module trans-fers the target objects under arbitrary deformation to a pre-deﬁned canonical state (i.e., Nocs state), thus effec-tively eliminating the disturbance caused by deformation in feature matching; 2) the Neural Spatial Encoding (NSE) module learns to encode the Nocs coordinates ob-tained from the NST into category-level features via self-reconstruction and contrastive losses. By encoding and con-strating between similar geometric structures, the NSE fea-tures can generalize well, thus further enabling effective manipulation pose transfer on novel objects. Our frame-work needs to be pre-trained only once for the whole ob-ject category, i.e., without re-training for some speciﬁc new instance. For different manipulation tasks on the objects within the same category, only one demonstration is needed.
Then, the robot could plan the related manipulation path ac-cordingly.
The main contributions are presented as follows:
• We present a novel framework which learns to ma-nipulate similar non-rigid/deformable objects via only one robot demonstration. To the best of our knowl-edge, this is the earliest exploration about generaliza-tion learning of deformable object manipulation.
• Our framework can generalize the learned skills from known instances to other novel/similar instances with-out tedious data collection or model re-training, which expands its application possibilities in the real world.
• We contribute a simulated caps dataset containing 4000 annotated frames of 40 deformable caps; more-over, a real robotic system is also designed to serve people wearing caps automatically. Both the simu-lated results and real-world experiments justify the ef-fectiveness of our proposed framework and system. 2.