Abstract
Despite advances in Visual Question Answering (VQA), the ability of models to assess their own correctness remains under-explored. Recent work has shown that VQA mod-els, out-of-the-box, can have difficulties abstaining from an-swering when they are wrong. The option to abstain, also called Selective Prediction, is highly relevant when deploy-ing systems to users who must trust the system’s output (e.g.,
VQA assistants for users with visual impairments). For such scenarios, abstention can be especially important as users may provide out-of-distribution (OOD) or adversarial in-puts that make incorrect answers more likely. In this work, we explore Selective VQA in both in-distribution (ID) and
OOD scenarios, where models are presented with mixtures of ID and OOD data. The goal is to maximize the number of questions answered while minimizing the risk of error on those questions. We propose a simple yet effective Learning from Your Peers (LYP) approach for training multimodal selection functions for making abstention decisions. Our approach uses predictions from models trained on distinct subsets of the training data as targets for optimizing a Se-lective VQA model. It does not require additional manual labels or held-out data and provides a signal for identify-ing examples that are easy/difficult to generalize to. In our extensive evaluations, we show this benefits a number of models across different architectures and scales. Overall, for ID, we reach 32.92% in the selective prediction metric coverage at 1% risk of error (C@1%) which doubles the previous best coverage of 15.79% on this task. For mixed
ID/OOD, using models’ softmax confidences for abstention decisions performs very poorly, answering <5% of ques-tions at 1% risk of error even when faced with only 10%
OOD examples, but a learned selection function with LYP can increase that to 25.38% C@1%.
*Equal contribution.
†Work primarily done during internship at FAIR.
Code: https://github.com/facebookresearch/selective-vqa_ood
Figure 1. VQA Models are able to answer straightforward ID questions, as in the top example where a SotA model [62] with and without our Learning from Your Peers (LYP) approach an-swers correctly. However, difficult OOD examples can arise, like the bottom example. With LYP, the model is able to abstain from answering to avoid outputting the incorrect answer, whereas the existing model is overconfident and outputs the answer anyways. 1.

Introduction
Recent successes of deep learning models for multi-modal tasks have created the potential for many exciting real-world applications that require a large degree of relia-bility, such as providing assistance to users with visual im-pairments [23, 51]. However, with these novel, high-stakes applications come responsibilities towards the users, as well as the need to revise problem setups and the general ap-proach to evaluating model performance. One particularly important consideration when developing models for real-world applications is reliability, i.e., the ability of the model to avoid making errors when facing uncertainty.
One way to approach reliability is to frame the problem as a selective prediction task [9, 14, 63]. In selective predic-tion, models are able to either output an answer or abstain from answering (i.e., effectively saying “I don’t know”) based on the model’s confidence/uncertainty in order to avoid making incorrect predictions. A prevalent cause of such incorrect predictions in real-world settings is distri-bution shifts [13, 20, 42], where the test environment may differ from the training environment and models could en-counter a wide variety of input examples at test time that
may not satisfy the independent and identically distributed assumption often made by practitioners when developing models. This is especially true in open-ended tasks like Vi-sual Question Answering (VQA) where models may receive adversarial, out-of-distribution (OOD) inputs that are diffi-cult to answer correctly. For example, in Fig. 1, a model is asked a question that requires background knowledge that it simply does not possess. While the ability to answer open-ended questions has been a point of focus in VQA, having a model perfectly answer all questions, ID and OOD, is likely unattainable [19, 29]. Therefore, framing this problem as a selective prediction task provides an avenue to handle such
OOD examples more gracefully as the model can abstain from answering on many of these inputs, while still attempt-ing to answer as many questions as possible. Doing this re-quires models to recognize OOD examples for abstention decisions (OOD detection) and generalize to OOD exam-ples (OOD generalization) in order to make predictions on examples that the model will get right.
However, previous evaluations for selective prediction in
VQA [63] have been done on ID data, where the questions and images all come from the VQA v2 dataset [21]. In NLP, there are efforts on selective prediction with OOD exam-ples [29, 59], although they tend to not address some prac-tical considerations, such as assuming access to OOD data or threshold generalization. More broadly, selective predic-tion and OOD generalization have largely been studied as independent problems in the literature [58].
In this work, we explore selective prediction for VQA with distribution shifts, where we present models with mix-tures of both ID and OOD examples, and measure the abil-ity of different approaches to optimize answering as many questions as possible while maintaining a low risk of error (or high accuracy) on those questions. We perform exten-sive experiments on VQA v2 [21] as our ID data and Ad-VQA [50] as our adversarial, OOD data.
We evaluate a number of state-of-the-art approaches to this problem and find that existing models’ softmax proba-bilities are generally poor confidence estimates for absten-tion decisions on OOD data, leading models to answer <5% of questions to achieve 1% risk of error in some settings.
Further, we show that training a selection function [63] im-proves performance ID and OOD, but integrating features from OOD detection methods as well as augmenting with known-OOD data (i.e., OOD data different from the un-known target distribution) do not improve beyond simply training this selection function on ID data. However, we observe that existing methods for training multimodal se-lection functions can require a held-out dataset in order to be most effective.
Therefore, we propose a Learning from Your Peers (LYP) approach that alleviates the need for held-out data while also allowing both the VQA model and selection function to learn from the additional data that would have been withheld. LYP works by breaking the training data into N subsets and training different VQA models on dis-tinct combinations of N − 1 subsets, leaving one subset out at a time. Our approach then uses these trained mod-els to predict answers on their respective N th left-out sub-sets. We recombine this data into an updated training set that has predictions from the different models. We utilize these predictions and the associated accuracies as labels to train a multimodal selection function, which learns to pre-dict the accuracies. By using predictions on the training data from models that have not seen these examples, our approach provides a signal for which examples in the train-ing data can be generalized to for a given model class, and which are too hard and should be abstained on.
Overall, our contributions are: We present an evaluation benchmark for Selective VQA on both ID and OOD data.
We show that model and data scaling are important factors for selective prediction and evaluate multiple baselines from prior works. Finally, we propose LYP and demonstrate that it can benefit performance over standard selection function training in both ID and mixed ID/OOD settings. 2.