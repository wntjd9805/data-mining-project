Abstract
Very low-resolution face recognition (VLRFR) poses unique challenges, such as tiny regions of interest and poor resolution due to extreme standoff distance or wide viewing angle of the acquisition devices.
In this paper, we study principled approaches to elevate the recognizability of a face in the embedding space instead of the visual quality.
We first formulate a robust learning-based face recogniz-ability measure, namely recognizability index (RI), based on two criteria: (i) proximity of each face embedding against the unrecognizable faces cluster center and (ii) closeness of each face embedding against its positive and negative class prototypes. We then devise an index diversion loss to push the hard-to-recognize face embedding with low RI away from unrecognizable faces cluster to boost the RI, which reflects better recognizability. Additionally, a percep-tibility attention mechanism is introduced to attend to the most recognizable face regions, which offers better explana-tory and discriminative traits for embedding learning. Our proposed model is trained end-to-end and simultaneously serves recognizability-aware embedding learning and face quality estimation. To address VLRFR, our extensive eval-uations on three challenging low-resolution datasets and face quality assessment demonstrate the superiority of the proposed model over the state-of-the-art methods. 1.

Introduction
In real-world face recognition deployment scenarios, the pixel resolution of the detected face images is signif-icantly deflated, due to extreme long-range distance and broad viewing angle of the acquisition devices, especially in surveillance applications. These tiny regions of interest are, in general, ranging from 16×16 to 32×32 pixels [60], thereby suffering from poor pixel resolution, in addition to unrestricted noises such as poor illumination conditions, non-frontal poses with awful angles, unconstrained facial expressions, blurriness, and occlusions [45].
It is note-worthy that these contaminated very low-resolution (VLR) face images undermine the overall performance of a face model trained with its high-resolution (HR) counterparts; therefore, there is a lack of generalizability to resolve the
VLR face recognition (VLRFR) problem [6]. Apart from that, training of a VLRFR model often suffers from very limited representative face examples to extract meaningful
identity-specific patterns. These issues are further escalated due to ambiguous inter-class variations for the heavily dis-torted face instances with perceptually similar identities in particular
[40]. Whilst matching a probe to a gallery set of the same resolution (i.e. VLR to VLR) is still an open challenge, the resolution gap between galleries and probes triggers another problem in cross-resolution matching (typ-ically HR galleries to VLR probes). Hence, the generaliza-tion performance of the prevalent deep learning models for
VLRFR is still far from satisfactory.
As a whole, most existing works designated for VLRFR improve the face quality of the VLR instances based on an auxiliary set of HR face images [28]. The generic operation modes are either in image domain (super-resolution, im-age synthesis) [52, 54, 58], embedding domain (resolution-invariant features, coupled mappings) [33, 44], or at clas-sifier level (transfer learning, knowledge distillation) [13, 14, 20, 39]. However, most of these state-of-the-art models require mated HR-VLR pairs of the same subject. This is unrealistic in practice as the HR-VLR pairs are often un-available.
As far as face recognition is concerned, face recogniz-ability (also known as face quality [17, 18]) can be deemed as a utility of how well a face image is for discrimina-tion purposes.
In other words, face quality is closely re-lated to face recognition performance. Some works thus focus on predicting a face image’s suitability for face recog-nition, commonly known as Face Image Quality Assess-ment (FIQA) [1, 18]. FIQA focuses either on (i) creating propositions to label the training data with face image qual-ity scores and solve a regression problem [17, 18, 36], or (ii) linking the face embedding properties to FIQ scores
[4,25,35,38,45]. The second approach shows better quality estimation, with the possible reason that the first approach is prone to mislabeling of ground truth quality [35, 45]. How-ever, the second approach may not be optimal since the
FIQ scores are estimated based on the embedding proper-ties rather than through a learning process [2].
Recently, [9] reported an intriguing observation that a deep learning-based face model induces an unrecognizable cluster in the embedding space. The cluster, known as un-recognizable identities (UIs), is formed by unrecognizable face examples, owing to diverse inferior quality factors, in-cluding VLR, motion blurred, poor illumination, occlusion, etc. Hence, these face examples with varying ground truths incline to lie close to the UIs, rather than their respective identity clusters. This observation inspires us to analyze the embedding distribution of the VLR face images against the UIs center. Interestingly, the extreme bimodal distribu-tion in Fig. 1 discloses that a significant number of the VLR faces in TinyFace [6], i.e., a realistic VLR face dataset, are hard-to-recognize from the human perspective and there-fore rendered next to the UIs cluster. We reckon that mining representative patterns from these hard-to-recognize faces is more meaningful for face recognition, in place of defining them as the elements of UIs. Apart from that, a more re-liable objective quality metric is needed to better interpret each VLR face example in terms of its embedding recog-nizability for recognizability-aware embedding learning.
Instead of perceptual quality, this work aims to elevate the recognizability of every VLR face embedding. In a nut-shell, we formulate a learning-based recognizability index (RI) with respect to the Cosine proximity of each embed-ding instance with (i) the UIs cluster, and (ii) the associ-ated positive and negative prototypes. In the meantime, the index diversion (ID) loss is presented to detach the hard-to-recognize embeddings from the UIs cluster, alongside a per-ceptibility attention mechanism. We underline that embed-ding learning in the direction opposing the UIs contributes to a higher explanatory power whilst promoting inter-class separation, particularly for hard-to-recognize instances. For clarity, we summarize our contributions as follows:
• A novel approach is proposed to address the VLRFR, including VLR-VLR and HR-VLR matching, by lever-aging the face recognizability notion in the embedding space to improve the hard-to-recognize instances.
• A robust learning-based face recognizability, dubbed
RI, is put forward. RI relies on the face embeddings’ intrinsic proximity relationship against the UIs cluster, positive, and negative class prototypes.
• An index diversion (ID) loss is devised to enhance the
RI for face embeddings. Further, we put forward a per-ceptibility attention mechanism to guide embedding learning from the most salient face regions.
• Our proposed model trained in an end-to-end man-ner not only renders a more discriminative embed-ding space for VLRFR but simultaneously serves recognizability-aware embedding learning and face recognizability estimation. 2.