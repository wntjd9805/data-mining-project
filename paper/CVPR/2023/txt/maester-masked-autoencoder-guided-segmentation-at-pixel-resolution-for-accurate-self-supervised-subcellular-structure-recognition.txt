Abstract 1.

Introduction
Accurate segmentation of cellular images remains an elusive task due to the intrinsic variability in morphology of biological structures. Complete manual segmentation is unfeasible for large datasets, and while supervised meth-ods have been proposed to automate segmentation, they often rely on manually generated ground truths which are especially challenging and time consuming to generate in biology due to the requirement of domain expertise. Fur-thermore, these methods have limited generalization capac-ity, requiring additional manual labels to be generated for each dataset and use case. We introduce MAESTER (Masked
AutoEncoder guided SegmenTation at pixEl Resolution), a self-supervised method for accurate, subcellular structure segmentation at pixel resolution. MAESTER treats segmen-tation as a representation learning and clustering problem.
Specifically, MAESTER learns semantically meaningful to-ken representations of multi-pixel image patches while si-multaneously maintaining a sufficiently large field of view for contextual learning. We also develop a cover-and-stride inference strategy to achieve pixel-level subcellular struc-ture segmentation. We evaluated MAESTER on a publicly available volumetric electron microscopy (VEM) dataset of primary mouse pancreatic islets β cells and achieved up-wards of 29.1% improvement over state-of-the-art under the same evaluation criteria. Furthermore, our results are competitive against supervised methods trained on the same tasks, closing the gap between self-supervised and super-vised approaches. MAESTER shows promise for alleviating the critical bottleneck of ground truth generation for imaging related data analysis and thereby greatly increasing the rate of biological discovery.
Code available at https : / / github . com / bowang-lab/MAESTER
*Equal contribution
†Project lead
‡Co-senior author
Imaging is widely used in biology to study the organi-zation, morphology, and function of cells and subcellular structures [13, 26, 28, 31, 32]. Segmentation of structures and objects of interest in the acquired images is often crit-ical for downstream analysis. Recent innovations in high throughput imaging technology enables larger scale datasets to be collected more quickly and cost efficiently [23, 24, 32].
Scalable and accurate segmentation hence becomes a crucial bottleneck to overcome. For example, volumetric electron microscopy (VEM) can generate terabytes of imaging data in a single run, enabling biologists to uncover ultrastructural features of cells at unprecedented resolution and scale in 3D [24]. Manual segmentation of such datasets are unfea-sible and especially when substantial domain knowledge is required for annotation of structures captured in the imaging volume.
With recent advancements in the field of machine learn-ing, automatic methods involving convolutional neural net-works (CNNs) have been developed to aid the segmentation process to great success [8, 18]. However, these methods often require extensive manual labels to train in the first place. Furthermore, supervised models often exhibit lim-ited generalization capacity, necessitating additional ground truth generation efforts for each new dataset or use case.
Presently, there is a dire need for a self-supervised segmenta-tion method to bypass the initial bottleneck of manual label generation, particularly when the cost and time of acquir-ing training supervision far exceeds the capacity to generate unlabelled data.
In addition to being self-supervised, the method needs to incorporate a few inductive biases to tackle the challenges of biological image segmentation. First, the texture of objects from the same class often remains consistent, despite great variability in shapes and sizes that cellular structures can exhibit. Therefore, the model needs to learn semantically meaningful representation of small image patches belonging to each structure of interest and distinguish between different textures. Second, the model needs to be capable of producing
Figure 1. MAESTER achieves self-supervised representation learning and segmentation through: (a) patchifying large sample from EM imaging (b) learning patch-level representation through predicting randomly masked region, (c) inferring the representation for the center voxel of each patch, (d) showing the 3D-rendered volume of our MAESTER generated segmentation, (e) demonstrating cover-and-stride strategy. features that precisely correspond to small regions in the original image. Not only will this increase the resolution of the resulting segmentation, it will also allow the method to take advantage of the locality assumption, which posits that small groups of adjacent pixels are more likely to belong to the same class. Third, the model needs to be context aware. While distinguishing between multi-pixel patches of images alone can achieve good segmentation results [5], we hypothesize that including a greater field of view (FOV) as context is crucial for better representation learning for the purpose of subcellular structure segmentation.
Transformer based architectures have seen recent suc-cesses in computer vision [3, 15, 29]. The token-wise rep-resentation of image patches offers a natural way to inject inductive bias into our self-supervised segmentation model.
We introduce MAESTER (Masked AutoEncoder guided Seg-menTation at pixEl Resolution), a self-supervised method that can achieve accurate, pixel-level segmentation of sub-cellular structures. MAESTER works in two stages. During training, MAESTER takes as input a large FOV (F × F pixels) containing ample local context which is further bro-ken down into multi-pixel patches of size P × P pixels.
The choice of P is sufficiently small to allow each patch to be treated as a single class under the locality assump-tion while achieving higher spatial precision. The attention mechanism of a vision transformer (ViT) [3] encoder then allows information sharing between nearby patches. Fur-thermore, taking inspiration from the Masked Autoencoder (MAE) [6] learning paradigm, we incorporate the surrogate task of multi-pixel patch masking and reconstruction via a light weight ViT decoder for each sampled FOV of a given image to simultaneously learn semantically meaningful to-ken representations of all patches in the FOV. During infer-ence, we deploy the trained encoder to generate millions of representations of unlabelled image patches via a novel cover-and-stride inference strategy. These representations are then clustered to produce a desired number of classes for self-supervised segmentation, leading to the final segmenta-tion of the given VEM dataset.
To our knowledge, we are the first to use the transformer architecture to incorporate the inductive biases needed for self-supervised subcellular structure segmentation. We also repurposed and optimized the MAE learning paradigm for generating semantically relevant token representations of multi-pixel sized image patches for classification into bio-logically concordant clusters for segmentation rather than for pretraining or representation learning at the image level.
Lastly, we introduce a cover-and-stride inference strategy to achieve pixel level segmentation of the given biological images. We tested MAESTER on the betaSeg dataset [20], consisting of primary mouse pancreatic islets β cells and yielded upwards of 29.1% increase in performance compared to prior state-of-the-art [5]. We also benchmarked against
Segmenter [27] and vanilla ViT [3], two supervised seg-mentation models with access to all ground truth labels in addition to the raw images used to train MAESTER. We find
MAESTER achieved competitive results for the predomi-nant classes, closing the gap between supervised and self-supervised segmentation models. We believe MAESTER has the potential to drastically speed up the experimental cycle of biological imaging experiments by alleviating the critical bottleneck of manual label generation and greatly increasing the rate of scientific inquiry in cell biology. 2.