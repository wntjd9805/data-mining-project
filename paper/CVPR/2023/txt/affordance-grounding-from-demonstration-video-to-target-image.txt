Abstract
Humans excel at learning from expert demonstrations and solving their own problems. To equip intelligent robots and assistants, such as AR glasses, with this ability, it is essential to ground human hand interactions (i.e., affor-dances) from demonstration videos and apply them to a target image like a user’s AR glass view. This video-to-image affordance grounding task is challenging due to (1) the need to predict fine-grained affordances, and (2) the lim-ited training data, which inadequately covers video-image discrepancies and negatively impacts grounding. To tackle them, we propose Affordance Transformer (Afformer), which has a fine-grained transformer-based decoder that gradually refines affordance grounding. Moreover, we introduce Mask Affordance Hand (MaskAHand), a self-supervised pre-training technique for synthesizing video-image data and simulating context changes, enhancing af-fordance grounding across video-image discrepancies. Af-former with MaskAHand pre-training achieves state-of-the-art performance on multiple benchmarks, including a sub-stantial 37% improvement on the OPRA dataset. Code is made available at https://github.com/showlab/afformer. 1.

Introduction
Humans frequently learn from observing others interact with objects to enhance their own experiences, such as fol-lowing an online tutorial to operate a novel appliance. To equip AI systems with this ability, a key challenge lies in comprehending human interaction across videos and im-ages. Specifically, a robot must ascertain the points of inter-action (i.e., affordances [19]) in a demonstration video and apply them to a new target image, such as the user’s view through AR glasses.
This process is formulated as video-to-image affordance grounding, recently proposed by [13], which presents a more challenging setting than previous affordance-related tasks, including affordance detection [11, 59], action-to-image grounding [18,40,41,44], and forecasting [16,35,36].
†Corresponding Author.
Figure 1. This figure demonstrates the video-to-image affordance grounding task, which aims to identify the area of human hand in-teraction (i.e., affordance) in a demonstration video and map it to a target image (e.g., AR glass view). Our contributions include (1) proposing a self-supervised pre-training approach for affordance grounding, and (2) establishing a new model that excels remark-ably in fine-grained heatmap decoding.
The complexity of this setting stems from two factors: (1)
Fine-grained grounding: Unlike conventional grounding tasks that usually localize coarse affordance positions (e.g., identifying all buttons related to “press”), video-to-image affordance grounding predicts fine-grained positions spe-cific to the query video (e.g., only buttons pressed in the video). (2) Grounding accross various video-image discrep-ancies: Demonstration videos and images are often cap-tured in distinct environments, such as a store camera’s per-spective versus a user’s view in a kitchen, which compli-cates the grounding of affordances from videos to images.
Moreover, annotating for this task is labor-intensive, as it necessitates thoroughly reviewing the entire video, corre-lating it with the image, and pinpointing affordances. As a result, affordance grounding performance may be limited by insufficient data on diverse video-image discrepancies.
To enable fine-grained affordance grounding, we pro-pose a simple yet effective Affordance transformer (Af-former), which progressively refines coarse-grained predic-tions into fine-grained affordance grounding outcomes. Pre-vious methods [13,40,44] either simply employ large stride upsampling or deconvolution for coarse-grained affordance heatmap prediction (e.g., 8 × 8 → 256 × 256 [13]), or just evaluate at low resolution (e.g., 28 × 28 [40, 44]). As a result, these methods struggle with fine-grained affordance grounding, particularly when potential affordance regions are closely situated (e.g., densely packed buttons on a mi-crowave). Our approach employs cross-attention [4, 5] be-tween multi-scale feature pyramids, to facilitate gradual de-coding of fine-grained affordance heatmaps.
To address the limited data issue that inadequately covers video-image differences and hampers affordance ground-ing performance, we present a self-supervised pre-training method, Masked Affordance Hand (MaskAHand), which can leverage vast online videos to improve video-to-image affordance grounding. MaskAHand can automatically gen-erate target images from demonstration videos by mask-ing hand interactions and simulating contextual differences between videos and images.
In the generated target im-age, the task involves estimating the interacting hand re-gions by watching the original video, thereby enhancing the similarity capabilities crucial for video-to-image affor-dance grounding. Our approach uniquely simulates context changes, an aspect overlooked by previous affordance pre-training techniques [18, 36]. Furthermore, we also rely less on external off-the-shelf tools [1,39,48,50] used in [18,36].
We conducted comprehensive experiments to evaluate our Afformer and MaskAHand methods on three video-to-image affordance benchmarks, namely OPRA [13], EPIC-Hotspot [44], and AssistQ [57]. Compared to prior architec-tures, our most lightweight Afformer variant achieves a rel-ative 33% improvement in fine-grained affordance ground-ing (256 × 256) on OPRA and relative gains of 10% to 20% for coarse-grained affordance prediction (28×28) on OPRA and EPIC-Hotspot. Utilizing MaskAHand pre-training for
Afformer results in zero-shot prediction performance that is comparable to previously reported fully-supervised meth-ods on OPRA [13], with fine-tuning further enhancing the improvement to 37% relative gains. Moreover, we demon-strate the advantage of MaskAHand when the data scale for downstream tasks is limited: on AssistQ, with only approx-imately 600 data samples, MaskAHand boosts Afformer’s performance by a relative 28% points. 2.