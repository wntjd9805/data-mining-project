Abstract
We propose a visual-linguistic representation learning approach within a self-supervised learning framework by introducing a new operation, loss, and data augmentation strategy. First, we generate diverse features for the image-text matching (ITM) task via soft-masking the regions in an image, which are most relevant to a certain word in the cor-responding caption, instead of completely removing them.
Since our framework relies only on image-caption pairs with no ﬁne-grained annotations, we identify the relevant regions to each word by computing the word-conditional vi-sual attention using multi-modal encoder. Second, we en-courage the model to focus more on hard but diverse exam-ples by proposing a focal loss for the image-text contrastive learning (ITC) objective, which alleviates the inherent lim-itations of overﬁtting and bias issues. Last, we perform multi-modal data augmentations for self-supervised learn-ing via mining various examples by masking texts and ren-dering distortions on images. We show that the combina-tion of these three innovations is effective for learning a pretrained model, leading to outstanding performance on multiple vision-language downstream tasks. 1.

Introduction
Vision-language representation learning aims at opti-mizing a joint embedding model for the data in both the image and text domains, where the learned representa-tions are transferred to solve various vision-language down-stream tasks including image-text retrieval [20, 27, 32], vi-sual question answering [1, 3, 29, 44], visual reasoning [37, 41], etc. With the introduction of large-scale image-text datasets [18, 27, 31, 36] and the recent advances in language models [2, 7, 40], research for vision-language representa-tion learning [4, 17, 21, 24, 28] has been actively exploring self-supervision tasks such as image-text matching (ITM), masked language modeling (MLM), and image-text con-trastive learning (ITC).
During the pretraining stage, the model solves the tasks based only on image-caption pairs with no ﬁne-grained an-notations, i.e., annotations of region-word relations. Thus,
Figure 1. Illustration of the difference between the proposed soft feature masking and the random hard feature masking. The ran-dom hard feature masking has a risk of completely removing the important regions while our soft feature masking strategy still pre-serves the regions. the model focuses on the most discriminative regions in im-ages, which typically correspond to the primary objects in the scenes. Hence, the model lacks a comprehensive under-standing of various attributes observed in images despite de-tailed descriptions in captions. To alleviate this limitation, we propose the following three techniques for diversifying observations, eventually resulting in performance improve-ment of learned representations.
First, we devise a soft masking technique on visual fea-tures for training, which suppresses activations at the dis-criminative parts in an input image with respect to a cer-tain word in its caption. The proposed soft feature mask-ing makes a model focus on the regions that are initially paid less attention to and synthesizes diverse samples dur-ing the training procedure. To be speciﬁc, we ﬁrst utilize the cross-attention map of image and text features derived by the multi-modal fusion encoder and identify salient re-gions in the image by computing the word-speciﬁc Grad-CAM [35] of the image-text matching score. Then we feed a softly-masked image feature to the fusion encoder for learning to match the features of both modalities. While the most straightforward way to augment diverse visual features is to employ discrete hard masks as recent trends [12, 42], which aim to reconstruct images, our approach suppresses information in images using the soft masks with real-valued
weights instead of completely removing the content relevant to input text. Figure 1 illustrates the difference between the proposed soft mask and the random hard (discrete) mask.
Second, we also encourage the model to focus more on hard examples by adopting a focal version of the ITC loss.
The focal loss [26] is originally designed for object detec-tion to prevent overﬁtting and class imbalance problems, and a similar idea turns out to be effective for our contrastive learning task. This is partly because our task still has over-ﬁtting issues for easy examples and is also biased toward negative examples. Since large-scale image-caption corpora are composed of multiple datasets with large domain gaps, a model distinguishes a lot of samples from different datasets very easily [6]. Hence, we propose the focal ITC loss for our model to achieve more balanced training.
In addition to the two components discussed above, we perform multi-modal data augmentations, which include di-verse strong augmentations proposed in [43] and binary masks to captions for the ITM task. Strong data augmen-tations have been believed to harm vision-language mod-els [16, 39] by breaking the semantic consistency of the image-text pair, but we argue that this does not hold for the large-scale vision-language representation pretraining, as also shown in recent studies [15,25]. Note that the use of binary masks in ITM also homogenizes text inputs with the masked language model (MLM) task.
The organization of this paper is as follows. We ﬁrst discuss related works about vision-language representation learning in Section 2 and provide the preliminaries about vision-language representation learning in Section 3. Sec-tion 4 discusses the vision-language representation learning approach equipped with our ideas. We demonstrate the per-formance of the proposed approach on multiple downstream tasks in Section 5 and conclude this paper in Section 6. 2.