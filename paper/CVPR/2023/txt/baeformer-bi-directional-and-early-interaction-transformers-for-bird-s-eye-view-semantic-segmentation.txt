Abstract
Bird’s Eye View (BEV) semantic segmentation is a critical task in autonomous driving. However, existing
Transformer-based methods confront difficulties in trans-forming Perspective View (PV) to BEV due to their unidi-rectional and posterior interaction mechanisms. To address this issue, we propose a novel Bi-directional and Early In-teraction Transformers framework named BAEFormer, con-sisting of (i) an early-interaction PV-BEV pipeline and (ii) a bi-directional cross-attention mechanism. Moreover, we find that the image feature maps’ resolution in the cross-attention module has a limited effect on the final perfor-mance. Under this critical observation, we propose to en-large the size of input images and downsample the multi-view image features for cross-interaction, further improving the accuracy while keeping the amount of computation con-trollable. Our proposed method for BEV semantic segmen-tation achieves state-of-the-art performance in real-time in-ference speed on the nuScenes dataset, i.e., 38.9 mIoU at 45
FPS on a single A100 GPU. 1.

Introduction
Recently, pure vision-based perception methods [16, 18, 25, 38] have occupied a significant position in autonomous driving due to their higher signal-to-noise ratio and lower cost compared to LIDAR-based methods [6, 14, 35, 39, 42].
Among them, Bird’s-Eye-View (BEV) perception has be-come the mainstream method. The BEV representation learning in vision-centric autonomous driving is to take con-secutive frames from multiple surrounding cameras as input and transform the pixel panel view to Bird’s-Eye-View to
∗ This work was done during Cong Pan’s internship at Horizon
Robotics. (cid:66) Zhaoxiang Zhang is the Corresponding author. conduct perception tasks such as 3D object detection, map-view semantic segmentation, and motion prediction.
The BEV representation offers several inherent advan-tages for visual perception. Firstly, it facilitates the inte-gration of pure vision-based results with those from other modalities. Secondly, it provides a natural means to unify and express different perspectives under BEV to simplify the subsequent module development and deployment, such as planning and control. Finally, the object representation in BEV circumvents the common scale and occlusion diffi-culties that arise in 2D tasks.
The enhancement of BEV perception performance hinges on the rapid and graceful acquisition of road and ob-ject feature representations. Figure 1 illustrates that there are two categories of BEV perception pipelines based on distinct interaction mechanisms: (a) Late-interaction and (b) Middle-interaction. The late-interaction pipeline [24] employs independent perception on each camera view, fol-lowed by temporal and spatial fusion of the results into a unified BEV space. Recently, the most widely used pipeline is the middle-interaction [16, 18, 25, 38, 41]. It concatenates all camera inputs as a whole into the network, transforms them into the BEV space, and then outputs the result di-rectly. The middle-interaction pipeline comprises a well-defined workflow for feature extraction, space transforma-tion, and BEV space learning. Nevertheless, the transfor-mation of PV to BEV using these two interaction strategies remains arduous. To tackle this challenge, we propose a novel paradigm: (c) Early-interaction method, which de-serves further attention from the research community.
Our proposed early-interaction method offers distinct advantages when compared to the two existing strategies.
Firstly, the image-space backbone only extracts image fea-tures with different resolutions sequentially without any in-formation integration across resolutions.
In contrast, our proposed method advocates for the integration of global
Figure 1. View-transformation taxonomy. The illustrations of the previous (a) late-interaction pipeline, (b) middle-interaction pipeline, and (c) our proposed early-interaction pipeline. contextual information and local details, which enables the delivery of richer semantic information to the BEV space.
Secondly, the computation of the core module in the exist-ing strategies is predominantly occupied by the image-space backbone, which does not incorporate any BEV space in-formation. Moreover, the information flow in the forward processing of late-interaction and middle-interaction strate-gies is unidirectional, with information flowing from the image space to the BEV space, while the information in the BEV space does not effectively affect the features in the image space. To address these issues, we suggest that the view transformation of image features to BEV features should occur not only after the image feature extraction but can be converted gradually during the extraction process.
This way, the information flow can implicitly interact bi-laterally, aligning features in the PV and BEV. Ultimately, the core challenge of vision-based BEV perception is the transformation from image space to BEV space. We pro-pose a solution by distributing the learning of cross-space alignment throughout the entire structure, with the image network learning not only good feature representation but also cross-space alignment.
To this end, we propose a novel framework named
Bi-directional and Early Interaction Transformers (BAE-Former) to effectively aggregate multi-scale image features into a better BEV feature representation and perform map-view semantic segmentation. Firstly, we initialize grid-shape BEV Queries and encode the camera parameters into positional embeddings, following the CVT [41] method.
Subsequently, we utilize the cross-attention mechanism in
Transformer [8, 37] to interact BEV features with multi-scale image features in both directions. This bi-directional interaction involves the use of an unshared attention map to update BEV and image features simultaneously, with image features from the previous scale influencing the extraction of the following scale’s features. Furthermore, we observe that the resolution of the multi-scale feature maps during the interaction has a negligible effect on the final accuracy.
As such, we can maintain the full amount of interaction while managing the number of parameters and computa-tional costs by increasing the input image resolution and downsampling the image features at each scale before inter-action.
Our contributions can be summarized as follows:
• We propose a novel framework, Bi-directional and
Early Interaction Transformers to achieve a better view transformation from image fea-ture space to BEV feature space. (BAEFormer),
• We find that the image features’ resolution during the interaction does not significantly influence perfor-mance, so we can increase the input image resolution and downsample the image features at each scale be-fore an interaction. This will lead to superior per-formance while simultaneously controlling parameters and computational costs.
• We conduct extensive experiments on nuScenes [3] and Lyft [13] datasets with comprehensive ablation studies, demonstrating the efficiency and effective-ness of our proposed BAEFormer method. We achieve state-of-the-art performance at real-time infer-ence speed on BEV semantic segmentation.
2.