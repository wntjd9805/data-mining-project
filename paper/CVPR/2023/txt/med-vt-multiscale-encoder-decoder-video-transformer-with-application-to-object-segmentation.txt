Abstract
Multiscale video transformers have been explored in a wide variety of vision tasks. To date, however, the multiscale processing has been confined to the encoder or decoder alone. We present a unified multiscale encoder-decoder transformer that is focused on dense prediction tasks in videos. Multiscale representation at both encoder and de-coder yields key benefits of implicit extraction of spatiotem-poral features (i.e. without reliance on input optical flow) as well as temporal consistency at encoding and coarse-to-fine detection for high-level (e.g. object) semantics to guide precise localization at decoding. Moreover, we pro-pose a transductive learning scheme through many-to-many label propagation to provide temporally consistent predic-tions. We showcase our Multiscale Encoder-Decoder Video
Transformer (MED-VT) on Automatic Video Object Seg-mentation (AVOS) and actor/action segmentation, where we outperform state-of-the-art approaches on multiple bench-marks using only raw images, without using optical flow. 1.

Introduction
Transformers have been applied to a wide range of image and video understanding tasks as well as other areas [13].
The ability of such architectures to establish data relation-ships across space and time without the local biases in-herent in convolutional and other similarly constrained ap-proaches arguably is key to the success. Multiscale process-ing has potential to enhance further the learning abilities of transformers through cross-scale learning [4, 6, 8, 19, 43].
A gap exists, however, as no approach has emerged that makes full use of multiscale processing during both encod-ing and decoding in video transformers. Recent work has focused on multiscale transformer encoding [8,19], yet does not incorporate multiscale processing in the transformer de-coder. Other work has proposed multiscale transformer de-coding [6], yet was designed mainly for single images and
Figure 1. Comparison of state-of-the-art multiscale video trans-formers and our approach. Video transformers take an input clip and feed features to a transformer encoder-decoder, with alterna-tive approaches using multiscale processing only in the encoder or decoder. We present a unified multiscale encoder-decoder video transformer (MED-VT), while predicting temporally consistent segmentations through transductive learning. We showcase MED-VT on two tasks, video object and actor/action segmentation. did not consider the structured prediction nature inherent to video tasks, i.e. the importance of temporal consistency.
In response, we present the first Multiscale Encoder
Decoder Video Transformer (MED-VT). At encoding, its within and between-scale attention mechanisms allow it to capture both spatial, temporal and integrated spatiotemporal information. At decoding, it introduces learnable coarse-to-fine queries that allow for precise target delineation, while enforcing temporal consistency among the predicted masks through transductive learning [38, 52].
We primarily illustrate the utility of MED-VT on the task
of Automatic Video Object Segmentation (AVOS). AVOS separates primary foreground object(s) from background in a video without any supervision, i.e. without information on the objects of interest [53]. This task is important and chal-lenging, as it is a key enabler of many subsequent visually-guided operations, e.g., autonomous driving and augmented reality. AVOS shares challenges common to any VOS task (e.g., object deformation and clutter). Notably, however, the requirement of complete automaticity imposes extra chal-lenges to AVOS, as it does not benefit from any per video initialization. Lacking prior information, solutions must ex-ploit appearance (e.g., colour and shape) as well as motion to garner as much information as possible.
MED-VT responds to these challenges. Its within and between scale attention mechanisms capture both appear-ance and motion information as well as yield temporal con-sistency. Its learnable coarse-to-fine queries allow seman-tically laden information at deeper layers to guide finer scale features for precise object delineation. Its transductive learning through many-to-many label propagation ensures temporally consistent predictions. To showcase our model beyond AVOS, we also apply it to actor/action segmenta-tion. Figure 1 overviews MED-VT compared to others. 2.