Abstract
Deep neural networks have made huge progress in the last few decades. However, as the real-world data often ex-hibits a long-tailed distribution, vanilla deep models tend to be heavily biased toward the majority classes. To ad-dress this problem, state-of-the-art methods usually adopt a mixture of experts (MoE) to focus on different parts of the long-tailed distribution. Experts in these methods are with the same model depth, which neglects the fact that different classes may have different preferences to be fit by models with different depths. To this end, we propose a novel MoE-based method called Self-Heterogeneous Integration with
Knowledge Excavation (SHIKE). We first propose Depth-wise Knowledge Fusion (DKF) to fuse features between dif-ferent shallow parts and the deep part in one network for each expert, which makes experts more diverse in terms of representation. Based on DKF, we further propose Dynamic
Knowledge Transfer (DKT) to reduce the influence of the hardest negative class that has a non-negligible impact on the tail classes in our MoE framework. As a result, the clas-sification accuracy of long-tailed data can be significantly improved, especially for the tail classes. SHIKE achieves the state-of-the-art performance of 56.3%, 60.3%, 75.4% and 41.9% on CIFAR100-LT (IF100), ImageNet-LT, iNatu-ralist 2018, and Places-LT, respectively. The source code is available at https://github.com/jinyan-06/SHIKE. 1.

Introduction
Deep learning has made incredible progress in visual recognition tasks during the past few years. With well-designed models, e.g., ResNet [18], and Transformer [57],
*Corresponding author: Yang Lu, luyang@xmu.edu.cn
Figure 1. Comparison of test accuracy of a ResNet-32 model with two shallow branches and a deep branch. The model is jointly trained on CIFAR100-LT with an imbalance factor of 100. Only the highest accuracy among the three branches is shown for each class. deep learning techniques have outperformed humans in many visual applications, like image classification [32], se-mantic segmentation [17, 42], and object detection [50, 52].
One key factor in the success of deep learning is the avail-ability of large-scale datasets [13, 55, 70], which are usually manually constructed and annotated with balanced train-ing samples for each class. However, in real-world ap-plications, data typically follows a long-tailed distribution, where a small fraction of classes possess massive samples, but the others are with only a few samples [5,12,26,41,44].
Such imbalanced data distribution leads to a significant ac-curacy drop for deep learning models trained by empirical risk minimization (ERM) [56] as the model tends to be bi-ased towards the head classes and ignore the tail classes to a great extent. Thus, the model’s generalization ability on tail classes is severely degraded.
The most straightforward action for long-tailed recog-nition often focuses on re-balancing the learning process from either a data processing [3, 27, 46] or cost-sensitive perspective [12, 28, 71]. Recently, methods proposed for
long-tailed data have drawn more attention to representa-tion learning. For example, the decoupling strategy [27] is proposed to deal with the inferior representation caused by re-balancing methods. Contrastive learning [11,26] special-izes in learning better and well-distributed representations.
Among them, the methods that achieve state-of-the-art per-formance are usually based on a mixture of experts (MoE), also known as multi-experts. Some MoE-based methods prompt different experts to learn different parts of the long-tailed distribution (head, medium, tail) [4, 10, 39, 61], while some others were designed to reduce the overall model’s prediction variance or uncertainty [33, 60].
Unlike traditional ensemble learning methods that adopt independent models for joint prediction, the MoE-based methods for long-tailed learning often adopt a multi-branch model architecture with shared shallow layers and exclu-sive deep layers. Thus, the features generated by different experts are actually from the model with the same depth, although the methods force them to be diverse from various perspectives. Recently, self-distillation [64] is proposed to enable shallow networks to have the ability to predict cer-tain samples in the data distribution. This brings us to a new question: can we integrate the knowledge from shal-low networks into some experts in MoE to fit the long-tailed data in a self-adaptive manner regardless of the number of samples? With this question, we conduct a quick experi-ment to reveal the preference of the deep neural network on different classes in long-tailed data. A ResNet-32 model with branches directly from shared layers is adopted. Each branch contains an independent classifier after feature align-ment, and all classifiers are re-trained with balanced soft-max cross entropy [49]. Fig. 1 shows the highest accuracy among the three branches for each class. We can clearly observe that shallow parts of the deep model are able to perform better on certain tail classes. This implies that dif-ferent parts of the long-tailed distribution might accommo-date the network differently according to the depth. Thus the shallow part of the deep model can provide more useful information for learning the long-tailed distribution.
Driven by the observation above, we propose a novel
MoE-based method called Self-Heterogeneous Integration with Knowledge Excavation (SHIKE). SHIKE adopts an
MoE-based model consisting of heterogeneous experts along with knowledge fusion and distillation. To fuse the knowledge diversely, we first introduce Depth-wise Knowl-edge Fusion (DKF) as a fundamental component to incor-porate different intermediate features into deep features for each expert. The proposed DKF architecture can not only provide more informative features for experts but also opti-mize more directly to shallower layers of the networks by mutual distillation. In addition, we design Dynamic Knowl-edge Transfer (DKT) to address the problem of the hard-est negatives during knowledge distillation between experts.
DKT elects the non-target logits with large values to re-form non-target predictions from all experts to one grand teacher, which can be used in distilling non-target predic-tions to suppress the hardest negative, especially for the tail classes. DKT can fully utilize the structure of MoE and di-versity provided by DKF for better model optimization. In this paper, our contributions can be summarized as follow:
• We propose Depth-wise Knowledge Fusion (DKF) to encourage feature diversity in knowledge distillation among experts, which releases the potential of the
MoE in long-tailed representation learning.
• We propose a novel knowledge distillation strategy
DKT for MoE training to address the hardest negative problem for long-tailed data, which further exploits the diverse features fusing enabled by DKF.
• We outperform other state-of-the-art methods on four benchmarks by achieving performance 56.3%, 60.3%, 75.4% and 41.9% accuracy for CIFAR100-LT (IF100),
ImageNet-LT, iNaturalist 2018, and Places-LT, respec-tively. 2.