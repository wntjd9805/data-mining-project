Abstract
Federated learning (FL) has recently attracted increas-ing attention from academia and industry, with the ultimate goal of achieving collaborative training under privacy and communication constraints. Existing iterative model av-eraging based FL algorithms require a large number of communication rounds to obtain a well-performed model due to extremely unbalanced and non-i.i.d data partitioning among different clients. Thus, we propose FedDM to build the global training objective from multiple local surrogate functions, which enables the server to gain a more global view of the loss landscape.
In detail, we construct syn-thetic sets of data on each client to locally match the loss landscape from original data through distribution match-ing. FedDM reduces communication rounds and improves model quality by transmitting more informative and smaller synthesized data compared with unwieldy model weights.
We conduct extensive experiments on three image classifica-tion datasets, and show that our method outperforms other
FL counterparts in terms of efficiency and model perfor-mance given a limited number of communication rounds.
Moreover, we demonstrate that FedDM can be adapted to preserve differential privacy with Gaussian mechanism and train a better model under the same privacy budget. 1.

Introduction
Traditional machine learning methods are designed with the assumption that all training data can be accessed from a central location. However, due to the growing data size together with the model complexity [10, 23, 26], distributed optimization [7, 8, 43] is necessary over different machines.
This leads to the problem of Federated Learning [32] (FL) – multiple clients (e.g. mobile devices or local organizations) collaboratively train a global model under the orchestration of a central server (e.g. service provider) while the training
*Equal contribution. data are kept decentralized and private. Such a practical set-ting poses two primary challenges [20,24,29,31,32]: train-ing data of the FL system are highly unbalanced and non-i.i.d. across downstream clients and more efficient communication with fewer costs is expected because of unreliable devices with limited transmission bandwidth.
Most of the existing FL methods [22,28,31,32,48] adopt an iterative training procedure from FedAvg [32], in which each round takes the following steps: 1) The global model is synchronized with a selected subset of clients; 2) Each client trains the model locally and sends its weight or gra-dient back to the server; 3) The server updates the global model by aggregating messages from selected clients. This framework works effectively for generic distributed opti-mization while the difficult and challenging setting of FL, unbalanced data partition in particular, would result in sta-tistical heterogeneity in the whole system [30] and make the gradient from each client inconsistent. It poses a great challenge to the training of the shared model, which re-quires a substantial number of communication rounds to converge [28]. Although some improvements have been made over FedAvg [32] including modifying loss func-tions [31], correcting client-shift with control variates [22] and the like, the reduced number of communication round is still considerable and even the amount of information re-quired by the server rises [56].
In our paper, we propose a different iterative surrogate minimization based method, FedDM, referred to Federated
Learning with iterative Distribution Matching. Instead of the commonly-used scheme where each client maintains a locally trained model respectively and sends its gradi-ent/weight to the server for aggregation, we take a distinct perspective at the client’s side and attempt to build a lo-cal surrogate function to approximate the local training ob-jective. By sending those local surrogate functions to the server, the server can then build a global surrogate function around the current solution and conduct the update by min-imizing this surrogate. The question is then how to build local surrogate functions that are informative and with a rel-ative succinct representation. Inspired by recent progresses in data condensation [54, 55] we build local surrogate func-tions by learning a synthetic dataset to replace the origi-nal one to approximate the objective.
It can be achieved by matching the original data distribution in the embed-ding space [16]. After the optimization of synthesized data, the client can transmit them to the server, which can then leverage the synthetic dataset to recover the global objec-tive function for training. Our method enables the server to have implicit access to the global objective defined by the whole balanced dataset from all clients, and thus out-performs previous algorithms involved in training a local model with unbalanced data in terms of communication ef-ficiency and effectiveness. We also demonstrate that our method can be adapted to preserve differential privacy un-der, an important factor to the deployment of FL systems.
Our contributions are primarily summarized as follows:
• We propose FedDM, which is based on iterative distri-bution matching to learn a surrogate function. It sends synthesized data to the server rather than commonly-used local model updates and improves communica-tion efficiency and effectiveness significantly.
• We analyze how to protect privacy of client’s data for our method and show that it is able to guarantee (ϵ, δ)-differential privacy with the Gaussian mechanism and train a better model under the same privacy budget.
• We conduct comprehensive experiments on three tasks and demonstrate that FedDM is better than its FL counterparts in communication efficiency and the final model performance. 2.