Abstract
Recently, the sizes of deep neural networks and train-ing datasets both increase drastically to pursue better performance in a practical sense. With the prevalence of transformer-based models in vision tasks, even more pressure is laid on the GPU platforms to train these heavy time and models, which consumes a large amount of computing resources as well. Therefore, it’s crucial to accelerate the training process of deep neural networks.
In this paper, we propose a general network expansion method to reduce the practical time cost of the model training process. Specifically, we utilize both width- and depth-level sparsity of dense models to accelerate the training of deep neural networks. Firstly, we pick a sparse sub-network from the original dense model by reducing the number of parameters as the starting point of training.
Then the sparse architecture will gradually expand during the training procedure and finally grow into a dense one.
We design different expanding strategies to grow CNNs and ViTs respectively, due to the great heterogeneity in between the two architectures. Our method can be easily integrated into popular deep learning frameworks, which saves considerable training time and hardware resources.
Extensive experiments show that our acceleration method can significantly speed up the training process of modern vision models on general GPU devices with negligible performance drop (e.g. 1.42× faster for ResNet-101 and 1.34× faster for DeiT-base on ImageNet-1k). The code is available at https : / / github . com / huawei -noah / Efficient - Computing / tree / master /
TrainingAcceleration / NetworkExpansion and https : / / gitee . com / mindspore / hub / blob / master / mshub _ res / assets / noah - cvlab / gpu / 1.8/networkexpansion_v1.0_imagenet2012.md. 1.

Introduction
Deep neural networks have demonstrate their excellent performance on multiple vision tasks, such as classifica-⋆ Corresponding authors. tion [15, 30, 44], object detection [12, 43], semantic seg-mentation [32, 35], etc. In spite of their success, these net-works usually come with heavy architectures and severe over-parameterization, and therefore it takes many days or even weeks to train such networks from scratch. The ever-increasing model complexity [23, 24, 34, 42] and train-ing time cause not only a serious slowdown for the re-search schedule, but also a huge waste of time and com-puting resources. However, CNNs are still going deeper and bigger for higher capacity to cope with extremely large datasets [27, 45]. Recently, a new type of architecture named vision transformers (ViTs) have emerged and soon achieved state-of-the-art performances on multiple com-puter vision tasks [16, 48, 52, 57]. Originating from Natural
Language Processing, the vision transformer has a different network topology and larger computational complexity than
CNNs. Besides, transformer-based models usually require more epochs to converge.
From another perspective, compared with purchasing ex-pensive GPU servers, many researchers and personal users nowadays choose cloud computing service to run experi-ments and pay their bills by GPU-hours. Thus, an acceler-ated training framework is obviously cost-efficient. On the other hand, shortened training time leads to not only quicker idea verification but also more refined hyper-parameter tun-ing, which is crucial to the punctual completion of the project and on-time product delivery.
There are some existing methods about efficient model training [36, 51, 53, 55], but few of them can achieve high practical acceleration on geneal GPU platforms. [53] pro-poses to prune the gradients of feature maps during back-propagation to reduce train-time FLOPs, and achieve train-ing speedup on CPU platform. [51] conducts efficient CNN training on ARM and FPGA devices to reduce power con-sumption. [36] prunes weights of the network to achieve training acceleration but eventually yield a pruned sparse model with non-negligible performance drop. [55] skips easy samples that contribute little to loss reduction by using an assistant model asynchronously running on CPU. Yet it
requires sophisticated engineering implementation. Though the prior works claim an ideal theoretical acceleration ra-tio, none of them achieve obviously practical acceleration on common GPU platforms. Most of these works overlook the most general scenario, i.e. accelerating training on gen-eral GPU platforms with popular deep learning frameworks such as PyTorch [40] and TensorFlow [1]. The lack of re-lated research is probably because GPU servers are not so power-constrained as edge devices.
In this paper, we propose a general training accelera-tion framework (network expansion) for both CNN and ViT models to reduce the practical training time. We first sample a sub-network from the original dense model as the starting point of training. Then this sparse architecture will grad-ually expand its network topology by adding new parame-ters, which increases the model capacity along the training procedure. When performing network expansion, we fol-low the principle of avoiding the introduction of redundant parameters. For CNN, new filters are progressively added whose weights are initialized by imposing filter-level or-thogonality. This reduces the correlation between old and new feature maps and improves the expressiveness of the convolutional network. For vision transformers, we first train a shallow sub-network with fewer layers, and create an exponential moving average (EMA) version of the trained model. As the training continues, some layers of the EMA model will be inserted into the trained model to construct a deeper one. With the network expansion training paradigm, the sampled sub-network eventually grows into the desired dense architecture, and thus the total training FLOPs and time are greatly reduced.
Our method can be easily integrated into popular deep learning frameworks on general GPU platforms. Without changing the original optimizer and hyper-parameters (such as epochs and learning rate), our method can achieve 1.42× wall-time acceleration for training ResNet-101, 1.34× wall-time acceleration for training DeiT-base, on ImageNet-1k dataset with negligible top-1 accuracy gap, compared with normal training baseline. Moreover, experiments show that our acceleration framework can generalize to downstream tasks such as semantic segmentation. 2.