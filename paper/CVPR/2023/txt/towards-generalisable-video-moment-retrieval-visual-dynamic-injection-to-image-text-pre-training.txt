Abstract
The correlation between the vision and text is essen-tial for video moment retrieval (VMR), however, existing methods heavily rely on separate pre-training feature ex-tractors for visual and textual understanding. Without suf-ficient temporal boundary annotations, it is non-trivial to learn universal video-text alignments.
In this work, we explore multi-modal correlations derived from large-scale image-text data to facilitate generalisable VMR. To address the limitations of image-text pre-training models on captur-ing the video changes, we propose a generic method, re-ferred to as Visual-Dynamic Injection (VDI), to empower the model’s understanding of video moments. Whilst ex-isting VMR methods are focusing on building temporal-aware video features, being aware of the text descriptions about the temporal changes is also critical but originally overlooked in pre-training by matching static images with sentences. Therefore, we extract visual context and spa-tial dynamic information from video frames and explicitly enforce their alignments with the phrases describing video changes (e.g. verb). By doing so, the potentially relevant visual and motion patterns in videos are encoded in the cor-responding text embeddings (injected) so to enable more accurate video-text alignments. We conduct extensive ex-periments on two VMR benchmark datasets (Charades-STA and ActivityNet-Captions) and achieve state-of-the-art per-formances. Especially, VDI yields notable advantages when being tested on the out-of-distribution splits where the test-ing samples involve novel scenes and vocabulary. 1.

Introduction
Video moment retrieval (VMR) aims at locating a video moment by its temporal boundary in a long and untrimmed video according to a natural language sentence [3, 13]. It
*Corresponding authors
Figure 1. Contemporary methods lack moment-text correlations.
Our method takes the advantage of image-text pre-trained models and learns moment-text correlations by visual-dynamic injection. is a critical task which has been extensively studied in a va-riety of real-world applications including human-computer interaction [5], and intelligent surveillance [9]. In practice, raw videos are usually unscripted and unstructured, while the words being chosen for describing the same video mo-ments can be varied from person to person [45, 63]. To be generalisable to different scenes, VMR is fundamentally challenging as it requires the comprehension of arbitrary complex visual and motion patterns in videos and an un-bounded vocabulary with their intricate relationships.
For the fine-grained retrieval objective of VMR, the pre-cise segment-wise temporal boundary labels are intuitively harder to be collected than conventional image/video-level annotations. In this case, rather than training from scratch with a limited number of temporally labelled videos, ex-isting VMR solutions [3, 13, 14, 62] heavily rely on single-modal pre-training [8, 48] for visual and textual understand-ing (Fig. 1 (a)). By doing so, they focus on modelling the correlations between the pre-learned features of videos and sentences. Nonetheless, without sufficient training data, it is non-trivial to derive universal video-text alignments so to
generalise to novel scenes and vocabulary.
Separately, the recent successes achieved by joint vision-language pre-training in zero-shot learning [21, 42] demon-strate the potential of adapting the multi-modal correlations derived from large-scale visual-textual data to facilitate gen-eralisable VMR. Whilst it is intuitive to adopt the video-text pre-learned features [34, 38, 50] for moment retrieval (Fig. 1 (b)), it has been shown that the models pre-trained with coarse-grained video-level labels can not transfer well to localisation-based tasks like VMR due to their unaware-ness of fine-grained alignments between text and frames or clips [2]. Such a misalignment problem is less likely to exist in pre-training by image-text matching. However, image-based pre-training models [21, 42] are less sensitive to the changes in videos and the words describing such dynamics in text [17]. This is inherent in matching sentences and im-ages with static content but is significant in understanding video actions and activities (Fig. 1 (c)). It is suboptimal to directly apply image-text pre-learned features on VMR.
In this work, we propose a generic method for exploit-ing large-scale image-text pre-training models to benefit generalisable VMR by the universal visual-textual corre-lations derived in pre-training, dubbed as Visual-Dynamic
Injection (VDI). The key idea is to explore the visual con-text and spatial dynamic information from videos and in-ject that into text embeddings to explicitly emphasise the phrases describing video changes (e.g. verb) in sentences (Fig. 1 (d)). Such visual and dynamic information in text is critical for locating video moments composed of arbitrary evolving events but unavailable or overlooked in image-text pre-training. Specifically, we consider it essential for
VMR models to answer two questions: “what are the ob-jects” and “how do the objects change”. The visual context information indicates the content in the frames, e.g. back-grounds (scenes), appearances of objects, poses of subjects, etc. Meanwhile, the spatial dynamic is about the location changes of different salient entities in a video, which po-tentially implies the development of their interactions. VDI is a generic formulation, which can be integrated into any existing VMR model. The only refinement is to adapt the text encoder by visual-dynamic information injection dur-ing training. Hence, no additional computation costs are introduced in inference.
Our contributions are three-folded: (1) To our best knowledge, this is the first attempt on injecting visual and dynamic information to image-text pre-training models to enable generalisable VMR. (2) We propose a novel method for VMR called Visual-Dynamic Injection (VDI). The VDI method is a generic formulation that can be integrated into existing VMR models and benefits them from the universal visual-textual alignments derived from large-scale image-text data. (3) The VDI achieves the state-of-the-art perfor-mances on two standard VMR benchmark datasets. More importantly, it yields notable performance advantages when being tested on the out-of-distribution splits where the test-ing samples involve novel scenes and vocabulary. VDI’s superior generalisation ability demonstrates its potential for adapting image-text pre-training to video understanding tasks requiring fine-grained visual-textual comprehensions. 2.