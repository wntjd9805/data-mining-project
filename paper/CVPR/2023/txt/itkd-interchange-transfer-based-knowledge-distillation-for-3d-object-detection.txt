Abstract
Point-cloud based 3D object detectors recently have achieved remarkable progress. However, most studies are limited to the development of network architectures for im-proving only their accuracy without consideration of the computational efficiency.
In this paper, we first propose an autoencoder-style framework comprising channel-wise compression and decompression via interchange transfer-based knowledge distillation. To learn the map-view feature of a teacher network, the features from teacher and student networks are independently passed through the shared au-toencoder; here, we use a compressed representation loss that binds the channel-wised compression knowledge from both student and teacher networks as a kind of regulariza-tion. The decompressed features are transferred in opposite directions to reduce the gap in the interchange reconstruc-tions. Lastly, we present an head attention loss to match the 3D object detection information drawn by the multi-head self-attention mechanism. Through extensive experiments, we verify that our method can train the lightweight model that is well-aligned with the 3D point cloud detection task and we demonstrate its superiority using the well-known public datasets; e.g., Waymo and nuScenes.1 1.

Introduction
Convolutional neural network (CNN)-based 3D object detection methods using point clouds [13] [35] [36] [43]
[49] have attracted wide attention based on their outstand-ing performance for self-driving cars. Recent CNN-based works have required more computational complexity to achieve higher precision under the various wild situation.
Some studies [23] [36] [43] have proposed methods to im-prove the speed of 3D object detection through which the non-maximum suppression (NMS) or anchor procedures are removed but the network parameters are still large. 1Our code is available at https://github.com/hyeon-jo/interchange-transfer-KD.
Figure 1. Performance comparison between teacher and stu-dent networks for a point-cloud based 3D object detection. The top example images are qualitatively compared between the results of teacher, student and our networks. Specifically, the first row im-ages are an input sample with labels and the center heatmap head of the teacher network. The second row examples are responses of teacher, student, and ours for the yellow circle on the heatmap (or the blue dash circle on the input). The bottom image quantita-tively shows the computational complexity and the corresponding accuracy of teacher, student and our networks, respectively. Best viewed in color.
Knowledge distillation (KD) is one of the parameter compression techniques, which can effectively train a com-pact student network through the guidance of a deep teacher network, as shown in the example images of Fig. 1. Starting with Hinton’s work [9], many KD studies [10] [20] [28] [44] have transferred the discriminative teacher knowledge to the student network for classification tasks. From the viewpoint of the detection task, KD should be extended to the regres-sion problem, including the object locations, which is not easy to straight-forwardly apply the classification-based KD methods to the detection task. To alleviate this problem, KD methods for object detection have been developed for mim-icking the output of the backbone network [15] (e.g., region
proposal network) or individual detection head [2] [32].
Nevertheless, these methods have only been studied for de-tecting 2D image-based objects, and there is a limit to ap-plying them to sparse 3D point cloud-based data that have not object-specific color information but only 3D position-based object structure information.
Taking a closer look at differences between 2D and 3D data, there is a large gap in that 2D object detection usually predicts 2D object locations based on inherent color infor-mation with the corresponding appearances, but 3D object detection estimates 3D object boxes from inputs consist-ing of only 3D point clouds. Moreover, the number of the point clouds constituting objects varies depending on the distances and presence of occlusions [42]. Another chal-lenge in 3D object detection for KD is that, compared to 2D object detection, 3D object detection methods [4] [6]
[43] [21] have more detection head components such as 3D boxes, and orientations. These detection heads are highly correlated with each other and represent different 3D char-acteristics. In this respect, when transferring the detection heads of the teacher network to the student network using
KD, it is required to guide the distilled knowledge under the consideration of the correlation among the multiple detec-tion head components.
In this paper, we propose a novel interchange transfer-based KD (itKD) method designed for the lightweight point-cloud based 3D object detection. The proposed itKD comprises two modules: (1) a channel-wise autoencoder based on the interchange transfer of reconstructed knowl-edge and (2) a head relation-aware self-attention on multi-ple 3D detection heads. First of all, through a channel-wise compressing and decompressing processes for KD, the in-terchange transfer-based autoencoder effectively represents the map-view features from the viewpoint of 3D representa-tion centric-knowledge. Specifically, the encoder provides an efficient representation by compressing the map-view feature in the channel direction to preserve the spatial po-sitions of the objects and the learning of the student net-work could be regularized by the distilled position infor-mation of objects in the teacher network. For transferring the interchange knowledge to the opposite networks, the decoder of the student network reconstructs the map-view feature under the guidance of the teacher network while the reconstruction of the teacher network is guided by the map-view feature of the student network. As a result, the student network can effectively learn how to represent the 3D map-view feature of the teacher. Furthermore, to refine the teacher’s object detection results as well as its repre-sentation, our proposed head relation-aware self-attention gives a chance to learn the pivotal information that should be taught to the student network for improving the 3D de-tection results by considering the inter-head relation among the multiple detection head and the intra-head relation of the individual detection head.
In this way, we implement a unified KD framework to successfully learn the 3D representation and 3D detection results of the teacher network for the lightweight 3D point cloud object detection. We also conduct extensive ablation studies for thoroughly validating our approach in Waymo and nuScenes datasets. The results reveal the outstanding potential of our approach for transferring distilled knowl-edge that can be utilized to improve the performance of 3D point cloud object detection models.
Our contributions are summarized as follows:
• For learning the 3D representation-centric knowledge from the teacher network, we propose the channel-wise autoencoder regularized in the compressed do-main and the interchange knowledge transfer method wherein the reconstructed features are guided by the opposite networks.
• For detection head-centric knowledge of the teacher, we suggest the head relation-aware self-attention which can efficiently distill the detection properties under the consideration of the inter-head relation and intra-head relation of the multiple 3D detection heads.
• Our work is the best attempt to reduce the parame-ters of point cloud-based 3D object detection using
KD. Additionally, we validate its superiority using two large datasets that reflect real-world driving con-ditions, e.g., Waymo and NuScenes. 2.