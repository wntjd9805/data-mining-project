Abstract
Despite recent achievements, existing 3D interacting hands recovery methods have shown results mainly on mo-tion capture (MoCap) environments, not on in-the-wild (ITW) ones. This is because collecting 3D interacting hands data in the wild is extremely challenging, even for the 2D data. We present InterWild, which brings MoCap and ITW samples to shared domains for robust 3D interacting hands recovery in the wild with a limited amount of ITW 2D/3D interacting hands data. 3D interacting hands recovery con-sists of two sub-problems: 1) 3D recovery of each hand and 2) 3D relative translation recovery between two hands. For the first sub-problem, we bring MoCap and ITW samples to a shared 2D scale space. Although ITW datasets provide a limited amount of 2D/3D interacting hands, they contain large-scale 2D single hand data. Motivated by this, we use a single hand image as an input for the first sub-problem regardless of whether two hands are interacting. Hence, in-teracting hands of MoCap datasets are brought to the 2D scale space of single hands of ITW datasets. For the sec-ond sub-problem, we bring MoCap and ITW samples to a shared appearance-invariant space. Unlike the first sub-problem, 2D labels of ITW datasets are not helpful for the second sub-problem due to the 3D translation’s ambiguity.
Hence, instead of relying on ITW samples, we amplify the generalizability of MoCap samples by taking only a geo-metric feature without an image as an input for the second sub-problem. As the geometric feature is invariant to ap-pearances, MoCap and ITW samples do not suffer from a huge appearance gap between the two datasets. The code is publicly available1. 1.

Introduction 3D interacting hands recovery aims to reconstruct a sin-gle person’s interacting right and left hands in the 3D space.
The recent introduction of a large-scale motion capture 1https://github.com/facebookresearch/InterWild
Figure 1. (a) Only a very small amount of 2D interacting hands are available in ITW datasets despite a relaxed threshold of intersection-over-union (IoU). We consider two hands are in-teracting if the IoU between the two hands’ boxes is bigger than 0.1. (b) The 2D-based weak supervision from ITW datasets often results in a wrong 3D relative translation (the orange arrow). (MoCap) dataset [20] motivated many 3D interacting hands recovery methods [4, 6, 14, 25, 32].
Although they have shown robust results on MoCap datasets, none of them explicitly tackled robustness on in-the-wild (ITW) datasets. Simply training networks on Mo-Cap datasets and testing them on ITW datasets results in un-stable results due to a huge domain gap between MoCap and
ITW datasets. The most representative domain gap is an ap-pearance gap. For example, images in InterHand2.6M [20] (IH2.6M) have black backgrounds and artificial illumina-tions, far from those of ITW datasets. The fundamental so-lution for this is collecting large-scale ITW data with 3D groundtruths (GTs); however, this is extremely challenging.
For example, capturing 3D data requires tens of calibrated
Figure 2. Mini-batch comparison for the first sub-problem (i.e., estimation of separate 3D meshes of left and right hands).
Figure 3. Mini-batch comparison for the second sub-problem (i.e., estimation of 3D relative translation between two hands). and synchronized cameras. Preparing such a setup at di-verse places in the wild requires a huge amount of manual effort. Furthermore, collecting even large-scale ITW 2D in-teracting hand data with manual annotation is greatly chal-lenging due to the severe occlusions and self-similarities.
Due to such challenges, there is no large-scale ITW 2D/3D interacting hand dataset.
Nevertheless,
ITW datasets provide large-scale 2D single-hand data, as shown in Fig. 1 (a). Utilizing such large-scale 2D single-hand data of ITW datasets can be an orthogonal research direction to the 2D/3D interacting hands data collection in the wild. Mixed-batch training is the most dominant approach to utilize 2D data of ITW datasets for the 3D human recovery [3, 11, 12, 17, 19, 24].
During the mixed-batch training, half of the samples in a mini-batch are taken from MoCap datasets and the rest of the samples from ITW datasets. The MoCap samples are fully supervised with 3D GTs, and the ITW samples are weakly supervised with 2D GTs. The ITW samples make networks exposed to diverse appearances, which leads to successful generalization to unseen ITW images. The 2D-based weak supervision is enabled by the MANO [23] hand model, which produces a 3D hand mesh from pose and shape parameters in a differentiable way. To be specific, 3D joint coordinates, extracted from the 3D mesh, are pro-jected to the 2D space using an estimated 3D global trans-lation (i.e., 3D translation from the camera to the hand root joint) and fixed virtual camera intrinsics. Then, the pro-jected 2D joint coordinates are supervised with the 2D GTs.
In this way, the 2D GTs weakly supervise MANO parame-ters, which can make all vertices of the 3D mesh fit to the 2D GTs.
However, naively re-training networks of previous 3D interacting hands recovery methods [4, 6, 14, 25, 32] with the mixed-batch training does not result in robust results. 3D interacting hands mesh recovery consists of two sub-problems: 1) estimation of separate 3D right and left hands and 2) estimation of 3D relative translation between two hands. For the first sub-problem, previous works [4, 6, 14, 25, 32] take an image of two hands when hands are in-teracting, and an image of a single hand when hands are not interacting (Fig. 2 (a)). As ITW datasets mostly con-tain single-hand data (Fig. 1 (a)), most samples from ITW datasets contain a single hand during the mixed-batch train-ing. The problem is that the images of two hands from Mo-Cap datasets have very different 2D hand scale distribution compared to that of single-hand images from ITW datasets, as shown in Fig. 5. For example, when two hands are in-cluded in the input image, the 2D scale of each hand is much smaller than that from a cropped image of a single hand.
Unlike the first sub-problem, the second sub-problem hardly gets benefits from the 2D-based weak supervision from ITW datasets. Fig. 1 (b) shows the failure case of the 2D-based weak supervision. When the 3D global transla-tion, estimated for the 2D-based weak supervision, is wrong ( 1⃝ in the figure), the 3D relative translation is supervised to be wrong one (the orange arrow in the figure). The wrong 3D global translation also can happen to the first sub-problem; however, the critical difference is that the 3D scale of the 3D relative translation (i.e., output of the second sub-problem) is very weakly constrained, while the 3D scale of hands (i.e., output of the first sub-problem) are strongly constrained by the shape parameter of MANO [23]. For example, we can place two hands close or far freely based on how they are interacting with each other, while the size of adults’ hands is usually around 15 cm. As there is no such strong constraint to the relative translation, the relative translation can be an arbitrary value and is prone to wrong supervision (the orange arrow in the figure). Please note that estimating a 3D global translation from a single image involves a high ambiguity and can be often wrong as the camera position is not provided in the input image. In this regard, we observed that the 2D-based weak supervision for the 3D relative translation, used in IntagHand [14] (Fig. 3 (a)) deteriorates results, which is shown in the experimental section. However, without the 2D-based weak supervision of ITW datasets, the network is trained only on images of
MoCap datasets like previous works [4,6,25,32] (Fig. 3 (a)), which results in generalization failure due to the appearance gap between MoCap and ITW datasets.
We present InterWild, a framework for 3D interacting hands mesh recovery in the wild. For the first sub-problem,
InterWild takes a cropped single-hand image regardless of whether two hands are interacting or not, as shown in Fig. 2
In this way, the 2D scales of interacting hands are (b). normalized to those of a single hand. Such normaliza-tion brings all single and interacting hands to the shared 2D scale space; hence, large-scale single-hand data of ITW datasets can be much more helpful compared to the coun-terpart without the normalization.
For the second sub-problem, InterWild takes geometric features without images, as shown in Fig. 3 (b).
In par-ticular, the output of the second sub-problem (i.e., 3D rela-tive translation) is fully supervised only for MoCap samples to prevent the 2D-based weak supervision of ITW samples from deteriorating the 3D relative translation. The geomet-ric features are invariant to appearances, such as colors and illuminations, which can reduce the huge appearance gap between MoCap and ITW datasets and bring samples from two datasets to a shared appearance-invariant space. There-fore, although the estimated 3D relative translation is super-vised only on MoCap datasets and is not supervised on ITW datasets, our InterWild produces robust 3D relative transla-tions on ITW datasets.
We show that our InterWild produces highly robust 3D interacting hand meshes from ITW images. As 3D interact-ing hands recovery in the wild is barely studied, we hope that ours can give useful insight into future works. For the continual study, we released our codes and trained models.
Our contributions can be summarized as follows.
• We present InterWild, a framework for the 3D interact-ing hands recovery in the wild.
• For the separate left and right 3D hands,
Inter-Wild takes a cropped single-hand image regardless of whether hands are interacting or not so that all hands are brought to a shared 2D scale space.
• For the 3D relative translation between two hands, In-terWild takes only geometric features, which are in-variant to appearances. 2.