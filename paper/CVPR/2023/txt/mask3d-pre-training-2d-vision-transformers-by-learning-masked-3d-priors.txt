Abstract
Current popular backbones in computer vision, such as
Vision Transformers (ViT) and ResNets are trained to per-ceive the world from 2D images. However, to more effec-tively understand 3D structural priors in 2D backbones, we propose Mask3D to leverage existing large-scale RGB-D data in a self-supervised pre-training to embed these 3D priors into 2D learned feature representations.
In con-trast to traditional 3D contrastive learning paradigms re-quiring 3D reconstructions or multi-view correspondences, our approach is simple: we formulate a pre-text reconstruc-tion task by masking RGB and depth patches in individual
RGB-D frames. We demonstrate the Mask3D is particu-larly effective in embedding 3D priors into the powerful 2D ViT backbone, enabling improved representation learn-ing for various scene understanding tasks, such as semantic segmentation, instance segmentation and object detection.
Experiments show that Mask3D notably outperforms exist-ing self-supervised 3D pre-training approaches on ScanNet,
NYUv2, and Cityscapes image understanding tasks, with an improvement of +6.5% mIoU against the state-of-the-art
Pri3D on ScanNet image semantic segmentation. 1.

Introduction
Recent years have seen remarkable advances in 2D im-age understanding as well as 3D scene understanding, al-though their representation learning has generally been
Powerful 2D architectures such as treated separately.
ResNets [21] and Vision Transformers (ViT) [14] have achieved notable success in various 2D recognition and seg-mentation tasks, but focus on learning from 2D image data.
Current large-scale RGB-D datasets [1,4,10,34,35] provide an opportunity to learn key geometric and structural priors to provide more informed reasoning about the scale and cir-cumvent view-dependent effects, which can provide more efficient representation learning. In 3D, various successful methods have been leveraging the RGB-D datasets for con-strastive point discrimination [6, 24, 39, 44] for downstream 3D tasks, including high-level scene understanding tasks as well as low-level point matching tasks [15, 42, 43]. How-ever, the other direction from 3D to 2D is less explored.
We thus aim to embed such 3D priors into 2D back-bones to effectively learn the structural and geometric pri-ors underlying the 3D scenes captured in 2D image pro-jections. Recently, Pri3D [25] adopted similar multi-view and reconstruction-based constraints to induce 3D priors in learned 2D representations. However, this relies on not only acquiring RGB-D frame data but also the robust registration of multiple views to obtain camera pose information for each frame. Instead, we consider how to effectively learn such geometric priors from only single-view RGB-D data in a more broadly applicable setting for 3D-based pre-training.
We thus propose Mask3D, which learns effective 3D pri-ors for 2D backbones in a self-supervised fashion by pre-training with single-view RGB-D frame data. We propose a pre-text reconstruction task to reconstruct the depth map by masking different random RGB and depth patches of an in-put frame. These masked input RGB and depth are encoded simultaneously in separate encoding branches and decoded to reconstruct the dense depth map. This imbues 3D priors into the RGB backbone which can then be used for fine-tuning downstream image based scene understanding tasks.
In particular, our self-supervised approach to embedding 3D priors from single-view RGB-D data to 2D learned fea-tures is not only more generally applicable, but we also demonstrate that it is particularly effective for pre-training vision transformers. Our experiments demonstrate the ef-fectiveness of Mask3D on a variety of datasets and image understanding tasks. We pre-train on ScanNet [10] with our masked 3D pre-training paradigm and fine-tune for 2D semantic segmentation, instance segmentation, and object detection. This enables notable improvements not only on
ScanNet data but also generalizes to NYUv2 [34] and even
Cityscapes [8] data. We believe that Mask3D makes an im-portant step to shed light on the paradigm of incorporating 3D representation learning to powerful 2D backbones.
In summary, our contributions are:
• We introduce a self-supervised pre-training approach to learn masked 3D priors for 2D image understanding tasks based on learning from only single-view RGB-D data, without requiring any camera pose or 3D recon-struction information, and thus enabling more general applicability.
• We demonstrate that our masked depth reconstruction pre-training is particularly effective for the modern, powerful ViT architecture, across a variety of datasets and image understanding tasks. 2.