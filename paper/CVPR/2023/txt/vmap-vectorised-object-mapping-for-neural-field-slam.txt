Abstract
We present vMAP, an object-level dense SLAM system using neural field representations. Each object is repre-sented by a small MLP, enabling efficient, watertight object modelling without the need for 3D priors.
As an RGB-D camera browses a scene with no prior in-formation, vMAP detects object instances on-the-fly, and dynamically adds them to its map. Specifically, thanks to the power of vectorised training, vMAP can optimise as many as 50 individual objects in a single scene, with an extremely efficient training speed of 5Hz map update. We experimentally demonstrate significantly improved scene-level and object-level reconstruction quality compared to prior neural field SLAM systems. Project page: https:
//kxhit.github.io/vMAP. 1.

Introduction
For robotics and other interactive vision applications, an object-level model is arguably semantically optimal, with scene entities represented in a separated, composable way, but also efficiently focusing resources on what is important in an environment.
The key question in building an object-level mapping system is what level of prior information is known about the objects in a scene in order to segment, classify and re-construct them. If no 3D object priors are available, then usually only the directly observed parts of objects can be reconstructed, leading to holes and missing parts [4, 46].
Prior object information such as CAD models or category-level shape space models enable full object shape estima-tion from partial views, but only for the subset of objects in a scene for which these models are available.
In this paper, we present a new approach which applies to the case where no 3D priors are available but still often en-ables watertight object reconstruction in realistic real-time scene scanning. Our system, vMAP, builds on the attractive properties shown by neural fields as a real-time scene repre-sentation [31], with efficient and complete representation of shape, but now reconstructs a separate tiny MLP model of each object. The key technical contribution of our work is to show that a large number of separate MLP object models can be simultaneously and efficiently optimised on a single
GPU during live operation via vectorised training.
We show that we can achieve much more accurate and complete scene reconstruction by separately modelling ob-jects, compared with using a similar number of weights in a single neural field model of the whole scene. Our real-time system is highly efficient in terms of both computation and memory, and we show that scenes with up to 50 objects can be mapped with 40KB per object of learned parameters across the multiple, independent object networks.
We also demonstrate the flexibility of our disentangled object representation to enable recomposition of scenes with new object configurations. Extensive experiments have been conducted on both simulated and real-world datasets, showing state-of-the-art scene-level and object-level recon-struction performance. 2.