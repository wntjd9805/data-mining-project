Abstract
Implicit neural representation (INR) can describe the target scenes with high fidelity using a small number of param-eters, and is emerging as a promising data compression technique. However, limited spectrum coverage is intrinsic to INR, and it is non-trivial to remove redundancy in diverse complex data effectively. Preliminary studies can only exploit either global or local correlation in the target data and thus of limited performance. In this paper, we propose a Tree-structured Implicit Neural Compression (TINC) to conduct compact representation for local regions and extract the shared features of these local representations in a hierarchical manner. Specifically, we use Multi-Layer
Perceptrons (MLPs) to fit the partitioned local regions, and these MLPs are organized in tree structure to share pa-rameters according to the spatial distance. The parameter sharing scheme not only ensures the continuity between adjacent regions, but also jointly removes the local and non-local redundancy. Extensive experiments show that
TINC improves the compression fidelity of INR, and has shown impressive compression capabilities over commer-cial tools and other deep learning based methods. Besides, the approach is of high flexibility and can be tailored for different data and parameter settings. The source code can be found at https://github.com/RichealYoung/TINC. 1.

Introduction
In the big data era, there exist massive and continuously growing amount of visual data from different fields, from surveillance, entertainment, to biology and medical diagno-sis. The urgent need for efficient data storage, sharing and transmission all requires effective compression techniques.
Although image and video compression have been studied for decades and there are a number of widely used commer-cial tools, the compression of medical and biological data
This work was done in collaboration with Tingxiong Xiao and Yux-iao Cheng, under supervision of Prof. Jinli Suo and Prof. Qionghai Dai affiliated with Department of Automation, Tsinghua Univ. are not readily available.
Implicit neural representation (INR) is becoming widely-used for scene rendering [19, 21, 23, 31, 44], shape estimation [8, 11, 25, 29], and dynamics modeling [15, 26, 28]. Due to its powerful representation capability, INR can describe nature scenes at high fidelity with a much smaller number of parameters than raw discrete grid representation, and thus serves as a promising data compression technique.
In spite of the big potential, INR based compression is quite limited confronted with large sized data [20], since
INR is intrinsically of limited spectrum coverage and can-not envelop the spectrum of the target data, as analyzed in [41]. Two pioneering works using INR for data com-pression, including NeRV [7] and SCI [41], have attempted to handle this issue in their respective ways. Specifically,
NeRV introduces the convolution operation into INR, which can reduce the required number of parameters using the weight sharing mechanism. However, convolution is spa-tially invariant and thus limits NeRV’s representation accu-racy on complex data with spatial varying feature distribu-tion. Differently, to obtain high fidelity on complex data,
SCI adopts divide-and-conquer strategy and partitions the data into blocks within INR’s concentrated spectrum en-velop. This improves the local fidelity, but cannot remove non-local redundancies for higher compression ratio and tend to cause blocking artifacts.
To compress large and complex data with INR, in this pa-per we propose to build a tree-structured Multi-Layer Per-ceptrons (MLPs), which consists of a set of INRs to rep-resent local regions in a compact manner and organizes them under a hierarchical architecture for parameter sharing and higher compression ratio. Specifically, we first draw on the idea of ensemble learning [12] and use a divide-and-conquer strategy [20, 41] to compress different regions with multiple MLPs separately. Then we incorporate these
MLPs with a tree structure to extract their shared param-eters hierarchically, following the rule that spatially closer regions are of higher similarity and share more parameters.
Such a joint representation strategy can remove the redun-dancy both within each local region and among non-local
regions, and also ensures the continuity between adjacent regions. The scheme of the proposed network is illustrated in Fig. 1, and we name our approach TINC (Tree-structured
Implicit Neural Compression).
Using the massive and diverse biomedical data, we con-duct extensive experiments to validate that TINC greatly improves the capability of INR and even outperforms the commercial compression tools (H.264 and HEVC) under high compression ratios. The proposed TINC is also a gen-eral framework, which can be flexibly adapted to diverse data and varying settings. 2.