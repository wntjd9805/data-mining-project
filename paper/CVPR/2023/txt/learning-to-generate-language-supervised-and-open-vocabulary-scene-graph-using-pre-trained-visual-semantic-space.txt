Abstract
Scene graph generation (SGG) aims to abstract an im-age into a graph structure, by representing objects as graph nodes and their relations as labeled edges. However, two knotty obstacles limit the practicability of current SGG methods in real-world scenarios: 1) training SGG mod-els requires time-consuming ground-truth annotations, and 2) the closed-set object categories make the SGG mod-els limited in their ability to recognize novel objects out-side of training corpora. To address these issues, we nov-elly exploit a powerful pre-trained visual-semantic space (VSS) to trigger language-supervised and open-vocabulary
SGG in a simple yet effective manner. Specifically, cheap scene graph supervision data can be easily obtained by parsing image language descriptions into semantic graphs.
Next, the noun phrases on such semantic graphs are di-rectly grounded over image regions through region-word alignment in the pre-trained VSS. In this way, we enable open-vocabulary object detection by performing object cat-egory name grounding with a text prompt in this VSS. On the basis of visually-grounded objects, the relation repre-sentations are naturally built for relation recognition, pur-suing open-vocabulary SGG. We validate our proposed ap-proach with extensive experiments on the Visual Genome benchmark across various SGG scenarios (i.e., supervised
/ language-supervised, closed-set / open-vocabulary). Con-sistent superior performances are achieved compared with existing methods, demonstrating the potential of exploiting pre-trained VSS for SGG in more practical scenarios. 1.

Introduction
Scene graph [10] is a structured representation for de-scribing image semantics.
It abstracts visual objects as graph nodes and represents their relations as labeled graph edges. The task of scene graph generation (SGG) [6, 14,
*Corresponding author
Figure 1. An illustration of exploiting a pre-trained visual-semantic space (VSS) to trigger language-supervised and open-vocabulary scene graph generation (SGG). (a) We acquire weak scene graph supervision by semantically parsing the image lan-guage description and grounding noun phrases on image re-gions via VSS. (b) At SGG inference time, thanks to the open-vocabulary generalization naturally rooted in VSS, the novel ob-ject name (e.g., player) in the text prompt input can be well aligned to one image region, which is regarded as its detection. 20, 26, 40, 47, 48, 50, 51, 57, 60, 63, 64] plays an important role for fine-grained visual understanding, which has shown promising results in facilitating various downstream appli-cations, such as image-text retrieval [24, 38, 49], image cap-tioning [2, 22, 32, 35, 52, 54, 55, 66], cross-media knowledge graph construction [18, 45] and robot planning [1].
Though great effort has been made, SGG of the current stage still faces two knotty obstacles that limit its practi-cability in real-world scenarios. 1) Training SGG mod-els requires massive ground-truth scene graphs that are ex-pensive for manual annotation. Annotators have to draw bounding boxes for all objects in an image and connect possible interacted object pairs, and assign object/relation labels. Since assigned labels might be ambiguous, further verification and canonicalization processing are usually re-quired [14]. Finally, a scene graph in the form of a set of
⟨subject, predicate, object⟩ triplets with subject and ob-ject bounding boxes is constructed. Such annotating pro-cess is time-consuming and tedious, costing much human labor and patience. 2) Almost all existing SGG methods
[20,21,26,47,48,50,51,60] involve a pre-defined closed set of object categories, making them limited in recognizing novel objects outside of training corpora. However, real-world scenes contain a boarder set of visual concepts than any pre-defined category pool. It is very likely to encounter unseen/novel categories. When this happens, current SGG models either classify novel objects to a known category or fail to detect them like background regions. Accordingly, the prediction of their interactions/relations with other ob-jects is negatively affected or just neglected. This may lead to problems. For example, a real-world robot may take inap-propriate actions using such closed-set SGG models [1,42].
Recently, there is a trend of leveraging free-form lan-guage supervision for benefiting visual recognition tasks via large-scale language-image pre-training [7, 15, 17, 36, 53, 59, 67]. These methods (e.g., CLIP [36]) perform pre-training on massive easily-obtained image-text pairs to learn a visual-semantic space (VSS), and have demon-strated great zero-shot transferability. Especially, the re-cent grounded language-image pre-training (GLIP) [17] has learned an object-level and semantic-rich VSS. Based on the learned VSS, it has established new state-of-the-art per-formances in phrase grounding and zero-shot object de-tection. This indicates such pre-trained VSS has power-ful multi-modal alignment ability (i.e., image regions and text phrases that have similar semantics get close embed-dings) and open-vocabulary generalization ability (i.e., cov-ering virtually any concepts in the pre-training image-text corpus). This inspires our thought of addressing the afore-mentioned obstacles in SGG using the pre-trained VSS. On the one hand, taking advantage of its multi-modal align-ment ability, we can cheaply acquire scene graph supervi-sion from an image description (e.g., retrieving image re-gions aligned with noun phrases and re-arranging the de-scription into a scene-graph-like form). On the other hand, by leveraging its open-vocabulary generalization ability, it is promising to enable novel category prediction in SGG.
In this work, we investigate the opportunity of fully ex-ploiting the VSS learned by language-image pre-training to trigger language-supervised and open-vocabulary SGG.
Specifically, we obtain weak scene graph supervision by se-mantically parsing an image language description into a se-mantic graph, then grounding its noun phrases over image regions through region-word alignment in the pre-trained
VSS (Figure 1 (a)). Moreover, we propose a novel SGG model, namely Visual-Semantic Space for Scene graph gen-eration (VS3). It takes a raw image and a text prompt con-taining object category names as inputs, and projects them into the shared VSS as embeddings. Next, VS3 performs object detection by aligning the embeddings of category names and image regions. Based on high-confidence de-tected objects, VS3 builds relation representations for object pairs with a devised relation embedding module that fully mines relation patterns from visual and spatial perspec-tives. Finally, a relation prediction module takes relation representations to infer relation labels. The predicted scene graph is composed by combining object detections and in-ferred relation labels. During training, visually-grounded semantic graphs parsed from image descriptions could be used as weak scene graph supervision, achieving language-supervised SGG. At SGG inference time, when using a text prompt input containing novel categories, VS3 manages to detect novel objects thanks to the open-vocabulary general-ization ability natually rooted in VSS, hence allowing for open-vocabulary SGG (Figure 1 (b)).
In summary, we have made the following contributions: (1) the exploitation of a pre-trained VSS provides an el-egant solution for addressing obstacles to triggering both language-supervised and open-vocabulary SGG, making a solid step toward real-world usage of SGG. (2) The pro-posed VS3 model is a new and versatile framework, which effectively transfers language-image pre-training knowl-edge for benefiting SGG. (3) We fully validate the effective-ness of our approach through extensive experiments on the
Visual Genome benchmark, and have set new state-of-the-art performances spanning across all settings (i.e., super-vised / language-supervised, closed-set / open-vocabulary). 2.