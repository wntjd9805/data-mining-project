Abstract
Attribution methods, which employ heatmaps to iden-tify the most inﬂuential regions of an image that impact model decisions, have gained widespread popularity as a type of explainability method. However, recent research has exposed the limited practical value of these methods, at-tributed in part to their narrow focus on the most prominent regions of an image – revealing "where" the model looks, but failing to elucidate "what" the model sees in those areas.
In this work, we try to ﬁll in this gap with CRAFT – a novel approach to identify both “what” and “where” by generat-ing concept-based explanations. We introduce 3 new ingre-dients to the automatic concept extraction literature: (i) a recursive strategy to detect and decompose concepts across layers, (ii) a novel method for a more faithful estimation of concept importance using Sobol indices, and (iii) the use of implicit differentiation to unlock Concept Attribution Maps.
We conduct both human and computer vision experi-ments to demonstrate the beneﬁts of the proposed approach.
We show that the proposed concept importance estimation technique is more faithful to the model than previous meth-ods. When evaluating the usefulness of the method for hu-man experimenters on a human-centered utility benchmark, we ﬁnd that our approach signiﬁcantly improves on two of the three test scenarios. 1.

Introduction
Interpreting the decisions of modern machine learning models such as neural networks remains a major challenge.
Given the ever-increasing range of machine learning appli-cations, the need for robust and reliable explainability meth-ods continues to grow [11, 35]. Recently enacted Euro-pean laws (including the General Data Protection Regula-tion (GDPR) [37] and the European AI act [43]) require the assessment of explainable decisions, especially those made by algorithms.
In order to try to meet this growing need, an array of explainability methods have already been proposed [14, 50, 1
Figure 2. CRAFT results for the prediction “chain saw”. First, our method uses Non-Negative Matrix Factorization (NMF) to extract the most relevant concepts used by the network (ResNet50V2) from the train set (ILSVRC2012 [10]). The global inﬂuence of these concepts on the predictions is then measured using Sobol indices (right panel). Finally, the method provides local explanations through concept attribution maps (heatmaps associated with a concept, and computed using grad-CAM by backpropagating through the NMF concept values with implicit differentiation). Besides, concepts can be interpreted by looking at crops that maximize the NMF coefﬁcients. For the class “chain saw”, the detected concepts seem to be: • the chainsaw engine, • the saw blade, • the human head, • the vegetation, • the jeans and • the tree trunk. 59, 61, 62, 66, 69, 71, 76]. One of the main classes of meth-ods called attribution methods yields heatmaps that indicate the importance of individual pixels for driving a model’s decision. However, these methods exhibit critical limita-tions [1, 27, 63, 65], as they have been shown to fail – or only marginally help – in recent human-centered bench-marks [8, 26, 41, 49, 60, 64]. It has been suggested that their limitations stem from the fact that they are only capable of explaining where in an image are the pixels that are criti-cal to the decision but they cannot tell what visual features are actually driving decisions at these locations. In other words, they show where the model looks but not what it sees. For example, in the scenario depicted in Fig. 1, where an ImageNet-trained ResNet mistakenly identiﬁes an image as containing a shovel, the attribution map displayed on the left fails to explain the reasoning behind this misclassiﬁca-tion.
A recent approach has sought to move past attribution methods [40] by using so-called “concepts” to communi-cate information to users on how a model works. The goal is to ﬁnd human-interpretable concepts in the activation space of a neural network. Although the approach exhibited po-tential, its practicality is signiﬁcantly restricted due to the need for prior knowledge of pertinent concepts in its orig-inal formulation and, more critically, the requirement for a labeled dataset of such concepts. Several lines of work have focused on trying to automate the concept discovery process based only on the training dataset and without ex-plicit human supervision. The most prominent of these tech-niques, ACE [24], uses a combination of segmentation and clustering techniques but requires heuristics to remove out-liers. However, ACE provides a proof of concept that it might be possible to discover concepts automatically and at scale – without additional labeling or human supervision.
Nevertheless, the approach suffers several limitations: by construction, each image segment can only belong to a sin-gle cluster, a layer has to be selected by the user to be used to retrieve the relevant concepts, and the amount of infor-mation lost during the outlier rejection phase can be a cause of concern. More recently, Zhang et al. [77] proposes to leverage matrix decompositions on internal feature maps to discover concepts.
Here, we try to ﬁll these gaps with a novel method called CRAFT which uses Non-Negative Matrix Factoriza-tion (NMF) [46] for concept discovery. In contrast to other concept-based explanation methods, our approach provides an explicit link between their global and local explanations (Fig. 2) and identiﬁes the relevant layer(s) to use to repre-sent individual concepts (Fig. 3). Our main contributions can be described as follows: (i) A novel approach for the automated extraction of high-level concepts learned by deep neural networks. We validate its practical utility to users with human psy-chophysics experiments. (ii) A recursive procedure to automatically identify con-cepts and sub-concepts at the right level of granularity – starting with our decomposition at the top of the model and working our way upstream. We validate the beneﬁt of this approach with human psychophysics experiments showing that (i) the decomposition of a concept yields more coherent sub-concepts and (ii) that the groups of points formed by these sub-concepts are more reﬁned and appear meaningful to humans. (iii) A novel technique to quantify the importance of in-dividual concepts for a model’s prediction using Sobol in-dices [34, 67, 68] – a technique borrowed from Sensitivity
Analysis. (iv) The ﬁrst concept-based explainability method which produces concept attribution maps by backpropagating con-2
cept scores into the pixel space by leveraging the implicit function theorem in order to localize the pixels associated with the concept of a given input image. This effectively opens up the toolbox of both white-box [15, 59, 62, 66, 69, 71,78] and black-box [14,47,55,57] explainability methods to derive concept-wise attribution maps. 2.