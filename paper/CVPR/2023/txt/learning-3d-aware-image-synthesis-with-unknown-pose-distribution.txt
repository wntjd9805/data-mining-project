Abstract
Existing methods for 3D-aware image synthesis largely depend on the 3D pose distribution pre-estimated on the training set. An inaccurate estimation may mislead the model into learning faulty geometry. This work proposes
PoF3D that frees generative radiance fields from the re-quirements of 3D pose priors. We first equip the generator with an efficient pose learner, which is able to infer a pose from a latent code, to approximate the underlying true pose distribution automatically. We then assign the discriminator a task to learn pose distribution under the supervision of the generator and to differentiate real and synthesized
The images with the predicted pose as the condition. pose-free generator and the pose-aware discriminator are jointly trained in an adversarial manner. Extensive results on a couple of datasets confirm that the performance of our approach, regarding both image quality and geometry quality,
To our best knowledge, PoF3D demonstrates the feasibility of learning high-quality 3D-aware image synthesis without using 3D pose priors for the first time. Project page can be found here. is on par with state of the art. 1.

Introduction 3D-aware image generation has recently received grow-ing attention due to its potential applications [3, 4, 9, 21, 30, 34, 47]. Compared with 2D synthesis, 3D-aware image synthesis requires the understanding of the geometry underlying 2D images, which is commonly achieved by incorporating 3D representations, such as neural radiance fields (NeRF) [2, 16, 17, 24, 25, 50], into generative models like generative adversarial networks (GANs) [8]. Such a formulation allows explicit camera control over the synthe-sized results, which fits better with our 3D world.
To enable 3D-aware image synthesis from 2D image collections, existing attempts usually rely on adequate camera pose priors [3, 4, 9, 20, 21, 30, 47] for training.
† indicates equal contribution.
* This work was done during an internship at Ant Group.
Figure 1. Sensitivity to pose priors in existing 3D-aware image synthesis approaches. π-GAN [4] works well given an adequate pose distribution in (a), but fails to learn decent geometries given a slightly changed distribution in (b). CAMPARI [20] delivers reasonable results relying on a good initial pose distribution in (c), but suffers from a wrongly estimated initialization in (d).
The priors are either estimated by conducting multiple pre-experiments [4, 9, 47], or obtained by borrowing external pose predictors or annotations [3]. Besides using a fixed distribution for pose sampling, some studies also propose to tune the pose priors together with the learning of image gen-eration [20], but they are still dependent on the initial pose distribution. Consequently, although previous methods can produce satisfying images and geometry, their performance is highly sensitive to the given pose prior. For example, on the Cats dataset [51], π-GAN [4] works well with a uniform pose distribution [-0.5, 0.5] (Fig. 1a), but fails to generate a decent underlying geometry when changing the distribution to [-0.3, 0.3] (Fig. 1b). Similarly, on the
CelebA dataset [15], CAMPARI [20] learns 3D rotation when using Gaussian distribution N (0, 0.24) as the initial prior (Fig. 1c), but loses the canonical space (e.g., the middle image of Fig. 1d should be under the frontal view) when changing the initialization to N (0, 0.12) (Fig. 1d).
Such a reliance on pose priors causes unexpected instability of 3D-aware image synthesis, which may burden this task with heavy experimental cost.
In this work, we present a new approach for 3D-aware image synthesis, which removes the requirements for pose priors. Typically, a latent code is bound to the 3D content alone, where the camera pose is independently sampled from a manually designed distribution. Our method, how-ever, maps a latent code to an image, which is implicitly composed of a 3D representation (i.e., neural radiance field) and a camera pose that can render that image. In this way, the camera pose is directly inferred from the latent code and jointly learned with the content, simplifying the input requirement. To facilitate the pose-free generator better capturing the underlying pose distribution, we re-design the discriminator to make it pose-aware. Concretely, we tailor the discriminator with a pose branch which is required to predict a camera pose from a given image. Then, the estimated pose is further treated as the conditional pseudo label when performing real/fake discrimination. The pose branch in the discriminator learns from the synthesized data and its corresponding pose that is encoded in the latent code, and in turn use the discrimination score to update the pose branch in the generator. With such a loop-back optimization process, two pose branches can align the fake data with the distribution of the dataset.
We evaluate our pose-free method, which we call
PoF3D for short, on various datasets, including FFHQ [12],
Cats [51], and Shapenet Cars [5]. Both qualitative and quantitative results demonstrate that PoF3D frees 3D-aware image synthesis from hyper-parameter tuning on pose distributions and dataset labeling, and achieves on par performance with state-of-the-art in terms of image quality and geometry quality. 2.