Abstract
Real-time and robust photorealistic avatars for telepres-ence in AR/VR have been highly desired for enabling im-mersive photorealistic telepresence. However, there still exists one key bottleneck: the considerable computational expense needed to accurately infer facial expressions cap-tured from headset-mounted cameras with a quality level that can match the realism of the avatar’s human appearance.
To this end, we propose a framework called Auto-CARD, which for the first time enables real-time and robust driving of Codec Avatars when exclusively using merely on-device computing resources. This is achieved by minimizing two sources of redundancy. First, we develop a dedicated neural architecture search technique called AVE-NAS for avatar encoding in AR/VR, which explicitly boosts both the searched architectures’ robustness in the presence of extreme facial ex-pressions and hardware friendliness on fast evolving AR/VR headsets. Second, we leverage the temporal redundancy in consecutively captured images during continuous rendering and develop a mechanism dubbed LATEX to skip the com-putation of redundant frames. Specifically, we first identify an opportunity from the linearity of the latent space derived by the avatar decoder and then propose to perform adaptive latent extrapolation for redundant frames. For evaluation, we demonstrate the efficacy of our Auto-CARD framework in real-time Codec Avatar driving settings, where we achieve a 5.05× speed-up on Meta Quest 2 while maintaining a compa-rable or even better animation quality than state-of-the-art avatar encoder designs. 1.

Introduction
Enabling immersive real-time experiences has been the key factor in driving the advances of Augmented- and
Virtual-Reality (AR/VR) platforms in recent years. Pho-torealistic telepresence [29, 31, 37, 46] is emerging as a tech-nology for enabling remote interactions in AR/VR that aims
*Work done during an internship at Meta. to impart a compelling sense of co-location among partici-pants in a shared virtual space. One state-of-the-art (SOTA) approach, coined Codec Avatars [29], is comprised of two components: (1) an encoder, which estimates a participant’s behavior from sensors mounted on an AR/VR headset, and (2) a decoder, which re-renders the aforementioned behavior to the other parties’ headset display using an avatar rep-resentation. Both the SOTA encoder and decoder designs have leveraged the expressive power of deep neural networks (DNNs) to enable the precise estimation of human behaviors as well as the high fidelity of rendering, which are critical for immersive photorealistic telepresence.
Despite its big promise, one of the main challenges posed by photorealistic telepresence is the competing requirements between ergonomics and computing resources. On the one hand, power, form factor, and other comfort factors strictly limit the available computing resources on an AR/VR head-set device. On the other hand, the DNNs used in SOTA
Codec Avatars are computationally expensive and require continuous execution during a telepresence call. It is worth noting that the limited computing resource on an AR/VR device must additionally be shared with other core AR/VR workloads, such as the SLAM-based tracking service, con-troller tracking, hand tracking, and environment rendering.
Therefore, it is highly desirable and imperative to minimize the computation overhead and resource utilization of Codec
Avatars, while not hurting their precise estimation of hu-man behaviors and rendering fidelity. This has become a bottleneck limiting their practical and broad adoption.
To close the above gap towards real-time Codec Avatars on AR/VR devices, existing work has focused on reduc-ing the computational cost of the decoder. For example,
PiCA [31] leverages the compute characteristics of modern
DSP processors to enable simultaneously rendering up to five avatars on a Meta Quest 2 headset [31]. On the other hand, efficient encoder designs that can fit the AR/VR com-puting envelope have been less explored, with most existing works assuming off-device computing scenarios. Specifi-cally, SOTA methods for the encoder such as [37, 46] are prohibitively heavy with ∼3 Giga-floating-point-operations
(GFLOPs) for encoding merely from one image, which is too costly to be continuously executed on SOTA AR/VR head-sets. Although cloud-based solutions have been explored as an alternative for other AR/VR use cases, on-device en-coder processing for Codec Avatars is particularly desired for telepresence applications as a way to better protect the privacy and overcome internet bandwidth limitations.
In this work, we aim to enable real-time encoder infer-ence for Codec Avatars on AR/VR devices. Specifically, the encoder takes in image data captured from headset-mounted cameras (HMC) and outputs facial expression codes for a
Variational Auto-Encoder (VAE) [22], which is used as a decoder following prior works [29, 38]. This target prob-lem is particularly challenging due to two reasons. First, naively reducing the encoder capacity, e.g., by compress-ing the encoder models to have fewer channels and/or shal-lower layers, typically results in accuracy degradation, es-pecially for extreme expressions at the tail ends of the data distribution which are often precisely the expressions that contain the most informative social signal. Second, since hardware backends are still nascent for AR/VR use cases, heuristics for hardware-specific optimization may quickly be-come obsolete. For example, the Qualcomm Snapdragon 865 system-on-a-chip (SoC) [1] on Meta Quest 2 headsets and customized accelerators [40] exhibit different latency/energy constraints. As such, it is important for our developed tech-niques to be able to automatically adapt to different hardware backends for ensuring their practical and wide adoption, instead of relying on manual optimization strategies that require costly laboring efforts.
To tackle the aforementioned challenges, we develop a framework, dubbed Auto-CARD, for enabling efficient and robust real-time Codec Avatar driving. Auto-CARD auto-matically minimizes two sources of redundancy in the encod-ing process of SOTA solutions: architectural and temporal redundancy. We summarize our contributions as follows:
• Our proposed framework, Auto-CARD, is the first method that has enabled real-time and robust driving of Codec Avatars in AR/VR, exclusively using merely on-device computing resources.
• Auto-CARD integrates a neural architecture search technique that is tailored for avatar encoding (AVE-NAS), minimizing potential model redundancy while explicitly accounting for the fast-evolving hardware de-sign trends of AR/VR headsets. AVE-NAS comprises three NAS components: (1) a view-decoupled supernet for enabling distributed near-sensor encoding, (2) a hy-brid differentiable search scheme for an efficient and effective joint search, and (3) an extreme-expression-aware search objective.
• To further reduce temporal redundancy towards real-time encoders for Codec Avatars on AR/VR de-vices, Auto-CARD additionally integrates a mechanism, dubbed LATEX, to skip the computation of redundant frames. Specifically, we first identify an opportunity from the linearity of the latent space determined by the avatar decoder and then propose to perform adaptive latent extrapolation for redundant frames.
• Extensive experiments on real-device measurements using AR/VR headsets, i.e., Meta Quest 2 [32], show that our method can achieve a 5.05× speed-up while maintaining a comparable or even better accuracy than
SOTA avatar encoder designs. 2.