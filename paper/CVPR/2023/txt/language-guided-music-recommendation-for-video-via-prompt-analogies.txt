Abstract
We propose a method to recommend music for an input video while allowing a user to guide music selection with free-form natural language. A key challenge of this prob-lem setting is that existing music video datasets provide the needed (video, music) training pairs, but lack text descrip-tions of the music. This work addresses this challenge with the following three contributions. First, we propose a text-synthesis approach that relies on an analogy-based prompt-ing procedure to generate natural language music descrip-tions from a large-scale language model (BLOOM-176B) given pre-trained music tagger outputs and a small num-ber of human text descriptions. Second, we use these syn-thesized music descriptions to train a new trimodal model, which fuses text and video input representations to query music samples. For training, we introduce a text dropout regularization mechanism which we show is critical to model performance. Our model design allows for the re-trieved music audio to agree with the two input modalities by matching visual style depicted in the video and musical genre, mood, or instrumentation described in the natural language query. Third, to evaluate our approach, we col-lect a testing dataset for our problem by annotating a subset of 4k clips from the YT8M-MusicVideo dataset with natural language music descriptions which we make publicly avail-able. We show that our approach can match or exceed the performance of prior methods on video-to-music retrieval while significantly improving retrieval accuracy when using text guidance.
Figure 1. Language-guided music retrieval. Our ViML model takes a video and text prompt as input to retrieve a suitable music track from a database. The model learns to fuse video and lan-guage representations in order to guide retrieval. Notice how our approach retrieves audio matching both the video and language content. For the same video query (top two rows), we can change the music style to match the language query, and for the same
Upbeat pop query (bottom two rows) we can change the vo-calist to match the video content. To fully appreciate our results, please view and listen to the companion video on our website. 1.

Introduction
A key part of the video editing process for creators is choosing a musical soundtrack. Especially given the rise of short-form videos on social media platforms, automated music recommendation systems have become an increas-*Work done as an intern with Adobe Research ingly common and important part of video editing applica-tions. While these systems can be helpful for finding rel-evant music, they often provide limited capability for user control over the types of music recommended.
In previ-ous work, music is retrieved based solely on the visual con-tent and style from a video [32, 37]. However, music itself can convey critical information about how a video should
be perceived. Music selection alone can transform a visual scene into one that is perceived as happy, scary, or sad1. As a result, the lack of user input capability to describe a target music for an inputted video is a key limitation on the utility of current music recommendation methods.
In this work, we propose a more flexible music-for-video recommendation approach that allows a user to guide rec-ommendations towards specific musical attributes includ-ing mood, genre, or instrumentation, illustrated in Figure 1.
To maximize flexibility and user convenience, we propose to take user musical attribute descriptions in the form of free-form natural language (e.g., “Folk music with guitar” in Figure 1). There are two key challenges in learning a model for language-guided music recommendation for video. First, while there are datasets which include mu-sic+text [5, 7, 19, 30] or music+video [1], there are no avail-able datasets which include music, video, and text together.
Further, the existing datasets that do include text and music focus on a limited vocabulary of tags rather than free-form text. Second, previous works have explored jointly learning visual, audio, and text embeddings [2–4, 34, 47], and with-out careful regularization, a network can overfit and possi-bly learn to ignore one of the input modalities. We seek to train a model that keeps the information flow through the network and does not ignore one of the modalities.
In order to meet the challenges outlined above, our work makes the following contributions: (1) We propose a new approach to automatically gener-ate natural language descriptions for a music video dataset.
This approach combines a pre-trained music tagger with a large-scale language model to output natural language de-scriptions for any music clip, illustrated in Figure 2 (left).
First, the tagger predicts tags from a pre-defined vocabu-lary describing musical genre, mood, or instrumentation.
Second, these predicted tags, together with their probabil-ities, are converted into a rich natural language description for the music video using a carefully designed large-scale language model prompting procedure based on analogies with a small number of human-provided text descriptions (i.e., A (tags) : A′ (description) :: B (tags) : B′ (description)), where A and B are music tags auto-matically provided by the tagger, A′ is a human-provided text description, and B′ is the natural language description output by the large-scale language model. (2) We propose a Transformer-based model architecture with a video-text fusion module. Our model, which we call
Video to Music with Language (ViML), is able to retrieve music that matches both the visual content/style of the in-put video and described musical genre, mood, and instru-mentation in the natural language query. Similar to prior work [16, 28, 39], we find that training with text dropout as a regularization mechanism is critical to achieve music re-1https://www.youtube.com/watch?v=iSkJFs7myn0 trieval performance improvements from added text inputs. (3) We release a dataset of 4000 high quality text anno-tations for clips from a subset of the YT8M-MusicVideo dataset [1] to evaluate language-guided music recommen-dation. We show that our method can achieve substantial improvements over prior works on music retrieval when in-corporating text inputs. Moreover, our model can match or even exceed performance of baseline music-for-video rec-ommendation models when the text input is ignored. 2.