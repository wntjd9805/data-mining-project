Abstract
For years, researchers have been devoted to general-izable object perception and manipulation, where cross-category generalizability is highly desired yet underex-plored.
In this work, we propose to learn such cross-category skills via Generalizable and Actionable Parts (GAParts). By identifying and defining 9 GAPart classes (lids, handles, etc.) in 27 object categories, we construct a large-scale part-centric interactive dataset, GAPartNet, where we provide rich, part-level annotations (semantics, poses) for 8,489 part instances on 1,166 objects. Based on
GAPartNet, we investigate three cross-category tasks: part segmentation, part pose estimation, and part-based object manipulation. Given the significant domain gaps between seen and unseen object categories, we propose a robust 3D segmentation method from the perspective of domain gen-eralization by integrating adversarial learning techniques.
Our method outperforms all existing methods by a large
*Equal contribution with the order determined by rolling dice.
†Corresponding author: hewang@pku.edu.cn. margin, no matter on seen or unseen categories. Further-more, with part segmentation and pose estimation results, we leverage the GAPart pose definition to design part-based manipulation heuristics that can generalize well to unseen object categories in both the simulator and the real world. 1.

Introduction
Generalizable object perception and manipulation are at the core of building intelligent and multi-functional robots.
Recent efforts on generalizing the vision have been devoted to category-level object perception that deals with perceiv-ing novel object instances from known object categories, including object detectors from RGB images [17, 21, 46], point clouds [5, 19], and category-level pose estimation works on rigid [4, 53] and articulated objects [27, 59]. On the front of generalizable manipulation, complex tasks that involve interacting with articulated objects have also been proposed in a category-level fashion, as in the recent chal-lenge on learning category-level manipulation skills [38].
Additionally, to boost robot perception and manipulation
with indoor objects, researchers have already proposed sev-eral datasets [37, 57, 61, 66, 68] with part segmentation and motion annotations, and have devoted work to part segmen-tation [37, 68] and articulation estimation [27].
However, these works all approach the object perception and manipulation problems in an intra-category manner, while humans can well perceive and interact with instances from unseen object categories based on prior knowledge of functional parts such as buttons, handles, lids, etc. In fact, parts from the same classes have fewer variations in their shapes and the ways that we manipulate them, compared to objects from the same categories. We thus argue that part classes are more elementary and fundamental compared to object categories, and generalizable visual perception and manipulation tasks should be conducted at part-level.
Then, what defines a part class? Although there is no sin-gle answer, we propose to identify part classes that are gen-eralizable in both recognition and manipulation. After care-ful thoughts and expert designs, we propose the concept of
Generalizable and Actionable Part (GAPart) classes. Parts from the same GAPart class share similar shapes which al-low generalizable visual recognition; parts from the same
GAPart class also have aligned actionability and can be in-teracted with in a similar way, which ensures minimal hu-man effort when designing interaction guidance to achieve generalizable and robust manipulation policies.
Along with the GAPart definition, we present GAPart-Net, a large-scale interactive part-centric dataset where we gather 1,166 articulated objects from the PartNet-Mobility dataset [61] and the AKB-48 dataset [32]. We put in great effort in identifying and annotating semantic labels to 8,489
GAPart instances. Moreover, we systematically align and annotate the GAPart poses, which we believe serve as the bridge between visual perception and manipulation. Our class-level GAPart pose definition highly couples the part poses with how we want to interact with the parts. We show that this is highly desirable – once the part poses are known, we can easily manipulate the parts using simple heuristics.
Based on the proposed dataset, we further explore three cross-category tasks based on GAParts: part segmentation, part pose estimation, and part-based object manipulation, where we aim at recognizing and interacting with the parts from novel objects in both known categories and, moreover, unseen object categories. In this work, we propose to use learning-based methods to deal with perception tasks, after which, based on the GAPart definition, we devise simple heuristics to achieve cross-category object manipulation.
However, different object categories may contain differ-ent kinds of GAParts and provide different contexts for the parts. Each object category thus forms a unique domain for perceiving and manipulating GAParts. Therefore, all three tasks demand domain-generalizable methods that can work on unseen object categories without seeing them during training, which is very challenging for existing vision and robotic algorithms. We thus consult the generalization liter-ature [12,13,25] and propose to learn domain-invariant rep-resentation, which is often achieved by domain adversarial learning with a domain classifier. During training, the clas-sifier tries to distinguish the domains while the feature ex-tractor tries to fool the classifier, which encourages domain-invariant feature learning. However, it is highly non-trivial to adopt adversarial learning in our domain-invariant fea-ture learning, due to the following challenges. 1) Handling huge variations in part contexts across different domains.
The context of a GAPart class can vary significantly across different object categories. For example, in training data, round handles usually sit on the top of lids for the Cof-feeMachine category, whereas for the test category Table, round handles often stand to the front face of the drawers.
To robustly segment GAParts in objects from unseen cate-gories, we need the part features to be context-invariant. 2)
Handling huge variations in part sizes. Parts from different
GAPart classes may be in different sizes, e.g., a button is usually much smaller than a door. Given that the input is a point cloud, the variations in part sizes will result in huge variations in the number of points across different GAParts, which makes feature learning very challenging. 3) Han-dling the imbalanced part distribution and part-object rela-tions. Object parts in the real world distribute naturally un-evenly and a particular part class may appear with different frequencies throughout various object categories. For ex-ample, there can be more buttons than doors on a washing machine while the opposite is true in the case of a storage furniture. This imbalanced distribution also adds difficulties to the learning of domain-invariant features.
Accordingly, we integrate several important techniques from domain adversarial learning. To improve context in-variance, we propose a part-oriented feature query tech-nique that mainly focuses on foreground parts and ignores the background. To handle diverse part sizes, we propose a multi-resolution technique. Finally, we employ the fo-cal loss to handle the distribution imbalance. Our method significantly outperforms previous 3D instance segmenta-tion methods and achieves 76.5% AP50 on seen object cat-egories and 37.2% AP50 on unseen categories.
To summarize, our main contributions are as follows: 1. We provide the concept of GAPart and present a large-scale interactive dataset, GAPartNet, with rich part seman-tics and pose annotations that facilitates generalizable part perception and part-based object manipulation. 2. We propose a first-ever pipeline for domain-generalizable 3D part segmentation and pose estimation via learning domain-invariant features, which significantly out-performs the baselines. 3. We provide a new solution to generalizable object ma-nipulation by leveraging the concept of GAParts. Thanks
a fast shape-based network that extracts efficient category-level pose features. [54] uses a cascaded relation network to relate 2D, 3D, shape priors, and proposes a recurrent recon-struction network to make iterative improvements.
Generalizable Object Manipulation. On the front of ob-ject manipulation, Mu et al. proposes [38] a challenge to learn generalizable manipulation skills for articulated objects from known categories. Although some previous methods [14, 15, 36] have certain generalizability, robotic manipulation in a novel environment still calls for the abil-ity to handle novel object categories. Although, for sim-ple rigid objects, there is existing literature on robust and object-agnostic object grasping [2, 9, 20] and planar push-ing [26, 69] algorithms, while very few works have been devoted to interacting with articulated objects that contain movable parts. Recently, Mo et al. [36] and Wu et al. [60] tackle this problem by leveraging low-level generalizability.
The most related work to us is Gadre et al. [11] which pro-poses an interactive perception pipeline learning to touch, watch, then segment the object into movable parts. How-ever, this work does not consider the consistent geometry and actionability patterns behind parts from the same class and can only deal with simple objects with up to three parts on the table surfaces, e.g., scissors and eyeglasses. to innate generalizability and actionability, minimal human effort is needed when designing interaction guidance to achieve generalizable and robust manipulation policies. 2.