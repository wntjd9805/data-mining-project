Abstract
Multimodal fusion can make semantic segmentation more robust. However, fusing an arbitrary number of modalities remains underexplored. To delve into this prob-lem, we create the DELIVER arbitrary-modal segmenta-tion benchmark, covering Depth, LiDAR, multiple Views,
Events, and RGB. Aside from this, we provide this dataset in four severe weather conditions as well as ﬁve sensor failure cases to exploit modal complementarity and resolve par-tial outages. To make this possible, we present the arbi-trary cross-modal segmentation model CMNEXT.
It en-compasses a Self-Query Hub (SQ-Hub) designed to extract effective information from any modality for subsequent fu-sion with the RGB representation and adds only negligible amounts of parameters (∼0.01M ) per additional modal-ity. On top, to efﬁciently and ﬂexibly harvest discrimina-tive cues from the auxiliary modalities, we introduce the simple Parallel Pooling Mixer (PPX). With extensive experi-ments on a total of six benchmarks, our CMNEXT achieves state-of-the-art performance on the DELIVER, KITTI-360,
MFNet, NYU Depth V2, UrbanLF, and MCubeS datasets, allowing to scale from 1 to 81 modalities. On the freshly collected DELIVER, the quad-modal CMNEXT reaches up to 66.30% in mIoU with a +9.10% gain as compared to the mono-modal baseline.1 1.

Introduction (a) RGB-D-E-L fusion. (b) RGB-A-D-N fusion. (c) RGB-Light Field.
Figure 1. Arbitrary-modal segmentation results of CMNeXt using: (a). {RGB, Depth, Event, LiDAR} on our DELIVER dataset; (b). {RGB, Angle of Linear Polarization (AoLP), Degree of Lin-ear Polarization (DoLP), Near-Infrared (NIR)} on MCubeS [44]; (c). {RGB, 8/33/80 sub-aperture Light Fields (LF8/LF33/LF80) on UrbanLF-Syn [59], respectively.
LiDAR
Event
Depth
RGB 
Figure 2. Comparing CMX [48], HRFuser [4], and our CMNeXt in sensor failure (i.e., LiDAR Jitter) on the DELIVER dataset.
With the explosion of modular sensors, multimodal fu-sion for semantic segmentation has progressed rapidly re-cently [5, 11, 48] and in turn has stirred growing inter-est to assemble more and more sensors to reach higher and higher segmentation accuracy aside from more robust scene understanding. However, most works [34, 75, 103] and multimodal benchmarks [29, 61, 91] focus on speciﬁc sensor pairs, which lack behind the current trend of fusing
*Equal contribution.
†Corresponding author (e-mail: kailun.yang@hnu.edu.cn). 1The DELIVER dataset and our code will be made publicly available at: https://jamycheung.github.io/DELIVER.html. more and more modalities [4, 70], i.e., progressing towards
Arbitrary-Modal Semantic Segmentation (AMSS).
When looking into AMSS, two observations become ap-parent. Firstly, an increasing amount of modalities should provide more diverse complementary information, mono-tonically increasing segmentation accuracy. This is di-rectly supported by our results when incrementally adding and fusing modalities as illustrated in Fig. 1a (RGB-Depth-Event-LiDAR), Fig. 1b (RGB-AoLP-DoLP-NIR), and Fig. 1c when adding up to 80 sub-aperture light-ﬁeld modalities (RGB-LF8/-LF33/-LF80). Unfortunately, this great potential cannot be uncovered by previous cross-R
D
E
L merge
.
.
R
D
E
L distribute
R
D
E
L hub2fuse
Hub (a) Separate (b) Joint (c) Asymmetric
Figure 3. Comparison of multimodal fusion paradigms, such as (a) merging with separate branches [4], (b) distributing with a joint branch [70], and (c) our hub2fuse with asymmetric branches. modal fusion methods [9, 77, 99], which follow designs for pre-deﬁned modality combinations. The second observa-tion is that the cooperation of multiple sensors is expected to effectively combat individual sensor failures. Most of the existing works [67, 72, 76] are built on the assump-tion that each modality is always accurate. Under par-tial sensor faults, which are common in real-life robotic systems, e.g. LiDAR Jitter, fusing misaligned sensing data might even degrade the segmentation performance, as de-picted with CMX [48] and HRFuser [4] in Fig. 2. These two critical observations remain to a large extent neglected.
To address these challenges, we create a benchmark based on the CARLA simulator [19], with Depth, LiDAR,
Views, Events, and RGB images: The DELIVER Multi-modal dataset. It features severe weather conditions and ﬁve sensor failure modes to exploit complementary modalities and resolve partial sensor outages. To proﬁt from all this, we present the arbitrary cross-modal CMNeXt segmenta-tion model. Without increasing the computation overhead substantially when adding more modalities CMNeXt incor-porates a novel Hub2Fuse paradigm (Fig. 3c). Unlike re-lying on separate branches (Fig. 3a) which tend to be com-putationally costly or using a single joint branch (Fig. 3b) which often discards valuable information, CMNeXt is an asymmetric architecture with two branches, one for RGB and another for diverse supplementary modalities.
The key challenge lies in designing the two branches to pick up multimodal cues. Speciﬁcally, at the hub step of Hub2Fuse, to gather useful complementary informa-tion from auxiliary modalities, we design a Self-Query
Hub (SQ-Hub), which dynamically selects informative fea-tures from all modality-sources before fusion with the RGB branch. Another great beneﬁt of SQ-Hub is the ease of ex-tending it to an arbitrary number of modalities, at negligible parameters increase (∼0.01M per modality). At the fusion step, fusing sparse modalities such as LiDAR or Event data can be difﬁcult to handle for joint branch architectures with-out explicit fusion such as TokenFusion [70]. To circum-vent this issue and make best use of both dense and sparse modalities, we leverage cross-fusion modules [48] and cou-ple them with our proposed Parallel Pooling Mixer (PPX) which efﬁciently and ﬂexibly harvests the most discrimina-tive cues from any auxiliary modality. These design choices come together in our CMNeXt architecture, which paves the way for AMSS (Fig. 1). By carefully putting together alter-native modalities, CMNeXt can overcome individual sensor failures and enhances segmentation robustness (Fig. 2).
With comprehensive experiments on DELIVER and
ﬁve additional public datasets, we gather insight into the strength of the CMNeXt model. On DELIVER, CMNeXt obtains 66.30% in mIoU with a +9.10% gain compared to the RGB-only baseline [78]. On UrbanLF-Real [59] and MCubeS [44] datasets, CMNeXt surpasses the previ-ous best methods by +3.90% and +8.68%, respectively.
Compared to previous state-of-the-art methods, our model achieves comparable perfomance on bi-modal NYU Depth
V2 [61] as well as MFNet [29] and outperforms all previous modality-speciﬁc methods on KITTI-360 [45].
On a glance, we deliver the following contributions:
• We create the new benchmark DELIVER for
Arbitrary-Modal Semantic Segmentation (AMSS) with four modalities, four adverse weather conditions, and ﬁve sensor failure modes.
• We revisit and compare different multimodal fusion paradigms and present the Hub2Fuse paradigm with an asymmetric architecture to attain AMSS.
• The universal arbitrary cross-modal fusion model CM-NeXt is proposed, with a Self-Query Hub (SQ-Hub) for selecting informative features and a Parallel Pool-ing Mixer (PPX) for harvesting discriminative cues.
• We investigate AMSS by fusing up to a total of 80 modalities and notice that CMNeXt achieves state-of-the-art performances on six datasets. 2.