Abstract
Existing image captioning methods are under the as-sumption that the training and testing data are from the same domain or that the data from the target domain (i.e., the domain that testing data lie in) are accessible. How-ever, this assumption is invalid in real-world applications where the data from the target domain is inaccessible. In this paper, we introduce a new setting called Domain Gen-eralization for Image Captioning (DGIC), where the data from the target domain is unseen in the learning process.
We first construct a benchmark dataset for DGIC, which helps us to investigate models’ domain generalization (DG) ability on unseen domains. With the support of the new benchmark, we further propose a new framework called language-guided semantic metric learning (LSML) for the
DGIC setting. Experiments on multiple datasets demon-strate the challenge of the task and the effectiveness of our newly proposed benchmark and LSML framework. 1.

Introduction
Image captioning (IC) builds a bridge between vision and language, and it aims at understanding images [20, 28, 36, 37, 51, 69] and generating correct natural language de-scriptions. Many novel methods [3, 13, 26, 27, 45, 55, 58] have made impressive progress under a domain-specific set-ting; namely, they assume the training and testing data are from the same domain. However, this assumption may not hold in real-world applications. To relax the reliance of dif-ferent domains, many methods [10, 19, 24] are recently pro-posed. However, these approaches also have a strong as-sumption that the data from the target domain are available.
In real-world applications, this assumption will also be in-valid. For example, in the medical report generation task, the data from the target domain related to patient privacy is hard to obtain. As a result, it is important to design an im-age captioning approach with domain generalization ability to different unseen domains.
*Zhendong Mao is the corresponding author.
Task
Test
Training data data
PivotIC Dsrc-p, Dp-tar Dsrc-tar
Dsrc, Dtar
NOIC
Dsrc, Dtar
DAIC
Dsrc
DGIC
Dtar
Dtar
Dtar
YS = YT
✓
×
×
×
Target access
✓
✓
✓
×
Table 1. Comparison of domain-related tasks for image caption-ing. YS/T : distribution of source/target label space. Dsrc/tar/p
: source/target/pivot domain. Pivot image captioning (PivotIC), novel object image captioning (NOIC), and domain adaptive im-age captioning (DAIC) are all assumed to be able to obtain the data of the target domain. Domain generalizable image captioning (DGIC) does not require the target domain data.
To this end, we propose a new benchmark setting called
Domain Generalization for Image Captioning (DGIC) with multi-source domain and cross-dataset setting in this work.
Specifically, we employ existing popular datasets from five domains: common domain sourced from MSCOCO [35], assistive domain sourced from Vizwiz [21], social domain sourced from Flickr30k [62], avian domain sourced from
CUB-200 [44, 57], and floral domain sourced from Oxford-102 [44, 57]. To explore the DGIC, we divide these do-mains into two parts: multiple source domains for training and a target domain for testing, mimicking the unseen do-main scenario and mining underlying patterns from multi-ple datasets. The difference between our DGIC setting and other image captioning settings is summarized in Tab. 1.
With the help of this benchmark, we analyze the exist-ing methods for unseen domains and observe the following limitations: (1) The model can generate fluent captions but cannot ensure semantic correctness when meeting an un-seen domain without target data (Fig. 1a). In other words, the generated captions are prone to overfit domain-specific bias and only learn the domain-specific features. We ar-gue that this is because the existing image captioning mod-els are trained with maximum likelihood estimation, which will cause the model lacks discriminative semantic infor-mation between different instances [5, 25]. So it is difficult to distinguish the relationship between unseen domain data
and learned data. Therefore, it is desirable to introduce se-mantic information in the learning process. (2) The exist-ing domain generalization (DG) method designed for other tasks [6, 7, 31, 32, 39, 56] cannot be well applied directly to image captioning because the image and label of most DG tasks are simple. So these methods only use coarse-grained information in the learning process. But for the DGIC task, the image and label are often complex and contain rich tex-tual information (Fig. 1b). Therefore, simply applying the existing DG method to the image captioning task may not work well, and it is beneficial to utilize the rich contextual information in images and labels when performing feature alignment of different domains.
To tackle the aforementioned two challenges, we pro-pose a new framework called language-guided semantic metric learning (LSML) for the DGIC task. To solve the first issue (i.e., lack of semantic information), we intro-duce both inter-domain and intra-domain metric learning to help captioning models considers the semantic relationship among different instances in the learning process. Specifi-cally, we leverage contrastive learning to pull semantically similar visual features closer and push the irrelevant visual features far away from each other, which makes features more discriminative, allowing the model to learn domain-independent features that are more easily generalized to un-seen domains. To solve the second issue (i.e., contextual information utilization), we propose a visual word guid-ance and sentence guidance strategy in the learning process.
Specifically, we use the visual word and sentence similar-ity to sample discriminative triplets, allowing the model to capture fine-grained contextual information. As a conse-quence, our LSML framework aims to achieve promising performance under the DGIC setting.
In a nutshell, our contributions are summarized below: (1) We make the first attempt to conduct the task of do-main generalization for image captioning (DGIC), which is used to explore the generalization ability of existing mod-els. To achieve this, we construct a benchmark from exist-ing datasets for this task. (2) We propose a new framework called language-guided semantic metric learning (LSML), which uses both inter- and intra-domain metric learning to help the model better learn discriminative semantic infor-mation among different instances. We also introduce a lan-guage guidance strategy in the learning process to utilize the rich contextual information in the image and labels dur-ing the learning process. (3) Extensive experiments demon-strate that our language-guided semantic metric learning framework outperforms previous state-of-the-art methods by a large margin under the DGIC setting. 2.