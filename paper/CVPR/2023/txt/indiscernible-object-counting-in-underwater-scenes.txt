Abstract
Recently, indiscernible scene understanding has at-tracted a lot of attention in the vision community. We further advance the frontier of this field by systematically studying a new challenge named indiscernible object count-ing (IOC), the goal of which is to count objects that are blended with respect to their surroundings. Due to a lack of appropriate IOC datasets, we present a large-scale dataset IOCfish5K which contains a total of 5,637 high-resolution images and 659,024 annotated center points.
Our dataset consists of a large number of indiscernible ob-jects (mainly fish) in underwater scenes, making the an-notation process all the more challenging.
IOCfish5K is superior to existing datasets with indiscernible scenes be-cause of its larger scale, higher image resolutions, more annotations, and denser scenes. All these aspects make it the most challenging dataset for IOC so far, supporting progress in this area. For benchmarking purposes, we se-lect 14 mainstream methods for object counting and care-fully evaluate them on IOCfish5K. Furthermore, we propose
IOCFormer, a new strong baseline that combines density and regression branches in a unified framework and can effectively tackle object counting under concealed scenes.
Experiments show that IOCFormer achieves state-of-the-art scores on IOCfish5K. The resources are available at github.com/GuoleiSun/Indiscernible-Object-Counting. 1.

Introduction
Object counting – to estimate the number of object in-stances in an image – has always been an essential topic in computer vision. Understanding the counts of each cate-gory in a scene can be of vital importance for an intelligent agent to navigate in its environment. The task can be the end goal or can be an auxiliary step. As to the latter, counting objects has been proven to help instance segmentation [14], action localization [54], and pedestrian detection [83]. As to the former, it is a core algorithm in surveillance [78], crowd
*Corresponding author (dengpfan@gmail.com)
Figure 1. Illustration of different counting tasks. Top left: Generic
Object Counting (GOC), which counts objects of various classes in natural scenes. Top right: Dense Object Counting (DOC), which counts objects of a foreground class in scenes packed with instances. Down: Indiscernible Object Counting (IOC), which counts objects of a foreground class in indiscernible scenes. Can you find all fishes in the given examples? For GOC, DOC, and
IOC, the images shown are from PASCAL VOC [18], Shang-haiTech [91], and the new IOCfish5K dataset, respectively. monitoring [6], wildlife conservation [56], diet patterns un-derstanding [55] and cell population analysis [1].
Previous object counting research mainly followed two directions: generic/common object counting (GOC) [8, 14, 32, 68] and dense object counting (DOC) [28, 36, 50, 57, 64, 67, 91]. The difference between these two sub-tasks lies in the studied scenes, as shown in Fig. 1. GOC tack-les the problem of counting object(s) of various categories in natural/common scenes [8], i.e., images from PASCAL
VOC [18] and COCO [41]. The number of objects to be es-timated is usually small, i.e., less than 10. DOC, on the other hand, mainly counts objects of a foreground class in crowded scenes. The estimated count can be hundreds
Dataset
Year 2008
UCSD [6]
Mall [10] 2012 2013
UCF CC 50 [27] 2016
WorldExpo’10 [90] 2016
ShanghaiTech B [91]
ShanghaiTech A [91] 2016
UCF-QNRF [28] 2018 2019
Crowd surv [87]
GCC (synthetic) [80] 2019
JHU-CROWD++ [65] 2019 2020
NWPU-Crowd [79] 2021
NC4K [51] 2021
CAMO++ [33]
COD [19] 2022
IOCfish5K (Ours) 2023
Indiscernible
Scene
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✓
✓
✓
✓
#Ann. IMG Avg. Resolution Free View 158×238 480×640 2101×2888 576×720 768×1024 589×868 2013×2902 840×1342 1080×1920 910×1430 2191×3209 530×709
N/A 737×964 1080×1920 2,000 2,000 50 3,980 716 482 1,535 13,945 15,212 4,372 5,109 4,121 5,500 5,066 5,637
✗
✗
✓
✗
✗
✓
✓
✗
✗
✓
✓
✓
✓
✓
✓
Web
Max
Count Statistics
Ave
Total Min 46 Link 25 11 53 Link 13 31 4,543 Link 94 1,279 253 Link 50 578 Link 123 501 3,139 Link 815 12,865 Link 1420 Link 35 501 3,995 Link 346 25,791 Link 418 20,033 Link 8 Link
N/A Link 8 Link 2,371 Link 49,885 62,325 63,974 1 199,923 9 88,488 33 241,677 49 1,251,642 2 386,513 0 7,625,843 0 1,515,005 0 2,133,375 4,584 1 32,756 N/A 1 5,899 0 659,024 1 6 1 117
Table 1. Statistics of existing datasets for dense object counting (DOC) and indiscernible object counting (IOC). or even tens of thousands. The counted objects are often persons (crowd counting) [36, 39, 88], vehicles [26, 57] or plants [50]. Thanks to large-scale datasets [10,18,28,65,79, 91] and deep convolutional neural networks (CNNs) trained on them, significant progress has been made both for GOC and DOC. However, to the best of our knowledge, there is no previous work on counting indiscernible objects.
Under indiscernible scenes, foreground objects have a similar appearance, color, or texture to the background and are thus difficult to be detected with a traditional visual system. The phenomenon exists in both natural and arti-ficial scenes [20, 33]. Hence, scene understanding for in-discernible scenes has attracted increasing attention since the appearance of some pioneering works [20, 34]. Various tasks have been proposed and formalized: camouflaged ob-ject detection (COD) [20], camouflaged instance segmen-tation (CIS) [33] and video camouflaged object detection (VCOD) [12, 31]. However, no previous research has fo-cused on counting objects in indiscernible scenes, which is an important aspect.
In this paper, we study the new indiscernible object counting (IOC) task, which focuses on counting foreground objects in indiscernible scenes. Fig. 1 illustrates this chal-lenge. Tasks such as image classification [17, 24], seman-tic segmentation [11, 42] and instance segmentation [3, 23] all owe their progress to the availability of large-scale datasets [16, 18, 41]. Similarly, a high-quality dataset for
IOC would facilitate its advancement. Although existing datasets [20, 33, 51] with instance-level annotations can be used for IOC, they have the following limitations: 1) the total number of annotated objects in these datasets is lim-ited, and image resolutions are low; 2) they only contain scenes/images with a small instance count; 3) the instance-level mask annotations can be converted to point supervi-sion by computing the centers of mass, but the computed points do not necessarily fall inside the objects.
To facilitate the research on IOC, we construct a large-scale dataset, IOCfish5K. We collect 5,637 images with indiscernible scenes and annotate them with 659,024 cen-ter points. Compared with the existing datasets, the pro-posed IOCfish5K has several advantages: 1) it is the largest-scale dataset for IOC in terms of the number of images, image resolution, and total object count; 2) the images in
IOCfish5K are carefully selected and contain diverse indis-cernible scenes; 3) the point annotations are accurate and located at the center of each object. Our dataset is com-pared with existing DOC and IOC datasets in Table 1, and example images are shown in Fig. 2.
Based on the proposed IOCfish5K dataset, we provide a systematic study on 14 mainstream baselines [32,36,39,40, 45, 47, 52, 66, 73, 76, 89, 91]. We find that methods which perform well on existing DOC datasets do not necessarily preserve their competitiveness on our challenging dataset.
Hence, we propose a simple and effective approach named
IOCFormer. Specifically, we combine the advantages of density-based [76] and regression-based [39] counting ap-proaches. The former can estimate the object density across the image, while the latter directly regresses the co-ordinates of points, which is straightforward and elegant.
IOCFormer contains two branches: density and regression.
The density-aware features from the density branch help make indiscernible objects stand out through the proposed density-enhanced transformer encoder (DETE). Then the refined features are passed through a conventional trans-former decoder, after which predicted object points are gen-erated. Experiments show that IOCFormer outperforms all considered algorithms, demonstrating its effectiveness on
IOC. To summarize, our contributions are three-fold.
• We propose the new indiscernible object counting (IOC) task. To facilitate research on IOC, we con-tribute a large-scale dataset IOCfish5K, containing 5,637 images and 659,024 accurate point labels.
• We select 14 classical and high-performing approaches for object counting and evaluate them on the proposed
IOCfish5K for benchmarking purposes.
• We propose a novel baseline, namely IOCFormer, which integrates density-based and regression-based methods in a unified framework. In addition, a novel density-based transformer encoder is proposed to grad-ually exploit density information from the density branch to help detect indiscernible objects. 2.