Abstract
Weakly-supervised temporal action localization aims to detect action boundaries in untrimmed videos with only video-level annotations. Most existing schemes detect tem-poral regions that are most responsive to video-level classi-fication, but they overlook the semantic consistency between frames.
In this paper, we hypothesize that snippets with similar representations should be considered as the same action class despite the absence of supervision signals on each snippet. To this end, we devise a learnable dictionary where entries are the class centroids of the corresponding action categories. The representations of snippets identified as the same action category are induced to be close to the same class centroid, which guides the network to perceive the semantics of frames and avoid unreasonable localiza-tion. Besides, we propose a two-stream framework that in-tegrates the attention mechanism and the multiple-instance learning strategy to extract fine-grained clues and salient features respectively. Their complementarity enables the model to refine temporal boundaries. Finally, the developed model is validated on the publicly available THUMOS-14 and ActivityNet-1.3 datasets, where substantial experiments and analyses demonstrate that our model achieves remark-able advances over existing methods. 1.

Introduction
Temporal action localization (TAL) is committed to de-tecting action intervals in untrimmed videos.
It has re-ceived increasing popularity recently due to its wide ap-plication in surveillance analysis, video summarization and retrieval [39, 44, 47], etc. Typically, fully-supervised TAL
[51, 58, 59] is prohibitively expensive and unrealistic due to frame-level annotations, thus the weakly-supervised TAL (WS-TAL) [32, 34, 40, 45, 55] that only video-level annota-tions are required has been advocated recently.
Most WS-TAL methods [8, 9, 31, 43, 45, 46] transform
Figure 1. An example contains the action of “Bicycle Motocross” and the background. Representations depicted in monochromatic color are similar, which should be regarded to describe the same action (or background) and grouped together. The color brightness indicates the degree of similarity. localization into classification tasks that detect temporal regions contributing the most to video-level classification.
They divide raw videos into fixed-length non-overlapping snippets, on which snippet-wise attention activations or class activation sequence (CAS) is generated. Temporal re-gions are detected by thresholding and merging these ac-tivations along the time dimension. Specifically, multiple instance learning (MIL) [36, 42] and the attention mecha-nism [11, 14, 53] are typically employed. The former ag-gregates snippets that are considered action instances with top-k confidence. However, such a regime over-emphasizes these snippets with top-k confidence, resulting in discard-ing potential clues in remaining snippets with lower confi-dence. Besides, MIL chooses the most discriminative snip-pets ignoring the completeness of action instances, which is incompatible with the localization task. Different from
MIL, the attention-based mechanism independently yields class-agnostic confidence for each snippet, which is utilized as a weight to perform temporal pooling over all snippets and generate video-level representations for classification.
Despite the usage of all snippet features for fine-grained patterns, class-agnostic confidence is semantically ambigu-ous and harmful to precise boundary detection. As a con-sequence, we design a two-stream network that integrates
MIL and attention-based mechanisms to overcome their re-spective drawbacks. Then a late-fusion operation on the outputs of the two branches is conducted to acquire the final classification results.
Furthermore, a crux of these works lies in accurately pre-dicting confidence scores that each snippet belongs to the foreground or background, which has a nontrivial impact on the subsequent boundary regression. Since the weakly-supervision paradigm does not provide explicit supervision signals, this problem becomes more intractable. A com-mon solution employs temporal class activation map [40] (TCAM) to discover snippets that respond to the video-level classification and assign them high confidence. Other alter-natives [11, 14, 45, 53] attempt to mitigate this problem by carefully formulating some attention generation and aggre-gation mechanisms. Nevertheless, these strategies neglect the semantic consistency between snippets.
Intuitively, snippets with similar representations should be considered to be the same class despite the infeasibility of accessing snippet-level annotations. An example is also illustrated in
Figure 1. We argue that it is unreasonable that there are no constraints to guarantee such a semantic relation. To address this intractable issue, we set a learnable dictionary where entries are class centroids of the corresponding action categories. The representations of snippets identified as the same action are induced to be close to the same class cen-troid. In this manner, the semantic relationship of snippets is explicitly explored to encourage a reasonable localization in the weakly-supervised paradigm.
In a nutshell, the main contributions and innovations of this paper are summarized as follows: (1) A novel two-stream network that absorbs the merits of MIL and atten-tion mechanism is proposed to resolve WS-TAL. (2) To per-ceive semantic information, a learnable dictionary with eu-clidean constraint is designed to facilitate similar represen-tations to be considered as the same action class. (3) Ex-tensive experiments on THUMOS-14 and ActivityNet-1.3 benchmarks demonstrate that our model acquires remark-able advances. Besides, substantial ablation studies also re-veal that the proposed two-stream structure and semantic-aware modules are of effectiveness. 2.