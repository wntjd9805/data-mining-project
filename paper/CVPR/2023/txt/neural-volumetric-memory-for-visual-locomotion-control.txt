Abstract
Legged robots have the potential to expand the reach of autonomy beyond paved roads. In this work, we consider the difﬁcult problem of locomotion on challenging terrains using a single forward-facing depth camera. Due to the partial observability of the problem, the robot has to rely on past observations to infer the terrain currently beneath it. To solve this problem, we follow the paradigm in com-puter vision that explicitly models the 3D geometry of the scene and propose Neural Volumetric Memory (NVM), a ge-ometric memory architecture that explicitly accounts for the
SE(3) equivariance of the 3D world. NVM aggregates fea-ture volumes from multiple camera views by ﬁrst bringing them back to the ego-centric frame of the robot. We test the learned visual-locomotion policy on a physical robot and show that our approach, which explicitly introduces geo-metric priors during training, offers superior performance than more na¨ıve methods. We also include ablation studies and show that the representations stored in the neural vol-umetric memory capture sufﬁcient geometric information to reconstruct the scene. Our project page with videos is https://rchalyang.github.io/NVM 1.

Introduction
Consider difﬁcult locomotion tasks such as walking up and down a ﬂight of stairs and stepping over gaps (Figure 1).
The control of such behaviors requires tight coupling with perception because vision is needed to provide details of the terrain right beneath the robot and the 3D scene imme-diately around it. This problem is also partially-observable.
Immediately relevant terrain information is often occluded from the robot’s current frame of observation, forcing it to rely on past observations for control decisions. For this rea-son, while blind controllers that are learned in simulation using reinforcement learning have achieved impressive re-sults in agility and robustness [33, 36, 38], there are clear limitations on how much they can do. How to incorporate perception into the pipeline to produce an integrated visuo-motor controller thus remains an open problem.
A recent line of work combines perception with loco-motion using ego-centric cameras mounted on the robot.
The predominant approach for addressing partial observ-ability is to do frame-stacking, where the robot maintains a visual buffer of recent images. This na¨ıve heuristic suf-fers from two major problems: ﬁrst, frame-stacking on a moving robot ignores the equivariance structure of the 3D environment, making learning a lot more difﬁcult as pol-icy success now relies on being able to learn to account for spurious changes in camera poses. A second but subtler is-sue is that biological systems do not have the ability to save detailed visual observations pixel-by-pixel. These concerns motivate the creation of an intermediary, short-term mem-ory mechanism to functionally aggregate streams of obser-Stages
Stairs
Obstacles
Stones
Sim2Real
Stages
Stairs
Obstacles
Stones
Front-Facing 
Depth Camera
Figure 2. Overview of Simulated Environment & Real World Environment: Our simulated environments are shown on the left and real-world environments are shown on the right. For the real-world environment, the corresponding visual observations for each real-world environment are shown in the bottom row. All policies are trained in the simulation and transferred into the real world without ﬁne-tuning. vation into a single, coherent representation of the world.
Motivated by these observations, we introduce a novel volumetric memory architecture for legged locomotion con-trol. Our architecture consists of a 2D to 3D feature volume encoder, and a pose estimator that can estimate the relative camera poses given two input images. When combined, the two networks constitute a Neural Volumetric Memory (NVM) that takes as input a sequence of depth images taken by a forward-looking, ego-centric camera and fuses them together into a coherent latent representation for locomo-tion control. We encourage the memory space to be SE(3) equivariant to changes in the camera pose of the robot by in-corporating translation and rotation operations based on es-timated relative poses from the pose network. This inverse transformation allows NVM to align feature volumes from the past to the present ego-centric frame, making both inte-grating over multiple timesteps into a coherent scene repre-sentation and learning a policy, less difﬁcult.
Our training pipeline follows a two-step teacher-student process where the primary goal of the ﬁrst stage is to pro-duce behaviors in the form of a policy. After training com-pletes, this policy can traverse these difﬁcult terrains ro-bustly, but it relies on privileged sensory information such as an elevation map and ground-truth velocity. Elevation maps obtained in the real world are often biased, incom-plete, and full of errors [40], whereas ground-truth velocity information is typically only available in instrumented en-vironments. Hence in the visuomotor distillation stage of the pipeline, which still runs in the simulator, we feed the stream of ego-centric views from the forward depth cam-era into the neural volumetric memory. We feed the content of this memory into a small policy network and train ev-erything end-to-end including the two network components of the NVM using a behavior cloning loss where the state-only policy acts as the teacher. For completeness, we offer an additional self-supervised learning objective (Figure 4) that relies on novel-view consistency for learning. The end product of this visuomotor distillation pipeline is a memory-equipped visuomotor policy that can operate directly on the
UniTree A1 robot hardware (see Figure 1). A single policy is used to handle all environments covered by this paper.
We provide comprehensive experiment and ablation studies in both simulation and the real world, and show that our method outperforms all baselines by a large margin. It is thus essential to model the 3D structure of the environment. 2.