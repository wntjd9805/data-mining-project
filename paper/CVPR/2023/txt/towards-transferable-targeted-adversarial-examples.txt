Abstract 1.

Introduction
Transferability of adversarial examples is critical for black-box deep learning model attacks. While most existing studies focus on enhancing the transferability of untargeted adversarial attacks, few of them studied how to generate transferable targeted adversarial examples that can mislead models into predicting a specific class. Moreover, existing transferable targeted adversarial attacks usually fail to suf-ficiently characterize the target class distribution, thus suf-fering from limited transferability.
In this paper, we pro-pose the Transferable Targeted Adversarial Attack (TTAA), which can capture the distribution information of the tar-get class from both label-wise and feature-wise perspec-tives, to generate highly transferable targeted adversarial examples. To this end, we design a generative adversar-ial training framework consisting of a generator to produce targeted adversarial examples, and feature-label dual dis-criminators to distinguish the generated adversarial exam-ples from the target class images. Specifically, we design the label discriminator to guide the adversarial examples to learn label-related distribution information about the target class. Meanwhile, we design a feature discrimina-tor, which extracts the feature-wise information with strong cross-model consistency, to enable the adversarial exam-ples to learn the transferable distribution information. Fur-thermore, we introduce the random perturbation dropping to further enhance the transferability by augmenting the di-versity of adversarial examples used in the training process.
Experiments demonstrate that our method achieves excel-lent performance on the transferability of targeted adver-sarial examples. The targeted fooling rate reaches 95.13% when transferred from VGG-19 to DenseNet-121, which significantly outperforms the state-of-the-art methods.
∗Zhibo Wang is the corresponding author.
As an important branch of artificial intelligence (AI), deep neural networks (DNNs) contribute to many real-life applications, e.g., image classification [1, 2], speech recognition [3, 4], face detection [5, 6], automatic driv-ing technology [7], etc. Such broad impacts have mo-tivated a wide range of investigations into the adversar-ial attacks on DNNs, exploring the vulnerability and un-certainty of DNNs. For instance, Szegedy et al. showed that DNNs could be fooled by adversarial examples crafted by adding human-indistinguishable perturbations to origi-nal inputs [8]. As successful adversarial examples must be imperceptible to humans but cause DNNs to make a false prediction, how to design adversarial attacks to generate high-quality adversarial examples in a general manner re-mains challenging.
Adversarial attack methods can be divided into untar-geted attacks and targeted attacks. Untargeted attacks try to misguide the model to predict arbitrary incorrect la-bels, while targeted adversarial attacks expect the gener-ated adversarial examples can trigger the misprediction for a specific label. The transferability of adversarial examples is crucial for both untargeted and targeted attacks, espe-cially in black-box attack scenarios where the target model is inaccessible. However, most existing studies focus on enhancing the transferability of untargeted adversarial at-tacks, through data augmentation [9, 10], model aggrega-tion [11,12], feature information utilization [13–15], or gen-erative methods [16–18]. Although some of them could be extended to the targeted adversarial attacks by simply mod-ifying the loss function, they could not extract sufficient transferable information about the target class due to the overfitting of the source model and the lack of distribution information of the target class, thus demonstrating limited transferability.
Some recent works [15, 19–21] studied how to boost the transferability of targeted adversarial attacks by learning the target class information from either the label or the feature perspective, leveraging the label probability distribution and feature maps respectively. The label-wise information, of-ten output by the last layer of the classification model, can effectively reflect the direct correlation between image dis-tribution and class labels. However, learning with just the label-wise information is proven to retain high-level seman-tic information of the original class [22], thus leading to low cross-model transferability. The feature-wise information, which can be obtained from the intermediate layer of the classification model, has been proven [23] to have transfer-ability due to the mid-level layer of different DNNs follow-ing similar activation patterns. However, the feature-wise information is not sensitive to the target label and fails to trigger the targeted misclassification.
In summary, only the information extracted from the la-bel or the feature cannot generate high-transferability tar-geted adversarial examples. Meanwhile, most existing tar-geted attacks assume that the training dataset of the target model (i.e., target domain) is accessible and could be lever-aged by the attackers to train a shadow model in the target domain as the simulation of the target model, which how-ever is not practical in realistic scenarios.
In this paper, we aim to generate highly transferable tar-geted adversarial examples in a more realistic but challeng-ing scenario where the attacker cannot access the data of the target domain. To this end, we are facing two main challenges. The first challenge is how to generate targeted adversarial examples with both cross-model and cross-domain transferability? Cross-domain images usually vary significantly in characteristic distribution (e.g., have differ-ent attributes even labels), making it difficult for models to transfer the knowledge to unknown domains. Besides, when involved models adopt different architectures, do-main knowledge learned by the source model becomes less transferable, resulting in more difficulties in cross-model and cross-domain transferable adversarial attacks. The sec-ond challenge is how to improve the transferability of tar-geted adversarial attacks? As the targeted adversarial at-tack needs to distort the ground-truth label and trigger the model to predict the target label simultaneously, causing the transferability of the targeted attack hard to achieve due to the dual objective of obfuscating original class information and recognizing target class information.
To solve such challenges, we propose Transferable Tar-geted Adversarial Attack (TTAA) to generate highly trans-ferable targeted adversarial examples. The main idea of our method is to capture the distribution information of the target class from both label-wise and feature-wise perspec-tives. We design a generative adversarial network, which generates targeted adversarial examples by a generator and captures the distribution information of the target class by label-feature dual discrimination, consisting of the label dis-criminator and the feature discriminator. More specifically, the label discriminator learns the label-related distribution information from the label probability distribution and the feature discriminator extracts transferable distribution in-formation of the target class via feature maps output by the intermediate layer of the label discriminator. Meanwhile, we propose random perturbation dropping to enhance our training samples, which applies random transformations to augment the diversity of the adversarial examples used dur-ing the training process to improve the robustness of the distribution information of the target class on the adversar-ial examples. In generative adversarial training, such dis-tribution information extracted by discriminators guides the generator to acquire the label-related and transferable dis-tribution information of the target class. Therefore the tar-geted adversarial examples achieve both high cross-model and cross-domain transferability.
Our main contributions are summarized as follows.
• We propose a general framework for targeted adver-sarial attack that works in both non-cross-domain and cross-domain scenarios. This framework could be eas-ily integrated with other targeted adversarial attacks to improve their cross-model and cross-domain targeted transferability.
• Our proposed Transferable Targeted Adversarial At-tack could extract the target class distribution from feature-wise and label-wise levels to promote pertur-bations to acquire label-related and transferable distri-bution information of the target class, thus generating highly transferable targeted adversarial examples.
• Extensive experiments on diverse classification models demonstrate the superior targeted transferability of ad-versarial examples generated by the proposed TTAA as compared to state-of-the-art transferable attacking methods, no matter whether the attack scenario is cross-domain or non-cross-domain. 2.