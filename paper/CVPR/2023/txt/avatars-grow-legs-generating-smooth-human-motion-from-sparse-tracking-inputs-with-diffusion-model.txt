Abstract 1.

Introduction
With the recent surge in popularity of AR/VR applica-tions, realistic and accurate control of 3D full-body avatars has become a highly demanded feature. A particular chal-lenge is that only a sparse tracking signal is available from standalone HMDs (Head Mounted Devices), often limited to tracking the user’s head and wrists. While this signal is resourceful for reconstructing the upper body motion, the lower body is not tracked and must be synthesized from the limited information provided by the upper body joints. In this paper, we present AGRoL, a novel conditional diffu-sion model specifically designed to track full bodies given sparse upper-body tracking signals. Our model is based on a simple multi-layer perceptron (MLP) architecture and a novel conditioning scheme for motion data. It can pre-dict accurate and smooth full-body motion, particularly the challenging lower body movement. Unlike common dif-fusion architectures, our compact architecture can run in real-time, making it suitable for online body-tracking appli-cations. We train and evaluate our model on AMASS motion capture dataset, and demonstrate that our approach outper-forms state-of-the-art methods in generated motion accu-racy and smoothness. We further justify our design choices through extensive experiments and ablation studies.
*Work done during an internship at Meta AI.
Code is available at github.com/facebookresearch/AGRoL.
Humans are the primary actors in AR/VR applications.
As such, being able to track full-body movement is in high demand for these applications. Common approaches are able to accurately track upper bodies only [25, 56]. Moving to full-body tracking unlocks engaging experiences where users can interact with the virtual environment with an in-creased sense of presence. However, in the typical AR/VR setting there is no strong tracking signal for the entire hu-man body – only the head and hands are usually tracked by means of Inertial Measurement Unit (IMU) sensors em-bedded in Head Mounted Displays (HMD) and hand con-trollers. Some works suggest adding additional IMUs to track the lower body joints [22,25], those additions come at higher costs and the expense of the user’s comfort [24, 27].
In an ideal setting, we want to enable high-fidelity full-body tracking using the standard three inputs (head and hands) provided by most HMDs.
Given the position and orientation information of the head and both hands, predicting full-body pose, especially the lower body, is inherently an underconstrained problem.
To address this challenge, different methods rely on genera-tive models such as normalizing flows [44] and Variational
Autoencoders (VAE) [11] to synthesize lower body mo-tions. In the realm of generative models, diffusion models have recently shown impressive results in image and video generation [21, 39, 46], especially for conditional genera-tion. This inspires us to employ the diffusion model to gen-erate the fully-body poses conditioned on the sparse track-ing signals. To the best of our knowledge, there is no exist-ing work leveraging the diffusion model solely for motion reconstruction from sparse tracking information.
However, it is not trivial to employ the diffusion model in this task. Existing approaches for conditional generation with diffusion models are widely used for cross-modal con-ditional generation. Unfortunately, these methods can not be directly applied to the task of motion synthesis, given the disparity in data representations, e.g. human body joints feature vs. images.
In this paper, we propose a novel diffusion architecture –
Avatars Grow Legs (AGRoL), which is specifically tailored for the task of conditional motion synthesis. Inspired by re-cent work in future motion prediction [18], which uses an
MLP-based architecture, we find that a carefully designed
MLP network can achieve comparable performance to the state-of-the-art methods. However, we discovered that the predicted motions of MLP networks may contain jittering artifacts. To address this issue and generate smooth realis-tic full body motion from sparse tracking signals, we design a novel lightweight diffusion model powered by our MLP architecture. Diffusion models require time step embed-ding [21, 38] to be injected in the network during training and inference; however, we found that our MLP architecture is not sensitive to the positional embedding in the input. To tackle this problem, we propose a novel strategy to effec-tively inject the time step embedding during the diffusion process. With the proposed strategy, we can significantly mitigate the jittering issues and further improve the model’s performance and robustness against the loss of tracking sig-nal. Our model accurately predicts full-body motions, out-performing state-of-the-art methods as demonstrated by the experiments on AMASS [35], large motion capture dataset.
We summarize our contributions as follows:
• We propose AGRoL, a conditional diffusion model specifically designed for full-body motion synthesis based on sparse IMU tracking signals. AGRoL is a simple and yet efficient MLP-based diffusion model with a lightweight architecture. To enable gradual de-noising and produce smooth motion sequences we pro-pose a block-wise injection scheme that adds diffusion timestep embedding before every intermediate block of the neural network. With this timestep embedding strategy, AGRoL achieves state-of-the-art performance on the full-body motion synthesis task without any ex-tra losses that are commonly used in other motion pre-diction methods.
• We show that our lightweight diffusion-based model
AGRoL can generate realistic smooth motions while achieving real-time inference speed, making it suitable for online applications. Moreover, it is more robust against tracking signals loss then existing approaches.
Figure 2. The architecture of our MLP-based network. FC,
LN, and SiLU denote the fully connected layer, the layer normal-ization, and the SiLU activation layer respectively. 1 × 1 Conv denotes the 1D convolution layer with kernel size 1. Note that 1 × 1 Conv here is equivalent to a fully connected layer operating on the first dimension of the input tensor RN ×D, while the FC layers operate on the last dimension. N denotes the temporal dimension and D denotes the dimension of the latent space. The middle block is repeated M times. The first FC layer projects input data to a la-tent space RN ×D and the last one converts from latent space to the output space of full-body poses RN ×S. 2.