Abstract
Existing multi-view representation learning methods typi-cally follow a specific-to-uniform pipeline, extracting latent features from each view and then fusing or aligning them to obtain the unified object representation. However, the manually pre-specified fusion functions and aligning criteria could potentially degrade the quality of the derived represen-tation. To overcome them, we propose a novel uniform-to-specific multi-view learning framework from a meta-learning perspective, where the unified representation no longer in-volves manual manipulation but is automatically derived from a meta-learner named MetaViewer. Specifically, we formulated the extraction and fusion of view-specific latent features as a nested optimization problem and solved it by us-ing a bi-level optimization scheme. In this way, MetaViewer automatically fuses view-specific features into a unified one and learns the optimal fusion scheme by observing recon-struction processes from the unified to the specific over all views. Extensive experimental results in downstream classi-fication and clustering tasks demonstrate the efficiency and effectiveness of the proposed method. 1.

Introduction
Multi-view representation learning aims to learn a uni-fied representation of the entity from its multiple observable views for the benefit of downstream tasks [35, 43, 58]. Each view acquired by different sensors or sources contains both view-shared consistency information and view-specific in-formation [8]. The view-specific part further consists of complementary and redundant components, where the for-mer can be considered as a supplement to the consistency information, while the latter is view-private and may be
∗Corresponding authors
Figure 1. (a), (b) and (c) show three multi-view learning frame-works following the specific-to-uniform pipeline, where the unified representation is obtained by fusing or concatenating view-specific features. (d) illustrates our uniform-to-specific manner, where a meta-learner learns to fusion by observing reconstruction from uni-fied representation to specific views. adverse for the unified representation [16]. Therefore, a high-quality representation is required to retain the consis-tency and complementary information, as well as filter out the view-private redundant ones [51].
Given the data containing two views, x1 and x2, prevail-ing multi-view learning methods typically follow a specific-to-uniform pipeline and can be roughly characterized as:
H := f (x1; Wf ) ◦ g(x2; Wg), (1) where f and g are encoding (or embedding [27]) functions that map the original view data into the corresponding latent
features with the trainable parameters Wf and Wg. These latent features are subsequently aggregated into the unified representation H using the designed aggregation operator
◦. With different aggregation strategies, existing approaches can be further subdivided into the joint, alignment, and a combined share-specific (S&S) representation [22, 26].
Fig. 1 (a) - (c) show the above three branches of the specific-to-uniform framework. Joint representation focuses on the integration of complementary information by directly fusing latent features, where the ◦ is represented as fusion strategies, such as graph-based modules [33], neural net-works [10], or other elaborate functions [7, 34]. While align-ment representation seeks alignment between view-specific features to retain consistency information and specifies ◦ as the alignment operator, measured by distance [11], correla-tion [24, 47], or similarity [25, 38, 39]. The aligned features can be concatenated as the unified representation for down-stream tasks. As a trade-off strategy, S&S representation ex-plicitly distinguishes latent features into shared and specific representations and only aligns the shared part [20, 30, 53].
Despite demonstrating promising results, the specific-to-uniform framework inherently suffers from potential risks in the following two aspects: (1) Compared with the data-oriented fusion rules, manually pre-specified rules are de-signed to compute the unified representation by concatenat-ing or fusing latent features, restricting its extensibility in complex real-world applications. (2) Even if we can find a well-performing fusion scheme, the sequential training manner limits the model to constrain the various component information separately. For example, it is difficult to automat-ically separate out view-private redundant information from the feature level [22, 37]. Recent researches have attempted to address the second issue by decomposing latent features using matrix factorization [61] or hierarchical feature model-ing [51], but still cannot avoid manually pre-specified fusion or even decomposition strategies.
In this work, we provide a new meta-learning perspec-tive for multi-view representation learning and propose a novel uniform-to-specific framework to address these poten-tial risks. Fig. 1 (d) shows the overall schematic. In contrast to the specific-to-uniform pipeline, the unified representation no longer involves manual manipulation but is automatically derived from a meta-learner named MetaViewer. To train the
MetaViewer, we first decouple the learning of view-specific latent features and unified meta representation. These two learning processes can then be formulated as a nested opti-mization problem and eventually solved by a bi-level opti-mization scheme. In detail, MetaViewer fuses view-specific features into a unified one at the outer level and learns the op-timal fusion scheme by observing reconstruction processes from the unified to the specific over all views at the inner level. In addition, our uniform-to-specific framework is com-patible with most existing objective functions and pre-text tasks to cope with complex real-world scenarios. Extensive experiments validate that MetaViewer achieves comparable performance to the state-of-the-art methods in downstream clustering and classification tasks. The core contributions of this work are as follows. 1. We provide a new meta-learning perspective and de-velop a novel uniform-to-specific framework to learn a unified multi-view representation. To the best of our knowledge, it could be the first meta-learning-based work in multi-view representation learning community. 2. We propose MetaViewer, a meta-learner that formu-lates the modeling of view-specific features and unified representation as a nested bi-level optimization and ultimately meta-learns a data-driven optimal fusion. 3. Extensive experiments on multiple benchmarks validate that our MetaViewer achieves comparable performance to the existing methods in two downstream tasks. 2.