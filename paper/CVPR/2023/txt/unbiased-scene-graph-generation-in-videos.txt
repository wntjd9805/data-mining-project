Abstract
The task of dynamic scene graph generation (SGG) from videos is complicated and challenging due to the inherent dynamics of a scene, temporal fluctuation of model predictions, and the long-tailed distribution of the visual relationships in addition to the already existing challenges in image-based SGG.
Existing methods for dynamic SGG have primarily focused on capturing spatio-temporal context using complex architectures without addressing the challenges mentioned above, especially the long-tailed distribution of relationships. This often leads to the generation of biased scene graphs. To address these challenges, we introduce a new framework called TEMPURA:
TEmporal consistency and Memory Prototype guided UnceR-tainty Attenuation for unbiased dynamic SGG. TEMPURA employs object-level temporal consistencies via transformer-based sequence modeling, learns to synthesize unbiased relationship representations using memory-guided training, and attenuates the predictive uncertainty of visual relations using a Gaussian Mixture Model (GMM). Extensive experiments demonstrate that our method achieves significant (up to 10% in some cases) performance gain over existing methods highlight-ing its superiority in generating more unbiased scene graphs.
Code: https://github.com/sayaknag/unbiasedSGG.git 1.

Introduction
Scene graphs provide a holistic scene understanding that can bridge the gap between vision and language [25, 31]. This has made image scene graphs very popular for high-level reasoning tasks such as captioning [14, 37], image retrieval [49, 60], human-object interaction (HOI) [40], and visual question answering (VQA) [23]. Although significant strides have been made in scene graph generation (SGG) from static images [12, 31, 33, 37, 39, 42, 56, 62–64], research on dynamic
SGG is still in its nascent stage.
Dynamic SGG involves grounding visual relationships jointly in space and time. It is aimed at obtaining a structured representation of a scene in each video frame along with learning the temporal evolution of the relationships between each pair of objects. Such a detailed and structured form of video understanding is akin to how humans perceive real-world (a) (b)
Figure 1. (a) Long-tailed distribution of the predicate classes in Ac-tion Genome [25]. (b) Visual relationship or predicate classification performance of two state-of-the-art dynamic SGG methods, namely
STTran [10] and TRACE [57], falls off significantly for the tail classes. activities [4, 25, 43] and with the exponential growth of video data, it is necessary to make similar strides in dynamic SGG.
In recent years, a handful of works have attempted to address dynamic SGG [10, 16, 24, 38, 57], with a majority of them leveraging the superior sequence processing capability of transformers [1, 5, 19, 26, 44, 53, 58]. These methods simply focused on designing more complex models to aggregate spatio-temporal contextual information in a video but fail to address the data imbalance of the relationship/predicate classes, and although their performance is encouraging under the Recall@k metric, this metric is biased toward the highly populated classes. An alternative metric was proposed in [7, 56] called mean-Recall@k which quantifies how SGG models perform over all the classes and not just the high-frequent ones.
Fig 1a shows the long-tailed distribution of predicate classes in the benchmark Action Genome [25] dataset and Fig 1b highlights the failure of some existing state-of-the-art methods is classifying the relationships/predicates in the tail of the distribution. The high recall values in prior works suggest that they may have a tendency to overfit on popular predicate classes (e.g. in front of / not looking at), without considering how the performances on rare classes (e.g. eating/wiping) are getting impacted [12]. Predicates lying in the tails often provide more informative depictions of underlying actions and activities in the video. Thus, it is important to be able to measure a model’s long-term performance not only on the frequently occurring relationships but also on the infrequently occurring ones.
Data imbalance is, however, not the only challenge in dynamic SGG. As shown in Fig. 2 and Fig. 3, several other factors, including noisy annotations, motion blur, temporal fluctuations of predictions, and a need to focus on only
(a) Incomplete annotations and multiple correct predicates. (b) Triplet variability (multiple possible object pairs for the same relationship)
Figure 2. Noisy scene graph annotations in Action Genome [25] increase the uncertainty of predicted scene graphs. active objects that are involved in an action contribute to the bias in training dynamic SGG models [3]. As a result, the visual relationship predictions have high uncertainty, thereby increasing the challenge of dynamic SGG manyfold.
In this paper, we address these sources of bias in dynamic
SGG and propose methods to compensate for them. We identify missing annotations, multi-label mapping, and triplet (<subject−predicate−object>) variability (Fig 2) as label-ing noise, which coupled with the inherent temporal fluctuations in a video can be attributed as data noise that can be modeled as the aleatoric uncertainty [11]. Another form of uncertainty called the epistemic uncertainty, relates to misleading model predictions due to a lack of sufficient observations [28] and is more prevalent for long-tailed data [22]. To address the bias in training SGG models [3] and generate more unbiased scene graphs, it is necessary to model and attenuate the predictive uncertainty of an SGG model. While multi-model deep ensembles [13, 32] can be effective, they are computationally expensive for large-scale video understanding. Therefore, we employ the concepts of single model uncertainty based on
Mixture Density Networks (MDN) [9, 28, 54] and design the predicate classification head as a Gaussian Mixture Model (GMM) [8, 9]. The GMM-based predicate classification loss penalizes the model if the predictive uncertainty of a sample is high, thereby, attenuating the effects of noisy SGG annotations.
Due to the long-tailed bias of SGG datasets, the predicate embeddings learned by existing dynamic SGG frameworks significantly underfit to the data-poor classes. Since each object pair can have multiple correct predicates (Fig 2), many relationship classes share similar visual characteristics.
Exploiting this factor, we propose a memory-guided training strategy to debias the predicate embeddings by facilitating knowledge transfer from the data-rich to the data-poor classes sharing similar characteristics. This approach is inspired by recent advances in meta-learning and memory-guided training for low-shot, and long-tail image recognition [17, 45, 48, 51, 65], whereby a memory bank, composed of a set of prototypical abstractions [51] each compressing information about a predicate class, is designed. We propose a progressive memory computation approach and an attention-based information diffu-sion strategy [58]. Backpropagating while using this approach, teaches the model to learn how to generate more balanced predicate representations generalizable to all the classes.
Figure 3. Occlusion and motion blur caused by moving objects in videos renders off-the-self object detectors such as FasterRCNN [47] ineffective in producing consistent object classification.
Finally, to ensure the correctness of a generated graph, accurate classification of both nodes (objects) and edges (pred-icates) is crucial. While existing dynamic SGG methods focus on innovative visual relation classification [10, 24, 38], object classification is typically based on proposals from off-the-shelf object detectors [47]. These detectors may fail to compensate for dynamic nuances in videos such as motion blur, abrupt leading to inconsistent background changes, occlusion, etc. object classification. While some works use bulky tracking algorithms to address this issue [57], we propose a simpler yet highly effective learning-based approach combining the superior sequence processing capability of transformers [58], with the discriminative power of contrastive learning [18] to ensure more temporally consistent object classification. Therefore, com-bining the principles of temporally consistent object detection, uncertainty-aware learning, and memory-guided training, we design our framework called TEMPURA: TEmporal consis-tency and Memory Prototype guided UnceRtainty Atentuation for unbiased dynamic SGG. To the best of our knowledge, 2.