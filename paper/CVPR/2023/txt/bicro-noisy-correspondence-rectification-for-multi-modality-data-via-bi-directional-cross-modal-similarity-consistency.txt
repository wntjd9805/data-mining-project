Abstract
As one of the most fundamental techniques in multi-modal learning, cross-modal matching aims to project various sensory modalities into a shared feature space. To achieve this, massive and correctly aligned data pairs are required for model training. However, unlike unimodal datasets, multimodal datasets are extremely harder to collect and annotate precisely. As an alternative, the co-occurred data pairs (e.g., image-text pairs) collected from the Internet have been widely exploited in the area.
Unfortunately, the cheaply collected dataset unavoidably contains many mismatched data pairs, which have been proven to be harmful to the model’s performance. To address this, we propose a general framework called BiCro (Bidirectional Cross-modal similarity consistency), which can be easily integrated into existing cross-modal matching models and improve their robustness against noisy data.
Specifically, BiCro aims to estimate soft labels for noisy their true correspondence degree. data pairs to reflect
The basic idea of BiCro is motivated by that – taking image-text matching as an example – similar images should have similar textual descriptions and vice versa. Then the consistency of these two similarities can be recast as the estimated soft labels to train the matching model. The ex-periments on three popular cross-modal matching datasets demonstrate that our method significantly improves the noise-robustness of various matching models, and surpass the state-of-the-art by a clear margin.The code is available at https://github.com/xu5zhao/BiCro. 1.

Introduction
As a key step towards general intelligence, multimodal learning aims to endow an agent with the ability to extract and understand information from various sensory modali-ties. One of the most fundamental techniques in multimodal
*Corresponding Author(min.xu@uts.edu.au).
Figure 1. Illustration of the Bidirectional Cross-modal similarity consistency (BiCro).
In an image-text shared feature space, as-sume (I1, T1) and (I2, T2) are two clean positive data pairs. The image ˜I is very close to the image I1, but their corresponding texts
˜T and T1 are far away from each other. Similarly, the text ˜T is very close to the text T2 while their corresponding images ˜I and I2 are irrelevant. Therefore the data pair ( ˜I, ˜T ) is possibly noisy (mis-matched). learning is cross-modal matching. Cross-modal matching tries to project different modalities into a shared feature space, and it has been successfully applied to many areas, e.g., image/video captioning [1, 21, 23], cross-modal hash-ing [14, 18], and visual question answering [15, 58].
Like many other tasks in deep learning, cross-modal matching also requires massive and high-quality labeled training data. But even worse, the multimodal datasets (e.g., in image-text matching) are significantly harder to be man-ually annotated than those unimodal datasets (e.g., in image classification). This is because the textual description of an image is very subjective, while the categorical label of an image is easier to be determined. Alternatively, many works collect the co-occurred image-text pairs from the Internet to train the cross-modal matching model, e.g., CLIP [32] was trained over 400 million image-text pairs collected from the
Internet. Furthermore, one of the largest public datasets in cross-modal matching, Conceptual Caption [36], was also
harvested automatically from the Internet. Such cheap data collection methods inevitably introduce noise into the col-lected data, e.g., the Conceptual Caption dataset [36] con-tains 3% ∼ 20% mismatched image-text pairs. The noisy data pairs undoubtedly hurt the generalization of the cross-modal matching models due to the memorization effect of deep neural networks [55].
Different from the noisy labels in classification, which refer to categorical annotation errors, the noisy labels in cross-modal matching refer to alignment errors in paired data. Therefore most existing noise-robust learning meth-ods developed for classification cannot be directly applied to the cross-modal matching problem. Generally, all the data pairs in a noisily-collected cross-modal dataset can be categorized into the following three types based on their noise level: well-matched, weakly-matched, and mis-matched. The well-matched data pairs can be treated as clean examples, and the weakly-matched and mismatched data pairs are noisy examples since they are all labeled as positive (y = 1) in the dataset. We illustrate some noisy data pairs, including both weakly-matched and mismatched pairs in Fig. 2. The key challenge in noise-robust cross-modal matching is how to estimate accurate soft correspon-dence labels for those noisy data pairs. The estimated soft label is expected to be able to depict the true correspon-dence degree between different modalities in a data pair.
In this work, we propose a general framework for soft correspondence label estimation given only noisily-collected data pairs. Our framework is based on the Bidirec-tional Cross-modal similarity consistency (BiCro), which inherently exists in paired data. An intuitive explanation of the BiCro is – taking image-text matching task as an example – similar images should have similar textual de-scriptions and vice versa.
In other words, if two images are highly similar but their corresponding texts are irrele-vant, there possibly exists image-text mismatch issue (see
Fig. 1). Specifically, we first select a set of clean positive data pairs as anchor points out of the given noisy data. This step is achieved by modeling the per-sample loss distribu-tion of the dataset using a Beta-Mixture-Model (BMM) and selecting those samples with a high probability of being clean. Then, with the help of the selected anchor points, we compute the Bidirectional Cross-modal similarity con-sistency (BiCro) scores for every noisy data pair as their soft correspondence labels. Finally, the estimated soft cor-respondence labels are recast as soft margins to train the cross-modal matching model in a co-teaching manner [11] to avoid the self-sample-selection error accumulation prob-lem.
Before delving into details, we summarize our main con-tributions below:
• This paper tackles a widely-exist but rarely-explored problem in cross-modal matching, i.e., the noisy cor-respondence issue. We identify and explore the key challenge of noise-robust cross-modal learning: how to rectify the binary noisy correspondence labels into more accurate soft correspondence labels for noisy data pairs.
• We propose a general framework called Bidirectional
Cross-modal similarity consistency (BiCro) for soft correspondence label estimation given only noisy data pairs. The BiCro is motivated by a rational assumption that – taking the image-text matching as an example – similar images should have similar textual descrip-tions and vice versa. The BiCro can be easily adapted to any cross-modal matching models to improve their noise robustness.
• To compute the BiCro score for noisy data pairs, we propose to first select some clean positive data pairs as anchor points out of the noisy dataset by mod-eling the per-sample loss distribution of the dataset through a Beta-Mixture-Model (BMM). Then a set of examples with high clean possibilities can be se-lected. The BMM has a better modeling ability on the skewed clean data loss distribution compared to the
Gaussian-Mixture-Model (GMM) used in the previous method [12].
• Extensive experiments on three cross-modal match-ing datasets, including both synthetic and real-world noisy labels, demonstrate that our method surpasses the state-of-the-art by a large margin. The visualiza-tion results also indicate that our estimated soft corre-spondence labels are highly-aligned with the correla-tion degree between different modalities in data pairs. 2.