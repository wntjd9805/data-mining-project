Abstract
Learning with large-scale unlabeled data has become a powerful tool for pre-training Visual Transformers (VTs).
However, prior works tend to overlook that, in real-world scenarios, the input data may be corrupted and unreli-able. Pre-training VTs on such corrupted data can be chal-lenging, especially when we pre-train via the masked au-toencoding approach, where both the inputs and masked
“ground truth” targets can potentially be unreliable in this case. To address this limitation, we introduce the To-ken Boosting Module (TBM) as a plug-and-play compo-nent for VTs that effectively allows the VT to learn to ex-tract clean and robust features during masked autoencod-ing pre-training. We provide theoretical analysis to show how TBM improves model pre-training with more robust and generalizable representations, thus benefiting down-stream tasks. We conduct extensive experiments to analyze
TBM’s effectiveness, and results on four corrupted datasets demonstrate that TBM consistently improves performance on downstream tasks. 1.

Introduction
Having rapidly risen in popularity in recent years, Vi-sion Transformer (ViT) [16] and its variants [19] have shown impressive performance across various computer vi-sion tasks, such as image classification, video recognition and 3D action analysis [2, 20, 31, 51, 60]. Amongst this wave of research on Visual Transformers (VTs), there has emerged a popular paradigm – self-supervised VT pre-training [2, 6, 20, 60] – which has attracted a lot of attention in the research community. These works [2, 6, 20, 60] gen-erally pre-train VTs on a large dataset in a self-supervised manner, allowing them to extract semantically meaningful and generalizable features without the need for annotated
† equal contribution
‡ corresponding author data. These pre-trained VTs are practical and effective, showing good downstream performance with only slight fine-tuning, and have quickly become an important research direction.
Among the various self-supervised pre-training strate-gies, masked autoencoding [2, 16, 20] is a prominent ap-proach that has been widely explored. Masked autoencod-ing [2, 11,16,20, 33,60,61, 71] works by randomly masking a portion of input tokens or patches, and letting the VT re-construct them. Thus, in order to successfully reconstruct the masked tokens or patches, the VT is driven to learn the underlying structure and semantic information of the in-put data. Due to its natural compatibility with the token-wise representation in VTs, masked autoencoding has been explored for pre-training VTs on data across many fields, such as RGB images [2, 20, 61], pose data [11, 33] and 3D data [71].
However, in many real-world scenarios, the input data can be of low quality and can be unreliable, which is of-ten overlooked. For example, in adverse weather conditions
[22], the quality of captured images can be quite bad, with many corrupted pixels that degrade the image. Depth im-ages, which are commonly used for 3D analysis, also often contain errors and noticeable perturbations due to measure-ment errors introduced by the depth camera sensors [26].
Skeleton data can also have distortions in the joint coordi-nates [14, 37, 59] due to noise introduced by the sensors, e.g., the pose tracking function of Kinect v2 often intro-duces average offsets of 50-100 mm per joint [59]. Exam-ples of such corrupted data are depicted in Fig. 1.
In many cases, we have access to large-scale unlabeled data containing such corruptions and noise, and would like to exploit them to pre-train a VT that can be effectively adapted to a wide range of downstream tasks [2, 11, 20, 61].
Yet, this can be rather challenging since the data corrup-tions can interfere with the learning of representations dur-ing self-supervised pre-training, leading to unreliable fea-tures and predictions in downstream tasks. This is the case
recognition, and depth image classification with cor-rupted data. Experiments consistently show that our
TBM provides significant improvements in down-stream tasks. 2.