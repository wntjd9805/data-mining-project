Abstract
Endeavors have been recently made to leverage the vi-sion transformer (ViT) for the challenging unsupervised domain adaptation (UDA) task. They typically adopt the cross-attention in ViT for direct domain alignment. However, as the performance of cross-attention highly relies on the quality of pseudo labels for targeted samples, it becomes less effective when the domain gap becomes large. We solve this problem from a game theory’s perspective with the pro-posed model dubbed as PMTrans, which bridges source and target domains with an intermediate domain. Speciﬁcally, we propose a novel ViT-based module called PatchMix that effectively builds up the intermediate domain, i.e., proba-bility distribution, by learning to sample patches from both domains based on the game-theoretical models. This way, it learns to mix the patches from the source and target do-mains to maximize the cross entropy (CE), while exploiting two semi-supervised mixup losses in the feature and label spaces to minimize it. As such, we interpret the process of
UDA as a min-max CE game with three players, including the feature extractor, classiﬁer, and PatchMix, to ﬁnd the
Nash Equilibria. Moreover, we leverage attention maps from
ViT to re-weight the label of each patch by its importance, making it possible to obtain more domain-discriminative feature representations. We conduct extensive experiments on four benchmark datasets, and the results show that
PMTrans signiﬁcantly surpasses the ViT-based and CNN-based SoTA methods by +3.6% on Ofﬁce-Home, +1.4% on
Ofﬁce-31, and +17.7% on DomainNet, respectively. https:
//vlis2022.github.io/cvpr23/PMTrans 1.

Introduction
Convolutional neural networks (CNNs) have achieved tremendous success on numerous vision tasks; however, they still suffer from the limited generalization capability to a new domain due to the domain shift problem [50]. Unsupervised domain adaptation (UDA) tackles this issue by transferring
*These authors contributed equally to this work.
†Corresponding Author. y c a r u c c
A 55 50 45 40 35  30 25 20 clp inf pnt qdr rel skt
Target(cid:172) tasks
Figure 1. The classiﬁcation accuracy of our PMTrans surpasses the
SoTA methods by +17.7% on the most challenging DomainNet dataset. Note that the target tasks treat one domain of DomainNet as the target domain and the others as the source domains. knowledge from a labeled source domain to an unlabeled target domain [30]. A signiﬁcant line of solutions reduces the domain gap based on the category-level alignment which produces pseudo labels for the target samples, such as metric learning [14,53], adversarial training [12,17,34], and optimal transport [44]. Furthermore, several works [11, 36] explore the potential of ViT for the non-trivial UDA task. Recently,
CDTrans [45] exploits the cross-attention in ViT for direct domain alignment, buttressed by the crafted pseudo labels for target samples. However, CDTrans has a distinct limita-tion: as the performance of cross-attention highly depends on the quality of pseudo labels, it becomes less effective when the domain gap becomes large. As shown in Fig. 1, due to the signiﬁcant gap between the domain qdr and the other domains, aligning distributions directly between them performs poorly.
In this paper, we probe a new problem for UDA: how to smoothly bridge the source and target domains by con-structing an intermediate domain with an effective ViT-based solution? The intuition behind this is that, compared to direct aligning domains, alleviating the domain gap be-tween the intermediate and source/target domain can facili-(cid:2)(cid:3)(cid:1)(cid:4) (cid:2)(cid:3)(cid:7)(cid:4)(cid:5) (cid:1) (cid:6)(cid:8) (cid:3)(cid:4)(cid:13)(cid:8)(cid:7)(cid:14) (cid:2)(cid:12)(cid:15)(cid:13)(cid:5)(cid:7) (cid:1) (cid:1)(cid:11)(cid:14)(cid:7)(cid:13) (cid:10) (cid:7) (cid:6)(cid:9)(cid:4)(cid:14)(cid:7) (cid:2)(cid:3)(cid:1)(cid:5) (cid:1)(cid:4)(cid:3)(cid:2)(cid:5)
Figure 2. PMTrans builds up the intermediate domain (green patches) via a novel PatchMix module by learning to sample patches from the source (blue patches) and target (pink patches) domains. PatchMix tries to maximize the CE (↑) between the intermediate domain and source/target domain, while the feature extractor and classiﬁer try to minimize it (↓) for aligning domains. tate domain alignment. Accordingly, we propose a novel and effective method, called PMTrans (PatchMix Transformer) to construct the intermediate representations. Overall, PM-Trans interprets the process of domain alignment as a min-max cross entropy (CE) game with three players, i.e., the feature extractor, a classiﬁer, and a PatchMix module, to ﬁnd the Nash Equilibria. Importantly, the PatchMix module is proposed to effectively build up the intermediate domain, i.e., probability distribution, by learning to sample patches from both domains with weights generated from a learnable Beta distribution based on the game-theoretical models [1, 3, 28], as shown in Fig. 2. That is, we aim to learn to mix patches from two domains to maximize the CE between the inter-mediate domain and source/target domain. Moreover, two semi-supervised mixup losses in the feature and label spaces are proposed to minimize the CE. Interestingly, we conclude that the source and target domains are aligned if mixing the patch representations from two domains is equivalent to mixing the corresponding labels. Therefore, the domain discrepancy can be measured based on the CE between the mixed patches and mixed labels. Eventually, the three play-ers have no incentive to change their parameters to disturb
CE, meaning the source and target domains are well aligned.
Unlike existing mixup methods [38, 47, 49], our proposed
PatchMix subtly learns to combine the element-wise global and local mixture by mixing patches from the source and tar-get domains for ViT-based UDA. Moreover, we leverage the class activation mapping (CAM) from ViT to allocate the se-mantic information to re-weight the label of each patch, thus enabling us to obtain more domain-discriminative features.
We conduct experiments on four benchmark datasets, in-cluding Ofﬁce-31 [33], Ofﬁce-Home [40], VisDA-2017 [32], and DomainNet [31]. The results show that the performance of PMTrans signiﬁcantly surpasses that of the ViT-based
[36, 45, 46] and CNN-based SoTA methods [18, 29, 35] by
+3.6% on Ofﬁce-Home, +1.4% on Ofﬁce-31, and +17.7% on DomainNet (See Fig. 1), respectively.
Our main contributions are four-fold: (I) We propose a novel ViT-based UDA framework, PMTrans, to effectively bridge source and target domains by constructing the inter-mediate domain. (II) We propose PatchMix, a novel module to build up the intermediate domain via the game-theoretical models. (III) We propose two semi-supervised mixup losses in the feature and label spaces to reduce CE in the min-max
CE game. (IV) Our PMTrans surpasses the prior methods by a large margin on three benchmark datasets. 2.