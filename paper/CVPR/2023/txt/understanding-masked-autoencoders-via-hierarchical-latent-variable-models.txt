Abstract
Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoreti-cally principled understanding is still lacking. In this work, we formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE. We for-mulate the underlying data-generating process as a hierar-chical latent variable model, and show that under reason-able assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limita-tions of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights. 1.

Introduction
Self-supervised learning (SSL) has achieved tremendous success in learning transferable representations without la-bels, showing strong results in a variety of downstream tasks [12, 14, 16, 23, 49]. As a major SSL paradigm, masked image modeling (MIM) [1–3,11,13,22,41,63,69] performs the reconstruction of purposely masked image pixels as the pretraining task. Among MIM methods, masked autoen-coding (MAE) [22] has gained significant traction due to its computational efficiency and state-of-the-art performance in a wide range of downstream tasks.
Empirical observations from previous work reveal vari-ous intriguing properties of MAE. In particular, aggressive
*Joint first author. † Joint senior author.
Figure 1. Masking-reconstruction under a hierarchical generating process. In a hierarchical data-generating process, high-level latent vari-ables (e.g., z1) represent high-level information such as semantics, and low-level latent variables (e.g., [z2, z3, z4]) represent low-level informa-tion such as texture. We show that through proper masking, MAE learns to recover high-level latent variables with identifiability guarantees. masking has been shown critical to downstream task per-formances [22, 28, 61, 63]. It is conjectured that such mask-ing forces the model to learn meaningful high-level seman-tic understanding of the objects and scenes rather than the low-level information such as texture. However, it remains largely unclear whether such intuitions are sound in princi-ple. Theoretically verifying and characterizing these empir-ical insights would not only grant a certificate to the current approaches but would also offer theoretical insights for al-gorithmic advancements.
In this work, we establish a principled yet intuitive framework for understanding MAE and providing identifia-bility guarantees. Concretely, we first formulate the under-lying data-generating process as a hierarchical latent vari-able model (Figure 1), with high-level variables correspond-ing to abstract and semantic information like classes, and low-level variables corresponding to elaborate and granular information like texture. Such latent variable models have been studied in causal discovery [29, 62]. In [27, 50], it is hypothesized that complex data, such as images, follow a hierarchical latent structure.
Stemming from this formulation, we show that under reasonable assumptions, MAE can recover a subset of the
true latent variables within the hierarchy, where the levels of the learned latent variables are explicitly determined by how masking is performed. Our theoretical framework not only unifies existing empirical observations in a coherent fashion but also gives rise to insights for potential empir-ical improvements and fundamental limitations of MAE.
Our theory improves the existing nonlinear identifiability results [45, 58] and can be of independent interest.
Empirically, we deduce several insights from our theo-retical results and verify them with experiments. Unlike common belief, MAE trained with extremely high masking ratios (e.g., 90%) captures low-level information, similar to models trained with extremely low ratios (e.g., 10%). Our results suggest that learning high-level semantic informa-tion is only possible in the non-extreme masking regime.
We also discuss masking designs that can potentially im-prove current empirical performance.
Contributions. We highlight the following contributions:
• We formulate the underlying data-generating process as a hierarchical latent variable model. Under such a formulation, we provide a theoretical guarantee for
MAE by showing that it can recover true latent vari-ables in the hierarchical model.
• Based on our theoretical results, we establish the con-nection between masking hyperparameters (i.e., mask-ing ratios and patch sizes) and the learned representa-tion and discuss potential improvements and inherent limitations of MAE.
• We validate our theoretical insights with extensive ex-perimental results. We illustrate how the semantic level of the learned representation varies with the ag-gressiveness of the masking strategy.
Interestingly, representations learned under overly aggressive mask-ing (e.g. 90% masking ratio) exhibit similar properties to their counterparts learned with overly conservative masking (e.g. 10% masking ratio). 2. Theoretical understanding 2.1. A hierarchical data-generating process
Images, despite their high dimensionality, are well struc-tured – there is a multitude of statistical dependencies among pixels determined by their relative distances and vi-sual semantics. For instance, pixels in close proximity are often highly dependent, whereas pixels far apart typically share less information. There has been a plethora of work adopting this intuition for vision tasks such as image gen-eration [47, 55, 67]. Similar insights are also addressed in attempts to learn a part-whole image representation [27,50]. z1 z2 z3 z4 z5 z6 z7 z8 z9 z10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11
Figure 2. A hierarchical data-generating process. z represents the latent variables and x stands for the observable variables (i.e. image pixels). The hierarchical model is generic and is capable of modeling arbitrary DAGs in the latent space.
In this work, we formulate such an underlying struc-ture of images with a hierarchical data-generating pro-cess [1, 29, 62] (Figure 2). Under this formulation, we re-veal the underpinning principle of MAE and provide iden-tifiability guarantees. In particular, we show that through masking-reconstruction, MAE learns the long-range statis-tical dependencies within the image, which renders it capa-ble of extracting high-level semantic representations.
Formally, the generating process is defined with a graph structure G := (V, E) where E is the set of all directed edges and V := (X, Z) comprises all observable variables
X := {x1, . . . , xm} (i.e., all pixels) and all latent variables
Z := {z1, . . . , zn}. Each variable xi or zj represents a multidimensional vector. 1 The hierarchical latent structure
G fulfills the following assumption:
Assumption 1. (Data-generating process): There is no di-rect edge between any two observables: ∀xi, xj ∈ X , (xi, xj) /∈ E and (xj, xi) /∈ E. Each variable is generated by its parents in a directed acyclic graph (DAG) according to: zi = gzi(Pa(zi), εi), xj = gxj (Pa(xj), εj), (1) where gzi and gxj are invertible functions, εi denotes ex-ogenous random variables, and Pa(·) denotes the parents of a certain node.
The invertible data-generating-module assumption (gi and gj being invertible) is adopted from prior work identify-ing latent variables in deep generative models [18, 58]. We make the following remarks on the hierarchical generating process. First, we note that we impose minimal constraints on the graph structure among the latent variables (i.e., the connectivity among latent variables z); therefore, the hierar-chical model class is generic and encompasses all possible
DAG structures over latent variables (Figure 2). Next, we interpret the latent variables z as information related to se-mantic/content information, such as the shape and contour 1In high-dimensional data like images, there is a larger degree of infor-mation redundancy, e.g., neighboring pixels. Thus, it is sensible to lump one-dimensional variables into vectors.
in the image, whereas the exogenous variables ε injected in each layer represent nuanced information such as the tex-ture and contrast of the image. Each structural function gi mixes the two sources of information and generates a more low-level variable until pixels x. Lastly, for the upcoming theoretical results, as long as the data-generating process conforms to the hierarchical graph assumption, our theory holds and the insights do not rely on the knowledge of a specific graph structure. 2.2. Masked Autoencoder
As a canonical method of masking-reconstruction learn-ing, MAE [22] randomly masks a subset of pixel patches in the original image and then reconstructs the masked patches from the encoded representation of the visible part. More formally, we formulate the MAE training as follows.
Mask sampling: random masks m are sampled from a distribution pm which is parameterized by the masking ratio r (i.e., the ratio between the number of masked pixels and the number of all pixels) and patch size s (i.e., the size of the minimal masking unit).
MAE encoding: Emc (xmc) maps the unmasked part xmc to a latent representation ˆc 2, where mc denotes the complement of the mask index set m and is passed to the encoder as positional embeddings to indicate the positions of the visible patches.
MAE decoding: Dm(ˆc, ˆsm) reconstructs the masked image xm from the estimated latent variable ˆc (i.e., the en-coder output), and the auxiliary information ˆsm embodying positional embeddings and [MASK] token which are fed to the decoder in MAE. Although ˆsm is deterministic in
MAE implementation, we view it as a random variable in our analysis.
With the notation above, the MAE training objective can be expressed as follows:
L(E, D) := Em,x,ˆsm (cid:2)∥Dm (Emc (xmc ), ˆsm) − xm∥2(cid:3) . (2) 2.3. Identifiability theory
Building upon the formalization above, we show in The-orem 1 that each random mask m would induce a specific (sub)set of latent variables that fully captures the statistical dependency between the masked part and the visible part.
We denote this relationship as c ⊂ Z where c is the subset of the latent variable set Z.
Theorem 1. (Locating the shared information c): In a hier-archical latent variable structure G, for each specific mask m, there exists a corresponding minimal set of latent vari-ables c such that the generating process of x can be ex-pressed as in Figure 3 where the following conditions are satisfied: 2To avoid notation cluttering, we adopt ˆ· to distinguish the estimated variables from the true ones in the generating process. sm c smc xm xmc
Figure 3. Information sharing latent models. Here, xm and xmc denote the masked part and the visible part of the image x, respectively. c stands for the maximally shared information be-tween xm and xmc . sm and smc refer to the information specific to xm and xmc respectively. The dashed line indicates the poten-tial existence of statistical dependence. 1. xm = gxm (c, sm) and xmc = gxmc (c, smc) where both gxm and gxmc are invertible; 2. sm ⊥⊥ (c, smc); 3. c is minimal: ∀c′ ⊂ Z such that dim(c′) < dim(c), c′ cannot satisfy the two conditions above.
Such c and the corresponding sm are unique and can be located from the hierarchical structure by executing Algo-rithm 1. Furthermore, smc can be found through Algo-rithm 2.
The proof, Algorithm 1, and Algorithm 2 can be found in Appendix A. We note that although the minimal c and its corresponding sm are unique for a given mask m, there is no unique smc in general. Algorithm 2 returns one such instance.
Theorem 1 states that for each mask m, there exists a corresponding c that represents all the information con-tained in the visible part xmc that is conducive to recon-structing the masked part xm. Algorithm 1 can locate such c in the hierarchy and directly characterizes the impact of masking on the property of c.
Next, in Theorem 2, we show that MAE learning objec-tive (Equation 2) estimates c specified in Theorem 1, and
MAE attains a form of identifiability of c. We first lay out the assumptions:
Assumption 2. (MAE model): For any mask m, the MAE decoder Dm(ˆc, ˆsm) has a non-singular Jacobian matrix almost anywhere, and there exists an invertible function
˜gmc(·) such that MAE encoder Emc (·) = [˜g−1 mc(·)]1:dc where [·]1:dc denotes the dimensions corresponding to c.
Moreover, (Dm, ˜gmc) forms an invertible mapping between (ˆc, ˆsm, ˆsmc) and (xm, xmc)
Next, we show MAE identifies the shared information c:
Theorem 2. (Identifiability of c): For each mask m, given the dimensions (dc, dsm ) the encoder function Emc (·) re-covers all information of c located in Theorem 1, i.e., there exists a one-to-one mapping h, s.t., h(c) = ˆc.
In the following, we discuss our assumptions and results.
The proof can be found in Appendix B. z1 z2 z1 z2 z1 z2 z3 z4 z5 z6 z3 z4 z5 z6 z3 z4 z5 z6
Assumption interpretation. Assumption 1 follows prior work identifying latent variables in deep generative mod-els [18, 58] to ensure that latent variables are recoverable from pixels. Assumption 2 requires the MAE encoder Emc to be part of an invertible function output – this is mild and allows the encoder to be more flexible than invertible func-tions. The decoder Dm(ˆc, ˆsm) is assumed to be locally in-vertible in ˆc almost surely, allowing for a broader class than invertible functions, e.g., nondegenerate polynomials. The joint invertibility of (Dm, ˜gmc ) is assumed to ensure no in-formation during the estimation process.
How does MAE work? Theorem 2 states that the MAE objective (Equation 2) essentially serves to estimate the shared variable c and is able to restore all information in c. Therefore, the efficacy of MAE stems from its ability to extract high-level semantic representations from low-level features like image pixels. Moreover, our theory indicates the possibility of fully identifying a latent hierarchical struc-ture via properly designed self-supervised objectives, open-ing up research avenues for future work.
Takeaway: MAE provably recovers high-level represen-tations from low-level features like pixels.
How does masking influence the learned representa-tion? Theorem 1 establishes a direct connection between the mask m and the shared information c which is further connected to the MAE estimate ˆc in Theorem 2. We can ob-serve that conservative masking with overly small masking ratios and masking patch sizes inevitably leads to low-level latent variables. To see this, in Figure 4a, the mask is not large enough to cover all observable descendants of a de-sirable high-level variable z1, thus following Algorithm 1 a low-level variable z3 will mix in ˆc, preventing the model from learning z1. This insight highlights the necessity of nontrivial masking ratios and patch sizes and resonates with the empirical observations in [22, 28, 63].
Surprisingly, the above reasoning can be applied to the case with extremely aggressive masking: in Figure 4b low-level latent variables z6 will be learned by MAE when the visible part is too small to cover all observable descendants of a desirable high-level variable z2. Thus, the learned rep-resentation does not become monotonically more high-level with increasing masking aggressiveness – overly aggressive masking also gives rise to low-level representations. This insight echoes the empirical finding in [61, 63] where the extremely large masking degrades the performance of high-level downstream tasks like classification [63] but yields rel-atively low-level representations like the object locations / x1 x2 x3 x4 x5 x6 (a) Conservative mask x1 x2 x3 x4 x5 x6 (b) Aggressive mask x1 x2 x3 x4 x5 x6 (c) Ideal mask
Figure 4. The impact of masking on the learned representation.
We label the masked pixels with x. We locate the MAE learned latent variables with Algorithm 1 and label them with blue. We can observe that extremely low (left) and high (middle) masking intensities lead to low-level representations, whereas the desirable masking intensity that yields a high-level representation lies in the intermediate masking aggressiveness. scales in the image [61]. In Section 3, we present empirical evidence to verify our theoretical insights.
Takeaway: (1) MAE under different masking intensi-ties learns representations of different abstraction levels; (2) Learning high-level representations is very hard with extreme masking.
Is current MAE optimal for representation learning?
As reflected in the discussion above, although MAE offers the flexibility of tuning the masking scheme to learn repre-sentations of various levels, it is inherently challenging to learn high-level representations by random masking with-out prior knowledge of the latent structure. In contrast, con-trastive learning [5,9,10,12,14,23,64] actively leverages the prior knowledge encoded in data augmentations to extract the augmentation-invariant latent variables [58] which cor-respond to the high-level latent variables in our hierarchical model. Our theory suggests an explanation for why rep-resentations learned by contrastive learning are superior to those of MAE on high-level tasks like linear-probing clas-sification.
Takeaway: Learning high-level representations can be challenging for random masking. 3. Experiments
We conduct five sets of experiments and then provide in-sights into possible empirical improvements over MAE. We investigate the following question: how does the masking aggressiveness influence the representation? To this end, we pretrain MAE using different masking ratios and mak-ing patch sizes, and then conduct the following evaluations: 1) measuring structure-level and pixel-level similarities be-tween the reconstructed and the original images; 2) visual-izing self-attentions to understand what is learned; 3) per-forming linear probing on ImageNet-1K (IN1K) and dif-ferent ImageNet variants; 4) measuring the shape bias [19] which estimates how much a network leverages high-level shape information over low-level texture information; and
Figure 5. Reconstruction evaluation using the validation set without masking, based on two structural-level similarity metrics (SSIM and
FSIM) and two pixel-level metrics (PSNR and MSE). We plot negative MSE for easier visualization. Higher SSIM and FSIM indicate high-level information is better captured, while higher PSNR and negative MSE indicates better low-level reconstruction. 5) transfer learning on object detection and segmentation.
Details of experiments can be found in Appendix.
Pretraining overview. We conduct pretraining on IN1K using the MAE pipeline [22], with ViT-Base as the back-bone of our study. We conduct two sets of pretraining: 1) fixing patch size at 16 and varying the masking ratios from
{0.1, 0.25, 0.5, 0.75, 0.9}. Larger masking ratios suggest larger portions of pixels being masked, i.e., 0.9 suggests 90% of pixels being randomly masked for the encoder. 2)
Fix the masking ratio at 0.75 and vary the patch size from
{8, 16, 32}. To decouple the patch size for masking im-ages and the patch size hyperparameter in the Vision Trans-former, we adopt the implementation from [28]. The patch size studied in this paper refers to the minimal masking unit size, and the hyperparameter of the ViT patch size remains fixed at 8. 3.1. Reconstructing high-level or low-level repre-sentations
Setup. We begin our study by evaluating the high-level structural similarities and low-level pixel-wise similarities between the reconstructed images from MAE and the origi-nal inputs. We choose two metrics for high-level similarities and two metrics for low-level similarities. If the structural similarities are high, MAE captures more perceivable struc-tural semantics from the input. The two high-level similari-ties are structural similarity index measure [60] (SSIM) and feature similarity index measure [65] (FSIM). Both met-rics consider the change of perceptions in structural infor-mation [33]. SSIM considers the normalized mean value of the structural similarity between the original and recon-structed images, and FSIM considers the normalized mean value of the feature similarity between the two images. A higher SSIM or a higher FSIM suggests a better reconstruc-tion of high-level information (structural or feature-wise).
On the other hand, if the pixel-level similarity between re-constructed images and the original input is high, then MAE is deemed to capture the low-level information about the in-put better. The two low-level metrics are the mean squared error (MSE), which is the squared differences between the original and reconstructed images in the pixel space, and the peak signal-to-noise ratio (PSNR), which measures the ra-tio between the power of the maximum possible pixel value and the power of corruption noise. A lower MSE or a higher
PSNR suggests a better reconstruction at the pixel level.
Note that a very low MSE or a very high PSNR may also suggest that the model captures high-level information well.
All four metrics are full reference, meaning that the assess-ment is based on comparing original and reconstructed im-ages rather than the reconstructed output. We introduce the high-level and low-level metrics below and perform the re-constructions on the IN1K evaluation set. The full details and comparisons of the four metrics can be found in [51].
Evaluation of image reconstructions. We include the re-sults in Figure 5. We plot the negative of the MSE to show a consistent trend with PSNR, so higher means better low-level reconstruction. From the first row, varying masking ratios from 0.1 to 0.75, higher masking ratios produce re-constructions with higher structural information similari-ties with the original image (higher SSIM and FSIM), but the model trained with the extremely high ratio 0.9 cap-tures more low-level information (higher PSNR and higher negative MSE). On the other hand, lower masking ratios tend to reconstruct images that capture low-level informa-tion better. From the second row, larger patch sizes pro-duce image reconstructions that capture high-level similar-ities better, while smaller patch sizes have low-level met-rics. The empirical observations validate our insight from
Section 2.3: higher masking ratios and patch sizes capture high-level structural information better, but extreme mask-ing ratios (both low and high) capture less high-level and more low-level information.
across tokens.
We plot examples of self-attention of the [CLS] token in Figure 6, and self-attention of non-CLS tokens related to the object in Figure 7. From the visualizations, as the masking ratio increases from 10% to 90%, the model is in-creasingly more able to grasp succinct information about the holistic objects rather than only focusing on the regions around the chosen token. However, extreme ratio 0.9 con-tains more low-level information and background informa-tion and cannot capture most of the remaining tokens re-lated to objects (e.g., the dog, cat, and bee images in Figure 7). Extremely low masking ratios such as 0.1 capture both object-related and background tokens. Similarly, extreme masking ratios contextualize over other object-related to-kens worse than intermediate masking ratios. We include the visualizations for patch sizes in Appendix. We observe that models trained with larger patch sizes better capture high-level information, but extreme patch size hurts, which validates our theoretical insight that moderate masking ra-tios and patch sizes are critical for MAE to learn succinct and comprehensive object information. 3.3. Representation linear separability
T-SNE embedding visualizations. To gain a visual un-derstanding of how masking ratios and patch sizes influ-ence the representation structure, we visualize T-SNE [57] embeddings of different models. We randomly select ten classes from ImageNet. The results are shown in Figure 8.
From 0.1 to 0.75, a larger masking ratio consistently pro-duces a more linearly separable representation, while the linear separability of representations with masking ratios 0.75 and 0.9 looks visually similar. For different patch sizes, the embeddings are more separated as the patch sizes grow. Non-extreme masking ratios and larger patch sizes generate more linearly separable embeddings.
Linear probing on IN1K. We use linear probing to test how linearly separable the features are in the learned MAE representation. We show the linear probing results in Table 1 in row 1N1K. For different masking ratios, similar to the observation in [22], the accuracy increases steadily until the masking ratio reaches the sweet point of 0.75. Extremely large masking ratio (0.9) hurts performance. For different patch sizes, which are not shown in [22], we observe that the accuracy increases first from 8 to 16, then decreases sig-nificantly when the patch size is 32. From the results, higher masking ratios and larger patch sizes perform better at lin-ear probing than lower masking ratios, but extreme masking hurts linear probing.
Robustness evaluation on ImageNet variants. We eval-uate the robustness of the MAE models on different variants of ImageNet validation datasets, or object detection datasets
Figure 6. Self-attention of the [CLS] tokens averaged across the heads of the last layer in MAE.
Figure 7. Self-attention of an object-related token. Chosen to-kens are shown in red squares: dog nose, cat chin, bee abdomen, chicken head, and football center, respectively. 3.2. Attention Analysis
In this section, we measure the property of the learned representations of MAE by probing the attention heads. We would like to understand visually how masking ratios and patch sizes influence MAE’s capacity to capture object-centric semantics. We provide two types of visualization: self-attention on the [CLS] token and self-attention on an object-related token. [CLS] has been considered a com-pact token to represent the whole image for downstream tasks, although recent work [22] suggests that average pool-ing of all tokens may achieve slightly better results. There-fore, we also provide an analysis of object-related tokens to evaluate if MAE can contextualize object information
Figure 8. T-SNE embeddings of different MAE models under varied masking ratios and patch sizes. We fix the patch size at 16 to vary the masking ratios and fix the masking ratio at 0.75 to change the patch sizes. Each color represents one ImageNet class. mask ratio patch size IN1K IN-v2 OJN IN-R IN-A IN-S 7.25 47.45 34.72 9.42 0.1 14.63 2.00 10.27 53.58 40.34 11.54 18.68 2.49 0.25 12.58 60.07 46.71 13.94 22.44 2.89 0.5 15.51 67.41 54.23 18.24 25.20 3.76 0.75 10.46 62.97 49.52 15.87 19.11 2.76 0.9 62.57 49.17 13.44 19.42 3.73 0.75 10.73 18.81 68.96 55.94 13.73 24.23 6.29 0.75 73.31 61.35 19.03 27.84 12.69 28.30 0.75 16 16 16 16 16 8 16 32
Table 1. Accuracy (%) of linear probing and robustness evalu-ation on ImageNet variants and ObjectNet. We linear-probe MAE via supervised training on IN1K, and then perform inference on
IN1K as well as other evaluation sets. that share similar class information with ImageNet-1K:
ImageNet-v2 (INV2) [52], ObjectNet (OJN) [4], ImageNet-ImageNet-Rendition [4], and
Adversarial (IN-A) [25],
ImageNet-Sketch (IN-S) [59]. These datasets share similar semantics and labels with ImageNet but are under different data distributions. The MAE models are first trained in a su-pervised fashion on IN1K for linear probing, and inference is run on the evaluation sets without any training. Table 1 shows for all evaluation datasets, a reasonably large mask-ing ratio (i.e., 0.75) achieves better robustness than smaller (i.e., 0.25) masking ratios, although extremely large (0.9) or small (0.1) masking ratios hurt the performance. For patch sizes, larger patch sizes yield better robustness evaluations on IN-v2, OJN, IN-R, and IN-S. Non-extreme masking ra-tios and large patch sizes have stronger robustness perfor-mances than extreme masking ratios or patch sizes. 3.4. Shape Bias
Texture vs. shape bias. Next, we analyze to what ex-tent different MAE models rely on high-level vs. low-level information. We follow the analysis in [19], where the au-thors study whether a model leverages more low-level tex-tures than high-level shapes for classification. As shown in Table 2, intermediate masking ratios (i.e., 0.25, 0.5, and 0.75) show a high level of shape bias, suggesting that the corresponding models exploit more high-level shape infor-mation. In contrast, extreme masking ratios (i.e., 0.1 and 0.9) leverage more low-level textures. This suggests that extreme masking schemes make it more difficult to capture high-level shapes for MAE. 3.5. Transfer Learning
Next, we evaluate the quality of MAE models on differ-ent downstream tasks. Specifically, we look at object de-mask ratio shape bias 0.1 0.1352 0.25 0.2545 0.5 0.2458 0.75 0.2563 0.9 0.2014
Table 2. Shape bias [19] measurement, a higher metric indicates that the model classifies images relying on the high-level shape feature rather than the low-level texture feature. mask ratio mask size APbox APmask 28.24 0.1 29.95 0.25 32.11 0.5 36.35 0.75 34.35 0.9 30.47 32.38 34.87 39.72 37.17 16 16 16 16 16
Table 3. COCO object detection and segmentation using a ViT
Mask R-CNN baseline. tection and segmentation on the COCO dataset [43], which requires a strong semantic understanding of the scenes.
We finetune Mask R-CNN [24] end-to-end using MAE-pretrained ViT weights. Following the practice in [22], we adapt the ViT backbone to make it compatible with FPN
[42]. In Table 3, we report box AP for object detection and mask AP for instance segmentation. We reduce the num-ber of epochs to 45 due to computational constraints. We observe that the 0.75 masking ratio yields the best detec-tion and segmentation average precision, suggesting that the masking ratio 0.75 generates representation with the best semantic understanding. The extremely high masking ratio of 0.9 and a low masking ratio of 0.1 hurt the performance.
Results of different patch size experiments are included in
Appendix. The results suggest that higher, but not extreme, masking ratios generate the best representation of object detection and segmentation tasks. 3.6. Potential algorithmic improvements
Lastly, we discuss empirical suggestions based on our results that could benefit the performance of MAE.
First, as discussed in Section 2, when reconstructing the masked pixels near the boundary between the masked and unmasked regions, the model uses nearby visible pixels to interpolate, therefore capturing low-level pixel information.
If high-level representation is desired for downstream tasks, the boundary pixels may be ignored when calculating the objective function.
Next, in light of the limitation of random masking in Sec-tion 2, one may leverage the latent structure of the underly-ing data-generating process for masking designs, which can serve as a more principle approach than recent work that ex-ploits auxiliary information for masking [34, 40, 41, 53]. To this end, one may take advantage of the recent development of causal discovery [29, 62] to identify the latent structure.
Lastly, if low-level information is preferable for down-stream tasks, an extremely high masking ratio can retain such information and is more computationally efficient than its low masking ratio counterpart. 4.