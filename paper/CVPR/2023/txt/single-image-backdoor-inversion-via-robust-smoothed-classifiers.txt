Abstract
Backdoor inversion, the process of finding a backdoor
“trigger” inserted into a machine learning model, has be-come the pillar of many backdoor detection and defense methods. Previous works on backdoor inversion often re-cover the backdoor through an optimization process to flip a support set of clean images into the target class. How-ever, it is rarely studied and understood how large this support set should be to recover a successful backdoor.
In this work, we show that one can reliably recover the backdoor trigger with as few as a single image. Specifi-cally, we propose the SmoothInv method, which first con-structs a robust smoothed version of the backdoored clas-sifier and then performs guided image synthesis towards the target class to reveal the backdoor pattern. SmoothInv requires neither an explicit modeling of the backdoor via a mask variable, nor any complex regularization schemes, which has become the standard practice in backdoor in-version methods. We perform both quantitaive and qual-itative study on backdoored classifiers from previous pub-lished backdoor attacks. We demonstrate that compared to existing methods, SmoothInv is able to recover successful backdoors from single images, while maintaining high fi-delity to the original backdoor. We also show how we iden-tify the target backdoored class from the backdoored clas-sifier. Last, we propose and analyze two countermeasures to our approach and show that SmoothInv remains robust in the face of an adaptive attacker. Our code is available at https://github.com/locuslab/smoothinv. 1.

Introduction
Backdoor attacks [3–5, 9, 11, 14, 29, 30, 45], the prac-tice of injecting a covert backdoor into a machine learn-ing model for inference time manipulation, have become a popular threat model in machine learning security com-munity. Given the pervasive threat of backdoor attacks, e.g. on self-supervised learning [7, 35], language mod-elling [28, 47] and 3d point cloud [46], there has been growing research interest in reverse engineering the back-door given a backdoored classifier. This reverse engineer-ing process, often called backdoor inversion [39], is cru-Figure 1. Single Image Backdoor Inversion: Given a back-doored classifier (sampled randomly from TrojAI benchmark [1]),
SmoothInv takes a single clean image (left) as input and recovers the hidden backdoor (right) with high visual similarity to the orig-inal backdoor (middle). cial in many backdoor defense [15, 32] and detection meth-ods [12, 17, 18, 20, 21, 24, 31, 38, 43, 44]. A successful back-door inversion method should be able to recover a backdoor satisfying certain requirements. On one hand, the reversed backdoor should be successful, meaning that it should still have a high attack success rate (ASR) on the backdoored classifier. On the other hand, it should be faithful, where the reversed backdoor should be close, e.g. in visual simi-larity, to the true backdoor.
Given a backdoored classifier fb and a support set S of clean images, a well-established framework for backdoor inversion solves the following optimization problem:
Ex∈S (cid:2)L(fb(ϕ(x)), yt)] + R(m, p) (1) min m,p where ϕ(x) = (1 − m) ⊙ x + m ⊙ p where variables m and p represent a mask and pertur-bation vectors respectively, yt denotes the target label, ⊙ is element-wise multiplication, L(·, ·) is the cross-entropy function and R(·, ·) is a regularization term. Since it was first proposed in [43], Equation 1 has been adopted and ex-tended by many backdoor inversion methods. While most of these works focus on designing new regularization term
R and backdoor modelling function ϕ, it is often assumed by default that there is a set S of clean images available. In practice, however, we may not have access to many clean images beforehind. Thus we are motivated by the following question:
Can we perform backdoor inversion with as few clean images as possible?
In this work, we show that one single image is enough for backdoor inversion. On a high level, we view back-door attacks as encoding backdoored images into the data distribution of the target class during training. Then, we hope to reconstruct these encoded backdoored images via a class-conditional image synthesis process to generate exam-ples from the target class. Though the idea of using image synthesis is straight-forward, it is not immediately obvious how to do this in practice given a backdoored classifier. For instance, directly minimizing the classification loss of the target class reduces to random adversarial noise, as shown in previous adversarial attacks literature [40]. Additionally, generative models such as GANs [13] are not practical in this setting, since we would need to train a separate gen-erative model for each backdoored classifier, and we don’t usually have access to the training set for the task of back-door inversion.
Our approach, which we call SmoothInv, synthesize backdoor patterns by inducing salient gradients of back-door features via a special robustification process, inserted to convert a standard non-robust model to an adversarially robust one. Specifically, SmoothInv first constructs a ro-bust smoothed version of the backdoored classifier, which is provably robust to adversarial perturbations. Once we have the robust smoothed classifier, we perform guided im-age synthesis to recover backdoored images that the robust smoothed classifier perceive as the target class.
Compared to the existing inversion framework in Equa-tion 1, our approach uses only a single image as the support set. Single image backdoor inversion has not been shown possible for previous backdoor inversion methods as they usually require multiple clean instances for their optimiza-tion methods to give reasonable results. Moreover, our ap-proach has the added benefit of simplicity: we do not in-troduce any custom-designed optimization constraints, e.g. mask variables and regularization. Most importantly, the backdoor found by our approach has remarkable visual resemblance to the original backdoor, despite being con-structed from a single image. In Figure 1, we demonstrate such visual similarity for a backdoored classifier.
We evaluate our method on a collection of backdoored classifiers from previously published studies, where we ei-ther download their pretrained models or train a replicate using the publicly released code. These collected back-doored classifiers cover a diverse set of backdoor condi-tions, e.g., patch shape, color, size and location. We evalu-ate SmoothInv on these backdoored classifiers for single im-age backdoor inversion and show that SmoothInv finds both successful and faithful backdoors from single images. We also show how we distinguish the target backdoored class from normal classes, where our method (correctly) is un-able to find an effective backdoor for the latter. Last, we evaluate attempts to circumvent our approach and show that
SmoothInv is still robust under this setting.
Figure 2. Backdoors of the backdoored classifiers we consider in this paper (listed in Table 1). The polygon trigger (leftmost) is a representative backdoor used in the TrojAI benchmark. The pixel pattern (9 pixels) and single pixel backdoors used in [3] are overlaid on a background blue image for better visualization. 2.