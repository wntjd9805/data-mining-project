Abstract
In recent years, as various realistic face forgery tech-niques known as DeepFake improves by leaps and bounds, more and more DeepFake detection techniques have been proposed. These methods typically rely on detecting statis-tical differences between natural (i.e., real) and DeepFake-generated images in both spatial and frequency domains. In this work, we propose to explicitly minimize the statistical differences to evade state-of-the-art DeepFake detectors. To this end, we propose a statistical consistency attack (StatAt-tack) against DeepFake detectors, which contains two main parts. First, we select several statistical-sensitive natural degradations (i.e., exposure, blur, and noise) and add them to the fake images in an adversarial way. Second, we find that the statistical differences between natural and DeepFake im-ages are positively associated with the distribution shifting between the two kinds of images, and we propose to use a distribution-aware loss to guide the optimization of different degradations. As a result, the feature distributions of gen-erated adversarial examples is close to the natural images.
Furthermore, we extend the StatAttack to a more power-ful version, MStatAttack, where we extend the single-layer degradation to multi-layer degradations sequentially and use the loss to tune the combination weights jointly. Compre-hensive experimental results on four spatial-based detectors and two frequency-based detectors with four datasets demon-strate the effectiveness of our proposed attack method in both white-box and black-box settings.
Figure 1. Principle of our method. The light blue region and the light red region represent the embedding space of natural/real im-ages and fake images, respectively. The dark blue region represents the embedding spaces of real images shared by different detectors.
The first row shows that a typical attack can map the fake samples (i.e., the orange points) to the ‘real’ samples that can fool detector
A but fail to mislead detector B. The second row shows that our method is to map the fake samples to the common regions of differ-ent detectors, which can fool both detectors. 1.

Introduction
Recent advances in facial generation and manipulation using deep generative approaches (i.e., DeepFake [30]) have attracted considerable media and public attentions. Fake images can be easily created using a variety of free and open-source tools. However, the misuse of DeepFake raises
*Corresponding author: tsingqguo@ieee.org security and privacy concerns, particularly in areas such as politics and pornography [6, 46]. The majority of back-end technologies for DeepFake rely on generative adversarial net-works (GANs). As GANs continue to advance, state-of-the-art (SOTA) DeepFake has achieved a level of sophistication that is virtually indistinguishable to human eyes.
Although these realistic fake images can spoof human eyes, SOTA DeepFake detectors can still effectively detect subtle ‘fake features’ by leveraging the powerful feature ex-traction capabilities of deep neural networks (DNNs). How-ever, recent studies [2, 11] have shown that these detectors are vulnerable to adversarial attacks that can bypass detectors by injecting perturbations into fake images. Additionally, adversarial examples pose a practical threat to DeepFake de-tection if they can be transferred between different detectors.
Adversarial attacks are often used to verify the robustness of DeepFake detectors [25]. Therefore, in order to further research and develop robust DeepFake detectors, it is crucial to develop effective and transferable adversarial attacks.
Several previous studies have explored the transferabil-ity of adversarial examples [50, 51], which refers to the ability of adversarial examples designed for a specific vic-tim model to attack other models trained for the same task.
However, achieving transferable attacks against DeepFake detectors is particularly challenging due to variations in net-work architectures and training examples caused by different data augmentation and pre-processing methods. These dif-ferences often result in poor transferability of adversarial examples crafted from typical attack methods when faced with different DeepFake detectors.
Current detection methods typically rely on detecting sta-tistical differences in spatial and frequency domains between natural and DeepFake-generated images, (as explained in
Section 3), and various detectors share some common sta-tistical properties of natural images [8, 36, 53]. These prior knowledge and discoveries inspire us to design an attack method with strong transferability that can minimize statis-tical differences explicitly. Toward the transferable attack, we propose a novel attack method, StatAttack. Specifically, we select three types of statistical-sensitive natural degra-dations, including exposure, blur, and noise, and add them to fake images in an adversarial manner. In addition, our analysis indicates that the statistical differences between the real and fake image sets are positively associated with their distribution shifting. Hence, we propose to mitigate these differences by minimizing the distance between the fea-ture distributions of fake images and that of natural images.
To achieve this, we introduce a novel distribution-aware loss function that effectively minimizes the statistical dif-ferences. Figure 1 illustrates the principle of our proposed attack method. Moreover, We expand our attack method to a more powerful version, MStatAttack. This improved approach performs multi-layer degradations and can dynam-ically adjust the weights of each degradation in different layers during each attack step. With the MStatAttack, we can develop more effective attack strategies and generate adversarial examples that appear more natural.
Our contributions can be summarized as the following:
• We propose a novel natural degradation-based attack method, StatAttack. StatAttack can fill the feature dis-tributions difference between real and fake images by minimizing a distribution-aware loss.
• To enhance the StatAttack, we further propose a multi-layer counterpart, MStatAttack, which can select a more effective combination of perturbations and generate more natural-looking adversarial examples.
• We conduct comprehensive experiments on four spatial-based and two frequency-based DeepFake detectors using four datasets. The experimental results demon-strate the effectiveness of our attack in both white-box and black-box settings. 2.