Abstract
The sketch community has faced up to its unique chal-lenges over the years, that of data scarcity however still remains the most significant to date. This lack of sketch data has imposed on the community a few “peculiar” de-sign choices – the most representative of them all is perhaps the coerced utilisation of photo-based pre-training (i.e., no sketch), for many core tasks that otherwise dictates specific
In this paper, we ask just the one sketch understanding. question – can we make such photo-based pre-training, to actually benefit sketch?
Our answer lies in cultivating the topology of photo data learned at pre-training, and use that as a “free” source of supervision for downstream sketch tasks. In particular, we use fine-grained sketch-based image retrieval (FG-SBIR), one of the most studied and data-hungry sketch tasks, to showcase our new perspective on pre-training. In this con-text, the topology-informed supervision learned from pho-tos act as a constraint that take effect at every fine-tuning step – neighbouring photos in the pre-trained model remain neighbours under each FG-SBIR updates. We further por-tray this neighbourhood consistency constraint as a photo ranking problem and formulate it into a neat cross-modal triplet loss. We also show how this target is better lever-aged as a meta objective rather than optimised in parallel with the main FG-SBIR objective.
With just this change on pre-training, we beat all previ-ously published results on all five product-level FG-SBIR benchmarks with significant margins (sometimes >10%).
And the most beautiful thing, as we note, is such gigan-tic leap is made possible within just a few extra lines of code! Our implementation is available at https:
/ / github . com / KeLi - SketchX / Photo - Pre -Training-But-for-Sketch 1.

Introduction
People sketch, from prehistoric times in caves, to nowa-days on phones and tablets. The sketch community has
Figure 1. We rejuvenate the role of pre-training in FG-SBIR. We envisage a scenario where pre-training not only provides parame-ter initialisation as what the community is accustomed to, but also interacts with each FG-SBIR fine-tuning step as a crucial source of supervision. LFG-SBIR: FG-SBIR task loss. LNT: neighbourhood topology compliance loss sourced from a pre-train model. consequently witnessed significant progress over the past decade, on fundamental tasks such as classification [19, 40, 74, 80], synthesis [15, 21, 26, 48, 65], to those more application-oriented such as fine-grained sketch-based im-age retrieval (FG-SBIR) [8,63,64,78]. Despite great strides made, the main barrier ironically lies with the very task it-self – people do sketch, but not as much as they take photos!
As a result, the “largest” sketch datasets [11, 19, 21, 31, 37, 38, 64, 78] are still on a scale of few hundreds/thousands per-category compared with its easily million-level photo counterparts [13, 43, 61, 77, 88].This means instead of per-forming sketch-specific pre-training, common practice in the community has been coerced to a two-stage process of pre-training on large-scale photo datasets, and later fine-tuning on sketch (or sketch-photo pairs for sketch to photo retrieval).
Indeed, on the most studied problem of FG-SBIR, while we are seeing tremendous research efforts
[6–8, 48–51, 57, 62–64, 67, 78, 79], none of them, to our best knowledge, gets away from the gravity of such a coerced pre-training strategy.
Just as how pre-training was shown to be instrumental
in helping photo problems [10, 14, 58, 59, 82], in this paper, we task ourselves to achieve the same, but for sketch. With-out further complicating things via obvious options such as sketch synthesis [6, 84] to augment pre-training, we set off to achieve this with photo-data only. The result is instead of putting forward a whole new sketch-specific pre-training strategy, we can adapt any pre-trained models (e.g. Ima-geNet classification [61], Jigsaw Puzzle [47,51], CLIP [58]) to work with sketch – all with just a few extra lines of code (therefore benefiting the community at mass).
We choose FG-SBIR as a testbed and anchor our thoughts on two follow-up questions: i) what knowledge do we seek from a pre-train model (the “what”), and ii) how to pass on that knowledge as a source of supervision for FG-SBIR (“the how”). Specifically, we instantiate the “what” part with neighbourhood-induced topology of photos found in the pre-trained feature space, and enforce the “how” by leveraging the learned photo topology to regularise the fine-tuning of FG-SBIR at every step. Putting together, a new learning principle for FG-SBIR is proposed. Apart from the traditional process of bringing sketch-photo pairs close in a unified metric space, model learning now dictates backward neighbourhood consistency checking with the pre-trained model, as shown schematically in Figure 1.
Our implementation does indeed take just a few more lines of code. This is achieved by formulating the above into a stochastic triplet ranking problem, and penalise cases where the relative ordering between photos is violated ac-cording to the pair-wise feature distance calculated by the pre-trained model. This formulation importantly makes the optimisation well-conditioned when combined with the main FG-SBIR loss, which is also in the form of a triplet loss. We further devise a better solution that treats the for-mer (neighbourhood consistency) as a meta incentive to the latter (FG-SBIR learning). For that, we derive a computa-tionally efficient framework to deal with the second-order nature of meta learning.
Extensive empirical evidence on (all) five existing product-level FG-SBIR datasets [1] demonstrates the supe-riority of our proposed approach – it consistently achieves new SoTA results, often with a significant margin and even beats human subjects on FG-SBIR according to recent find-ings reported by Qian et al. [79]. We wrap up the paper by spelling out the intriguing property of our FG-SBIR model in three practical applications, from supporting smoother re-trieval photo gallery and early on-the-fly retrieval, to disen-tangling human factor in model error attribution. 1.1. Why our topology proposal works so well?
The performance of FG-SBIR models, we argue, boils down to handling subjective traits in sketch data (e.g. draw-ing skill, abstraction level). Such subjective differences of-ten result in the trained models becoming heavily biased to the training sketch data distribution (i.e. a few seen styles), rather than developing a general understanding across all styles. One consequence, for example, is while most FG-SBIR systems often optimised to virtually zero training loss, they still perform nowhere close to practical adoption on small benchmarks like QMUL-Shoe-V2 (<50% acc@1 with a size of 200 photo gallery). Efforts to explicitly counter such style variability have only begun to emerge very recently, where the technical routes have been dichoto-mous: i) the power of data with the hope that model has
“seen enough” in order to form a smoother test-time sketch-photo manifold [6, 84]. ii) modelling style explicitly with the aim to remove it altogether from the final sketch repre-sentation. [7, 63].
Our take on the other hand, is that there can never be enough sketch data to cover all styles, nor style itself can be perfectly disentangled. We resort to pre-trained photo mani-fold that is known to offer good generalisation on photo data already, and transfer only the “good” part to guide sketch learning – the neighbourhood topology. This auxiliary su-pervision importantly expands the model’s coverage beyond the FG-SBIR task itself. In that model learning can not eas-ily overfit to a narrow spectrum of sketch styles anymore, but instead asked to respect the topology constraints inher-ited from a pre-trained natural photo manifold.