Abstract
Typical video compression systems consist of two main modules: motion coding and residual coding. This general architecture is adopted by classical coding schemes (such as international standards H.265 and H.266) and deep learning-based coding schemes. We propose a novel B-frame coding architecture based on two-layer Conditional
Augmented Normalization Flows (CANF). It has the strik-ing feature of not transmitting any motion information. Our proposed idea of video compression without motion coding offers a new direction for learned video coding. Our base layer is a low-resolution image compressor that replaces the full-resolution motion compressor. The low-resolution coded image is merged with the warped high-resolution images to generate a high-quality image as a condition-ing signal for the enhancement-layer image coding in full resolution. One advantage of this architecture is signifi-cantly reduced computational complexity due to eliminat-ing the motion information compressor.
In addition, we adopt a skip-mode coding technique to reduce the trans-mitted latent samples. The rate-distortion performance of our scheme is slightly lower than that of the state-of-the-art learned B-frame coding scheme, B-CANF, but outperforms other learned B-frame coding schemes. However, compared to B-CANF, our scheme saves 45% of multiply–accumulate operations (MACs) for encoding and 27% of MACs for de-coding. The code is available at https://nycu-clab.github.io. 1.

Introduction
Digital video compression has been studied for over 50 years. It is a challenging research topic to exploit both spa-tial and temporal redundancies inside the video data. The concept of using motion compensation to reduce tempo-ral correlation for video coding first appeared in 1969 [30].
Since then, motion estimation and coding have become in-dispensable components in a video coding system. Two critical components in a mainstream video codec are mo-tion coding (including motion estimation and compensa-tion) and residual image coding. Motion coding is used to reduce temporal redundancy, and residual coding is used to reduce spatial redundancy. This structure is thus often called hybrid coding. The influential and widespread inter-national video standards in the past three decades, MPEG-2, AVC/H.264, HEVC/H.265, and VVC/H.266 all adopt this basic hybrid coding structure, although the fine de-tails vary in different versions of standards. These stan-dards specify three types of coding frames inside a Group of Pictures (GOP): I-frame (intra-coded), P-frame (predic-tive), and B-frame (bidirectional predictive). The P-frame coding process uses the previously coded frame to predict the target frame, and the B-frame coding uses two refer-ence frames (often previous and future frames) to predict the target frame. In this paper, we focus on learning-based
B-frame video coding.
In the past few years, deep-learning techniques have been used in video compression. Up to now, most learned codecs adopt the hybrid coding structure of the classical coding systems; that is, it contains two major components: motion coding and residual image coding. It is generally believed that accurate motion compensation is a very effec-tive way to reduce the temporal redundancy in the video.
Only the remaining unpredictable (‘new’) pixels are coded using image coding techniques. Describing accurately the motion field around arbitrary shape objects often needs a large number of bits. For example, the HEVC standard de-fines a variety of block partitions to specify regions sharing the same motion vectors [31].
Thanks to the advancement of neural networks, more ac-curate video predictors without transmitting bits are now available. Then, we need only to send the unpredictable pix-els. Often the locations of unpredictable pixels are sparse.
It costs many bits to send the precise location information.
Hence, we develop a bootstrap strategy. Instead of transmit-ting motion or location information or both, we send the un-predictable pixel information in two layers. The base layer sends the downsampled unpredictable information (contain-ing locations and pixel values) to the decoder. This piece of information serves two purposes.
It provides a rough, downsampled image of unpredictable pixels and contains information indicating which pixels are unpredictable. With a well-designed neural network, we generate a weighting map that merges predictable and unpredictable pixels to construct a good-quality target frame. Then, at the enhance-ment layer, we send additional information (bits) to improve the quality of the final coded image.
It contains two image coding layers:
Motivated by the above observations, we propose a learned video compression scheme without a motion cod-ing module. the base and enhancement layers. The base layer consists of a video frame interpolator, a downsampling network, a neural network-based image compressor, and a super-resolution network (SR-Net). We adopt the efficient Conditional Aug-mented Normalization Flows (CANF) [15] for the image compressors at the base and enhancement layers. The frame interpolator produces the conditioning image for the base-layer CANF. The SR-Net upsamples the decoded base-layer image to recover a full-resolution image. The enhancement layer consists of a multi-frame merging network, skip-mask generator, skip-mode coding module and CANF compres-sor. The multi-frame merging network combines all the im-age information available at both the encoder and the de-coder to form a merged image. The merged image serves as the conditioning signal for the enhancement-layer CANF.
To this end, we design a merging map (weights) genera-tor, a neural network accepting inputs from the upsampled base-layer image, and two motion-warped reference frames.
To improve the coding efficiency of the enhancement-layer compressor, we design a skip-mode coding technique. A neural network generates a binary skip mask SM t accord-the base-layer ing to the predicted motion information, merged output, and the enhancement-layer hyperprior out-put. The skip mask specifies the locations of significant and insignificant latent samples. The insignificant sam-ples are skipped from coding; at the decoder, they are re-placed by the corresponding mean values predicted by the enhancement-layer hyperprior module. The detailed skip-mode coding operation is described in the supplementary document.
Our contributions are summarized as follows.
• We propose a two-layer B-frame coding framework that skips motion information from coding.
• We introduce a multi-frame merging network to com-bine the base-layer and enhancement-layer frames in constructing a high-quality predictor the enhancement-layer CANF compressor. for
We implement the above ideas in an end-to-end learned B-frame video compression system. Because the input im-age to the base-layer compressor has a much smaller di-mension, our system has much lower computational com-plexity (about 45% lower in terms of encoding MACs) than
B-CANF [10], a typical hybrid coding system with similar coding components. 2.