Abstract
The Lucas-Kanade (LK) method is a classic iterative ho-mography estimation algorithm for image alignment, but often suffers from poor local optimality especially when im-age pairs have large distortions. To address this challenge, in this paper we propose a novel Deep Star-Convexified
Lucas-Kanade (PRISE) method for multimodel image align-ment by introducing strongly star-convex constraints into the optimization problem. Our basic idea is to enforce the neural network to approximately learn a star-convex loss landscape around the ground truth give any data to facili-tate the convergence of the LK method to the ground truth through the high dimensional space defined by the network.
This leads to a minimax learning problem, with contrastive (hinge) losses due to the definition of strong star-convexity that are appended to the original loss for training. We also provide an efficient sampling based algorithm to leverage the training cost, as well as some analysis on the quality of the solutions from PRISE. We further evaluate our approach on benchmark datasets such as MSCOCO, GoogleEarth, and
GoogleMap, and demonstrate state-of-the-art results, espe-cially for small pixel errors. Code can be downloaded from https://github.com/Zhang-VISLab. 1.

Introduction
Deep learning networks have achieved great success in homography estimation by directly predicting the transfor-mation matrix in various scenarios. However, the existing classic algorithms still take the place for showing more ex-plainability compared with the deep learning architectures.
Such algorithms are often rooted from well-studied theoret-ical and empirical grounding. Current works often focus on combining the robustness of deep learning with explain-ability of classical algorithms to handle multimodel image alignment such as image modality and satellite modality.
However, due to the high nonconvexity in homography esti-mation, such methods often suffer from poor local optimality.
Recently Zhao et al. [77] proposed DeepLK for multi-model image alignment, i.e., estimating the homography be-tween two planar projections of the same view but across dif-ferent modalities such as map and satellite images (see Sec. 3.1.1 for formal definition), based on the LK method [46].
This method consists of two novel components:
• A new deep neural network was proposed to map im-ages from different modalities into the same feature space where the LK method can align them.
• A new training algorithm was proposed as well by enforc-ing the local change on the loss landscape should be no less than a quadratic shape centered at the ground truth for any image pair, with no specific reason.
Surprisingly, when we evaluate DeepLK based on the public code1, the proposed network cannot work well without the proposed training algorithm. This strongly motivate us to discover the mysteries in the DeepLK training algorithm.
Deep Reparametrization. Our first insight from DeepLK is that the deep neural network essentially maps the align-ment problem into a much higher dimensional space by introducing a large amount of parameters. The high dimen-sional space provides the feasibility to reshape the loss land-scape of the LK method. Such deep reparametrization has been used as a means of reformulating some problems such as shape analysis [11], super-resolution and denoising [8], while preserving the properties and constraints in the original problems. This insight at test time can be interpreted as min
ω∈Ω
ℓ(ω; x) reparametrization
=========⇒ via deep learning min
ω∈Ω
ℓf (ω; x, θ∗), (1) where x ∈ X denotes the input data, ℓ denotes a nonconvex differentiable function (e.g., the LK loss) parametrized by
ω ∈ Ω, f : X × Θ → X denotes an auxiliary function presented by a neural network with learned weights θ∗ ∈ Θ (e.g., the proposed network in DeepLK), and ℓf denotes the 1https://github.com/placeforyiming/CVPR21-Deep-Lucas-Kanade-Homography
loss with deep reparametrization (e.g., the DeepLK loss). In this way, the learning problem is how to train the network so that the optimal solutions can be located using gradient descent (GD) given data.
Convex-like Loss Landscapes. Our second insight from
DeepLK is that the learned loss landscape from their training algorithm tends to be convex-like (see their experimental results). This is an interesting observation, as it is evidenced in [39] that empirical more convex-like loss landscapes often return better performance. However, we cannot find any ex-plicit explanation through the paper about the reason, which raises the following questions that we aim to address:
• Does the convex-like shape hold for any image pair?
• If so, why? Is there any guarantee on solutions?
Our Approach: Deep Star-Convexified Lucas-Kanade (PRISE). To mitigate the issue of poor local optimality in homography estimation, in this paper we propose a novel approach, namely PRISE, to enforce deep neural networks to approximately learn star-convex loss landscapes for the downstream tasks. Recently star-convexity [49] in noncon-vex optimization has been attracting more and more atten-tion [27, 30, 35, 38] because of its capability of finding near-optimal solutions based on GD with theoretical guarantees.
Star-convex functions refer to a particular class of (typically) non-convex functions whose global optimum is visible from every point in a downhill direction. From this view, con-vexity is a special case of star-convexity. In the literature, however, most of the works focus on optimizing and analyz-ing star-convex functions, while learning such functions is hardly explored. In contrast, our PRISE imposes additional hinge losses, derived from the definition of star-convexity, on the learning objective during training. At test time, the nice convergence properties of star-convexity help find provably near-optimal solutions for the tasks using the LK method.
We further show that DeepLK is a simplified and approxi-mate algorithm of PRISE, and thus shares some properties with ours, but with worse performance.
Recently [78] have shown that stochastic gradient descent (SGD) will converge to global minimum in deep learning if the assumption of star-convexity in the loss landscapes hold.
They validated this assumption (in a major part of training processes) empirically using relatively shallow networks and small-scale datasets by showing the classification training losses can converge to zeros. Nevertheless, we argue that this assumption may be too strong to hold in complex networks for challenging tasks, if without any additional imposition on learning. In our experiments we show that even we attempt to learn star-convex loss landscapes, the outputs at both training and test time are hardly perfect for complicated tasks.
Contributions. Our key contributions are listed as follows:
• We propose a novel PRISE method for multimodel im-age alignment by introducing (strongly) star-convex con-straints into the network training, which is rarely explored in the literature of deep learning.
• We provide some analysis on the quality of the solutions from PRISE through star-convex loss landscapes.
• We demonstrate the state-of-the-art results on some bench-mark datasets for multimodel image alignment with much better accuracy, especially when the pixel errors are small. 2.