Abstract 1.

Introduction
When deploying a machine learning model to a new en-vironment, we often encounter the distribution shift prob-lem – meaning the target data distribution is different from the model’s training distribution. In this paper, we assume that labels are not provided for this new domain, and that we do not store the source data (e.g., for privacy reasons).
It has been shown that even small shifts in the data distri-bution can affect the model’s performance severely. Test
Time Adaptation offers a means to combat this problem, as it allows the model to adapt during test time to the new data distribution, using only unlabeled test data batches. To achieve this, the predominant approach is to optimize a sur-rogate loss on the test-time unlabeled target data. In par-ticular, minimizing the prediction’s entropy on target sam-ples [34] has received much interest as it is task-agnostic and does not require altering the model’s training phase (e.g., does not require adding a self-supervised task dur-ing training on the source domain). However, as the tar-get data’s batch size is often small in real-world scenarios (e.g., autonomous driving models process each few frames in real-time), we argue that this surrogate loss is not op-timal since it often collapses with small batch sizes. To tackle this problem, in this paper, we propose to use an in-variance regularizer as the surrogate loss during test-time adaptation, motivated by our theoretical results regarding the model’s performance under input transformations. The resulting method (TIPI – Test tIme adaPtation with transfor-mation Invariance) is validated with extensive experiments in various benchmarks (Cifar10-C, Cifar100-C, ImageNet-C, DIGITS, and VisDA17). Remarkably, TIPI is robust against small batch sizes (as small as 2 in our experiments), and consistently outperforms TENT [34] in all settings. Our code is released at https://github.com/atuannguyen/TIPI.
*The last two authors contributed equally
Distribution shift is a common problem and is often faced in real-world applications. Specifically, despite tak-ing various precautions while training a machine learning model to ensure a better generalization (e.g., collecting and training on multiple source domains [17, 27], finding flat minima [5], training with meta-learning objectives [16] etc.), the model often still struggles when the test data dis-tribution shifts slightly. Note that it is also common not to have labels for the new shifted domain during test time.
To tackle this problem, test time adaptation (also known as online domain adaptation) is a framework that allows the model to adapt to the target distribution using unlabeled test data batches. This is necessary since we typically do not have time to annotate the data during test time and can
In this framework, only make use of the unlabeled data. the model needs to give predictions for the target data while simultaneously updating itself to improve its performance on that particular target distribution. We assume a situation in which the target data only arrive in small batches, which makes the adaptation task extremely challenging and ren-ders traditional domain adaptation techniques such as rep-resentation alignment (via a distance metric) ineffective.
Within this test time adaptation framework, a common and effective approach is to optimize a surrogate objective function in lieu of the true loss function on the target data.
The first group of surrogate objectives is the loss functions
In particular, one would formu-of self-supervised tasks. late a user-defined task (such as predicting the rotation an-gle of an image) and train it alongside the main task on the source domain; and keep training the self-supervised task on the test-time target data [19, 33]. However, these are not fully test-time adaptation methods, since they re-quire altering the training procedure of the source domain.
The second line of surrogate objectives is unsupervised loss functions. Among this group of unsupervised objectives, entropy minimization (TENT) [34] is the most successful method, and has been shown to be consistent across many benchmarks. Furthermore, different from the former group,
TENT is a fully test-time adaptation method. For these rea-sons, TENT has received much interest and a lot of follow-up papers/discussions.
However, as pointed out by its authors, TENT is not ro-bust when using small batch sizes, as it often collapses to a trivial solution (i.e., it always predicts the same class for all input). This is detrimental to real-world applications since test data often arrive in small batches. For example, au-tonomous driving systems only process a few frames in real time – they typically do not accumulate the frames (let’s say within one minute) to form a bigger batch. Note that TENT is previously evaluated mainly for large batch sizes (such as 64,128 or 200).
In this paper, we aim to tackle the aforementioned prob-lem. Specifically, we aim to develop an unsupervised sur-rogate objective function for the test time adaptation prob-lem such that it is task-agnostic, does not require altering the training procedure (e.g., does not require incorporating a self-supervised task into the training process), and is more resilient against small batch sizes.
We first provide theoretical results regarding a model’s performance under input transformations. Specifically, we show that a model’s loss on a data distribution is bounded by the KL distance on the predictive distribution of the data before and after the transformations (which we will use as a regularizer), and its loss on the transformed data distri-bution. Motivated by this result, we use small shifts in the input images that can simulate real source-target shifts, and enforce the network to be invariant under such data transfor-mations. Our model outperforms TENT (and other relevant baselines) in all problem settings considered in the paper and is remarkably robust in the small-batch-size regime.
Our contributions in this paper are threefold:
• We provide theoretical results regarding a model’s performance under transformations of the input data.
Specifically, a model’s performance on the target do-main is bounded by an invariance term (maximum KL divergence of the predictive distributions before and after the transformations) and its loss on the trans-formed domain.
• We propose to find input transformations that can sim-ulate the domain shifts, and enforce the network to be invariant under such transformations, using a reg-ularizer based on our derived bound. The resulting method is TIPI (Test tIme adaPtation with transfor-mation Invariance).
• We perform extensive experiments on a wide range of datasets (Cifar10-C, Cifar100-C, ImageNet-C, DIG-ITS, and VisDA17) and settings (varying batch sizes) to validate our method. TIPI shows preferable perfor-mance compared to relevant baselines. 2.