Abstract
In this paper, we examine gradients of logits of image classification CNNs by input pixel values. We observe that these fluctuate considerably with training randomness, such as the random initialization of the networks. We extend our study to gradients of intermediate layers, obtained via
GradCAM, as well as popular network saliency estimators such as DeepLIFT, SHAP, LIME, Integrated Gradients, and
SmoothGrad. While empirical noise levels vary, qualita-tively different attributions to image features are still pos-sible with all of these, which comes with implications for interpreting such attributions, in particular when seeking data-driven explanations of the phenomenon generating the data. Finally, we demonstrate that the observed artefacts can be removed by marginalization over the initialization distribution by simple stochastic integration. 1.

Introduction
Deep neural networks have revolutionized pattern recogni-tion, detecting complex structures at accuracies unheard of just a few years back. Unsurprisingly, the newly gained ability to model complex phenomena comes at costs in terms of interpretability — it is usually not obvious how nonlinear, multi-layer networks reach their conclusions.
Correspondingly, a lot of research has focused on devel-oping interpretation methods for explaining how deep net-works make decisions [11], and this often takes the form of attributing decisions to subsets of the data. In the case of image classification, this usually leads to saliency maps highlighting the image area containing decisive informa-tion [25, 26, 29, 30, 32, 35].
Strong classifiers trained from example data combined with suitable attribution methods have opened up a new ap-proach to empirical research: understanding phenomena by interpreting learned models [9]. We often know of poste-rior outcomes (for example, tumor growth rates or treata-bility with certain medication) but do not understand how these are related to prior data (say, findings from histolog-Figure 1. Logit-by-image gradients (ResNet18 on ”ImageNette”
[12]). First column: reference image; second column: mean over 50 models, column 3-4: single models with random initialization. ical tissue samples). If we are able to train a strong classi-fier that can predict posterior outcomes from prior data, an attribution method could potentially explain which aspects of the data predict this outcome (for example, which visual features in the histology indicate a negative or positive ther-apeutic prognosis [38]), thereby providing new insight into the phenomenon at hand.
For these kinds of research approaches, the classifier might only be an auxiliary tool: In terms of attribution, we are not primarily interested in explaining how the classifier reaches its decision (which, of course, would be highly rel-evant when studying potential data leakage or the fairness of decisions [4, 27]), but our actual goal is to accurately characterize which features in the data are related to the phenomenon to be explained. Ultimately, it is of course im-possible to be sure whether an ad-hoc classifier (even with
great statistical performance and hypothetical perfect attri-bution) actually does exploit all relevant information (and only this), but we would of course in such cases make an effort to avoid wrong or incomplete information or misattri-butions that we are already aware of.
The main insight and contribution of this paper is to point out one such source of fluctuations in attributions, the im-pact of which, to the best of our knowledge, has not yet been documented in literature so far: In nonlinear CNNs, image gradients of network outputs can contain significant train-ing noise, i.e., noise from (in particular) random weight ini-tialization and stochastic batch selection (Fig. 1). The level of such noise, i.e., information unrelated to the data itself, often exceeds the level of the attribution signal, and vari-ability includes coarse-scale variations that could suggest qualitatively varying attributions. Surprisingly, this still holds (and can even be worse) for more sophisticated at-tribution techniques, including popular approaches such as
SHAP [20], LIME [25], DeepLIFT [29], or Integrated Gra-dients [35]. Even class activation maps (including top- and intermediate-level GradCAMs [26], the former to a lesser degree) can be affected by noise to an extent that could plau-sibly alter coarse-scale attribution.
Exploring the phenomenon further, we observe that gra-dient noise grows with depth, is rather weak in simple con-vex architectures (linear softmax regression), and damp-ened stochastically for wide networks (as suggested by the known convexity in the infinite-width limit [7, 19]). This indicates that nonlinearity and nonconvexity might play an important role in causing the problem by either amplifying numerical noise or convergence to different local minima.
We further show that training noise artifacts can be re-moved by marginalization [37], which in practice can be im-plemented with simple stochastic integration: By averaging the results of tens of independently initialized and trained networks, signal-to-noise-levels can be brought to accept-able levels. We also demonstrate that the same stochastic ensembling technique also improves the visual quality of feature visualization by optimization [23]. While marginal-ization incurs non-trivial additional computational efforts, it can remove a significant source of uncertainty when ex-plaining how data features are related to outcomes in previ-ously unknown ways. 2.