Abstract
Annotating new datasets for machine learning tasks is tedious, time-consuming, and costly. For segmentation ap-plications, the burden is particularly high as manual delin-eations of relevant image content are often extremely expen-sive or can only be done by experts with domain-specific knowledge. Thanks to developments in transfer learning and training with weak supervision, segmentation models can now also greatly benefit from annotations of different kinds. However, for any new domain application looking to use weak supervision, the dataset builder still needs to define a strategy to distribute full segmentation and other weak annotations. Doing so is challenging, however, as it is a priori unknown how to distribute an annotation budget for a given new dataset. To this end, we propose a novel ap-proach to determine annotation strategies for segmentation datasets, whereby estimating what proportion of segmen-tation and classification annotations should be collected given a fixed budget. To do so, our method sequentially determines proportions of segmentation and classification annotations to collect for budget-fractions by modeling the expected improvement of the final segmentation model. We show in our experiments that our approach yields annota-tions that perform very close to the optimal for a number of different annotation budgets and datasets. 1.

Introduction
Semantic segmentation is a fundamental computer vi-sion task with applications in numerous domains such as autonomous driving [11, 43], scene understanding [45], surveillance [50] and medical diagnosis [9, 18]. As the ad-vent of deep learning has significantly advanced the state-of-the-art, many new application areas have come to light and continue to do so too. This growth has brought and continues to bring exciting domain-specific datasets for seg-mentation tasks [6, 19, 29, 32, 52].
Today, the process of establishing machine learning-based segmentation models for any new application is rel-atively well understood and standard. Only once an image dataset is gathered and curated, can machine learning mod-els be trained and validated. In contrast, building appropri-ate datasets is known to be difficult, time-consuming, and yet paramount. Beyond the fact that collecting images can be tedious, a far more challenging task is producing ground-truth segmentation annotations to subsequently train (semi) supervised machine learning models. This is mainly be-cause producing segmentation annotations often remains a manual task. As reported in [4], generating segmentation annotations for a single PASCAL image [15] takes over 200 seconds on average. This implies over 250 hours of annotation time for a dataset containing a modest 5â€™000 images. What often further exacerbates the problem for domain-specific datasets is that only the dataset designer, or a small group of individuals, have enough expertise to produce the annotations (e.g., doctors, experts, etc.), mak-ing crowd-sourcing ill-suited.
To overcome this challenge, different paradigms have been suggested over the years. Approaches such as Active
Learning [7, 8, 26] aim to iteratively identify subsets of im-ages to annotate so as to yield highly performing models.
Transfer learning has also proved to be an important tool in reducing annotation tasks [13, 17, 24, 25, 30, 36]. For in-stance, [37] show that training segmentation models from scratch is often inferior to using pre-training models de-rived from large image classification datasets, even when the target application domain differs from the source do-main. Finally, weakly-supervised methods [2, 40] combine pixel-wise annotations with other weak annotations that are faster to acquire, thereby reducing the annotation burden.
Figure 1. Illustration of different semantic segmentation applications; OCT: Pathologies of the eye in OCT images, SUIM: Underwater scene segmentation [19], Cityscape: street level scene segmentation [11], PASCAL VOC: natural object segmentation.
In particular, Papandreou et al. [40] showed that combina-tions of strong and weak annotations (e.g., bounding boxes, keypoints, or image-level tags) delivered competitive results with a reduced annotation effort. In this work, we rely on these observations and focus on the weakly supervised seg-mentation setting.
In the frame of designing annotation campaigns, weakly-supervised approaches present opportunities for efficiency as well. Instead of completely spending a budget on a few expensive annotations, weakly-supervised methods allow a proportion of the budget to be allocated to inexpensive, or weak, labels. That is, one could spend the entire annotation budget to manually segment available images, but would ultimately lead to relatively few annotations. Conversely, weak annotations such as image-level labels are roughly 100 times cheaper to gather than their segmentation coun-terparts [4]. Thus, a greater number of weakly-annotated images could be used to train segmentation models at an equal cost. In fact, under a fixed budget, allocating a pro-portion of the budget to inexpensive image-level class labels has been shown to yield superior performance compared to entirely allocating a budget to segmentation labels [4].
Yet, allocating how an annotation budget should be dis-tributed among strong and weak annotations is challenging, and inappropriate allocations may severely impact the qual-ity of the final segmentation model. For example, spend-ing the entire budget on image-level annotations will clearly hurt the performance of a subsequent segmentation model.
Instead, a naive solution would be to segment and classify a fixed proportion of each (e.g., say 80% - 20%). Knowing what proportion to use for a given dataset is unclear, how-ever. Beyond this, there is no reason why the same fixed proportion would be appropriate across different datasets or application domains. That is, it would be highly unlikely that the datasets shown in Fig. 1 all require the same pro-portion of strong and weak annotations to yield optimal seg-mentation models.
Despite its importance, choosing the best proportion of annotation types remains a largely unexplored research question. Weakly-supervised and transfer-learning meth-ods generally assume that the annotation campaign and the model training are independent and that all annotations are simply available at training time. While active learning methods do alternate between annotation and training, they focus on choosing optimal samples to annotate rather than choosing the right type of annotations. Moreover, most ac-tive learning methods ignore constraints imposed by an an-notation budget. More notable, however, is the recent work of Mahmood et. al. [33, 34] which aims to determine what weak and strong annotation strategy is necessary to achieve a target performance level. While noteworthy, this objective differs from that here, whereby given a fixed budget, what strategy is best suited for a given new dataset?
To this end, we propose a novel method to find an opti-mal budget allocation strategy in an online manner. Using a collection of unlabeled images and a maximum budget, our approach selects strong and weak annotations, constrained by a given budget, that maximize the performance of the subsequent trained segmentation model. To do this, our method iteratively alternates between partial budget alloca-tions, label acquisition, and model training. At each step, we use the annotations performed so far to train multiple models to estimate how different proportions of weak and strong annotations affect model performance. A Gaussian
Process models these results and maps the number of weak and strong annotations to the expected model improvement.
Computing the Pareto optima between expected improve-ment and costs, we choose a new sub-budget installment
and its associated allocation so to yield the maximum ex-pected improvement. We show in our experiments that our approach is beneficial for a broad range of datasets, and il-lustrate that our dynamic strategy allows for high perfor-mances, close to optimal fixed strategies that cannot be de-termined beforehand. 2.