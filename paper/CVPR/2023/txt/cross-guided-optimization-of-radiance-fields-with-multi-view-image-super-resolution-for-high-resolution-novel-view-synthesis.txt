Abstract
Novel View Synthesis (NVS) aims at synthesizing an im-age from an arbitrary viewpoint using multi-view images and camera poses. Among the methods for NVS, Neural
Radiance Fields (NeRF) is capable of NVS for an arbitrary resolution as it learns a continuous volumetric representa-tion. However, radiance fields rely heavily on the spectral characteristics of coordinate-based networks. Thus, there is a limit to improving the performance of high-resolution novel view synthesis (HRNVS). To solve this problem, we propose a novel framework using cross-guided optimiza-tion of the single-image super-resolution (SISR) and radi-ance fields. We perform multi-view image super-resolution (MVSR) on train-view images during the radiance fields op-timization process. It derives the updated SR result by fus-ing the feature map obtained from SISR and voxel-based un-certainty fields generated by integrated errors of train-view images. By repeating the updates during radiance fields op-timization, train-view images for radiance fields optimiza-tion have multi-view consistency and high-frequency de-tails simultaneously, ultimately improving the performance of HRNVS. Experiments of HRNVS and MVSR on various benchmark datasets show that the proposed method signifi-cantly surpasses existing methods. 1.

Introduction
Novel View Synthesis (NVS) is an approach to synthe-sizing an image from an arbitrary viewpoint using multi-view images and camera poses. This is an essential task in computer vision and graphics, and it can be actively used in street-view navigation, AR/VR, and robotics. Recently,
Neural Radiance Fields [28] (NeRF) significantly improved the performance of NVS by learning multi-layer perceptron (MLP) from 5d coordinate input. Since then, many studies have been conducted to shorten the long learning time of
NeRF [4, 10, 29, 36, 42, 43, 48], increase the performance of
NVS using depth priors [5, 8, 32, 41], and enable NVS from few-shot views [13, 16, 31, 49].
Figure 1. Cross-guided optimization between single image super-resolution and radiance fields. They complement weaknesses of one another with their respective strengths by using the SR update module, rendered train-view RGBs, and uncertainty maps.
Continuous scene representations such as NeRF [28] can be rendered at arbitrary resolution. Thus, there are many studies to improve the performance of multi-scale scene representation. Mip-NeRF [2] proposes scale-dependent positional encoding, which makes a network be trained on multiple scales. In addition, BACON [22] proposes a net-work capable of band-limited multi-scale decomposition by giving a constraint to the bandwidth of network outputs.
Both papers showed significant down-scaling performance on volume rendering. On the other hand, NeRF-SR [39] improves the performance of high-resolution novel view synthesis (HRNVS) by learning in an unsupervised manner through super-sampling in the radiance fields optimization process.
Radiance fields have the ability to find scene geometry and optimize 5D functions simultaneously. Still, radiance fields have a low ability to perform super-resolution, and even if they synthesize high-resolution (HR) images, they
only depend on the characteristic of continuous scene repre-sentation. On the other hand, single-image super-resolution (SISR) generally specializes in learning the inverse func-tion of image degradation. Therefore, SISR could be benefi-cial to HRNVS by super-resolving train-view images; how-ever, SISR is an ill-posed problem for which multiple so-lutions exist, and multi-view consistency cannot be main-tained when multi-view images are processed separately.
To solve this problem, we propose a novel framework us-ing cross-guided optimization between radiance fields and
SISR. As shown in Fig. 1, our framework aims to ensure that radiance fields are guided by superior high-frequency details from SISR, and conversely, SISR is guided by multi-view consistency from radiance fields. We perform train-view synthesis during the radiance fields optimization pro-cess. Then, we generate voxel-based uncertainty fields to obtain uncertainty maps to find reliable regions in rendered train-view RGB images. The rendered train-view outputs and feature maps from the SISR network make it possible to do multi-view image super-resolution (MVSR) through the
SR update module (SUM). Then, we continue optimizing the radiance fields using the updated SR outputs. Repeating the update process makes train-view images for radiance fields optimization have multi-view consistency and high-frequency details simultaneously, ultimately improving the performance of HRNVS.
Our method shows that the performance of HRNVS and
MVSR on various benchmark datasets significantly sur-passes existing methods.
It also shows consistent perfor-mance improvements for various SISR models and radiance fields models in our method.
In summary, our contributions are as follows:
• We propose a novel framework for performing cross-guided radiance fields optimization using the SISR model for HRNVS.
• We propose voxel-based uncertainty fields to find reliable regions of synthesized images.
• We propose an SR update module (SUM) using voxel-based uncertainty fields and train-view synthesis outputs for MVSR.
• Experiments on various benchmark datasets show that the proposed method significantly surpasses existing meth-ods in terms of performance for HRNVS and MVSR. 2.