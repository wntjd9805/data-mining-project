Abstract
Most of the existing salient object detection (SOD) mod-els focus on improving the overall model performance, with-out explicitly explaining the discrepancy between the train-ing and testing distributions. In this paper, we investigate a particular type of epistemic uncertainty, namely distribu-tional uncertainty, for salient object detection. Specifically, for the first time, we explore the existing class-aware dis-tribution gap exploration techniques, i.e. long-tail learning, single-model uncertainty modeling and test-time strategies, and adapt them to model the distributional uncertainty for our class-agnostic task. We define test sample that is dissim-ilar to the training dataset as being “out-of-distribution” (OOD) samples. Different from the conventional OOD def-inition, where OOD samples are those not belonging to the closed-world training categories, OOD samples for SOD are those break the basic priors of saliency, i.e. center prior, color contrast prior, compactness prior and etc., indicat-ing OOD as being “continuous” instead of being discrete for our task. We’ve carried out extensive experimental re-sults to verify effectiveness of existing distribution gap mod-eling techniques for SOD, and conclude that both train-time single-model uncertainty estimation techniques and weight-regularization solutions that preventing model activation from drifting too much are promising directions for mod-eling distributional uncertainty for SOD. 1.

Introduction
Saliency detection (or salient object detection, SOD) [6, 11, 14, 40, 44, 64, 65, 67–69, 74–76, 78] aims to localize object(s) that attract human attention. Most of the exist-ing techniques focus on improving model performance on benchmark testing dataset without explicitly explaining the distribution gap issue within this task.
In this paper, we aim to explore the distributional uncertainty for better un-derstanding of the trained saliency detection models.
Jing
Zhang (zjnwpu@gmail.com) and
Yuchao
Dai (daiyuchao@nwpu.edu.cn) are the corresponding authors.
Our code is publicly available at: https://npucvr.github.io/
Distributional_uncer/.
Figure 1. Visualization of different types of uncertainty, where aleatoric uncertainty (p(y|x∗, θ)) is caused by the inherent ran-domness of the data, model uncertainty (p(θ|D)) happens when leading to multiple solutions there exists low-density region, within this region, and distributional uncertainty (p(x∗|D)) oc-curs when the test sample x∗ fails to fit in the model based on the training dataset D. is D =