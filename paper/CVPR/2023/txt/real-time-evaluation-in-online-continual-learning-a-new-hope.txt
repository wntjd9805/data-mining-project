Abstract
Current evaluations of Continual Learning (CL) meth-ods typically assume that there is no constraint on train-ing time and computation. This is an unrealistic assump-tion for any real-world setting, which motivates us to pro-pose: a practical real-time evaluation of continual learn-ing, in which the stream does not wait for the model to com-plete training before revealing the next data for predictions.
To do this, we evaluate current CL methods with respect to their computational costs. We conduct extensive experi-ments on CLOC, a large-scale dataset containing 39 million time-stamped images with geolocation labels. We show that a simple baseline outperforms state-of-the-art CL methods under this evaluation, questioning the applicability of ex-isting methods in realistic settings. In addition, we explore various CL components commonly used in the literature, in-cluding memory sampling strategies and regularization ap-proaches. We find that all considered methods fail to be competitive against our simple baseline. This surprisingly suggests that the majority of existing CL literature is tai-lored to a specific class of streams that is not practical. We hope that the evaluation we provide will be the first step to-wards a paradigm shift to consider the computational cost in the development of online continual learning methods. 1.

Introduction
Deep Neural Networks (DNNs) have demonstrated im-pressive success in solving complex tasks [20, 29, 42] when trained offline, for several passes, over large well-curated la-beled datasets. However, in many real-world scenarios, data is only available in the form of a stream with a changing distribution. Due to this challenge, there has been a grow-ing interest in the problem of learning from a time-varying stream, also known as Continual Learning (CL), which is
* Equal Contribution.
Correspondence to: yasir.ghunaim@kaust.edu.sa
Code: github.com/Yasir-Ghunaim/RealtimeOCL a key challenge for DNNs due to a phenomenon known as catastrophic forgetting [17, 36]. In particular, when a DNN is trained with data from a new distribution, the DNN per-formance significantly drops on previously learned data.
While mitigation efforts have been proposed, e.g. through regularizing the training [2, 25, 53], replaying pre-viously seen examples [11, 23, 39], and many other ap-proaches [16,40,51], current evaluation approaches are still far from real-world scenarios. For example, the majority of literature is on Offline Continual Learning, under which methods are allowed unlimited budget, both time and com-putation. Furthermore, the majority of CL evaluations are conducted on small-scale datasets with well-defined tem-poral distribution boundaries in the form of learning a se-quence of tasks.
To that end, there has recently been a growing interest in the more realistic setting – Online Continual Learning (OCL). In such a setup [1, 4, 21, 32], CL methods are re-stricted to a single training pass over a shuffled split of ex-isting offline CL benchmarks. This is certainly a step for-ward towards resolving some of the unrealistic assumptions of offline CL. However, current evaluations do not suffi-ciently address the challenges of real-time learning for high-throughput streams with rapid distribution changes.
To illustrate this, consider the problem of continuously learning a Twitter stream where 350K tweets are uploaded per minute on various trending topics [41]. Every uploaded tweet needs to be predicted with a DNN for misinformation and hate speech, among other things, while simultaneously learning and adapting to them. Given the scale at which data is being updated, there is an inherent key limitation on the time and computational budget affordable to learning incoming tweets, an aspect that is often overlooked in the prior art from the OCL literature. Consider an OCL method that is 10 times slower than the Twitter high throughput stream, i.e., it takes 10 minutes to train on one minute worth of tweets (350K tweets). This inefficiency results in an ac-cumulation of 3.1 million new samples that need to be predicted and trained on. Since it is not acceptable to pause
∼
Figure 1. OCL Real-Time Evaluation Example. We show an example of real-time evaluation, using the CLOC dataset [7], of two different OCL methods A and B. Method B is twice as slow as method A. Both methods are evaluated on every incoming sample. Since
A has a stream-model relative complexity of one, i.e. CS (A) = 1, it is able to train on all the stream samples. In contrast, B, which has a relative complexity of two, requires two time steps to train on a single stream batch. Thus, B only trains on half of the stream samples. all tweets from appearing online until the method training is complete, predictions for all new samples will be performed with an older version of the model. This poses a key chal-lenge where efficient learning from streams becomes nec-essary. This is because slow-training OCL methods can re-sult in subpar performance, as they resort to predicting new stream data using an older model. This behavior worsens for streams that experience a faster change in distribution.
B
A
A
A
A
, then is twice as expensive as
In this paper, we propose a real-time evaluation protocol for OCL that factors in training computational complexity.
Given a stream, consider an OCL method that is as fast as the stream, i.e., can train on every step of revealed data before the stream presents new samples. Then, if an OCL will update the model
B for evaluation every other stream step, i.e., the model will be updated half the number of times compared to
. Figure 1 illustrates our proposed real-time evaluation. This is in contrast to all prior art [3, 4, 6] that (1) unreasonably allows an unlimited computational budget to train on any given stream data, and (2) unfairly compares OCL methods de-spite having different training complexity levels. Using our real-time evaluation protocol, we benchmark many existing
OCL methods against a simple and inexpensive baseline, which mitigates forgetting by simply storing and replaying recently seen samples.
Contributions. We summarize our conclusions as follows: (1) We show that under our practical real-time evaluation, our simple baseline outperforms all the considered meth-ods from the OCL literature, including recent SOTA ap-proaches like ACE [6]. (2) We consider a complementary setup where the stream is as slow as the most training-expensive OCL method and compare that method against the compute-equivalent baseline. Under this computation-ally normalized setting, we find that the compute-equivalent baseline outperforms all existing methods. (3) Our experi-ments are consistent, holding for all the considered contin-ual learning strategies, and extensive, amounting to more than 2 GPU-months. Our results highlight that the current progress in OCL needs to be rethought and a paradigm shift is needed. We hope our work will lead to a new direction for continual learning that takes into account the computational cost of each method. 2.