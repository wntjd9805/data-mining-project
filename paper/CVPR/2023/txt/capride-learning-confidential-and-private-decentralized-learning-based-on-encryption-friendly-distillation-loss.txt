Abstract
Large volumes of data required to train accurate deep neural networks (DNNs) are seldom available with any sin-gle entity. Often, privacy concerns prevent entities from sharing data with each other or with a third-party learning service provider. While cross-silo federated learning (FL) allows collaborative learning of large DNNs without shar-ing the data itself, most existing cross-silo FL algorithms have an unacceptable utility-privacy trade-off. In this work, we propose a framework called Confidential and Private
Decentralized (CaPriDe) learning, which optimally lever-ages the power of fully homomorphic encryption (FHE) to enable collaborative learning without compromising on the confidentiality and privacy of data.
In CaPridDe learn-ing, participating entities release their private data in an encrypted form allowing other participants to perform in-ference in the encrypted domain. The crux of CaPriDe learning is mutual knowledge distillation between multi-ple local models through a novel distillation loss, which is an approximation of the Kullback-Leibler (KL) divergence between the local predictions and encrypted inferences of other participants on the same data that can be computed in the encrypted domain. Extensive experiments on three datasets show that CaPriDe learning can improve the ac-curacy of local models without any central coordination, provide strong guarantees of data confidentiality and pri-vacy, and has the ability to handle statistical heterogene-ity. Constraints on the model architecture (arising from the need to be FHE-friendly), limited scalability, and compu-tational complexity of encrypted domain inference are the main limitations of the proposed approach. The code can be found at https://github.com/tnurbek/capride-learning. 1.

Introduction
Rapid strides have been made in machine learning (ML) (in particular, deep learning) over the past decade. How-ever, in many important application domains such as health-care and finance, the absence of large, centralized datasets is a significant obstacle to realizing the full benefits of deep learning algorithms. Data in these applications often resides in silos and is governed by strict regulations (e.g., HIPAA,
GDPR, etc.) because of its privacy sensitive nature [22].
Competing business interests of data owners and the lack of appropriate incentives for data sharing further accentuate the problem. To overcome these issues, there is a need for collaborative learning algorithms that ideally satisfy the fol-lowing requirements [15]: (i) confidentiality - no sharing of raw data, (ii) privacy - minimal leakage of information via the knowledge exchange mechanism (e.g., gradients or pre-dictions), (iii) utility - gain in accuracy (over the individual models) resulting from the collaboration, even in the pres-ence of statistical heterogeneity, (iv) efficiency - minimize computational complexity and communication burden, (v) robustness - handle unintentional failures and attacks em-anating from malicious entities, and (vi) fairness - utility should be proportional to the individual contributions.
Federated learning (FL) [15] is a special case of collab-orative learning, which works under the orchestration of a central server. FL allows multiple entities to collaboratively solve a ML problem by sharing of focused updates (e.g., gradients), instead of raw data. Specifically, cross-silo FL (typically between 2-100 participants) has been touted as a promising solution to address the data fragmentation prob-lem in finance [8] and healthcare [16, 23]. Most prior cross-silo FL algorithms assume that all the parties are collec-tively training a single model with a common architecture, which is too restrictive in practice. Furthermore, knowl-edge exchange usually happens through sharing of gradi-ents or model parameters. Recent gradient inversion at-tacks [9, 14] demonstrate that it is indeed possible to re-cover high fidelity information from individual gradient up-dates, thus violating the privacy requirement. While dif-ferential privacy (DP) [24], secure multi-party computation (MPC) [26], and trusted execution environment (TEE) [21] have been proposed as potential remedies to safeguard pri-vacy in FL, none of the existing solutions offer an accept-able privacy-utility trade-off with reasonable efficiency. As
noted in [5], sharing of high dimensional gradients/model parameters is a fundamental privacy limitation of standard
FL methods, which cannot be addressed by simply wrap-ping around a DP, MPC, or TEE solution.
Confidential and private collaborative learning (CaPC)
[7] is the only method that claims to achieve both confi-dentiality and privacy in a collaborative setting. In CaPC learning, each participant is able to query other parties via a private inference protocol based on 2-party computation (2PC). However, the answering parties cannot directly re-veal the inference logits to the querying party because it leaks information about their local models (e.g., through model extraction attacks). To circumvent this problem, dif-ferential privacy, private aggregation of teacher ensembles, and secure argmax computation through a central privacy guardian (PG) are employed to output only the predicted la-bel to the querying party. A drawback of the CaPC approach is that it achieves all privacy guarantees using the help of a semi-trusted PG. Moreover, the use of differential pri-vacy reduces the performance of FL algorithms, especially if strong privacy guarantees are required. Finally, the use of
MPC requires multiple rounds of communication between the parties for each query. All other approaches that have been proposed to achieve knowledge transfer through distil-lation [18] require a non-private labeled/unlabeled/synthetic dataset that can be shared among participants.
In this work, we propose a new protocol called
Confidential and Private Decentralized (CaPriDe) learning, where participants learn from each other in a collaborative setting while preserving their confidentiality and privacy (see Figure 1). Unlike the 2PC protocols used in CaPC, we leverage fully homomorphic encryption (FHE) to en-able participants to publish their encrypted data. Since the published data is encrypted using the data owner’s pub-lic key and only the owner can decrypt the data (using the secret key), confidentiality is preserved. However, the
CaPriDe framework allows other collaborators to perform encrypted inference on the published (encrypted) data by applying their own local model. Mutual knowledge distilla-tion (KD) [13] between the data owner’s local model and the local models of the collaborators is used to transfer knowl-edge between models and ensure a collaborative gain. KD is typically achieved through minimizing the distillation loss between the student and teacher responses (logits). Since the collaborators in CaPriDe learning make predictions on encrypted data, a loss function that can be computed with-out any decryption is required. Hence, we propose a new distillation loss, which is an approximation of the KL di-vergence that can be securely computed. Only an encrypted value of the distillation loss aggregated over an entire batch is sent back to the data owner, which ensures strong privacy.
Data owners can decrypt this aggregate loss value and use it for updating their local models. Our contributions are:
• We introduce the CaPriDe learning framework, which exploits FHE-based encrypted inference and knowl-edge distillation to achieve confidential and private collaborative learning without any central orchestra-tion and any need for non-private shared data.
• To enable CaPriDe learning, we propose an encryption-friendly distillation loss that estimates the approximate KL divergence between two model predictions and design a protocol to securely compute this loss in the encrypted domain.
• We conduct extensive experiments to show that
CaPriDe learning enables participants to achieve col-laborative gain, even in the non-iid setting. To prove feasibility, we implement encrypted inference using the Tile Tensors library with a FHE-friendly model. 2.