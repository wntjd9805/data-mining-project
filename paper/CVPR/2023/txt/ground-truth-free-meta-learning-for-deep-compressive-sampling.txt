Abstract needs to solve an under-determined system:
Compressive sampling (CS) is an efﬁcient technique for imaging. This paper proposes a ground-truth (GT) free meta-learning method for CS, which leverages both ex-ternal and internal deep learning for unsupervised high-quality image reconstruction. The proposed method ﬁrst trains a deep neural network (NN) via external meta-learning using only CS measurements, and then efﬁciently adapts the trained model to a test sample for exploit-ing sample-speciﬁc internal characteristic for performance gain. The meta-learning and model adaptation are built on an improved Stein’s unbiased risk estimator (iSURE) that provides efﬁcient computation and effective guidance for accurate prediction in the range space of the adjoint of the measurement matrix. To improve the learning and adap-tion on the null space of the measurement matrix, a modi-ﬁed model-agnostic meta-learning scheme and a null-space consistency loss are proposed. In addition, a bias tuning scheme for unrolling NNs is introduced for further acceler-ation of model adaption. Experimental results have demon-strated that the proposed GT-free method performs well and can even compete with supervised methods. 1.

Introduction
CS is an imaging technique that captures an image by collecting a limited number of measurements using a spe-ciﬁc measurement matrix [5]. This technique has a wide range of applications, including magnetic resonance (MR) imaging, computed tomography (CT), and many others.
One main challenges in CS is reconstructing an image from its limited measurements. Let x ∈ RN denote an image of interest, y ∈ CM its CS measurement vector with M (cid:28) N , and Φ the measurement matrix. Then the CS reconstruction
*Corresponding author: Yuhui Quan
†This work was supported by Natural Science Foundation of Guang-dong Province (Grant No. 2022A1515011755 and 2023A1515012841), and Singapore MOE AcRF Tier 1 Grant (WBS No. A-8000981-00-00). y = Φx + (cid:15), (1) where (cid:15) ∈ CM denotes measurement noise. As Φ is under-determined, a direct inversion suffers from solution ambi-guity and noise sensitivity.
Supervised learning is one popular approach for CSR, which trains an end-to-end NN for CSR over a dataset with pairs of a GT image and its measurements; see e.g. [11, 42, 45, 47]. However, supervised CSR has two limitations.
Firstly, it is often expensive or even impractical to collect a sufﬁcient number of GT images in practice. Consequently, the NN trained using a limited amount of external training data may not generalize well. Secondly, the training data may be biased or insufﬁcient to cover the full range of pat-terns and characteristics of the images being reconstructed, resulting in poor generalization performance.
Recently, many works (e.g. [6, 7, 27, 50]) have sought to address the ﬁrst limitation of supervised CSR by developing
GT-free external learning techniques, which train an end-to-end NN without accessing GT images. Quite a few meth-ods along this line are based on the Stein’s unbiased risk estimator (SURE) [37]. While they have shown promise, there remains a noticeable performance gap between these
GT-free techniques and existing supervised methods.
To address the second limitation of supervised CSR, there are also some works on self-supervised internal learn-ing, which train a deep NN model directly on a test sam-ple to exploit sample-speciﬁc internal characteristics; see e.g. [15, 30, 39]. These methods are based on the deep im-age prior (DIP) [38] induced by the structures of a convo-lutional NN (CNN). The downside of these methods is their computational efﬁciency. Since each test example requires learning an individual model, processing a large number of test samples can be time-consuming and overwhelming.
Motivated by the pros and cons of existing works on ex-ternal and internal learning, this paper aims at developing a
GT-free joint external and internal learning method for CSR that takes the full advantages of both:
• Same as existing external learning methods, it trains a model using an external dataset in an ofﬂine manner. The advantage is that the resulting model trained without GTs can compete well against its supervised counterparts.
• Same as existing internal learning methods, it is GT-free and its model is adaptive to test samples to reduce dataset bias. The advantage is that it is much more computation-ally efﬁcient when processing many test samples.
This paper proposes a GT-free meta-learning method for
CSR that achieves these goals. Meta-learning is about
“learning to learn” which develops a learning model that can learn new concepts or fast adapt to new environments with efﬁcient updates to an existing model. Different from existing works which access GT images for meta-learning (e.g. [21, 40]), we consider a GT-free environment where only measurement data is available during training. In the training stage, our proposed method trains a deep NN model so that not only it can reconstruct high-quality images, but also its weights are suitable for efﬁcient test-time model adaption (i.e. internal learning). Then, in the test stage, the meta-trained model can be efﬁciently adapted to each test sample for further performance improvement.
Since no GT images are exposed to meta-learning and model adaptation, the key challenge is how to train the model to make accurate predictions solely based on mea-surement data. Motivated by SURE-based unsupervised learning, we propose a SURE-motivated self-supervised loss function, called iSURE (“i” for “improved”). Simi-lar to SURE, iSURE provides an unbiased estimate of the mean squared error in range(ΦH) (range space of adjoint of Φ), but using noisier input. The gradients derived from iSURE result in a more computationally efﬁcient iteration scheme than existing SURE-based loss functions. Further-more, the noise injection in iSURE also helps to alleviate potential overﬁtting during model training and adaptation.
Built upon the iSURE, an unsupervised training scheme is developed with the integration of model-agnostic meta-learning (MAML) [12]. MAML is a gradient-based meta-learning algorithm, which trains a model such that a small number of gradient updates on it will give a model to per-form well on a new task. In our method, each target task is deﬁned as the self-supervised reconstruction on a test sam-ple via iSURE. Similar to other SURE variants, the iSURE deﬁned on range(ΦH) does not address the solution ambi-guity caused by non-empty null(Φ) (null space of Φ). For-tunately, the DIP from a CNN imposes implicit regulariza-tion on the output, partially addressing such ambiguity. To further reduce ambiguity, an ensemble-based sub-process is introduced into MAML. The basic idea is that the CNN with re-corrupted inputs in iSURE is likely to generate estimates with some degree of diversity on null(Φ), and the ensemble of these estimates will reﬁne the prediction in null(Φ).
The adaption that only updates the gradients via iSURE ignores the inﬂuence on the prediction on null(Φ). Thus, an additional loss on the prediction consistency in null(Φ) between the original and adapted models is adopted, which provides guidance on reducing the solution ambiguity in null(Φ) during adaption. To further accelerate the adapta-tion process, we use an unrolling CNN and propose a bias-tuning scheme that only adapts the bias parameters of the unrolling CNN. The motivation stems from the similarity between the bias parameters in an unrolling CNN and the threshold values in a shrinkage-based scheme for CSR.
To summarize, this paper proposes a GT-free meta-learning method for CSR, with the following contributions:
• An iSURE loss function for GT-free meta-learning and model adaption, which results in a more efﬁcient and ef-fective scheme for mitigating overﬁtting compared to ex-isting SURE-based loss functions;
• A meta-learning process based on MAML and iSURE for learning CSR from external GT-less data, with an en-semble sub-process for improving null-space learning;
• A model adaption scheme using iSURE and null-space consistency, which exploits internal characteristics of a test sample for more accurate prediction;
• A bias-tuning scheme for accelerating the adaption of unrolling CNNs, with little reconstruction accuracy loss.
The experiments demonstrate that the proposed method out-performs GT-free learning methods and competes with su-pervised methods, while being faster than internal methods. 2.