Abstract
Computer vision models are excellent at identifying and exploiting regularities in the world. However, it is com-putationally costly to learn these regularities from scratch.
This presents a challenge for low-parameter models, like those running on edge devices (e.g. smartphones). Can the performance of models with low representational power be improved by supplementing training with additional in-formation about these statistical regularities? We explore this in the domains of action recognition and action antic-ipation, leveraging the fact that actions are typically em-bedded in stereotypical sequences. We introduce the Event
Transition Matrix (ETM), computed from action labels in an untrimmed video dataset, which captures the temporal con-text of a given action, operationalized as the likelihood that it was preceded or followed by each other action in the set.
We show that including information from the ETM during training improves action recognition and anticipation per-formance on various egocentric video datasets. Through ablation and control studies, we show that the coherent se-quence of information captured by our ETM is key to this effect, and we find that the benefit of this explicit represen-tation of temporal context is most pronounced for smaller models. Code, matrices and models are available in our project page: https://camilofosco.com/etm_ website 1.

Introduction
A strength of computer vision models is their ability to identify and exploit statistical regularities in the world.
Learning these regularities from scratch is computationally costly, which limits the accuracy of low-parameters models.
It is critical to boost the performance of small models, since many devices lack the resources to host current state of the art models. One way to make the learning problem more tractable for small models may be to supplement them at training time with an explicit representation of the statisti-cal regularities in the target domain. Here, we test whether
Figure 1. Action recognition performance difference between models trained with and without the proposed ETM approach. We train models with various model architectures, from small (left) to large (right): AVT-b [14], MoViNets [24] family, X3D [11] family, LambdaResNet-50 [4], and EfficientNets [57] family on
EPIC-KITCHENS-100 [14]. We show that incorporating the ETM into training improves performance, and the impact is higher on smaller models. video understanding models can be improved by providing them with information about typical event sequences during training.
We introduce the Event Transition Matrix (ETM), il-lustrated in Figure 2, which leverages the fact that events in real-world videos often occur in predictable sequences.
Each row and column indexes an event. In the rows, the
ETM captures the likelihood that the event was followed by each of the other events in the set. In the columns, it captures the likelihood that the event was preceded by each other event. To compute a cellâ€™s value, we combine infor-mation from all previous and subsequent events, weighted by their temporal distance from the queried action. Cru-cially, this breaks markovian assumptions and allows the matrix to capture long-range relationships. The ETM has two important properties. First, it acts as an explicit rep-resentation of the likelihood of event transitions, which provides additional pertinent information that a model can
(a) Low-level descriptions in [10] (b) Event Transition Matrix
Figure 2. We construct a given Event Transition Matrix (ETM) from action labels drawn from a given dataset of untrimmed videos. (a)
Each video depicts a continuous activity, and is paired with human annotations indicating the individual, low-level actions that compose the activity. (b) Using these action labels, we created the ETM by recording the frequency with which a given action label was preceded or followed by each other action label, accumulated across the videos in our set. leverage without the cost of learning it for itself. Second, it augments representations of a given event with information about what came before and after it, providing additional target for the model to train on.
In the present paper, across multiple datasets and model architectures, we test the effectiveness of incorporating the
ETM into training for low-parameter models. We test our approach on action recognition and action anticipation in egocentric video datasets, where actions occur in stereotyp-ical sequences. However, this approach can apply to any kind of sequence in videos. We show that leveraging the
ETM improves action recognition relative to baselines, and that this improvement relies on the coherence of the action sequence. We also show that action anticipation is improved with our ETM approach. In both cases, we show that the
ETM approach can be incorporated into multiple different model architectures, and that the addition of the ETM has the highest impact on smaller models, as shown in Figure 1.
Overall, this work demonstrates a potential path to more efficient models, based on supplementing small models at training time with explicit representations of regularities ex-pected in the data. 2.