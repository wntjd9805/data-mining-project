Abstract
Today single image deweathering is arguably more sen-sitive to the dataset type, rather than the model. We intro-duce WeatherStream, an automatic pipeline capturing all real-world weather effects (rain, snow, and rain fog degra-dations), along with their clean image pairs. Previous state-of-the-art methods that have attempted the all-weather re-moval task train on synthetic pairs, and are thus limited by the Sim2Real domain gap. Recent work has attempted to manually collect time multiplexed pairs, but the use of hu-man labor limits the scale of such a dataset. We introduce a pipeline that uses the power of light-transport physics and a model trained on a small, initial seed dataset to re-ject approximately 99.6% of unwanted scenes. The pipeline is able to generalize to new scenes and degradations that can, in turn, be used to train existing models just like fully human-labeled data. Training on a dataset collected through this procedure leads to significant improvements on multiple existing weather removal methods on a care-fully human-collected test set of real-world weather effects.
The dataset and code can be found in the following website: http://visual.ee.ucla.edu/wstream.htm/. 1.

Introduction
Single-image deweathering aims to remove image degra-Single-image dations caused by rain, fog, or snow.
*Equal contribution. deweathering is a mainstay of modern computer vision, val-ued for the aesthetic appeal of removing weather degrada-tions, as well as the ability to reuse pre-trained computer vi-sion models, which work on clear weather conditions. Un-fortunately, the field is dataset bottlenecked. State-of-the-art techniques use deep networks, but suffer from a common issue: the same scene cannot be observed at the same time, with and without weather artifacts. Therefore, it is not pos-sible to train deep networks on ideal pairs, a pair of clean and degraded images of the same scene at the same time.
Previous work has attempted to solve the dataset bottle-neck by using simulated pairs. A simulated pair is formed by starting with a clean image of a scene and artificially adding weather degradations. For example, one could care-fully simulate the effect of raindrop streaks on a clean im-age. Unfortunately, simulating the diverse weather condi-tions that one can encounter is a very difficult path. Exist-ing simulators for rain are difficult to generalize, and scaling simulators for rain, fog, and snow poses a further challenge.
Nonetheless, simulated pairs have been the most common approach, and researchers have accepted the generalization errors that are encountered. Another emerging way to ob-tain pairs is to use psuedo-real pairs. A pseudo-real pair is a pair of clean and degraded images that is formed with-out the use of simulators. One way of doing so is to use a video-based deraining method to remove rain (which is dynamic) from a scene [60]. This form of ground-truthing
assumes video-based deraining is itself a solved problem, which leads to limited performance, particularly in rain ef-fects that are less dynamic (such as far-field veiling).
Perhaps the highest-quality pairs were obtained in the most recent work known as GT-RAIN [2]. This paper took a different tack, introducing time multiplexed pairs. A time multiplexed pair is obtained by taking an input video se-quence and grabbing closely spaced frames in the video, with and without rain. This approach only works if nearby frames are grabbed in a magic moment when the scene conditions are just right, e.g., the rain is on the cusp of stop-ping, illumination constancy is observed, limited dynamic agents, and so on. In the less than one percent of videos that have suitable conditions, a time-multiplexed pair per-forms almost like real, ground truth.
Unfortunately, approaching time multiplexed pairing us-ing human annotation (as has been done in previous work) is hard to scale to 100K+ pairs. As a generous lower bound, it would take 1 human labeler, 1 minute to carefully parse through 5 video sequences. Scaling this up to 2 million videos would take a whole year. Moreover, 99.6% of the video sequences do not meet the criteria for a magic mo-ment. A further limit to scalability is that human observers must be highly trained to control for factors such as illumi-nation shifts, weather API errors, and dynamic objects that are prevalent in over 99% of the videos.
In this paper, we formulate light transport techniques, which model the flow of light in a scene [29] to help us decide if frames should be included in training data. A key contribution is to formulate four principles of light transport to decide if a time-multiplexed pair is valid: (1)