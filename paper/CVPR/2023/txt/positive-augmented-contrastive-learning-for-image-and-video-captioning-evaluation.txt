Abstract
The CLIP model has been recently proven to be very effective for a variety of cross-modal tasks, including the evaluation of captions generated from vision-and-language architectures.
In this paper, we propose a new recipe for a contrastive-based evaluation metric for image cap-tioning, namely Positive-Augmented Contrastive learning
Score (PAC-S), that in a novel way unifies the learning of a contrastive visual-semantic space with the addition of generated images and text on curated data. Experiments spanning several datasets demonstrate that our new metric achieves the highest correlation with human judgments on both images and videos, outperforming existing reference-based metrics like CIDEr and SPICE and reference-free metrics like CLIP-Score. Finally, we test the system-level correlation of the proposed metric when considering pop-ular image captioning approaches, and assess the impact of employing different cross-modal features. Our source code and trained models are publicly available at: https:
//github.com/aimagelab/pacscore. 1.

Introduction
The task of image captioning, which requires an algo-rithm to describe visual contents with natural language sen-tences, has been gaining considerable attention from the re-search community in the past few years [22, 53, 61]. As such, the task has witnessed methodological and architec-tural innovations, ranging from the usage of self-attentive models [10, 16, 19, 36] to the development of better con-nections between visual and textual modalities with the ad-dition of objects [3, 62, 66] and tags [29, 63] or the use of more powerful cross-modal features [5, 6, 45].
Together with an increase in generation quality, the auto-matic evaluation of captions has also witnessed a significant effort. While early evaluation scores were based on transla-tion metrics [4, 30, 37], more effective text-based [2, 52, 67] and multimodal solutions [21,56] have been proposed in the
Figure 1. Evaluation scores generated by our proposed metric,
PAC-S, in comparison with existing metrics for captioning. The caption highlighted in green is the one preferred by humans. last few years. Among these, the usage of cross-modal mod-els in which both visual and textual data can be matched has proven to be a viable strategy that can lead to high-quality metrics [17, 24–26]. Recently, the large-scale CLIP model [38] was tested for image captioning evaluation, re-sulting in the CLIP-Score [17] which proved to have a sig-nificant correlation with human judgment.
While these advancements demonstrate the appropriate-ness of using contrastive-based embedding spaces for eval-uating image captions, large-scale models pre-trained on web-collected data also have limitations, due to the lack in style of captions collected from alt-tags and of the distribu-tion of web-scale images which is not aligned with those on which captioning systems are evaluated. While cleaned data sources, on the contrary, are limited in size, recent advances in both image [14,39,43,44] and text generation [28,59,66] have made it possible to synthetically generate data in both modalities, with controlled style and quality.
Following this insight, in this paper we propose a learn-able metric that fuses together the advantages of both these scenarios, by leveraging the quality of the pre-training on
web-collected data and that of cleaned data, and also regu-larizing the training by considering additional positive sam-ples hailing from visual and textual generators. Specifically, our proposed metric, PAC-S, is trained via a newly con-ceived positive-augmented contrastive learning approach, in which pairs of generated images and texts act as additional positives in addition to real images and human-annotated captions taken from a cleaned data source. We demon-strate that the combination of these factors, i.e. the usage of a cleaned data source and the pairing with multimodal generated data, when used to finetune a large-scale con-trastive model, results in an embedding space with signif-icantly higher alignment with the human judgment (Fig. 1).
We apply the resulting metric to evaluate both images and videos, both in reference-based and reference-free settings.
We investigate the quality of the proposed metric by conducting extensive experiments on a variety of image and video datasets, including Flickr8k-Expert and Flickr8k-CF [18], Composite [1], Pascal-50S, and Abstract-50S [52] for the image scenario and the VATEX-EVAL dataset [49] to evaluate video-caption pairs.
Further, we verify its sensitivity to object hallucination on the FOIL [47] and
ActivityNet-FOIL [49] datasets and compare the perfor-mance of state-of-the-art caption generators with respect to the proposed metric. Our proposal outperforms previous reference-based and reference-free metrics and showcases superior performance with respect to CLIP-Score [17] and the corresponding video-based version (i.e. EMScore [49]), which also employ a contrastive-based embedding space.
Overall, our metric ranks first in terms of correlation with human judgment with respect to all existing image and video captioning metrics.
To sum up, the main contribution of this paper is a novel metric for image and video captioning, based on a positive-augmented training of a multimodal embedding space, which exploits both curated image-caption pairs and additional synthetically generated positives. Extensive ex-periments on several datasets demonstrate a higher correla-tion with human judgment and an increased sensitivity to object hallucination. coverage of ground-truth named entities [8, 9]. A new trend, instead, is to exploit the capabilities of pre-trained models to compare textual-only [64, 67] or visual-textual contents [17, 20, 21, 25, 26, 56]. Among them, the BERT score [67] and its improved version [64] use pre-trained
BERT embeddings [12] to represent and compare word to-kens in the generated and ground-truth sentences.
In addition to these text-based metrics, other solutions leverage the multimodal nature of vision-and-language models to exploit not only textual information but also the visual content of images and potentially video frames. For example, Jiang et al. [21] introduced the TIGEr metric, which considers the similarities between words and im-age regions computed according to a cross-modal match-ing model [27] trained on COCO [31]. Other approaches, instead, exploit the effectiveness of web-scale vision-and-language models such as VilBERT [33], UNITER [7], and CLIP [38], pre-trained on millions or even billions of image-text pairs, to obtain more robust metrics [17, 24–26].
Among them, the recent CLIP-Score [17] is based on a modified cosine similarity between image and candidate caption representations coming from the CLIP model. Re-cently, Kim et al. [24] proposed using CLIP visual-textual features to compute the negative Gaussian cross-mutual in-formation, obtaining a more effective evaluation metric.
While all the aforementioned evaluation metrics have originally been introduced for image captioning, there is only one attempt to evaluate video descriptions through learnable metrics also taking into account the visual con-tent appearing in video frames. In particular, Shi et al. [49] presented the EMScore, in its both reference-free and reference-based versions, that computes fine-grained sim-ilarities between video frames and words of the candidate caption using CLIP visual-textual embeddings.
Another related work is that proposed in [69] where dif-fusion models are used to evaluate text-only tasks. Differ-ently from our proposal, the introduced metric exploits sim-ilarities between machine-generated images obtained by a visual generator [43] starting from reference and candidate textual items during evaluation. 2.