Abstract 3D content manipulation is an important computer vi-sion task with many real-world applications (e.g., prod-uct design, cartoon generation, and 3D Avatar edit-ing). Recently proposed 3D GANs can generate diverse photorealistic 3D-aware contents using Neural Radiance fields (NeRF). However, manipulation of NeRF still remains a challenging problem since the visual quality tends to degrade after manipulation and suboptimal control han-dles such as 2D semantic maps are used for manipula-tions. While text-guided manipulations have shown po-tential in 3D editing, such approaches often lack local-ity. To overcome these problems, we propose Local Edit-ing NeRF (LENeRF), which only requires text inputs for fine-grained and localized manipulation. Specifically, we present three add-on modules of LENeRF, the Latent Resid-ual Mapper, the Attention Field Network, and the Deforma-tion Network, which are jointly used for local manipulations of 3D features by estimating a 3D attention field. The 3D attention field is learned in an unsupervised way, by distill-ing the zero-shot mask generation capability of CLIP to the 3D space with multi-view guidance. We conduct diverse ex-periments and thorough evaluations both quantitatively and qualitatively.1 1.

Introduction 3D content editing has many real-world applications in-cluding but not limited to product design, cartoon gener-ation, and 3D Avatar editing. However, it often necessi-tates the use of sophisticated tools with complex interfaces, which can be difficult for novice users and labor-intensive even for seasoned professionals. While explicit 3D repre-†This work was done during an internship at Kakao Enterprise Corp. 1We will make our code publicly available.
demands a certain degree of localization. However, achiev-ing this with text-only control proves to be a challenging objective. Alternative methods [44, 45] that rely on seman-tic masks for editing face their own limitations: 2D guid-ance is not ideal for 3D editing and lacks the descriptive-ness required for fine-grained editing. Furthermore, these approaches require inversion steps and are difficult to gener-alize across different domains, as they depend on the avail-ability of labeled semantic masks.
To overcome the existing limitations, we propose Lo-cal Editing NeRF (LENeRF), which focuses on the impor-tant aspects of 3D editing: photorealism, multi-view con-sistency, usability, diversity, and locality. With LENeRF, high-resolution photo-realistic radiance fields can be edited while maintaining their quality and multi-view consistency.
One notable advantage of LENeRF is its text-only editing, making it more usable than other methods. This allows our approach to be applied to any domain by leveraging the multi-modal embedding space of Contrastive Language
Image Pre-training (CLIP) [39]. Additionally, our method achieves real-time editing as it does not require any test-time optimization process.
Our proposed approach exhibits particularly robust per-formance in local 3D editing. This is achieved through a unique method of editing features in the 3D space indepen-dently by granting position-wise freedom to the features.
The naive approach of directly manipulating the latent code often results in global changes to the 3D content, because features in the 3D space are spatially entangled with each other as the entire radiance field is conditioned with a single latent code. To address this issue, we propose to generate a 3D mask on the region of interest with a masking prompt (e.g., ”hair”) and manipulate the features inside the region
Inspired by the previ-while leaving the rest unchanged. ous approach which introduces the explanation method for capturing the regions of the interest [25], we estimate 3D masks in an unsupervised fashion by using 3D distillation of the 2D CLIP model. Although the CLIP model is not 3D-aware and the 3D GAN lacks text-conditioned mask genera-tion capability, our method enables the collaboration of two pre-trained models to generate a text-conditioned 3D mask, as demonstrated in Figure 1 (c).
LENeRF comprises three add-on modules, namely La-tent Residual Mapper (LRM), Attention Field Network (AFN), and Deformation Network (DN) as depicted in Fig-ure 3. LRM generates a latent code that produces a target feature field. AFN generates a soft 3D mask indicating our region of interest. The source feature field is distorted using
DN and subsequently interpolated with the target field to synthesize the final feature field. LENeRF is trained with
CLIP guidance [38, 39], and AFN is additionally trained with CLIP-generated zero-shot pseudo labels.
The main contributions of our paper are as follows:
Figure 2. Concept figure of LENeRF. Our method enables local editing of 3D assets by generating the target feature and estimating a 3D mask which guides the model on where to make changes at the feature level. Note that the mask is estimated for tri-plane features, not for raw RGB outputs. sentations such as voxels and meshes are commonly used for 3D generation and editing [17,31,54], they are memory-In contrast, recent ad-intensive and lack photorealism. vances in Neural Radiance Fields (NeRF) [33] have shown promising progress in representing 3D environments using implicit representations [14, 21, 33, 37] combined with vol-ume rendering techniques that enable high-quality novel view synthesis. NeRF-based 3D GANs [4, 5, 11, 16, 36, 43, 50,52] have made further progress towards generating a cat-egory of 3D aware contents with a single model, extending the per-scene optimization scheme of NeRF.
Several studies [29, 44, 45, 48] have attempted to address the challenges of NeRF editing, yet certain limitations per-sist. Works such as Edit-NeRF [29] and CLIP-NeRF [48] have pioneered NeRF manipulations, but they are con-strained to low-resolution synthetic datasets and lack the capability to perform localized editing. Opposed to trans-lation [9, 53] or style transfer [13] tasks, editing typically
• We introduce Local Editing NeRF (LENeRF), a 3D content editing framework capable of localized, photo-realistic editing using a convenient real-time text-based interface.
• Our method consists of add-on modules and does not require any domain-specific labels, allowing the method to be generalized to other models and domains.
• Our proposed technique involves a novel 3D distilla-tion of CLIP knowledge, specifically an unsupervised approach that utilizes the 3D GAN and CLIP models jointly to generate 3D masks.
• We present diverse quantitative and qualitative results, along with various applications such as sequential edit-ing, real image editing, and out-of-distribution editing. 2.