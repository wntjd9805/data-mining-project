Abstract
Real-world visual search systems involve deployments on multiple platforms with different computing and stor-age resources. Deploying a unified model that suits the minimal-constrain platforms leads to limited accuracy. It is expected to deploy models with different capacities adapt-ing to the resource constraints, which requires features ex-tracted by these models to be aligned in the metric space.
The method to achieve feature alignments is called “com-patible learning”. Existing research mainly focuses on the one-to-one compatible paradigm, which is limited in learn-ing compatibility among multiple models. We propose a
Switchable representation learning Framework with Self-Compatibility (SFSC). SFSC generates a series of compati-ble sub-models with different capacities through one train-ing process. The optimization of sub-models faces gradients conflict, and we mitigate this problem from the perspective of the magnitude and direction. We adjust the priorities of sub-models dynamically through uncertainty estimation to co-optimize sub-models properly. Besides, the gradients with conflicting directions are projected to avoid mutual in-terference. SFSC achieves state-of-the-art performance on the evaluated datasets. 1.

Introduction
Visual search systems are widely deployed, which recall the nearest neighbors in gallery features according to their distances to the query feature. Real-world visual search systems consist of multiple models deployed on different platforms [17, 38, 45] (e.g. clouds, mobiles, smart cam-eras), where different platforms interact with each other by visual features. Typically, as for person re-identification systems, images are captured and processed into features on edge sides. And such features are sent to the cloud side to compare with the database features for identifying
*Corresponding author. a specific person by feature similarities. As diverse plat-forms meet different computing and storage resource limita-tions, deploying the unified model which suits the minimal-constrain platforms leads to a waste of resources on other platforms and limited accuracy. To make better use of re-sources and achieve higher accuracy, it is expected to deploy models with different capacities which adapt to the resource limitations. Such a solution requires compatibility among the models to satisfy their interaction requirements, which means that the similar data processed by different models are close to each other in the feature space while dissimilar data are far apart.
To achieve compatibility, compatible learning methods have been proposed. Existing research focuses on the one-to-one compatible paradigm [6,49,62], which constrains the learned features of a latter (learnable) model to be compat-ible with its previous (fixed) version. They are limited in achieving many-to-many compatibilities, which means any two in a series of models are compatible with each other.
In this work, we propose a Switchable representation learning Framework with Self-Compatibility (SFSC) to achieve many-to-many compatibilities. As shown in Figure 1, SFSC can deploy different sub-models to suit the diverse computing and storage resource limitation of different plat-forms. The compatibility between any two sub-models can be achieved, termed as “self-compatibility”. However, there are gradients conflicts between sub-models as they are co-optimized during the training process. We summarize such conflicts into the gradient magnitude and gradient direc-tion. Specifically, a gradient of a large magnitude will dom-inate the optimization process, which may lead to the over-fitting of its corresponding sub-model and impair the im-provements of other sub-models. To solve this problem, we estimate the uncertainty of different sub-models to weight the gradient, denoting the optimization priorities. Besides, as the sub-models may produce gradients in various direc-tions, there is mutual interference between them. Thus the improvement in different sub-models may be overestimated
Figure 1. An example of SFSC in multi-platform deployments. The switchable neural network generates a series of sub-networks
{ϕ1, ϕ2, ..., ϕm} with different capacities for different computing resources. ∀i, j ∈ {1, 2, ..., m}, |ϕi(a) − ϕj(p)| < |ϕi(a) − ϕj(n)|, where (a, p) is a pair of samples with the same class ID, while (a, n) is a pair of samples with different class IDs. or underestimated. To tackle such conflict, the gradients are projected onto planes that are orthogonal to each other.
The contributions of this paper are summarized as fol-lows:
• We propose a switchable representation learning framework with self-compatibility (SFSC). SFSC gen-erates a series of feature-compatible sub-models with different capacities for visual search systems, which can be deployed on different platforms flexibly.
• We resolve the conflicts between sub-models from the aspect of gradient magnitude and gradient direction. A compatible loss based on uncertainty estimation is pro-posed to guide optimization priorities and alleviate the imbalance of gradient magnitude between sub-models.
An aggregation method based on the gradient projec-tion is proposed to avoid mutual interference and find a generic optimal direction for all sub-models.
• SFSC achieves state-of-the-art performance on the evaluated benchmark datasets. Compared to deploy-ing a unified model, adopting SFSC to obtain different sub-models can achieve 6% to 8% performance im-provements on three different datasets. 2.