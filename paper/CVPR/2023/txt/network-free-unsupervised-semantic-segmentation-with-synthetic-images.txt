Abstract 1.

Introduction
We derive a method that yields highly accurate semantic segmentation maps without the use of any additional neu-ral network, layers, manually annotated training data, or supervised training. Our method is based on the observa-tion that the correlation of a set of pixels belonging to the same semantic segment do not change when generating syn-thetic variants of an image using the style mixing approach in GANs. We show how we can use GAN inversion to ac-curately semantically segment synthetic and real photos as well as generate large training image-semantic segmenta-tion mask pairs for downstream tasks.
Semantic segmentation is a computer vision prob-lem with countless important applications, including self-driving cars, medical image analysis, and image content generation and editing [5, 11, 12, 19]. Yet, attaining accu-rate semantic segmentation masks remains an open prob-lem [18, 28]. A recent proposed solution is to synthesize large training data-sets of photo-realistic images and their masks using generative models like Generative Adversarial
Networks (GANs) [1, 6, 18, 19, 28]. However, these meth-ods require 1. adding and training an extra neural network to synthesize the mask, increasing model and training com-plexity, and 2. very costly pixel-wise human annotations on a large set of training images for every type of object and
scene of interest [19, 28].
Here, we propose a new algorithm that does not require the addition of any extra network, costly pixel-wise human annotations, or supervised training. Our key observation is that the correlation of a set of pixels belonging to the same semantic segment do not change when generating synthetic variants of an image using the style mixing approach [14].
This allows us to derive an unsupervised algorithm to gen-erate highly accurate semantic segmentation masks without the need to incorporate new nets or layers to existing ones or the need to re-train them. We show how our algorithm can be used to semantically segment real photos, generate synthetic data to successfully train semantic segmentation algorithms, and create semantic segmentation masks for ap-plications like style mixing Fig. 1.
In recent years, a number of works [1, 4, 6, 19, 21, 24, 28] have emerged to address the problem of semantic segmenta-tion with synthetic images. The difference of this solution, compared to a classical semantic segmentation methods on photos, is that we can take advantage of the rich semantic structured in models like StyleGAN2. This, combined with cheap photo-realistic image synthesis at scale, provides the possibility to synthesize large training sets with their se-mantic masks to train semantic segmentation algorithms at low cost while attaining better or state-of-the-art results [4]. 2.