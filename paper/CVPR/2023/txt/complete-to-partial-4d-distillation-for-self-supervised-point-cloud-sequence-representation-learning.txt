Abstract
Recent work on 4D point cloud sequences has attracted a lot of attention. However, obtaining exhaustively labeled 4D datasets is often very expensive and laborious, so it is especially important to investigate how to utilize raw unla-beled data. However, most existing self-supervised point cloud representation learning methods only consider ge-ometry from a static snapshot omitting the fact that se-quential observations of dynamic scenes could reveal more comprehensive geometric details. To overcome such is-sues, this paper proposes a new 4D self-supervised pre-training method called Complete-to-Partial 4D Distillation.
Our key idea is to formulate 4D self-supervised represen-tation learning as a teacher-student knowledge distilla-tion framework and let the student learn useful 4D repre-sentations with the guidance of the teacher. Experiments show that this approach significantly outperforms previ-ous pre-training approaches on a wide range of 4D point cloud sequence understanding tasks. Code is available at: https://github.com/dongyh20/C2P. 1.

Introduction
Recently, there is a surge of interest in understanding point cloud sequences in 4D (3D space + 1D time) [7, 8, 11, 21, 30]. As the direct sensor input for a wide range of applications including robotics and augmented reality, point cloud sequences faithfully depict a dynamic environ-ment regarding its geometric content and object movements in the context of the camera ego-motion. Though widely accessible, such 4D data is prohibitively expensive to an-notate in large scale with fine details. As a result, there is a strong need for leveraging the colossal amount of un-labeled sequences. Among the possible solutions, self-supervised representation learning has shown its effective-ness in a wide range of fields including images [2, 12, 13], videos [6, 9, 15, 22, 28] and point clouds [16, 24, 31, 33, 34].
*Equal contribution with the order determined by rolling dice.
â€ Corresponding author.
Figure 1. We propose a complete-to-partial 4D distillation (C2P) approach. Our key idea is to formulate 4D self-supervised rep-resentation learning as a teacher-student knowledge distillation framework in which students learn useful 4D representations un-der the guidance of a teacher. The learned features can be trans-ferred to a range of 4D downstream tasks.
We therefore aim to fill in the absence of self-supervised point cloud sequence representation learning in this work.
Learning 4D representations in a self-supervised man-ner seems to be a straightforward extension of 3D cases.
However, a second thought reveals its challenging nature since such representations need to unify the geometry and motion information in a synergetic manner. From the ge-ometry aspect, a 4D representation learner needs to under-stand 3D geometry in a dynamic context. However, most existing self-supervised point cloud representation learn-ing methods [16, 24, 34] only consider geometry from a static snapshot, omitting the fact that sequential observa-tions of dynamic scenes could reveal more comprehensive geometric details. From the motion aspect, a 4D represen-tation learner needs to understand motion in the 3D space, which requires an accurate cross-time geometric associa-tion. Nevertheless, existing video representation learning frameworks [6, 9, 22] mostly model motion as image space flows so geometric-aware motion cues are rarely encoded.
Due to such challenges, 4D has been rarely discussed in the self-supervised representation learning literature, with only a few works [4, 23] designing learning objectives in 4D to facilitate static 3D scene understanding.
To address the above challenges, we examine the na-ture of 4D dynamic point cloud sequences, and draw two
main observations. First, most of a point cloud sequence depicts the same underlying 3D content with an optional dynamic motion. Motion understanding could help aggre-gate temporal observations to form a more comprehensive geometric description of the scene. Second, geometric cor-respondences across time could help estimate the relative motion between two frames. Therefore better geometric un-derstanding should facilitate a better motion estimation. At the core are the synergetic nature of geometry and motion.
To facilitate the synergy of geometry and motion, we de-velop a Complete-to-Partial 4D Distillation (C2P) method.
Our key idea is to formulate 4D self-supervised represen-tation learning as a teacher-student knowledge distillation framework and let the student learn useful 4D representa-tions with the guidance of the teacher. And we present a unified solution to the following three questions: How to teach the student to aggregate sequential geometry for more complete geometric understanding leveraging motion cues?
How to teach the student to predict motion based upon bet-ter geometric understanding? How to form a stable and high-quality teacher?
In particular, our C2P method consists of three key de-signs. First, we design a partial-view 4D sequence genera-tion method to convert an input point cloud sequence which is already captured partially into an even more partial se-quence. This is achieved by conducting view projection of input frames following a generated camera trajectory. The generated partial 4D sequence allows bootstrapping multi-frame geometry completion. This is achieved by feeding the input sequence and the generated partial-view sequence to teacher and student networks respectively and distill the teacher knowledge to a 4D student network. Second, the student network not only needs to learn completion by mim-icking the corresponding frames of the teacher network, but also needs to predict the teacher features of other frames within a time window, to achieve so-called 4D distillation.
Notice the teacher feature corresponds to more complete ge-ometry, which also encourages the student to exploit the benefit of geometry completion in motion prediction. Fi-nally, we design an asymmetric teacher-student distillation framework for stable training and high-quality representa-tion, i.e., the teacher network has weaker expressivity com-pared with the student but is easier to optimize.
We evaluate our method on three downstream tasks including indoor and outdoor scenarios: 4D action seg-mentation on HOI4D [21], 4D semantic segmentation on
HOI4D [21], 4D semantic segmentation on Synthia 4D [27] and 3D action recognition on MSR-action3D [17]. We demonstrate significant improvements over the previous method(+2.5% accuracy on HOI4D action segmentation,
+1.0% mIoU on HOI4D semantic segmentation, +1.0% mIoU on Synthia 4D semantic segmentation and +2.1% ac-curacy on MSR-Action3D).
The contributions of this paper are fourfold: First, we propose a new 4D self-supervised representation learning method named Complete-to-Partial 4D Distillation which facilitates the synergy of geometry and motion learning.
Second, we propose a natural and effective way to generate partial-view 4D sequences and demonstrate that it can work well as learning material for knowledge distillation. Third, we find that asymmetric design is crucial in the complete-to-partial knowledge distillation process and we propose a new asymmetric distillation architecture. Fourth, extensive experiments on three tasks show that our method outper-forms previous state-of-the-art methods by a large margin. 2.