Abstract
Clustering has been a major research topic in the field of machine learning, one to which Deep Learning has re-cently been applied with significant success. However, an aspect of clustering that is not addressed by existing deep clustering methods, is that of efficiently producing multi-ple, diverse partitionings for a given dataset. This is par-ticularly important, as a diverse set of base clusterings are necessary for consensus clustering, which has been found to produce better and more robust results than relying on a single clustering. To address this gap, we propose Div-Clust, a diversity controlling loss that can be incorporated into existing deep clustering frameworks to produce mul-tiple clusterings with the desired degree of diversity. We conduct experiments with multiple datasets and deep clus-tering frameworks and show that: a) our method effectively controls diversity across frameworks and datasets with very small additional computational cost, b) the sets of clus-terings learned by DivClust include solutions that signifi-cantly outperform single-clustering baselines, and c) using an off-the-shelf consensus clustering algorithm, DivClust produces consensus clustering solutions that consistently outperform single-clustering baselines, effectively improv-ing the performance of the base deep clustering frame-work. Code is available at https://github.com/
ManiadisG/DivClust. 1.

Introduction
The exponentially increasing volume of visual data, along with advances in computing power and the develop-ment of powerful Deep Neural Network architectures, have revived the interest in unsupervised learning with visual data. Deep clustering in particular has been an area where significant progress has been made in the recent years. Ex-isting works focus on producing a single clustering, which is evaluated in terms of how well that clustering matches
*Corresponding author the ground truth labels of the dataset in question. However, consensus, or ensemble, clustering remains under-studied in the context of deep clustering, despite the fact that it has been found to consistently improve performance over single clustering outcomes [3, 17, 45, 73].
Consensus clustering consists of two stages, specifically generating a set of base clusterings, and then applying a consensus algorithm to aggregate them. Identifying what properties ensembles should have in order to produce better outcomes in each setting has been an open problem [18].
However, research has found that inter-clustering diver-sity within the ensemble is an important, desirable fac-tor [14,20,24,34,51], along with individual clustering qual-ity, and that diversity should be moderated [15,22,51]. Fur-thermore, several works suggest that controlling diversity in ensembles is important toward studying its impact and determining its optimal level in each setting [22, 51].
The typical way to produce diverse clusterings is to pro-mote diversity by clustering the data multiple times with different initializations/hyperparameters or subsets of the data [3, 17]. This approach, however, does not guaran-tee or control the degree of diversity, and is computation-ally costly, particularly in the context of deep clustering, where it would require the training of multiple models.
Some methods have been proposed that find diverse clus-terings by including diversity-related objectives to the clus-tering process, but those methods have only been applied to clustering precomputed features and cannot be trivially incorporated into Deep Learning frameworks. Other meth-ods tackle diverse clustering by creating and clustering di-verse feature subspaces, including some that apply this ap-proach in the context of deep clustering [48, 61]. Those methods, however, do not control inter-clustering diversity.
Rather, they influence it indirectly through the properties of the subspaces they create. Furthermore, typically, exist-ing methods have been focusing on producing orthogonal clusterings or identifying clusterings based on independent attributes of relatively simple visual data (e.g. color/shape).
Consequently, they are oriented toward maximizing inter-clustering diversity, which is not appropriate for consensus
Figure 1. Overview of DivClust. Assuming clusterings A and B, the proposed diversity loss Ldiv calculates their similarity matrix SAB and restricts the similarity between cluster pairs to be lower than a similarity upper bound d. In the figure, this is represented by the model adjusting the cluster boundaries to produce more diverse clusterings. Best seen in color. clustering [15, 22, 51].
To tackle this gap, namely generating multiple cluster-ings with deep clustering frameworks efficiently and with the desired degree of diversity, we propose DivClust. Our method can be straightforwardly incorporated into exist-ing deep clustering frameworks to learn multiple cluster-ings whose diversity is explicitly controlled. Specifically, the proposed method uses a single backbone for feature ex-traction, followed by multiple projection heads, each pro-ducing cluster assignments for a corresponding clustering.
Given a user defined diversity target, in this work expressed in terms of the average NMI between clusterings, DivClust restricts inter-clustering similarity to be below an appropri-ate, dynamically estimated threshold. This is achieved with a novel loss component, which estimates inter-clustering similarity based on soft cluster assignments produced by the model, and penalizes values exceeding the threshold. Im-portantly, DivClust introduces minimal computational cost and requires no hyperparameter tuning with respect to the base deep clustering framework, which makes its use sim-ple and computationally efficient.
Experiments on four datasets (CIFAR10, CIFAR100,
Imagenet-10, Imagenet-Dogs) with three recent deep clus-tering methods (IIC [37], PICA [32], CC [44]) show that
DivClust can effectively control inter-clustering diversity without reducing the quality of the clusterings. Further-more, we demonstrate that, with the use of an off-the-shelf consensus clustering algorithm, the diverse base clusterings learned by DivClust produce consensus clustering solutions that outperform the base frameworks, effectively improving them with minimal computational cost. Notably, despite the sensitivity of consensus clustering to the properties of the ensemble, our method is robust across various diversity lev-els, outperforming baselines in most settings, often by large margins. Our work then provides a straightforward way for improving the performance of deep clustering frameworks, as well as a new tool for studying the impact of diversity in deep clustering ensembles [51].
In summary, DivClust: a) can be incorporated in ex-isting deep clustering frameworks in a plug-and-play way with very small computational cost, b) can explicitly and effectively control inter-clustering diversity to satisfy user-defined targets, and c) learns clusterings that can improve the performance of deep clustering frameworks via consen-sus clustering. 2.