Abstract
Depth estimation from a monocular 360◦ image is a bur-geoning problem owing to its holistic sensing of a scene.
Recently, some methods, e.g., OmniFusion, have applied the tangent projection (TP) to represent a 360◦ image and predicted depth values via patch-wise regressions, which are merged to get a depth map with equirectangular pro-jection (ERP) format. However, these methods suffer from 1) non-trivial process of merging plenty of patches; 2) cap-turing less holistic-with-regional contextual information by
In this directly regressing the depth value of each pixel. paper, we propose a novel framework, HRDFuse, that sub-tly combines the potential of convolutional neural networks (CNNs) and transformers by collaboratively learning the holistic contextual information from the ERP and the re-gional structural information from the TP. Firstly, we pro-pose a spatial feature alignment (SFA) module that learns feature similarities between the TP and ERP to aggregate the TP features into a complete ERP feature map in a pixel-wise manner. Secondly, we propose a collaborative depth distribution classification (CDDC) module that learns the holistic-with-regional histograms capturing the ERP and
TP depth distributions. As such, the final depth values can be predicted as a linear combination of histogram bin cen-ters. Lastly, we adaptively combine the depth predictions from ERP and TP to obtain the final depth map. Extensive experiments show that our method predicts more smooth and accurate depth results while achieving favorably bet-ter results than the SOTA methods.
Multimedia Material
For videos, code, demo and more information, you can visit https://VLIS2022.github.io/HRDFuse/ 1.

Introduction
The 360◦ camera is becoming increasingly popular as a 360◦ image provides holistic sensing of a scene with a wide
*Corresponding author (e-mail: linwang@ust.hk)
Figure 1. (a) Our HRDFuse employs the SFA module to align the regional information in discrete TP patches and holistic informa-tion in a complete ERP image. The CDDC module is proposed to estimate ERP format depth outputs from both the ERP image and
TP patches based on holistic-with-regional depth histograms. (b)
Compared with OmniFusion [30], our depth predictions are more smooth and more accurate. field of view (FoV) [1, 4, 19, 44, 48, 52]. Therefore, the abil-ity to infer the 3D structure of a 360◦ camera’s surroundings has sparked the research for monocular 360◦ depth estima-tion [23, 36, 43, 45]. Generally, raw 360◦ images are trans-mitted into 2D planar representations while preserving the omnidirectional information [12, 50]. Equirectangular pro-jection (ERP) is the most commonly used projection for-mat [38, 49] and can provide a complete view of a scene.
Cubemap projection (CP) [9] projects 360◦ contents into six discontinuous faces of a cube to reduce the distortion; thus, the pre-trained 2D convolutional neural networks (CNNs) can be applied. However, ERP images suffer from severe distortions in the polar regions, while CP patches are ham-pered by geometric discontinuity and limited FoV.
For this reason, some works [53, 54] have proposed distortion-aware convolution filters to tackle the ERP dis-tortion problem for depth estimation. BiFuse [43] and Uni-Fuse [23] explore the complementary information from the
ERP image and CP patches to predict the depth map.
Recently, research has shown that it is promising to use tangent projection (TP) because TP patches have less dis-tortion, and many pre-trained CNN models designed for perspective images can be directly applied [16]. How-ever, there exist unavoidable overlapping areas between two neighbouring TP patches, as can be justified by the geomet-ric relationship in Fig. 2. Therefore, directly re-projecting the results from TP patches into the ERP format is compu-tationally complex. Accordingly, 360MonoDepth [34] pre-dicts the patch-wise depth maps from a set of TP patches using the state-of-the-art (SOTA) perspective depth estima-tors, which are aligned and merged to obtain an ERP format depth map. OmniFusion [30] proposes a framework lever-aging CNNs and transformers to predict depth maps from the TP inputs and merges these patch-wise predictions to the
ERP space based on geometric prior information to get the final depth output with ERP format. However, these meth-ods suffer from two critical limitations because: 1) geomet-rically merging a large number of patches is computation-ally heavy; 2) they ignore the holistic contextual informa-tion contained only in the ERP image and directly regress the depth value of each pixel, leading to less smooth and accurate depth estimation results.
To tackle these issues, we propose a novel framework, called HRDFuse, that subtly combines the potential of con-volutional neural networks (CNNs) and transformers by collaboratively exploring the holistic contextual informa-tion from the ERP and regional structural information from the TP (See Fig. 1(a) and Fig. 3). Compared with previ-ous methods, our method achieves more smooth and more accurate depth estimation results while maintaining high ef-ficiency with three key components. Firstly, for each pro-jection, we employ a CNN-based feature extractor to extract spatially consistent feature maps and a transformer encoder to learn the depth distribution with long-range feature de-pendencies.
In particular, to efficiently aggregate the in-dividual TP information into an ERP space, we propose a spatial feature alignment (SFA) module to learn a spatially aligned index map based on feature similarities between
ERP and TP. With this index map, we can efficiently mea-sure the spatial location of each TP patch in the ERP space and achieve pixel-level fusion of TP information to obtain a smooth output in ERP format. Secondly, we propose a col-laborative depth distribution classification (CDDC) module to learn the holistic depth distribution histogram from the
ERP image and regional depth distribution histograms from the collection of TP patches. Consequently, the pixel-wise depth values can be predicted as a linear combination of his-togram bin centers. Lastly, the final result is adaptive fused by two ERP format depth predictions from ERP and TP.
We conduct extensive experiments on three bench-mark datasets: Stanford2D3D [2], Matterport3D [7], and 3D60 [54]. The results show that our method can achieve
Figure 2. Geometric relationship between TP and ERP. Two TP patches are projected from the red area and yellow area. more smooth and more accurate depth results while favor-ably surpassing the existing methods by a significant margin on 3D60 and Stanford2D3D datasets (See Fig. 1 and Tab. 1).
In summary, our main contributions are four-fold: (I) We propose HRDFuse that combines the holistic contextual in-formation from the ERP and regional structural information from the TP. (II) We introduce the SFA module to efficiently aggregate the TP features into the ERP format, relieving the need for expensive re-projection operations. (III) We pro-pose the CDDC module to learn the holistic-with-regional depth distributions and estimate the depth value based on the histogram bin centers. 2.