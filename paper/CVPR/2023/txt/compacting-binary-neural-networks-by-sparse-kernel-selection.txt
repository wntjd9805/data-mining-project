Abstract
Binary Neural Network (BNN) represents convolution weights with 1-bit values, which enhances the efficiency of storage and computation. This paper is motivated by a pre-viously revealed phenomenon that the binary kernels in suc-cessful BNNs are nearly power-law distributed: their val-ues are mostly clustered into a small number of codewords.
This phenomenon encourages us to compact typical BNNs and obtain further close performance through learning non-repetitive kernels within a binary kernel subspace. Specifi-cally, we regard the binarization process as kernel grouping in terms of a binary codebook, and our task lies in learning to select a smaller subset of codewords from the full code-book. We then leverage the Gumbel-Sinkhorn technique to approximate the codeword selection process, and develop the Permutation Straight-Through Estimator (PSTE) that is able to not only optimize the selection process end-to-end but also maintain the non-repetitive occupancy of se-lected codewords. Experiments verify that our method re-duces both the model size and bit-wise computational costs, and achieves accuracy improvements compared with state-of-the-art BNNs under comparable budgets. 1.

Introduction
It is crucial to design compact Deep Neural Networks (DNNs) which allow the model deployment on resource-constrained embedded devices, since most powerful DNNs including ResNets [10] and DenseNets [13] are storage costly with deep and rich building blocks piled up. Plenty of approaches have been proposed to compress DNNs, among which network quantization [15, 43, 45] is able to reduce memory footprints as well as accelerate the inference speed by converting full-precision weights to discrete values. Bi-nary Neural Networks (BNNs) [2, 14] belong to the fam-ily of network quantization but they further constrict the parameter representations to binary values (±1).
In this way, the model is largely compressed. More importantly, floating-point additions and multiplications in conventional
DNNs are less required and mostly reduced to bit-wise op-erations that are well supported by fast inference acceler-ators [32], particularly when activations are binarized as well. To some extent, this makes BNNs more computation-ally efficient than other compression techniques, e.g., net-work pruning [9, 11, 25] and switchable models [37, 41, 42].
Whilst a variety of methods are proposed to improve the performance of BNNs, seldom is there a focus on discussing how the learnt binary kernels are distributed in BNNs. A re-cent work SNN [38] demonstrates that, by choosing typical convolutional BNN models [27, 31, 32] well trained on Im-ageNet and displaying the distribution of the 3 × 3 kernels along all possible 23×3 binary values (a.k.a. codewords), these kernels nearly obey the power-law distribution: only a small portion of codewords are activated for the most time.
Such a phenomenon is re-illustrated in Figure 1(b). This observation motivates SNN to restrict the size of the code-book by removing those hardly-selected codewords. As a result, SNN is able to compact BNN further since index-ing the kernels with a smaller size of codebook results in a compression ratio of log2(n)/ log2(N ), where n and N are separately the sizes of the compact and full codebooks.
However, given that the size of codebook is limited (only 512), the sub-codebook degenerates during training since codewords are likely to become repetitive. Therefore, we believe the clustering property of kernels can be further ex-ploited during the training of BNNs. To do so, we refor-mulate the binary quantization process as a grouping task that selects, for each kernel, the nearest codeword from a binary sub-codebook which is obtained by selecting opti-mal codewords from the full one. To pursue an optimal solution and retain the non-repetitive occupancy of the se-lected codewords, we first convert the sub-codebook se-lection problem to a permutation learning task. However, learning the permutation matrix is non-differential since the permutation matrix is valued with only 0/1 entries. Inspired by the idea in [29], we introduce the Gumbel-Sinkhorn op-Figure 1. Codebook distributions under different decomposition approaches. Statistics in both sub-figures are collected from the same
BNN model (XNOR-Net [32] upon ResNet-18 [10]) well-trained on ImageNet [4]. In (a), each codeword is a flattened sub-vector (of size 1 × 9). In (b), each codeword is a 3 × 3 convolution kernel. The codebook in either sub-figure consists of 29 = 512 different codewords. Upper tables provide the total percentages of the top-n most frequent codewords. In (c), we observe that the sub-codebook highly degenerates during training, since codewords tend to be repetitive when being updated independently. While in (d), the diversity of codewords preserves, which implies the superiority of our selection-based learning. eration to generate a continuous and differential approxima-tion of the permutation matrix. During training, we further develop Permutation Straight-Through Estimator (PSTE), a novel method that tunes the approximated permutation ma-trix end-to-end while maintaining the binary property of the selected codewords. The details are provided in § 3.2 and
§ 3.3. We further provide the complexity analysis in § 3.4.
Extensive results on image classification and object de-tection demonstrate that our architecture noticeably reduces the model size as well as the computational burden. For ex-ample, by representing ResNet-18 with 0.56-bit per weight on ImageNet, our method brings in 214× saving of bit-wise operations, and 58× reduction of the model size. Though state-of-the-art BNNs have achieved remarkable compres-sion efficiency, we believe that further compacting BNNs is still beneficial, by which we can adopt deeper, wider, and thus more expressive architectures without exceeding the complexity budget than BNNs. For example, our 0.56-bit ResNet-34 obtains 1.7% higher top-1 accuracy than the state-of-the-art BNN on ResNet-18, while its computational costs are lower and the storage costs are almost the same.
Existing methods [20, 21] (apart from SNN [38]) that also attempt to obtain more compact models than BNNs are quite different with ours as will be described in § 2. One of the crucial points is that their codewords are sub-vectors from (flattened) convolution weights across multiple chan-nels, whereas our each codeword corresponds to a com-plete kernel that maintains the spatial dimensions (weight and height) of a single channel. The reason why we formu-late the codebook in this way stems from the observation in
Figure 1(b), where the kernels are sparsely clustered. Dif-ferently, as shown in Figure 1(a), the codewords are nearly uniformly activated if the codebook is constructed from flat-tened sub-vectors, which could because the patterns of the input are spatially selective but channel-wise uniformly dis-tributed. It is hence potential that our method may recover better expressivity of BNNs by following this natural char-acteristic. In addition, we optimize the codewords via non-repetitive selection from a fixed codebook, which rigorously ensures the dissimilarity between every two codewords and thus enables more capacity than the product quantization method used in [20], as compared in Figure 1(c)(d). On Im-ageNet with the same backbone, our method exceeds [20] and [21] by 6.6% and 4.5% top-1 accuracies, respectively. 2.