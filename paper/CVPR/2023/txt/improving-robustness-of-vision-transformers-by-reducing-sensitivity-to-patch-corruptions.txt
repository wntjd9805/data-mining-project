Abstract
Despite their success, vision transformers still remain vulnerable to image corruptions, such as noise or blur. In-deed, we find that the vulnerability mainly stems from the unstable self-attention mechanism, which is inherently built upon patch-based inputs and often becomes overly sensi-tive to the corruptions across patches. For example, when we only occlude a small number of patches with random noise (e.g., 10%), these patch corruptions would lead to se-vere accuracy drops and greatly distract intermediate at-tention layers. To address this, we propose a new training method that improves the robustness of transformers from a new perspective – reducing sensitivity to patch corruptions (RSPC). Specifically, we first identify and occlude/corrupt the most vulnerable patches and then explicitly reduce sen-sitivity to them by aligning the intermediate features be-tween clean and corrupted examples. We highlight that the construction of patch corruptions is learned adversarially to the following feature alignment process, which is partic-ularly effective and essentially different from existing meth-ods. In experiments, our RSPC greatly improves the sta-bility of attention layers and consistently yields better ro-bustness on various benchmarks, including CIFAR-10/100-C, ImageNet-A, ImageNet-C, and ImageNet-P. 1.

Introduction
Despite the success of vision transformers [10] in recent years, they still lack robustness against common image cor-ruptions [24, 52], such as noise or blur, and adversarial per-turbations [13, 15, 42]. For example, even for the state-of-the-art robust architectures, e.g., RVT [34] and FAN [61], the accuracy drops by more than 15% on corrupted exam-ples, e.g., with Gaussian noise, as shown in Figure 2 (blue star on the right). We suspect that this vulnerability is in-herent to the used self-attention mechanism, which relies on patch-based inputs and may easily become overly sensitive to corruptions or perturbations upon them.
Figure 1. Sensitivity to patch perturbations/corruptions in terms of confidence score of the ground-truth class. We randomly select 10% patches to be perturbed/corrupted for RVT-Ti [34]. In prac-tice, adversarial patch perturbations (often invisible) significantly reduce the confidence, indicating the high sensitivity of transform-ers to patches. However, directly adding random noise only yields marginal degradation even with the highest severity in ImageNet-C [24]. By contrast, occluding patches with noise greatly reduces the confidence and can be used as a good proxy of adversarial patch perturbations to reveal the patch sensitivity issue.
A piece of empirical evidence is that transformers can be easily misled by the adversarial perturbations only on very few patches (even a single patch [13]). As shown in Figure 1, given a clean image, we randomly sample a small number of patches, e.g., 10%, and introduce pertur-bations/corruptions into them. Considering RVT [34] as a strong baseline, when we generate adversarial perturba-tions using PGD-5, these perturbed patches greatly reduce the confidence score from 63.8% to 3.1% and result in a misclassification. Nevertheless, generating adversarial per-turbations can be very computationally expensive (e.g., 5× longer training time for PGD-5), which makes adversarial training often infeasible on large-scale datasets [26, 39, 53], e.g., ImageNet. Instead, an efficient way is directly adding corruptions, e.g., random noise, on top of these patches. In practice, even with the highest severity in ImageNet-C [24], these corrupted patches only yield a marginal degradation in terms of confidence score. Thus, how to construct patch corruptions that greatly misleads the model and can be pro-duced very efficiently becomes a critical problem.
Interestingly, if we totally discard these patches and oc-clude them with random noise, the model becomes very vul-nerable again, e.g., with the confidence score dropping from 63.8% to 17.3% in Figure 1. More critically, these corrupted
Figure 2. Sensitivity to patch-based corruptions in terms of attention stability (left) and accuracy (right). Left: We randomly occlude 10% patches with noise and show the attention maps of different layers in RVT-Ti [34] and our RSPC-RVT-Ti. Following [13], we choose the center patch (red square) as the query and average the attention scores across all the attention heads for visualization. Regarding this example, we also compute the average cosine similarity (Cos-Sim) between the clean and corrupted attentions across different layers.
Clearly, our RSPC model yields more stable attention maps. Right: On ImageNet, we plot the distribution of accuracy on the occluded examples with different occlusion masks. Here, we randomly sample 100 different masks for each image. We show that RVT is very sensitive to the patch-based corruptions and has a much larger variance of accuracy than our RSPC model. patches also have significant impact on the attention maps across layers, as shown in Figure 2 (left). We suspect this to be the case due to the global interactions across tokens in the attention mechanism – even when occluding only few patches. Quantitatively, this can be captured by comput-ing the average cosine similarity between the attentions on clean and corrupted images across layers, denoted by Cos-Sim. Regarding the considered example in Figure 2, the
Cos-Sim of only 0.43 for RVT indicates a significant shift in attention – a phenomenon that we can observe across the en-tire ImageNet dataset (see Figure 5). In fact, these attention shifts also have direct and severe impact on accuracy: In
Figure 2 (right), we randomly sample 100 occlusion masks for each image and show the distribution of accuracy (blue box). Unsurprisingly, the accuracy decreases significantly when facing patch-based corruptions, compared to the orig-inal examples (blue star). These experiments highlight the need for an inherently more robust attention mechanism in order to improve the overall robustness of transformers.
We address this problem by finding particularly vulnera-ble patches to construct patch-based corruptions and stabi-lizing the intermediate attention layers against them. Since we use random noise to occlude patches, we move the focus from how to perturb patch content to finding which patch should be occluded. As shown in Figure 2 (right), with a fixed occlusion ratio, the accuracy varies a lot when occlud-ing different patches (e.g., ranging from 60% to 75% in the blue box). Since we seek to reduce the sensitivity to patch corruptions, occluding the most vulnerable (often very im-portant) patches and explicitly reducing the impact of them should bring the largest robustness improvement. Inspired by this, we seek to identify the most vulnerable patches to construct patch-based corruptions and then align the inter-mediate features to make the attention less sensitive to the corruptions in individual patches. In practice, we are able to reduce the impact of patch-based corruptions significantly, improving the Cos-Sim from 0.43 (for RVT-Ti) to 0.91 in
Figure 2 (left). This is also directly observed in the visual results where these corruptions have little impact on the in-termediate attention maps of our robust model. The stable attention mechanism also greatly improves the robustness of transformers. As shown in Figure 2 (right), compared with RVT, we obtain significantly higher accuracy when facing examples with different occlusion masks (red box), alongside the improved overall accuracy and robustness on full images (red star).
Contributions: In this paper, we study the sensitivity of transformers to patch corruptions and explicitly stabi-lize models against them to improve the robustness. Here, we make three key contributions: 1) We propose a new training method that improves robustness by reducing sen-sitivity to patch corruptions (RSPC). To this end, we first construct effective patch-based corruptions and then reduce the sensitivity to them by aligning the intermedi-ate features. 2) When constructing patch corruptions, we develop a patch corruption model to find particularly vul-nerable patches that severely distract intermediate attention layers. In practice, the corruption model is trained adver-sarially to the classification model, which, however, is es-sentially different from adversarial training methods. To be specific, we only learn which patch should be corrupted in-stead of the pixel-level perturbations. 3) In experiments, we demonstrate that the robustness improvement against patch corruptions (shown in Figure 2 (right)) can generalize well to diverse architectures on various robustness benchmarks, including ImageNet-A/C/P [24,60]. More critically, we can show, both qualitatively and quantitatively, that these im-provements stem from the more stable attention mechanism across layers. It is worth noting that, when compared with adversarial training methods, RSPC obtains a better tradeoff between accuracy and corruption robustness while keeping significantly lower training cost [57] (see Figure 7).
Figure 3. Overview of the proposed reducing sensitivity to patch corruptions (RSPC) training procedure. We present a patch corruption model to produce patch-based corruptions and align the features of each self-attention block between the clean and corrupted examples (the alignment loss is highlighted by green box). Unlike existing methods, we select the patches to be occluded/corrupted in an adversarial way, i.e., corrupting the most vulnerable patches that would greatly distract intermediate attention layers. 2.