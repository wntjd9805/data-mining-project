Abstract 1.

Introduction
Diffusion models, which learn to reverse a signal de-struction process to generate new data, typically require the signal at each step to have the same dimension. We argue that, considering the spatial redundancy in image signals, there is no need to maintain a high dimension-ality in the evolution process, especially in the early generation phase. To this end, we make a theoretical generalization of the forward diffusion process via signal decomposition. Concretely, we manage to decompose an image into multiple orthogonal components and control the attenuation of each component when perturbing the image. That way, along with the noise strength increasing, we are able to diminish those inconsequential components and thus use a lower-dimensional signal to represent the source, barely losing information. Such a reformulation allows to vary dimensions in both training and inference of diffusion models. Extensive experiments on a range of datasets suggest that our approach substantially reduces the computational cost and achieves on-par or even better synthesis performance compared to baseline methods. We also show that our strategy facilitates high-resolution image synthesis and improves FID of diffusion model trained on
FFHQ at 1024×1024 resolution from 52.40 to 10.46. Code is available at https://github.com/damo-vilab/dvdp.
∗corresponding author.
† Work performed at Alibaba DAMO Academy.
Diffusion models [2, 6, 9, 15, 21, 24, 28] have recently shown great potential in image synthesis. Instead of directly learning the observed distribution, it constructs a multi-step forward process through gradually adding noise onto the real data (i.e., diffusion). After a sufficiently large number of steps, the source signal could be considered completely destroyed, resulting in a pure noise distribution that naturally supports sampling. In this way, starting from sampled noises, we can expect new instances after reversing the diffusion process step by step.
As it can be seen, the above pipeline does not change the dimension of the source signal throughout the entire diffusion process [6,26,28]. It thus requires the reverse pro-cess to map a high-dimensional input to a high-dimensional output at every single step, causing heavy computation overheads [10, 22]. However, images present a measure of spatial redundancy [4] from the semantic perspective (e.g., an image pixel could usually be easily predicted according to its neighbours). Given such a fact, when the source signal is attenuated to some extent along with the noise strength growing, it should be possible to get replaced by a lower-dimensional signal. We therefore argue that there is no need to follow the source signal dimension along the entire distribution evolution process, especially at early steps (i.e., steps close to the pure noise distribution) for coarse generation.
Figure 2. Conceptual comparison between DDPM [6] and our proposed DVDP, where our approach allows using a varying dimension in the diffusion process.
In this work, we propose dimensionality-varying diffu-sion process (DVDP), which allows dynamically adjusting the signal dimension when constructing the forward path.
The varying dimensionality concept is shown in Fig. 2. For this purpose, we first decompose an image into multiple orthogonal components, each of which owns dimension lower than the original data. Then, based on such a decomposition, we theoretically generalize the conventional diffusion process such that we can control the attenuation of each component when adding noise. Thanks to this refor-mulation, we manage to drop those inconsequential com-ponents after the noise strength reaches a certain level, and thus represent the source image using a lower-dimensional signal with little information lost. The remaining diffusion process could inherit this dimension and apply the same technique to further reduce the dimension.
We evaluate our approach on various datasets, including objects, human faces, animals, indoor scenes, and outdoor scenes. Experimental results suggest that DVDP achieves on-par or even better synthesis performance than baseline models on all datasets. More importantly, DVDP relies on much fewer computations, and hence speeds up both train-ing and inference of diffusion models. We also demonstrate the effectiveness of our approach in learning from high-resolution data. For example, we are able to start from a 64×64 noise to produce an image under 1024×1024 resolu-tion. With FID [5] as the evaluation metric, our 1024×1024 model trained on FFHQ improves the baseline [28] from 52.40 to 10.46. All these advantages benefit from using a lower-dimensional signal, which reduces the computational cost and mitigates the optimization difficulty. 2.