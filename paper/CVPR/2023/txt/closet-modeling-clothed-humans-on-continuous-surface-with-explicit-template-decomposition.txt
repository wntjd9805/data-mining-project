Abstract
Creating animatable avatars from static scans re-quires the modeling of clothing deformations in different poses. Existing learning-based methods typically add pose-dependent deformations upon a minimally-clothed mesh template or a learned implicit template, which have limita-tions in capturing details or hinder end-to-end learning. In this paper, we revisit point-based solutions and propose to decompose explicit garment-related templates and then add pose-dependent wrinkles to them. In this way, the clothing deformations are disentangled such that the pose-dependent wrinkles can be better learned and applied to unseen poses.
Additionally, to tackle the seam artifact issues in recent state-of-the-art point-based methods, we propose to learn point features on a body surface, which establishes a contin-uous and compact feature space to capture the fine-grained and pose-dependent clothing geometry. To facilitate the re-search in this field, we also introduce a high-quality scan dataset of humans in real-world clothing. Our approach is validated on two existing datasets and our newly introduced dataset, showing better clothing deformation results in un-seen poses. The project page with code and dataset can be found at https://www.liuyebin.com/closet. 1.

Introduction
Animating 3D clothed humans requires the modeling of pose-dependent deformations in various poses. The diver-sity of clothing styles and body poses makes this task ex-tremely challenging. Traditional methods are based on ei-ther simple rigging and skinning [4,20,33] or physics-based simulation [14, 23, 24, 49], which heavily rely on artist ef-forts or computational resources. Recent learning-based methods [10, 36, 39, 59] resort to modeling the clothing de-formation directly from raw scans of clothed humans. De-spite the promising progress, this task is still far from be-ing solved due to the challenges in clothing representations, generalization to unseen poses, and data acquisition, etc.
Figure 1. Our method learns to decompose garment templates (top row) and add pose-dependent wrinkles upon them (bottom row).
For the modeling of pose-dependent garment geome-try, the representation of clothing plays a vital role in a learning-based scheme. As the relationship between body poses and clothing deformations is complex, an effec-tive representation is desirable for neural networks to cap-ture pose-dependent deformations. In the research of this line, meshes [2, 12, 38], implicit fields [10, 59], and point clouds [36, 39] have been adopted to represent clothing. In accordance with the chosen representation, the clothing de-formation and geometry features are learned on top of a fixed-resolution template mesh [8, 38], a 3D implicit sam-pling space [10,59], or an unfolded UV plane [2,12,36,39].
Among these representations, the mesh is the most efficient one but is limited to a fixed topology due to its discretization scheme. The implicit fields naturally enable continuous fea-ture learning in a resolution-free manner but are too flexible to satisfy the body structure prior, leading to geometry arti-facts in unseen poses. The point clouds enjoy the compact nature and topology flexibility and have shown promising results in the recent state-of-the-art solutions [36,39] to rep-resent clothing, but the feature learning on UV planes still leads to discontinuity artifacts between body parts.
To model the pose-dependent deformation of clothing, body templates such as SMPL [34] are typically leveraged to account for articulated motions. However, a body tem-plate alone is not ideal, since the body template only models the minimally-clothed humans and may hinder the learning of actual pose-dependent deformations, especially in cases of loose clothing. To overcome this issue, recent implicit approaches [59] make attempts to learn skinning weights in the 3D space to complement the imperfect body templates.
However, their pose-dependent deformations are typically coarse due to the difficulty in learning implicit fields. For explicit solutions, the recent approach [32] suggests learn-ing coarse templates implicitly at first and then the pose-dependent deformations explicitly. Despite its effective-ness, such a workaround requires a two-step modeling pro-cedure and hinders end-to-end learning.
In this work, we propose CloSET, an end-to-end method to tackle the above issues by modeling Clothed humans on a continuous Surface with Explicit Template decomposition.
We follow the spirit of recent state-of-the-art point-based approaches [32, 36, 39] as they show the efficiency and po-tential in modeling real-world garments. We take steps for-ward in the following aspects for better point-based model-ing of clothed humans. First, we propose to decompose the clothing deformations into explicit garment templates and pose-dependent wrinkles. Specifically, our method learns a garment-related template and adds the pose-dependent dis-placement upon them, as shown in Fig. 1. Such a garment-related template preserves a shared topology for various poses and enables better learning of pose-dependent wrin-kles. Different from the recent solution [32] that needs two-step procedures, our method can decompose the ex-plicit templates in an end-to-end manner with more gar-ment details. Second, we tackle the seam artifact issues
In-that occurred in recent point-based methods [36, 39]. stead of using unfolded UV planes, we propose to learn point features on a body surface, which supports a continu-ous and compact feature space. We achieve this by learn-ing hierarchical point-based features on top of the body surface and then using barycentric interpolation to sample features continuously. Compared to feature learning in the
UV space [39], on template meshes [8, 38], or in the 3D implicit space [57, 59], our body surface enables the net-work to capture not only fine-grained details but also long-range part correlations for pose-dependent geometry mod-eling. Third, we introduce a new scan dataset of humans in real-world clothing, which contains more than 2,000 high-quality scans of humans in diverse outfits, hoping to facili-tate the research in this field. The main contributions of this work are summarized below:
• We propose a point-based clothed human modeling method by decomposing clothing deformations into explicit garment templates and pose-dependent wrin-kles in an end-to-end manner. These learnable tem-plates provide a garment-aware canonical space so that pose-dependent deformations can be better learned and applied to unseen poses.
• We propose to learn point-based clothing features on a continuous body surface, which allows a continu-ous feature space for fine-grained detail modeling and helps to capture long-range part correlations for pose-dependent geometry modeling.
• We introduce a new high-quality scan dataset of clothed humans in real-world clothing to facilitate the research of clothed human modeling and animation from real-world scans. 2.