Abstract
Learning dense visual representations without labels is an arduous task and more so from scene-centric data. We propose to tackle this challenging problem by proposing a
Cross-view consistency objective with an Online Clustering mechanism (CrOC) to discover and segment the semantics of the views. In the absence of hand-crafted priors, the re-sulting method is more generalizable and does not require a cumbersome pre-processing step. More importantly, the clustering algorithm conjointly operates on the features of both views, thereby elegantly bypassing the issue of content not represented in both views and the ambiguous matching of objects from one crop to the other. We demonstrate excel-lent performance on linear and unsupervised segmentation transfer tasks on various datasets and similarly for video object segmentation. Our code and pre-trained models are publicly available at https://github.com/stegmuel/CrOC. 1.

Introduction
Self-supervised learning (SSL) has gone a long and suc-cessful way since its beginning using carefully hand-crafted proxy tasks such as colorization [26], jigsaw puzzle solv-ing [32], or image rotations prediction [14]. In recent years, a consensus seems to have been reached, and cross-view consistency is used in almost all state-of-the-art (SOTA) vi-sual SSL methods [5–7, 15, 19]. In that context, the whole training objective revolves around the consistency of rep-resentation in the presence of information-preserving trans-formations [7], e.g., blurring, cropping, solarization, etc.
Although this approach is well grounded in learning image-level representations in the unrealistic scenario of object-centric datasets, e.g., ImageNet [11], it cannot be trivially extended to accommodate scene-centric datasets and even
* denotes equal contribution.
Figure 1. Schematic for different categories of self-supervised learning methods for dense downstream tasks. a) Prior to the training, a pre-trained model or color-based heuristic is used to c) The produce the clustering/matching of the whole dataset. matching/clustering is identiﬁed online but restrains the domain of application of the loss to the intersection of the two views. b)
Our method takes the best of both worlds, leverages online cluster-ing, and enforces constraints on the whole spatiality of the views. less to learn dense representations. Indeed, in the presence of complex scene images, the random cropping operation used as image transformation loses its semantic-preserving property, as a single image can yield two crops bearing an-tipodean semantic content [31,35–37]. Along the same line, it’s not clear how to relate sub-regions of the image from one crop to the other, which is necessary to derive a local-ized supervisory signal.
To address the above issue, some methods [31, 36] con-strain the location of the crops based on some heuristics and using a pre-processing step. This step is either not learnable or requires the use of a pre-trained model. Alternatively, the location of the crops (geometric pooling [45, 51]) and/or an
attention mechanism (attentive pooling [33, 42, 44, 48, 51]) can be used to infer the region of overlap in each view and only apply the consistency objective to that region (Fig. 1.c). A consequence of these pooling mechanisms is that only a sub-region of each view is exploited, which mis-lays a signiﬁcant amount of the image and further questions the usage of cropping. There are two strategies to tackle the issue of locating and linking the objects from the two views: the ﬁrst is a feature-level approach that extends the global consistency criterion to the spatial features after inferring pairs of positives through similarity bootstrapping or posi-tional cues [2, 28, 30, 41, 48, 51]. It is unclear how much semantics a single spatial feature embeds, and this strat-egy can become computationally intensive. These issues motivate the emergence of the second line of work which operates at the object-level [20, 21, 38, 40, 43, 44, 47].
In that second scenario, the main difﬁculty lies in generating the object segmentation masks and matching objects from one view to the other. The straightforward approach is to leverage unsupervised heuristics [20] or pre-trained mod-els [47] to generate pseudo labels prior to the training phase (Fig. 1.a), which is not an entirely data-driven approach and cannot be trivially extended to any modalities. Alter-natively, [21] proposed to use K-Means and an additional global image (encompassing the two main views) to gener-ate online pseudo labels, but this approach is computation-ally intensive.
To address these limitations, we propose CrOC, whose underpinning mechanism is an efﬁcient Cross-view Online
Clustering that conjointly generates segmentation masks for the union of both views (Fig. 1.b).
Our main contributions are: 1) we propose a novel object-level self-supervised learning framework that lever-ages an online clustering algorithm yielding segmentation masks for the union of two image views. 2) The intro-duced method is inherently compatible with scene-centric datasets and does not require a pre-trained model. 3) We empirically and thoroughly demonstrate that our approach rivals or out-competes existing SOTA self-supervised meth-ods even when pre-trained in an unfavorable setting (smaller and more complex dataset). 2.