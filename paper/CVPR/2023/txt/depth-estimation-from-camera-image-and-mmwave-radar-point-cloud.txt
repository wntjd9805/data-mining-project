Abstract
We present a method for inferring dense depth from a camera image and a sparse noisy radar point cloud. We first describe the mechanics behind mmWave radar point cloud formation and the challenges that it poses, i.e. am-biguous elevation and noisy depth and azimuth components that yields incorrect positions when projected onto the im-age, and how existing works have overlooked these nuances in camera-radar fusion. Our approach is motivated by these mechanics, leading to the design of a network that maps each radar point to the possible surfaces that it may project onto in the image plane. Unlike existing works, we do not process the raw radar point cloud as an erroneous depth map, but query each raw point independently to associate it with likely pixels in the image – yielding a semi-dense radar depth map. To fuse radar depth with an image, we propose a gated fusion scheme that accounts for the confi-dence scores of the correspondence so that we selectively combine radar and camera embeddings to yield a dense depth map. We test our method on the NuScenes benchmark and show a 10.3% improvement in mean absolute error and a 9.1% improvement in root-mean-square error over the best method. Code: https://github.com/nesl/ radar-camera-fusion-depth. 1.

Introduction
Understanding the 3-dimensional (3D) structure of the scene surrounding us can support a variety of spatial tasks such as navigation [33] and manipulation [9]. To perform these tasks, an agent is generally equipped with multiple sensors, including optical i.e., RGB camera and range i.e., lidar, radar. The images from a camera are “dense” in that they provide an intensity value at each pixel. Yet, they are also sparse in that much of the image does not allow for the
Figure 1. Depth estimation using a mmWave radar and a camera. (a) RGB image. (b) Semi-dense depth generated from associating the radar point cloud to probable image pixels. (c) Predicted depth.
Boxes highlight mapping of radar points to objects in the scene. establishing of unique correspondence due to occlusions or the aperture problem to recover the 3D structure lost to the image formation process. On the other hand, range sensors are typically sparse in returns, but provide the 3D coordi-nates for a subset of points in the scene i.e., a point cloud.
The goal then is to leverage the complementary properties of both sensor observations – an RGB image and a radar point cloud that is synchronized with the frame – to recover the dense 3D scene i.e., camera-radar depth estimation.
While sensor platforms that pair lidar with camera have been of recent interest i.e., in autonomous vehicles, they are expensive in cost, heavy in payload, and have high energy and bandwidth consumption [36] – limiting their applica-tions at the edge [37]. On the other hand, mmWave [20] radars are orders of magnitude cheaper, light weight, and power efficient. Over the last few years, developments in mmWave radars and antenna arrays [18] have signifi-cantly advanced the performance of these sensors. Radars are already ubiquitous in automotive vehicles as they en-able services such as cruise control and collision warn-ing [10]. Methods to perform 3D reconstruction with cam-era and radar are also synergistic with the joint communica-tion and sensing (JCAS) paradigm in 6G cellular communi-cation [1, 43, 57], where cellular base-stations will not only be the hub of communication, but also act as radars, to sense the environment.
The challenge, however, is that a mmWave radar is a point scatterer and only a very small subset of points (50
to 80 per frame [6]) in the scene, often noisy due to its large beam width, are reflected back into the radar’s re-ceiver. Compared to the returns of a lidar, this is 1000x more sparse. Additionally, most radars used in automotive vehicles either do not have enough antenna elements along the elevation axis or do not process the radar returns along the elevation axis. Hence, the elevation obtained from them is either too noisy or completely erroneous (see Sec. 3).
As a result, camera-radar depth estimation requires (i) mapping noisy radar points without elevation components to their 3D coordinates (and with calibrating their 2D im-age coordinates i.e., radar-to-camera correspondence) and (ii) fusing the associated sparse points with images to ob-tain the dense depth. Existing works have projected the radar points onto the image and “extended” the elevation or y-coordinate in the image space as a vertical line [28] or relied on multiple camera images to compute the optical-flow which in-turn has been used to learn the radar-to-pixel mapping [30]. These approaches overlook that radar returns have noisy depth, azimuth and erroneous elevation. They also assume access to multiple consecutive image and radar frames, so that they may use the extra points to densify radar returns in both the close (from past frames) and far (from future frames) regions. In the scenario of obtaining instan-taneous depth for a given frame, the requirement of future frames makes it infeasible; if delays are permitted, then an order of hundreds of milliseconds in latency is incurred.
Instead, we propose to estimate depth from a single radar and image frame by first learning a one to many mapping of correspondence between each radar point and the probable surfaces in the image that it belongs to. Each radar point is corresponded to a region within the image (based on empir-ical error of projection due to noise) via a ROI alignment mechanism – yielding a semi-dense radar depth map. The information in the radar depth map is further modulated by a gated fusion mechanism to learn the error modes in the correspondence (due to possible noisy returns) and adap-tively weight its contribution for image-radar fusion. The result of which is used to augment the image information and decoded to a dense depth map.
Our contributions are: (i) to the best of our knowledge, the first approach to learn radar to camera correspondence using a single radar scan and a single camera image for mapping arbitrary number of ambiguous and noisy radar points to the object surfaces in the image, (ii) a method to introduce confidence scores of the mapping for fusing radar and image modalities, and (iii) a learned gated fusion be-tween radar and image to adaptively modulate the trade-off between the noisy radar depth and image information. (iv)
We outperform the best method that uses multiple image and radar frames by 10.3% in mean absolute error (MAE) and 9.1% in root-mean-square error (RMSE) to achieve the state of the art on the NuScenes [6] benchmark, despite only using a single image and radar frame. 2.