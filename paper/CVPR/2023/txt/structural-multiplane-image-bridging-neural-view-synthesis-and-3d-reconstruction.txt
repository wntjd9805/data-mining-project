Abstract
The Multiplane Image (MPI), containing a set of fronto-parallel RGBα layers, is an effective and efficient represen-tation for view synthesis from sparse inputs. Yet, its fixed structure limits the performance, especially for surfaces im-aged at oblique angles. We introduce the Structural MPI (S-MPI), where the plane structure approximates 3D scenes concisely. Conveying RGBα contexts with geometrically-faithful structures, the S-MPI directly bridges view synthe-sis and 3D reconstruction.
It can not only overcome the critical limitations of MPI, i.e., discretization artifacts from sloped surfaces and abuse of redundant layers, and can also acquire planar 3D reconstruction. Despite the intu-ition and demand of applying S-MPI, great challenges are introduced, e.g., high-fidelity approximation for both RGBα layers and plane poses, multi-view consistency, non-planar regions modeling, and efficient rendering with intersected planes. Accordingly, we propose a transformer-based net-work based on a segmentation model [4]. It predicts com-pact and expressive S-MPI layers with their corresponding masks, poses, and RGBα contexts. Non-planar regions are inclusively handled as a special case in our unified frame-work. Multi-view consistency is ensured by sharing global proxy embeddings, which encode plane-level features cov-ering the complete 3D scenes with aligned coordinates. In-tensive experiments show that our method outperforms both previous state-of-the-art MPI-based view synthesis methods and planar reconstruction methods. 1.

Introduction
Novel view synthesis [30, 51] aims to generate new im-ages from specifically transformed viewpoints given one or multiple images. It finds wide applications in augmented or mixed reality for immersive user experiences.
The advance of neural networks mostly drives the re-cent progress. NeRF-based methods [28, 30] achieve im-pressive results but are limited in rendering speed and gen-*This work was done when Mingfang Zhang was an intern at MSRA.
Figure 1. We propose the Structural Multiplane Image (S-MPI) representation to bridge the tasks of neural view synthesis and 3D reconstruction. It consists of a set of posed RGBα images with ge-ometries approximating the 3D scene. The scene-adaptive S-MPI overcomes the critical limitations of standard MPI [20], e.g., dis-cretization artifacts (D) and repeated textures (R), and achieves a better depth map compared with the previous planar reconstruc-tion method, PlaneFormer [1]. eralizability. The multiplane image (MPI) representation
[42, 50] shows superior abilities in these two aspects, es-pecially given extremely sparse inputs. Specifically, neural networks are utilized to construct MPI layers, containing a set of fronto-parallel RGBα planes regularly sampled in a reference view frustum. Then, novel views are rendered in real-time through simple homography transformation and integral over the MPI layers. Unlike NeRF models, MPI models do not need another training for a new scene.
Nevertheless, standard MPI has underlying limitations. 1) It is sensitive to discretization due to slanted surfaces in scenes. As all layered planes are parallel to the source im-age plane, slanted surfaces will be distributed to multiple
MPI layers causing discretization artifacts in novel views,
as shown in Fig. 1 (b). Increasing the number of layers can improve the representation capability [38] but also increase memory and computation costs. 2) It easily introduces re-dundancy. It tends to distribute duplicated textures into dif-ferent layers to mimic the lighting field [21], which can in-troduce artifacts with repeated textures as shown in Fig. 1 (b). The essential reason causing the above issues is that the
MPI construction is dependent on source views but neglects the explicit 3D geometry of the scenes. Intuitively, we raise a question: Is it possible to construct MPIs adaptive to the scenes, considering both depths and orientations?
Slanted planes are smartly utilized in 3D reconstruction to save stereo matching costs [11] or represent the scene compactly [12, 17], especially for man-made environments.
Recent advanced neural networks reach end-to-end planar reconstruction from images by formulating it as instance segmentation [25, 26], where planes are directly detected and segmented from the input image. However, non-planar regions are often not well-modeled or neglected [1, 25, 40], resulting in holes or discontinuities in depth maps, as shown in Fig. 1 (c).
In this paper, we aim to bridge the neural view synthe-sis and the planar 3D reconstruction, that is, to construct
MPIs adaptive to 3D scenes with planar reconstruction and achieve high-fidelity view synthesis. The novel representa-tion we propose is called Structural MPI (S-MPI), which is fully flexible in both orientations and depth offsets to approximate the scene geometry, as shown in Fig. 1 (a).
Although our motivation is straightforward, there are great challenges in the construction of an S-MPI. (1) The network not only needs to predict RGBα values but also the planar approximation of the scene. (2) It is difficult to correspond the plane projections across views since they may cover dif-ferent regions and present different appearances. Recent plane reconstruction works [1, 18] build matches of planes after they are detected in each view independently, which may increase costs and accumulate errors. (3) Non-planar regions are challenging to model even with free planes. Pre-vious plane estimation methods [1,40,46] cannot simultane-ously handle planar and non-planar regions well. (4) In the rendering process, as an S-MPI contains planes intersecting with each other, an efficient rendering pipeline needs to be designed so that the rendering advantages of MPI can be inherited.
To address these challenges, we propose to build an
S-MPI with an end-to-end transformer-based model for both planar geometry approximation and view synthesis, in which planar and non-planar regions are processed jointly.
We follow the idea [25] of formulating plane detection as instance segmentation and leverage the segmentation net-work [4]. Our S-MPI transformer uniformly takes planar and non-planar regions as two structure classes and predicts their representative attributes, which are for reconstruction (structure class, plane pose and plane mask) and view syn-thesis (RGBα image). We term each instance with such attributes as a proxy. Note that non-planar layers are inclu-sively handled as fronto-parallel planes with adaptive depth offsets and the total number of the predicted proxy instances is adaptive to the scene.
Our model can manipulate both single-view and multi-view input.
It aims to generate a set of proxy embed-dings in the full extent of the scene, covering all planar and non-planar regions aligned in a global coordinate frame.
For multi-view input, the proxy embeddings progressively evolve to cover larger regions and refine plane poses as the number of views increases. In this way, the predicted proxy instances are directly aligned, which avoids the so-phisticated matching in two-stage methods [1, 18]. The global proxy embeddings are effectively learned with the ensembled supervision from all local view projections. Our model achieves state-of-the-art performance for single-view view synthesis (10% ↑ PSNR) and planar reconstruction (20% ↑ recall) in datasets [6, 36] of man-made scenes and also achieve encouraging results for multi-view input com-pared to NeRF-based methods with high costs.
In summary, our main contributions are as follows:
• We introduce the Structural MPI representation, con-sisting of geometrically-faithful RGBα images to the 3D scene, for both neural view synthesis and 3D re-construction.
• We propose an end-to-end network to construct S-MPI, where planar and non-planar regions are uniformly handled with high-fidelity approximations for both ge-ometries and light filed.
• Our model ensures multi-view consistency of planes by introducing the global proxy embeddings compre-hensively encoding the full 3D scene, and they effec-tively evolve with the ensembled supervision from all views. 2.