Abstract
Knowledge distillation (KD) is a technique that transfers the knowledge from a large teacher network to a small stu-dent network. It has been widely applied to many different tasks, such as model compression and federated learning.
However, existing KD methods fail to generalize to gen-eral deep directed graphical models (DGMs) with arbitrary layers of random variables. We refer by deep DGMs to
DGMs whose conditional distributions are parameterized by deep neural networks. In this work, we propose a novel unified knowledge distillation framework for deep DGMs on various applications. Specifically, we leverage the repa-rameterization trick to hide the intermediate latent variables, resulting in a compact DGM. Then we develop a surrogate distillation loss to reduce error accumulation through mul-tiple layers of random variables. Moreover, we present the connections between our method and some existing knowl-edge distillation approaches. The proposed framework is evaluated on four applications: data-free hierarchical varia-tional autoencoder (VAE) compression, data-free variational recurrent neural networks (VRNN) compression, data-free
Helmholtz Machine (HM) compression, and VAE continual learning. The results show that our distillation method out-performs the baselines in data-free model compression tasks.
We further demonstrate that our method significantly im-proves the performance of KD-based continual learning for data generation. Our source code is available at https:
//github.com/YizhuoChen99/KD4DGM-CVPR. 1.

Introduction
Knowledge distillation (KD) aims at transferring the knowledge of a large teacher model to a small student model, which tries to mimic the behavior of the teacher model to
*Work is completed during internship at William and Mary
†Corresponding author attain a competitive or superior performance [13, 20]. The goal of this work is to develop a unified knowledge distil-lation (KD) framework for deep directed graphical models (DGMs). Applications of the proposed framework include: (i) data-free hierarchical variational autoencoder (VAE) com-pression [50], (ii) data-free variational recurrent neural net-works (VRNN) compression [8], (iii) data-free Helmholtz
Machine (HM) compression [49], and (iv) KD based contin-ual learning.
Deep directed graphical models (DGMs) refer to DGMs whose conditional distributions are parameterized by deep neural networks (DNNs), which is in contrast to the regular
DGMs with tabular conditional probability. One good ex-ample is variational autoencoders (VAEs), whose posterior probability of latent variables is parameterized by DNNs. A general deep DGM may have a complex structure, consisting of an arbitrary number of input variables, target variables, and latent variables. Deep DGMs have been widely used in various applications, such as image generation [53], text generation [5], and video prediction [55].
This work is motivated by the growing popularity of re-cent over-parameterized deep DGMs with millions of param-eters to improve their accuracy in various tasks. However, the large models are very computationally expensive. As a result, it is not practical to deploy them on resource-constrained edge devices, such as mobile phones and IoT systems [33].
One possible solution to this problem is KD, which enables a smaller student model to approximate the performance of a large teacher. Recently, KD has been widely applied to many different tasks, such as model compression [20], continual learning [34, 59], and federated learning [30, 36].
To our knowledge, the existing KD methods, however, are only applicable to some specific DGMs, including genera-tive adversarial networks (GANs) [2, 33], auto-regressive models in natural language processing (NLP) [35], and VQ-VAE [48]. They fail to generalize to the general deep DGMs, especially to those with multiple latent variables or complex
Figure 1. Toy example of DGM in four different forms. Diamonds are deterministic variables and circles are random variables. (a)
Original form; (b) Auxiliary form; (c) Our semi-auxiliary form; (d)
Compact semi-auxiliary form.
Figure 2. Toy example of accumulated error (KL divergence) between the teacher and student for local distillation and our method. Experimental settings are presented in the last paragraph in Section 3.2. dependence structures, as illustrated in Fig. 1.
Generalizing knowledge distillation to deep DGMs poses two major challenges. First, distillation by marginalizing all latent variables is generally intractable (as explained in
Appendix A). Secondly, distilling each layer locally and independently may suffer from error accumulation, as shown in Fig. 2. We can observe that the accumulated error (i.e., KL divergence) between the teacher and student grows linearly for local distillation. To address these challenges, we propose a novel unified knowledge distillation framework for deep
DGMs. Specifically, we first adopt the reparameterization trick [23,24] to convert a DGM into a compact semi-auxiliary form. By semi-auxiliary form, we mean the latent variables, z, in both the student and teacher models are converted into deterministic variables with auxiliary variables, while the input variables and target variables remain unchanged, as shown in Fig. 1 (c). Note that different from the classical reparameterization for VAE model training [25], ours can be applied to both continuous and discrete variables. Then a surrogate distillation loss is derived as a new objective of
KD. To mitigate gradient vanishing, we further incorporate a latent distillation loss that penalizes the dissimilarity of latent variables between the teacher and student into our objective.
We also present the connections between our approach and some existing KD methods for specific DGMs and show that our method is a proper generalization of these existing methods.
We evaluate the performance of our distillation method on four different tasks: hierarchical VAE compression, VRNN compression, Helmholtz Machine compression, and KD-based continual learning with VAEs. For model compression tasks, the student model distilled by our method in a data-free manner outperforms that trained from scratch and the other baselines. In addition to model compression, we also illustrate that our method can better mitigate the catastrophic forgetting issue than the generative replay approaches in continual learning.
In summary, our contributions include: 1) a new unified
KD framework is proposed for general deep DGMs based on reparameterization trick, 2) we derive a novel distillation loss that combines the latent distillation loss and surrogate distillation loss to improve the performance of KD, and 3) evaluation results on multiple benchmark datasets show that our approach can not only achieve high accuracy for deep
DGMs compression but also improve the performance of
KD-based continual learning. 2. Preliminaries
• Directed Graphical Models (DGMs) DGMs such as
Bayesian Networks [9] belong to an expressive class of probabilistic graphical models [26], in which the joint dis-tribution is factorized into the product of many conditional distributions according to a directed acyclic graph (DAG) that captures variable conditional dependencies. In this work, we primarily study knowledge distillation for deep DGMs, especially for those with complicated dependency structures.
For deep DGMs, we are interested in modeling the con-ditional distribution pθ(y, z|x) for target variables y and latent variables z given input variables x, parameterized by θ. Specifically, when there are no input variable, i.e., x = ∅, we actually model the joint distribution pθ(y, z).
Let Pa(·) denote the parent random variables of a certain variable defined by the DAG of a DGM. Then the conditional distribution pθ(y, z|x) has its factorized form below, pθ(y, z|x) = (cid:89) j pθ(yj|Pa(yj), x) (cid:89) i pθ(zi|Pa(zi), x), (1) where yj denotes the jth target variable in y, zi denotes the ith latent variable in z. Without loss of generality, we
assume for two variables yi and yj, if yj is an ancestor of yi, then it holds that i > j.
• Knowledge Distillation (KD) KD aims to transfer the knowledge of a large teacher model to a smaller student model. One commonly used vanilla distillation method is to encourage the student to mimic the output of the teacher model [20]. Given an empirical distribution of the training data pdata (x), its distillation loss is an expected dissimilarity measure between the output of the student and that of the teacher. A general form of distillation loss is given by
Lkd = Epdata (x) [d(pϕ(y|x), pθ(y|x))] , (2) where pϕ(y|x) and pθ(y|x) denote the output conditional distribution of the teacher and student models, respectively.
ϕ and θ denote their corresponding parameters. d(·, ·) is a dissimilarity measure between two probability distributions.
Kullback-Leibler (KL) divergence [7, 20], for example, is one of some typical choices.
The above Eq. (2) and its extended version have been ap-plied to different DGMs, such as vanilla neural networks [7],
GANs [2], and fully-visible auto-regressive models (e.g.,
Transformer) [22, 35]. Take fully-visible auto-regressive models as an example. When we set d(·, ·) to KL divergence,
Eq. (2) can be factorized as
Lkd = (cid:88) j
Epdata (x)Epϕ(y<j |x)KLj(y<j, x), (3)
KLj(y<j, x) = KL(pϕ(yj|y<j, x) ∥ pθ(yj|y<j, x)).
A tractable estimation to Lkd above can be obtained by
Monte Carlo method. It can also be viewed as distilling each conditional distribution in a local and independent manner, as shown in Fig. 3 (d).
However, to our best knowledge, the existing KD ap-proaches are only designed for some specific DGMs. They fail to be applied to a general DGM with multiple latent vari-ables or complicated dependence structures. Hence, the ques-tion is, how can we distill the knowledge from the teacher to the student given a general DGM structure?
Intuitively, there are two naive methods: (i) marginal-ized distillation: it marginalizes all latent variables to get p(y|x) = (cid:82) p(y, z|x)dz. However, the integration (cid:82) p(y, z|x) dz is generally intractable and thus the loss as in Eq. (2) is also intractable. (ii) local distillation: it treats latent variables z equally as target variables and then distills each conditional distribution for both z and y locally and in-dependently, as shown in Fig. 3d. The local distillation may suffer from error accumulation through multiple layers if each conditional distribution in the student slightly deviates from the teacher. Fig. 2 shows a toy example that the accu-mulated error of local distillation, i.e., the KL-divergence between teacher and student, increases linearly as the number of layers of latent variables rises. For detailed discussions, please refer to Appendix A.
• Reparameterization Trick Reparameterization trick [23, 24], also called the auxiliary form of a DGM, is originally proposed to backpropagate through a random node that is not differentiable during training. Its basic idea is intro-duced as follows. Given a conditional distribution of random variable zi in a DGM, p(zi|Pa(zi), x), we convert it to a deterministic variable by adding an auxiliary variable ϵi to its dependence, as shown in Figure 1 (c). Here ϵi is a root node of the DGM with an independent marginal distribution of p(ϵi). By choosing appropriate p(ϵi) and deterministic transformation g(·) [23], we can have zi = g(Pa(zi), x, ϵi), where zi is determined by its parent variables Pa(zi), input variables x, and the corresponding auxiliary variable ϵi. ϵi serves as the source of stochasticity of zi.
In this work, note that we do not primarily use the repa-rameterization trick for model training. Rather, we leverage it to convert the latent variables z in DGMs to deterministic variables so that we can effectively distill knowledge from a compact form of DGM. Note that different from the classical reparameterization for model training that requires contin-uous latent variables, ours can be applied to a wider range of variables z, including both continuous and discrete vari-ables. Besides, the transformation function g(·) [25] in our framework can be either differentiable or non-differentiable.
Hence, our method can be applied to much more DGMs than the classical one. Below, we will elaborate this general idea in more detail. 3. Modeling
In this section, we first introduce the semi-auxiliary form of DGMs using reparameterization trick. Then we propose a new surrogate loss function and latent distillation loss for our KD method. 3.1. Semi-auxiliary Form
As discussed in Section 2, the two naive methods, marginalized distillation and local distillation, do not work well due to intractable distillation loss or error accumulation.
In order to address these issues, we propose a novel idea that converts DGM to its semi-auxiliary form based on repa-rameterization trick. Specifically, we convert all the latent variables, z, in both the teacher and student to deterministic variables with auxiliary variables, while keeping target vari-ables y and input variables x unchanged. This is because our ultimate goal is to encourage student to mimic the out-put (target variable) of the teacher based on input variables.
Hence, we can omit the deterministic (latent) variables in a DGM, yielding a compact semi-auxiliary form that only consists of target variables, input variables and auxiliary vari-ables. In this way, each target variable has a tractable and direct dependence on input variables or prior target variables, as shown in Fig. 1 (c).
(a) (b) (c) (d) (e)
Figure 3. Examples of different distillation methods. Each pair of conditional distributions marked with same color represents an independent distillation component. (a) Distillation on vanilla neural network. (b) GAN distillation. (c) Distillation on a 2-layer fully-visible auto-regressive DGM. (d) Local distillation on the original DGM. (e) Our distillation method with semi-auxiliary form.
Fig. 1 illustrates a toy example of converting DGM to its semi-auxiliary form. Fig. 1 (a) is the original form of a DGM, and its corresponding auxiliary form is shown in
Fig. 1 (b). In this paper, we only assign auxiliary variables to latent variables z, as shown in Fig. 1 (c). We can observe from it that latent variables z1 and z2 are deterministic when their ancestors are given. We thus can omit the determinis-tic variables, leading to a compact semi-auxiliary form, as shown in Fig. 1 (d). 3.2. Surrogate Distillation Loss
After obtaining the semi-auxiliary form of a DGM in Fig. 1 (d), our distillation loss becomes the dissimilarity between pϕ(y|ϵ, x) and pθ(y|ϵ, x). We call it surrogate distillation loss. Our goal is to minimize the surrogate distillation loss w.r.t. student’s parameters θ below.
Lsd = Epϕ(ϵ)pdata(x) [d(pϕ(y|ϵ, x), pθ(y|ϵ, x))] , (4) where ϵ denotes a set of auxiliary variables ϵ. The expec-tation of dissimilarity is taken over both empirical data dis-tribution pdata(x) and auxiliary variable distribution pϕ(ϵ).
The expectation can be estimated using Monte Carlo method.
Note that pϕ(ϵ) is generally chosen to be simple and fixed distribution with no parameters, such as unit Gaussian or standard uniform distribution. Thus, it implies that teacher’s pϕ(ϵ) is equivalent to student’s pθ(ϵ), so there is no need to distill pϕ(ϵ) to pθ(ϵ). An illustration is given in Fig. 3e.
Proposition 3.1. The surrogate distillation loss as defined in
Eq. (4) is an upper bound of the distillation loss as defined in Eq. (2) when the dissimilarity measure is chosen to be KL divergence.
We provide the detailed proof in Appendix B.
Next, we discuss the advantages of our method over the two naive methods mentioned above. Firstly, the proposed method bypasses the intractable computation in marginalized distillation. While marginalized distillation measures p(y|x) which is intractable in general, we instead measure p(y|ϵ, x) which is easily tractable by function composition with no need of integral or sum operation. Here p(y|ϵ, x) is tractable because p(y|ϵ, x) = (cid:81) i p(yi|ϵ≤i, y<i, x).
Secondly, our method can make the DGMs shallower than local distillation, mitigating error accumulation through multiple layers of latent variables. As illustrated in Fig. 3d and 3e, we can see our method only constraints the target variables y in the student while local distillation constrains both latent variables z and target variables y. As a result, local distillation may suffer from error accumulation issue.
Fig. 2 shows a toy example of comparing the accumulated error, i.e., KL divergence between the teacher and student, for our method and local distillation. In this illustrative ex-periment, we let student mimic the output distribution of the teacher with L-layers latent variables for L ∈ 1, . . . , 20.
Each layer of the teacher follows the Gaussian distribution p(zi+1|zi) = N (µ(zi), 0.01I), where µ(zi) is z1.1 for zi ≥ 0 and −(−zi)1.1 for zi < 0. p(z1) is a uniform dis-tribution U [−1, 1]. The student is parameterized by neural networks with proper residual structure [18]. Then density ratio estimation [43, 51] is used to measure the KL diver-gence between the teacher and student. We can observe from
Fig. 2 that the accumulated error (KL divergence) grows lin-early w.r.t the number of layers for local distillation because each layer of the student deviates from the teacher to an extent. In contrast, the accumulated error (KL divergence) of our method increases slowly. i 3.3. Latent Distillation Loss
The surrogate distillation loss in Eq.(4) can sufficiently achieve a satisfactory performance for knowledge distilla-tion. Nevertheless, there are still two limitations of the proposed method for some special DGMs. Firstly, it fails to back-propagate the gradients when there exist discrete latent variables. Secondly, it might suffer from gradient vanishing when the network structure with multiple latent variables is very deep and complex.
To deal with these issues, we propose to penalize the dissimilarity of latent variables z in the teacher and student model. The resulting latent distillation loss for latent vari-ables zi is given by
˜Lz,i = Epϕ(ϵ)pdata(x) [d [pϕ(zi|ϵ, x), pθ(zi|ϵ, x)]]
= Epϕ(ϵ)pdata(x) [d(pϕ(zi|ϵ≤i, x), pθ(zi|ϵ≤i, x))] , (5) where ϵ≤i is a set of all the ancestral auxiliary variables of zi. zi is deterministic when ϵ≤i and x are given. In order to better penalize the dissimilarity of latent variables zi, one good choice is to solely convert the current zi (not z<i) back to its original form by removing ϵi from its dependence.
Then we have the following latent distillation loss
Lz,i = Epϕ(ϵ)pdata(x) [d(pϕ(zi|ϵ<i, x), pθ(zi|ϵ<i, x))] . (6)
The proposed latent distillation loss can benefit our op-timization process. When latent variables are continuous, the latent distillation loss may provide shallower and sup-plementary supervisory signals to hasten the convergence.
When latent variables are discrete, it can deal with the back-propagation cutting off problem. 3.4. Final Target-Free Loss
Combining the above surrogate distillation loss with la-tent distillation loss, our final distillation loss is given by
Lour = Lsd + λ (cid:88) i
Lz,i, (7) where λ is a hyper-parameter that controls the importance of latent distillation loss. Similar to [20], our loss in Eq. (7) is a target-free distillation loss, which means target data is not required for calculating it. Eq. (7) can also be applied to DGMs with no input variables (i.e. x = ∅).
In this case, Eq. (7) can be computed in a completely data-free manner. We summarize our KD method for a general DGM in Algorithm 1 in Appendix C. 3.5. Connections to Other Distillation Methods
We present the connections between our method and some existing knowledge distillation methods.
• Vanilla KD and Sequence-Level KD. When there is no latent variable in a DGM, our distillation method in Eq. (4) is naturally reduced to Eq. (2) by removing the dependence on auxiliary variables ϵ, which is a typical vanilla KD [20].
Also, when the DGM is a fully visible auto-regressive model, by removing dependence on ϵ and letting d(·, ·) be KL di-vergence, Eq. (4) can be reduced to Eq. (3), which is the
Monte Carlo approximation of intractable sequence-level distillation loss [22]. Please note that further simplification has been made in [22] to enhance the practicability.
• Feature Based KD. Feature based knowledge distillation uses the intermediary representations of a teacher network to supervise a student network [47]. When the teacher and student share the same size of latent features, the feature distillation loss can be written as
Lf = rf (fϕ(x), fθ(x)), (8) where fϕ(x) and fθ(x) are intermediary deterministic fea-tures of the teacher and student, respectively. rf (·, ·) is a dis-tance between two vectorized feature maps. A vanilla neural network with multiple intermediate features can be viewed as a DGM with multiple deterministic latent variables. De-terministic variables can be viewed as following the degen-erate distributions. In general, for p ≥ 1, p-Wasserstein 1 distance (inf E[r(a1, a2)p]) p between two degenerate dis-tributions located at a1 and a2 is equivalent to r(a1, a2).
Thus, by viewing the intermediate features as latent vari-ables following degenerate distributions, and choosing d(·, ·) in our latent distillation loss Lz,i as Wasserstein distance 1 (inf E[rf (zϕ, zθ)p]) p , our latent distillation loss is reduced to feature distillation loss in Eq. (8).
• GAN Distillation. GAN distillation in [2, 33] also in-corporates the idea of feature distillation into their model, which is given by
Lgan = ro(Gϕ(z), Gθ(z)) + rf (fϕ(z), fθ(z)). (9)
The first term above is the output distillation loss of the generator and the second one is the intermediary feature distillation loss. Similar to feature distillation loss, by view-ing the intermediate features and generator output as la-tent and target variables following degenerate distributions, and choosing d(·, ·) in Lsd and Lz,i as p-Wasserstein dis-1 tance (inf E[ro(yϕ, yθ)p]) p re-spectively, our final distillation loss in Eq. (7) can be reduced to GAN distillation loss as well. p and (inf E[rf (zϕ, zθ)p]) 1 3.6. Applications
We evaluate the performance of our method on four rep-resentative applications selected as below.
• Data-Free Hierarchical VAE Compression. Model com-pression is one of the most representative application of KD.
Therefore, we first apply our method to compress a popular generative model, Hierarchical VAE in a data-free manner.
Recent studies show that VAEs generate better with over-parametrization [53]. However, it is challenging to deploy them to edge devices with limited computing resources. In this work, we apply our method to compress a large 5-layer hierarchical VAE model [50] to smaller models, as illus-trated in Fig. 4a.
• Data-Free VRNN Compression. Then our method is applied to compress a sequence generative model with more complicated structure, VRNN [11], as shown in Fig. 4b.
VRNN has as many latent and target variables as the length of the sequence in dataset.
(a) (b) (c) (d)
Figure 4. Structures of DGMs used in our experiments. (a) Hierar-chical VAE. (b) VRNN. (c) HM. (d) Hierarchical VAE for continual learning.
• Data-Free HM Compression. Next, we adopt our method to compress a model with discrete latent variables,
Helmholtz Machine (HM) [10]. The classical HM only has one target variable and each layer is parametrized by a linear transformation. In order to demonstrate the applicability of our method, we extend it to a 5-layer deep HM with two targets, as shown in Fig. 4c.
• KD-based VAE Continual Learning.
In addition to model compression, we evaluate the performance of our method on another popular KD based application, continual learning [59]. Our goal is to model a new distribution while retaining the ability of modeling a learned old distribution without access to the old dataset. Prior work on continual learning [45, 57] mainly resort to generative replay strategy, in which we generate a set of fake samples from VAE model learned on old training datasets, and mix them with new training dataset to train the new VAE model. In fact, our experiments will show that generative replay is inferior to our distillation method for VAE continual learning because of the blurry nature of VAE generation. 4. Experiments
In this section, we first present a toy experiment to demon-strate that our method can also mitigate error accumulation issue for DGM with discrete variables. Then we evaluate the performance of our method on the four applications as mentioned in Section 3.6. We carry out extensive experi-ments on five benchmark datasets: Old Faithful Geyser [17],
IAM online handwriting [38], SVHN [42], CIFAR10 [28], and CelebA [37]. The detailed data splitting for training and test is described in Appendix D. In addition, we present the detailed model configurations and hyperparameter settings of the teacher and student in Appendix E. 4.1. Toy Example for DGMs with Discrete Variables
In this experiment, we modify the illustrative toy exper-iment in Section 3.2 to demonstrate that, the error accu-Figure 5. Toy example of accumulated error (KL divergence) between the teacher and student for local distillation and our method in a DMG with discrete random variables. mulation issue still exists in DGMs with discrete variables, and our method can still successfully mitigate it. Specifi-cally, we change each layer of the teacher model in Section 3.2 to discrete variables following the binomial distribution p(zi+1|zi) = B(1000, p(zi)), where p(zi) = ( zi 1000 )1.2. p(z1) is changed to uniform distribution U [0, 1]. The exper-iment result is shown in Fig. 5. We can observe that, similar as its continuous counterpart, the accumulated error (KL di-vergence) grows as the number of layers increases for local distillation method. In contrast, our method can significantly reduce the accumulated error to a stable low level. 4.2. Evaluation on Data-Free Hierarchical VAE
Compression.
Next, we apply our distillation method to compress large 5-layer hierarchical VAE [50] models. We compare the per-formance of the student model using our method and two baselines: training from scratch and local distillation [1].
Experiments are carried out on three benchmark datasets:
SVHN, CIFAR10, and CelebA, with varying sizes of the student model. We adopt four widely-used metrics, Frechet
Inception Distance (FID, lower is better) [6,19], Earth Mover
Distance (EMD, lower is better, also known as Wasserstein
Distance) [56], Maximum Mean Discrepancy (MMD, lower is better) [16], and 1-Nearest-Neighbor Accuracy (1NN, lower is better) [39], to evaluate the generative performance of our method. To demonstrate the similarity of the teacher and student, we further calculate the FID, EMD, MMD and 1NN between the student and its corresponding teacher for different methods. Table 1 illustrates the comparison of dif-ferent methods averaged over three random seeds on SVHN,
Cifar10, and CelebA datasets. We can observe that our method consistently outperforms the baselines in all the met-rics, which means the proposed method can help students better imitate their teachers. Importantly, our data-free distil-lation method outperforms the student model trained from scratch, which suggests that directly optimizing a capacity-limited student VAE may not learn a decent model that can generate high-fidelity samples. Conversely, our method can
help the student VAE learn better performance even without accessing training data.
Moreover, we demonstrate that our method is stable and robust to the change of hyper-parameter λ in Fig. 7 in the appendix. For qualitative evaluation, we also show the sam-ples generated from different methods on CelebA in Fig. 6 in Appendix F. 4.3. Evaluation on Data-free VRNN Compression
& HM Compression.
Next, we evaluate the performance of our method on the other two tasks: data-free VRNN Compression and data-free
HM Compression. First, we adopt our distillation method to compress a complicated deep sequence generative model,
VRNN. Following the prior works [8, 14], we adopt qualita-tive measure to show the generated strokes by our method and the baselines in Fig 8 in Appendix G. It can be observed that our method can generate more readable and clearer strokes than the baselines. The student VRNN distilled by our method in Fig 8 (c) has comparable generative perfor-mance to that of the teacher in Fig 8 (b). Hence, we can conclude that the proposed method is able to achieve good performance for data-free VRNN compression.
In addition, our method is applied to compress Helmholtz
Machine (HM) with discrete latent variables. The exper-iment results, as illusrated in Fig. 7 in Appendix H, show that it can still achieve good performance as the teacher for HM compression. For more details, please refer to the experimental results in Appendix H. 4.4. Evaluation on KD-based Continual Learning
We also evaluate the performance of the proposed method on KD-based VAE continual learning using CelebA dataset.
For each group of experiments, one of forty ground-truth attributes is selected. Based on this attribute, we divide the whole dataset into two parts for continual learning. Specifi-cally, we first train a VAE Mold on the first part of images, and then we learn another VAE Mnew on the second part by jointly minimizing a new distribution standard training loss Lnew and an old knowledge preserving loss Lpre under the guidance of Mold. We conduct the experiment for three different attributes.
We compare our method with local distillation and other two generative replay approaches: CURL [46] and
LGM [45]. All 4 methods have the same Lnew but different
Lpre. Table 2 illustrates the comparison of different meth-ods on CelebA. We can observe from Columns Old that our method performs much better than the baselines on preserv-ing old knowledge. It is because local distillation method has error accumulation issue, and CURL and LGM can only preserve old knowledge by retraining on the generated low-quality images. Besides, we can see from Columns New that our method is comparable to LGM and CURL on learning new distribution, because these methods only impose nec-essary Lpre. However, local distillation does not work well on new distributions, indicating that its Lpre is too large yet ineffective. 5.