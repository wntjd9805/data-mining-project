Abstract
The emergence of neural networks has revolutionized the field of motion synthesis. Yet, learning to unconditionally synthesize motions from a given distribution remains chal-lenging, especially when the motions are highly diverse. In this work, we present MoDi – a generative model trained in an unsupervised setting from an extremely diverse, un-structured and unlabeled dataset. During inference, MoDi can synthesize high-quality, diverse motions. Despite the lack of any structure in the dataset, our model yields a well-behaved and highly structured latent space, which can be semantically clustered, constituting a strong motion prior that facilitates various applications including seman-tic editing and crowd animation. In addition, we present an encoder that inverts real motions into MoDi’s natural mo-tion manifold, issuing solutions to various ill-posed chal-lenges such as completion from prefix and spatial editing.
Our qualitative and quantitative experiments achieve state-of-the-art results that outperform recent SOTA techniques.
Code and trained models are available at https://sigal-raab.github.io/MoDi. 1.

Introduction
The field of motion synthesis includes a wide range of long-standing tasks whose goal is to generate a sequence of temporally coherent poses that satisfy given cues and/or spatio-temporal constraints and importantly, look natural.
In particular, learning to synthesize human motion from a given data distribution is a challenging task, especially when the dataset is highly diverse, unstructured, and unla-beled. In recent years, deep neural networks have become a popular tool for motion generation, and their excellent per-formance is imputed to their ability to learn motion priors from large-scale datasets. However, learning a motion prior from a diverse dataset remains a challenge.
Previous works focus on synthesizing specific types of motion of limited diversity [28], conditioned by a set of frames [62] or by a label indicating a text or an action [51].
In this work, we present MoDi, an unconditional gener-ative model that synthesizes diverse motions. MoDi is un-supervised and is trained on diverse, unstructured, and un-labeled datasets, yielding a well-behaved, highly semantic latent space, facilitating a variety of synthesis operations.
Our design is inspired by the powerful architecture of
StyleGAN [34], which has become a foundation for syn-thesis in the imaging domain, as it learns a well-structured latent space that allows incredible semantic editing capa-bilities [10]. However, there is a significant gap between the imaging and motion domains; Images possess a regu-larized 2D spatial structure with a relatively large number of degrees of freedom (DoF), while motion data is irregu-lar, consisting of a skeletal graph with a temporal axis that has a smaller number of DoF. To mitigate this gap, we have conducted a thorough study of potential operators (2D vs. 3D convolutions, with and without skeleton-aware [2]), ar-chitectural variations (order and number of blocks, resolu-tion, layers), and even propose a new building block (a con-volutional scaler) that enables us to achieve state-of-the-art results in unconditional motion synthesis.
Our results show that MoDi learns a structured latent space that can be clustered into regions of semantically sim-ilar motions without any supervision. This latent space fa-cilitates applications on diverse motions, including seman-tic editing, semantic interpolation between motions, and crowd animation.
In addition, we present an encoder architecture that leverages the knowledge we acquired on the generative model architecture, to invert unseen motions into MoDi’s latent space, facilitating the usage of our generative prior on real-world motions. Inversion by an encoder enables us to project motions into the latent space within a feed-forward pass, instead of optimizing the latent code which requires several minutes for a single input [45]. Importantly, an en-coder can better project a given motion to the well-behaved part of the latent space and can better assist in solving ill-posed problems (e.g., motion prediction from prefix) as it learns to target its projections to the healthy regions of the latent space instead of overfitting to the input motion.
We evaluate our model qualitatively and quantitatively on the Mixamo [5] and HumanAct12 [21] datasets and show that it outperforms SOTA methods in similar settings. The strength of our generative prior is demonstrated through the various applications we show in this work, such as motion fusion, denoising, and spatial editing. 2.