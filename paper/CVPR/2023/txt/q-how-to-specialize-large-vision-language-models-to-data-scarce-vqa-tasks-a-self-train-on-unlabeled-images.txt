Abstract
Finetuning a large vision language model (VLM) on a target dataset after large scale pretraining is a dominant paradigm in visual question answering (VQA). Datasets for specialized tasks such as knowledge-based VQA or VQA in non natural-image domains are orders of magnitude smaller than those for general-purpose VQA. While col-lecting additional labels for specialized tasks or domains can be challenging, unlabeled images are often available.
We introduce SelTDA (Self-Taught Data Augmentation), a strategy for finetuning large VLMs on small-scale VQA datasets. SelTDA uses the VLM and target dataset to build a teacher model that can generate question-answer pseudola-bels directly conditioned on an image alone, allowing us to pseudolabel unlabeled images. SelTDA then finetunes the initial VLM on the original dataset augmented with freshly pseudolabeled images. We describe a series of experiments showing that our self-taught data augmentation increases ro-bustness to adversarially searched questions, counterfactual examples and rephrasings, improves domain generalization, and results in greater retention of numerical reasoning skills.
The proposed strategy requires no additional annotations or architectural modifications, and is compatible with any mod-ern encoder-decoder multimodal transformer. Code avail-able at https://github.com/codezakh/SelTDA. 1.

Introduction
Large, pretrained vision language foundation models [3, 20, 25, 26, 35, 49] are approaching human-level performance on visual question answering (VQA) [26, 50–52, 54, 62], as measured by the standard VQAv2 [13] benchmark. Yet on more complex VQA tasks [37, 43] there is a larger gap be-tween humans and machines. One difficulty is the small scale of datasets for complex VQA tasks or those in domains beyond natural images. The first solution to deal with the
∗work done while at NEC Labs America
Figure 1. SelTDA expands the self-training paradigm to VQA. By self-generating supervision (orange line) for an image I without needing extra annotations, we can augment a target dataset with new images and their pseudo-questions and answers (Q, A). data scarcity is to employ transfer learning from a larger
VQA dataset (e.g. VQAv2) to the smaller, specialized VQA dataset. However weaknesses of VQA models such as lack of consistency [44], weakness to adversarially searched ques-tions [27] and tendency to cheat by learning shortcuts [8] can be exacerbated when fine-tuning on small datasets.
Collecting annotations to expand a dataset for knowledge-intensive tasks or specialized domains is often prohibitively expensive. However, unlabeled images are cheap and often available. How can we exploit unlabeled images for specific visual question answering tasks? One possibility is to gen-erate new question+answer pairs for the unlabeled images, and use them during training. However, existing methods for visual question generation require images with annotations — either ground truth captions [2,4], or bounding boxes [21,48].
Even if these annotations were to be acquired, they induce a limited set of possible questions; they are limited to objects and concepts included in the acquired annotation, which are in turn limited by the finite label space of pretrained object detectors and the information disparity between a caption and an image (an image usually contains much more content
Figure 2. Motivating experiment. We sample increasingly diverse captions from BLIP [26], convert them to questions, and pose the questions to BLIP after finetuning on VQAv2. As caption diversity increases, self-agreement decreases (right panel). Despite the diversity, many captions remain correct (middle panel), suggesting that the VLM has knowledge that is not exhausted by task-specific finetuning. than a short caption can describe).
Motivating Experiment: In Fig 2, we show that a large vision-language model (VLM) pretrained on web-scale data contains knowledge that can be drawn out with image-conditional text generation, but which the model cannot verify when posed as a visual question-answering task. We prompt the BLIP [26] VLM (pretrained on 129M image-text pairs) to caption 1000 images from the CC3M [45] dataset starting with the phrase “this is a”. We convert each caption into a boolean question where the correct answer is
“yes” by inserting the caption into the template is this a <caption>? Next, we ask a BLIP VLM finetuned on the VQAv2 dataset [13] to choose between “yes” and “no” for each caption turned into a question. Surprisingly, the
VQA-finetuned BLIP answers “no” to at least 5% of the questions, increasing to 15% as the diversity of captions increases (adjusted by top-p parameter in nucleus sampling).
This suggests the possibility that the VLM has knowledge it cannot exploit when answering questions, but is accessible when directly generating text conditioned on an image.
Approach: To exploit unlabeled images for VQA, we pro-pose SelTDA, a three-stage framework for Self-Taught Data
Augmentation (Fig 1 bottom panel). We adapt the paradigm of self-training used in object detection [29, 65] and image classification [41, 60] for VQA. In classification / detection, the task of labeling an image is identical to prediction, and the teacher and student optimize identically structured ob-jectives. In VQA self-training, the student and teacher tasks are different. A teacher must pose and answer a question given an image, while the student provides an answers given a question and image. To handle this, we first cast the task of the teacher as a direct image-to-text generation task, and introduce a teacher model by updating the weights of the
VLM to learn an image-conditional visual question gener-ation model VQGIC. Next, we use VQGIC as a teacher to pseudolabel unlabeled images by sampling questions and answers from VQGIC with stochastic decoding. Finally, we augment the original VQA dataset with the newly labeled image-question-answer pairs, and finetune the VLM for vi-sual question answering on the augmented VQA dataset.
Benefits: SelTDA allows us generate synthetic training data by approximating the distribution P (Q, A|I) of the target
VQA task, where Q, A, I represents a question, answer, and image respectively. One benefit is that the synthetic data in-creases the number of training pairs available for finetuning, which effects an increase in raw performance. A second ben-efit is an increase in the diversity of questions and answers due to the introduction of new images and the stochastic nature of the text decoding, which results in increased ro-bustness and domain generalization. A third benefit is the distillation of knowledge from pretraining and transfer learn-ing into the synthetic training data, which can teach new skills (e.g. domain generalization) or prevent the forget-ting of specific skills (e.g. numerical reasoning). Finally
SelTDA is architecture-agnostic given a vision-language model capable of image-conditional text-generation. Our contributions can be summarized as follows: 1. We introduce SelTDA, a variant of the self-training paradigm that is designed for VQA and large gener-ative pretrained VLMs. 2. We propose treating visual question generation as a di-rect image-to-text task by leveraging the autoregressive decoder of a large, pretrained VLM, enabling us to gen-erate questions and answers from an unlabeled image with no auxillary annotations needed. 3. We show that a large VLM trained with the proposed
SelTDA gains increased robustness, domain general-ization, numerical reasoning, and performance when finetuning on small-scale VQA datasets.
2.