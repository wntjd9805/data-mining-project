Abstract
Self-supervised representation learning techniques have been developing rapidly to make full use of unlabeled im-ages. They encode images into rich features that are oblivi-ous to downstream tasks. Behind their revolutionary repre-sentation power, the requirements for dedicated model de-signs and a massive amount of computation resources ex-pose image encoders to the risks of potential model steal-ing attacks - a cheap way to mimic the well-trained en-coder performance while circumventing the demanding re-quirements. Yet conventional attacks only target supervised classiﬁers given their predicted labels and/or posteriors, which leaves the vulnerability of unsupervised encoders un-explored.
In this paper, we ﬁrst instantiate the conventional steal-ing attacks against encoders and demonstrate their severer vulnerability compared with downstream classiﬁers. To bet-ter leverage the rich representation of encoders, we fur-ther propose Cont-Steal, a contrastive-learning-based at-tack, and validate its improved stealing effectiveness in var-ious experiment settings. As a takeaway, we appeal to our community’s attention to the intellectual property protec-tion of representation learning techniques, especially to the defenses against encoder stealing attacks like ours. 1 1.

Introduction
Recent years have witnessed the great success of apply-ing deep learning (DL) to computer vision tasks. Different from supervised DL models, self-supervised learning which transforms unlabeled data samples into rich representations, has gained more and more popularity.
Behind its powerful representation, it is non-trivial to ob-tain a state-of-the-art image encoder. For instance, Sim-CLR [6] uses 128 TPU v3 cores to pre-train a ResNet-Figure 1. Model stealing attacks against classiﬁers (previous) v.s. model stealing attacks against encoders (ours). Previous works aim to steal a whole classiﬁer using the predicted label or posteri-ors of a target model. In our work, we aim to steal the target en-coder using its embeddings. The target encoder (Et) is pre-trained and ﬁxed, as shown in the solid frame. The surrogate encoder (Es) is trainable by the adversary, as shown in the dashed frame. 50 encoder with a batch size of 4096. Therefore, many big companies provide cloud-based self-supervised learn-ing encoder services for users. For instance, Cohere,2 Ope-nAI,3 and Clarifai4 provide the embedding API of images and texts for commercial usage. There are many works
[10, 26, 30] exploring security issues of encoder-based API.
Therefore, it is a very important and urgent problem.
These kinds of service leave the possibility of model stealing attacks [5, 25, 28, 37, 44, 48, 50, 51].
In these at-tacks, the adversary aims to steal the parameters or func-tionalities of target models with only query access to them.
A successful model stealing attack does not only threaten 2https://cohere.ai/ 3https : / / beta . openai . com / docs / api - reference / embeddings/ 1See our code in https://github.com/zeyangsha/Cont-4https://www.clarifai.com/models/general-image-Steal. embedding/
the intellectual property of the target model, but also serves as a stepping stone for further attacks such as adversarial ex-amples [3, 4, 16, 38, 47, 52], backdoor attacks [7, 26, 41, 43], and membership inference attacks [22–24, 30, 33, 34, 40, 42, 45,46]. So far, model stealing attacks concentrate on the su-pervised classiﬁers, i.e., the model responses are prediction posteriors or labels for a speciﬁc downstream task. The vul-nerability of unsupervised image encoders is unfortunately unexplored.
To ﬁll this gap, we pioneer the systematic
Our Work. investigation of model stealing attacks against image en-coders.
In this work, the adversary’s goal is to steal the functionalities of the target model. See Figure 1 for an overview and a comparison with previous works. More speciﬁcally, we focus on encoders trained by contrastive learning, which is one of the most cutting-edge unsuper-vised representation learning strategies that unleash the in-formation of unlabeled data.
We ﬁrst instantiate the conventional stealing attacks against encoders and expose their vulnerability. Given an input image, the target encoder outputs its representation (referred to as embedding). Similar to model stealing at-tacks against classiﬁers, we consider the embedding as the
“ground truth” label to guide the training procedure of a sur-rogate encoder on the adversary side. To measure the effec-tiveness of stealing attacks, we train an extra linear layer for the target and surrogate encoders towards the same down-stream classiﬁcation task. Preferably, the surrogate model should achieve both high classiﬁcation accuracy and high agreement with the target predictions.
We evaluate our attacks on ﬁve datasets against four con-trastive learning encoders. Our results demonstrate that the conventional attacks are more effective against encoders than against downstream classiﬁers. For instance, when we steal the downstream classiﬁer pre-trained by SimCLR on
CIFAR10 (with posteriors as its responses) using STL10 as the surrogate dataset, the adversary can only achieve an ac-curacy of 0.359. The accuracy, however, increases to 0.500 instead when we steal its encoder (with the embedding as its response).
Despite its encouraging performance, conventional at-tacks are not the most suitable ones against encoders. This is because they treat each image-embedding pair individu-ally without interacting across pairs. Different embeddings are beneﬁcial to each other as they can serve as anchors to better locate the position of the other embeddings in their space. Contrastive learning [6, 8, 17, 20, 27, 49, 53] is a straightforward idea to achieve this goal. It is formulated to enforce the embeddings of different augmentations of the same images closer and those of different images further.
In a similar spirit, we propose Cont-Steal, a contrastive-learning-based model stealing attack against the encoder.
The goal of Cont-Steal is to enforce the surrogate embed-ding of an image close to its target embedding (deﬁned as a positive pair) and also push away embeddings of different images irrespective of being generated by the target or the surrogate encoders (deﬁned as negative pairs).
The comprehensive evaluation shows that Cont-Steal outperforms the conventional model stealing attacks to a large extent. For instance, when CIFAR10 is the target dataset, Cont-Steal achieves an accuracy of 0.714 on the
SimCLR encoder pretrained on CIFAR10 with the surro-gate dataset and downstream dataset being STL10, while the conventional attack only achieves 0.457 accuracy. Also,
Cont-Steal is more query-efﬁcient and dataset-independent (see Figure 9 for more details). This is because Cont-Steal leverages higher-order information across samples to mimic the functionality of the target encoder. To mitigate the at-tacks, we evaluate different defense mechanisms including noise, top-k, rounding, and watermark. Our evaluations show that in most cases, these mechanisms cannot effec-tively defend against Cont-Steal. Among them, top-k can reduce the attack performance to the largest extent. How-ever, it also strongly limits the target model’s utility.
As a takeaway, our attack further exposes the severe vulnerability of pre-trained encoders. We appeal to our community’s attention to the intellectual property protec-tion of representation learning techniques, especially to the defenses against encoder stealing attacks like ours. 2. Threat Model
In this work, for the encoder pre-trained with images, we consider image classiﬁcation as the downstream task. We refer to the encoder as the target encoder. Then we treat both the encoder and the linear layer trained for the downstream task together as the target model. We ﬁrst introduce the adversary’s goal and then characterize different background knowledge that the adversary might have.
Adversary’s Goal. Following previous work [25, 28, 44], we taxonomize the adversary’s goal into two dimensions, i.e., theft and utility. The theft adversary aims to build a sur-rogate encoder that has similar performance on the down-stream tasks as the target encoder. Different from the thief adversary, the goal of the utility adversary is to construct a surrogate encoder that behaves normally on different down-stream tasks. In this case, the surrogate encoder not only faithfully “copies” the behaviors of the target encoder, but also serves as a stepping stone to conduct other attacks.
Adversary’s