Abstract
To address the challenges of long-tailed classification, researchers have proposed several approaches to reduce model bias, most of which assume that classes with few samples are weak classes. However, recent studies have shown that tail classes are not always hard to learn, and model bias has been observed on sample-balanced datasets, suggesting the existence of other factors that affect model
In this work, we systematically propose a series of bias. geometric measurements for perceptual manifolds in deep neural networks, and then explore the effect of the geo-metric characteristics of perceptual manifolds on classi-fication difficulty and how learning shapes the geometric characteristics of perceptual manifolds. An unanticipated finding is that the correlation between the class accuracy and the separation degree of perceptual manifolds grad-ually decreases during training, while the negative cor-relation with the curvature gradually increases, implying that curvature imbalance leads to model bias. Therefore, we propose curvature regularization to facilitate the model to learn curvature-balanced and flatter perceptual mani-folds. Evaluations on multiple long-tailed and non-long-tailed datasets show the excellent performance and exciting generality of our approach, especially in achieving signifi-cant performance improvements based on current state-of-the-art techniques. Our work opens up a geometric analysis perspective on model bias and reminds researchers to pay attention to model bias on non-long-tailed and even sample-balanced datasets. The code and model will be made public. 1.

Introduction
The imbalance of sample numbers in the dataset gives rise to the challenge of long-tailed visual recognition. Most previous works assume that head classes are always easier to be learned than tail classes, e.g., class re-balancing [8,14, 24, 34, 37, 46, 52], information augmentation [23, 31, 35, 38, 39, 44, 56, 64, 67], decoupled training [10, 16, 29, 30, 71, 76], and ensemble learning [20, 36, 57, 58, 61, 72, 77] have been proposed to improve the performance of tail classes. How-ever, recent studies [3,50] have shown that classification dif-Figure 1. Curvature regularization reduces the model bias present in multiple methods on CIFAR-100-LT and ImageNet-LT. The model bias is measured with the variance of the accuracy of all classes, and it is zero when the accuracy of each class is the same. ficulty is not always correlated with the number of samples, e.g., the performance of some tail classes is even higher than that of the head classes. Also, [49] observes differences in model performance across classes on non-long-tailed data, and even on balanced data. Therefore, it is necessary to ex-plore the impact of other inherent characteristics of the data on the classification difficulty, and then improve the overall performance by mitigating the model bias under multiple sample number distribution scenarios.
Focal loss [37] utilizes the DNNâ€™s prediction confidence on instances to evaluate the instance-level difficulty. [50] ar-gues that for long-tailed problems, determining class-level difficulty is more important than determining instance-level difficulty, and therefore defines classification difficulty by evaluating the accuracy of each class in real-time. How-ever, both methods rely on the model output and still can-This work was supported in part by the Key Scientific Technolog-ical Innovation Research Project by Ministry of Education, the State
Key Program and the Foundation for Innovative Research Groups of the National Natural Science Foundation of China (61836009), the Ma-jor Research Plan of the National Natural Science Foundation of China (91438201, 91438103, and 91838303), the National Natural Science Foun-dation of China (U22B2054, U1701267, 62076192, 62006177, 61902298, the Program 61573267, 61906150, and 62276199), for Cheung Kong Scholars and Innovative Research Team in Univer-sity (IRT 15R53), the ST Innovation Project from the Chinese Ministry of Education, the Key Research and Development Program in Shaanxi
Province of China(2019ZDLGY03-06), the National Science Basic Re-search Plan in Shaanxi Province of China(2022JQ-607), the China Post-doctoral fund(2022T150506), the Scientific Research Project of Educa-tion Department In Shaanxi Province of China (No.20JY023), the National
Natural Science Foundation of China (No. 61977052). the 111 Project,
not explain why the model performs well in some classes and poorly in others. Similar to the number of samples, we would like to propose a measure that relies solely on the data itself to model class-level difficulty, which helps to un-derstand how deep neural networks learn from the data. The effective number of samples [14] tries to characterize the di-versity of features in each class, but it introduces hyperpa-rameters and would not work in a sample-balanced dataset.
Most data distributions obey the manifold distribution law [33, 54], i.e., samples of each class are distributed near a low-dimensional manifold in the high-dimensional space.
The manifold consisting of features in the embedding space is called a perceptual manifold [11]. The classification task is equivalent to distinguishing each perceptual manifold, which has a series of geometric characteristics. We specu-late that some geometric characteristics may affect the clas-sification difficulty, and therefore conduct an in-depth study.
The main contributions of our work are: (1) We sys-tematically propose a series of measurements for the geo-metric characteristics of point cloud perceptual manifolds in deep neural networks (Sec 3). (2) The effect of learn-ing on the separation degree (Sec 4.1) and curvature (Sec 4.2) of perceptual manifolds is explored. We find that the correlation between separation degree and class accuracy decreases with training, while the negative correlation be-tween curvature and class accuracy increases with training (Sec 4.3), implying that existing methods can only mitigate the effect of separation degree among perceptual manifolds on model bias, while ignoring the effect of perceptual man-ifold complexity on model bias. (3) Curvature regulariza-tion is proposed to facilitate the model to learn curvature-balanced and flatter feature manifolds, thus improving the overall performance (Sec 5). Our approach effectively re-duces the model bias on multiple long-tailed (Fig 1) and non-long-tailed datasets (Fig 8), showing excellent perfor-mance (Sec 6). 2.