Abstract 1.

Introduction
The recent growth in the consumption of online media by children during early childhood necessitates data-driven tools enabling educators to filter out appropriate educa-tional content for young learners. This paper presents an approach for detecting educational content in online videos.
We focus on two widely used educational content classes: literacy and math. For each class, we choose prominent codes (sub-classes) based on the Common Core Standards.
For example, literacy codes include ‘letter names’, ‘letter sounds’, and math codes include ‘counting’, ‘sorting’. We pose this as a fine-grained multilabel classification problem as videos can contain multiple types of educational content and the content classes can get visually similar (e.g., ‘letter names’ vs ‘letter sounds’). We propose a novel class proto-types based supervised contrastive learning approach that can handle fine-grained samples associated with multiple labels. We learn a class prototype for each class and a loss function is employed to minimize the distances between a class prototype and the samples from the class. Similarly, distances between a class prototype and the samples from other classes are maximized. As the alignment between vi-sual and audio cues are crucial for effective comprehen-sion, we consider a multimodal transformer network to cap-ture the interaction between visual and audio cues in videos while learning the embedding for videos. For evaluation, we present a dataset, APPROVE, employing educational videos from YouTube labeled with fine-grained education classes by education researchers. APPROVE consists of 193 hours of expert-annotated videos with 19 classes. The proposed approach outperforms strong baselines on AP-PROVE and other benchmarks such as Youtube-8M, and
COIN. The dataset is available at https://nusci. csl.sri.com/project/APPROVE.
*Work partly done during an internship at SRI International.
With the expansion of internet access and the ubiquitous availability of smart devices, children increasingly spend a significant amount of time watching online videos. A recent nationally representative survey reported that 89% of par-ents of children aged 11 or younger say their child watches videos on YouTube [4]. Moreover, it is estimated that young children in the age range of two to four years consume 2.5 hours and five to eight years consume 3.0 hours per day on average [45,46]. Childhood is typically a key period for ed-ucation, especially for learning basic skills such as literacy and math [20, 25]. Unlike generic online videos, watch-ing appropriate educational videos supports healthy child development and learning [7, 22, 23]. Thus, analyzing the content of these videos may help parents, teachers, and me-dia developers increase young children’s exposure to high-quality education videos, which has been shown to produce meaningful learning gains [22]. As the amount of online content produced grows exponentially, automated content understanding methods are essential to facilitate this.
In this work, given a video, our goal is to determine whether the video contains any educational content and characterize the content. Detecting educational content re-quires identifying multiple distinct types of content in a video while distinguishing between similar content types.
The task is challenging as the education codes by Com-mon Core Standards [3, 40] can be similar such as ‘letter names’ and ‘letter sounds’, where the former focuses on the name of the letter and the latter is based on the phonetic sound of the letter. Also, understanding education content requires analyzing both visual and audio cues simultane-ously as both signals are to be present to ensure effective learning [3, 40]. This is in contrast to standard video classi-fication benchmarks such as the sports or generic YouTube videos in UCF101 [54] Kinetics400 [53], YouTube-8M [1], where visual cues are often sufficient to detect the different classes. Finally, unlike standard well-known action videos,
education codes are more structured and not accessible to common users. Thus, it requires a carefully curated set of videos and expert annotations to create a dataset to en-able a data-driven approach. In this work, we focus on two widely used educational content classes: literacy and math.
For each class, we choose prominent codes (sub-classes) based on the Common Core Standards that outline age-appropriate learning standards [3,40]. For example, literacy codes include ‘letter names’, ‘letter sounds’, ‘rhyming’, and math codes include ‘counting’, ‘addition subtraction’, ‘sort-ing’, ‘analyze shapes’.
We formulate the problem as a multilabel fine-grained video classification task as a video may contain multiple types of content that can be similar. We employ multi-modal cues since besides visual cues, audio cues provide important cues to distinguish between similar types of ed-ucational content. We propose a class prototypes based su-pervised contrastive learning approach to address the above-mentioned challenges. We learn a prototype embedding for each class. Then a loss function is employed to minimize the distance between a class prototype and the samples asso-ciated with the class label. Similarly, the distance between a class prototype and the samples without that class label is maximized. This is unlike the standard supervised con-trastive learning setup where inter-class distance is maxi-mized and intra-class distance is minimized by considering classwise positive and negative samples. This approach is shown to be effective for single-label setups [26]. However, it is not straightforward to extend this for the proposed mul-tilabel setup as samples cannot be identified as positive or negative due to the multiple labels. We jointly learn the em-bedding of the class prototypes and the samples. The em-beddings are learned by a multimodal transformer network (MTN) that captures the interaction between visual and au-dio cues in videos. We employ automatic speech recogni-tion (ASR) to transcribe text from the audio. The MTN con-sists of video and text encoders that learn modality-specific embedding and a cross-attention mechanism is employed to capture the interaction between them. The MTN is end-to-end learned through the contrastive loss.
Due to the lack of suitable datasets for evaluating fine-grained classification of education videos, we propose a new dataset, called APPROVE, of curated YouTube videos annotated with educational content. We follow Common
Core Standards [3, 40] to select education content suitable for the kindergarten level. We consider two high-level classes of educational content: literacy and math. For each of these content classes, we select a set of codes. For the lit-eracy class, we select 7 codes and for the math class, we se-lect 11 codes. Each video is associated with multiple labels corresponding to these codes. The videos are annotated by trained education researchers following standard validation protocol [41] to ensure correctness. APPROVE also con-sists of carefully chosen background videos, i.e., without educational content, that are visually similar to the videos with educational content. APPROVE consists of 193 hours of expert-annotated videos with 19 classes (7 literacy codes, 11 math codes, and a background) where each video has 3 labels on average.
Our contributions can be summarized as follows:
• APPROVE, a fine-grained multi-label dataset of edu-cation videos, to promote exploration in this field.
• Class prototypes based contrastive learning frame-work along with a multi-modal fusion transformer suit-able for the problem where videos have multiple fine-grained labels.
• Outperforming relevant baselines on three datasets:
APPROVE, YouTube-8M [1] and COIN [55]. 2.