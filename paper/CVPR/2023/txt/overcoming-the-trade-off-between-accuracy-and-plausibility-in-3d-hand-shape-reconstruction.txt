Abstract
Direct mesh ﬁtting for 3D hand shape reconstruction is highly accurate. However, the reconstructed meshes are prone to artifacts and do not appear as plausible hand shapes. Conversely, parametric models like MANO ensure plausible hand shapes but are not as accurate as the non-parametric methods.
In this work, we introduce a novel weakly-supervised hand shape estimation framework that integrates non-parametric mesh ﬁtting with MANO model in an end-to-end fashion. Our joint model overcomes the tradeoff in accuracy and plausibility to yield well-aligned and high-quality 3D meshes, especially in challenging two-hand and hand-object interaction scenarios. 1.

Introduction
State-of-the-art monocular RGB-based 3D hand recon-struction methods [6, 10, 17, 21, 22, 28] focus on recover-ing highly accurate 3D hand meshes. As accuracy is mea-sured by an average joint or vertex position error, recov-ered hand meshes may be well-aligned in 3D space but still be physically implausible. The 3D mesh surface may have irregular protrusions or collapsed regions (see Fig. 1), especially around the ﬁngers. The meshes may also suf-fer from incorrect contacts or penetrations when there are hand-object or two-hand interactions. Yet methods that pri-oritize physical plausibility, especially in interaction set-tings [3,8,10,14,20,21], are signiﬁcantly less accurate in 3D alignment. In summary, the current body of work predom-inantly favours either 3D alignment accuracy or physical plausibility, but cannot achieve both.
A closer examination reveals that the trade-off between 3D alignment and plausibility is also split methodology-wise. Methods that use the MANO model [30] produce plausible hand poses and hand shapes [2, 7, 38, 40, 42] due to the statistical parameterization of MANO. However, it is challenging to directly regress these parameters, since
Figure 1. The vertex error vs. edge length error indicates that existing methods trade-off alignment accuracy with plausibility.
MANO-based methods (triangles) vs. non-parametric model-based methods (circles) have a trade-off between vertex error and edge length error; our combined method (stars) can overcome this trade-off to yield well-aligned and plausible meshes. Plot of re-sults from InterHand2.6M [27]; visualization from FreiHand [42]. the mapping from an image to the MANO parameter space is highly non-linear. As a consequence, MANO-based methods lag in 3D alignment accuracy compared to non-parametric methods.
Non-parametric methods [6, 10, 11, 17, 18, 21, 22, 28] di-rectly ﬁt a 3D mesh to image observations. Direct mesh
ﬁtting is accurate but is prone to surface artifacts. In scenar-ios with hand-object or hand-hand interactions, mesh pen-etrations cannot be resolved meaningfully even with regu-larizers such as contact losses [14] due to the unconstrained
optimization. Attention mechanism [21, 32] can mitigate some penetrations and artifacts, but the inherent problem remains. As such, the favoured approaches for hand-object and hand-hand interactions are still driven by MANO mod-els [3, 8, 13, 14, 38].
In this work, we aim to recover high-quality hand meshes that are accurately aligned and have minimal artifacts and penetrations. To avoid a trade-off, we leverage direct mesh
ﬁtting for alignment accuracy and guidance from MANO for plausibility. Combine the non-parametric and paramet-ric models is straightforward in terms of motivation. How-ever, merging the two is non-trivial because it requires a mapping from non-parametric mesh vertices to parametric model parameters. This mapping, analogous to the map-ping from an RGB image, is highly non-linear and difﬁcult to achieve directly [16].
One of our key contributions in this work is a method to accurately map non-parametric hand mesh vertices to
MANO joint parameters θ. To do so, we perform a two-step mapping, from mesh-vertices to the joint coordinates, and joints coordinates to θ. In the literature, the common prac-tice for the former is to leverage the J matrix in MANO and linearly regress the joints from the mesh [17,20,21]. Yet the
J matrix was designed to only map MANO-derived meshes to joints in a rest pose (see Eq. 10 in [25]). As we show in our experiments, applying J to non-rest poses introduces a gap of around 2 mm. Furthermore, we postulate that there is a domain gap between the estimated non-parametric meshes and MANO-derived meshes, even if both meshes have the same topology. To close this gap – we propose a VAE cor-rection module, to be applied after the linear regression with
J . To map the recovered joints from the mesh to θ, we use a twist-swing decomposition and analytically compute the
θ. It has been shown previously in [20] that decomposing joint rotations into twist-swing rotations [1] can simplify the estimation of human SMPL [25] model pose parame-ters. Inspired by [20], we also leverage the decomposition and further verify that the twist angle has minimal impact on the hand.
Note that obtaining ground truth labels for hand mesh vertices is non-trivial. Our framework lends itself well for weak-supervision. Since the estimated 3D mesh from the non-parametric decoder is regressed into 3D joints, it can also be supervised with 3D joints as weak labels (see Fig 2). At the same time, the parametric mesh estimated from these joints can be used as a pseudo-label for learning the non-parametric mesh vertices. Such a procedure distills the knowledge from the parametric model and is effective with-out ground truth mesh annotations. We name our method
WSIM 3D Hand, in reference to Weakly-supervised Self-distillation Integration Model for 3D hand shape recon-struction. Our contributions include:
• A novel framework that integrates a parametric and a non-parametric mesh model for accurate and plausible 3D hand reconstruction.
• A VAE correction module that closes the overlooked gap between non-parametric and MANO 3D poses;
• A weakly-supervised pipeline, competitive to a fully-supervised counterpart, using only 3D joint labels to learn 3D meshes.
• Signiﬁcant improvements over state-of-the-art on two-hand interaction benchmark interaction on hand-object or datasets, especially in hand-object
DexYCB. 2.