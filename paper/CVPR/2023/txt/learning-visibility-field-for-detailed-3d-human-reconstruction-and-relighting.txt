Abstract cation [24, 63], movie and gaming industry [7].
Detailed 3D reconstruction and photo-realistic relight-ing of digital humans are essential for various applications.
To this end, we propose a novel sparse-view 3d human re-construction framework that closely incorporates the oc-cupancy field and albedo field with an additional visibil-ity field–it not only resolves occlusion ambiguity in multi-view feature aggregation, but can also be used to evalu-ate light attenuation for self-shadowed relighting. To en-hance its training viability and efficiency, we discretize vis-ibility onto a fixed set of sample directions and supply it with coupled geometric 3D depth feature and local 2D im-age feature. We further propose a novel rendering-inspired loss, namely TransferLoss, to implicitly enforce the align-ment between visibility and occupancy field, enabling end-to-end joint training. Results and extensive experiments demonstrate the effectiveness of the proposed method, as it surpasses state-of-the-art in terms of reconstruction ac-curacy while achieving comparably accurate relighting to ray-traced ground truth. 1.

Introduction 3D reconstruction and relighting are of great impor-tance in human digitization, especially in supporting real-istic rendering in varying virtual environments, that can be widely applied in AR/VR [32, 37], holographic communi-Traditional methods often require dense camera setups using multi-view stereo, non-rigid registration and texture mapping [9, 13]. To enhance capture realism, researchers have extended them with additional synchronous variable illumination systems, which aid photometric stereo for de-tail reconstruction and material acquisition [12]. However, these systems are often too complex, expensive and difficult to maintain, thus preventing widespread applications.
By leveraging deep prior and neural representation, so-phisticated dense camera setups can be reduced to a sin-gle camera, leading to blossoms in learning-based human
In particular, encoding human geometry reconstruction. and appearance as continuous fields using Multi-Layer Per-ceptron (MLP) has emerged as a promising lead. Starting from Siclope [36] and PIFu [43], a series of methods [15] improve the reconstruction performance in speed [11, 25], quality [44], robustness [65, 66] and light decoupling [1].
However, single-view reconstruction quality is restricted by its inherent depth ambiguity, thus limiting its application under view-consistent high-quality requirements.
Therefore, as the trade-off between view coverage and system accessibility, sparse-view reconstruction has be-come a research hotpot. The predominant practice is to project the query point onto each view to interpolate lo-cal features, which are then aggregated and fed to MLP for inference [6, 16, 43, 47, 52, 61]. This method suffers from occlusion ambiguity, where some views may well
be occluded, and mixing their features with visible ones causes inefficient feature utilization, thus penalizing the re-construction quality [43]. A natural solution is to filter fea-tures based on view visibility. Human templates such as
SMPL [29] can serve as effective guidance [2,40,56,64], but introduce additional template alignment errors and there-fore cannot guarantee complete occlusion awareness. Func-tion4D [61] leverages the truncated Projective Signed Dis-tance Function (PSDF) for visibility indication, but its level of details is susceptible to depth noise.
To this end, we directly model a continuous visibility field, which can be efficiently learned with our proposed framework and discretization technique using sparse-view
RGB-D input. The visibility field enables efficient visibil-ity query, which effectively guides multi-view feature ag-gregation for more accurate occupancy and albedo infer-ence. Moreover, visibility can also be directly used for light attenuation evaluation–the key ingredient in achiev-ing realistic self-shadowing. When supervising jointly with our novel TransferLoss, the alignment between the visibility field and occupancy field can be implicitly enforced without between-field constraints, such as matching visibility with occupancy ray integral. We train our framework end-to-end and demonstrate its effectiveness in detailed 3D human re-construction by quantity and quality comparison with the state-of-the-art. We directly relight the reconstructed geom-etry with inferred visibility using diffuse Bidirectional Re-flectance Distribution Function (BRDF) as in Fig. 1, which achieves photo-realistic self-shadowing without any post ray-tracing steps. To conclude, our contributions include:
• An end-to-end framework for sparse-view detailed 3D human reconstruction that also supports direct self-shadowed relighting.
• A novel method of visibility field learning, with the specifically designed TransferLoss significantly im-proves field alignment.
• A visibility-guided multi-view feature aggregation strategy that guarantees occlusion awareness. 2.