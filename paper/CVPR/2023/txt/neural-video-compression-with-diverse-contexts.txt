Abstract
For any video codecs, the coding efficiency highly re-lies on whether the current signal to be encoded can find the relevant contexts from the previous reconstructed sig-nals. Traditional codec has verified more contexts bring substantial coding gain, but in a time-consuming manner.
However, for the emerging neural video codec (NVC), its contexts are still limited, leading to low compression ra-tio. To boost NVC, this paper proposes increasing the context diversity in both temporal and spatial dimensions.
First, we guide the model to learn hierarchical quality pat-terns across frames, which enriches long-term and yet high-quality temporal contexts. Furthermore, to tap the poten-tial of optical flow-based coding framework, we introduce a group-based offset diversity where the cross-group interac-tion is proposed for better context mining. In addition, this paper also adopts a quadtree-based partition to increase spatial context diversity when encoding the latent repre-sentation in parallel. Experiments show that our codec obtains 23.5% bitrate saving over previous SOTA NVC.
Better yet, our codec has surpassed the under-developing next generation traditional codec/ECM in both RGB and
YUV420 colorspaces, in terms of PSNR. The codes are at https://github.com/microsoft/DCVC. 1.

Introduction
The philosophy of video codec is that, for the current signal to be encoded, the codec will find the relevant con-texts (e.g., various predictions as the contexts) from previ-ous reconstructed signals to reduce the spatial-temporal re-dundancy. The more relevant contexts are, the higher bitrate saving is achieved.
If looking back the development of traditional codecs (from H.261 [17] in 1988 to H.266 [7] in 2020), we find that the coding gain mainly comes from the continuously expanded coding modes, where each mode uses a specific manner to extract and utilize context. For example, the numbers of intra prediction directions [42] in H.264, H.265,
H.266 are 9, 35, and 65, respectively. So many modes can
Figure 1. Average results on UVG, MCL-JCV, and HEVC datasets. All traditional codecs use their best compression ratio configuration. DCVC-HEM [29] is the previous SOTA NVC and only has released the model for RGB colorspace. extract diverse contexts to reduce redundancy, but also bring huge complexity as rate distortion optimization (RDO) is used to search the best mode. For encoding a 1080p frame, the under-developing ECM (the prototype of next genera-tion traditional codec) needs up to half an hour [49]. Al-though some DL-based methods [24,51,52] proposed accel-erating traditional codecs, the complexity is still very high.
By contrast, neural video codec (NVC) changes the ex-traction and utilization of context from hand-crafted de-sign to automatic-learned manner. Mainstream frame-works of NVC can be classified into residual coding-based
[1, 13, 31, 32, 34, 36, 47, 59, 61] and condition coding-based
[21, 27–29, 33, 38, 50]. The residual coding explicitly uses the predicted frame as the context, and the context utiliza-tion is restricted to use subtraction for redundancy removal.
By comparison, conditional coding implicitly learns feature domain contexts. The high dimension contexts can carry richer information to facilitate encoding, decoding, as well as entropy modelling.
However, for most NVCs, the manners of context extrac-tion and utilization are still limited, e.g., only using optical flow to explore temporal correlation. This makes NVC eas-ily suffer from the uncertainty [12, 16, 37] in parameters or fall into local optimum [25]. One solution is adding tradi-tional codec-like coding modes into NVC [25]. But it brings
large computational complexity as RDO is used. So the question comes up: how to better learn and use the contexts while yielding low computational cost?
[28]
To this end, based on DCVC (deep contextual video framework and its following work compression)
DCVC-HEM [29], we propose a new model DCVC-DC which efficiently utilizes the Diverse Contexts to further boost compression ratio. At first, we guide DCVC-DC to learn hierarchical quality pattern across frames. With this guidance during the training, the long-term and yet high-quality contexts which are vital for the reconstruction of the following frames are implicitly learned during the fea-ture propagation. This helps further exploit the long-range temporal correlation in video and effectively alleviate the quality degradation problem existed in most NVCs. In ad-dition, we adopt the offset diversity [8] to strengthen the optical flow-based codec, where multiple offsets can reduce the warping errors for complex or large motions. In particu-lar, inspired by the weighted prediction in traditional codec, the offsets are divided into groups and the cross-group fu-sion is proposed to improve the temporal context mining.
Besides from temporal dimension, this paper also pro-poses increasing the spatial context diversity when encod-ing the latent representation. Based on recent checkerboard model [19] and dual spatial model [29, 56], we design a quadtree-based partition to improve the distribution estima-tion. When compared with [19, 29], the types of correlation modelling are more diverse hence the model has a larger chance to find more relevant context.
It is noted that all our designs are parallel-efficient. To further reduce the computational cost, we also adopt depth-wise separable convolution [10], and assign unequal chan-nel numbers for features with different resolutions. Experi-ments show that our DCVC-DC achieves much higher effi-ciency over previous SOTA NVC and pushes the compres-sion ratio to a new height. When compared with DCVC-HEM [29], 23.5 % bitrate saving is achieved while MACs (multiply–accumulate operations) are reduced by 19.4%.
Better yet, besides H.266-VTM 17.0, our codec also already outperforms ECM-5.0 (its best compression ratio config-uration for low delay coding is used) in both RGB and
YUV420 colorspaces, as shown in Fig. 1. To the best of our knowledge, this is the first NVC which can achieve such accomplishment. In summary, our contributions are:
• We propose efficiently increasing context diversity to boost NVC. Diverse contexts are complementary to each other and have larger chance to provide good ref-erence for reducing redundancy.
• From temporal dimension, we guide model to extract high-quality contexts to alleviate the quality degrada-tion problem. In addition, the group-based offset di-versity is designed for better temporal context mining.
• From spatial dimension, we adopt a quadtree-based partition for latent representation. This provides di-verse spatial contexts for better entropy coding.
• Our DCVC-DC obtains 23.5% bitrate saving over the previous SOTA NVC. In particular, our DCVC-DC has surpassed the best traditional codec ECM in both RGB and YUV420 colorspaces, which is an important mile-stone in the development of NVC. 2.