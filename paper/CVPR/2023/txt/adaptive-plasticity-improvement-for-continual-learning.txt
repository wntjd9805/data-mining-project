Abstract
Many works have tried to solve the catastrophic forget-ting (CF) problem in continual learning (lifelong learning).
However, pursuing non-forgetting on old tasks may dam-age the model’s plasticity for new tasks. Although some methods have been proposed to achieve stability-plasticity trade-off, no methods have considered evaluating a model’s plasticity and improving plasticity adaptively for a new task.
In this work, we propose a new method, called adaptive plasticity improvement (API), for continual learning. Be-sides the ability to overcome CF on old tasks, API also tries to evaluate the model’s plasticity and then adaptively im-prove the model’s plasticity for learning a new task if nec-essary. Experiments on several real datasets show that API can outperform other state-of-the-art baselines in terms of both accuracy and memory usage. 1.

Introduction
Continual learning is a challenging setting in which agents learn multiple tasks sequentially [21]. However, neu-ral network models lack the ability to perform continual learning. Specifically, many studies [15, 21] have shown that directly training a network on a new task makes the model forget the old knowledge. This phenomenon is often called catastrophic forgetting (CF) [10, 21].
Continual learning models need to overcome CF, which is referred to as stability [21]. Many types of works are pro-posed for stability. For example, regularization-based meth-ods [13, 35] add a penalty to the loss function and minimize penalty loss with new task loss together for overcoming CF.
Memory-based methods [5, 6, 24, 29] maintain a memory to save the information of the old tasks and use saved informa-tion to keep old task performance. Expansion-based meth-ods [12, 16] expand the network’s architecture and usually freeze old tasks’ parameters to overcome CF.
However, having stability alone fails to give the model
*Wu-Jun Li is the corresponding author. continual learning ability. The model also needs plasticity to learn new tasks in continual learning. The term plas-ticity came from neuroscience and was originally used to describe the brain’s ability to yield physical changes in the neural structure. Plasticity allows us to learn, remem-ber, and adapt to dynamic environments [22].
In neu-ral networks, plasticity is used to describe the ability of a network to change itself for learning new tasks. How-ever, existing works [17, 18, 30] show that when overcom-ing CF for stability, the model’s plasticity will decrease, which will affect the performance of the model for learning new tasks. Specifically, regularization-based methods and memory-based methods use penalty or memory to constrain the parameters when the model learns a new task. When the number of old tasks increases, the constraints for the model parameters should become stronger and stronger to ensure stability. As a result, the model’s plasticity for learning new tasks decreases. Expansion-based methods [28,32] increase the model’s plasticity by expanding additional parameters.
However, most of these methods freeze the old part of the network, making the old part of the network underutilized.
Furthermore, all these methods do not consider how to eval-uate the model’s plasticity and improve it adaptively.
When overcoming CF, the model should improve its plasticity if it finds that current plasticity is insufficient to learn the new task. In this work, we propose a new method, called adaptive plasticity improvement (API), for continual learning. The main contributions of API are as follows:
• API overcomes CF through a new memory-based method called dual gradient projection mem-ory (DualGPM), which learns a gradient subspace that can represent the gradients of old tasks.
• Based on DualGPM, API evaluates the model’s plas-ticity for a new task by average gradient retention ra-tio (AGRR) and improves the model’s plasticity adap-tively for a new task if necessary.
• Experiments on several real datasets show that API can outperform other state-of-the-art baselines in terms of accuracy and memory usage.
2. Problem Formulation and