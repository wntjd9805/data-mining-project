Abstract
Large-scale training of modern deep learning models heavily relies on publicly available data on the web. This potentially unauthorized usage of online data leads to con-cerns regarding data privacy. Recent works aim to make unlearnable data for deep learning models by adding small, specially designed noises to tackle this issue. However, these methods are vulnerable to adversarial training (AT)
In this work, we pro-and/or are computationally heavy. pose a novel, model-free, Convolution-based Unlearnable
DAtaset (CUDA) generation technique. CUDA is generated using controlled class-wise convolutions with filters that are randomly generated via a private key. CUDA encourages the network to learn the relation between filters and labels rather than informative features for classifying the clean data. We develop some theoretical analysis demonstrat-ing that CUDA can successfully poison Gaussian mixture data by reducing the clean data performance of the optimal
Bayes classifier. We also empirically demonstrate the effec-tiveness of CUDA with various datasets (CIFAR-10, CIFAR-100, ImageNet-100, and Tiny-ImageNet), and architectures (ResNet-18, VGG-16, Wide ResNet-34-10, DenseNet-121,
DeIT, EfficientNetV2-S, and MobileNetV2). Our experi-ments show that CUDA is robust to various data augmenta-tions and training approaches such as smoothing, AT with different budgets, transfer learning, and fine-tuning. For instance, training a ResNet-18 on ImageNet-100 CUDA achieves only 8.96%, 40.08%, and 20.58% clean test accu-racies with empirical risk minimization (ERM), L∞ AT, and
L2 AT, respectively. Here, ERM on the clean training data achieves a clean test accuracy of 80.66%. CUDA exhibits unlearnability effect with ERM even when only a fraction of the training dataset is perturbed. Furthermore, we also show that CUDA is robust to adaptive defenses designed specifically to break it. 1.

Introduction
This encourages deep learning practitioners to scrape data from the web for data collection [8, 31, 38, 41]. Since a lot of the data is publicly available online, sometime this scrap-ping of data is unauthorized. For instance, a recent arti-cle [15] discloses that a private company trained a commer-cial face recognition system using over three billion facial images collected from the internet without any user consent.
Although such massive data can significantly boost the per-formance of deep learning models, it raises serious concerns about data privacy and security.
To prevent the unauthorized usage of personal data, a se-ries of recent papers [10,17,53] propose to poison data with additive noise. The idea is to make datasets unlearnable for deep learning models by ensuring that they learn the cor-respondence between noises and labels. Thereby, they do not learn much useful information about the clean data, sig-nificantly degrading their clean test accuracy. However, in recent works [11, 17, 47], these unlearnability methods are shown to be vulnerable to adversarial training (AT) frame-works [34]. Motivated by this problem, Fu et al. [11] devel-oped Robust Error-Minimization (REM) noises to make un-learnable data that is protected from AT. While the authors show the effectiveness of REM in multiple scenarios, we demonstrate that these methods are still not robust against different data augmentations or training settings (see Sec-tion 3.2). Furthermore, current unlearnability frameworks
[10, 11, 17, 53] are model-dependent and require expensive optimization steps on deep learning models to obtain the additive noises. They also need to train the deep learning models from scratch to obtain noises for each new data set.
In this paper, we propose a novel Convolution-based
Unlearnable DAtaset (CUDA) generation technique. We address limitations of existing unlearnable data generation techniques in Section 3.2 and motivate our CUDA tech-nique in Section 3.3. For generating CUDA, an attacker randomly generates different convolutional filters for each class in the dataset using a private key or seed value. These filters are used to perform controlled class-wise convolu-tions on the clean training dataset to obtain CUDA. As we
Modern deep learning training frameworks heavily de-pend on large-scale datasets for achieving high accuracy.
Code available here: https://github.com/vinusankars/
Convolution-based-Unlearnability
Figure 1. CIFAR-10 CUDA images from each of the 10 classes convolved using convolutional filters generated with blur parameter pb.
As seen in the plot, a higher pb value results in better unlearnability (lower clean test accuracy), but increased blurring. CIFAR-10 CUDA does not break with ERM for all the three pb values. In Section 5.2, we propose Deconvolution-based Adversarial Training (DAT) that we specifically design to break CUDA. DAT adversarially learns class-wise filters to deconvolve CUDA images. However, CIFAR-10 CUDA with pb ∈ {0.3, 0.5} does not break with DAT. Therefore, we select pb = 0.3 (lesser blurring) for CIFAR-10 CUDA as discussed in
Section 5.1. More details on DAT in supplementary material. describe in Sections 3.3 and 5.2, CUDA generation per-forms controlled convolutions using a blur parameter pb to ensure that the semantics of the dataset are preserved (see
Figure 1). CUDA generation with a lower blurring param-eter pb adds less perceptible noises to clean samples. A network trained by a defender on CUDA is encouraged to learn the shortcut relation between class-wise convolutional filters and labels rather than useful features for classifying the clean data. Since the seed value for generating the filters are private, its not possible for the defender to obtain clean data from CUDA alone. Additionally, CUDA exhibits un-learnability effect with ERM even when only a fraction of the training dataset is perturbed (see Section 5). While the existing unlearnability works use additive noises, CUDA generation technique enjoys the advantage of introducing multiplicative noises in the Fourier domain due to the con-volution theorem (since convolution of signals is the same as element-wise multiplication in the Fourier domain). This lets CUDA generation add higher amounts of noise in the image space, specifically along the edges in images, and makes it resilient to AT with small additive noise budgets.
In Figure 2, with the help of t-SNE plots [49], we also find that the noises added by CUDA generation is not linearly separable while they are linearly separable for the existing works on unlearnability [52].
In Section 4, we theoretically show that CUDA genera-tion can successfully poison Gaussian mixture data by de-grading the clean data accuracy of the optimal Bayes classi-fier. We state our result informally below while the formal version is presented in Theorem 2.
Theorem 1 (Informal) Let D denote a Gaussian mixture data with two modes, PD denote the optimal Bayes clas-sifier trained on D, and τD(PD) denote the accuracy of the classifier PD on D. Then, under some assumptions, for every clean data D, there is a CUDA ˜D such that
τD(P ˜D) < τD(PD).
Furthermore, our empirical experiments in Section 5 demonstrate the effectiveness of CUDA under various train-ing scenarios such as ERM with various augmentations and regularizations, AT with different budgets, randomized smoothing [6, 22, 27], transfer learning, and fine-tuning.
For instance, training a ResNet-18 on CIFAR-10 CUDA achieves only 18.48%, 44.4%, and 51.14% clean test ac-curacies with ERM, L∞ AT, and L2 AT, respectively (see
Figure 2). Here, ERM on the clean training data achieves a clean test accuracy of 94.66%. In addition, we also de-sign adaptive defenses to investigate if CUDA breaks with random or adversarial defense mechanisms. We find that
CUDA is robust to the adaptive defenses that we specifi-cally design to break it. 2.