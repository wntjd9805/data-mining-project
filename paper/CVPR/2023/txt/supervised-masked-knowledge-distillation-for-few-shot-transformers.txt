Abstract
Vision Transformers (ViTs) emerge to achieve impres-sive performance on many data-abundant computer vision tasks by capturing long-range dependencies among local features. However, under few-shot learning (FSL) settings on small datasets with only a few labeled data, ViT tends to overﬁt and suffers from severe performance degradation due to its absence of CNN-alike inductive bias. Previous works in FSL avoid such problem either through the help of self-supervised auxiliary losses, or through the dextile uses of label information under supervised settings. But the gap between self-supervised and supervised few-shot
Transformers is still unﬁlled. Inspired by recent advances in self-supervised knowledge distillation and masked image modeling (MIM), we propose a novel Supervised Masked
Knowledge Distillation model (SMKD) for few-shot Trans-formers which incorporates label information into self-distillation frameworks. Compared with previous self-supervised methods, we allow intra-class knowledge dis-tillation on both class and patch tokens, and introduce the challenging task of masked patch tokens reconstruc-tion across intra-class images. Experimental results on four few-shot classiﬁcation benchmark datasets show that our method with simple design outperforms previous meth-ods by a large margin and achieves a new start-of-the-art.
Detailed ablation studies conﬁrm the effectiveness of each component of our model. Code for this paper is available here: https://github.com/HL-hanlin/SMKD. 1.

Introduction
Vision Transformers (ViTs) [16] have emerge as a competitive alternative to Convolutional Neural Networks (CNNs) [31] in recent years, and have achieved impressive performance in many vision tasks including image classi-ﬁcation [16, 38, 59, 66], object detection [3, 10, 26–28, 79], and object segmentation [50, 55]. Compared with CNNs, which introduce inductive bias through convolutional ker-nels with ﬁxed receptive ﬁelds [35], the attention layers in
∗Equal contribution. †Corresponding author.
Figure 1. Comparison of the proposed idea and other exist-ing methods for few-shot Transformers. Our model mitigates the overﬁtting issue of few-shot Transformers, by extending the masked knowledge distillation framework into the supervised set-ting, and enforcing the alignment of [cls] and corresponding
[patch] tokens for intra-class images.
ViT allow it to model global token relations to capture long-range token dependencies. However, such ﬂexibility also comes at a cost: ViT is data-hungry and it needs to learn token dependencies purely from data. This property often makes it easy to overﬁt to datasets with small training set and suffer from severe generalization performance degra-dation [36, 37]. Therefore, we are motivated to study how to make ViTs generalize well on these small datasets, espe-cially under the few-shot learning (FSL) setting [19, 39, 62] which aims to recognize unseen new instances at test time just from only a few (e.g. one or ﬁve) labeled samples from each new categories.
Most of the existing methods mitigate the overﬁtting is-sue of few-shot Transformers [14] using various regulariza-tions. For instance, some works utilize label information in a weaker [70], softer [42] way, or use label information efﬁciently through patch-level supervision [14]. However, these models usually design sophisticated learning targets.
On the other hand, self-distillation techniques [4, 8], and particularly, the recent masked self-distillation [29, 45, 77], which distills knowledge learned from an uncorrupted im-age to the knowledge predicted from a masked image, have lead an emerging trend in self-supervised Transformers in various ﬁelds [15, 68].
Inspired by such success, recent works in FSL attempt to incorporate self-supervised pretext tasks into the standard supervised learning through auxil-iary losses [37, 43, 46], or to adopt a self-supervised pre-training, supervised training two-stage framework to train few-shot Transformers [18, 32]. Compared with traditional supervised methods, self-supervision can learn less biased representations towards base class, which usually leads to better generalization ability for novel classes [40]. How-ever, the two learning objectives of self-supervision and supervision are conﬂicting and it is hard to balance them during training. Therefore, how to efﬁciently leverage the strengths of self-supervised learning to alleviate the overﬁt-ting issue of supervised training remains a challenge.
In this work, we propose a novel supervised masked knowledge distillation framework (SMKD) for few-shot
Transformers, which handles the aforementioned challenge through a natural extension of the self-supervised masked knowledge distillation framework into the supervised set-ting (shown in Fig. 1). Different from supervised con-trastive learning [33] which only utilizes global image fea-tures for training, we leverage multi-scale information from the images (both global [cls] token and local [patch] tokens) to formulate the learning objectives, which has been demonstrated to be effective in the recent self-supervised
Transformer methods [29, 77]. For global [cls] tokens, we can simply maximize the similarity for intra-class im-ages. However, it is non-trivial and challenging to formulate the learning objectives for local [patch] tokens because we do not have ground-truth patch-level annotations. To address this problem, we propose to estimate the similarity between [patch] tokens across intra-class images using cross-attention, and enforce the alignment of correspond-ing [patch] tokens. Particularly, reconstructing masked
[patch] tokens across intra-class images increases the difﬁculty of model learning, thus encouraging learning gen-eralizable few-shot Transformer models by jointly exploit-ing the holistic knowledge of the image and the similarity of intra-class images.
As shown in Fig. 2, we compare our model with the existing self-supervised/supervised learning methods. Our model is a natural extension of the supervised contrastive learning method [33] and self-supervised knowledge dis-tillation methods [4, 77]. Thus our model inherits both the advantage of method [33] for effectively leveraging la-bel information, and the advantages of methods [4, 77] for not needing large batch size and negative samples. Mean-while, the newly-introduced challenging task of masked
Figure 2. Comparison of other self-supervised/supervised frameworks. Our method (d) is a natural extension of (b) and (c), with the newly-introduced challenging task of masked [patch] tokens reconstruction across intra-class images.
[patch] tokens reconstruction across intra-class images makes our method more powerful for learning generalizable few-shot Transformer models.
Compared with contemporary works on few-shot Trans-formers [14, 32, 70], our framework enjoys several good properties from a practical point of view. (1) Our method does not introduce any additional learnable parameters be-sides the ViT backbone and projection head, which makes it easy to be combined with other methods [32,70,74]. (2) Our method is both effective and training-efﬁcient, with stronger performance and less training time on four few-shot classi-ﬁcation benchmarks, compared with [32, 70]. In a nutshell, our main contributions can be summarized as follows:
• We propose a new supervised knowledge distillation framework (SMKD) that incorporates class label in-formation into self-distillation, thus ﬁlling the gap be-tween self-supervised knowledge distillation and tra-ditional supervised learning.
• Within the proposed framework, we design two supervised-contrastive losses on both class and patch levels, and introduce the challenging task of masked patch tokens reconstruction across intra-class images.
• Given its simple design, we test our SMKD on four few-shot datasets, and show that it achieves a new
SOTA on CIFAR-FS and FC100 by a large margin, as well as competitive performance on mini-ImageNet and tiered-ImageNet using the simple prototype clas-siﬁcation method for few-shot evaluation. 2.