Abstract
The transformer extends its success from the language to the vision domain. Because of the stacked self-attention and cross-attention blocks, the acceleration deployment of vi-sion transformer on GPU hardware is challenging and also rarely studied. This paper thoroughly designs a compres-sion scheme to maximally utilize the GPU-friendly 2:4 fine-grained structured sparsity and quantization. Specially, an original large model with dense weight parameters is first pruned into a sparse one by 2:4 structured pruning, which considers the GPU’s acceleration of 2:4 structured sparse pattern with FP16 data type, then the floating-point sparse model is further quantized into a fixed-point one by sparse-distillation-aware quantization aware training, which con-siders GPU can provide an extra speedup of 2:4 sparse cal-culation with integer tensors. A mixed-strategy knowledge distillation is used during the pruning and quantization pro-cess. The proposed compression scheme is flexible to sup-port supervised and unsupervised learning styles. Exper-iment results show GPUSQ-ViT scheme achieves state-of-the-art compression by reducing vision transformer models 6.4-12.7× on model size and 30.3-62× on FLOPs with neg-ligible accuracy degradation on ImageNet classification,
COCO detection and ADE20K segmentation benchmarking tasks. Moreover, GPUSQ-ViT can boost actual deployment performance by 1.39-1.79× and 3.22-3.43× of latency and throughput on A100 GPU, and 1.57-1.69× and 2.11-2.51× improvement of latency and throughput on AGX Orin. 1.

Introduction
Transformer-based neural models [48] have garnered im-mense interest recently due to their effectiveness and gen-eralization across various applications. Equipped with the attention mechanism [52] as the core of its architecture, transformer-based models specialize in handling long-range dependencies, which are also good at extracting non-local
*Tao Chen and Zhongxue Gan are corresponding authors. features [9] [5] in the computer vision domain. With com-parable and even superior accuracy than the traditional con-volution neural networks (CNN) [12] [49], more vision transformer models are invented and gradually replace the
CNN with state-of-the-art performance on image classifi-cation [27] [26], object detection [70] [59], and segmenta-tion [58] [68] tasks. Due to the vision transformer models having a generally weaker local visual inductive bias [9] in-herent in CNN counterparts, many transformer blocks are stacked for compensation. Moreover, the attention module in the transformer block contains several matrix-to-matrix calculations between key, query, and value parts [52]. Such designs give the naive vision transformers more parameters and higher memory and computational resource require-ments, causing high latency and energy consuming during the inference stage. It is challenging for actual acceleration deployment in GPU hardware.
Model compression techniques to transfer the large-scale vision transformer models to a lightweight version can bring benefits to more efficient computation with less on-device memory and energy consumption. There are some previous studies to inherit CNN compression methods, in-cluding pruning [43] [15], quantization [28] [23], distilla-tion [61], and architecture search [6] on vision transformers.
However, there are some drawbacks in previous studies:
• Most of these common methods aim to reduce the theoretical model size and Floating Point Operations (FLOPs). But it has been proved [33] [37] that smaller model sizes and FLOPs are not directly proportional to better efficiency on deployed hardware.
• The compression patterns do not match hardware char-acteristics. For example, pruned [43] or searched [6] vision transformer models have the unstructured sparse pattern in weight parameters, i.e., the distribution of non-zero elements is random. So deployed hardware can not provide actual speedup due to lacking the char-acteristics support for unstructured sparsity [35].
• How to keep the accuracy to the best with multiple compression methods and how to generalize on mul-tiple vision tasks lack systematical investigation.
for the final compressed model’s accuracy. We demonstrate that GPUSQ-ViT can generally apply to vision transformer models and benchmarking tasks, with state-of-the-art the-oretical metrics on model size and FLOPs. Moreover, as
GPUSQ-ViT compresses with GPU-friendly patterns, the compressed models can achieve state-of-the-art deployment efficacy on GPU platforms. Our main contributions include:
• Unlike previous compression methods only aiming at reducing theoretical metrics, we propose GPUSQ-ViT from the perspective of GPU-friendly 2:4 sparse pat-tern with low-precision quantization for the first time, achieving GPU acceleration of 4 times than prior arts.
• GPUSQ-ViT combines feature-based KD with sparse pruning and QAT, which can best compensate for sparse and quantized models’ accuracy.
• GPUSQ-ViT can apply to various vision transformer models and benchmarking tasks, with proven state-of-the-art efficacy on model size, FLOPs, and actual de-ployment performance on multiple GPUs. Moreover,
GPUSQ-ViT can work without ground truth label an-notations in an unsupervised learning style. 2.