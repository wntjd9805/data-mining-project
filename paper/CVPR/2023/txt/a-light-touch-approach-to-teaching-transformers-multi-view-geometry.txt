Abstract
Transformers are powerful visual learners, in large part due to their conspicuous lack of manually-specified pri-ors. This flexibility can be problematic in tasks that in-volve multiple-view geometry, due to the near-infinite possi-ble variations in 3D shapes and viewpoints (requiring flexi-bility), and the precise nature of projective geometry (obey-ing rigid laws). To resolve this conundrum, we propose a “light touch” approach, guiding visual Transformers to learn multiple-view geometry but allowing them to break free when needed. We achieve this by using epipolar lines to guide the Transformer’s cross-attention maps during train-ing, penalizing attention values outside the epipolar lines and encouraging higher attention along these lines since they contain geometrically plausible matches. Unlike pre-vious methods, our proposal does not require any camera pose information at test-time. We focus on pose-invariant object instance retrieval, where standard Transformer net-works struggle, due to the large differences in viewpoint between query and retrieved images. Experimentally, our method outperforms state-of-the-art approaches at object retrieval, without needing pose information at test-time. 1.

Introduction
Recent advances in computer vision have been charac-terized by using increasingly generic models fitted with large amounts of data, with attention-based models (e.g.
Transformers) at one extreme [12, 13, 20, 24, 34, 41]. There are many such recent examples, where shedding priors in favour of learning from more data has proven to be a suc-cessful strategy, from image classification [1, 13, 20, 29, 90], action recognition [7, 23, 27, 50, 58], to text-image match-ing [36, 45, 62, 71] and 3D recognition [40, 91]. One area where this strategy has proven more difficult to apply is solving tasks that involve reasoning about multiple-view ge-ometry, such as object retrieval – i.e. finding all instances of an object in a database given a single query image. This has applications in image search [37, 39, 51, 82, 92], including
Figure 1. Top-4 retrieved images with (1) global retrieval (left column), (2) Reranking Transformer (RRT) [74] (middle), and (3)
RRT trained with our proposed Epipolar Loss (right column). Cor-rect retrievals are green, incorrect ones are red. The Epipolar Loss imbues RRT with an implicit geometric understanding, allowing it to match images from extremely diverse viewpoints. identifying landmarks from images [53, 61, 85], recogniz-ing artworks in images [80], retrieving relevant product im-ages in e-commerce databases [14,55] or retrieving specific objects from a scene [3, 38, 46, 60].
The main challenges in object retrieval include over-coming variations in viewpoint and scale. The difficulty in viewpoint-invariant object retrieval can be partially ex-plained by the fact that it requires disambiguating similar objects by small differences in their unique details, which can have a smaller impact on an image than a large varia-tion in viewpoint. For this reason, several works have em-phasized geometric priors in deep networks that deal with multiple-view geometry [22,88]. It is natural to ask whether these priors are too restrictive, and harm a network’s ability to model the data when it deviates from the geometric as-sumptions. As a step in this direction, we explore how to
“guide” attention-based networks with soft guardrails that encourage them to respect multi-view geometry, without constraining them with any rigid mechanism to do so.
In this work, we focus on post-retrieval reranking meth-ods, wherein an initial ranking is obtained using global (image-level) representations and then local (region- or patch-level) representations are used to rerank the top-ranked images either with the classic Geometric Verifica-tion [57], or by directly predicting similarity scores of im-age pairs using a trained deep network [31, 74]. Rerank-ing can be easily combined with any other retrieval method while significantly boosting the precision of the underly-ing retrieval algorithm. Recently, PatchNetVLAD [31],
DELG [11], and Reranking Transformers [74] have shown that learned reranking can achieve state-of-the-art perfor-mance on object retrieval. We show that the performance of such reranking methods can be further improved by implic-itly inducing geometric knowledge, specifically the epipo-lar relations between two images arising from relative pose, into the underlying image similarity computation.
This raises the question of whether multiple view rela-tions should be incorporated into the two view architec-ture explicitly rather than implicitly.
In the explicit case, the epipolar relations between the two images are supplied as inputs. For example, this is the approach taken in the
Epipolar Transformers architecture [33] where candidate correspondences are explicitly sampled along the epipolar line, and in [88] where pixels are tagged with their epipolar planes using a Perceiver IO architecture [34]. The disadvan-tage of the explicit approach is that epipolar geometry must be supplied at inference time, requiring a separate process for its computation, and being problematic when images are not of the same object (as the epipolar geometry is then not defined). In contrast, in the implicit approach the epipolar geometry is only required at training time and is applied as a loss to encourage the model to learn to (implicitly) take ad-vantage of epipolar constraints when determining a match.
We bring the following three contributions in this work:
First, we propose a simple but effective Epipolar Loss to in-duce epipolar constraints into the cross-attention layer(s) of transformer-based reranking models. We only need the rel-ative pose (or epipolar geometry) information during train-ing to provide the epipolar constraint. Once trained, the reranking model develops an implicit understanding of the relative geometry between any given image pair and can ef-fectively match images containing an object instance from very diverse viewpoints without any additional input. Sec-ond, we set up an object retrieval benchmark on top of the
CO3Dv2 [63] dataset which contains ground-truth camera poses and provide a comprehensive evaluation of the pro-posed method, including a comparison between implicit and explicit incorporation of epipolar constraints. The bench-mark configuration is detailed in Sec. 4. Third, we evalu-ate on the Stanford Online Products [55] dataset using both zero-shot and fine-tuning, outperforming previous methods on this standard object instance retrieval benchmark. 2.