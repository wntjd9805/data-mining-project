Abstract
The objective of this paper is self-supervised learning of video object segmentation. We develop a unified framework which simultaneously models cross-frame dense correspon-dence for locally discriminative feature learning and embeds object-level context for target-mask decoding. As a result, it is able to directly learn to perform mask-guided sequential segmentation from unlabeled videos, in contrast to previous efforts usually relying on an oblique solution — cheaply
“copying” labels according to pixel-wise correlations. Con-cretely, our algorithm alternates between i) clustering video pixels for creating pseudo segmentation labels ex nihilo; and ii) utilizing the pseudo labels to learn mask encoding and de-coding for VOS. Unsupervised correspondence learning is further incorporated into this self-taught, mask embedding scheme, so as to ensure the generic nature of the learnt repre-sentation and avoid cluster degeneracy. Our algorithm sets state-of-the-arts on two standard benchmarks (i.e., DAVIS17 and YouTube-VOS), narrowing the gap between self- and fully-supervised VOS, in terms of both performance and net-work architecture design. 1.

Introduction
In this article, we focus on a classic computer vision task: accurately segmenting desired object(s) in a video sequence, where the target object(s) are defined by pixel-wise mask(s) in the first frame. This task is referred as (one-shot) video ob-ject segmentation (VOS) or mask propagation [1], playing a vital role in video editing and self-driving. Prevalent solu-tions [2–25] are built upon fully supervised learning techni-ques, costing intensive labeling efforts. In contrast, we aim to learn VOS from unlabeled videos — self-supervised VOS.
Due to the absence of mask annotation during training, existing studies typically degrade such self-supervised yet mask-guided segmentation task as a combo of unsupervised correspondence learning and correspondence based, non-*Work done during an internship at Baidu VIS.
†Corresponding author: Wenguan Wang.
ResNet-18
ResNet-50
F
&
J 75 71 67 63 59 80 70
F
&
J 60 50
Ours
LIIR[31]
VFS[39]
CRW[34]
MAST[28]
ConCor[41]
UVC[35] 55 40 20 60 30 80 70
[28]
M AST
ConCor[41]
Ours
LIIR[31]
[39]
VFS 40 50 frame number
U V C[35]
Figure 1. (a) Correspondence learning based self-supervised VOS, where mask tracking is simply degraded as correspondence match-ing mask warping. (b) We achieve self-supervised VOS by jointly learning mask embedding and correspondence matching. Our algo-rithm explicitly embeds masks for target object modeling, hence enabling mask-guided segmentation. (c) Performance comparison and (d) Performance over time, reported on DAVIS17 [42] val. learnable mask warping (cf. Fig. 1(a)). They first learn pixel-/patch-wise matching (i.e., cross-frame correspondence) by exploring the inherent continuity in raw videos as free super-visory signals, in the form of i) a photometric reconstruc-tion problem where each pixel in a target frame is desired to be recovered by copying relevant pixels in reference frame(s)
[26–31]; ii) a cycle-consistency task that enforces matching of pixels/patches after forward-backward tracking [32–36]; and iii) a contrastive matching scheme that contrasts confi-dent correspondences against unreliable ones [37–40]. Once trained, the dense matching model is used to approach VOS in a cheap way (Fig.1(a)): the label of a query pixel/patch is simply borrowed from previously segmented ones, accord-ing to their appearance similarity (correspondence score).
Though straightforward, these correspondence based “ex-pedient” solutions come with two severe limitations: First,
they learn to match pixels instead of customizing VOS tar-get – mask-guided segmentation, leaving a significant gap between the training goal and task/inference setup. During training, the model is optimized purely to discovery reliable, target-agnostic visual correlations, with no sense of object-mask information. Spontaneously, during testing/inference, the model struggles in employing first-/prior-frame masks to guide the prediction of succeeding frames. Second, from the view of mask-tracking, existing self-supervised solutions, in essence, adopt an obsolete, matching-/flow-based mask pro-pagation strategy [43–47]. As discussed even before the deep learning era [48–50], such a strategy is sub-optimal. Specifi-cally, without modeling the target objects, flow-based mask warping is sensitive to outliers, resulting in error accumula-tion over time [1]. Subject to the primitive matching-and-copy mechanism, even trivial errors are hard to be corrected, and often lead to much worse results caused by drifts or occlu-sions. This is also why current top-leading fully supervised
VOS solutions [4, 5, 10–22] largely follow a mask embedding learning philosophy — embedding frame-mask pairs, in-stead of only frame images, into the segmentation network.
With such explicit modeling of the target object, more ro-bust and accurate mask-tracking can be achieved [1, 51].
Motivated by the aforementioned discussions, we inte-grate mask embedding learning and dense correspondence modeling into a compact, end-to-end framework for self-supervised VOS (cf. Fig. 1(b)). This allows us to inject the mask-tracking nature of the task into the very heart of our algorithm and model training. However, bringing the idea of mask embedding into self-supervised VOS is not trivial, due to the lack of mask annotation. We therefore achieve mask embedding learning in a self-taught manner. Concretely, our model is trained by alternating between i) space-time pixel clustering, and ii) mask-embedded segmentation learning.
Pixel clustering is to automatically discover spatiotempo-rally coherent object(-like) regions from raw videos. By uti-lizing such pixel-level video partitions as pseudo ground-truths of target objects, our model can learn how to extract target-specific context from frame-mask pairs, and how to leverage such high-level context to predict the next-frame mask. At the same time, such self-taught mask embedding scheme is consolidated by self-supervised dense correspon-dence learning. This allows our model to learn transferable, locally discriminative representations by making full use of the spatiotemporal coherence in natural videos, and prevent the degenerate solution of the deterministic clustering.
Our approach owns a few distinctive features: First, it has the ability of directly learning to conduct mask-guided se-quential segmentation; its training objective is completely aligned with the core nature of VOS. Second, by learning to embed object-masks into mask tracking, target-oriented context can be efficiently mined and explicitly leveraged for object modeling, rather than existing methods merely relying on local appearance correlations for label “copy-ing”. Hence our approach can reduce error accumulation (cf. Fig. 1(d)) and perform more robust when the latent cor-respondences are ambiguous, e.g., deformation, occlusion or one-to-many matches. Third, our mask embedding strategy endows our self-supervised framework with the potential of being empowered by more advanced VOS model designs developed in the fully-supervised learning setting.
Through embracing the powerful idea of mask embed-ding learning as well as inheriting the merits of correspon-dence learning, our approach favorably outperforms state-of-the-art competitors, i.e., 3.2%, 2.5%, and 2.2% mIoU gains on DAVIS17 [42] val, DAVIS17 test-dev and YouTube-VOS [52] val, respectively. In addition to narrowing the performance gap between self- and fully-supervised VOS, our approach establishes a tight coupling between them in the aspect of model design. We expect this work can foster the mutual collaboration between these two relevant fields. 2.