Abstract
We introduce ensembles of stochastic neural networks to approximate the Bayesian posterior, combining stochastic methods such as dropout with deep ensembles. The stochas-tic ensembles are formulated as families of distributions and trained to approximate the Bayesian posterior with varia-tional inference. We implement stochastic ensembles based on Monte Carlo dropout, DropConnect and a novel non-parametric version of dropout and evaluate them on a toy problem and CIFAR image classiﬁcation. For both tasks, we test the quality of the posteriors directly against Hamil-tonian Monte Carlo simulations. Our results show that stochastic ensembles provide more accurate posterior esti-mates than other popular baselines for Bayesian inference. 1.

Introduction
Bayesian neural networks provide a principled way of reasoning about model selection and assessment with pos-terior distributions of model parameters [6, 8, 35, 49]. Al-though the analytical Bayesian posteriors can answer ques-tions about model parameter uncertainty, the immense com-putational challenge for commonly used neural network ar-chitectures makes them practically infeasible1. Instead, we are forced to resign to approximation methods that makes a trade-off between posterior accuracy and computational complexity [27].
A prominent method to approximate the Bayesian pos-terior is deep ensembles [20, 31, 55], that can be shown to correspond to a variational inference approximation with a delta distribution family [24]. This method is implemented by simply training an ensemble of models and treating them as samples from the model posterior. In the variational in-ference formulation, this corresponds to approximating the posterior by sampling from many sets of maximum a poste-riori parameters. 1There are examples of closed-form solutions for small architectures [56].
To further reduce the computational effort in evaluat-ing the approximate posterior, stochastic methods such as
Monte Carlo dropout [47] and DropConnect [51] inference have also been used extensively [13, 14, 40]. They beneﬁt from computationally cheaper inference by virtue of sam-pling stochastically from a single model. Formulated as a variational approximation to the posterior, dropout samples from a family of parameter distributions where parameters can be randomly set to zero. Although this particular fam-ily of distributions might seem unnatural [11], it turns out that the stochastic property can help to ﬁnd more robust re-gions of the parameter space, a fact well-known from a long history of using dropout as a regularization method.
Recently, there has been great progress towards under-standing the analytical posteriors of larger neural networks by means of direct Markov Chain Monte Carlo sampling of the parameter posterior [26]. Through impressive com-putational efforts, the posteriors for models as large as
ResNet-20 have been sampled using Hamiltonian Monte
Carlo (HMC) simulations. The sampled posterior has been shown to provide more accurate predictions that are surpris-ingly sensitive to data distribution shift as compared to stan-dard maximum likelihood estimation training procedures.
These HMC computations have made it possible to compare approximation methods such as dropout inference and deep ensembles directly to the Bayesian posterior. Ensembling and stochastic methods such as dropout have been used suc-cessfully to ﬁnd posterior approximations in many settings, but without a Bayesian formulation that includes both en-sembling and stochastic methods it is difﬁcult to understand if and how the two approaches can complement each other.
Recent work have shown that uncertainty quantiﬁcation is subjective to neural network architectures, and that the accuracy of posterior approximations depend non-trivially on model architecture and dataset complexity [33]. To ﬁnd computationally efﬁcient methods that can accurately ap-proximate the Bayesian posterior, for different data domains and architectures, is therefore an important goal with practi-cal implications for applications that require accurate uncer-tainty quantiﬁcation to assess network predictions [1, 17].
In this paper we combine deep ensembles and stochastic regularization methods into stochastic ensembles of neu-ral networks. We formulate the stochastic ensemble con-struction within the Bayesian variational formalism, where multiple stochastic methods such as regular Monte Carlo dropout, DropConnect, and others are combined with en-sembling into one general variational ansatz. We then con-duct a series of tests using a simple toy model (synthetic data) and CIFAR (image classiﬁcation), where stochastic deep ensembles are found to provide more accurate poste-riors than MultiSWA [55] and regular deep ensembles in a number of settings. In particular, for CIFAR we use a neu-ral network architecture evaluated by Izmailov et al. [26] in their large-scale experiments, allowing us to make a direct comparison of the ensemble methods to the “ground truth”
HMC posterior. 2.