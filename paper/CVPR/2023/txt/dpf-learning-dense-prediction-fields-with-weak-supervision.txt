Abstract
Nowadays, many visual scene understanding problems are addressed by dense prediction networks. But pixel-wise dense annotations are very expensive (e.g., for scene pars-ing) or impossible (e.g., for intrinsic image decomposition), motivating us to leverage cheap point-level weak supervi-sion. However, existing pointly-supervised methods still use the same architecture designed for full supervision. In stark contrast to them, we propose a new paradigm that makes predictions for point coordinate queries, as inspired by the recent success of implicit representations, like distance or radiance fields. As such, the method is named as dense pre-diction fields (DPFs). DPFs generate expressive interme-diate features for continuous sub-pixel locations, thus al-lowing outputs of an arbitrary resolution. DPFs are nat-urally compatible with point-level supervision. We show-case the effectiveness of DPFs using two substantially dif-ferent tasks: high-level semantic parsing and low-level in-trinsic image decomposition. In these two cases, supervi-sion comes in the form of single-point semantic category and two-point relative reflectance, respectively. As bench-marked by three large-scale public datasets PASCALCon-text, ADE20K and IIW, DPFs set new state-of-the-art per-formance on all of them with significant margins. Code can be accessed at https://github.com/cxx226/DPF. 1.

Introduction
The field of visual scene understanding aims to recover various scene properties from input images, e.g., seman-tic labels [24], depth values [49] [66], edge existence [1] or action affordance [10]. Successful and comprehensive scene understanding is the cornerstone of various emerging artificial intelligence applications, like autonomous driving, intelligent robots or smart manufacturing. Albeit difficult, this field has seen great progress thanks to end-to-end dense prediction networks like DPT [48] and large-scale densely-labelled datasets like ADE20K [67]. If we can densely label
Figure 1. (a) Existing dense prediction formulation. (b) Our DPF formulation. (c) Semantic annotation for single points. (d) Pair-wise reflectance annotation between two points. every property that we care about, totally solving the visual scene understanding problem seems a matter of time.
However, dense annotations are usually too expensive or impossible to obtain. According to the Cityscapes pa-per [13], it takes 1.5 hours to generate a high-quality seman-tic annotation map for a single image. What’s worse, for the problem of decomposing an image into reflectance and shading 1, it’s impossible for humans to provide pixel-wise ground truth values. As such, the largest intrinsic image decomposition dataset IIW [7] is annotated in the form of pair-wise reflectance comparison between two points. An-notators are guided to judge whether the reflectance of one point is darker than that of another point or not.
Given the importance of dense prediction and the diffi-culty of obtaining dense annotations, we focus on learning with point-level weak supervision. Fig. 1-c shows an exam-ple of point-level semantic scene parsing annotation. The sole red point on the cat is annotated as cat, which is much more cheaper than delineating the cat’s contours. Fig. 1-d shows human judgement of relative reflectance annotation between every pair of two points. Since the floor has a con-stant reflectance, point pairs on the floor are annotated with 1Intrinsic image decomposition.
the equal label. Since the table has a darker reflectance than the floor, pairs between the table point and the floor point are annotated with the darker label.
How could we effectively learn dense prediction mod-els from these kinds of point-level weak supervision? To this end, existing pointly-supervised methods leverage un-labelled points using various techniques like online expan-sion [47], uncertainty mixture [64] [57] or edge guidance
[18]. But they all exploit conventional formulations shown in Fig. 1-a, by converting point-level supervision into dense ground truth maps with padded ignore values. By contrast, we seek alternative network architectures that are naturally compatible with point-level supervision. Specifically, we take inspiration from the success of neural implicit repre-sentations. DeepSDF [46] takes 3D coordinates as input and predicts signed distance values. NeRF [41] takes 5D coordinates as input and predicts radiance/transparency val-ues. Similarly, our method takes 2D coordinates as input and predicts semantic label or reflectance values, as shown in Fig. 1-b. An intriguing feature of this new scheme is that high-resolution images can be encoded as guidance in a nat-ural way, because this new continuous formulation allows outputs of arbitrarily large or small resolution. Borrowing names from the research community of distance or radiance fields, our method is called dense prediction fields (DPFs).
In order to show that DPF is a strong and generic method, we use two pointly-supervised tasks: semantic scene pars-ing and intrinsic image decomposition. These two tasks dif-fer in many aspects: (1) Scene parsing is a high-level cog-nitive understanding task while intrinsic decomposition is a low-level physical understanding task; (2) Scene parsing outputs discrete probability vectors while intrinsic decom-position outputs continuous reflectance/shading values; (3)
Scene parsing is annotated with single points while intrinsic decomposition is annotated with two-point pairs. Interest-ingly and surprisingly, our method achieves new state-of-the-art results on both of them, as benchmarked by three widely used datasets PASCALContext, ADE20K and IIW.
To summarize, the contributions of our work include:
• We propose a novel methodology for learning dense prediction models from point-level weak supervision, named DPF. DPF takes 2D coordinates as inputs and allows outputs of an arbitrary resolution.
• We set new state-of-the-art performance on PASCAL-Context and ADE20K datasets for scene parsing and
IIW dataset for intrinsic decomposition with point-level weak supervision. Codes are publicly available.
• With systematic ablations, visualization and analysis, we delve into the mechanism of DPF and reveal that its superior performance is credited to locally smooth embeddings and high-resolution guidance. 2.