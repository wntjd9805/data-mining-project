Abstract
Most existing text-video retrieval methods focus on cross-modal matching between the visual content of videos and textual query sentences. However, in real-world sce-narios, online videos are often accompanied by relevant text information such as titles, tags, and even subtitles, which can be utilized to match textual queries. This insight has motivated us to propose a novel approach to text-video retrieval, where we directly generate associated captions from videos using zero-shot video captioning with knowl-edge from web-scale pre-trained models (e.g., CLIP and
GPT-2). Given the generated captions, a natural ques-tion arises: what benefits do they bring to text-video re-trieval? To answer this, we introduce Cap4Video, a new framework that leverages captions in three ways: i) Input data: video-caption pairs can augment the training data. ii)
Intermediate feature interaction: we perform cross-modal feature interaction between the video and caption to pro-duce enhanced video representations. iii) Output score: the Query-Caption matching branch can complement the original Query-Video matching branch for text-video re-trieval. We conduct comprehensive ablation studies to demonstrate the effectiveness of our approach. Without any post-processing, Cap4Video achieves state-of-the-art per-formance on four standard text-video retrieval benchmarks:
MSR-VTT (51.4%), VATEX (66.6%), MSVD (51.8%), and
DiDeMo (52.0%). The code is available at https:// github.com/whwu95/Cap4Video. 1.

Introduction
Text-video retrieval is a fundamental task in video-language learning. With the rapid advancements in image-language pre-training [15, 30, 46, 47], researchers have fo-cused on expanding pre-trained image-language models, es-pecially CLIP [30], to tackle the text-video retrieval task.
The research path has evolved from the most direct global
*Equal contribution.
Figure 1. (a) An existing end-to-end learning paradigm for text-video retrieval. (b) Zero-shot video captioning achieved by guid-ing a large language model (LLM) such as GPT-2 [31] with
CLIP [30]. (c) Our Cap4Video framework leverages the generated captions in three aspects: input data augmentation, intermediate feature interaction, and output score fusion. matching (i.e., video-sentence alignment [11, 24]) to fine-grained matching (e.g., frame-word alignment [36], video-word alignment [13], multi-hierarchical alignment [9, 28], etc.). These studies have demonstrated remarkable per-formance and significantly outperformed previous models.
Two key factors contribute to this improvement. Firstly,
CLIP offers powerful visual and textual representations that are pre-aligned in the semantic embedding space, thereby reducing the challenge of cross-modal learning in video-text matching. Secondly, these methods can fine-tune the pre-trained vision and text encoders using sparsely sampled frames in an end-to-end manner. All of these methods aim to learn cross-modal alignment between the visual represen-tation of videos and the textual representation of the corre-sponding query, as depicted in Figure 1(a).
However, in real-life scenarios, online videos usually come with related content such as the video’s title or tag on the video website. In addition to the visual signal in the video, the associated textual information can also be used to some extent to describe the video content and match the query (i.e., the common text-to-text retrieval). This raises a pertinent question: How can we generate associated text de-scriptions for videos? One possible solution is to crawl the video title from the video website. However, this method relies on annotations, and there is a risk that the video URL may have become invalid. Another automated solution is to generate captions using zero-shot video caption mod-els. Therefore, we turn our attention to knowledge-rich pre-trained models to handle such challenging open-set scenar-ios. We find that the recent study ZeroCap [34] provides a good practice to use frozen CLIP [30] and GPT-2 [31] for zero-shot image captioning. Thus, we leverage a video ex-tension [33] of ZeroCap for generating captions in the video domain without any further training.
When provided with auxiliary captions, a natural ques-tion naturally arises: How can we leverage these captions to enhance the text-video retrieval task? In this paper, we propose the Cap4Video learning framework, as illustrated in Figure 1(c), which utilizes captions in three key ways: (i)
Input Data: One simple approach is to augment the train-ing data with the generated captions. Specifically, the given video and its generated caption can be treated as a matched pair, which serves as an additional positive sample pair for (ii) Intermediate training beyond the query-video pairs.
Feature Interaction: Cross-modal interaction between the video and captions can be leveraged to improve the video representation. Specifically, we can exploit the complemen-tary information between videos and captions to reduce re-dundant features from videos and learn more discrimina-tive video representations. (iii) Output score: The gener-ated caption can also represent the video’s content, allowing us to employ query-caption matching to complement stan-dard query-video matching for the text-video retrieval task.
Moreover, a two-stream architecture can be utilized to re-duce model bias and produce more robust results.
We hope that our novel paradigm will encourage further investigation into the video-language learning. In summary, our contributions are as follows:
• We explore a novel problem: leveraging auxiliary cap-tions to further enhance existing text-video retrieval.
Besides labor-intensive manual crawling of video web-site titles, we investigate the potential of rich cap-tions automatically generated by large language mod-els (LLMs) to benefit text-video retrieval. which maximizes the utility of the auxiliary captions through three aspects: input data, feature interaction, and output score. Our framework improves the perfor-mance of existing query-video matching mechanisms, including global matching and fine-grained matching.
• Extensive experiments conducted on four video bench-marks demonstrate the effectiveness of our method.
Our Cap4Video achieves state-of-the-art performance on MSR-VTT [44] (51.4%), VATEX [38] (66.6%),
MSVD [43] (51.8%), and DiDeMo [1] (52.0%). 2. Methodology 2.1.