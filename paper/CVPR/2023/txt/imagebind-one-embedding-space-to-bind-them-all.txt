Abstract 1.

Introduction
We present IMAGEBIND, an approach to learn a joint embedding across six different modalities - images, text, au-dio, depth, thermal, and IMU data. We show that all combi-nations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. IMAGEBIND can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natu-ral pairing with images. It enables novel emergent applica-tions ‘out-of-the-box’ including cross-modal retrieval, com-posing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modal-ities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that IMAGEBIND serves as a new way to evaluate vision models for visual and non-visual tasks.
∗Equal technical contribution.
A single image can bind together many experiences – an image of a beach can remind us of the sound of waves, the texture of the sand, a breeze, or even inspire a poem. This
‘binding’ property of images offers many sources of super-vision to learn visual features, by aligning them with any of the sensory experiences associated with images. Ideally, for a single joint embedding space, visual features should be learned by aligning to all of these sensors. However, this requires acquiring all types and combinations of paired data with the same set of images, which is infeasible.
Recently, many methods learn image features aligned with text [1, 30, 45, 59, 63, 80, 81], audio [3, 4, 49, 54, 55, 68] etc. These methods use a single pair of modali-ties or, at best, a few visual modalities. However, the fi-nal embeddings are limited to the pairs of modalities used for training. Thus, video-audio embeddings cannot directly be used for image-text tasks and vice versa. A major ob-stacle in learning a true joint embedding is the absence of large quantities of multimodal data where all modalities are present together.
In this paper, we present IMAGEBIND, which learns a single shared representation space by leveraging multiple types of image-paired data. It does not need datasets where all modalities co-occur with each other. Instead, we lever-age the binding property of images and we show that just aligning each modality’s embedding to image embeddings leads to an emergent alignment across all of the modalities.
In practice, IMAGEBIND leverages web-scale (image, text) paired data and combines it with naturally occurring paired data such as (video, audio), (image, depth) etc. to learn a single joint embedding space. This allows IMAGEBIND to implicitly align the text embeddings to other modalities such as audio, depth etc., enabling zero-shot recognition ca-pabilities on that modality without explicit semantic or tex-tual pairing. Moreover, we show that it can be initialized with large-scale vision-language models such as CLIP [59], thereby leveraging the rich image and text representations of these models. Thus, IMAGEBIND can be applied to a variety of different modalities and tasks with little training.
We use large-scale image-text paired data along with nat-urally paired ‘self-supervised’ data across four new modal-ities - audio, depth, thermal, and Inertial Measurement Unit (IMU) readings – and show strong emergent zero-shot clas-sification and retrieval performance on tasks for each of these modalities. These emergent properties improve as the underlying image representation is made stronger. On au-dio classification and retrieval benchmarks, IMAGEBIND’s emergent zero-shot classification matches or outperforms specialist models trained with direct audio-text supervision on benchmarks like ESC, Clotho, AudioCaps. IMAGEBIND representations also outperform specialist supervised mod-els on few-shot evaluation benchmarks. Finally, we show that IMAGEBIND’s joint embeddings can be used for a wide variety of compositional tasks as illustrated in Figure 1, in-cluding cross-modal retrieval, combining embeddings via arithmetic, detecting audio sources in images, and generat-ing images given audio input. 2.