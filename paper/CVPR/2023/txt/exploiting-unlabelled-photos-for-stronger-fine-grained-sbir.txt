Abstract
This paper advances the fine-grained sketch-based im-age retrieval (FG-SBIR) literature by putting forward a strong baseline that overshoots prior state-of-the-arts by
≈11%. This is not via complicated design though, but by addressing two critical issues facing the community (i) the gold standard triplet loss does not enforce holistic latent space geometry, and (ii) there are never enough sketches to train a high accuracy model. For the former, we propose a simple modification to the standard triplet loss, that explic-itly enforces separation amongst photos/sketch instances.
For the latter, we put forward a novel knowledge distilla-tion module can leverage photo data for model training.
Both modules are then plugged into a novel plug-n-playable training paradigm that allows for more stable training.
More specifically, for (i) we employ an intra-modal triplet loss amongst sketches to bring sketches of the same in-stance closer from others, and one more amongst photos to push away different photo instances while bringing closer a structurally augmented version of the same photo (offering a gain of ≈4-6%). To tackle (ii), we first pre-train a teacher on the large set of unlabelled photos over the aforemen-tioned intra-modal photo triplet loss. Then we distill the contextual similarity present amongst the instances in the teacher’s embedding space to that in the student’s embed-ding space, by matching the distribution over inter-feature distances of respective samples in both embedding spaces (delivering a further gain of ≈4-5%). Apart from outper-forming prior arts significantly, our model also yields satis-factory results on generalising to new classes. Project page: https:// aneeshan95.github.io/ Sketch PVT/ 1.

Introduction
Sketch [7, 17, 39] has long established itself as a worthy query modality that is complementary to text [16, 23, 26].
Conceived as a category-level retrieval task [10,18], sketch-based image retrieval (SBIR) has recently taken a turn to a
“fine-grained” setting (i.e., FG-SBIR), where the emphasis
*Interned with SketchX
Figure 1. (left) A strong FG-SBIR baseline with strong PVT [74] backbone trained on intra-modal triplet loss, stabilised by EMA. (right) It additionally leverages unlabelled photos to enrich its la-tent space by distilling instance-wise discriminative knowledge of a teacher pre-trained on unlabelled photos. is on fully utilising the faithful nature of sketches to conduct instance-level retrieval [70, 81].
Despite great strides made in the field, without excep-tion, all existing FG-SBIR models [60, 82] work around a cross-modal triplet objective [81] to learn a discriminative embedding to conduct retrieval. The general intuition has always been to make a sketch sit closer to its paired photo while pushing away non-matching ones (Fig. 1). However, a conventional triplet setup does not enforce sufficient sepa-ration amongst different photos or sketch instances – largely because the conventional objective fails to retain the holistic latent space geometry, being overly restrictive on learning within-triplet feature separation.
For that and as our first contribution, we utilise an intra-modal triplet objective in both modalities, in addition to the regular cross-modal [81] triplet objective. For sketch, a query-sketch (anchor), is brought closer to another sketch of the same target-photo (positive) while distancing it from a sketch of any non-matching photo (negative). For the photo modality, as there is only one uniquely paired photo (an-chor), we treat a morphologically augmented version of the photo as positive, and a non-matching one as negative. Im-portantly, we show that the best morphological operations are those that operate on visual attributes bearing relevance with information exclusive to sketches (e.g., shape) Fig. 1.
The other, perhaps more pressing problem, facing the
community is that of sketch data scarcity – there are just not enough sketches for the extra performance gain [5, 6].
Instead of going for the more explicit route of photo to pseudo sketch synthesis [4], we vote for the simple and more implicit route of leveraging unlabelled photos. For that, we adapt a knowledge distillation setup – we first train a model on unlabelled photos only via an intra-modal triplet loss, then distill its instance-wise discriminative knowledge to a FG-SBIR model. This apparently simple aim is how-ever non-trivial to train owing to our cross-modal setup as naively using standard knowledge distillation paradigms involving logit distillation [33] from teacher’s embedding space to the student’s would be infeasible. For a cross-modal problem like ours, we need to preserve the instance-wise separation amongst photos, as well as corresponding sketches. For that, we propose to preserve the contextual similarity between instances and their nearest neighbours, modelled as a distribution over pairwise distances, from the pre-trained teacher (photo model), to that of the student (FG-SBIR). Inspired from recent literature, we introduce a novel distillation token into our PVT-backboned student, that is dedicated towards distilling contextual similarity.
However fitting this token into the existing PVT-architecture is non-trivial. Unlike other vision transformers like ViT [25], PVT employs a reshape operation [74], to re-shape the resultant individual patch tokens at one level back to the feature map, for input to the next level. It follows that adding a token here naively would break this operation.
We thus use our distillation token only during input to the transformer layer at each level [74], and set the modified token aside before the reshaping operation. A residual con-nection [70] thereafter connects this modified token as input to the transformer layer at the next level. Engaging the dis-tillation token at every level like ours, helps it imbibe the inductive bias modelled by the pyramidal structure of PVT, thus facilitating better distillation.
Finally, on the back of a pilot study (Sec. 3), we uncover a widespread problem with standard triplet loss training – that training is highly unstable, reflected in the highly os-cillating evaluation accuracies noted at every 100th training iteration. For that, we take inspiration from literature on stabilising GAN training [78] on Exponential Moving Av-erage [78] – a strategy that employs a moving average on model parameters iteratively, with a higher priority (mathe-matically exponential) to recent iterations over earlier ones.
To sum up: (i) We propose a strong baseline for FG-SBIR, that overshoots prior arts by ≈10% (ii) We achieve this by putting forward two simple designs each tackling a key problem facing the community: inadequate latent space separation, and sketch data scarcity. (iii) We introduce a simple modification to the standard triplet loss to explic-itly enforces separation amongst photos/sketch instances. (iv) We devise a knowledge distillation token in PVT [74] that facilitates better knowledge distillation in training from unlabelled data. Finally, apart from surpassing prior arts significantly, our model also shows encouraging results on generalising to new classes, without any paired sketches. 2.