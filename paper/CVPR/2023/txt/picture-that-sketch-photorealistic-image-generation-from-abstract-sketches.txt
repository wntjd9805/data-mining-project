Abstract Sketches
Subhadeep Koley1,2 Ayan Kumar Bhunia1 Aneeshan Sain1,2 Pinaki Nath Chowdhury1,2
Tao Xiang1,2 Yi-Zhe Song1,2 1SketchX, CVSSP, University of Surrey, United Kingdom. 2iFlyTek-Surrey Joint Research Centre on Artificial Intelligence.
{s.koley, a.bhunia, a.sain, p.chowdhury, t.xiang, y.song}@surrey.ac.uk
Figure 1. (a) Set of photos generated by the proposed method. (b) While existing methods can generate faithful photos from perfectly pixel-aligned edgemaps, they fall short drastically in case of highly deformed and sparse free-hand sketches. In contrast, our autoregressive sketch-to-photo generation model produces highly photorealistic outputs from highly abstract sketches.
Abstract 1.

Introduction
Given an abstract, deformed, ordinary sketch from un-trained amateurs like you and me, this paper turns it into a photorealistic image – just like those shown in Fig. 1(a), all non-cherry-picked. We differ significantly from prior art in that we do not dictate an edgemap-like sketch to start with, but aim to work with abstract free-hand human sketches. In doing so, we essentially democratise the sketch-to-photo pipeline, “picturing” a sketch regardless of how good you sketch. Our contribution at the outset is a de-coupled encoder-decoder training paradigm, where the de-coder is a StyleGAN trained on photos only. This impor-tantly ensures that generated results are always photoreal-istic. The rest is then all centred around how best to deal with the abstraction gap between sketch and photo. For that, we propose an autoregressive sketch mapper trained on sketch-photo pairs that maps a sketch to the StyleGAN latent space. We further introduce specific designs to tackle the abstract nature of human sketches, including a fine-grained discriminative loss on the back of a trained sketch-photo retrieval model, and a partial-aware sketch augmen-tation strategy. Finally, we showcase a few downstream tasks our generation model enables, amongst them is show-ing how fine-grained sketch-based image retrieval, a well-studied problem in the sketch community, can be reduced to an image (generated) to image retrieval task, surpass-ing state-of-the-arts. We put forward generated results in the supplementary for everyone to scrutinise. Project page: https:// subhadeepkoley.github.io/ PictureThatSketch
People sketch, some better than others. Given a shoe image like ones shown in Fig. 1(a), everyone can scribble a few lines to depict the photo, again mileage may vary – top left sketch arguably lesser than that at bottom left. The opposite, i.e., hallucinating a photo based on even a very ab-stract sketch, is however something humans are very good at having evolved on the task over millions of years. This seemingly easy task for humans, is exactly one that this pa-per attempts to tackle, and apparently does fairly well at – given an abstract sketch from untrained amateurs like us, our paper turns it into a photorealistic image (see Fig. 1).
This problem falls into the general image-to-image trans-lation literature [41, 64]. Indeed, some might recall prior arts (e.g., pix2pix [41], CycleGAN [105], MUNIT [38], Bi-cycleGAN [106]), and sketch-specific variants [33, 86] pri-marily based on pix2pix [41] claiming to have tackled the exact problem. We are strongly inspired by these works, but significantly differ on one key aspect – we aim to generate from abstract human sketches, not accurate photo edgemaps which are already “photorealistic”.
This is apparent in Fig. 1(b), where when edgemaps are used prior works can hallucinate high-quality photorealis-tic photos, whereas rather “peculiar” looking results are ob-tained when faced with amateur human sketches. This is be-cause all prior arts assume pixel-alignment during transla-tion – so your drawing skill (or lack of it), got accurately re-flected in the generated result. As a result, chance is you and me will not fetch far on existing systems if not art-trained to
sketch photorealistic edgemaps – we, in essence, democra-tise the sketch-to-photo generation technology, “picturing” a sketch regardless of how good you sketch.
Our key innovation comes after a pilot study where we discovered that the pixel-aligned artefact [69] in prior art is a direct result of the typical encoder-decoder [41] archi-tecture being trained end-to-end – this enforces the gener-ated results to strictly follow boundaries defined in the in-put sketch (edgemap). Our first contribution is therefore a decoupled encoder-decoder training, where the decoder is pre-trained StyleGAN [46] trained on photos only, and is frozen once trained. This importantly ensures generated re-sults are sampled from the StyleGAN [46] manifold there-fore of photorealistic quality.
The second, perhaps more important innovation lies with how we bridge the abstraction gap [20, 21, 36] between sketch and photo. For that, we propose to train an encoder that performs a mapping from abstract sketch representa-tion to the latent space of the learned latent space of Style-GAN [46] (i.e., not actual photos as per the norm). To train this encoder, we use ground-truth sketch-photo pairs, and impose a novel fine-grained discriminative loss between the input sketch and the generated photo, together with a con-ventional reconstruction loss [102] between the input sketch and the ground-truth photo, to ensure the accuracy of this mapping process. To double down on dealing with the ab-stract nature of sketches, we further propose a partial-aware augmentation strategy where we render partial versions of a full sketch and allocate latent vectors accordingly (the more partial the input, the lesser vectors assigned).
Our autoregressive generative model enjoys a few in-teresting properties once trained: (i) abstraction level (i.e., how well the fine-grained features in a sketch are reflected in the generated photo) can be easily controlled by alter-ing the number of latent vectors predicted and padding the rest with Gaussian noise, (ii) robustness towards noisy and partial sketches, thanks to our partial-aware sketch aug-mentation strategy, and (iii) good generalisation on input sketches across different abstraction levels (from edgemaps, to sketches across two datasets). We also briefly show-case two potential downstream tasks our generation model enables: fine-grained sketch-based image retrieval (FG-SBIR), and precise semantic editing. On the former, we show how FG-SBIR, a well-studied task in the sketch com-munity [10, 71–73], can be reduced to an image (generated) to image retrieval task, and that a simple nearest-neighbour model based on VGG-16 [78] features can already surpass state-of-the-art. On the latter, we demonstrate how pre-cise local editing can be done that is more fine-grained than those possible with text and attributes.
We evaluate using conventional metrics (FID, LPIPS), plus a new retrieval-informed metric to demonstrate supe-rior performance. But, as there is no better way to convince
Photorealistic the jury other than presenting all facts, we offer all gener-ated results in the supplementary for everyone to scrutinise. 2.