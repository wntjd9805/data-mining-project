Abstract
Cross-Domain Few-Shot Learning (CD-FSL) is a recently emerging task that tackles few-shot learning across different domains. It aims at transferring prior knowledge learned on the source dataset to novel target datasets. The CD-FSL task is especially challenged by the huge domain gap between different datasets. Critically, such a domain gap actually comes from the changes of visual styles, and wave-SAN [10] empirically shows that spanning the style distribution of the source data helps alleviate this issue. However, wave-SAN simply swaps styles of two images. Such a vanilla operation makes the generated styles “real” and “easy”, which still fall into the original set of the source styles. Thus, inspired by vanilla adversarial learning, a novel model-agnostic meta Style Adversarial training (StyleAdv) method together with a novel style adversarial attack method is proposed for CD-FSL. Particularly, our style attack method synthe-sizes both “virtual” and “hard” adversarial styles for model training. This is achieved by perturbing the original style with the signed style gradients. By continually attacking styles and forcing the model to recognize these challenging adversarial styles, our model is gradually robust to the vi-sual styles, thus boosting the generalization ability for novel target datasets. Besides the typical CNN-based backbone, we also employ our StyleAdv method on large-scale pre-trained vision transformer. Extensive experiments conducted on eight various target datasets show the effectiveness of our method. Whether built upon ResNet or ViT, we achieve the new state of the art for CD-FSL. Code is available at https://github.com/lovelyqian/StyleAdv-CDFSL. 1.

Introduction
This paper studies the task of Cross-Domain Few-Shot
Learning (CD-FSL) which addresses the Few-Shot Learn-ing (FSL) problem across different domains. As a gen-eral recipe for FSL, episode-based meta-learning strategy
* indicates corresponding author has also been adopted for training CD-FSL models, e.g.,
FWT [48], LRP [42], ATA [51], and wave-SAN [10]. Gener-ally, to mimic the low-sample regime in testing stage, meta learning samples episodes for training the model. Each episode contains a small labeled support set and an unla-beled query set. Models learn meta knowledge by predicting the categories of images contained in the query set according to the support set. The learned meta knowledge generalizes the models to novel target classes directly.
Empirically, we ﬁnd that the changes of visual appear-ances between source and target data is one of the key causes that leads to the domain gap in CD-FSL. Interestingly, wave-SAN [10], our former work, shows that the domain gap issue can be alleviated by augmenting the visual styles of source images. Particularly, wave-SAN proposes to augment the styles, in the form of Adaptive Instance Normalization (AdaIN) [22], by randomly sampling two source episodes and exchanging their styles. However, despite the efﬁcacy of wave-SAN, such a na¨ıve style generation method suffers from two limitations: 1) The swap operation makes the styles always be limited in the “real” style set of the source dataset; 2) The limited real styles further lead to the generated styles too “easy” to learn. Therefore, a natural question is whether we can synthesize “virtual” and “hard” styles for learning a more robust CD-FSL model? Formally, we use “real/virtual” to indicate whether the styles are originally presented in the set of source styles, and deﬁne “easy/hard” as whether the new styles make meta tasks more difﬁcult.
To that end, we draw inspiration from the adversarial training, and propose a novel meta Style Adversarial train-ing method (StyleAdv) for CD-FSL. StyleAdv plays the minimax game in two iterative optimization loops of meta-training. Particularly, the inner loop generates adversarial styles from the original source styles by adding perturba-tions. The synthesized adversarial styles are supposed to be more challenging for the current model to recognize, thus, in-creasing the loss. Whilst the outer loop optimizes the whole network by minimizing the losses of recognizing the images with both original and adversarial styles. Our ultimate goal is to enable learning a model that is robust to various styles, be-1
yond the relatively limited and simple styles from the source data. This can potentially improve the generalization ability on novel target domains with visual appearance shifts.
Formally, we introduce a novel style adversarial attack method to support the inner loop of StyleAdv. Inspired yet different from the previous attack methods [14, 34], our style attack method perturbs and synthesizes the styles rather than image pixels or features. Technically, we ﬁrst extract the style from the input feature map, and include the extracted style in the forward computation chain to obtain its gradient for each training step. After that, we synthesize the new style by adding a certain ratio of gradient to the original style. Styles synthesized by our style adversarial attack method have the good properties of “hard” and “virtual”.
Particularly, since we perturb styles in the opposite direction of the training gradients, our generation leads to the “hard” styles. Our attack method results in totally “virtual” styles that are quite different from the original source styles.
Critically, our style attack method makes progressive style synthesizing, with changing style perturbation ratios, which makes it signiﬁcantly different from vanilla adver-sarial attacking methods. Speciﬁcally, we propose a novel progressive style synthesizing strategy. The na¨ıve solution of directly plugging-in perturbations is to attack each block of the feature embedding module individually, which however, may results in large deviations of features from the high-level block. Thus, our strategy is to make the synthesizing signal of the current block be accumulated by adversarial styles from previous blocks. On the other hand, rather than attacking the models by ﬁxing the attacking ratio, we syn-thesize new styles by randomly sampling the perturbation ratio from a candidate pool. This facilitates the diversity of the synthesized adversarial styles. Experimental results have demonstrated the efﬁcacy of our method: 1) our style adversarial attack method does synthesize more challenging styles, thus, pushing the limits of the source visual distribu-tion; 2) our StyleAdv signiﬁcantly improves the base model and outperforms all other CD-FSL competitors.
We highlight our StyleAdv is model-agnostic and com-plementary to other existing FSL or CD-FSL models, e.g.,
GNN [12] and FWT [48]. More importantly, to beneﬁt from the large-scale pretrained models, e.g., DINO [2], we fur-ther explore adapting our StyleAdv to improve the Vision
Transformer (ViT) [5] backbone in a non-parametric way.
Experimentally, we show that StyleAdv not only improves
CNN-based FSL/CD-FSL methods, but also improves the large-scale pretrained ViT model.
Finally, we summarize our contributions. 1) A novel meta style adversarial training method, termed StyleAdv, is pro-posed for CD-FSL. By ﬁrst perturbing the original styles and then forcing the model to learn from such adversarial styles, StyleAdv improves the robustness of CD-FSL models. 2) We present a novel style attack method with the novel progressive synthesizing strategy in changing attacking ra-tios. Diverse “virtual” and “hard” styles thus are generated. 3) Our method is complementary to existing FSL and CD-FSL methods; and we validate our idea on both CNN-based and ViT-based backbones. 4) Extensive results on eight un-seen target datasets indicate that our StyleAdv outperforms previous CD-FSL methods, building a new SOTA result. 2.