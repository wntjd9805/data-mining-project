Abstract
Vision-and-language navigation (VLN) is the task to en-able an embodied agent to navigate to a remote location following the natural language instruction in real scenes.
Most of the previous approaches utilize the entire fea-tures or object-centric features to represent navigable can-didates. However, these representations are not efficient enough for an agent to perform actions to arrive the tar-get location. As knowledge provides crucial information which is complementary to visible content, in this paper, we propose a Knowledge Enhanced Reasoning Model (KERM) to leverage knowledge to improve agent navigation ability.
Specifically, we first retrieve facts (i.e., knowledge described by language descriptions) for the navigation views based on local regions from the constructed knowledge base. The re-trieved facts range from properties of a single object (e.g., color, shape) to relationships between objects (e.g., action, spatial position), providing crucial information for VLN. We further present the KERM which contains the purification, fact-aware interaction, and instruction-guided aggregation modules to integrate visual, history, instruction, and fact features. The proposed KERM can automatically select and gather crucial and relevant cues, obtaining more accurate action prediction. Experimental results on the REVERIE,
R2R, and SOON datasets demonstrate the effectiveness of the proposed method. The source code is available at https://github.com/XiangyangLi20/KERM . 1.

Introduction
Vision-and-language navigation (VLN) [3, 12, 23, 24, 36, 38] is one of the most attractive embodied AI tasks, where agents should be able to understand natural language in-structions, perceive visual content in dynamic 3D environ-ments, and perform actions to navigate to the target loca-tion. Most previous methods [9,15,22,31,34] depend on se-Figure 1. Illustration of knowledge related navigable candidates, which provides crucial information such as attributes and relation-ships between objects for VLN. Best viewed in color. quential models (e.g., LSTMs and Transformers) to contin-uously receive visual observations and align them with the instructions to predict actions at each step. More recently, transformer-based architectures [5, 7, 25] which make use of language instructions, current observations, and histori-cal information have been widely used.
Most of the previous approaches utilize the entire fea-tures [5, 12, 13, 25] or object-centric features [1, 7, 10, 20] to represent navigable candidates. For example, Qi et al. [22] and Gao et al. [10] encode discrete images within each panorama with detected objects. Moudgil et al. [20] utilize both object-level and scene-level features to represent visual observations. However, these representations are not effi-cient enough for an agent to navigate to the target location.
For example, as shown in Figure 1, there are three candi-dates. According to the instruction and the current location, candidate2 is the correct navigation. Based on the entire features of a candidate view, it is hard to select the correct one, as candidate2 and candidate3 belong to the same cat-egory (i.e., “dining room”). Meanwhile, it is also hard to differentiate them from individual objects, as “lamp” and
“light” are the common components for them.
As humans make inferences under their knowledge [11], it is important to incorporate knowledge related to navigable candidates for VLN tasks. First, knowledge provides cru-cial information which is complementary to visible content.
In addition to visual information, high-level abstraction of the objects and relationships contained by knowledge pro-vides essential information. Such information is indispens-able to align the visual objects in the view image with the concepts mentioned in the instruction. As shown in Fig-ure 1, with the knowledge related to candidate2 (i.e., <light hanging over kitchen island>), the agent is able to navigate to the target location. Second, the knowledge improves the generalization ability of the agent. As the alignment be-tween the instruction and the navigable candidate is learned in limited-seen environments, leveraging knowledge ben-efits the alignment in the unseen environment, as there is no specific regularity for target object arrangement. Third, knowledge increases the capability of VLN models. As rich conceptual information is injected into VLN models, the correlations among numerous concepts are learned. The learned correlations are able to benefit visual and language alignment, especially for tasks with high-level instructions.
In this work, we incorporate knowledge into the VLN task. To obtain knowledge for view images, facts (i.e., knowledge described by language descriptions) are re-trieved from the knowledge base constructed on the Vi-sual Genome dataset [16]. The retrieved facts by CLIP
[26] provide rich and complementary information for vi-sual view images. And then, a knowledge enhanced rea-soning model (KERM) which leverages knowledge for suf-ficient interaction and better alignment between vision and language information is proposed. Especially, the proposed
KERM consists of a purification module, a fact-aware inter-action module, and an instruction-guided aggregation mod-ule. The purification model aims to extract key informa-tion in the fact representations, the visual region representa-tions, and the historical representations respectively guided by the instruction. The fact-aware interaction module al-lows visual and historical representations to obtain the in-teraction of the facts with cross-attention encoders. And the instruction-guided aggregation module extracts the most relevant components of the visual and historical representa-tions according to the instruction for fusion.
We conduct the experiments on three VLN datasets, i.e., the REVERIE [24], SOON [36], and R2R [3]. Our ap-proach outperforms state-of-the-art methods on all splits of these datasets under most metrics. The further experimental analysis demonstrates the effectiveness of our method.
In summary, we make the following contributions:
• We incorporate region-centric knowledge to compre-hensively depict navigation views in VLN tasks. For the retrieved facts (i.e., each navigable candidate, knowledge described by language descriptions) are complementary to visible content.
• We propose the knowledge enhanced reasoning model (KERM) to inject fact features into the visual represen-tations of navigation views for better action prediction.
• We conduct extensive experiments to validate the ef-fectiveness of our method and show that it outperforms existing methods with a better generalization ability. 2.