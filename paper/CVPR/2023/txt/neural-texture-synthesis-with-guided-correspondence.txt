Abstract
Markov random ﬁelds (MRFs) are the cornerstone of classical approaches to example- based texture synthesis.
Yet, it is not fully valued in the deep learning era. This pa-per aims to re- promote the combination of MRFs and neural networks, i.e., the CNNMRF model, for texture synthesis, with two key observations made. We ﬁrst propose to com-pute the Guided Correspondence Distance in the nearest neighbor search, based on which a Guided Correspondence loss is deﬁned to measure the similarity of the output texture to the example. Experiments show that our approach sur-passes existing neural approaches in uncontrolled and con-trolled texture synthesis. More importantly, the Guided Cor-respondence loss can function as a general textural loss in, e.g., training generative networks for real- time controlled synthesis and inversion- based single- image editing. In con-trast, existing textural losses, such as the Sliced Wasserstein loss, cannot work on these challenging tasks. 1.

Introduction
Example-based texture synthesis has been a long-standing topic in vision and graphics. It aims to synthesize
*Corresponding author new textures of any resolution that retain the patterns of a given exemplar, with no apparent visual ﬂaws and having realism. Classical approaches formulate the synthesis as a
Markov Random Field (MRF) problem and solve it by iter-atively optimizing the output patches to be similar to their nearest neighbor in the input. This MRF-based optimization framework is not only widely used both in texture synthe-sis [18–20, 25, 39], but also adopted in more general tasks such as image synthesis and editing [1, 7].
Despite the success of MRF optimization, recent atten-tion has been devoted to utilizing deep neural networks, ei-ther matching the statistics of deep features [12,16] or train-ing generative adversarial networks (GANs) [5, 30, 33, 40].
In this paper, we retake the MRF optimization framework, given its versatility and ﬂexibility in texture synthesis, and combine it with deep neural networks. We ﬁrst search the nearest neighbor for each output patch according to Guided
Correspondence Distance over multi-layer deep features.
Then, unlike traditional methods that copy and paste source patches, we deﬁne a Guided Correspondence loss that mea-sures the overall similarity based on all the corresponding patches, and update the output pixels via back-propagation.
Actually, Li et al. [22] used to explore a CNNMRF model in 2016, which combines MRF and neural networks for style transfer. Champandard [6] applied it to texture syn-thesis later. Comparing to traditional texture optimization,
follow-up researches focus on such as PatchMatch acceler-ation [1, 7], self-guided optimization [18], and applications in various controlled scenarios [3, 24, 25, 29, 35, 39].
Deep learning based approaches. The ﬁrst work us-ing neural networks for texture synthesis is proposed by
Gatys et al. [12]. They synthesize new textures by match-ing the correlations between deep features to be similar to the source. Heitz et al. [16] also leverage deep features but choose to align them between images by minimizing the Sliced Wasserstein Distance, resulting in better visual quality. They assume a strong prior of stationary statis-tics for textures and emphasize matching the distribution in the feature space while “overlooking” the spatial coher-ence on the target image. When an exemplar has large-scale structures or non-stationary changes in the spatial domain, matching global statistics will cause discontinuities or inco-herent color distributions in the synthesized textures.
Another direction utilizing deep techniques is training generative networks [4,5,11,17,23,28,31,32,34]. Recently, several single-image GANs are proposed to learn the inter-nal patch distribution [30,33,40]. They can reasonably deal with challenging non-stationary textures at the cost of long-time training but always suffer from visual artifacts, espe-cially near the border areas. Another critical problem is that
GANs have no explicit measure about the synthesis quality with respect to the exemplar. We may not know when to stop training to prevent overﬁtting. 3. Method
We start by describing the classical MRF global objec-tive deﬁned in [20] that measures the overall similarity be-tween two images. The key idea behind MRF prior is to em-phasize the visual coherence across all overlapping output patches. Formally, let It denote the output/target texture to be optimized, and Is denote the input/source example. For controlled synthesis, there are also a certain target guidance map Gt and a corresponding source guidance map Gs. We represent each texture as a collection of overlapping patch samples: T =
S
|
{ are the patch numbers. Usually, we assume nt > ns, as we always synthesize textures larger than the exemplar. The global MRF energy that measures the similarity of It to Is can be written as: and ns = and S =
. nt = sj} ti}
T
{
|
|
|
EM RF (It, Is) = p(ti, N N (i)), (1)
T (cid:31)ti∈
,
·
·
∈
S is the nearest neighbor of ti, and where N N (i)
) is a similarity function, which is usually the sum of p( squared color distance in classical approaches. Classical ap-proaches solve the MRF objective above by an Expectation-Maximization (EM)-like algorithm which iterates between updating the output pixels (E-step) and ﬁnding the nearest neighborhoods in input (M-step). In our framework, we will
Figure 2. The synthesized textures from CNNMRF model [6] have obvious repetition and blurry issues. the main issue of CNNMRF is that its results have a poor patch diversity and severe blurry artifacts; see, e.g., Fig-ure 2. To address that, we made two critical changes in our approach. First, the Guided Correspondence Distance is de-ﬁned as a weighted sum of various penalty terms. Thus, in the nearest neighbor search, we can take more factors such as matching diversity into account rather than only con-sider patch similarity. Second, inspired by the Contextual loss [26] used for matching image statistics in style transfer, we modify the conventional L2-based MRF energy to ac-count for contextual similarities. The motivation is that we hope the nearest neighbor we found for a target patch is sig-niﬁcantly closer to it than all other source patches. The so-designed Guided Correspondence loss improves the sharp-ness of the synthesized results substantially. Our framework can be easily extended to various guided scenarios, includ-ing (but not limited to) user annotations, progression maps, and orientation ﬁelds. We just add the corresponding penal-ties to the Guided Correspondence Distance.
Experiments show that our approach performs remark-ably well for texture optimization both in uncontrolled and controlled scenarios, reaching state-of-the-art visual qual-ity. Moreover, the Guided Correspondence loss can be used as a general textural loss. We demonstrate its usage in, e.g., training feedforward networks for real-time controlled syn-thesis and inversion-based single-image editing. Existing statistic-based losses, such as the Sliced Wasserstein loss, cannot handle these challenging tasks. Code is available at https://github.com/EliotChenKJ/Guided-Correspondence- Loss. 2.