Abstract
Pruning—that is, setting a signiﬁcant subset of the pa-rameters of a neural network to zero—is one of the most popular methods of model compression. Yet, several recent works have raised the issue that pruning may induce or ex-acerbate bias in the output of the compressed model. De-spite existing evidence for this phenomenon, the relation-ship between neural network pruning and induced bias is not well-understood. In this work, we systematically inves-tigate and characterize this phenomenon in Convolutional
Neural Networks for computer vision. First, we show that it is in fact possible to obtain highly-sparse models, e.g. with less than 10% remaining weights, which do not de-crease in accuracy nor substantially increase in bias when compared to dense models. At the same time, we also
ﬁnd that, at higher sparsities, pruned models exhibit higher uncertainty in their outputs, as well as increased correla-tions, which we directly link to increased bias. We pro-pose easy-to-use criteria which, based only on the uncom-pressed model, establish whether bias will increase with pruning, and identify the samples most susceptible to bi-ased predictions post-compression. Our code can be found at https://github.com/IST-DASLab/pruned-vision-model-bias. 1.

Introduction
The concept of “bias” in machine learning models spans a range of considerations in terms of statistical, perfor-mance, and social metrics. Different deﬁnitions can lead to different relationships between bias and accuracy. For instance, if bias is deﬁned in terms of accuracy disparity between identity groups, then accuracy in the “stronger” group may have to be reduced in order to reduce model bias. Several sources of bias have been identiﬁed in this context. For example, bias in datasets commonly used to train machine learning models [4,5,53] can severely impact outputs, and may be difﬁcult or even impossible to correct during training. The choice of model architecture, training methods, evaluation, and deployment can create or exacer-bate bias [2, 42, 43].
One potential source of bias which is relatively less in-vestigated is the fact that machine learning models, and in particular deep neural networks, are often compressed for efﬁciency before being deployed. Seminal work by
Hooker et al. [29] and its follow-ups, e.g. [28, 38] pro-vided examples where model compression, and in particular pruning, can exacerbate bias by leading models to perform poorly on “unusual” data, which can frequently coincide with marginalized groups. Given the recent popularity of compression methods in deployment settings [13,18,19,27] and the fact that, for massive models, compression is often necessary to enable model deployment, these ﬁndings raise the question of whether the bias due to compression can be exactly characterized, and in particular whether bias is an inherent side-effect of the model compression process.
In this paper, we perform an in-depth analysis of bias in compressed vision models, providing new insights on this phenomenon, as well as a set of practical, effective crite-ria for identifying samples susceptible to biased predictions, which can be used to signiﬁcantly attenuate bias.
Our work starts from a common setting to study bias and bias mitigation [28, 29, 40, 50]: we study properties of sparse residual convolutional neural networks [25], in par-ticular ResNet18, applied for classiﬁcation on the CelebA dataset [41]. Then, we validate our ﬁndings across other
CNN architectures and other datasets. To study the impact of sparsity, we train highly accurate models with sparsity ranging from 80% to 99.5%, using the standard gradual magnitude pruning (GMP) approach [18, 21, 22, 55]. We consider bias in dense and sparse models from two perspec-tives: systematic bias, which refers to consistent errors in the model output, and category bias, which refers to viola-tions of fairness metrics associated with protected groups.
On the positive side, our analysis shows that the GMP approach can produce models that are highly sparse, i.e. 90-95% of pruned weights, without signiﬁcant increase in any bias-related metrics. Yet, this requires care: we show that shared, jointly-trained representations are signiﬁcantly less susceptible to bias, and so careful choices of training pro-cedure are needed for good results. On the other hand, at very high sparsities (95%-99.5%) we do observe non-trivial increase in category bias for the sparse models, for speciﬁc protected attributes. We perform an in-depth study of this
phenomenon, correlating increase in bias with increased un-certainty in the model outputs, induced by sparsity. Lever-aging insights from our analysis, we provide a simple set of criteria and techniques based on threshold calibration and overriding decisions for sensitive samples, which we show to have a signiﬁcant effect on bias reduction. The latter only use information found in the original dense model. 2. Methodology 2.1. Notions of Bias
We now deﬁne the notions of bias we will use in the rest of the paper. We emphasize these categories should not be seen as exclusive: instead, they allow us to study different aspects of the given phenomena.
Systematic Bias. A standard, broad meaning of bias is sys-tematic error [12]: for example, we can measure whether models are biased toward overconﬁdence in their predic-tions, or if they tend to generalize poorly to data from a shifted distribution. We call this Systematic Bias; a full list of the metrics we use is given in section 2.3.
Category Bias. A complementary approach to deﬁning bias centers around the notion of subgroup/category of sam-ples in the dataset. Here, bias refers to violations of group fairness metrics with respect to given categories [2] for in-stance by measuring differences in false positive, false neg-ative, or error rates across subgroups. Other related metrics are worst subgroup performance [47], or the standard devi-ation of accuracy across identity categories [40].
Inherent to these deﬁnitions is that the choice of at-tributes that deﬁne the subgroups must be meaningful in a sociological context and relevant to the model’s applica-tion. For example, it is appropriate to measure the accuracy difference with respect to race and gender in facial identiﬁ-cation software, since even a moderate difference in accu-racy can lead to discrimination in real-world settings. Mod-els that are highly-accurate on standard metrics, e.g. top-1 accuracy, may still be considered biased, for instance with respect to demographic parity. In order to distinguish the concept of bias from that of fairness, here we focus on al-gorithmic bias, which we deﬁne as cases in which a model ampliﬁes bias found in the training data. A classic example is when a model tends to have worse accuracy on samples from poorly-represented subgroups of the dataset. We call this type of bias Category Bias.
These notions are complementary: category biases are likely associated with systematic biases, and therefore, studying systematic bias can help us understand cases where models show socially-relevant category bias. This is a common assumption that is frequently used to study bias, for instance in the work on compression-identiﬁed ex-emplars of [28, 29], which ﬁrst identiﬁes a consistent set of examples on which compressed models frequently struggle, and then demonstrates that these are enriched for certain identity groups. Generally, we are also interested in under-standing the relationship between statistical notions of bias, examined via speciﬁc metrics, and potential systematic bias across protected categories. 2.2. Category Bias Metric: Bias Ampliﬁcation
Following prior work [29, 54], we consider datasets where samples are classiﬁed according to binary attributes, and use a subset of these as “identity” attributes. For this, we introduce as our main metric a variant of Bias Ampliﬁ-cation (BA) [54]. Intuitively, bias ampliﬁcation will mea-sure the extent to which correlations between identity cate-gories and predicted attributes in the training data are exag-gerated by the model. While positive correlation between an identity category and a predicted attribute can be reason-able (a model can predict that women wear earrings more frequently than men), models that amplify such input rela-tionships in their output may be stereotyping, by relying on identity markers as a proxy for other attributes.
To encode this formally, we compute bias ampliﬁcation.
We deﬁne the function N (
) to provide the count of the
· number of samples with a speciﬁc binary attribute value, e.g. Young = 1, over a given sample set. We then deﬁne the bias b of a binary attribute A with respect to a 0, 1 binary identity category I
N (A = 1, I = 1)
N (A = 1) 0, 1
} 2{
} b = 2{ as
, if the attribute and identity category are positively corre-lated in the training data, and b =
N (A = 1, I = 0)
N (A = 1)
, otherwise.
The bias ampliﬁcation is then the difference between the bias computed on the predicted attribute ˜A and the true value of the attribute A, computed on the test set:
BA =
N ( ˜A = 1, I = 1)
N ( ˜A = 1)
N (A = 1, I = 1)
N (A = 1)
,
  if the predicted attribute is positively correlated with the identity category, and
BA =
N ( ˜A = 1, I = 0)
N ( ˜A = 1)
N (A = 1, I = 0)
N (A = 1)
,
  if the predicted attribute is negatively correlated with the identity category. We do not compute the Bias Ampliﬁca-tion on any attribute that is not signiﬁcantly biased toward either value of the identity category, or if some combination of the predicted and protected attribute is very infrequent (e.g., occurring less than 10 times in the test data).
Discussion. This metric has several advantages. Firstly, it is clear that high BA values signal stereotyping by the model. Unlike the original BA metric of [54], our deﬁni-tion uses the label distribution in the test data as the true
baseline for the predicted label distribution of the model, allowing us to separate the effect of the model itself from the effect of the underlying data, and also allowing us to test the model for bias in settings where the test distribution does not closely resemble the training distribution.
Additionally, BA is not directly affected by other pos-sible biases in the model, such as a tendency to un-derpredict rare attributes. Moreover, unlike direct false-positive/negative analysis, BA directly takes into account predictions over both values of the protected attribute, and can be meaningfully aggregated across attributes. 2.3. Systematic Bias Metrics
We use several other ﬁne-grained metrics to measure the systematic bias of dense and sparse models.
Threshold Calibration Bias (TCB). On many datasets, the majority of attributes are not evenly split across samples: e.g., for CelebA, the average imbalance is 80%/20%. We measure the change (typically, decline) of the proportion of predictions into the less common value of the attribute using the default threshold. Note that values near 1 show minimal TCB, while values away from 1 in either direction show higher TCB.
T CB = (
N ( ˜A=1)
N (A=1) ,
N ( ˜A=0)
N (A=0) , if Mean(A) < 0.5 otherwise.
Uncertainty and Calibration. Attribute predictions after applying the sigmoid function range between 0 and 1. For a converged model, they tend to cluster around the extremes, with some smaller number of predictions falling nearer the center of the interval. We consider prediction values be-tween 0.1 and 0.9 to be uncertain. These uncertainty met-rics simply compute the proportion of predictions that fall into the uncertain interval. We then check if the uncer-tainty correctly estimates the proportion correct by buck-eting [8, 45]. The prediction range is split into ten equal-width buckets, and average per-bucket difference of the con-ﬁdence and the proportion correct. These are then weighted by the bucket size and aggregated. The weighted average difference of the accuracy and conﬁdence of the buckets is presented as the Expected Calibration Error (ECE).
Bm|
| 10 n=1 |
Bn|
Label Interrelation. Finally, we look at the strength of re-lationship between predicted labels on the various attribute.
Speciﬁcally, for each attribute A, we train a linear regres-sion using all other attributes as the features and A as the variable to be predicted; the coefﬁcient of determination (R2) of this model tells us the extent to which the model output for A can be predicted from the model outputs of the other attributes in a co-trained model. Note that this does not imply a causal relationship - we cannot say that conf(Bm) acc(Bm)
ECE = m=1
X
P
  10
|
|
. the model is using some of the attributes to predict oth-ers. Rather, a high interrelation suggests that the hidden feature layer is less expressive, forcing a closer relationship between linear classiﬁers using it as the features. 2.4. Evaluation Setup
In our primary study, we focus on
CelebA Setup.
ResNet18 [25] models that predict human-annotated binary attributes from cropped-and-centered photos of celebrities in the CelebA dataset [41].
CelebA attribute prediction is frequently used for bias measurement [28, 29, 40, 50]. This is in part due to its size and widespread availability. Yet, CelebA is an imperfect proxy for real-world human photographs, as it skews sub-stantially in both age and skin color, as well as make-up, hairstyles, and overall presentation of the human subjects.
As previous works have looked at both models that jointly co-train all or most CelebA attributes [40, 50] and models that train only a single attribute [29], we conduct both types of experiments. For the all-in-one/joint training, we train a
ResNet18 model with 40 logistic classiﬁers after the fully-connected layer. Additionally, we train models with a single head for 7 CelebA attributes: Blond, Smiling, Oval Face,
Big Nose, Mustache, Receding Hairline, Bags Under Eyes.
We validate our results by repeating our experiments on the ResNet50 and MobileNetV1 [30] architectures, as well as on structured sparsity (2:4, 1:4 and 1:8) sparsity pat-terns, which are better supported by current NVIDIA hard-ware [44]. We also validate some of our ﬁndings on the uncropped CelebA dataset, as well as on the iWildcam [3] and Animals with Attributes2 [51] datasets.
For CelebA, we use four attributes for computing Cate-gory Bias: “Male”, “Young”, “Chubby”, and “Pale Skin”1.
These attributes were chosen because they loosely corre-spond to categories traditionally used to measure bias and discrimination. Examples of these categories can be found in Appendix M. In the rest of the paper, we use “cate-gories” to refer to these four attributes when they are used as the group identiﬁer to compute BA, and “at-tribute” to refer to any CelebA attribute that is used as a prediction target.
For both ResNet and MobileNet
Model Architectures. models, we use the standard model architecture, with only one fully-connected layer and a logit transformation fol-lowing the convolutional blocks, and Binary Cross-Entropy loss. Unlike other studies using CelebA [50], we found that including an additional fully-connected layer did not improve accuracy. Nor did it increase accuracy to initial-ize with ImageNet weights as in [40, 50], and therefore all models were randomly initialized following [24]. Consis-1The choices to present gender as a binary attribute, and the speciﬁc words to describe the attributes were chosen by the creators of the CelebA dataset. We continue their use here to avoid confusion and enable compar-isons with other works.
tent with other work, we use the cropped-and-centered ver-sion of the dataset described in [41], and perform training data augmentations consistent with [50]. We also validate on the uncropped version. We report results after running each experiment from 5 random seeds.
Model Compression. We perform unstructured pruning, by gradually removing the lowest magnitude weights dur-ing training, known as Global Magnitude Pruning (GMP)
[18, 21, 22, 55]. GMP is a standard baseline, which, de-spite its simplicity, is competitive with more complex ap-proaches [17,18,34,35,49]. We prune all ResNet18 models to 80%, 90%, 95%, 98%, 99%, and 99.5% sparsity. Follow-ing earlier work [31], we considered two variants of GMP.
The main variant starts from a random initialization (RI), and gradually removes parameters after the tenth training epoch, while simultaneously training the model [55]; we refer to this setup as GMP-RI. The second variant starts from a pre-trained dense model, then gradually removes pa-rameters with the lowest global magnitude while continuing to ﬁnetune the model at a lower learning rate; this second variant will be referred to as GMP-PT. We train models us-ing SGD with momentum, with the exception of pre-trained (PT) pruning, for which we found Adam [33] to yield bet-ter results. We use the model state at the end of the epoch which reached highest performance on a held-out validation dataset. All the experiments presented are performed for
ResNet18 models under the GMP-RI setup; we provide ad-ditional validation for GMP-PT in Appendix E, which sup-ports our conclusions.
Our setup makes some complementary choices relative to prior work [28, 29]. Speciﬁcally, we prune weights by magnitude globally as opposed to per-layer. This will allow us to reach much higher sparsity levels relative to [28, 29] before model breakdown. Further, we chose relatively long model training times (100 epochs for 40-attribute dense and
GMP-RI models, 80 epochs for GMP-PT models, and 20 epochs for all single-attribute models), as this leads to both higher accuracy and lower bias metrics.
 
Accuracy Results. Using GMP and an extended training schedule, we are able to obtain sparse models that match or outperform the dense baseline, both in terms of accu-99%) sparsi-racy and ROC-AUC values, even at high ( ties, while providing substantial improvements in theoret-ical FLOPs (computed as in [14]), and practical inference speed on CPU when using the DeepSparse inference engine
[10]. We present our results for dense and sparse (GMP-RI) models trained to predict all 40 attributes in Table 1, which show that sparse models can outperform the dense one, even at high sparsities. This is also conﬁrmed by the more robust AUC metric, which is agnostic to the prediction threshold; at all sparsity levels, except for 99.5%, we can observe a slight improvement in AUC scores over the dense models. We observe a similar trend regarding the quality of sparse models over the dense baseline with single-attribute
Metric
Dense
Accuracy (%)
AUC (%)
Inf. FLOPs (B)
Inf. items/sec 90.4 80.5 0.2
± 3.64 130 80 90.8 0.2 81.0
± 1.40 138 90 91.0
Sparsity (%) 95 91.3 98 91.5 99 91.5 99.5 91.1 81.3 0.3 81.5 0.2 81.5 0.2 81.0 0.1 79.7 0.1
± 0.998 181
± 0.683 234
± 0.386 318
± 0.241 373
± 0.145 403
Table 1. Average Accuracy AUC, estimated inference FLOPs, and inference times on CPU (using the DeepSparse Engine [36]) for
ResNet18 models jointly trained on all 40 binary attributes. We report results after running each experiment from 5 random seeds.
For better readability, we present AUC scores as percentages. We omit variances for the accuracies, as they are all 0.1.
 training. This is in contrast to previous work [29], which observes a degradation of sparse models over dense even at 90% sparsity. We believe our improved results are due to the use of a better pruner (global over uniform layer-wise magnitude pruning), and improved training schedule. Nev-ertheless, they further motivate our study of properties of sparse models, beyond accuracy.
Additionally, we examined randomly-selected images in each category manually, to validate the quality of the human ratings and the images presented to the automated classiﬁer (see Appendix M for screenshots). We provide our tool, the example viewer, for the convenience of other researchers. 3. The Effects of Sparsity on Bias 3.1. Baseline: Analysis of Dense Models
Systematic Bias in Uncompressed Models. Examining bias in dense models, we ﬁnd that, when jointly-trained across all attributes, they tend to under-predict the less prevalent output value for each attribute, with an average
TCB of 0.9. Models trained on a single attribute have a worse under-prediction error than jointly-trained models at lower sparsities; for instance, predictions for Oval Face had a TCB of 0.84 when trained jointly with all other attributes, but 0.52 when trained singly. Additionally, dense models were overconﬁdent with respect to the prediction probabil-ity, with an average ECE of 0.054 for jointly-trained mod-els. Single-attribute dense models showed higher uncer-tainty (Figure 3 and Appendix C), despite having higher accuracy than jointly-trained models.
Category Bias in Uncompressed Models. Dense models exhibit non-trivial bias ampliﬁcation (BA), for both singly and jointly-trained attributes. The results show two trends.
The ﬁrst, shown in Figure 1 (left), is that BA is substan-tially higher with respect to speciﬁc categories: for instance, with respect to Male and Young, relative to Chubby and
Pale Skin. The attributes with highest BA value for dense joint training are Double Chin (Male, 0.053), Wavy Hair (Male, 0.047), Wearing Necktie (Young, 0.046), Pointy
Nose (Male, 0.045), Chubby (Male, 0.043), and Oval Face (Male, 0.042). (See Appendix J for a full table.) These at-tributes rank in the top ﬁve for several identity categories,
suggesting that they are prone to correlations.
The second trend is that single-attribute training shows a much higher BA than joint training. (See the bottom row of Figure 3, 0% sparsity.) For instance, BA with respect to ‘Male’ is about three times higher when training singly rather than jointly in the case of Oval Face and Big Nose (0.15 vs 0.04 and 0.11 vs 0.03). (Left) Bias Ampliﬁcation by category for dense
Figure 1.
ResNet18 CelebA models. (Right) Distribution of Worst-Case
Bias Ampliﬁcation across identity categories, for all attributes and sparsities, CelebA on ResNet18.
It appears that both compressed and uncom-Discussion. pressed models are still prone to bias ampliﬁcation. From the point of view of our analysis, the presence of bias in the dense model allows us to compare against sparse models.
It is tempting to as-Manual Review of Celeb-A Samples. cribe intuitive explanations to the above correlations. How-ever, examining the above attributes more closely, we ob-serve that they have low accuracy and high uncertainty val-ues. Inspecting randomly chosen images, we noticed that attributes such as Pointy Nose often appear difﬁcult to clas-sify, even for human raters. Others, such as Wearing Neck-tie, are often impossible to observe directly on the cropped version of the image typically used for this task2. Finally, an inspection of images shows that Wearing Lipstick appears difﬁcult to judge from the appearance of the mouth, without relying on indirect information, such as the person’s gen-der, or presence of other makeup. Thus, even though we do not detect large bias ampliﬁcation for this attribute, we consider this measurement unreliable. See Appendix M for examples from these categories. 3.2. The Effect of Sparsity on Systematic Bias
Figure 2 shows the effect of pruning CelebA models jointly-trained on all attributes on systematic bias, in the random initialization (RI) setup. First, notice that, as we increase model sparsity, accuracy stays largely unchanged.
Yet, other characteristics of the model change considerably.
Threshold Calibration Bias (TCB) worsens with sparsity for jointly trained models, with an ever-lower proportion of pre-dictions of the less popular value of each attribute. (Con-sider that the average TCB for dense models is 0.90, while for 99.5%-sparse models it is 0.81.) Uncertainty goes up 2Human raters were asked to assign labels using the uncropped version of the image.
Figure 2. Systematic Bias metrics (TCB, ECE, Interdependence) of ResNet18 models jointly trained on all CelebA attributes. The thick black line denotes the mean value at each sparsity level. In this and all boxplots, the horizontal line represents the median across all CelebA attributes, the edges of the box denote the 25th and 75th quartiles, and dots indicate all points more than 2.5 times the distance from the mean to the respective quartile. considerably for almost every attribute, roughly doubling from dense to 99.5%-sparse models.
Combining these two observations, we note that in our experiments, jointly-trained sparse models are better cali-brated than dense with an average ECE of 0.013 for jointly-trained 99.5% sparse models versus 0.054 for dense models. (Note that [8] observe similar behavior of ECE for Lottery
Tickets [16], at lower sparsity, and on different datasets.)
Finally, label interdependence increases with sparsity, from an average R2 of 0.31 to 0.36, suggesting that the more compact feature representation in sparse models results in greater entanglement between the features for every at-tribute.
For singly-trained models, uncertainty is largely un-changed as sparsity increases, perhaps due to already hav-ing high values in the dense model, relative to the jointly-trained model. In effect, jointly-trained models have lower uncertainty than singly-trained ones at lower sparsities, but roughly equal uncertainty at higher sparsities. (See Figure 3 and Appendix C for full data.) Threshold Calibration Bias conﬁrms this trend: TCB is roughly constant with sparsity for singly-trained models, but gets worse (decreases) for jointly-trained models. Thus, jointly-trained models are less miscalibrated at lower sparsities relative to singly-trained ones, but similarly miscalibrated at higher sparsities. 3.3. The Effect of Sparsity on Category Bias
Next, we focus on the effect of sparsity on bias ampliﬁca-tion. Here, the expectation is that, if sparse models exhibit more bias, for instance by picking up on spurious correla-tions, bias ampliﬁcation should increase. We ﬁrst examine this trend in Figure 1 (right), for jointly-trained models. We
Another observation is the high correlation between the evolution of uncertainty (second row in Figure 3), TCB (third row), and that of bias ampliﬁcation (fourth row), rel-ative to the sparsity increase. Speciﬁcally, the increase in output uncertainty is linked to stronger bias ampliﬁcation.
We further investigated whether co-training the identity category with the attribute of interest encourages more di-versity in the representation. In this case, we observed a very similar trend regarding BA as for singly-trained at-tributes, which indicates that the source of bias goes beyond the relationship between the two attributes. These results are shown in Appendix D. 3.4. Injecting Backdoor Features in Sparse Models
Figure 3. Effect of single versus joint training of attributes on accuracy (ﬁrst row), uncertainty (second row), Threshold Calibra-tion Bias (third row), and Bias Ampliﬁcation for the ‘Male’ at-tribute (fourth row), on the ResNet18 CelebA model, predicting
Oval Face (left) and Big Nose (right). observe that BA presents a slight increase w.r.t. sparsity between 90 and 95%, after which the increase is more pro-nounced. The values for BA at the highest sparsity levels are largely determined by the BA values of dense models, with a coefﬁcient of determination R2 = 73.2.
In contrast, when we examine runs with single-attribute training (bottom row of Figure 3 and Appendix C), we ob-serve that, in this case, sparsity has very little effect on bias ampliﬁcation for the hidden ‘Male’ category, which stays roughly constant, within noise bounds. However, re-call from our previous discussion that the baseline (dense) bias ampliﬁcation is signiﬁcantly higher for single-attribute training relative to jointly-trained attributes. Speciﬁcally,
BA for dense singly-trained models is roughly as high as for 99.5%-sparse jointly-trained models. One interpreta-tion is that the additional prediction heads of the jointly-trained models encourage a more robust feature representa-tion which discourages bias at low sparsity; at high sparsity, however, the compactness of representation induces more bias. Thus, switching to singly-trained attributes may be a good strategy at high sparsity levels.
Figure 4. Effect on BA of adding a backdoor feature when per-forming single-attribute training for four attributes.
To study the ampliﬁcation of bias by sparse models, we artiﬁcially introduce bias in the data through speciﬁc mod-iﬁcations to the samples, via “backdoor attacks”. We then measure the effects on a similarly “backdoored” test set, for dense and sparse models for single-attribute prediction. We follow a similar approach to [48, 50] for backdooring: we apply a ﬁxed transformation—grayscaling of the entire im-age [50], or inserting a small yellow square [48] — to the majority of training samples with a positive label, and to a smaller subset of those having the negative label. On the test set, we keep an even ratio of backdoored samples. We per-form both the grayscale and yellow square backdoor attacks when training with four separate attributes: Blond, Smiling,
Oval Face and Big Nose. We use a backdooring split of 95% positive /5% negative for Blond and Smiling, and 65% positive /35% negative for Oval Face and Big Nose. The smaller split prevents the model from simply memorizing the backdoor on harder tasks.
Targeted backdoors enable us to better control and iso-late the source of bias introduced in the models. We con-sider category bias, and focus on bias ampliﬁcation (BA) as our main metric. Speciﬁcally, in the deﬁnition of BA described in Section 2.1 we consider backdooring as our
identity category, i.e. if a sampled is backdoored, then it has identity category 1, and 0 otherwise.
Our results in Figure 4 show that, as expected, BA in-creases substantially for all models considered. Moreover, we observe that bias is slightly ampliﬁed with sparsity, for example on the Big Nose or Smiling attributes. Overall, our study on bias for backdoored models results in similar con-clusions to the “clean” single label experiments. For exam-ple, when examining the BA scores for single label training in Figure 3, we notice that the values have only a slight in-crease with sparsity. This suggests that bias is more likely to follow from less diverse feature representations, whereas here the relationship with sparsity is weaker. 4. Mitigating Sparsity-Induced Bias 4.1. Threshold Calibration
Inspired by our earlier observation that sparser models tend to show worse threshold calibration bias, we consider what happens when we adjust the thresholds to better ﬁt the true distribution of each attribute. We note that the de-cision to adjust the threshold is not clear-cut; the logistic loss encourages the correct prediction, rather than the cor-rect ranking for each attribute. Further, the threshold ad-justment does not take the identity feature into account, and should not be confused with fairness-aware threshold ad-justments [23]. Instead, we set a single threshold for each attribute so that the predictions are correctly calibrated on the original CelebA validation set.
Figure 5. (Left) Effect of threshold calibration on ResNet18 mod-els jointly trained on all attributes. (Right) Proportion of uncertain predictions for dense models across all attributes for all elements in the CelebA test set, and for Compression-Identiﬁed Exemplars at different sparsities.
The results of threshold calibration are shown in Figure 5 (Left). Despite the fact that the threshold adjustment pro-cess is agnostic to identity categories, this simple correction reduces the bias ampliﬁcation across all sparsities, almost eliminating bias effects at up to 90% sparsity. 4.2. Overriding Sensitive Samples
Since the additional bias ampliﬁcation in sparse mod-els must be due to test samples whose classiﬁcation has changed between dense and sparse models, we examine these examples more closely. We focus on Compression-Identiﬁed Exemplars (CIEs) [28, 29], which are the test
Figure 6. Effect of label overrides on Male Bias Ampliﬁcation. examples on which the modal dense label across multiple training runs disagrees with the modal sparse label, regard-less of which one is correct. For each sparsity, we com-pute the CIEs across ﬁve runs each of the dense and sparse models. Our results in Figure 5 (Right) show that CIEs are greatly enriched for prediction uncertainty, suggesting that improving the predictions of these examples may assist in reducing BA, especially in the sparse models. However,
CIEs are expensive to compute due to requiring multiple models for consensus, and are speciﬁc to the sparsity level.
Prediction overrides, where a ﬁxed label for a small sub-set of data is distributed along with the model, and selected over the model prediction at inference time, are common in model deployment. Inspired by our observation that CIEs are highly enriched for uncertain examples, we propose to prioritize the highest-uncertainty data as classiﬁed by a dense model, in cases where the dense model already shows positive BA. We replicate this setting on the test dataset.
This is consistent with standard practice for override prior-itization to improve accuracy, since the most uncertain ex-amples are presumed to have the highest chance of having the wrong label.
We consider two possible override labels: the correct la-bel, which simulates human overrides, and the dense label, which simulates the best possible label if human labeling is impractical. We apply these overrides to all sparse la-bels and measure the bias ampliﬁcation. Our results (Fig-ure 6 and Figure B.1) show that overrides with both human and dense labels substantially decrease the bias ampliﬁca-tion of models of all sparsities. For instance, using manual overrides for the most uncertain 5% of examples lowers the mean BA of the 99.5% sparse model by 23%, and replacing the top 10% lowers the mean BA by 35%. This suggests that the use of uncertainty-based override pipelines is an effec-tive tool for reducing bias ampliﬁcation on sparse models, even when only the dense model is used to set prioritization. 5. Additional Validation
We emphasize the fact that the above observations have been validated on additional datasets and models, so our
ﬁndings hold generally. We discuss these experiments brieﬂy below, and present them in full in the Appendix.
Additional Validation on CelebA. We experiment with the setup where pruning starts from a pretrained model,
for which we include the results in Appendix E, showing similar results. We additionally prune to N:M (2:4, 1:4 and 1:8) sparsity patterns [44] in Appendix F, with sim-ilar results to lower-sparsity models pruned without this restriction. Experiments validating our results for singly-and jointly-trained attributes on the MobileNetV1 architec-ture [30] can be found in Appendix G, showing the same trends, but at slightly lower sparsities. We additionally val-idate the joint training results on the ResNet50 architecture in Appendix H, with very similar results to ResNet18. Fi-nally, we repeat the ResNet18 joint training experiments us-ing the uncropped CelebA dataset, which ensures that fea-tures such as the presence of neckwear are available to the model (as they were to the human labellers). We discuss these results in Appendix I.
Additional Datasets. We further validated our ﬁndings on two additional datasets. The Animals with Attributes (AwA) dataset [51] serves as a useful validation for our observations regarding the effect of sparsity on bias in bi-nary prediction (Appendix K). The challenging iWildcam dataset [3] validates our observations regarding increased uncertainty relative to sparsity in the context of multiclass classiﬁcation (Appendix L). 6.