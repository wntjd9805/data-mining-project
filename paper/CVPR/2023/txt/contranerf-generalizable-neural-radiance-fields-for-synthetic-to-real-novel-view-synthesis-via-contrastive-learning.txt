Abstract
Although many recent works have investigated general-izable NeRF-based novel view synthesis for unseen scenes, they seldom consider the synthetic-to-real generalization, which is desired in many practical applications.
In this work, we first investigate the effects of synthetic data in synthetic-to-real novel view synthesis and surprisingly ob-serve that models trained with synthetic data tend to pro-duce sharper but less accurate volume densities. For pix-els where the volume densities are correct, fine-grained de-tails will be obtained. Otherwise, severe artifacts will be produced. To maintain the advantages of using synthetic data while avoiding its negative effects, we propose to intro-duce geometry-aware contrastive learning to learn multi-view consistent features with geometric constraints. Mean-while, we adopt cross-view attention to further enhance the geometry perception of features by querying features across input views. Experiments demonstrate that under the synthetic-to-real setting, our method can render images with higher quality and better fine-grained details, outper-forming existing generalizable novel view synthesis meth-ods in terms of PSNR, SSIM, and LPIPS. When trained on real data, our method also achieves state-of-the-art results. https://haoy945.github.io/contranerf/ 1.

Introduction
Novel view synthesis is a classical problem in computer vision, which aims to produce photo-realistic images for un-seen viewpoints [2,5,10,36,40]. Recently, Neural Radiance
Fields (NeRF) [25] proposes to achieve novel view synthe-sis through continuous scene modeling through a neural net-*Joint last authorship. work, which quickly attracts widespread attention due to its surprising results. However, the vanilla NeRF is actu-ally designed to fit the continuous 5D radiance field of a given scene, which often fails to generalize to new scenes and datasets. How to improve the generalization ability of neural scene representation is a challenging problem.
Recent works, such as pixelNeRF [46], IBRNet [37], investigate how to
MVSNeRF [4] and GeoNeRF [19], achieve generalizable novel view synthesis based on neu-ral radiance fields. However, these works mainly focus on the generalization of NeRF to unseen scenes and seldom consider the synthetic-to-real generalization, i.e., training
NeRF with synthetic data while testing it on real data. On the other hand, synthetic-to-real novel view synthesis is de-sired in many practical applications where the collection of dense view 3D data is expensive (e.g., autonomous driv-ing, robotics, and unmanned aerial vehicle [34]). Although some works directly use synthetic data such as Google
Scanned Objects [14] in model training, they usually over-look the domain gaps between the synthetic and real data as well as possible negative effects of using synthetic data.
In 2D computer vision, it is common sense that synthetic training data usually hurts the model’s generalization abil-ity to real-world applications [1, 8, 44]. Will synthetic data be effective in novel view synthesis?
In this work, we first investigate the effectiveness of syn-thetic data in NeRF’s training via extensive experiments.
Specifically, we train generalizable NeRF models using a synthetic dataset of indoor scenes called 3D-FRONT [15], and test the models on a real indoor dataset called Scan-Net [9]. Surprisingly, we observe that the use of synthetic data tends to result in more artifacts on one hand but bet-ter fine-grained details on the other hand (see Fig.1 and
Sec.3.2 for more details). Moreover, we observe that mod-els trained on synthetic data tend to predict sharper but less
(a) Ground truth (b) Pred (ScanNet) (c) Pred (3D-FRONT) (d) Density (ScanNet) (e) Density (3D-FRONT)
Figure 1. Fig.1a is the ground truth of the target image to be rendered. Fig.1b and Fig.1c are the rendered images when models are trained on ScanNet [9] and 3D-FRONT [15], respectively. Compare to Fig.1b, Fig.1c is more detailed (see purple box) but has more artifacts (see pink box). Fig.1d and Fig.1e further show the volume density (the redder the color, the higher the density) along the epipolar line projected from the orange points in Fig.4a to the source view. The model trained on 3D-FRONT prefers to predict the volume density with a sharper distribution, but sometimes the predicted volume density is not accurate (see line 1 in Fig.1e), resulting in severe artifacts. views (see Fig.3). Specifically, for pixels of each source view, we first aggregate information along the ray projected to other source views to get the geometry-enhanced fea-tures. Then, we sample a batch of target pixels from each source view as the training batch for contrastive learning and project them to other views to get positive and neg-ative samples. The InfoNCE loss [35] is calculated in a weighted manner. Finally, we render the ray by learning a general view interpolation function following [37]. Ex-periments show that when trained on the synthetic data, our method outperforms the recent concurrent generalizable
NeRF works [4, 19, 24, 37, 46] and can render high-quality novel view while preserving fine-grained details for unseen scenes. Moreover, under the real-to-real setting, our method also performs better than existing neural radiance field gen-eralization methods. In summary, our contributions are: 1. Investigate the effects of synthetic data in NeRF-based novel view synthesis and observe that models trained on synthetic data tend to predict sharper but less accu-rate volume densities when tested on real data; 2. Propose geometry-aware contrastive learning to learn multi-view consistent features with geometric con-straints, which significantly improves the model’s synthetic-to-real generalization ability; 3. Our method achieves state-of-the-art results for gener-alizable novel view synthesis under both synthetic-to-real and real-to-real settings. 2.