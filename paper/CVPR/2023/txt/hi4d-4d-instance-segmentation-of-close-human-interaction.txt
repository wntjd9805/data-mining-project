Abstract 1.

Introduction
We propose Hi4D, a method and dataset for the auto-matic analysis of physically close human-human interaction under prolonged contact. Robustly disentangling several in-contact subjects is a challenging task due to occlusions and complex shapes. Hence, existing multi-view systems typically fuse 3D surfaces of close subjects into a single, connected mesh. To address this issue we leverage i) in-dividually fitted neural implicit avatars; ii) an alternating optimization scheme that refines pose and surface through periods of close proximity; and iii) thus segment the fused raw scans into individual instances. From these instances we compile Hi4D dataset of 4D textured scans of 20 sub-ject pairs, 100 sequences, and a total of more than 11K frames. Hi4D contains rich interaction-centric annotations in 2D and 3D alongside accurately registered parametric body models. We define varied human pose and shape esti-mation tasks on this dataset and provide results from state-of-the-art methods on these benchmarks. Hi4D dataset can be found at https://ait.ethz.ch/Hi4D.
While computer vision systems have made rapid progress in estimating the 3D body pose and shape of individuals and well-spaced groups, currently there are no methods that can robustly disentangle and reconstruct closely interacting people. This is in part due to the lack of suitable datasets. While some 3D datasets exist that contain human-human interactions, like ExPI [72] and CHI3D [22], they typically lack high-fidelity dynamic textured geometry, do not always provide registered parametric body models and do not always provide rich contact information and are therefore not well suited to study closely interacting people.
Taking a first step towards future AI systems that are able to interpret the interactions of multiple humans in close physical interaction and under strong occlusion, we pro-pose a method and dataset that enables the study of this new setting. Specifically, we propose Hi4D, a compre-hensive dataset that contains segmented, yet complete 4D textured geometry of closely interacting humans, along-side corresponding registered parametric human models, in-stance segmentation masks in 2D and 3D, and vertex-level contact annotations (see Fig. 1). To enable research to-Dataset
Multi-view Images
Temporal
ShakeFive2 [23]
MuPoTS-3D [53]
ExPI [72]
MultiHuman [77]
CHI3D [22]
Hi4D (Ours)
✓
✓
✓
✓
✓
✓
✓
✓
✓ 3D Pose Format
Joint Positions
Joint Positions
Joint Positions
Parametric Body Model†
Parametric Body Model
Parametric Body Model
Reference Data Modalities
Textured Scans
Contact Annotations
Instance Masks
✓
✓
✓ region-level (631 events) vertex-level (> 6K events)
✓
Table 1. Comparison of datasets containing close human interaction. †In [77] registrations are not considered as ground-truth. wards automated analysis of close human interactions, we contribute experimental protocols for computer vision tasks that are enabled by Hi4D.
Capturing such a dataset and the corresponding anno-tations is a very challenging endeavor in itself. While multi-view, volumetric capture setups can reconstruct high-quality 4D textured geometry of individual subjects, even modern multi-view systems typically fuse 3D surfaces of spatially proximal subjects into a single, connected mesh (see Fig. 1, A). Thus deriving and maintaining complete, per subject 4D surface geometry, parametric body registra-tion, and contact information from such reconstructions is non-trivial. In contrast to the case of rigid objects, simple tracking schemes fail due to very complex articulations and thus strong changes in terms of geometry. Moreover, con-tact itself will further deform the shape.
To address these problems, we propose a novel method to track and segment the 4D surface of multiple closely in-teracting people through extended periods of dynamic phys-ical contact. Our key idea is to make use of emerging neu-ral implicit surface representations for articulated shapes, specifically SNARF [13], and create personalized human avatars of each individual (see Fig. 2, A). These avatars then serve as strong personalized priors to track and thus segment the fused geometry of multiple interacting people (see Fig. 2, B). To this end, we alternate between pose opti-mization and shape refinement (see Fig. 3). The optimized pose and refined surfaces yield precise segmentations of the merged input geometry. The tracked 3D instances (Fig. 1,
B) then provide 2D and 3D instance masks (Fig. 1, C), vertex-level contact annotations (Fig. 1, B), and can be used to register parametric human models (Fig. 1, D).
Equipped with this method, we capture Hi4D, which stands for Humans interacting in 4D, a dataset of humans in close physical interaction alongside high-quality 4D an-notations. The dataset contains 20 pairs of subjects (24 male, 16 female), and 100 sequences with more than 11K frames. To our best knowledge, ours is the first dataset con-taining rich interaction-centric annotations and high-quality 4D textured geometry of closely interacting humans.
To provide baselines for future work, we evaluate several state-of-the-art methods for multi-person pose and shape modeling from images on Hi4D in different settings such as monocular and multi-view human pose estimation and detailed geometry reconstruction. Our baseline experi-ments show that our dataset provides diverse and challeng-ing benchmarks, opening up new directions for research. In summary, we contribute:
• A novel method based on implicit avatars to track and segment 4D scans of closely interacting humans.
• Hi4D, a dataset of 4D textured scans with correspond-ing multi-view RGB images, parametric body models, instance segmentation masks and vertex-level contact.
• Several experimental protocols for computer vision tasks in the close human interaction setting. 2.