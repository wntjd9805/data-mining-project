Abstract
Inspired by the success of recent vision transformers and large kernel design in convolutional neural networks (CNNs), in this paper, we analyze and explore essential reasons for their success. We claim two factors that are critical for 3D large-scale scene understanding: a larger receptive field and operations with greater non-linearity.
The former is responsible for providing long range con-texts and the latter can enhance the capacity of the net-work. To achieve the above properties, we propose a simple yet effective long range pooling (LRP) module using dila-tion max pooling, which provides a network with a large adaptive receptive field. LRP has few parameters, and can be readily added to current CNNs. Also, based on LRP, we present an entire network architecture, LRPNet, for 3D understanding. Ablation studies are presented to support our claims, and show that the LRP module achieves bet-ter results than large kernel convolution yet with reduced computation, due to its non-linearity. We also demonstrate the superiority of LRPNet on various benchmarks: LRPNet performs the best on ScanNet and surpasses other CNN-based methods on S3DIS and Matterport3D. Code will be avalible at https://github.com/li-xl/LRPNet. 1.

Introduction
With the rapid development of 3D sensors, more and more 3D data is becoming available from a variety of ap-plications such as autonomous driving, robotics, and aug-mented/virtual reality. Analyzing and understanding this 3D data has become essential, which requires efficient and effective processing. Various data structures, including point clouds, meshes, multi-view images, voxels, etc, have been proposed for representing 3D data [73]. Unlike 2D image data, a point cloud or mesh, is irregular and un-ordered. These characteristics mean that typical CNNs can-not directly be applied to such 3D data. Thus, specially designed networks such as MLPs [49], CNNs [27, 40, 69],
GNNs [53, 68] and transformers [16, 46, 79] are proposed
Figure 1.
Qualitative results of Effective Receptive Field (ERF) [45]. Left: the input and ground truth. Middle: the ERF and results of baseline described in section 3.3. Right: the ERF and results of LRPNet (ours). The stained areas around the posi-tions of interest (red dots) represent the range of receptive fields, with green to red representing the increasing strength of response.
Our method correctly segments the cabinet from the wall with the proposed effective receptive field. to perform effective deep learning on them. However, pro-cessing large-scale point cloud or mesh is computationally expensive. Multi-view images contain multiple different views of a scene. Typically, CNNs are used to process each view independently and a fusion module is used to combine the results. Nevertheless, there is unavoidable 3D informa-tion loss due to the finite or insufficient number of views.
Voxel data has the benefit of regularity like images, even if the on-surface voxels only contain sparse data. The simplic-ity of the structure helps to maintain high performance, so this paper focuses on processing voxel data.
Learning on 3D voxels can be easily implemented by directly extending the well studied 2D CNN networks to 3D [52, 66, 72]. Considering that 3D voxel data is inher-ently sparse, some works usually adopt specially designed 3D sparse CNNs [7,15,56] for large-scale scene understand-ing. Since only the surface of the real scene data has val-ues, the neighbors around each voxel do not necessarily be-long to the same object, so a large receptive field is more
conducive to feature extraction. Sparse convolution has ad-vantages in modeling local structures, but it ignores long range contexts, which are critical for 3D scene understand-ing tasks. Transformers [16, 35, 70, 71] have proved useful in processing 3D scenes, as they can capture the global re-lationship with their larger receptive field and a better way to interact. However, they usually have a quadratic compu-tational complexity, which brings a heavy computing cost.
A straightforward way to incorporate the long range con-texts into the learning of 3D voxel data is to exploit a large kernel 3D convolution. However, the number of network parameters and the amount of computation would increase cubically due to the additional dimension compared to 2D images. Besides, current ways of feature interaction or ag-gregation, such as average pooling and convolution, usually adopt a linear combination of features of all the locations in the receptive field. This works for 2D images since all the locations in the receptive field have valid values, which, however, does not hold for 3D voxels due to the sparsity na-ture of 3D scenes. Directly applying such linear interaction or aggregation on the voxel that has few neighbors in the re-ceptive field would make the feature of that voxel too small or over-smoothed and thus less informative.
Taking the above into consideration, and to achieve a trade-off between the quality of results and computation, we propose long range pooling (LRP), a simple yet effec-tive module for 3D scene segmentation. Compared with previous sparse convolution networks [7, 15], LRP is capa-ble of increasing the effective receptive field with a negli-gible amount of computational overhead. Specifically, we achieve a large receptive field by proposing a novel dila-tion max pooling to enhance the non-linearity of the neural network. We further add a receptive field selection mod-ule, so that each voxel can choose a suitable receptive field, which is adaptive to the distribution of voxels. The above two components comprise the LRP module. Unlike dilation convolution, which is often used to enlarge the receptive field for 2D images [18,19], our method can achieve a large receptive field with fewer parameters and computation by using dilation max pooling. Furthermore, LRP is a simple, and efficient module that can be readily incorporated into other networks. We construct a more capable neural net-work, LRPNet, by adding LRP at the end of each stage of the sparse convolution network, introduced by VMNet [29].
Experimental results show that LRPNet achieves a significant in 3D segmentation accuracy
[8], on large-scale scene datasets, including ScanNet
S3DIS [1] and Matterport3D [4]. Qualitative results are il-lustrated in Figure 1, which shows the improvement of a larger receptive field. We also experimentally compare the effects of the different receptive fields by reducing the num-ber of dilation max pooling of LRP. Moreover, we explore the influence of non-linearity of LRP module by replacing improvement max pooling with average pooling or convolution. Ablation results show that a larger receptive field and operations with greater non-linearity for feature aggregation will improve the segmentation accuracy.
Our contributions are thus:
• a simple and effective module, the long range pooling (LRP) module, which provides a network with a large adaptive receptive field without a large number of pa-rameters,
• a demonstration that a larger receptive field and aggre-gation operations with greater non-linearity enhance the capacity of a sparse convolution network, and
• a simple sparse convolution network using the LRP module, which achieves superior 3D segmentation re-sults on various large-scale 3D scene benchmarks. 2.