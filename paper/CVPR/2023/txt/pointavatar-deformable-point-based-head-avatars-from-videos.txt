Abstract 1.

Introduction
The ability to create realistic animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertain-ment. Current methods either build on explicit 3D mor-phable meshes (3DMM) or exploit neural implicit repre-sentations. The former are limited by ﬁxed topology, while the latter are non-trivial to deform and inefﬁcient to render.
Furthermore, existing approaches entangle lighting and albedo, limiting the ability to re-render the avatar in new environments. In contrast, we propose PointAvatar, a de-formable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, com-bining high-quality geometry and appearance with topo-logical ﬂexibility, ease of deformation and rendering efﬁ-ciency. We show that our method is able to generate an-imatable 3D avatars using monocular videos from multi-ple sources including hand-held smartphones, laptop web-cams and internet videos, achieving state-of-the-art qual-ity in challenging cases where previous methods fail, e.g., thin hair strands, while being signiﬁcantly more efﬁcient in training than competing methods. contact: yufeng.zheng@inf.ethz.ch project page: https://zhengyuf.github.io/PointAvatar/
Personalized 3D avatars will enable new forms of com-munication and entertainment. Successful tools for creat-ing avatars should enable easy data capture, efﬁcient com-putation, and create a photo-realistic, animatable, and re-lightable 3D representation of the user. Unfortunately, ex-isting approaches fall short of meeting these requirements.
Recent methods that create 3D avatars from videos ei-ther build on 3D morphable models (3DMMs) [26, 36] or leverage neural implicit representations [32, 33, 35]. The former methods [8, 13, 22, 23] allow efﬁcient rasterization and inherently generalize to unseen deformations, but they cannot easily model individuals with eyeglasses or com-plex hairstyles, as 3D meshes are limited by a-priori ﬁxed topologies and surface-like geometries. Recently, neu-ral implicit representations have also been used to model 3D heads [5, 11, 16, 54]. While they outperform 3DMM-based methods in capturing hair strands and eyeglasses, they are signiﬁcantly less efﬁcient to train and render, since rendering a single pixel requires querying many points along the camera ray. Moreover, deforming implicit rep-resentations in a generalizable manner is non-trivial and existing approaches have to revert to an inefﬁcient root-ﬁnding loop, which impacts training and testing time nega-tively [10, 18, 25, 45, 54].
To address these issues, we propose PointAvatar, a novel avatar representation that uses point clouds to represent the canonical geometry and learns a continuous deformation
Efﬁcient
Rendering
Easy
Animation
Flexible
Topology
Thin
Strands
Surface
Geometry
Meshes
Implicit Surfaces
Volumetric NeRF
Points (ours)
✓
✗
✗
✓
✓
✗
✗
✓
✗
✓
✓
✓
✗
✗
✓
✓
✓
✓
✗
✓
Table 1. PointAvatar is efﬁcient to render and deform which en-ables straightforward rendering of full images during training. It can also handles ﬂexible topologies and thin structures and can re-construct good surface normals in surface-like regions, e.g., skin.
ﬁeld for animation. Speciﬁcally, we optimize an oriented point cloud to represent the geometry of a subject in a canonical space. For animation, the learned deformation
ﬁeld maps the canonical points to the deformed space with learned blendshapes and skinning weights, given expression and pose parameters of a pretrained 3DMM. Compared to implicit representations, our point-based representation can be rendered efﬁciently with a standard differentiable raster-izer. Moreover, they can be deformed effectively using es-tablished techniques, e.g., skinning. Compared to meshes, points are considerably more ﬂexible and versatile. Be-sides the ability to conform the topology to model acces-sories such as eyeglasses, they can also represent complex volume-like structures such as ﬂuffy hair. We summarize the advantanges of our point-based representation in Tab. 1.
One strength of our method is the disentanglement of lighting effects. Given a monocular video captured in un-constrained lighting, we disentangle the apparent color into the intrinsic albedo and the normal-dependent shading; see
Fig. 1. However, due to the discrete nature of points, accu-rately computing normals from point clouds is a challenging and costly task [6, 17, 29, 37], where the quality can deteri-orate rapidly with noise, and insufﬁcient or irregular sam-pling. Hence we propose two techniques to (a) robustly and accurately obtain normals from learned canonical points, and (b) consistently transform the canonical point normals with the non-rigid deformation. For the former, we exploit the low-frequency bias of MLPs [38] and estimate the nor-mals by ﬁtting a smooth signed distance function (SDF) to the points; for the latter, we leverage the continuity of the deformation mapping and transform the normals analyt-ically using the deformation’s Jacobian. The two techniques lead to high-quality normal estimation, which in turn propa-gates the rich geometric cues contained in shading to further improve the point geometry. With disentangled albedo and detailed normal directions, PointAvatar can be relit and ren-dered under novel scene lighting.
As demonstrated using various videos captured with
DSLR, smartphone, laptop cameras, or obtained from the internet, the proposed representation combines the advan-tages of popular mesh and implicit representations, and sur-passes both in many challenging scenarios. In summary, our contributions include: 1. We propose a novel representation for 3D animatable avatars based on an explicit canonical point cloud and continuous deformation, which shows state-of-the-art photo-realism while being considerably more efﬁcient than existing implicit 3D avatar methods; 2. We disentangle the RGB color into a pose-agnostic albedo and a pose-dependent shading component; 3. We demonstrate the advantage of our methods on a variety of subjects captured through various commod-ity cameras, showing superior results in challenging cases, e.g., for voluminous curly hair and novel poses with large deformation. 2.