Abstract
This paper presents a method to reconstruct a complete human geometry and texture from an image of a person with only partial body observed, e.g., a torso. The core challenge arises from the occlusion: there exists no pixel to reconstruct where many existing single-view human reconstruction meth-ods are not designed to handle such invisible parts, leading to missing data in 3D. To address this challenge, we intro-duce a novel coarse-to-fine human reconstruction framework.
For coarse reconstruction, explicit volumetric features are learned to generate a complete human geometry with 3D con-volutional neural networks conditioned by a 3D body model and the style features from visible parts. An implicit network combines the learned 3D features with the high-quality sur-face normals enhanced from multiviews to produce fine local details, e.g., high-frequency wrinkles. Finally, we perform progressive texture inpainting to reconstruct a complete ap-pearance of the person in a view-consistent way, which is not possible without the reconstruction of a complete geometry.
In experiments, we demonstrate that our method can recon-struct high-quality 3D humans, which is robust to occlusion. 1.

Introduction
How many portrait photos in your albums have the whole body captured? Usually, the answer is not many. Taking a photo of the whole body is often limited by a number of factors of occlusion such as camera angles, objects, other people, and self. While existing single-view human recon-struction methods [3, 43] have shown promising results, they often fail to handle such incomplete images, leading to sig-nificant artifacts with distortion and missing data in 3D for invisible body parts. In this paper, we introduce a method to reconstruct a complete 3D human model from a single image of a person with occlusions as shown in Figure 1.
The complete 3D model can be the foundation for a wide range of applications such as film production, video games, virtual teleportation, and 3D avatar printing from a group-shot photo. 3D human reconstruction from an image [2, 16]
Figure 1. The complete reconstruction results using our method from the image of a person with occlusion by other people. has been studied for two decades. The recent progress in this topic indicates the neural network based implicit ap-proach [3, 44] is a promising way for accurate detail recon-struction. Such an approach often formulated the 3D human reconstruction problem as a classification task: an implicit network is designed to learn image features at each pixel, e.g., pixel-aligned features [18, 43, 44], which enable con-tinual classification of the position in 3D along the camera ray. While the implicit approaches have shown a strong per-formance to produce the geometry with high-quality local details, the learning of such an implicit model is often char-acterized as 1) reconstructive: it estimates 3D only for the pixels that are captured from a camera, i.e., no 3D recon-struction is possible for missing pixels of invisible parts; and 2) globally incoherent: the ordinal relationship (front-back relationship in 3D) of the reconstructed 3D points is often not globally coherent, e.g., while the reconstruction of the face surface is locally plausible, its combination with other parts such as torso looks highly distorted. These properties fundamentally limit the implicit network to reconstruct the complete and coherent 3D human model from the image with a partial body.
In this paper, we overcome these fundamental limitations
of the implicit network by modeling generative and globally coherent 3D volumetric features. To this end, we use a 3D convolutional neural network that can explicitly capture the global ordinal relation of a human body in the canonical 3D volume space. It generates volumetric features by encoding an incomplete image and a 3D body model, i.e., SMPL [30, 37], where the 3D body model provides the unified guidance of the body pose in the coherent 3D space. These volumetric features are jointly learned with a 3D discriminator in a way that generates a coarse yet complete 3D geometry.
The complete 3D geometry enables the coherent render-ing of its shape over different viewpoints, which makes it possible to enhance surface normals and inpaint textures in a multiview-consistent way. Specifically, for surface nor-mal enhancement, a neural network takes as input a coarse rendering of the surface normal and style features; and out-puts the fine surface normal with plausible high-frequency details. We design a novel normal fusion network that can combine the fine surface normals from multiviews with the learned volumetric features to upgrade the quality of local geometry details. For texture inpainting, a neural network conditioned on the fine surface normal and an incomplete image generates the complete textures. The inpainted tex-tures are progressively combined from multiviews through the 3D geometry.
Unlike previous methods [43, 43, 44, 52] which have uti-lized the surface normals from limited views (e.g., front and back), our multiview normal fusion approach can produce more coherent and refined reconstruction results by incorpo-rating fine-grained surface normals from many views.
Our experiments demonstrate that our method can ro-bustly reconstruct a complete 3D human model with plausi-ble details from the image of a partial body, outperforming previous methods while still obtaining comparable results in the full-body case.
The technical contributions of this work include (1) a new design of generative and coherent volumetric features which make an implicit network possible to reconstruct a complete 3D human from an incomplete image; (2) a novel multiview normal fusion approach that upgrades the quality of local geometry details in a view-coherent way; and (3) an effective texture inpainting pipeline using the reconstructed 3D geometry. 2.