Abstract
Recently reported state-of-the-art results in visual speech recognition (VSR) often rely on increasingly large amounts of video data, while the publicly available tran-scribed video datasets are limited in size. In this paper, for the first time, we study the potential of leveraging synthetic visual data for VSR. Our method, termed SynthVSR, sub-stantially improves the performance of VSR systems with synthetic lip movements. The key idea behind SynthVSR is to leverage a speech-driven lip animation model that gen-erates lip movements conditioned on the input speech. The speech-driven lip animation model is trained on an unla-beled audio-visual dataset and could be further optimized towards a pre-trained VSR model when labeled videos are available. As plenty of transcribed acoustic data and face images are available, we are able to generate large-scale synthetic data using the proposed lip animation model for semi-supervised VSR training. We evaluate the perfor-mance of our approach on the largest public VSR bench-mark - Lip Reading Sentences 3 (LRS3). SynthVSR achieves a WER of 43.3% with only 30 hours of real labeled data,
∗Work done during an internship at Meta AI. outperforming off-the-shelf approaches using thousands of hours of video. The WER is further reduced to 27.9% when using all 438 hours of labeled data from LRS3, which is on par with the state-of-the-art self-supervised AV-HuBERT method. Furthermore, when combined with large-scale pseudo-labeled audio-visual data SynthVSR yields a new state-of-the-art VSR WER of 16.9% using publicly available data only, surpassing the recent state-of-the-art approaches trained with 29 times more non-public machine-transcribed video data (90,000 hours). Finally, we perform extensive ablation studies to understand the effect of each component in our proposed method. 1.

Introduction
Visual speech recognition (VSR), also known as lip read-ing, is the task of recognizing speech content based on vi-sual lip movements. VSR has a wide range of applica-tions in real-world scenarios such as helping the hearing-impaired perceive human speech and improving automatic speech recognition (ASR) in noisy environments.
VSR is a challenging task, as it requires capturing speech from high-dimensional spatio-temporal videos, while mul-tiple words are visually ambiguous (e.g., “world” and
“word”) in the visual streams. Recently, with the release of large-scale transcribed audio-visual datasets such as LRS2
[1] and LRS3 [2], deep neural networks have become the mainstream approach for VSR. However, even the largest public dataset for English VSR, LRS3, does not exceed 500 hours of transcribed video data. The lack of large-scale transcribed audio-visual datasets potentially results in VSR models which could only work in a laboratory environment i.e. limited word vocabulary and lip sources diversity [28].
A common solution to this issue is to collect and annotate large-scale audio-visual datasets. For example, [41, 42] col-lected 90,000 hours of YouTube videos with user-uploaded transcriptions to achieve state-of-the-art performance on standard benchmarks. However, such a procedure is expen-sive and time-consuming, especially for most of the world’s 7,000 languages [43]. If annotations are missing, the ASR can be used to generate the transcriptions automatically and this has been shown to be an effective approach to signifi-cantly improve VSR performance [28]. The other promis-ing direction is to learn audio-visual speech representations from large amounts of parallel unlabeled audio-visual data in a self-supervised approach, and then fine-tune them on the limited labeled video dataset [43]. Nevertheless, pub-licly available video datasets are also limited and their us-age may raise license-related1 concerns, barring their use in commercial applications.
Human perception of speech is inherently multimodal, involving both audition and vision [43]. ASR, which is a complementary task to VSR, has achieved impressive per-formance in recent years, with tens of thousands of hours of annotated speech datasets [4, 20, 33] available for large-scale training. It is intuitive to ask: Can we improve VSR with large amounts of transcribed acoustic-only ASR train-ing data? The key to this question is to take advantage of recent advances in speech-driven visual generative mod-els [49,50]. By leveraging visual generative models, we can produce parallel synthetic videos for large-scale labeled au-dio datasets. Synthetic videos provide advantages such as having control over the target text and lip image as well as the duration of a generated utterance. To the best of our knowledge, the potential of leveraging synthetic visual data for improving VSR has never been studied in the literature.
In this work, we present SynthVSR, a novel semi-supervised framework for VSR. In particular, we first pro-pose a speech-driven lip animation model that can generate synthetic lip movements video conditioned on the speech content. Next, the proposed lip animation model is used to generate synthetic video clips from transcribed speech datasets (e.g., Librispeech [33]) and human face datasets (e.g., CelebA [23]). Then, the synthetic videos together with the corresponding transcriptions are used in combi-1Such as LRW [11] and LRS2 [1] datasets which are only permitted for the purpose of academic research. nation with the real video-text pairs (e.g., LRS3 [2]) for large-scale semi-supervised VSR training. The pipeline of
SynthVSR is illustrated in Fig. 1. Unlike existing studies in exploiting unlabeled video data for VSR using meth-ods such as pseudo-labeling [28] and self-supervised learn-ing [43], we use the unlabeled audio-visual data to train a cross-modal generative model in order to bridge ASR train-ing data and VSR. Furthermore, we propose to optimize the lip animation model towards a pre-trained VSR model when labeled videos are available. We empirically demonstrate that the semantically high level, spatio-temporal supervi-sion signal from the pre-trained VSR model offers the lip animation model more accurate lip movements.
SynthVSR achieves remarkable performance gains with labeled video data of different scales. We evaluate the performance of SynthVSR on the largest public VSR benchmark LRS3 with a Conformer-Transformer encoder-In the low-resource setup us-decoder VSR model [28]. ing only 30 hours of labeled video data from LRS3 [2], our approach achieves a VSR WER of 43.3%, substantially outperforming the former VSR methods using hundreds or thousands of hours of video data for supervised training
[1,27,37,44,52] and self-supervised learning [3,26,43]. No-tably, we demonstrate the first successful attempt that trains a VSR model with considerable WER performance using only 30 hours of real video data. Using the complete 438 hours from LRS3 further improves WER to 27.9% which is on par with the state-of-the-art self-supervised method AV-HuBERT-LARGE [43] that uses external 1,759 hours of un-labeled audio-visual data, but with fewer model parameters.
Furthermore, following a recent high-resource setup [25] which uses additional 2,630 hours of ASR pseudo-labeled publicly available audio-visual data, our proposed method yields a new state-of-the-art VSR WER of 16.9%, surpass-ing the former state-of-the-art approaches [41, 42] trained on 90,000 hours of non-public machine-transcribed data.
Finally, we present extensive ablation studies to analyze where the improvement of SynthVSR comes from (e.g., the diversity of lip sources, the scale of ASR data). We also show considerable VSR improvement using synthetic video data derived from Text-To-Speech (TTS)-generated speech, indicating the great potential of our method for VSR. 2.