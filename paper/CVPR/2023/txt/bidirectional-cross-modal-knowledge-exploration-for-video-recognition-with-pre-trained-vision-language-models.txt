Abstract
Vision-language models (VLMs) pre-trained on large-scale image-text pairs have demonstrated impressive trans-ferability on various visual tasks. Transferring knowledge from such powerful VLMs is a promising direction for build-ing effective video recognition models. However, current exploration in this field is still limited. We believe that the greatest value of pre-trained VLMs lies in building a bridge between visual and textual domains. In this paper, we pro-pose a novel framework called BIKE, which utilizes the cross-modal bridge to explore bidirectional knowledge: i)
We introduce the Video Attribute Association mechanism, which leverages the Video-to-Text knowledge to gen-erate textual auxiliary attributes for complementing video recognition. ii) We also present a Temporal Concept Spot-ting mechanism that uses the Text-to-Video expertise to capture temporal saliency in a parameter-free manner, leading to enhanced video representation. Extensive stud-ies on six popular video datasets, including Kinetics-400 & 600, UCF-101, HMDB-51, ActivityNet and Charades, show that our method achieves state-of-the-art performance in various recognition scenarios, such as general, zero-shot, and few-shot video recognition. Our best model achieves a state-of-the-art accuracy of 88.6% on the challenging
Kinetics-400 using the released CLIP model. The code is available at https://github.com/whwu95/BIKE. 1.

Introduction
In recent years, the remarkable success of large-scale pre-training in NLP (e.g., BERT [9], GPT [4, 38],
ERNIE [69] and T5 [39]) has inspired the computer vi-sion community. Vision-language models (VLMs) lever-age large-scale noisy image-text pairs with weak correspon-dence for contrastive learning (e.g., CLIP [37], ALIGN
[19], CoCa [65], Florence [66]), and demonstrate impres-sive transferability across a wide range of visual tasks.
Figure 1. Illustration of the difference between our paradigm (c) with existing unimodality paradigm (a) and cross-modal paradigm (b). Please zoom in for the best view.
Naturally, transferring knowledge from such powerful pre-trained VLMs is emerging as a promising paradigm for building video recognition models. Currently, exploration in this field can be divided into two lines. As depicted in
Figure 1(a), one approach [27,35,64] follows the traditional unimodal video recognition paradigm, initializing the video encoder with the pre-trained visual encoder of VLM. Con-versely, the other approach [21, 34, 48, 58] directly transfers the entire VLM into a video-text learning framework that utilizes natural language (i.e., class names) as supervision, as shown in Figure 1(b). This leads to an question: have we fully utilized the knowledge of VLMs for video recognition?
In our opinion, the answer is No. The greatest charm of
VLMs is their ability to build a bridge between the visual and textual domains. Despite this, previous research em-ploying pre-aligned vision-text features of VLMs for video recognition has only utilized unidirectional video-to-text matching. In this paper, we aim to facilitate bidirectional knowledge exploration through the cross-modal bridge for enhanced video recognition. With this in mind, we mine
Video-to-Text and Text-to-Video knowledge by 1) generating textual information from the input video and
2) utilizing category descriptions to extract valuable video-related signals. narios, e.g., general, zero-shot, and few-shot recognition.
Our main contributions can be summarized as follows:
In the first Video-to-Text direction, a common practice for mining VLM knowledge is to embed the in-put video and category description into a pre-aligned fea-ture space, and then select the category that is closest to the video, as illustrated in Figure 1(b), which serves as our baseline. One further question naturally arises: Can we incorporate auxiliary textual information for video recog-nition? To address this question, we introduce an Video-Attributes Association mechanism, which leverages the zero-shot capability of VLMs to retrieve the most relevant phrases from a pre-defined lexicon for the video. These phrases are considered potential “attributes” of the video and can predict the video category directly. For example, a video of someone kicking a soccer ball may be associated with relevant phrases such as “running on the grass”, “jug-gling soccer ball” and “shooting goal”. Surprisingly, us-ing only the generated attributes, we can achieve 69% top-1 accuracy on the challenging Kinetics-400 dataset. Further-more, these attributes provide additional information that the video visual signal may not capture, allowing us to build an Attributes Recognition Branch for video recognition.
In the second Text-to-Video direction, we believe that temporal saliency in videos can be leveraged to improve video representations. For instance, in a video with the cat-egory “kicking soccer ball”, certain frames of kicking the ball should have higher saliency, while other frames that are unrelated to the category or background frames should have lower saliency. This insight motivates us to propose the Video Concept Spotting mechanism, which utilizes the cross-model bridge to generate category-dependent tempo-ral saliency.
In previous works [34, 48, 58], this intuitive exploration was disregarded. To be more specific, instead of treating each video frame equally, we use the correlation between each frame and the given concept (e.g., category) as a measure of frame-level saliency. This saliency is then used to temporally aggregate the frames, resulting in a com-pact video representation.
In the light of the above explorations, we propose BIKE, a simple yet effective framework via BIdirectional cross-modal Knowledge Exploration for enhanced video recog-nition. Our BIKE comprises two branches: the Attributes branch, which utilizes the Video-Attributes Association mechanism to introduce auxiliary attributes for complemen-tary video recognition, and the Video branch, which uses the
Video Concept Spotting mechanism to introduce tempo-ral saliency to enhance video recognition. To demonstrate the effectiveness of our BIKE, we conduct comprehensive experiments on popular video datasets, including Kinetics-400 [22] & 600 [6], UCF-101 [43], HMDB-51 [24], Ac-tivityNet [5] and Charades [41]. The results show that our method achieves state-of-the-art performance in most sce-• We propose a novel framework called BIKE that explores bidirectional knowledge from pre-trained vision-language models for video recognition.
• In the Video-to-Text direction, we introduce the
Video-Attributes Association mechanism to generate extra attributes for complementary video recognition.
• In the Text-to-Video direction, we introduce the
Video Concept Spotting mechanism to generate tem-poral saliency, which is used to yield the compact video representation for enhanced video recognition. 2. Methodology
An overview of our proposed BIKE is shown in Figure 2.
We next elaborate on each component in more detail. 2.1. Preliminary: Video Recognition with VLM
In this section, we describe the typical cross-modal video recognition pipeline [21,34,48,58] based on the pre-trained vision-language model (VLM). Given a video, we sample
T frames from the video as input v. We also have a col-lection of categories C = {c1, c2, · · · , cK}, where K is the number of classes. The goal of the video recognition task is to classify the video v into a category c ∈ C. Under the for-mulation of video recognition, the video v is encoded with a vision encoder f (·|θv) to obtain the video embedding ev, and the category c is encoded with a text encoder g(·|ϕc) to obtain the category embedding ec, where ev = f (v|θv), ec = g(c|ϕc).
Finally, we obtain the similarity score SV as follows:
SV = s(ev, ec), (1) (2) where s(·, ·) is the cosine similarity function. The ob-jective during training is to maximize SV if v and c are matched, and minimize it in all other cases. During infer-ence, we compute the score between the video embedding and each category embedding, and choose the category with the highest SV as the top-1 prediction. The parameter θv and ϕc of the video encoder and text encoder are initialized with weights from the pre-trained VLM (e.g., CLIP [37]).
Throughout the rest of this work, we use the same notation. 2.2. Video-to-Text: Video-Attributes Association
First we focus on exploring Video-to-Text auxiliary signals. We present an Attributes branch as a complement to the regular Video branch in Sec. 2.1 for video recognition.
Pre-generated Attributes. We begin by describing how to generate auxiliary attributes. As depicted in Figure 2(b), we
Figure 2. An overview of our BIKE for video recognition. (a) BIKE explores bidirectional cross-modal knowledge from the pre-trained vision-language model (e.g., CLIP) to introduce auxiliary attributes and category-dependent temporal saliency for improved video recog-nition. BIKE comprises an auxiliary Attributes branch and a main Video branch. (b) In the Video-to-Text direction, we present the
Video-Attribute Association mechanism, which retrieves semantically relevant phrases from a pre-defined lexicon as video attributes for the input video. These attributes are concatenated and combined with a textual prefix to form an attribute sentence for text recognition. (c)
In the Text-to-Video direction, we present the Video Concept Spotting mechanism, which computes the similarity between video frames and a given category as a measure of temporal saliency to enhance video representation. D is the dimension of embedding, T is the number of frames, and N is the number of words in the category name. utilize the zero-shot capability of the VLM (e.g., CLIP [37]) to identify the most relevant phases from a pre-defined lex-icon as possible “Attributes” of the video. To achieve this, we first apply the CLIP’s image encoder to the input video
V to extract frame-level features. These features are then combined using average pooling to yield a video embed-ding. Next, we feed each phase in the pre-defined lexi-con into the CLIP’s text encoder to produce a set of text embeddings. We then calculate the similarity between this video embedding and each text embedding, sort the results, and select the top few phrases as the “Attributes”. Once we have obtained the attributes, we employ a simple fusion method that concatenates them into a single attributes sen-tence a. We also add a manually-designed prompt as a pre-fix to the sentence, such as “This is a video about {}”.
Attributes Recognition. As shown in Figure 2(a), the at-tributes sentence a is encoded with a text encoder g(·|ϕa) to produce the attribute embedding ea: ea = g(a|ϕa). (3)
We use this attribute embedding to perform Attributes
Recognition by calculating the similarity SA between the attribute embedding and category embeddings. Note that both the attribute sentence and categories are encoded us-ing the same text encoder from CLIP. Interestingly, the At-tributes branch can achieve a certain level of recognition performance (e.g., ∼56%) without any extra training, even though it’s a lightweight text recognition pipeline. During inference, we combine the well-trained Video branch with the plug-and-play Attributes branch using the following fu-sion equation:
S = λSV + (1 − λ)SA, (4) where λ is the fusion weight. Without any additional train-ing, the Attributes Recognition surprisingly improve the video recognition performance, e.g., 78.8% +1.2%−−−−→ 80.0% on the challenging Kinetics-400. Naturally, the text encoder g(·|ϕa) can be further trained in an end-to-end manner to improve the Attributes branch and provide a stronger com-plementary capability, e.g., 78.8% +2.6%−−−−→ 81.4%.
2.3. Text-to-Video: Video Concept Spotting 2.4. Objectives of BIKE
In Sec. 2.2, the Video-to-Text knowledge is em-ployed to generate auxiliary attributes, thereby construct-ing a complementary Attributes branch. Naturally, we also conduct an exploration to leverage the Text-to-Video knowledge to enhance the standard Video branch for video recognition. Specifically, we propose the use of category-dependent temporal saliency to guide the temporal aggre-gation process, resulting in a compact video representation that enhances video recognition.