Abstract
Multimodal models are becoming increasingly effective, in part due to uniﬁed components, such as the Transformer architecture. However, multimodal models still often consist of many task- and modality-speciﬁc pieces and training pro-cedures. For example, CLIP (Radford et al., 2021) trains in-dependent text and image towers via a contrastive loss. We explore an additional uniﬁcation: the use of a pure pixel-based model to perform image, text, and multimodal tasks.
Our model is trained with contrastive loss alone, so we call it CLIP-Pixels Only (CLIPPO). CLIPPO uses a single en-coder that processes both regular images and text rendered as images. CLIPPO performs image-based tasks such as re-trieval and zero-shot image classiﬁcation almost as well as
CLIP-style models, with half the number of parameters and no text-speciﬁc tower or embedding. When trained jointly via image-text contrastive learning and next-sentence con-trastive learning, CLIPPO can perform well on natural language understanding tasks, without any word-level loss (language modelling or masked language modelling), out-performing pixel-based prior work. Surprisingly, CLIPPO can obtain good accuracy in visual question answering, simply by rendering the question and image together. Fi-nally, we exploit the fact that CLIPPO does not require a tokenizer to show that it can achieve strong performance on multilingual multimodal retrieval without modiﬁcations. 1.

Introduction
In recent years, large-scale multimodal training of
Transformer-based models has led to improvements in the state-of-the-art in different domains including vision [2, 10, 74–76], language [6, 11], and audio [5].
In particular, in computer vision and image-language understanding, a sin-gle large pretrained model can outperform task-speciﬁc ex-pert models [10, 74, 75]. However, large multimodal mod-els often use modality or dataset-speciﬁc encoders and de-coders, and accordingly lead to involved protocols. For example, such models frequently involve training different
Code and pretrained models are available as part of big vision [4] https://github.com/google-research/big_vision.
Figure 1. CLIP [56] trains separate image and text encoders, each with a modality-speciﬁc preprocessing and embedding, on image/alt-text pairs with a contrastive objective. CLIPPO trains a pure pixel-based model with equivalent capabilities by rendering the alt-text as an image, encoding the resulting image pair using a shared vision encoder (in two separate forward passes), and ap-plying same training objective as CLIP. parts of the model in separate phases on their respective datasets, with dataset-speciﬁc preprocessing, or transferring different parts in a task-speciﬁc manner [75]. Such modality and task-speciﬁc components can lead to additional engi-neering complexity, and poses challenges when introducing new pretraining losses or downstream tasks. Developing a single end-to-end model that can process any modality, or combination of modalities, would be a valuable step for multimodal learning. Here, we focus on images and text.
A number of key uniﬁcations have accelerated the progress of multimodal learning. First, the Transformer architecture has been shown to work as a universal back-bone, performing well on text [6, 15], vision [16], au-dio [5, 24, 54], and other domains [7, 34]. Second, many papers have explored mapping different modalities into a single shared embedding space to simplify the input/output interface [21, 22, 46, 69], or develop a single interface to many tasks [31, 37]. Third, alternative representations of modalities allow harnessing in one domain neural archi-tectures or training procedures designed for another do-main [28, 49, 54, 60]. For example, [60] and [28, 54] repre-sent text and audio, respectively, by rendering these modal-ities as images (via a spectogram in the case of audio).
In this paper, we explore the use of a pure pixel-based
model for multimodal learning of text and images. Our model is a single Vision Transformer [16] that processes visual input, or text, or both together, all rendered as RGB images. The same model parameters are used for all modal-ities, including low-level feature processing; that is, there are no modality-speciﬁc initial convolutions, tokenization algorithms, or input embedding tables. We train our model using only a single task: contrastive learning, as popular-ized by CLIP [56] and ALIGN [32]. We therefore call our model CLIP-Pixels Only (CLIPPO).
We ﬁnd that CLIPPO performs similarly to CLIP-style models (within 1-2%) on the main tasks CLIP was de-signed for—image classiﬁcation and text/image retrieval—
Surpris-despite not having modality-speciﬁc towers. ingly, CLIPPO can perform complex language understand-ing tasks to a decent level without any left-to-right lan-guage modelling, masked language modelling, or explicit word-level losses. In particular, on the GLUE benchmark
[73] CLIPPO outperforms classic NLP baselines, such as
ELMO+BiLSTM+attention, outperforms prior pixel-based masked language models [60], and approaches the score of BERT [15]. Interestingly, CLIPPO obtains good perfor-mance on VQA when simply rendering the image and text together, despite never having been pretrained on such data.
Pixel-based models have an immediate advantage over regular language models because they do not require pre-determining the vocabulary/tokenizer and navigating the corresponding intricate trade-offs; consequently, we ob-serve improved performance on multilingual retrieval com-pared to an equivalent model that uses a classical tokenizer. 2.