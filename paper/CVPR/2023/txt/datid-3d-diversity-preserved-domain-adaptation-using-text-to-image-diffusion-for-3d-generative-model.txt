Abstract
Recent 3D generative models have achieved remarkable performance in synthesizing high resolution photorealistic images with view consistency and detailed 3D shapes, but training them for diverse domains is challenging since it re-quires massive training images and their camera distribution information. Text-guided domain adaptation methods have shown impressive performance on converting the 2D genera-tive model on one domain into the models on other domains with different styles by leveraging the CLIP (Contrastive
Language-Image Pre-training), rather than collecting mas-sive datasets for those domains. However, one drawback of them is that the sample diversity in the original generative model is not well-preserved in the domain-adapted genera-tive models due to the deterministic nature of the CLIP text encoder. Text-guided domain adaptation will be even more challenging for 3D generative models not only because of catastrophic diversity loss, but also because of inferior text-image correspondence and poor image quality. Here we pro-pose DATID-3D, a domain adaptation method tailored for
†Corresponding author. 3D generative models using text-to-image diffusion models that can synthesize diverse images per text prompt without collecting additional images and camera information for the target domain. Unlike 3D extensions of prior text-guided domain adaptation methods, our novel pipeline was able to fine-tune the state-of-the-art 3D generator of the source domain to synthesize high resolution, multi-view consistent images in text-guided targeted domains without additional data, outperforming the existing text-guided domain adap-tation methods in diversity and text-image correspondence.
Furthermore, we propose and demonstrate diverse 3D image manipulations such as one-shot instance-selected adapta-tion and single-view manipulated 3D reconstruction to fully enjoy diversity in text. 1.

Introduction
Recently, 3D generative models [5, 6, 13, 18, 19, 22, 31, 40–42, 59, 60, 65, 69, 74, 75] have been developed to extend 2D generative models for multi-view consistent and explic-itly pose-controlled image synthesis. Especially, some of them [5, 18, 74] combined 2D CNN generators like Style-GAN2 [28] with 3D inductive bias from the neural ren-dering [38], enabling efficient synthesis of high-resolution photorealistic images with remarkable view consistency and detailed 3D shapes. These 3D generative models can be trained with single-view images and then can sample infinite 3D images in real-time, while 3D scene representation as neural implicit fields using NeRF [38] and its variants [3, 4, 8, 10, 14, 17, 20, 32–34, 36, 45, 47, 50, 53, 54, 64, 66, 70–73] require multi-view images and training for each scene.
Training these state-of-the-art 3D generative models is challenging because it requires not only a large set of images but also the information on the camera pose distribution of those images. This requirement, unfortunately, has restricted these 3D models to the handful domains where camera pa-rameters are annotated (ShapeNetCar [7,61]) or off-the-shelf pose extractors are available (FFHQ [27], AFHQ [9, 26]).
StyleNeRF [18] assumed the camera pose distribution as either Gaussian or uniform, but this assumption is valid only for a few pre-processed datasets. Transfer learning methods for 2D generative models [30, 39, 43, 44, 48, 55, 67, 68] with small dataset can widen the scope of 3D models potentially for multiple domains, but are also limited to a handful of domains with similar camera pose distribution as the source domain in practice.
Text-guided domain adaptation methods [1,16] have been developed for 2D generative models as a promising approach to bypass the additional data curation issue for the target do-main. Leveraging the CLIP (Contrastive Language-Image
Pre-training) models [51] pre-trained on a large number of image-text pairs with non-adversarial fine-tuning strate-gies, these methods perform text-driven domain adaptation.
However, one drawback of them is the catastrophic loss of diversity inherent in a text prompt due to the determinis-tic embedding of the CLIP text encoder so that the sample diversity of the source domain 2D generative model is not preserved in the target domain 2D generative models.
We confirmed this diversity loss with experiments. A text prompt “a photo of a 3D render of a face in Pixar style” should include lots of different characters’ styles in
Pixar films such as Toy Story, Incredible, etc. However,
CLIP-guided adapted generator can only synthesize samples with alike styles as illustrated in Figure 1 (see StyleGAN-NADA∗). Thus, we confirmed that naive extensions of these for 3D generative models show inferior text-image corre-spondence and poor quality of generated images in diversity.
Optimizing with one text embedding yielded almost similar results even with different training seeds as shown in Fig-ure 2(a). Paraphrasing the text for obtaining different CLIP embeddings was also trained, but it also did not yield that many different results as illustrated in Figure 2(b). Using different CLIP encoders for a single text as in Figure 2(c) did provide different samples, but it was not an option in general since only a few CLIP encoders have been released, and retraining them requires massive servers in practice.
Figure 2. Existing text-guided domain adaptation [1, 16] did not preserve the diversity in the source domain for the target domain.
We propose a novel DATID-3D, a method of Domain
Adaptation using Text-to-Image Diffusion tailored for 3D-aware Generative Models. Recent progress in text-to-image diffusion models enables to synthesize diverse high-quality images from one text prompt [52, 56, 58]. We first lever-age them to convert the samples from the pre-trained 3D generator into diverse pose-aware target images. Then, the target images are rectified through our novel CLIP and pose reconstruction-based filtering process. Using these filtered target images, 3D domain adaptation is performed while pre-serving diversity in the text as well as multi-view consistency.
We apply our novel pipeline to the EG3D [5], a state-of-the-art 3D generator, enabling the synthesis of high-resolution multi-view consistent images in text-guided target domains as illustrated in Figure 1, without collecting additional im-ages with camera information for the target domains. Our results demonstrate superior quality, diversity, and high text-image correspondence in qualitative comparison, KID, and human evaluation compared to those of existing 2D text-guided domain adaptation methods for the 3D generative models. Furthermore, we propose one-shot instance-selected adaptation and single-view manipulated 3D reconstruction to fully enjoy diversity in the text by extending useful 2D applications of generative models. 2.