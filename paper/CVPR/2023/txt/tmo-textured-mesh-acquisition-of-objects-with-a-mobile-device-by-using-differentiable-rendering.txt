Abstract
We present a new pipeline for acquiring a textured mesh in the wild with a single smartphone which offers access to images, depth maps, and valid poses. Our method first in-troduces an RGBD-aided structure from motion, which can yield filtered depth maps and refines camera poses guided by corresponding depth. Then, we adopt the neural im-plicit surface reconstruction method, which allows for high-quality mesh and develops a new training process for apply-ing a regularization provided by classical multi-view stereo methods. Moreover, we apply a differentiable rendering to fine-tune incomplete texture maps and generate textures which are perceptually closer to the original scene. Our pipeline can be applied to any common objects in the real world without the need for either in-the-lab environments or accurate mask images. We demonstrate results of cap-tured objects with complex shapes and validate our method numerically against existing 3D reconstruction and texture mapping methods. 1.

Introduction
Recovering the 3D geometry of objects and scenes is a longstanding challenge in computer vision and is essential to a broad range of applications. Depth sensing technolo-gies range from highly specialized and expensive turn-table 3D scanners and structured-light scanners to commodity depth sensors. More recently, advances in mobile devices have developed a new method for 3D capture of real-world environments with high-resolution imaging and miniatur-ized LiDAR. Specifically, the modern smartphone such as iPhone 13 Pro are equipped with RGB camera, accelerom-eter, gyroscope, magnetometer, and LiDAR scanner. These various sensors can provide high-resolution images, low-resolution depth from the LiDAR scanner, and associated camera poses offered by off-the-shelf visual-inertial odom-etry (VIO) systems such as ARKit [1] and ARCore [2].
Today’s smartphones offer low-resolution depth maps
Figure 1. Example reconstruction results collected from a smart-phone in the wild. (a) Data acquisition setup. (b) Images captured from a smartphone. (c) A reconstructed mesh. (d) A novel view of textured mesh. Our proposed method can reconstruct the high-quality geometric mesh with a visually realistic texture.
[5] and valid poses. However, depth maps are very noisy and suffer from the limited range of depth sensors. Al-though this depth sensor can build a simple 3D structure such as a wall or floor, it cannot reconstruct objects with complex and varying shapes. Thus, the RGBD scanning methods [9, 23, 42, 57] are not suitable for these objects.
Instead of the depth sensor, multi-view stereo (MVS) al-gorithms [13, 47, 62] reconstruct high-quality 3D geometry by matching feature correspondences across different RGB images and optimizing photometric consistency. While this pipeline is very robust in real-world environments, it misses the surface of low-textured areas [62]. Additionally, the re-sulting mesh generated by post-processing like Poisson re-construction [27] heavily depends on the quality of match-ing, and the accumulated errors in correspondence matching often cause severe artifacts and missing surfaces. Because of the cumulative error from the above pipeline, the texture mapping process [53, 69] leads to undesirable results. Re-constructing high-fidelity texture and 3D geometry of real-world 3D objects remains an open challenge.
Main Results: In this paper, we present a practical method to capture a high-quality textured mesh of a 3D object in the wild, shown in Fig. 1. We first develop a smartphone app based on ARKit [1] to collect images, LiDAR depths, and poses. Although the smartphone provides quite valid pose estimates, acquiring fine detail geometry and realis-Figure 2. Example objects collected by a smartphone in the wild tic texture requires highly accurate camera poses. Thus, we present an RGBD-aided Structure from Motion (SfM) which combines the advantages of both VIO [12] and SfM
[46]. Since ARKit based on VIO is robust to the degradation of visual information such as low-textured surface, we per-form incremental triangulation with initial poses obtained from ARKit. We also propose a depth filtering method to handle noisy depth from the smartphone. These filtered depth points are exploited as an additional depth factor for bundle adjustment. Our RGBD-aided SfM can estimate highly precise camera poses due to the good initial poses and additional constraints from the filtered depth.
Our 3D geometry reconstruction process adopts a neu-ral implicit representation [50, 55] for surface reconstruc-tion with volumetric rendering. We observe that the neu-ral implicit representation can perform more complete and smoother reconstruction than the classical methods which are known to be robust for 3D reconstruction in the wild.
Furthermore, we introduce a new training method for neu-ral implicit representations. In the early stage of training, we propose a regularization method that leverages the prior information from the classical MVS method. After obtain-ing the decent shape, we avoid using the prior information and generate sparse voxel octree to enable efficient sam-pling for training. Our training method can improve the performance of the neural implicit representations. Conse-quently, we show the generalization capabilities of our sur-face reconstruction method in real world objects collected by the smartphone.
Given a mesh extracted from the trained neural implicit representation, we can run classical texture mapping algo-rithms [53, 69] to generate texture maps that often exhibit blurring artifacts and color misalignment. We propose ap-plying differential rendering [31] to fine-tune these texture maps obtained from classical texture mapping via a pho-tometric loss. Compared to classical 3D reconstruction
[23, 47, 62] and texture mapping [53, 69] approaches, our method shows a better ability to reconstruct the 3D model and produce realistic textures. We evaluate our approach on the data collected by our smartphone application. The main contributions of this paper are summarized as follows:
• We present a unified framework to reconstruct the tex-tured mesh using a smartphone.
• We propose a depth filtering scheme for noisy depths and refine initial poses from ARKit by using bundle adjustment with depth factor.
• Our pipeline builds on classical 3D reconstruction and texture mapping. We leverage a neural geometry repre-sentation to enable surface reconstruction of complex shapes and a differentiable rendering to generate high-fidelity texture maps. 2.