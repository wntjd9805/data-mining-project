Abstract
In this work, we propose an inverse rendering model that estimates 3D shape, spatially-varying reflectance, homoge-neous subsurface scattering parameters, and an environ-ment illumination jointly from only a pair of captured im-ages of a translucent object. In order to solve the ambigu-ity problem of inverse rendering, we use a physically-based renderer and a neural renderer for scene reconstruction and material editing. Because two renderers are differentiable, we can compute a reconstruction loss to assist parameter estimation. To enhance the supervision of the proposed neu-ral renderer, we also propose an augmented loss. In addi-tion, we use a flash and no-flash image pair as the input.
To supervise the training, we constructed a large-scale syn-thetic dataset of translucent objects, which consists of 117K scenes. Qualitative and quantitative results on both syn-thetic and real-world datasets demonstrated the effective-ness of the proposed model. Code and Data are available at https://github.com/ligoudaner377/homo translucent 1.

Introduction
Inverse rendering is a long-standing problem in com-puter vision, which decomposes captured images into mul-tiple intrinsic factors such as geometry, illumination, and material. With the estimated factors, many challenging ap-plications like relighting [40,47,51,60,65,72], material edit-ing [41, 52] and object manipulation [29, 57, 62, 70] become feasible. In this work, we focus on a very complicated ob-ject for inverse rendering: translucent objects. From biolog-ical tissue to various minerals, from industrial raw materials to a glass of milk, translucent objects are everywhere in our daily lives. A critical physical phenomenon in translucent objects is SubSurface Scattering (SSS): a photon penetrates the object’s surface and, after multiple bounces, is even-tually absorbed or exits at a different surface point. This non-linear, multiple bounces, multiple path process makes inverse rendering extremely ill-posed. To reduce the com-plexity of the task, we assume that the SSS parameters of the object are constant in a continuous 3-dimensional space (homogeneous subsurface scattering). So, given a translu-cent object, our task is to simultaneously estimate shape, spatially-varying reflectance, homogeneous SSS parame-ters, and environment illumination.
However, a well-known issue of inverse rendering is the ambiguity problem—the final appearance of an object re-sults from a combination of illumination, geometry, and ma-terial. For instance, a brighter area in the image may be due to the specular highlight. It may be very close to the light source; the coloration may also come from the light source or the object itself. The situation becomes more compli-cated when SSS is also considered because it is hard to tell the object intensity comes whether from the surface, sub-surface, or both.
Solutions to the ambiguity problem can be roughly di-vided into two groups. The first group of researchers addresses the ambiguity problem by providing more in-formation to the model. For example, some works use multi-view [5–7, 38, 59, 63, 69] or multi-light [8, 59] set up. The solution of the second group is based on vari-ous assumptions and simplifications. For example, some researchers simplify the reflectance model by assuming a
Lambertian reflectance [51], some simplify the illumina-tion by assuming that the object is illuminated by a sin-gle point light source [15], some simplify the geometry by assuming a near-planar [35]. For the SSS, most existing works [4, 8, 9, 12, 16, 34, 37, 45, 49, 50, 56, 58] simply ig-nore it by assuming that light does not penetrate the ob-ject’s surface, and the final appearance only depends on the pure surface reflectance. On the other hand, some re-searchers [11, 21, 22, 28, 30, 71] consider a pure SSS with-out surface reflectance. Some works [14, 17, 26, 61] use the
BSSRDF model to simplify SSS of optically thick materials in the form of complex surface reflectance. The limitation of these works is obvious: most translucent objects (e.g. wax, plastic, crystal) in our world can easily break their as-sumptions, as it contains both surface reflectance and SSS.
We tackle a more challenging problem by considering both surface reflectance and SSS of translucent objects with an arbitrary shape under an environment illumination. How-ever, using a complex scene representation means introduc-Figure 1. Inverse rendering results of real-world translucent objects. Our model takes (a) a flash image, (b) a no-flash image, and (c) a mask as input. Then, it decomposes them into (d) homogeneous subsurface scattering parameters (denoted as SSS in the figure), (e) surface normal, (f) depth, and (g) spatially-varying roughness. We use the predicted parameters to re-render the image that only considers (h) the surface reflectance, and (i) both surface reflectance and subsurface scattering. Note that (d) subsurface scattering only contains 7 parameters (3 for extinction coefficient σt, 3 for volumetric albedo α, 1 for phase function parameter g), we use these parameters to render a sphere using Mitsuba [46] for visualization. Brighter areas in the depth map represent a greater distance, and brighter areas in the roughness map represent a rougher surface. ing more ambiguity.
In this work, we propose an inverse rendering framework that handles both surface reflectance and SSS. Specifically, we use a deep neural network for parameter estimation. To enable the proposed model to train and estimate the intrinsic physical parameters more efficiently, we employ two dif-ferentiable renderers: a physically-based renderer that only considers the surface reflectance of the direct illumination and a neural renderer that creates the multiple bounce il-lumination as well as SSS effect. Two renderers work to-gether to re-render the input scene based on the estimated parameters and at the same time enable material editing. To enhance the supervision of the proposed neural renderer, we also propose an augmented loss by editing the SSS param-eters. Moreover, inspired by the recent BRDF estimation methods [2, 8] that use a flash and no-flash set up to ad-dress the problem of unpredictability in saturated highlight, we also adopt this two-shot setup not only to deal with the saturated highlight problem but also to disentangle the sur-face reflectance and SSS. To train our model, we construct a synthetic dataset consisting of more than 117K translu-cent scenes because there is no sufficient dataset supporting translucent objects. Each scene contains a human-created 3D model and is rendered with a spatially-varying micro-facet BSDF, homogeneous SSS, under an environment illu-mination.
Our contributions are summarized as follows:
• We first tackle the problem of estimating shape, spatially-varying surface reflectance, homogeneous
SSS, and illumination simultaneously from a flash and no-flash pair of captured images at a single viewpoint.
• We build a novel model that combines a physically-based renderer and a neural renderer for explicitly sep-arating the SSS and the other parameters.
• We introduce the augmented loss to train the neural renderer supervised by altered images whose SSS pa-rameters were edited.
• We construct a large-scale photorealistic synthetic dataset that consists of more than 117K scenes. 2.