Abstract
We build rearticulable models for arbitrary everyday man-made objects containing an arbitrary number of parts that are connected together in arbitrary ways via 1 degree-of-freedom joints. Given point cloud videos of such every-day objects, our method identifies the distinct object parts, what parts are connected to what other parts, and the prop-erties of the joints connecting each part pair. We do this by jointly optimizing the part segmentation, transformation, and kinematics using a novel energy minimization frame-work. Our inferred animatable models, enables retargeting to novel poses with sparse point correspondences guidance.
We test our method on a new articulating robot dataset, and the Sapiens dataset with common daily objects. Ex-periments show that our method outperforms two leading prior works on various metrics. 1.

Introduction
Consider the sequence of points clouds observations of articulating everyday objects shown in Figure 1. As hu-*equal advising, alphabetic order
Figure 2. Many man-made everyday objects can be explained with rigid parts connected in a kinematic tree with 1DOF joints. mans, we can readily infer the kinematic structure of the underlying object, i.e. the different object parts and their connectivity and articulation relative with one another [21].
This paper develops computational techniques with similar abilities. Given point cloud videos of arbitrary everyday objects (with an arbitrary number of parts) undergoing ar-ticulation, we develop techniques to build animatable 3D re-constructions of the underlying object by a) identifying the distinct object parts, b) inferring what parts are connected to what other parts, and c) the properties of the joint be-tween each connected part pair. Success at this task enables
Arbitrary Realistic Joint Arbitrary
Constraints Kinematics
Parts
Category-specific e.g. people [31] quadrupeds [51, 61] cartoons [53]
DeepPart [57]
NPP [11]
ScrewNet [17]
UnsupMotion [42]
Ditto [20]
MultiBodySync [15]
WatchItMove [35]
Ours no no no yes yes yes no yes yes yes yes yes yes yes no no yes yes yes no no yes no no no no no no no no no yes yes
Table 1. Most past work on inferring rearticulable models is cat-egory specific. Building rearticulable models for arbitrary every-day man-made objects requires reasoning about arbitrary part ge-ometries, arbitrary part connectivity, and realistic joint constraints (1DOF w.r.t. parent part). We situate past work along these 3 di-mensions, and discuss major trends in Sec. 2. rearticulation of objects. Given just a few user clicks spec-ifying what point goes where, we can fill in the remaining geometry as shown on the right side in Figure 1.
Most past work on inferring how objects articulate tack-les it in a category-specific manner, be it for people [31, 36, 40], quadrupeds [51, 61], or even everyday objects [34].
Category-specific treatment allows the use of specialized shape models (such as the SMPL model [31] for people), or defines the output space (e.g. location of 2 hinge joints for the category eye-glasses). This limits applicability of such methods to categories that have a canonical topology, leaving out categories with large intra-class variation (e.g. boxes that can have 1-4 hinge joints), or in-the-wild objects which may have an arbitrary number of parts connected in arbitrary ways (e.g. robots).
Only a very few past works tackle the problem of in-ferring rearticulable models in a category-agnostic manner.
Huang et al. [15] only infer part segmentations, which by itself, is insufficient for rearticulation. Jiang et al. [20] only consider a single 1-DOF joint per object, dramatically re-stricting its application (think about a humanoid robot with four limbs, but the articulable model can only articulate one). Noguchi et al. [35] present the most complete solution but instead work with visual input and don’t incorporate the 1DOF constraint, i.e. a part can only rotate or translate about a fixed axis on the parent part, common to a large number of man-made objects as can be seen in Fig. 2.
Inferring 3DOF / 6DOF joints leads to unrealistic rearticulation and is thus undesirable (consider the leg of eyeglasses can freely move or rotate). Our work fills this gap in the literature. Our method extracts 3D rearticulable models for arbitrary every-day objects (containing an arbitrary number of parts that are connected together in arbitrary ways via 1DOF joint) from point cloud videos. To the best of our knowledge, this is the first work to tackle this specific problem.
Our proposed method jointly reasons about part geome-tries and their 1-DOF inter-connectivity with one another.
At the heart of our approach is a novel continuous-discrete energy formulation that seeks to jointly learn parameters of the object model (i.e. assignments of points in the canon-ical view to parts, and the connectivity of parts to one an-other) by minimizing shape and motion reconstruction error (after appropriate articulation of the inferred model) on the given views. As it is difficult to directly optimize in the presence of continuous and discrete variables with struc-tured constraints, we first estimate a relaxed model that in-fers parts that are free to follow an arbitrary 6DOF trajec-tory over time (i.e. doesn’t require parts to be connected in a kinematic tree with 1DOF joints). We project the es-timated relaxed model to a kinematic model and continue to optimize with the reconstruction error to further finetune the estimated joint parameters. Our joint approach leads to better models and improved rearticulation as compared to adaptations of past methods [15, 35] to this task. 2.