Abstract
Generating synthetic images of handwritten text in a writer-specific style is a challenging task, especially in the case of unseen styles and new words, and even more when these latter contain characters that are rarely encountered during training. While emulating a writer’s style has been recently addressed by generative models, the generalization towards rare characters has been disregarded. In this work, we devise a Transformer-based model for Few-Shot styled handwritten text generation and focus on obtaining a ro-bust and informative representation of both the text and the style. In particular, we propose a novel representation of the textual content as a sequence of dense vectors obtained from images of symbols written as standard GNU Unifont glyphs, which can be considered their visual archetypes.
This strategy is more suitable for generating characters that, despite having been seen rarely during training, pos-sibly share visual details with the frequently observed ones.
As for the style, we obtain a robust representation of unseen writers’ calligraphy by exploiting specific pre-training on a large synthetic dataset. Quantitative and qualitative results demonstrate the effectiveness of our proposal in generat-ing words in unseen styles and with rare characters more faithfully than existing approaches relying on independent one-hot encodings of the characters. 1.

Introduction
Styled handwritten text generation (HTG) is an emerg-ing research area aimed at producing writer-specific hand-written text images mimicking their calligraphic style [7, 14, 29]. The practical applications of this research topic range from the synthesis of high-quality training data for personalized Handwritten Text Recognition (HTR) mod-els [5, 6, 8, 27, 28, 48] to the automatic generation of hand-written notes for physically impaired people. Moreover, the writer-specific style representations that can be obtained as a by-product of models designed for this task can be applied to other tasks such as writer identification, signature verifi-Figure 1. Different from previous approaches that use indepen-dent one-hot vectors as input text tokens (e.g., the State-of-the-Art
HWT [7]), we exploit visual archetypes, i.e., geometrically-related binary images of characters. By resorting to similarities between the archetypes, we are able to generate both characters that are rarely seen during training (highlighted in red) and frequently ob-served ones more faithfully. cation, and handwriting style manipulation. When focus-ing on styled handwriting generation, simply adopting style transfer is limiting. In fact, imitating a specific writer’s cal-ligraphy does not only concern texture (e.g., the color and texture of background and ink), nor just stroke thickness, slant, skew, and roundness, but also single characters shape and ligatures. Moreover, these visual aspects must be han-dled properly to avoid artifacts that might result in content change (e.g., even small additional or missing strokes).
In sight of this, specific approaches have been designed for HTG. The handwriting can be handled in the form of a trajectory (made of the underlying strokes), as done in [1, 2, 19, 24, 31], or of an image that captures its appear-ance, as done in [3,7,11,14–16,20,29,32,35,36,39,43,45].
The former approaches adopt online HTG strategies that en-tail predicting the pen trajectory point-by-point, while the
latter ones are offline HTG models that output entire text images directly. We follow the offline HTG paradigm since it has the advantage, over the online one, of not requiring costly pen-recording training data, and thus, being applica-ble also to scenarios where the information on online hand-writing is not available for a specific author (e.g., in the case of historical data) and being easier to train for not suffering of vanishing gradient and being parallelizable.
Specifically, in this work, we focus on the Few-Shot styled offline HTG task, in which we have just a few exam-ple images of the writer’s style to mimic. State-of-the-Art (SotA) approaches tackling this scenario feature an encoder that extracts writer-specific style features and a generative component, which is fed with the style features and the con-tent representations, and produces styled text images condi-tioned on the desired content. These approaches usually exploit Generative Adversarial Networks (GANs [18, 40]), for example [3, 11, 14–16, 29, 32, 36]. A more recent ap-proach [7] is based on an encoder-decoder generative Trans-former model [44] that captures character-level style varia-tions better than previous GAN-based strategies thanks to the cross-attention mechanism between style representation and content tokens. In the approaches mentioned above, the encoding of the text content is obtained by starting from one-hot vectors, each representing a different character in a fixed charset. In this way, the characters are all independent by design. Thus, possible geometric and visual similarity among them cannot be modeled nor exploited for genera-tion, which might result in a quality gap between the images generated by these approaches for characters that are highly represented in the training set and rare ones (i.e., long-tail characters). Moreover, for computational tractability, the fixed charset that the approaches relying on a one-hot rep-resentation of text tokens can handle is relatively small.
Contribution. Our proposed approach entails represent-ing characters as continuous variables and using them as query content vectors of a Transformer decoder for gener-ation. In this way, the generation of characters appearing rarely in the training set (such as numbers, capital letters, and punctuation) is eased by exploiting the low distance in the latent space between rare symbols and more frequent ones (see Figure 1). In particular, we start from the GNU
Unifont font and render each character as a 16×16 binary image, which can be considered as the visual archetype of that character. Then, we learn a dense encoding of the char-acter images and feed such encodings to a Transformer de-coder as queries to attend the style vectors extracted by a
Transformed encoder. Note that, by resorting to charac-ter images rendered in the richer GNU Unifont, which is the most complete in terms of contained Unicode charac-ters, we can handle a huge charset (more than 55k charac-ters) seamlessly, i.e., without the need for additional param-eters, as it is the case for the commonly-adopted one-hot encoding. Moreover, as for the style encoding part, we ex-ploit a backbone to represent the style example images that has been pre-trained on a large synthetic dataset specifically built to focus on the calligraphic style attributes. This strat-egy, widely adopted for other tasks, is usually disregarded in HTG. Nonetheless, we demonstrate its effectiveness in leading to strong style representations, especially for unseen styles. We validate our proposal with extensive experimen-tal comparison against recent generative SotA approaches, both quantitatively and qualitatively, and demonstrate the effectiveness of our proposal in generating words with both common and rare characters and in both seen and unseen styles. We call our approach VATr: Visual Archetypes-based Transformer. The code and trained models are avail-able at https://github.com/aimagelab/VATr. 2.