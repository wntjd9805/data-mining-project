Abstract
Understanding the motion behavior of dynamic environ-ments is vital for autonomous driving, leading to increas-ing attention in class-agnostic motion prediction in LiDAR point clouds. Outdoor scenes can often be decomposed into mobile foregrounds and static backgrounds, which enables us to associate motion understanding with scene parsing.
Based on this observation, we study a novel weakly su-pervised motion prediction paradigm, where fully or par-tially (1%, 0.1%) annotated foreground/background binary masks are used for supervision, rather than using expen-sive motion annotations. To this end, we propose a two-stage weakly supervised approach, where the segmentation model trained with the incomplete binary masks in Stage1 will facilitate the self-supervised learning of the motion prediction network in Stage2 by estimating possible mov-ing foregrounds in advance. Furthermore, for robust self-supervised motion learning, we design a Consistency-aware
Chamfer Distance loss by exploiting multi-frame informa-tion and explicitly suppressing potential outliers. Compre-hensive experiments show that, with fully or partially bi-nary masks as supervision, our weakly supervised models surpass the self-supervised models by a large margin and perform on par with some supervised ones. This further demonstrates that our approach achieves a good compro-mise between annotation effort and performance. 1.

Introduction
Understanding the dynamics of surrounding environ-ments is vital for autonomous driving [27]. Particularly, motion prediction, which generates the future positions of objects from previous information, plays an important role in path planning and navigation.
Classical approaches [7, 9, 47] achieve motion predic-tion by object detection, tracking, and trajectory forecast-∗Corresponding author: G. Lin. (e-mail: gslin@ntu.edu.sg )
Figure 1.
Illustration of our weak supervision concept. Out-door scenes can be decomposed into mobile foregrounds and static backgrounds, which enables us to achieve motion learning with fully or partially annotated FG/BG masks as weak supervision to replace expensive ground truth motion data. ing. These detection-based approaches may fail when en-countering unknown categories not included in training data [42]. To address this issue, many approaches [8,41,42] propose to directly estimate class-agnostic motion from bird’s eye view (BEV) map of point clouds and achieve a good trade-off between accuracy and computational cost.
However, sensors can not capture motion information in complex environments [27], which makes motion data scarce and expensive. Therefore, most existing real-world motion data are produced by semi-supervised learning methods with auxiliary information, e.g., KITTI [10, 27], or bootstrapped from human-annotated object detection and
tracking data, e.g., Waymo [16]. To circumvent the de-pendence on motion annotations, PillarMotion [26] utilizes point clouds and camera images for self-supervised motion learning. Although achieving promising results, there is still a large performance gap between the self-supervised method, PillarMotion, and fully supervised methods.
Outdoor scenes can often be decomposed into moving objects and backgrounds [27], which enables us to asso-ciate motion understanding with scene parsing. As shown in Fig. 1 (a) and (b), with ego-motion compensation, motion only exists in foreground points. Therefore, if we distin-guish the mobile foregrounds from the static backgrounds, we can focus on mining valuable dynamic motion super-vision from these potentially moving foreground objects, leading to more effective self-supervised motion learning.
Based on this intuition, we propose a novel weakly super-vised paradigm, where expensive motion annotations are replaced by fully or partially (1%, 0.1%) annotated fore-ground/background (FG/BG) masks to achieve a good com-promise between annotation effort and performance. To this end, we design a two-stage weakly supervised motion prediction approach, where we train a FG/BG segmenta-tion network with partially annotated masks in Stage1 and train a motion prediction network in Stage2. Speciﬁcally, in
Stage2, the segmentation network from Stage1 will gener-ate foreground points for training samples, so that the mo-tion prediction network can be trained on these foreground points in a self-supervised manner.
In self-supervised 3D motion learning [18, 26, 44],
Chamfer distance (CD) is preferred. However, the CD is sensitive to outliers [37]. Unfortunately, outliers are common in our setting. This is partly due to the view-changes, occlusions, and noise of point clouds and also due to the possible errors in the FG points estimated by the FG/BG segmentation network. To alleviate the impact of outliers, we propose a novel Consistency-aware Cham-fer Distance (CCD) loss. Different from the typical CD loss, our CCD loss exploits supervision from multi-frame point clouds and leverages multi-frame consistency to mea-sure the conﬁdence of points. By assigning uncertain points lower weights, our CCD loss suppresses potential outliers.
Our main contributions can be summarized as follows:
• Without using expensive motion data, we propose a weakly supervised motion prediction paradigm with fully or partially annotated foreground/background (FG/BG) masks as supervision to achieve a good com-promise between annotation effort and performance.
To the best of our knowledge, this is the ﬁrst work on weakly supervised class-agnostic motion prediction.
• By associating motion understanding with scene pars-ing, we present a two-stage weakly supervised mo-tion prediction approach, where the FG/BG segmen-tation generated from Stage1 will facilitate the self-supervised motion learning in Stage2.
• We design a novel Consistency-aware Chamfer Dis-tance loss, where multi-frame information is used to suppresses potential outliers for robust self-supervised motion learning.
• With FG/BG masks as weak supervision, our weakly supervised models outperform the self-supervised models by a large margin, and performs on par with some supervised ones. 2.