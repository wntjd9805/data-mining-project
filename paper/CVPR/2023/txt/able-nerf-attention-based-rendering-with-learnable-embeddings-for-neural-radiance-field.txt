Abstract
Neural Radiance Field (NeRF) is a popular method in representing 3D scenes by optimising a continuous volumet-ric scene function. Its large success which lies in applying volumetric rendering (VR) is also its Achilles’ heel in pro-ducing view-dependent effects. As a consequence, glossy and transparent surfaces often appear murky. A remedy to reduce these artefacts is to constrain this VR equation by excluding volumes with back-facing normal. While this approach has some success in rendering glossy surfaces, translucent objects are still poorly represented. In this pa-per, we present an alternative to the physics-based VR ap-proach by introducing a self-attention-based framework on volumes along a ray. In addition, inspired by modern game engines which utilise Light Probes to store local lighting passing through the scene, we incorporate Learnable Em-beddings to capture view dependent effects within the scene.
Our method, which we call ABLE-NeRF, significantly re-duces ‘blurry’ glossy surfaces in rendering and produces realistic translucent surfaces which lack in prior art. In the
Blender dataset, ABLE-NeRF achieves SOTA results and surpasses Ref-NeRF in all 3 image quality metrics PSNR,
SSIM, LPIPS. 1.

Introduction
Neural Radiance Field (NeRF) has become the de facto method for 3D scene representation. By representing the scene as a continuous function, NeRF is able to generate photo-realistic novel view images by marching camera rays through the scene. NeRF first samples a set of 3D points along a camera ray and outputs its outgoing radiance. The final pixel colour of a camera ray is then computed us-ing volumetric rendering (VR) which colours are alpha-composited. This simple approach allows NeRF to gen-erate impressive photo-realistic novel views of a complex 3D scene. However, NeRF is unable to produce accurate colours of objects with view-dependent effects. Colours of
Figure 1. We illustrate two views of the Blender ’Drums’ Scene.
The surface of the drums exhibit either a translucent surface or a reflective surface at different angles. As shown, Ref-NeRF model has severe difficulties interpolating between the translu-cent and reflective surfaces as the viewing angle changes. Our method demonstrates its superiority over NeRF rendering models by producing such accurate view-dependent effects. In addition, the specularity of the cymbals are rendered much closer to ground truth compared to Ref-NeRF. translucent objects often appear murky and glossy objects have blurry specular highlights. Our work aims to reduce these artefacts.
The exhibited artefacts of the NeRF rendering model is largely due to the inherent usage of VR as features are ac-cumulated in the colour space. Variants of NeRF attempt to tackle this defect by altering the basis of this VR equation.
For instance, Ref-NeRF first predicts the normal vector of each point on the ray. If a point has a predicted normal fac-ing backwards from the camera, its colour is excluded from computation via regularisation. However, prediction of nor-mals in an object’s interior is ill-posed since these points are not on actual surfaces. As a consequence, Ref-NeRF achieves some success over the baseline NeRF model, al-beit imperfectly.
When rendering translucent objects with additional specular effects, NeRF and its variants suffer from the same deficiency. This is due to the computation of σ which is analogous to the ‘opacity’ attribute of a point used in the VR equation. It is also related to the point’s transmissivity and its contribution of radiance of to its ray. As per the Fresnel effect [5], this property should depend on viewing angles.
Similarly, [19] describes a notion of ‘alphasphere’, which describes an opacity hull of a point that stores an opacity value viewed at direction ω. Most NeRF methods disregard the viewing angle in computing σ.
In fig. 1, the surface of the uttermost right drum in the Blender scene exhibits changing reflective and translucent properties at different viewing angles. Ref-Nerf and other variants, by discount-ing the dependency of σ on viewing angle, may not render accurate colours of such objects.
Additionally, learning to model opacity and colour sepa-rately may be inadequate in predicting the ray’s colour. Ac-cumulating high-frequency features directly in the colour space causes the model to be sensitive to both opacity and sampling intervals of points along the ray. Therefore we re-work how volumetric rendering can be applied to view syn-thesis. Inspecting the VR equation reveals that this method-ology is similar to a self-attention mechanism; a point’s contribution to its ray colour is dependent on points lying in-front of it. By this principle we designed ABLE-NeRF as an attention-based framework. To mimic the VR equa-tion, mask attention is applied to points, preventing them from attending to others behind it.
The second stage of ABLE-NeRF takes inspiration from modern game engines in relighting objects by invoking
In a form of memorisation framework called ‘baking’. practice, traditional computer graphics rendering methods would capture indirect lighting by applying Monte Carlo path tracing to cache irradiance and then apply interpola-tion during run-time. Similarly, game engines would use lightmaps to cache global illumination for lower computa-tional costs. For relighting dynamic objects, localised light probes are embedded in the scene to capture light passing through free space. At run-time, moving objects query from these light probes for accurate relighting. The commonal-ity between all these approaches is the process of ‘memo-rising’ lighting information and interpolating them during run time for accurate relighting. As such, we take inspi-ration from these methods by creating a memorisation net-work for view synthesis. Given a static scene, we incorpo-rate Learnable Embeddings (LE), which are learnable mem-ory tokens, to store scene information in latent space during training. Specifically, the LE attends to points sampled dur-ing ray casting via cross-attention to memorise scene infor-mation. To render accurate view dependent effects a direc-tional view token, comprising of camera pose, would de-code from these embeddings.
ABLE-NeRF provides high quality rendering on novel view synthesis tasks. The memorisation network achieves significant improvements in producing precise specular ef-fects over Ref-NeRF. Moreover, by reworking volumetric rendering as an attention framework, ABLE-NeRF renders much more accurate colours of translucent objects than prior art. On the blender dataset, ABLE-NeRF excels both quantitatively and qualitatively relative to Ref-NeRF.
In summary, our technical contributions are: (1) An approach demonstrating the capability and superi-ority of transformers modelling a physics based volumetric rendering approach. (2) A memorisation based framework with Learnable
Embeddings (LE) to capture and render detailed view-dependent effects with a cross-attention network. 2.