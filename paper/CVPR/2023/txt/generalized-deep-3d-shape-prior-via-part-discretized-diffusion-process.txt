Abstract
We develop a generalized 3D shape generation prior model, tailored for multiple 3D tasks including uncondi-tional shape generation, point cloud completion, and cross-modality shape generation, etc. On one hand, to pre-cisely capture local fine detailed shape information, a vec-tor quantized variational autoencoder (VQ-VAE) is utilized to index local geometry from a compactly learned code-book based on a broad set of task training data. On the other hand, a discrete diffusion generator is introduced to model the inherent structural dependencies among different tokens. In the meantime, a multi-frequency fusion module (MFM) is developed to suppress high-frequency shape fea-ture fluctuations, guided by multi-frequency contextual in-formation. The above designs jointly equip our proposed 3D shape prior model with high-fidelity, diverse features as well as the capability of cross-modality alignment, and extensive experiments have demonstrated superior perfor-mances on various 3D shape generation tasks. 1.

Introduction
While pre-trained 2D prior models [24, 43] have shown great power in various downstream vision tasks such as im-age classification, editing and cross-modality generation, etc., their counterpart 3D prior models which are gener-ally beneficial for three-dimensional shape generation tasks have NOT been well developed, unfortunately. On the contrary, the graphics community has developed a num-ber of task-specific pre-trained models, tailored for unary tasks such as 3D shape generation [22, 31, 60], points cloud completion [62, 66, 67] and conditional shape pre-diction [14, 30, 34, 54]. Since above individual 3D gen-erative representations do NOT share common knowledge among tasks, to migrate a trained 3D shape network from one task to another related one requires troublesome end-to-end model re-work and training resources are also wasted.
For instance, a good shape encoding of “chairs” based on a general prior 3D model could benefit shape completion of a given partial chair and text-guided novel chair generation.
†Corresponding author: Bingbing Ni.
Figure 1. Our shape generation model is a unified and efficient prior model to produce high-fidelity, diverse results on multiple tasks, while most previous approaches are task-specific.
This work aims at designing a unified 3D shape gener-ative prior model, which serves as a generalized backbone for multiple downstream tasks, i.e., with few requirements for painstaking adaptation of the prior model itself. Such a general 3D prior model should possess the following good properties. On one hand, it should be expressive enough to generate high-fidelity shape generation results with fine-grained local details. On the other hand, this general prior should cover a large probabilistic support region so that it could sample diverse shape prototypes in both conditional and unconditional generation tasks. Moreover, to well sup-port cross-modal generation, e.g., text-to-shape, the sam-pled representation from the prior model should achieve good semantic consistency between different modalities, making it easy to encode and match partial shapes, images and text prompts.
The above criteria, however, are rarely satisfied by exist-ing 3D shape modeling approaches. Encoder-decoder based structures [62, 66] usually focus on dedicated tasks and fail to capture diverse shape samples because the generation process mostly relies on features sampled from determinis-tic encoders. On the contrary, probabilistic models such as
GAN [29, 48], Flow [60] and diffusion models [22, 31, 67],
cannot either adapt to multiple tasks flexibly or generate high-quality shapes without artifacts [64]. Note that re-cent models such as AutoSDF [34] and Lion [64] explore multiple conditional generative frameworks. However, Au-toSDF [34] is defective in diversity and shows mode col-lapse in unconditional and text-guided generation, while training and inference of Lion [64] are costly.
In view of above limitations, this work proposes an efficient 3D-Disassemble-Quantization-and-Diffusion (3DQD) prior model to simultaneously address above chal-lenges in 3D shape generation: high-fidelity, diversity, and good cross-modality alignment ability, as shown in Fig. 1.
This 3DQD prior is a unified, probabilistic and powerful backbone for both unconditional shape generation and mul-tiple conditional shape completion applications. On one hand, instead of using holistic codes for whole-shape, we adopt a vector quantized variational autoencoder [52] (VQ-VAE) that learns a compact representation for disassembled local parts of the shape, which is expected to well repre-sent diverse types of local geometry information out of a broad set of training shapes from shared tasks, forming a generalized part-level codebook. Note that this disassem-bled and quantized representation effectively eliminates the intrinsic structural bias between different modalities, and thus is able to perform good cross-modality data alignment on conditional generation tasks. On the other hand, a dis-crete diffusion generator [4, 16] with reverse Markov chain is introduced to model the inherent semantic dependen-cies among geometric tokens. Namely, the forward process corrupts the latent variables with progressively increasing noise, transferring parts of variables into random or masked ones; and the reverse process gradually recovers the vari-ables towards the desired data distribution by learning the structural connections among geometry tokens. It is worth mentioning that random corruption in the forward process facilitates diverse samples, while discrete embedding re-sults in a dramatic cost-down in computational budget, with stable and iterative sampling. Furthermore, during shape generation we introduce Multi-frequency Fusion Module (MFM) to suppress high-frequency outliers, encouraging smoother and high-fidelity samples.
Comprehensive and various downstream experiments on shape generation tasks demonstrate that the proposed 3DQD prior is capable of helping synthesize high-fidelity, diverse shapes efficiently, outperforming multiple state-of-the-art methods. Furthermore, our 3DQD prior model can serve as a generalized backbone with highly competitive samples for extended applications and cross-domain tasks requiring NO or little tuning. 2.