Abstract
Recently, finetuning pretrained vision-language models (VLMs) has been a prevailing paradigm for achieving state-of-the-art performance in VQA. However, as VLMs scale, it becomes computationally expensive, storage inefficient, and prone to overfitting when tuning full model parame-ters for a specific task in low-resource settings. Although current parameter-efficient tuning methods dramatically re-duce the number of tunable parameters, there still exists a significant performance gap with full finetuning. In this pa-per, we propose MixPHM, a redundancy-aware parameter-efficient tuning method that outperforms full finetuning in low-resource VQA. Specifically, MixPHM is a lightweight module implemented by multiple PHM-experts in a mixture-of-experts manner. To reduce parameter redundancy, we reparameterize expert weights in a low-rank subspace and share part of the weights inside and across MixPHM. More-over, based on our quantitative analysis of representa-tion redundancy, we propose Redundancy Regularization, which facilitates MixPHM to reduce task-irrelevant redun-dancy while promoting task-relevant correlation. Experi-ments conducted on VQA v2, GQA, and OK-VQA with dif-ferent low-resource settings show that our MixPHM out-performs state-of-the-art parameter-efficient methods and is the only one consistently surpassing full finetuning. 1.

Introduction
Adapting pretrained vision-language models (VLMs) [4, 5, 24, 29, 30, 50, 57] to the downstream VQA task [1] in a finetuning manner has emerged as a dominant paradigm to achieve state-of-the-art performance. As the scale of VLMs continues to grow, finetuning the full model with millions or billions of parameters causes a substantial rise in com-putation and storage costs, as well as exposing the over-fitting (poor performance) issue in low-resource learning.
Parameter-efficient tuning methods [15, 16, 23, 38, 51, 56],
*Corresponding author.
Figure 1. Comparison between parameter-efficient methods.
In a low-resource setting (i.e., with 64 training samples), we show the average score across five seeds on VQA v2 (y-axis) and the percentage of tunable parameters w.r.t. pretrained VL-T5 (x-axis). updating only a tiny number of original parameters of pre-trained models or the newly-added lightweight modules, are thus proposed to handle such challenges.
However, as illustrated in Figure 1, the aforementioned parameter-efficient tuning methods substantially reduce the number of tunable parameters, but their performance still lags behind full finetuning. Among them, the adapter-based methods (Houlsby [15], Pfeiffer [38], Compacter [23], and
AdaMix [51]) are more storage-efficient, as they only store newly-added modules instead of a copy of entire VLMs, and they allow more flexible parameter sharing [43].
In particular, AdaMix enhances the capacity of adapters with a mixture-of-experts (MoE) [42] architecture and achieves comparable performance to full finetuning while slightly in-creasing the number of tunable parameters.
In this paper, we build upon adapter-based methods to in-vestigate more parameter-efficient tuning methods that can outperform full finetuning on low-resource VQA. Specif-ically, when adapting pretrained VLMs to the given task, we consider two improvements: (i) Reducing parameter re-dundancy while maintaining adapter capacity. However, an excessive reduction of tunable parameters can lead to un-derfitting, preventing adapters from learning enough task-relevant information [23]. Therefore, it is crucial to strike
a compromise between parameter efficiency and capacity. (ii) Reducing task-irrelevant redundancy while promoting task-relevant correlation in representations. Practically, through residual connection, adapters integrate task-specific information learned from a target dataset and prior knowl-edge already implied in pretrained VLMs. However, recent works [21, 33, 48] have suggested that pretrained models inevitably contain redundant and irrelevant information for target tasks, resulting in a statistically spurious correlation between representations and labels, thereby hindering per-formance and generalization [46, 49]. To improve their ef-fectiveness, we thus expect adapters to learn as much task-relevant information as possible while discarding the task-irrelevant information from versatile pretrained VLMs.
To this end, we propose MixPHM, a redundancy-aware parameter-efficient tuning method, which can efficiently re-duce the tunable parameters and task-irrelevant redundancy, and promote task-relevant correlation in representations.
MixPHM is implemented with multiple PHM-experts in a
MoE fashion. To reduce (i) parameter redundancy in Mix-PHM, we first decompose and reparameterize the expert weights into a low-rank subspace. Afterwards, we further reduce the number of parameters and transfer information with global and local weight sharing. To achieve the im-provement (ii), we first quantify representation redundancy in adapter. The result shows that representations of adapters are redundant with representations of pretrained VLMs but exhibit limited correlation with the final task-used repre-sentations. Inspired by this insight, we then propose Re-dundancy Regularization. In MixPHM, the regularizer re-duces task-irrelevant redundancy via decorrelating the sim-ilarly matrix between representations learned by MixPHM and representations obtained by pretrained VLMs. Simulta-neously, it promotes task-relevant correlation by maximiz-ing the mutual information between the learned representa-tions and the final task-used representations.
We conduct extensive experiments on three datasets, i.e.,
VQA v2 [11], GQA [19], and OK-VQA [36]. The pro-posed MixPHM consistently outperforms full finetuning and state-of-the-art parameter-efficient tuning methods. To gain more insights, we discuss the generalizability of our method and the effectiveness of its key components. Our contributions are summarized as follows: (1) We propose
MixPHM, a redundancy-aware parameter-efficient tuning method that outperforms full finetuning in adapting pre-trained VLMs to low-resource VQA. (2) We quantitatively analyze representation redundancy and propose redundancy regularization, which can efficiently reduce task-irrelevant redundancy while prompting task-relevant correlation. (3)
Extensive experiments show that MixPHM achieves a bet-ter trade-off between performance and parameter efficiency, and a significant performance improvement over current parameter-efficient tuning methods. 2.