Abstract
In the fashion domain, there exists a variety of vision-and-language (V+L) tasks, including cross-modal retrieval, text-guided image retrieval, multi-modal classiﬁcation, and image captioning. They differ drastically in each individ-ual input/output format and dataset size. It has been com-mon to design a task-speciﬁc model and ﬁne-tune it in-dependently from a pre-trained V+L model (e.g., CLIP).
This results in parameter inefﬁciency and inability to ex-ploit inter-task relatedness. To address such issues, we pro-pose a novel FAshion-focused Multi-task Efﬁcient learn-ing method for Vision-and-Language tasks (FAME-ViL) in this work. Compared with existing approaches, FAME-ViL applies a single model for multiple heterogeneous fashion tasks, therefore being much more parameter-efﬁcient.
It is enabled by two novel components: (1) a task-versatile architecture with cross-attention adapters and task-speciﬁc adapters integrated into a uniﬁed V+L model, and (2) a sta-ble and effective multi-task training strategy that supports learning from heterogeneous data and prevents negative transfer. Extensive experiments on four fashion tasks show that our FAME-ViL can save 61.5% of parameters over alternatives, while signiﬁcantly outperforming the conven-tional independently trained single-task models. Code is available at https://github.com/BrandonHanx/FAME-ViL. 1.

Introduction
A variety of real-world multi-modal, particularly Vision-and-Language (V+L) tasks exist in the fashion domain, in-cluding multi-modal recognition [44, 53, 61], multi-modal retrieval [21, 83] and image captioning [85]. The models developed for these tasks have been applied in diverse e-commerce applications, improving product discoverability, seller-buyer engagement, and customer conversion rate af-ter catalogue browsing. Intrinsically, those V+L tasks are
Figure 1. By multi-task learning a single model for heterogeneous fashion tasks, our FAME-ViL can signiﬁcantly improve parameter efﬁciency, while boosting the model performance per task over existing independently ﬁne-tuned single-task models. Note, each axis is normalized according to the respective maximum value for easier visualization. heterogeneous in terms of (1) different input and output formats (e.g., text-guided garment retrieval [83] and image captioning [85] have completely different inputs and out-puts); (2) different dataset sizes as the annotation difﬁculty of each task differ (e.g., the labeling effort for text-guided image retrieval is much harder than that for text-to-image retrieval [48, 83]).
Due to the heterogeneous nature of the V+L fashion tasks, existing methods [21, 24, 33, 87, 94] typically take a pre-trained generic V+L model [7, 38, 41–43, 49, 60, 67, 72, 79] and ﬁne-tune it on every single task independently.
Such an approach suffers from two limitations. (1) Low pa-rameter efﬁciency: Each real-world application requires the deployment of its dedicated ﬁne-tuned model, where there is no parameter or inference computation sharing. This leads to a linearly increasing storage and inference com-pute redundancy in the long run. (2) Lack of inter-task relatedness: Though the fashion tasks are heterogeneous
in nature, the fundamental components of the models are closely related in that all tasks require a deep content (im-age/sentence) understanding. Exploiting the shared infor-mation across tasks thus has the potential to improve model generalization capability leading to a performance boost.
Perhaps a natural solution would be applying Multi-Task
Learning (MTL) [13]. However, most existing multi-task training methods [8, 36, 46, 56, 63] are designed for homo-geneous tasks (i.e., one dataset with multi-task labels) and thus cannot be directly applied to the heterogeneous fashion tasks. In our case, we are facing two challenges in build-ing the fashion-domain MTL model: (1) Architecturally, it is non-trivial to model the diverse tasks in one uniﬁed ar-chitecture. Taking the popular CLIP [60] as an example, its two-stream architecture is designed for image-text align-ment [52] and thus lacks the modality fusion mechanism as required by many V+L fashion tasks (e.g., text-guided im-age retrieval [2,83] and image captioning [85]). (2) In terms of optimization, a fashion-domain MTL model is prone to the notorious negative transfer problem [8,13,36,46,56,63] due to both task input/output format differences and imbal-anced dataset sizes. To the best of our knowledge, there has been no attempt at V+L MTL for the fashion domain.
In this work, we introduce a novel FAshion-focused
Multi-task Efﬁcient learning method for various Vision-and-Language based fashion tasks, dubbed as FAME-ViL.
It achieves superior performance across a set of diverse fashion tasks with much fewer parameters as in Fig. 1.
Speciﬁcally, we design a task-versatile architecture on top of a pre-trained generic V+L model (i.e., CLIP [60]). To adapt the simple two-stream architecture of CLIP to various fashion tasks, we introduce a lightweight Cross-Attention
Adapter (XAA) to enable the cross-modality interaction be-tween the two streams.
It makes the model ﬂexible to support multiple task modes (e.g., contrastive mode for retrieval, fusion mode for understanding, and generative mode for generation). To address the negative transfer chal-lenge, we introduce a Task-Speciﬁc Adapter (TSA) to ab-sorb inter-task input/output format incompatibilities by in-troducing lightweight additional per-task parameters. For further handling the dataset imbalance problem, a multi-teacher distillation scheme [12] is formulated for our het-erogeneous MTL problem. It leverages the pre-trained per-task teachers to guide the optimization of our multi-task model, mitigating the overﬁtting risks of those tasks with smaller training dataset sizes.
Our contributions are summarized as follows: (I) For the
ﬁrst time, we investigate the problem of multi-task learning on heterogeneous fashion tasks, eliminating the parameter redundancy and exploiting the inter-task relatedness. (II)
We propose FAME-ViL with two novel adapters, adapting a pre-trained CLIP model to all tasks. (III) We introduce an efﬁcient and effective multi-task training strategy sup-Cross-Modal Retrieval (XMR)
Text Query: Long sleeve relaxed-fit silk blazer in light peach. Shawl collar. Single-button  closure and patch pockets at front. Breast pocket. Slits at sleeve cuffs. Vented at back. 
Reference Image                    Modifying Text: is a black and white dress, is strapless
Text-Guided Image Retrieval (TGIR)
Sub-Category Recognition (SCR)
Fashion Image Captioning (FIC)
Slouchy lamb nubuck patrol  hat in black. Wrinkling and  light distressing throughout. 
Fully lined.
Predicted Class: [FLAT CAPS]
Generated Caption: Grey & brown  camo print tank top. Relaxed-fit  tank top in tones of  grey, brown,  and black. Signature snake  graphic print throughout. Ribbed  crewneck collar. Tonal stitching.
Figure 2. An illustration of four diverse fashion V+L Tasks studied in this work: cross-modal retrieval, text-guided image retrieval, sub-category recognition, and fashion image captioning. Note, all predictions shown in this ﬁgure are made by our FAME-ViL.
Green box indicates the ground truth matches of retrieval tasks. porting heterogeneous task modes in one uniﬁed model. (IV) Comprehensive experiments on four diverse fashion tasks (i.e., cross-modal retrieval [52, 61], text-guided im-age retrieval [75, 83], multi-modal classiﬁcation [61, 94], and image captioning [85]) show that our method signiﬁ-cantly outperforms the previous single-task state-of-the-art with 61.5% parameter saving (see Fig. 1). 2.