Abstract
The performance of modern object detectors drops when the test distribution differs from the training one. Most of the methods that address this focus on object appearance changes caused by, e.g., different illumination conditions, or gaps between synthetic and real images. Here, by con-trast, we tackle geometric shifts emerging from variations in the image capture process, or due to the constraints of the environment causing differences in the apparent geometry of the content itself. We introduce a self-training approach that learns a set of geometric transformations to minimize these shifts without leveraging any labeled data in the new domain, nor any information about the cameras. We evalu-ate our method on two different shifts, i.e., a cameraâ€™s field of view (FoV) change and a viewpoint change. Our results evidence that learning geometric transformations helps de-tectors to perform better in the target domains. 1.

Introduction
While modern object detectors [1, 2, 17, 23, 24] achieve impressive results, their performance decreases when the test data depart from the training distribution. This prob-lem arises in the presence of appearance variations due to, for example, differing illumination or weather conditions.
Considering the difficulty and cost of acquiring annotated data in the test (i.e., target) domain, Unsupervised Domain
Adaptation (UDA) has emerged as the standard strategy to address such scenarios [3, 4, 9, 26, 38].
In this context, much effort has been made to learn do-main invariant features, such that the source and target dis-tributions in this feature space are similar. This has led to great progress in situations where the appearance of the ob-jects changes drastically from one domain to the other, as in case of real-to-sketch adaptation (e.g., Pascal VOC [10] to Comics [15]), or weather adaptation (e.g., Cityscapes [6] to Foggy Cityscapes [27]). Nevertheless, such object ap-pearance changes are not the only sources of domain shifts.
They can also have geometric origins. For example, as shown in Fig. 1, they can be due to a change in camera view-Figure 1. Geometric shifts. (Left) Due to a different FoV, the cars highlighted in green, undergo different distortions even (Right) Different though they appear in similar image regions. camera viewpoints (front facing vs downward facing) yield dif-ferent distortions and occlusion patterns for pedestrian detection. (Bottom) The distributions of pedestrian bounding box sizes in
Cityscapes [6] and MOT [8] differ significantly as the pedestrians are usually far away or in the periphery in Cityscapes. The top im-ages are taken from Cityscapes [6], and the bottom-left and right ones from KITTI [12] and MOT [8], respectively. point or field-of-view (FoV), or a change of object scale due to different scene setups. In practice, such geometric shifts typically arise from a combination of various factors, in-cluding but not limited to the ones mentioned above.
In this paper, we introduce a domain adaptation approach tackling such geometric shifts. To the best of our knowl-edge, the recent work of [13] constitutes the only attempt at considering such geometric distortions. However, it intro-duces a method solely dedicated to FoV variations, assum-ing that the target FoV is fixed and known. Here, we de-velop a more general framework able to cope with a much broader family of geometric shifts.
To this end, we model geometric transformations as a combination of multiple homographies. We show both the-oretically and empirically that this representation is suffi-cient to encompass a broad variety of complex geometric transformations. We then design an aggregator block that can be incorporated to the detector to provide it with the capacity to tackle geometric shifts. We use this modified detector to generate pseudo labels for the target domain, which let us optimize the homographies so as to reduce the geometric shift.
Our contributions can be summarized as follows. (i)
We tackle the problem of general geometric shifts for ob-ject detection. (ii) We learn a set of homographies using unlabeled target data, which alleviates the geometric bias arising in source-only training. (iii) Our method does not require prior information about the target geometric distor-tions and generalizes to a broad class of geometric shifts.
Our experiments demonstrate the benefits of our approach in several scenarios.
In the presence of FoV shifts, our approach yields similar performance to the FoV-dedicated framework of [13] but without requiring any camera infor-mation. As such, it generalizes better to other FoVs. Fur-thermore, we show the generality of our method by using it to adapt to a new camera viewpoint in the context of pedestrian detection.Our implementation can be accessed at https://github.com/vidit09/geoshift. 2.