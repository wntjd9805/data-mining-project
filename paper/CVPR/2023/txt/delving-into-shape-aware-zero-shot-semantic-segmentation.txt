Abstract
Thanks to the impressive progress of large-scale vision-language pretraining, recent recognition models can clas-sify arbitrary objects in a zero-shot and open-set manner, with a surprisingly high accuracy. However, translating this success to semantic segmentation is not trivial, because this dense prediction task requires not only accurate semantic understanding but also fine shape delineation and existing vision-language models are trained with image-level lan-guage descriptions. To bridge this gap, we pursue shape-aware zero-shot semantic segmentation in this study.
In-spired by classical spectral methods in the image segmenta-tion literature, we propose to leverage the eigen vectors of
Laplacian matrices constructed with self-supervised pixel-wise features to promote shape-awareness. Despite that this simple and effective technique does not make use of the masks of seen classes at all, we demonstrate that it out-performs a state-of-the-art shape-aware formulation that aligns ground truth and predicted edges during training.
We also delve into the performance gains achieved on dif-ferent datasets using different backbones and draw several interesting and conclusive observations: the benefits of pro-moting shape-awareness highly relates to mask compact-ness and language embedding locality. Finally, our method sets new state-of-the-art performance for zero-shot seman-tic segmentation on both Pascal and COCO, with significant margins. Code and models will be accessed at SAZS. 1.

Introduction
Semantic segmentation has been an established research area for some time now, which aims to predict the categories of an input image in a pixel-wise manner. In real-world ap-plications including autonomous driving [17], medical diag-nosis [31,46] and robot vision and navigation [9,63], an ac-curate semantic segmentation module provides a pixel-wise
Figure 1. Without retraining, SAZS is able to precisely segments both seen and unseen objects in the zero-shot setting, largely out-performing a strong baseline. * denotes unseen categories during training). understanding of the input image and is crucial for subse-quent tasks (like decision making or treatment selection).
Despite that significant progress has been made in the field of semantic segmentation [6,7,32,49,52,54,57,59,61], most existing methods focus on the closed-set setting in which dense prediction is performed on the same set of cat-egories in training and testing time. Thus, methods that are trained and perform well in the closed-set setting may fail when applied to the open world, as pixels of unseen ob-jects in the open world are likely to be assigned categories that are seen during training, causing catastrophic conse-quences in safety-critical applications such as autonomous driving [62]. Straightforward solutions include fine-tuning or retraining the existing neural networks, but it is impracti-cal to enumerate unlimited unseen categories during retrain-ing, let along large quantities of time and efforts needed.
More recent works [4, 14, 24, 27, 40] address this issue by shifting to the zero-shot setting, in which the methods are evaluated with semantic categories that are unseen dur-ing training. While large-scale pre-trained visual-language models such as CLIP [41] or ALIGN [18] shed light on the
potential of solving zero-shot tasks with priors contained in large-scale pre-trained model, how to perform dense predic-tion task in this setting is still under-explored. One recent approach by Li et.al. [24] closes the gap by leveraging the shared embedding space for languages and images, but fails to effectively segment regions with fine shape delineation.
If the segmented shape of the target object is not accurate, it will be a big safety hazard in practical applications, such as in autonomous driving.
Inspired by the classical spectral methods and their in-trinsic capability of enhancing shapeawareness, we pro-pose a novel Shape-Aware Zero-Shot semantic segmenta-tion framework (SAZS) to address the task of zero-shot semantic segmentation. Firstly, the framework enforces vision-language alignment on the training set using known categories, which exploits rich language priors in the large-scale pre-trained vision-language model CLIP [41]. Mean-while, the framework also jointly enforces the boundary of predicted semantic regions to be aligned with that of the ground truth regions. Lastly, we leverage the eigenvectors of Laplacian of affinity matrices that is constructed by fea-tures learned in a self-supervised manner, to decompose in-puts into eigensegments. They are then fused with learning-based predictions from the trained model. The fusion out-puts are taken as the final predictions of the framework.
As illustrated in Fig. 1, compared with [24], the predic-tions of our approach are better aligned with the shapes of objects. We also demonstrate the effectiveness of our approach with elaborate experiments on PASCAL-5i and
COCO-20i, the results of which show that our method out-performs former state-of-the-arts [4, 24, 36, 37, 51, 53] by large margins. By examining a) the correlation between shape compactness of target object and IoU and b) the correlation between the language embedding locality and
IoU, we discover the large impacts on the performance brought by the distribution of language anchors and ob-ject shapes. Via extensive analyses, we demonstrate the ef-fectiveness and generalization of SAZS frameworkâ€™s shape perception for segmenting semantic categories in the open world. 2.