Abstract
As a pioneering work, PointContrast conducts unsuper-vised 3D representation learning via leveraging contrastive learning over raw RGB-D frames and proves its effective-ness on various downstream tasks. However, the trend of large-scale unsupervised learning in 3D has yet to emerge due to two stumbling blocks: the inefficiency of match-ing RGB-D frames as contrastive views and the annoying mode collapse phenomenon mentioned in previous works.
Turning the two stumbling blocks into empirical stepping stones, we first propose an efficient and effective contrastive learning framework, which generates contrastive views di-rectly on scene-level point clouds by a well-curated data augmentation pipeline and a practical view mixing strat-egy. Second, we introduce reconstructive learning on the contrastive learning framework with an exquisite design of contrastive cross masks, which targets the reconstruction of point color and surfel normal. Our Masked Scene Con-trast (MSC) framework is capable of extracting comprehen-sive 3D representations more efficiently and effectively. It accelerates the pre-training procedure by at least 3× and still achieves an uncompromised performance compared with previous work. Besides, MSC also enables large-scale 3D pre-training across multiple datasets, which fur-ther boosts the performance and achieves state-of-the-art fine-tuning results on several downstream tasks, e.g., 75.5% mIoU on ScanNet semantic segmentation validation set. 1.

Introduction
Unsupervised visual representation learning aims at learning visual representations from vast amounts of unla-beled data. The learned representations are proved to be beneficial for various downstream tasks like segmentation and detection. It has attracted lots of attention and achieved remarkable progress in 2D image understanding, exceeding the upper bound of human supervision [20, 25].
*Corresponding Author. Email: hszhao@cs.hku.hk
Figure 1. Comparison of unsupervised 3D representation learning.
The previous method [56] (top) relies on raw RGB-D frames with restricted views for contrastive learning, resulting in low efficiency and inferior versatility. Our approach (bottom) directly operates on scene-level views with contrastive learning and masked point modeling, leading to high efficiency and superior generality, fur-ther enabling large-scale pre-training across multiple datasets.
Despite the impressive success of unsupervised visual representation learning in 2D, it is underexplored in 3D.
Modern 3D scene understanding algorithms [10, 52] are fo-cused on supervised learning, where models are trained di-rectly from scratch on targeted datasets and tasks. Well-pre-trained visual representations can undoubtedly boost the performance of these algorithms and are currently in ur-gent demand. Recent work PointContrast [56] conducts a preliminary exploration in 3D unsupervised learning. How-ever, it is limited to raw RGB-D frames with an inefficient learning paradigm, which is not scalable and applicable to large-scale unsupervised learning. To address this essential and inevitable challenge, we focus on building a scalable framework for large-scale 3D unsupervised learning.
One technical stumbling block towards large-scale pre-training is the inefficient learning strategy introduced by
matching RGB-D frames as contrastive views. PointCon-trast [56] opens the door to pre-training on real indoor scene datasets and proposes frame matching to generate con-trastive views with natural camera views, as in Figure 1 top.
However, frame matching is inefficient since duplicated en-coding exists for matched frames, resulting in limited scene diversity in batch training and optimization. Meanwhile, not all of the 3D scene data contains raw RGB-D frames, leading to failure deployments of the algorithm. Inspired by the great success of SimCLR [7], we investigate gen-erating strong contrastive views by directly applying a se-ries of well-curated data augmentations to scene-level point clouds, eliminating the dependence on raw RGB-D frames, as in Figure 1 bottom. Combined with an effective mech-anism that mixes up query views, our contrastive learning design accelerates the pre-training procedure by 4.4× and achieves superior performance with purely point cloud data, compared to PointContast with raw data. The superior de-sign also enables large-scale pre-training across multiple datasets like ScanNet [16] and ArkitScenes [5].
Another obstacle is the mode collapse phenomenon that occurs when scaling up the optimization iterations. We owe the culprit for this circumstance to the insufficient difficulty of unsupervised learning tasks. To further tackle the mode collapse challenge in unsupervised learning and scale up the optimization iterations, inspired by recent masked au-toencoders [23, 57], we construct a masked point modeling paradigm where both point color reconstruction objective and surfel normal reconstruction objective are proposed to recover the masked color and geometric information of the point cloud respectively. We incorporate the mask point modeling strategy into our contrastive learning framework via an exquisite design of contrastive cross masks, leading towards a scalable unsupervised 3D representation learning framework, namely Masked Scene Contrast (MSC).
Our framework is efficient, effective, and scalable. We conduct extensive experimental evaluations to validate its capability. On the popular point cloud dataset ScanNet, our algorithm accelerates the pre-training procedure by more than 3×, and achieves better performance on downstream tasks, when compared to the previous representative Point-Contrast. Besides, our method also enables large-scale 3D pre-training across multiple datasets, leading to state-of-the-art fine-tuning results on several downstream tasks, e.g. 75.5% mIoU on ScanNet semantic segmentation validation set. In conclusion, our work opens up new possibilities for large-scale unsupervised 3D representation learning. 2.