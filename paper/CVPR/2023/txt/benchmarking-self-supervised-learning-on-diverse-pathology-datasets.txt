Abstract
Computational pathology can lead to saving human lives, but models are annotation hungry and pathology images are notoriously expensive to annotate. Self-supervised learning (SSL) has shown to be an effective method for utilizing un-labeled data, and its application to pathology could greatly benefit its downstream tasks. Yet, there are no principled studies that compare SSL methods and discuss how to adapt them for pathology. To address this need, we execute the largest-scale study of SSL pre-training on pathology image data, to date. Our study is conducted using 4 representative
SSL methods on diverse downstream tasks. We establish that large-scale domain-aligned pre-training in pathology con-sistently out-performs ImageNet pre-training in standard
SSL settings such as linear and fine-tuning evaluations, as well as in low-label regimes. Moreover, we propose a set of domain-specific techniques that we experimentally show leads to a performance boost. Lastly, for the first time, we apply SSL to the challenging task of nuclei instance segmen-tation and show large and consistent performance improve-ments. We release the pre-trained model weights1. 1.

Introduction
The computational analysis of microscopic images of hu-man tissue – also known as computational pathology – has emerged as an important topic of research, as its clinical implementations can result in the saving of human lives
*The first two authors contributed equally. 1 https://lunit-io.github.io/research/publications/pathology_ssl by improving cancer diagnosis [49] and treatment [42].
Deep Learning and Computer Vision methods in pathol-ogy allow for objectivity [15], large-scale analysis [20], and triaging [5] but often require large amounts of annotated data [52]. However, the annotation of pathology images re-quires specialists with many years of clinical residency [37], resulting in scarce labeled public datasets and the need for methods to train effectively on them.
When annotated data is scarce for a given Computer Vi-sion task, one common and practical solution is to fine-tune a model that was pre-trained in a supervised manner us-ing the ImageNet dataset [19, 34]. This paradigm of trans-fer learning [34] was recently challenged by self-supervised learning (SSL), which trains on large amounts of unlabeled data only, yet out-performs supervised pre-training on Ima-geNet [8, 10, 26]. In the field of pathology, large unlabeled datasets are abundant [4,37,38, 57] in contrast to the lack of annotated datasets [52]. If we were to apply SSL effectively to this huge amount of unlabeled data, downstream pathol-ogy tasks could benefit greatly even if they contain limited amount of annotated training data. Naturally, we ask the question: How well does self-supervised learning help in improving the performance of pathology tasks?
ImageNet pre-trained weights are commonly used in medical imaging and are known to be helpful in attaining high task performance [30, 32, 43, 59]. Due to the differ-ence between natural images and medical images, large-scale domain-aligned pre-training has the potential to push performance beyond ImageNet pre-training [39]. Accord-ingly, recent works show that SSL pre-training on pathol-ogy data can improve performance on downstream pathol-ogy tasks [3, 16, 23, 55]. Our study aims to expand on these previous works by evaluating multiple SSL methods on di-verse downstream pathology tasks. In addition, we propose techniques to adapt SSL methods that were designed for nat-ural image data, to better learn from pathology data.
To understand how to adapt existing SSL methods to work on pathology image data, we must identify several key differences between natural and pathology imagery. Unlike natural images, pathology images can be rotated arbitrar-ily (impossible to determine a canonical orientation) and exhibit fewer variations in color. Also, pathology images can be interpreted differently depending on the field-of-view (FoV) due to the multiple hierarchies and contextual differ-ences involved in each task. We propose to overcome these differences when adapting SSL methods for pathology data, via changes to the training data augmentation scheme in par-ticular, during pre-training.
In this paper, we carry out an in-depth analysis of 4 recent and representative SSL methods; MoCo v2 [12], SwAV [7],
Barlow Twins [61], and DINO [8], when applied to large-scale pathology data. For this purpose, we source 19 mil-lion image patches from Whole Slide Images (WSI) in The
Cancer Genome Atlas (TCGA) dataset [57] and apply our domain-specific techniques in training the SSL methods on this data. The evaluations are conducted for 2 different downstream tasks over 5 datasets: (1) pathological image classification using BACH [1], CRC [31], MHIST [56], and
PatchCamelyon [54] datasets, and (2) nuclei instance seg-mentation and classification using the CoNSeP dataset [25].
Our large-scale study yields several useful contributions: (a) we conduct the largest-scale study of SSL pre-training on pathology image data to date, and show its benefit over using
ImageNet pre-trained weights on diverse downstream tasks (see Fig. 1), (b) we propose a set of carefully designed data curation and data augmentation techniques that can further improve downstream performance, (c) we demonstrate that
SSL is label-efficient, and is therefore a practical solution in pathology where gathering annotation is particularly ex-pensive, and (d) for the first time, we apply SSL to the dense prediction task of nuclei instance segmentation and show its value under diverse evaluation settings. We release our pre-trained model weights at https://lunit-io.github. io/research/publications/pathology_ssl to fur-ther contribute to the research community. 2.