Abstract
Generative DNNs are a powerful tool for image synthe-sis, but they are limited by their computational load. On the other hand, given a trained model and a task, e.g. faces generation within a range of characteristics, the output im-age quality will be unevenly distributed among images with different characteristics. It follows, that we might restrain the model’s complexity on some instances, maintaining a high quality. We propose a method for diminishing compu-tations by adding so-called early exit branches to the orig-inal architecture, and dynamically switching the computa-tional path depending on how difficult it will be to render the output. We apply our method on two different SOTA models performing generative tasks: generation from a semantic map, and cross-reenactment of face expressions; showing it is able to output images with custom lower-quality thresh-olds. For a threshold of LPIPS ≤ 0.1, we diminish their computations by up to a half. This is especially relevant for real-time applications such as synthesis of faces, when quality loss needs to be contained, but most of the inputs need fewer computations than the complex instances. 1.

Introduction
Image synthesis by generative adversarial networks (GANs) received great attention in the last years [64, 71], its applications span from image-to-image translation [35] to text-to-image rendering [21], neural head avatars gener-ation [17] and many more. However, this approach suffers from heavy computational burdens when challenged with producing photo-realistic images. Our work stems from the observation that deep neural networks (DNNs) output im-ages with different but consistent quality when conditioned on certain parameters. Since their expressivity is uneven within the set of possibly generated images, it follows that for some examples, a simpler DNN may suffice in generat-*These authors contributed equally to this work
†aleksei.ivakhnenko@mail.ru ing an output with the required quality.
On the other hand, approaches aimed at easing the heavy computational load of DNNs have been applied with great results, significantly decreasing redundant computa-tions [2,13]. While strategies such as pruning [46,53,65] or knowledge distillation [4, 9, 26] generate a DNN with fewer parameters, early exit (EE) [42,78] is a setup that allows for dynamic variation of the computational burden, and there-fore presents itself as an ideal candidate for an image gen-eration strategy aimed at outputting pictures of consistent quality, while avoiding excessive computation due to their irregular rendering difficulty.
Despite this, implementing EE strategies has remained out of the scope of studies on generative models. This is perhaps due to the fact that EE processes logits of inter-mediate layers, thus restricting their field of application to tasks where the latter are meaningful (e.g. in classification), while excluding pipelines in which a meaningful output is given only at the last layer (e.g. generative convolutional networks).
We propose a method that employs an EE strategy for image synthesis, dynamically routing the computational flow towards the needed exit in accordance to pictures’ complexity, therefore reducing computational redundancy while maintaining consistent quality. To accomplish this, we employ three main elements, which constitute the novel contributions of our work.
First, we attach exit branches to the original DNN (re-ferred as the backbone), as portrayed in Fig. 1. These branches are built of lightweight version of the modules constituting the backbone architecture, their complexity can be tuned in accordance with the desired quality-cost rela-tion. Their depth (i.e. number of modules) varies in ac-cordance to the number of backbone modules left after the point they get attached to. In this way, intermediate back-bone logits are fairly processed.
In second place, we make use of a small database of fea-tures, from which guiding examples are selected and used to condition image generation by concatenating them to the in-put of each branch. These features are obtained by process-ing a selection of images by the first layers of the backbone.
Its presence yields a quality gain for earlier exits, at the ex-pense of a small amount of memory and computations, thus harmonizing exits’ output quality. This is extremely handy for settings where real-time rendering is needed and guid-ing examples can be readily provided, such as neural avatar generation.
Lastly, the third component of our workflow is a predic-tor, namely a DNN trained on the outputs of our branches, and capable of indicating the exit needed for outputting an image of a given quality. This element is fundamental for ensuring a consistent lower-quality threshold, as we will see.
Our method is applicable to already trained models, but requires additional training for the newly introduced com-ponents. We report its application to two distinct tasks of the image synthesis family, namely generation from a semantic map, and cross-reenactment of face expressions.
Our main result may be summarized in this way: the method is easily applicable to already existing and trained genera-tive models, it is capable of outputting images with custom lower-quality threshold by routing easier images to shorter computational paths, and the mean gain in terms of saved computations per quality loss is, respectively, 1.2 × 103, and 1.3 × 103 GFLOPs/LPIPS for the two applications. 2.