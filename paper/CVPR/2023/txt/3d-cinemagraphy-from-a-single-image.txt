Abstract 1.

Introduction
We present 3D Cinemagraphy, a new technique that mar-ries 2D image animation with 3D photography. Given a single still image as input, our goal is to generate a video that contains both visual content animation and camera mo-tion. We empirically find that naively combining existing 2D image animation and 3D photography methods leads to ob-vious artifacts or inconsistent animation. Our key insight is that representing and animating the scene in 3D space offers a natural solution to this task. To this end, we first convert the input image into feature-based layered depth images using predicted depth values, followed by unproject-ing them to a feature point cloud. To animate the scene, we perform motion estimation and lift the 2D motion into the 3D scene flow. Finally, to resolve the problem of hole emer-gence as points move forward, we propose to bidirectionally displace the point cloud as per the scene flow and synthe-size novel views by separately projecting them into target image planes and blending the results. Extensive experi-ments demonstrate the effectiveness of our method. A user study is also conducted to validate the compelling rendering results of our method.
*Corresponding author.
Nowadays, since people can easily take images using smartphone cameras, the number of online photos has in-creased drastically. However, with the rise of online video-sharing platforms such as YouTube and TikTok, people are no longer content with static images as they have grown ac-customed to watching videos. It would be great if we could animate those still images and synthesize videos for a bet-ter experience. These living images, termed cinemagraphs, have already been created and gained rapid popularity on-line [1, 71]. Although cinemagraphs may engage people with the content for longer than a regular photo, they usu-ally fail to deliver an immersive sense of 3D to audiences.
This is because cinemagraphs are usually based on a static camera and fail to produce parallax effects. We are there-fore motivated to explore ways of animating the photos and moving around the cameras at the same time. As shown in
Fig. 1, this will bring many still images to life and provide a drastically vivid experience.
In this paper, we are interested in making the first step towards 3D cinemagraphy that allows both realistic anima-tion of the scene and camera motions with compelling par-allax effects from a single image. There are plenty of at-tempts to tackle either of the two problems. Single-image animation methods [12, 19, 35] manage to produce a real-istic animated video from a single image, but they usually operate in 2D space, and therefore they cannot create cam-era movement effects. Classic novel view synthesis meth-ods [5, 6, 9, 14, 25] and recent implicit neural representa-tions [37, 40, 58] entail densely captured views as input to render unseen camera perspectives. Single-shot novel view synthesis approaches [21, 39, 52, 66] exhibit the potential for generating novel camera trajectories of the scene from a single image. Nonetheless, these methods usually hypoth-esize that the observed scene is static without moving el-ements. Directly combining existing state-of-the-art solu-tions of single-image animation and novel view synthesis yields visual artifacts or inconsistent animation.
To address the above challenges, we present a novel framework that solves the joint task of image animation and novel view synthesis. This framework can be trained to cre-ate 3D cinemagraphs from a single still image. Our key intuition is that handling this new task in 3D space would naturally enable both animation and moving cameras simul-taneously. With this in mind, we first represent the scene as feature-based layered depth images (LDIs) [50] and unpro-ject the feature LDIs into a feature point cloud. To ani-mate the scene, we perform motion estimation and lift the 2D motion to 3D scene flow using depth values predicted by DPT [45]. Next, we animate the point cloud according to the scene flow. To resolve the problem of hole emer-gence as points move forward, we are inspired by prior works [3, 19, 38] and propose a 3D symmetric animation technique to bidirectionally displace point clouds, which can effectively fill in those unknown regions. Finally, we synthesize novel views at time t by rendering point clouds into target image planes and blending the results. In this manner, our proposed method can automatically create 3D cinemagraphs from a single image. Moreover, our frame-work is highly extensible, e.g., we can augment our motion estimator with user-defined masks and flow hints for accu-rate flow estimation and controllable animation.
In summary, our main contributions are:
• We propose a new task of creating 3D cinemagraphs from single images. To this end, we propose a novel framework that jointly learns to solve the task of image animation and novel view synthesis in 3D space.
• We design a 3D symmetric animation technique to ad-dress the hole problem as points move forward.
• Our framework is flexible and customized. We can achieve controllable animation by augmenting our mo-tion estimator with user-defined masks and flow hints. 2.