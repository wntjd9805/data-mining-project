Abstract
A big convergence of language, vision, and multimodal pretraining is emerging.
In this work, we introduce a general-purpose multimodal foundation model BEIT-3, which achieves excellent transfer performance on both vi-sion and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architec-ture, pretraining task, and model scaling up. We use Mul-tiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared back-bone, we perform masked “language” modeling on images (Imglish), texts (English), and image-text pairs (“parallel sentences”) in a unified manner. Experimental results show that BEIT-3 obtains remarkable performance on object de-tection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO). 1.

Introduction: The Big Convergence
Recent years have featured a trend toward the big con-vergence of language [14, 15, 46], vision [3, 43], and mul-timodal [45, 62, 69] pretraining. By performing large-scale pretraining on massive data, we can easily transfer the mod-els to various downstream tasks. It is appealing that we can pretrain a general-purpose foundation model that handles multiple modalities. In this work, we advance the conver-gence trend for vision-language pretraining from the fol-lowing three aspects.
First, the success of Transformers [59] is translated from language to vision [16] and multimodal [26, 62] problems.
The unification of network architectures enables us to seam-For vision-language lessly handle multiple modalities. modeling, there are various ways to apply Transformers
* Equal contribution. † Corresponding author.
Figure 1. Overview of BEIT-3 pretraining. We perform masked data modeling on monomodal (i.e., images, and texts) and multi-modal (i.e., image-text pairs) data with a shared Multiway Trans-former as the backbone network. due to the different natures of downstream tasks. For ex-ample, the dual-encoder architecture is used for efficient retrieval [45], encoder-decoder networks for generation tasks [63], and the fusion-encoder architecture for image-text encoding [26]. However, most foundation models have to manually convert the end-task formats according to the specific architectures. Moreover, the parameters are usu-ally not effectively shared across modalities. In this work, we adopt Multiway Transformers [62] for general-purpose modeling, i.e., one unified architecture shared for various downstream tasks. The modular network also compre-hensively considers modality-specific encoding and cross-modality fusion.
Second, the pretraining task based on masked data mod-eling has been successfully applied to various modalities, such as texts [14] and images [3, 43]. Current vision-language foundation models usually multitask other pre-training objectives (such as image-text matching), render-ing scaling-up unfriendly and inefficient.
In contrast, we only use one pretraining task, i.e., mask-then-predict, to
train a general-purpose multimodal foundation model. By regarding the image as a foreign language (i.e., Imglish), we handle texts and images in the same manner without funda-mental modeling differences. Consequentially, image-text pairs are utilized as “parallel sentences” in order to learn the alignments between modalities. We also show that the sim-ple yet effective method learns strong transferable represen-tations, achieving remarkable performance on both vision and vision-language tasks. The prominent success demon-strates the superiority of generative pretraining [3, 14].
Third, scaling up the model size and data size universally improves the generalization quality of foundation models, so that we can transfer them to various downstream tasks.
We follow the philosophy and scale up the model size to billions of parameters. Moreover, we scale up the pretrain-ing data size while only using publicly accessible resources for academic reproducibility. Although without using any private data, our method outperforms state-of-the-art foun-dation models that rely on in-house data by a decent margin.
In addition, the scaling up benefits from treating images as a foreign language, as we can directly reuse the pipeline developed for large-scale language model pretraining.
In this work, we take advantage of the above ideas to pretrain a general-purpose multimodal foundation model
BEIT-3. We pretrain a Multiway Transformer by perform-ing masked data modeling on images, texts, and image-text pairs. During the pretraining procedure, we randomly mask some proportion of text tokens or image patches. The self-supervised learning objective is to recover the original to-kens (i.e., text tokens, or visual tokens) given corrupted in-puts. The model is general-purpose in the sense that it can be repurposed for various tasks regardless of input modali-ties or output formats.
As shown in Table 1, BEIT-3 achieves remarkable trans-fer performance across a broad range of vision and vision-language tasks. We evaluate BEIT-3 on extensive down-stream tasks and datasets, i.e., object detection (COCO), instance segmentation (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reason-ing (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K,
COCO). Specifically, our model outperforms previous strong foundation models [1,69,70] despite that we only use public resources for pretraining and finetuning. The model also obtains better results than specialized models. More-over, BEIT-3 not only performs well on vision-language tasks but also on vision tasks (such as object detection). 2. BEIT-3: A General-Purpose Multimodal
Foundation Model
BEIT-3 is pretrained by masked data modeling on monomodal and multimodal data, using a shared Multiway
Transformer network. The model can be transferred to var-ious vision and vision-language downstream tasks. 2.1. Backbone Network: Multiway Transformers
We use Multiway Transformers [62] as the backbone model to encode different modalities. As shown in Figure 1, each Multiway Transformer block consists of a shared self-attention module, and a pool of feed-forward networks (i.e., modality experts) used for different modalities. We route each input token to the experts depending on its modality.
Each layer contains a vision expert and a language expert.
Moreover, the top three layers have vision-language experts designed for fusion encoders. Refer to Figure 2 (a)(b)(c) for detailed modeling layouts. Using a pool of modality experts encourages the model to capture more modality-specific information. The shared self-attention module learns the alignment between different modalities and enables deep fusion for multimodal (such as vision-language) tasks.
As shown in Figure 2, the unified architecture enables
BEIT-3 to support a wide range of downstream tasks. For example, BEIT-3 can be used as an image backbone for various vision tasks, including image classification, object detection, instance segmentation, and semantic segmenta-tion. It can also be finetuned as a dual encoder for efficient image-text retrieval, and a fusion model for multimodal un-derstanding and generation tasks. 2.2. Pretraining Task: Masked Data Modeling
We pretrain BEIT-3 via a unified masked data modeling objective on monomodal (i.e., images, and texts) and multi-modal data (i.e., image-text pairs).
Masked Language Modeling BEIT-3 uses masked lan-guage modeling (MLM) to learn language representations from large-scale text-only data. Following BERT [14], we randomly mask 15% tokens of monomodal text data. Each masked token is replaced by a [MASK] token 80% of the time, a random token 10% of the time, and kept the origi-nal tokens 10% of the time. The pretraining objective is to recover the masked tokens from the corrupted input text.
Masked Image Modeling
In addition to masked lan-guage modeling, we employ masked image modeling (MIM) to learn vision representations from large-scale im-age data. Following BEIT [3], given an input image, we apply a block-wise masking strategy to mask 40% of image patches. The pretraining objective of MIM is to reconstruct the discrete visual tokens of masked patches. We use the im-age tokenizer VQ-KDCLIP proposed in BEIT v2 [43], which is trained under the supervision of CLIP [45], to obtain the discrete tokens as the reconstruction targets.
Masked Vision-Language Modeling We introduce masked vision-language modeling (MVLM), which ex-Figure 2. BEIT-3 can be transferred to various vision and vision-language downstream tasks. With a shared Multiway Transformer, we can reuse the model as (a)(b) vision or language encoders; (c) fusion encoders that jointly encode image-text pairs for deep interaction; (d) dual encoders that separately encode modalities for efficient retrieval; (e) sequence-to-sequence learning for image-to-text generation. tends masked language modeling and masked image modeling to multimodal data. The task aims at recovering masked image patches and text tokens based on visual and linguistic clues. Specifically, we randomly mask text tokens (with 50% mask ratio) as in masked language modeling, and recover the masked text tokens based on the joint image-text representations.
In addition, we mask image patches as in MIM and predict their corresponding visual tokens based on the image-text pair. The masking strategy is the same as in masked image modeling. The MVLM task encourages the model to learn alignments between the pairs of image and text.
We only use one pretraining task, which makes the train-ing process scaling-up friendly. In contrast, previous vision-language models [26, 30, 31, 34, 62, 69, 74] usually em-ploy multiple pretraining tasks, such as image-text contrast, image-text matching, and word-patch/region alignment. We show that a much smaller batch size can be used with the mask-then-predict task.
In comparison, contrastive-based models [24, 45, 69, 70] usually need a very large batch size for pretraining, which brings more engineering challenges, such as GPU memory cost. For example, CoCa [69] uses 65k batch size, CLIP [45] uses 32k batch size, and Flo-rence [70] uses 24k batch size. In contrast, BEIT-3 enables a much smaller 6k batch size for pretraining. Moreover, unlike the global dependency between examples as in con-trastive learning, it is straightforward to implement gradient accumulation for masked data modeling. 2.3. Scaling Up: BEIT-3 Pretraining
Backbone Network We scale up the model capacity of
BEIT-3 to a giant-size Transformer model following the setup of ViT-giant [71]. The giant-size model consists of a 40-layer Multiway Transformer with 1408 hidden size, 6144 intermediate size, and 16 attention heads. All layers contain both vision experts and language experts. Vision-language experts are also employed in the top three Mul-tiway Transformer layers. The self-attention module is shared across different modalities. BEIT-3 giant model consists of 1.9B parameters in total, including 692M param-eters for vision experts, 692M for language experts, 52M for vision-language experts, 90M for word embeddings, and 317M for the shared self-attention module. Notice that only vision-related parameters (i.e., comparable size as ViT-giant; about 1B) are activated when the model is used as a vision encoder. Similarly, only text-related weights are used for language tasks. is pretrained
Pretraining Data BEIT-3 both on
For multimodal monomodal and multimodal data. there are about 15M images and 21M image-text data, pairs collected from five public datasets: Conceptual 12M (CC12M) [8], Conceptual Captions (CC3M) [52],
SBU Captions (SBU) [42], COCO [37] and Visual Genome (VG) [27]. Notice that the image tokenizer VQ-KDCLIP [43] is learned with the guidance from CLIP [45]. Given CLIP
Task
Dataset Metric
Semantic Segmentation ADE20K mIoU
Object Detection
Instance Segmentation
Image Classification
COCO
COCO
ImageNet† Top-1 acc.
AP
AP
Previous
Systems 61.4 [64] 63.3 [72] 54.7 [29] 89.0 [64]
BEIT-3 62.8 (+1.4) 63.7 (+0.4) 54.8 (+0.1) 89.6 (+0.6)
Visual Reasoning
Visual QA
Image Captioning
Finetuned Retrieval
Finetuned Retrieval
Zero-shot Retrieval
Acc.
NLVR2
VQA acc.
VQAv2
COCO‡
CIDEr
R@1
COCO
Flickr30K R@1
Flickr30K R@1 92.6 (+5.6) 87.0 [69] 84.0 (+1.7) 82.3 [69] 145.3 [61] 147.6 (+2.3) 76.0 (+3.5) 72.5 [70] 94.2 (+1.6) 92.6 [70] 88.2 (+1.7) 86.5 [69]
Table 1. Overview of BEIT-3 results on various vision and vision-language benchmarks. We compare with previous strong models, including FD-SwinV2 [64], DINO [72], Mask DINO [29], FD-CLIP [64], CoCa [69], OFA [61], Florence [70]. The comparison models are state-of-the-art when we collect the results (timestamp: 08/22/2022). We report the average of top-1 image-to-text and text-to-image results for retrieval tasks. “†” indicates ImageNet results only using publicly accessible resources. “‡” indicates im-age captioning results without CIDEr optimization. is trained with 400M image-text pairs, we note that our model also indirectly touches these data. For monomodal data, we use 14M images from ImageNet-21K and 160GB text corpora [5] from English Wikipedia, BookCorpus [76],
OpenWebText1, CC-News [38], and Stories [57].
Pretraining Settings We pretrain the model for 1M steps.
Each batch contains 6144 samples in total, including 2048 images, 2048 texts, and 2048 image-text pairs. The batch size is much smaller than contrastive models [24, 45, 69].
BEIT-3 giant model uses 14 × 14 patch size and is pre-trained at resolution 224×224. We use the same image aug-mentation as in BEIT [3], including random resized crop-ping, horizontal flipping, and color jittering [66]. A Senten-cePiece tokenizer [28] with 64k vocab size is employed to tokenize the text data. We use the AdamW [40] optimizer with β1 = 0.9, β2 = 0.98 and ϵ =1e-6 for optimization.
We use a cosine learning rate decay scheduler with a peak learning rate of 1e-3 and a linear warmup of 10k steps. The weight decay is 0.05. Stochastic depth [20] with a rate of 0.1 is used. The pretraining process takes about two weeks using 256 A100 40GB GPUs. 3. Experiments
We extensively evaluate BEIT-3 on major public bench-marks for both vision-language and vision tasks. Table 1 presents the overview of the results. BEIT-3 obtains re-markable performance on a wide range of vision and vision-language tasks. 1http://skylion007.github.io/OpenWebTextCorpus
Model
Oscar [34]
VinVL [74]
ALBEF [31]
BLIP [30]
SimVLM [63]
Florence [70]
OFA [61]
Flamingo [1]
CoCa [69]
BEIT-3
VQAv2
NLVR2 test-dev test-std dev test-P 73.61 76.52 75.84 78.25 80.03 80.16 82.00 82.00 82.30 84.19 73.82 76.60 76.04 78.32 80.34 80.36 82.00 82.10 82.30 79.12 82.67 82.55 82.15 84.53
---86.10 80.37 83.98 83.14 82.24 85.15
---87.00 84.03 91.51 92.58
Table 2. Results of visual question answering and visual reasoning tasks. We report vqa-score on VQAv2 test-dev and test-standard (test-std) splits, accuracy for NLVR2 development set (dev) and public test set (test-P). 3.1. Vision-Language Downstream Tasks
We evaluate the capabilities of BEIT-3 on the widely used vision-language understanding and generation bench-marks, including visual question answering [18], visual rea-soning [55], image-text retrieval [37, 44], and image cap-tioning [37].
Visual Question Answering (VQA) The task requires the model to answer natural language questions about input images. Following previous work [2, 26, 74], we conduct finetuning experiments on the VQA v2.0 dataset [18] and formulate the task as a classification problem. The model is trained to predict answers from the 3129 most frequent an-swer candidates in the training set. BEIT-3 is finetuned as a fusion encoder to model deep interactions of images and questions for the VQA task. We concatenate the embed-dings of a given question and an image, and then feed the input embeddings into Multiway Transformers to jointly en-code the image-question pair. The final pooled output is fed into a classifier layer to predict the answer. The results are reported in Table 2, BEIT-3 outperforms previous models by a large margin (more than 1.7 points), achieving 84.03 with a single model.
Visual Reasoning The task needs models to perform joint reasoning about images and natural language descriptions.
We evaluate the model on the popular NLVR2 [55] bench-mark, which is to determine whether a textual descrip-tion is true about a pair of images. Following previous work [26, 74], we construct two image-text pairs based on the triplet input. We finetune BEIT-3 as a fusion encoder to jointly encode the image-text pairs. The final pooled out-puts of the two pairs are concatenated and then fed into a classifier layer to predict the label. As shown in Table 2,
BEIT-3 achieves prominent results for visual reasoning,
MSCOCO (5K test set)
Flickr30K (1K test set)
Flickr30K (1K test set)
Model
Image → Text
Text → Image
Image → Text
Text → Image
Model
Image → Text
Text → Image
R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
R@1 R@5 R@10 R@1 R@5 R@10
UNITER 65.7 88.6 75.4 92.9
VinVL 77.6 94.3
ALBEF
BLIP 82.4 95.4
ALIGN 77.0 93.5
FILIP 78.9 94.4
Florence 81.8 95.2 93.8 96.2 97.2 97.9 96.9 97.4
-52.9 79.9 58.8 83.5 60.7 84.3 65.1 86.3 59.9 83.3 61.2 84.3 63.2 85.7 88.0 90.3 90.5 91.8 89.8 90.6
-----75.6 94.1 87.3 98.0 99.2
-95.9 99.8 100.0 85.6 97.5 97.4 99.8 87.6 97.7 99.9 95.3 99.8 100.0 84.9 97.4 96.6 100.0 100.0 87.1 97.7 87.9 98.1
-97.2 99.9 96.8
-98.9 99.0 98.6 99.1
-FLAVA 67.7 94.0 88.0 98.7
CLIP
ALIGN 88.6 98.7 89.8 99.2
FILIP
Florence 90.9 99.1
Flamingo 89.3 98.8 92.5 99.5
CoCa
-99.4 99.7 99.8
-99.7 99.9 65.2 89.4 68.7 90.6 75.7 93.8 75.0 93.4 76.7 93.6 79.5 95.3 80.4 95.7
-95.2 96.8 96.3
-97.9 97.7
BEIT-3 84.8 96.5 98.3 67.2 87.7 92.8 98.0 100.0 100.0 90.3 98.7 99.5
BEIT-3 94.9 99.9 100.0 81.5 95.6 97.8 (a) Finetuned results on COCO and Flickr30K. (b) Zero-shot results on Flickr30K.
Table 3. Finetuning and zero-shot results of image-to-text and text-to-image retrieval. UNITER [9] and VinVL [74] are fusion-encoder models. ALBEF [31] and BLIP [30] first obtain top-k candidates using their dual encoders and then use fusion encoders to rerank the candidates. BEIT-3 and the other models [1, 24, 45, 53, 68–70] are dual-encoder models. Notice that dual-encoder models are more efficient than fusion-encoder-based models for retrieval tasks because of representation caching. outperforming CoCa by about 5.6 points. The performance on NLVR2 reaches above 90% for the first time.
Image-Text Retrieval The task is to measure the sim-There are two di-ilarity between images and texts. rections depending on the modality of the retrieved tar-get: image-to-text retrieval, and text-to-image retrieval.
Two popular retrieval benchmarks, i.e., COCO [37], and
Flickr30K [44], are used to evaluate the model. Following previous work [26, 74], we use the Karpathy split [25] for the two benchmarks. BEIT-3 is finetuned as a dual encoder for efficient image-text retrieval. Dual-encoder models sep-arately encode images and texts to obtain their represen-tations. Then we calculate the cosine similarity scores of these representations. Dual-encoder models are more effi-cient than fusion-encoder models. Because they do not have to jointly encode all possible image-text pairs.
We directly finetune BEIT-3 on COCO and Flickr30K, although the model is not pretrained with image-text con-trastive loss. Surprisingly, BEIT-3 outperforms previous strong models only using a small amount of contrastive training. The results demonstrate that BEIT-3 effectively learns alignments between images and texts via masked data modeling.
In order to improve the performance, we per-form intermediate finetuning with an image-text contrastive objective on the pretraining image-text pairs. We finetune the model with much fewer steps than pretraining. Then we use the model to evaluate zero-shot and finetuned image-text retrieval. The finetuned results are reported in Table 3a, dual-encoder BEIT-3 outperforms prior models by a large margin, achieving 3.0/4.0 absolute improvement on COCO top-1 image-to-text/text-to-image retrieval, and 0.8/2.4 ab-solute improvement on Flickr30K top-1 image-to-text/text-to-image retrieval. BEIT-3 also significantly outperforms fusion-encoder-based models, which require more compu-tation cost than dual-encoder models for inference. As
Model
Oscar [34]
VinVL [74]
BLIP [30]
SimVLM [63]
OFA [61]
Flamingo [1]
CoCa [69]
BEIT-3
COCO Captioning
BLEU@4 METEOR CIDEr
SPICE 37.4 38.5 40.4 40.6 43.9
-40.9 44.1 30.7 30.4
-33.7 31.8
-33.9 32.4 127.8 130.8 136.7 143.3 145.3 138.1 143.6 147.6 23.5 23.4
-25.4 24.8
-24.7 25.4
Table 4. Results of COCO image captioning. We report BLEU@4,
METEOR, CIDEr, and SPICE on the Karpathy test split. For sim-plicity, we report results without using CIDEr optimization. shown in Table 3b, BEIT-3 also achieves better perfor-mance on Flickr30K zero-shot retrieval.
Image Captioning The task aims to generate a natu-ral language caption for the given image. We use the
COCO [37] benchmark, finetune and evaluate the model on Karpathy split [25]. Following UNILM [15] and s2s-ft [4], BEIT-3 is used as a conditional generation model via masked finetuning. To be more specific, a special self-attention mask is employed for the image captioning task.
Image tokens (i.e., image patches) can only attend to each other bidirectionally within the image sequence. Tokens of the caption can attention to image tokens, their leftward caption tokens, and themselves. During finetuning, we ran-domly mask some percentage of caption tokens. The model is trained to recover these tokens based on the clues of the image and its leftward caption context. We also mask the special boundary token [SEP] to help the model learn to terminate the generation. For simplicity, BEIT-3 is trained with simple cross-entropy loss, without using CIDEr opti-mization. During inference, we generate the caption tokens
Model
DyHead [12]
Soft Teacher [67]
GLIP [33]
GLIPv2 [73]
Florence [70]
SwinV2-G [39]
Mask DINO [29]
DINO [72]
BEIT-3
Maximum
Image Size
COCO test-dev
APbox APmask 2000
---2500 1536 1280 2000 1280 60.6 61.3 61.5 62.4 62.4 63.1
-63.3 63.7
-53.0
---54.4 54.7
-54.8
Table 5. Results of object detection and instance segmentation on
COCO benchmark. BEIT-3 uses Cascade Mask R-CNN [7] as the detection head. Our results are reported with multi-scale evalua-tion. We report the maximum image size used for training. Dy-Head [12] generates pseudo labels on ImageNet and use the data as extra OD data. Florence [70] uses FLOD-9M and GLIP [33,73] is trained with FourODs. The other models use Object365 as the extra OD data. FLOD-9M and FourODs also contain Object365.
The results of the comparison systems are from the paperswith-code.com leaderboard (timestamp: 08/22/2022). one by one in an autoregressive manner. Table 4 presents the results on COCO captioning. BEIT-3 achieves bet-ter results compared with previous image captioning mod-els. The results demonstrate the superiority of BEIT-3 for vision-language generation.
Model
Crop Size
HorNet [48]
SeMask [23]
SwinV2-G [39]
ViT-Adapter [10]
Mask DINO [29]
FD-SwinV2-G [64]
BEIT-3 6402 6402 8962 8962
-8962 8962
ADE20K mIoU +MS 57.5 57.0 59.3 59.4 59.5
-57.9 58.3 59.9 60.5 60.8 61.4 62.0 62.8
Table 6. Results of semantic segmentation on ADE20K. “MS” is short for multi-scale. The results of the comparison sys-tems are from the paperswithcode.com leaderboard (timestamp: 08/22/2022).
Model
Extra Data
Image Size
ImageNet
With extra private image-tag data
SwinV2-G [39]
ViT-G [71]
CoAtNet-7 [13]
Model Soups [65]
CoCa [69]
IN-22K-ext-70M
JFT-3B
JFT-3B
JFT-3B
JFT-3B
With only public image-tag data
IN-21K
IN-21K
IN-21K
IN-21K
IN-21K
IN-21K
BEIT [3]
CoAtNet-4 [13]
MaxViT [58]
MViTv2 [36]
FD-CLIP [64]
BEIT-3 6402 5182 5122 5002 5762 5122 5122 5122 5122 3362 3362 90.2 90.5 90.9 91.0 91.0 88.6 88.6 88.7 88.8 89.0 89.6 3.2. Vision Downstream Tasks
Table 7. Top-1 accuracy on ImageNet-1K.
In addition to vision-language downstream tasks,
BEIT-3 can be transferred to a wide range of vision down-stream tasks, including object detection, instance segmenta-tion, semantic segmentation, and image classification. No-tice that only vision-related parameters are activated when
BEIT-3 is used as a vision encoder. So the number of ef-fective parameters is comparable to ViT-giant [71], i.e., the effective model size is about 1B.
Object Detection and Instance Segmentation We con-duct finetuning experiments on the COCO 2017 bench-mark [37], which consists of 118k training, 5k validation, and 20k test-dev images. We use BEIT-3 as the back-bone and follow ViTDet [35], including a simple feature pyramid and window attention, for the object detection and instance segmentation tasks. Following common prac-tices [39, 72], we first conduct intermediate finetuning on the Objects365 [51] dataset. Then we finetune the model on the COCO dataset. Soft-NMS [6] is used during inference.
Table 5 compares BEIT-3 with previous strong models on
COCO object detection and instance segmentation. BEIT-3 achieves the best results on the COCO test-dev set with a smaller image size used for finetuning, reaching up to 63.7 box AP and 54.8 mask AP.
Semantic Segmentation Semantic segmentation aims to predict the label for each pixel of the given image. We evaluate BEIT-3 on the challenging ADE20K dataset [75], which includes 150 semantic categories. ADE20K con-tains 20k images for training and 2k images for valida-tion. We directly follow the task transfer settings of ViT-Adapter [10]. We use a dense prediction task adapter and employ Mask2Former [11] as the segmentation framework.
As shown in Table 6, BEIT-3 achieves 62.8 mIoU, outper-forming FD-SwinV2 [64] giant model with 3B parameters by 1.4 points. It shows that BEIT-3 achieves superior per-formance on the dense prediction task. evaluate the model
Image Classification We on
ImageNet-1K [50], which contains 1.28M training images and 50k validation images in 1k classes. Rather than appending a task layer to the vision encoder [3, 16], we formulate the task as an image-to-text retrieval task.
We use the category names as texts to construct image-text pairs. BEIT-3 is trained as a dual encoder to find the most relevant label for an image. During inference, we first compute the feature embeddings of possible class names and the feature embedding of the image. Their
cosine similarity scores are then calculated to predict the most probable label for each image. Table 7 reports the results on ImageNet-1K. We first perform intermediate finetuning on ImageNet-21K, then we train the model on
ImageNet-1K. For a fair comparison, we compare with the previous models only using public image-tag data.
BEIT-3 outperforms prior models when only using public image-tag data. 3.3. Ablation Studies
We conduct ablation studies on base-size models, hav-ing 12-layer Multiway Transformer blocks with 768 hid-den size and 3072 intermediate size. The base-size mod-els use 16 × 16 patch size and are trained at resolution 224 × 224. Most settings and hyperparameters are kept the same as in Section 2.3. We use multimodal data including
CC3M, SBU, COCO, and VG to pretrain the model. The monomodal data include ImageNet-21K and 16GB text cor-pora from English Wikipedia and BookCorpus. Notice that we use the same text corpora as BERT [14] so that we can directly compare the language-only performance in Table 9.
The models are pretrained for 200K steps with 2e-3 peak learning rate and 6144 batch size.
Backbone Architecture We study the effects of differ-ent model architectures. Table 8a shows that Multiway
Transformers perform better than standard Transformers on three benchmarks. Modality experts introduced in Multi-way Transformers effectively capture modality-specific in-formation and improve performance.
Masking Strategy in MVLM We compare two mask-ing strategies for MVLM, i.e., joint masking, and separate masking. Specifically, for joint masking, we simultane-ously mask image patches and text tokens for the same in-put image-text pair. In contrast, for separate masking, given an input pair, we randomly mask tokens of one modality (image or text) while keeping tokens of another modality unmasked. As shown in Table 8b, separate masking outper-forms joint masking and learns the alignment of images and texts more effectively.
Monomodal and Multimodal Data We analyze the ef-fects of monomodal and multimodal data in Table 8c. Ex-perimental results indicate that monomodal and multimodal data positively contribute to performance. Using both types of pretraining data achieves the best results.
Image Reconstruction Target We compare different tar-gets used for image reconstruction. As shown in Table 8d,
VQ-KDCLIP [43] performs better than the DALL-E [47] to-kenizer used in BEIT [3] and per-patch-normalized pixels proposed by MAE [19].
Text Reconstruction We study the effects of text recon-struction on monomodal and multimodal data. As shown in Table 8e, the text reconstruction tasks on monomodal and multimodal data bring improvements. Text reconstruc-tion on text corpora learns language representations. More-over, text reconstruction on multimodal data encourages the model to learn cross-modal alignments. In addition, we find that masked language modeling on multimodal data plays a more important role than on text-only data.
Image Reconstruction Table 8f presents the ablation study of masked image modeling on monomodal and mul-timodal data. The results indicate that the image recon-struction tasks on both types of pretraining data improve the results. In contrast to text reconstruction, we find that monomodal data and multimodal data contribute similarly to image reconstruction.
Language Downstream Tasks Table 9 shows that our method also achieves competitive performance on language-only tasks. Following previous work [53, 63], we conduct experiments on the widely used GLUE [60] benchmark with a base-size model. Compared with pre-vious vision-language pretrained models [9, 32, 41, 45, 53, 54, 56, 63], BEIT-3 achieves better performance. BEIT-3 even outperforms SimVLM [63] trained on a much larger text corpora. 4.