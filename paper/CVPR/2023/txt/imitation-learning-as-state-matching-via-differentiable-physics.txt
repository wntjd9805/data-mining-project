Abstract
Existing imitation learning (IL) methods such as inverse reinforcement learning (IRL) usually have a double-loop training process, alternating between learning a reward function and a policy and tend to suffer long training time and high variance.
In this work, we identify the beneﬁts of differentiable physics simulators and propose a new IL method, i.e., Imitation Learning as State Matching via Dif-ferentiable Physics (ILD), which gets rid of the double-loop design and achieves signiﬁcant improvements in ﬁnal per-formance, convergence speed, and stability. The proposed
ILD incorporates the differentiable physics simulator as a physics prior into its computational graph for policy learn-ing. ILD unrolls the dynamics by sampling actions from a parameterized policy and minimizing the distance between the expert trajectory and the agent trajectory.
It back-propagates the gradient into the policy via temporal physics operators, which improves the transferability to unseen en-vironments and yields higher ﬁnal performance. ILD has a single-loop structure that stabilizes and speeds up training.
It dynamically selects learning objectives for each state dur-ing optimization to simplify the complex optimization land-scape. Experiments show that ILD outperforms state-of-the-art methods in continuous control tasks with Brax, and can be applied to deformable object manipulation tasks, gener-alized to unseen conﬁgurations. 1 1.

Introduction
In a variety of applications ranging from games to real-world robotic tasks [13, 18, 38], imitation learning (IL) is popularly applied. However, collecting high-quality expert data is expensive, and existing IL methods tend to suffer long training time, unstable training process, high variance of learned IL policies, and suboptimal ﬁnal performance.
Classical behavioral cloning (BC) methods learn poli-cies directly from labeled data, but often suffer the covari-ate shift problem. This problem can be tackled in DAG-‡This work is completed at the SEA AI Lab. 1The link to the code: https://github.com/sail-sg/ILD
GER [32] by interacting with the environment and querying experts online, which however requires signiﬁcant human effort to label the actions. Other IL methods mainly include inverse reinforcement learning (IRL), adversarial imitation learning (AIL), and combinations of them. IRL learns a re-ward function to match expert demonstrations [11, 19, 41], and AIL learns a discriminator to identify whether the ac-tion comes from an expert demonstration [18,24]. However, both IRL and AIL learn an additional intermediate signal, which introduces three main limitations: 1) the intermedi-ate signal learning leads to a double-loop training process, which means long training time and complex implementa-tion; 2) the learning signal is a noisy and frequently updated moving target, and as a result, the policy learning tends to have a high variance; 3) the intermediate signal, e.g., the re-ward function in IRL, inevitably loses the rich information embedded in the trajectories, e.g., environment dynamics.
In this work, we propose a new approach to IL, named
Imitation Learning as State Matching via Differentiable
Physics (ILD), which recovers expert behavior by exploiting the Differentiable Physics Simulator (DPS) [12,20]. Differ-ent from standard environments, DPS implements low-level physics operations with a differentiable function and allows the gradients to ﬂow through the dynamics. ILD takes ad-vantage of DPSs by considering the environment dynamics as a physics prior and incorporating it into its computational graph during back-propagation of the policy, such that the learned policy fully captures both the expert demonstration and the environment speciﬁcations. To achieve this, ILD simply minimizes the state-wise distance of a rollout tra-jectory generated by a parameterized policy to the expert demonstration, which also gives a single-loop design and avoids learning intermediate signals. Nevertheless, the gra-dients of physics operators are highly non-convex, which often introduces a complex optimization landscape, and consequently, a naive implementation is often stuck in local minimum [12]. To alleviate this issue, we introduce a sim-ple yet effective Chamfer-α distance for trajectory match-ing. For each state in the rollout trajectory, instead of ex-actly matching the corresponding expert state, we dynami-cally select the easiest local goal as the optimization target
Table 1. Useful Properties among IL Methods
Property / Method Family
IRL
AIL
ILD (ours)
Layers of training loop
Source of the learning signal
Transferability in changing dynamics
Double-loop
Double-loop
Single-loop
Reward function Discriminator Differentiable dynamics
Depends
No
Yes and gradually proceed to the harder ones as training pro-gresses. Chamfer-α distance naturally forms a curriculum learning setup, simpliﬁes the optimization task, and eventu-ally gives better ﬁnal performance.
A short comparison of some useful properties of the IL methods can be found in Table 1. In contrast to the IRL and AIL methods, ILD does not introduce new intermediate signals and therefore requires no switching between policy learning and intermediate signal learning. In terms of the learning paradigm, IRL learns a reward function, AIL learns a discriminator, and ILD uses the differentiable dynamics which makes the learned policy aware of the environment dynamics and transferable to unseen environment conﬁgu-rations.
Empirically, we validate ILD on a set of MuJoCo-like continuous control tasks from Brax [12] and a challeng-ing cloth manipulation task. We show that ILD achieves signiﬁcant improvements over the state-of-the-art IRL and
AIL methods in terms of convergence time, training stabil-ity, and ﬁnal performance. Given a ﬁxed one-hour training time, ILD achieves 36% higher performance based on the normalized score over all the tasks and baselines. 2.