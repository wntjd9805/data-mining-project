Abstract
Many real-world recognition problems are characterized by long-tailed label distributions. These distributions make representation learning highly challenging due to limited generalization over the tail classes. If the test distribution differs from the training distribution, e.g. uniform versus long-tailed, the problem of the distribution shift needs to be addressed. A recent line of work proposes learning multiple diverse experts to tackle this issue. Ensemble di-versity is encouraged by various techniques, e.g. by spe-cializing different experts in the head and the tail classes.
In this work, we take an analytical approach and extend the notion of logit adjustment to ensembles to form a Bal-anced Product of Experts (BalPoE). BalPoE combines a family of experts with different test-time target distributions, generalizing several previous approaches. We show how to properly define these distributions and combine the ex-perts in order to achieve unbiased predictions, by proving that the ensemble is Fisher-consistent for minimizing the balanced error. Our theoretical analysis shows that our balanced ensemble requires calibrated experts, which we achieve in practice using mixup. We conduct extensive exper-iments and our method obtains new state-of-the-art results on three long-tailed datasets: CIFAR-100-LT, ImageNet-LT, and iNaturalist-2018. Our code is available at https:
//github.com/emasa/BalPoE-CalibratedLT. 1.

Introduction
Recent developments within the field of deep learning, enabled by large-scale datasets and vast computational re-sources, have significantly contributed to the progress in many computer vision tasks [33]. However, there is a discrep-ancy between common evaluation protocols in benchmark datasets and the desired outcome for real-world problems.
Many benchmark datasets assume a balanced label distribu-tion with a sufficient number of samples for each class. In
*Affiliation: Husqvarna Group, Huskvarna, Sweden.
†Co-affiliation: University of KwaZulu-Natal, Durban, South Africa. (a) SOTA comparison (b) BS (c) SADE (d) Our approach
Figure 1. (a) SOTA comparison. Many SOTA approaches employ
Balanced Softmax (BS) [51] and its extensions to mitigate the long-tailed bias [12, 38, 70, 72]. However, we observe that single-expert bias-adjusted models, as well as multi-expert extensions, are still poorly calibrated by default. Reliability plots for (b) BS, (c)
SADE [70] and (d) Calibrated BalPoE (our approach). this setting, empirical risk minimization (ERM) has been widely adopted to solve multi-class classification and is the key to many state-of-the-art (SOTA) methods, see Figure 1. Unfortunately, ERM is not well-suited for imbalanced or long-tailed (LT) datasets, in which head classes have many more samples than tail classes, and where an unbiased pre-dictor is desired at test time. This is a common scenario for real-world problems, such as object detection [42], med-ical diagnosis [18] and fraud detection [50]. On the one hand, extreme class imbalance biases the classifier towards head classes [31, 57]. On the other hand, the paucity of data prevents learning good representations for less-represented classes, especially in few-shot data regimes [67]. Addressing class imbalance is also relevant from the perspective of al-gorithmic fairness, since incorporating unfair biases into the models can have life-changing consequences in real-world decision-making systems [46].
Previous work has approached the problem of class imbal-ance by different means, including data re-sampling [4, 8], cost-sensitive learning [1, 65], and margin modifications
[6, 56]. While intuitive, these methods are not without limi-tations. Particularly, over-sampling can lead to overfitting of rare classes [13], under-sampling common classes might hin-der feature learning due to the omitting of valuable informa-tion [31], and loss re-weighting can result in optimization in-stability [6]. Complementary to these approaches, ensemble learning has empirically shown benefits over single-expert models in terms of generalization [34] and predictive un-certainty [36] on balanced datasets. Recently, expert-based approaches [5, 61] also show promising performance gains in the long-tailed setting. However, the theoretical reasons for how expert diversity leads to better generalization over different label distributions remain largely unexplored.
In this work, we seek to formulate an ensemble where each expert targets different classes in the label distribution, and the ensemble as a whole is provably unbiased. We accomplish this by extending the theoretical background for logit adjustment to the case of learning diverse expert ensembles. We derive a constraint for the target distributions, defined in terms of expert-specific biases, and prove that fulfilling this constraint yields Fisher consistency with the balanced error.
In our formulation, we assume that the experts are calibrated, which we find not to be the case by default in practice. Thus, we need to assure that the assumption is met, which we realize using mixup [69]. Our contributions can be summarized as follows:
• We extend the notion of logit adjustment based on label frequencies to balanced ensembles. We show that our approach is theoretically sound by proving that it is Fisher-consistent for minimizing the balanced error.
• Proper calibration is a necessary requirement to apply the previous theoretical result. We find that mixup is vital for expert calibration, which is not fulfilled by default in practice. Meeting the calibration assumption ensures
Fisher consistency, and performance gains follow.
• Our method reaches new state-of-the-art results on three long-tailed benchmark datasets. 2.