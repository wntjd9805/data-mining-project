Abstract (a) Procedure Knowledge 
Graph Building
Supervision (b) Pre-training
Our goal is to learn a video representation that is useful for downstream procedure understanding tasks in instruc-tional videos. Due to the small amount of available an-notations, a key challenge in procedure understanding is to be able to extract from unlabeled videos the procedu-ral knowledge such as the identity of the task (e.g., ‘make latte’), its steps (e.g., ‘pour milk’), or the potential next steps given partial progress in its execution. Our main in-sight is that instructional videos depict sequences of steps that repeat between instances of the same or different tasks, and that this structure can be well represented by a Proce-dural Knowledge Graph (PKG), where nodes are discrete steps and edges connect steps that occur sequentially in the instructional activities. This graph can then be used to generate pseudo labels to train a video representation that encodes the procedural knowledge in a more accessi-ble form to generalize to multiple procedure understand-ing tasks. We build a PKG by combining information from a text-based procedural knowledge database and an unla-beled instructional video corpus and then use it to gener-ate training pseudo labels with four novel pre-training ob-jectives. We call this PKG-based pre-training procedure and the resulting model Paprika, Procedure-Aware PRe-training for Instructional Knowledge Acquisition. We eval-uate Paprika on COIN and CrossTask for procedure un-derstanding tasks such as task recognition, step recogni-tion, and step forecasting. Paprika yields a video rep-resentation that improves over the state of the art: up to 11.23% gains in accuracy in 12 evaluation settings. Im-plementation is available at https://github.com/ salesforce/paprika. 1.

Introduction
Instructional videos depict humans demonstrating how to perform multi-step tasks such as cooking, making up and embroidering, repairing, or creating new objects. For a holistic instructional video understanding, an agent has to acquire procedural knowledge: structural information about
Instructional Videos
Procedure-Aware Model
What task?
What step?
What next step?
...
Downstream Procedural Tasks
Figure 1. Training a video representation for procedure un-derstanding with supervision from a procedural knowledge graph: the structure observed in instructions for procedures (from text, from videos) corresponds to sequences of steps that repeat between instances of the same or different tasks; this structure is well represented by a Procedural Knowledge Graph (PKG). (a) We build a PKG combining text instructions with unlabeled video data, and (b) obtain a video representation by encoding the human pro-cedural knowledge from the PKG into a more general procedure-aware model (Paprika) generating pseudo labels with the PKG for several procedure understanding objectives. Paprika can then be easily applied to multiple downstream procedural tasks. tasks such as the identiﬁcation of the task, its steps, or fore-casting the next steps. An agent that has acquired procedu-ral knowledge is said to have gained procedure understand-ing of instructional videos, which can be then exploited in multiple real-world applications such as instructional video labeling, video chapterization, process mining and, when connected to a robot, robot task planning.
Our goal is to learn a novel video representation that can be applicable to a variety of procedure understanding tasks in instructional videos. Unfortunately, prior methods for video representation learning are inadequate for this goal, as they lack the ability to capture procedural knowledge. This is because most of them are trained to learn the (weak) cor-respondence between visual and text modalities, where the text comes either from automatic-speech recognition (ASR) on the audio [43, 77], which is noisy and error-prone, or from a caption-like descriptive sentence (e.g., “a video of a dog”) [33], which does not contain sufﬁcient informa-tion for ﬁne-grained procedure understanding tasks such as step recognition or anticipation. Others are pre-trained on masked frame modeling [34], frame order modeling [34] or video-audio matching [1], which gives them basic video
spatial, temporal or multimodal understanding but is too generic for procedure understanding tasks.
Closer to our goal, Lin et al. [38] propose a video foun-dation model for procedure understanding of instructional videos by matching the videos’ ASR transcription (i.e., sub-title/narration) to procedural steps from a text procedural knowledge database (wikiHow [30]) and training the video-representation-learning model to match each part of an in-structional video to the corresponding step. Their method only acquires isolated step knowledge in pre-training and is not as suitable to gain sophisticated procedural knowledge.
We propose Paprika, from Procedure-Aware PRe-training for Instructional Knowledge Acquisition, a method to learn a novel video representation that encodes procedu-ral knowledge (Fig. 1). Our main insight is that the structure observed in instructional videos corresponds to sequences of steps that repeat between instances of the same or differ-ent tasks. This structure can be captured by a Procedural
Knowledge Graph (PKG) where nodes are discretized steps annotated with features, and edges connect steps that occur sequentially in the instructional activities. We build such a graph by combining the text and step information from wik-iHow and the visual and step information from unlabeled instructional video datasets such as HowTo100M [45] auto-matically. The resulting graph encodes procedural knowl-edge about tasks and steps, and about the temporal order and relation information of steps.
We then train our Paprika model on multiple pre-training objectives using the PKG to obtain the training la-bels. The proposed four pre-training objectives (Sec. 3.3) respectively focuses on procedural knowledge about the step of a video, tasks that a step may belong to, steps that a task would require, and the general order of steps. These pre-training objectives are designed to allow a model to an-swer questions about the subgraph of the PKG that a video segment may belong to. The PKG produces pseudo labels for these questions as supervisory signals to adapt video representations produced by a video foundation model [9] for robust and generalizable procedure understanding.
Our contributions are summarized as follows: (i) We propose a Procedural Knowledge Graph (PKG) that encodes human procedural knowledge from collectively leveraging a text procedural knowledge database (wikiHow) and an unlabeled instructional video corpus (HowTo100M). (ii) We propose to elicit the knowledge in the PKG into
Paprika, a procedure-aware model, using four pre-training objectives. To that end, we produce pseudo lables with the PKG that serve as supervisory signals to train
Paprika to learn to answer multiple questions about the subgraph of the PKG that a video segment may belong to. (iii) We evaluate our method on the challenging COIN and
CrossTask datasets on downstream procedure understand-ing tasks: task recognition, step recognition, and step fore-casting. Regardless of the capacity of the downstream model (from simple MLP to the powerful Transformer), our method yields a representation that outperforms the state of the art – up to 11.23% gains in accuracy out of 12 evalua-tion settings. 2.