Abstract (NLOS)
Time-resolved non-line-of-sight imaging is based on the multi-bounce indirect reflections from the hid-den objects for 3D sensing. Reconstruction from NLOS measurements remains challenging especially for compli-cated scenes. To boost the performance, we present NLOST, the first transformer-based neural network for NLOS recon-struction. Specifically, after extracting the shallow features with the assistance of physics-based priors, we design two spatial-temporal self attention encoders to explore both lo-cal and global correlations within 3D NLOS data by split-ting or downsampling the features into different scales, re-spectively. Then, we design a spatial-temporal cross atten-tion decoder to integrate local and global features in the token space of transformer, resulting in deep features with high representation capabilities. Finally, deep and shallow features are fused to reconstruct the 3D volume of hidden scenes. Extensive experimental results demonstrate the su-perior performance of the proposed method over existing solutions on both synthetic data and real-world data cap-tured by different NLOS imaging systems. 1.

Introduction
Traditional imaging methods mainly focus on recover-ing information in line-of-sight scenarios, where there are no obstacles on the direct light path between the target and the camera. In contrast, non-line-of-sight (NLOS) imaging targets recovering the hidden scene beyond the direct line of the cameras’ sight, where a diffuse relay surface scat-ters the light from the scene with dramatic loss. Recently,
NLOS imaging has brought tremendous revolutions to au-tonomous driving [2, 20, 35], disaster rescue [18, 43], and medical diagnosis [27].
The time-of-flight (ToF) based imaging scheme is a com-mon configuration in NLOS [9,14,21,29,36], where a laser source projects a short-pulse light to the relay wall. The light propagates from the relay wall to the hidden object,
∗Equal contribution. † Corresponding author. then reflects back to the relay wall and is finally captured with a time-resolved single-photon avalanche diode (SPAD) detector. The hidden volume could be reconstructed by modeling the three bounces of the traveling light, achiev-ing “seeing around corners”.
Existing NLOS reconstruction algorithms have achieved decent results, but are still confronted with great challenges.
Methods based on filter back projection [17, 41] or light path transport [13, 14, 29] often impose restrictive condi-tions, such as an ideal diffuse surface and no occlusions be-hind the wall, resulting in detailed texture loss and heavy noise. Methods based on wave propagation [21, 24] are sensitive to the depth range of hidden objects, making dis-tant regions indistinct. Recently, deep-learning-based meth-ods [9, 10, 28, 36] have been introduced to NLOS recon-struction with improved detailed textures and geometries.
However, there still remains a large room for boosting their performance on complicated scenes and generalization ca-pabilities toward different real-world systems.
Inspired by the success of transformer [25, 40] in a va-riety of vision tasks including 3D reconstruction [7, 19, 39, 42, 45], we propose the first transformer-based method for NLOS reconstruction, termed as NLOST. Our method leverages the powerful representation capability of the transformer for capturing local and global spatial-temporal correlations in 3D NLOS measurements. Specifically, to exploit these correlations, we design an end-to-end neu-ral network with two elaborate attention mechanisms tai-lored for NLOS reconstruction. The network first extracts the shallow features from the NLOS measurements with a feature extractor incorporating physics-based priors. Then, two spatial-temporal self attention encoders built on trans-formers are proposed to extract local and global information from the shallow features, respectively. For the local en-coder, the input features are split into patches, and the local information is exploited in each patch along the spatial and temporal dimensions, successively. For the global encoder, the input features are downsampled to a smaller scale, and the global information is exploited along spatial and tempo-ral dimensions in the whole feature space. The complemen-tary local and global information is further integrated with
each other into the token space of transformers by the pro-posed spatial-temporal cross attention decoder, generating deep local and global features with high representation ca-pabilities. Finally, the above-obtained shallow, deep local, and deep global features are fused together to reconstruct the 3D volume of hidden scenes.
Extensive experiments are performed on both synthetic and real-world datasets. In addition to the publicly avail-able data, we also capture a set of real-world measure-ments with a self-built NLOS imaging system. Compared with existing traditional and deep-learning-based solutions, our method achieves superior reconstruction performance as well as improved generalization capability to real-world scenarios. Contributions are summarized as follows:
• We propose the first transformer-based neural network for NLOS reconstruction.
• We exploit the complementary local and global corre-lations in 3D NLOS measurements with two elaborate spatial-temporal attention mechanisms.
• Our method achieves superior performance on both synthetic data and real-world data from different imag-ing systems.
• We capture a set of NLOS transient measurements with a self-built confocal system and release them for future researches in this field (https://github.com/
Depth2World/NLOST). 2.