Abstract
Implicit Neural Representations (INR) have recently shown to be powerful tool for high-quality video compres-sion. However, existing works are are limiting as they do not exploit the temporal redundancy in videos, leading to a long encoding time. Additionally, these methods have fixed architectures which do not scale to longer videos or higher resolutions. To address these issues, we propose NIRVANA, which treats videos as groups of frames and fits separate networks to each group performing patch-wise prediction.
The video representation is modeled autoregressively, with networks fit on a current group initialized using weights from the previous group’s model. To enhance efficiency, we quantize the parameters during training, requiring no post-hoc pruning or quantization. When compared with previ-ous works on the benchmark UVG dataset, NIRVANA im-proves encoding quality from 37.36 to 37.70 (in terms of
PSNR) and the encoding speed by 12×, while maintain-ing the same compression rate. In contrast to prior video
INR works which struggle with larger resolution and longer videos, we show that our algorithm scales naturally due to its patch-wise and autoregressive design. Moreover, our method achieves variable bitrate compression by adapting to videos with varying inter-frame motion. NIRVANA also achieves 6× decoding speed scaling well with more GPUs, making it practical for various deployment scenarios.1. 1.

Introduction
In the information age today, where petabytes of con-tent is generated and consumed every hour, the ability to compress data fast and reliably is important. Not only does compression make data cheaper for server hosting, it
*First two authors contributed equally
†Work done during internship at Snap Inc. 1The project site can be found here
Figure 1. Overview of NIRVANA: Prior video INR works per-form either pixel-wise or frame-wise prediction. We instead per-form spatio-temporal patch-wise prediction and fit individual neu-ral networks to groups of frames (clips) which are initialized using networks trained on the previous group. Such an autoregressive patch-wise approach exploits both spatial and temporal redundan-cies present in videos while promoting scalability and adaptability to varying video content, resolution or duration.
makes content accessible to population/regions with low-bandwidth. Conventionally, such compression is achieved through codecs like JPEG [44] for images and HEVC [39],
AV1 [8] for videos, each of which compresses data via tar-getted hand-crafted algorithms. These techniques achieve acceptable trade-offs, leading to their widespread usage.
With the rise of deep learning, machine learning-based codecs [3,4,30,42] showed that it is possible to achieve bet-ter performance in some aspects than conventional codecs.
However, these techniques often require large networks as they attempt to generalize to compress all data from the distribution. Furthermore, such generalization is contin-gent on the training dataset used by these models, leading to poor performance for Out-of-Distribution (OOD) data across different domains [46] or even when the resolution changes [5]. This greatly limits its real-world practical-ity especially if the input data to be compressed is sig-nificantly different from what the model has seen during training.
In recent years, a new paradigm, Implicit Neu-ral Representations (INR), emerged to solve the drawbacks of model-learned compression methods. Rather than at-tempting to generalize to all data from a particular distri-bution, its key idea is to train a network that specifically fits to a signal, which can be an image [36], video [6], or even a 3D scene [29]. With this conceptual shift, a neu-ral network is no longer just a predictive tool, rather it is now an efficient storage of data. Treating the neural net-work as the data, INR translate the data compression task to that of model compression. Such a continuous function mapping further benefits downstream tasks such as image super-resolution [23], denoising [31], and inpainting [36].
Despite these advances, videos vary widely in both spa-tial resolutions and temporal lengths, making it challenging for networks to encode videos in a practical setting. To-wards solving this task, SIREN [36], attempted to learn a direct mapping from 3D spatio-temporal coordinates of a video to each pixel’s color values. While simple, this is computationally inefficient and does not factor in the spatio-temporal redundancies within the video. Later, NeRV [6] proposed to map 1D temporal coordinates in the form of frame indices directly to generate a whole frame. While this improves the reconstruction quality, such a mapping still does not capture the temporal redundancies between frames as it treats each frame individually as a separate image en-coding task. Finally, mapping only the temporal coordinate also means one would need to modify the architecture in order to encode videos of different spatial resolutions.
To address the above issues, we propose NIRVANA, a method that exploits spatio-temporal redundancies to en-code videos of arbitrary lengths and resolutions. Rather than performing a pixel-wise prediction (e.g., SIREN) or a whole-frame prediction (e.g., NeRV), we predict patches, which allows our model to adapt to videos of different spa-tial resolutions without modifying the architecture. Our method takes in the centroids of patches (patch coordinates) (xp, yp) as inputs and outputs a corresponding patch vol-ume. Since patches can be arranged for different resolu-tions, we do not require any architectural modification when the input video resolution changes. Furthermore, to exploit the temporal nature of videos, we propose to train individ-ual, small models for each group of video frames (“clips”) in an autoregressive manner: the model weights for pre-dicting each frame group is initialized from the weights of the model for the previous frame group. Apart from the obvious advantage that we can scale to longer sequences without changing the model architecture, this design ex-ploits the temporal nature of videos that, intuitively, frame groups with similar visual information (e.g., static video frames) would have similar weights, allowing us to further perform residual encoding for greater compression gains.
This adaptive nature, that static videos gain greater com-pression than dynamic ones, is a big advantage over NeRV where the compression for identical frames remain fixed as it models each frame individually. To obtain further com-pression gains, we employ recent advances in the literature to add entropy regularization for quantization, and encode model weights for each video during training [17]. This fur-ther adapts the compression level to the complexity of each video, and avoids any post-hoc pruning and fine-tuning as in NeRV, which can be slow.
Finally, we show that despite its autoregressive nature, our model is linearly parallelizable with the number of
GPUs by chunking each video into disjoint groups to be processed. This strategy largely improves the speed while maintaining the superior compression gains of our method, making it practical for various deployment scenarios.
We evaluate NIRVANA on the benchmark UVG dataset
[28]. We show that NIRVANA reaches the same levels of
PSNR and Bits-per-Pixel (BPP) compression rate with al-most 12× the encoding speed of NeRV. We verify that our algorithm adapts to varying spatial and temporal scales by providing results on videos in the UVG dataset with 4K res-olution at 120fps, as well as on significantly longer videos from the YouTube-8M dataset, both of which are challeng-ing extensions which have not been attempted on for this task. We show that our algorithm outperforms the base-line with much smaller encoding times and that it naturally scales with no performance degradation. We conduct abla-tion studies to show the effectiveness of various components of our algorithm in achieving high levels of reconstruction quality and understand the sources of improvements.
Our contributions are summarized below:
• We present NIRVANA, a patch-wise autoregressive video INR framework which exploits both spatial and temporal redundancies in videos to achieve high levels of encoding speedups (12×) at similar reconstruction
quality and compression rates.
• We achieve a 6× speedup in decoding times and scale well with increasing number of GPUs, making it prac-tical in various deployment scenarios.
• Our framework adapts to varying video resolutions and lengths without performance degradations. Different from prior works, it achieves adaptive bitrate compres-sion based on the amount of inter-frame motion for each video. 2.