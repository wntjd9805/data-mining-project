Abstract
In this work, we focus on the challenging problem of La-bel Enhancement (LE), which aims to exactly recover label distributions from logical labels, and present a novel Label
Information Bottleneck (LIB) method for LE. For the recov-ery process of label distributions, the label irrelevant infor-mation contained in the dataset may lead to unsatisfactory recovery performance. To address this limitation, we make efforts to excavate the essential label relevant information to improve the recovery performance. Our method formu-lates the LE problem as the following two joint processes: 1) learning the representation with the essential label rel-evant information, 2) recovering label distributions based on the learned representation. The label relevant informa-tion can be excavated based on the “bottleneck” formed by the learned representation. Significantly, both the label rel-evant information about the label assignments and the label relevant information about the label gaps can be explored in our method. Evaluation experiments conducted on several benchmark label distribution learning datasets verify the ef-fectiveness and competitiveness of LIB. Our source codes are available at https://github.com/qinghai-zheng/LIBLE 1.

Introduction
Learning with label ambiguity is important in computer vision and machine learning. Different from the traditional
Multi-Label Learning (MLL), which employs multiple log-ical labels to annotate one instance to address the label am-biguity issue [20], Label Distribution Learning (LDL) con-siders the relative importance of different labels and draws much attention in recent years [6, 8, 14, 18, 26]. By distin-guishing the description degrees of all labels, LDL anno-tates one instance with a label distribution. Therefore, LDL is a more general learning paradigm, MLL can be regarded as a special case of LDL [8, 10, 12].
*Corresponding author, E-mail: zhujh@xjtu.edu.cn
Recently, many LDL methods are proposed and achieve great success in practice [3, 9, 14, 18]. Instances with exact label distributions are vital for the training process of LDL methods. Nevertheless, annotating instances with label dis-tributions is time-consuming [24,28]. We take the label dis-tribution annotation process of SJAFFE dataset for example here. SJAFFE dataset is the facial expression dataset, which contains 213 grayscale images collected from 10 Japanese female models, each facial expression image is rated by 60 persons on 6 basic emotions, including happiness, surprise, sadness, fear, anger, and disgust, with a five-level scale from 1 - 5, the higher value indicates the higher emotion intensity.
Consequently, the average score of each emotion is served as the emotion label distribution [14,28]. Clearly, the above annotation process is costly and it is unpractical to annotate data with label distributions manually, especially when the number of data is large. Fortunately, most existing datasets in the field of computer vision and machine learning are an-notated by single-label or multi-labels [7, 29], therefore, a highly recommended promising solution is Label Enhance-ment (LE), which attempts to recover the desired label dis-tributions exactly from existing logical labels [24, 28, 32].
Driven by the urgent requirement of obtaining label dis-tributions and the convenience of LE, some LE methods are proposed in recent years [5, 7, 11, 13, 15, 17, 21, 24, 28, 29].
Given a dataset X = {x1, x2, · · · , xn} ∈ Rq×n, in which q and n denote the number of dimensions and the number of instances, the potential label set is {y1, y2, · · · , yc}. The available logical labels and the desired distribution labels of X are separately indicated by L = {l1, l2, · · · , ln} and
D = {d1, d2, · · · , dn}, where li and di are:
, ly2 i
, · · · , lyc li = (ly1 i i )T , di = (dy1
To be specific, LE aims to recover D based on the informa-tion provided by X and L. For most existing LE methods, their objectives can be concisely summarized as follows: i , · · · , dyc i , dy2 i )T . (1) min
θ
∥fθ(X) − L∥2
F + γreg(fθ(X)), (2) in which D = fθ(X), fθ(·) indicates the mapping from
X to D, reg(·) denotes the regularization function, and γ
decompose the label relevant information into two compo-nents, namely the assignments of labels to the instance and the label gaps between label distributions and logical labels.
Inspired by Information Bottleneck (IB) [25], LIB utilizes the existing logical labels to explore the information about the assignments of labels to the instance. Unlike simply em-ploying the original IB on the LE task, our method further considers the information about the label gaps between la-bel distributions and logical labels. It is noteworthy that the above two components of the label relevant information are jointly explored in our method, and that is why we term the proposed method Label Information Bottleneck (LIB). The main contributions can be summarized as follows:
• We decompose the label relevant information into the information about the assignments of labels to instance and the information about the label gaps between logi-cal labels, both of which can be jointly explored during the learning process of our method.
• We introduce a novel LE method, termed LIB, which excavates the label relevant information to exactly re-cover the label distributions. Based on the original IB, which explores the label assignments information for
LE, LIB further explores the label gaps information.
• We verify the effectiveness of LIB by performing ex-tensive experiments on several datasets. Experimental results show that the proposed method can achieve the competitive performance, compared to state-of-the-art
LE methods. 2.