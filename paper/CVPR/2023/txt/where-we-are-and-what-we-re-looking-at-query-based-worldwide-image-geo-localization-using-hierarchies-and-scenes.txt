Abstract 1.

Introduction
Determining the exact latitude and longitude that a photo was taken is a useful and widely applicable task, yet it remains exceptionally difficult despite the acceler-ated progress of other computer vision tasks. Most previ-ous approaches have opted to learn single representations of query images, which are then classified at different lev-els of geographic granularity. These approaches fail to exploit the different visual cues that give context to differ-ent hierarchies, such as the country, state, and city level.
To this end, we introduce an end-to-end transformer-based architecture that exploits the relationship between differ-ent geographic levels (which we refer to as hierarchies) and the corresponding visual scene information in an im-age through hierarchical cross-attention. We achieve this by learning a query for each geographic hierarchy and scene type. Furthermore, we learn a separate representation for different environmental scenes, as different scenes in the same location are often defined by completely different vi-sual features. We achieve state of the art accuracy on 4 standard geo-localization datasets : Im2GPS, Im2GPS3k,
YFCC4k, and YFCC26k, as well as qualitatively demon-strate how our method learns different representations for different visual hierarchies and scenes, which has not been demonstrated in the previous methods. Above previous test-ing datasets mostly consist of iconic landmarks or images taken from social media, which makes the dataset a sim-ple memory task, or makes it biased towards certain places.
To address this issue we introduce a much harder testing dataset, Google-World-Streets-15k, comprised of images taken from Google Streetview covering the whole planet and present state of the art results. Our code can be found at https://github.com/AHKerrigan/GeoGuessNet.
*These authors contributed equally to the work
Image geo-localization is the task of determining the
GPS coordinates of where a photo was taken as precisely as possible. For certain locations, this may be an easy task, as most cities will have noticeable buildings, landmarks, or statues that give away their location. For instance, given an image of the Eiffel Tower one could easily assume it was taken somewhere in Paris. Noticing some of the finer fea-tures, like the size of the tower in the image and other build-ings that might be visible, a prediction within a few meters could be fairly easy. However, given an image from a small town outside of Paris, it may be very hard to predict its loca-tion. Certain trees or a building’s architecture may indicate the image is in France, but localizing finer than that can pose a serious challenge. Adding in different times of day, varying weather conditions, and different views of the same location makes this problem even more complex as two im-ages from the same location could look wildly different.
Many works have explored solutions to this problem, with nearly all works focusing on the retrieval task, where query images are matched to a gallery of geo-tagged images to retrieve matching geo-tagged image [14,16,17,20,24,25].
There are two variations of the retrieval approach to this problem, same-view and cross-view. In same-view both the query and gallery images are taken at ground level. How-ever, in cross-view the query images are ground level while the gallery images are from an aerial view, either by satel-lite or drone. This creates a challenging task as images with the exact same location look very different from one an-other. Regardless of same-view or cross-view, the evalu-ation of the retrieval task is costly as features need to be extracted and compared for every possible match with geo-tagged gallery images, making global scale geo-localization costly if not infeasible.
If, instead, the problem is approached as a classifica-tion task, it’s possible to localize on the global scale given enough training data [8,11,12,15,21,22]. These approaches
segment the Earth into Google’s S21 cells that are assigned
GPS locations and serve as classes, speeding up evaluation.
Most previous classification-based visual geo-localization approaches use the same strategy as any other classifica-tion task: using an image backbone (either a Convolutional
Neural Network or a Vision Transformer [2]), they learn a set of image features and output a probability distribution for each possible location (or class) using an MLP. In more recent works [11,12], using multiple sets of classes that rep-resent different global scales, as well as utilizing informa-tion about the scene characteristics of the image has shown to improve results. These approaches produce one feature vector for an image and presume that it is good enough to localize at every geographic level. However, that is not how a human would reason about finding out their location. If a person had no idea where they were, they would likely search for visual cues for a broad location (country, state) before considering finer areas. Thus, a human would look for a different set of features for each geographic level they want to predict.
In this paper, we introduce a novel approach toward world-wide visual geo-localization inspired by human ex-perts. Typically, humans do not evaluate the entirety of a scene and reason about its features, but rather identify im-portant objects, markers, or landmarks and match them to a cache of knowledge about various known locations. In our approach, we emulate this by using a set of learned latent arrays called “hierarchy queries” that learn a different set of features for each geographic hierarchy. These queries also learn to extract features relative to specific scene types (e.g. forests, sports fields, industrial, etc.). We do this so that our queries can focus more specifically on features relevant to their assigned scene as well as the features related to their assigned hierarchy. This is done via a Transformer Decoder that cross-attends our hierarchy and scene queries with im-age features that are extracted from a backbone. We also implement a “hierarchy dependent decoder” that ensures our model learns the specifics of each individual hierarchy.
To do this our “hierarchy dependent decoder” separates the queries according to their assigned hierarchy, and has inde-pendent weights for the Self-Attention and Feed-Forward stages that are specific to each hierarchy.
We also note that the existing testing datasets contain implicit biases which make them unfit to truly measure a model’s geo-location accuracy. For instance, Im2GPS
[4, 21] datasets contain many images of iconic landmarks, which only tests whether a model has seen and memorized the locations of those landmarks. Also, YFCC [18, 21] testing sets are composed entirely of images posted on-line that contained geo-tags in their metadata. This cre-ates a bias towards locations that are commonly visited and posted online, like tourist sites. Previous work has found 1https://code.google.com/archive/p/s2-geometry-library/ this introduces significant geographical and often racial bi-ases into the datasets [7] which we demonstrate in Fig-ure 4. To this end, we introduce a challenging new test-ing dataset called Google-World-Streets-15k, which is more evenly distributed across the Earth and consists of real-world images from Google Streetview.
The contributions of our paper (1) The include: first Transformer Decoder for worldwide image geo-localization. (2) The first model to produce multiple sets of features for an input image, and the first model capable of extracting scene-specific information without needing a separate network for every scene. (3) A new testing dataset that reduces landmark bias and reduces biases created by social media. (4) A significant improvement over previous
SOTA methods on all datasets. (5) A qualitative analysis of the features our model learns for every hierarchy and scene query. 2.