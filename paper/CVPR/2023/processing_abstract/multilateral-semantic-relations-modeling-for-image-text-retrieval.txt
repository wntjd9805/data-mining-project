Image-text retrieval is a fundamental task to bridge vi-sion and language by exploiting various strategies to ﬁne-grained alignment between regions and words. This is still tough mainly because of one-to-many correspondence, where a set of matches from another modality can be ac-cessed by a random query. While existing solutions to this problem including multi-point mapping, probabilistic distribution, and geometric embedding have made promis-ing progress, one-to-many correspondence is still under-explored. In this work, we develop a Multilateral SemanticRelations Modeling (termed MSRM) for image-text re-trieval to capture the one-to-many correspondence between multiple samples and a given query via hypergraph model-ing. Speciﬁcally, a given query is ﬁrst mapped as a prob-abilistic embedding to learn its true semantic distribution based on Mahalanobis distance. Then each candidate in-stance in a mini-batch is regarded as a hypergraph node with its mean semantics while a Gaussian query is mod-eled as a hyperedge to capture the semantic correlations beyond the pair between candidate points and the query.Comprehensive experimental results on two widely used datasets demonstrate that our MSRM method can outper-form state-of-the-art methods in the settlement of multi-ple matches while still maintaining the comparable perfor-mance of instance-level matching. 