] 0 1[Current top-leading solutions for video object segmen-tation (VOS) typically follow a matching-based regime: for each query frame, the segmentation mask is inferred accor-ding to its correspondence to previously processed and theﬁrst annotated frames. They simply exploit the supervisory signals from the groundtruth masks for learning mask pre-diction only, without posing any constraint on the space-time correspondence matching, which, however, is the fundamen-tal building block of such regime. To alleviate this crucial yet commonly ignored issue, we devise a correspondence-aware training framework, which boosts matching-based VOS so-lutions by explicitly encouraging robust correspondence ma-tching during network learning. Through comprehensively exploring the intrinsic coherence in videos on pixel and ob-ject levels, our algorithm reinforces the standard, fully su-pervised training of mask segmentation with label-free, con-trastive correspondence learning. Without neither requiring extra annotation cost during training, nor causing speed de-lay during deployment, nor incurring architectural modiﬁ-cation, our algorithm provides solid performance gains on four widely used benchmarks, i.e., DAVIS2016&2017, andYouTube-VOS2018&2019, on the top of famous matching-based VOS solutions. 