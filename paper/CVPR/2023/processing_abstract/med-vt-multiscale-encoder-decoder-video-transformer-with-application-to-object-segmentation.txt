Multiscale video transformers have been explored in a wide variety of vision tasks. To date, however, the multiscale processing has been confined to the encoder or decoder alone. We present a unified multiscale encoder-decoder transformer that is focused on dense prediction tasks in videos. Multiscale representation at both encoder and de-coder yields key benefits of implicit extraction of spatiotem-poral features (i.e. without reliance on input optical flow) as well as temporal consistency at encoding and coarse-to-fine detection for high-level (e.g. object) semantics to guide precise localization at decoding. Moreover, we pro-pose a transductive learning scheme through many-to-many label propagation to provide temporally consistent predic-tions. We showcase our Multiscale Encoder-Decoder VideoTransformer (MED-VT) on Automatic Video Object Seg-mentation (AVOS) and actor/action segmentation, where we outperform state-of-the-art approaches on multiple bench-marks using only raw images, without using optical flow. 