In the context of incremental class learning, deep neu-ral networks are prone to catastrophic forgetting, where the accuracy of old classes declines substantially as new knowl-edge is learned. While recent studies have sought to address this issue, most approaches suffer from either the stability-plasticity dilemma or excessive computational and param-eter requirements. To tackle these challenges, we propose a novel framework, the Diverse Knowledge Transfer Trans-former (DKT), which incorporates two knowledge trans-fer mechanisms that use attention mechanisms to transfer both task-specific and task-general knowledge to the current task, along with a duplex classifier to address the stability-plasticity dilemma. Additionally, we design a loss func-tion that clusters similar categories and discriminates be-tween old and new tasks in the feature space. The pro-posed method requires only a small number of extra param-eters, which are negligible in comparison to the increas-ing number of tasks. We perform extensive experiments on CIFAR100, ImageNet100, and ImageNet1000 datasets, which demonstrate that our method outperforms other com-petitive methods and achieves state-of-the-art performance.Our source code is available at https://github.com/MIV-XJTU/DKT. 