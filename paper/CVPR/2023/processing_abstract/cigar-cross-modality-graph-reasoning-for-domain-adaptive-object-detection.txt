Unsupervised domain adaptive object detection (UDA-OD) aims to learn a detector by generalizing knowledge from a labeled source domain to an unlabeled target do-main. Though the existing graph-based methods for UDA-OD perform well in some cases, they cannot learn a proper node set for the graph. In addition, these methods build the graph solely based on the visual features and do not con-sider the linguistic knowledge carried by the semantic pro-totypes, e.g., dataset labels. To overcome these problems, we propose a cross-modality graph reasoning adaptation (CIGAR) method to take advantage of both visual and lin-guistic knowledge. Specifically, our method performs cross-modality graph reasoning between the linguistic modality graph and visual modality graphs to enhance their repre-sentations. We also propose a discriminative feature selec-tor to find the most discriminative features and take them as the nodes of the visual graph for both efficiency and effec-tiveness. In addition, we employ the linguistic graph match-ing loss to regulate the update of linguistic graphs and maintain their semantic representation during the training process. Comprehensive experiments validate the effective-ness of our proposed CIGAR. 