Counterfactual explanations and adversarial attacks have a related goal: flipping output labels with minimal perturbations regardless of their characteristics. Yet, ad-versarial attacks cannot be used directly in a counterfac-tual explanation perspective, as such perturbations are per-ceived as noise and not as actionable and understandable image modifications. Building on the robust learning liter-ature, this paper proposes an elegant method to turn adver-sarial attacks into semantically meaningful perturbations, without modifying the classifiers to explain. The proposed approach hypothesizes that Denoising Diffusion Probabilis-tic Models are excellent regularizers for avoiding high-frequency and out-of-distribution perturbations when gen-erating adversarial attacks. The paperâ€™s key idea is to build attacks through a diffusion model to polish them. This al-lows studying the target model regardless of its robustifi-cation level. Extensive experimentation shows the advan-tages of our counterfactual explanation approach over cur-rent State-of-the-Art in multiple testbeds. 