Recent studies in Vision-and-Language Navigation (VLN) train RL agents to execute natural-language navi-gation instructions in photorealistic environments, as a step towards robots that can follow human instructions. How-ever, given the scarcity of human instruction data and lim-ited diversity in the training environments, these agents still struggle with complex language grounding and spa-tial language understanding. Pretraining on large text and image-text datasets from the web has been extensively explored but the improvements are limited. We investi-gate large-scale augmentation with synthetic instructions.We take 500+ indoor environments captured in densely-sampled 360◦ panoramas, construct navigation trajecto-ries through these panoramas, and generate a visually-grounded instruction for each trajectory using Marky [63], a high-quality multilingual navigation instruction genera-tor. We also synthesize image observations from novel view-points using an image-to-image GAN [27]. The resulting dataset of 4.2M instruction-trajectory pairs is two orders of magnitude larger than existing human-annotated datasets, and contains a wider variety of environments and view-points. To efﬁciently leverage data at this scale, we train a simple transformer agent with imitation learning. On the challenging RxR dataset, our approach outperforms all ex-isting RL agents, improving the state-of-the-art NDTW from 71.1 to 79.1 in seen environments, and from 64.6 to 66.8 in unseen test environments. Our work points to a new path to improving instruction-following agents, emphasizing large-scale training on near-human quality synthetic instructions. 