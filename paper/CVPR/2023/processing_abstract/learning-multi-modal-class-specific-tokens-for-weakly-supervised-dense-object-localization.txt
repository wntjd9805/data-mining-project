Weakly supervised dense object localization (WSDOL) relies generally on Class Activation Mapping (CAM), which exploits the correlation between the class weights of the im-age classifier and the pixel-level features. Due to the lim-ited ability to address intra-class variations, the image clas-sifier cannot properly associate the pixel features, leading to inaccurate dense localization maps.In this paper, we propose to explicitly construct multi-modal class represen-tations by leveraging the Contrastive Language-Image Pre-training (CLIP), to guide dense localization. More specifi-cally, we propose a unified transformer framework to learn two-modalities of class-specific tokens, i.e., class-specific visual and textual tokens. The former captures semantics from the target visual data while the latter exploits the class-related language priors from CLIP, providing complemen-tary information to better perceive the intra-class diversi-In addition, we propose to enrich the multi-modal ties. class-specific tokens with sample-specific contexts compris-ing visual context and image-language context. This en-ables more adaptive class representation learning, which further facilitates dense localization. Extensive experiments show the superiority of the proposed method for WSDOL on two multi-label datasets, i.e., PASCAL VOC and MS COCO, and one single-label dataset, i.e., OpenImages. Our dense localization maps also lead to the state-of-the-art weakly supervised semantic segmentation (WSSS) results on PAS-CAL VOC and MS COCO. 1 