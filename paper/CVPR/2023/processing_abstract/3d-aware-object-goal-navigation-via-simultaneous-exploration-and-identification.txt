Object goal navigation (ObjectNav) in unseen environ-ments is a fundamental task for Embodied AI. Agents in ex-isting works learn ObjectNav policies based on 2D maps, scene graphs, or image sequences. Considering this task happens in 3D space, a 3D-aware agent can advance itsObjectNav capability via learning from fine-grained spa-tial information. However, leveraging 3D scene representa-tion can be prohibitively unpractical for policy learning in this floor-level task, due to low sample efficiency and expen-sive computational cost. In this work, we propose a frame-work for the challenging 3D-aware ObjectNav based on two straightforward sub-policies. The two sub-polices, namely corner-guided exploration policy and category-aware iden-tification policy, simultaneously perform by utilizing on-line fused 3D points as observation. Through extensive ex-periments, we show that this framework can dramatically improve the performance in ObjectNav through learning from 3D scene representation. Our framework achieves the best performance among all modular-based methods on theMatterport3D and Gibson datasets, while requiring (up to 30x) less computational cost for training. The code will be released to benefit the community.1 