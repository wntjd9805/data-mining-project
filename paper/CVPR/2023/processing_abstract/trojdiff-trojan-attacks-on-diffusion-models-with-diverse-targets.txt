Diffusion models have achieved great success in a range of tasks, such as image synthesis and molecule design. As such successes hinge on large-scale training data collected from diverse sources, the trustworthiness of these collected data is hard to control or audit. In this work, we aim to explore the vulnerabilities of diffusion models under poten-tial training data manipulations and try to answer: How hard is it to perform Trojan attacks on well-trained diffu-sion models? What are the adversarial targets that suchTrojan attacks can achieve? To answer these questions, we propose an effective Trojan attack against diffusion mod-els, TrojDiff, which optimizes the Trojan diffusion and gen-erative processes during training.In particular, we de-sign novel transitions during the Trojan diffusion process to diffuse adversarial targets into a biased Gaussian dis-tribution and propose a new parameterization of the Tro-jan generative process that leads to an effective training objective for the attack.In addition, we consider three types of adversarial targets: the Trojaned diffusion models will always output instances belonging to a certain class from the in-domain distribution (In-D2D attack), out-of-domain distribution (Out-D2D-attack), and one specific in-stance (D2I attack). We evaluate TrojDiff on CIFAR-10 and CelebA datasets against both DDPM and DDIM dif-fusion models. We show that TrojDiff always achieves high attack performance under different adversarial targets us-ing different types of triggers, while the performance in be-nign environments is preserved. The code is available at https://github.com/chenweixin107/TrojDiff. 