Prompt learning is an efficient approach to adapt trans-formers by inserting learnable set of parameters into the input and intermediate representations of a pre-trained model. In this work, we present Expressive Prompts withResiduals (EXPRES) which modifies the prompt learn-ing paradigm specifically for effective adaptation of vi-sion transformers (ViT). Our method constructs down-stream representations via learnable “output” tokens (shal-low prompts), that are akin to the learned class tokens of theViT. Further for better steering of the downstream repre-sentation processed by the frozen transformer, we introduce residual learnable tokens that are added to the output of various computations. We apply EXPRES for image classi-fication and few-shot semantic segmentation, and show our method is capable of achieving state of the art prompt tun-ing on 3/3 categories of the VTAB benchmark. In addition to strong performance, we observe that our approach is an order of magnitude more prompt efficient than existing vi-sual prompting baselines. We analytically show the compu-tational benefits of our approach over weight space adap-tation techniques like finetuning. Lastly we systematically corroborate the architectural design of our method via a series of ablation experiments. 