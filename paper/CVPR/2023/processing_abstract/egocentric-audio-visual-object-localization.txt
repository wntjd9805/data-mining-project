Humans naturally perceive surrounding scenes by uni-fying sound and sight from a first-person view. Likewise, machines are advanced to approach human intelligence by learning with multisensory inputs from an egocentric per-spective.In this paper, we explore the challenging ego-centric audio-visual object localization task and observe that 1) egomotion commonly exists in first-person record-ings, even within a short duration; 2) The out-of-view sound components can be created when wearers shift their atten-tion. To address the first problem, we propose a geometry-aware temporal aggregation module that handles the ego-motion explicitly. The effect of egomotion is mitigated by estimating the temporal geometry transformation and ex-ploiting it to update visual representations. Moreover, we propose a cascaded feature enhancement module to over-come the second issue.It improves cross-modal local-ization robustness by disentangling visually-indicated au-dio representation. During training, we take advantage of the naturally occurring audio-visual temporal synchro-nization as the “free” self-supervision to avoid costly la-beling. We also annotate and create the Epic SoundingObject dataset for evaluation purposes. Extensive experi-ments show that our method achieves state-of-the-art local-ization performance in egocentric videos and can be gener-alized to diverse audio-visual scenes. Code is available at https://github.com/WikiChao/Ego-AV-Loc. 