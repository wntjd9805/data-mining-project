Recent vision-based reinforcement learning (RL) meth-ods have found extracting high-level features from raw pix-els with self-supervised learning to be effective in learning policies. However, these methods focus on learning global representations of images, and disregard local spatial struc-tures present in the consecutively stacked frames. In this paper, we propose a novel approach, termed self-supervisedPaired Similarity Representation Learning (PSRL) for effec-tively encoding spatial structures in an unsupervised manner.Given the input frames, the latent volumes are first gener-ated individually using an encoder, and they are used to capture the variance in terms of local spatial structures, i.e., correspondence maps among multiple frames. This enables for providing plenty of fine-grained samples for training the encoder of deep RL. We further attempt to learn the global se-mantic representations in the action aware transform module that predicts future state representations using action vec-tors as a medium. The proposed method imposes similarity constraints on the three latent volumes; transformed query representations by estimated pixel-wise correspondence, pre-dicted query representations from the action aware transform model, and target representations of future state, guiding action aware transform with locality-inherent volume. Ex-perimental results on complex tasks in Atari Games andDeepMind Control Suite demonstrate that the RL methods are significantly boosted by the proposed self-supervised learning of paired similarity representations. 