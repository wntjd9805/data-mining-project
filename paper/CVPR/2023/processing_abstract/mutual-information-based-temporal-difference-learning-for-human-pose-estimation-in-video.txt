Temporal modeling is crucial for multi-frame human pose estimation. Most existing methods directly employ full-optical flow or deformable convolution to predict spectrum motion fields, which might incur numerous irrele-vant cues, such as a nearby person or background. Without further efforts to excavate meaningful motion priors, their results are suboptimal, especially in complicated spatio-temporal interactions. On the other hand, the temporal difference has the ability to encode representative motion information which can potentially be valuable for pose es-timation but has not been fully exploited. In this paper, we present a novel multi-frame human pose estimation frame-work, which employs temporal differences across frames to model dynamic contexts and engages mutual information objectively to facilitate useful motion information disen-tanglement. To be specific, we design a multi-stage Tem-poral Difference Encoder that performs incremental cas-caded learning conditioned on multi-stage feature differ-ence sequences to derive informative motion representa-tion. We further propose a Representation Disentanglement module from the mutual information perspective, which can grasp discriminative task-relevant motion signals by explic-itly defining useful and noisy constituents of the raw motion features and minimizing their mutual information. These place us to rank No.1 in the Crowd Pose Estimation in Com-plex Events Challenge on benchmark dataset HiEve, and achieve state-of-the-art performance on three benchmarksPoseTrack2017, PoseTrack2018, and PoseTrack21. 