The success of multi-task learning (MTL) can largely be attributed to the shared representation of related tasks, al-lowing the models to better generalise. In deep learning, this is usually achieved by sharing a common neural net-work architecture and jointly training the weights. How-ever, the joint training of weighting parameters on mul-tiple related tasks may lead to performance degradation, known as negative transfer. To address this issue, this work proposes an evolutionary multi-tasking neural architecture search (EMT-NAS) algorithm to accelerate the search pro-cess by transferring architectural knowledge across mul-tiple related tasks.In EMT-NAS, unlike the traditionalMTL, the model for each task has a personalised network thus offering the ca-architecture and its own weights, pability of effectively alleviating negative transfer. A fit-ness re-evaluation method is suggested to alleviate fluctu-ations in performance evaluations resulting from param-eter sharing and the mini-batch gradient descent training method, thereby avoiding losing promising solutions during the search process. To rigorously verify the performance ofEMT-NAS, the classification tasks used in the empirical as-sessments are derived from different datasets, including theCIFAR-10 and CIFAR-100, and four MedMNIST datasets.Extensive comparative experiments on different numbers of tasks demonstrate that EMT-NAS takes 8% and up to 40% on CIFAR and MedMNIST, respectively, less time to find competitive neural architectures than its single-task coun-terparts. 