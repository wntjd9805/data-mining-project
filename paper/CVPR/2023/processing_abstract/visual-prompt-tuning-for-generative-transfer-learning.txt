Learning generative image models from various domains efﬁciently needs transferring knowledge from an image syn-thesis model trained on a large dataset. We present a recipe for learning vision transformers by generative knowledge transfer. We base our framework on generative vision trans-formers representing an image as a sequence of visual to-kens with the autoregressive or non-autoregressive trans-formers. To adapt to a new domain, we employ prompt tun-ing, which prepends learnable tokens called prompts to the image token sequence and introduces a new prompt design for our task. We study on a variety of visual domains with varying amounts of training images. We show the effective-ness of knowledge transfer and a signiﬁcantly better image generation quality.1 