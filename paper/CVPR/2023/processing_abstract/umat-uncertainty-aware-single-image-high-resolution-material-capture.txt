We propose a learning-based method to recover normals, specularity, and roughness from a single diffuse image of a material, using microgeometry appearance as our primary cue. Previous methods that work on single images tend to produce over-smooth outputs with artifacts, operate at lim-ited resolution, or train one model per class with little room for generalization. In contrast, in this work, we propose a novel capture approach that leverages a generative network with attention and a U-Net discriminator, which shows out-standing performance integrating global information at re-duced computational complexity. We showcase the perfor-mance of our method with a real dataset of digitized textile materials and show that a commodity flatbed scanner can produce the type of diffuse illumination required as input to our method. Additionally, because the problem might be ill-posed –more than a single diffuse image might be needed to disambiguate the specular reflection– or because the train-ing dataset is not representative enough of the real distribu-tion, we propose a novel framework to quantify the model’s confidence about its prediction at test time. Our method is the first one to deal with the problem of modeling un-certainty in material digitization, increasing the trustwor-thiness of the process and enabling more intelligent strate-gies for dataset creation, as we demonstrate with an active learning experiment. 