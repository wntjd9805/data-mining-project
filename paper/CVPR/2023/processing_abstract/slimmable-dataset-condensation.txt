Dataset distillation, also known as dataset condensation, aims to compress a large dataset into a compact synthetic one. Existing methods perform dataset condensation by as-suming a fixed storage or transmission budget. When the budget changes, however, they have to repeat the synthe-sizing process with access to original datasets, which is highly cumbersome if not infeasible at all. In this paper, we explore the problem of slimmable dataset condensation, to extract a smaller synthetic dataset given only previous condensation results. We first study the limitations of exist-ing dataset condensation algorithms on such a successive compression setting and identify two key factors: (1) the in-consistency of neural networks over different compression times and (2) the underdetermined solution space for syn-thetic data. Accordingly, we propose a novel training ob-jective for slimmable dataset condensation to explicitly ac-count for both factors. Moreover, synthetic datasets in our method adopt a significance-aware parameterization. The-oretical derivation indicates that an upper-bounded error can be achieved by discarding the minor components with-out training. Alternatively, if training is allowed, this strat-egy can serve as a strong initialization that enables a fast convergence. Extensive comparisons and ablations demon-strate the superiority of the proposed solution over existing methods on multiple benchmarks. 