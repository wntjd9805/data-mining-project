Recent advances in 3D scene representation and novel view synthesis have witnessed the rise of Neural RadianceFields (NeRFs). Nevertheless, it is not trivial to exploitNeRF for the photorealistic 3D scene stylization task, which aims to generate visually consistent and photorealistic styl-ized scenes from novel views. Simply coupling NeRF with photorealistic style transfer (PST) will result in cross-view inconsistency and degradation of stylized view syntheses.Through a thorough analysis, we demonstrate that this non-trivial task can be simplified in a new light: When trans-forming the appearance representation of a pre-trainedNeRF with Lipschitz mapping, the consistency and pho-torealism across source views will be seamlessly encoded into the syntheses. That motivates us to build a concise and flexible learning framework namely LipRF, which upgrades arbitrary 2D PST methods with Lipschitz mapping tailored for the 3D scene. Technically, LipRF first pre-trains a ra-diance field to reconstruct the 3D scene, and then emulates the style on each view by 2D PST as the prior to learn a Lip-schitz network to stylize the pre-trained appearance. In view of that Lipschitz condition highly impacts the expressivity of the neural network, we devise an adaptive regularization to balance the reconstruction and stylization. A gradual gra-dient aggregation strategy is further introduced to optimizeLipRF in a cost-efficient manner. We conduct extensive ex-periments to show the high quality and robust performance of LipRF on both photorealistic 3D stylization and object appearance editing. 