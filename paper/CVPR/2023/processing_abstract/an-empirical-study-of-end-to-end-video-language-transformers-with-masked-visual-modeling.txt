Masked visual modeling (MVM) has been recently proven effective for visual pre-training. While similar re-constructive objectives on video inputs (e.g., masked frame modeling) have been explored in video-language (VidL) pre-training, previous studies fail to find a truly effectiveMVM strategy that can largely benefit the downstream per-formance. In this work, we systematically examine the po-tential of MVM in the context of VidL learning. Specifically, we base our study on a fully end-to-end VIdeO-LanguagETransformer (VIOLET) [15], where the supervision fromMVM training can be backpropogated to the video pixel space.In total, eight different reconstructive targets ofMVM are explored, from low-level pixel values and oriented gradients to high-level depth maps, optical flow, discrete visual tokens and latent visual features. We conduct com-prehensive experiments and provide insights into the fac-tors leading to effective MVM training, resulting in an en-hanced model VIOLETv2. Empirically, we show VIOLETv2 pre-trained with MVM objective achieves notable improve-ments on 13 VidL benchmarks, ranging from video question answering, video captioning, to text-to-video retrieval.1 