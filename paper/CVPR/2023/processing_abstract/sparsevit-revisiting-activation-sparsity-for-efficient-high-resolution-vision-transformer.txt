High-resolution images enable neural networks to learn richer visual representations. However, this improved per-formance comes at the cost of growing computational com-plexity, hindering their usage in latency-sensitive applica-tions. As not all pixels are equal, skipping computations for less-important regions offers a simple and effective mea-sure to reduce the computation. This, however, is hard to be translated into actual speedup for CNNs since it breaks the regularity of the dense convolution workload. In this paper, we introduce SparseViT that revisits activation spar-sity for recent window-based vision transformers (ViTs). As window attentions are naturally batched over blocks, actual speedup with window activation pruning becomes possible: i.e., ∼50% latency reduction with 60% sparsity. Different layers should be assigned with different pruning ratios due to their diverse sensitivities and computational costs. We intro-duce sparsity-aware adaptation and apply the evolutionary search to efficiently find the optimal layerwise sparsity con-figuration within the vast search space. SparseViT achieves speedups of 1.5×, 1.4×, and 1.3× compared to its dense counterpart in monocular 3D object detection, 2D instance segmentation, and 2D semantic segmentation, respectively, with negligible to no loss of accuracy. 