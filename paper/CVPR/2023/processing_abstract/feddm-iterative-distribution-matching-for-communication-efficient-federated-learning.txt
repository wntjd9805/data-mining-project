Federated learning (FL) has recently attracted increas-ing attention from academia and industry, with the ultimate goal of achieving collaborative training under privacy and communication constraints. Existing iterative model av-eraging based FL algorithms require a large number of communication rounds to obtain a well-performed model due to extremely unbalanced and non-i.i.d data partitioning among different clients. Thus, we propose FedDM to build the global training objective from multiple local surrogate functions, which enables the server to gain a more global view of the loss landscape.In detail, we construct syn-thetic sets of data on each client to locally match the loss landscape from original data through distribution match-ing. FedDM reduces communication rounds and improves model quality by transmitting more informative and smaller synthesized data compared with unwieldy model weights.We conduct extensive experiments on three image classifica-tion datasets, and show that our method outperforms otherFL counterparts in terms of efficiency and model perfor-mance given a limited number of communication rounds.Moreover, we demonstrate that FedDM can be adapted to preserve differential privacy with Gaussian mechanism and train a better model under the same privacy budget. 