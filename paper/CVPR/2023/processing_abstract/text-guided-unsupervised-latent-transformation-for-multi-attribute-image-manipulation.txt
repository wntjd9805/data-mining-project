Great progress has been made in StyleGAN-based im-age editing. To associate with preset attributes, most ex-isting approaches focus on supervised learning for seman-tically meaningful latent space traversal directions, and each manipulation step is typically determined for an in-dividual attribute. To address this limitation, we propose aText-guided Unsupervised StyleGAN Latent Transformation (TUSLT) model, which adaptively infers a single transfor-mation step in the latent space of StyleGAN to simultane-ously manipulate multiple attributes on a given input image.Speciﬁcally, we adopt a two-stage architecture for a latent mapping network to break down the transformation process into two manageable steps. Our network ﬁrst learns a di-verse set of semantic directions tailored to an input image, and later nonlinearly fuses the ones associated with the tar-get attributes to infer a residual vector. The resulting tightly interlinked two-stage architecture delivers the ﬂexibility to handle diverse attribute combinations. By leveraging the cross-modal text-image representation of CLIP, we can per-form pseudo annotations based on the semantic similarity between preset attribute text descriptions and training im-ages, and further jointly train an auxiliary attribute clas-siﬁer with the latent mapping network to provide semantic guidance. We perform extensive experiments to demonstrate that the adopted strategies contribute to the superior perfor-mance of TUSLT. 