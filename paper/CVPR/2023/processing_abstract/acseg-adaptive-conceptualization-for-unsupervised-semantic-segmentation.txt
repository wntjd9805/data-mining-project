Recently, self-supervised large-scale visual pre-training models have shown great promise in representing pixel-level semantic relationships, significantly promoting the de-velopment of unsupervised dense prediction tasks, e.g., un-supervised semantic segmentation (USS). The extracted re-lationship among pixel-level representations typically con-tains rich class-aware information that semantically iden-tical pixel embeddings in the representation space gather together to form sophisticated concepts. However, lever-aging the learned models to ascertain semantically con-sistent pixel groups or regions in the image is non-trivial since over/ under-clustering overwhelms the conceptualiza-tion procedure under various semantic distributions of dif-ferent images. In this work, we investigate the pixel-level semantic aggregation in self-supervised ViT pre-trained models as image Segmentation and propose the AdaptiveConceptualization approach for USS, termed ACSeg. Con-cretely, we explicitly encode concepts into learnable proto-types and design the Adaptive Concept Generator (ACG), which adaptively maps these prototypes to informative con-cepts for each image. Meanwhile, considering the scene complexity of different images, we propose the modularity loss to optimize ACG independent of the concept number based on estimating the intensity of pixel pairs belonging to the same concept. Finally, we turn the USS task into clas-sifying the discovered concepts in an unsupervised manner.Extensive experiments with state-of-the-art results demon-strate the effectiveness of the proposed ACSeg. 