Generating talking head videos through a face image and a piece of speech audio still contains many challenges. i.e., unnatural head movement, distorted expression, and identity modification. We argue that these issues are mainly caused by learning from the coupled 2D motion fields. On the other hand, explicitly using 3D information also suffers problems of stiff expression and incoherent video. We presentSadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation. To learn the realistic motion coefficients, we explicitly model the connections between audio and differ-ent types of motion coefficients individually. Precisely, we present ExpNet to learn the accurate facial expression from audio by distilling both coefficients and 3D-rendered faces.As for the head pose, we design PoseVAE via a conditionalVAE to synthesize head motion in different styles. Finally, the generated 3D motion coefficients are mapped to the un-supervised 3D keypoints space of the proposed face render to synthesize the final video. We conducted extensive experi-ments to demonstrate the superiority of our method in terms of motion and video quality. 1* Equal Contributionâ€  Corresponding Author 1The code and demo videos are available at https://sadtalker. github.io.