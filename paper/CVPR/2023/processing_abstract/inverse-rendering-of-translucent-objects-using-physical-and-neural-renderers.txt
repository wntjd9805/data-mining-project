In this work, we propose an inverse rendering model that estimates 3D shape, spatially-varying reflectance, homoge-neous subsurface scattering parameters, and an environ-ment illumination jointly from only a pair of captured im-ages of a translucent object. In order to solve the ambigu-ity problem of inverse rendering, we use a physically-based renderer and a neural renderer for scene reconstruction and material editing. Because two renderers are differentiable, we can compute a reconstruction loss to assist parameter estimation. To enhance the supervision of the proposed neu-ral renderer, we also propose an augmented loss. In addi-tion, we use a flash and no-flash image pair as the input.To supervise the training, we constructed a large-scale syn-thetic dataset of translucent objects, which consists of 117K scenes. Qualitative and quantitative results on both syn-thetic and real-world datasets demonstrated the effective-ness of the proposed model. Code and Data are available at https://github.com/ligoudaner377/homo translucent 