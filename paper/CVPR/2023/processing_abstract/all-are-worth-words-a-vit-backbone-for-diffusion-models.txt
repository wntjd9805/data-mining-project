Vision transformers (ViT) have shown promise in vari-ous vision tasks while the U-Net based on a convolutional neural network (CNN) remains dominant in diffusion mod-els. We design a simple and general ViT-based architecture (named U-ViT) for image generation with diffusion mod-els. U-ViT is characterized by treating all inputs includ-ing the time, condition and noisy image patches as tokens and employing long skip connections between shallow and deep layers. We evaluate U-ViT in unconditional and class-conditional image generation, as well as text-to-image gen-eration tasks, where U-ViT is comparable if not superior to a CNN-based U-Net of a similar size. In particular, latent diffusion models with U-ViT achieve record-breaking FID scores of 2.29 in class-conditional image generation on Im-ageNet 256Ã—256, and 5.48 in text-to-image generation onMS-COCO, among methods without accessing large exter-nal datasets during the training of generative models.Our results suggest that, for diffusion-based image mod-eling, the long skip connection is crucial while the down-sampling and up-sampling operators in CNN-based U-Net are not always necessary. We believe that U-ViT can pro-vide insights for future research on backbones in diffu-sion models and benefit generative modeling on large scale cross-modality datasets. 