Deep learning has achieved great success in recent years with the aid of advanced neural network structures and large-scale human-annotated datasets. However, it is of-ten costly and difficult to accurately and efficiently anno-tate large-scale datasets, especially for some specialized domains where fine-grained labels are required. In this set-ting, coarse labels are much easier to acquire as they do not require expert knowledge. In this work, we propose a con-trastive learning method, called masked contrastive learn-ing (MaskCon) to address the under-explored problem set-ting, where we learn with a coarse-labelled dataset in or-der to address a finer labelling problem. More specifically, within the contrastive learning framework, for each sam-ple our method generates soft-labels with the aid of coarse labels against other samples and another augmented view of the sample in question. By contrast to self-supervised contrastive learning where only the sampleâ€™s augmentations are considered hard positives, and in supervised contrastive learning where only samples with the same coarse labels are considered hard positives, we propose soft labels based on sample distances, that are masked by the coarse labels.This allows us to utilize both inter-sample relations and coarse labels. We demonstrate that our method can obtain as special cases many existing state-of-the-art works and that it provides tighter bounds on the generalization error.Experimentally, our method achieves significant improve-ment over the current state-of-the-art in various datasets, including CIFAR10, CIFAR100, ImageNet-1K, StandfordOnline Products and Stanford Cars196 datasets. Code and annotations are available at https://github.com/MrChenFeng/MaskCon_CVPR2023. 