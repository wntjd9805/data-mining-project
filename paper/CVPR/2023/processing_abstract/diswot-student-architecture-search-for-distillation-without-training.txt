Knowledge distillation (KD) is an effective training strat-egy to improve the lightweight student models under the guidance of cumbersome teachers. However, the large ar-chitecture difference across the teacher-student pairs lim-its the distillation gains.In contrast to previous adap-tive distillation methods to reduce the teacher-student gap, we explore a novel training-free framework to search for the best student architectures for a given teacher. Our work ﬁrst empirically show that the optimal model un-der vanilla training cannot be the winner in distillation.Secondly, we ﬁnd that the similarity of feature semantics and sample relations between random-initialized teacher-student networks have good correlations with ﬁnal distil-lation performances. Thus, we efﬁciently measure similar-ity matrixs conditioned on the semantic activation maps to select the optimal student via an evolutionary algorithmIn this way, our student architec-without any training. ture search for Distillation WithOut Training (DisWOT) sig-niﬁcantly improves the performance of the model in the distillation stage with at least 180× training acceleration.Additionally, we extend similarity metrics in DisWOT as new distillers and KD-based zero-proxies. Our experiments on CIFAR, ImageNet and NAS-Bench-201 demonstrate that our technique achieves state-of-the-art results on different search spaces. Our project and code are available at https://lilujunai.github.io/DisWOT-CVPR2023/. 