Generative Adversarial Networks (GANs) are notori-ously difficult to train especially for complex distributions and with limited data. This has driven the need for tools to audit trained networks in human intelligible format, for example, to identify biases or ensure fairness. ExistingGAN audit tools are restricted to coarse-grained, model-data comparisons based on summary statistics such as FIDIn this paper, we propose an alternative ap-or recall. proach that compares a newly developed GAN against a prior baseline. To this end, we introduce Cross-GAN Audit-ing (xGA) that, given an established “reference” GAN and a newly proposed “client” GAN, jointly identifies intelligi-ble attributes that are either common across both GANs, novel to the client GAN, or missing from the client GAN.This provides both users and model developers an intuitive assessment of similarity and differences between GANs. We introduce novel metrics to evaluate attribute-based GAN auditing approaches and use these metrics to demonstrate quantitatively that xGA outperforms baseline approaches.We also include qualitative results that illustrate the com-mon, novel and missing attributes identified by xGA fromGANs trained on a variety of image datasets1. 