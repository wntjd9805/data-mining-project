Conditional image-to-video (cI2V) generation aims to synthesize a new plausible video starting from an image (e.g., a personâ€™s face) and a condition (e.g., an action class label like smile). The key challenge of the cI2V task lies in the simultaneous generation of realistic spatial appear-ance and temporal dynamics corresponding to the given image and condition.In this paper, we propose an ap-proach for cI2V using novel latent flow diffusion models (LFDM) that synthesize an optical flow sequence in the la-tent space based on the given condition to warp the given image. Compared to previous direct-synthesis-based works, our proposed LFDM can better synthesize spatial details and temporal motion by fully utilizing the spatial content of the given image and warping it in the latent space accord-ing to the generated temporally-coherent flow. The training of LFDM consists of two separate stages: (1) an unsuper-vised learning stage to train a latent flow auto-encoder for spatial content generation, including a flow predictor to es-timate latent flow between pairs of video frames, and (2) a conditional learning stage to train a 3D-UNet-based diffu-sion model (DM) for temporal latent flow generation. Un-like previous DMs operating in pixel space or latent feature space that couples spatial and temporal information, theDM in our LFDM only needs to learn a low-dimensional latent flow space for motion generation, thus being more computationally efficient. We conduct comprehensive ex-periments on multiple datasets, where LFDM consistently outperforms prior arts. Furthermore, we show that LFDM can be easily adapted to new domains by simply finetun-ing the image decoder. Our code is available at https://github.com/nihaomiao/CVPR23_LFDM . 