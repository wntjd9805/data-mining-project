Detecting pedestrians accurately in urban scenes is sig-nificant for realistic applications like autonomous driving or video surveillance. However, confusing human-like ob-jects often lead to wrong detections, and small scale or heavily occluded pedestrians are easily missed due to their unusual appearances. To address these challenges, only object regions are inadequate, thus how to fully utilize more explicit and semantic contexts becomes a key problem.Meanwhile, previous context-aware pedestrian detectors ei-ther only learn latent contexts with visual clues, or need laborious annotations to obtain explicit and semantic con-texts. Therefore, we propose in this paper a novel approach via Vision-Language semantic self-supervision for context-aware Pedestrian Detection (VLPD) to model explicitly se-mantic contexts without any extra annotations. Firstly, we propose a self-supervised Vision-Language Semantic (VLS) segmentation method, which learns both fully-supervised pedestrian detection and contextual segmentation via self-generated explicit labels of semantic classes by vision-language models. Furthermore, a self-supervised Prototyp-ical Semantic Contrastive (PSC) learning method is pro-posed to better discriminate pedestrians and other classes, based on more explicit and semantic contexts obtained fromVLS. Extensive experiments on popular benchmarks show that our proposed VLPD achieves superior performances over the previous state-of-the-arts, particularly under chal-lenging circumstances like small scale and heavy occlusion.Code is available at https://github.com/lmy98129/VLPD. 