Most state-of-the-art methods for action segmentation are based on single input modalities or na¨ıve fusion of mul-tiple data sources. However, effective fusion of comple-mentary information can potentially strengthen segmenta-tion models and make them more robust to sensor noise and more accurate with smaller training datasets. In order to improve multimodal representation learning for action segmentation, we propose to disentangle hidden features of a multi-stream segmentation model into modality-shared components, containing common information across data sources, and private components; we then use an attention bottleneck to capture long-range temporal dependencies in the data while preserving disentanglement in consecutive processing layers. Evaluation on 50salads, Breakfast andRARP45 datasets shows that our multimodal approach out-performs different data fusion baselines on both multiview and multimodal data sources, obtaining competitive or bet-ter results compared with the state-of-the-art. Our model is also more robust to additive sensor noise and can achieve performance on par with strong video baselines even with less training data. 