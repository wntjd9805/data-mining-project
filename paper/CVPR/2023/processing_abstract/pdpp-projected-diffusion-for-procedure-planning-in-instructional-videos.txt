In this paper, we study the problem of procedure plan-ning in instructional videos, which aims to make goal-directed plans given the current visual observations in un-structured real-life videos. Previous works cast this prob-lem as a sequence planning problem and leverage either heavy intermediate visual observations or natural language instructions as supervision, resulting in complex learningIn contrast, we schemes and expensive annotation costs. treat this problem as a distribution fitting problem. In this sense, we model the whole intermediate action sequence distribution with a diffusion model (PDPP), and thus trans-form the planning problem to a sampling process from this distribution.In addition, we remove the expensive inter-mediate supervision, and simply use task labels from in-structional videos as supervision instead. Our model is aU-Net based diffusion model, which directly samples ac-tion sequences from the learned distribution with the given start and end observations. Furthermore, we apply an ef-ficient projection method to provide accurate conditional guides for our model during the learning and sampling pro-cess. Experiments on three datasets with different scales show that our PDPP model can achieve the state-of-the-art performance on multiple metrics, even without the task supervision. Code and trained models are available at https://github.com/MCG-NJU/PDPP. 