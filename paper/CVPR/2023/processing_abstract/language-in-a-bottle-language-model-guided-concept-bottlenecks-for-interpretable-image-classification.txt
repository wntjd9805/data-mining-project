Concept Bottleneck Models (CBM) are inherently inter-pretable models that factor model decisions into human-readable concepts. They allow people to easily understand why a model is failing, a critical feature for high-stakes ap-plications. CBMs require manually speciﬁed concepts and often under-perform their black box counterparts, preventing their broad adoption. We address these shortcomings and are ﬁrst to show how to construct high-performance CBMs without manual speciﬁcation of similar accuracy to black box models. Our approach, Language Guided Bottlenecks (LaBo), leverages a language model, GPT-3, to deﬁne a large space of possible bottlenecks. Given a problem domain,LaBo uses GPT-3 to produce factual sentences about cate-gories to form candidate concepts. LaBo efﬁciently searches possible bottlenecks through a novel submodular utility that promotes the selection of discriminative and diverse informa-tion. Ultimately, GPT-3’s sentential concepts can be aligned to images using CLIP, to form a bottleneck layer. Experi-ments demonstrate that LaBo is a highly effective prior for concepts important to visual recognition. In the evaluation with 11 diverse datasets, LaBo bottlenecks excel at few-shot classiﬁcation: they are 11.7% more accurate than black box linear probes at 1 shot and comparable with more data.Overall, LaBo demonstrates that inherently interpretable models can be widely applied at similar, or better, perfor-mance than black box approaches.1 