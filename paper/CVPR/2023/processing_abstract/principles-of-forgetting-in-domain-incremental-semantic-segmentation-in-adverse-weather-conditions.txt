Deep neural networks for scene perception in automated vehicles achieve excellent results for the domains they were trained on. However, in real-world conditions, the do-main of operation and its underlying data distribution are subject to change. Adverse weather conditions, in partic-ular, can significantly decrease model performance when such data are not available during training. Addition-ally, when a model is incrementally adapted to a new do-main, it suffers from catastrophic forgetting, causing a sig-nificant drop in performance on previously observed do-mains. Despite recent progress in reducing catastrophic forgetting, its causes and effects remain obscure. Therefore, we study how the representations of semantic segmenta-tion models are affected during domain-incremental learn-ing in adverse weather conditions. Our experiments and representational analyses indicate that catastrophic forget-ting is primarily caused by changes to low-level features in domain-incremental learning and that learning more gen-eral features on the source domain using pre-training and image augmentations leads to efficient feature reuse in sub-sequent tasks, which drastically reduces catastrophic for-getting. These findings highlight the importance of meth-ods that facilitate generalized features for effective contin-ual learning algorithms. 