Effectively extracting inter-frame motion and appear-ance information is important for video frame interpolation (VFI). Previous works either extract both types of informa-tion in a mixed way or devise separate modules for each type of information, which lead to representation ambiguity and low efficiency. In this paper, we propose a new mod-ule to explicitly extract motion and appearance information via a unified operation. Specifically, we rethink the infor-mation process in inter-frame attention and reuse its at-tention map for both appearance feature enhancement and motion information extraction. Furthermore, for efficientVFI, our proposed module could be seamlessly integrated into a hybrid CNN and Transformer architecture. This hy-brid pipeline can alleviate the computational complexity of inter-frame attention as well as preserve detailed low-level structure information. Experimental results demon-strate that, for both fixed- and arbitrary-timestep interpo-lation, our method achieves state-of-the-art performance on various datasets. Meanwhile, our approach enjoys a lighter computation overhead over models with close per-formance. The source code and models are available at https://github.com/MCG-NJU/EMA-VFI. 