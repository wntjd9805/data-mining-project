Previous works on video object segmentation (VOS) are trained on densely annotated videos. Nevertheless, ac-quiring annotations in pixel level is expensive and time-consuming. In this work, we demonstrate the feasibility of training a satisfactory VOS model on sparsely annotated videosâ€”we merely require two labeled frames per train-ing video while the performance is sustained. We term this novel training paradigm as two-shot video object segmen-tation, or two-shot VOS for short. The underlying idea is to generate pseudo labels for unlabeled frames during train-ing and to optimize the model on the combination of la-beled and pseudo-labeled data. Our approach is extremely simple and can be applied to a majority of existing frame-works. We first pre-train a VOS model on sparsely an-notated videos in a semi-supervised manner, with the first frame always being a labeled one. Then, we adopt the pre-trained VOS model to generate pseudo labels for all un-labeled frames, which are subsequently stored in a pseudo-label bank. Finally, we retrain a VOS model on both labeled and pseudo-labeled data without any restrictions on the first frame. For the first time, we present a general way to trainVOS models on two-shot VOS datasets. By using 7.3% and 2.9% labeled data of YouTube-VOS and DAVIS benchmarks, our approach achieves comparable results in contrast to the counterparts trained on fully labeled set. Code and models are available at https://github.com/yk-pku/Two-shot-Video-Object-Segmentation. (a) Previous works on video object segmentation rely on densely annotated videos. We present two-shot video object segmentation, which merely ac-cesses two labeled frames per video. (b) Comparison among naive 2-shot STCN, STCN trained on full set and 2-shot STCN equipped with our approach on DAVIS 2016/2017 andYouTube-VOS 2018/2019.Figure 1. (a) Problem formulation. (b) Comparison among STCN variants on various datasets. 