Unsupervised domain adaptation (UDA) in semantic segmentation transfers the knowledge of the source domain to the target one to improve the adaptability of the seg-mentation model in the target domain. The need to access labeled source data makes UDA unable to handle adap-tation scenarios involving privacy, property rights protec-tion, and conﬁdentiality. In this paper, we focus on unsu-pervised model adaptation (UMA), also called source-free domain adaptation, which adapts a source-trained model to the target domain without accessing source data. We ﬁnd that the online self-training method has the potential to be deployed in UMA, but the lack of source domain loss will greatly weaken the stability and adaptability of the method.We analyze two reasons for the degradation of online self-training, i.e. inopportune updates of the teacher model and biased knowledge from the source-trained model. Based on this, we propose a dynamic teacher update mechanism and a training-consistency based resampling strategy to im-prove the stability and adaptability of online self-training.On multiple model adaptation benchmarks, our method ob-tains new state-of-the-art performance, which is compara-ble or even better than state-of-the-art UDA methods. The code is available at https://github.com/DZhaoXd/DT-ST. 