Bird’s Eye View (BEV) semantic segmentation is a critical task in autonomous driving. However, existingTransformer-based methods confront difficulties in trans-forming Perspective View (PV) to BEV due to their unidi-rectional and posterior interaction mechanisms. To address this issue, we propose a novel Bi-directional and Early In-teraction Transformers framework named BAEFormer, con-sisting of (i) an early-interaction PV-BEV pipeline and (ii) a bi-directional cross-attention mechanism. Moreover, we find that the image feature maps’ resolution in the cross-attention module has a limited effect on the final perfor-mance. Under this critical observation, we propose to en-large the size of input images and downsample the multi-view image features for cross-interaction, further improving the accuracy while keeping the amount of computation con-trollable. Our proposed method for BEV semantic segmen-tation achieves state-of-the-art performance in real-time in-ference speed on the nuScenes dataset, i.e., 38.9 mIoU at 45FPS on a single A100 GPU. 