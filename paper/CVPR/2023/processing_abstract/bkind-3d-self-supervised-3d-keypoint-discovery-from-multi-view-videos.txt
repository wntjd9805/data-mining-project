Quantifying motion in 3D is important for studying the behavior of humans and other animals, but manual pose an-notations are expensive and time-consuming to obtain. Self-supervised keypoint discovery is a promising strategy for estimating 3D poses without annotations. However, current keypoint discovery approaches commonly process single 2D views and do not operate in the 3D space. We propose a new method to perform self-supervised keypoint discovery in 3D from multi-view videos of behaving agents, without any keypoint or bounding box supervision in 2D or 3D. Our method, BKinD-3D, uses an encoder-decoder architecture with a 3D volumetric heatmap, trained to reconstruct spa-tiotemporal differences across multiple views, in addition to joint length constraints on a learned 3D skeleton of the sub-ject. In this way, we discover keypoints without requiring manual supervision in videos of humans and rats, demon-strating the potential of 3D keypoint discovery for studying behavior. 