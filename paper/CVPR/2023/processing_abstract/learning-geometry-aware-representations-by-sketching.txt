Understanding geometric concepts, such as distance and shape, is essential for understanding the real world and also for many vision tasks. To incorporate such information into a visual representation of a scene, we propose learning to represent the scene by sketching, inspired by human be-havior. Our method, coined Learning by Sketching (LBS), learns to convert an image into a set of colored strokes that explicitly incorporate the geometric information of the scene in a single inference step without requiring a sketch dataset. A sketch is then generated from the strokes whereCLIP-based perceptual loss maintains a semantic similar-ity between the sketch and the image. We show theoreti-cally that sketching is equivariant with respect to arbitrary affine transformations and thus provably preserves geomet-ric information. Experimental results show that LBS sub-stantially improves the performance of object attribute clas-sification on the unlabeled CLEVR dataset, domain trans-fer between CLEVR and STL-10 datasets, and for diverse downstream tasks, confirming that LBS provides rich geo-metric information. 