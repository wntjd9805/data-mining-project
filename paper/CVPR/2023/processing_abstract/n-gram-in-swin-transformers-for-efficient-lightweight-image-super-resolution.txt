While some studies have proven that Swin Transformer (Swin) with window self-attention (WSA) is suitable for sin-gle image super-resolution (SR), the plain WSA ignores the broad regions when reconstructing high-resolution im-In addition, many ages due to a limited receptive field. deep learning SR methods suffer from intensive computa-tions. To address these problems, we introduce the N-Gram context to the low-level vision with Transformers for the first time. We define N-Gram as neighboring local win-dows in Swin, which differs from text analysis that viewsN-Gram as consecutive characters or words. N-Grams in-teract with each other by sliding-WSA, expanding the re-gions seen to restore degraded pixels. Using the N-Gram context, we propose NGswin, an efficient SR network withSCDP bottleneck taking multi-scale outputs of the hierar-chical encoder. Experimental results show that NGswin achieves competitive performance while maintaining an efficient structure when compared with previous leading methods. Moreover, we also improve other Swin-based SR methods with the N-Gram context, thereby building an en-hanced model: SwinIR-NG. Our improved SwinIR-NG out-performs the current best lightweight SR approaches and establishes state-of-the-art results. Codes are available at https://github.com/rami0205/NGramSwin. 