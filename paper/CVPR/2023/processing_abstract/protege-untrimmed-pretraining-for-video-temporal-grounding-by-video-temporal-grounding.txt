Video temporal grounding (VTG) is the task of localiz-ing a given natural language text query in an arbitrarily long untrimmed video. While the task involves untrimmed videos, all existing VTG methods leverage features from video backbones pretrained on trimmed videos. This is largely due to the lack of large-scale well-annotated VTG dataset to perform pretraining. As a result, the pretrained features lack a notion of temporal boundaries leading to the video-text alignment being less distinguishable between correct and incorrect locations. We present ProT´eG´e as the first method to perform VTG-based untrimmed pretrain-ing to bridge the gap between trimmed pretrained back-bones and downstream VTG tasks. ProT´eG´e reconfigures the HowTo100M dataset, with noisily correlated video-text pairs, into a VTG dataset and introduces a novel Video-TextSimilarity-based Grounding Module and a pretraining ob-jective to make pretraining robust to noise in HowTo100M.Extensive experiments on multiple datasets across down-stream tasks with all variations of supervision validate that pretrained features from ProT´eG´e can significantly outper-form features from trimmed pretrained backbones on VTG. 