Pre-training by numerous image data has become de-facto for robust 2D representations. In contrast, due to the expensive data processing, a paucity of 3D datasets severely hinders the learning for high-quality 3D features. In this paper, we propose an alternative to obtain superior 3D representations from 2D pre-trained models via Image-to-Point Masked Autoencoders, named as I2P-MAE. By self-supervised pre-training, we leverage the well learned 2D knowledge to guide 3D masked autoencoding, which recon-structs the masked point tokens with an encoder-decoder ar-chitecture. Specifically, we first utilize off-the-shelf 2D mod-els to extract the multi-view visual features of the input point cloud, and then conduct two types of image-to-point learn-ing schemes. For one, we introduce a 2D-guided masking strategy that maintains semantically important point tokens to be visible. Compared to random masking, the network can better concentrate on significant 3D structures with key spatial cues. For another, we enforce these visible to-kens to reconstruct multi-view 2D features after the decoder.This enables the network to effectively inherit high-level 2D semantics for discriminative 3D modeling. Aided by our image-to-point pre-training, the frozen I2P-MAE, without any fine-tuning, achieves 93.4% accuracy for linear SVM on ModelNet40, competitive to existing fully trained meth-ods. By further fine-tuning on on ScanObjectNNâ€™s hard-est split, I2P-MAE attains the state-of-the-art 90.11% ac-curacy, +3.68% to the second-best, demonstrating supe-rior transferable capacity. Code is available at https://github.com/ZrrSkywalker/I2P-MAE. 