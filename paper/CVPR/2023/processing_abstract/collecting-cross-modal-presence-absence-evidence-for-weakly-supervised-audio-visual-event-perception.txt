Video-level Annotations(cid:28451)Dog, Speech, SingingOnly audible eventOnly visible eventAudi-visible eventWith only video-level event labels, this paper targets at the task of weakly-supervised audio-visual event perception (WS-AVEP), which aims to temporally localize and catego-rize events belonging to each modality. Despite the recent progress, most existing approaches either ignore the unsyn-chronized property of audio-visual tracks or discount the complementary modality for explicit enhancement. We ar-gue that, for an event residing in one modality, the modality itself should provide ample presence evidence of this event, while the other complementary modality is encouraged to afford the absence evidence as a reference signal. To this end, we propose to collect Cross-Modal Presence-AbsenceEvidence (CMPAE) in a uniﬁed framework. Speciﬁcally, by leveraging uni-modal and cross-modal representations, a presence-absence evidence collector (PAEC) is designed under Subjective Logic theory. To learn the evidence in a reliable range, we propose a joint-modal mutual learning (JML) process, which calibrates the evidence of diverse au-dible, visible, and audi-visible events adaptively and dy-namically. Extensive experiments show that our method surpasses state-of-the-arts (e.g., absolute gains of 3.6% and 6.1% in terms of event-level visual and audio metrics).Code is available in github.com/MengyuanChen21/CVPR2023-CMPAE. 