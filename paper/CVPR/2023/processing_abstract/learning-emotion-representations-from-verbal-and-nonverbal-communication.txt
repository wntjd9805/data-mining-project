Emotion understanding is an essential but highly chal-lenging component of artificial general intelligence. The absence of extensive annotated datasets has significantly impeded advancements in this field. We present Emotion-CLIP, the first pre-training paradigm to extract visual emo-tion representations from verbal and nonverbal communi-cation using only uncurated data. Compared to numerical labels or descriptions used in previous methods, commu-nication naturally contains emotion information. Further-more, acquiring emotion representations from communica-tion is more congruent with the human learning process.We guide EmotionCLIP to attend to nonverbal emotion cues through subject-aware context encoding and verbal emo-tion cues using sentiment-guided contrastive learning. Ex-tensive experiments validates the effectiveness and transfer-ability of EmotionCLIP. Using merely linear-probe evalua-tion protocol, EmotionCLIP outperforms the state-of-the-art supervised visual emotion recognition methods and ri-vals many multimodal approaches across various bench-marks. We anticipate that the advent of EmotionCLIP will address the prevailing issue of data scarcity in emotion understanding, thereby fostering progress in related do-mains. The code and pre-trained models are available at https://github.com/Xeaver/EmotionCLIP. 