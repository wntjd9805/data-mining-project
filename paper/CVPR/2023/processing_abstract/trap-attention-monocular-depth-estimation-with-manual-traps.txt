Predicting a high quality depth map from a single im-age is a challenging task, because it exists infinite pos-sibility to project a 2D scene to the corresponding 3D scene. Recently, some studies introduced multi-head at-tention (MHA) modules to perform long-range interaction, which have shown significant progress in regressing the depth maps. The main functions of MHA can be loosely summarized to capture long-distance information and re-port the attention map by the relationship between pixels.However, due to the quadratic complexity of MHA, these methods can not leverage MHA to compute depth features in high resolution with an appropriate computational com-plexity. In this paper, we exploit a depth-wise convolution to obtain long-range information, and propose a novel trap attention, which sets some traps on the extended space for each pixel, and forms the attention mechanism by the fea-ture retention ratio of convolution window, resulting in that the quadratic computational complexity can be converted to linear form. Then we build an encoder-decoder trap depth estimation network, which introduces a vision transformer as the encoder, and uses the trap attention to estimate the depth from single image in the decoder. Extensive experi-mental results demonstrate that our proposed network can outperform the state-of-the-art methods in monocular depth estimation on datasets NYU Depth-v2 and KITTI, with sig-nificantly reduced number of parameters. Code is available at: https://github.com/ICSResearch/TrapAttention. 