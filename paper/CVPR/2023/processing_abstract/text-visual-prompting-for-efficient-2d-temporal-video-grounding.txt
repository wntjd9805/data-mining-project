In this paper, we study the problem of temporal video grounding (TVG), which aims to predict the starting/ending time points of moments described by a text sentence within a long untrimmed video. Benefiting from fine-grained 3D visual features, the TVG techniques have achieved remark-able progress in recent years. However, the high complexity of 3D convolutional neural networks (CNNs) makes extract-ing dense 3D visual features time-consuming, which calls for intensive memory and computing resources. Towards efficient TVG, we propose a novel text-visual prompting (TVP) framework, which incorporates optimized perturba-tion patterns (that we call ‘prompts’) into both visual in-puts and textual features of a TVG model. In sharp con-trast to 3D CNNs, we show that TVP allows us to effec-tively co-train vision encoder and language encoder in a 2D TVG model and improves the performance of cross-modal feature fusion using only low-complexity sparse 2D visual features. Further, we propose a Temporal-DistanceIoU (TDIoU) loss for efficient learning of TVG. Experi-ments on two benchmark datasets, Charades-STA and Ac-tivityNet Captions datasets, empirically show that the pro-posed TVP significantly boosts the performance of 2D TVG (e.g., 9.79% improvement on Charades-STA and 30.77% improvement on ActivityNet Captions) and achieves 5× in-ference acceleration over TVG using 3D visual features.Codes are available at Open.Intel. 