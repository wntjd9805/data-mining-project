Convolutional neural networks (CNNs) have shown re-markable performance on various tasks. Despite its widespread adoption, the decision procedure of the network still lacks transparency and interpretability, making it diffi-cult to enhance the performance further. Hence, there has been considerable interest in providing explanation and in-terpretability for CNNs over the last few years. Explain-able artificial intelligence (XAI) investigates the relation-ship between input images or videos and output predic-tions. Recent studies have achieved outstanding success in explaining 2D image classification ConvNets. On the other hand, due to the high computation cost and complex-ity of video data, the explanation of 3D video recognitionConvNets is relatively less studied. And none of them are able to produce a high-level explanation. In this paper, we propose a STCE (Spatial-temporal Concept-based Expla-nation) framework for interpreting 3D ConvNets.In our approach: (1) videos are represented with high-level su-pervoxels, similar supervoxels are clustered as a concept, which is straightforward for human to understand; and (2) the interpreting framework calculates a score for each con-cept, which reflects its significance in the ConvNet decision procedure. Experiments on diverse 3D ConvNets demon-strate that our method can identify global concepts with dif-ferent importance levels, allowing us to investigate the im-pact of the concepts on a target task, such as action recog-nition, in-depth. The source codes are publicly available at https://github.com/yingji425/STCE. 