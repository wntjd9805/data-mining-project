Beneﬁting from large-scale vision-language pre-training on image-text pairs, open-world detection methods have shown superior generalization ability under the zero-shot or few-shot detection settings. However, a pre-deﬁned cate-gory space is still required during the inference stage of ex-isting methods and only the objects belonging to that space will be predicted. To introduce a “real” open-world de-tector, in this paper, we propose a novel method namedCapDet to either predict under a given category list or di-rectly generate the category of predicted bounding boxes.Speciﬁcally, we unify the open-world detection and dense caption tasks into a single yet effective framework by in-troducing an additional dense captioning head to gener-ate the region-grounded captions. Besides, adding the cap-tioning task will in turn beneﬁt the generalization of detec-tion performance since the captioning dataset covers more concepts. Experiment results show that by unifying the dense caption task, our CapDet has obtained signiﬁcant performance improvements (e.g., +2.1% mAP on LVIS rare classes) over the baseline method on LVIS (1203 classes).Besides, our CapDet also achieves state-of-the-art perfor-mance on dense captioning tasks, e.g., 15.44% mAP on VGV1.2 and 13.98% on the VG-COCO dataset. 