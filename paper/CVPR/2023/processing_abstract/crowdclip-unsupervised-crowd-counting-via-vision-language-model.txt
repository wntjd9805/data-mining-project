Supervised crowd counting relies heavily on costly man-ual labeling, which is difficult and expensive, especially in dense scenes. To alleviate the problem, we propose a novel unsupervised framework for crowd counting, namedCrowdCLIP. The core idea is built on two observations: 1) the recent contrastive pre-trained vision-language model (CLIP) has presented impressive performance on various downstream tasks; 2) there is a natural mapping between crowd patches and count text. To the best of our knowl-edge, CrowdCLIP is the first to investigate the vision-language knowledge to solve the counting problem. Specif-ically, in the training stage, we exploit the multi-modal ranking loss by constructing ranking text prompts to match the size-sorted crowd patches to guide the image encoder learning.In the testing stage, to deal with the diversity of image patches, we propose a simple yet effective pro-gressive filtering strategy to first select the highly poten-tial crowd patches and then map them into the language space with various counting intervals. Extensive exper-iments on five challenging datasets demonstrate that the proposed CrowdCLIP achieves superior performance com-pared to previous unsupervised state-of-the-art counting methods. Notably, CrowdCLIP even surpasses some pop-ular fully-supervised methods under the cross-dataset set-ting. The source code will be available at https:// github.com/dk-liang/CrowdCLIP. 