Object detectors often suffer from the domain gap be-tween training (source domain) and real-world applications (target domain). Mean-teacher self-training is a powerful paradigm in unsupervised domain adaptation for object de-tection, but it struggles with low-quality pseudo-labels. In this work, we identify the intriguing alignment and syn-ergy between mean-teacher self-training and contrastive learning. Motivated by this, we propose Contrastive MeanTeacher (CMT) â€“ a unified, general-purpose framework with the two paradigms naturally integrated to maximize beneficial learning signals. Instead of using pseudo-labels solely for final predictions, our strategy extracts object-level features using pseudo-labels and optimizes them via contrastive learning, without requiring labels in the target domain. When combined with recent mean-teacher self-training methods, CMT leads to new state-of-the-art target-domain performance: 51.9% mAP on Foggy Cityscapes, outperforming the previously best by 2.1% mAP. Notably,CMT can stabilize performance and provide more signifi-cant gains as pseudo-label noise increases. 