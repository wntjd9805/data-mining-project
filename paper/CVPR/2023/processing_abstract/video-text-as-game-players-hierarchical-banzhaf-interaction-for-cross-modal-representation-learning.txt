Contrastive learning-based video-language representa-tion learning approaches, e.g., CLIP, have achieved out-standing performance, which pursue semantic interaction upon pre-defined video-text pairs. To clarify this coarse-grained global interaction and move a step further, we have to encounter challenging shell-breaking interactions for fine-grained cross-modal learning. In this paper, we creatively model video-text as game players with multivariate coopera-tive game theory to wisely handle the uncertainty during fine-grained semantic interaction with diverse granularity, flex-ible combination, and vague intensity. Concretely, we pro-pose Hierarchical Banzhaf Interaction (HBI) to value pos-sible correspondence between video frames and text words for sensitive and explainable cross-modal contrast. To effi-ciently realize the cooperative game of multiple video frames and multiple text words, the proposed method clusters the original video frames (text words) and computes the BanzhafInteraction between the merged tokens. By stacking token merge modules, we achieve cooperative games at different semantic levels. Extensive experiments on commonly used text-video retrieval and video-question answering bench-marks with superior performances justify the efficacy of ourHBI. More encouragingly, it can also serve as a visualization tool to promote the understanding of cross-modal interac-tion, which have a far-reaching impact on the community.Project page is available at https://jpthu17.github.io/HBI/. 