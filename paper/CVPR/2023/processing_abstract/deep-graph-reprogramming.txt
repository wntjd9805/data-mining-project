In this paper, we explore a novel model reusing task tai-lored for graph neural networks (GNNs), termed as “deep graph reprogramming”. We strive to reprogram a pre-trained GNN, without amending raw node features nor model parameters, to handle a bunch of cross-level down-stream tasks in various domains. To this end, we propose an innovative Data Reprogramming paradigm alongside aModel Reprogramming paradigm. The former one aims to address the challenge of diversiﬁed graph feature dimen-sions for various tasks on the input side, while the latter alleviates the dilemma of ﬁxed per-task-per-model behav-ior on the model side. For data reprogramming, we specif-ically devise an elaborated Meta-FeatPadding method to deal with heterogeneous input dimensions, and also de-velop a transductive Edge-Slimming as well as an induc-tive Meta-GraPadding approach for diverse homogenous samples. Meanwhile, for model reprogramming, we pro-pose a novel task-adaptive Reprogrammable-Aggregator, to endow the frozen model with larger expressive capaci-ties in handling cross-domain tasks. Experiments on four-teen datasets across node/graph classiﬁcation/regression, 3D object recognition, and distributed action recognition, demonstrate that the proposed methods yield gratifying re-sults, on par with those by re-training from scratch. 