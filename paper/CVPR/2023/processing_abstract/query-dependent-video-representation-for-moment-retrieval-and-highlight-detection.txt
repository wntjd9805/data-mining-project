Recently, video moment retrieval and highlight detec-tion (MR/HD) are being spotlighted as the demand for video understanding is drastically increased. The key objective of MR/HD is to localize the moment and estimate clip-wise accordance level, i.e., saliency score, to the given text query.Although the recent transformer-based models brought some advances, we found that these methods do not fully exploit the information of a given query. For example, the relevance between text query and video contents is sometimes neglected when predicting the moment and its saliency. To tackle this issue, we introduce Query-Dependent DETR (QD-DETR), a detection transformer tailored for MR/HD. As we observe the insigniﬁcant role of a given query in transformer architec-tures, our encoding module starts with cross-attention layers to explicitly inject the context of text query into video repre-sentation. Then, to enhance the model’s capability of exploit-ing the query information, we manipulate the video-query pairs to produce irrelevant pairs. Such negative (irrelevant) video-query pairs are trained to yield low saliency scores, which in turn, encourages the model to estimate precise ac-cordance between query-video pairs. Lastly, we present an input-adaptive saliency predictor which adaptively deﬁnes the criterion of saliency scores for the given video-query pairs. Our extensive studies verify the importance of build-ing the query-dependent representation for MR/HD. Specif-ically, QD-DETR outperforms state-of-the-art methods onQVHighlights, TVSum, and Charades-STA datasets. Codes are available at github.com/wjun0830/QD-DETR. 