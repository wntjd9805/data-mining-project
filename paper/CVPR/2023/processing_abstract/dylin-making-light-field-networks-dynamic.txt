Light Field Networks, the re-formulations of radiance fields to oriented rays, are magnitudes faster than their co-ordinate network counterparts, and provide higher fidelity with respect to representing 3D structures from 2D obser-vations. They would be well suited for generic scene rep-resentation and manipulation, but suffer from one problem: they are limited to holistic and static scenes.In this pa-per, we propose the Dynamic Light Field Network (DyLiN) method that can handle non-rigid deformations, including topological changes. We learn a deformation field from in-put rays to canonical rays, and lift them into a higher di-mensional space to handle discontinuities. We further in-troduce CoDyLiN, which augments DyLiN with controllable attribute inputs. We train both models via knowledge distil-lation from pretrained dynamic radiance fields. We eval-uated DyLiN using both synthetic and real world datasets that include various non-rigid deformations. DyLiN qual-itatively outperformed and quantitatively matched state-of-the-art methods in terms of visual fidelity, while being 25 − 71× computationally faster. We also tested CoDyLiN on at-tribute annotated data and it surpassed its teacher model.Project page: https://dylin2023.github.io. 