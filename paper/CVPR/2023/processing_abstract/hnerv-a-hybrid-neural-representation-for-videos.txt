Implicit neural representations store videos as neural networks and have performed well for various vision tasks such as video compression and denoising. With frame in-dex or positional index as input, implicit representations (NeRV, E-NeRV, etc.) reconstruct video frames from fixed and content-agnostic embeddings. Such embedding largely limits the regression capacity and internal generalization for video interpolation.In this paper, we propose a Hy-brid Neural Representation for Videos (HNeRV), where a learnable encoder generates content-adaptive embeddings, which act as the decoder input. Besides the input em-bedding, we introduce HNeRV blocks, which ensure model parameters are evenly distributed across the entire net-work, such that higher layers (layers near the output) can have more capacity to store high-resolution content and video details. With content-adaptive embeddings and re-designed architecture, HNeRV outperforms implicit meth-ods in video regression tasks for both reconstruction qual-ity (+4.7 PSNR) and convergence speed (16Ã— faster), and shows better internal generalization. As a simple and effi-cient video representation, HNeRV also shows decoding ad-vantages for speed, flexibility, and deployment, compared to traditional codecs (H.264, H.265) and learning-based compression methods. Finally, we explore the effectiveness of HNeRV on downstream tasks such as video compression and video inpainting. 