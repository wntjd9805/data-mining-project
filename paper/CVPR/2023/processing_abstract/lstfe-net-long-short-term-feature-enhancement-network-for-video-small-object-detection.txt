Video small object detection is a difficult task due to the lack of object information. Recent methods focus on adding more temporal information to obtain more potent high-level features, which often fail to specify the most vital informa-tion for small objects, resulting in insufficient or inappro-priate features. Since information from frames at differ-ent positions contributes differently to small objects, it is not ideal to assume that using one universal method will extract proper features. We find that context information from the long-term frame and temporal information from the short-term frame are two useful cues for video small ob-ject detection. To fully utilize these two cues, we propose a long short-term feature enhancement network (LSTFE-Net) for video small object detection. First, we develop a plug-and-play spatio-temporal feature alignment module to cre-ate temporal correspondences between the short-term and current frames. Then, we propose a frame selection mod-ule to select the long-term frame that can provide the most additional context information. Finally, we propose a long short-term feature aggregation module to fuse long short-term features. Compared to other state-of-the-art meth-ods, our LSTFE-Net achieves 4.4% absolute boosts in AP on the FL-Drones dataset. More details can be found at https://github.com/xiaojs18/LSTFE-Net.Figure 1. The architecture of the proposed LSTFE-Net. The current frame (Cur-frame), short-term frames (ST-frames) near the Cur-frame, and long-term frames (LT-frames) sampled from the whole video first go through the feature extraction network.Then the Cur-frame feature and ST-frame features are connected through the spatio-temporal feature alignment module, and the frame selection module searches the background context of LT-frame features. After getting the Proposal-Level (PL) features, the long short-term feature aggregation module finally integrates the long short-term features into the Cur-frame to make feature en-hancement. Best viewed in color and zoomed in. 