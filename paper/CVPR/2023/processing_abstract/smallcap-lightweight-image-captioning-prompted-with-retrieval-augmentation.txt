Recent advances in image captioning have focused on scaling the data and model size, substantially increasing the cost of pre-training and ﬁnetuning. As an alternative to large models, we present SMALLCAP, which generates a caption conditioned on an input image and related captions retrieved from a datastore. Our model is lightweight and fast to train, as the only learned parameters are in newly in-troduced cross-attention layers between a pre-trained CLIP encoder and GPT-2 decoder. SMALLCAP can transfer to new domains without additional ﬁnetuning and can exploit large-scale data in a training-free fashion since the contents of the datastore can be readily replaced. Our experiments show that SMALLCAP, trained only on COCO, has com-petitive performance on this benchmark, and also trans-fers to other domains without retraining, solely through re-trieval from target-domain data. Further improvement is achieved through the training-free exploitation of diverse human-labeled and web data, which proves to be effective for a range of domains, including the nocaps benchmark, designed to test generalization to unseen visual concepts.1 