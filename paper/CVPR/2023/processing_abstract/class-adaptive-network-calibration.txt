Recent studies have revealed that, beyond conventional accuracy, calibration should also be considered for train-ing modern deep neural networks. To address miscalibra-tion during learning, some methods have explored different penalty functions as part of the learning objective, along-side a standard classiﬁcation loss, with a hyper-parameter controlling the relative contribution of each term. Never-theless, these methods share two major drawbacks: 1) the scalar balancing weight is the same for all classes, hinder-ing the ability to address different intrinsic difﬁculties or imbalance among classes; and 2) the balancing weight is usually ﬁxed without an adaptive strategy, which may pre-vent from reaching the best compromise between accuracy and calibration, and requires hyper-parameter search for each application. We propose Class Adaptive Label Smooth-ing (CALS) for calibrating deep networks, which allows to learn class-wise multipliers during training, yielding a powerful alternative to common label smoothing penalties.Our method builds on a general Augmented Lagrangian approach, a well-established technique in constrained opti-mization, but we introduce several modiﬁcations to tailor it for large-scale, class-adaptive training. Comprehensive eval-uation and multiple comparisons on a variety of benchmarks, including standard and long-tailed image classiﬁcation, se-mantic segmentation, and text classiﬁcation, demonstrate the superiority of the proposed method. The code is available at https://github.com/by-liu/CALS . 