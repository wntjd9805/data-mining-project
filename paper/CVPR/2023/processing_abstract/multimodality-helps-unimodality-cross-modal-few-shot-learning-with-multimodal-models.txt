The ability to quickly learn a new task with minimal in-struction – known as few-shot learning – is a central as-pect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufﬁcient to characterize an entire concept class. In contrast, humans use cross-modal infor-mation to learn new concepts efﬁciently. In this work, we demonstrate that one can indeed build a better visual dog classiﬁer by reading about dogs and listening to them bark.To do so, we exploit the fact that recent multimodal founda-tion models such as CLIP are inherently cross-modal, map-ping different modalities to the same representation space.Speciﬁcally, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning dif-ferent modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classiﬁer for vision-language adaptation. Furthermore, we show that our approach can beneﬁt existing methods such as preﬁx tuning, adapters, and classiﬁer ensembling. Finally, to explore other modalities beyond vision and language, we construct the ﬁrst (to our knowledge) audiovisual few-shot benchmark and use cross-modal training to improve the performance of both image and audio classiﬁcation. Project site at link. 