Few-shot font generation (FFG), aiming at generating font images with a few samples, is an emerging topic in re-cent years due to the academic and commercial values. Typ-ically, the FFG approaches follow the style-content disen-tanglement paradigm, which transfers the target font styles to characters by combining the content representations of source characters and the style codes of reference samples.Most existing methods attempt to increase font generation ability via exploring powerful style representations, which may be a sub-optimal solution for the FFG task due to the lack of modeling spatial transformation in transferring font styles. In this paper, we model font generation as a continu-ous transformation process from the source character image to the target font image via the creation and dissipation of font pixels, and embed the corresponding transformations into a neural transformation ﬁeld. With the estimated trans-formation path, the neural transformation ﬁeld generates a set of intermediate transformation results via the sampling process, and a font rendering formula is developed to ac-cumulate them into the target font image. Extensive exper-iments show that our method achieves state-of-the-art per-formance on few-shot font generation task, which demon-strates the effectiveness of our proposed model. Our imple-mentation is available at: https://github.com/fubinfb/NTF. 