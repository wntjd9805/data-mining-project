In recent years, as various realistic face forgery tech-niques known as DeepFake improves by leaps and bounds, more and more DeepFake detection techniques have been proposed. These methods typically rely on detecting statis-tical differences between natural (i.e., real) and DeepFake-generated images in both spatial and frequency domains. In this work, we propose to explicitly minimize the statistical differences to evade state-of-the-art DeepFake detectors. To this end, we propose a statistical consistency attack (StatAt-tack) against DeepFake detectors, which contains two main parts. First, we select several statistical-sensitive natural degradations (i.e., exposure, blur, and noise) and add them to the fake images in an adversarial way. Second, we find that the statistical differences between natural and DeepFake im-ages are positively associated with the distribution shifting between the two kinds of images, and we propose to use a distribution-aware loss to guide the optimization of different degradations. As a result, the feature distributions of gen-erated adversarial examples is close to the natural images.Furthermore, we extend the StatAttack to a more power-ful version, MStatAttack, where we extend the single-layer degradation to multi-layer degradations sequentially and use the loss to tune the combination weights jointly. Compre-hensive experimental results on four spatial-based detectors and two frequency-based detectors with four datasets demon-strate the effectiveness of our proposed attack method in both white-box and black-box settings.Figure 1. Principle of our method. The light blue region and the light red region represent the embedding space of natural/real im-ages and fake images, respectively. The dark blue region represents the embedding spaces of real images shared by different detectors.The first row shows that a typical attack can map the fake samples (i.e., the orange points) to the ‘real’ samples that can fool detectorA but fail to mislead detector B. The second row shows that our method is to map the fake samples to the common regions of differ-ent detectors, which can fool both detectors. 