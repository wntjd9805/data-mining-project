Multimodal fusion can make semantic segmentation more robust. However, fusing an arbitrary number of modalities remains underexplored. To delve into this prob-lem, we create the DELIVER arbitrary-modal segmenta-tion benchmark, covering Depth, LiDAR, multiple Views,Events, and RGB. Aside from this, we provide this dataset in four severe weather conditions as well as ﬁve sensor failure cases to exploit modal complementarity and resolve par-tial outages. To make this possible, we present the arbi-trary cross-modal segmentation model CMNEXT.It en-compasses a Self-Query Hub (SQ-Hub) designed to extract effective information from any modality for subsequent fu-sion with the RGB representation and adds only negligible amounts of parameters (∼0.01M ) per additional modal-ity. On top, to efﬁciently and ﬂexibly harvest discrimina-tive cues from the auxiliary modalities, we introduce the simple Parallel Pooling Mixer (PPX). With extensive experi-ments on a total of six benchmarks, our CMNEXT achieves state-of-the-art performance on the DELIVER, KITTI-360,MFNet, NYU Depth V2, UrbanLF, and MCubeS datasets, allowing to scale from 1 to 81 modalities. On the freshly collected DELIVER, the quad-modal CMNEXT reaches up to 66.30% in mIoU with a +9.10% gain as compared to the mono-modal baseline.1 