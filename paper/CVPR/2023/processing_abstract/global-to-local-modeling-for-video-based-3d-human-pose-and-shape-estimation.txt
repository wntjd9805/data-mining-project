Video-based 3D human pose and shape estimations are evaluated by intra-frame accuracy and inter-frame smooth-ness. Although these two metrics are responsible for dif-ferent ranges of temporal consistency, existing state-of-the-art methods treat them as a unified problem and use monotonous modeling structures (e.g., RNN or attention-based block) to design their networks. However, using a single kind of modeling structure is difficult to balance the learning of short-term and long-term temporal corre-lations, and may bias the network to one of them, lead-ing to undesirable predictions like global location shift, temporal inconsistency, and insufficient local details. To solve these problems, we propose to structurally decou-ple the modeling of long-term and short-term correlations in an end-to-end framework, Global-to-Local Transformer (GLoT). First, a global transformer is introduced with aMasked Pose and Shape Estimation strategy for long-term modeling. The strategy stimulates the global transformer to learn more inter-frame correlations by randomly mask-ing the features of several frames. Second, a local trans-former is responsible for exploiting local details on the hu-man mesh and interacting with the global transformer by leveraging cross-attention. Moreover, a Hierarchical Spa-tial Correlation Regressor is further introduced to refine intra-frame estimations by decoupled global-local repre-sentation and implicit kinematic constraints. Our GLoT surpasses previous state-of-the-art methods with the low-est model parameters on popular benchmarks, i.e., 3DPW,MPI-INF-3DHP, and Human3.6M. Codes are available at https://github.com/sxl142/GLoT. 