Universal Image Segmentation is not a new concept.Past attempts to unify image segmentation include scene parsing, panoptic segmentation, and, more recently, new panoptic architectures. However, such panoptic architec-tures do not truly unify image segmentation because they need to be trained individually on the semantic, instance, or panoptic segmentation to achieve the best performance.Ideally, a truly universal framework should be trained only once and achieve SOTA performance across all three image segmentation tasks. To that end, we propose OneFormer, a universal image segmentation framework that unifies seg-mentation with a multi-task train-once design. We first pro-pose a task-conditioned joint training strategy that enables training on ground truths of each domain (semantic, in-stance, and panoptic segmentation) within a single multi-task training process. Secondly, we introduce a task token to condition our model on the task at hand, making our model task-dynamic to support multi-task training and inference.Thirdly, we propose using a query-text contrastive loss dur-ing training to establish better inter-task and inter-class distinctions. Notably, our single OneFormer model out-performs specialized Mask2Former models across all three segmentation tasks on ADE20k, Cityscapes, and COCO, de-spite the latter being trained on each task individually. We believe OneFormer is a significant step towards making im-age segmentation more universal and accessible. 