The last several years have witnessed remarkable progress in video-and-language (VidL) understanding.However, most modern VidL approaches use complex and specialized model architectures and sophisticated pretrain-ing protocols, making the reproducibility, analysis and com-parisons of these frameworks difficult. Hence, instead of proposing yet another new VidL model, this paper conducts a thorough empirical study demystifying the most important factors in the VidL model design. Among the factors that we investigate are (i) the spatiotemporal architecture design, (ii) the multimodal fusion schemes, (iii) the pretraining ob-jectives, (iv) the choice of pretraining data, (v) pretraining and finetuning protocols, and (vi) dataset and model scal-ing. Our empirical study reveals that the most important de-sign factors include: temporal modeling, video-to-text mul-timodal fusion, masked modeling objectives, and joint train-ing on images and videos. Using these empirical insights, we then develop a step-by-step recipe, dubbed VINDLU, for effective VidL pretraining. Our final model trained us-ing our recipe achieves comparable or better than state-of-the-art results on several VidL tasks without relying on ex-ternal CLIP pretraining. In particular, on the text-to-video retrieval task, our approach obtains 61.2% on DiDeMo, and 55.0% on ActivityNet, outperforming current SOTA by 7.8% and 6.1% respectively. Furthermore, our model also obtains state-of-the-art video question-answering re-sults on ActivityNet-QA, MSRVTT-QA, MSRVTT-MC andTVQA. Our code and pretrained models are publicly avail-able at: https://github.com/klauscc/VindLU . 