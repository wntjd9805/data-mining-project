Given sparse depths and the corresponding RGB images, depth completion aims at spatially propagating the sparse measurements throughout the whole image to get a dense depth prediction. Despite the tremendous progress of deep-learning-based depth completion methods, the locality of the convolutional layer or graph model makes it hard for the network to model the long-range relationship between pixels. While recent fully Transformer-based architecture has reported encouraging results with the global recep-tive ﬁeld, the performance and efﬁciency gaps to the well-developed CNN models still exist because of its deteriora-tive local feature details. This paper proposes a Joint Con-volutional Attention and Transformer block (JCAT), which deeply couples the convolutional attention layer and VisionTransformer into one block, as the basic unit to construct our depth completion model in a pyramidal structure. This hybrid architecture naturally beneﬁts both the local connec-tivity of convolutions and the global context of the Trans-former in one single model. As a result, our Completion-Former outperforms state-of-the-art CNNs-based methods on the outdoor KITTI Depth Completion benchmark and indoor NYUv2 dataset, achieving signiﬁcantly higher efﬁ-ciency (nearly 1/3 FLOPs) compared to pure Transformer-based methods. Code is available at https://github. com/youmi-zym/CompletionFormer. 