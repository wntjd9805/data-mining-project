Transformer-based methods have shown impressive per-formance in low-level vision tasks, such as image super-resolution. However, we find that these networks can only utilize a limited spatial range of input information through attribution analysis. This implies that the potential ofTransformer is still not fully exploited in existing networks.In order to activate more input pixels for better recon-struction, we propose a novel Hybrid Attention Transformer (HAT). It combines both channel attention and window-based self-attention schemes, thus making use of their com-plementary advantages of being able to utilize global statis-tics and strong local fitting capability. Moreover, to better aggregate the cross-window information, we introduce an overlapping cross-attention module to enhance the interac-tion between neighboring window features.In the train-ing stage, we additionally adopt a same-task pre-training strategy to exploit the potential of the model for further im-provement. Extensive experiments show the effectiveness of the proposed modules, and we further scale up the model to demonstrate that the performance of this task can be greatly improved. Our overall method significantly outperforms the state-of-the-art methods by more than 1dB. 