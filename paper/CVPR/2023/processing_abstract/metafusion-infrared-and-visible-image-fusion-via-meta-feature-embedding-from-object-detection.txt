Fusing infrared and visible images can provide more tex-ture details for subsequent object detection task. Converse-ly, detection task furnishes object semantic information to improve the infrared and visible image fusion. Thus, a joint fusion and detection learning to use their mutual promotion is attracting more attention. However, the feature gap be-tween these two different-level tasks hinders the progress.Addressing this issue, this paper proposes an infrared and visible image fusion via meta-feature embedding from ob-ject detection. The core idea is that meta-feature embedding model is designed to generate object semantic features ac-cording to fusion network ability, and thus the semantic fea-tures are naturally compatible with fusion features. It is op-timized by simulating a meta learning. Moreover, we further implement a mutual promotion learning between fusion and detection tasks to improve their performances. Comprehen-sive experiments on three public datasets demonstrate the effectiveness of our method. Code and model are available at: https://github.com/wdzhao123/MetaFusion. 