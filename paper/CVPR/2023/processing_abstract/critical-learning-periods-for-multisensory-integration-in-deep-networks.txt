We show that the ability of a neural network to integrate information from diverse sources hinges critically on be-ing exposed to properly correlated signals during the earlyInterfering with the learning process phases of training. during this initial stage can permanently impair the devel-opment of a skill, both in artiﬁcial and biological systems where the phenomenon is known as a critical learning pe-riod. We show that critical periods arise from the complex and unstable early transient dynamics, which are decisive of ﬁnal performance of the trained system and their learned representations. This evidence challenges the view, engen-dered by analysis of wide and shallow networks, that early learning dynamics of neural networks are simple, akin to those of a linear model. Indeed, we show that even deep linear networks exhibit critical learning periods for multi-source integration, while shallow networks do not. To bet-ter understand how the internal representations change ac-cording to disturbances or sensory deﬁcits, we introduce a new measure of source sensitivity, which allows us to track the inhibition and integration of sources during training.Our analysis of inhibition suggests cross-source reconstruc-tion as a natural auxiliary training objective, and indeed we show that architectures trained with cross-sensor recon-struction objectives are remarkably more resilient to crit-ical periods. Our ﬁndings suggest that the recent success in self-supervised multi-modal training compared to previ-ous supervised efforts may be in part due to more robust learning dynamics and not solely due to better architectures and/or more data. 