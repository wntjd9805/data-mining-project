Multimodal learning has shown great potentials in nu-merous scenes and attracts increasing interest recently.However, it often encounters the problem of missing modal-ity data and thus suffers severe performance degradation in practice. To this end, we propose a general framework called MMANet to assist incomplete multimodal learn-ing. It consists of three components: the deployment net-work used for inference, the teacher network transferring comprehensive multimodal information to the deployment network, and the regularization network guiding the de-ployment network to balance weak modality combinations.Specifically, we propose a novel margin-aware distilla-tion (MAD) to assist the information transfer by weigh-ing the sample contribution with the classification uncer-tainty. This encourages the deployment network to focus on the samples near decision boundaries and acquire the refined inter-class margin. Besides, we design a modality-aware regularization (MAR) algorithm to mine the weak modality combinations and guide the regularization net-work to calculate prediction loss for them. This forces the deployment network to improve its representation abil-ity for the weak modality combinations adaptively. Fi-nally, extensive experiments on multimodal classification and segmentation tasks demonstrate that our MMANet out-performs the state-of-the-art significantly. Code is available at: https://github.com/shicaiwei123/MMANet 