Layout generation aims to synthesize realistic graphic scenes consisting of elements with different attributes in-cluding category, size, position, and between-element rela-tion. It is a crucial task for reducing the burden on heavy-duty graphic design works for formatted scenes, e.g., publi-cations, documents, and user interfaces (UIs). Diverse ap-plication scenarios impose a big challenge in unifying var-ious layout generation subtasks, including conditional and unconditional generation. In this paper, we propose a Lay-out Diffusion Generative Model (LDGM) to achieve such uniÔ¨Åcation with a single decoupled diffusion model. LDGM views a layout of arbitrary missing or coarse element at-tributes as an intermediate diffusion status from a com-pleted layout. Since different attributes have their individ-ual semantics and characteristics, we propose to decouple the diffusion processes for them to improve the diversity of training samples and learn the reverse process jointly to ex-ploit global-scope contexts for facilitating generation. As a result, our LDGM can generate layouts either from scratch or conditional on arbitrary available attributes. Exten-sive qualitative and quantitative experiments demonstrate our proposed LDGM outperforms existing layout genera-tion models in both functionality and performance. 