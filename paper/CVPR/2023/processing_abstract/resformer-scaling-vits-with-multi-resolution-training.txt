Vision Transformers (ViTs) have achieved overwhelming success, yet they suffer from vulnerable resolution scalabil-ity, i.e., the performance drops drastically when presented with input resolutions that are unseen during training. We introduce, ResFormer, a framework that is built upon the seminal idea of multi-resolution training for improved per-formance on a wide spectrum of, mostly unseen, testing res-olutions. In particular, ResFormer operates on replicated images of different resolutions and enforces a scale con-sistency loss to engage interactive information across dif-ferent scales. More importantly, to alternate among vary-ing resolutions effectively, especially novel ones in testing, we propose a global-local positional embedding strategy that changes smoothly conditioned on input sizes. We con-duct extensive experiments for image classification on Im-ageNet. The results provide strong quantitative evidence that ResFormer has promising scaling abilities towards a wide range of resolutions. For instance, ResFormer-B-MR achieves a Top-1 accuracy of 75.86% and 81.72% when evaluated on relatively low and high resolutions respec-tively (i.e., 96 and 640), which are 48% and 7.49% better than DeiT-B. We also demonstrate, moreover, ResFormer is flexible and can be easily extended to semantic segmenta-tion, object detection and video action recognition. 