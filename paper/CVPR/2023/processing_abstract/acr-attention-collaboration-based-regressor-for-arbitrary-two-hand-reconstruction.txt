Reconstructing two hands from monocular RGB images is challenging due to frequent occlusion and mutual con-fusion. Existing methods mainly learn an entangled rep-resentation to encode two interacting hands, which are in-credibly fragile to impaired interaction, such as truncated hands, separate hands, or external occlusion. This pa-per presents ACR (Attention Collaboration-based Regres-sor), which makes the ﬁrst attempt to reconstruct hands in arbitrary scenarios. To achieve this, ACR explicitly miti-gates interdependencies between hands and between parts by leveraging center and part-based attention for feature extraction. However, reducing interdependence helps re-lease the input constraint while weakening the mutual rea-soning about reconstructing the interacting hands. Thus, based on center attention, ACR also learns cross-hand prior that handle the interacting hands better. We eval-uate our method on various types of hand reconstruction datasets. Our method signiﬁcantly outperforms the best interacting-hand approaches on the InterHand2.6M dataset while yielding comparable performance with the state-of-the-art single-hand methods on the FreiHand dataset. More qualitative results on in-the-wild and hand-object interac-tion datasets and web images/videos further demonstrate the effectiveness of our approach for arbitrary hand recon-struction. Our code is available at this link 1. 