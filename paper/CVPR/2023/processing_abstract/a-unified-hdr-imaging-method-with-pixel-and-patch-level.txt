Mapping Low Dynamic Range (LDR) images with differ-ent exposures to High Dynamic Range (HDR) remains non-trivial and challenging on dynamic scenes due to ghosting caused by object motion or camera jitting. With the success of Deep Neural Networks (DNNs), several DNNs-based methods have been proposed to alleviate ghosting, they can-not generate approving results when motion and saturation occur. To generate visually pleasing HDR images in various cases, we propose a hybrid HDR deghosting network, calledHyHDRNet, to learn the complicated relationship between reference and non-reference images. The proposed HyH-DRNet consists of a content alignment subnetwork and aTransformer-based fusion subnetwork. Specifically, to ef-fectively avoid ghosting from the source, the content align-ment subnetwork uses patch aggregation and ghost atten-tion to integrate similar content from other non-reference images with patch level and suppress undesired compo-nents with pixel level. To achieve mutual guidance between patch-level and pixel-level, we leverage a gating module to sufficiently swap useful information both in ghosted and saturated regions. Furthermore, to obtain a high-qualityHDR image, the Transformer-based fusion subnetwork uses a Residual Deformable Transformer Block (RDTB) to adap-tively merge information for different exposed regions. We examined the proposed method on four widely used publicHDR image deghosting datasets. Experiments demonstrate that HyHDRNet outperforms state-of-the-art methods both quantitatively and qualitatively, achieving appealing HDR visualization with unified textures and colors. 