The goal of open-vocabulary detection is to identify novel objects based on arbitrary textual descriptions. In this paper, we address open-vocabulary 3D point-cloud detec-tion by a dividing-and-conquering strategy, which involves: 1) developing a point-cloud detector that can learn a gen-eral representation for localizing various objects, and 2) connecting textual and point-cloud representations to en-able the detector to classify novel object categories basedSpecifically, we resort to rich im-on text prompting. age pre-trained models, by which the point-cloud detec-tor learns localizing objects under the supervision of pre-dicted 2D bounding boxes from 2D pre-trained detectors.Moreover, we propose a novel de-biased triplet cross-modal contrastive learning to connect the modalities of image, point-cloud and text, thereby enabling the point-cloud de-tector to benefit from vision-language pre-trained models, i.e., CLIP. The novel use of image and vision-language pre-trained models for point-cloud detectors allows for open-vocabulary 3D object detection without the need for 3D annotations. Experiments demonstrate that the proposed method improves at least 3.03 points and 7.47 points over a wide range of baselines on the ScanNet and SUN RGB-D datasets, respectively. Furthermore, we provide a compre-hensive analysis to explain why our approach works. Code is available at https://github.com/lyhdet/OV-3DET 