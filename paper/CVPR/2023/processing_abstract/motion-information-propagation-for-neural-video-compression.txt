In most existing neural video codecs, the information flow therein is uni-directional, where only motion coding provides motion vectors for frame coding. In this paper, we argue that, through information interactions, the synergy between motion coding and frame coding can be achieved.We effectively introduce bi-directional information interac-tions between motion coding and frame coding via our Mo-tion Information Propagation. When generating the tempo-ral contexts for frame coding, the high-dimension motion feature from the motion decoder serves as motion guidance to mitigate the alignment errors. Meanwhile, besides as-sisting frame coding at the current time step, the feature from context generation will be propagated as motion con-dition when coding the subsequent motion latent. Through the cycle of such interactions, feature propagation on mo-tion coding is built, strengthening the capacity of exploiting long-range temporal correlation. In addition, we propose hybrid context generation to exploit the multi-scale context features and provide better motion condition. Experiments show that our method can achieve 12.9% bit rate saving over the previous SOTA neural video codec. 