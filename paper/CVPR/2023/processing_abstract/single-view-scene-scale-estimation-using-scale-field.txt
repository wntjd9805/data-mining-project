In this paper, we propose a single image scale estima-tion method based on a novel scale field representation. A scale field defines the local pixel-to-metric conversion ratio along the gravity direction on all the ground pixels. This representation resolves the ambiguity in camera parame-ters, allowing us to use a simple yet effective way to collect scale annotations on arbitrary images from human annota-tors. By training our model on calibrated panoramic image data and the in-the-wild human annotated data, our sin-gle image scene scale estimation network generates robust scale field on a variety of image, which can be utilized in various 3D understanding and scale-aware image editing applications. 