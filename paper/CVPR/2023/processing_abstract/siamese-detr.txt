Recent self-supervised methods are mainly designed for representation learning with the base model, e.g., ResNetsThey cannot be easily transferred to DETR, or ViTs. with task-specific Transformer modules.In this work, we present Siamese DETR, a Siamese self-supervised pretraining approach for the Transformer architecture inDETR. We consider learning view-invariant and detection-oriented representations simultaneously through two com-localization and discrimination, i.e., plementary tasks, in a novel multi-view learning framework.Two self-supervised pretext tasks are designed: (i) Multi-View Re-gion Detection aims at learning to localize regions-of-interest between augmented views of the input, and (ii)Multi-View Semantic Discrimination attempts to improve object-level discrimination for each region. The proposedSiamese DETR achieves state-of-the-art transfer perfor-mance on COCO and PASCAL VOC detection using dif-ferent DETR variants in all setups. Code is available at https://github.com/Zx55/SiameseDETR. 