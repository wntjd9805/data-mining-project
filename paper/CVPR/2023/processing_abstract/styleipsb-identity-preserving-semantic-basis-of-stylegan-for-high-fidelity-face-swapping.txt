Recent researches reveal that StyleGAN can generate highly realistic images, inspiring researchers to use pre-trained StyleGAN to generate high-fidelity swapped faces.However, existing methods fail to meet the expectations in two essential aspects of high-fidelity face swapping. Their results are blurry without pore-level details and fail to pre-serve identity for challenging cases. To overcome the above artifacts, we innovatively construct a series of identity-preserving semantic bases of StyleGAN (called StyleIPSB) in respect of pose, expression, and illumination. Each ba-sis of StyleIPSB controls one specific semantic attribute and disentangles with the others. The StyleIPSB constrains style code in the subspace of W+ space to preserve pore-level details and gives us a novel tool for high-fidelity face swapping, and we propose a three-stage framework for face swapping with StyleIPSB. Firstly, we transform the*Co-Corresponding authors†The research is supported in part by the National Natural ScienceFoundation of China (61972342,61832016,61972341,61902277) target facial images’ attributes to the source image. We learn the mapping from 3D Morphable Model (3DMM) pa-rameters, which capture the prominent semantic variance, to the coordinates of StyleIPSB that show higher identity-preserving and fidelity. Secondly, to transform detailed at-tributes which 3DMM does not capture, we learn the resid-ual attribute between the reenacted face and the target face.Finally, the face is blended into the background of the tar-get image. Extensive results and comparisons demonstrate that StyleIPSB can effectively preserve identity and pore-level details. The results of face swapping can achieve state-of-the-art performance. We will release our code at https://github.com/a686432/StyleIPSB 