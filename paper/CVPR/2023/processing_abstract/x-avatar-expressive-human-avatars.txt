We present X-Avatar, a novel avatar model that captures the full expressiveness of digital humans to bring about life-like experiences in telepresence, AR/VR and beyond. Our method models bodies, hands, facial expressions and ap-pearance in a holistic fashion and can be learned from ei-ther full 3D scans or RGB-D data. To achieve this, we pro-pose a part-aware learned forward skinning module that can be driven by the parameter space of SMPL-X, allow-ing for expressive animation of X-Avatars. To efficiently learn the neural shape and deformation fields, we pro-pose novel part-aware sampling and initialization strate-gies. This leads to higher fidelity results, especially for smaller body parts while maintaining efficient training de-spite increased number of articulated bones. To capture the appearance of the avatar with high-frequency details, we extend the geometry and deformation fields with a texture network that is conditioned on pose, facial expression, ge-*These authors contributed equally to this workâ€ Corresponding author ometry and the normals of the deformed surface. We show experimentally that our method outperforms strong base-lines both quantitatively and qualitatively on the animation task. To facilitate future research on expressive avatars we contribute a new dataset, called X-Humans, contain-ing 233 sequences of high-quality textured scans from 20 participants, totalling 35,500 data frames. Project page: https://ait.ethz.ch/X-Avatar. 