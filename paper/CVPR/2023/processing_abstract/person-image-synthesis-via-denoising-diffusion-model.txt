The pose-guided person image generation task requires synthesizing photorealistic images of humans in arbitrary poses.The existing approaches use generative adver-sarial networks that do not necessarily maintain realistic textures or need dense correspondences that struggle to handle complex deformations and severe occlusions.In this work, we show how denoising diffusion models can be applied for high-ﬁdelity person image synthesis with strong sample diversity and enhanced mode coverage of the learnt data distribution. Our proposed Person ImageDiffusion Model (PIDM) disintegrates the complex trans-fer problem into a series of simpler forward-backward de-noising steps. This helps in learning plausible source-to-target transformation trajectories that result in faithful textures and undistorted appearance details. We intro-duce a ‘texture diffusion module’ based on cross-attention to accurately model the correspondences between appear-ance and pose information available in source and target images. Further, we propose ‘disentangled classiﬁer-free guidance’ to ensure close resemblance between the condi-tional inputs and the synthesized output in terms of both pose and appearance information. Our extensive results on two large-scale benchmarks and a user study demon-strate the photorealism of our proposed approach under challenging scenarios. We also show how our generated images can help in downstream tasks. Code is available at https://github.com/ankanbhunia/PIDM. 