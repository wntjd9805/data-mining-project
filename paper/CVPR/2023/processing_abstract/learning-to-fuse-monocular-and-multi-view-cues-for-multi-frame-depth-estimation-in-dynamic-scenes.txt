Multi-frame depth estimation generally achieves high accuracy relying on the multi-view geometric consistency.When applied in dynamic scenes, e.g., autonomous driving, this consistency is usually violated in the dynamic areas, leading to corrupted estimations. Many multi-frame meth-ods handle dynamic areas by identifying them with explicit masks and compensating the multi-view cues with monocu-lar cues represented as local monocular depth or features.The improvements are limited due to the uncontrolled qual-ity of the masks and the underutilized benefits of the fu-sion of the two types of cues. In this paper, we propose a novel method to learn to fuse the multi-view and monocu-lar cues encoded as volumes without needing the heuristi-cally crafted masks. As unveiled in our analyses, the multi-view cues capture more accurate geometric information in static areas, and the monocular cues capture more useful contexts in dynamic areas. To let the geometric percep-tion learned from multi-view cues in static areas propagate to the monocular representation in dynamic areas and let monocular cues enhance the representation of multi-view cost volume, we propose a cross-cue fusion (CCF) module, which includes the cross-cue attention (CCA) to encode the spatially non-local relative intra-relations from each source to enhance the representation of the other. Experiments on real-world datasets prove the significant effectiveness and generalization ability of the proposed method. 