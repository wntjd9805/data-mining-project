Attribution methods, which employ heatmaps to iden-tify the most inﬂuential regions of an image that impact model decisions, have gained widespread popularity as a type of explainability method. However, recent research has exposed the limited practical value of these methods, at-tributed in part to their narrow focus on the most prominent regions of an image – revealing "where" the model looks, but failing to elucidate "what" the model sees in those areas.In this work, we try to ﬁll in this gap with CRAFT – a novel approach to identify both “what” and “where” by generat-ing concept-based explanations. We introduce 3 new ingre-dients to the automatic concept extraction literature: (i) a recursive strategy to detect and decompose concepts across layers, (ii) a novel method for a more faithful estimation of concept importance using Sobol indices, and (iii) the use of implicit differentiation to unlock Concept Attribution Maps.We conduct both human and computer vision experi-ments to demonstrate the beneﬁts of the proposed approach.We show that the proposed concept importance estimation technique is more faithful to the model than previous meth-ods. When evaluating the usefulness of the method for hu-man experimenters on a human-centered utility benchmark, we ﬁnd that our approach signiﬁcantly improves on two of the three test scenarios. 