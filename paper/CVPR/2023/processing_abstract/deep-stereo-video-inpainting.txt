Stereo video inpainting aims to fill the missing regions on the left and right views of the stereo video with plausi-ble content simultaneously. Compared with the single video inpainting that has achieved promising results using deep convolutional neural networks, inpainting the missing re-gions of stereo video has not been thoroughly explored. In essence, apart from the spatial and temporal consistency that single video inpainting needs to achieve, another key challenge for stereo video inpainting is to maintain the stereo consistency between left and right views and hence alleviate the 3D fatigue for viewers. In this paper, we pro-pose a novel deep stereo video inpainting network namedSVINet, which is the first attempt for stereo video inpainting task utilizing deep convolutional neural networks. SVINet first utilizes a self-supervised flow-guided deformable tem-poral alignment module to align the features on the left and right view branches, respectively. Then, the aligned features are fed into a shared adaptive feature aggrega-tion module to generate missing contents of their respec-tive branches. Finally, the parallax attention module (PAM) that uses the cross-view information to consider the signif-icant stereo correlation is introduced to fuse the completed features of left and right views. Furthermore, we develop a stereo consistency loss to regularize the trained parame-ters, so that our model is able to yield high-quality stereo video inpainting results with better stereo consistency. Ex-perimental results demonstrate that our SVINet outperforms state-of-the-art single video inpainting models. 