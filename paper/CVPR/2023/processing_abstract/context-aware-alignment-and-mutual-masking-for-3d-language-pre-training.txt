3D visual language reasoning plays an important role in effective human-computer interaction. The current ap-proaches for 3D visual reasoning are task-speciﬁc, and lack pre-training methods to learn generic representations that can transfer across various tasks. Despite the encourag-ing progress in vision-language pre-training for image-text data, 3D-language pre-training is still an open issue due to limited 3D-language paired data, highly sparse and ir-regular structure of point clouds and ambiguities in spa-tial relations of 3D objects with viewpoint changes. In this paper, we present a generic 3D-language pre-training ap-proach, that tackles multiple facets of 3D-language rea-soning by learning universal representations. Our learn-ing objective constitutes two main parts. 1) Context aware spatial-semantic alignment to establish ﬁne-grained corre-spondence between point clouds and texts. It reduces rela-tional ambiguities by aligning 3D spatial relationships with textual semantic context. 2) Mutual 3D-Language Masked modeling to enable cross-modality information exchange.Instead of reconstructing sparse 3D points for which lan-guage can hardly provide cues, we propose masked pro-posal reasoning to learn semantic class and mask-invariantCorresponding Author: Yinjie Lei (yinjie@scu.edu.cn) representations. Our proposed 3D-language pre-training method achieves promising results once adapted to vari-ous downstream tasks, including 3D visual grounding, 3D dense captioning and 3D question answering. Our codes are available at https://github.com/leolyj/3D-VLP 