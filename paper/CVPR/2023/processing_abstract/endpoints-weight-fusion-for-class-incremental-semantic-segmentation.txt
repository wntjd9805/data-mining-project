Class incremental semantic segmentation (CISS) fo-cuses on alleviating catastrophic forgetting to improve dis-crimination. Previous work mainly exploits regularization (e.g., knowledge distillation) to maintain previous knowl-edge in the current model. However, distillation alone of-ten yields limited gain to the model since only the repre-sentations of old and new models are restricted to be con-sistent.In this paper, we propose a simple yet effective method to obtain a model with a strong memory of old knowledge, named Endpoints Weight Fusion (EWF). In our method, the model containing old knowledge is fused with the model retaining new knowledge in a dynamic fusion manner, strengthening the memory of old classes in ever-changing distributions.In addition, we analyze the rela-tionship between our fusion strategy and a popular mov-ing average technique EMA, which reveals why our method is more suitable for class-incremental learning. To facili-tate parameter fusion with closer distance in the parameter space, we use distillation to enhance the optimization pro-cess. Furthermore, we conduct experiments on two widely used datasets, achieving state-of-the-art performance. 