In this paper, we investigate an open research task of generating controllable 3D textured shapes from the given textual descriptions. Previous works either require ground truth caption labeling or extensive optimization time. To resolve these issues, we present a novel frame-work, TAPS3D, to train a text-guided 3D shape generator with pseudo captions. Specifically, based on rendered 2D images, we retrieve relevant words from the CLIP vocab-ulary and construct pseudo captions using templates. Our constructed captions provide high-level semantic supervi-sion for generated 3D shapes. Further, in order to pro-duce fine-grained textures and increase geometry diversity, we propose to adopt low-level image regularization to en-able fake-rendered images to align with the real ones. Dur-ing the inference phase, our proposed model can generate 3D textured shapes from the given text without any addi-tional optimization. We conduct extensive experiments to analyze each of our proposed components and show the efficacy of our framework in generating high-fidelity 3D textured and text-relevant shapes. Code is available at https://github.com/plusmultiply/TAPS3D 