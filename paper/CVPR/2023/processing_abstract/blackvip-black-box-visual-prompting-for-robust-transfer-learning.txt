With the surge of large-scale pre-trained models (PTMs), fine-tuning these models to numerous downstream tasks be-comes a crucial problem. Consequently, parameter effi-cient transfer learning (PETL) of large models has grasped huge attention. While recent PETL methods showcase im-pressive performance, they rely on optimistic assumptions: 1) the entire parameter set of a PTM is available, and 2) a sufficiently large memory capacity for the fine-tuning is equipped. However, in most real-world applications, PTMs are served as a black-box API or proprietary software with-out explicit parameter accessibility. Besides, it is hard to meet a large memory requirement for modern PTMs.In this work, we propose black-box visual prompting (Black-VIP), which efficiently adapts the PTMs without knowl-edge about model architectures and parameters. Black-VIP has two components; 1) Coordinator and 2) simul-taneous perturbation stochastic approximation with gradi-ent correction (SPSA-GC). The Coordinator designs input-dependent image-shaped visual prompts, which improves few-shot adaptation and robustness on distribution/location shift. SPSA-GC efficiently estimates the gradient of a tar-get model to update Coordinator. Extensive experiments on 16 datasets demonstrate that BlackVIP enables robust adaptation to diverse domains without accessing PTMsâ€™ parameters, with minimal memory requirements. Code: https://github.com/changdaeoh/BlackVIP 