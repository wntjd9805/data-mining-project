Vision-language models (VLMs) pre-trained on large-scale image-text pairs have demonstrated impressive trans-ferability on various visual tasks. Transferring knowledge from such powerful VLMs is a promising direction for build-ing effective video recognition models. However, current exploration in this field is still limited. We believe that the greatest value of pre-trained VLMs lies in building a bridge between visual and textual domains. In this paper, we pro-pose a novel framework called BIKE, which utilizes the cross-modal bridge to explore bidirectional knowledge: i)We introduce the Video Attribute Association mechanism, which leverages the Video-to-Text knowledge to gen-erate textual auxiliary attributes for complementing video recognition. ii) We also present a Temporal Concept Spot-ting mechanism that uses the Text-to-Video expertise to capture temporal saliency in a parameter-free manner, leading to enhanced video representation. Extensive stud-ies on six popular video datasets, including Kinetics-400 & 600, UCF-101, HMDB-51, ActivityNet and Charades, show that our method achieves state-of-the-art performance in various recognition scenarios, such as general, zero-shot, and few-shot video recognition. Our best model achieves a state-of-the-art accuracy of 88.6% on the challengingKinetics-400 using the released CLIP model. The code is available at https://github.com/whwu95/BIKE. 