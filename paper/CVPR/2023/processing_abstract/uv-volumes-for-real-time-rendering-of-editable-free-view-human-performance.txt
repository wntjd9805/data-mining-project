Neural volume rendering enables photo-realistic render-ings of a human performer in free-view, a critical task in immersive VR/AR applications. But the practice is severely limited by high computational costs in the rendering pro-cess. To solve this problem, we propose the UV Volumes, a new approach that can render an editable free-view video of a human performer in real-time. It separates the high-frequency (i.e., non-smooth) human appearance from the 3D volume, and encodes them into 2D neural texture stacks (NTS). The smooth UV volumes allow much smaller and shallower neural networks to obtain densities and texture*Authors contributed equally to this work.†Corresponding Author. This work is partly supported by the Na-tional Key Research and Development Program of China under Grant 2022YFB3303800 and National Key Projects of China, 2021XJTU0040. coordinates in 3D while capturing detailed appearance in 2D NTS. For editability, the mapping between the parame-terized human model and the smooth texture coordinates al-lows us a better generalization on novel poses and shapes.Furthermore, the use of NTS enables interesting applica-tions, e.g., retexturing. Extensive experiments on CMUPanoptic, ZJU Mocap, and H36M datasets show that our model can render 960 × 540 images in 30FPS on average with comparable photo-realism to state-of-the-art methods.The project and supplementary materials are available at https://fanegg.github.io/UV-Volumes. 