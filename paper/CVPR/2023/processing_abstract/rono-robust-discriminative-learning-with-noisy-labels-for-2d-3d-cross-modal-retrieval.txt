Recently, with the advent of Metaverse and AI Gener-ated Content, cross-modal retrieval becomes popular with a burst of 2D and 3D data. However, this problem is challenging given the heterogeneous structure and semantic discrepancies. Moreover, imperfect annotations are ubiq-uitous given the ambiguous 2D and 3D content, thus in-evitably producing noisy labels to degrade the learning per-formance. To tackle the problem, this paper proposes a ro-bust 2D-3D retrieval framework (RONO) to robustly learn from noisy multimodal data. Specifically, one novel RobustDiscriminative Center Learning mechanism (RDCL) is pro-posed in RONO to adaptively distinguish clean and noisy samples for respectively providing them with positive and negative optimization directions, thus mitigating the nega-tive impact of noisy labels. Besides, we present a SharedSpace Consistency Learning mechanism (SSCL) to capture the intrinsic information inside the noisy data by minimizing the cross-modal and semantic discrepancy between com-mon space and label space simultaneously. Comprehen-sive mathematical analyses are given to theoretically prove the noise tolerance of the proposed method. Furthermore, we conduct extensive experiments on four 3D-model mul-timodal datasets to verify the effectiveness of our method by comparing it with 15 state-of-the-art methods. Code is available at https://github.com/penghu-cs/RONO. 