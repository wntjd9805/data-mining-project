Existing semantic segmentation approaches are often limited by costly pixel-wise annotations and predefinedIn this work, we present CLIP-S4 that leverages classes. self-supervised pixel representation learning and vision-language models to enable various semantic segmenta-tion tasks (e.g., unsupervised, transfer learning, language-driven segmentation) without any human annotations and unknown class information. We first learn pixel embeddings with pixel-segment contrastive learning from different aug-mented views of images. To further improve the pixel em-beddings and enable language-driven semantic segmenta-tion, we design two types of consistency guided by vision-language models: 1) embedding consistency, aligning our pixel embeddings to the joint feature space of a pre-trained vision-language model, CLIP [34]; and 2) semantic con-sistency, forcing our model to make the same predictions as CLIP over a set of carefully designed target classes with both known and unknown prototypes. Thus, CLIP-S4 en-ables a new task of class-free semantic segmentation where no unknown class information is needed during training. As a result, our approach shows consistent and substantial per-formance improvement over four popular benchmarks com-pared with the state-of-the-art unsupervised and language-driven semantic segmentation methods. More importantly, our method outperforms these methods on unknown class recognition by a large margin. 