Recent works on diffusion models have demonstrated a strong capability for conditioning image generation, e.g., text-guided image synthesis. Such success inspires many efforts trying to use large-scale pre-trained diffusion mod-els for tackling a challenging problemâ€“real image editing.Works conducted in this area learn a unique textual token corresponding to several images containing the same ob-ject. However, under many circumstances, only one im-age is available, such as the painting of the Girl with aPearl Earring. Using existing works on fine-tuning the pre-trained diffusion models with a single image causes severe overfitting issues. The information leakage from the pre-trained diffusion models makes editing can not keep the same content as the given image while creating new fea-tures depicted by the language guidance. This work aims to address the problem of single-image editing. We propose a novel model-based guidance built upon the classifier-free guidance so that the knowledge from the model trained on a single image can be distilled into the pre-trained diffu-sion model, enabling content creation even with one given image. Additionally, we propose a patch-based fine-tuning that can effectively help the model generate images of arbi-trary resolution. We provide extensive experiments to vali-date the design choices of our approach and show promis-ing editing capabilities, including changing style, content addition, and object manipulation. Our code is made pub-licly available here. 