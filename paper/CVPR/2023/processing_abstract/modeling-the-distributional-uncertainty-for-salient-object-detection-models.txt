Most of the existing salient object detection (SOD) mod-els focus on improving the overall model performance, with-out explicitly explaining the discrepancy between the train-ing and testing distributions. In this paper, we investigate a particular type of epistemic uncertainty, namely distribu-tional uncertainty, for salient object detection. Specifically, for the first time, we explore the existing class-aware dis-tribution gap exploration techniques, i.e. long-tail learning, single-model uncertainty modeling and test-time strategies, and adapt them to model the distributional uncertainty for our class-agnostic task. We define test sample that is dissim-ilar to the training dataset as being “out-of-distribution” (OOD) samples. Different from the conventional OOD def-inition, where OOD samples are those not belonging to the closed-world training categories, OOD samples for SOD are those break the basic priors of saliency, i.e. center prior, color contrast prior, compactness prior and etc., indicat-ing OOD as being “continuous” instead of being discrete for our task. We’ve carried out extensive experimental re-sults to verify effectiveness of existing distribution gap mod-eling techniques for SOD, and conclude that both train-time single-model uncertainty estimation techniques and weight-regularization solutions that preventing model activation from drifting too much are promising directions for mod-eling distributional uncertainty for SOD. 