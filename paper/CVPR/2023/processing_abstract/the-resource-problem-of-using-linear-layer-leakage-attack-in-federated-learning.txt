Secure aggregation promises a heightened level of pri-vacy in federated learning, maintaining that a server only has access to a decrypted aggregate update. Within this setting, linear layer leakage methods are the only data re-construction attacks able to scale and achieve a high leak-age rate regardless of the number of clients or batch size.This is done through increasing the size of an injected fully-connected (FC) layer. However, this results in a resource overhead which grows larger with an increasing number of clients. We show that this resource overhead is caused by an incorrect perspective in all prior work that treats an at-tack on an aggregate update in the same way as an individ-ual update with a larger batch size. Instead, by attacking the update from the perspective that aggregation is combin-ing multiple individual updates, this allows the application of sparsity to alleviate resource overhead. We show that the use of sparsity can decrease the model size overhead by over 327× and the computation time by 3.34× compared toSOTA while maintaining equivalent total leakage rate, 77% even with 1000 clients in aggregation. 