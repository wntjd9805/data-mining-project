Federated Distillation (FD) has recently attracted in-creasing attention for its efficiency in aggregating multi-ple diverse local models trained from statistically hetero-geneous data of distributed clients. Existing FD methods generally treat these models equally by merely computing the average of their output soft predictions for some given input distillation sample, which does not take the diversity across all local models into account, thus leading to de-graded performance of the aggregated model, especially when some local models learn little knowledge about the sample. In this paper, we propose a new perspective that treats the local data in each client as a specific domain and design a novel domain knowledge aware federated distilla-tion method, dubbed DaFKD, that can discern the impor-tance of each model to the distillation sample, and thus is able to optimize the ensemble of soft predictions from di-verse models. Specifically, we employ a domain discrimi-nator for each client, which is trained to identify the corre-lation factor between the sample and the corresponding do-main. Then, to facilitate the training of the domain discrim-inator while saving communication costs, we propose shar-ing its partial parameters with the classification model. Ex-tensive experiments on various datasets and settings show that the proposed method can improve the model accuracy by up to 6.02% compared to state-of-the-art baselines. 