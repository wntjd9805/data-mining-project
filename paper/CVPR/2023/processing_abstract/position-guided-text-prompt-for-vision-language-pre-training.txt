Vision-Language Pre-Training (VLP) has shown promis-ing capabilities to align image and text pairs, facilitating a broad variety of cross-modal learning tasks. However, we observe that VLP models often lack the visual ground-ing/localization capability which is critical for many down-stream tasks such as visual reasoning. In this work, we pro-pose a novel Position-guided Text Prompt (PTP) paradigm to enhance the visual grounding ability of cross-modal mod-els trained with VLP. Specifically, in the VLP phase, PTP divides the image into N × N blocks, and identifies the ob-jects in each block through the widely used object detector in VLP. It then reformulates the visual grounding task into a fill-in-the-blank problem given a PTP by encouraging the model to predict the objects in the given blocks or regress the blocks of a given object, e.g. filling “[P]” or “[O]” in a PTP “The block [P] has a [O]”. This mechanism im-proves the visual grounding capability of VLP models and thus helps them better handle various downstream tasks. By introducing PTP into several state-of-the-art VLP frame-works, we observe consistently significant improvements across representative cross-modal learning model architec-tures and several benchmarks, e.g. zero-shot Flickr30KRetrieval (+4.8 in average recall@1) for ViLT [16] base-line, and COCO Captioning (+5.3 in CIDEr) for SOTABLIP [19] baseline. Moreover, PTP achieves comparable results with object-detector based methods [8, 23, 45], and much faster inference speed since PTP discards its object detector for inference while the later cannot. 