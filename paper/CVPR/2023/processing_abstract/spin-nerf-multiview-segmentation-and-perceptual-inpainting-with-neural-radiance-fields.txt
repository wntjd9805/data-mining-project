Neural Radiance Fields (NeRFs) have emerged as a pop-ular approach for novel view synthesis. While NeRFs are quickly being adapted for a wider set of applications, intu-itively editing NeRF scenes is still an open challenge. One important editing task is the removal of unwanted objects from a 3D scene, such that the replaced region is visually plausible and consistent with its context. We refer to this task as 3D inpainting. In 3D, solutions must be both con-sistent across multiple views and geometrically valid.In this paper, we propose a novel 3D inpainting method that addresses these challenges. Given a small set of posed im-ages and sparse annotations in a single input image, our framework ﬁrst rapidly obtains a 3D segmentation mask for a target object. Using the mask, a perceptual optimization-based approach is then introduced that leverages learned 2D image inpainters, distilling their information into 3D space, while ensuring view consistency. We also address the lack of a diverse benchmark for evaluating 3D scene inpainting methods by introducing a dataset comprised ofIn particular, our dataset challenging real-world scenes. contains views of the same scene with and without a target object, enabling more principled benchmarking of the 3D inpainting task. We ﬁrst demonstrate the superiority of our approach on multiview segmentation, comparing to NeRF-based methods and 2D segmentation approaches. We then evaluate on the task of 3D inpainting, establishing state-of-the-art performance against other NeRF manipulation al-gorithms, as well as a strong 2D image inpainter baseline. 