3D interacting hand pose estimation from a single RGB image is a challenging task, due to serious self-occlusion and inter-occlusion towards hands, confusing similar ap-pearance patterns between 2 hands, ill-posed joint posi-tion mapping from 2D to 3D, etc.. To address these, we propose to extend A2J-the state-of-the-art depth-based 3D single hand pose estimation method-to RGB domain under interacting hand condition. Our key idea is to equip A2J with strong local-global aware ability to well capture in-teracting hands’ local fine details and global articulated clues among joints jointly. To this end, A2J is evolved un-der Transformer’s non-local encoding-decoding framework to build A2J-Transformer. It holds 3 main advantages overA2J. First, self-attention across local anchor points is built to make them global spatial context aware to better cap-ture joints’ articulation clues for resisting occlusion. Sec-ondly, each anchor point is regarded as learnable query with adaptive feature learning for facilitating pattern fitting capacity, instead of having the same local representation with the others. Last but not least, anchor point locates in 3D space instead of 2D as in A2J, to leverage 3D pose prediction. Experiments on challenging InterHand 2.6M demonstrate that, A2J-Transformer can achieve state-of-the-art model-free performance (3.38mm MPJPE advance-ment in 2-hand case) and can also be applied to depth domain with strong generalization. The code is avaliable at https://github.com/ChanglongJiangGit/A2J-Transformer.†Yang Xiao is corresponding author(Yang Xiao@hust.edu.cn).Figure 1. The main idea of A2J-Transformer. 3D anchors are uniformly set and act as local regressors to predict each hand joint. Meanwhile, they are also used as queries, and the interaction among them is established to acquire global context. 