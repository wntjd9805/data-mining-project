Owing to the non-i.i.d. nature of client data, channel neu-rons in federated-learned models may specialize to distinct features for different clients. Yet, existing channel-sparse federated learning (FL) algorithms prescribe ﬁxed sparsity strategies for client models, and may thus prevent clients from training channel neurons collaboratively. To minimize the impact of sparsity on FL convergence, we propose Flado to improve the alignment of client model update trajectories by tailoring the sparsities of individual neurons in each client.Empirical results show that while other sparse methods are surprisingly impactful to convergence, Flado can not only at-tain the highest task accuracies with unlimited budget across a range of datasets, but also signiﬁcantly reduce the amount of ﬂoating-point operations (FLOPs) required for training more than by 10 under the same communications budget, and push the Pareto frontier of communication/computation trade-off notably further than competing FL algorithms.⇥ 