networks [17, 19, 45, 46].While impressive progress has been achieved, video in-stance segmentation (VIS) methods with per-clip input of-ten fail on challenging videos with occluded objects and crowded scenes. This is mainly because instance queries in these methods cannot encode well the discriminative em-beddings of instances, making the query-based segmenter difficult to distinguish those ‘hard’ instances. To address these issues, we propose to mine discriminative query em-beddings (MDQE) to segment occluded instances on chal-lenging videos. First, we initialize the positional embed-dings and content features of object queries by considering their spatial contextual information and the inter-frame ob-ject motion. Second, we propose an inter-instance mask repulsion loss to distance each instance from its nearby non-target instances. The proposed MDQE is the first VIS method with per-clip input that achieves state-of-the-art re-sults on challenging videos and competitive performance on simple videos. In specific, MDQE with ResNet50 achieves 33.0% and 44.5% mask AP on OVIS and YouTube-VIS 2021, respectively. Code of MDQE can be found at https://github.com/MinghanLi/MDQE_CVPR2023. 