We present AssemblyHands, a large-scale benchmark dataset with accurate 3D hand pose annotations, to facili-tate the study of egocentric activities with challenging hand-object interactions. The dataset includes synchronized ego-centric and exocentric images sampled from the recent As-sembly101 dataset, in which participants assemble and dis-assemble take-apart toys. To obtain high-quality 3D hand pose annotations for the egocentric images, we develop an efficient pipeline, where we use an initial set of manual an-notations to train a model to automatically annotate a much larger dataset. Our annotation model uses multi-view fea-ture fusion and an iterative refinement scheme, and achieves an average keypoint error of 4.20 mm, which is 85% lower than the error of the original annotations in Assembly101.AssemblyHands provides 3.0M annotated images, includ-ing 490K egocentric images, making it the largest existing benchmark dataset for egocentric 3D hand pose estimation.Using this data, we develop a strong single-view baseline of 3D hand pose estimation from egocentric images. Further-more, we design a novel action classification task to evalu-ate predicted 3D hand poses. Our study shows that having higher-quality hand poses directly improves the ability to recognize actions.Figure 1. High-quality 3D hand poses as an effective represen-tation for egocentric activity understanding. AssemblyHands provides high-quality 3D hand pose annotations computed from multi-view exocentric images sampled from Assembly101 [28], which originally comes with inaccurate annotations computed from egocentric images (see the incorrect left-hand pose predic-tion). As we experimentally demonstrate on an action classifica-tion task, models trained on high-quality annotations achieve sig-nificantly higher accuracy. 