Modern 3D-GANs synthesize geometry and texture by training on large-scale datasets with a consistent structure.Training such models on stylized, artistic data, with often unknown, highly variable geometry, and camera informa-tion has not yet been shown possible. Can we train a 3DGAN on such artistic data, while maintaining multi-view consistency and texture quality? To this end, we propose an adaptation framework, where the source domain is a pre-trained 3D-GAN, while the target domain is a 2D-GAN trained on artistic datasets. We, then, distill the knowl-edge from a 2D generator to the source 3D generator. To do that, we first propose an optimization-based method to align the distributions of camera parameters across do-mains. Second, we propose regularizations necessary to learn high-quality texture, while avoiding degenerate ge-ometric solutions, such as flat shapes. Third, we show a deformation-based technique for modeling exaggerated geometry of artistic domains, enablingÐas a byproductÐ personalized geometric editing. Finally, we propose a novel inversion method for 3D-GANs linking the latent spaces of the source and the target domains. Our contributionsÐfor the first timeÐallow for the generation, editing, and anima-tion of personalized artistic 3D avatars on artistic datasets.Project Page: https:/rameenabdal.github.io/3DAvatarGAN 