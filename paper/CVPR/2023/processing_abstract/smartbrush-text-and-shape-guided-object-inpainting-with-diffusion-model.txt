Generic image inpainting aims to complete a corrupted image by borrowing surrounding information, which barely generates novel content. By contrast, multi-modal inpaint-ing provides more flexible and useful controls on the in-painted content, e.g., a text prompt can be used to describe an object with richer attributes, and a mask can be used to constrain the shape of the inpainted object rather than be-ing only considered as a missing area. We propose a new diffusion-based model named SmartBrush for completing a missing region with an object using both text and shape-guidance. While previous work such as DALLE-2 and Sta-ble Diffusion can do text-guided inapinting they do not sup-port shape guidance and tend to modify background tex-ture surrounding the generated object. Our model incor-porates both text and shape guidance with precision con-trol. To preserve the background better, we propose a novel training and sampling strategy by augmenting the diffusionU-net with object-mask prediction. Lastly, we introduce a multi-task training strategy by jointly training inpaint-* Work done during internship at Adobe. ing with text-to-image generation to leverage more training data. We conduct extensive experiments showing that our model outperforms all baselines in terms of visual quality, mask controllability, and background preservation. 