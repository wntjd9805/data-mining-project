Recent research on video moment retrieval has mostly focused on enhancing the performance of accuracy, effi-ciency, and robustness, all of which largely rely on the abun-dance of high-quality annotations. While the precise frame-level annotations are time-consuming and cost-expensive, few attentions have been paid to the labeling process. In this work, we explore a new interactive manner to stimu-late the process of human-in-the-loop annotation in video moment retrieval task. The key challenge is to select “am-biguous” frames and videos for binary annotations to fa-cilitate the network training. To be specific, we propose a new hierarchical uncertainty-based modeling that explicitly considers modeling the uncertainty of each frame within the entire video sequence corresponding to the query descrip-tion, and selecting the frame with the highest uncertainty.Only selected frame will be annotated by the human ex-perts, which can largely reduce the workload. After ob-taining a small number of labels provided by the expert, we show that it is sufficient to learn a competitive video mo-ment retrieval model in such a harsh environment. More-over, we treat the uncertainty score of frames in a video as a whole, and estimate the difficulty of each video, which can further relieve the burden of video selection. In gen-eral, our active learning strategy for video moment retrieval works not only at the frame level but also at the sequence level. Experiments on two public datasets validate the ef-fectiveness of our proposed method. Our code is released at https://github.com/renjie-liang/HUAL. (a) (b)Figure 1. We propose a new interactive method named HUAL which only requires binary annotations to reduce the annotation cost. (a) In each round, user (student) selects a frame with the largest uncertainty, and the expert (teacher) returns the binary la-bel of this frame as feedback. (b) With more labels provided, theVMR model is retrained and the whole process can be treated in a human-in-the-loop manner. 