3D single object tracking plays an essential role in many applications, such as autonomous driving.It remains a challenging problem due to the large appearance varia-tion and the sparsity of points caused by occlusion and lim-ited sensor capabilities. Therefore, contextual information across two consecutive frames is crucial for effective ob-ject tracking. However, points containing such useful in-formation are often overlooked and cropped out in existing methods, leading to insufficient use of important contextual knowledge. To address this issue, we propose CXTrack, a novel transformer-based network for 3D object tracking, which exploits ConteXtual information to improve the track-ing results. Specifically, we design a target-centric trans-former network that directly takes point features from two consecutive frames and the previous bounding box as in-put to explore contextual information and implicitly propa-gate target cues. To achieve accurate localization for ob-jects of all sizes, we propose a transformer-based local-ization head with a novel center embedding module to dis-tinguish the target from distractors. Extensive experiments on three large-scale datasets, KITTI, nuScenes and WaymoOpen Dataset, show that CXTrack achieves state-of-the-art tracking performance while running at 34 FPS. 