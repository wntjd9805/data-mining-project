The main challenge in domain generalization (DG) is to handle the distribution shift problem that lies between the training and test data. Recent studies suggest that test-time training (TTT), which adapts the learned model with test data, might be a promising solution to the problem. Gen-erally, a TTT strategy hinges its performance on two main factors: selecting an appropriate auxiliary TTT task for up-dating and identifying reliable parameters to update during the test phase. Both previous arts and our experiments in-dicate that TTT may not improve but be detrimental to the learned model if those two factors are not properly consid-ered. This work addresses those two factors by proposing an Improved Test-Time Adaptation (ITTA) method. First, in-stead of heuristically defining an auxiliary objective, we pro-pose a learnable consistency loss for the TTT task, which con-tains learnable parameters that can be adjusted toward bet-ter alignment between our TTT task and the main prediction task. Second, we introduce additional adaptive parameters for the trained model, and we suggest only updating the adap-tive parameters during the test phase. Through extensive ex-periments, we show that the proposed two strategies are ben-eficial for the learned model (see Figure 1), and ITTA could achieve superior performance to the current state-of-the-art methods on several DG benchmarks. Code is available at https://github.com/liangchen527/ITTA. 