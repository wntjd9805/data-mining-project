An effective framework for learning 3D representations for perception tasks is distilling rich self-supervised image features via contrastive learning. However, image-to-point representation learning for autonomous driving datasets faces two main challenges: 1) the abundance of self-similarity, which results in the contrastive losses pushing away semantically similar point and image regions and thus disturbing the local semantic structure of the learned rep-resentations, and 2) severe class imbalance as pretraining gets dominated by over-represented classes. We propose to alleviate the self-similarity problem through a novel seman-tically tolerant image-to-point contrastive loss that takes into consideration the semantic distance between positive and negative image regions to minimize contrasting seman-tically similar point and image regions. Additionally, we address class imbalance by designing a class-agnostic bal-anced loss that approximates the degree of class imbalance through an aggregate sample-to-samples semantic similar-ity measure. We demonstrate that our semantically-tolerant contrastive loss with class balancing improves state-of-the-art 2D-to-3D representation learning in all evaluation set-tings on 3D semantic segmentation. Our method con-sistently outperforms state-of-the-art 2D-to-3D representa-tion learning frameworks across a wide range of 2D self-supervised pretrained models. 