This paper explores the tasks of leveraging auxiliary modalities which are only available at training to enhance multimodal representation learning through cross-modalKnowledge Distillation (KD). The widely adopted mutual information maximization-based objective leads to a short-cut solution of the weak teacher, i.e., achieving the max-imum mutual information by simply making the teacher model as weak as the student model. To prevent such a weak solution, we introduce an additional objective term, i.e., the mutual information between the teacher and the auxiliary modality model. Besides, to narrow down the in-formation gap between the student and teacher, we further propose to minimize the conditional entropy of the teacher given the student. Novel training schemes based on con-trastive learning and adversarial learning are designed to optimize the mutual information and the conditional en-tropy, respectively. Experimental results on three popular multimodal benchmark datasets have shown that the pro-posed method outperforms a range of state-of-the-art ap-proaches for video recognition, video retrieval and emotion classification. 