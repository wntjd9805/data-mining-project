Knowledge distillation (KD) is a technique that transfers the knowledge from a large teacher network to a small stu-dent network. It has been widely applied to many different tasks, such as model compression and federated learning.However, existing KD methods fail to generalize to gen-eral deep directed graphical models (DGMs) with arbitrary layers of random variables. We refer by deep DGMs toDGMs whose conditional distributions are parameterized by deep neural networks. In this work, we propose a novel unified knowledge distillation framework for deep DGMs on various applications. Specifically, we leverage the repa-rameterization trick to hide the intermediate latent variables, resulting in a compact DGM. Then we develop a surrogate distillation loss to reduce error accumulation through mul-tiple layers of random variables. Moreover, we present the connections between our method and some existing knowl-edge distillation approaches. The proposed framework is evaluated on four applications: data-free hierarchical varia-tional autoencoder (VAE) compression, data-free variational recurrent neural networks (VRNN) compression, data-freeHelmholtz Machine (HM) compression, and VAE continual learning. The results show that our distillation method out-performs the baselines in data-free model compression tasks.We further demonstrate that our method significantly im-proves the performance of KD-based continual learning for data generation. Our source code is available at https://github.com/YizhuoChen99/KD4DGM-CVPR. 