We argue that there are many notions of ‘similarity’ and that models, like humans, should be able to adapt to these dynamically. This contrasts with most representation learn-ing methods, supervised or self-supervised, which learn aﬁxed embedding function and hence implicitly assume a sin-gle notion of similarity. For instance, models trained on Im-ageNet are biased towards object categories, while a user might prefer the model to focus on colors, textures or spe-ciﬁc elements in the scene. In this paper, we propose theGeneCIS (‘genesis’) benchmark, which measures models’ ability to adapt to a range of similarity conditions. Ex-tending prior work, our benchmark is designed for zero-shot evaluation only, and hence considers an open-set of similarity conditions. We ﬁnd that baselines from powerfulCLIP models struggle on GeneCIS and that performance on the benchmark is only weakly correlated with ImageNet ac-curacy, suggesting that simply scaling existing methods is not fruitful. We further propose a simple, scalable solution based on automatically mining information from existing image-caption datasets. We ﬁnd our method offers a sub-stantial boost over the baselines on GeneCIS, and further improves zero-shot performance on related image retrieval benchmarks. In fact, though evaluated zero-shot, our model surpasses state-of-the-art supervised models on MIT-States.We, the architects of the machine, must decide a-priori what constitutes its ‘world’; what things are to be taken as ‘similar’ or ‘equal’ — Karl Popper, 1963 