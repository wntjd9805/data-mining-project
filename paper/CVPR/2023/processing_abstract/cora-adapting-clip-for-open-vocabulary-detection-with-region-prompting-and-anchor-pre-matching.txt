Open-vocabulary detection (OVD) is an object detection task aiming at detecting objects from novel categories be-yond the base categories on which the detector is trained.Recent OVD methods rely on large-scale visual-language pre-trained models, such as CLIP, for recognizing novel objects. We identify the two core obstacles that need to be tackled when incorporating these models into detec-tor training: (1) the distribution mismatch that happens when applying a VL-model trained on whole images to region recognition tasks; (2) the difficulty of localizing objects of unseen classes. To overcome these obstacles, we propose CORA, a DETR-style framework that adaptsCLIP for Open-vocabulary detection by Region prompt-ing and Anchor pre-matching. Region prompting mitigates the whole-to-region distribution gap by prompting the re-gion features of the CLIP-based region classifier. Anchor pre-matching helps learning generalizable object localiza-tion by a class-aware matching mechanism. We evaluateCORA on the COCO OVD benchmark, where we achieve 41.7 AP50 on novel classes, which outperforms the pre-vious SOTA by 2.4 AP50 even without resorting to extra training data. When extra training data is available, we train CORA+ on both ground-truth base-category annota-tions and additional pseudo bounding box labels computed by CORA. CORA+ achieves 43.1 AP50 on the COCO OVD benchmark and 28.1 box APr on the LVIS OVD benchmark.The code is available at https://github.com/tgxs002/CORA. 