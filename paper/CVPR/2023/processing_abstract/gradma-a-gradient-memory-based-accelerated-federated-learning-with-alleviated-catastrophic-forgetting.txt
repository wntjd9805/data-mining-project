Federated Learning (FL) has emerged as a de facto ma-chine learning area and received rapid increasing research interests from the community. However, catastrophic forget-ting caused by data heterogeneity and partial participation poses distinctive challenges for FL, which are detrimental to the performance. To tackle the problems, we propose a new FL approach (namely GradMA), which takes inspira-tion from continual learning to simultaneously correct the server-side and worker-side update directions as well as take full advantage of serverâ€™s rich computing and mem-ory resources. Furthermore, we elaborate a memory reduc-tion strategy to enable GradMA to accommodate FL with a large scale of workers. We then analyze convergence ofGradMA theoretically under the smooth non-convex setting and show that its convergence rate achieves a linear speed up w.r.t the increasing number of sampled active workers.At last, our extensive experiments on various image classi-fication tasks show that GradMA achieves significant per-formance gains in accuracy and communication efficiency compared to SOTA baselines. We provide our code here: https://github.com/lkyddd/GradMA. 