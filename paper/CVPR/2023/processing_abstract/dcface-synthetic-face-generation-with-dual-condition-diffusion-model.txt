Generating synthetic datasets for training face recogni-tion models is challenging because dataset generation en-tails more than creating high fidelity images.It involves generating multiple images of same subjects under different factors (e.g., variations in pose, illumination, expression, aging and occlusion) which follows the real image condi-tional distribution. Previous works have studied the gener-ation of synthetic datasets using GAN or 3D models. In this work, we approach the problem from the aspect of combin-ing subject appearance (ID) and external factor (style) con-ditions. These two conditions provide a direct way to con-trol the inter-class and intra-class variations. To this end, we propose a Dual Condition Face Generator (DCFace) based on a diffusion model. Our novel Patch-wise style ex-tractor and Time-step dependent ID loss enables DCFace to consistently produce face images of the same subject under different styles with precise control. Face recognition mod-els trained on synthetic images from the proposed DCFace provide higher verification accuracies compared to previ-ous works by 6.11% on average in 4 out of 5 test datasets,LFW, CFP-FP, CPLFW, AgeDB and CALFW. Code Link 