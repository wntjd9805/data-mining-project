Humans have the ability of recognizing visual semantics in an unlimited granularity, but existing visual recognition algorithms cannot achieve this goal. In this paper, we estab-lish a new paradigm named visual recognition by request (ViRReq1) to bridge the gap. The key lies in decompos-ing visual recognition into atomic tasks named requests and leveraging a knowledge base, a hierarchical and text-based dictionary, to assist task deﬁnition. ViRReq allows for (i) learning complicated whole-part hierarchies from highly incomplete annotations and (ii) inserting new concepts with minimal efforts. We also establish a solid baseline by in-tegrating language-driven recognition into recent seman-tic and instance segmentation methods, and demonstrate its ﬂexible recognition ability on CPP and ADE20K, two datasets with hierarchical whole-part annotations. 