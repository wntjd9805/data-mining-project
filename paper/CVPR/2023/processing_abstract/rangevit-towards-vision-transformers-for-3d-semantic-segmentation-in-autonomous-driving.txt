Casting semantic segmentation of outdoor LiDAR point clouds as a 2D problem, e.g., via range projection, is an effective and popular approach. These projection-based methods usually benefit from fast computations and, when combined with techniques which use other point cloudToday, representations, achieve state-of-the-art results. projection-based methods leverage 2D CNNs but recent ad-vances in computer vision show that vision transformers (ViTs) have achieved state-of-the-art results in many image-based benchmarks. In this work, we question if projection-based methods for 3D semantic segmentation can benefit from these latest improvements on ViTs. We answer posi-tively but only after combining them with three key ingre-dients: (a) ViTs are notoriously hard to train and require a lot of training data to learn powerful representations.By preserving the same backbone architecture as for RGB images, we can exploit the knowledge from long training on large image collections that are much cheaper to ac-quire and annotate than point clouds. We reach our best results with pre-trained ViTs on large image datasets. (b)We compensate ViTsâ€™ lack of inductive bias by substituting a tailored convolutional stem for the classical linear em-bedding layer. (c) We refine pixel-wise predictions with a convolutional decoder and a skip connection from the con-volutional stem to combine low-level but fine-grained fea-tures of the the convolutional stem with the high-level but coarse predictions of the ViT encoder. With these ingre-dients, we show that our method, called RangeViT, out-performs existing projection-based methods on nuScenes and SemanticKITTI. The code is available at https:// github.com/valeoai/rangevit. 