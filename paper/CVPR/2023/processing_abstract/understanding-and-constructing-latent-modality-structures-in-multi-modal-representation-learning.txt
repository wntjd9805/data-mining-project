Contrastive loss has been increasingly used in learning representations from multiple modalities. In the limit, the nature of the contrastive loss encourages modalities to ex-actly match each other in the latent space. Yet it remains an open question how the modality alignment affects the downstream task performance.In this paper, based on an information-theoretic argument, we ﬁrst prove that ex-act modality alignment is sub-optimal in general for down-stream prediction tasks. Hence we advocate that the key of better performance lies in meaningful latent modality struc-tures instead of perfect modality alignment. To this end, we propose three general approaches to construct latent modality structures. Speciﬁcally, we design 1) a deep fea-ture separation loss for intra-modality regularization; 2) aBrownian-bridge loss for inter-modality regularization; and 3) a geometric consistency loss for both intra- and inter-modality regularization. Extensive experiments are con-ducted on two popular multi-modal representation learn-ing frameworks: the CLIP-based two-tower model and theALBEF-based fusion model. We test our model on a va-riety of tasks including zero/few-shot image classiﬁcation, image-text retrieval, visual question answering, visual rea-soning, and visual entailment. Our method achieves consis-tent improvements over existing methods, demonstrating the effectiveness and generalizability of our proposed approach on latent modality structure regularization. 