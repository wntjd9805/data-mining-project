In recent years, vision-centric perception has flourished in various autonomous driving tasks, including 3D detec-tion, semantic map construction, motion forecasting, and depth estimation. Nevertheless, the latency of vision-centric approaches is too high for practical deployment (e.g., most camera-based 3D detectors have a runtime greater than 300ms). To bridge the gap between ideal researches and real-world applications, it is necessary to quantify the trade-off between performance and efficiency. Tradition-ally, autonomous-driving perception benchmarks perform the offline evaluation, neglecting the inference time de-lay. To mitigate the problem, we propose the Autonomous-driving StreAming Perception (ASAP) benchmark, which is the first benchmark to evaluate the online performance of vision-centric perception in autonomous driving. On the basis of the 2Hz annotated nuScenes dataset, we first propose an annotation-extending pipeline to generate high-frame-rate labels for the 12Hz raw images. Referring to the practical deployment, the Streaming Perception Under constRained-computation (SPUR) evaluation protocol is further constructed, where the 12Hz inputs are utilized for streaming evaluation under the constraints of differ-In the ASAP benchmark, ent computational resources. comprehensive experiment results reveal that the model rank alters under different constraints, suggesting that the model latency and computation budget should be consid-ered as design choices to optimize the practical deployment.To facilitate further research, we establish baselines for camera-based streaming 3D detection, which consistently enhance the streaming performance across various hard-ware. ASAP project page: https://github.com/JeffWang987/ASAP. 