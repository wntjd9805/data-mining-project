Spatio-Temporal Video Grounding (STVG) aims to lo-calize the target object spatially and temporally accord-It is a challenging task ing to the given language query. in which the model should well understand dynamic visual cues (e.g., motions) and static visual cues (e.g., object ap-pearances) in the language description, which requires ef-fective joint modeling of spatio-temporal visual-linguistic dependencies.In this work, we propose a novel frame-work in which a static vision-language stream and a dy-namic vision-language stream are developed to collabora-tively reason the target tube. The static stream performs cross-modal understanding in a single frame and learns to attend to the target object spatially according to intra-frame visual cues like object appearances. The dynamic stream models visual-linguistic dependencies across mul-tiple consecutive frames to capture dynamic cues like mo-tions. We further design a novel cross-stream collabora-tive block between the two streams, which enables the static and dynamic streams to transfer useful and complementary information from each other to achieve collaborative rea-soning. Experimental results show the effectiveness of the collaboration of the two streams and our overall frame-work achieves new state-of-the-art performance on bothHCSTVG and VidSTG datasets. 