Video Frame Interpolation (VFI) aims to generate in-termediate video frames between consecutive input frames.Since the event cameras are bio-inspired sensors that only encode brightness changes with a micro-second temporal resolution, several works utilized the event camera to en-hance the performance of VFI. However, existing methods estimate bidirectional inter-frame motion fields with only events or approximations, which can not consider the com-plex motion in real-world scenarios.In this paper, we propose a novel event-based VFI framework with cross-modal asymmetric bidirectional motion field estimation. In detail, our EIF-BiOFNet utilizes each valuable charac-teristic of the events and images for direct estimation of inter-frame motion fields without any approximation meth-ods. Moreover, we develop an interactive attention-based frame synthesis network to efficiently leverage the comple-mentary warping-based and synthesis-based features. Fi-nally, we build a large-scale event-based VFI dataset, ERF-X170FPS, with a high frame rate, extreme motion, and dynamic textures to overcome the limitations of previous event-based VFI datasets. Extensive experimental results validate that our method shows significant performance im-provement over the state-of-the-art VFI methods on vari-ous datasets. Our project pages are available at: https://github.com/intelpro/CBMNet 