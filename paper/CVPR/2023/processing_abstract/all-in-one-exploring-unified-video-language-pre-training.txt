Mainstream Video-Language Pre-training (VLP) mod-els [10, 26, 64] consist of three parts, a video encoder, a text encoder, and a video-text fusion Transformer. They pursue better performance via utilizing heavier unimodal encoders or multimodal fusion Transformers, resulting in increased parameters with lower efficiency in downstream tasks. In this work, we for the first time introduce an end-to-end VLP model, namely all-in-one Transformer, that em-beds raw video and textual signals into joint representations using a unified backbone architecture. We argue that the unique temporal information of video data turns out to be a key barrier hindering the design of a modality-agnosticTransformer. To overcome the challenge, we introduce a novel and effective token rolling operation to encode tem-poral representations from video clips in a non-parametric manner. The careful design enables the representation learning of both video-text multimodal inputs and unimodal inputs using a unified model. Our pre-trained all-in-oneTransformer is transferred to various downstream video-text tasks after fine-tuning, including text-video retrieval, video-question answering, multiple choice and video cap-tioning. State-of-the-art performances with the minimal model FLOPs on ten datasets demonstrate the superior-ity of our method compared to the competitive counter-parts. The code and pretrained models are available at https://github.com/showlab/all-in-one. 