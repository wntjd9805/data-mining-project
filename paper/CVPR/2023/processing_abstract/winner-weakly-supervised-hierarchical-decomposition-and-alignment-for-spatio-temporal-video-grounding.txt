Spatio-temporal video grounding aims to localize the aligned visual tube corresponding to a language query.Existing techniques achieve such alignment by exploiting dense boundary and bounding box annotations, which can be prohibitively expensive. To bridge the gap, we inves-tigate the weakly-supervised setting, where models learn from easily accessible video-language data without anno-tations. We identify that intra-sample spurious correlations among video-language components can be alleviated if the model captures the decomposed structures of video and lan-guage data. In this light, we propose a novel framework, namely WINNER, for hierarchical video-text understand-ing. WINNER first builds the language decomposition tree in a bottom-up manner, upon which the structural attention mechanism and top-down feature backtracking jointly build a multi-modal decomposition tree, permitting a hierarchi-cal understanding of unstructured videos. The multi-modal decomposition tree serves as the basis for multi-hierarchy language-tube matching. A hierarchical contrastive learn-ing objective is proposed to learn the multi-hierarchy cor-respondence and distinguishment with intra-sample and inter-sample video-text decomposition structures, achieving video-language decomposition structure alignment. Exten-sive experiments demonstrate the rationality of our design and its effectiveness beyond state-of-the-art weakly super-vised methods, even some supervised methods. 