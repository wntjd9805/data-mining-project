Temporal action detection aims to predict the time inter-vals and the classes of action instances in the video. Despite the promising performance, existing two-stream models ex-hibit slow inference speed due to their reliance on compu-tationally expensive optical ﬂow.In this paper, we intro-duce a decomposed cross-modal distillation framework to build a strong RGB-based detector by transferring knowl-edge of the motion modality. Speciﬁcally, instead of direct distillation, we propose to separately learn RGB and motion representations, which are in turn combined to perform ac-tion localization. The dual-branch design and the asymmet-ric training objectives enable effective motion knowledge transfer while preserving RGB information intact. In addi-tion, we introduce a local attentive fusion to better exploit the multimodal complementarity. It is designed to preserve the local discriminability of the features that is important for action localization. Extensive experiments on the bench-marks verify the effectiveness of the proposed method in en-hancing RGB-based action detectors. Notably, our frame-work is agnostic to backbones and detection heads, bring-ing consistent gains across different model combinations. 