Although the impressive performance in visual ground-ing, the prevailing approaches usually exploit the visual backbone in a passive way, i.e., the visual backbone ex-tracts features with fixed weights without expression-related hints. The passive perception may lead to mismatches (e.g., redundant and missing), limiting further performance im-provement.Ideally, the visual backbone should actively extract visual features since the expressions already pro-vide the blueprint of desired visual features. The active perception can take expressions as priors to extract rel-evant visual features, which can effectively alleviate theInspired by this, we propose an active per-mismatches. ception Visual Grounding framework based on LanguageAdaptive Weights, called VG-LAW. The visual backbone serves as an expression-specific feature extractor through dynamic weights generated for various expressions. Ben-efiting from the specific and relevant visual features ex-tracted from the language-aware visual backbone, VG-LAW does not require additional modules for cross-modal inter-action. Along with a neat multi-task head, VG-LAW can be competent in referring expression comprehension and seg-mentation jointly. Extensive experiments on four represen-tative datasets, i.e., RefCOCO, RefCOCO+, RefCOCOg, and ReferItGame, validate the effectiveness of the proposed framework and demonstrate state-of-the-art performance. 