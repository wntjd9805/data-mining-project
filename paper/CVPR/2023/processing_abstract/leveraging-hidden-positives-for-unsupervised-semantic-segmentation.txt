Dramatic demand for manpower to label pixel-level an-notations triggered the advent of unsupervised semantic segmentation. Although the recent work employing the vi-sion transformer (ViT) backbone shows exceptional perfor-mance, there is still a lack of consideration for task-speciﬁc training guidance and local semantic consistency. To tackle these issues, we leverage contrastive learning by excavating hidden positives to learn rich semantic relationships and ensure semantic consistency in local regions. Speciﬁcally, we ﬁrst discover two types of global hidden positives, task-agnostic and task-speciﬁc ones for each anchor based on the feature similarities deﬁned by a ﬁxed pre-trained back-bone and a segmentation head-in-training, respectively. A gradual increase in the contribution of the latter induces the model to capture task-speciﬁc semantic features. In addi-tion, we introduce a gradient propagation strategy to learn semantic consistency between adjacent patches, under the inherent premise that nearby patches are highly likely to possess the same semantics. Speciﬁcally, we add the loss propagating to local hidden positives, semantically similar nearby patches, in proportion to the predeﬁned similarity scores. With these training schemes, our proposed method achieves new state-of-the-art (SOTA) results in COCO-stuff,Cityscapes, and Potsdam-3 datasets. Our code is available at: https://github.com/hynnsk/HP. 