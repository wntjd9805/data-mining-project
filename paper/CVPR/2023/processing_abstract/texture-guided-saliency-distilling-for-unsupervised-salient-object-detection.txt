Deep Learning-based Unsupervised Salient Object De-tection (USOD) mainly relies on the noisy saliency pseudo labels that have been generated from traditional handcraft methods or pre-trained networks. To cope with the noisy la-bels problem, a class of methods focus on only easy samples with reliable labels but ignore valuable knowledge in hard samples. In this paper, we propose a novel USOD method to mine rich and accurate saliency knowledge from both easy and hard samples. First, we propose a Confidence-awareSaliency Distilling (CSD) strategy that scores samples con-ditioned on samplesâ€™ confidences, which guides the model to distill saliency knowledge from easy samples to hard sam-ples progressively. Second, we propose a Boundary-awareTexture Matching (BTM) strategy to refine the boundaries of noisy labels by matching the textures around the pre-dicted boundaries. Extensive experiments on RGB, RGB-D, RGB-T, and video SOD benchmarks prove that our method achieves state-of-the-art USOD performance. Code is available at www.github.com/moothes/A2S-v2. 