Is she looking at me? When did I   knead dough? What am I doing? Different video understanding tasks are typically treated in isolation, and even with distinct types of curated data (e.g., classifying sports in one dataset, tracking animals in another). However, in wearable cameras, the immer-sive egocentric perspective of a person engaging with the world around them presents an interconnected web of video understanding tasks—hand-object manipulations, naviga-tion in the space, or human-human interactions—that un-fold continuously, driven by the person’s goals. We argue that this calls for a much more uniﬁed approach. We pro-pose EgoTask Translation (EgoT2), which takes a collec-tion of models optimized on separate tasks and learns to translate their outputs for improved performance on any or all of them at once. Unlike traditional transfer or multi-task learning, EgoT2’s “ﬂipped design” entails separate task-speciﬁc backbones and a task translator shared across all tasks, which captures synergies between even heteroge-neous tasks and mitigates task competition. Demonstrat-ing our model on a wide array of video tasks from Ego4D, we show its advantages over existing transfer paradigms and achieve top-ranked results on four of the Ego4D 2022 benchmark challenges.1 