Given an untrimmed video, temporal sentence ground-ing (TSG) aims to locate a target moment semantically ac-cording to a sentence query. Although previous respectable works have made decent success, they only focus on high-level visual features extracted from the consecutive de-coded frames and fail to handle the compressed videos for query modelling, suffering from insufﬁcient representation capability and signiﬁcant computational complexity during training and testing.In this paper, we pose a new set-ting, compressed-domain TSG, which directly utilizes com-pressed videos rather than fully-decompressed frames as the visual input. To handle the raw video bit-stream in-put, we propose a novel Three-branch Compressed-domainSpatial-temporal Fusion (TCSF) framework, which extracts and aggregates three kinds of low-level visual features (I-frame, motion vector and residual features) for effective and efﬁcient grounding. Particularly, instead of encoding the whole decoded frames like previous works, we capture the appearance representation by only learning the I-frame feature to reduce delay or latency. Besides, we explore the motion information not only by learning the motion vector feature, but also by exploring the relations of neighboring frames via the residual feature. In this way, a three-branch spatial-temporal attention layer with an adaptive motion-appearance fusion module is further designed to extract and aggregate both appearance and motion information for the ﬁnal grounding. Experiments on three challenging datasets shows that our TCSF achieves better performance than other state-of-the-art methods with lower complexity. 