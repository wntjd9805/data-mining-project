With the success of the 3D deep learning models, var-ious perception technologies for autonomous driving have been developed in the LiDAR domain. While these mod-els perform well in the trained source domain, they strug-gle in unseen domains with a domain gap.In this pa-per, we propose a single domain generalization method forLiDAR semantic segmentation (DGLSS) that aims to en-sure good performance not only in the source domain but also in the unseen domain by learning only on the source domain. We mainly focus on generalizing from a dense source domain and target the domain shift from differentLiDAR sensor configurations and scene distributions. To this end, we augment the domain to simulate the unseen do-mains by randomly subsampling the LiDAR scans. With the augmented domain, we introduce two constraints for gen-eralizable representation learning: sparsity invariant fea-ture consistency (SIFC) and semantic correlation consis-tency (SCC). The SIFC aligns sparse internal features of the source domain with the augmented domain based on the feature affinity. For SCC, we constrain the correlation be-tween class prototypes to be similar for every LiDAR scan.We also establish a standardized training and evaluation setting for DGLSS. With the proposed evaluation setting, our method showed improved performance in the unseen domains compared to other baselines. Even without ac-cess to the target domain, our method performed better than the domain adaptation method. The code is available at https://github.com/gzgzys9887/DGLSS. 