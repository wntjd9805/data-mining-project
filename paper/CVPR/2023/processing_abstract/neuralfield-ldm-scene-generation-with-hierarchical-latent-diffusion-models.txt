Automatically generating high-quality real world 3D scenes is of enormous interest for applications such as vir-tual reality and robotics simulation. Towards this goal, we introduce NeuralField-LDM, a generative model capable of synthesizing complex 3D environments. We leverage LatentDiffusion Models that have been successfully utilized for ef-ficient high-quality 2D content creation. We first train a scene auto-encoder to express a set of image and pose pairs as a neural field, represented as density and feature voxel grids that can be projected to produce novel views of the scene. To further compress this representation, we train a latent-autoencoder that maps the voxel grids to a set of la-tent representations. A hierarchical diffusion model is then fit to the latents to complete the scene generation pipeline.We achieve a substantial improvement over existing state-of-the-art scene generation models. Additionally, we show how NeuralField-LDM can be used for a variety of 3D con-tent creation applications, including conditional scene gen-eration, scene inpainting and scene style manipulation. 