Continuous convolution has recently gained prominence due to its ability to handle irregularly sampled data and model long-term dependency. Also, the promising exper-imental results of using large convolutional kernels have catalyzed the development of continuous convolution since they can construct large kernels very efficiently. Lever-aging neural networks, more specifically multilayer per-ceptrons (MLPs), is by far the most prevalent approach to implementing continuous convolution. However, there are a few drawbacks, such as high computational costs, complex hyperparameter tuning, and limited descriptive power of filters. This paper suggests an alternative ap-proach to building a continuous convolution without neu-ral networks, resulting in more computationally efficient and improved performance. We present self-moving point representations where weight parameters freely move, and interpolation schemes are used to implement continuous functions. When applied to construct convolutional ker-nels, the experimental results have shown improved per-formance with drop-in replacement in the existing frame-works. Due to its lightweight structure, we are first to demonstrate the effectiveness of continuous convolution in a large-scale setting, e.g., ImageNet, presenting the im-provements over the prior arts. Our code is available on https://github.com/sangnekim/SMPConv 