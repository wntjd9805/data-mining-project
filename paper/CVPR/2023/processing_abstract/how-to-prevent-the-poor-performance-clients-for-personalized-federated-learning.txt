Personalized federated learning (pFL) collaboratively trains personalized models, which provides a customized model solution for individual clients in the presence of het-erogeneous distributed local data. Although many recent studies have applied various algorithms to enhance per-sonalization in pFL, they mainly focus on improving the performance from averaging or top perspective. How-ever, part of the clients may fall into poor performance and are not clearly discussed. Therefore, how to prevent these poor clients should be considered critically.Intu-itively, these poor clients may come from biased univer-sal information shared with others. To address this issue, we propose a novel pFL strategy, called Personalize Lo-cally, Generalize Universally (PLGU). PLGU generalizes the fine-grained universal information and moderates its bi-ased performance by designing a Layer-Wised SharpnessAware Minimization (LWSAM) algorithm while keeping the personalization local. Specifically, we embed our proposedPLGU strategy into two pFL schemes concluded in this pa-per: with/without a global model, and present the training procedures in detail. Through in-depth study, we show that the proposed PLGU strategy achieves competitive general-ization bounds on both considered pFL schemes. Our exten-sive experimental results show that all the proposed PLGU based-algorithms achieve state-of-the-art performance. 