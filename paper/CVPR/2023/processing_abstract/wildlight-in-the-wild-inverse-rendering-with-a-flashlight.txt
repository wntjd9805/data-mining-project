trial renderers. Our source code and data are released to https://github.com/za-cheng/WildLight.This paper proposes a practical photometric solution for the challenging problem of in-the-wild inverse render-ing under unknown ambient lighting. Our system recov-ers scene geometry and reﬂectance using only multi-view images captured by a smartphone. The key idea is to ex-ploit smartphone’s built-in ﬂashlight as a minimally con-trolled light source, and decompose image intensities into two photometric components – a static appearance corre-sponds to ambient ﬂux, plus a dynamic reﬂection induced by the moving ﬂashlight. Our method does not requireﬂash/non-ﬂash images to be captured in pairs. Building on the success of neural light ﬁelds, we use an off-the-shelf method to capture the ambient reﬂections, while theﬂashlight component enables physically accurate photomet-ric constraints to decouple reﬂectance and illumination.Compared to existing inverse rendering methods, our setup is applicable to non-darkroom environments yet sidesteps the inherent difﬁculties of explicit solving ambient reﬂec-tions. We demonstrate by extensive experiments that our method is easy to implement, casual to set up, and con-sistently outperforms existing in-the-wild inverse rendering techniques. Finally, our neural reconstruction can be eas-ily exported to PBR textured triangle mesh ready for indus-