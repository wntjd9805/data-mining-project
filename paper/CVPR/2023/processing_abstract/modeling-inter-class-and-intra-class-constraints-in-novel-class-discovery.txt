Novel class discovery (NCD) aims at learning a model that transfers the common knowledge from a class-disjoint labelled dataset to another unlabelled dataset and discov-ers new classes (clusters) within it. Many methods, as well as elaborate training pipelines and appropriate objectives, have been proposed and considerably boosted performance on NCD tasks. Despite all this, we find that the existing methods do not sufficiently take advantage of the essence of the NCD setting. To this end, in this paper, we pro-pose to model both inter-class and intra-class constraints in NCD based on the symmetric Kullback-Leibler diver-gence (sKLD). Specifically, we propose an inter-class sKLD constraint to effectively exploit the disjoint relationship be-tween labelled and unlabelled classes, enforcing the sep-arability for different classes in the embedding space. In addition, we present an intra-class sKLD constraint to ex-plicitly constrain the intra-relationship between a sample and its augmentations and ensure the stability of the train-ing process at the same time. We conduct extensive exper-iments on the popular CIFAR10, CIFAR100 and ImageNet benchmarks and successfully demonstrate that our method can establish a new state of the art and can achieve signif-icant performance improvements, e.g., 3.5%/3.7% cluster-ing accuracy improvements on CIFAR100-50 dataset split under the task-aware/-agnostic evaluation protocol, over previous state-of-the-art methods. Code is available at https://github.com/FanZhichen/NCD-IIC. 