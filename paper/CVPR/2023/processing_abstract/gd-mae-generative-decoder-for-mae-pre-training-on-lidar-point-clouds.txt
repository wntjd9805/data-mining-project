Despite the tremendous progress of Masked Autoen-coders (MAE) in developing vision tasks such as image and video, exploring MAE in large-scale 3D point clouds re-mains challenging due to the inherent irregularity. In con-trast to previous 3D MAE frameworks, which either design a complex decoder to infer masked information from main-tained regions or adopt sophisticated masking strategies, we instead propose a much simpler paradigm. The core idea is to apply a Generative Decoder for MAE (GD-MAE) to automatically merges the surrounding context to restore the masked geometric knowledge in a hierarchical fusion manner. In doing so, our approach is free from introducing the heuristic design of decoders and enjoys the flexibility of exploring various masking strategies. The correspond-ing part costs less than 12% latency compared with con-ventional methods, while achieving better performance. We demonstrate the efficacy of the proposed method on several large-scale benchmarks: Waymo, KITTI, and ONCE. Con-sistent improvement on downstream detection tasks illus-trates strong robustness and generalization capability. Not only our method reveals state-of-the-art results, but remark-ably, we achieve comparable accuracy even with 20% of the labeled data on the Waymo dataset. Code will be released. 