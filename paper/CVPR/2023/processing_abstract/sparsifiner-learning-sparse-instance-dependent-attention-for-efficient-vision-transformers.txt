Vision Transformers (ViT) have shown competitive advan-tages in terms of performance compared to convolutional neural networks (CNNs), though they often come with high computational costs. To this end, previous methods explore different attention patterns by limiting a fixed number of spatially nearby tokens to accelerate the ViT’s multi-head self-attention (MHSA) operations. However, such struc-tured attention patterns limit the token-to-token connections to their spatial relevance, which disregards learned seman-tic connections from a full attention mask.In this work, we propose an approach to learn instance-dependent at-tention patterns, by devising a lightweight connectivity pre-dictor module that estimates the connectivity score of each pair of tokens. Intuitively, two tokens have high connectiv-ity scores if the features are considered relevant either spa-tially or semantically. As each token only attends to a small number of other tokens, the binarized connectivity masks are often very sparse by nature and therefore provide the opportunity to reduce network FLOPs via sparse compu-tations. Equipped with the learned unstructured attention pattern, sparse attention ViT (Sparsifiner) produces a supe-rior Pareto frontier between FLOPs and top-1 accuracy onImageNet compared to token sparsity. Our method reduces 48% ∼ 69% FLOPs of MHSA while the accuracy drop is within 0.4%. We also show that combining attention and token sparsity reduces ViT FLOPs by over 60%. 