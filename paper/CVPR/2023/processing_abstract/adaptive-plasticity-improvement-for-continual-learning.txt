Many works have tried to solve the catastrophic forget-ting (CF) problem in continual learning (lifelong learning).However, pursuing non-forgetting on old tasks may dam-age the model’s plasticity for new tasks. Although some methods have been proposed to achieve stability-plasticity trade-off, no methods have considered evaluating a model’s plasticity and improving plasticity adaptively for a new task.In this work, we propose a new method, called adaptive plasticity improvement (API), for continual learning. Be-sides the ability to overcome CF on old tasks, API also tries to evaluate the model’s plasticity and then adaptively im-prove the model’s plasticity for learning a new task if nec-essary. Experiments on several real datasets show that API can outperform other state-of-the-art baselines in terms of both accuracy and memory usage. 