In theory, vector quantization (VQ) is always better than scalar quantization (SQ) in terms of rate-distortion (R-D) performance [33]. Recent state-of-the-art methods for neural image compression are mainly based on nonlinear transform coding (NTC) with uniform scalar quantization, overlooking the benefits of VQ due to its exponentially in-creased complexity.In this paper, we first investigate on some toy sources, demonstrating that even if modern neu-ral networks considerably enhance the compression per-formance of SQ with nonlinear transform, there is still an insurmountable chasm between SQ and VQ. Therefore, re-volving around VQ, we propose a novel framework for neu-ral image compression named Nonlinear Vector TransformCoding (NVTC). NVTC solves the critical complexity is-sue of VQ through (1) a multi-stage quantization strategy and (2) nonlinear vector transforms. In addition, we apply entropy-constrained VQ in latent space to adaptively deter-mine the quantization boundaries for joint rate-distortion optimization, which improves the performance both theo-retically and experimentally. Compared to previous NTC approaches, NVTC demonstrates superior rate-distortion performance, faster decoding speed, and smaller model size. Our code is available at https://github.com/USTC-IMCL/NVTC. 