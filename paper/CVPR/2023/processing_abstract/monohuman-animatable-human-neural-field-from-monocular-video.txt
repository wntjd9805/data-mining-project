Animating virtual avatars with free-view control is cru-cial for various applications like virtual reality and digi-tal entertainment. Previous studies have attempted to uti-lize the representation power of the neural radiance field (NeRF) to reconstruct the human body from monocular videos. Recent works propose to graft a deformation net-work into the NeRF to further model the dynamics of the hu-man neural field for animating vivid human motions. How-ever, such pipelines either rely on pose-dependent repre-sentations or fall short of motion coherency due to frame-independent optimization, making it difficult to generalize to unseen pose sequences realistically.In this paper, we propose a novel framework MonoHuman, which robustly renders view-consistent and high-fidelity avatars under ar-bitrary novel poses. Our key insight is to model the de-formation field with bi-directional constraints and explic-itly leverage the off-the-peg keyframe information to reason the feature correlations for coherent results. Specifically, we first propose a Shared Bidirectional Deformation mod-ule, which creates a pose-independent generalizable defor-mation field by disentangling backward and forward defor-mation correspondences into shared skeletal motion weight and separate non-rigid motions. Then, we devise a ForwardCorrespondence Search module, which queries the corre-spondence feature of keyframes to guide the rendering net-work. The rendered results are thus multi-view consistent with high fidelity, even under challenging novel pose set-tings. Extensive experiments demonstrate the superiority of our proposed MonoHuman over state-of-the-art methods. 