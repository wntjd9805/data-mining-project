Federated Domain Generalization (FedDG) attempts to learn a global model in a privacy-preserving manner that generalizes well to new clients possibly with domain shift.Recent exploration mainly focuses on designing an unbi-ased training strategy within each individual domain. How-ever, without the support of multi-domain data jointly in the mini-batch training, almost all methods cannot guar-antee the generalization under domain shift. To overcome this problem, we propose a novel global objective incorpo-rating a new variance reduction regularizer to encourage fairness. A novel FL-friendly method named Generaliza-tion Adjustment (GA) is proposed to optimize the above ob-jective by dynamically calibrating the aggregation weights.The theoretical analysis of GA demonstrates the possibility to achieve a tighter generalization bound with an explicit re-weighted aggregation, substituting the implicit multi-domain data sharing that is only applicable to the con-ventional DG settings. Besides, the proposed algorithm is generic and can be combined with any local client training-based methods. Extensive experiments on several bench-mark datasets have shown the effectiveness of the proposed method, with consistent improvements over several FedDG algorithms when used in combination. The source code is released at https://github.com/MediaBrain-SJTU/FedDG-GA 