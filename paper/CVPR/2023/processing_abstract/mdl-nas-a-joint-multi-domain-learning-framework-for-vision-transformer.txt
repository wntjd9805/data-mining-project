In this work, we introduce MDL-NAS, a unified frame-work that integrates multiple vision tasks into a manage-able supernet and optimizes these tasks collectively un-der diverse dataset domains. MDL-NAS is storage-efficient since multiple models with a majority of shared parame-ters can be deposited into a single one. Technically, MDL-NAS constructs a coarse-to-fine search space, where the coarse search space offers various optimal architectures for different tasks while the fine search space provides fine-grained parameter sharing to tackle the inherent obstacles of multi-domain learning. In the fine search space, we sug-gest two parameter sharing policies, i.e., sequential shar-ing policy and mask sharing policy. Compared with pre-vious works, such two sharing policies allow for the par-tial sharing and non-sharing of parameters at each layer of the network, hence attaining real fine-grained parameter sharing. Finally, we present a joint-subnet search algorithm that finds the optimal architecture and sharing parameters for each task within total resource constraints, challeng-ing the traditional practice that downstream vision tasks are typically equipped with backbone networks designed for image classification. Experimentally, we demonstrate thatMDL-NAS families fitted with non-hierarchical or hierar-chical transformers deliver competitive performance for all tasks compared with state-of-the-art methods while main-taining efficient storage deployment and computation. We also demonstrate that MDL-NAS allows incremental learn-ing and evades catastrophic forgetting when generalizing to a new task. 