Integrating multiple sensors and addressing diverse tasks in an end-to-end algorithm are challenging yet crit-ical topics for autonomous driving. To this end, we in-troduce BEVGuide, a novel Birdâ€™s Eye-View (BEV) repre-sentation learning framework, representing the first attempt to unify a wide range of sensors under direct BEV guid-ance in an end-to-end fashion. Our architecture accepts in-put from a diverse sensor pool, including but not limited to Camera, Lidar and Radar sensors, and extracts BEV feature embeddings using a versatile and general trans-former backbone. We design a BEV-guided multi-sensor attention block to take queries from BEV embeddings and learn the BEV representation from sensor-specific features.BEVGuide is efficient due to its lightweight backbone de-sign and highly flexible as it supports almost any input sen-sor configurations. Extensive experiments demonstrate that our framework achieves exceptional performance in BEV perception tasks with a diverse sensor set. Project page is at https://yunzeman.github.io/BEVGuide. 