Vision Transformers (ViTs) emerge to achieve impres-sive performance on many data-abundant computer vision tasks by capturing long-range dependencies among local features. However, under few-shot learning (FSL) settings on small datasets with only a few labeled data, ViT tends to overﬁt and suffers from severe performance degradation due to its absence of CNN-alike inductive bias. Previous works in FSL avoid such problem either through the help of self-supervised auxiliary losses, or through the dextile uses of label information under supervised settings. But the gap between self-supervised and supervised few-shotTransformers is still unﬁlled. Inspired by recent advances in self-supervised knowledge distillation and masked image modeling (MIM), we propose a novel Supervised MaskedKnowledge Distillation model (SMKD) for few-shot Trans-formers which incorporates label information into self-distillation frameworks. Compared with previous self-supervised methods, we allow intra-class knowledge dis-tillation on both class and patch tokens, and introduce the challenging task of masked patch tokens reconstruc-tion across intra-class images. Experimental results on four few-shot classiﬁcation benchmark datasets show that our method with simple design outperforms previous meth-ods by a large margin and achieves a new start-of-the-art.Detailed ablation studies conﬁrm the effectiveness of each component of our model. Code for this paper is available here: https://github.com/HL-hanlin/SMKD. 