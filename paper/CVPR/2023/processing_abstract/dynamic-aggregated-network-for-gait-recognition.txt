Gait recognition is beneficial for a variety of applica-tions, including video surveillance, crime scene investi-gation, and social security, to mention a few. However, gait recognition often suffers from multiple exterior fac-tors in real scenes, such as carrying conditions, wear-ing overcoats, and diverse viewing angles. Recently, var-ious deep learning-based gait recognition methods have achieved promising results, but they tend to extract one of the salient features using fixed-weighted convolutional net-works, do not well consider the relationship within gait fea-tures in key regions, and ignore the aggregation of complete motion patterns. In this paper, we propose a new perspec-tive that actual gait features include global motion patterns in multiple key regions, and each global motion pattern is composed of a series of local motion patterns. To this end, we propose a Dynamic Aggregation Network (DANet) to learn more discriminative gait features. Specifically, we create a dynamic attention mechanism between the features of neighboring pixels that not only adaptively focuses on key regions but also generates more expressive local motion patterns. In addition, we develop a self-attention mecha-nism to select representative local motion patterns and fur-ther learn robust global motion patterns. Extensive exper-iments on three popular public gait datasets, i.e., CASIA-B, OUMVLP, and Gait3D, demonstrate that the proposed method can provide substantial improvements over the cur-rent state-of-the-art methods.1 