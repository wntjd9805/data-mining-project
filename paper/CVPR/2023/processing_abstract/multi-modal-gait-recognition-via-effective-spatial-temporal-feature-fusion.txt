Gait recognition is a biometric technology that iden-tifies people by their walking patterns. The silhouettes-based method and the skeletons-based method are the two most popular approaches. However, the silhouette data are easily affected by clothing occlusion, and the skeleton data lack body shape information. To obtain a more ro-bust and comprehensive gait representation for recognition, we propose a transformer-based gait recognition frame-work called MMGaitFormer, which effectively fuses and ag-gregates the spatial-temporal information from the skele-tons and silhouettes. Specifically, a Spatial Fusion Mod-ule (SFM) and a Temporal Fusion Module (TFM) are pro-posed for effective spatial-level and temporal-level feature fusion, respectively. The SFM performs fine-grained body parts spatial fusion and guides the alignment of each part of the silhouette and each joint of the skeleton through the at-tention mechanism. The TFM performs temporal modeling through Cycle Position Embedding (CPE) and fuses tempo-ral information of two modalities. Experiments demonstrate that our MMGaitFormer achieves state-of-the-art perfor-mance on popular gait datasets. For the most challenging“CL” (i.e., walking in different clothes) condition in CASIA-B, our method achieves a rank-1 accuracy of 94.8%, which outperforms the state-of-the-art single-modal methods by a large margin. 