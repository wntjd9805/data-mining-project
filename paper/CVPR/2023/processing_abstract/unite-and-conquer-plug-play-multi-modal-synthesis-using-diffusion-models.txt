Generating photos satisfying multiple constraints finds broad utility in the content creation industry. A key hur-dle to accomplishing this task is the need for paired data consisting of all modalities (i.e., constraints) and their cor-responding output. Moreover, existing methods need re-training using paired data across all modalities to intro-duce a new condition. This paper proposes a solution to this problem based on denoising diffusion probabilistic models (DDPMs). Our motivation for choosing diffusion models over other generative models comes from the flexible in-ternal structure of diffusion models. Since each sampling step in the DDPM follows a Gaussian distribution, we show that there exists a closed-form solution for generating an image given various constraints. Our method can unite multiple diffusion models trained on multiple sub-tasks and conquer the combined task through our proposed samplingstrategy. We also introduce a novel reliability parame-ter that allows using different off-the-shelf diffusion mod-els trained across various datasets during sampling time alone to guide it to the desired outcome satisfying multi-ple constraints. We perform experiments on various stan-dard multimodal tasks to demonstrate the effectiveness of our approach. More details can be found at: https://nithin-gk.github.io/projectpages/Multidiff 