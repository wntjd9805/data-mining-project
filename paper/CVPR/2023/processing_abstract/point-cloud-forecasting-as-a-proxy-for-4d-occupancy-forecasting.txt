Predicting how the world can evolve in the future is cru-cial for motion planning in autonomous systems. Classi-cal methods are limited because they rely on costly human annotations in the form of semantic class labels, bounding boxes, and tracks or HD maps of cities to plan their mo-tion — and thus are difﬁcult to scale to large unlabeled datasets. One promising self-supervised task is 3D point cloud forecasting [11, 18–20] from unannotated LiDAR se-quences. We show that this task requires algorithms to im-plicitly capture (1) sensor extrinsics (i.e., the egomotion of the autonomous vehicle), (2) sensor intrinsics (i.e., the sampling pattern speciﬁc to the particular LiDAR sensor), and (3) the shape and motion of other objects in the scene.But autonomous systems should make predictions about the world and not their sensors! To this end, we factor out (1) and (2) by recasting the task as one of spacetime (4D) oc-∗Equal contribution cupancy forecasting. But because it is expensive to obtain ground-truth 4D occupancy, we “render” point cloud data from 4D occupancy predictions given sensor extrinsics and intrinsics, allowing one to train and test occupancy algo-rithms with unannotated LiDAR sequences. This also al-lows one to evaluate and compare point cloud forecasting algorithms across diverse datasets, sensors, and vehicles. 