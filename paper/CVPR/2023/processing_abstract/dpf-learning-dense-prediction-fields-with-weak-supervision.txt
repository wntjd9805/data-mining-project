Nowadays, many visual scene understanding problems are addressed by dense prediction networks. But pixel-wise dense annotations are very expensive (e.g., for scene pars-ing) or impossible (e.g., for intrinsic image decomposition), motivating us to leverage cheap point-level weak supervi-sion. However, existing pointly-supervised methods still use the same architecture designed for full supervision. In stark contrast to them, we propose a new paradigm that makes predictions for point coordinate queries, as inspired by the recent success of implicit representations, like distance or radiance fields. As such, the method is named as dense pre-diction fields (DPFs). DPFs generate expressive interme-diate features for continuous sub-pixel locations, thus al-lowing outputs of an arbitrary resolution. DPFs are nat-urally compatible with point-level supervision. We show-case the effectiveness of DPFs using two substantially dif-ferent tasks: high-level semantic parsing and low-level in-trinsic image decomposition. In these two cases, supervi-sion comes in the form of single-point semantic category and two-point relative reflectance, respectively. As bench-marked by three large-scale public datasets PASCALCon-text, ADE20K and IIW, DPFs set new state-of-the-art per-formance on all of them with significant margins. Code can be accessed at https://github.com/cxx226/DPF. 