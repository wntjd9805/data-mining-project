Recent deep learning methods have achieved promising results in image shadow removal. However, their restored images still suffer from unsatisfactory boundary artifacts, due to the lack of degradation prior embedding and the de-ficiency in modeling capacity. Our work addresses these issues by proposing a unified diffusion framework that in-tegrates both the image and degradation priors for highly effective shadow removal.In detail, we first propose a shadow degradation model, which inspires us to build a novel unrolling diffusion model, dubbed ShandowDiffusion.It remarkably improves the model’s capacity in shadow re-moval via progressively refining the desired output with both degradation prior and diffusive generative prior, which by nature can serve as a new strong baseline for image restoration. Furthermore, ShadowDiffusion progressively refines the estimated shadow mask as an auxiliary task of the diffusion generator, which leads to more accurate and robust shadow-free image generation. We conduct exten-sive experiments on three popular public datasets, includingISTD, ISTD+, and SRD, to validate our method’s effective-ness. Compared to the state-of-the-art methods, our model achieves a significant improvement in terms of PSNR, in-creasing from 31.69dB to 34.73dB over SRD dataset.1 