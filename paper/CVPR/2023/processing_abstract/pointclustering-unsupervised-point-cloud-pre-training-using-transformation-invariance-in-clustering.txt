Feature invariance under different data transformations, i.e., transformation invariance, can be regarded as a type of self-supervision for representation learning. In this pa-per, we present PointClustering, a new unsupervised rep-resentation learning scheme that leverages transformation invariance for point cloud pre-training. PointClustering formulates the pretext task as deep clustering and employs transformation invariance as an inductive bias, following the philosophy that common point cloud transformation will not change the geometric properties and semantics. Techni-cally, PointClustering iteratively optimizes the feature clus-ters and backbone, and delves into the transformation in-variance as learning regularization from two perspectives: point level and instance level. Point-level invariance learn-ing maintains local geometric properties through gathering point features of one instance across transformations, while instance-level invariance learning further measures cluster-s over the entire dataset to explore semantics of instances.Our PointClustering is architecture-agnostic and readily applicable to MLP-based, CNN-based and Transformer-based backbones. We empirically demonstrate that the models pre-learnt on the ScanNet dataset by PointClus-tering provide superior performances on six benchmark-s, across downstream tasks of classification and segmen-tation. More remarkably, PointClustering achieves an ac-curacy of 94.5% on ModelNet40 with Transformer back-bone. Source code is available at https://github. com/FuchenUSTC/PointClustering. 