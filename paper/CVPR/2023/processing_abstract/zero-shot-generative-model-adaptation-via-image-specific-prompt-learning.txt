Recently, CLIP-guided image synthesis has shown ap-pealing performance on adapting a pre-trained source-domain generator to an unseen target domain. It does not require any target-domain samples but only the textual do-main labels. The training is highly efficient, e.g., a few minutes. However, existing methods still have some lim-itations in the quality of generated images and may suf-fer from the mode collapse issue. A key reason is that a fixed adaptation direction is applied for all cross-domain image pairs, which leads to identical supervision signals.To address this issue, we propose an Image-specific PromptLearning (IPL) method, which learns specific prompt vec-*Equal contribution.â€ Corresponding authors. tors for each source-domain image. This produces a more precise adaptation direction for every cross-domain image pair, endowing the target-domain generator with greatly enhanced flexibility. Qualitative and quantitative evalua-tions on various domains demonstrate that IPL effectively improves the quality and diversity of synthesized images and alleviates the mode collapse. Moreover, IPL is inde-pendent of the structure of the generative model, such as generative adversarial networks or diffusion models. Code is available at https://github.com/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation. 