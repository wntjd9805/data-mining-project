3D dense captioning aims to generate multiple cap-tions localized with their associated object regions. Exist-ing methods follow a sophisticated “detect-then-describe” pipeline equipped with numerous hand-crafted components.However, these hand-crafted components would yield sub-optimal performance given cluttered object spatial andIn this pa-class distributions among different scenes. per, we propose a simple-yet-effective transformer frame-work Vote2Cap-DETR based on recent popular DEtectionTRansformer (DETR). Compared with prior arts, our framework has several appealing advantages: 1) With-out resorting to numerous hand-crafted components, our method is based on a full transformer encoder-decoder ar-chitecture with a learnable vote query driven object de-coder, and a caption decoder that produces the dense cap-tions in a set-prediction manner. 2) In contrast to the two-stage scheme, our method can perform detection and cap-tioning in one-stage. 3) Without bells and whistles, exten-sive experiments on two commonly used datasets, ScanRe-fer and Nr3D, demonstrate that our Vote2Cap-DETR sur-passes current state-of-the-arts by 11.13% and 7.11% inCIDEr@0.5IoU, respectively. Codes will be released soon. 