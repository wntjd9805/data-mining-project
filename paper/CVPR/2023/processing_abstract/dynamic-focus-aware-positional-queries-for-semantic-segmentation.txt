The DETR-like segmentors have underpinned the most recent breakthroughs in semantic segmentation, which end-to-end train a set of queries representing the class proto-types or target segments. Recently, masked attention [8] is proposed to restrict each query to only attend to the fore-ground regions predicted by the preceding decoder block for easier optimization. Although promising, it relies on the learnable parameterized positional queries which tend to encode the dataset statistics, leading to inaccurate local-ization for distinct individual queries. In this paper, we pro-pose a simple yet effective query design for semantic seg-mentation termed Dynamic Focus-aware Positional Queries (DFPQ), which dynamically generates positional queries conditioned on the cross-attention scores from the preced-ing decoder block and the positional encodings for the cor-responding image features, simultaneously. Therefore, ourDFPQ preserves rich localization information for the tar-get segments and provides accurate and fine-grained posi-tional priors. In addition, we propose to efficiently deal with high-resolution cross-attention by only aggregating the con-textual tokens based on the low-resolution cross-attention scores to perform local relation aggregation. Extensive ex-periments on ADE20K and Cityscapes show that with the two modifications on Mask2former, our framework achievesSOTA performance and outperforms Mask2former by clear margins of 1.1%, 1.9%, and 1.1% single-scale mIoU withResNet-50, Swin-T, and Swin-B backbones on the ADE20K validation set, respectively. Source code is available at https://github.com/ziplab/FASeg. 