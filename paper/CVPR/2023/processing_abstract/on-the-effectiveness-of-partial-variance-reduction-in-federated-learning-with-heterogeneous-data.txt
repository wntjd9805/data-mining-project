Data heterogeneity across clients is a key challenge in federated learning. Prior works address this by either aligning client and server models or using control vari-ates to correct client model drift. Although these methods achieve fast convergence in convex or simple non-convex problems, the performance in over-parameterized models such as deep neural networks is lacking.In this paper, we ﬁrst revisit the widely used FedAvg algorithm in a deep neural network to understand how data heterogeneity in-ﬂuences the gradient updates across the neural network layers. We observe that while the feature extraction lay-ers are learned efﬁciently by FedAvg, the substantial diver-sity of the ﬁnal classiﬁcation layers across clients impedes the performance. Motivated by this, we propose to correct model drift by variance reduction only on the ﬁnal layers.We demonstrate that this signiﬁcantly outperforms existing benchmarks at a similar or lower communication cost. We furthermore provide proof for the convergence rate of our algorithm. 