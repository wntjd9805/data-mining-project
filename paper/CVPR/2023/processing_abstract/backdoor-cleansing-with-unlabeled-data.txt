Due to the increasing computational demand of DeepNeural Networks (DNNs), companies and organizations have begun to outsource the training process. However, the externally trained DNNs can potentially be backdoor attacked. It is crucial to defend against such attacks, i.e., to postprocess a suspicious model so that its backdoor be-havior is mitigated while its normal prediction power on clean inputs remain uncompromised. To remove the ab-normal backdoor behavior, existing methods mostly rely on additional labeled clean samples. However, such re-quirement may be unrealistic as the training data are of-ten unavailable to end users. In this paper, we investigate the possibility of circumventing such barrier. We propose a novel defense method that does not require training la-bels. Through a carefully designed layer-wise weight re-initialization and knowledge distillation, our method can effectively cleanse backdoor behaviors of a suspicious net-work with negligible compromise in its normal behavior.In experiments, we show that our method, trained with-out labels, is on-par with state-of-the-art defense methods trained using labels. We also observe promising defense results even on out-of-distribution data. This makes our method very practical. Code is available at: https://github.com/luluppang/BCU . 