In this work, we introduce Vid2Seq, a multi-modal single-stage dense event captioning model pretrained on narrated videos which are readily-available at scale. TheVid2Seq architecture augments a language model with spe-cial time tokens, allowing it to seamlessly predict event boundaries and textual descriptions in the same output se-quence. Such a unified model requires large-scale training data, which is not available in current annotated datasets.We show that it is possible to leverage unlabeled narrated videos for dense video captioning, by reformulating sen-tence boundaries of transcribed speech as pseudo event boundaries, and using the transcribed speech sentences as pseudo event captions. The resulting Vid2Seq model pre-trained on the YT-Temporal-1B dataset improves the state of the art on a variety of dense video captioning bench-marks including YouCook2, ViTT and ActivityNet Captions.Vid2Seq also generalizes well to the tasks of video para-graph captioning and video clip captioning, and to few-shot settings. Our code is publicly available at [1]. 