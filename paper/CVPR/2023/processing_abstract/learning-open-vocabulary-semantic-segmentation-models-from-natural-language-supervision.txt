This paper considers the problem of open-vocabulary se-mantic segmentation (OVS), that aims to segment objects of arbitrary classes beyond a pre-defined, closed-set cat-egories. The main contributions are as follows: First, we propose a transformer-based model for OVS, termed as OVSegmentor, which only exploits web-crawled image-text pairs for pre-training without using any mask annota-tions. OVSegmentor assembles the image pixels into a set of learnable group tokens via a slot-attention based bind-ing module, then aligns the group tokens to correspond-ing caption embeddings. Second, we propose two proxy tasks for training, namely masked entity completion and cross-image mask consistency. The former aims to infer all masked entities in the caption given group tokens, that enables the model to learn fine-grained alignment between visual groups and text entities. The latter enforces consis-tent mask predictions between images that contain shared entities, encouraging the model to learn visual invariance.Third, we construct CC4M dataset for pre-training by fil-tering CC12M with frequently appeared entities, which sig-nificantly improves training efficiency. Fourth, we per-form zero-shot transfer on four benchmark datasets, PAS-CAL VOC, PASCAL Context, COCO Object, and ADE20K.OVSegmentor achieves superior results over state-of-the-art approaches on PASCAL VOC using only 3% data (4M vs 134M) for pre-training. 