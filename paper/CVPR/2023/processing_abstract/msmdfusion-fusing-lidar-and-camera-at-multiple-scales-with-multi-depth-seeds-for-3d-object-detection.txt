Fusing LiDAR and camera information is essential for accurate and reliable 3D object detection in autonomous driving systems. This is challenging due to the difficulty of combining multi-granularity geometric and semantic fea-tures from two drastically different modalities. Recent ap-proaches aim at exploring the semantic densities of cam-era features through lifting points in 2D camera images (re-ferred to as “seeds”) into 3D space, and then incorporate 2D semantics via cross-modal interaction or fusion tech-niques. However, depth information is under-investigated in these approaches when lifting points into 3D space, thus 2D semantics can not be reliably fused with 3D points.Moreover, their multi-modal fusion strategy, which is im-plemented as concatenation or attention, either can not effectively fuse 2D and 3D information or is unable to perform fine-grained interactions in the voxel space. To this end, we propose a novel framework with better uti-lization of the depth information and fine-grained cross-modal interaction between LiDAR and camera, which con-sists of two important components. First, a Multi-DepthUnprojection (MDU) method is used to enhance the depth quality of the lifted points at each interaction level. Sec-ond, a Gated Modality-Aware Convolution (GMA-Conv) block is applied to modulate voxels involved with the cam-era modality in a fine-grained manner and then aggre-gate multi-modal features into a unified space. Together they provide the detection head with more comprehensive features from LiDAR and camera. On the nuScenes test benchmark, our proposed method, abbreviated as MSMD-Fusion, achieves state-of-the-art results on both 3D ob-ject detection and tracking tasks without using test-time-augmentation and ensemble techniques. The code is avail-able at https://github.com/SxJyJay/MSMDFusion.*Equal contribution.†Corresponding authors.Figure 1. Illustration of our MSMDFusion pipeline. The yellow arrows indicate information passing or interaction between the Li-DAR and camera modalities. 