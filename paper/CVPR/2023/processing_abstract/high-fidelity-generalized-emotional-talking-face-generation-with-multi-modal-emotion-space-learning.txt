Recently, emotional talking face generation has received considerable attention. However, existing methods only adopt one-hot coding, image, or audio as emotion condi-tions, thus lacking flexible control in practical applications and failing to handle unseen emotion styles due to limited semantics. They either ignore the one-shot setting or the quality of generated faces. In this paper, we propose a more flexible and generalized framework. Specifically, we supple-ment the emotion style in text prompts and use an AlignedMulti-modal Emotion encoder to embed the text, image, and audio emotion modality into a unified space, which inher-its rich semantic prior from CLIP. Consequently, effective multi-modal emotion space learning helps our method sup-port arbitrary emotion modality during testing and could generalize to unseen emotion styles. Besides, an Emotion-aware Audio-to-3DMM Convertor is proposed to connect the emotion condition and the audio sequence to struc-tural representation. A followed style-based High-fidelityEmotional Face generator is designed to generate arbitrary high-resolution realistic identities. Our texture generator hierarchically learns flow fields and animated faces in a residual manner. Extensive experiments demonstrate the flexibility and generalization of our method in emotion con-trol and the effectiveness of high-quality face synthesis. 