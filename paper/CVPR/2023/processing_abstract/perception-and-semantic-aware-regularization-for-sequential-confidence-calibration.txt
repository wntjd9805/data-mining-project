Deep sequence recognition (DSR) models receive in-creasing attention due to their superior application to var-ious applications. Most DSR models use merely the tar-get sequences as supervision without considering other re-lated sequences, leading to over-confidence in their pre-dictions. The DSR models trained with label smoothing regularize labels by equally and independently smoothing each token, reallocating a small value to other tokens for mitigating overconfidence. However, they do not consider tokens/sequences correlations that may provide more ef-fective information to regularize training and thus lead to sub-optimal performance.In this work, we find to-kens/sequences with high perception and semantic correla-tions with the target ones contain more correlated and effec-tive information and thus facilitate more effective regular-ization. To this end, we propose a Perception and Semantic aware Sequence Regularization framework, which explore perceptively and semantically correlated tokens/sequences as regularization. Specifically, we introduce a semantic context-free recognition and a language model to acquire similar sequences with high perceptive similarities and se-mantic correlation, respectively. Moreover, over-confidence degree varies across samples according to their difficul-ties. Thus, we further design an adaptive calibration in-tensity module to compute a difficulty score for each sam-ples to obtain finer-grained regularization. Extensive ex-periments on canonical sequence recognition tasks, includ-ing scene text and speech recognition, demonstrate that our method sets novel state-of-the-art results. Code is available at https://github.com/husterpzh/PSSR. 