Latent Diffusion Models (LDMs) enable high-quality im-age synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-latent space. Here, we apply the LDM dimensional paradigm to high-resolution video generation, a particu-larly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded im-age sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-videoIn particular, we validate our Video LDM on modeling. real driving videos of resolution 512 × 1024, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pre-trained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an ef-ficient and expressive text-to-video model with resolution up to 1280 × 2048. We show that the temporal layers trained in this way generalize to different fine-tuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation. Project page: https://nv-tlabs.github.io/VideoLDM/*Equal contribution.†Andreas, Robin and Tim did the work during internships at NVIDIA.Figure 2. Temporal Video Fine-Tuning.We turn pre-trained image diffusion mod-els into temporally consistent video gener-ators. Initially, different samples of a batch synthesized by the model are independent.After temporal video fine-tuning, the sam-ples are temporally aligned and form co-herent videos. The stochastic generation process before and after fine-tuning is visu-alised for a diffusion model of a one-dim. toy distribution. For clarity, the figure cor-responds to alignment in pixel space.In practice, we perform alignment in LDM’s latent space and obtain videos after ap-plying LDM’s decoder (see Fig. 3). We also video fine-tune diffusion model up-samplers in pixel or latent space (Sec. 3.4). 