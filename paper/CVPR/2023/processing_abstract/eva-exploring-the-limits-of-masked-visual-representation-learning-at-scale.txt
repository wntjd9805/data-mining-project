We launch EVA, a vision-centric foundation model toExplore the limits of Visual representation at scAle using only publicly accessible data. EVA is a vanilla ViT pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches. Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, in-stance segmentation and semantic segmentation without heavy supervised training. Moreover, we observe quanti-tative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models. For instance, EVA takes a great leap in the challeng-ing large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance onLVIS dataset with over a thousand categories and COCO dataset with only eighty categories. Beyond a pure vision en-coder, EVA can also serve as a vision-centric, multi-modal pivot to connect images and text. We find initializing the vision tower of a giant CLIP from EVA can greatly stabi-lize the training and outperform the training from scratch counterpart with much fewer samples and less compute, pro-viding a new direction for scaling up and accelerating the costly training of multi-modal foundation models. 