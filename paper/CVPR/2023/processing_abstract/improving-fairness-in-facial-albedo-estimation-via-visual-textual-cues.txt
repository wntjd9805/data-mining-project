Recent 3D face reconstruction methods have made sig-nificant advances in geometry prediction, yet further cos-metic improvements are limited by lagged albedo because inferring albedo from appearance is an ill-posed problem.Although some existing methods consider prior knowledge from illumination to improve albedo estimation, they still produce a light-skin bias due to racially biased albedo mod-els and limited light constraints.In this paper, we recon-sider the relationship between albedo and face attributes and propose a ID2Albedo to directly estimate albedo with-out constraining illumination. Our key insight is that in-trinsic semantic attributes such as race, skin color, and age can be used to constrain the albedo map. We first introduce visual-textual cues and design a semantic loss to supervise facial albedo estimation. Specifically, we pre-define text la-bels such as race, skin color, age, and wrinkles. Then, we employ the text-image model (CLIP) to compute the simi-larity between the text and the input image, and assign a* Corresponding authors. pseudo-label to each facial image. We constrain generated albedos in the training phase to have the same attributesIn addition, we train a high-quality, unbi-as the inputs. ased facial albedo generator and utilize the semantic loss to learn the mapping from illumination-robust identity fea-tures to the albedo latent codes. Finally, our ID2Albedo is trained in a self-supervised way and outperforms state-of-the-art albedo estimation methods in terms of accuracy and fidelity. It is worth mentioning that our approach has excel-lent generalizability and fairness, especially on in-the-wild data. 