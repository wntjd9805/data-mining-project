Different from conventional image matting, which either requires user-defined scribbles/trimap to extract a specific foreground object or directly extracts all the foreground ob-jects in the image indiscriminately, we introduce a new task named Referring Image Matting (RIM) in this paper, which aims to extract the meticulous alpha matte of the specific object that best matches the given natural language descrip-tion, thus enabling a more natural and simpler instruction for image matting. First, we establish a large-scale challeng-ing dataset RefMatte by designing a comprehensive image composition and expression generation engine to automat-ically produce high-quality images along with diverse text attributes based on public datasets. RefMatte consists of 230 object categories, 47,500 images, 118,749 expression-region entities, and 474,996 expressions. Additionally, we construct a real-world test set with 100 high-resolution natural im-ages and manually annotate complex phrases to evaluate the out-of-domain generalization abilities of RIM methods.Furthermore, we present a novel baseline method CLIPMat for RIM, including a context-embedded prompt, a text-driven semantic pop-up, and a multi-level details extractor. Exten-sive experiments on RefMatte in both keyword and expres-sion settings validate the superiority of CLIPMat over repre-sentative methods. We hope this work could provide novel insights into image matting and encourage more follow-up studies. The dataset, code and models are available at https://github.com/JizhiziLi/RIM. 