Temporal action localization (TAL) requires long-form reasoning to predict actions of various durations and com-plex content. Given limited GPU memory, training TAL end to end (i.e., from videos to predictions) on long videos is a significant challenge. Most methods can only train on pre-extracted features without optimizing them for the lo-calization problem, consequently limiting localization per-formance. In this work, to extend the potential in TAL net-works, we propose a novel end-to-end method Re2TAL, which rewires pretrained video backbones for reversibleTAL. Re2TAL builds a backbone with reversible modules, where the input can be recovered from the output such that the bulky intermediate activations can be cleared from memory during training.Instead of designing one single type of reversible module, we propose a network rewiring mechanism, to transform any module with a residual con-nection to a reversible module without changing any pa-rameters. This provides two benefits: (1) a large vari-ety of reversible networks are easily obtained from exist-ing and even future model designs, and (2) the reversible models require much less training effort as they reuse the pre-trained parameters of their original non-reversible ver-sions. Re2TAL, only using the RGB modality, reaches 37.01% average mAP on ActivityNet-v1.3, a new state-of-the-art record, and mAP 64.9% at tIoU=0.5 on THUMOS-14, outperforming all other RGB-only methods. Code is available at https://github.com/coolbay/Re2TAL. 