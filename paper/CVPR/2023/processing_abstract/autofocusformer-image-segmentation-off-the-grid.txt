Real world images often have highly imbalanced content density. Some areas are very uniform, e.g., large patches of blue sky, while other areas are scattered with many small objects. Yet, the commonly used successive grid downsam-pling strategy in convolutional deep networks treats all ar-eas equally. Hence, small objects are represented in very few spatial locations, leading to worse results in tasks such as segmentation.Intuitively, retaining more pixels repre-senting small objects during downsampling helps to pre-serve important information. To achieve this, we proposeAutoFocusFormer (AFF), a local-attention transformer im-age recognition backbone, which performs adaptive down-sampling by learning to retain the most important pixels for the task. Since adaptive downsampling generates a set of pixels irregularly distributed on the image plane, we aban-don the classic grid structure. Instead, we develop a novel point-based local attention block, facilitated by a balanced clustering module and a learnable neighborhood merging module, which yields representations for our point-based versions of state-of-the-art segmentation heads. Experi-ments show that our AutoFocusFormer (AFF) improves sig-nificantly over baseline models of similar sizes. 