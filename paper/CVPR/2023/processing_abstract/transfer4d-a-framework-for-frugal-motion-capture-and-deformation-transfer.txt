rig is embedded into the target object for motion retargeting.Animating a virtual character based on a real perfor-mance of an actor is a challenging task that currently re-quires expensive motion capture setups and additional effort by expert animators, rendering it accessible only to large pro-duction houses. The goal of our work is to democratize this task by developing a frugal alternative termed “Transfer4D” that uses only commodity depth sensors and further reduces animators’ effort by automating the rigging and animation transfer process. Our approach can transfer motion from an incomplete, single-view depth video to a semantically similar target mesh, unlike prior works that make a stricter assump-tion on the source to be noise-free and watertight. To handle sparse, incomplete videos from depth video inputs and varia-tions between source and target objects, we propose to use skeletons as an intermediary representation between motion capture and transfer. We propose a novel unsupervised skele-ton extraction pipeline from a single-view depth sequence that incorporates additional geometric information, result-ing in superior performance in motion reconstruction and transfer in comparison to the contemporary methods and making our approach generic. We use non-rigid reconstruc-tion to track motion from the depth sequence, and then we rig the source object using skinning decomposition. Finally, the 