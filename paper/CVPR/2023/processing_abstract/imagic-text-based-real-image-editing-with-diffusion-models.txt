Text-conditioned image editing has recently attracted considerable interest. However, most methods are cur-rently limited to one of the following: speciﬁc editing types (e.g., object overlay, style transfer), synthetically generated images, or requiring multiple input images of a common object.In this paper we demonstrate, for the very ﬁrst time, the ability to apply complex (e.g., non-rigid) text-based semantic edits to a single real image. For exam-ple, we can change the posture and composition of one or multiple objects inside an image, while preserving its original characteristics. Our method can make a stand-ing dog sit down, cause a bird to spread its wings, etc. – each within its single high-resolution user-provided nat-ural image. Contrary to previous work, our proposed method requires only a single input image and a targetIt operates on real images, and text (the desired edit).˚ Equal contribution.The ﬁrst author performed this work as an intern at Google Research.Project page: https://imagic-editing.github.io/. does not require any additional inputs (such as image masks or additional views of the object). Our method, called Imagic, leverages a pre-trained text-to-image diffu-sion model for this task.It produces a text embedding that aligns with both the input image and the target text, while ﬁne-tuning the diffusion model to capture the image-speciﬁc appearance. We demonstrate the quality and versa-tility of Imagic on numerous inputs from various domains, showcasing a plethora of high quality complex semantic image edits, all within a single uniﬁed framework. To better assess performance, we introduce TEdBench, a highly chal-lenging image editing benchmark. We conduct a user study, whose ﬁndings show that human raters prefer Imagic to pre-vious leading editing methods on TEdBench. 