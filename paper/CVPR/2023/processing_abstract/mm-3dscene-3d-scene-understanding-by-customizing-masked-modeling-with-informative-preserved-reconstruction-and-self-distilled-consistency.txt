Masked Modeling (MM) has demonstrated widespread success in various vision challenges, by reconstructing masked visual patches. Yet, applying MM for large-scale 3D scenes remains an open problem due to the data spar-sity and scene complexity. The conventional random mask-ing paradigm used in 2D images often causes a high risk of ambiguity when recovering the masked region of 3D scenes.To this end, we propose a novel informative-preserved re-construction, which explores local statistics to discover and preserve the representative structured points, effectively en-hancing the pretext masking task for 3D scene understand-ing.Integrated with a progressive reconstruction man-ner, our method can concentrate on modeling regional ge-ometry and enjoy less ambiguity for masked reconstruc-tion. Besides, such scenes with progressive masking ra-* Equal contribution.â€  Corresponding authors. tios can also serve to self-distill their intrinsic spatial con-sistency, requiring to learn the consistent representations from unmasked areas. By elegantly combining informative-preserved reconstruction on masked areas and consistency self-distillation from unmasked areas, a unified framework called MM-3DScene is yielded. We conduct comprehensive experiments on a host of downstream tasks. The consistent improvement (e.g., +6.1% mAP@0.5 on object detection and +2.2% mIoU on semantic segmentation) demonstrates the superiority of our approach. 