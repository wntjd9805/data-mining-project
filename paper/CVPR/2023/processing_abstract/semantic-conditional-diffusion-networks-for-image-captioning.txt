Recent advances on text-to-image generation have wit-nessed the rise of diffusion models which act as power-ful generative models. Nevertheless, it is not trivial to exploit such latent variable models to capture the depen-dency among discrete words and meanwhile pursue com-plex visual-language alignment in image captioning. In this paper, we break the deeply rooted conventions in learningTransformer-based encoder-decoder, and propose a new diffusion model based paradigm tailored for image cap-tioning, namely Semantic-Conditional Diffusion Networks (SCD-Net). Technically, for each input image, we first search the semantically relevant sentences via cross-modal retrieval model to convey the comprehensive semantic in-formation. The rich semantics are further regarded as se-mantic prior to trigger the learning of Diffusion Trans-former, which produces the output sentence in a diffusion process. In SCD-Net, multiple Diffusion Transformer struc-tures are stacked to progressively strengthen the output sentence with better visional-language alignment and lin-guistical coherence in a cascaded manner. Furthermore, to stabilize the diffusion process, a new self-critical se-quence training strategy is designed to guide the learn-ing of SCD-Net with the knowledge of a standard autore-gressive Transformer model. Extensive experiments onCOCO dataset demonstrate the promising potential of us-ing diffusion models in the challenging image captioning task. Source code is available at https://github. com/YehLi/xmodaler/tree/master/configs/ image_caption/scdnet. 