This paper deals with the problem of localizing objects in image and video datasets from visual exemplars. In par-ticular, we focus on the challenging problem of egocen-tric visual query localization. We first identify grave im-plicit biases in current query-conditioned model design and visual query datasets. Then, we directly tackle such bi-ases at both frame and object set levels. Concretely, our method solves these issues by expanding limited annota-tions and dynamically dropping object proposals during training. Additionally, we propose a novel transformer-based module that allows for object-proposal set context to be considered while incorporating query information. We name our module Conditioned Contextual Transformer orCocoFormer. Our experiments show the proposed adapta-tions improve egocentric query detection, leading to a better visual query localization system in both 2D and 3D configu-rations. Thus, we can improve frame-level detection perfor-mance from 26.28% to 31.26% in AP, which correspond-ingly improves the VQ2D and VQ3D localization scores by significant margins. Our improved context-aware query object detector ranked first and second respectively in theVQ2D and VQ3D tasks in the 2nd Ego4D challenge. In ad-dition to this, we showcase the relevance of our proposed model in the Few-Shot Detection (FSD) task, where we also achieve SOTA results. Our code is available at https://github.com/facebookresearch/vq2d_cvpr. 