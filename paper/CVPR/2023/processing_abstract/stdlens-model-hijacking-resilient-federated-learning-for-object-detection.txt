ScenarioObject Detection Results APperson APcarFederated Learning (FL) has been gaining popularity as a collaborative learning framework to train deep learning-based object detection models over a distributed popula-tion of clients. Despite its advantages, FL is vulnerable to model hijacking. The attacker can control how the ob-ject detection system should misbehave by implanting Tro-janed gradients using only a small number of compromised clients in the collaborative learning process. This paper introduces STDLens, a principled approach to safeguard-ing FL against such attacks. We first investigate existing mitigation mechanisms and analyze their failures caused by the inherent errors in spatial clustering analysis on gradi-ents. Based on the insights, we introduce a three-tier foren-sic framework to identify and expel Trojaned gradients and reclaim the performance over the course of FL. We con-sider three types of adaptive attacks and demonstrate the ro-bustness of STDLens against advanced adversaries. Exten-sive experiments show that STDLens can protect FL against different model hijacking attacks and outperform existing methods in identifying and removing Trojaned gradients with significantly higher precision and much lower false-positive rates. The source code is available at https://github.com/git-disl/STDLens. 