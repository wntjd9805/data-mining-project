Pretraining visual models on web-scale image-caption datasets has recently emerged as a powerful alternative to traditional pretraining on image classiﬁcation data.Image-caption datasets are more “open-domain”, contain-ing broader scene types and vocabulary words, and re-sult in models that have strong performance in few- and zero-shot recognition tasks. However large-scale classi-ﬁcation datasets can provide ﬁne-grained categories with a balanced label distribution.In this work, we study a pretraining strategy that uses both classiﬁcation and cap-tion datasets to unite their complementary beneﬁts. First, we show that naively unifying the datasets results in sub-optimal performance in downstream zero-shot recognition tasks, as the model is affected by dataset bias: the coverage of image domains and vocabulary words is different in each dataset. We address this problem with novel Preﬁx Condi-tioning, a simple yet effective method that helps disentangle dataset biases from visual concepts. This is done by intro-ducing preﬁx tokens that inform the language encoder of the input data type (e.g., classiﬁcation vs caption) at training time. Our approach allows the language encoder to learn from both datasets while also tailoring feature extraction to each dataset. Preﬁx conditioning is generic and can be eas-ily integrated into existing VL pretraining objectives, such as CLIP or UniCL. In experiments, we show that it improves zero-shot image recognition and robustness to image-level distribution shift. 