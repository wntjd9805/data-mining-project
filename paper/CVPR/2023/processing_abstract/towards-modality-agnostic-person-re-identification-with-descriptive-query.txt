Person re-identiﬁcation (ReID) with descriptive query (text or sketch) provides an important supplement for gen-eral image-image paradigms, which is usually studied in a single cross-modality matching manner, e.g., text-to-image or sketch-to-photo. However, without a camera-captured photo query, it is uncertain whether the text or sketch is available or not in practical scenarios. This motivates us to study a new and challenging modality-agnostic person re-identiﬁcation problem. Towards this goal, we propose a uniﬁed person re-identiﬁcation (UNIReID) architecture that can effectively adapt to cross-modality and multi-modality tasks. Speciﬁcally, UNIReID incorporates a simple dual-encoder with task-speciﬁc modality learning to mine and fuse visual and textual modality information.To deal with the imbalanced training problem of different tasks inUNIReID, we propose a task-aware dynamic training strat-egy in terms of task difﬁculty, adaptively adjusting the train-ing focus. Besides, we construct three multi-modal ReID datasets by collecting the corresponding sketches from pho-tos to support this challenging study. The experimental results on three multi-modal ReID datasets show that ourUNIReID greatly improves the retrieval accuracy and gen-eralization ability on different tasks and unseen scenarios. 