Most of the artificial lights fluctuate in response to the gridâ€™s alternating current and exhibit subtle variations in terms of both intensity and spectrum, providing the poten-tial to estimate the Electric Network Frequency (ENF) from conventional frame-based videos. Nevertheless, the perfor-mance of Video-based ENF (V-ENF) estimation largely re-lies on the imaging quality and thus may suffer from signif-icant interference caused by non-ideal sampling, motion, and extreme lighting conditions.In this paper, we show that the ENF can be extracted without the above limita-tions from a new modality provided by the so-called event camera, a neuromorphic sensor that encodes the light in-tensity variations and asynchronously emits events with ex-tremely high temporal resolution and high dynamic range.Specifically, we first formulate and validate the physical mechanism for the ENF captured in events, and then pro-pose a simple yet robust Event-based ENF (E-ENF) estima-tion method through mode filtering and harmonic enhance-ment. Furthermore, we build an Event-Video ENF Dataset (EV-ENFD) that records both events and videos in diverse scenes. Extensive experiments on EV-ENFD demonstrate that our proposed E-ENF method can extract more accu-rate ENF traces, outperforming the conventional V-ENF by a large margin, especially in challenging environments with object motions and extreme lighting conditions. The code and dataset are available at https://github.com/ xlx-creater/E-ENF.Figure 1. Illustrations of (a) the processing pipeline for E-ENF extraction and comparative experimental results under (b) different recording environments, in terms of (c) recorded event streams and (d) extracted V-ENF and E-ENF. 