With the wide and deep adoption of deep learning mod-els in real applications, there is an increasing need to model and learn the representations of the neural networks them-selves. These models can be used to estimate attributes of different neural network architectures such as the accu-racy and latency, without running the actual training or inference tasks.In this paper, we propose a neural ar-chitecture representation model that can be used to esti-mate these attributes holistically. Specifically, we first pro-pose a simple and effective tokenizer to encode both the operation and topology information of a neural network into a single sequence. Then, we design a multi-stage fu-sion transformer to build a compact vector representation from the converted sequence. For efficient model train-ing, we further propose an information flow consistency augmentation and correspondingly design an architecture consistency loss, which brings more benefits with less aug-mentation samples compared with previous random aug-mentation strategies. Experiment results on NAS-Bench-101, NAS-Bench-201, DARTS search space and NNLQP show that our proposed framework can be used to pre-dict the aforementioned latency and accuracy attributes of both cell architectures and whole deep neural networks, and achieves promising performance. Code is available at https://github.com/yuny220/NAR-Former. 