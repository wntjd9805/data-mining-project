Vision-language models (VLMs) can effectively transfer to various vision tasks via prompt learning. Real-world sce-narios often require adapting a model to multiple similar yet distinct tasks. Existing methods focus on learning a specific prompt for each task, limiting the ability to exploit poten-tially shared information from other tasks. Naively training a task-shared prompt using a combination of all tasks ig-nores fine-grained task correlations. Significant discrepan-cies across tasks could cause negative transferring. Consid-ering this, we present Hierarchical Prompt (HiPro) learn-ing, a simple and effective method for jointly adapting a pre-trained VLM to multiple downstream tasks. Our method quantifies inter-task affinity and subsequently constructs a hierarchical task tree. Task-shared prompts learned by in-ternal nodes explore the information within the correspond-ing task group, while task-individual prompts learned by leaf nodes obtain fine-grained information targeted at each task. The combination of hierarchical prompts provides high-quality content of different granularity. We evaluateHiPro on four multi-task learning datasets. The results demonstrate the effectiveness of our method. 