Image-text matching, a bridge connecting image and language, is an important task, which generally learns a holistic cross-modal embedding to achieve a high-quality semantic alignment between the two modalities. How-ever, previous studies only focus on capturing fragment-level relation within a sample from a particular modal-ity, e.g., salient regions in an image or text words in a sentence, where they usually pay less attention to captur-ing instance-level interactions among samples and modal-ities, e.g., multiple images and texts.In this paper, we argue that sample relations could help learn subtle dif-ferences for hard negative instances, and thus transfer shared knowledge for infrequent samples should be promis-ing in obtaining better holistic embeddings. Therefore, we propose a novel hierarchical relation modeling frame-work (HREM), which explicitly capture both fragment-and instance-level relations to learn discriminative and ro-bust cross-modal embeddings. Extensive experiments onFlickr30K and MS-COCO show our proposed method out-performs the state-of-the-art ones by 4%-10% in terms of rSum. Our code is available at https://github.com/CrossmodalGroup/HREM . 