Although an object may appear in numerous contexts, we often describe it in a limited number of ways. Language al-lows us to abstract away visual variation to represent and communicate concepts. Building on this intuition, we pro-pose an alternative approach to visual representation learn-ing: using language similarity to sample semantically sim-ilar image pairs for contrastive learning. Our approach diverges from image-based contrastive learning by sam-pling view pairs using language similarity instead of hand-crafted augmentations or learned clusters. Our approach also differs from image-text contrastive learning by relying on pre-trained language models to guide the learning rather than directly minimizing a cross-modal loss. Through a se-ries of experiments, we show that language-guided learning yields better features than image-based and image-text rep-resentation learning approaches. 