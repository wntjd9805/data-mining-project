Pruning—that is, setting a signiﬁcant subset of the pa-rameters of a neural network to zero—is one of the most popular methods of model compression. Yet, several recent works have raised the issue that pruning may induce or ex-acerbate bias in the output of the compressed model. De-spite existing evidence for this phenomenon, the relation-ship between neural network pruning and induced bias is not well-understood. In this work, we systematically inves-tigate and characterize this phenomenon in ConvolutionalNeural Networks for computer vision. First, we show that it is in fact possible to obtain highly-sparse models, e.g. with less than 10% remaining weights, which do not de-crease in accuracy nor substantially increase in bias when compared to dense models. At the same time, we alsoﬁnd that, at higher sparsities, pruned models exhibit higher uncertainty in their outputs, as well as increased correla-tions, which we directly link to increased bias. We pro-pose easy-to-use criteria which, based only on the uncom-pressed model, establish whether bias will increase with pruning, and identify the samples most susceptible to bi-ased predictions post-compression. Our code can be found at https://github.com/IST-DASLab/pruned-vision-model-bias. 