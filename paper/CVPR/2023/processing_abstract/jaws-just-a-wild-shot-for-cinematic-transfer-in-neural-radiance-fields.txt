This paper presents JAWS, an optimization-driven ap-proach that achieves the robust transfer of visual cinematic features from a reference in-the-wild video clip to a newly generated clip. To this end, we rely on an implicit-neural-representation (INR) in a way to compute a clip that shares the same cinematic features as the reference clip. We pro-pose a general formulation of a camera optimization prob-lem in an INR that computes extrinsic and intrinsic camera parameters as well as timing. By leveraging the differen-tiability of neural representations, we can back-propagate our designed cinematic losses measured on proxy estima-tors through a NeRF network to the proposed cinematic parameters directly. We also introduce speciﬁc enhance-ments such as guidance maps to improve the overall qual-ity and efﬁciency. Results display the capacity of our sys-tem to replicate well known camera sequences from movies, adapting the framing, camera parameters and timing of the generated video clip to maximize the similarity with the ref-erence clip. 