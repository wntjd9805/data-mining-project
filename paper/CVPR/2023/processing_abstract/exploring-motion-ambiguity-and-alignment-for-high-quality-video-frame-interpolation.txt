For video frame interpolation (VFI), existing deep-learning-based approaches strongly rely on the ground-truth (GT) intermediate frames, which sometimes ignore the non-unique nature of motion judging from the given adja-cent frames. As a result, these methods tend to produce averaged solutions that are not clear enough. To alleviate this issue, we propose to relax the requirement of recon-structing an intermediate frame as close to the GT as possi-ble. Towards this end, we develop a texture consistency loss (TCL) upon the assumption that the interpolated content should maintain similar structures with their counterparts in the given frames. Predictions satisfying this constraint are encouraged, though they may differ from the prede-fined GT. Without the bells and whistles, our plug-and-playTCL is capable of improving the performance of existingVFI frameworks consistently. On the other hand, previous methods usually adopt the cost volume or correlation map to achieve more accurate image or feature warping. How-ever, the O(N 2) (N refers to the pixel count) computational complexity makes it infeasible for high-resolution cases. In this work, we design a simple, efficient O(N ) yet power-ful guided cross-scale pyramid alignment (GCSPA) module, where multi-scale information is highly exploited. Exten-sive experiments justify the efficiency and effectiveness of the proposed strategy. 