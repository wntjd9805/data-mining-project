Federated learning-based semantic segmentation (FSS) has drawn widespread attention via decentralized training on local clients. However, most FSS models assume cate-gories are fixed in advance, thus heavily undergoing forget-ting on old categories in practical applications where local clients receive new categories incrementally while have no memory storage to access old classes. Moreover, new clients collecting novel classes may join in the global training ofFSS, which further exacerbates catastrophic forgetting. To surmount the above challenges, we propose a Forgetting-Balanced Learning (FBL) model to address heterogeneous forgetting on old classes from both intra-client and inter-client aspects. Specifically, under the guidance of pseudo labels generated via adaptive class-balanced pseudo label-ing, we develop a forgetting-balanced semantic compensa-tion loss and a forgetting-balanced relation consistency loss to rectify intra-client heterogeneous forgetting of old cate-gories with background shift. It performs balanced gradient propagation and relation consistency distillation within lo-cal clients. Moreover, to tackle heterogeneous forgetting from inter-client aspect, we propose a task transition mon-itor. It can identify new classes under privacy protection and store the latest old global model for relation distillation.Qualitative experiments reveal large improvement of our model against comparison methods. The code is available at https://github.com/JiahuaDong/FISS. 