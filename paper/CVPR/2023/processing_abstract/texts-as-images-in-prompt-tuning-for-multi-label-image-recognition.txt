Prompt tuning has been employed as an efﬁcient way to adapt large vision-language pre-trained models (e.g. CLIP) to various downstream tasks in data-limited or label-limited settings. Nonetheless, visual data (e.g., images) is by de-fault prerequisite for learning prompts in existing methods.In this work, we advocate that the effectiveness of image-text contrastive learning in aligning the two modalities (for training CLIP) further makes it feasible to treat texts as im-ages for prompt tuning and introduce TaI prompting.In contrast to the visual data, text descriptions are easy to col-lect, and their class labels can be directly derived. Particu-larly, we apply TaI prompting to multi-label image recogni-tion, where sentences in the wild serve as alternatives to im-ages for prompt tuning. Moreover, with TaI, double-grained prompt tuning (TaI-DPT) is further presented to extract both coarse-grained and ﬁne-grained embeddings for enhanc-ing the multi-label recognition performance. Experimen-tal results show that our proposed TaI-DPT outperforms zero-shot CLIP by a large margin on multiple benchmarks, e.g., MS-COCO, VOC2007, and NUS-WIDE, while it can be combined with existing methods of prompting from im-ages to improve recognition performance further. The code is released at https://github.com/guozix/TaI-DPT. 