Current popular backbones in computer vision, such asVision Transformers (ViT) and ResNets are trained to per-ceive the world from 2D images. However, to more effec-tively understand 3D structural priors in 2D backbones, we propose Mask3D to leverage existing large-scale RGB-D data in a self-supervised pre-training to embed these 3D priors into 2D learned feature representations.In con-trast to traditional 3D contrastive learning paradigms re-quiring 3D reconstructions or multi-view correspondences, our approach is simple: we formulate a pre-text reconstruc-tion task by masking RGB and depth patches in individualRGB-D frames. We demonstrate the Mask3D is particu-larly effective in embedding 3D priors into the powerful 2D ViT backbone, enabling improved representation learn-ing for various scene understanding tasks, such as semantic segmentation, instance segmentation and object detection.Experiments show that Mask3D notably outperforms exist-ing self-supervised 3D pre-training approaches on ScanNet,NYUv2, and Cityscapes image understanding tasks, with an improvement of +6.5% mIoU against the state-of-the-artPri3D on ScanNet image semantic segmentation. 