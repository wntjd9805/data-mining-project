The task of key-points detection and description is to es-timate the stable location and discriminative representa-tion of local features, which is a fundamental task in vi-sual applications. However, either the rough hard positive or negative labels generated from one-to-one correspon-dences among images may bring indistinguishable samples, like false positives or negatives, which acts as inconsis-tent supervision. Such resultant false samples mixed with hard samples prevent neural networks from learning de-scriptions for more accurate matching. To tackle this chal-lenge, we propose to learn the transformation-predictive representations with self-supervised contrastive learning.We maximize the similarity between corresponding views of the same 3D point (landmark) by using none of the neg-ative sample pairs and avoiding collapsing solutions. Fur-thermore, we adopt self-supervised generation learning and curriculum learning to soften the hard positive labels into soft continuous targets. The aggressively updated soft la-bels contribute to overcoming the training bottleneck (de-rived from the label noise of false positives) and facili-tating the model training under a stronger transformation paradigm. Our self-supervised training pipeline greatly de-creases the computation load and memory usage, and out-performs the sota on the standard image matching bench-marks by noticeable margins, demonstrating excellent gen-eralization capability on multiple downstream tasks. 