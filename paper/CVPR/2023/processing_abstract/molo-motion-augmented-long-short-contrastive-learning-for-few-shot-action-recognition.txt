Current state-of-the-art approaches for few-shot action recognition achieve promising performance by conducting frame-level matching on learned visual features. However, they generally suffer from two limitations: i) the matching procedure between local frames tends to be inaccurate due to the lack of guidance to force long-range temporal percep-tion; ii) explicit motion learning is usually ignored, leading to partial information loss. To address these issues, we de-velop a Motion-augmented Long-short Contrastive Learn-ing (MoLo) method that contains two crucial components, including a long-short contrastive objective and a motion autodecoder. Specifically, the long-short contrastive objec-tive is to endow local frame features with long-form tem-poral awareness by maximizing their agreement with the global token of videos belonging to the same class. The motion autodecoder is a lightweight architecture to recon-struct pixel motions from the differential features, which ex-plicitly embeds the network with motion dynamics. By this means, MoLo can simultaneously learn long-range tempo-ral context and motion cues for comprehensive few-shot matching. To demonstrate the effectiveness, we evaluateMoLo on five standard benchmarks, and the results show that MoLo favorably outperforms recent advanced meth-ods. The source code is available at https://github. com/alibaba-mmai-research/MoLo. 