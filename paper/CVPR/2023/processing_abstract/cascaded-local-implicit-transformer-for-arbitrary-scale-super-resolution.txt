Implicit neural representation has recently shown a promising ability in representing images with arbitrary res-olutions. In this paper, we present a Local Implicit Trans-former (LIT), which integrates the attention mechanism and frequency encoding technique into a local implicit image function. We design a cross-scale local attention block to ef-fectively aggregate local features and a local frequency en-coding block to combine positional encoding with Fourier domain information for constructing high-resolution im-ages. To further improve representative power, we pro-pose a Cascaded LIT (CLIT) that exploits multi-scale fea-tures, along with a cumulative training strategy that grad-ually increases the upsampling scales during training. We have conducted extensive experiments to validate the effec-tiveness of these components and analyze various training strategies. The qualitative and quantitative results demon-strate that LIT and CLIT achieve favorable results and out-perform the prior works in arbitrary super-resolution tasks. 