Human spatial attention conveys information about the regions of visual scenes that are important for perform-ing visual tasks. Prior work has shown that the informa-tion about human attention can be leveraged to beneﬁt var-ious supervised vision tasks. Might providing this weak form of supervision be useful for self-supervised represen-tation learning? Addressing this question requires collect-ing large datasets with human attention labels. Yet, col-lecting such large scale data is very expensive. To address this challenge, we construct an auxiliary teacher model to predict human attention, trained on a relatively small la-beled dataset. This teacher model allows us to generate im-age (pseudo) attention labels for ImageNet. We then train a model with a primary contrastive objective; to this stan-dard conﬁguration, we add a simple output head trained to predict the attention map for each image, guided by the pseudo labels from teacher model. We measure the qual-ity of learned representations by evaluating classiﬁcation performance from the frozen learned embeddings as well as performance on image retrieval tasks (see supplementary material). We ﬁnd that the spatial-attention maps predicted from the contrastive model trained with teacher guidance aligns better with human attention compared to vanilla con-trastive models. Moreover, we ﬁnd that our approach im-proves classiﬁcation accuracy and robustness of the con-trastive models on ImageNet and ImageNet-C. Further, weﬁnd that model representations become more useful for im-age retrieval task as measured by precision-recall perfor-mance on ImageNet, ImageNet-C, CIFAR10, and CIFAR10-C datasets.Figure 1. Illustration. A teacher model is trained to predict human spatial-attention from a small dataset. Then the model is used to provide attention labels for larger dataset, which are used as addi-tional targets for contrastive models. 