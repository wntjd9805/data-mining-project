Reliable uncertainty from deterministic single-forward pass models is sought after because conventional methods of uncertainty quantiﬁcation are computationally expensive.We take two complex single-forward-pass uncertainty ap-proaches, DUQ and SNGP, and examine whether they mainly rely on a well-regularized feature space. Crucially, without using their more complex methods for estimating uncertainty, we ﬁnd that a single softmax neural net with such a reg-ularized feature-space, achieved via residual connections and spectral normalization, outperforms DUQ and SNGP’s epistemic uncertainty predictions using simple Gaussian Dis-criminant Analysis post-training as a separate feature-space density estimator—without ﬁne-tuning on OoD data, fea-ture ensembling, or input pre-procressing. Our conceptually simple Deep Deterministic Uncertainty (DDU) baseline can also be used to disentangle aleatoric and epistemic uncer-tainty and performs as well as Deep Ensembles, the state-of-the art for uncertainty prediction, on several OoD bench-marks (CIFAR-10/100 vs SVHN/Tiny-ImageNet, ImageNet vsImageNet-O), active learning settings across different model architectures, as well as in large scale vision tasks like se-mantic segmentation, while being computationally cheaper. 