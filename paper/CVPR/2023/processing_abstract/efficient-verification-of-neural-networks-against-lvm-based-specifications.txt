The deployment of perception systems based on neu-ral networks in safety critical applications requires assur-ance on their robustness. Deterministic guarantees on net-work robustness require formal verification. Standard ap-proaches for verifying robustness analyse invariance to an-alytically defined transformations, but not the diverse and ubiquitous changes involving object pose, scene viewpoint, occlusions, etc. To this end, we present an efficient ap-proach for verifying specifications definable using LatentVariable Models that capture such diverse changes. The ap-proach involves adding an invertible encoding head to the network to be verified, enabling the verification of latent space sets with minimal reconstruction overhead. We re-port verification experiments for three classes of proposed latent space specifications, each capturing different types of realistic input variations. Differently from previous work in this area, the proposed approach is relatively independent of input dimensionality and scales to a broad class of deep networks and real-world datasets by mitigating the ineffi-ciency and decoder expressivity dependence in the present state-of-the-art. 