Many defenses against adversarial attacks (e.g. robust classifiers, randomization, or image purification) use coun-termeasures put to work only after the attack has been crafted. We adopt a different perspective to introduce A5 (Adversarial Augmentation Against Adversarial Attacks), a novel framework including the first certified preemptive de-fense against adversarial attacks. The main idea is to craft a defensive perturbation to guarantee that any attack (up to a given magnitude) towards the input in hand will fail. To this aim, we leverage existing automatic perturbation analysis tools for neural networks. We study the conditions to applyA5 effectively, analyze the importance of the robustness of the to-be-defended classifier, and inspect the appearance of the robustified images. We show effective on-the-fly defen-sive augmentation with a robustifier network that ignores the ground truth label, and demonstrate the benefits of ro-In our tests, A5 con-bustifier and classifier co-training. sistently beats state of the art certified defenses on MNIST,CIFAR10, FashionMNIST and Tinyimagenet. We also show how to apply A5 to create certifiably robust physical ob-jects. Our code at https://github.com/NVlabs/A5 allows experimenting on a wide range of scenarios be-yond the man-in-the-middle attack tested here, including the case of physical attacks. 