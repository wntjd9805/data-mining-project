Although deep neural networks (DNNs) have shown great successes in computer vision tasks, they are vulner-able to perturbations on inputs, and there exists a trade-off between the natural accuracy and robustness to such per-turbations, which is mainly caused by the existence of robust non-predictive features and non-robust predictive features.Recent empirical analyses find Vision Transformers (ViTs) are inherently robust to various kinds of perturbations, butIn this the aforementioned trade-off still exists for them. work, we propose Trade-off between Robustness and Accu-racy of Vision Transformers (TORA-ViTs), which aims to efficiently transfer ViT models pretrained on natural tasks for both accuracy and robustness. TORA-ViTs consist of two major components, including a pair of accuracy and robustness adapters to extract predictive and robust fea-tures, respectively, and a gated fusion module to adjust the trade-off. The gated fusion module takes outputs of a pre-trained ViT block as queries and outputs of our adapters as keys and values, and tokens from different adapters at different spatial locations are compared with each other to generate attention scores for a balanced mixing of predic-tive and robust features. Experiments on ImageNet with various robust benchmarks show that our TORA-ViTs can efficiently improve the robustness of naturally pretrainedViTs while maintaining competitive natural accuracy. Our most balanced setting (TORA-ViTs with Î» = 0.5) can main-tain 83.7% accuracy on clean ImageNet and reach 54.7% and 38.0% accuracy under FGSM and PGD white-box at-tacks, respectively. In terms of various ImageNet variants, it can reach 39.2% and 56.3% accuracy on ImageNet-A andImageNet-R and reach 34.4% mCE on ImageNet-C. 