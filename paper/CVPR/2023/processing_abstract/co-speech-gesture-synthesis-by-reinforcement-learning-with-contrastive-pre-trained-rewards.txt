There is a growing demand of automatically synthesizing co-speech gestures for virtual characters. However, it re-mains a challenge due to the complex relationship between input speeches and target gestures. Most existing works fo-cus on predicting the next gesture that ﬁts the data best, however, such methods are myopic and lack the ability to plan for future gestures. In this paper, we propose a novel reinforcement learning (RL) framework called RACER to generate sequences of gestures that maximize the overall satisfactory. RACER employs a vector quantized varia-tional autoencoder to learn compact representations of ges-tures and a GPT-based policy architecture to generate co-herent sequence of gestures autoregressively. In particular, we propose a contrastive pre-training approach to calculate the rewards, which integrates contextual information into action evaluation and successfully captures the complex re-lationships between multi-modal speech-gesture data. Ex-perimental results show that our method signiﬁcantly out-performs existing baselines in terms of both objective met-rics and subjective human judgements. Demos can be found at https://github.com/RLracer/RACER.git. 