We present a novel paradigm for high-ﬁdelity face swap-ping that faithfully preserves the desired subtle geometry and texture details. We rethink face swapping from the per-spective of ﬁne-grained face editing, i.e., “editing for swap-ping” (E4S), and propose a framework that is based on the explicit disentanglement of the shape and texture of facial components. Following the E4S principle, our framework enables both global and local swapping of facial features, as well as controlling the amount of partial swapping spec-iﬁed by the user. Furthermore, the E4S paradigm is in-herently capable of handling facial occlusions by means of facial masks. At the core of our system lies a novel Re-gional GAN Inversion (RGI) method, which allows the ex-plicit disentanglement of shape and texture. It also allows face swapping to be performed in the latent space of Style-GAN. Speciﬁcally, we design a multi-scale mask-guided en-coder to project the texture of each facial component into regional style codes. We also design a mask-guided injec-tion module to manipulate the feature maps with the styleWork done when Zhian Liu was an intern at Tencent AI LabEqual contribution.Corresponding author: nieyongwei@scut.edu.cn†⇤ codes. Based on the disentanglement, face swapping is re-formulated as a simpliﬁed problem of style and mask swap-ping. Extensive experiments and comparisons with current state-of-the-art methods demonstrate the superiority of our approach in preserving texture and shape details, as well as working with high resolution images. The project page is https://e4s2022.github.io 