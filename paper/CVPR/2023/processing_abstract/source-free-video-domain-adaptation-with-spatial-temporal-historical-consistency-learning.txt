Source-free domain adaptation (SFDA) is an emerging research topic that studies how to adapt a pretrained source model using unlabeled target data. It is derived from unsu-pervised domain adaptation but has the advantage of not requiring labeled source data to learn adaptive models.This makes it particularly useful in real-world applications where access to source data is restricted. While there has been some SFDA work for images, little attention has been paid to videos. Naively extending image-based methods to videos without considering the unique properties of videos often leads to unsatisfactory results. In this paper, we pro-pose a simple and highly flexible method for Source-FreeVideo Domain Adaptation (SFVDA), which extensively ex-ploits consistency learning for videos from spatial, tempo-ral, and historical perspectives. Our method is based on the assumption that videos of the same action category are drawn from the same low-dimensional space, regardless of the spatio-temporal variations in the high-dimensional space that cause domain shifts. To overcome domain shifts, we simulate spatio-temporal variations by applying spatial and temporal augmentations on target videos and encour-age the model to make consistent predictions from a video and its augmented versions. Due to the simple design, our method can be applied to various SFVDA settings, and ex-periments show that our method achieves state-of-the-art performance for all the settings. 