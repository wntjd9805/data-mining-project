Video frame interpolation (VFI) is the task that synthe-sizes the intermediate frame given two consecutive frames.Most of the previous studies have focused on appropriate frame warping operations and refinement modules for the warped frames. These studies have been conducted on natural videos containing only continuous motions. How-ever, many practical videos contain various unnatural ob-jects with discontinuous motions such as logos, user in-terfaces and subtitles. We propose three techniques that can make the existing deep learning-based VFI architec-tures robust to these elements. First is a novel data aug-mentation strategy called figure-text mixing (FTM) which can make the models learn discontinuous motions during training stage without any extra dataset. Second, we pro-pose a simple but effective module that predicts a map called discontinuity map (D-map), which densely distin-guishes between areas of continuous and discontinuous mo-tions. Lastly, we propose loss functions to give supervi-sions of the discontinuous motion areas which can be ap-plied along with FTM and D-map. We additionally collect a special test benchmark called Graphical DiscontinuousMotion (GDM) dataset consisting of some mobile games and chatting videos. Applied to the various state-of-the-artVFI networks, our method significantly improves the inter-polation qualities on the videos from not only GDM dataset, but also the existing benchmarks containing only continu-ous motions such as Vimeo90K, UCF101, and DAVIS. 