Zero-shot quantization is a promising approach for de-veloping lightweight deep neural networks when data is in-accessible owing to various reasons, including cost and is-sues related to privacy. By exploiting the learned parame-ters (µ and σ) of batch normalization layers in an FP32-pre-trained model, zero-shot quantization schemes focus on generating synthetic data. Subsequently, they distill knowl-edge from the pre-trained model (teacher) to the quantized model (student) such that the quantized model can be op-timized with the synthetic dataset. However, thus far, zero-shot quantization has primarily been discussed in the con-text of quantization-aware training methods, which require task-specific losses and long-term optimization as much as retraining. We thus introduce a post-training quantiza-tion scheme for zero-shot quantization that produces high-quality quantized networks within a few hours. Further-more, we propose a framework called GENIE that gener-ates data suited for quantization. With the data synthesized by GENIE, we can produce robust quantized models with-out real datasets, which is comparable to few-shot quanti-zation. We also propose a post-training quantization algo-rithm to enhance the performance of quantized models. By combining them, we can bridge the gap between zero-shot∗Equal contribution. Correspondence to: dragwon.jeon@samsung.com and few-shot quantization while significantly improving the quantization performance compared to that of existing ap-proaches. In other words, we can obtain a unique state-of-the-art zero-shot quantization approach. The code is available at https://github.com/SamsungLabs/Genie. 