Novel Class Discovery (NCD) aims to discover unknown classes without any annotation, by exploiting the transfer-able knowledge already learned from a base set of known classes. Existing works hold an impractical assumption that the novel class distribution prior is uniform, yet neglectIn this paper, the imbalanced nature of real-world data. we relax this assumption by proposing a new challenging task: distribution-agnostic NCD, which allows data drawn from arbitrary unknown class distributions and thus ren-ders existing methods useless or even harmful. We tackle this challenge by proposing a new method, dubbed “Boot-strapping Your Own Prior (BYOP)”, which iteratively es-timates the class prior based on the model prediction it-self. At each iteration, we devise a dynamic temperature technique that better estimates the class prior by encour-aging sharper predictions for less-confident samples. Thus,BYOP obtains more accurate pseudo-labels for the novel samples, which are beneficial for the next training itera-tion. Extensive experiments show that existing methods suf-fer from imbalanced class distributions, while BYOP1 out-performs them by clear margins, demonstrating its effec-tiveness across various distribution scenarios. 