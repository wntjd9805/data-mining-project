To design fast neural networks, many works have been focusing on reducing the number of floating-point opera-tions (FLOPs). We observe that such reduction in FLOPs, however, does not necessarily lead to a similar level of re-duction in latency. This mainly stems from inefficiently low floating-point operations per second (FLOPS). To achieve faster networks, we revisit popular operators and demon-strate that such low FLOPS is mainly due to frequent mem-ory access of the operators, especially the depthwise con-volution. We hence propose a novel partial convolution (PConv) that extracts spatial features more efficiently, by cutting down redundant computation and memory access simultaneously. Building upon our PConv, we further pro-pose FasterNet, a new family of neural networks, which attains substantially higher running speed than others on a wide range of devices, without compromising on accu-racy for various vision tasks. For example, on ImageNet-1k, our tiny FasterNet-T0 is 2.8×, 3.3×, and 2.4× faster than MobileViT-XXS on GPU, CPU, and ARM processors, respectively, while being 2.9% more accurate. Our largeFasterNet-L achieves impressive 83.5% top-1 accuracy, on par with the emerging Swin-B, while having 36% higher in-ference throughput on GPU, as well as saving 37% compute time on CPU. Code is available at https://github. com/JierunChen/FasterNet. 