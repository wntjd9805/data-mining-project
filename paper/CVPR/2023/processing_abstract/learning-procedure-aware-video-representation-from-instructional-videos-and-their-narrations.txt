The abundance of instructional videos and their narra-tions over the Internet offers an exciting avenue for un-In this work, we pro-derstanding procedural activities. pose to learn video representation that encodes both ac-tion steps and their temporal ordering, based on a large-scale dataset of web instructional videos and their narra-tions, without using human annotations. Our method jointly learns a video representation to encode individual step con-cepts, and a deep probabilistic model to capture both tem-poral dependencies and immense individual variations in the step ordering. We empirically demonstrate that learn-ing temporal ordering not only enables new capabilities for procedure reasoning, but also reinforces the recognition of individual steps. Our model significantly advances the state-of-the-art results on step classification (+2.8%/+3.3% on COIN / EPIC-Kitchens) and step forecasting (+7.4% on COIN). Moreover, our model attains promising re-sults in zero-shot inference for step classification and fore-casting, as well as in predicting diverse and plausible steps for incomplete procedures. Our code is available at https://github.com/facebookresearch/ProcedureVRL. 