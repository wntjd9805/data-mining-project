Controllability, generalizability and efﬁciency are the major objectives of constructing face avatars represented by neural implicit ﬁeld. However, existing methods have not managed to accommodate the three requirements si-multaneously. They either focus on static portraits, restrict-ing the representation ability to a speciﬁc subject, or suffer from substantial computational cost, limiting their ﬂexibil-ity. In this paper, we propose One-shot Talking face Avatar (OTAvatar), which constructs face avatars by a general-ized controllable tri-plane rendering solution so that each personalized avatar can be constructed from only one por-trait as the reference. Speciﬁcally, OTAvatar ﬁrst inverts a portrait image to a motion-free identity code. Second, the identity code and a motion code are utilized to modulate an efﬁcient CNN to generate a tri-plane formulated volume, which encodes the subject in the desired motion. Finally, volume rendering is employed to generate an image in any view. The core of our solution is a novel decoupling-by-inverting strategy that disentangles identity and motion in the latent code via optimization-based inversion. Beneﬁt-ing from the efﬁcient tri-plane representation, we achieve controllable rendering of generalized face avatar at 35 FPS on A100. Experiments show promising performance of cross-identity reenactment on subjects out of the training set and better 3D consistency. The code is available at https://github.com/theEricMa/OTAvatar. 