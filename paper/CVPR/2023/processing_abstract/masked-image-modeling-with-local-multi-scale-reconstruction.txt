Masked Image Modeling (MIM) achieves outstanding success in self-supervised representation learning. Unfortu-nately, MIM models typically have huge computational bur-den and slow learning process, which is an inevitable ob-stacle for their industrial applications. Although the lower layers play the key role in MIM, existing MIM models con-duct reconstruction task only at the top layer of encoder.The lower layers are not explicitly guided and the interac-tion among their patches is only used for calculating new activations. Considering the reconstruction task requires non-trivial inter-patch interactions to reason target signals, we apply it to multiple local layers including lower and up-per layers. Further, since the multiple layers expect to learn the information of different scales, we design local multi-scale reconstruction, where the lower and upper layers re-construct fine-scale and coarse-scale supervision signals respectively. This design not only accelerates the represen-tation learning process by explicitly guiding multiple lay-ers, but also facilitates multi-scale semantical understand-ing to the input. Extensive experiments show that with sig-nificantly less pre-training burden, our model achieves com-parable or better performance on classification, detection and segmentation tasks than existing MIM models. Code is available with both MindSpore and PyTorch. 