Despite excellent performance in image generation,Generative Adversarial Networks (GANs) are notorious for its requirements of enormous storage and intensive com-putation. As an awesome “performance maker”, knowl-edge distillation is demonstrated to be particularly effica-cious in exploring low-priced GANs. In this paper, we in-vestigate the irreplaceability of teacher discriminator and present an inventive discriminator-cooperated distillation, abbreviated as DCD, towards refining better feature maps from the generator.In contrast to conventional pixel-to-pixel match methods in feature map distillation, our DCD utilizes teacher discriminator as a transformation to drive intermediate results of the student generator to be percep-tually close to corresponding outputs of the teacher gen-erator. Furthermore, in order to mitigate mode collapse in GAN compression, we construct a collaborative adver-sarial training paradigm where the teacher discriminator is from scratch established to co-train with student gener-ator in company with our DCD. Our DCD shows superior results compared with existing GAN compression methods.For instance, after reducing over 40× MACs and 80× pa-rameters of CycleGAN, we well decrease FID metric from 61.53 to 48.24 while the current SoTA method merely has 51.92. This work’s source code has been made accessible at https://github.com/poopit/DCD-official. 