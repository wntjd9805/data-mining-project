The attention mechanism has become the de facto mod-ule in scene text recognition (STR) methods, due to its capa-bility of extracting character-level representations. These methods can be summarized into implicit attention based and supervised attention based, depended on how the at-tention is computed, i.e., implicit attention and supervised attention are learned from sequence-level text annotations and or character-level bounding box annotations, respec-tively. Implicit attention, as it may extract coarse or even incorrect spatial regions as character attention, is prone to suffering from an alignment-drifted issue. Supervised at-tention can alleviate the above issue, but it is character category-speciﬁc, which requires extra laborious character-level bounding box annotations and would be memory-intensive when handling languages with larger character categories. To address the aforementioned issues, we pro-pose a novel attention mechanism for STR, self-supervised implicit glyph attention (SIGA). SIGA delineates the glyph structures of text images by jointly self-supervised text seg-mentation and implicit attention alignment, which serve as the supervision to improve attention correctness with-out extra character-level annotations. Experimental results demonstrate that SIGA performs consistently and signiﬁ-cantly better than previous attention-based STR methods, in terms of both attention correctness and ﬁnal recognition performance on publicly available context benchmarks and our contributed contextless benchmarks. 