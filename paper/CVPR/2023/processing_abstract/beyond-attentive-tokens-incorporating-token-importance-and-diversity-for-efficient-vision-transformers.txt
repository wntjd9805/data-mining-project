Vision transformers have achieved significant improve-ments on various vision tasks but their quadratic interac-tions between tokens significantly reduce computational ef-ficiency. Many pruning methods have been proposed to re-move redundant tokens for efficient vision transformers re-cently. However, existing studies mainly focus on the token importance to preserve local attentive tokens but completely ignore the global token diversity. In this paper, we empha-size the cruciality of diverse global semantics and propose an efficient token decoupling and merging method that can jointly consider the token importance and diversity for to-ken pruning. According to the class token attention, we de-couple the attentive and inattentive tokens. In addition to preserving the most discriminative local tokens, we merge similar inattentive tokens and match homogeneous atten-tive tokens to maximize the token diversity. Despite its sim-plicity, our method obtains a promising trade-off between model complexity and classification accuracy. On DeiT-S, our method reduces the FLOPs by 35% with only a 0.2% accuracy drop. Notably, benefiting from maintaining the to-ken diversity, our method can even improve the accuracy ofDeiT-T by 0.1% after reducing its FLOPs by 40%. 