Stage 1: Unsupervised PretrainingUnlabeled PoolStage 2: Supervised FinetuningGiven the large-scale data and the high annotation cost, pretraining-ﬁnetuning becomes a popular paradigm in mul-tiple computer vision tasks. Previous research has covered both the unsupervised pretraining and supervised ﬁnetuning in this paradigm, while little attention is paid to exploiting the annotation budget for ﬁnetuning. To ﬁll in this gap, we formally deﬁne this new active ﬁnetuning task focusing on the selection of samples for annotation in the pretraining-ﬁnetuning paradigm. We propose a novel method called Ac-tiveFT for active ﬁnetuning task to select a subset of data distributing similarly with the entire unlabeled pool and maintaining enough diversity by optimizing a parametric model in the continuous space. We prove that the EarthMover’s distance between the distributions of the selected subset and the entire data pool is also reduced in this pro-cess. Extensive experiments show the leading performance and high efﬁciency of ActiveFT superior to baselines on both image classiﬁcation and semantic segmentation. Our code is released at https://github.com/yichen928/ActiveFT. 