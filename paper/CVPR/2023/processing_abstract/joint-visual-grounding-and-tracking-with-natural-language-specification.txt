Tracking by natural language specification aims to lo-cate the referred target in a sequence based on the nat-ural language description. Existing algorithms solve this issue in two steps, visual grounding and tracking, and ac-cordingly deploy the separated grounding model and track-ing model to implement these two steps, respectively. Such a separated framework overlooks the link between visual grounding and tracking, which is that the natural language descriptions provide global semantic cues for localizing the target for both two steps. Besides, the separated frame-work can hardly be trained end-to-end. To handle these issues, we propose a joint visual grounding and tracking framework, which reformulates grounding and tracking as a unified task: localizing the referred target based on the given visual-language references. Specifically, we propose a multi-source relation modeling module to effectively build the relation between the visual-language references and the test image.In addition, we design a temporal modeling module to provide a temporal clue with the guidance of the global semantic information for our model, which ef-fectively improves the adaptability to the appearance vari-ations of the target. Extensive experimental results onTNL2K, LaSOT, OTB99, and RefCOCOg demonstrate that our method performs favorably against state-of-the-art al-gorithms for both tracking and grounding. Code is avail-able at https://github.com/lizhou-cs/JointNLT.Figure 1. Illustration of two different frameworks for tracking by natural language specification. (a) The separated visual ground-ing and tracking framework, which consists of two independent models for visual grounding and tracking, respectively. (b) The proposed joint visual grounding and tracking framework, which employs a single model for both visual grounding and tracking. 