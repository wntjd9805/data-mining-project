ESPCNSRResNetResNet18MobileNet-v2Neural network quantization is a popular approach for model compression. Modern hardware supports quanti-zation in mixed-precision mode, which allows for greater compression rates but adds the challenging task of search-ing for the optimal bit width. The majority of existing searchers find a single mixed-precision architecture. To select an architecture that is suitable in terms of perfor-mance and resource consumption, one has to restart search-ing multiple times. We focus on a specific class of methods that find tensor bit width using gradient-based optimization.First, we theoretically derive several methods that were em-pirically proposed earlier. Second, we present a novel One-Shot method that finds a diverse set of Pareto-front architec-tures in O(1) time. For large models, the proposed method is 5 times more efficient than existing methods. We verify the method on two classification and super-resolution mod-els and show above 0.93 correlation score between the pre-dicted and actual model performance. The Pareto-front ar-chitecture selection is straightforward and takes only 20 to 40 supernet evaluations, which is the new state-of-the-art result to the best of our knowledge. 