Transfer learning is a popular method for tuning pre-trained (upstream) models for different downstream tasks using limited data and computational resources. We study how an adversary with control over an upstream model used in transfer learning can conduct property inference attacks on a victimâ€™s tuned downstream model. For example, to in-fer the presence of images of a specific individual in the downstream training set. We demonstrate attacks in which an adversary can manipulate the upstream model to con-duct highly effective and specific property inference attacks (AUC score > 0.9), without incurring significant perfor-mance loss on the main task. The main idea of the ma-nipulation is to make the upstream model generate acti-vations (intermediate features) with different distributions for samples with and without a target property, thus en-abling the adversary to distinguish easily between down-stream models trained with and without training examples that have the target property. Our code is available at https:// github.com/ yulongt23/ Transfer-Inference. 