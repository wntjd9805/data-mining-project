Deep neural networks (DNNs) are known to be vulnera-ble to both backdoor attacks as well as adversarial attacks.In the literature, these two types of attacks are commonly treated as distinct problems and solved separately, since they belong to training-time and inference-time attacks re-spectively. However, in this paper we find an intriguing con-nection between them: for a model planted with backdoors, we observe that its adversarial examples have similar be-haviors as its triggered images, i.e., both activate the same subset of DNN neurons. It indicates that planting a back-door into a model will significantly affect the modelâ€™s ad-versarial examples. Based on these observations, a novelProgressive Backdoor Erasing (PBE) algorithm is proposed to progressively purify the infected model by leveraging un-targeted adversarial attacks. Different from previous back-door defense methods, one significant advantage of our ap-proach is that it can erase backdoor even when the clean extra dataset is unavailable. We empirically show that, against 5 state-of-the-art backdoor attacks, our PBE can effectively erase the backdoor without obvious performance degradation on clean samples and outperforms existing de-fense methods. 