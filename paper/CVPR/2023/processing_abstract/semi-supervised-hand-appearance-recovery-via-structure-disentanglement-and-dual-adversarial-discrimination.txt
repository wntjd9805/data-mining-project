Enormous hand images with reliable annotations are collected through marker-based MoCap . Unfortunately, degradations caused by markers limit their application in hand appearance reconstruction. A clear appearance re-covery insight is an image-to-image translation trained with unpaired data. However, most frameworks fail because there exists structure inconsistency from a degraded hand to a bare one. The core of our approach is to first disentan-gle the bare hand structure from those degraded images and then wrap the appearance to this structure with a dual ad-versarial discrimination (DAD) scheme. Both modules take full advantage of the semi-supervised learning paradigm:The structure disentanglement benefits from the modeling ability of ViT, and the translator is enhanced by the dual dis-crimination on both translation processes and translation results. Comprehensive evaluations have been conducted to prove that our framework can robustly recover photo-realistic hand appearance from diverse marker-contained and even object-occluded datasets. It provides a novel av-enue to acquire bare hand appearance data for other down-stream learning problems. 