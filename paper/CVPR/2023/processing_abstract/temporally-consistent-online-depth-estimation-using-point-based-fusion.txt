Depth estimation is an important step in many computer vision problems such as 3D reconstruction, novel view syn-thesis, and computational photography. Most existing work focuses on depth estimation from single frames. When ap-plied to videos, the result lacks temporal consistency, show-ing flickering and swimming artifacts.In this paper we aim to estimate temporally consistent depth maps of video streams in an online setting. This is a difficult problem as future frames are not available and the method must choose between enforcing consistency and correcting errors from previous estimations. The presence of dynamic objects fur-ther complicates the problem. We propose to address these challenges by using a global point cloud that is dynami-cally updated each frame, along with a learned fusion ap-proach in image space. Our approach encourages consis-tency while simultaneously allowing updates to handle er-rors and dynamic objects. Qualitative and quantitative re-sults show that our method achieves state-of-the-art quality for consistent video depth estimation. 