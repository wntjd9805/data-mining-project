Most real-world 3D measurements from depth sensors are incomplete, and to address this issue the point cloud completion task aims to predict the complete shapes of objects from partial observations. Previous works often adapt an encoder-decoder architecture, where the encoder is trained to extract embeddings that are used as inputs to generate predictions from the decoder. However, the learned embeddings have sparse distribution in the feature space, which leads to worse generalization results during testing. To address these problems, this paper proposes a hyperspherical module, which transforms and normal-izes embeddings from the encoder to be on a unit hyper-sphere. With the proposed module, the magnitude and direc-tion of the output hyperspherical embedding are decoupled and only the directional information is optimized. We the-oretically analyze the hyperspherical embedding and show that it enables more stable training with a wider range of learning rates and more compact embedding distributions.Experiment results show consistent improvement of point cloud completion in both single-task and multi-task learn-ing, which demonstrates the effectiveness of the proposed method. 