3D style transfer aims to render stylized novel views of a 3D scene with multi-view consistency. However, most ex-isting work suffers from a three-way dilemma over accu-rate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We proposeStyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which high-fidelity geometry can be reliably restored via volume ren-dering.In addition, it transforms the grid features ac-cording to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant con-tent transformation that makes the transformation invari-*Shijian Lu is the corresponding author. ant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly re-duces memory footprint without degrading multi-view con-sistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry recon-struction and it can generalize to various new styles in a zero-shot manner. Project website: https://kunhao-liu.github.io/StyleRF/ 