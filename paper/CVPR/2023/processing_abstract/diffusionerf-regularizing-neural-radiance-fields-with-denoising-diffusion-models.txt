Under good conditions, Neural Radiance Fields (NeRFs) have shown impressive results on novel view synthesis tasks.NeRFs learn a sceneâ€™s color and density fields by minimiz-ing the photometric discrepancy between training views and differentiable renderings of the scene. Once trained from a sufficient set of views, NeRFs can generate novel views from arbitrary camera positions. However, the scene geom-etry and color fields are severely under-constrained, which can lead to artifacts, especially when trained with few input views.To alleviate this problem we learn a prior over scene geometry and color, using a denoising diffusion model (DDM). Our DDM is trained on RGBD patches of the syn-thetic Hypersim dataset and can be used to predict the gra-dient of the logarithm of a joint probability distribution of color and depth patches. We show that, these gradients of logarithms of RGBD patch priors serve to regularize geom-etry and color of a scene. During NeRF training, randomRGBD patches are rendered and the estimated gradient of the log-likelihood is backpropagated to the color and den-sity fields. Evaluations on LLFF, the most relevant dataset, show that our learned prior achieves improved quality in the reconstructed geometry and improved generalization to novel views. Evaluations on DTU show improved recon-struction quality among NeRF methods. 