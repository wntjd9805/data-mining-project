Video-text retrieval is an emerging stream in both com-puter vision and natural language processing communi-ties, which aims to find relevant videos given text queries.In this paper, we study the notoriously challenging task, i.e., Unsupervised Domain Adaptation Video-text Retrieval (UDAVR), wherein training and testing data come from dif-ferent distributions. Previous works merely alleviate the domain shift, which however overlook the pairwise mis-alignment issue in target domain, i.e., there exist no se-mantic relationships between target videos and texts. To tackle this, we propose a novel method named Dual Align-ment Domain Adaptation (DADA). Specifically, we first in-troduce the cross-modal semantic embedding to generate discriminative source features in a joint embedding space.Besides, we utilize the video and text domain adaptations to smoothly balance the minimization of the domain shifts.To tackle the pairwise misalignment in target domain, we propose the Dual Alignment Consistency (DAC) to fully ex-ploit the semantic information of both modalities in target domain. The proposed DAC adaptively aligns the video-text pairs which are more likely to be relevant in target do-main, enabling that positive pairs are increasing progres-sively and the noisy ones will potentially be aligned in the later stages. To that end, our method can generate more truly aligned target pairs and ensure the discriminability of target features. Compared with the state-of-the-art meth-ods, DADA achieves 20.18% and 18.61% relative improve-ments on R@1 under the setting of TGIF→MSR-VTT andTGIF→MSVD respectively, demonstrating the superiority of our method. 