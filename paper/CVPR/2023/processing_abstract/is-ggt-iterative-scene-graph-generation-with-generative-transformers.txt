Scene graphs provide a rich, structured representation of a scene by encoding the entities (objects) and their spa-tial relationships in a graphical format. This representation has proven useful in several tasks, such as question answer-ing, captioning, and even object detection, to name a few.Current approaches take a generation-by-classification ap-proach where the scene graph is generated through labeling of all possible edges between objects in a scene, which adds computational overhead to the approach. This work in-troduces a generative transformer-based approach to gen-erating scene graphs beyond link prediction. Using two transformer-based components, we first sample a possible scene graph structure from detected objects and their visual features. We then perform predicate classification on the sampled edges to generate the final scene graph. This ap-proach allows us to efficiently generate scene graphs from images with minimal inference overhead. Extensive exper-iments on the Visual Genome dataset demonstrate the effi-ciency of the proposed approach. Without bells and whis-tles, we obtain, on average, 20.7% mean recall (mR@100) across different settings for scene graph generation (SGG), outperforming state-of-the-art SGG approaches while offer-ing competitive performance to unbiased SGG approaches. 