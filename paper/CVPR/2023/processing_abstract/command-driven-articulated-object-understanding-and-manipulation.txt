We present Cart, a new approach towards articulated-object manipulations by human commands. Beyond the existing work that focuses on inferring articulation struc-tures, we further support manipulating articulated shapes to align them subject to simple command templates. The key of Cart is to utilize the prediction of object structures to connect visual observations with user commands for effective manipulations. It is achieved by encoding com-mand messages for motion prediction and a test-time adap-tation to adjust the amount of movement from only com-mand supervision. For a rich variety of object categories,Cart can accurately manipulate object shapes and outper-form the state-of-the-art approaches in understanding the inherent articulation structures. Also, it can well general-ize to unseen object categories and real-world objects. We hope Cart could open new directions for instructing ma-chines to operate articulated objects. Code is available at https:// github.com/ dvlab-research/ Cart. 