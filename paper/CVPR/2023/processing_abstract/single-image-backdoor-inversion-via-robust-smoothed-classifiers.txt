Backdoor inversion, the process of finding a backdoor“trigger” inserted into a machine learning model, has be-come the pillar of many backdoor detection and defense methods. Previous works on backdoor inversion often re-cover the backdoor through an optimization process to flip a support set of clean images into the target class. How-ever, it is rarely studied and understood how large this support set should be to recover a successful backdoor.In this work, we show that one can reliably recover the backdoor trigger with as few as a single image. Specifi-cally, we propose the SmoothInv method, which first con-structs a robust smoothed version of the backdoored clas-sifier and then performs guided image synthesis towards the target class to reveal the backdoor pattern. SmoothInv requires neither an explicit modeling of the backdoor via a mask variable, nor any complex regularization schemes, which has become the standard practice in backdoor in-version methods. We perform both quantitaive and qual-itative study on backdoored classifiers from previous pub-lished backdoor attacks. We demonstrate that compared to existing methods, SmoothInv is able to recover successful backdoors from single images, while maintaining high fi-delity to the original backdoor. We also show how we iden-tify the target backdoored class from the backdoored clas-sifier. Last, we propose and analyze two countermeasures to our approach and show that SmoothInv remains robust in the face of an adaptive attacker. Our code is available at https://github.com/locuslab/smoothinv. 