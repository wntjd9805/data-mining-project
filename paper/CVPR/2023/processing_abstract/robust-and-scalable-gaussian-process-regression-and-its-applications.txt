This paper introduces a robust and scalable Gaussian process regression (GPR) model via variational learning.This enables the application of Gaussian processes to a wide range of real data, which are often large-scale and contaminated by outliers. Towards this end, we employ a mixture likelihood model where outliers are assumed to be sampled from a uniform distribution. We next derive a variational formulation that jointly infers the mode of data, i.e., inlier or outlier, as well as hyperparameters by maximizing a lower bound of the true log marginal like-lihood. Compared to previous robust GPR, our formula-tion approximates the exact posterior distribution. The in-ducing variable approximation and stochastic variational inference are further introduced to our variational frame-work, extending our model to large-scale data. We ap-ply our model to two challenging real-world applications, namely feature matching and dense gene expression impu-tation. Extensive experiments demonstrate the superiority of our model in terms of robustness and speed. Notably, when matching 4k feature points, its inference is completed in milliseconds with almost no false matches. The code is at github.com/YifanLu2000/Robust-Scalable-GPR. 