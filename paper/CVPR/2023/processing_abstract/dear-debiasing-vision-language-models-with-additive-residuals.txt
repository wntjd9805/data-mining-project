Large pre-trained vision-language models (VLMs) re-duce the time for developing predictive models for various vision-grounded language downstream tasks by providing rich, adaptable image and text representations. However, these models suffer from societal biases owing to the skewed distribution of various identity groups in the training data.These biases manifest as the skewed similarity between the representations for speciﬁc text concepts and images of peo-ple of different identity groups and, therefore, limit the use-fulness of such models in real-world high-stakes applica-tions.In this work, we present DEAR (Debiasing withAdditive Residuals), a novel debiasing method that learns additive residual image representations to offset the orig-inal representations, ensuring fair output representations.In doing so, it reduces the ability of the representations to distinguish between the different identity groups. Further, we observe that the current fairness tests are performed*Equal Contribution on limited face image datasets that fail to indicate why a speciﬁc text concept should/should not apply to them. To bridge this gap and better evaluate DEAR, we introduce the PROTECTED ATTRIBUTE TAG ASSOCIATION (PATA) dataset – a new context-based bias benchmarking dataset for evaluating the fairness of large pre-trained VLMs. Ad-ditionally, PATA provides visual context for a diverse human population in different scenarios with both positive and neg-ative connotations. Experimental results for fairness and zero-shot performance preservation using multiple datasets demonstrate the efﬁcacy of our framework. The dataset is released here. 