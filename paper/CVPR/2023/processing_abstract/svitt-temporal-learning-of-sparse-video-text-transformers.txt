Do video-text transformers learn to model temporal re-lationships across frames? Despite their immense capacity and the abundance of multimodal training data, recent work has revealed the strong tendency of video-text models to-wards frame-based spatial representations, while temporal reasoning remains largely unsolved. In this work, we iden-tify several key challenges in temporal learning of video-text transformers: the spatiotemporal trade-off from limited network size; the curse of dimensionality for multi-frame modeling; and the diminishing returns of semantic informa-tion by extending clip length. Guided by these findings, we propose SViTT, a sparse video-text architecture that per-forms multi-frame reasoning with significantly lower cost than na¨ıve transformers with dense attention. Analogous to graph-based networks, SViTT employs two forms of sparsity: edge sparsity that limits the query-key commu-nications between tokens in self-attention, and node spar-sity that discards uninformative visual tokens. Trained with a curriculum which increases model sparsity with the clip length, SViTT outperforms dense transformer baselines on multiple video-text retrieval and question answering bench-marks, with a fraction of computational cost. Project page: http://svcl.ucsd.edu/projects/svitt. 