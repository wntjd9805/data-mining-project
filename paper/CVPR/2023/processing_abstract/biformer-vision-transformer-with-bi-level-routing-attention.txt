As the core building block of vision transformers, atten-tion is a powerful tool to capture long-range dependency.However, such power comes at a cost: it incurs a huge computation burden and heavy memory footprint as pair-wise token interaction across all spatial locations is com-puted. A series of works attempt to alleviate this problem by introducing handcrafted and content-agnostic sparsity into attention, such as restricting the attention operation to be inside local windows, axial stripes, or dilated windows.In contrast to these approaches, we propose a novel dy-namic sparse attention via bi-level routing to enable a moreﬂexible allocation of computations with content awareness.Speciﬁcally, for a query, irrelevant key-value pairs are ﬁrstﬁltered out at a coarse region level, and then ﬁne-grained token-to-token attention is applied in the union of remain-ing candidate regions (i.e., routed regions). We provide a simple yet effective implementation of the proposed bi-level routing attention, which utilizes the sparsity to save both computation and memory while involving only GPU-friendly dense matrix multiplications. Built with the pro-posed bi-level routing attention, a new general vision trans-former, named BiFormer, is then presented. As BiFormer attends to a small subset of relevant tokens in a query adap-tive manner without distraction from other irrelevant ones, it enjoys both good performance and high computational efﬁciency, especially in dense prediction tasks. Empirical results across several computer vision tasks such as image classiﬁcation, object detection, and semantic segmentation verify the effectiveness of our design. Code is available at https://github.com/rayleizhu/BiFormer. 