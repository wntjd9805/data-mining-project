Vision-centric joint perception and prediction (PnP) has become an emerging trend in autonomous driving research.It predicts the future states of the traffic participants in the surrounding environment from raw RGB images. How-ever, it is still a critical challenge to synchronize features obtained at multiple camera views and timestamps due to inevitable geometric distortions and further exploit those spatial-temporal features. To address this issue, we pro-pose a temporal birdâ€™s-eye-view pyramid transformer (TBP-Former) for vision-centric PnP, which includes two novel designs. First, a pose-synchronized BEV encoder is pro-posed to map raw image inputs with any camera pose at any time to a shared and synchronized BEV space for bet-ter spatial-temporal synchronization. Second, a spatial-temporal pyramid transformer is introduced to compre-hensively extract multi-scale BEV features and predict fu-ture BEV states with the support of spatial priors. Ex-tensive experiments on nuScenes dataset show that our proposed framework overall outperforms all state-of-the-art vision-based prediction methods. Code is available at: https://github.com/MediaBrain-SJTU/TBP-Former 