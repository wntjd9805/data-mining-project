Research on continual learning has recently led to a va-riety of work in unimodal community, however little atten-tion has been paid to multimodal tasks like visual question answering (VQA). In this paper, we establish a novel VQAContinual Learning setting named VQACL, which contains two key components: a dual-level task sequence where vi-sual and linguistic data are nested, and a novel composi-tion testing containing new skill-concept combinations. The former devotes to simulating the ever-changing multimodal datastream in real world and the latter aims at measuring models’ generalizability for cognitive reasoning. Based on our VQACL, we perform in-depth evaluations of ﬁve well-established continual learning methods, and observe that they suffer from catastrophic forgetting and have weak gen-eralizability. To address above issues, we propose a novel representation learning method, which leverages a sample-speciﬁc and a sample-invariant feature to learn represen-tations that are both discriminative and generalizable forVQA. Furthermore, by respectively extracting such repre-sentation for visual and textual input, our method can ex-plicitly disentangle the skill and concept. Extensive exper-imental results illustrate that our method signiﬁcantly out-performs existing models, demonstrating the effectiveness and compositionality of the proposed approach. The code is available at https://github.com/zhangxi1997/VQACL. 