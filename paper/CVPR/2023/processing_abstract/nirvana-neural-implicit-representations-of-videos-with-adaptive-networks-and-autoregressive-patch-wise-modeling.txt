Implicit Neural Representations (INR) have recently shown to be powerful tool for high-quality video compres-sion. However, existing works are are limiting as they do not exploit the temporal redundancy in videos, leading to a long encoding time. Additionally, these methods have fixed architectures which do not scale to longer videos or higher resolutions. To address these issues, we propose NIRVANA, which treats videos as groups of frames and fits separate networks to each group performing patch-wise prediction.The video representation is modeled autoregressively, with networks fit on a current group initialized using weights from the previous group’s model. To enhance efficiency, we quantize the parameters during training, requiring no post-hoc pruning or quantization. When compared with previ-ous works on the benchmark UVG dataset, NIRVANA im-proves encoding quality from 37.36 to 37.70 (in terms ofPSNR) and the encoding speed by 12×, while maintain-ing the same compression rate. In contrast to prior videoINR works which struggle with larger resolution and longer videos, we show that our algorithm scales naturally due to its patch-wise and autoregressive design. Moreover, our method achieves variable bitrate compression by adapting to videos with varying inter-frame motion. NIRVANA also achieves 6× decoding speed scaling well with more GPUs, making it practical for various deployment scenarios.1. 