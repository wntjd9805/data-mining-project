While adaptive learning rate methods, such as Adam, have achieved remarkable improvement in optimizing DeepNeural Networks (DNNs), they consider only the diago-nal elements of the full preconditioned matrix. Though the full-matrix preconditioned gradient methods theoretically have a lower regret bound, they are impractical for use to train DNNs because of the high complexity. In this paper, we present a general regret bound with a constrained full-matrix preconditioned gradient, and show that the updat-ing formula of the preconditioner can be derived by solving a cone-constrained optimization problem. With the block-diagonal and Kronecker-factorized constraints, a specific guide function can be obtained. By minimizing the up-per bound of the guide function, we develop a new DNN optimizer, termed AdaBK. A series of techniques, includ-ing statistics updating, dampening, efficient matrix inverse root computation, and gradient amplitude preservation, are developed to make AdaBK effective and efficient to imple-ment. The proposed AdaBK can be readily embedded into many existing DNN optimizers, e.g., SGDM and AdamW, and the corresponding SGDM BK and AdamW BK algo-rithms demonstrate significant improvements over existingDNN optimizers on benchmark vision tasks, including im-age classification, object detection and segmentation. The code is publicly available at https://github.com/Yonghongwei/AdaBK. 