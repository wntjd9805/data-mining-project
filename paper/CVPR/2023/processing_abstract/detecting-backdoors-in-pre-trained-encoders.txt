Self-supervised learning in computer vision trains on unlabeled data, such as images or (image, text) pairs, to obtain an image encoder that learns high-quality embed-dings for input data. Emerging backdoor attacks towards encoders expose crucial vulnerabilities of self-supervised learning, since downstream classifiers (even further trained on clean data) may inherit backdoor behaviors from en-coders. Existing backdoor detection methods mainly fo-cus on supervised learning settings and cannot handle pre-trained encoders especially when input labels are not avail-able. In this paper, we propose DECREE, the first back-door detection approach for pre-trained encoders, requir-ing neither classifier headers nor input labels. We eval-uate DECREE on over 400 encoders trojaned under 3 paradigms. We show the effectiveness of our method on im-age encoders pre-trained on ImageNet and OpenAIâ€™s CLIP 400 million image-text pairs. Our method consistently has a high detection accuracy even if we have only limited or no access to the pre-training dataset. Code is available at https://github.com/GiantSeaweed/DECREE. 