Masked image modeling (MIM) performs strongly in pre-training large vision Transformers (ViTs). However, small models that are critical for real-world applications can-not or only marginally benefit from this pre-training ap-proach. In this paper, we explore distillation techniques to transfer the success of large MIM-based pre-trained mod-els to smaller ones. We systematically study different op-tions in the distillation framework, including distilling tar-gets, losses, input, network regularization, sequential dis-tillation, etc, revealing that: 1) Distilling token relations is more effective than CLS token- and feature-based distil-lation; 2) An intermediate layer of the teacher network as target perform better than that using the last layer when the depth of the student mismatches that of the teacher; 3) Weak regularization is preferred; etc. With these find-ings, we achieve significant fine-tuning accuracy improve-ments over the scratch MIM pre-training on ImageNet-1K classification, using all the ViT-Tiny, ViT-Small, and ViT-base models, with +4.2%/+2.4%/+1.4% gains, respectively.Our TinyMIM model of base size achieves 52.2 mIoU inAE20K semantic segmentation, which is +4.1 higher than the MAE baseline. Our TinyMIM model of tiny size achieves 79.6% top-1 accuracy on ImageNet-1K image classifica-tion, which sets a new record for small vision models of the same size and computation budget. This strong perfor-mance suggests an alternative way for developing small vision Transformer models, that is, by exploring better train-ing methods rather than introducing inductive biases into architectures as in most previous works. Code is available at https://github.com/OliverRensu/TinyMIM. 