Pointly supervised instance segmentation (PSIS) learns to segment objects using a single point within the object extent as supervision. Challenged by the non-negligible se-mantic variance between object parts, however, the single supervision point causes semantic bias and false segmenta-tion. In this study, we propose an AttentionShift method, to solve the semantic bias issue by iteratively decomposing the instance attention map to parts and estimating fine-grained semantics of each part. AttentionShift consists of two mod-ules plugged on the vision transformer backbone: (i) to-ken querying for pointly supervised attention map genera-tion, and (ii) key-point shift, which re-estimates part-based attention maps by key-point filtering in the feature space.These two steps are iteratively performed so that the part-based attention maps are optimized spatially as well as in the feature space to cover full object extent. Experiments on PASCAL VOC and MS COCO 2017 datasets show thatAttentionShift respectively improves the state-of-the-art of by 7.7% and 4.8% under mAP@0.5, setting a solid PSIS baseline using vision transformer.Figure 1.Comparison of existing methods with the pro-posed AttentionShift. Upper: The class activation map (CAM) method [51] suffers partial activation for the lack of spatial constraint. The self-attention map generated by vision trans-former [16] encounters false and missed object parts. Lower: At-tentionShift literately optimizes part-based attention maps (indi-cated by key-points) by shifting the key-points in the feature space to precisely localize the full object extent. (Best viewed in color) 