We propose DisCo-CLIP, a distributed memory-efficientCLIP training approach, to reduce the memory consump-tion of contrastive loss when training contrastive learning models. Our approach decomposes the contrastive loss and its gradient computation into two parts, one to cal-culate the intra-GPU gradients and the other to compute the inter-GPU gradients. According to our decomposition, only the intra-GPU gradients are computed on the cur-rent GPU, while the inter-GPU gradients are collected via all reduce from other GPUs instead of being repeatedly computed on every GPU. In this way, we can reduce theGPU memory consumption of contrastive loss computation fromN ), where B and N are the batch size and the number of GPUs used for training. Such a dis-tributed solution is mathematically equivalent to the orig-inal non-distributed contrastive loss computation, without sacrificing any computation accuracy. It is particularly ef-ficient for large-batch CLIP training. For instance, DisCo-CLIP can enable contrastive training of a ViT-B/32 model with a batch size of 32K or 196K using 8 or 64 A100 40GBGPUs, compared with the original CLIP solution which re-quires 128 A100 40GB GPUs to train a ViT-B/32 model with a batch size of 32K. (B2) to ( B2OO 