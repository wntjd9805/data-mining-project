Multimodal semantic understanding often has to deal with uncertainty, which means the obtained messages tend to refer to multiple targets. Such uncertainty is problematic for our interpretation, including inter- and intra-modal un-certainty. Little effort has studied the modeling of this uncer-tainty, particularly in pre-training on unlabeled datasets and fine-tuning in task-specific downstream datasets. In this paper, we project the representations of all modali-ties as probabilistic distributions via a Probability Distri-bution Encoder (PDE) by utilizing sequence-level interac-tions. Compared to the existing deterministic methods, such uncertainty modeling can convey richer multimodal seman-tic information and more complex relationships. Further-more, we integrate uncertainty modeling with popular pre-training frameworks and propose suitable pre-training tasks:Distribution-based Vision-Language Contrastive learning (D-VLC), Distribution-based Masked Language Modeling (D-MLM), and Distribution-based Image-Text Matching (D-ITM). The fine-tuned models are applied to challenging downstream tasks, including image-text retrieval, visual question answering, visual reasoning, and visual entailment, and achieve state-of-the-art results.Figure 1. Multimodal uncertainties and an example for language un-certainty (b) by modeling as point representations and distribution representations. The images and text are from MSCOCO [30]. 