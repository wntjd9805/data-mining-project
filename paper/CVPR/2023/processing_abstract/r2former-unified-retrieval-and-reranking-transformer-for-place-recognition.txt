Visual Place Recognition (VPR) estimates the location of query images by matching them with images in a reference database. Conventional methods generally adopt aggre-gated CNN features for global retrieval and RANSAC-based geometric verification for reranking. However, RANSAC only employs geometric information but ignores other pos-sible information that could be useful for reranking, e.g. lo-cal feature correlations, and attention values. In this paper, we propose a unified place recognition framework that han-dles both retrieval and reranking with a novel transformer model, named R2Former. The proposed reranking module takes feature correlation, attention value, and xy coordi-nates into account, and learns to determine whether the image pair is from the same location. The whole pipeline is end-to-end trainable and the reranking module alone can also be adopted on other CNN or transformer back-bones as a generic component. Remarkably, R2Former significantly outperforms state-of-the-art methods on ma-jor VPR datasets with much less inference time and mem-ory consumption.It also achieves the state-of-the-art on the hold-out MSLS challenge set and could serve as a sim-ple yet strong solution for real-world large-scale applica-tions. Experiments also show vision transformer tokens are comparable and sometimes better than CNN local fea-tures on local matching. The code is released at https://github.com/Jeff-Zilence/R2Former. 