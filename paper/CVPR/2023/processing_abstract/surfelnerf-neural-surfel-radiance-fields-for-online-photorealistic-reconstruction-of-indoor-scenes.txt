Online reconstructing and rendering of large-scale in-door scenes is a long-standing challenge. SLAM-based methods can reconstruct 3D scene geometry progressively in real time but can not render photorealistic results. WhileNeRF-based methods produce promising novel view syn-thesis results, their long offline optimization time and lack of geometric constraints pose challenges to efficiently han-dling online input. Inspired by the complementary advan-tages of classical 3D reconstruction and NeRF, we thus in-vestigate marrying explicit geometric representation withNeRF rendering to achieve efficient online reconstruction and high-quality rendering. We introduce SurfelNeRF, a variant of neural radiance field which employs a flexible and scalable neural surfel representation to store geomet-ric attributes and extracted appearance features from input images. We further extend the conventional surfel-based fusion scheme to progressively integrate incoming input frames into the reconstructed global neural scene represen-tation. In addition, we propose a highly-efficient differen-tiable rasterization scheme for rendering neural surfel radi-ance fields, which helps SurfelNeRF achieve 10Ã— speedups in training and inference time, respectively. Experimental results show that our method achieves the state-of-the-art 23.82 PSNR and 29.58 PSNR on ScanNet in feedforward inference and per-scene optimization settings, respectively.1 