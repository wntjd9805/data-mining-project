Recovering 3D human mesh from videos has recently made signiﬁcant progress. However, most of the existing methods focus on the temporal consistency of videos, while ignoring the spatial representation in complex scenes, thus failing to recover a reasonable and smooth human mesh sequence under extreme illumination and chaotic back-grounds. To alleviate this problem, we propose a two-stage co-segmentation network based on discriminative rep-resentation for recovering human body meshes from videos.Speciﬁcally, the ﬁrst stage of the network segments the video spatial domain to spotlight spatially ﬁne-grained informa-tion, and then learns and enhances the intra-frame discrimi-native representation through a dual-excitation mechanism and a frequency domain enhancement module, while sup-*represents corresponding author, † represents the equal contribution. pressing irrelevant information (e.g., background). The sec-ond stage focuses on temporal context by segmenting the video temporal domain, and models inter-frame discrimina-tive representation via a dynamic integration strategy. Fur-ther, to efﬁciently generate reasonable human discrimina-tive actions, we carefully elaborate a landmark anchor area loss to constrain the variation of the human motion area.Extensive experimental results on large publicly available datasets indicate superiority in comparison with most state-of-the-art. The Code will be made public. 