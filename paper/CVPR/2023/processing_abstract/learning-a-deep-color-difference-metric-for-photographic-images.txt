Most well-established and widely used color differ-ence (CD) metrics are handcrafted and subject-calibrated against uniformly colored patches, which do not general-ize well to photographic images characterized by natural scene complexities. Constructing CD formulae for photo-graphic images is still an active research topic in imag-ing/illumination, vision science, and color science commu-nities. In this paper, we aim to learn a deep CD metric for photographic images with four desirable properties. First, it well aligns with the observations in vision science that color and form are linked inextricably in visual cortical processing. Second, it is a proper metric in the mathemat-ical sense. Third, it computes accurate CDs between pho-tographic images, differing mainly in color appearances.Fourth, it is robust to mild geometric distortions (e.g., trans-lation or due to parallax), which are often present in pho-tographic images of the same scene captured by different digital cameras. We show that all four properties can be satisfied at once by learning a multi-scale autoregressive normalizing flow for feature transform, followed by the Eu-clidean distance which is linearly proportional to the hu-man perceptual CD. Quantitative and qualitative experi-ments on the large-scale SPCD dataset demonstrate the promise of the learned CD metric. Source code is available at https://github.com/haoychen3/CD-Flow. 