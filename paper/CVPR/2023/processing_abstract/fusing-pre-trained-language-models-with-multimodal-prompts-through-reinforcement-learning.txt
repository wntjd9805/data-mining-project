Language models are capable of commonsense reason-ing: while domain-specific models can learn from ex-plicit knowledge (e.g. commonsense graphs [6], ethical norms [25]), and larger models like GPT-3 [7] mani-fest broad commonsense reasoning capacity. Can their knowledge be extended to multimodal inputs such as im-ages and audio without paired domain data?In this work, we proposeESPER (Extending Sensory PErception with Reinforcement learning) which enables text-only pre-trained models to address multimodal tasks such as visual commonsense reasoning. Our key novelty is to use rein-forcement learning to align multimodal inputs to language model generations without direct supervision: for example, our reward optimization relies only on cosine similarity de-rived from CLIP [52] and requires no additional paired (image, text) data. Experiments demonstrate that ESPER outperforms baselines and prior work on a variety of mul-timodal text generation tasks ranging from captioning to commonsense reasoning; these include a new benchmark we collect and release, the ESP dataset, which tasks mod-els with generating the text of several different domains for each image. Our code and data are publicly released at https://github.com/JiwanChung/esper. 