Recent years have witnessed significant developments in point cloud processing, including classification and seg-mentation. However, supervised learning approaches need a lot of well-labeled data for training, and annotation is labor- and time-intensive. Self-supervised learning, on the other hand, uses unlabeled data, and pre-trains a back-bone with a pretext task to extract latent representations to be used with the downstream tasks. Compared to 2D im-ages, self-supervised learning of 3D point clouds is under-explored. Existing models, for self-supervised learning of 3D point clouds, rely on a large number of data sam-ples, and require significant amount of computational re-sources and training time. To address this issue, we pro-pose a novel contrastive learning approach, referred to asToThePoint. Different from traditional contrastive learning methods, which maximize agreement between features ob-tained from a pair of point clouds formed only with dif-ferent types of augmentation, ToThePoint also maximizes the agreement between the permutation invariant features and features discarded after max pooling. We first per-form self-supervised learning on the ShapeNet dataset, and then evaluate the performance of the network on different downstream tasks.In the downstream task experiments, performed on the ModelNet40, ModelNet40C, Scanob-jectNN and ShapeNet-Part datasets, our proposed ToThe-Point achieves competitive, if not better results compared to the state-of-the-art baselines, and does so with significantly less training time (200 times faster than baselines). 