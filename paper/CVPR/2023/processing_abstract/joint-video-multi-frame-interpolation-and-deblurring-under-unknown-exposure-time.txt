Natural videos captured by consumer cameras often suf-fer from low framerate and motion blur due to the combi-nation of dynamic scene complexity, lens and sensor imper-fection, and less than ideal exposure setting. As a result, computational methods that jointly perform video frame in-terpolation and deblurring begin to emerge with the unreal-istic assumption that the exposure time is known and fixed.In this work, we aim ambitiously for a more realistic and challenging task - joint video multi-frame interpolation and deblurring under unknown exposure time. Toward this goal, we first adopt a variant of supervised contrastive learn-ing to construct an exposure-aware representation from in-put blurred frames. We then train two U-Nets for intra-motion and inter-motion analysis, respectively, adapting to the learned exposure representation via gain tuning. We finally build our video reconstruction network upon the ex-posure and motion representation by progressive exposure-adaptive convolution and motion refinement. Extensive ex-periments on both simulated and real-world datasets show that our optimized method achieves notable performance gains over the state-of-the-art on the joint video ×8 interpo-lation and deblurring task. Moreover, on the seemingly im-plausible ×16 interpolation task, our method outperforms existing methods by more than 1.5 dB in terms of PSNR. 