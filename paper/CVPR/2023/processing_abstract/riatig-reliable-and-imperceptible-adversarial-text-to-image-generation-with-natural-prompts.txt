The field of text-to-image generation has made remark-able strides in creating high-fidelity and photorealistic im-ages. As this technology gains popularity, there is a grow-ing concern about its potential security risks. However, there has been limited exploration into the robustness of these models from an adversarial perspective. Existing re-search has primarily focused on untargeted settings, and lacks holistic consideration for reliability (attack success rate) and stealthiness (imperceptibility).In this paper, we propose RIATIG, a reliable and im-perceptible adversarial attack against text-to-image mod-els via inconspicuous examples. By formulating the exam-ple crafting as an optimization process and solving it using a genetic-based method, our proposed attack can generate imperceptible prompts for text-to-image generation models in a reliable way. Evaluation of six popular text-to-image generation models demonstrates the efficiency and stealthi-ness of our attack in both white-box and black-box settings.To allow the community to build on top of our findings, weâ€™ve made the artifacts available1. 