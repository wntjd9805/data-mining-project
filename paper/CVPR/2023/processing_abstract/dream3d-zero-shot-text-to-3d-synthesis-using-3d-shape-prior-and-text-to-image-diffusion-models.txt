Recent CLIP-guided 3D optimization methods, such asDreamFields [19] and PureCLIPNeRF [24], have achieved impressive results in zero-shot text-to-3D synthesis. How-ever, due to scratch training and random initialization with-out prior knowledge, these methods often fail to generate accurate and faithful 3D structures that conform to the in-put text. In this paper, we make the first attempt to introduce explicit 3D shape priors into the CLIP-guided 3D optimiza-tion process. Specifically, we first generate a high-quality 3D shape from the input text in the text-to-shape stage as a 3D shape prior. We then use it as the initialization of a neu-ral radiance field and optimize it with the full prompt. To address the challenging text-to-shape generation task, we present a simple yet effective approach that directly bridges the text and image modalities with a powerful text-to-image diffusion model. To narrow the style domain gap between the images synthesized by the text-to-image diffusion model and shape renderings used to train the image-to-shape generator, we further propose to jointly optimize a learn-able text prompt and fine-tune the text-to-image diffusion model for rendering-style image generation. Our method,*Work done during an internship at ARC Lab, Tencent PCG.â€ Corresponding Author.Dream3D, is capable of generating imaginative 3D con-tent with superior visual quality and shape accuracy com-pared to state-of-the-art methods. Our project page is at https://bluestyle97.github.io/dream3d/. 