Scene graph generation (SGG) aims to abstract an im-age into a graph structure, by representing objects as graph nodes and their relations as labeled edges. However, two knotty obstacles limit the practicability of current SGG methods in real-world scenarios: 1) training SGG mod-els requires time-consuming ground-truth annotations, and 2) the closed-set object categories make the SGG mod-els limited in their ability to recognize novel objects out-side of training corpora. To address these issues, we nov-elly exploit a powerful pre-trained visual-semantic space (VSS) to trigger language-supervised and open-vocabularySGG in a simple yet effective manner. Specifically, cheap scene graph supervision data can be easily obtained by parsing image language descriptions into semantic graphs.Next, the noun phrases on such semantic graphs are di-rectly grounded over image regions through region-word alignment in the pre-trained VSS. In this way, we enable open-vocabulary object detection by performing object cat-egory name grounding with a text prompt in this VSS. On the basis of visually-grounded objects, the relation repre-sentations are naturally built for relation recognition, pur-suing open-vocabulary SGG. We validate our proposed ap-proach with extensive experiments on the Visual Genome benchmark across various SGG scenarios (i.e., supervised/ language-supervised, closed-set / open-vocabulary). Con-sistent superior performances are achieved compared with existing methods, demonstrating the potential of exploiting pre-trained VSS for SGG in more practical scenarios. 