We study the problem of how to train a “personalization-friendly” model such that given only the task descriptions, the model can be adapted to different end-users’ needs, e.g., for accurately classifying different subsets of objects. One baseline approach is to train a “generic” model for classi-fying a wide range of objects, followed by class selection. In our experiments, we however found it suboptimal, perhaps because the model’s weights are kept frozen without being personalized. To address this drawback, we propose Train-once-for-All PERsonalization (TAPER), a framework that is trained just once and can later customize a model for different end-users given their task descriptions. TAPER learns a set of “basis” models and a mixer predictor, such that given the task description, the weights (not the pre-dictions!) of the basis models can be on the fly combined into a single “personalized” model. Via extensive experi-ments on multiple recognition tasks, we show that TAPER consistently outperforms the baseline methods in achieving a higher personalized accuracy. Moreover, we show thatTAPER can synthesize a much smaller model to achieve comparable performance to a huge generic model, mak-ing it “deployment-friendly” to resource-limited end de-vices. Interestingly, even without end-users’ task descrip-tions, TAPER can still be specialized to the deployed con-text based on its past predictions, making it even more“personalization-friendly”. 