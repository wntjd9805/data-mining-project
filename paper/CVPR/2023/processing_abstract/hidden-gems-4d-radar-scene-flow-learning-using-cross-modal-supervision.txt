This work proposes a novel approach to 4D radar-based scene flow estimation via cross-modal learning. Our ap-proach is motivated by the co-located sensing redundancy in modern autonomous vehicles. Such redundancy implic-itly provides various forms of supervision cues to the radar scene flow estimation. Specifically, we introduce a multi-task model architecture for the identified cross-modal learn-ing problem and propose loss functions to opportunistically engage scene flow estimation using multiple cross-modal constraints for effective model training. Extensive experi-ments show the state-of-the-art performance of our method and demonstrate the effectiveness of cross-modal super-vised learning to infer more accurate 4D radar scene flow.We also show its usefulness to two subtasks - motion seg-mentation and ego-motion estimation. Our source code will be available on https://github.com/Toytiny/CMFlow. 