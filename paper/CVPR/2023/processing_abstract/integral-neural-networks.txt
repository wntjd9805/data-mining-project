We introduce a new family of deep neural networks, where instead of the conventional representation of net-work layers as N -dimensional weight tensors, we use a continuous layer representation along the filter and chan-nel dimensions. We call such networks Integral Neural Net-works (INNs). In particular, the weights of INNs are rep-resented as continuous functions defined on N -dimensional hypercubes, and the discrete transformations of inputs to the layers are replaced by continuous integration opera-tions, accordingly. During the inference stage, our con-tinuous layers can be converted into the traditional tensor representation via numerical integral quadratures. Such kind of representation allows the discretization of a net-work to an arbitrary size with various discretization in-tervals for the integral kernels. This approach can be ap-plied to prune the model directly on an edge device while suffering only a small performance loss at high rates of structural pruning without any fine-tuning. To evaluate the practical benefits of our proposed approach, we have con-ducted experiments using various neural network architec-tures on multiple tasks. Our reported results show that the proposed INNs achieve the same performance with their conventional discrete counterparts, while being able to pre-serve approximately the same performance (2% accuracy loss for ResNet18 on Imagenet) at a high rate (up to 30%) of structural pruning without fine-tuning, compared to 65% accuracy loss of the conventional pruning methods under the same conditions. Code is available at gitee. 