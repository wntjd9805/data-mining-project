Due to the lack of temporal annotation, current Weakly-supervised Temporal Action Localization (WTAL) methods are generally stuck into over-complete or incomplete local-In this paper, we aim to leverage the text infor-ization. i.e., (a) the dis-mation to boost WTAL from two aspects, criminative objective to enlarge the inter-class difference, thus reducing the over-complete; (b) the generative objec-tive to enhance the intra-class integrity, thus ﬁnding more complete temporal boundaries. For the discriminative ob-jective, we propose a Text-Segment Mining (TSM) mecha-nism, which constructs a text description based on the ac-tion class label, and regards the text as the query to mine all class-related segments. Without the temporal annotation of actions, TSM compares the text query with the entire videos across the dataset to mine the best matching segments while ignoring irrelevant ones. Due to the shared sub-actions in different categories of videos, merely applying TSM is too strict to neglect the semantic-related segments, which results in incomplete localization. We further introduce a generative objective named Video-text Language Comple-tion (VLC), which focuses on all semantic-related segments from videos to complete the text sentence. We achieve the state-of-the-art performance on THUMOS14 and Activi-tyNet1.3. Surprisingly, we also ﬁnd our proposed method can be seamlessly applied to existing methods, and improve their performances with a clear margin. The code is avail-able at https://github.com/lgzlIlIlI/Boosting-WTAL. 