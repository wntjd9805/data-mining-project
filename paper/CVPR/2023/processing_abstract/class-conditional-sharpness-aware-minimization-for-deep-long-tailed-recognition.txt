It’s widely acknowledged that deep learning models withﬂatter minima in its loss landscape tend to generalize bet-ter. However, such property is under-explored in deep long-tailed recognition (DLTR), a practical problem where the model is required to generalize equally well across all classes when trained on highly imbalanced label distribu-tion. In this paper, through empirical observations, we ar-gue that sharp minima are in fact prevalent in deep long-tailed models, whereas na¨ıve integration of existing ﬂatten-ing operations into long-tailed learning algorithms brings little improvement. Instead, we propose an effective two-stage sharpness-aware optimization approach based on the decoupling paradigm in DLTR. In the ﬁrst stage, both the feature extractor and classiﬁer are trained under param-eter perturbations at a class-conditioned scale, which is theoretically motivated by the characteristic radius of ﬂatIn the sec-minima under the PAC-Bayesian framework. ond stage, we generate adversarial features with class-balanced sampling to further robustify the classiﬁer with the backbone frozen. Extensive experiments on multiple long-tailed visual recognition benchmarks show that, our pro-posed Class-Conditional Sharpness-Aware Minimization (CC-SAM), achieves competitive performance compared to the state-of-the-arts. Code is available at https:// github.com/zzpustc/CC-SAM . 