Quantizing images into discrete representations has been a fundamental problem in unified generative modeling.Predominant approaches learn the discrete representation either in a deterministic manner by selecting the best-matching token or in a stochastic manner by sampling from a predicted distribution. However, deterministic quantiza-tion suffers from severe codebook collapse and misalign-ment with inference stage while stochastic quantization suf-fers from low codebook utilization and perturbed recon-struction objective. This paper presents a regularized vec-tor quantization framework that allows to mitigate above issues effectively by applying regularization from two per-spectives. The first is a prior distribution regularization which measures the discrepancy between a prior token dis-tribution and the predicted token distribution to avoid code-book collapse and low codebook utilization. The second is a stochastic mask regularization that introduces stochastic-ity during quantization to strike a good balance between in-ference stage misalignment and unperturbed reconstruction objective. In addition, we design a probabilistic contrastive loss which serves as a calibrated metric to further miti-gate the perturbed reconstruction objective. Extensive ex-periments show that the proposed quantization framework outperforms prevailing vector quantization methods con-sistently across different generative models including auto-regressive models and diffusion models. 