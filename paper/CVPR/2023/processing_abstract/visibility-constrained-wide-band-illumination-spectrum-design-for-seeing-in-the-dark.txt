Seeing-in-the-dark is one of the most important and challenging computer vision tasks due to its wide appli-cations and extreme complexities of in-the-wild scenar-ios. Existing arts can be mainly divided into two threads: 1) RGB-dependent methods restore information using de-graded RGB inputs only (e.g., low-light enhancement), 2)RGB-independent methods translate images captured under auxiliary near-infrared (NIR) illuminants into RGB domain (e.g., NIR2RGB translation). The latter is very attractive since it works in complete darkness and the illuminants are visually friendly to naked eyes, but tends to be unstable due to its intrinsic ambiguities. In this paper, we try to robustifyNIR2RGB translation by designing the optimal spectrum of auxiliary illumination in the wide-band VIS-NIR range, while keeping visual friendliness. Our core idea is to quan-tify the visibility constraint implied by the human vision sys-tem and incorporate it into the design pipeline. By model-ing the formation process of images in the VIS-NIR range, the optimal multiplexing of a wide range of LEDs is auto-matically designed in a fully differentiable manner, within the feasible region defined by the visibility constraint. We also collect a substantially expanded VIS-NIR hyperspec-tral image dataset for experiments by using a customized 50-band filter wheel. Experimental results show that the task can be significantly improved by using the optimized wide-band illumination than using NIR only. Codes Avail-able: https://github.com/MyNiuuu/VCSD. 