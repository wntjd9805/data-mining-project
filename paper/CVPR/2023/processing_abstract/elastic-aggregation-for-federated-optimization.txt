Federated learning enables the privacy-preserving train-ing of neural network models using real-world data across distributed clients. FedAvg has become the preferred opti-mizer for federated learning because of its simplicity and effectiveness. FedAvg uses naïve aggregation to update the server model, interpolating client models based on the num-ber of instances used in their training. However, naïve ag-gregation suffers from client drift when the data is heteroge-nous (non-IID), leading to unstable and slow convergence.In this work, we propose a novel aggregation approach, elastic aggregation, to overcome these issues. Elastic ag-gregation interpolates client models adaptively according to parameter sensitivity, which is measured by computing how much the overall prediction function output changes when each parameter is changed. This measurement is performed in an unsupervised and online manner. Elastic aggregation reduces the magnitudes of updates to the more sensitive pa-rameters so as to prevent the server model from drifting to any one client distribution, and conversely boosts updates to the less sensitive parameters to better explore different client distributions. Empirical results on real and synthetic data as well as analytical results show that elastic aggregation leads to efficient training in both convex and non-convex settings while being fully agnostic to client heterogeneity and robust to large numbers of clients, partial participation, and imbalanced data. Finally, elastic aggregation works well with other federated optimizers and achieves significant improvements across the board. 