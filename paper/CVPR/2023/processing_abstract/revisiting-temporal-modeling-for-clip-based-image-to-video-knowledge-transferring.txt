Image-text pretrained models, e.g., CLIP, have shown impressive general multi-modal knowledge learned from large-scale image-text data pairs, thus attracting increas-ing attention for their potential to improve visual represen-tation learning in the video domain. In this paper, based on the CLIP model, we revisit temporal modeling in the context of image-to-video knowledge transferring, which is the key point for extending image-text pretrained models to the video domain. We find that current temporal model-ing mechanisms are tailored to either high-level semantic-dominant tasks (e.g., retrieval) or low-level visual pattern-dominant tasks (e.g., recognition), and fail to work on the two cases simultaneously. The key difficulty lies in modeling temporal dependency while taking advantage of both high-level and low-level knowledge in CLIP model. To tackle this problem, we present Spatial-Temporal Auxiliary Net-work (STAN) â€“ a simple and effective temporal modeling mechanism extending CLIP model to diverse video tasks.Specifically, to realize both low-level and high-level knowl-edge transferring, STAN adopts a branch structure with decomposed spatial-temporal modules that enable multi-level CLIP features to be spatial-temporally contextual-ized. We evaluate our method on two representative video tasks: Video-Text Retrieval and Video Recognition. Exten-sive experiments demonstrate the superiority of our model over the state-of-the-art methods on various datasets, in-cluding MSR-VTT, DiDeMo, LSMDC, MSVD, Kinetics-400, and Something-Something-V2. Codes will be available at https://github.com/farewellthree/STAN 