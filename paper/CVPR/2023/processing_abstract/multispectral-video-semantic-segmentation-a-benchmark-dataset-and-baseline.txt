Robust and reliable semantic segmentation in complex scenes is crucial for many real-life applications such as au-tonomous safe driving and nighttime rescue. In most ap-proaches, it is typical to make use of RGB images as in-put. They however work well only in preferred weather conditions; when facing adverse conditions such as rainy, overexposure, or low-light, they often fail to deliver sat-isfactory results. This has led to the recent investigation into multispectral semantic segmentation, where RGB and thermal infrared (RGBT) images are both utilized as input.This gives rise to significantly more robust segmentation of image objects in complex scenes and under adverse condi-tions. Nevertheless, the present focus in single RGBT im-age input restricts existing methods from well addressing dynamic real-world scenes.Motivated by the above observations, in this paper, we set out to address a relatively new task of semantic seg-mentation of multispectral video input, which we refer to as Multispectral Video Semantic Segmentation, or MVSS in short. An in-house MVSeg dataset is thus curated, con-sisting of 738 calibrated RGB and thermal videos, accom-panied by 3,545 fine-grained pixel-level semantic annota-âˆ—Corresponding author. tions of 26 categories. Our dataset contains a wide range of challenging urban scenes in both daytime and nighttime.Moreover, we propose an effective MVSS baseline, dubbedMVNet, which is to our knowledge the first model to jointly learn semantic representations from multispectral and tem-poral contexts. Comprehensive experiments are conducted using various semantic segmentation models on the MVSeg dataset. Empirically, the engagement of multispectral video input is shown to lead to significant improvement in seman-tic segmentation; the effectiveness of our MVNet baseline has also been verified. 