Recent transformer-based solutions have shown great success in 3D human pose estimation. Nevertheless, to cal-culate the joint-to-joint affinity matrix, the computational cost has a quadratic growth with the increasing number of joints. Such drawback becomes even worse especially for pose estimation in a video sequence, which necessitates spatio-temporal correlation spanning over the entire video.In this paper, we facilitate the issue by decomposing cor-relation learning into space and time, and present a novelSpatio-Temporal Criss-cross attention (STC) block. Tech-nically, STC first slices its input feature into two partitions evenly along the channel dimension, followed by perform-ing spatial and temporal attention respectively on each par-tition. STC then models the interactions between joints in an identical frame and joints in an identical trajectory simulta-neously by concatenating the outputs from attention layers.On this basis, we devise STCFormer by stacking multipleSTC blocks and further integrate a new Structure-enhancedPositional Embedding (SPE) into STCFormer to take the structure of human body into consideration. The embedding function consists of two components: spatio-temporal con-volution around neighboring joints to capture local struc-ture, and part-aware embedding to indicate which part each joint belongs to. Extensive experiments are conducted onHuman3.6M and MPI-INF-3DHP benchmarks, and supe-rior results are reported when comparing to the state-of-the-art approaches. More remarkably, STCFormer achieves to-date the best published performance: 40.5mm P1 error on the challenging Human3.6M dataset. 