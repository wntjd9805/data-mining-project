This paper presents a simple yet effective approach that improves continual test-time adaptation (TTA) in a memory-efficient manner. TTA may primarily be conducted on edge devices with limited memory, so reducing memory is cru-cial but has been overlooked in previous TTA studies. In addition, long-term adaptation often leads to catastrophic forgetting and error accumulation, which hinders apply-ing TTA in real-world deployments. Our approach con-sists of two components to address these issues. First, we present lightweight meta networks that can adapt the frozen original networks to the target domain. This novel archi-tecture minimizes memory consumption by decreasing the size of intermediate activations required for backpropaga-tion. Second, our novel self-distilled regularization controls the output of the meta networks not to deviate significantly from the output of the frozen original networks, thereby preserving well-trained knowledge from the source domain.Without additional memory, this regularization prevents er-ror accumulation and catastrophic forgetting, resulting in stable performance even in long-term test-time adaptation.We demonstrate that our simple yet effective strategy out-performs other state-of-the-art methods on various bench-marks for image classification and semantic segmentation tasks. Notably, our proposed method with ResNet-50 andWideResNet-40 takes 86% and 80% less memory than the recent state-of-the-art method, CoTTA. 