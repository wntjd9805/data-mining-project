Self-supervised learning can extract representations of good quality from solely unlabeled data, which is ap-pealing for point cloud videos due to their high labelling cost.In this paper, we propose a contrastive mask pre-diction (PointCMP) framework for self-supervised learn-ing on point cloud videos. Specifically, our PointCMP em-ploys a two-branch structure to achieve simultaneous learn-ing of both local and global spatio-temporal information.On top of this two-branch structure, a mutual similarity based augmentation module is developed to synthesize hard samples at the feature level. By masking dominant tokens and erasing principal channels, we generate hard samples to facilitate learning representations with better discrimi-nation and generalization performance. Extensive experi-ments show that our PointCMP achieves the state-of-the-art performance on benchmark datasets and outperforms exist-ing full-supervised counterparts. Transfer learning results demonstrate the superiority of the learned representations across different datasets and tasks. 