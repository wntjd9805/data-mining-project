Any-scale image synthesis offers an efficient and scal-able solution to synthesize photo-realistic images at any scale, even going beyond 2K resolution. However, exist-ing GAN-based solutions depend excessively on convolu-tions and a hierarchical architecture, which introduce in-consistency and the “texture sticking” issue when scal-ing the output resolution. From another perspective, INR-based generators are scale-equivariant by design, but their huge memory footprint and slow inference hinder these net-works from being adopted in large-scale or real-time sys-tems.In this work, we propose Column-Row EntangledPixel Synthesis (CREPS), a new generative model that is both efficient and scale-equivariant without using any spa-tial convolutions or coarse-to-fine design. To save memory footprint and make the system scalable, we employ a novel bi-line representation that decomposes layer-wise feature maps into separate “thick” column and row encodings. Ex-periments on various datasets, including FFHQ, LSUN-Church, MetFaces, and Flickr-Scenery, confirm CREPS’ ability to synthesize scale-consistent and alias-free images at any arbitrary resolution with proper training and infer-ence speed. Code is available at https://github. com/VinAIResearch/CREPS. 