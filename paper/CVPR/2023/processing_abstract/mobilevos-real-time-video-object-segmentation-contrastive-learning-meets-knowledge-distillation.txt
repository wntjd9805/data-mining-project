This paper tackles the problem of semi-supervised video object segmentation on resource-constrained devices, such as mobile phones. We formulate this problem as a dis-tillation task, whereby we demonstrate that small space-time-memory networks with finite memory can achieve com-petitive results with state of the art, but at a fraction of the computational cost (32 milliseconds per frame on aSamsung Galaxy S22). Specifically, we provide a theo-retically grounded framework that unifies knowledge dis-tillation with supervised contrastive representation learn-ing. These models are able to jointly benefit from both pixel-wise contrastive learning and distillation from a pre-trained teacher. We validate this loss by achieving compet-itive J &F to state of the art on both the standard DAVIS and YouTube benchmarks, despite running up to ˆ5 faster, and with ˆ32 fewer parameters. 