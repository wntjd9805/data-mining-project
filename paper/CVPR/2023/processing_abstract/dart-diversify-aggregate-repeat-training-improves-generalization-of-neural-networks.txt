Generalization of Neural Networks is crucial for deploy-ing them safely in the real world. Common training strate-gies to improve generalization involve the use of data aug-mentations, ensembling and model averaging. In this work, we first establish a surprisingly simple but strong bench-mark for generalization which utilizes diverse augmenta-tions within a training minibatch, and show that this can learn a more balanced distribution of features. Further, we propose Diversify-Aggregate-Repeat Training (DART) strategy that first trains diverse models using different aug-mentations (or domains) to explore the loss basin, and fur-ther Aggregates their weights to combine their expertise and obtain improved generalization. We find that Repeating the step of Aggregation throughout training improves the over-all optimization trajectory and also ensures that the indi-vidual models have sufficiently low loss barrier to obtain improved generalization on combining them. We theoret-ically justify the proposed approach and show that it in-deed generalizes better. In addition to improvements in In-Domain generalization, we demonstrate SOTA performance on the Domain Generalization benchmarks in the popularDomainBed framework as well. Our method is generic and can easily be integrated with several base training algo-rithms to achieve performance gains. Our code is available here: https://github.com/val-iisc/DART. 