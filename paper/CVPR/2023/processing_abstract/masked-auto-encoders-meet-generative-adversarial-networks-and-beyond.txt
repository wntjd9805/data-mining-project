Masked Auto-Encoder (MAE) pretraining methods ran-domly mask image patches and then train a vision Trans-former to reconstruct the original pixels based on the un-masked patches. While they demonstrates impressive per-formance for downstream vision tasks, it generally requires a large amount of training resource. In this paper, we intro-duce a novel Generative Adversarial Networks alike frame-work, referred to as GAN-MAE, where a generator is used to generate the masked patches according to the remain-ing visible patches, and a discriminator is employed to pre-dict whether the patch is synthesized by the generator. We believe this capacity of distinguishing whether the image patch is predicted or original is benefit to representation learning. Another key point lies in that the parameters of the vision Transformer backbone in the generator and discriminator are shared. Extensive experiments demon-strate that adversarial training of GAN-MAE framework is more efficient and accordingly outperforms the standardMAE given the same model size, training data, and com-putation resource. The gains are substantially robust for different model sizes and datasets, in particular, a ViT-B model trained with GAN-MAE for 200 epochs outperforms the MAE with 1600 epochs on fine-tuning top-1 accuracy ofImageNet-1k with much less FLOPs. Besides, our approach also works well at transferring downstream tasks. 