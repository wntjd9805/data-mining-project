This paper addresses the challenge of capturing perfor-mance for the clothed humans from sparse-view or monoc-ular videos. Previous methods capture the performance of full humans with a personalized template or recover the garments from a single frame with static human poses.However, it is inconvenient to extract cloth semantics and capture clothing motion with one-piece template, while sin-gle frame-based methods may suffer from instable tracking across videos. To address these problems, we propose a novel method for human performance capture by tracking clothing and human body motion separately with a double-layer neural radiance ﬁelds (NeRFs). Speciﬁcally, we pro-pose a double-layer NeRFs for the body and garments, and track the densely deforming template of the clothing and body by jointly optimizing the deformation ﬁelds and the canonical double-layer NeRFs. In the optimization, we in-troduce a physics-aware cloth simulation network which can help generate physically plausible cloth dynamics and body-cloth interactions. Compared with existing method-s, our method is fully differentiable and can capture both the body and clothing motion robustly from dynamic videos.Also, our method represents the clothing with an indepen-dent NeRFs, allowing us to model implicit ﬁelds of general clothes feasibly. The experimental evaluations validate its effectiveness on real multi-view or monocular videos. 