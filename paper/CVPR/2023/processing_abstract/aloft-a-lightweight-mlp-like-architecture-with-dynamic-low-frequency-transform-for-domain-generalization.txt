Domain generalization (DG) aims to learn a model that generalizes well to unseen target domains utilizing mul-tiple source domains without re-training. Most existingDG works are based on convolutional neural networks (CNNs). However, the local operation of the convolution kernel makes the model focus too much on local represen-tations (e.g., texture), which inherently causes the model more prone to overfit to the source domains and hampers its generalization ability. Recently, several MLP-based meth-ods have achieved promising results in supervised learn-ing tasks by learning global interactions among different patches of the image.Inspired by this, in this paper, we first analyze the difference between CNN and MLP meth-ods in DG and find that MLP methods exhibit a better generalization ability because they can better capture the global representations (e.g., structure) than CNN methods.Then, based on a recent lightweight MLP method, we ob-tain a strong baseline that outperforms most state-of-the-art CNN-based methods. The baseline can learn global structure representations with a filter to suppress structure-irrelevant information in the frequency space. Moreover, we propose a dynAmic LOw-Frequency spectrum Trans-form (ALOFT) that can perturb local texture features while preserving global structure features, thus enabling the fil-ter to remove structure-irrelevant information sufficiently.Extensive experiments on four benchmarks have demon-strated that our method can achieve great performance im-provement with a small number of parameters compared toSOTA CNN-based DG methods. Our code is available at https://github.com/lingeringlight/ALOFT/.*Corresponding authors: Yinghuan Shi and Lei Qi. Work supported byNSFC Program (62222604, 62206052, 62192783), CAAI-Huawei Mind-Spore (CAAIXSJLJJ-2021-042A), China Postdoctoral Science Founda-tion Project (2021M690609), Jiangsu Natural Science Foundation Project (BK20210224), and CCF-Lenovo Bule Ocean Research Fund.Figure 1. Comparison of the SOTA CNN-based methods, the latestMLP-like models, and our method on PACS. Among the SOTACNN-based and MLP-based methods, our method can achieve the best performance with a relatively small-sized network. 