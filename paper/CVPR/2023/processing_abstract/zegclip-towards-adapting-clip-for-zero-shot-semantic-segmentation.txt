100Seen Unseen 71.1 76.5 75.9 89.9 91.9 77.8) (%U oI m 80 60 40 20 0 40.4 28.3 16.3 13.8Recently, CLIP has been applied to pixel-level zero-shot learning tasks via a two-stage scheme. The general idea is to ﬁrst generate class-agnostic region proposals and then feed the cropped proposal regions to CLIP to utilize its image-level zero-shot classiﬁcation capability. While ef-fective, such a scheme requires two image encoders, one for proposal generation and one for CLIP, leading to a complicated pipeline and high computational cost. In this work, we pursue a simpler-and-efﬁcient one-stage solu-tion that directly extends CLIP’s zero-shot prediction ca-pability from image to pixel level. Our investigation starts with a straightforward extension as our baseline that gen-erates semantic masks by comparing the similarity between text and patch embeddings extracted from CLIP. However, such a paradigm could heavily overﬁt the seen classes and fail to generalize to unseen classes. To handle this is-sue, we propose three simple-but-effective designs and ﬁg-ure out that they can signiﬁcantly retain the inherent zero-shot capacity of CLIP and improve pixel-level generaliza-tion ability.Incorporating those modiﬁcations leads to an efﬁcient zero-shot semantic segmentation system calledZegCLIP. Through extensive experiments on three public benchmarks, ZegCLIP demonstrates superior performance, outperforming the state-of-the-art methods by a large mar-gin under both “inductive” and “transductive” zero-shot settings. In addition, compared with the two-stage method, our one-stage ZegCLIP achieves a speedup of about 5 times faster during inference. We release the code at https://github.com/ZiqinZhou66/ZegCLIP.git. 