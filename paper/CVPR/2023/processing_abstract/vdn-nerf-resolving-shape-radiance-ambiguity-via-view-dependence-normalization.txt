We propose VDN-NeRF, a method to train neural ra-diance fields (NeRFs) for better geometry under non-Lambertian surface and dynamic lighting conditions that cause significant variation in the radiance of a point when viewed from different angles. Instead of explicitly model-ing the underlying factors that result in the view-dependent phenomenon, which could be complex yet not inclusive, we develop a simple and effective technique that normalizes the view-dependence by distilling invariant information al-ready encoded in the learned NeRFs. We then jointly trainNeRFs for view synthesis with view-dependence normal-ization to attain quality geometry. Our experiments show that even though shape-radiance ambiguity is inevitable, the proposed normalization can minimize its effect on geom-etry, which essentially aligns the optimal capacity needed for explaining view-dependent variations. Our method ap-plies to various baselines and significantly improves geom-etry without changing the volume rendering pipeline, even if the data is captured under a moving light source. Code is available at: https://github.com/BoifZ/VDN-NeRF. 