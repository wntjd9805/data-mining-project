The quadratic computational complexity to the number of tokens limits the practical applications of Vision Trans-formers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) ap-plication difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks.In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks.The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image to-kens, for both global and local vision transformers. For in-stance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can achieve the same accuracy with more than 100% in-ference speed improvement and nearly 60% FLOPs reduc-tion; on Swin-(Tiny,Small,Base), we can employ 16 seman-tic tokens in each window to further speed it up by around 20% with slight accuracy increase. Besides great success in image classification, we also extend our method to video recognition. In addition, we design a STViT-R(ecovery) net-work to restore the detailed spatial information based on the STViT, making it work for downstream tasks, which is powerless for previous token sparsification methods. Ex-periments demonstrate that our method can achieve com-petitive results compared to the original networks in object detection and instance segmentation, with over 30% FLOPs reduction for backbone. 