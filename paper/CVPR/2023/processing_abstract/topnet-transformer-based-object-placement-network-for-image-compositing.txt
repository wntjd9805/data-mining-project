We investigate the problem of automatically placing an object into a background image for image compositing.Given a background image and a segmented object, the goal is to train a model to predict plausible placements (loca-tion and scale) of the object for compositing. The quality of the composite image highly depends on the predicted loca-tion/scale. Existing works either generate candidate bound-ing boxes or apply sliding-window search using global rep-resentations from background and object images, which fail to model local information in background images. How-ever, local clues in background images are important to de-termine the compatibility of placing the objects with certain locations/scales. In this paper, we propose to learn the cor-relation between object features and all local background features with a transformer module so that detailed infor-mation can be provided on all possible location/scale con-figurations. A sparse contrastive loss is further proposed to train our model with sparse supervision. Our new for-mulation generates a 3D heatmap indicating the plausibil-ity of all location/scale combinations in one network for-ward pass, which is > 10Ã— faster than the previous sliding-window method. It also supports interactive search when users provide a pre-defined location or scale. The pro-posed method can be trained with explicit annotation or in a self-supervised manner using an off-the-shelf inpainting model, and it outperforms state-of-the-art methods signifi-cantly. User study shows that the trained model generalizes well to real-world images with diverse challenging scenes and object categories. 