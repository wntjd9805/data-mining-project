Recently, finetuning pretrained vision-language models (VLMs) has been a prevailing paradigm for achieving state-of-the-art performance in VQA. However, as VLMs scale, it becomes computationally expensive, storage inefficient, and prone to overfitting when tuning full model parame-ters for a specific task in low-resource settings. Although current parameter-efficient tuning methods dramatically re-duce the number of tunable parameters, there still exists a significant performance gap with full finetuning. In this pa-per, we propose MixPHM, a redundancy-aware parameter-efficient tuning method that outperforms full finetuning in low-resource VQA. Specifically, MixPHM is a lightweight module implemented by multiple PHM-experts in a mixture-of-experts manner. To reduce parameter redundancy, we reparameterize expert weights in a low-rank subspace and share part of the weights inside and across MixPHM. More-over, based on our quantitative analysis of representa-tion redundancy, we propose Redundancy Regularization, which facilitates MixPHM to reduce task-irrelevant redun-dancy while promoting task-relevant correlation. Experi-ments conducted on VQA v2, GQA, and OK-VQA with dif-ferent low-resource settings show that our MixPHM out-performs state-of-the-art parameter-efficient methods and is the only one consistently surpassing full finetuning. 