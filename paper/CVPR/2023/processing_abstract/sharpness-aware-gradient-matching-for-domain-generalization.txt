The goal of domain generalization (DG) is to enhance the generalization capability of the model learned from a source domain to other unseen domains. The recently devel-oped Sharpness-Aware Minimization (SAM) method aims to achieve this goal by minimizing the sharpness measure of the loss landscape. Though SAM and its variants have demonstrated impressive DG performance, they may not al-ways converge to the desired ﬂat region with a small loss value. In this paper, we present two conditions to ensure that the model could converge to a ﬂat minimum with a small loss, and present an algorithm, named Sharpness-Aware Gradient Matching (SAGM), to meet the two condi-tions for improving model generalization capability. Specif-ically, the optimization objective of SAGM will simultane-ously minimize the empirical risk, the perturbed loss (i.e., the maximum loss within a neighborhood in the parameter space), and the gap between them. By implicitly aligning the gradient directions between the empirical risk and the perturbed loss, SAGM improves the generalization capabil-ity over SAM and its variants without increasing the com-putational cost. Extensive experimental results show that our proposed SAGM method consistently outperforms the state-of-the-art methods on ﬁve DG benchmarks, includingPACS, VLCS, OfﬁceHome, TerraIncognita, and DomainNet.Codes are available at https://github.com/Wang-pengfei/SAGM . 