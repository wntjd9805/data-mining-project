Large-scale multi-modal training with image-text pairs imparts strong generalization to CLIP model. Since train-ing on a similar scale for videos is infeasible, recent ap-proaches focus on the effective transfer of image-basedCLIP to the video domain. In this pursuit, new paramet-ric modules are added to learn temporal information and inter-frame relationships which require meticulous design efforts. Furthermore, when the resulting models are learned on videos, they tend to overﬁt on the given task distribu-tion and lack in generalization aspect. This begs the follow-ing question: How to effectively transfer image-level CLIP representations to videos? In this work, we show that a simple Video Fine-tuned CLIP (ViFi-CLIP) baseline is gen-erally sufﬁcient to bridge the domain gap from images to videos. Our qualitative analysis illustrates that the frame-level processing from CLIP image-encoder followed by fea-ture pooling and similarity matching with corresponding*Equally contributing authors. text embeddings helps in implicitly modeling the temporal cues within ViFi-CLIP. Such ﬁne-tuning helps the model to focus on scene dynamics, moving objects and inter-object relationships. For low-data regimes where full ﬁne-tuning is not viable, we propose a ‘bridge and prompt’ approach that ﬁrst uses ﬁne-tuning to bridge the domain gap and then learns prompts on language and vision side to adaptCLIP representations. We extensively evaluate this simple yet strong baseline on zero-shot, base-to-novel generaliza-tion, few-shot and fully supervised settings across ﬁve video benchmarks. Our code and pre-trained models are avail-able at https://github.com/muzairkhattak/ViFi-CLIP. 