ized data-fitting formulation (λ: regularization parameter):Deep image prior (DIP) has shown great promise in tackling a variety of image restoration (IR) and general vi-sual inverse problems, needing no training data. However, the resulting optimization process is often very slow, in-evitably hindering DIP’s practical usage for time-sensitiveIn this paper, we focus on IR, and propose scenarios. two crucial modifications to DIP that help achieve sub-stantial speedup: 1) optimizing the DIP seed while freez-ing randomly-initialized network weights, and 2) reduc-In addition, we reintroduce ex-ing the network depth. plicit priors, such as sparse gradient prior—encoded by total-variation regularization, to preserve the DIP peak per-formance. We evaluate the proposed method on three IR tasks, including image denoising, image super-resolution, and image inpainting, against the original DIP and vari-ants, as well as the competing metaDIP that uses meta-learning to learn good initializers with extra data. Our method is a clear winner in obtaining competitive restora-tion quality in a minimal amount of time. Our code is avail-able at https://github.com/sun- umn/Deep-Random-Projector. 