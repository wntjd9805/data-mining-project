Generalized few-shot semantic segmentation (GFSS) distinguishes pixels of base and novel classes from the back-ground simultaneously, conditioning on sufficient data of base classes and a few examples from novel class. A typicalGFSS approach has two training phases: base class learn-ing and novel class updating. Nevertheless, such a stand-alone updating process often compromises the well-learnt features and results in performance drop on base classes.In this paper, we propose a new idea of leveraging Projec-tion onto Orthogonal Prototypes (POP), which updates fea-tures to identify novel classes without compromising base classes. POP builds a set of orthogonal prototypes, each of which represents a semantic class, and makes the prediction for each class separately based on the features projected onto its prototype. Technically, POP first learns prototypes on base data, and then extends the prototype set to novel classes. The orthogonal constraint of POP encourages the orthogonality between the learnt prototypes and thus miti-gates the influence on base class features when generalizing to novel prototypes. Moreover, we capitalize on the residual of feature projection as the background representation to dynamically fit semantic shifting (i.e., background no longer includes the pixels of novel classes in updating phase). Ex-tensive experiments on two benchmarks demonstrate that our POP achieves superior performances on novel classes without sacrificing much accuracy on base classes. No-tably, POP outperforms the state-of-the-art fine-tuning by 3.93% overall mIoU on PASCAL-5i in 5-shot scenario. 