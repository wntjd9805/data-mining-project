Large vision Transformers (ViTs) driven by self-supervised pre-training mechanisms achieved unprece-dented progress. Lightweight ViT models limited by the little from those pre-model capacity, however, benefit training mechanisms. Knowledge distillation defines a paradigm to transfer representations from large (teacher) models to small (student) ones. However, the conventional single-stage distillation easily gets stuck on task-specific transfer, failing to retain the task-agnostic knowledge cru-cial for model generalization.In this study, we propose generic-to-specific distillation (G2SD), to tap the potential of small ViT models under the supervision of large mod-els pre-trained by masked autoencoders. In generic distil-lation, decoder of the small model is encouraged to align feature predictions with hidden representations of the large model, so that task-agnostic knowledge can be transferred.In specific distillation, predictions of the small model are constrained to be consistent with those of the large model, to transfer task-specific features which guarantee task per-formance. With G2SD, the vanilla ViT-Small model respec-tively achieves 98.7%, 98.1% and 99.3% the performance of its teacher (ViT-Base) for image classification, object de-tection, and semantic segmentation, setting a solid baseline for two-stage vision distillation. Code will be available at https://github.com/pengzhiliang/G2SD. 