Vision Transformers convert images to sequences by slic-ing them into patches. The size of these patches controls a speed/accuracy tradeoff, with smaller patches leading to higher accuracy at greater computational cost, but chang-ing the patch size typically requires retraining the model.In this paper, we demonstrate that simply randomizing the patch size at training time leads to a single set of weights that performs well across a wide range of patch sizes, mak-ing it possible to tailor the model to different compute bud-(cid:63)All authors made signiÔ¨Åcant technical contributions.Lucas started and led the project. 1 Google Research, Brain Team. 3 work done at Google Brain, while being a PhD student at NYU. 2 Google Research.