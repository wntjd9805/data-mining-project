Model ScalabilityModel DeploymentScalable deep Super-Resolution (SR) models are in-creasingly in demand, whose memory can be customized and tuned to the computational recourse of the platform.The existing dynamic scalable SR methods are not memory-friendly enough because multi-scale models have to be saved with a ﬁxed size for each model. Inspired by the suc-cess of Lottery Tickets Hypothesis (LTH) on image classi-ﬁcation, we explore the existence of unstructured scalableSR deep models, that is, we ﬁnd gradual shrinkage sub-networks of extreme sparsity named winning tickets. In this paper, we propose a Memory-friendly Scalable SR frame-work (MSSR). The advantage is that only a single scalable model covers multiple SR models with different sizes, in-stead of reloading SR models of different sizes. Concretely,MSSR consists of the forward and backward stages, the for-mer for model compression and the latter for model expan-sion. In the forward stage, we take advantage of LTH with rewinding weights to progressively shrink the SR model and the pruning-out masks that form nested sets. Moreover, stochastic self-distillation (SSD) is conducted to boost the performance of sub-networks. By stochastically selecting multiple depths, the current model inputs the selected fea-tures into the corresponding parts in the larger model and improves the performance of the current model based on the feedback results of the larger model. In the backward stage, the smaller SR model could be expanded by recov-ering and ﬁne-tuning the pruned parameters according to the pruning-out masks obtained in the forward. Extensive experiments show the effectiveness of MMSR. The smallest-scale sub-network could achieve the sparsity of 94% and outperforms the compared lightweight SR methods.*Equal contribution†Corresponding authors 20% sparsest sub-model 40% sparse sub-modelLMH (cid:42)(cid:75)(cid:86)(cid:82)(cid:85)(cid:95) 80% sparse sub-model 100% whole model   :S_n   :M_n   :Cur_n   :Free_n   :S_w   :M_w   :Cur_wLL   :<20% MM   :40%-80% H   :80%-100% Figure 1. The ﬂowchart of scalable SR network. S n and S w denote the neurons and the neural connections (weights) of the simplest subnetwork; M n and M w denote the intermediate neu-rons and neural connections; Cur n and Cur w denote the speciﬁc neurons and neural connections belonging to the current subnet-work. Free n denotes the pruning-out neurons. The ﬁnal model (with 100% recovered parameters) reaches the original size. The scalable model is adjustable to the memory resource allocation. 