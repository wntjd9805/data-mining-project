Content and style disentanglement is an effective way to achieve few-shot font generation. It allows to transfer the style of the font image in a source domain to the style de-fined with a few reference images in a target domain. How-ever, the content feature extracted using a representative font might not be optimal. In light of this, we propose a con-tent fusion module (CFM) to project the content feature into a linear space defined by the content features of basis fonts, which can take the variation of content features caused by different fonts into consideration. Our method also al-lows to optimize the style representation vector of reference images through a lightweight iterative style-vector refine-ment (ISR) strategy. Moreover, we treat the 1D projection of a character image as a probability distribution and leverage the distance between two distributions as the reconstruc-tion loss (namely projected character loss, PCL). Compared to L2 or L1 reconstruction loss, the distribution distance pays more attention to the global shape of characters. We*This work was done during an internship at Alibaba Group.â€ Corresponding author. have evaluated our method on a dataset of 300 fonts with 6.5k characters each. Experimental results verify that our method outperforms existing state-of-the-art few-shot font generation methods by a large margin. The source code can be found at https:// github.com/ wangchi95/ CF-Font. 