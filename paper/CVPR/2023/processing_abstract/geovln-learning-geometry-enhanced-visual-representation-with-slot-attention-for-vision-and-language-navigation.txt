Most existing works solving Room-to-Room VLN prob-lem only utilize RGB images and do not consider lo-cal context around candidate views, which lack sufficient visual cues about surrounding environment. Moreover, natural language contains complex semantic information thus its correlations with visual inputs are hard to modelIn this paper, we propose merely with cross attention.GeoVLN, which learns Geometry-enhanced visual repre-sentation based on slot attention for robust Visual-and-Language Navigation. The RGB images are compensated with the corresponding depth maps and normal maps pre-dicted by Omnidata as visual inputs. Technically, we intro-duce a two-stage module that combine local slot attention and CLIP model to produce geometry-enhanced represen-tation from such input. We employ V&L BERT to learn a cross-modal representation that incorporate both language and vision informations. Additionally, a novel multiway at-tention module is designed, encouraging different phrases of input instruction to exploit the most related features from visual input. Extensive experiments demonstrate the effec-tiveness of our newly designed modules and show the com-pelling performance of the proposed method. 