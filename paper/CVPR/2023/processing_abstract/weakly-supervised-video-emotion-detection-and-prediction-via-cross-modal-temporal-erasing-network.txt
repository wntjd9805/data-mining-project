Automatically predicting the emotions of user-generated videos (UGVs) receives increasing interest recently. How-ever, existing methods mainly focus on a few key visual frames, which may limit their capacity to encode the con-text that depicts the intended emotions. To tackle that, in this paper, we propose a cross-modal temporal eras-ing network that locates not only keyframes but also con-text and audio-related information in a weakly-supervised manner. leverage the intra- and inter-modal relationship among different segments to ac-Then, we iteratively erase curately select keyframes. keyframes to encourage the model to concentrate on the contexts that include complementary information. Exten-sive experiments on three challenging video emotion bench-marks demonstrate that our method performs favorably against state-of-the-art approaches. The code is released on https://github.com/nku-zhichengzhang/WECL.In specific, we firstFigure 1. Illustration of the keyframes with larger boxes that are detected by off-the-shelf method [65] on the Ekman-6 dataset [54].Note that the deeper color represents the higher impact on overall video emotion, and “GT” represents the category label from the ground truth. The texts with orange color are the categorical pre-dicting results. 