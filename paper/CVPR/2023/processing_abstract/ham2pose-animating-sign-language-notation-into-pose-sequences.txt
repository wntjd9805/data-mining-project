GlossHOUSE3Translating spoken languages into Sign languages is necessary for open communication between the hearing and hearing-impaired communities. To achieve this goal, we propose the first method for animating a text written inHamNoSys, a lexical Sign language notation, into signed pose sequences. As HamNoSys is universal by design, our proposed method offers a generic solution invariant to the target Sign language. Our method gradually generates pose predictions using transformer encoders that create mean-ingful representations of the text and poses while consider-ing their spatial and temporal information. We use weak su-pervision for the training process and show that our method succeeds in learning from partial and inaccurate data. Ad-ditionally, we offer a new distance measurement that con-siders missing keypoints, to measure the distance between pose sequences using DTW-MJE. We validate its correct-ness using AUTSL, a large-scale Sign language dataset, show that it measures the distance between pose sequences more accurately than existing measurements, and use it to assess the quality of our generated pose sequences. Code for the data pre-processing, the model, and the distance measurement is publicly released for future research. 