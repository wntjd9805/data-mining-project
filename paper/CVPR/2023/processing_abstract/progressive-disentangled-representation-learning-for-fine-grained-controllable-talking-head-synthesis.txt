We present a novel one-shot talking head synthesis method that achieves disentangled and fine-grained control over lip motion, eye gaze&blink, head pose, and emotional expression. We represent different motions via disentangled latent representations and leverage an image generator to synthesize talking heads from them. To effectively disen-tangle each motion factor, we propose a progressive disen-tangled representation learning strategy by separating the factors in a coarse-to-fine manner, where we first extract unified motion feature from the driving signal, and then iso-late each fine-grained motion from the unified feature. We leverage motion-specific contrastive learning and regress-ing for non-emotional motions, and introduce feature-level decorrelation and self-reconstruction for emotional expres-sion, to fully utilize the inherent properties of each motion factor in unstructured video data to achieve disentangle-ment. Experiments show that our method provides high quality speech&lip-motion synchronization along with pre-cise and disentangled control over multiple extra facial mo-tions, which can hardly be achieved by previous methods. 