Human-Object Interaction (HOI) detection aims to lo-calize human-object pairs and recognize their interactions.Recently, Contrastive Language-Image Pre-training (CLIP) has shown great potential in providing interaction prior forHOI detectors via knowledge distillation. However, such approaches often rely on large-scale training data and suf-fer from inferior performance under few/zero-shot scenar-ios. In this paper, we propose a novel HOI detection frame-work that efficiently extracts prior knowledge from CLIP and achieves better generalization. In detail, we first intro-duce a novel interaction decoder to extract informative re-gions in the visual feature map of CLIP via a cross-attention mechanism, which is then fused with the detection backbone by a knowledge integration block for more accurate human-object pair detection. In addition, prior knowledge in CLIP text encoder is leveraged to generate a classifier by embed-ding HOI descriptions. To distinguish fine-grained interac-tions, we build a verb classifier from training data via visual semantic arithmetic and a lightweight verb representation adapter. Furthermore, we propose a training-free enhance-ment to exploit global HOI predictions from CLIP. Exten-sive experiments demonstrate that our method outperforms the state of the art by a large margin on various settings, e.g.+4.04 mAP on HICO-Det. The source code is available in https://github.com/Artanic30/HOICLIP. 