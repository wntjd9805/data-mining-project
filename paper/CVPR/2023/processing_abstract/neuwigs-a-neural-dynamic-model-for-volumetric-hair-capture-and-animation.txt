The capture and animation of human hair are two of the major challenges in the creation of realistic avatars for the virtual reality. Both problems are highly challenging, be-cause hair has complex geometry and appearance and ex-hibits challenging motion. In this paper, we present a two-stage approach that models hair independently of the head to address these challenges in a data-driven manner. The first stage, state compression, learns a low-dimensional la-tent space of 3D hair states including motion and appear-ance via a novel autoencoder-as-a-tracker strategy. To bet-ter disentangle the hair and head in appearance learning, we employ multi-view hair segmentation masks in combi-nation with a differentiable volumetric renderer. The sec-ond stage optimizes a novel hair dynamics model that per-forms temporal hair transfer based on the discovered latent codes. To enforce higher stability while driving our dynam-ics model, we employ the 3D point-cloud autoencoder from the compression stage for de-noising of the hair state. Our model outperforms the state of the art in novel view synthe-sis and is capable of creating novel hair animations without relying on hair observations as a driving signal.â€  