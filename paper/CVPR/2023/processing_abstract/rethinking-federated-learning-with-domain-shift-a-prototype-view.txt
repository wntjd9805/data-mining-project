Federated learning shows a bright promise as a privacy-preserving collaborative learning technique. However, prevalent solutions mainly focus on all private data sampled from the same domain. An important challenge is that when distributed data are derived from diverse domains. The pri-vate model presents degenerative performance on other do-mains (with domain shift). Therefore, we expect that the global model optimized after the federated learning pro-cess stably provides generalizability performance on mul-tiple domains. In this paper, we propose Federated Proto-types Learning (FPL) for federated learning under domain shift. The core idea is to construct cluster prototypes and unbiased prototypes, providing fruitful domain knowledge and a fair convergent target. On the one hand, we pull the sample embedding closer to cluster prototypes belonging to the same semantics than cluster prototypes from distinct classes. On the other hand, we introduce consistency reg-ularization to align the local instance with the respective unbiased prototype. Empirical results on Digits and OfﬁceCaltech tasks demonstrate the effectiveness of the proposed solution and the efﬁciency of crucial modules. 