Deep neural networks are widely known to be suscep-tible to adversarial examples, which can cause incorrect predictions through subtle input modifications. These ad-versarial examples tend to be transferable between mod-els, but targeted attacks still have lower attack success rates due to significant variations in decision boundaries.To enhance the transferability of targeted adversarial ex-amples, we propose introducing competition into the op-timization process. Our idea is to craft adversarial per-turbations in the presence of two new types of competi-tor noises: adversarial perturbations towards different tar-get classes and friendly perturbations towards the correct class. With these competitors, even if an adversarial ex-ample deceives a network to extract specific features lead-ing to the target class, this disturbance can be suppressed by other competitors. Therefore, within this competition, adversarial examples should take different attack strategies by leveraging more diverse features to overwhelm their in-terference, leading to improving their transferability to dif-ferent models. Considering the computational complexity, we efficiently simulate various interference from these two types of competitors in feature space by randomly mixing up stored clean features in the model inference and named this method Clean Feature Mixup (CFM). Our extensive exper-imental results on the ImageNet-Compatible and CIFAR-10 datasets show that the proposed method outperforms the ex-isting baselines with a clear margin. Our code is available at https://github.com/dreamflake/CFM . 