challenging setups where the source and target domains are more distant. Project Page: yunqing-me.github.io/RICK.Few-shot image generation (FSIG) learns to generate di-verse and high-fidelity images from a target domain using a few (e.g., 10) reference samples. Existing FSIG methods se-lect, preserve and transfer prior knowledge from a source generator (pretrained on a related domain) to learn the tar-get generator. In this work, we investigate an underexplored issue in FSIG, dubbed as incompatible knowledge transfer, which would significantly degrade the realisticness of syn-thetic samples. Empirical observations show that the issue stems from the least significant filters from the source gen-erator. To this end, we propose knowledge truncation to mitigate this issue in FSIG, which is a complementary op-eration to knowledge preservation and is implemented by a lightweight pruning-based method. Extensive experiments show that knowledge truncation is simple and effective, con-sistently achieving state-of-the-art performance, including 