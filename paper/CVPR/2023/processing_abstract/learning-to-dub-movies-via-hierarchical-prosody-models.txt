Given a piece of text, a video clip and a reference audio, the movie dubbing (also known as visual voice clone, V2C) task aims to generate speeches that match the speakerâ€™s emotion presented in the video using the de-sired speaker voice as reference. V2C is more challeng-ing than conventional text-to-speech tasks as it additionally requires the generated speech to exactly match the vary-ing emotions and speaking speed presented in the video.Unlike previous works, we propose a novel movie dub-bing architecture to tackle these problems via hierarchi-cal prosody modeling, which bridges the visual informa-tion to corresponding speech prosody from three aspects:Specifically, we align lip move-lip, ment to the speech duration, and convey facial expres-sion to speech energy and pitch via attention mechanism based on valence and arousal representations inspired by the psychology findings. Moreover, we design an emo-tion booster to capture the atmosphere from global video scenes. All these embeddings are used together to gener-ate mel-spectrogram, which is then converted into speech waves by an existing vocoder. Extensive experimental re-sults on the V2C and Chem benchmark datasets demon-strate the favourable performance of the proposed method.The code and trained models will be made available at https://github.com/GalaxyCong/HPMDubbing. face, and scene. 