This paper studies the fundamental problem of learn-ing multi-layer generator models. The multi-layer gener-ator model builds multiple layers of latent variables as a prior model on top of the generator, which benefits learn-ing complex data distribution and hierarchical represen-tations. However, such a prior model usually focuses on modeling inter-layer relations between latent variables by assuming non-informative (conditional) Gaussian distribu-tions, which can be limited in model expressivity. To tackle this issue and learn more expressive prior models, we pro-pose an energy-based model (EBM) on the joint latent space over all layers of latent variables with the multi-layer gen-erator as its backbone. Such joint latent space EBM prior model captures the intra-layer contextual relations at each layer through layer-wise energy terms, and latent variables across different layers are jointly corrected. We develop a joint training scheme via maximum likelihood estima-tion (MLE), which involves Markov Chain Monte Carlo (MCMC) sampling for both prior and posterior distribu-tions of the latent variables from different layers. To ensure efficient inference and learning, we further propose a varia-tional training scheme where an inference model is used to amortize the costly posterior MCMC sampling. Our experi-ments demonstrate that the learned model can be expressive in generating high-quality images and capturing hierarchi-cal features for better outlier detection. 