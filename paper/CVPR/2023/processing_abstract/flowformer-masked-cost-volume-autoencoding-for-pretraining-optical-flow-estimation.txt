FlowFormer [24] introduces a transformer architecture into optical flow estimation and achieves state-of-the-art performance. The core component of FlowFormer is the transformer-based cost-volume encoder. Inspired by the re-cent success of masked autoencoding (MAE) pretraining in unleashing transformersâ€™ capacity of encoding visual rep-resentation, we propose Masked Cost Volume Autoencod-ing (MCVA) to enhance FlowFormer by pretraining the cost-volume encoder with a novel MAE scheme. Firstly, we introduce a block-sharing masking strategy to prevent masked information leakage, as the cost maps of neigh-boring source pixels are highly correlated. Secondly, we propose a novel pre-text reconstruction task, which encour-ages the cost-volume encoder to aggregate long-range in-formation and ensures pretraining-finetuning consistency.We also show how to modify the FlowFormer architecture to accommodate masks during pretraining. Pretrained withMCVA, FlowFormer++ ranks 1st among published meth-ods on both Sintel and KITTI-2015 benchmarks. Specifi-cally, FlowFormer++ achieves 1.07 and 1.94 average end-point error (AEPE) on the clean and final pass of Sintel benchmark, leading to 7.76% and 7.18% error reductions from FlowFormer. FlowFormer++ obtains 4.52 F1-all on the KITTI-2015 test set, improving FlowFormer by 0.16. 