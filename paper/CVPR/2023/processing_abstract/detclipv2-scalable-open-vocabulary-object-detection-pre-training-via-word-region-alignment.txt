This paper presents DetCLIPv2, an efficient and scalable training framework that incorporates large-scale image-text pairs to achieve open-vocabulary object detection (OVD). Unlike previous OVD frameworks that typically rely on a pre-trained vision-language model (e.g., CLIP) or ex-ploit image-text pairs via a pseudo labeling process, Det-CLIPv2 directly learns the fine-grained word-region align-ment from massive image-text pairs in an end-to-end man-ner. To accomplish this, we employ a maximum word-region similarity between region proposals and textual words to guide the contrastive objective. To enable the model to gain localization capability while learning broad concepts, Det-CLIPv2 is trained with a hybrid supervision from detection, grounding and image-text pair data under a unified data formulation. By jointly training with an alternating scheme and adopting low-resolution input for image-text pairs, Det-CLIPv2 exploits image-text pair data efficiently and ef-fectively: DetCLIPv2 utilizes 13× more image-text pairs than DetCLIP with a similar training time and improves†Corresponding xu.hang@huawei.com author: liangxd9@mail.sysu.edu.cn, performance. With 13M image-text pairs for pre-training,DetCLIPv2 demonstrates superior open-vocabulary detec-tion performance, e.g., DetCLIPv2 with Swin-T backbone achieves 40.4% zero-shot AP on the LVIS benchmark, which outperforms previous works GLIP/GLIPv2/DetCLIP by 14.4/11.4/4.5% AP, respectively, and even beats its fully-supervised counterpart by a large margin. 