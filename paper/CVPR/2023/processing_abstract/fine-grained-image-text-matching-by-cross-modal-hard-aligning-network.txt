Current state-of-the-art image-text matching methods implicitly align the visual-semantic fragments, like regions in images and words in sentences, and adopt cross-attention mechanism to discover ﬁne-grained cross-modal semantic correspondence. However, the cross-attention mechanism may bring redundant or irrelevant region-word alignments, degenerating retrieval accuracy and limiting efﬁciency. Al-though many researchers have made progress in mining meaningful alignments and thus improving accuracy, the problem of poor efﬁciency remains unresolved. In this work, we propose to learn ﬁne-grained image-text matching from the perspective of information coding. Speciﬁcally, we sug-gest a coding framework to explain the fragments aligning process, which provides a novel view to reexamine the cross-attention mechanism and analyze the problem of redundant alignments. Based on this framework, a Cross-modal HardAligning Network (CHAN) is designed, which comprehen-sively exploits the most relevant region-word pairs and elim-inates all other alignments. Extensive experiments con-ducted on two public datasets, MS-COCO and Flickr30K, verify that the relevance of the most associated word-region pairs is discriminative enough as an indicator of the image-text similarity, with superior accuracy and efﬁciency over the state-of-the-art approaches on the bidirectional image and text retrieval tasks. Our code will be available at https://github.com/ppanzx/CHAN.Figure 1. Illustration of different semantic corresponding methods: (a) Global Embedding methods, (b) Fragment Embedding meth-ods, (c) existing Fragment Aligning methods, and (d) our CHAN method. Here ω in (c) and (d) is the attention weight/assignment between the word "pajamas" and the image region, where the re-gion with the maximum attention weight is outlined in yellow below. Compared to existing Fragment Aligning methods which bring redundant alignments, we improve them by attending to the most relevant region while neglecting all of the misalignments. 