In this paper, we efficiently transfer the surpassing rep-resentation power of the vision foundation models, such asViT and Swin, for video understanding with only a few train-able parameters. Previous adaptation methods have simul-taneously considered spatial and temporal modeling with a unified learnable module but still suffered from fully lever-aging the representative capabilities of image transformers.We argue that the popular dual-path (two-stream) architec-ture in video models can mitigate this problem. We pro-pose a novel DUALPATH adaptation separated into spatial and temporal adaptation paths, where a lightweight bottle-neck adapter is employed in each transformer block. Espe-cially for temporal dynamic modeling, we incorporate con-secutive frames into a grid-like frameset to precisely imi-tate vision transformersâ€™ capability that extrapolates rela-tionships between tokens. In addition, we extensively inves-tigate the multiple baselines from a unified perspective in video understanding and compare them with DUALPATH.Experimental results on four action recognition benchmarks prove that pretrained image transformers with DUALPATH can be effectively generalized beyond the data domain. 