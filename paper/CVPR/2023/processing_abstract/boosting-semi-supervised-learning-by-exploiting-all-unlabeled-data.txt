Semi-supervised learning (SSL) has attracted enormous attention due to its vast potential of mitigating the depen-dence on large labeled datasets. The latest methods (e.g.,FixMatch) use a combination of consistency regulariza-tion and pseudo-labeling to achieve remarkable successes.However, these methods all suffer from the waste of compli-cated examples since all pseudo-labels have to be selected by a high threshold to filter out noisy ones. Hence, the ex-amples with ambiguous predictions will not contribute to the training phase. For better leveraging all unlabeled ex-amples, we propose two novel techniques: Entropy Mean-ing Loss (EML) and Adaptive Negative Learning (ANL).EML incorporates the prediction distribution of non-target classes into the optimization objective to avoid competition with target class, and thus generating more high-confidence predictions for selecting pseudo-label. ANL introduces the additional negative pseudo-label for all unlabeled data to leverage low-confidence examples.It adaptively allo-cates this label by dynamically evaluating the top-k per-formance of the model. EML and ANL do not introduce any additional parameter and hyperparameter. We inte-grate these techniques with FixMatch, and develop a sim-ple yet powerful framework called FullMatch. Extensive experiments on several common SSL benchmarks (CIFAR-10/100, SVHN, STL-10 and ImageNet) demonstrate thatFullMatch exceeds FixMatch by a large margin. Integrated with FlexMatch (an advanced FixMatch-based framework), we achieve state-of-the-art performance. Source code is available at https://github.com/megvii-research/FullMatch. 