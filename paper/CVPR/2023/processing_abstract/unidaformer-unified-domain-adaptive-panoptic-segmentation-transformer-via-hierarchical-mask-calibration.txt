Domain adaptive panoptic segmentation aims to miti-gate data annotation challenge by leveraging off-the-shelf annotated data in one or multiple related source domains.However, existing studies employ two separate networks for instance segmentation and semantic segmentation which lead to excessive network parameters as well as compli-cated and computationally intensive training and inference processes. We design UniDAformer, a unified domain adap-tive panoptic segmentation transformer that is simple but can achieve domain adaptive instance segmentation and semantic segmentation simultaneously within a single net-work. UniDAformer introduces Hierarchical Mask Calibra-tion (HMC) that rectifies inaccurate predictions at the level of regions, superpixels and pixels via online self-training on the fly. It has three unique features: 1) it enables uni-fied domain adaptive panoptic adaptation; 2) it mitigates false predictions and improves domain adaptive panoptic segmentation effectively; 3) it is end-to-end trainable with a much simpler training and inference pipeline. Exten-sive experiments over multiple public benchmarks show thatUniDAformer achieves superior domain adaptive panoptic segmentation as compared with the state-of-the-art.Figure 1. Existing domain adaptive panoptic segmentation [20] adapts things and stuff separately with two isolated networks (for instance segmentation and semantic segmentation) and fuses their outputs to produce the final panoptic segmentation as in (a), leading to excessive network parameters as well as complicated and computationally intensive training and inference. Differently,UniDAformer employs a single unified network to jointly adapt things and stuff as in (b), which involves much less parameters and simplifies the training and inference pipeline greatly. 