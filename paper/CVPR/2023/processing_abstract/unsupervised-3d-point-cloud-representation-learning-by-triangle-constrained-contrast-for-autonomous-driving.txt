Due to the difﬁculty of annotating the 3D LiDAR data of autonomous driving, an efﬁcient unsupervised 3D represen-tation learning method is important. In this paper, we de-sign the Triangle Constrained Contrast (TriCC) framework tailored for autonomous driving scenes which learns 3D un-supervised representations through both the multimodal in-formation and dynamic of temporal sequences. We treat one camera image and two LiDAR point clouds with different timestamps as a triplet. And our key design is the consistent constraint that automatically ﬁnds matching relationships among the triplet through “self-cycle” and learns represen-tations from it. With the matching relations across the tem-poral dimension and modalities, we can further conduct a triplet contrast to improve learning efﬁciency. To the best of our knowledge, TriCC is the ﬁrst framework that uniﬁes both the temporal and multimodal semantics, which means it utilizes almost all the information in autonomous driving scenes. And compared with previous contrastive methods, it can automatically dig out contrasting pairs with higher difﬁculty, instead of relying on handcrafted ones. Extensive experiments are conducted with Minkowski-UNet and Vox-elNet on several semantic segmentation and 3D detection datasets. Results show that TriCC learns effective repre-sentations with much fewer training iterations and improves the SOTA results greatly on all the downstream tasks. Code and models can be found at https://bopang1996.github.io/. 