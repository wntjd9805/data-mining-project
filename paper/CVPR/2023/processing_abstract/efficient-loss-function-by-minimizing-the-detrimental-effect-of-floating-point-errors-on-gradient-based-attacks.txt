Attackers can deceive neural networks by adding hu-man imperceptive perturbations to their input data; this reveals the vulnerability and weak robustness of current deep-learning networks. Many attack techniques have been proposed to evaluate the model’s robustness. Gradient-based attacks suffer from severely overestimating the ro-bustness. This paper identifies that the relative error in cal-culated gradients caused by floating-point errors, including floating-point underflow and rounding errors, is a funda-mental reason why gradient-based attacks fail to accurately assess the model’s robustness. Although it is hard to elim-inate the relative error in the gradients, we can control its effect on the gradient-based attacks. Correspondingly, we propose an efficient loss function by minimizing the detri-mental impact of the floating-point errors on the attacks.Experimental results show that it is more efficient and reli-able than other loss functions when examined across a wide range of defence mechanisms. 