Vision Transformers (ViTs) are built on the assumption of treating image patches as “visual tokens” and learn patch-to-patch attention. The patch embedding based to-kenizer has a semantic gap with respect to its counterpart, the textual tokenizer. The patch-to-patch attention suffers from the quadratic complexity issue, and also makes it non-trivial to explain learned ViTs. To address these issues inViT, this paper proposes to learn Patch-to-Cluster atten-tion (PaCa) in ViT. Queries in our PaCa-ViT starts with patches, while keys and values are directly based on clus-tering (with a predeﬁned small number of clusters). The clusters are learned end-to-end, leading to better tokenizers and inducing joint clustering-for-attention and attention-for-clustering for better and interpretable models. The quadratic complexity is relaxed to linear complexity. The proposed PaCa module is used in designing efﬁcient and in-terpretable ViT backbones and semantic segmentation head networks. In experiments, the proposed methods are tested on ImageNet-1k image classiﬁcation, MS-COCO object de-tection and instance segmentation and MIT-ADE20k se-mantic segmentation. Compared with the prior art, it ob-tains better performance in all the three benchmarks than the SWin [32] and the PVTs [47, 48] by signiﬁcant mar-gins in ImageNet-1k and MIT-ADE20k.It is also signiﬁ-cantly more efﬁcient than PVT models in MS-COCO andMIT-ADE20k due to the linear complexity. The learned clusters are semantically meaningful. Code and model checkpoints are available at https://github.com/ iVMCL/PaCaViT. 