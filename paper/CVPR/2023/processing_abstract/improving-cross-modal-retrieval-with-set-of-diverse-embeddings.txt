Cross-modal retrieval across image and text modalities is a challenging task due to its inherent ambiguity: An im-age often exhibits various situations, and a caption can be coupled with diverse images. Set-based embedding has been studied as a solution to this problem. It seeks to en-code a sample into a set of different embedding vectors that capture different semantics of the sample. In this paper, we present a novel set-based embedding method, which is dis-tinct from previous work in two aspects. First, we present a new similarity function called smooth-Chamfer similar-ity, which is designed to alleviate the side effects of existing similarity functions for set-based embedding. Second, we propose a novel set prediction module to produce a set of embedding vectors that effectively captures diverse seman-tics of input by the slot attention mechanism. Our method is evaluated on the COCO and Flickr30K datasets across dif-ferent visual backbones, where it outperforms existing meth-ods including ones that demand substantially larger compu-tation at inference. 