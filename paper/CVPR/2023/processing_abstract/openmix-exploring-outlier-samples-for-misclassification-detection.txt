Reliable confidence estimation for deep neural classi-fiers is a challenging yet fundamental requirement in high-stakes applications. Unfortunately, modern deep neural networks are often overconfident for their erroneous predic-tions. In this work, we exploit the easily available outlier samples, i.e., unlabeled samples coming from non-target classes, for helping detect misclassification errors. Partic-ularly, we find that the well-known Outlier Exposure, which is powerful in detecting out-of-distribution (OOD) samples from unknown classes, does not provide any gain in identi-fying misclassification errors. Based on these observations, we propose a novel method called OpenMix, which incor-porates open-world knowledge by learning to reject uncer-tain pseudo-samples generated via outlier transformation.OpenMix significantly improves confidence reliability un-der various scenarios, establishing a strong and unified framework for detecting both misclassified samples from known classes and OOD samples from unknown classes.The code is publicly available at https://github. com/Impression2805/OpenMix. 