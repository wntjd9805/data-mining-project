Self supervision and natural language supervision have emerged as two exciting ways to train general purpose im-age encoders which excel at a variety of downstream tasks.Recent works such as M3AE [31] and SLIP [63] have sug-gested that these approaches can be effectively combined, but most notably their results use small (<20M examples) pre-training datasets and don’t effectively reﬂect the large-scale regime (>100M samples) that is commonly used for these approaches. Here we investigate whether a similar approach can be effective when trained with a much larger amount of data. We ﬁnd that a combination of two state of the art approaches: masked auto-encoders, MAE [37] and contrastive language image pre-training, CLIP [68] provides a beneﬁt over CLIP when trained on a corpus of 11.3M image-text pairs, but little to no beneﬁt (as evaluated on a suite of common vision tasks) over CLIP when trained on a large corpus of 1.4B images. Our work provides some much needed clarity into the effectiveness (or lack thereof) of self supervision for large-scale image-text training. 