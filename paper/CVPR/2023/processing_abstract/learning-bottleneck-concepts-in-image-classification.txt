Input imageConcept bottleneck representationInterpreting and explaining the behavior of deep neural networks is critical for many tasks. Explainable AI pro-vides a way to address this challenge, mostly by provid-ing per-pixel relevance to the decision. Yet, interpreting such explanations may require expert knowledge. Some re-cent attempts toward interpretability adopt a concept-based framework, giving a higher-level relationship between some concepts and model decisions. This paper proposes Bot-tleneck Concept Learner (BotCL), which represents an im-age solely by the presence/absence of concepts learned through training over the target task without explicit super-vision over the concepts. It uses self-supervision and tai-lored regularizers so that learned concepts can be human-understandable. Using some image classiﬁcation tasks as our testbed, we demonstrate BotCL’s potential to rebuild neural networks for better interpretability 1. 