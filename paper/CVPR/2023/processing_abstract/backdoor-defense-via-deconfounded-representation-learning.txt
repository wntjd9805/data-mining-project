Deep neural networks (DNNs) are recently shown to be vulnerable to backdoor attacks, where attackers embed hid-den backdoors in the DNN model by injecting a few poi-soned examples into the training dataset. While exten-sive efforts have been made to detect and remove back-doors from backdoored DNNs, it is still not clear whether a backdoor-free clean model can be directly obtained from poisoned datasets. In this paper, we first construct a causal graph to model the generation process of poisoned data and find that the backdoor attack acts as the confounder, which brings spurious associations between the input im-ages and target labels, making the model predictions less reliable.Inspired by the causal understanding, we pro-pose the Causality-inspired Backdoor Defense (CBD), to learn deconfounded representations for reliable classifi-cation. Specifically, a backdoored model is intentionally trained to capture the confounding effects. The other clean model dedicates to capturing the desired causal effects by minimizing the mutual information with the confounding representations from the backdoored model and employ-ing a sample-wise re-weighting scheme. Extensive exper-iments on multiple benchmark datasets against 6 state-of-the-art attacks verify that our proposed defense method is effective in reducing backdoor threats while maintaining high accuracy in predicting benign samples. Further anal-ysis shows that CBD can also resist potential adaptive at-tacks. The code is available at https://github.com/ zaixizhang/CBD. 