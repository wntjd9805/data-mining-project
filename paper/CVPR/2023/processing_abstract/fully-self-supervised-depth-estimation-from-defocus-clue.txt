Depth-from-defocus (DFD), modeling the relationship between depth and defocus pattern in images, has demon-strated promising performance in depth estimation. Re-cently, several self-supervised works try to overcome the difficulties in acquiring accurate depth ground-truth. How-ever, they depend on the all-in-focus (AIF) images, which cannot be captured in real-world scenarios. Such limi-tation discourages the applications of DFD methods. To tackle this issue, we propose a completely self-supervised framework that estimates depth purely from a sparse fo-cal stack. We show that our framework circumvents the needs for the depth and AIF image ground-truth, and re-ceives superior predictions, thus closing the gap between the theoretical success of DFD works and their applica-tions in the real world.In particular, we propose (i) a more realistic setting for DFD tasks, where no depth orAIF image ground-truth is available; (ii) a novel self-supervision framework that provides reliable predictions of depth and AIF image under the challenging setting. The proposed framework uses a neural model to predict the depth and AIF image, and utilizes an optical model to val-idate and refine the prediction. We verify our framework on three benchmark datasets with rendered focal stacks and real focal stacks. Qualitative and quantitative evaluations show that our method provides a strong baseline for self-supervised DFD tasks. The source code is publicly avail-able at https://github.com/Ehzoahis/DEReD. 