Incremental object detection (IOD) aims to train an ob-ject detector in phases, each with annotations for new ob-ject categories. As other incremental settings, IOD is sub-ject to catastrophic forgetting, which is often addressed by techniques such as knowledge distillation (KD) and exem-plar replay (ER). However, KD and ER do not work well if applied directly to state-of-the-art transformer-based ob-ject detectors such as Deformable DETR [59] and UP-DETR [9]. In this paper, we solve these issues by proposing a ContinuaL DEtection TRansformer (CL-DETR), a new method for transformer-based IOD which enables effective usage of KD and ER in this context. First, we introduce aDetector Knowledge Distillation (DKD) loss, focusing on the most informative and reliable predictions from old ver-sions of the model, ignoring redundant background predic-tions, and ensuring compatibility with the available ground-truth labels. We also improve ER by proposing a calibration strategy to preserve the label distribution of the training set, therefore better matching training and testing statistics. We conduct extensive experiments on COCO 2017 and demon-strate that CL-DETR achieves state-of-the-art results in theIOD setting.1 