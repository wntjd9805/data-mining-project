Current attention algorithms (e.g., self-attention) are stimulus-driven and highlight all the salient objects in an image. However, intelligent agents like humans often guide their attention based on the high-level task at hand, fo-cusing only on task-related objects. This ability of task-guided top-down attention provides task-adaptive represen-tation and helps the model generalize to various tasks. In this paper, we consider top-down attention from a classicAnalysis-by-Synthesis (AbS) perspective of vision. Prior work indicates a functional equivalence between visual at-tention and sparse reconstruction; we show that an AbS visual system that optimizes a similar sparse reconstruc-tion objective modulated by a goal-directed top-down sig-nal naturally simulates top-down attention. We further pro-pose Analysis-by-Synthesis Vision Transformer (AbSViT), which is a top-down modulated ViT model that variationally approximates AbS, and achieves controllable top-down at-tention. For real-world applications, AbSViT consistently improves over baselines on Vision-Language tasks such as VQA and zero-shot retrieval where language guides the top-down attention. AbSViT can also serve as a gen-eral backbone, improving performance on classification, se-mantic segmentation, and model robustness. Project page: https://sites.google.com/view/absvit. 