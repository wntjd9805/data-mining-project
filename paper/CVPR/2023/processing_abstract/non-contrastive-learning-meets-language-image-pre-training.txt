Contrastive language-image pre-training (CLIP) serves as a de-facto standard to align images and texts. Nonethe-less, the loose correlation between images and texts of web-crawled data renders the contrastive objective data ineffi-In this cient and craving for a large training batch size. work, we explore the validity of non-contrastive language-image pre-training (nCLIP), and study whether nice proper-ties exhibited in visual self-supervised models can emerge.We empirically observe that the non-contrastive objective benefits representation learning while sufficiently underper-forming under zero-shot recognition. Based on the above study, we further introduce xCLIP, a multi-tasking frame-work combining CLIP and nCLIP, and show that nCLIP aids CLIP in enhancing feature semantics. The synergy between two objectives lets xCLIP enjoy the best of both worlds: superior performance in both zero-shot transfer and representation learning. Systematic evaluation is con-ducted spanning a wide variety of downstream tasks in-cluding zero-shot classification, out-of-domain classifica-tion, retrieval, visual representation learning, and textual representation learning, showcasing a consistent perfor-mance gain and validating the effectiveness of xCLIP. The code and pre-trained models will be publicly available at https://github.com/shallowtoil/xclip. 