Inferring a meaningful geometric scene representation from a single image is a fundamental problem in computer vision. Approaches based on traditional depth map predic-tion can only reason about areas that are visible in the im-age. Currently, neural radiance fields (NeRFs) can capture true 3D including color, but are too complex to be gener-ated from a single image. As an alternative, we propose to predict an implicit density field from a single image. It maps every location in the frustum of the image to volumetric den-sity. By directly sampling color from the available views instead of storing color in the density field, our scene rep-resentation becomes significantly less complex compared toNeRFs, and a neural network can predict it in a single for-ward pass. The network is trained through self-supervision from only video data. Our formulation allows volume ren-dering to perform both depth prediction and novel view syn-thesis. Through experiments, we show that our method is able to predict meaningful geometry for regions that are oc-cluded in the input image. Additionally, we demonstrate the potential of our approach on three datasets for depth pre-diction and novel-view synthesis. 