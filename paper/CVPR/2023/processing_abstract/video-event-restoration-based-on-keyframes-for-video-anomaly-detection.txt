Video anomaly detection (VAD) is a signiÔ¨Åcant computer vision problem. Existing deep neural network (DNN) basedVAD methods mostly follow the route of frame reconstruc-tion or frame prediction. However, the lack of mining and learning of higher-level visual features and temporal con-text relationships in videos limits the further performance of these two approaches. Inspired by video codec theory, we introduce a brand-new VAD paradigm to break through these limitations: First, we propose a new task of video event restoration based on keyframes. Encouraging DNN to infer missing multiple frames based on video keyframes so as to restore a video event, which can more effectively mo-tivate DNN to mine and learn potential higher-level visual features and comprehensive temporal context relationships in the video. To this end, we propose a novel U-shaped SwinTransformer Network with Dual Skip Connections (USTN-DSC) for video event restoration, where a cross-attention and a temporal upsampling residual skip connection are in-troduced to further assist in restoring complex static and dynamic motion object features in the video. In addition, we propose a simple and effective adjacent frame differ-ence loss to constrain the motion consistency of the video sequence. Extensive experiments on benchmarks demon-strate that USTN-DSC outperforms most existing methods, validating the effectiveness of our method. 