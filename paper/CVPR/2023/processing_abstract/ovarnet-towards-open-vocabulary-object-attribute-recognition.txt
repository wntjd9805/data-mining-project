In this paper, we consider the problem of simultaneously detecting objects and inferring their visual attributes in an image, even for those with no manual annotations provided at the training stage, resembling an open-vocabulary sce-nario. To achieve this goal, we make the following con-tributions: (i) we start with a na¨ıve two-stage approach for open-vocabulary object detection and attribute classi-fication, termed CLIP-Attr. The candidate objects are first proposed with an offline RPN and later classified for se-mantic category and attributes; (ii) we combine all avail-able datasets and train with a federated strategy to fine-tune the CLIP model, aligning the visual representation with attributes, additionally, we investigate the efficacy of leveraging freely available online image-caption pairs un-der weakly supervised learning; (iii) in pursuit of efficiency, we train a Faster-RCNN type model end-to-end with knowl-edge distillation, that performs class-agnostic object pro-posals and classification on semantic categories and at-tributes with classifiers generated from a text encoder; Fi-nally, (iv) we conduct extensive experiments on VAW, MS-COCO, LSA, and OVAD datasets, and show that recog-⋆ Equal contribution. † Corresponding author. nition of semantic category and attributes is complemen-tary for visual scene understanding, i.e., jointly training object detection and attributes prediction largely outper-form existing approaches that treat the two tasks indepen-dently, demonstrating strong generalization ability to novel attributes and categories. 