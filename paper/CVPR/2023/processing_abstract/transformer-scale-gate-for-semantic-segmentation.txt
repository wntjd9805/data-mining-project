Effectively encoding multi-scale contextual information is crucial for accurate semantic segmentation. Most of the existing transformer-based segmentation models combine features across scales without any selection, where features on sub-optimal scales may degrade segmentation outcomes.Leveraging from the inherent properties of Vision Trans-formers, we propose a simple yet effective module, Trans-former Scale Gate (TSG), to optimally combine multi-scale features. TSG exploits cues in self and cross attentions inVision Transformers for the scale selection. TSG is a highly flexible plug-and-play module, and can easily be incorpo-rated with any encoder-decoder-based hierarchical visionTransformer. Extensive experiments on the Pascal Context,ADE20K and Cityscapes datasets demonstrate that the pro-posed feature selection strategy achieves consistent gains. 