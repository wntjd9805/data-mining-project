Active speaker detection is a challenging task in audio-visual scenarios, with the aim to detect who is speaking in one or more speaker scenarios. This task has received con-siderable attention because it is crucial in many applica-tions. Existing studies have attempted to improve the per-formance by inputting multiple candidate information and designing complex models. Although these methods have achieved excellent performance, their high memory and computational power consumption render their application to resource-limited scenarios difficult. Therefore, in this study, a lightweight active speaker detection architecture is constructed by reducing the number of input candidates, splitting 2D and 3D convolutions for audio-visual feature extraction, and applying gated recurrent units with low computational complexity for cross-modal modeling. Ex-perimental results on the AVA-ActiveSpeaker dataset reveal that the proposed framework achieves competitive mAP per-formance (94.1% vs. 94.2%), while the resource costs are significantly lower than the state-of-the-art method, partic-ularly in model parameters (1.0M vs. 22.5M, approximately 23×) and FLOPs (0.6G vs. 2.6G, approximately 4×). Ad-ditionally, the proposed framework also performs well on the Columbia dataset, thus demonstrating good robustness.The code and model weights are available at https://github.com/Junhua-Liao/Light-ASD. 