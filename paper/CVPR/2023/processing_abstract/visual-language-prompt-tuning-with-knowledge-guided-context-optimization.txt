Prompt tuning is an effective way to adapt the pretrained visual-language model (VLM) to the downstream task us-ing task-related textual tokens. Representative CoOp-based work combines the learnable textual tokens with the class tokens to obtain speciﬁc textual knowledge. However, the speciﬁc textual knowledge is worse generalization to the unseen classes because it forgets the essential general tex-tual knowledge having a strong generalization ability. To tackle this issue, we introduce a novel Knowledge-guidedContext Optimization (KgCoOp) to enhance the generaliza-tion ability of the learnable prompt for unseen classes. The key insight of KgCoOp is that the forgetting about essen-tial knowledge can be alleviated by reducing the discrep-ancy between the learnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the discrepancy be-tween the textual embeddings generated by learned prompts and the hand-crafted prompts. Finally, adding the Kg-CoOp upon the contrastive loss can make a discriminative prompt for both seen and unseen tasks. Extensive eval-uation of several benchmarks demonstrates that the pro-posed Knowledge-guided Context Optimization is an efﬁ-cient method for prompt tuning, i.e., achieves better perfor-mance with less training time. code. 