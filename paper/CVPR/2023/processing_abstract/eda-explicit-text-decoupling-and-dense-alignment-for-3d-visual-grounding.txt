3D visual grounding aims to find the object within point clouds mentioned by free-form natural language descrip-tions with rich semantic cues. However, existing methods either extract the sentence-level features coupling all words or focus more on object names, which would lose the word-level information or neglect other attributes. To alleviate these issues, we present EDA that Explicitly Decouples the textual attributes in a sentence and conducts DenseAlignment between such fine-grained language and point cloud objects. Specifically, we first propose a text decou-pling module to produce textual features for every seman-tic component. Then, we design two losses to supervise∗Corresponding author. This work was supported in part by ShenzhenResearch Project under Grant JCYJ20220531093215035. the dense matching between two modalities: position align-ment loss and semantic alignment loss. On top of that, we further introduce a new visual grounding task, locat-ing objects without object names, which can thoroughly evaluate the model’s dense alignment capacity. Through experiments, we achieve state-of-the-art performance on two widely-adopted 3D visual grounding datasets, Scan-Refer and SR3D/NR3D, and obtain absolute leadership on our newly-proposed task. The source code is available at https://github.com/yanmin-wu/EDA. 