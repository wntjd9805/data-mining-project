Source DomainTarget Domain UpcyclingDeep neural networks (DNNs) often perform poorly in the presence of domain shift and category shift. How to up-cycle DNNs and adapt them to the target task remains an important open problem. Unsupervised Domain Adapta-tion (UDA), especially recently proposed Source-free Do-main Adaptation (SFDA), has become a promising tech-nology to address this issue. Nevertheless, existing SFDA methods require that the source domain and target domain share the same label space, consequently being only ap-plicable to the vanilla closed-set setting. In this paper, we take one step further and explore the Source-free Univer-sal Domain Adaptation (SF-UniDA). The goal is to iden-tify “known” data samples under both domain and category shift, and reject those “unknown” data samples (not present in source classes), with only the knowledge from standard pre-trained source model. To this end, we introduce an innovative global and local clustering learning technique (GLC). Speciﬁcally, we design a novel, adaptive one-vs-all global clustering algorithm to achieve the distinction across different target classes and introduce a local k-NN cluster-ing strategy to alleviate negative transfer. We examine the superiority of our GLC on multiple benchmarks with differ-ent category shift scenarios, including partial-set, open-set, and open-partial-set DA. Remarkably, in the most challeng-ing open-partial-set DA scenario, GLC outperforms UMAD by 14.8% on the VisDA benchmark. The code is available at https://github.com/ispc-lab/GLC. 