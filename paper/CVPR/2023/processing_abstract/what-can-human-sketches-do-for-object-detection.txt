Sketches are highly expressive, inherently capturing sub-jective and fine-grained visual cues. The exploration of such innate properties of human sketches has, however, been lim-ited to that of image retrieval. In this paper, for the first time, we cultivate the expressiveness of sketches but for the fundamental vision task of object detection. The end result is a sketch-enabled object detection framework that detects based on what you sketch – that “zebra” (e.g., one that is eating the grass) in a herd of zebras (instance-aware de-tection), and only the part (e.g., “head” of a “zebra”) that you desire (part-aware detection). We further dictate that our model works without (i) knowing which category to ex-pect at testing (zero-shot) and (ii) not requiring additional bounding boxes (as per fully supervised) and class labels (as per weakly supervised).Instead of devising a model from the ground up, we show an intuitive synergy between foundation models (e.g., CLIP) and existing sketch models build for sketch-based image retrieval (SBIR), which can al-ready elegantly solve the task – CLIP to provide model gen-eralisation, and SBIR to bridge the (sketch→photo) gap.In particular, we first perform independent prompting on both sketch and photo branches of an SBIR model to build highly generalisable sketch and photo encoders on the back of the generalisation ability of CLIP. We then devise a train-ing paradigm to adapt the learned encoders for object de-tection, such that the region embeddings of detected boxes are aligned with the sketch and photo embeddings fromSBIR. Evaluating our framework on standard object de-tection datasets like PASCAL-VOC and MS-COCO outper-forms both supervised (SOD) and weakly-supervised ob-ject detectors (WSOD) on zero-shot setups. Project Page: https:// pinakinathc.github.io/ sketch-detect 