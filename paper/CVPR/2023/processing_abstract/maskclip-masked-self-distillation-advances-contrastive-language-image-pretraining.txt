This paper presents a simple yet effective frameworkMaskCLIP, which incorporates a newly proposed masked self-distillation into contrastive language-image pretraining.The core idea of masked self-distillation is to distill repre-sentation from a full image to the representation predicted from a masked image. Such incorporation enjoys two vital beneﬁts. First, masked self-distillation targets local patch representation learning, which is complementary to vision-language contrastive focusing on text-related representa-tion. Second, masked self-distillation is also consistent with vision-language contrastive from the perspective of train-ing objective as both utilize the visual encoder for feature aligning, and thus is able to learn local semantics getting indirect supervision from the language. We provide specially designed experiments with a comprehensive analysis to vali-date the two beneﬁts. Symmetrically, we also introduce the local semantic supervision into the text branch, which further improves the pretraining performance. With extensive exper-iments, we show that MaskCLIP, when applied to various challenging downstream tasks, achieves superior results in linear probing, ﬁnetuning, and zero-shot performance with the guidance of the language encoder. Code will be release at https://github.com/LightDXY/MaskCLIP. 