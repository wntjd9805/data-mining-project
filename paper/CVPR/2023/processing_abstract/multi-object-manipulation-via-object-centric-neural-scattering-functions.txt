Learned visual dynamics models have proven effective for robotic manipulation tasks. Yet, it remains unclear how best to represent scenes involving multi-object interactions.Current methods decompose a scene into discrete objects, but they struggle with precise modeling and manipulation amid challenging lighting conditions as they only encode appearance tied with speciÔ¨Åc illuminations. In this work, we propose using object-centric neural scattering functions (OSFs) as object representations in a model-predictive con-trol framework. OSFs model per-object light transport, en-abling compositional scene re-rendering under object re-arrangement and varying lighting conditions. By combin-ing this approach with inverse parameter estimation and graph-based neural dynamics models, we demonstrate im-proved model-predictive control performance and general-ization in compositional multi-object environments, even in previously unseen scenarios and harsh lighting conditions. 