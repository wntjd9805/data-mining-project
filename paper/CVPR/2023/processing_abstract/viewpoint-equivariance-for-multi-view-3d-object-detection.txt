3D object detection from visual sensors is a corner-stone capability of robotic systems. State-of-the-art meth-ods focus on reasoning and decoding object bounding boxes from multi-view camera input. In this work we gain intu-ition from the integral role of multi-view consistency in 3D scene understanding and geometric learning. To this end, we introduce VEDet, a novel 3D object detection frame-work that exploits 3D multi-view geometry to improve lo-calization through viewpoint awareness and equivariance.VEDet leverages a query-based transformer architecture and encodes the 3D scene by augmenting image features with positional encodings from their 3D perspective geom-etry. We design view-conditioned queries at the output level, which enables the generation of multiple virtual frames dur-ing training to learn viewpoint equivariance by enforcing multi-view consistency. The multi-view geometry injected at the input level as positional encodings and regularized at the loss level provides rich geometric cues for 3D ob-ject detection, leading to state-of-the-art performance on the nuScenes benchmark. The code and model are made available at https://github.com/TRI-ML/VEDet. 