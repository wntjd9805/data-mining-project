also provide a variety of baseline methods and share useful design practices for future work.Towards building comprehensive real-world visual per-ception systems, we propose and study a new problem called panoptic scene graph generation (PVSG). PVSG is related to the existing video scene graph generation (VidSGG) problem, which focuses on temporal interactions between humans and objects localized with bounding boxes in videos. However, the limitation of bounding boxes in detecting non-rigid objects and backgrounds often causesVidSGG systems to miss key details that are crucial for comprehensive video understanding. In contrast, PVSG re-quires nodes in scene graphs to be grounded by more pre-cise, pixel-level segmentation masks, which facilitate holis-tic scene understanding. To advance research in this new area, we contribute a high-quality PVSG dataset, which consists of 400 videos (289 third-person + 111 egocentric videos) with totally 150K frames labeled with panoptic seg-mentation masks as well as fine, temporal scene graphs. We 