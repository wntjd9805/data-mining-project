Knowledge Distillation (KD) aims at distilling the knowledge from the large teacher model to a lightweight student model. Mainstream KD methods can be divided into two categories, logit distillation, and feature distilla-tion. The former is easy to implement, but inferior in per-formance, while the latter is not applicable to some prac-tical circumstances due to concerns such as privacy and safety. Towards this dilemma, in this paper, we explore a stronger logit distillation method via making better uti-lization of logit outputs. Concretely, we propose a sim-ple yet effective approach to logit distillation via multi-level prediction alignment. Through this framework, the prediction alignment is not only conducted at the instance level, but also at the batch and class level, through which the student model learns instance prediction, input corre-lation, and category correlation simultaneously.In addi-tion, a prediction augmentation mechanism based on model calibration further boosts the performance. Extensive ex-periment results validate that our method enjoys consis-tently higher performance than previous logit distillation methods, and even reaches competitive performance with mainstream feature distillation methods. Code is avail-able at https://github.com/Jin-Ying/Multi-Level-Logit-Distillation. 