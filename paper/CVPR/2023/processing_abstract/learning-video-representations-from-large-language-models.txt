We introduce LAVILA, a new approach to learning video-language representations by leveraging Large Lan-guage Models (LLMs). We repurpose pre-trained LLMs to be conditioned on visual input, and ﬁnetune them to create automatic video narrators. Our auto-generated narrations offer a number of advantages, including dense coverage of long videos, better temporal synchronization of the vi-sual information and text, and much higher diversity of text.The video-language embedding learned contrastively with these narrations outperforms the previous state-of-the-art on multiple ﬁrst-person and third-person video tasks, both in zero-shot and ﬁnetuned setups. Most notably, LAVILA obtains an absolute gain of 10.1% on EGTEA classiﬁca-tion and 5.9% Epic-Kitchens-100 multi-instance retrieval benchmarks. Furthermore, LAVILA trained with only half the narrations from the Ego4D dataset outperforms models trained on the full set, and shows positive scaling behavior on increasing pre-training data and model size. 