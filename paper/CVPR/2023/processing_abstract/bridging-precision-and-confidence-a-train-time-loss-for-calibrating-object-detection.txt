Deep neural networks (DNNs) have enabled astound-ing progress in several vision-based problems. Despite showing high predictive accuracy, recently, several works have revealed that they tend to provide overconfident pre-dictions and thus are poorly calibrated. The majority of the works addressing the miscalibration of DNNs fall un-der the scope of classification and consider only in-domain predictions. However, there is little to no progress in study-ing the calibration of DNN-based object detection models, which are central to many vision-based safety-critical ap-plications. In this paper, inspired by the train-time calibra-tion methods, we propose a novel auxiliary loss formulation that explicitly aims to align the class confidence of bound-ing boxes with the accurateness of predictions (i.e. preci-sion). Since the original formulation of our loss depends on the counts of true positives and false positives in a mini-batch, we develop a differentiable proxy of our loss that can be used during training with other application-specific loss functions. We perform extensive experiments on challeng-ing in-domain and out-domain scenarios with six bench-mark datasets including MS-COCO, Cityscapes, Sim10k, and BDD100k. Our results reveal that our train-time loss surpasses strong calibration baselines in reducing calibra-tion error for both in and out-domain scenarios. Our source code and pre-trained models are available at https:// github.com/akhtarvision/bpc_calibration 