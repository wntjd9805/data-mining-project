Pre-training of deep convolutional neural networks (DC-NNs) plays a crucial role in the field of visual sentiment anal-ysis (VSA). Most proposed methods employ the off-the-shelf backbones pre-trained on large-scale object classification datasets (i.e., ImageNet). While it boosts performance for a big margin against initializing model states from random, we argue that DCNNs simply pre-trained on ImageNet may excessively focus on recognizing objects, but failed to pro-vide high-level concepts in terms of sentiment. To address this long-term overlooked problem, we propose a sentiment-oriented pre-training method that is built upon human visual sentiment perception (VSP) mechanism. Specifically, we fac-torize the process of VSP into three steps, namely stimuli taking, holistic organizing, and high-level perceiving. From imitating each VSP step, a total of three models are sepa-rately pre-trained via our devised sentiment-aware tasks that contribute to excavating sentiment-discriminated represen-tations. Moreover, along with our elaborated multi-model amalgamation strategy, the prior knowledge learned from each perception step can be effectively transferred into a sin-gle target model, yielding substantial performance gains. Fi-nally, we verify the superiorities of our proposed method over extensive experiments, covering mainstream VSA tasks from single-label learning (SLL), multi-label learning (MLL), to label distribution learning (LDL). Experiment results demon-strate that our proposed method leads to unanimous improve-ments in these downstream tasks. Our code is released on https://github.com/tinglyfeng/sentiment_pretraining. 