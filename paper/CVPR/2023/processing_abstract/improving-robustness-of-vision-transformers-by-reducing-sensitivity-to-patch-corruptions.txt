Despite their success, vision transformers still remain vulnerable to image corruptions, such as noise or blur. In-deed, we find that the vulnerability mainly stems from the unstable self-attention mechanism, which is inherently built upon patch-based inputs and often becomes overly sensi-tive to the corruptions across patches. For example, when we only occlude a small number of patches with random noise (e.g., 10%), these patch corruptions would lead to se-vere accuracy drops and greatly distract intermediate at-tention layers. To address this, we propose a new training method that improves the robustness of transformers from a new perspective â€“ reducing sensitivity to patch corruptions (RSPC). Specifically, we first identify and occlude/corrupt the most vulnerable patches and then explicitly reduce sen-sitivity to them by aligning the intermediate features be-tween clean and corrupted examples. We highlight that the construction of patch corruptions is learned adversarially to the following feature alignment process, which is partic-ularly effective and essentially different from existing meth-ods. In experiments, our RSPC greatly improves the sta-bility of attention layers and consistently yields better ro-bustness on various benchmarks, including CIFAR-10/100-C, ImageNet-A, ImageNet-C, and ImageNet-P. 