ViT-Tiny ViT-SmallViT-BaseViT-LargeThe real-world data tends to be heavily imbalanced and severely skew the data-driven deep neural networks, which makes Long-Tailed Recognition (LTR) a massive challeng-ing task. Existing LTR methods seldom train Vision Trans-formers (ViTs) with Long-Tailed (LT) data, while the off-the-shelf pretrain weight of ViTs always leads to unfair comparisons. In this paper, we systematically investigate the ViTsâ€™ performance in LTR and propose LiVT to trainViTs from scratch only with LT data. With the observa-tion that ViTs suffer more severe LTR problems, we con-duct Masked Generative Pretraining (MGP) to learn gener-alized features. With ample and solid evidence, we show that MGP is more robust than supervised manners. Al-though Binary Cross Entropy (BCE) loss performs well withViTs, it struggles on the LTR tasks. We further propose the balanced BCE to ameliorate it with strong theoreti-cal groundings. Specially, we derive the unbiased exten-sion of Sigmoid and compensate extra logit margins for de-ploying it. Our Bal-BCE contributes to the quick conver-gence of ViTs in just a few epochs. Extensive experiments demonstrate that with MGP and Bal-BCE, LiVT success-fully trains ViTs well without any additional data and out-performs comparable state-of-the-art methods significantly, e.g., our ViT-B achieves 81.0% Top-1 accuracy in iNatural-ist 2018 without bells and whistles. Code is available at https://github.com/XuZhengzhuo/LiVT. 