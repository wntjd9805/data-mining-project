Adopting contrastive image-text pretrained models likeCLIP towards video classification has gained attention due to its cost-effectiveness and competitive performance. How-ever, recent works in this area face a trade-off. Fine-tuning the pretrained model to achieve strong supervised performance results in low zero-shot generalization. Sim-ilarly, freezing the backbone to retain zero-shot capabil-ity causes significant drop in supervised accuracy. Be-cause of this, recent works in literature typically train sep-arate models for supervised and zero-shot action recog-In this work, we propose a multimodal prompt nition. learning scheme that works to balance the supervised and zero-shot performance under a single unified train-ing. Our prompting approach on the vision side caters for three aspects: 1) Global video-level prompts to model the data distribution; 2) Local frame-level prompts to provide per-frame discriminative conditioning; and 3) a summary prompt to extract a condensed video representation. Ad-ditionally, we define a prompting scheme on the text side to augment the textual context. Through this prompting scheme, we can achieve state-of-the-art zero-shot perfor-mance on Kinetics-600, HMDB51 and UCF101 while re-maining competitive in the supervised setting. By keep-ing the pretrained backbone frozen, we optimize a much lower number of parameters and retain the existing gen-eral representation which helps achieve the strong zero-shot performance. Our codes/models will be released at https://github.com/TalalWasim/Vita-CLIP.. 