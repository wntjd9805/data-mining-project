Integrated Gradients (IG) as well as its variants are well-known techniques for interpreting the decisions of deep neu-ral networks. While IG-based approaches attain state-of-the-art performance, they often integrate noise into their ex-planation saliency maps, which reduce their interpretabil-ity. To minimize the noise, we examine the source of the noise analytically and propose a new approach to re-duce the explanation noise based on our analytical find-ings. We propose the Important Direction Gradient Integra-tion (IDGI) framework, which can be easily incorporated into any IG-based method that uses the Reimann Integra-tion for integrated gradient computation. Extensive exper-iments with three IG-based methods show that IDGI im-proves them drastically on numerous interpretability met-rics. The source code for IDGI is available at https://github.com/yangruo1226/IDGI. 