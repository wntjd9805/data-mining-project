Both masked image modeling (MIM) and natural lan-guage supervision have facilitated the progress of trans-ferable visual pre-training. In this work, we seek the syn-ergy between two paradigms and study the emerging prop-erties when MIM meets natural language supervision. To this end, we present a novel masked visual ReconstructionIn Language semantic Space (RILS) pre-training frame-work, in which sentence representations, encoded by the text encoder, serve as prototypes to transform the vision-only signals into patch-sentence probabilities as semantically meaningful MIM reconstruction targets. The vision models can therefore capture useful components with structured in-formation by predicting proper semantic of masked tokens.Better visual representations could, in turn, improve the text encoder via the image-text alignment objective, which is essential for the effective MIM target transformation. Ex-tensive experimental results demonstrate that our method not only enjoys the best of previous MIM and CLIP but also achieves further improvements on various tasks due to their mutual benefits. RILS exhibits advanced transferabil-ity on downstream classification, detection, and segmenta-tion, especially for low-shot regimes. Code is available at https://github.com/hustvl/RILS. 