Existing studies indicate that deep neural networks (DNNs) can eventually memorize the label noise. We ob-serve that the memorization strength of DNNs towards each instance is different and can be represented by the con-fidence value, which becomes larger and larger during the training process. Based on this, we propose a Dy-namic Instance-specific Selection and Correction method (DISC) for learning from noisy labels (LNL). We first use a two-view-based backbone for image classification, ob-taining confidence for each image from two views. Then we propose a dynamic threshold strategy for each instance, based on the momentum of each instance’s memorization strength in previous epochs to select and correct noisy la-beled data. Benefiting from the dynamic threshold strategy and two-view learning, we can effectively group each in-stance into one of the three subsets (i.e., clean, hard, and purified) based on the prediction consistency and discrep-ancy by two views at each epoch. Finally, we employ differ-ent regularization strategies to conquer subsets with differ-ent degrees of label noise, improving the whole network’s robustness. Comprehensive evaluations on three control-lable and four real-world LNL benchmarks show that our method outperforms the state-of-the-art (SOTA) methods to leverage useful information in noisy data while allevi-ating the pollution of label noise. Code is available at https://github.com/JackYFL/DISC. 