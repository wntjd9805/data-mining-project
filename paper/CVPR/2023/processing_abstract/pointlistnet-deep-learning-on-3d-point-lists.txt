tiveness of the proposed PointListNet.Deep neural networks on regular 1D lists (e.g., natural languages) and irregular 3D sets (e.g., point clouds) have made tremendous achievements. The key to natural lan-guage processing is to model words and their regular or-der dependency in texts. For point cloud understanding, the challenge is to understand the geometry via irregular point coordinates, in which point-feeding orders do not matter.However, there are a few kinds of data that exhibit both reg-ular 1D list and irregular 3D set structures, such as proteins and non-coding RNAs. In this paper, we refer to them as 3D point lists and propose a Transformer-style PointListNet to model them. First, PointListNet employs non-parametric distance-based attention because we find sometimes it is the distance, instead of the feature or type, that mainly deter-mines how much two points, e.g., amino acids, are corre-lated in the micro world. Second, different from the vanillaTransformer that directly performs a simple linear transfor-mation on inputs to generate values and does not explicitly model relative relations, our PointListNet integrates the 1D order and 3D Euclidean displacements into values. We con-duct experiments on protein fold classification and enzyme reaction classification. Experimental results show the effec-