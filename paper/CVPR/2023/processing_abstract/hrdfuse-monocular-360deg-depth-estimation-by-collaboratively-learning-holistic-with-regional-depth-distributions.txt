Depth estimation from a monocular 360◦ image is a bur-geoning problem owing to its holistic sensing of a scene.Recently, some methods, e.g., OmniFusion, have applied the tangent projection (TP) to represent a 360◦ image and predicted depth values via patch-wise regressions, which are merged to get a depth map with equirectangular pro-jection (ERP) format. However, these methods suffer from 1) non-trivial process of merging plenty of patches; 2) cap-turing less holistic-with-regional contextual information byIn this directly regressing the depth value of each pixel. paper, we propose a novel framework, HRDFuse, that sub-tly combines the potential of convolutional neural networks (CNNs) and transformers by collaboratively learning the holistic contextual information from the ERP and the re-gional structural information from the TP. Firstly, we pro-pose a spatial feature alignment (SFA) module that learns feature similarities between the TP and ERP to aggregate the TP features into a complete ERP feature map in a pixel-wise manner. Secondly, we propose a collaborative depth distribution classification (CDDC) module that learns the holistic-with-regional histograms capturing the ERP andTP depth distributions. As such, the final depth values can be predicted as a linear combination of histogram bin cen-ters. Lastly, we adaptively combine the depth predictions from ERP and TP to obtain the final depth map. Extensive experiments show that our method predicts more smooth and accurate depth results while achieving favorably bet-ter results than the SOTA methods.Multimedia MaterialFor videos, code, demo and more information, you can visit https://VLIS2022.github.io/HRDFuse/ 