Recent popular Role-Playing Games (RPGs) saw the great success of character auto-creation systems. The bone-driven face model controlled by continuous parameters (like the position of bones) and discrete parameters (like the hairstyles) makes it possible for users to personalize and customize in-game characters. Previous in-game character auto-creation systems are mostly image-driven, where fa-cial parameters are optimized so that the rendered charac-ter looks similar to the reference face photo. This paper pro-poses a novel text-to-parameter translation method (T2P) to achieve zero-shot text-driven game character auto-creation.With our method, users can create a vivid in-game char-acter with arbitrary text description without using any ref-erence photo or editing hundreds of parameters manually.In our method, taking the power of large-scale pre-trained multi-modal CLIP and neural rendering, T2P searches both continuous facial parameters and discrete facial parame-*Corresponding Authors. ters in a unified framework. Due to the discontinuous pa-rameter representation, previous methods have difficulty in effectively learning discrete facial parameters. T2P, to our best knowledge, is the first method that can handle the op-timization of both discrete and continuous parameters. Ex-perimental results show that T2P can generate high-quality and vivid game characters with given text prompts. T2P outperforms other SOTA text-to-3D generation methods on both objective evaluations and subjective evaluations. 