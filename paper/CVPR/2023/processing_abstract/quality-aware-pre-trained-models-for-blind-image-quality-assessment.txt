Blind image quality assessment (BIQA) aims to auto-matically evaluate the perceived quality of a single image, whose performance has been improved by deep learning-based methods in recent years. However, the paucity of la-beled data somewhat restrains deep learning-based BIQA methods from unleashing their full potential.In this pa-per, we propose to solve the problem by a pretext task customized for BIQA in a self-supervised learning manner, which enables learning representations from orders of mag-nitude more data. To constrain the learning process, we propose a quality-aware contrastive loss based on a simple assumption: the quality of patches from a distorted image should be similar, but vary from patches from the same im-age with different degradations and patches from different images. Further, we improve the existing degradation pro-cess and form a degradation space with the size of roughly 2 Ã— 107. After pre-trained on ImageNet using our method, models are more sensitive to image quality and perform sig-nificantly better on downstream BIQA tasks. Experimental results show that our method obtains remarkable improve-ments on popular BIQA datasets. 