Dataset Distillation (DD), a newly emerging field, aims at generating much smaller but efficient synthetic training datasets from large ones. Existing DD methods based on gradient matching achieve leading performance; however, they are extremely computationally intensive as they re-quire continuously optimizing a dataset among thousands of randomly initialized models. In this paper, we assume that training the synthetic data with diverse models leads to better generalization performance. Thus we propose two model augmentation techniques, i.e. using early-stage models and parameter perturbation to learn an informative synthetic set with significantly reduced training cost. Exten-sive experiments demonstrate that our method achieves up to 20× speedup and comparable performance on par with state-of-the-art methods.Figure 1.Performances of condensed datasets for trainingConvNet-3 v.s. GPU hours to learn the 10 images per class con-densed CIFAR-10 datasets with a single RTX-2080 GPU. Ours5,Ours10, and Ours20 accelerates the training speed of the state-of-the-art method IDC [22] 5×, 10×, and 20× faster. 