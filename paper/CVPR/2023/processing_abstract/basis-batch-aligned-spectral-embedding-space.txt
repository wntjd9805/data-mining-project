Graph is a highly generic and diverse representation, suitable for almost any data processing problem. Spec-tral graph theory has been shown to provide powerful algo-rithms, backed by solid linear algebra theory. It thus can be extremely instrumental to design deep network build-ing blocks with spectral graph characteristics. For in-stance, such a network allows the design of optimal graphs for certain tasks or obtaining a canonical orthogonal low-dimensional embedding of the data. Recent attempts to solve this problem were based on minimizing Rayleigh-quotient type losses. We propose a different approach of directly learning the graphâ€™s eigensapce. A severe prob-lem of the direct approach, applied in batch-learning, is the inconsistent mapping of features to eigenspace coordinates in different batches. We analyze the degrees of freedom of learning this task using batches and propose a stable align-ment mechanism that can work both with batch changes and with graph-metric changes. We show that our learnt spec-tral embedding is better in terms of NMI, ACC, Grassman distnace, orthogonality and classification accuracy, com-pared to SOTA. In addition, the learning is more stable. 