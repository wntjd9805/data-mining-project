The appearance of an object can be fleeting when it transforms. As eggs are broken or paper is torn, their color, shape and texture can change dramatically, preserving vir-tually nothing of the original except for the identity itself.Yet, this important phenomenon is largely absent from ex-isting video object segmentation (VOS) benchmarks. In this work, we close the gap by collecting a new dataset forVideo Object Segmentation under Transformations (VOST).It consists of more than 700 high-resolution videos, cap-tured in diverse environments, which are 20 seconds long on average and densely labeled with instance masks. We adopt a careful, multi-step approach to ensure that these videos focus on complex object transformations, capturing their full temporal extent. We then extensively evaluate state-of-the-art VOS methods and make a number of important discoveries. In particular, we show that existing methods struggle when applied to this novel task and that their main limitation lies in over-reliance on static appearance cues.This motivates us to propose a few modifications for the top-performing baseline that improve its capabilities by better modeling spatio-temporal information. More broadly, our work highlights the need for further research on learning more robust video object representations.Nothing is lost or created, all things are merely transformed.Antoine Lavoisier 