Deep neural networks have made huge progress in the last few decades. However, as the real-world data often ex-hibits a long-tailed distribution, vanilla deep models tend to be heavily biased toward the majority classes. To ad-dress this problem, state-of-the-art methods usually adopt a mixture of experts (MoE) to focus on different parts of the long-tailed distribution. Experts in these methods are with the same model depth, which neglects the fact that different classes may have different preferences to be fit by models with different depths. To this end, we propose a novel MoE-based method called Self-Heterogeneous Integration withKnowledge Excavation (SHIKE). We first propose Depth-wise Knowledge Fusion (DKF) to fuse features between dif-ferent shallow parts and the deep part in one network for each expert, which makes experts more diverse in terms of representation. Based on DKF, we further propose DynamicKnowledge Transfer (DKT) to reduce the influence of the hardest negative class that has a non-negligible impact on the tail classes in our MoE framework. As a result, the clas-sification accuracy of long-tailed data can be significantly improved, especially for the tail classes. SHIKE achieves the state-of-the-art performance of 56.3%, 60.3%, 75.4% and 41.9% on CIFAR100-LT (IF100), ImageNet-LT, iNatu-ralist 2018, and Places-LT, respectively. The source code is available at https://github.com/jinyan-06/SHIKE. 