Unsupervised domain adaptation (UDA) approaches fo-cus on adapting models trained on a labeled source do-main to an unlabeled target domain. In contrast to UDA, source-free domain adaptation (SFDA) is a more practi-cal setup as access to source data is no longer required during adaptation. Recent state-of-the-art (SOTA) methods on SFDA mostly focus on pseudo-label refinement based self-training which generally suffers from two issues: i) in-evitable occurrence of noisy pseudo-labels that could lead to early training time memorization, ii) refinement process requires maintaining a memory bank which creates a signif-icant burden in resource constraint scenarios. To address these concerns, we propose C-SFDA, a curriculum learn-ing aided self-training framework for SFDA that adapts effi-ciently and reliably to changes across domains based on se-lective pseudo-labeling. Specifically, we employ a curricu-lum learning scheme to promote learning from a restricted amount of pseudo labels selected based on their reliabili-ties. This simple yet effective step successfully prevents la-bel noise propagation during different stages of adaptation and eliminates the need for costly memory-bank based label refinement. Our extensive experimental evaluations on both image recognition and semantic segmentation tasks confirm the effectiveness of our method. C-SFDA is also applica-ble to online test-time domain adaptation and outperforms previous SOTA methods in this task. 