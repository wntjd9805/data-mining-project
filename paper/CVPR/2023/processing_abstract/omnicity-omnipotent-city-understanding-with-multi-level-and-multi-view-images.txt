This paper presents OmniCity, a new dataset for omnipo-tent city understanding from multi-level and multi-view im-ages. More precisely, OmniCity contains multi-view satel-lite images as well as street-level panorama and mono-view images, constituting over 100K pixel-wise annotated images that are well-aligned and collected from 25K geo-locations in New York City. To alleviate the substantial*Corresponding authors. pixel-wise annotation efforts, we propose an efficient street-view image annotation pipeline that leverages the exist-ing label maps of satellite view and the transformation re-lations between different views (satellite, panorama, and mono-view). With the new OmniCity dataset, we pro-vide benchmarks for a variety of tasks including build-ing footprint extraction, height estimation, and building plane/instance/fine-grained segmentation. Compared with existing multi-level and multi-view benchmarks, OmniCity contains a larger number of images with richer annota-tion types and more views, provides more benchmark resultsof state-of-the-art models, and introduces a new task for fine-grained building instance segmentation on street-level panorama images. Moreover, OmniCity provides new prob-lem settings for existing tasks, such as cross-view image matching, synthesis, segmentation, detection, etc., and fa-cilitates the developing of new methods for large-scale city understanding, reconstruction, and simulation. The Om-niCity dataset as well as the benchmarks will be released at https://city-super.github.io/omnicity/. 