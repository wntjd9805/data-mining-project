Self-supervised representation learning techniques have been developing rapidly to make full use of unlabeled im-ages. They encode images into rich features that are oblivi-ous to downstream tasks. Behind their revolutionary repre-sentation power, the requirements for dedicated model de-signs and a massive amount of computation resources ex-pose image encoders to the risks of potential model steal-ing attacks - a cheap way to mimic the well-trained en-coder performance while circumventing the demanding re-quirements. Yet conventional attacks only target supervised classiﬁers given their predicted labels and/or posteriors, which leaves the vulnerability of unsupervised encoders un-explored.In this paper, we ﬁrst instantiate the conventional steal-ing attacks against encoders and demonstrate their severer vulnerability compared with downstream classiﬁers. To bet-ter leverage the rich representation of encoders, we fur-ther propose Cont-Steal, a contrastive-learning-based at-tack, and validate its improved stealing effectiveness in var-ious experiment settings. As a takeaway, we appeal to our community’s attention to the intellectual property protec-tion of representation learning techniques, especially to the defenses against encoder stealing attacks like ours. 1 