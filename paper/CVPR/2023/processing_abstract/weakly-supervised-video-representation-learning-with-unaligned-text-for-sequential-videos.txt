Sequential video understanding, as an emerging video understanding task, has driven lots of researchersâ€™ atten-tion because of its goal-oriented nature. This paper studies weakly supervised sequential video understanding where the accurate time-stamp level text-video alignment is not provided. We solve this task by borrowing ideas from CLIP.Specifically, we use a transformer to aggregate frame-level features for video representation and use a pre-trained text encoder to encode the texts corresponding to each action and the whole video, respectively. To model the corre-spondence between text and video, we propose a multi-ple granularity loss, where the video-paragraph contrastive loss enforces matching between the whole video and the complete script, and a fine-grained frame-sentence con-trastive loss enforces the matching between each action and its description. As the frame-sentence correspondence is not available, we propose to use the fact that video ac-tions happen sequentially in the temporal domain to gen-erate pseudo frame-sentence correspondence and super-vise the network training with the pseudo labels. Exten-sive experiments on video sequence verification and text-to-video matching show that our method outperforms base-lines by a large margin, which validates the effectiveness of our proposed approach. Code is available at https://github.com/svip-lab/WeakSVR. 