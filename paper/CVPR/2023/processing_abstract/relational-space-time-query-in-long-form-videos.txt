Egocentric videos are often available in the form of un-interrupted, uncurated long videos capturing the camera wearersâ€™ daily life activities.Understanding these videos re-quires models to be able to reason about activities, objects, and their interactions. However, current video benchmarks study these problems independently and under short, cu-rated clips.In contrast, real-world applications, e.g. AR assistants, require bundling these problems for both model development and evaluation. In this paper, we propose to study these problems in a joint framework for long video understanding. Our contributions are three-fold. First, we propose an integrated framework, namely RelationalSpace-Time Query (ReST), for evaluating video under-standing models via templated spatiotemporal queries. Sec-ond, we introduce two new benchmarks, ReST-ADL andReST-Ego4D 1, which augment the existing egocentric video datasets with abundant query annotations generated by theReST framework. Finally, we present a set of baselines and 1The latest version of our benchmark and models will be available here. in-depth analysis on the two benchmarks and provide in-sights about the query tasks. We view our integrated frame-work and benchmarks as a step towards comprehensive, multi-step reasoning in long videos, and believe it will fa-cilitate the development of next generations of video under-standing models. 