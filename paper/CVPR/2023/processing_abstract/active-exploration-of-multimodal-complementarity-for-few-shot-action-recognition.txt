Recently, few-shot action recognition receives increasing attention and achieves remarkable progress. However, pre-vious methods mainly rely on limited unimodal data (e.g.,RGB frames) while the multimodal information remains rel-atively underexplored.In this paper, we propose a novelActive Multimodal Few-shot Action Recognition (AMFAR) framework, which can actively find the reliable modality for each sample based on task-dependent context information to improve few-shot reasoning procedure. In meta-training, we design an Active Sample Selection (ASS) module to or-ganize query samples with large differences in the reliabil-ity of modalities into different groups based on modality-specific posterior distributions. In addition, we design anActive Mutual Distillation (AMD) to capture discrimina-tive task-specific knowledge from the reliable modality to improve the representation learning of unreliable modal-In meta-test, ity by bidirectional knowledge distillation. we adopt Adaptive Multimodal Inference (AMI) to adap-tively fuse the modality-specific posterior distributions with a larger weight on the reliable modality. Extensive experi-mental results on four public benchmarks demonstrate that our model achieves significant improvements over existing unimodal and multimodal methods. 