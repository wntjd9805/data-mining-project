LiDAR and Radar are two complementary sensing ap-proaches in that LiDAR specializes in capturing an object’s 3D shape while Radar provides longer detection ranges as well as velocity hints. Though seemingly natural, how to efficiently combine them for improved feature representa-tion is still unclear. The main challenge arises from thatRadar data are extremely sparse and lack height informa-tion. Therefore, directly integrating Radar features intoLiDAR-centric detection networks is not optimal.In this work, we introduce a bi-directional LiDAR-Radar fusion framework, termed Bi-LRFusion, to tackle the challenges and improve 3D detection for dynamic objects. Technically,Bi-LRFusion involves two steps: first, it enriches Radar’s local features by learning important details from the LiDAR branch to alleviate the problems caused by the absence of height information and extreme sparsity; second, it com-bines LiDAR features with the enhanced Radar features in a unified bird’s-eye-view representation. We conduct ex-tensive experiments on nuScenes and ORR datasets, and show that our Bi-LRFusion achieves state-of-the-art perfor-mance for detecting dynamic objects. Notably, Radar data in these two datasets have different formats, which demon-strates the generalizability of our method. Codes are avail-able at https://github.com/JessieW0806/Bi-LRFusion.Figure 1. An illustration of (a) uni-directional LiDAR-Radar fu-sion mechanism, (b) our proposed bi-directional LiDAR-Radar fu-sion mechanism, and (c) the average precision gain (%) of uni-directional fusion method RadarNet∗ against the LiDAR-centric baseline CenterPoint [40] over categories with different average height (m). We use * to indicate it is re-produced by us on the Cen-terPoint. The improvement by involving Radar data is not consis-tent for objects with different height, i.e., taller objects like truck, bus and trailer do not enjoy as much performance gain. Note that all height values are transformed to the LiDAR coordinate system. 