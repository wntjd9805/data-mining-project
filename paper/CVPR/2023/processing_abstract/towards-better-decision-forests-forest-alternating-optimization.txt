Decision forests are among the most accurate models in machine learning. This is remarkable given that the way they are trained is highly heuristic: neither the individ-ual trees nor the overall forest optimize any well-deﬁned loss. While diversity mechanisms such as bagging or boost-ing have been until now critical in the success of forests, we think that a better optimization should lead to better forests—ideally eliminating any need for an ensembling heuristic. However, unlike for most other models, such as neural networks, optimizing forests or trees is not easy, be-cause they deﬁne a non-differentiable function. We show, for the ﬁrst time, that it is possible to learn a forest by op-timizing a desirable loss and regularization jointly over all its trees and parameters. Our algorithm, Forest AlternatingOptimization, is based on deﬁning a forest as a paramet-ric model with a ﬁxed number of trees and structure (rather than adding trees indeﬁnitely as in bagging or boosting).It then iteratively updates each tree in alternation so that the objective function decreases monotonically. The algo-rithm is so effective at optimizing that it easily overﬁts, but this can be corrected by averaging. The result is a forest that consistently exceeds the accuracy of the state-of-the-art while using fewer, smaller trees. 