Deep learning in general domains has constantly been extended to domain-speciﬁc tasks requiring the recognition of ﬁne-grained characteristics. However, real-world appli-cations for ﬁne-grained tasks suffer from two challenges: a high reliance on expert knowledge for annotation and ne-cessity of a versatile model for various downstream tasks in a speciﬁc domain (e.g., prediction of categories, bounding boxes, or pixel-wise annotations). Fortunately, the recent self-supervised learning (SSL) is a promising approach to pretrain a model without annotations, serving as an effective initialization for any downstream tasks. Since SSL does not rely on the presence of annotation, in general, it utilizes the large-scale unlabeled dataset, referred to as an open-set. In this sense, we introduce a novel Open-Set Self-SupervisedLearning problem under the assumption that a large-scale unlabeled open-set is available, as well as the ﬁne-grained target dataset, during a pretraining phase. In our problem setup, it is crucial to consider the distribution mismatch be-tween the open-set and target dataset. Hence, we proposeSimCore algorithm to sample a coreset, the subset of an open-set that has a minimum distance to the target dataset in the latent space. We demonstrate that SimCore signiﬁcantly improves representation learning performance through ex-tensive experimental settings, including eleven ﬁne-grained datasets and seven open-sets in various downstream tasks. 