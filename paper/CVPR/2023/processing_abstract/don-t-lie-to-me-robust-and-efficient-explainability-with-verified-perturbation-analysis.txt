A plethora of attribution methods have recently been de-veloped to explain deep neural networks. These methods use different classes of perturbations (e.g, occlusion, blur-ring, masking, etc) to estimate the importance of individ-ual image pixels to drive a model’s decision. Neverthe-less, the space of possible perturbations is vast and cur-rent attribution methods typically require signiﬁcant com-putation time to accurately sample the space in order to achieve high-quality explanations. In this work, we intro-duce EVA (Explaining using Veriﬁed Perturbation Analysis) – the ﬁrst explainability method which comes with guaran-tees that an entire set of possible perturbations has been exhaustively searched. We leverage recent progress in ver-iﬁed perturbation analysis methods to directly propagate bounds through a neural network to exhaustively probe a – potentially inﬁnite-size – set of perturbations in a single forward pass. Our approach takes advantage of the bene-ﬁcial properties of veriﬁed perturbation analysis, i.e., time efﬁciency and guaranteed complete – sampling agnostic – coverage of the perturbation space – to identify image pixels that drive a model’s decision. We evaluate EVA systemat-ically and demonstrate state-of-the-art results on multiple benchmarks. Our code is freely available: github.com/ deel-ai/formal-explainability 