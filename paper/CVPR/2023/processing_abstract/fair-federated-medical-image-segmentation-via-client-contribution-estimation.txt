How to ensure fairness is an important topic in feder-ated learning (FL). Recent studies have investigated how to reward clients based on their contribution (collaboration fairness), and how to achieve uniformity of performance across clients (performance fairness). Despite achieving progress on either one, we argue that it is critical to con-sider them together, in order to engage and motivate more diverse clients joining FL to derive a high-quality global model. In this work, we propose a novel method to opti-mize both types of fairness simultaneously. Speciﬁcally, we propose to estimate client contribution in gradient and data space. In gradient space, we monitor the gradient direc-tion differences of each client with respect to others. And in data space, we measure the prediction error on client data using an auxiliary model. Based on this contribu-tion estimation, we propose a FL method, federated train-ing via contribution estimation (FedCE), i.e., using estima-tion as global model aggregation weights. We have theo-retically analyzed our method and empirically evaluated it on two real-world medical datasets. The effectiveness of our approach has been validated with signiﬁcant perfor-mance improvements, better collaboration fairness, better performance fairness, and comprehensive analytical stud-ies. Code is available at https://nvidia.github. io/NVFlare/research/fed-ce 