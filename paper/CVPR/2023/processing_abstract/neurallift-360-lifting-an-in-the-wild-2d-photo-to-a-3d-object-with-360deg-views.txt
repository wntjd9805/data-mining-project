Virtual reality and augmented reality (XR) bring increas-ing demand for 3D content generation. However, creating high-quality 3D content requires tedious work from a hu-man expert. In this work, we study the challenging task of lifting a single image to a 3D object and, for the first time, demonstrate the ability to generate a plausible 3D object with 360â—¦ views that corresponds well with the given ref-erence image. By conditioning on the reference image, our model can fulfill the everlasting curiosity for synthesizing novel views of objects from images. Our technique sheds light on a promising direction of easing the workflows for 3D artists and XR designers. We propose a novel frame-work, dubbed NeuralLift-360, that utilizes a depth-aware neural radiance representation (NeRF) and learns to craft the scene guided by denoising diffusion models. By intro-ducing a ranking loss, our NeuralLift-360 can be guided with rough depth estimation in the wild. We also adopt a CLIP-guided sampling strategy for the diffusion prior to provide coherent guidance. Extensive experiments demon-strate that our NeuralLift-360 significantly outperforms ex-isting state-of-the-art baselines. Project page: https://vita-group.github.io/NeuralLift-360/ 