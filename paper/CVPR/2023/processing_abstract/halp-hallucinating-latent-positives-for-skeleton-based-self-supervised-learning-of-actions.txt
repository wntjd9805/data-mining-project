Supervised learning of skeleton sequence encoders for ac-tion recognition has received significant attention in recent times. However, learning such encoders without labels con-tinues to be a challenging problem. While prior works have shown promising results by applying contrastive learning to pose sequences, the quality of the learned representations is often observed to be closely tied to data augmentations that are used to craft the positives. However, augmenting pose sequences is a difficult task as the geometric constraints among the skeleton joints need to be enforced to make the augmentations realistic for that action. In this work, we pro-pose a new contrastive learning approach to train models for skeleton-based action recognition without labels. Our key contribution is a simple module, HaLP â€“ to Halluci-nate Latent Positives for contrastive learning. Specifically,HaLP explores the latent space of poses in suitable direc-tions to generate new positives. To this end, we present a novel optimization formulation to solve for the synthetic positives with an explicit control on their hardness. We pro-pose approximations to the objective, making them solv-able in closed form with minimal overhead. We show via experiments that using these generated positives within a standard contrastive learning framework leads to consistent improvements across benchmarks such as NTU-60, NTU-120, and PKU-II on tasks like linear evaluation, transfer learning, and kNN evaluation. Our code can be found at https://github.com/anshulbshah/HaLP.Figure 1. HaLP: We propose an approach to hallucinate latent posi-tives for use within a contrastive learning pipeline. Our approach works as follows: 1) We extract prototypes which succinctly repre-sent the data at a particular step in training, 2) We randomly select a prototype from the prototype set, 3) Our approach then determines an optimal vector which when added to the anchor can generate positives of varying hardness. We generate a number of positives using this approach which are then used within a contrastive learn-ing pipeline to train a model without labels. 