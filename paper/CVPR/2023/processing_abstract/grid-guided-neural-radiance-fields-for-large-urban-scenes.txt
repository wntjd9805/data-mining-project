Purely MLP-based neural radiance fields (NeRF-based methods) often suffer from underfitting with blurred ren-derings on large-scale scenes due to limited model capac-ity. Recent approaches propose to geographically divide the scene and adopt multiple sub-NeRFs to model each region individually, leading to linear scale-up in training costs and the number of sub-NeRFs as the scene expands. An alterna-tive solution is to use a feature grid representation, which is computationally efficient and can naturally scale to a large scene with increased grid resolutions. However, the feature grid tends to be less constrained and often reaches subop-timal solutions, producing noisy artifacts in renderings, es-pecially in regions with complex geometry and texture. In this work, we present a new framework that realizes high-fidelity rendering on large urban scenes while being com-putationally efficient. We propose to use a compact multi-resolution ground feature plane representation to coarsely capture the scene, and complement it with positional encod-ing inputs through another NeRF branch for rendering in a joint learning fashion. We show that such an integration can utilize the advantages of two alternative solutions: a light-weighted NeRF is sufficient, under the guidance of the feature grid representation, to render photorealistic novel views with fine details; and the jointly optimized ground fea-ture planes, can meanwhile gain further refinements, form-ing a more accurate and compact feature space and output much more natural rendering results. 