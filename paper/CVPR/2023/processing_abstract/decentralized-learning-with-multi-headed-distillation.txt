Decentralized learning with private data is a cen-tral problem in machine learning. We propose a novel distillation-based decentralized learning technique that al-lows multiple agents with private non-iid data to learn from each other, without having to share their data, weights or weight updates. Our approach is communication efﬁcient, utilizes an unlabeled public dataset and uses multiple aux-iliary heads for each client, greatly improving training ef-ﬁciency in the case of heterogeneous data. This approach allows individual models to preserve and enhance perfor-mance on their private tasks while also dramatically im-proving their performance on the global aggregated data distribution. We study the effects of data and model archi-tecture heterogeneity and the impact of the underlying com-munication graph topology on learning efﬁciency and show that our agents can signiﬁcantly improve their performance compared to learning in isolation. 