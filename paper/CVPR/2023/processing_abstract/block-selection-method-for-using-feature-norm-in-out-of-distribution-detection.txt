Detecting out-of-distribution (OOD) inputs during the inference stage is crucial for deploying neural networks in the real world. Previous methods typically relied on the highly activated feature map outputted by the network. In this study, we revealed that the norm of the feature map obtained from a block other than the last block can serve as a better indicator for OOD detection. To leverage this insight, we propose a simple framework that comprises two metrics: FeatureNorm, which computes the norm of the feature map, and NormRatio, which calculates the ra-tio of FeatureNorm for ID and OOD samples to evaluate the OOD detection performance of each block. To iden-tify the block that provides the largest difference betweenFeatureNorm of ID and FeatureNorm of OOD, we cre-ate jigsaw puzzles as pseudo OOD from ID training sam-ples and compute NormRatio, selecting the block with the highest value. After identifying the suitable block, OOD detection using FeatureNorm outperforms other methods by reducing FPR95 by up to 52.77% on CIFAR10 bench-mark and up to 48.53% on ImageNet benchmark. We demonstrate that our framework can generalize to vari-ous architectures and highlight the significance of block se-lection, which can also improve previous OOD detection methods. Our code is available at https://github.com/gist-ailab/block-selection-for-OOD-detection. 