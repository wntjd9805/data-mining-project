Clustering has been a major research topic in the field of machine learning, one to which Deep Learning has re-cently been applied with significant success. However, an aspect of clustering that is not addressed by existing deep clustering methods, is that of efficiently producing multi-ple, diverse partitionings for a given dataset. This is par-ticularly important, as a diverse set of base clusterings are necessary for consensus clustering, which has been found to produce better and more robust results than relying on a single clustering. To address this gap, we propose Div-Clust, a diversity controlling loss that can be incorporated into existing deep clustering frameworks to produce mul-tiple clusterings with the desired degree of diversity. We conduct experiments with multiple datasets and deep clus-tering frameworks and show that: a) our method effectively controls diversity across frameworks and datasets with very small additional computational cost, b) the sets of clus-terings learned by DivClust include solutions that signifi-cantly outperform single-clustering baselines, and c) using an off-the-shelf consensus clustering algorithm, DivClust produces consensus clustering solutions that consistently outperform single-clustering baselines, effectively improv-ing the performance of the base deep clustering frame-work. Code is available at https://github.com/ManiadisG/DivClust. 