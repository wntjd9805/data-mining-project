Many perception systems in mobile computing, au-tonomous navigation, and AR/VR face strict compute con-straints that are particularly challenging for high-resolution input images. Previous works propose nonuniform downsam-plers that "learn to zoom" on salient image regions, reducing compute while retaining task-relevant image information.However, for tasks with spatial labels (such as 2D/3D ob-ject detection and semantic segmentation), such distortions may harm performance. In this work (LZU), we "learn to zoom" in on the input image, compute spatial features, and then "unzoom" to revert any deformations. To enable ef-Ô¨Åcient and differentiable unzooming, we approximate the zooming warp with a piecewise bilinear mapping that is invertible. LZU can be applied to any task with 2D spa-tial input and any model with 2D spatial features, and we demonstrate this versatility by evaluating on a variety of tasks and datasets: object detection on Argoverse-HD, se-mantic segmentation on Cityscapes, and monocular 3D ob-ject detection on nuScenes. Interestingly, we observe boosts in performance even when high-resolution sensor data is unavailable, implying that LZU can be used to "learn to up-sample" as well. Code and additional visuals are available at https://tchittesh.github.io/lzu/. 