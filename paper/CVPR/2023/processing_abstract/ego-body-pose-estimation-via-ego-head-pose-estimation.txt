Estimating 3D human motion from an egocentric video sequence plays a critical role in human behavior understand-ing and has various applications in VR/AR. However, naively learning a mapping between egocentric videos and human motions is challenging, because the user’s body is often un-observed by the front-facing camera placed on the head of the user. In addition, collecting large-scale, high-quality datasets with paired egocentric videos and 3D human mo-tions requires accurate motion capture devices, which often limit the variety of scenes in the videos to lab-like environ-ments. To eliminate the need for paired egocentric video and human motions, we propose a new method, Ego-Body PoseEstimation via Ego-Head Pose Estimation (EgoEgo), which decomposes the problem into two stages, connected by the head motion as an intermediate representation. EgoEgo first integrates SLAM and a learning approach to estimate accu-rate head motion. Subsequently, leveraging the estimated head pose as input, EgoEgo utilizes conditional diffusion† indicates equal contribution. to generate multiple plausible full-body motions. This dis-entanglement of head and body pose eliminates the need for training datasets with paired egocentric videos and 3D human motion, enabling us to leverage large-scale egocen-tric video datasets and motion capture datasets separately.Moreover, for systematic benchmarking, we develop a syn-thetic dataset, AMASS-Replica-Ego-Syn (ARES), with paired egocentric videos and human motion. On both ARES and real data, our EgoEgo model performs significantly better than the current state-of-the-art methods. 