Despite the remarkable performance that modern deep neural networks have achieved on independent and iden-tically distributed (I.I.D.) data, they can crash under dis-tribution shifts. Most current evaluation methods for do-main generalization (DG) adopt the leave-one-out strat-egy as a compromise on the limited number of domains.We propose a large-scale benchmark with extensive labeled domains named NICO++ along with more rational eval-uation methods for comprehensively evaluating DG algo-rithms. To evaluate DG datasets, we propose two metrics to quantify covariate shift and concept shift, respectively.Two novel generalization bounds from the perspective of data construction are proposed to prove that limited con-cept shift and significant covariate shift favor the evalua-tion capability for generalization. Through extensive ex-periments, NICO++ shows its superior evaluation capabil-ity compared with current DG datasets and its contribu-tion in alleviating unfairness caused by the leak of oracle knowledge in model selection. The data and code for the benchmark based on NICO++ are available at https://github.com/xxgege/NICO-plus. 