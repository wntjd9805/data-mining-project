We extend neural radiance ﬁelds (NeRFs) to dynamic large-scale urban scenes. Prior work tends to reconstruct single video clips of short durations (up to 10 seconds). Two reasons are that such methods (a) tend to scale linearly with the number of moving objects and input videos because a separate model is built for each and (b) tend to require su-pervision via 3D bounding boxes and panoptic labels, ob-tained manually or via category-speciﬁc models. As a step towards truly open-world reconstructions of dynamic cities, we introduce two key innovations: (a) we factorize the scene into three separate hash table data structures to efﬁciently encode static, dynamic, and far-ﬁeld radiance ﬁelds, and (b) we make use of unlabeled target signals consisting ofRGB images, sparse LiDAR, off-the-shelf self-supervised 2D descriptors, and most importantly, 2D optical ﬂow. Op-erationalizing such inputs via photometric, geometric, and feature-metric reconstruction losses enables SUDS to de-compose dynamic scenes into the static background, indi-vidual objects, and their motions. When combined with our multi-branch table representation, such reconstructions can be scaled to tens of thousands of objects across 1.2 million frames from 1700 videos spanning geospatial footprints of hundreds of kilometers, (to our knowledge) the largest dy-namic NeRF built to date. We present qualitative initial re-sults on a variety of tasks enabled by our representations, including novel-view synthesis of dynamic urban scenes, unsupervised 3D instance segmentation, and unsupervised 3D cuboid detection. To compare to prior work, we also evaluate on KITTI and Virtual KITTI 2, surpassing state-of-the-art methods that rely on ground truth 3D bounding box annotations while being 10x quicker to train. 