Interpreting visual relationships is a core aspect of com-prehensive video understanding. Given a query visual re-lationship as <subject, predicate, object> and a test video, our objective is to localize the subject and object that are connected via the predicate. Given modern visio-lingual understanding capabilities, solving this problem is achiev-able, provided that there are large-scale annotated training examples available. However, annotating for every combi-nation of subject, object, and predicate is cumbersome, ex-pensive, and possibly infeasible. Therefore, there is a need for models that can learn to spatially and temporally lo-calize subjects and objects that are connected via an un-seen predicate using only a few support set videos shar-ing the common predicate. We address this challenging problem, referred to as few-shot referring relationships in videos for the first time. To this end, we pose the problem as a minimization of an objective function defined over aT-partite random field. Here, the vertices of the random field correspond to candidate bounding boxes for the sub-ject and object, and T represents the number of frames in the test video. This objective function is composed of frame-level and visual relationship similarity potentials. To learn these potentials, we use a relation network that takes query-conditioned translational relationship embedding as inputs and is meta-trained using support set videos in an episodic manner. Further, the objective function is minimized usinga belief propagation-based message passing on the random field to obtain the spatiotemporal localization or subject and object trajectories. We perform extensive experiments using two public benchmarks, namely ImageNet-VidVRD and VidOR, and compare the proposed approach with com-petitive baselines to assess its efficacy. 