Diffusion models arise as a powerful generative tool re-cently. Despite the great progress, existing diffusion models mainly focus on uni-modal control, i.e., the diffusion process is driven by only one modality of condition. To further un-leash the usersâ€™ creativity, it is desirable for the model to be controllable by multiple modalities simultaneously, e.g. gen-erating and editing faces by describing the age (text-driven) while drawing the face shape (mask-driven).In this work, we present Collaborative Diffusion, where pre-trained uni-modal diffusion models collaborate to achieve multi-modal face generation and editing without re-training. Our key insight is that diffusion models driven by different modalities are inherently complementary regard-ing the latent denoising steps, where bilateral connections can be established upon. Specifically, we propose dynamic diffuser, a meta-network that adaptively hallucinates multi-modal denoising steps by predicting the spatial-temporal (cid:66)Corresponding author.Project page: https://ziqihuangg.github.io/projects/collaborative-diffusion.htmlCode: https://github.com/ziqihuangg/Collaborative-Diffusion influence functions for each pre-trained uni-modal model.Collaborative Diffusion not only collaborates generation capabilities from uni-modal diffusion models, but also inte-grates multiple uni-modal manipulations to perform multi-modal editing. Extensive qualitative and quantitative experi-ments demonstrate the superiority of our framework in both image quality and condition consistency. 