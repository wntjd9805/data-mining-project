Visible-modal object tracking gives rise to a series of downstream multi-modal tracking tributaries. To inherit the powerful representations of the foundation model, a natural modus operandi for multi-modal tracking is full ﬁne-tuning on the RGB-based parameters. Albeit effective, this man-ner is not optimal due to the scarcity of downstream data and poor transferability, etc. In this paper, inspired by the recent success of the prompt learning in language models, we develop Visual Prompt multi-modal Tracking (ViPT), which learns the modal-relevant prompts to adapt the frozen pre-trained foundation model to various downstream multi-modal tracking tasks. ViPT ﬁnds a better way to stimulate the knowledge of the RGB-based model that is pre-trained at scale, meanwhile only introducing a few trainable pa-rameters (less than 1% of model parameters). ViPT outper-forms the full ﬁne-tuning paradigm on multiple downstream tracking tasks including RGB+Depth, RGB+Thermal, andRGB+Event tracking. Extensive experiments show the po-tential of visual prompt learning for multi-modal tracking, and ViPT can achieve state-of-the-art performance while satisfying parameter efﬁciency. Code and models are avail-able at https://github.com/jiawen-zhu/ViPT. 