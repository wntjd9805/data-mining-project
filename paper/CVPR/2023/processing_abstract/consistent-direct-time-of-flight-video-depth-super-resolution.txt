Direct time-of-flight (dToF) sensors are promising for next-generation on-device 3D sensing. However, limited by manufacturing capabilities in a compact module, the dToF data has a low spatial resolution (e.g. ∼ 20 × 30 for iPhone dToF), and it requires a super-resolution step before being passed to downstream tasks.In this paper, we solve this super-resolution problem by fusing the low-resolution dToF data with the corresponding high-resolution RGB guidance.Unlike the conventional RGB-guided depth enhancement approaches, which perform the fusion in a per-frame man-ner, we propose the first multi-frame fusion scheme to miti-gate the spatial ambiguity resulting from the low-resolutionIn addition, dToF sensors provide unique dToF imaging. depth histogram information for each local patch, and we incorporate this dToF-specific feature in our network design to further alleviate spatial ambiguity. To evaluate our mod-els on complex dynamic indoor environments and to pro-vide a large-scale dToF sensor dataset, we introduce Dy-DToF, the first synthetic RGB-dToF video dataset that fea-tures dynamic objects and a realistic dToF simulator fol-lowing the physical imaging process. We believe the meth-ods and dataset are beneficial to a broad community as dToF depth sensing is becoming mainstream on mobile de-vices. Our code and data are publicly available. https://github.com/facebookresearch/DVSR/ 