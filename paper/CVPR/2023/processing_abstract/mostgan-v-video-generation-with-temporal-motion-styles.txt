Video generation remains a challenging task due to spa-tiotemporal complexity and the requirement of synthesizing diverse motions with temporal consistency. Previous works attempt to generate videos in arbitrary lengths either in an autoregressive manner or regarding time as a continuous signal. However, they struggle to synthesize detailed and di-verse motions with temporal coherence and tend to generate repetitive scenes after a few time steps. In this work, we ar-gue that a single time-agnostic latent vector of style-based generator is insufficient to model various and temporally-consistent motions. Hence, we introduce additional time-dependent motion styles to model diverse motion patterns.In addition, a Motion Style Attention modulation mecha-nism, dubbed as MoStAtt, is proposed to augment frames with vivid dynamics for each specific scale (i.e., layer), which assigns attention score for each motion style w.r.t de-convolution filter weights in the target synthesis layer and softly attends different motion styles for weight modula-tion. Experimental results show our model achieves state-of-the-art performance on four unconditional 2562 video synthesis benchmarks trained with only 3 frames per clip and produces better qualitative results with respect to dy-namic motions. Code and videos have been made avail-able at https://github.com/xiaoqian- shen/MoStGAN-V . 