Federated learning (FL) is an attractive distributed learning paradigm supporting real-time continuous learn-ing and client privacy by default. In most FL approaches, all edge clients are assumed to have sufﬁcient computation capabilities to participate in the learning of a deep neural network (DNN) model. However, in real-life applications, some clients may have severely limited resources and can only train a much smaller local model. This paper presentsScaleFL, a novel FL approach with two distinctive mecha-nisms to handle resource heterogeneity and provide an equi-table FL framework for all clients. First, ScaleFL adaptively scales down the DNN model along width and depth dimen-sions by leveraging early exits to ﬁnd the best-ﬁt models for resource-aware local training on distributed clients. In this way, ScaleFL provides an efﬁcient balance of preserving basic and complex features in local model splits with vari-ous sizes for joint training while enabling fast inference for model deployment. Second, ScaleFL utilizes self-distillation among exit predictions during training to improve aggre-gation through knowledge transfer among subnetworks. We conduct extensive experiments on benchmark CV (CIFAR-10/100, ImageNet) and NLP datasets (SST-2, AgNews). We demonstrate that ScaleFL outperforms existing representa-tive heterogeneous FL approaches in terms of global/local model performance and provides inference efﬁciency, with up to 2x latency and 4x model size reduction with negligible performance drop below 2%. 