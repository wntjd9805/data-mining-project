Open-vocabulary object detection aims to provide ob-ject detectors trained on a fixed set of object categories with the generalizability to detect objects described by arbi-trary text queries. Previous methods adopt knowledge dis-tillation to extract knowledge from Pretrained Vision-and-Language Models (PVLMs) and transfer it to detectors.However, due to the non-adaptive proposal cropping and single-level feature mimicking processes, they suffer from information destruction during knowledge extraction and inefficient knowledge transfer. To remedy these limitations, we propose an Object-Aware Distillation Pyramid (OADP) framework, including an Object-Aware Knowledge Extrac-tion (OAKE) module and a Distillation Pyramid (DP) mech-anism. When extracting object knowledge from PVLMs, the former adaptively transforms object proposals and adopts object-aware mask attention to obtain precise and com-plete knowledge of objects. The latter introduces global and block distillation for more comprehensive knowledge transfer to compensate for the missing relation information in object distillation. Extensive experiments show that our method achieves significant improvement compared to cur-rent methods. Especially on the MS-COCO dataset, ourOADP framework reaches 35.6 mAPN 50, surpassing the cur-rent state-of-the-art method by 3.3 mAPN 50. Code is released at https://github.com/LutingWang/OADP. 