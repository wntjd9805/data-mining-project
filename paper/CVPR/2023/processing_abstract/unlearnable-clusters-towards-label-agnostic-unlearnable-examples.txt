There is a growing interest in developing unlearnable examples (UEs) against visual privacy leaks on the Internet.UEs are training samples added with invisible but unlearn-able noise, which have been found can prevent unauthorized training of machine learning models. UEs typically are gen-erated via a bilevel optimization framework with a surrogate model to remove (minimize) errors from the original sam-ples, and then applied to protect the data against unknown target models. However, existing UE generation methods all rely on an ideal assumption called label-consistency, where the hackers and protectors are assumed to hold the same label for a given sample. In this work, we propose and promote a more practical label-agnostic setting, where the hackers may exploit the protected data quite differently from the protectors. E.g., a m-class unlearnable dataset held by the protector may be exploited by the hacker as a n-class dataset. Existing UE generation methods are ren-dered ineffective in this challenging setting. To tackle this challenge, we present a novel technique called Unlearn-able Clusters (UCs) to generate label-agnostic unlearn-able examples with cluster-wise perturbations. Furthermore, we propose to leverage Vision-and-Language Pre-trainedModels (VLPMs) like CLIP as the surrogate model to im-prove the transferability of the crafted UCs to diverse do-mains. We empirically verify the effectiveness of our pro-posed approach under a variety of settings with different datasets, target models, and even commercial platforms Mi-crosoft Azure and Baidu PaddlePaddle. Code is avail-able at https://github.com/jiamingzhang94/Unlearnable-Clusters. 