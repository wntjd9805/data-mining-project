Vision-language models trained with contrastive learn-ing on large-scale noisy data are becoming increasingly popular for zero-shot recognition problems. In this paper we improve the following three aspects of the contrastive pre-training pipeline: dataset noise, model initialization and the training objective. First, we propose a straightfor-ward ﬁltering strategy titled Complexity, Action, and Text-spotting (CAT) that signiﬁcantly reduces dataset size, while achieving improved performance across zero-shot vision-language tasks. Next, we propose an approach titled Con-cept Distillation to leverage strong unimodal representa-tions for contrastive training that does not increase train-ing complexity while outperforming prior work. Finally, we modify the traditional contrastive alignment objective, and propose an importance-sampling approach to up-sample the importance of hard-negatives without adding additional complexity. On an extensive zero-shot benchmark of 29 tasks, our Distilled and Hard-negative Training (DiHT) ap-proach improves on 20 tasks compared to the baseline. Fur-thermore, for few-shot linear probing, we propose a novel approach that bridges the gap between zero-shot and few-shot performance, substantially improving over prior work.Models are available at github.com/facebookresearch/diht. 