The goal of multimodal summarization is to extract the most important information from different modalities to form summaries. Unlike unimodal summarization, the multimodal summarization task explicitly leverages cross-modal information to help generate more reliable and high-quality summaries. However, existing methods fail to lever-age the temporal correspondence between different modal-ities and ignore the intrinsic correlation between differ-ent samples. To address this issue, we introduce Align and Attend Multimodal Summarization (A2Summ), a uni-fied multimodal transformer-based model which can ef-In ad-fectively align and attend the multimodal input. dition, we propose two novel contrastive losses to model both inter-sample and intra-sample correlations. Exten-sive experiments on two standard video summarization datasets (TVSum and SumMe) and two multimodal sum-marization datasets (Daily Mail and CNN) demonstrate the superiority of A2Summ, achieving state-of-the-art perfor-mances on all datasets. Moreover, we collected a large-scale multimodal summarization dataset BLiSS, which contains livestream videos and transcribed texts with annotated sum-maries. Our code and dataset are publicly available at https://boheumd.github.io/A2Summ/. 