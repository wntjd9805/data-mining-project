There is growing interest in searching for information from large video corpora. Prior works have studied rele-vant tasks, such as text-based video retrieval, moment re-trieval, video summarization, and video captioning in iso-lation, without an end-to-end setup that can jointly search from video corpora and generate summaries. Such an end-to-end setup would allow for many interesting applications, e.g., a text-based search that finds a relevant video from a video corpus, extracts the most relevant moment from that video, and segments the moment into important steps with captions. To address this, we present the HIREST (HIerarchical REtrieval and STep-captioning) dataset and propose a new benchmark that covers hierarchical infor-mation retrieval and visual/textual stepwise summarization from an instructional video corpus. HIREST consists of 3.4K text-video pairs from an instructional video dataset, where 1.1K videos have annotations of moment spans rel-evant to text query and breakdown of each moment into key instruction steps with caption and timestamps (totaling 8.6K step captions). Our hierarchical benchmark consists of video retrieval, moment retrieval, and two novel moment segmentation and step captioning tasks. In moment segmen-tation, models break down a video moment into instructionIn step caption-steps and identify start-end boundaries. ing, models generate a textual summary for each step. We also present starting point task-specific and end-to-end joint baseline models for our new benchmark. While the baseline models show some promising results, there still exists large room for future improvement by the community.1 