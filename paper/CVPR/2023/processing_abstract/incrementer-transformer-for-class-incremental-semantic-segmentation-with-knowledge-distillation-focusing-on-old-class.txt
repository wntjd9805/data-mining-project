Class-incremental semantic segmentation aims to incre-mentally learn new classes while maintaining the capabil-ity to segment old ones, and suffers catastrophic forgetting since the old-class labels are unavailable. Most existing methods are based on convolutional networks and prevent forgetting through knowledge distillation, which (1) need to add additional convolutional layers to predict new classes, and (2) ignore to distinguish different regions correspond-ing to old and new classes during knowledge distillation and roughly distill all the features, thus limiting the learning of new classes. Based on the above observations, we pro-pose a new transformer framework for class-incremental semantic segmentation, dubbed Incrementer, which only needs to add new class tokens to the transformer decoder for new-class learning. Based on the Incrementer, we pro-pose a new knowledge distillation scheme that focuses on the distillation in the old-class regions, which reduces the constraints of the old model on the new-class learning, thus improving the plasticity. Moreover, we propose a class de-confusion strategy to alleviate the overﬁtting to new classes and the confusion of similar classes. Our method is sim-ple and effective, and extensive experiments show that our method outperforms the SOTAs by a large margin (5∼15 absolute points boosts on both Pascal VOC and ADE20k).We hope that our Incrementer can serve as a new strong pipeline for class-incremental semantic segmentation. 