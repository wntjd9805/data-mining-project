create detailed and realistic textures of the desired style that maintain structural context for scenes with multiple objects.We propose Text2Scene, a method to automatically cre-ate realistic textures for virtual scenes composed of multiple objects. Guided by a reference image and text descriptions, our pipeline adds detailed texture on labeled 3D geome-tries in the room such that the generated colors respect the hierarchical structure or semantic parts that are often com-posed of similar materials. Instead of applying flat styliza-tion on the entire scene at a single step, we obtain weak semantic cues from geometric segmentation, which are fur-ther clarified by assigning initial colors to segmented parts.Then we add texture details for individual objects such that their projections on image space exhibit feature embedding aligned with the embedding of the input. The decomposition makes the entire pipeline tractable to a moderate amount of computation resources and memory. As our framework uti-lizes the existing resources of image and text embedding, it does not require dedicated datasets with high-quality tex-tures designed by skillful artists. To the best of our knowl-edge, it is the first practical and scalable approach that can 