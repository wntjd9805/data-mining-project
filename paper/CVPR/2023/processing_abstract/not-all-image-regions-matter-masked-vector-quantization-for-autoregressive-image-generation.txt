Existing autoregressive models follow the two-stage gen-eration paradigm that first learns a codebook in the la-tent space for image reconstruction and then completes the image generation autoregressively based on the learned codebook. However, existing codebook learning simply models all local region information of images without dis-tinguishing their different perceptual importance, which brings redundancy in the learned codebook that not only limits the next stage’s autoregressive model’s ability to model important structure but also results in high train-ing cost and slow generation speed. In this study, we bor-row the idea of importance perception from classical im-age coding theory and propose a novel two-stage frame-work, which consists of Masked Quantization VAE (MQ-VAE) and Stackformer, to relieve the model from model-ing redundancy.Specifically, MQ-VAE incorporates an adaptive mask module for masking redundant region fea-tures before quantization and an adaptive de-mask mod-ule for recovering the original grid image feature map to faithfully reconstruct the original images after quantiza-tion. Then, Stackformer learns to predict the combination of the next code and its position in the feature map. Com-prehensive experiments on various image generation vali-date our effectiveness and efficiency. Code will be released at https : / / github . com / CrossmodalGroup /MaskedVectorQuantization. 