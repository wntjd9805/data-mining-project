Modern mobile burst photography pipelines capture and merge a short sequence of frames to recover an enhanced image, but often disregard the 3D nature of the scene they capture, treating pixel motion between images as a 2D ag-gregation problem. We show that in a “long-burst”, forty-two 12-megapixel RAW frames captured in a two-second se-quence, there is enough parallax information from natural hand tremor alone to recover high-quality scene depth. To this end, we devise a test-time optimization approach that fits a neural RGB-D representation to long-burst data and simultaneously estimates scene depth and camera motion.Our plane plus depth model is trained end-to-end, and per-forms coarse-to-fine refinement by controlling which multi-resolution volume features the network has access to at what time during training. We validate the method experi-mentally, and demonstrate geometrically accurate depth re-constructions with no additional hardware or separate data pre-processing and pose-estimation steps. 