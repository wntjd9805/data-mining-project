Masked Autoencoders learn strong visual representa-tions and achieve state-of-the-art results in several inde-pendent modalities, yet very few works have addressed their capabilities in multi-modality settings. In this work, we fo-cus on point cloud and RGB image data, two modalities that are often presented together in the real world, and explore their meaningful interactions. To improve upon the cross-modal synergy in existing works, we propose Pi-MAE, a self-supervised pre-training framework that pro-motes 3D and 2D interaction through three aspects. Specif-ically, we first notice the importance of masking strategies between the two sources and utilize a projection module to complementarily align the mask and visible tokens of the two modalities. Then, we utilize a well-crafted two-branch MAE pipeline with a novel shared decoder to pro-mote cross-modality interaction in the mask tokens. Fi-nally, we design a unique cross-modal reconstruction mod-ule to enhance representation learning for both modali-ties. Through extensive experiments performed on large-scale RGB-D scene understanding benchmarks (SUN RGB-D and ScannetV2), we discover it is nontrivial to interac-tively learn point-image features, where we greatly improve multiple 3D detectors, 2D detectors, and few-shot classi-fiers by 2.9%, 6.7%, and 2.4%, respectively. Code is avail-able at https://github.com/BLVLab/PiMAE. 