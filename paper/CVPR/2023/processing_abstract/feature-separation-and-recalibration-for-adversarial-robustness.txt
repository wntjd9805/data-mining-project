Deep neural networks are susceptible to adversarial at-tacks due to the accumulation of perturbations in the fea-ture level, and numerous works have boosted model robust-ness by deactivating the non-robust feature activations that cause model mispredictions. However, we claim that these malicious activations still contain discriminative cues and that with recalibration, they can capture additional use-ful information for correct model predictions. To this end, we propose a novel, easy-to-plugin approach named Fea-ture Separation and Recalibration (FSR) that recalibrates the malicious, non-robust activations for more robust fea-ture maps through Separation and Recalibration. The Sep-aration part disentangles the input feature map into the robust feature with activations that help the model make correct predictions and the non-robust feature with activa-tions that are responsible for model mispredictions upon adversarial attack. The Recalibration part then adjusts the non-robust activations to restore the potentially useful cues for model predictions. Extensive experiments verify the superiority of FSR compared to traditional deactivation techniques and demonstrate that it improves the robustness of existing adversarial training methods by up to 8.57% with small computational overhead. Codes are available at https://github.com/wkim97/FSR. 