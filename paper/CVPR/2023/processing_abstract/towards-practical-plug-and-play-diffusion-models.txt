Diffusion-based generative models have achieved re-markable success in image generation. Their guidance for-mulation allows an external model to plug-and-play con-trol the generation process for various tasks without fine-tuning the diffusion model. However, the direct use of pub-licly available off-the-shelf models for guidance fails due to their poor performance on noisy inputs. For that, the existing practice is to fine-tune the guidance models with labeled data corrupted with noises. In this paper, we ar-gue that this practice has limitations in two aspects: (1) performing on inputs with extremely various noises is too hard for a single guidance model; (2) collecting labeled datasets hinders scaling up for various tasks. To tackle the limitations, we propose a novel strategy that lever-ages multiple experts where each expert is specialized in a particular noise range and guides the reverse process of the diffusion at its corresponding timesteps. However, as it is infeasible to manage multiple networks and uti-lize labeled data, we present a practical guidance frame-work termed Practical Plug-And-Play (PPAP), which lever-ages parameter-efficient fine-tuning and data-free knowl-edge transfer. We exhaustively conduct ImageNet class con-ditional generation experiments to show that our method can successfully guide diffusion with small trainable pa-rameters and no labeled data. Finally, we show that im-age classifiers, depth estimators, and semantic segmenta-tion models can guide publicly available GLIDE through our framework in a plug-and-play manner. Our code is available at https://github.com/riiid/PPAP. 