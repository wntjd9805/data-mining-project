In this paper, we present a new sequence-to-sequence learning framework for visual tracking, dubbed SeqTrack.It casts visual tracking as a sequence generation problem, which predicts object bounding boxes in an autoregres-sive fashion. This is different from prior Siamese track-ers and transformer trackers, which rely on designing com-plicated head networks, such as classification and regres-sion heads. SeqTrack only adopts a simple encoder-decoder transformer architecture. The encoder extracts visual fea-tures with a bidirectional transformer, while the decoder generates a sequence of bounding box values autoregres-sively with a causal transformer. The loss function is a plain cross-entropy. Such a sequence learning paradigm not only simplifies tracking framework, but also achieves competitive performance on benchmarks. For instance, Se-qTrack gets 72.5% AUC on LaSOT, establishing a new state-of-the-art performance. Code and models are available at https://github.com/microsoft/VideoX. 