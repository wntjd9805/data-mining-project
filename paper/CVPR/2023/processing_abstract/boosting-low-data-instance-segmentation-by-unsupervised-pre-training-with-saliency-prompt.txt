Inspired by DETR variants, query-based end-to-end in-stance segmentation (QEIS) methods have recently outper-formed CNN-based models on large-scale datasets. Yet they would lose efficacy when only a small amount of training data is available since itâ€™s hard for the crucial queries/kernels to learn localization and shape priors. To this end, this work offers a novel unsupervised pre-training solution for low-data regimes.Inspired by the recent success of the Prompting technique, we introduce a new pre-training method that boosts QEIS models by givingSaliency Prompt for queries/kernels. Our method contains three parts: 1) Saliency Masks Proposal is responsible for generating pseudo masks from unlabeled images based on the saliency mechanism. 2) Prompt-Kernel Matching transfers pseudo masks into prompts and injects the corre-sponding localization and shape priors to the best-matched kernels. 3) Kernel Supervision is applied to supply super-vision at the kernel level for robust learning. From a practi-cal perspective, our pre-training method helps QEIS models achieve a similar convergence speed and comparable per-formance with CNN-based models in low-data regimes. Ex-perimental results show that our method significantly boosts several QEIS models on three datasets.1 