Image-based Virtual Try-ON aims to transfer an in-shop garment onto a specific person. Existing methods employ a global warping module to model the anisotropic defor-mation for different garment parts, which fails to preserve the semantic information of different parts when receiving challenging inputs (e.g, intricate human poses, difficult gar-ments). Moreover, most of them directly warp the input garment to align with the boundary of the preserved re-gion, which usually requires texture squeezing to meet the boundary shape constraint and thus leads to texture dis-tortion. The above inferior performance hinders existing methods from real-world applications. To address these problems and take a step towards real-world virtual try-on, we propose a General-Purpose Virtual Try-ON framework, named GP-VTON, by developing an innovative Local-FlowGlobal-Parsing (LFGP) warping module and a DynamicGradient Truncation (DGT) training strategy. Specifically, compared with the previous global warping mechanism,LFGP employs local flows to warp garments parts individ-ually, and assembles the local warped results via the global garment parsing, resulting in reasonable warped parts and a semantic-correct intact garment even with challenging in-puts.On the other hand, our DGT training strategy dynam-ically truncates the gradient in the overlap area and the warped garment is no more required to meet the bound-ary constraint, which effectively avoids the texture squeez-ing problem. Furthermore, our GP-VTON can be easily extended to multi-category scenario and jointly trained by using data from different garment categories. Extensive ex-periments on two high-resolution benchmarks demonstrate our superiority over the existing state-of-the-art methods. 1 1Corresponding author is Xiaodan Liang. Code is available at gp-vton.