Current semantic segmentation models have achieved great success under the independent and identically dis-tributed (i.i.d.) condition. However, in real-world appli-cations, test data might come from a different domain than training data. Therefore, it is important to improve model robustness against domain differences. This work stud-ies semantic segmentation under the domain generalization setting, where a model is trained only on the source domain and tested on the unseen target domain. Existing works show that Vision Transformers are more robust than CNNs and show that this is related to the visual grouping property of self-attention. In this work, we propose a novel hierarchi-cal grouping transformer (HGFormer) to explicitly group pixels to form part-level masks and then whole-level masks.The masks at different scales aim to segment out both parts and a whole of classes. HGFormer combines mask clas-siﬁcation results at both scales for class label prediction.We assemble multiple interesting cross-domain settings by using seven public semantic segmentation datasets. Exper-iments show that HGFormer yields more robust semantic segmentation results than per-pixel classiﬁcation methods and ﬂat-grouping transformers, and outperforms previous methods signiﬁcantly. Code will be available at https://github.com/dingjiansw101/HGFormer.Figure 1. Semantic segmentation can be considered as partition-ing an image into classiﬁcation units (regions), then classifying the units. The units can range from pixels to large masks. Intuitively, mask classiﬁcation is more robust than per-pixel classiﬁcation, as masks allow to aggregate features over large image regions of the same class to predict a ‘global’ label. Despite this promise, the process of grouping pixels into whole-level masks directly from pixels is very challenging under the distribution shift (e.g., Gaus-sian Noise). In order to tackle this problem, we present a hierar-chical grouping paradigm to group pixels to part-level masks ﬁrst and then to group part-level masks to whole-level masks to get re-liable masks. Then we combine both part-level and whole-level mask classiﬁcation for robust semantic segmentation, given that the masks at the two levels capture complementary information. 