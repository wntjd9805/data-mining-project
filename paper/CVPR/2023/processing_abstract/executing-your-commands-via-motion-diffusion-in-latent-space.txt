We study a challenging task, conditional human motion generation, which produces plausible human motion se-quences according to various conditional inputs, such as action classes or textual descriptors. Since human mo-tions are highly diverse and have a property of quite dif-ferent distribution from conditional modalities, such as tex-tual descriptors in natural languages, it is hard to learn a probabilistic mapping from the desired conditional modal-ity to the human motion sequences. Besides, the raw mo-tion data from the motion capture system might be redun-dant in sequences and contain noises; directly modeling the joint distribution over the raw motion sequences and condi-tional modalities would need a heavy computational over-head and might result in artifacts introduced by the cap-tured noises. To learn a better representation of the var-ious human motion sequences, we ﬁrst design a powerfulVariational AutoEncoder (VAE) and arrive at a representa-tive and low-dimensional latent code for a human motion sequence. Then, instead of using a diffusion model to es-tablish the connections between the raw motion sequences and the conditional inputs, we perform a diffusion process on the motion latent space. Our proposed Motion Latent-based Diffusion model (MLD) could produce vivid motion sequences conforming to the given conditional inputs and substantially reduce the computational overhead in both the training and inference stages. Extensive experiments on various human motion generation tasks demonstrate that our MLD achieves signiﬁcant improvements over the state-of-the-art methods among extensive human motion genera-tion tasks, with two orders of magnitude faster than previous diffusion models on raw motion sequences. 