In recent years, deep learning-based approaches have shown great strength in multi-view stereo because of their outstanding ability to extract robust visual features. How-ever, most learning-based methods need to build the cost volume and increase the receptive field enormously to get a satisfactory result when dealing with large-scale tex-tureless regions, consequently leading to prohibitive mem-ory consumption. To ensure both memory-friendly and textureless-resilient, we innovatively transplant the spirit of deformable convolution from deep learning into the tra-ditional PatchMatch-based method. Specifically, for each pixel with matching ambiguity (termed unreliable pixel), we adaptively deform the patch centered on it to extend the receptive field until covering enough correlative reliable pixels (without matching ambiguity) that serve as anchors.When performing PatchMatch, constrained by the anchor pixels, the matching cost of an unreliable pixel is guar-anteed to reach the global minimum at the correct depth and therefore increases the robustness of multi-view stereo significantly. To detect more anchor pixels to ensure bet-ter adaptive patch deformation, we propose to evaluate the matching ambiguity of a certain pixel by checking the con-vergence of the estimated depth as optimization proceeds.As a result, our method achieves state-of-the-art perfor-mance on ETH3D and Tanks and Temples while preserving low memory consumption. 