Recently, deep Stereo Matching (SM) networks have shown impressive performance and attracted increasing at-tention in computer vision. However, existing deep SM networks are prone to learn dataset-dependent shortcuts, which fail to generalize well on unseen realistic dataset-s. This paper takes a step towards training robust models for the domain generalized SM task, which mainly focuses on learning shortcut-invariant representation from synthet-ic data to alleviate the domain shifts. Speciﬁcally, we pro-pose a Hierarchical Visual Transformation (HVT) network to 1) ﬁrst transform the training sample hierarchically in-to new domains with diverse distributions from three levels:Global, Local, and Pixel, 2) then maximize the visual dis-crepancy between the source domain and new domains, and minimize the cross-domain feature inconsistency to capture domain-invariant features. In this way, we can prevent the model from exploiting the artifacts of synthetic stereo im-ages as shortcut features, thereby estimating the dispari-ty maps more effectively based on the learned robust and shortcut-invariant representation. We integrate our pro-posed HVT network with SOTA SM networks and evaluate its effectiveness on several public SM benchmark datasets.Extensive experiments clearly show that the HVT network can substantially enhance the performance of existing SM networks in synthetic-to-realistic domain generalization. 