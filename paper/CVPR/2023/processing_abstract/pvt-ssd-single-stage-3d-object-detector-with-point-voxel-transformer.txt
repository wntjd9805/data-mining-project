Recent Transformer-based 3D object detectors learn point cloud features either from point- or voxel-based rep-resentations. However, the former requires time-consuming sampling while the latter introduces quantization errors.In this paper, we present a novel Point-Voxel Transformer for single-stage 3D detection (PVT-SSD) that takes advan-tage of these two representations. Specifically, we first use voxel-based sparse convolutions for efficient feature encod-ing. Then, we propose a Point-Voxel Transformer (PVT) module that obtains long-range contexts in a cheap manner from voxels while attaining accurate positions from points.The key to associating the two different representations is our introduced input-dependent Query Initialization mod-ule, which could efficiently generate reference points and content queries. Then, PVT adaptively fuses long-range contextual and local geometric information around refer-ence points into content queries. Further, to quickly find the neighboring points of reference points, we design theVirtual Range Image module, which generalizes the native range image to multi-sensor and multi-frame. The experi-ments on several autonomous driving benchmarks verify the effectiveness and efficiency of the proposed method. Code will be available. 