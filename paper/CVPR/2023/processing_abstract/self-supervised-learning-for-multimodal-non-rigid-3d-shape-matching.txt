The matching of 3D shapes has been extensively stud-ied for shapes represented as surface meshes, as well as for shapes represented as point clouds. While point clouds are a common representation of raw real-world 3D data (e.g. from laser scanners), meshes encode rich and expres-sive topological information, but their creation typically re-quires some form of (often manual) curation. In turn, meth-ods that purely rely on point clouds are unable to meet the matching quality of mesh-based methods that utilise the ad-ditional topological structure.In this work we close this gap by introducing a self-supervised multimodal learning strategy that combines mesh-based functional map regular-isation with a contrastive loss that couples mesh and point cloud data. Our shape matching approach allows to ob-tain intramodal correspondences for triangle meshes, com-plete point clouds, and partially observed point clouds, as well as correspondences across these data modalities. We demonstrate that our method achieves state-of-the-art re-sults on several challenging benchmark datasets even in comparison to recent supervised methods, and that our method reaches previously unseen cross-dataset generali-sation ability. Our code is available at https://github.com/ dongliangcao/Self-Supervised-Multimodal-Shape-Matching. 