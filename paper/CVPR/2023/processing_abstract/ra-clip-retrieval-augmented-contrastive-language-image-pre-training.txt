Contrastive Language-Image Pre-training (CLIP) is at-tracting increasing attention for its impressive zero-shot recognition performance on different down-stream tasks.However, training CLIP is data-hungry and requires lots of image-text pairs to memorize various semantic concepts.In this paper, we propose a novel and efﬁcient frame-work: Retrieval Augmented Contrastive Language-ImagePre-training (RA-CLIP) to augment embeddings by online retrieval. Speciﬁcally, we sample part of image-text data as a hold-out reference set. Given an input image, relevant image-text pairs are retrieved from the reference set to enrich the representation of input image. This process can be considered as an open-book exam: with the reference set as a cheat sheet, the proposed method doesn’t needIt to memorize all visual concepts in the training data. explores how to recognize visual concepts by exploiting correspondence between images and texts in the cheat sheet. The proposed RA-CLIP implements this idea and comprehensive experiments are conducted to show howRA-CLIP works. Performances on 10 image classiﬁcation datasets and 2 object detection datasets show that RA-CLIP outperforms vanilla CLIP baseline by a large margin on zero-shot image classiﬁcation task (+12.7%), linear probe image classiﬁcation task (+6.9%) and zero-shot ROI classiﬁcation task (+2.8%). 