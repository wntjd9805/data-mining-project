In Composed Image Retrieval (CIR), a user combines a query image with text to describe their intended target.Existing methods rely on supervised learning of CIR mod-els using labeled triplets consisting of the query image, text speciÔ¨Åcation, and the target image. Labeling such triplets is expensive and hinders broad applicability of CIR. In this work, we propose to study an important task, Zero-ShotComposed Image Retrieval (ZS-CIR), whose goal is to build a CIR model without requiring labeled triplets for training.To this end, we propose a novel method, called Pic2Word, that requires only weakly labeled image-caption pairs and unlabeled image datasets to train. Unlike existing super-vised CIR models, our model trained on weakly labeled or unlabeled datasets shows strong generalization across diverse ZS-CIR tasks, e.g., attribute editing, object com-position, and domain conversion. Our approach outper-forms several supervised CIR methods on the common CIR benchmark, CIRR and Fashion-IQ. Code will be made pub-licly available at https://github.com/google-research/composed_image_retrieval 