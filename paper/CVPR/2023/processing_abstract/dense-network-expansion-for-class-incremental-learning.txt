The problem of class incremental learning (CIL) is con-sidered. State-of-the-art approaches use a dynamic archi-tecture based on network expansion (NE), in which a task expert is added per task. While effective from a computa-tional standpoint, these methods lead to models that grow quickly with the number of tasks. A new NE method, dense network expansion (DNE), is proposed to achieve a better trade-off between accuracy and model complexity. This is accomplished by the introduction of dense connections be-tween the intermediate layers of the task expert networks, that enable the transfer of knowledge from old to new tasks via feature sharing and reusing. This sharing is imple-mented with a cross-task attention mechanism, based on a new task attention block (TAB), that fuses information across tasks. Unlike traditional attention mechanisms, TAB operates at the level of the feature mixing and is decoupled with spatial attentions. This is shown more effective than a joint spatial-and-task attention for CIL. The proposedDNE approach can strictly maintain the feature space of old classes while growing the network and feature scale at a much slower rate than previous methods. In result, it out-performs the previous SOTA methods by a margin of 4% in terms of accuracy, with similar or even smaller model scale. 