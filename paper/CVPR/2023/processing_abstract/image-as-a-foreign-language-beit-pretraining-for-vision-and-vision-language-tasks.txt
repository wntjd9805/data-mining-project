A big convergence of language, vision, and multimodal pretraining is emerging.In this work, we introduce a general-purpose multimodal foundation model BEIT-3, which achieves excellent transfer performance on both vi-sion and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architec-ture, pretraining task, and model scaling up. We use Mul-tiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared back-bone, we perform masked “language” modeling on images (Imglish), texts (English), and image-text pairs (“parallel sentences”) in a unified manner. Experimental results show that BEIT-3 obtains remarkable performance on object de-tection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO). 