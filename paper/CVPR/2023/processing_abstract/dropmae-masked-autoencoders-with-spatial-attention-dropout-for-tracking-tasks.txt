In this paper, we study masked autoencoder (MAE) pre-training on videos for matching-based downstream tasks, including visual object tracking (VOT) and video object seg-mentation (VOS). A simple extension of MAE is to randomly mask out frame patches in videos and reconstruct the frame pixels. However, we find that this simple baseline heav-ily relies on spatial cues while ignoring temporal relations for frame reconstruction, thus leading to sub-optimal tem-poral matching representations for VOT and VOS. To al-leviate this problem, we propose DropMAE, which adap-tively performs spatial-attention dropout in the frame re-construction to facilitate temporal correspondence learning in videos. We show that our DropMAE is a strong and effi-cient temporal matching learner, which achieves better fine-tuning results on matching-based tasks than the ImageNet-based MAE with 2Ã— faster pre-training speed. Moreover, we also find that motion diversity in pre-training videos is more important than scene diversity for improving the performance on VOT and VOS. Our pre-trained DropMAE model can be directly loaded in existing ViT-based track-ers for fine-tuning without further modifications. Notably,DropMAE sets new state-of-the-art performance on 8 out of 9 highly competitive video tracking and segmentation datasets. Our code and pre-trained models are available at https://github.com/jimmy-dq/DropMAE.git. 