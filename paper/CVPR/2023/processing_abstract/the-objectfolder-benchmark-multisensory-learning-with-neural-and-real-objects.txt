We introduce the OBJECTFOLDER BENCHMARK, a benchmark suite of 10 tasks for multisensory object-centric learning, centered around object recognition, reconstruc-tion, and manipulation with sight, sound, and touch. We also introduce the OBJECTFOLDER REAL dataset, in-cluding the multisensory measurements for 100 real-world household objects, building upon a newly designed pipeline for collecting the 3D meshes, videos, impact sounds, and tactile readings of real-world objects. We conduct system-atic benchmarking on both the 1,000 multisensory neural objects from OBJECTFOLDER, and the real multisensory data from OBJECTFOLDER REAL. Our results demon-strate the importance of multisensory perception and reveal the respective roles of vision, audio, and touch for differ-ent object-centric learning tasks. By publicly releasing our dataset and benchmark suite, we hope to catalyze and en-able new research in multisensory object-centric learning in computer vision, robotics, and beyond. Project page: https://objectfolder.stanford.edu 