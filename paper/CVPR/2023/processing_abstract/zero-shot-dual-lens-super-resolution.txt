The asymmetric dual-lens conﬁguration is commonly available on mobile devices nowadays, which naturally stores a pair of wide-angle and telephoto images of the same scene to support realistic super-resolution (SR). Even on the same device, however, the degradation for model-ing realistic SR is image-speciﬁc due to the unknown ac-quisition process (e.g., tiny camera motion). In this paper, we propose a zero-shot solution for dual-lens SR (ZeDuSR), where only the dual-lens pair at test time is used to learn an image-speciﬁc SR model. As such, ZeDuSR adapts itself to the current scene without using external training data, and thus gets rid of generalization difﬁculty. However, there are two major challenges to achieving this goal: 1) dual-lens alignment while keeping the realistic degradation, and 2) effective usage of highly limited training data. To overcome these two challenges, we propose a degradation-invariant alignment method and a degradation-aware training strat-egy to fully exploit the information within a single dual-lens pair. Extensive experiments validate the superiority of Ze-DuSR over existing solutions on both synthesized and real-world dual-lens datasets. The implementation code is avail-able at https://github.com/XrKang/ZeDuSR. 