Self-supervised facial representation has recently at-tracted increasing attention due to its ability to perform face understanding without relying on large-scale anno-tated datasets heavily. However, analytically, current contrastive-based self-supervised learning (SSL) still per-forms unsatisfactorily for learning facial representation.More specifically, existing contrastive learning (CL) tends to learn pose-invariant features that cannot depict the pose details of faces, compromising the learning performance.To conquer the above limitation of CL, we propose a novelPose-disentangled Contrastive Learning (PCL) method for general self-supervised facial representation. Our PCL first devises a pose-disentangled decoder (PDD) with a deli-cately designed orthogonalizing regulation, which disen-tangles the pose-related features from the face-aware fea-tures; therefore, pose-related and other pose-unrelated fa-cial information could be performed in individual subnet-works and do not affect each otherâ€™s training. Furthermore, we introduce a pose-related contrastive learning scheme that learns pose-related information based on data augmen-tation of the same image, which would deliver more effective face-aware representation for various downstream tasks.We conducted linear evaluation on four challenging down-stream facial understanding tasks, i.e., facial expression recognition, face recognition, AU detection and head pose estimation. Experimental results demonstrate that PCL sig-nificantly outperforms cutting-edge SSL methods. Our Code is available at https://github.com/DreamMr/PCL. 