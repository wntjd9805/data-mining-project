We introduce the SceneDiffuser, a conditional genera-tive model for 3D scene understanding. SceneDiffuser pro-vides a unified model for solving scene-conditioned gen-eration, optimization, and planning. In contrast to prior work, SceneDiffuser is intrinsically scene-aware, physics-based, and goal-oriented. With an iterative sampling strategy,SceneDiffuser jointly formulates the scene-aware genera-tion, physics-based optimization, and goal-oriented planning via a diffusion-based denoising process in a fully differen-tiable fashion. Such a design alleviates the discrepancies among different modules and the posterior collapse of previ-ous scene-conditioned generative models. We evaluate theSceneDiffuser on various 3D scene understanding tasks, in-cluding human pose and motion generation, dexterous graspËš These authors contributed equally to this work. (cid:0) Corresponding authors: Siyuan Huang (syhuang@bigai.ai) and Wei Liang (liangwei@bit.edu.cn). generation, path planning for 3D navigation, and motion planning for robot arms. The results show significant im-provements compared with previous models, demonstrating the tremendous potential of the SceneDiffuser for the broad community of 3D scene understanding. 