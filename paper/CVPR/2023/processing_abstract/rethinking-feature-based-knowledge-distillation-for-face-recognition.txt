With the continual expansion of face datasets, feature-based distillation prevails for large-scale face recognition.In this work, we attempt to remove identity supervision in student training, to spare the GPU memory from saving massive class centers. However, this naive removal leads to inferior distillation result. We carefully inspect the perfor-mance degradation from the perspective of intrinsic dimen-sion, and argue that the gap in intrinsic dimension, namely the intrinsic gap, is intimately connected to the infamous capacity gap problem. By constraining the teacherâ€™s search space with reverse distillation, we narrow the intrinsic gap and unleash the potential of feature-only distillation. Re-markably, the proposed reverse distillation creates univer-sally student-friendly teacher that demonstrates outstand-ing student improvement. We further enhance its effective-ness by designing a student proxy to better bridge the intrin-sic gap. As a result, the proposed method surpasses state-of-the-art distillation techniques with identity supervision on various face recognition benchmarks, and the improve-ments are consistent across different teacher-student pairs. 