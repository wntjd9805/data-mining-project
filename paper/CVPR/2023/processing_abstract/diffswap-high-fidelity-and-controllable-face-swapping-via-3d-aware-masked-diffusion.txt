In this paper, we propose DiffSwap, a diffusion model based framework for high-fidelity and controllable face swapping. Unlike previous work that relies on carefully de-signed network architectures and loss functions to fuse the information from the source and target faces, we reformu-late the face swapping as a conditional inpainting task, per-formed by a powerful diffusion model guided by the desired face attributes (e.g., identity and landmarks). An impor-tant issue that makes it nontrivial to apply diffusion mod-els to face swapping is that we cannot perform the time-consuming multi-step sampling to obtain the generated im-age during training. To overcome this, we propose a mid-point estimation method to efficiently recover a reasonable diffusion result of the swapped face with only 2 steps, which enables us to introduce identity constraints to improve the face swapping quality. Our framework enjoys several fa-vorable properties more appealing than prior arts: 1) Con-trollable. Our method is based on conditional masked diffu-sion on the latent space, where the mask and the conditions can be fully controlled and customized. 2) High-fidelity.The formulation of conditional inpainting can fully exploit the generative ability of diffusion models and can preserve the background of target images with minimal artifacts. 3)â€ Corresponding authorShape-preserving. The controllability of our method en-ables us to use 3D-aware landmarks as the condition during generation to preserve the shape of the source face. Ex-tensive experiments on both FF++ and FFHQ demonstrate that our method can achieve state-of-the-art face swapping results both qualitatively and quantitatively. 