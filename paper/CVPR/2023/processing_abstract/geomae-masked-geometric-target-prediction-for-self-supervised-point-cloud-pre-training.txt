This paper tries to address a fundamental question in point cloud self-supervised learning: what is a good sig-nal we should leverage to learn features from point clouds without annotations? To answer that, we introduce a point cloud representation learning framework, based on geo-metric feature reconstruction. In contrast to recent papers that directly adopt masked autoencoder (MAE) and only predict original coordinates or occupancy from masked point clouds, our method revisits differences between im-ages and point clouds and identifies three self-supervised learning objectives peculiar to point clouds, namely cen-troid prediction, normal estimation, and curvature predic-tion. Combined, these three objectives yield an nontrivial self-supervised learning task and mutually facilitate mod-els to better reason fine-grained geometry of point clouds.Our pipeline is conceptually simple and it consists of two major steps: first, it randomly masks out groups of points, followed by a Transformer-based point cloud encoder; sec-ond, a lightweight Transformer decoder predicts centroid, normal, and curvature for points in each voxel. We transfer the pre-trained Transformer encoder to a downstream pe-ception model. On the nuScene Datset, our model achieves 3.38 mAP improvment for object detection, 2.1 mIoU gain for segmentation, and 1.7 AMOTA gain for multi-object tracking. We also conduct experiments on the Waymo OpenDataset and achieve significant performance improvements over baselines as well. 1 