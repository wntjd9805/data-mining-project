Computer vision models suffer from a phenomenon known as catastrophic forgetting when learning novel con-cepts from continuously shifting training data. Typical solu-tions for this continual learning problem require extensive rehearsal of previously seen data, which increases memory costs and may violate data privacy. Recently, the emer-gence of large-scale pre-trained vision transformer mod-els has enabled prompting approaches as an alternative to data-rehearsal. These approaches rely on a key-query mechanism to generate prompts and have been found to be highly resistant to catastrophic forgetting in the well-established rehearsal-free continual learning setting. How-ever, the key mechanism of these methods is not trained end-to-end with the task sequence. Our experiments show that this leads to a reduction in their plasticity, hence sac-rificing new task accuracy, and inability to benefit from ex-panded parameter capacity. We instead propose to learn a set of prompt components which are assembled with input-conditioned weights to produce input-conditioned prompts, resulting in a novel attention-based end-to-end key-query scheme. Our experiments show that we outperform the cur-rent SOTA method DualPrompt on established benchmarks by as much as 4.5% in average final accuracy. We also outperform the state of art by as much as 4.4% accuracy on a continual learning benchmark which contains both class-incremental and domain-incremental task shifts, cor-responding to many practical settings. Our code is avail-able at https://github.com/GT-RIPL/CODA-Prompt 