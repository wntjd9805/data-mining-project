Audiovisual automatic speech recognition (AV-ASR) aims to improve the robustness of a speech recognition sys-tem by incorporating visual information. Training fully supervised multimodal models for this task from scratch, however is limited by the need for large labelled audiovi-sual datasets (in each downstream domain of interest). We present AVFormer, a simple method for augmenting audio-only models with visual information, at the same time per-forming lightweight domain adaptation. We do this by (i) injecting visual embeddings into a frozen ASR model using lightweight trainable adaptors. We show that these can be trained on a small amount of weakly labelled video data with minimum additional training time and parame-ters. (ii) We also introduce a simple curriculum scheme dur-ing training which we show is crucial to enable the model to jointly process audio and visual information effectively; and finally (iii) we show that our model achieves state of the art zero-shot results on three different AV-ASR bench-marks (How2, VisSpeech and Ego4D), while also crucially preserving decent performance on traditional audio-only speech recognition benchmarks (LibriSpeech). Qualitative results show that our model effectively leverages visual in-formation for robust speech recognition. 