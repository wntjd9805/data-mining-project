We present Recurrent Vision Transformers (RVTs), a novel backbone for object detection with event cam-eras. Event cameras provide visual information with sub-millisecond latency at a high-dynamic range and with strong robustness against motion blur. These unique prop-erties offer great potential for low-latency object detec-tion and tracking in time-critical scenarios. Prior work in event-based vision has achieved outstanding detection per-formance but at the cost of substantial inference time, typ-ically beyond 40 milliseconds. By revisiting the high-level design of recurrent vision backbones, we reduce inference time by a factor of 6 while retaining similar performance.To achieve this, we explore a multi-stage design that uti-lizes three key concepts in each stage: ﬁrst, a convolutional prior that can be regarded as a conditional positional em-bedding. Second, local and dilated global self-attention for spatial feature interaction. Third, recurrent temporal fea-ture aggregation to minimize latency while retaining tempo-ral information. RVTs can be trained from scratch to reach state-of-the-art performance on event-based object detec-tion - achieving an mAP of 47.2% on the Gen1 automotive dataset. At the same time, RVTs offer fast inference (< 12 ms on a T4 GPU) and favorable parameter efﬁciency (5× fewer than prior art). Our study brings new insights into effective design choices that can be fruitful for research be-yond event-based vision.Code: https://github.com/uzh-rpg/RVT 