Multi-modality (MM) image fusion aims to render fused images that maintain the merits of different modalities, e.g., functional highlight and detailed textures. To tackle the chal-lenge in modeling cross-modality features and decomposing desirable modality-speciﬁc and modality-shared features, we propose a novel Correlation-Driven feature Decompo-sition Fusion (CDDFuse) network. Firstly, CDDFuse usesRestormer blocks to extract cross-modality shallow features.We then introduce a dual-branch Transformer-CNN feature extractor with Lite Transformer (LT) blocks leveraging long-range attention to handle low-frequency global features andInvertible Neural Networks (INN) blocks focusing on ex-tracting high-frequency local information. A correlation-driven loss is further proposed to make the low-frequency features correlated while the high-frequency features un-correlated based on the embedded information. Then, theLT-based global fusion and INN-based local fusion layers output the fused image. Extensive experiments demonstrate that our CDDFuse achieves promising results in multiple fusion tasks, including infrared-visible image fusion and medical image fusion. We also show that CDDFuse can boost the performance in downstream infrared-visible se-mantic segmentation and object detection in a uniﬁed bench-mark. The code is available at https://github.com/Zhaozixiang1228/MMIF-CDDFuse. 