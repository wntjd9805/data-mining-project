For robots to be generally useful, they must be able to ﬁnd arbitrary objects described by people (i.e., be language-driven) even without expensive navigation train-ing on in-domain data (i.e., perform zero-shot inference).We explore these capabilities in a uniﬁed setting: language-driven zero-shot object navigation (L-ZSON). Inspired by the recent success of open-vocabulary models for image classiﬁcation, we investigate a straightforward framework,CLIP on Wheels (CoW), to adapt open-vocabulary models to this task without ﬁne-tuning. To better evaluate L-ZSON, we introduce the PASTURE benchmark, which considersﬁnding uncommon objects, objects described by spatial and appearance attributes, and hidden objects described rel-ative to visible objects. We conduct an in-depth empiri-cal study by directly deploying 22 CoW baselines acrossHABITAT, ROBOTHOR, and PASTURE. In total, we eval-uate over 90k navigation episodes and ﬁnd that (1) CoW baselines often struggle to leverage language descriptions but are proﬁcient at ﬁnding uncommon objects. (2) A sim-ple CoW, with CLIP-based object localization and classical exploration—and no additional training—matches the nav-igation efﬁciency of a state-of-the-art ZSON method trained for 500M steps on HABITAT MP3D data. This same CoW provides a 15.6 percentage point improvement in success over a state-of-the-art ROBOTHOR ZSON model.1 