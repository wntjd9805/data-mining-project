When it comes to deploying deep vision models, the be-havior of these systems must be explicable to ensure confi-dence in their reliability and fairness. A common approach to evaluate deep learning models is to build a labeled test set with attributes of interest and assess how well it per-forms. However, creating a balanced test set (i.e., one that is uniformly sampled over all the important traits) is of-ten time-consuming, expensive, and prone to mistakes. The question we try to address is: can we evaluate the sensi-tivity of deep learning models to arbitrary visual attributes without an annotated test set?This paper argues the case that Zero-shot Model Diag-nosis (ZOOM) is possible without the need for a test set nor labeling. To avoid the need for test sets, our system relies on a generative model and CLIP. The key idea is enabling the user to select a set of prompts (relevant to the prob-lem) and our system will automatically search for seman-tic counterfactual images (i.e., synthesized images that flip the prediction in the case of a binary classifier) using the generative model. We evaluate several visual tasks (classi-fication, key-point detection, and segmentation) in multiple visual domains to demonstrate the viability of our method-ology. Extensive experiments demonstrate that our method is capable of producing counterfactual images and offering sensitivity analysis for model diagnosis without the need for a test set. 