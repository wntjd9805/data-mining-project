We introduce submodel co-training, a regularization method related to co-training, self-distillation and stochas-tic depth. Given a neural network to be trained, for each sample we implicitly instantiate two altered networks, “sub-models”, with stochastic depth: we activate only a subset of the layers. Each network serves as a soft teacher to the other, by providing a loss that complements the regular loss provided by the one-hot label. Our approach, dubbed “co-sub”, uses a single set of weights, and does not involve a pre-trained external model or temporal averaging.Experimentally, we show that submodel co-training is effective to train backbones for recognition tasks such as image classiﬁcation and semantic segmentation. Our ap-proach is compatible with multiple architectures, includingRegNet, ViT, PiT, XCiT, Swin and ConvNext. Our training strategy improves their results in comparable settings. For instance, a ViT-B pretrained with cosub on ImageNet-21k obtains 87.4% top-1 acc. @448 on ImageNet-val. 