In this paper, we tackle two challenges in multimodal learning for visual recognition: 1) when missing-modality occurs either during training or testing in real-world sit-uations; and 2) when the computation resources are not available to finetune on heavy transformer models. To this end, we propose to utilize prompt learning and miti-gate the above two challenges together. Specifically, our modality-missing-aware prompts can be plugged into mul-timodal transformers to handle general missing-modality cases, while only requiring less than 1% learnable param-eters compared to training the entire model. We further ex-plore the effect of different prompt configurations and an-alyze the robustness to missing modality. Extensive experi-ments are conducted to show the effectiveness of our prompt learning framework that improves the performance under various missing-modality cases, while alleviating the re-quirement of heavy model re-training. Code is available.1 