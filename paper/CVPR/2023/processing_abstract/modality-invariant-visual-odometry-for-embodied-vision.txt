Effectively localizing an agent in a realistic, noisy setting is crucial for many embodied vision tasks. Visual Odome-try (VO) is a practical substitute for unreliable GPS and compass sensors, especially in indoor environments. WhileSLAM-based methods show a solid performance without large data requirements, they are less flexible and robust w.r.t. to noise and changes in the sensor suite compared to learning-based approaches. Recent deep VO models, however, limit themselves to a fixed set of input modalities, e.g., RGB and depth, while training on millions of sam-ples. When sensors fail, sensor suites change, or modali-ties are intentionally looped out due to available resources, e.g., power consumption, the models fail catastrophically.Furthermore, training these models from scratch is even more expensive without simulator access or suitable exist-ing models that can be fine-tuned. While such scenarios get mostly ignored in simulation, they commonly hinder a modelâ€™s reusability in real-world applications. We propose a Transformer-based modality-invariant VO approach that can deal with diverse or changing sensor suites of naviga-tion agents. Our model outperforms previous methods while training on only a fraction of the data. We hope this method opens the door to a broader range of real-world applica-tions that can benefit from flexible and learned VO models. 