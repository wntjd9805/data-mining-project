Open-set fine-grained retrieval is an emerging challenge that requires an extra capability to retrieve unknown sub-categories during evaluation. However, current works focus on close-set visual concepts, where all the subcategories are pre-defined, and make it hard to capture discrimina-tive knowledge from unknown subcategories, consequently failing to handle unknown subcategories in open-world sce-narios. In this work, we propose a novel Prompting vision-Language Evaluator (PLEor) framework based on the re-cently introduced contrastive language-image pretraining (CLIP) model, for open-set fine-grained retrieval. PLEor could leverage pre-trained CLIP model to infer the discrep-ancies encompassing both pre-defined and unknown subcat-egories, called category-specific discrepancies, and trans-fer them to the backbone network trained in the close-set scenarios. To make pre-trained CLIP model sensitive to category-specific discrepancies, we design a dual prompt scheme to learn a vision prompt specifying the category-specific discrepancies, and turn random vectors with cate-gory names in a text prompt into category-specific discrep-ancy descriptions. Moreover, a vision-language evaluator is proposed to semantically align the vision and text prompts based on CLIP model, and reinforce each other. In addi-tion, we propose an open-set knowledge transfer to transfer the category-specific discrepancies into the backbone net-work using knowledge distillation mechanism. Quantitative and qualitative experiments show that our PLEor achieves promising performance on open-set fine-grained datasets. 