Video Question Answering (VideoQA) is challenging as it requires capturing accurate correlations between modal-ities from redundant information. Recent methods focus on the explicit challenges of the task, e.g. multimodal feature extraction, video-text alignment and fusion. Their frameworks reason the answer relying on statistical evi-dence causes, which ignores potential bias in the multi-modal data. In our work, we investigate relational structure from a causal representation perspective on multimodal data and propose a novel inference framework. For vi-sual data, question-irrelevant objects may establish simple matching associations with the answer. For textual data, the model prefers the local phrase semantics which may de-viate from the global semantics in long sentences. There-fore, to enhance the generalization of the model, we dis-cover the real association by explicitly capturing visual features that are causally related to the question seman-tics and weakening the impact of local language seman-tics on question answering. The experimental results on two large causal VideoQA datasets verify that our pro-posed framework 1) improves the accuracy of the existingVideoQA backbone, 2) demonstrates robustness on com-plex scenes and questions. The code will be released at https://github.com/Chuanqi-Zang/Discovering-the-Real-Association. 