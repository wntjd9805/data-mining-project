Talking head synthesis is a promising approach for the video production industry. Recently, a lot of effort has been devoted in this research area to improve the gener-ation quality or enhance the model generalization. How-ever, there are few works able to address both issues simul-taneously, which is essential for practical applications. To this end, in this paper, we turn attention to the emerging powerful Latent Diffusion Models, and model the Talking head generation as an audio-driven temporally coherent denoising process (DiffTalk). More specifically, instead of employing audio signals as the single driving factor, we investigate the control mechanism of the talking face, and incorporate reference face images and landmarks as conditions for personality-aware generalized synthesis. In this way, the proposed DiffTalk is capable of producing high-quality talking head videos in synchronization with the source audio, and more importantly, it can be naturallyâˆ—Corresponding author generalized across different identities without further fine-tuning. Additionally, our DiffTalk can be gracefully tai-lored for higher-resolution synthesis with negligible extra computational cost. Extensive experiments show that the proposed DiffTalk efficiently synthesizes high-fidelity audio-driven talking head videos for generalized novel identi-ties. For more video results, please refer to https:// sstzal.github.io/DiffTalk/. 