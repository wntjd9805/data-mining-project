Generalized Zero-Shot Learning (GZSL) identifies un-seen categories by knowledge transferred from the seen domain, relying on the intrinsic interactions between vi-sual and semantic information. Prior works mainly lo-calize regions corresponding to the sharing attributes.When various visual appearances correspond to the same attribute, the sharing attributes inevitably introduce se-mantic ambiguity, hampering the exploration of accurateIn this paper, we deploy semantic-visual interactions. the dual semantic-visual transformer module (DSVTM) to progressively model the correspondences between at-tribute prototypes and visual features, constituting a pro-gressive semantic-visual mutual adaption (PSVMA) net-work for semantic disambiguation and knowledge trans-ferability improvement. Specifically, DSVTM devises an instance-motivated semantic encoder that learns instance-centric prototypes to adapt to different images, enabling the recast of the unmatched semantic-visual pair into the matched one. Then, a semantic-motivated instance decoder strengthens accurate cross-domain interactions between the matched pair for semantic-related instance adaption, en-couraging the generation of unambiguous visual represen-tations. Moreover, to mitigate the bias towards seen classes in GZSL, a debiasing loss is proposed to pursue response consistency between seen and unseen predictions. ThePSVMA consistently yields superior performances against other state-of-the-art methods. Code will be available at: https://github.com/ManLiuCoder/PSVMA. 