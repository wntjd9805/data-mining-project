As black-box models increasingly power high-stakes ap-plications, a variety of data-driven explanation methods have been introduced. Meanwhile, machine learning mod-els are constantly challenged by distributional shifts. A question naturally arises: Are data-driven explanations ro-bust against out-of-distribution data? Our empirical re-sults show that even though predict correctly, the model might still yield unreliable explanations under distribu-tional shifts. How to develop robust explanations against out-of-distribution data?To address this problem, we propose an end-to-end model-agnostic learning frameworkDistributionally Robust Explanations (DRE). The key idea is, inspired by self-supervised learning, to fully utilizes the inter-distribution information to provide supervisory sig-nals for the learning of explanations without human anno-tation. Can robust explanations benefit the model’s general-ization capability? We conduct extensive experiments on a wide range of tasks and data types, including classification and regression on image and scientific tabular data. Our results demonstrate that the proposed method significantly improves the model’s performance in terms of explanation and prediction robustness against distributional shifts. 