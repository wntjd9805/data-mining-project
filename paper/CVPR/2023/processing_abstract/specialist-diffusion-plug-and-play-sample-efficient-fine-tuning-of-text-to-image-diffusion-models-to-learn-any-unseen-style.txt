Diffusion models have demonstrated impressive capabil-ity of text-conditioned image synthesis, and broader appli-cation horizons are emerging by personalizing those pre-trained diffusion models toward generating some special-ized target object or style. In this paper, we aim to learn an unseen style by simply fine-tuning a pre-trained diffu-sion model with a handful of images (e.g., less than 10), so that the fine-tuned model can generate high-quality im-ages of arbitrary objects in this style. Such extremely low-shot fine-tuning is accomplished by a novel toolkit of fine-tuning techniques, including text-to-image customized data augmentations, a content loss to facilitate content-style dis-entanglement, and sparse updating that focuses on only a few time steps. Our framework, dubbed Specialist Dif-fusion, is plug-and-play to existing diffusion model back-bones and other personalization techniques. We demon-strate it to outperform the latest few-shot personalization alternatives of diffusion models such as Textual Inversion[7] and DreamBooth [24], in terms of learning highly so-phisticated styles with ultra-sample-efficient tuning. We further show that Specialist Diffusion can be integrated on top of textual inversion to boost performance further, even on highly unusual styles. Our codes are available at: https://github.com/Picsart-AI-Research/Specialist-Diffusion. 