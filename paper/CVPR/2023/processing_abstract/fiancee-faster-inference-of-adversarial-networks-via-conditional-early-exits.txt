Generative DNNs are a powerful tool for image synthe-sis, but they are limited by their computational load. On the other hand, given a trained model and a task, e.g. faces generation within a range of characteristics, the output im-age quality will be unevenly distributed among images with different characteristics. It follows, that we might restrain the model’s complexity on some instances, maintaining a high quality. We propose a method for diminishing compu-tations by adding so-called early exit branches to the orig-inal architecture, and dynamically switching the computa-tional path depending on how difficult it will be to render the output. We apply our method on two different SOTA models performing generative tasks: generation from a semantic map, and cross-reenactment of face expressions; showing it is able to output images with custom lower-quality thresh-olds. For a threshold of LPIPS ≤ 0.1, we diminish their computations by up to a half. This is especially relevant for real-time applications such as synthesis of faces, when quality loss needs to be contained, but most of the inputs need fewer computations than the complex instances. 