Diffusion models currently achieve state-of-the-art per-formance for both conditional and unconditional image generation. However, so far, image diffusion models do not support tasks required for 3D understanding, such as view-consistent 3D generation or single-view object reconstruc-tion. In this paper, we present RenderDiffusion, the first dif-fusion model for 3D generation and inference, trained using only monocular 2D supervision. Central to our method is a novel image denoising architecture that generates and ren-ders an intermediate three-dimensional representation of a scene in each denoising step. This enforces a strong induc-tive structure within the diffusion process, providing a 3D consistent representation while only requiring 2D supervi-sion. The resulting 3D representation can be rendered from any view. We evaluate RenderDiffusion on FFHQ, AFHQ,ShapeNet and CLEVR datasets, showing competitive per-formance for generation of 3D scenes and inference of 3D scenes from 2D images. Additionally, our diffusion-based approach allows us to use 2D inpainting to edit 3D scenes.Project page: https : / / github . com / Anciukevicius /RenderDiffusion 