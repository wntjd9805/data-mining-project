Transformers are powerful visual learners, in large part due to their conspicuous lack of manually-specified pri-ors. This flexibility can be problematic in tasks that in-volve multiple-view geometry, due to the near-infinite possi-ble variations in 3D shapes and viewpoints (requiring flexi-bility), and the precise nature of projective geometry (obey-ing rigid laws). To resolve this conundrum, we propose a “light touch” approach, guiding visual Transformers to learn multiple-view geometry but allowing them to break free when needed. We achieve this by using epipolar lines to guide the Transformer’s cross-attention maps during train-ing, penalizing attention values outside the epipolar lines and encouraging higher attention along these lines since they contain geometrically plausible matches. Unlike pre-vious methods, our proposal does not require any camera pose information at test-time. We focus on pose-invariant object instance retrieval, where standard Transformer net-works struggle, due to the large differences in viewpoint between query and retrieved images. Experimentally, our method outperforms state-of-the-art approaches at object retrieval, without needing pose information at test-time. 