We revisit and advance visual prompting (VP), an input prompting technique for vision tasks. VP can reprogram aﬁxed, pre-trained source model to accomplish downstream tasks in the target domain by simply incorporating univer-sal prompts (in terms of input perturbation patterns) into downstream data points. Yet, it remains elusive why VP stays effective even given a ruleless label mapping (LM) between the source classes and the target classes. Inspired by the above, we ask: How is LM interrelated with VP? And how to exploit such a relationship to improve its accuracy on target tasks? We peer into the inﬂuence of LM on VP and provide an afﬁrmative answer that a better ‘quality’ ofLM (assessed by mapping precision and explanation) can consistently improve the effectiveness of VP. This is in con-trast to the prior art where the factor of LM was missing.To optimize LM, we propose a new VP framework, termedILM-VP (iterative label mapping-based visual prompting), which automatically re-maps the source labels to the target labels and progressively improves the target task accuracy of VP. Further, when using a contrastive language–image pretrained (CLIP) model for VP, we propose to integrate an LM process to assist the text prompt selection of CLIP and to improve the target task accuracy. Extensive exper-iments demonstrate that our proposal signiﬁcantly outper-forms state-of-the-art VP methods. As highlighted below, we show that when reprogramming an ImageNet-pretrainedResNet-18 to 13 target tasks, ILM-VP outperforms base-lines by a substantial margin, e.g., 7.9% and 6.7% accuracy improvements in transfer learning to the target Flowers102 and CIFAR100 datasets. Besides, our proposal on CLIP-based VP provides 13.7% and 7.1% accuracy improvements on Flowers102 and DTD respectively. Code is available at https://github.com/OPTML-Group/ILM-VP. 