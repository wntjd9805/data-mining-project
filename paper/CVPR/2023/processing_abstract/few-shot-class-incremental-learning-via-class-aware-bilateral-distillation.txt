Few-Shot Class-Incremental Learning (FSCIL) aims to continually learn novel classes based on only few train-ing samples, which poses a more challenging task than the well-studied Class-Incremental Learning (CIL) due to data scarcity. While knowledge distillation, a prevailing tech-nique in CIL, can alleviate the catastrophic forgetting of older classes by regularizing outputs between current and previous model, it fails to consider the overﬁtting risk of novel classes in FSCIL. To adapt the powerful distillation technique for FSCIL, we propose a novel distillation struc-ture, by taking the unique challenge of overﬁtting into ac-count. Concretely, we draw knowledge from two comple-mentary teachers. One is the model trained on abundant data from base classes that carries rich general knowledge, which can be leveraged for easing the overﬁtting of cur-rent novel classes. The other is the updated model from last incremental session that contains the adapted knowl-edge of previous novel classes, which is used for alleviat-ing their forgetting. To combine the guidances, an adaptive strategy conditioned on the class-wise semantic similari-ties is introduced. Besides, for better preserving base class knowledge when accommodating novel concepts, we adopt a two-branch network with an attention-based aggregation module to dynamically merge predictions from two com-plementary branches. Extensive experiments on 3 popularFSCIL datasets: mini-ImageNet, CIFAR100 and CUB200 validate the effectiveness of our method by surpassing ex-isting works by a signiﬁcant margin. Code is available at https://github.com/LinglanZhao/BiDistFSCIL. 