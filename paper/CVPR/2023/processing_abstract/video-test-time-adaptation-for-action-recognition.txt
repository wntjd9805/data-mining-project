Although action recognition systems can achieve top per-formance when evaluated on in-distribution test points, they are vulnerable to unanticipated distribution shifts in test data. However, test-time adaptation of video action recog-nition models against common distribution shifts has so far not been demonstrated. We propose to address this prob-lem with an approach tailored to spatio-temporal models that is capable of adaptation on a single video sample at a step. It consists in a feature distribution alignment tech-nique that aligns online estimates of test set statistics to-wards the training statistics. We further enforce prediction consistency over temporally augmented views of the same test video sample. Evaluations on three benchmark ac-tion recognition datasets show that our proposed technique is architecture-agnostic and able to significantly boost the performance on both, the state of the art convolutional ar-chitecture TANet and the Video Swin Transformer. Our pro-posed method demonstrates a substantial performance gain over existing test-time adaptation approaches in both eval-uations of a single distribution shift and the challenging case of random distribution shifts. Code will be available at https://github.com/wlin-at/ViTTA. 