Classiﬁcation has been the focal point of research on ad-versarial attacks, but only a few works investigate methods suited to denser prediction tasks, such as semantic segmenta-tion. The methods proposed in these works do not accurately solve the adversarial segmentation problem and, therefore, overestimate the size of the perturbations required to fool models. Here, we propose a white-box attack for these mod-els based on a proximal splitting to produce adversarial perturbations with much smaller ℓ∞ norms. Our attack can handle large numbers of constraints within a nonconvex minimization framework via an Augmented Lagrangian ap-proach, coupled with adaptive constraint scaling and mask-ing strategies. We demonstrate that our attack signiﬁcantly outperforms previously proposed ones, as well as classiﬁca-tion attacks that we adapted for segmentation, providing aﬁrst comprehensive benchmark for this dense task. 