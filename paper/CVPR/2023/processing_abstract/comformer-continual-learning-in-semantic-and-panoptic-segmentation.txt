Continual learning for segmentation has recently seen increasing interest. However, all previous works focus on narrow semantic segmentation and disregard panoptic seg-mentation, an important task with real-world impacts. In this paper, we present the first continual learning model capable of operating on both semantic and panoptic seg-mentation. Inspired by recent transformer approaches that consider segmentation as a mask-classification problem, we design CoMFormer. Our method carefully exploits the prop-erties of transformer architectures to learn new classes over time. Specifically, we propose a novel adaptive distilla-tion loss along with a mask-based pseudo-labeling tech-nique to effectively prevent forgetting. To evaluate our ap-proach, we introduce a novel continual panoptic segmenta-tion benchmark on the challenging ADE20K dataset. OurCoMFormer outperforms all the existing baselines by for-getting less old classes but also learning more effectively new classes. In addition, we also report an extensive eval-uation in the large-scale continual semantic segmentation scenario showing that CoMFormer also significantly out-performs state-of-the-art methods. 1 