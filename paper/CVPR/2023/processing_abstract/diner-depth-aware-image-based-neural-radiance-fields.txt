We present Depth-aware Image-based NEural Radianceﬁelds (DINER). Given a sparse set of RGB input views, we predict depth and feature maps to guide the reconstruction of a volumetric scene representation that allows us to ren-der 3D objects under novel views. Speciﬁcally, we propose novel techniques to incorporate depth information into fea-ture fusion and efﬁcient scene sampling. In comparison to the previous state of the art, DINER achieves higher synthe-sis quality and can process input views with greater dispar-ity. This allows us to capture scenes more completely with-out changing capturing hardware requirements and ulti-mately enables larger viewpoint changes during novel view synthesis. We evaluate our method by synthesizing novel views, both for human heads and for general objects, and observe signiﬁcantly improved qualitative results and in-creased perceptual metrics compared to the previous state of the art. The code is publicly available through the ProjectWebpage. 