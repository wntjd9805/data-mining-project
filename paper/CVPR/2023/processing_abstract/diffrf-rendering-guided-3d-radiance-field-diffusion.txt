We introduce DiffRF, a novel approach for 3D radiance field synthesis based on denoising diffusion probabilistic models. While existing diffusion-based methods operate on images, latent codes, or point cloud data, we are the first to directly generate volumetric radiance fields. To this end, we propose a 3D denoising model which directly operates on an explicit voxel grid representation. However, as radi-ance fields generated from a set of posed images can be am-biguous and contain artifacts, obtaining ground truth radi-ance field samples is non-trivial. We address this challenge by pairing the denoising formulation with a rendering loss, enabling our model to learn a deviated prior that favours good image quality instead of trying to replicate fitting er-rors like floating artifacts. In contrast to 2D-diffusion mod-els, our model learns multi-view consistent priors, enabling free-view synthesis and accurate shape generation. Com-pared to 3D GANs, our diffusion-based approach naturally enables conditional generation such as masked completion or single-view 3D synthesis at inference time. 