To effectively interrogate UAV-based images for detect-ing objects of interest, such as humans, it is essential to acquire large-scale UAV-based datasets that include human instances with various poses captured from widely varying viewing angles. As a viable alternative to laborious and costly data curation, we introduce Progressive Transfor-mation Learning (PTL), which gradually augments a train-ing dataset by adding transformed virtual images with en-hanced realism. Generally, a virtual2real transformation generator in the conditional GAN framework suffers from quality degradation when a large domain gap exists be-tween real and virtual images. To deal with the domain gap,PTL takes a novel approach that progressively iterates the following three steps: 1) select a subset from a pool of vir-tual images according to the domain gap, 2) transform the selected virtual images to enhance realism, and 3) add the transformed virtual images to the training set while remov-ing them from the pool. In PTL, accurately quantifying the domain gap is critical. To do that, we theoretically demon-strate that the feature representation space of a given object detector can be modeled as a multivariate Gaussian dis-tribution from which the Mahalanobis distance between a virtual object and the Gaussian distribution of each object category in the representation space can be readily com-puted. Experiments show that PTL results in a substantial performance increase over the baseline, especially in the small data and the cross-domain regime. 