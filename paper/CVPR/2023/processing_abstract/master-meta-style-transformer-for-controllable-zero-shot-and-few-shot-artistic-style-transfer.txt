Transformer-based models achieve favorable perfor-mance in artistic style transfer recently thanks to its global receptive field and powerful multi-head/layer atten-tion operations. Nevertheless, the over-paramerized multi-layer structure increases parameters significantly and thus presents a heavy burden for training. Moreover, for the task of style transfer, vanilla Transformer that fuses con-tent and style features by residual connections is prone to content-wise distortion.In this paper, we devise a novelTransformer model termed as Master specifically for style transfer. On the one hand, in the proposed model, differ-ent Transformer layers share a common group of parame-ters, which (1) reduces the total number of parameters, (2) leads to more robust training convergence, and (3) is read-ily to control the degree of stylization via tuning the num-ber of stacked layers freely during inference. On the other hand, different from the vanilla version, we adopt a learn-able scaling operation on content features before content-style feature interaction, which better preserves the original similarity between a pair of content features while ensuring the stylization quality. We also propose a novel meta learn-ing scheme for the proposed model so that it can not only work in the typical setting of arbitrary style transfer, but also adaptable to the few-shot setting, by only fine-tuning the Transformer encoder layer in the few-shot stage for one*Equal contribution.â€ Corresponding author. specific style. Text-guided few-shot style transfer is firstly achieved with the proposed framework. Extensive exper-iments demonstrate the superiority of Master under both zero-shot and few-shot style transfer settings. 