In this paper, we propose an end-to-end Retrieval-Augmented Visual Language Model (REV EAL) that learns to encode world knowledge into a large-scale memory, and to retrieve from it to answer knowledge-intensive queries.REV EAL consists of four key components: the memory, the encoder, the retriever and the generator. The large-scale memory encodes various sources of multimodal world knowl-edge (e.g. image-text pairs, question answering pairs, knowl-edge graph triplets, etc.) via a unified encoder. The retriever finds the most relevant knowledge entries in the memory, and the generator fuses the retrieved knowledge with the input query to produce the output. A key novelty in our approach is that the memory, encoder, retriever and generator are all pre-trained end-to-end on a massive amount of data. Fur-thermore, our approach can use a diverse set of multimodal knowledge sources, which is shown to result in significant gains. We show that REV EAL achieves state-of-the-art re-sults on visual question answering and image captioning.The project page of this work is reveal.github.io. 