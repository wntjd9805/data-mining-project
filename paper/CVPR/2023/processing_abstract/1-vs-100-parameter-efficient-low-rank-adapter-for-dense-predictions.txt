Fine-tuning large-scale pre-trained vision models to downstream tasks is a standard technique for achieving state-of-the-art performance on computer vision bench-marks. However, ﬁne-tuning the whole model with millions of parameters is inefﬁcient as it requires storing a same-sized new model copy for each task. In this work, we pro-pose LoRand, a method for ﬁne-tuning large-scale vision models with a better trade-off between task performance and the number of trainable parameters. LoRand gener-ates tiny adapter structures with low-rank synthesis while keeping the original backbone parameters ﬁxed, resulting in high parameter sharing. To demonstrate LoRand’s ef-fectiveness, we implement extensive experiments on object detection, semantic segmentation, and instance segmenta-tion tasks. By only training a small percentage (1% to 3%) of the pre-trained backbone parameters, LoRand achieves comparable performance to standard ﬁne-tuning on COCO and ADE20K and outperforms ﬁne-tuning in low-resourcePASCAL VOC dataset. 