Multimodal models are becoming increasingly effective, in part due to uniﬁed components, such as the Transformer architecture. However, multimodal models still often consist of many task- and modality-speciﬁc pieces and training pro-cedures. For example, CLIP (Radford et al., 2021) trains in-dependent text and image towers via a contrastive loss. We explore an additional uniﬁcation: the use of a pure pixel-based model to perform image, text, and multimodal tasks.Our model is trained with contrastive loss alone, so we call it CLIP-Pixels Only (CLIPPO). CLIPPO uses a single en-coder that processes both regular images and text rendered as images. CLIPPO performs image-based tasks such as re-trieval and zero-shot image classiﬁcation almost as well asCLIP-style models, with half the number of parameters and no text-speciﬁc tower or embedding. When trained jointly via image-text contrastive learning and next-sentence con-trastive learning, CLIPPO can perform well on natural language understanding tasks, without any word-level loss (language modelling or masked language modelling), out-performing pixel-based prior work. Surprisingly, CLIPPO can obtain good accuracy in visual question answering, simply by rendering the question and image together. Fi-nally, we exploit the fact that CLIPPO does not require a tokenizer to show that it can achieve strong performance on multilingual multimodal retrieval without modiﬁcations. 