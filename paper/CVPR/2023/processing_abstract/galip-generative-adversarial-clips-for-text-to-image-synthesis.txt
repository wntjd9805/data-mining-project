Synthesizing high-fidelity complex images from text is challenging. Based on large pretraining, the autoregres-sive and diffusion models can synthesize photo-realistic im-ages. Although these large models have shown notable progress, there remain three flaws. 1) These models re-quire tremendous training data and parameters to achieve good performance. 2) The multi-step generation design slows the image synthesis process heavily. 3) The synthe-sized visual features are challenging to control and require delicately designed prompts. To enable high-quality, effi-cient, fast, and controllable text-to-image synthesis, we pro-pose Generative Adversarial CLIPs, namely GALIP. GALIP leverages the powerful pretrained CLIP model both in the discriminator and generator. Specifically, we propose aCLIP-based discriminator. The complex scene understand-ing ability of CLIP enables the discriminator to accurately assess the image quality. Furthermore, we propose a CLIP-empowered generator that induces the visual concepts fromCLIP through bridge features and prompts. The CLIP-integrated generator and discriminator boost training ef-ficiency, and as a result, our model only requires about 3% training data and 6% learnable parameters, achieving com-parable results to large pretrained autoregressive and diffu-sion models. Moreover, our model achieves ∼120×faster synthesis speed and inherits the smooth latent space fromGAN. The extensive experimental results demonstrate the excellent performance of our GALIP. Code is available at https://github.com/tobran/GALIP. 