Due to data privacy issues, accelerating networks with tiny training sets has become a critical need in practice.Previous methods mainly adopt filter-level pruning to ac-celerate networks with scarce training samples. In this pa-per, we reveal that dropping blocks is a fundamentally su-perior approach in this scenario.It enjoys a higher ac-celeration ratio and results in a better latency-accuracy performance under the few-shot setting. To choose which blocks to drop, we propose a new concept namely recov-erability to measure the difficulty of recovering the com-pressed network. Our recoverability is efficient and effec-tive for choosing which blocks to drop. Finally, we propose an algorithm named PRACTISE to accelerate networks us-ing only tiny sets of training images. PRACTISE outper-forms previous methods by a significant margin. For 22% latency reduction, PRACTISE surpasses previous methods by on average 7% on ImageNet-1k.It also enjoys high generalization ability, working well under data-free or out-of-domain data settings, too. Our code is at https://github.com/DoctorKey/Practise. 