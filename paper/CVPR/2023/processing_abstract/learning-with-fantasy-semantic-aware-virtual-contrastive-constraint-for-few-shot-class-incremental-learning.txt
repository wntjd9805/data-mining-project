Few-shot class-incremental learning (FSCIL) aims at learning to classify new classes continually from limited samples without forgetting the old classes. The mainstream framework tackling FSCIL is first to adopt the cross-entropy (CE) loss for training at the base session, then freeze the feature extractor to adapt to new classes. However, in this work, we find that the CE loss is not ideal for the base ses-sion training as it suffers poor class separation in terms of representations, which further degrades generalization to novel classes. One tempting method to mitigate this prob-lem is to apply an additional na¨ıve supervised contrastive learning (SCL) in the base session. Unfortunately, we find that although SCL can create a slightly better representa-tion separation among different base classes, it still strug-gles to separate base classes and new classes.Inspired by the observations made, we propose Semantic-Aware Vir-tual Contrastive model (SAVC), a novel method that facili-tates separation between new classes and base classes by introducing virtual classes to SCL. These virtual classes, which are generated via pre-defined transformations, not only act as placeholders for unseen classes in the repre-sentation space, but also provide diverse semantic infor-mation. By learning to recognize and contrast in the fan-tasy space fostered by virtual classes, our SAVC signifi-cantly boosts base class separation and novel class gen-eralization, achieving new state-of-the-art performance on the three widely-used FSCIL benchmark datasets. Code is available at: https://github.com/zysong0113/SAVC. 