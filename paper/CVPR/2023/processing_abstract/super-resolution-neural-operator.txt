We propose Super-resolution Neural Operator (SRNO), a deep operator learning framework that can resolve high-resolution (HR) images at arbitrary scales from the low-resolution (LR) counterparts. Treating the LR-HR image pairs as continuous functions approximated with different grid sizes, SRNO learns the mapping between the corre-sponding function spaces. From the perspective of ap-proximation theory, SRNO ﬁrst embeds the LR input into a higher-dimensional latent representation space, trying to capture sufﬁcient basis functions, and then iteratively ap-proximates the implicit image function with a kernel inte-gral mechanism, followed by a ﬁnal dimensionality reduc-tion step to generate the RGB representation at the target coordinates. The key characteristics distinguishing SRNO from prior continuous SR works are: 1) the kernel integral in each layer is efﬁciently implemented via the Galerkin-type attention, which possesses non-local properties in the spatial domain and therefore beneﬁts the grid-free contin-uum; and 2) the multilayer attention architecture allows for the dynamic latent basis update, which is crucial for SR problems to “hallucinate” high-frequency information from the LR image. Experiments show that SRNO outperforms existing continuous SR methods in terms of both accuracy and running time. Our code is at https://github.com/ 2y7c3/Super-Resolution-Neural-Operator. 