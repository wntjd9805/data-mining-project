Denoising diffusion (score-based) generative models have recently achieved significant accomplishments in gen-erating realistic and diverse data. Unfortunately, the gener-ation process of current denoising diffusion models is noto-riously slow due to the lengthy iterative noise estimations, which rely on cumbersome neural networks.It prevents the diffusion models from being widely deployed, especially on edge devices. Previous works accelerate the generation process of diffusion model (DM) via finding shorter yet ef-fective sampling trajectories. However, they overlook the cost of noise estimation with a heavy network in every iter-ation. In this work, we accelerate generation from the per-spective of compressing the noise estimation network. Due to the difficulty of retraining DMs, we exclude mainstream training-aware compression paradigms and introduce post-training quantization (PTQ) into DM acceleration. How-ever, the output distributions of noise estimation networks change with time-step, making previous PTQ methods fail in DMs since they are designed for single-time step sce-narios. To devise a DM-specific PTQ method, we explorePTQ on DM in three aspects: quantized operations, cali-bration dataset, and calibration metric. We summarize and use several observations derived from all-inclusive inves-tigations to formulate our method, which especially tar-gets the unique multi-time-step structure of DMs. Exper-imentally, our method can directly quantize full-precisionDMs into 8-bit models while maintaining or even improv-ing their performance in a training-free manner.Impor-tantly, our method can serve as a plug-and-play module on other fast-sampling methods, e.g., DDIM [24]. The code is available at https://https://github.com/ 42Shawn/PTQ4DM . 