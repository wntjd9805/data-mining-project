This paper introduces a new framework called the side adapter network (SAN) for open-vocabulary semantic segmentation. The SAN utilizes the CLIP model for both mask prediction and recognition by leveraging a decoupled design with two branches. The paper presents a single-forward design to minimize the cost of CLIP and improve segmentation performance. The study is based on officially released CLIP models and focuses on ViT CLIP models. The paper also addresses the issue of conflicting input resolutions and explores fine-tuning the positional embedding of the ViT model. The proposed method achieves state-of-the-art performance on various benchmarks while requiring fewer trainable parameters and GFLOPs compared to previous methods.