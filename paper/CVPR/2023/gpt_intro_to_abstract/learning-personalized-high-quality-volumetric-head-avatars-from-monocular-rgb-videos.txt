Creating a controllable human avatar is an important technology for various applications in augmented reality/virtual reality (AR/VR) communication, virtual try-on, virtual tourism, games, and visual effects for movies. Previous avatar generation methods often require complex hardware setups or manual intervention. In contrast, reconstructing avatars from monocular RGB videos allows for more flexibility in equipment setup and application scenarios. However, generating avatars with realistic facial geometry and appearance from monocular videos is challenging. Traditional methods using 3D Mor-phable Models (3DMM) have limitations in capturing subject-specific details and dynamic variations. Recent works have incorporated neural radiance fields in combination with 3DMMs to achieve photorealistic renderings of avatars. However, these methods lack fine-grain control over geometry and articulation, resulting in a loss of high-frequency details and an uncanny appearance. In this paper, we propose a method to learn a neural head avatar from a monocular RGB video, controlled by a 3DMM model. Our method delivers high-quality rendering of arbitrary facial expressions, head poses, and viewpoints, preserving fine-grained details and accurate articulations. We achieve this by learning to predict expression-dependent local features on the surface of the 3DMM mesh. These features are used to obtain a radiance field for each 3D point on the mesh, resulting in high-quality rendering. Our approach relies on local features to model appearance and deformation details, while leveraging the 3DMM to model global geometry. We train a convolutional neural network to learn local features from UV-space deformations of the 3DMM. Our experiments show that our method provides competitive controllability and generates sharper and detail-enriched rendering compared to state-of-the-art approaches.