In recent years, there has been an increasing demand for stylizing and editing 3D scenes and objects in various fields such as augmented reality, game scene design, and digital artwork. Traditionally, professionals have relied on creating 2D reference images and converting them into stylized 3D textures, but establishing direct cross-modal correspondence is challenging and time-consuming. This paper addresses the challenge of ensuring that stylized results are perceptually similar to the given style reference. Previous methods using radiance fields have facilitated style transfer from 2D style references to 3D representations, but they lack explicit control over the generated results. On the other hand, reference-based video stylization methods allow for controllable generation of stylized novel views, but they may diverge from the desired style when stylizing a frame sequence with unseen content. To overcome these limitations, the authors propose a new paradigm called Reference-Based Non-Photorealistic Radiance Fields (Ref-NPR) for stylizing 3D scenes using a single stylized reference view. Ref-NPR utilizes stylized views from radiance fields as references to achieve both flexible controllability and multi-view consistency. It employs a reference-based ray registration process to project the 2D style reference into 3D space and perform template-based feature matching for semantic style correspondence in occluded regions. The results demonstrate that Ref-NPR generates visually appealing stylized views that maintain geometric and semantic consistency with the given style reference, outperforming state-of-the-art scene stylization methods. Overall, this paper introduces a new paradigm for stylizing 3D scenes, proposes the Ref-NPR approach for controllable scene stylization, and presents experimental results demonstrating its superiority over existing methods.