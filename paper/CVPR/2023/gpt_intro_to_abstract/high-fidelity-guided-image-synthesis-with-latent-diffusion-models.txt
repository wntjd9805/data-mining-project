The paper introduces a diffusion-based guided image synthesis framework for generating realistic images based on user scribbles and text prompts. The existing methods combine text-based conditioning with unsupervised guidance from a reference image, but they often lack details and realism. The proposed framework formulates the image synthesis as a constrained optimization problem and achieves a practical approximation through gradient descent. However, the fine-grain semantics of different painting regions may not accurately reflect the user's intent. To address this, the paper introduces a cross-attention based correspondence between the input text tokens and user stroke-painting to control the semantics of different regions without requiring semantic-segmentation based training or finetuning.