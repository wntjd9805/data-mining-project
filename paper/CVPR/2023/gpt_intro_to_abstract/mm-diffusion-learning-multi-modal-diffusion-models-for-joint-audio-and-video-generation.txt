This paper addresses the task of multi-modality generation in the open domain, specifically focusing on joint audio-video generation. While there have been advancements in content generation in single modalities using diffusion models, such as image or text, the generation of multi-modal content using diffusion models remains largely unexplored. The challenges in designing multi-modal diffusion models lie in processing the distinct data patterns of video and audio, as well as capturing the synchronous relationship between the two modalities. To address these challenges, the authors propose the Multi-Modal Diffusion model (MM-Diffusion), which consists of two-coupled denoising autoencoders for joint audio-video generation. This design allows for the learning of a joint distribution over both modalities. Additionally, a novel cross-modal attention block is introduced to ensure the generated video frames and audio segments are correlated at each moment. An efficient random-shift mechanism facilitates cross-modal interactions by reducing temporal redundancies in video and audio.The proposed MM-Diffusion model is evaluated on the Landscape dataset and the AIST++ dancing dataset. The evaluation results demonstrate the superiority of the model over state-of-the-art modality-specific unconditional generation models, with significant gains in visual and audio quality. The model also showcases its capability for zero-shot conditional generation without task-driven fine-tuning. Moreover, Turing tests with 10k votes confirm the high-fidelity performance of the generated results for common users.