Image synthesis using text-guided diffusion models has gained attention for its realism and diversity. However, editing real images with these models requires inverting the image and textual prompt, which has not been fully addressed for text-guided diffusion models. In this paper, we propose an effective inversion scheme that achieves near-perfect reconstruction while preserving the editing capabilities of the original model. Our approach leverages classifier-free guidance and DDIM inversion techniques. We optimize the embedding used in the unconditional part to invert the input image and prompt, referred to as null-text optimization. We also use the sequence of noised latent codes obtained from initial DDIM inversion as a pivot for optimization, resulting in Diffusion Pivotal Inversion. Our approach enables text editing on real images and avoids damaging the trained model's prior. Through extensive study and comparisons, we demonstrate the effectiveness of our approach in achieving high-fidelity reconstructions and intuitive editing abilities. Code for our approach will be publicly available.