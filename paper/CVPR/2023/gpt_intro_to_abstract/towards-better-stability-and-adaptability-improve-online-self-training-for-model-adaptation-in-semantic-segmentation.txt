Unsupervised Domain Adaptation (UDA) has gained significant attention in semantic segmentation tasks, aiming to transfer knowledge from source domains to target domains. UDA helps reduce the reliance on dense annotations and enhance the generalization ability of deep neural network-based models. However, the unavailability of source domain data poses challenges for UDA. To address this, the setting of Unsupervised Model Adaptation (UMA) is proposed, which aims to adapt source-trained models to unlabeled target domains without using source domain data. Existing UMA methods primarily rely on self-training with pseudo-labels, but they often require expert intervention and can be under-adapted. Online self-training (ONST) methods have shown promise in UDA by avoiding iterative training, but their application to UMA scenarios without accessing source data remains uncertain. This paper focuses on improving the stability and adaptability of ONST methods in UMA. The authors explore the reasons for the poor performance of ONST in UMA and propose solutions to address them. Specifically, they introduce a Dynamic Teacher Update (DTU) mechanism to control the update interval of the teacher model based on student feedback, and a Training-Consistency based Resampling (TCR) strategy to mitigate bias towards minority categories in UMA. Experimental results demonstrate that the proposed method, named DT-ST, significantly improves the stability and adaptability of ONST in UMA and achieves state-of-the-art performance on UMA benchmarks.