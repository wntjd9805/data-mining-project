Deep neural networks have shown remarkable performance in computer vision tasks, but they are vulnerable to adversarial examples, which are carefully crafted inputs designed to mislead the model. These adversarial examples have the unique characteristic of being transferable between models, allowing adversaries to attack black-box models without knowledge of their internals. However, targeted attacks, which aim to deceive models into predicting a specific harmful class, have lower success rates due to differences in decision boundaries. This paper proposes a novel transfer-based attack that improves the transferability of targeted adversarial examples by introducing competition into their optimization. The approach involves crafting adversarial perturbations towards different target classes and friendly perturbations towards the correct class, causing interference and suppressing the effect of adversarial perturbations. By leveraging a diverse set of features and attack strategies, the transferability of adversarial examples is enhanced. The proposed method, Clean Feature Mixup (CFM), efficiently mimics the behaviors of these perturbations in feature space by mixing clean features of images. Experimental results demonstrate the effectiveness of CFM in improving the transferability of adversarial examples compared to state-of-the-art baselines.