Modern deep learning techniques have achieved remarkable success in various fields, but they are vulnerable to adversarial attacks, where imperceptible perturbations can lead to incorrect predictions with high confidence. Several defense approaches have been proposed to protect deep neural networks, with adversarial training being the most effective method. However, adversarial training often results in a tradeoff between natural accuracy and robustness. Previous works have attempted to address this tradeoff through different methods such as incorporating additional data or relaxing perturbations. In this paper, we propose a new training paradigm called Generalist, inspired by the divide-and-conquer method. Generalist decouples the objective function of adversarial training into two sub-tasks and trains base learners individually for each task. The parameters of the base learners are then combined to form a global learner, which is distributed back to the base learners for continued training. Our experimental results demonstrate that Generalist significantly improves both clean accuracy and robustness compared to existing techniques. The proposed framework provides a novel approach to effectively address the tradeoff between natural and robust generalization in deep learning models.