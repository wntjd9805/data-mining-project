The paper addresses the challenge of out-of-the-box spatiotemporal representation learning for generic video understanding. While previous self-supervised pre-training methods have achieved promising results, they still fall short in terms of transferability and applicability to uncurated datasets. The authors propose a new pretext task called Turning to Video for Transcript Sorting (TVTS) to improve spatiotemporal representation learning. By leveraging the script knowledge present in transcribed speech, the authors aim to capture contextualized spatiotemporal representations that can be used to correctly order shuffled transcripts. The proposed approach combines encoded script representations, video representations, and self-attention to predict the correct order of the transcripts. The authors compare their method with existing approaches and achieve strong performance on downstream action recognition tasks and fine-tuning on various video datasets. The contributions of the paper include exploiting script knowledge for flexible pre-training, introducing the TVTS pretext task, and achieving state-of-the-art results on video understanding tasks.