Birdâ€™s-eye-view (BEV) recognition models are camera-based models for 3D object detection in autonomous driving. These models integrate partial observations from multiple sensors into a unified 3D output space. While much attention is given to the design of view transformation modules and downstream tasks, the study of image backbones in BEV models has received less attention. This paper focuses on unleashing the full power of modern image feature extractors for BEV recognition. The research community has predominantly used the VoVNet backbone due to its large-scale depth pre-training. However, simply employing modern image backbones without proper pre-training does not yield satisfactory results. There are domain gaps between natural images and autonomous driving scenes, and the complex structure of current BEV detectors hinders the adaptation of these backbones. To address these challenges, the authors introduce perspective supervision into BEVFormer, incorporating supervision signals from perspective-view tasks directly to the backbone. This approach guides the backbone to learn 3D knowledge and overcomes the complexity of BEV detectors, facilitating model optimization. A perspective 3D detection head is built upon the backbone, and a perspective loss is added as an auxiliary detection loss. The authors propose a two-stage BEV detector called BEV-Former v2, which combines the perspective head with the original BEV head. Extensive experiments confirm the effectiveness of the proposed perspective supervision, resulting in improved detection performance and faster model convergence. The authors successfully adapt modern image backbones to the BEV model, achieving significant improvements on the nuScenes dataset.