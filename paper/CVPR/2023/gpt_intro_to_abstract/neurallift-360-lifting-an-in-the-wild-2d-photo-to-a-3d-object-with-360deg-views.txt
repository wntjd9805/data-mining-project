Creating 3D content has been a long-standing problem in computer vision, with applications in game studios, home decoration, virtual reality, and augmented reality. Manual modeling has traditionally been the dominant approach, but it is tedious and time-consuming. Automatic 3D content creation pipelines have been developed to streamline the process, leveraging multi-view stereo to quickly model virtual landscapes. More recently, researchers have focused on the more ambitious goal of creating a 3D object from a single image. Existing approaches in this direction can be categorized into two major directions: learning-based approaches that use large-scale datasets to predict 3D information from input images, and pipeline approaches that use depth estimation techniques to transform 2D images into 3D representations. However, these approaches have limitations such as poor generalization ability and limited viewing directions.In this paper, we propose a novel framework called NeuralLift-360, which aims to convert diverse 2D photos into sophisticated 3D content in 360Â° views. Our framework is built on Neural Radiance Fields (NeRFs) and leverages diffusion priors and monocular depth estimation as cues for hallucination. We adopt modern diffusion models trained on large datasets to generate realistic 3D instances based on simple text inputs, guided by CLIP. Additionally, we use relative ranking information from rough depth during training to mitigate geometry errors in depth estimation. Our contributions include demonstrating promising results in lifting single images to 3D using NeRF and diffusion models, proposing a CLIP-guided sampling strategy for integrating prior knowledge with the reference image, finetuning the diffusion model on a single image for cases where the reference image is difficult to describe accurately, and introducing scale-invariant depth supervision using ranking information to broaden the application of our algorithms.