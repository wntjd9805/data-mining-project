Traditional object detection frameworks are limited in their ability to detect arbitrary categories, which hinders their usefulness in various application scenarios. Extending these frameworks to accommodate arbitrary categories requires extensive human effort to annotate instance-level bounding boxes, especially for rare classes. To address this challenge, recent works have explored transferring knowledge from pre-trained vision-language (VL) models. However, these approaches suffer from a domain gap problem and are not capable of recognizing objects with various scales.Another approach involves utilizing image-text pairs crawled from the internet. However, these methods heavily rely on limited human-annotated concepts or a VL model with domain gap issues, and using high-resolution inputs for massive image-text pairs is computationally expensive.To overcome these challenges, we propose DetCLIPv2, an end-to-end open-vocabulary detection pre-training framework that effectively incorporates large-scale image-text pairs. DetCLIPv2 simultaneously learns localization capability and knowledge of broad concepts without requiring pseudo labels from a teacher model. We achieve this by jointly training with heterogeneous data from multiple sources, including detection, grounding, and image-text pairs. Inspired by optimal matching-based set similarity, we use an optimal matching-based set similarity to guide contrastive learning between visual regions and textual concepts. This enables the detector to locate and recognize increasingly rich concepts.To improve training efficiency, we adopt a low-resolution input for image-text pair data since image captions typically describe only the main objects in the image. This reduces the computational burden. DetCLIPv2 demonstrates superior open-vocabulary detection performance and scalable behavior. Compared to previous works, it exploits significantly more image-text pairs while requiring a similar training time. It achieves state-of-the-art performance in zero-shot AP on the LVIS benchmark and exhibits strong generalization to downstream tasks.In conclusion, DetCLIPv2 highlights the potential of incorporating large-scale image-text pairs to achieve open-world detection and encourages further exploration in this direction.