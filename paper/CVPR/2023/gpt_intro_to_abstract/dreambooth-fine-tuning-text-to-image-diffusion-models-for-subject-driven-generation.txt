This paper introduces a new approach for personalizing text-to-image diffusion models in order to generate customized images based on user-specific needs. The goal is to expand the language-vision dictionary of the model to include new words that correspond to specific subjects the user wants to generate. The paper proposes a technique to represent a given subject with rare token identifiers and fine-tune a pre-trained text-to-image framework. To prevent the model from associating the class name with a specific instance, an autogenous, class-specific prior preservation loss is introduced. The approach is applied to various text-based image generation tasks, and the contribution of each component is evaluated through ablation studies. A user study is conducted to assess the fidelity of the synthesized images compared to alternative approaches. The paper also introduces a new dataset and evaluation protocol to measure subject fidelity and prompt fidelity. Overall, this paper presents a novel technique for subject-driven image generation that allows users to generate novel renditions of a subject while preserving its distinctive features.