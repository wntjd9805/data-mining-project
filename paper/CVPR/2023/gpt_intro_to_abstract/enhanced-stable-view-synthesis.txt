Image-based rendering, also known as view synthesis, is a well-known problem in computer vision and graphics. The goal of this problem is to develop a method that allows users to explore a scene by rendering it from a sparse set of captured images. The rendered images should be as realistic as possible to enhance the user experience. Among the existing approaches, the stable view synthesis (SVS) approach has shown excellent results in rendering photorealistic images from novel viewpoints for large-scale scenes. However, SVS assumes correct dense 3D scene reconstruction and camera poses, which may not always be accurate in outdoor scenes. In this paper, we argue that by making use of multiple-view geometry fundamentals and recent developments in deep learning techniques, we can improve the accuracy of geometric scaffold and camera poses for view synthesis. We propose a systematic approach that refines camera poses using graph neural networks and leverages monocular depth estimation to improve 3D scene geometry. By encoding image features and aggregating accurate feature vectors, we render new images using a convolutional network and refine camera poses simultaneously. Our approach achieves superior results in terms of photorealism and outperforms existing methods on benchmark datasets.