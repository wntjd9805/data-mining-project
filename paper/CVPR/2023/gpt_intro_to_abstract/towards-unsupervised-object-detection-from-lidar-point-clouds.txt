Unsupervised learning algorithms that can discover objects from raw sensor data have the potential to overcome the bottleneck caused by the need for manual annotations in data-driven learning algorithms. While unsupervised object detection has been studied in computer vision, current object-centric models struggle to scale to realistic data due to difficulties in object decomposition. In this work, we focus on unsupervised object detection from point clouds in self-driving vehicles, a challenging task due to occlusion and sparsity of observations. We propose a method called OYSTER, which combines density-based spatial clustering, temporal consistency, equivariance, and self-supervised learning to overcome these challenges. Our method exploits point clustering and unsupervised tracking to bootstrap an object detector in the near range, uses CNNs for zero-shot generalization to the long range, and introduces a random ray dropping strategy to bridge the density gap. We also incorporate a self-improvement loop that refines detections iteratively using temporal consistency. Experimental results on Pandaset and Argoverse V2 Sensor datasets demonstrate that OYSTER outperforms other unsupervised methods based on standard metrics and a proposed metric based on distance-to-collision. This work contributes to the development of perception systems that can improve automatically with more data and compute power, without relying heavily on human supervision.