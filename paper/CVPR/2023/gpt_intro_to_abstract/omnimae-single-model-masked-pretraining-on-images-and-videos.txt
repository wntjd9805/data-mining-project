The paper introduces the concept of a single vanilla Transformer architecture that can be trained to work across different visual modalities, such as images and videos. Previous attempts at training unified models have either resulted in worse performance or required the use of alternative architectures tailored towards specific vision tasks. The authors leverage masked pretraining, specifically the Masked Auto-Encoding (MAE) approach, to train an omnivorous visual encoder capable of learning from all modalities without supervision. This approach offers advantages over supervised and discriminative self-supervised objectives, as it does not require human labeling effort and scales well to different visual modalities. The authors showcase the contributions of their work, including the successful application of the Vision Transformer architecture to both images and videos, the development of the first single self-supervised model for both modalities, the ability to use higher masking ratios for training MAE, and improvements to the MAE training process. They also propose the use of a shared decoder for videos and images to achieve better performance with fewer parameters.