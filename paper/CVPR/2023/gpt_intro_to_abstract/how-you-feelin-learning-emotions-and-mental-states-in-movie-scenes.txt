This paper introduces EmoTx, a multimodal Transformer-based architecture for predicting emotions and mental states of characters in movie scenes. The authors argue that understanding the evolution of emotions and mental states is essential for machines to comprehend movies and stories. They leverage annotations from MovieGraphs and train models to analyze video frames, dialogues, and character appearances to predict emotions and mental states. The paper also discusses the relevance of emotions in psychology and previous works in emotion recognition. The authors propose a multi-label approach and demonstrate that EmoTx outperforms existing methods. They also analyze the self-attention mechanism of the model and its treatment of expressive emotions vs. mental states. The contributions of the paper include formulating emotion and mental state classification as a multi-label problem, proposing EmoTx, and demonstrating its superior performance.