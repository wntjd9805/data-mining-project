Large transformer models with increased parameters have achieved new benchmarks in vision and language tasks. These models store world knowledge implicitly in their parameters and achieve state-of-the-art results when fine-tuned for secondary tasks. However, scaling these models poses challenges in learning and serving, updating the model with changing facts, and interpreting their decision-making process. To address these issues, we propose a new perspective where world knowledge is transformed into a massive-scale index or memory, and a small model accesses this memory for inference tasks. This semi-parametric approach allows models to directly access a large database for predictions. We evaluate our approach on the problem of long-tailed recognition and learning with noisy labels, which is a common and practical problem. We compare our approach to existing methods and achieve state-of-the-art results on various benchmarks. Our contributions include proposing a retrieval-augmented recognition model that efficiently integrates a massive-scale memory, developing a memory attention module to incorporate retrieved knowledge with the input query, and achieving high accuracy on long-tail recognition and learning datasets.