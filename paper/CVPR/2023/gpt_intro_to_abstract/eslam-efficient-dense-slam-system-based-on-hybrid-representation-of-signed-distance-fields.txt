Dense visual Simultaneous Localization and Mapping (SLAM) is a significant challenge in 3D computer vision, with applications in autonomous driving, robotics, and virtual/augmented reality. While traditional SLAM systems primarily focus on localization accuracy, recent learning-based dense visual SLAM methods have shown promising results in constructing global 3D maps. However, these methods still have limitations in terms of reconstruction accuracy and computational demands for real-time applications. This paper proposes a novel approach that leverages an implicit Truncated Signed Distance Field (TSDF) representation to achieve faster convergence and higher quality reconstruction. Instead of storing features on voxel grids, the method employs multi-scale axis-aligned feature planes to reduce memory footprint growth rate. The proposed method is benchmarked on three challenging datasets, demonstrating its performance compared to existing methods. The method also produces higher-quality smooth surfaces without the need for explicit smoothness loss functions. Several other Radiance Fields-based SLAM systems are also discussed, highlighting their strengths and limitations.