This paper focuses on the prediction of 3D semantic scene graphs (3DSSG) in point clouds, which is important for tasks like AR/VR and navigation. However, predicting 3DSSG faces challenges such as the limited information provided by point cloud data and the imbalanced distribution of predicates. To address these challenges, the authors propose a Visual-Linguistic Semantics Assisted Training (VL-SAT) scheme. This scheme trains an oracle model that combines 2D semantics, language knowledge, and geometrical features, and aligns them with the 3D model. The benefits of the oracle model are then embedded into the 3D model, leading to improved 3DSSG prediction performance. The proposed scheme is applied to common 3DSSG prediction models and validated on a 3DSSG dataset. Results show significant performance gains, especially for tail relations. Overall, VL-SAT is the first visual-linguistic knowledge transfer work for 3DSSG prediction in point clouds and shows generalizability to different models.