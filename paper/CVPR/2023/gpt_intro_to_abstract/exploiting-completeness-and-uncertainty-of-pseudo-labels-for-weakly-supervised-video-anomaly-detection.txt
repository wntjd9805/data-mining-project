Automatically detecting abnormal events in videos has recently gained attention due to its applications in intelligent surveillance systems. Most existing methods for anomaly detection in videos are developed within the weakly supervised learning framework, where only video-level annotations are available. However, the goal of anomaly detection is to predict frame-level anomaly scores during testing, posing significant challenges for weakly supervised methods. Two main categories of existing methods for weakly supervised video anomaly detection are one-stage methods based on Multiple Instance Learning (MIL) and two-stage self-training methods. MIL-based methods treat normal and abnormal videos as negative and positive bags, respectively, with clips of a video serving as instances. These methods adopt ranking loss to encourage higher anomaly scores in positive bags than in negative bags. However, due to the lack of clip-level annotations, the anomaly scores generated by MIL-based methods are often less accurate.To address this problem, two-stage self-training methods have been proposed. In the first stage, pseudo labels are generated by MIL-based methods, and in the second stage, these pseudo labels are used to refine discriminative representations. However, existing methods have two main limitations. First, the ranking loss used in the pseudo label generator does not consider the completeness of abnormal events, as MIL is designed to detect only the most likely one. Second, the uncertainty of generated pseudo labels is not taken into account in the second stage, which can hinder the performance of the final classifier.In this paper, we propose an approach to enhance pseudo labels by exploiting completeness and uncertainty properties. Specifically, we introduce a multi-head module to generate pseudo labels, where each head serves as a classifier, and we employ a diversity loss to ensure different abnormal events are covered. To address the uncertainty issue, we design an iterative uncertainty-based training strategy using Monte Carlo Dropout. Clips with lower uncertainty are used to train the final classifier, gradually improving the quality of pseudo labels.The contributions of this paper are as follows: 1) a multi-head classifier scheme and diversity loss to cover as many abnormal clips as possible, 2) an iterative uncertainty aware self-training strategy to improve the quality of pseudo labels, and 3) experimental results on UCF-Crime, TAD, and XD-Violence datasets demonstrating the favorable performance compared to state-of-the-art methods.