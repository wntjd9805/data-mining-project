A key task in computer vision is understanding how humans interact with the world. This understanding can be applied to various fields such as sports training and digital entertainment. Previous methods for capturing and reproducing human interactions were time-consuming and prone to occlusions or lack of textures. Recent advances in neural rendering have shown promise in generating realistic views of dynamic scenes. However, there is a lack of dense-view human-object datasets, limiting research in neural human-object rendering. Existing neural techniques also suffer from training difficulties due to human-object occlusion. In this paper, we propose NeuralDome, a neural pipeline for accurate 3D modeling and rendering of complex human-object interactions. We introduce a new dataset called HODome, consisting of 274 sequences of human-object interactions recorded using a dome with 76 RGB cameras. We utilize a layer-wise neural modeling approach to capture the geometry of dynamic humans and objects, as well as render free-view sequences. Our approach effectively addresses occlusion and exploits temporal information. We also introduce object-aware ray sampling and geometry regularization techniques. Our contributions include the NeuralDome pipeline, the HODome dataset, and demonstrations of training networks for visual inference tasks related to human-object interactions.