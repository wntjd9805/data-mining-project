Acquiring accurate 3D scene geometry and appearance from 2D images has been a longstanding problem in computer vision. Traditional approaches represent the scene using explicit 3D representations such as point clouds, triangular meshes, or volumetric grids. Recent advancements in neural networks, specifically Neural Radiance Fields (NeRF), have enabled photo-realistic 3D reconstruction and novel view image synthesis. However, limited work has been done to address the challenge of training NeRF with motion blurred images, which are a common artifact in practical applications. Motion blur violates NeRF's assumption of infinitesimal exposure time and makes it difficult to obtain accurate camera poses. In this paper, we propose a solution to address these challenges by integrating the physical image formation process of motion blurred images into the training of NeRF. We use a linear motion model in the SE(3) space to represent camera motion trajectory and synthesize blurred images by averaging virtual sharp images along the motion trajectory. We call this modified model BAD-NeRF. We evaluated BAD-NeRF using both synthetic and real datasets and demonstrated superior performance compared to previous state-of-the-art methods. Our contributions include a photometric bundle adjustment formulation for motion blurred images, the ability to acquire high-quality 3D scene representation from motion blurred images, and the capability to deblur severe motion blurred images and synthesize high-quality novel view images.