Estimating the position and orientation of LiDAR from point clouds is crucial for various applications in computer vision, such as autonomous driving and virtual reality. Existing LiDAR-based localization methods either rely on pre-built 3D maps or pose regression without maps. The former requires expensive storage and communication, while the latter often lacks detailed scene geometry information and suffers from errors in the localization data. To address these limitations, this paper proposes a novel framework called SGLoc. SGLoc decouples the localization process into point cloud correspondences regression and pose estimation, effectively capturing scene geometry and improving performance. It introduces the Tri-Scale Spatial Feature Aggregation (TSFA) module and Inter-Geometric Consistency Constraint (IGCC) loss to improve scene geometry encoding. Additionally, a generalized Pose Quality Evaluation and Enhancement (PQEE) method is proposed to measure and correct pose errors in the localization data. Extensive experiments on Oxford Radar RobotCar and NCLT datasets demonstrate that SGLoc outperforms state-of-the-art methods in terms of position accuracy. Furthermore, it achieves sub-meter level error reduction in certain trajectories, which is a significant improvement. Our contributions include the decoupling of LiDAR localization, the TSFA module, the IGCC loss, and the PQEE method. Overall, SGLoc provides a comprehensive and effective solution for LiDAR localization, advancing the field of computer vision.