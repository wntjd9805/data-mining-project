Deep neural networks (DNNs) have demonstrated exceptional performance in various applications. However, they are susceptible to adversarial examples, which are malicious inputs that can deceive DNNs by including imperceptible modifications. Adversarial training has been proposed as an effective approach to enhance adversarial robustness by incorporating adversarial examples into the training data. Nevertheless, adversarial training often exhibits a significant gap between training and testing performance, and its robustness fails to generalize to unseen adversaries. To address this challenge, extensive research has been conducted to explore the characteristics of adversarially robust generalization and propose mitigation strategies. However, little is known about the role of architectural design in adversarially robust generalization.In this paper, we present a comprehensive investigation into the relationship between architectural design and adversarially robust generalization. We evaluate 20 adversarially trained architectures, including Vision Transformers (e.g., ViT, PVT, Swin Transformer) and representative CNNs (e.g., ResNet, VGG), on ImageNette and CIFAR-10 datasets against multiple adversarial attacks. Our experiments demonstrate that Vision Transformers generally exhibit better adversarially robust generalization compared to CNNs, although CNNs tend to overfit on specific attacks and fail to generalize to multiple adversaries. To understand the underlying mechanism, we conduct theoretical analysis using the concept of Rademacher complexity. Our theoretical analysis reveals that higher weight sparsity, which can be achieved through specially-designed attention blocks, significantly contributes to the better adversarially robust generalization of Transformers.Furthermore, we provide detailed analyses of generalizability from various perspectives, such as generalization on common corruptions and larger dataset ImageNet. We also discuss potential directions for improving architecture robustness. We make our adversarially-trained model zoo weights publicly available to assist researchers in building more robust DNN architectures in the future.1. We systematically study 20 adversarially-trained architectures and establish the close relationship between architectural design and robust generalization.2. We theoretically reveal that higher weight sparsity attained through attention blocks contributes to the improved adversarially robust generalization of Transformers.3. We provide detailed analyses of generalizability and propose potential pathways for enhancing architecture robustness.