Recent research in Birdâ€™s Eye-View (BEV) perception and multi-sensor fusion has made significant progress in autonomous driving. However, existing work still has limitations in fusion strategies and sensor support, hindering the development of a more general and flexible multi-sensor architecture for BEV 3D perception. This paper proposes BEVGuide, a comprehensive and versatile multi-modality fusion architecture designed for BEV perception. It emphasizes the integration of Radar sensors, which have been less studied compared to cameras and Lidar sensors. BEVGuide accommodates any sensor configuration and utilizes a BEV-guided sensor-agnostic attention module to fuse sensor features. It avoids explicit space transformations and focuses on positions of interest across sensors. Experimental results demonstrate that BEVGuide achieves state-of-the-art performance in various sensor configurations for BEV scene segmentation and velocity estimation tasks, proving its compatibility with other BEV perception tasks. This paper contributes to the development of a more advanced and robust multi-sensor architecture for autonomous driving.