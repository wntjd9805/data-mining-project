The pose-guided person image synthesis task aims to generate a person's image with a desired pose and appearance, which is important for various applications such as e-commerce, virtual reality, and content generation. Existing approaches using Generative Adversarial Networks (GAN) or Variational Autoencoders (VAE) face challenges in preserving coherent structure, appearance, and body composition, resulting in deformed textures and unrealistic body shapes. In this work, we propose a diffusion-based approach called PIDM, which breaks down the problem into a series of forward-backward diffusion steps to generate photorealistic outputs that closely adhere to the given pose and appearance information. PIDM can model the interplay between pose and appearance, offers higher diversity, and avoids texture deformations. Moreover, it does not require detailed annotations like parser maps or dense 3D correspondences. Our contributions include the development of the first diffusion-based approach for pose-guided person synthesis, a texture diffusion module to effectively model the interplay between appearance and pose, the introduction of disentangled classiﬁer-free guidance in the sampling procedure, and achieving new state-of-the-art results on benchmark datasets. Additionally, we demonstrate the potential of synthesized images in improving performance in downstream tasks such as person re-identiﬁcation.