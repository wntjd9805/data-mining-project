Novel view synthesis is a challenging problem in computer vision and computer graphics. Recent advancements in neural rendering techniques, such as NeRFs, have made significant progress in this area. NeRFs represent a static 3D scene as a radiance field parametrized by a neural network, allowing for rendering at novel views. However, most existing approaches require multiple matched views with calibrated camera poses, which is not always available. Some attempts have been made to relax this constraint using unsupervised training, but they still have limitations in generalizability. This paper proposes a single-image NeRF synthesis framework without 3D supervision by leveraging large-scale diffusion-based 2D image generation models. The framework utilizes a two-section semantic guidance to narrow down the general prior knowledge and ensure semantic and visual coherence between different views. A geometric regularization term is introduced to improve the underlying 3D structure. Experimental results demonstrate the effectiveness of the proposed approach in generating high-quality novel views from diverse in-the-wild images. The key contributions of this paper include formulating single-view reconstruction as a conditioned 3D generation problem, designing a two-section semantic guidance, introducing a geometric regularization term, and validating the results on various datasets.