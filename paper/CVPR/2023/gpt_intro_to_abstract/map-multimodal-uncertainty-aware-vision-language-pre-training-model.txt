The precise understanding of multimodal representations is a challenging task in artificial intelligence. Current methods face the challenge of uncertainty within and between modalities, leading to limited understanding ability and poor prediction diversity. To address this, we propose a new approach that models uncertainty using a Probability Distribution Encoder (PDE) to capture the inner connection between features. We also incorporate the self-attention mechanism to enhance interaction between text and image representations. Our approach enables diverse generations and provides multiple reasonable predictions. We integrate this uncertainty modeling into a pre-training framework, resulting in three new tasks: Distribution-based Vision-Language Contrastive learning (D-VLC), Distribution-based Masked Language Modeling (D-MLM), and Distribution-based Image-Text Matching (D-ITM). These tasks aim to handle cross-modality alignment at different levels of granularity. Our contributions include the development of the PDE module, the three uncertainty-aware pre-training tasks, and an end-to-end Multimodal uncertainty-Aware vision-language Pre-training model (MAP). Experimental results demonstrate that MAP achieves state-of-the-art performance. The code for our approach is available at the provided URL.