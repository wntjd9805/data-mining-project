Recently, transformers have become the standard pattern for natural language processing (NLP) tasks due to their efficacy in modelling long-range relationships. This success has led to a surge of research applying transformers to computer vision tasks such as image classification, object detection, semantic segmentation, and video understanding. However, these methods typically only apply transformers to a specific domain.In this paper, we explore the possibility of using transformers to simultaneously handle multiple vision tasks under different dataset domains. While some previous works have investigated the usage of transformers for multiple input modalities or different tasks under multiple domains, they often rely on diverse encoders or separate parameters for each domain, which can be inefficient in terms of storage deployment.To address this issue, we propose MDL-NAS, a unified framework based on vision transformers that allows multiple vision tasks under heterogeneous dataset domains to share parameters while maintaining promising performance. We first conduct experiments to observe the impact of treating different components of vision transformers as task-specific parameters. Our results show that certain task-specific parameters can improve performance for classification and detection tasks, while all parameters shared achieve better performance for semantic segmentation.Based on these observations, we design a coarse-to-fine search space to find optimal architectures for diverse tasks and provide fine-grained parameter sharing for all tasks. We introduce sequential sharing and mask sharing policies that allow customization of the parameter share ratio and flexibility for different tasks to share parameters with various proportions and channels. To address efficiency concerns, we leverage weight entanglement training to train MDL-NAS.During the search stage, we propose a joint-subnet search algorithm that finds the optimal architecture and sharing parameters for each task under resource constraints. The searched models outperform baselines and are comparable to state-of-the-art methods trained individually for specific tasks. We also demonstrate that MDL-NAS allows incremental learning and avoids catastrophic forgetting when generalizing to new tasks.In summary, our contributions include proposing MDL-NAS for optimizing multiple vision tasks under multiple dataset domains, constructing a coarse-to-fine search space, and introducing sequential and mask sharing policies. We also demonstrate the parameter efficiency and scalability of MDL-NAS with the number of tasks increasing.