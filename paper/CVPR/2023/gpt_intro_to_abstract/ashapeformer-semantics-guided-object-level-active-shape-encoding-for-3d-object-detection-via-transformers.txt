3D object detection is an important task in scene understanding that aims to detect 3D bounding boxes and semantic categories in 3D point cloud scenes. While significant progress has been made in 2D object detection, applying these techniques to 3D object detection is challenging due to the sparse and irregular nature of point cloud data. Early attempts at 3D object detection involved mapping point clouds to 3D voxels, but this resulted in a loss of fine-grained information. Recent methods directly process the point clouds with deep neural models and have achieved remarkable performance. However, these methods still face challenges such as the lack of object-level features encoding in candidate points and the influence of background points on predictions. To address these challenges, we propose a novel neural module called AShapeFormer, which can be easily integrated with existing 3D object detection methods to improve performance. AShapeFormer utilizes multi-head attention to encode object shape information and incorporates semantic guidance to sample more foreground points and assign different weights to their features. Experimental results demonstrate that AShapeFormer significantly boosts the performance of point-based and Transformer-based 3D object detection methods on challenging datasets. Our contributions include the introduction of AShapeFormer module, which combines multi-head attention and semantic guidance for robust classification and accurate bounding box regression, and the demonstration of considerable mAP improvements on SUN RGB-D and ScanNet V2 datasets.