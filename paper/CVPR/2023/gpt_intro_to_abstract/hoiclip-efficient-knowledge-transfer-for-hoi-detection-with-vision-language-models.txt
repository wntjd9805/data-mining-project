Abstract:Human-Object Interaction (HOI) detection plays a crucial role in understanding visual scenes and has various applications in fields such as assistive robotics, visual surveillance, and video analysis. While recent research on localizing human-object instances has made progress, identifying interaction classes between human-object pairs remains challenging. This paper introduces HOICLIP, a novel strategy that transfers knowledge from the Contrastive Vision-Language Pre-training (CLIP) model to the HOI detection task. Unlike conventional approaches that rely on knowledge distillation, HOICLIP directly retrieves knowledge from CLIP and mines prior knowledge from multiple aspects. It addresses the long-tail and zero-shot learning problems in verb recognition and improves data efficiency in HOI representation learning. HOICLIP achieves competitive performance in fully-supervised, zero-shot, and data-efficient settings, outperforming previous state-of-the-art methods in the zero-shot setting and improving data efficiency significantly. The contributions of this work include utilizing query-based knowledge retrieval for efficient knowledge transfer, leveraging regional visual features and verb representations for more expressive HOI representation, and exploiting zero-shot CLIP knowledge for improved performance.