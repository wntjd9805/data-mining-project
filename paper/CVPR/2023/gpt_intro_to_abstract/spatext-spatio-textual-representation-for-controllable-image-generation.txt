This paper introduces a new problem setting in image generation called image generation with free-form textual scene control. The current state-of-the-art text-to-image models have limitations in achieving precise control over the layout and specific characteristics of objects in the generated image. To address this, the authors propose a novel approach that uses spatial free-form text to represent each segment in a sparse map, allowing users to specify the desired local text description, position, and shape of objects. The authors also propose a CLIP-based spatio-textual representation and demonstrate its effectiveness on two types of text-to-image diffusion models. They extend classifier-free guidance to support multi-conditional inputs and propose several automatic evaluation metrics for comparing their method against baselines. The proposed method is evaluated through user studies and achieves state-of-the-art results. Overall, this paper contributes to advancing the controllability and expressivity of text-to-image models in generating images based on specific user instructions.