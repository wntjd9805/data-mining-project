The development of deep convolutional neural networks (DCNNs) has shown remarkable success in various recognition tasks, but their robustness still lags behind humans. Deep models trained on high-quality data often perform poorly when faced with low-quality data during deployment, which frequently includes common corruptions like blur, noise, and weather influences. This paper aims to improve the robustness of DCNNs in the presence of low-quality images. While previous methods have relied on augmented training with simulated low-quality images or aligning degraded features with high-quality ones, these approaches have limitations in mapping low-quality features to high-quality ones and may include irrelevant features affected by image quality. Inspired by vector quantization (VQ) in sparse representation, we propose incorporating VQ into the recognition model to bridge the gap between low- and high-quality features and learn a quality-independent representation. Our method quantizes the features using a codebook and concatenates the quantized features with the original ones, enhancing the quality-independent representation using a self-attention module. Experimental results demonstrate that our method achieves higher accuracy on benchmark low-quality datasets compared to current state-of-the-art methods.