This paper introduces Imagic, a semantic image editing method that can perform sophisticated non-rigid edits on real high-resolution images using only an input image and a single text prompt describing the desired edit. Unlike existing methods, Imagic is not limited to specific sets of edits or domains and does not require additional auxiliary inputs. It utilizes text-to-image diffusion models to generate images that align well with the target text and applies a simple 3-step process to optimize the text embedding, fine-tune the generative model, and interpolate between the target text embedding and optimized one. Experimental results demonstrate the generality, versatility, and high quality of Imagic compared to other methods, and a human perceptual evaluation study confirms its superior editing quality and faithfulness to the original image. The paper also presents TEdBench, a new complex image editing benchmark for comparing text-based image editing methods. The contributions of this work include the development of Imagic, the demonstration of compositional capabilities of text-to-image diffusion models, and the introduction of the TEdBench benchmark.