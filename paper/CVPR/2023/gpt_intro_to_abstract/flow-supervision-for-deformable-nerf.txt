Reconstructing dynamic scenes from monocular videos is a challenging task due to the lack of epipolar constraints and ambiguities between motion and structure. Recent advancements in differentiable rendering have led to solutions using an analysis-by-synthesis strategy. However, current implementations, such as deformable neural radiance fields, have limitations. These methods lack temporal regularization and only minimize photometric error, resulting in poor performance for videos with rapid object motions. Optical flow has been explored as a cue to supervise temporal transitions in other motion representations, but enforcing flow constraints using a generic backward warping field is non-trivial. In this paper, we propose a method to use optical flow supervision for deformable NeRF. We derive an analytical solution to compute velocities of objects directly from the backward warping field, which allows us to compute scene flows through temporal integration. Our approach significantly improves novel view synthesis for monocular videos with rapid object motions. Our method is applicable to all kinds of backward warping functions and is computationally more tractable compared to other approaches.