Compressed sensing (CS) is an imaging technique that captures an image using a limited number of measurements. One of the main challenges in CS is reconstructing an image from these limited measurements. Supervised learning methods for CS reconstruction (CSR) have been proposed, but they face limitations such as the need for a large number of ground truth (GT) images and potential bias in the training data. To address these limitations, recent works have focused on GT-free external learning and self-supervised internal learning techniques. However, there is still a performance gap between these methods and existing supervised approaches. In this paper, we propose a GT-free joint external and internal learning method for CSR. Our method combines the advantages of external learning, such as generalizability, with the efficiency of internal learning when processing multiple test samples. We introduce a GT-free meta-learning approach that trains a deep neural network (NN) model using only measurement data. To achieve accurate predictions, we propose a SURE-motivated self-supervised loss function called iSURE, which provides an unbiased estimate of the mean squared error. We integrate model-agnostic meta-learning (MAML) into our training scheme and address the solution ambiguity caused by non-empty null space using an ensemble-based sub-process. Additionally, we adopt an additional loss on prediction consistency in the null space to reduce solution ambiguity. To accelerate the adaptation process, we propose a bias-tuning scheme for unrolling CNNs. Experimental results demonstrate that our method outperforms GT-free learning methods and competes with supervised methods while being faster than internal approaches.