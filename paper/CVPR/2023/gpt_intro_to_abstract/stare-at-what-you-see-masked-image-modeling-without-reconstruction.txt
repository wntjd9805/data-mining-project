Vision Transformers have shown great potential in computer vision in recent years. Masked Image Modeling (MIM) has been successful in natural language processing, and there is interest in applying it to computer vision. Existing MIM methods focus on reconstructing masked image regions, either through inpainting-style or decoder-style approaches. However, this reconstruction process requires redundant computation and decreases training efficiency. We propose a more efficient MIM paradigm called MaskAlign, which aligns visible features from the student model with intact features from the teacher model, without the need for reconstruction. We use multi-level teacher model features as supervision and enhance the student's features with a Dynamic Alignment (DA) module. Experimental results show that MaskAlign outperforms existing MIM methods in terms of performance and efficiency. Our work contributes to the understanding of MIM paradigms, introduces a novel approach, and validates the effectiveness of the DA module. Our model achieves state-of-the-art results with higher efficiency in pre-training time.