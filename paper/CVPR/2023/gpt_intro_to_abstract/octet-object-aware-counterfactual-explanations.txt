Deep learning models, such as those used in autonomous driving systems, are increasingly being deployed in safety-critical applications. However, the black-box nature of these models raises concerns about their trustworthiness. To address this issue, explainability methods, such as counterfactual explanations, have been developed. These explanations provide insights into the decisions made by a model. Counterfactual explanations are data points that differ minimally from the original query but result in a different decision by the model. By analyzing the differences between the query and the counterfactual explanation, users can identify the essential factors influencing the model's decision. Most existing counterfactual methods have focused on explaining classifiers trained on single-object images, such as face portraits. Explaining decision models trained on complex scenes composed of multiple objects, like those in autonomous driving, presents unique challenges. Decisions in such scenarios are often multi-factorial, requiring consideration of factors such as the position of road users, road layout, traffic lights, and visibility. In this paper, we propose a new framework called OCTET (Object-aware CounTerfactual ExplanaTions) to generate counterfactual examples for autonomous driving. OCTET leverages advances in unsupervised compositional generative modeling to provide a flexible explanation method. By assessing the contribution of each object in the scene independently, OCTET can identify explanations related to object positions, styles, or combinations thereof.We validate our framework through extensive experiments using self-driving action decision models trained on the BDD100k dataset and its BDD-OIA extension. Our experiments demonstrate that OCTET can generate counterfactual explanations by manipulating objects in the scene, such as moving parked cars or adding road markings. These explanations provide insights into the factors influencing the model's decision.Our contributions include tackling the challenge of building counterfactual explanations for visual decision models operating on complex scenes, specifically targeting autonomous driving scenarios. Our method also enables users to investigate the role of specific objects in a model's decision, providing them with control over the type of explanation they seek. We evaluate the realism of our counterfactual images, the minimality and meaningfulness of changes, and compare against previous reference strategies. Furthermore, we demonstrate the versatility of our method by addressing explanations for a segmentation network. Finally, we conduct a user-centered study to assess the usefulness of our explanations in a practical case, as standard evaluation benchmarks for counterfactual explanations often lack a concrete measure of interpretability.