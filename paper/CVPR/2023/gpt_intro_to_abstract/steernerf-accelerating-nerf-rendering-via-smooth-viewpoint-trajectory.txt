Novel View Synthesis (NVS) is a prominent problem in computer vision and computer graphics with various applications such as navigation, telepresence, and free-viewpoint video. Currently, Neural Radiance Fields (NeRF) have gained popularity as a representation for NVS due to their ability to render high-quality images from novel viewpoints. However, NeRF is slow at rendering, hindering interactive view synthesis. Many recent approaches have focused on improving NeRF's rendering speed, but there is a trade-off between speed and memory cost. Existing acceleration methods sacrifice memory consumption for fast rendering, whereas low-memory approaches have not reached the performance of cache-based methods. To address this, we propose a new perspective on accelerating NeRF rendering by exploiting the smooth and continuous trajectory of the viewpoint. Unlike existing methods, which focus on reducing the rendering time for each viewpoint individually, our approach leverages information overlap between consecutive viewpoints to speed up rendering. We introduce SteerNeRF, a framework that utilizes efficient 2D neural rendering and low-resolution volume rendering to reduce the number of sample points while maintaining image fidelity. Our method achieves fast rendering with high fidelity and temporal consistency at a low memory cost. We conduct experiments on both synthetic and real-world datasets, demonstrating that our approach achieves high rendering speeds while narrowing the gap between low-memory and cache-based methods. Overall, our contributions include a new perspective on NeRF rendering acceleration, a framework for combining low-resolution volume rendering and high-resolution neural rendering, and experimental results showcasing the performance of our approach.