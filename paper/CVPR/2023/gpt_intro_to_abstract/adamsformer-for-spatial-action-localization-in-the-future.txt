Human action understanding plays a crucial role in computer vision applications like virtual reality/augmented reality, robotics, and autonomous vehicles. While significant progress has been made in human action perception, there is a growing need for predicting and localizing future actions. This is particularly important for applications like effective collaboration in human-robot interaction. In this work, we introduce Spatial Action Localization in the Future (SALF), a novel task that focuses on predicting spatial locations and categorizing actions in both long-term future and past observations. We propose AdamsFormer, a network based on the concept of NeuralODE, to address the challenges of SALF. AdamsFormer leverages the multi-step Adams method to extrapolate latent features from observed frames and predict future action locations. Extensive experiments on action video datasets demonstrate the efficacy and superiority of AdamsFormer over existing methods for the SALF task. Our contributions include the introduction of SALF as a novel task, the proposal of AdamsFormer architecture, and the demonstration of its superior performance in long-range feature modeling for predicting and localizing future actions.