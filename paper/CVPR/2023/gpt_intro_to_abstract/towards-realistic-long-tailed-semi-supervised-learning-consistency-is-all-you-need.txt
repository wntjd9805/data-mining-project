Semi-supervised learning (SSL) has proven to be effective in improving the generalization of deep neural networks when only limited labeled data is available. Most SSL algorithms generate pseudo-labels for unlabeled data to train models, assuming balanced class distributions in both labeled and unlabeled datasets. However, many real-world tasks have long-tailed class distributions, leading to biased classifiers and low accuracy on minority classes. Several long-tailed semi-supervised learning (LTSSL) algorithms have been proposed to address this issue, but they assume similar class distributions between labeled and unlabeled data. This assumption is often violated in real-world applications, leading to poor performance. In this paper, we study the LTSSL problem of learning from unlabeled data with unknown class distributions. We propose a new algorithm called Adaptive Consistency Regularizer (ACR) that effectively uses unlabeled data by refining pseudo-labels to match the true class distribution. ACR significantly improves the quality of pseudo-labels and achieves better performance compared to recent LTSSL algorithms under various class distributions. Our approach shows promise in alleviating the class imbalance problem and improving the accuracy of minority classes.