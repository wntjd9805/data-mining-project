Pre-training vision-language models for image-text retrieval has gained significant attention in recent years. Previous methods have utilized a "dual-encoder" architecture, but they lack the ability to capture detailed image and text semantics and associations between them. Inspired by human perception and semantic association capabilities, the authors propose a new proxy task called Visual-Language Error Modeling (ViLEM). ViLEM enhances fine-grained semantic perception and establishes detailed image-text association by "proofreading" plausible negative texts against image information. This is achieved through text error detection and correction sub-tasks, which are automatically constructed using a pre-trained language model. To further enhance semantic associations, a multi-granularity interaction framework is proposed, leveraging both global and local visual features for text error detection and correction. Experimental results demonstrate the superiority of the proposed method over previous state-of-the-art approaches in terms of image-text retrieval performance and discriminativeness to local text semantics. The proposed model also shows good generalization to video-text retrieval.