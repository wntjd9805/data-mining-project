This paper introduces a novel approach to capturing and synthesizing 3D human motion in highly contextual environments using virtual reality (VR) simulations. Traditional motion capture techniques require physical staging and props, limiting their ability to capture realistic human activities in real-world settings. The proposed method eliminates the need for physical construction by capturing human motion within VR simulations, allowing for the recording of motion in cluttered spaces and the simultaneous capture of associated first-person perspectives. This approach offers several advantages, including simplified and cost-effective contextual scene creation, access to the state of the virtual world from the simulator, elimination of occlusions in the capture space, and the generation of 3D human motions and corresponding videos from flexible camera views. The authors demonstrate the capabilities of their method by collecting a dataset called CIRCLE, containing ten hours of full-body reaching motion in various indoor household scenes, and training a model to generate scene-aware reaching motions based on the starting pose and target location. The paper also proposes two different methods to encode scene information and compares their performance against baselines. Overall, the contributions of this work include the development of a novel motion acquisition system, the creation of the CIRCLE dataset, and the training of a data-driven model for generating full-body reaching motion in contextual environments.