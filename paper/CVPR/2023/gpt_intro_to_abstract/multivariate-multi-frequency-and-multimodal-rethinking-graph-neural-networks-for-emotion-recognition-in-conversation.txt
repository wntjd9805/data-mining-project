In this paper, we discuss the challenge of Emotion Recognition in Conversation (ERC) and the complexity of multivariate relationships among multiple modalities and conversational context. We present an example of a multimodal dialogue to illustrate this complexity. Existing ERC models tend to overlook the multivariate relationships, and Graph Neural Networks (GNNs) have shown promise in addressing this challenge. However, GNNs still have limitations in capturing complex multivariate relationships and high-frequency information. To address these limitations, we propose a Multivariate Multi-frequency Multimodal Graph Neural Network (M3Net) that aims to capture sufficient multivariate relationships and leverage multi-frequency information. M3Net consists of two parallel components: multivariate propagation and multi-frequency propagation. We evaluate the effectiveness of M3Net on two multimodal ERC datasets and show that it outperforms previous state-of-the-art methods.