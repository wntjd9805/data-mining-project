Class-incremental learning (CIL) and few-shot class incremental learning (FSCIL) aim to continuously absorb new category knowledge in a phased manner, allowing for scattered and non-identical data. However, FSCIL introduces the constraint of limited data availability, making incremental learning more challenging. The major challenge in FSCIL is catastrophic forgetting, where the performance on old tasks significantly decreases after learning new tasks. This is caused by the lack of training data for old tasks, leading models to focus only on new tasks. To address this, conventional CIL approaches such as bias correction-based, regularization-based, and replay-based methods have been proposed. However, these approaches become obsolete in the few-shot constraint setting of FSCIL. Recent works have explored FSCIL techniques inspired by existing CIL variants or few-shot learning angles, but there is still room for improvement. In this paper, we propose a Gaussian kernel embedded analytic learning (GKEAL) approach for FSCIL, inspired by analytic learning techniques that convert network training into linear problems. We introduce GKEAL as a recursive learning problem to avoid forgetting and prove that it follows the same weight-invariant property as analytic learning. To bridge analytic learning into the FSCIL realm, we replace the classifier at the network's final layer with a kernel analytic module (KAM) that incorporates a Gaussian kernel embedding process and least squares (LS) solution. We also introduce an augmented feature concatenation (AFC) module to balance the network's preference between base and new tasks. Experimental results on benchmark datasets demonstrate that GKEAL outperforms state-of-the-art methods, and ablation studies provide analysis of hyperparameters and support for theoretical claims.