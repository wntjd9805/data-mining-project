Automated radiology report generation is an important task in clinical practice to alleviate the manual workload of radiologists while ensuring high-quality healthcare. Previous studies have focused on single-expert based diagnostic captioning methods using an encoder-decoder architecture. However, these methods face challenges in accurately identifying disease anomalies in radiology images and are limited to the expertise of a single specialist. In this paper, we propose a new diagnostic captioning framework called METransformer that simulates the multi-specialist consultation scenario. METransformer utilizes multiple "expert tokens" in a transformer backbone to capture reliable and complementary visual information. These expert tokens learn to attend distinct image regions and interact with each other to produce a diagnosis report in parallel. The optimal report is selected through an expert voting strategy. Our experimental results show that utilizing multiple experts improves the model's ability to capture visual patterns of clinical importance. The model incorporates expert tokens and visual tokens into an expert transformer encoder, which employs a vision transformer encoder and a bilinear transformer encoder. The expert tokens attend differently to different image regions and capture fine-grained image information. In the decoder, the expert tokens guide the learning of word and visual token embeddings in the report generation process, resulting in multiple candidate reports. A metric-based expert voting strategy is employed to generate the final report. Our model combines the benefits of ensemble-based approaches while being computationally more efficient and supporting more sophisticated interactions among experts. It achieves better performance than common ensemble approaches, as demonstrated in our experimental study. Our contributions lie in the development of METransformer as a conceptually "multi-expert joint diagnosis" framework, the reduction of training parameters and improvement of training efficiency, and the promising performance on benchmark datasets IU-Xray and MIMIC-CXR.