In this paper, we introduce a novel framework called PolyFormer for referring image segmentation (RIS) and instance segmentation. RIS aims to localize the segmentation mask of an object given a natural language query, combining vision-language understanding and instance segmentation. The conventional pipeline involves feature extraction from image and text inputs, followed by fusion of multi-modal features to predict the mask. However, most instance segmentation models rely on a dense binary classification network, neglecting the structure among output predictions. We propose a sequence-to-sequence (seq2seq) framework, PolyFormer, that takes image patches and text query tokens as input and outputs a sequence of polygon vertices. Each vertex prediction is conditioned on all preceding predicted vertices, enabling dependencies among predictions. The seq2seq framework also allows for the output to be a sequence of multiple polygons separated by separator tokens, covering scenarios where the segmentation masks are not connected. PolyFormer formulates localization as a regression task, eliminating the quantization error introduced by discrete bin classification. We evaluate PolyFormer on three major referring image segmentation benchmarks and achieve significant improvements in performance compared to the state of the art. Our main contributions include the introduction of PolyFormer as a flexible framework for RIS and instance segmentation, the proposal of a regression-based decoder for accurate coordinate prediction, and the demonstration of the superiority of polygon-based methods over mask-based ones.