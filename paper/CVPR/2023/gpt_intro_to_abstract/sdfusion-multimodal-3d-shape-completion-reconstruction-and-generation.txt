3D asset creation is a crucial aspect of immersive augmented/virtual reality experiences. However, the manual creation and editing of 3D assets can be difficult and inaccessible for inexperienced users. In this paper, we aim to democratize the 3D content creation process by introducing SDFusion, a diffusion-based generative model with a signed distance function (SDF) as its 3D representation. SDFs are known for effectively representing high-resolution shapes with arbitrary topology. To overcome the computational limitations of 3D representations, we utilize an auto-encoder to compress 3D shapes into a more compact low-dimensional representation, allowing for higher resolutions. We leverage diffusion models to learn the probability distribution over the latent space, enabling flexible conditioning with multiple modalities. Compared to existing models, SDFusion provides superior sample quality, increased flexibility in handling multiple conditions, and reduced memory usage. We demonstrate the effectiveness of SDFusion in shape completion, 3D reconstruction from images, and text-to-shape tasks, outperforming prior work. Additionally, we showcase the ability to texture 3D shapes given textual descriptions through an interplay between 2D and 3D generative models. Our contributions include the proposal of SDFusion, its conditional generation capabilities with multiple modalities, and the synthesis of textured 3D objects using a pipeline that combines 2D and 3D generative models.