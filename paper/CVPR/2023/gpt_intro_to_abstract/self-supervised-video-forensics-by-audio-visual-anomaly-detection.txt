Supervised learning is widely used in image and video forensics, but it is limited by the lack of comprehensive labeled datasets that cover all possible manipulations. This paper presents a novel approach to detect manipulated videos through anomaly detection using audio-visual features. The proposed model is trained on large amounts of real, unlabeled videos to learn the temporal co-occurrence of audio and visual data. During testing, the model flags videos with low probabilities, indicating inconsistencies between their video and audio streams. The problem is posed as detecting anomalies in synchronization features, which convey the temporal alignment between vision and sound. Various feature sets are evaluated, extracted from a model trained to align audio and visual streams in a video. No manipulated examples are required for training, and the model does not need to have seen the speakers in the test set during training. The model is evaluated on lip-synced and audio-driven face reenactment videos, including those manipulated by faceswap techniques. Strong performance is achieved on the FakeAVCeleb and KoDF datasets, demonstrating generalization to other languages and robustness to postprocessing operations. The experiments validate that video forensics can be framed as an audio-visual anomaly detection problem, synchronization features are informative about video manipulations, and the proposed model can successfully detect fake videos using only real examples for training.