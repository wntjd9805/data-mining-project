Abstract: 3D human pose estimation is a challenging task with applications in augmented reality, sign language translation, and human-robot interaction. The mainstream approach involves two stages: obtaining 2D pose and then lifting to 3D. However, accurately predicting 3D pose from monocular data is difficult due to depth ambiguity and potential occlusion. In this paper, we propose DiffPose, a diffusion-based framework for 3D pose estimation. DiffPose models the estimation procedure as a reverse diffusion process, progressively transforming a high-uncertainty 3D pose distribution to a low-uncertainty distribution. Inspired by diffusion models, we use intermediate distributions to guide model training. We introduce novel designs, including initialization of uncertainty distribution, a Gaussian Mixture Model (GMM)-based forward diffusion process, and a context-based reverse diffusion process. Experimental results demonstrate that DiffPose achieves state-of-the-art performance on human pose estimation benchmarks.