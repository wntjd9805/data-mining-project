The synthesis of photo-realistic novel view images of complex three-dimensional (3D) scenes has made significant progress with the emergence of Neural Radiance Field (NeRF). However, most existing NeRF variants are designed and tested for controlled environments with well-captured images, while real-world scenarios often involve various forms of noise that complicate geometric and appearance consistency in 3D representation. While some NeRF variants have addressed noise factors like exposure, motion, illumination changes, and aliasing, the issue of blur has not been sufficiently addressed yet.In this paper, we propose a deblurred NeRF called DP-NeRF, based on two physical scene priors: a rigid blurring kernel (RBK) and adaptive weight proposal (AWP). The RBK utilizes explicit physical scene priors to construct a consistent 3D scene representation from blurred images, incorporating a 3D deformation field and coarse composition weights based on view information. The AWP refines the composition weights using depth features and motion feature aggregation to generate more realistic results. Coarse-to-fine optimization is also introduced to gradually increase the effect of the AWP during training.Experimental results on synthetic and real scene datasets demonstrate that DP-NeRF significantly improves the quality of novel view synthesis, preserving 3D consistency and producing clean and realistic images. The proposed model offers a novel approach to address the issue of blur in 3D scene reconstruction, making significant contributions to the field.