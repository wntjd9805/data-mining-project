Deep learning has achieved significant advancements in computer vision, but it lacks the ability for continual learning, where tasks are learned incrementally over sequential time steps. Standard gradient-based training is not effective for this problem as it results in catastrophic forgetting. Previous research in continual learning focused mainly on class incremental (CIL) settings, where tasks are subsets of disjoint classes introduced sequentially. Various strategies have been proposed to solve the CIL problem, including distillation methods, parameter regularization methods, and gradient methods. Network expansion (NE) methods address the limitations of previous approaches by freezing the model for previous tasks and adding a new subnetwork for each subsequent task. NE methods are efficient but not suitable for transformer architectures. In this paper, we propose the Dense Network Expansion (DNE) approach, which disentangles spatial and cross-task attention to overcome the limitations of NE methods. We make four contributions: 1) highlighting the unsustainability of existing NE methods, 2) proposing the DNE approach to balance accuracy and model size in CIL, 3) introducing an implementation of DNE based on individual spatial and cross-task attentions, and 4) demonstrating through extensive experiments that DNE outperforms previous CIL methods in terms of accuracy and the trade-off between accuracy and scale.