Despite recent advances in deep learning, deep neural networks often suffer from performance degradation when the source and target domains differ significantly. Test-time adaptation (TTA) has received significant attention as a practical and widely applicable solution to address domain shifts, especially in on-device settings. TTA focuses on adapting the model to unlabeled online data from the target domain without access to the source data. While existing TTA methods show improved performance, minimizing memory resource sizes has been relatively underexplored. Several studies update entire model parameters to achieve large performance improvements, which may be impractical with limited memory sizes. Additionally, some TTA approaches update only batch normalization parameters, but this is still not memory efficient enough as the memory required significantly depends on the size of intermediate activations. Furthermore, most TTA studies assume a stationary target domain, but the target domain may continuously change in the real world. This presents challenges, such as catastrophic forgetting and error accumulation, which can degrade performance on both the source and target domains. To address these challenges, we propose memory-Efficient continual Test-Time Adaptation (EcoTTA), which enhances memory efficiency and prevents catastrophic forgetting and error accumulation. Our approach uses a memory-efficient architecture consisting of frozen original networks and lightweight meta networks attached to them. We freeze the original networks during test time to discard intermediate activations, adapting only the meta networks to the target domain. We also propose a self-distilled regularization method that leverages preserved source knowledge to prevent catastrophic forgetting and error accumulation. Our approach achieves improved memory efficiency and TTA performance compared to existing methods on image classification and semantic segmentation tasks.