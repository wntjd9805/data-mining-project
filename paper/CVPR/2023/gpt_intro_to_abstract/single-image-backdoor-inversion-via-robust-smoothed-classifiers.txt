Backdoor attacks, which involve injecting covert backdoors into machine learning models for manipulation during inference, have emerged as a prominent threat in the field of machine learning security. The practice of reverse engineering backdoors from backdoored classifiers, known as backdoor inversion, has gained significant attention due to the widespread threat of backdoor attacks on various domains. In this paper, we focus on the problem of backdoor inversion and aim to recover hidden backdoors from backdoored classifiers. We introduce a novel approach called SmoothInv, which utilizes a single clean image to perform backdoor inversion. Our approach involves synthesizing backdoor patterns by inducing salient gradients of backdoor features through a robustification process. We construct a robust smoothed version of the backdoored classifier, which is resistant to adversarial perturbations, and then perform guided image synthesis to recover backdoored images that resemble the target class. Unlike existing methods that require multiple clean instances for reasonable results, our approach demonstrates successful and faithful backdoor inversion using only a single image. We evaluate the effectiveness of SmoothInv on a collection of backdoored classifiers, showcasing its ability to find both successful and visually similar backdoors from single images. We also demonstrate the robustness of our approach against attempts to circumvent it. Overall, our work contributes to the advancement of backdoor inversion methods and addresses the need for performing backdoor inversion with limited access to clean images.