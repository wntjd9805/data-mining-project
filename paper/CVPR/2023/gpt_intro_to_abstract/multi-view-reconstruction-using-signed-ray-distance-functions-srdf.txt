Reconstructing 3D shapes from 2D images has long been a challenge in computer vision, with applications in robotics, augmented reality, and human digitization. Multi-view stereo (MVS) has emerged as a powerful strategy for building 3D models by finding surface locations where 2D image observations coincide. However, this strategy has been challenged by approaches that prioritize observation fidelity using differentiable rendering. Differentiable rendering allows for comparison of rendered and observed images, optimizing the shape model. Implicit shape representations, which include occupancy, distance functions, or densities, have provided state-of-the-art methods for recovering geometry and appearance from 2D images.To improve precision and computational efficiency, we propose an approach that combines the benefits of differentiable rendering and MVS strategies. We use a volumetric signed ray distance representation, called the Signed Ray Distance Function (SRDF), which parameterizes a volumetric signed ray distance representation with depths along viewing rays. This representation explicitly defines the shape surface with depths while maintaining distributed gradients through volumetric discretization. To optimize this representation, we introduce an unsupervised differentiable volumetric criterion that does not require color estimation. Instead, the criterion evaluates whether the signed distances along rays agree at a sample when it is photo-consistent and disagree otherwise.Our proposed approach combines the benefits of MVS, such as avoiding expensive ray tracing and enforcing pixel-wise accuracy, with the advantages of differentiable rendering. The optimization can be performed over groups of cameras defined with visibility considerations, allowing for parallelism while ensuring consistency over depth maps. Furthermore, our volumetric scheme provides a consistent platform for comparing different photo-consistency priors without relying on estimated surfaces.We evaluate our approach using real and synthetic datasets, including DTU Robot Image Data Sets, Blended-MVS, and Renderpeople, as well as real human capture data. Ablation tests demonstrate the contributions of the SRDF parametrization and volumetric integration in the shape reconstruction process. Comparisons with MVS and differentiable rendering methods show that our approach consistently outperforms state-of-the-art techniques, quantitatively and qualitatively, by generating better geometric details.