Human multimodal emotion recognition (MER) aims to understand the sentiment attitude of humans from video clips, utilizing data from various modalities such as language, acoustic, and vision. MER has become a popular research topic in affective computing with applications in intelligent tutoring systems, product feedback estimation, and robotics. However, the heterogeneities among different modalities pose challenges in robust multimodal representation learning for MER. To address this, we propose a decoupled multimodal distillation (DMD) method that automatically adapts distillation directions and weights according to different examples. DMD incorporates a self-regression mechanism and a margin loss to facilitate feature decoupling and improve multimodal representation learning. Based on the decoupled feature spaces, DMD utilizes a graph distillation unit (GD-Unit) to perform crossmodal knowledge distillation more effectively. We validate the effectiveness of DMD through comprehensive experiments on public MER datasets, achieving superior or comparable results compared to state-of-the-art methods. Visualization results demonstrate the meaningful distributional patterns of the graph edges in the homogeneous and heterogeneous graph knowledge distillation.