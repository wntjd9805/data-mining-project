In this paper, we address the problem of generating realistic and detailed 3D object models in a more efficient and controlled manner. Previous research has used deep generative models to automatically generate 3D objects, but these models lack the ability to generate objects as humans would. Other approaches have used text-to-3D generation methods, but they require pre-trained models and test-time optimization, limiting their practicality.To overcome these limitations, we propose a novel framework called TAPS3D for text-guided 3D textured shape generation. Our framework consists of two modules: a pseudo caption generation module and a text-guided 3D generator training module. In the pseudo caption generation module, we use the CLIP model to retrieve relevant words from rendered images and construct candidate sentences based on these words. We select the sentences with the highest similarity scores as pseudo captions for each 3D shape sample.In the text-guided 3D generator training module, we use a text-conditioned GAN architecture based on the pretrained GET3D model. We input the pseudo captions as generator conditions and supervise the training process with high-level CLIP supervision to control the generated 3D shapes. We also introduce a low-level image regularization loss to enhance the textures and increase geometry diversity. By training only the mapping networks of the pretrained GET3D model, we ensure stability and fast training while preserving the generation quality.Our proposed model, TAPS3D, can generate high-quality and fidelity 3D shapes with strong text control without requiring paired text and 3D shape training data or test-time optimization. We contribute to the field by introducing a new 3D textured shape generative framework, a simple pseudo caption generation method, and a low-level image regularization loss. The results of our experiments demonstrate the effectiveness of our approach in generating realistic and controlled 3D shapes.