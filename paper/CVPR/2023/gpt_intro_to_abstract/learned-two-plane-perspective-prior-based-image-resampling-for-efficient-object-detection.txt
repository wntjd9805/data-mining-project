This paper focuses on the importance of visual perception in autonomous driving and decision-making for smarter cities. Real-time efficient perception is crucial for advancing these areas. However, transmitting and processing visual data on the cloud is not feasible due to the large amount of data generated by traffic cameras and commuter buses. This has led to the rise of edge architectures. However, edge devices have limited resources and real-time inference requires down-sampling images, which can significantly impact accuracy.Humans, on the other hand, utilize visual shortcuts and high-level semantics rooted in scene geometry to efficiently recognize objects. Inspired by this, the paper proposes incorporating semantic priors about scene geometry in neural networks to improve object detection. The approach involves zooming into relevant image regions guided by the geometry of the scene and considering the geometric relationship between objects' sizes in the image.Previous approaches for model efficiency, such as quantization, pruning, distillation, and runtime optimization, are mentioned but spatial and temporal sampling methods are highlighted as key for enabling efficient real-time perception. Neural warping mechanisms have been employed for image classification, regression, and detection in self-driving.The paper also discusses prior work on saliency networks for object detection and highlights the suboptimal nature of heuristics such as dataset-wide priors and object locations from previous frames. The formulation of learnable geometric priors is identified as critical for learning end-to-end trained saliency networks for detection.The proposed approach is validated through experiments on various datasets related to self-driving and traffic cameras. Results show significant improvements in performance, particularly in detecting small far-away objects. The approach also improves object tracking and can be deployed in resource-constrained edge devices for real-time sensing.