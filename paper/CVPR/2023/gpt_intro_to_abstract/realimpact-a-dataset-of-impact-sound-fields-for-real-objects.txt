Object sounds play a significant role in our daily lives, providing information about an object's properties and the environment it is in. Understanding the cause-and-effect relationships in these sounds can enhance physical simulations in virtual reality, animation, and learning-based frameworks. While previous work has focused on virtual sound generation, there has been limited research on measuring sounds produced by everyday objects. To address this gap, we introduce REALIMPACT, a dataset comprising 150k recordings of 50 objects being struck from different positions. This dataset provides comprehensive coverage of the frequency components and spatial distribution of the sounds. We demonstrate the usefulness of REALIMPACT in various audio-visual and auditory learning tasks. Additionally, we present an automated setup for collecting high-fidelity recordings and highlight the potential of REALIMPACT in reducing the sim-to-real gap for future applications. Our contributions include the design of the automated setup, the acquisition of the REALIMPACT dataset, and the evaluation of its utility through comparisons with existing sound simulation frameworks and benchmark tasks for acoustic and audio-visual learning.