The research in white-box adversarial attacks has primarily focused on classification tasks, with numerous methods proposed for ℓp-norms. However, there has been a scarcity of studies on adversarial attacks for segmentation tasks, despite the importance of semantic understanding in computer vision applications. It is crucial to evaluate the robustness of segmentation models in safety-critical areas. Designing adversarial attacks for semantic segmentation is more challenging due to the optimization problem's complexity and the computational and memory costs. In this paper, we propose an adversarial attack for deep semantic segmentation models that generates minimal perturbations using the ℓ∞-norm. We introduce adaptive strategies to handle a large number of constraints and use proximal splitting for ℓ∞-norm minimization. Our approach outperforms existing methods for both segmentation and classification attacks. We evaluate our attack on various datasets and architectures, demonstrating its effectiveness. Our attack reveals the overestimation of the robustness of segmentation models, as previous attacks could only find adversarial examples with larger norms.