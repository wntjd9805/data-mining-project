In recent years, there has been significant progress in large vision-language (VL) pre-trained models and their performance on vision tasks. These models align image and text embeddings in a shared space using image-text contrastive loss. However, fine-tuning the entire model in data-limited or label-limited settings is ineffective due to its high complexity. Prompt tuning, a parameter-efficient learning paradigm, has emerged as a solution to adapt VL models to downstream tasks. Existing prompt tuning methods rely on visual data to learn prompts, which may be limited when obtaining sufficient image data or annotations is not feasible.To address this issue, we propose Texts as Images (TaI) prompting for prompt tuning. In many pre-trained VL models, the image and text encoders encode images and texts into a shared space. We leverage this shared space and extract alternative text features, such as descriptive sentences and captions, for prompt tuning. TaI prompting has several advantages, including easy accessibility of text descriptions and direct derivation of class labels from these descriptions. Compared to prompting from images, TaI prompting is less affected by data-limited and label-limited issues.We validate the effectiveness of TaI prompting in multi-label image recognition tasks. We crawl text descriptions from public image caption datasets and object detection datasets to form the training set. By using a noun filter, we map the nouns in the text descriptions to corresponding object categories, resulting in a set of text descriptions containing target objects. To handle multi-label classification, we introduce double-grained prompt tuning (TaI-DPT), which involves global prompts for classifying sentences or images and local prompts for discriminating text tokens or image patches. Global and local prompts are tuned using a ranking loss.Experimental results demonstrate that our TaI prompting outperforms zero-shot CLIP without using any labeled images on multiple benchmarks. Additionally, when labeled images are available during training, TaI prompting can be combined with existing prompting methods from images to further improve performance. Specifically, TaI-DPT can be integrated with CoOp and DualCoOp for prompt ensembles and consistent improvement in multi-label recognition. Overall, our contributions include the proposal of TaI prompting, the introduction of TaI-DPT for double-grained prompt tuning, and the demonstration of its effectiveness against state-of-the-art methods in multi-label image recognition.