This paper addresses the problem of estimating poses and actions in egocentric videos involving two hands and an object. This is a crucial task for applications such as augmented reality, virtual reality, and human-computer interaction. Previous research has made progress in separately estimating hand poses and object 6D poses, but there is a growing demand for joint pose estimation of hands and objects. However, most existing methods focus on either pose estimation or interaction recognition separately, and they often rely on unrealistic cropped bounding boxes of hands and objects. This affects the accuracy of pose estimation. To address this issue, Tekin et al. proposed a unified framework that estimates 3D hand pose, object 6D pose, and their action classes. Kwon et al. extended this framework to involve two hands. They used graph convolutional networks to model the hand-object interaction and utilized estimated hand and object poses for interaction recognition. In this paper, the authors propose a Transformer-based unified framework (H2OTR) to estimate poses of two hands, object pose, object types, and interaction classes. They utilize a contact map, which expresses the relational information between hands and objects, as a cue for interaction recognition. The authors demonstrate the effectiveness of their approach on various datasets and achieve state-of-the-art performance in pose estimation and interaction recognition tasks.