StyleGANs have achieved significant success in image generation, particularly on well-curated datasets. They not only generate high-fidelity and diverse images, but also produce a disentangled latent space that is useful for image editing and manipulation tasks. As a result, StyleGANs are widely used in various applications such as face editing, video generation, and face reenactment. However, training StyleGANs on in-the-wild and multi-category datasets remains challenging. While a large-scale conditional StyleGAN has been successfully trained on ImageNet, obtaining such models for distinctive image domains like medical and fine-grained data may not be feasible. In this paper, we aim to train a vanilla class-conditional StyleGAN without any pre-trained models on real-world long-tailed data distributions. We investigate the phenomenon of mode collapse and poor recall for minority classes in StyleGAN, and propose a solution called NoisyTwins. NoisyTwins decorrelates the latent variables in the StyleGAN's latent space, ensuring intra-class diversity in the generated images. We evaluate NoisyTwins on challenging benchmarks of large-scale long-tailed datasets and demonstrate its effectiveness in mitigating mode collapse and class confusion, as well as improving the performance of StyleGAN in generating diverse and class-consistent images. Our approach achieves a new state-of-the-art performance on the evaluated datasets and has the potential to enhance the performance of few-shot GANs.