Mobile phones have become the dominant devices for photography in our daily lives, surpassing DSLR cameras. However, due to the low dynamic range (LDR) of mobile phone sensors, images captured by mobile phones may lose details in dark and bright regions under challenging lighting conditions. High dynamic range (HDR) imaging is essential for improving the quality of mobile phone photography. HDR imaging has been a longstanding research topic in computational photography, even for DSLR cameras. One effective approach to construct an HDR image is to fuse a stack of LDR frames with different exposure levels. However, in dynamic scenes with camera shaking and/or object motion, the fused HDR image may introduce ghost artifacts caused by inaccurate alignment. Deep learning has shown its powerful capability in learning image priors from data, but the development of deep models for HDR imaging has been slow due to the lack of suitable training datasets. Although datasets with LDR-HDR image pairs captured by DSLR cameras have been used for deep learning algorithms, they are not well suited for investigating HDR imaging techniques for mobile phone cameras. Mobile phone cameras are more susceptible to noise and have smaller overexposure areas compared to DSLR cameras. To address these limitations and facilitate research on real-world mobile HDR imaging, we establish a new HDR dataset called Mobile-HDR, using mobile phone cameras. This dataset covers both daytime and nighttime scenes with different noise levels. We propose a new transformer-based model for joint HDR denoising and fusion, using a pyramid cross-attention module for feature alignment and a transformer module for HDR image recovery. Our contributions include building the first mobile HDR dataset with LDR-HDR image pairs and proposing a cross-attention based alignment module for effective joint HDR denoising and fusion. Our work provides a new platform for researchers to investigate and evaluate real-world mobile HDR imaging techniques.