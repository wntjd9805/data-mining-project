Semantic segmentation aims to group pixels into meaningful regions with corresponding semantic categories. While remarkable progress has been made in modern semantic segmentation models, they are primarily trained with pre-defined categories and fail to generalize to unseen classes. To address this limitation and approach human-level perception, this paper focuses on open-vocabulary semantic segmentation, where the model segments an image by arbitrary categories described by texts.Vision-language models, such as CLIP, have shown superior open-vocabulary classification ability by learning rich multi-modal features from billion-scale image-text pairs. Previous works have proposed using pre-trained vision-language models for open-vocabulary segmentation, particularly two-stage approaches that generate class-agnostic mask proposals and then utilize pre-trained CLIP for open-vocabulary classification.However, our analysis reveals that the performance bottleneck lies in the inability of pre-trained CLIP to perform satisfactory classification over masked images. We hypothesize that this is due to the significant domain gap between masked images and CLIP's training images, which are pre-trained on natural images with minimal data augmentation. Mask proposals, on the other hand, are cropped, resized, and further corrupted by noisy segmentation masks. To address this, we propose adapting CLIP by finetuning it on masked images and corresponding text labels. Instead of using segmentation labels, we collect training data by mining an existing image-caption dataset, such as COCO Captions, and generate class-agnostic masked region proposals based on the nouns extracted from the captions. By learning from weakly-supervised alignments between masked images and novel categories, the adapted CLIP better retains its generalization ability for open vocabulary classification.To effectively finetune CLIP, we propose mask prompt tuning, where we replace "zero tokens" in masked images with learnable prompt tokens during tokenization. We find that mask prompt tuning significantly improves CLIP's performance on masked images, especially for multi-task scenarios where CLIP's weights cannot be changed. Combined with full model finetuning, mask prompt tuning further improves performance.Experiments demonstrate that our approach achieves state-of-the-art results on challenging segmentation datasets, surpassing previous solutions and matching the performance of supervised specialist models without dataset-specific adaptations. Our contributions include revealing the performance bottleneck of two-stage approaches, collecting diverse mask-category pairs from captions for adaptation, proposing mask prompt tuning for masked image adaptation, and demonstrating the capability of open-vocabulary generalist models.