Vision transformers have recently gained popularity in computer vision due to their improved model performance. Unlike CNN models, vision transformers learn the association between image patches and classify images using a single token. However, the structural differences between CNN models and vision transformers pose challenges in explainability visualization approaches. Previous research has attempted to address these challenges by using attention score information and relevance propagation rules. In this paper, we propose a novel method called ICE (I Can visualize Everything) that leverages the output embedding vectors of each patch token in a vision transformer to visualize explainability. ICE assumes all patches are background and gradually learns the class of each patch, predicting foreground regions where objects are likely to exist. We evaluate ICE's performance on the ImageNet-Segmentation dataset and achieve improvements in pixel-wise accuracy and mean IoU compared to state-of-the-art methods. We also demonstrate ICE's effectiveness in foreground and background separation tasks using unsupervised semantic segmentation, weakly supervised object localization, and unsupervised object discovery. Furthermore, we incorporate ICE into the encoder of DeiT-S and achieve improved efficiency without sacrificing accuracy. Our contributions include the proposal of ICE, its improved explainability visualization performance, foreground and background separation capabilities, and efficiency gains in vision transformer-based models. Based on our experimental results, we discuss the scalability of our methodology in improving the efficiency of vision transformer-based models.