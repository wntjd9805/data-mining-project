Rendering in computer graphics involves generating realistic images from known properties of a scene. In contrast, inverse rendering aims to recover these unknown properties from images. However, due to the complexity of inverse rendering, existing methods often simplify the problem by making assumptions about lighting, geometry, or materials. Traditional photometric methods for inverse rendering are limited to controlled laboratory settings, while newer neural network methods can handle more complex scenarios but are still restricted to laboratory environments. Neural rendering techniques bypass physical reflection models by learning the appearance of a scene conditioned on a geometric representation, but they struggle to separate reflective properties from illumination and suffer from ill-constrained geometry. This paper proposes a solution that combines conventional methods with in-the-wild inverse rendering. By learning the ambient reflection with a neural light field and using the smartphone's flashlight as a controlled light source, the authors are able to infer reflectance while considering physical constraints. The proposed method is easy to set up, implement, and consistently outperforms competing techniques. The reconstructed objects can be directly used in game and rendering engines as high-fidelity virtual assets.