Object detection aims to localize object instances in images based on semantic object categories. Previous research has primarily focused on the close-set setting, where a detector is trained to recognize only the object categories annotated in the training set. However, collecting and annotating data for novel categories is costly. To address this challenge, open vocabulary object detection methods have been explored, which aim to generalize object detection to novel categories by training on a set of labeled categories and transferring image-text aligned features. These methods often employ class-agnostic network heads, which offer limited capacity to learn category-specific knowledge. This paper proposes a method called CondHead, which leverages semantic embedding as a conditional prior to parameterize class-wise bounding box regression and mask segmentation. By utilizing dynamic parameter generation and a combination of large complex network heads and small light network heads, CondHead achieves strong efficiency and generalizes well to novel categories. The proposed method is flexible in terms of semantic-visual representation and demonstrates improved performance over baselines on benchmark datasets. The contributions of this work include leveraging semantic-visual aligned representation for open vocabulary object detection, designing a semantic-conditioned head to bridge category-specific prediction from base categories to novel categories, and validating the method through extensive experiments and analysis.