Birdâ€™s-Eye-View (BEV) maps are crucial for autonomous driving as they provide a feature-rich and computationally-efficient representation of the environment. These maps encode both static and dynamic obstacles in a low-cost 2D format, allowing for various distance-based time-sensitive applications. Existing approaches for estimating BEV maps require large datasets with BEV annotations, making the generation of ground truth data difficult. In this paper, we propose SkyEye, the first self-supervised learning framework for generating semantic BEV maps from a single monocular frontal view (FV) image. Our approach overcomes the need for BEV ground truths by leveraging FV semantic labels and the spatial and temporal consistency of video sequences. During inference, our model only uses a single FV image to generate the semantic map in BEV. We employ implicit and explicit supervision signals to ensure spatial and temporal consistency and generate BEV semantic pseudolabels from FV semantic ground truths. We evaluate SkyEye on the KITTI-360 and Waymo datasets, demonstrating its competitive performance compared to fully-supervised approaches with only 1% of pseudolabels in BEV. Our contributions include the first self-supervised framework for generating semantic BEV maps from monocular FV images, an implicit supervision strategy, a pseudolabel generation pipeline, a novel semantic BEV dataset derived from Waymo, extensive evaluations, and publicly available code for our SkyEye framework.