Traditional visual representation learning systems are limited in their transferability due to their reliance on a fixed set of predetermined image categories. However, vision-language pre-training approaches, such as CLIP, have shown promise by introducing text descriptions as supervision. CLIP aligns image and text modalities by learning a modality-shared representation, enabling it to perform well in zero-shot image classification tasks. Despite its impressive performance, CLIP requires large amounts of image-text pairs for training, which can be costly and impractical for many laboratories and companies. To address this limitation, we propose a novel and efficient approach called Retrieval Augmented Contrastive Language-Image Pre-training (RA-CLIP). RA-CLIP utilizes a hold-out reference set of image-text pairs to augment the representation of input images. By retrieving similar images from the reference set and leveraging their relationships with texts, our model learns to recognize visual concepts without the need for memorization. Experimental results demonstrate that RA-CLIP achieves better zero-shot classification performance and linear probe classification performance compared to vanilla CLIP and other proposed methods. Through our contributions, we present a more efficient utilization of image-text pairs in contrastive language-image pre-training and provide a comprehensive framework and validation of RA-CLIP's effectiveness in visual recognition tasks.