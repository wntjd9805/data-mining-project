In this paper, we address the problem of visual sound source localization, which involves predicting the location of sound sources in a video. Existing methods mainly focus on single-source localization and struggle to perform well on sound localization from mixtures. To overcome this limitation, we propose a novel Audio-Visual Grouping Network (AVGN) that disentangles individual source representations from the mixture to guide source localization. Our key idea is to learn audio-visual category tokens that aggregate category-aware source features from the sound mixture and image, enabling us to localize multiple sources simultaneously. AVGN outperforms previous baselines on both single-source and multi-source localization tasks, and our approach is able to handle a flexible number of sources. Empirical experiments on benchmark datasets demonstrate the effectiveness of our AVGN in localizing individual sources from mixtures. Our contributions include the introduction of learnable audio-visual class tokens and category-aware grouping for sound localization, as well as the development of AVGN as a superior framework for both single-source and multi-source sounding object localization.