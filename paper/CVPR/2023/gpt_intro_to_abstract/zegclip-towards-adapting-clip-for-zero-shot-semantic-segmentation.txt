Semantic segmentation is a crucial task in computer vision that aims to predict the category of each pixel in an image. Several models have been proposed for this task, including Fully Convolutional Networks, U-net, DeepLab, and Vision Transformer-based methods. However, these models rely heavily on annotated training images, which require significant labor. As a result, there is growing interest in low-supervision-based semantic segmentation approaches, such as semi-supervised, weakly-supervised, few-shot, and zero-shot semantic segmentation.Zero-shot semantic segmentation is particularly challenging, as it requires directly producing segmentation results based on the semantic description of a given class. The pre-trained vision-language model CLIP has shown promising results in dense prediction tasks, including semantic segmentation. However, existing zero-shot segmentation methods based on CLIP follow a two-stage processing strategy, which involves generating region proposals and then classifying them using CLIP. This approach incurs additional computational overhead and does not fully leverage the knowledge of the CLIP encoder.To simplify the pipeline and extend the zero-shot capability of CLIP to pixel-level segmentation, this paper proposes a method called ZegCLIP. The basic idea is to use a lightweight decoder to match text prompts against local embeddings extracted from CLIP using a transformer-based structure. The CLIP image encoder is either fixed or fine-tuned based on a dataset with pixel-level annotations from a limited number of classes. However, the basic version tends to overfit the training set, leading to poor segmentations on unseen classes.To address this issue, three modified design choices are introduced in ZegCLIP. First, Deep Prompt Tuning (DPT) is used instead of fine-tuning or fixing the CLIP image encoder to retain its inherent zero-shot capacity. Second, a Non-mutually Exclusive Loss (NEL) function is applied for pixel-level classification to generate independent posterior probabilities for each class. Third, a Relationship Descriptor (RD) is introduced to incorporate image-level priors into text embeddings, preventing overfitting to seen classes.Experimental results on three public datasets show that ZegCLIP outperforms state-of-the-art methods in both the "inductive" and "transductive" settings, demonstrating the effectiveness of the proposed designs. ZegCLIP offers a simple yet powerful solution for zero-shot semantic segmentation by leveraging the capabilities of CLIP and incorporating innovative design choices.