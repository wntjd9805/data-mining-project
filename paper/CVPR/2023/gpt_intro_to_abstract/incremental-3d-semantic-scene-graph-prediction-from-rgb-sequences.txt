Scene understanding is crucial in various computer vision applications, including robotics, AR/VR, and autonomous systems. Semantic Scene Graphs (SSGs) have been proven to be valuable representations for complex scene understanding tasks. While earlier methods focused on estimating SSGs from 2D images, more recent approaches have explored estimating them from 3D data. However, existing 3D methods either require dense 3D geometry or constrain scene graph estimation at the image-level, limiting their applicability. In this paper, we propose a real-time framework that incrementally estimates a global 3D SSG using only RGB input. Our method reconstructs a segmented point cloud while estimating the SSGs of the current map, allowing for fusion into a consistent global scene graph. We address the challenges of fusing entities across frames and estimating scene graphs from sparse input points by proposing a confidence-based fusion scheme and a scene graph prediction network that relies on multi-view images. We extensively evaluate our method on the 3D SSG estimation task and show that it outperforms existing approaches. Our main contributions include the first incremental 3D scene graph prediction method using RGB images, an entity label association method for sparse point maps, and a novel network architecture that is flexible and generalizable.