Event cameras, which record asynchronous per-pixel brightness changes, have several advantages over conventional RGB cameras including no motion blur, higher dynamic range, ultra-low power consumption, and lower latency. These advantages have led to the application of event cameras in various computer vision problems. However, there is currently no event-based method for generating dense photorealistic renderings of a scene in a 3D-consistent manner. Existing methods usually reconstruct sparse 3D models from a single event stream, resulting in sparse novel views. In this work, we address the research question of whether an event stream from a moving event camera is sufficient to reconstruct a dense volumetric 3D representation of a static scene. We propose EventNeRF, the first approach for inferring a Neural Radiance Fields (NeRF) volume from a monocular color event stream, enabling 3D-consistent, dense, and photorealistic novel view synthesis in the RGB space. Our method is designed for event-based supervision, preserving the resolution of individual RGB event channels. We evaluate our technique on a new dataset of synthetic and real event sequences, demonstrating superior results compared to traditional RGB cameras in scenarios with high speed movements, motion blur, or insufficient lighting. We also demonstrate the extraction of depth maps from arbitrary viewpoints. Our primary technical contributions include the EventNeRF approach, a tailored ray sampling strategy for events, and an experimental evaluation protocol. The code and dataset will be released to establish a benchmark for future work.