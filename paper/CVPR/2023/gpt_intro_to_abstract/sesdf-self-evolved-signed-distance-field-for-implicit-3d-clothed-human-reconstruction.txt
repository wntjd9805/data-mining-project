Clothed human reconstruction is a popular topic in computer science, with applications in 3D telepresence, game modeling, and the metaverse. Early methods required expensive equipment and complex calibration procedures. Parametric models like SMPL and SMPL-X were introduced to model naked human bodies with constrained parameters. Deep learning methods have been proposed to regress the parameters of these models from single or multiple images, but they only reconstruct minimally-clothed models without many details. Recent methods based on implicit shape representation have shown promising results in reconstructing models with increased details in both single-view and multi-view settings. However, these reconstructions are still far from perfect, limiting their practical applications. State-of-the-art single-view methods struggle with self-occluding non-frontal poses, and multi-view methods rely on calibrated cameras which are difficult to obtain in practice. Effective multi-view feature fusion is also a challenge, as existing fusion techniques still produce artifacts in many cases. In this paper, we propose a novel framework called SeSDF that combines the parametric model SMPL-X with implicit and explicit representations to extract clothed human details from single or multi-view images. We introduce a self-evolved signed distance field module to deform the SMPL-X model based on input images, resulting in more accurate geometry details. Our framework can also generate clothed human avatars with enhanced appearance from uncalibrated multi-view images. We propose a self-calibration method and an occlusion-aware feature fusion strategy to improve reconstruction accuracy. We evaluate our framework on public benchmarks and it outperforms current state-of-the-art methods both qualitatively and quantitatively. Our contributions include a flexible framework for high-fidelity clothed human reconstruction, a self-calibration method for uncalibrated multi-view reconstruction, and an occlusion-aware feature fusion strategy.