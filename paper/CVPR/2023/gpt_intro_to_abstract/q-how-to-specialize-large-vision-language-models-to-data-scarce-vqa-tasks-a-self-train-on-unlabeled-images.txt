Large pretrained vision language models have achieved impressive performance on visual question answering (VQA) tasks, but there is still a significant gap between human and machine performance on complex VQA tasks. One challenge in addressing this gap is the limited availability of datasets for these tasks, particularly in domains beyond natural images. Previous solutions have relied on transfer learning from larger VQA datasets, but this approach can be hindered by weaknesses in VQA models and the fine-tuning process. This paper introduces SelTDA, a framework for self-taught data augmentation in VQA that leverages unlabeled images to generate new question-answer pairs. The proposed approach involves using a large pretrained vision-language model to generate questions conditioned on images, pseudolabeling the unlabeled images using these generated questions and answers, and then augmenting the original VQA dataset with the newly labeled image-question-answer pairs for fine-tuning. This framework offers several benefits, including an increase in the number and diversity of training samples, improved robustness and domain generalization, and the preservation and transfer of pretrained knowledge. The paper presents experimental results that demonstrate the effectiveness of SelTDA in improving VQA performance on small-scale datasets. This work contributes a novel approach to address the data scarcity challenge in complex VQA tasks and highlights the potential of utilizing unlabeled images for specific visual question answering scenarios.