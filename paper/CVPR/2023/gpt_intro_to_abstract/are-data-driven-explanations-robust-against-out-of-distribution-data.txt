The increasing use of black-box machine learning (ML) models for high-stakes applications has raised concerns about the lack of explainability, particularly in domains such as healthcare and criminal justice. ML models are also vulnerable to unseen distributions that differ from their training data, leading to potential failure on out-of-distribution (OOD) data. Existing works have focused on the reliability of data-driven explanation methods but have overlooked the robustness of explanations against distributional shifts. This paper investigates the robustness of data-driven explanations against OOD data and proposes an end-to-end model-agnostic training framework called Distributionally Robust Explanations (DRE). DRE utilizes inter-distribution information to provide supervisory signals for explanation learning, aiming to develop robust explanations that enhance the model's generalization capability. The proposed method is evaluated on various tasks, including image classification, regression, and scientific tabular data, demonstrating superior explanation consistency, fidelity, scientific plausibility, and prediction accuracy on OOD data compared to existing methods. The results highlight the potential of robust explanations to improve the safety and reliability of ML models in real-world applications.