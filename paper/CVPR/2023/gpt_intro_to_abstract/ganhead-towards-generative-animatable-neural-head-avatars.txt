Generating photorealistic and animatable head avatars without manual effort is a challenging problem in computer vision and computer graphics. This problem has numerous applications in virtual reality/augmented reality, games, movies, and the metaverse. In order to be useful in these applications, head avatar models should meet certain requirements, including completeness, realism, animatability, and generativity. Existing methods for modeling head avatars have limitations. 3D morphable models (3DMMs) built from registered meshes can only model the facial region or the full head without hair, and oversimplification of principal component analysis (PCA) leads to models that lack realism. Discriminative models and 3D-aware generative adversarial networks (GANs) can model the complete head geometry with realistic texture, but these methods are not capable of generating new samples. Implicit generative models can achieve realistic and animatable head avatars, but they may not generate a complete head with satisfactory geometry or lack the ability to be animated explicitly using common parametric face models.In this paper, we propose a generative animatable neural head avatar model called GANHead that addresses these limitations. GANHead represents the 3D head implicitly using neural occupancy functions learned by multilayer perceptrons (MLPs), where coarse geometry, fine-grained details, and texture are modeled separately. The model is supervised using unregistered ground truth 3D head scans. GANHead achieves complete and realistic generation results while maintaining desirable generative capacity.To enable explicit control of the implicit representation with animation parameters, we extend the multi-subject forward skinning method designed for human bodies to human faces. This enables the GANHead framework to achieve flexible animation explicitly controlled by FLAME pose and expression parameters. The deformation field in GANHead is defined by standard vertex-based linear blend skinning (LBS) with learned pose-dependent corrective bases, skinning weights, and expression bases to capture non-rigid deformations. This approach allows GANHead to be learned from textured scans without the need for registration or canonical shapes.Once trained, GANHead can generate diverse textured head avatars by sampling shape, detail, and color latent codes. These avatars can then be animated flexibly using FLAME parameters, with geometry consistency and pose/expression generalization capability. Experimental results demonstrate that our method outperforms state-of-the-art complete head generative models in head avatar generation and raw scan fitting.