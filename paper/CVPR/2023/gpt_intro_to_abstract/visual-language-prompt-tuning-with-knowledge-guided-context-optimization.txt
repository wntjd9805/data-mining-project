Abstract:This paper introduces KgCoOp, a novel method for training visual-language models (VLM) that effectively extracts visual and text descriptions. VLMs have shown promise in various tasks, but training them requires large-scale, high-quality datasets. However, collecting such datasets for real-world visual-language tasks is challenging. To address this problem, the authors propose prompt tuning, which uses task-related textual tokens to embed task-specific textual knowledge for prediction. They compare KgCoOp with existing methods and demonstrate that it achieves higher performance with less training time. Additionally, they evaluate KgCoOp in base-to-new generalization setting and few-shot classification, showing that it outperforms other methods. Overall, KgCoOp offers improved performance and efficiency in training VLMs for visual-language tasks.