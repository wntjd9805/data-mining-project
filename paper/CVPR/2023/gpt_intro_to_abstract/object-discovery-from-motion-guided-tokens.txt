This paper introduces a novel architecture called Motion-guided Tokens (MoTok) for unsupervised video object discovery. The authors address the challenge of learning unsupervised, object-centric representations in computer vision, which can enable the scaling up of computer vision to the real world. They propose leveraging motion to guide tokenization, the vector quantization process in transformer architectures, and introduce a new transformer decoder. The authors comprehensively evaluate the contributions of prior works and show that their method alleviates the need for labels, optical flow, or depth decoding, improving upon the state of the art. They also demonstrate that motion-guided tokens map to interpretable mid-level features, explaining why their method scales to challenging realistic videos. The paper concludes by providing access to the code, models, and synthetic data used in the study.