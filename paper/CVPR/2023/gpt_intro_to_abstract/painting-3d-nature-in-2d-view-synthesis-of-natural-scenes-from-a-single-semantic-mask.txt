This paper focuses on the problem of synthesizing novel views of natural scenes given a single semantic mask. The development of deep generative models has led to significant advancements in 2D semantic image synthesis methods. However, these methods do not consider the underlying 3D structure and cannot generate multi-view consistent free-viewpoint videos. To address this issue, the authors propose a novel framework for semantics-guided view synthesis of natural scenes. They divide the task into two sub-problems: generating semantic masks at novel views and translating them to RGB images. Experimental results demonstrate the effectiveness of the proposed approach in generating high-quality rendering results and achieving multi-view consistency.