Video anomaly detection (VAD) is a challenging research topic in computer vision due to the scarcity of anomalous samples. Unsupervised methods have been widely explored for VAD, while supervised methods struggle due to the lack of anomalous behavior patterns. Reconstruction and prediction-based approaches are commonly used for VAD using deep neural networks (DNN). However, these approaches have limitations in capturing higher-level visual features and comprehensive temporal context relationships. To address these limitations, this paper proposes a novel VAD paradigm called video event restoration based on keyframes. Inspired by video codec theory, keyframes are used as input cues to infer missing frames and restore the entire video event. A U-shaped Swin Transformer Network with Dual Skip Connections (USTN-DSC) is proposed for video event restoration, which incorporates cross-attention and temporal upsampling residual skip connections to handle complex motion patterns. Additionally, an adjacent frame difference (AFD) loss is introduced to ensure temporal consistency in the restored video sequence. Experimental results demonstrate the effectiveness of USTN-DSC in outperforming existing methods on three benchmarks. The contributions of this paper include the introduction of the video event restoration paradigm, the proposed USTN-DSC model, and the AFD loss for motion consistency.