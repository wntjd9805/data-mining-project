This paper introduces the concept of event-based frame interpolation (VFI) in computer vision. VFI methods are used to synthesize intermediate frames between consecutive input frames, increasing the frame rate of the input video. This technique has wide applications in various fields such as super-slow motion generation, video editing, virtual reality, and video compression. Previous frame-based methods utilize motion models to estimate inter-frame motion, but these models struggle to accurately capture the non-linear motion in real-world videos. Recent works propose the use of event cameras as a proxy to estimate inter-frame motion. Event cameras are bio-inspired sensors that report per-pixel intensity changes instead of synchronous full intensity images. These events are recorded at high temporal resolution and high dynamic range, providing valid compressed motion information. Event-based VFI methods have shown promising results, especially in high-speed non-linear motion scenarios. However, these methods assume that the input images are sharp, which is often violated in real-world scenes due to motion blur. This paper addresses the challenge of both sharp and blurry image interpolation by proposing a unified framework that combines event-based VFI and event-based single image deblurring. The proposed framework consists of a novel recurrent network with two branches: an image branch and an event branch. The recurrent structure enables the propagation of information from events across time. The network also includes an attention-based module for event-image fusion. To evaluate the proposed method, the authors have recorded a new high-resolution dataset with events and RGB videos, providing a real-world setting for evaluation. The contributions of this paper include the proposed framework and network architecture, as well as the new dataset. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on both synthetic and real-world datasets.