Diffusion models have enabled significant advancements in image synthesis in computer vision but often require large annotated datasets. Guidance methods using class labels and classifiers have been successful in improving image fidelity. However, these methods rely on ground-truth annotations, which can be unrealistic and costly in many domains. In this paper, we propose self-guided diffusion models that eliminate the need for annotated image-label pairs. Inspired by self-supervised learning, we leverage holistic image-level self-supervision and extend it to dense representations such as bounding boxes and segmentation masks. Our framework includes a feature extraction function and a self-annotation function compatible with recent self-supervised learning techniques. We demonstrate the effectiveness of our proposal on single-label and multi-label image datasets, where self-labeled guidance surpasses diffusion models without guidance and even outperforms guidance based on ground-truth labels. Additionally, our method generates visually diverse yet semantically consistent images without the need for class, box, or segment label annotation.