The paper discusses the reconstruction of geometry and appearance of a 3D scene from 2D images using neural radiance fields (NeRFs). While NeRFs have shown high-quality view synthesis, the reconstructed geometry is still unsatisfactory. Previous works have attempted to improve geometric reconstruction within volume rendering, but they do not explicitly address the shape-radiance ambiguity that can lead to degenerated reconstructions. This ambiguity is caused by the capacity needed to account for directional variations in radiance. The paper proposes a view-dependence normalization method that aligns the directional variations of different scenes to the same level, ensuring a single optimal capacity for good view synthesis and preventing shape-radiance ambiguity. The proposed method is evaluated on several datasets and is shown to achieve state-of-the-art geometry compared to baselines. The robustness of the method is also validated under dynamically changing light fields. The paper makes contributions to the study of the coupling between the capacity of the view-dependent radiance function and shape-radiance ambiguity, proposes the view-dependence normalization method, and verifies its effectiveness and robustness.