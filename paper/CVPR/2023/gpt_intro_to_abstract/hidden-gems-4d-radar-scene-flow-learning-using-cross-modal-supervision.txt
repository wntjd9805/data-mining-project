Scene flow estimation plays a crucial role in self-driving systems by providing motion cues for various tasks, such as ego-motion estimation, motion segmentation, point cloud accumulation, and multi-object tracking. Deep neural networks have been successful in point cloud processing, and current approaches to scene flow estimation from point clouds mainly rely on supervised or self-supervised learning. Supervised methods require costly annotations, while self-supervised methods do not provide reliable results for safety-critical scenarios. This becomes even more challenging in the case of 4D radar scene flow learning, where radar point clouds are sparse and suffer from noise. To address these challenges, this work aims to exploit cross-modal supervision signals from co-located sensors in autonomous vehicles. By leveraging the complementary information provided by sensors like LiDARs, cameras, and GPS/INS, multiple supervision cues can be used to bootstrap radar scene flow learning. This work presents a multi-task model architecture and proposes loss functions to effectively engage scene flow estimation using cross-modal constraints. Experimental results demonstrate the effectiveness of the proposed method in achieving state-of-the-art performance in radar scene flow estimation and other related tasks.