Predicting the 3D shape of articulated objects from a single image is a challenging task due to its under-constrained nature. While successful approaches exist for inferring the 3D shape of humans, similar breakthroughs for other categories of articulated objects, such as animals, remain elusive. This is primarily due to the scarcity of appropriate training data, as 2D labels are limited or non-existent for most object categories. In this paper, we propose an approach that requires as few as 50-150 images labeled with 2D keypoints, which can be easily and quickly created for any object category. Our approach involves training a category-specific keypoint estimation network using a small labeled set, generating 2D keypoint pseudo-labels on a large unlabeled set of web images, automatically curating the web images to create a subset with high-quality pseudo-labels, and training a model for 3D shape prediction using data from both the labeled set and the curated subset. We demonstrate through extensive experiments on various articulated object categories that training with the proposed data selection approaches leads to considerably better 3D reconstructions compared to the fully-supervised baseline. We also analyze the effectiveness of the data selection methods and show that even with a small number of annotated instances and web images, our models outperform fully-supervised models trained with more labels.