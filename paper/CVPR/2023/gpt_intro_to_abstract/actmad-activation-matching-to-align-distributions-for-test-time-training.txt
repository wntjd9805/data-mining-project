Deep networks have shown superior performance compared to humans in visual recognition tasks, but this performance relies on the assumption that the training and test data come from the same distribution. However, in practical scenarios such as autonomous vehicles, the test data can involve various distribution shifts due to changing weather conditions. This can result in a significant performance penalty for deep networks. Test-time-training (TTT) methods aim to adapt deep networks to the test data distribution at test-time, but existing techniques have limitations in terms of performance and applicability to different tasks and architectures. In this paper, we propose a versatile and task-agnostic TTT technique called Activation Matching to Align Distributions (ActMAD), which extends feature alignment to multiple feature maps across the network. ActMAD aligns the distribution of each feature in a location-aware manner and updates all network parameters. Our approach is architecture- and task-agnostic, does not require access to training data or labels, and maintains privacy in sensitive applications. We demonstrate the effectiveness of ActMAD by surpassing state-of-the-art performance on TTT benchmarks, particularly in online object detection in changing weather conditions.