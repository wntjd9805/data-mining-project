The introduction of this computer science paper discusses the effectiveness of Transformers in Natural Language Processing tasks and their exploration in computer vision. Vision Transformers (ViTs) use multi-head self-attention mechanisms to learn features by dividing input images into patches and processing them as sequences. ViTs have shown promising results in various visual tasks such as image classification, object detection, vision-language modeling, and video recognition. However, it remains unclear how ViTs can be scaled to handle inputs with varying sizes for different applications. Current strategies involve shrinking the spatial dimension of inputs or fine-tuning with higher resolutions, but limited efforts have been made to equip ViTs with the ability to handle different resolutions. The paper proposes ResFormer, a model that adjusts positional embeddings smoothly across different scales for multi-resolution training. It also explores information across different resolutions for improved performance. ResFormer is trained on multi-resolution images and can generalize to a wide range of testing resolutions. The paper presents experimental results on ImageNet-1K, demonstrating that ResFormer outperforms vanilla ViTs and achieves higher accuracy on both seen and unseen resolutions. The scalability of ResFormer is also validated on dense prediction tasks and video action recognition.