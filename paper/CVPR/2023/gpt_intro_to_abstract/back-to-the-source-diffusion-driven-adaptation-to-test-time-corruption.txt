Deep networks have shown impressive performance in visual recognition tasks, but their performance can deteriorate when there is a shift between the source data and the target data during testing. This shift can be caused by factors such as corruption, adversarial attacks, or natural differences between simulations and reality. To address this issue, adaptation and robustness techniques have been developed to update predictions and improve accuracy on target data. In this paper, we focus on two important aspects of adaptation: what to adapt (the model or the input) and how much to adapt (using update or not). We propose a test-time input adaptation method driven by a generative diffusion model to counteract shifts caused by image corruptions.Most existing adaptation approaches rely on joint optimization over source and target data during training. However, these methods face the challenge of unknown shifts that might occur during testing. In such cases, test-time updates are necessary to adapt the model without access to the source data and without interrupting the inference process. Source-free adaptation methods address the first challenge by re-training the model on new target data without access to the source data. Test-time adaptation methods, on the other hand, address both challenges by iteratively updating the model during inference. While updating the model can improve robustness, it comes with computational costs and risks. Model updates can be computationally expensive, making it challenging to scale to multiple targets, and they can also be sensitive to different amounts or orders of target data, leading to noisy updates that may hinder robustness.We propose a novel approach that updates the target data instead of the source model. Our diffusion-driven adaptation method, DDA, learns a diffusion model on the source data during training and then projects inputs from all target domains back to the source during testing. By training the diffusion model to replace the source data, we enable source-free adaptation. Additionally, we adapt target inputs while making predictions, allowing for test-time adaptation. Our experiments compare and contrast input and model updates in terms of their robustness to corruptions. We evaluate DDA against state-of-the-art diffusion-based methods for adversarial defense and entropy minimization methods for online and episodic test-time updates. The experiments demonstrate that DDA achieves higher robustness than existing methods across various data regimes and models. Our contributions include proposing DDA as the first diffusion-based method for test-time adaptation to corruption, identifying weak points of online model updates, and demonstrating the effectiveness of input updates in challenging data regimes.