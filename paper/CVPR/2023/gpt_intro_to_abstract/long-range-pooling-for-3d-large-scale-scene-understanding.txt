With the increasing availability of 3D data from various applications such as autonomous driving and augmented/virtual reality, there is a need for efficient processing and analysis of this data. Traditional convolutional neural networks (CNNs) cannot be directly applied to irregular and unordered 3D data like point clouds or meshes. Therefore, specially designed networks such as MLPs, CNNs, GNNs, and transformers have been proposed for 3D data processing. However, processing large-scale point clouds or meshes is computationally expensive, and multi-view images may result in 3D information loss. Voxel data, on the other hand, offers regularity like images and simplifies the data structure. This paper focuses on processing voxel data and extends 2D CNN networks to 3D. However, the sparsity of 3D voxel data limits the effectiveness of traditional sparse CNNs in capturing long-range contexts. Transformers have proven useful for processing 3D scenes but come with heavy computational costs. This paper proposes a simple and effective module called long range pooling (LRP) for 3D scene segmentation. LRP increases the effective receptive field without a significant amount of computational overhead. It achieves this by using dilation max pooling and a receptive field selection module that adapts to the voxel distribution. The LRP module can be readily incorporated into other networks, and when added to a sparse convolution network, LRPNet achieves superior 3D segmentation accuracy on large-scale scene datasets. The paper's contributions include the LRP module, the demonstration of the benefits of a larger receptive field and greater non-linearity in feature aggregation, and the improved segmentation results on various 3D scene benchmarks.