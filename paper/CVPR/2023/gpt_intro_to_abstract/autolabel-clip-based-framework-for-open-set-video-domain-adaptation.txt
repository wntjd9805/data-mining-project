Recognizing actions in video sequences plays a significant role in various computer vision applications. Deep learning techniques have been widely utilized for action recognition, particularly in supervised and unsupervised learning regimes. However, the discrepancy between source and target domains leads to sub-optimal performance when using a source trained model in the target domain. To address this problem, unsupervised video domain adaptation (UVDA) methods have been proposed in the literature. Most UVDA methods assume that the label space in the source and target domains is identical, which may not hold true in practical scenarios. Open-set unsupervised video domain adaptation (OUVDA) methods have been proposed to promote adaptation between shared classes while excluding target-private classes. In this paper, we propose a novel approach to OUVDA by leveraging the rich representations of the CLIP model, which is trained on web-scale image-text pairs. We argue that CLIP is suitable for OUVDA due to the prior knowledge it encodes about the real world and its zero-shot recognition capability. In addition, we introduce AutoLabel, an automatic labeling framework that utilizes CLIP to identify target-private instances in an unconstrained OUVDA scenario. AutoLabel augments the set of shared class names with candidate target-private class names extracted using an external image captioning model. We demonstrate the effectiveness of our approach through thorough experimental evaluations on multiple benchmarks, surpassing existing OUVDA state-of-the-art methods.