Learning from noisy labels poses challenges for training deep models, especially in domains such as medical and remote sensing images where annotating data with high confidence is resource-intensive. Label noise inevitably arises and degrades the generalization performance of deep models. Previous methods in noisy label learning (LNL) focus on generic classification and artificially construct random and dependent label noise. In this work, we extend LNL to fine-grained classification, which is more realistic due to the ambiguity in fine-grained images. We investigate the performance of prevailing LNL methods on our proposed LNL-FG task and find that these methods do not transfer well to LNL-FG. We introduce a novel framework, stochastic noise-tolerated supervised contrastive learning (SNSCL), which addresses the noise-sensitivity of supervised contrastive learning (SCL). SNSCL contains a noise-tolerated contrastive loss and a stochastic module to modify the label of noisy anchors and selectively update the momentum queue. We demonstrate the effectiveness of SNSCL on LNL-FG through extensive experiments on fine-grained and real-world datasets. Our proposed framework significantly improves the performance of prevailing LNL methods on LNL-FG.