Training a generalist model in computer vision that can perform multiple tasks and adapt to new tasks is a step closer to artificial general intelligence. While in-context learning has been explored in natural language processing (NLP), its application in computer vision remains unclear. This is due to differences in the modalities of NLP and computer vision tasks, as well as variations in output representations. Previous attempts in computer vision have tried to convert vision problems into NLP ones, but we propose that images themselves can serve as a natural interface for visual perception. In this work, we address these challenges by formulating dense-prediction vision problems as image inpainting. We use a 3-channel tensor representation as the output for various vision tasks, and specify task prompts using pairs of images. We train a generalist Painter model by stitching images from the same task and applying masked image modeling. This enables the model to perform tasks conditioned on visible image patches, allowing for in-context prediction. Our model achieves competitive performance on fundamental vision tasks, surpassing previous specialized models in certain tasks. Our approach does not rely on complex language instructions and can be applied to both in-domain and out-of-domain vision tasks.