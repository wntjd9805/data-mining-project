Abstract:Spatio-temporal video grounding (STVG) is a fundamental task in video-language understanding. Existing approaches for STVG heavily rely on dense annotations such as temporal boundaries and spatial bounding boxes, which are labor-intensive and not readily available. In this paper, we address the weakly-supervised setting for STVG, where spatial-temporal annotations are absent. We propose a novel WINNER framework that incorporates hierarchical video-language decomposition and alignment to mitigate spurious correlations caused by limited annotations. The framework utilizes multi-modal decomposition and contrastive learning to achieve hierarchical alignment between video and language components. Experimental results on two widely used datasets demonstrate the effectiveness of the proposed framework, outperforming state-of-the-art methods and achieving comparable performance to supervised methods. Our contributions include introducing a novel perspective for weakly supervised STVG, proposing the WINNER framework, and providing experimental validation of its effectiveness.