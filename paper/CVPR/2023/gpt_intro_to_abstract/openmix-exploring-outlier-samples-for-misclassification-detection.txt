In risk-sensitive applications such as clinical decision making and autonomous driving, it is crucial to provide reliable confidence estimates to avoid using wrong predictions. However, deep neural networks (DNNs) are known to be overconfident for their erroneous predictions. While there have been many efforts to enhance the ability of DNNs to detect out-of-distribution (OOD) samples, little attention has been paid to detecting misclassification errors from known classes. In this paper, we focus on the under-explored problem of misclassification detection (MisD) and propose a simple approach to help decide whether a prediction is likely to be misclassified. We draw inspiration from the fact that humans are good at confidence estimation because they have access to abundant prior knowledge. Humans use counterexamples from the open world to establish reliable confidence. Inspired by this, we propose to leverage outlier data, which are unlabeled random samples from non-target classes, as counterexamples to reduce the confidence of misclassifications. We investigate the well-known Outlier Exposure (OE) method for this purpose but find that it is more of a hindrance. Existing OOD detection methods also do not perform well in MisD because they compress the confidence region of in-distribution samples, making it difficult to distinguish correct samples from misclassifications. To address this, we propose a learning to reject framework called OpenMix, which breaks the closed-world classifier and assigns a separate reject class for outlier samples. We mix the in-distribution and outlier samples and assign soft labels to the mixed samples to reduce the distribution gap. Our experiments show that OpenMix significantly improves MisD performance without degrading accuracy. Our contributions include the exploration of outlier samples for detecting misclassification errors, the proposal of the OpenMix method, and extensive experiments that demonstrate its effectiveness in improving MisD and OOD detection performance.