Deep neural networks (DNNs) have become crucial in various domains, offering high predictive performance. However, recent research has highlighted a flaw in DNNs where they tend to make overconfident predictions, leading to miscalibration. This poses a problem in safety-critical applications like healthcare diagnosis, self-driving cars, and legal research tools. Existing calibration techniques for DNNs are either restrictive or target visual image classification, neglecting the calibration of object detection models. Additionally, these techniques primarily focus on in-domain predictions and struggle with out-of-domain predictions. In this paper, we aim to address these issues by studying the calibration of deep learning-based object detection methods and propose a train-time calibration approach that jointly calibrates multiclass confidence and bounding box localization. Our approach leverages predictive uncertainty and demonstrates superior calibration performance on challenging datasets, reducing calibration error in both in-domain and out-of-domain scenarios.