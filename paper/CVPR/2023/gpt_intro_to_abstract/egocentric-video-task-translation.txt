In recent years, the application of deep learning models to video understanding has been enabled by large-scale video datasets. However, these datasets have primarily focused on action recognition in curated third-person videos. In contrast, our daily life involves diverse interactions with other humans, objects, and environments from a first-person perspective. First-person videos captured by wearable cameras offer a continuous stream of the observer's perspective and attention. In this paper, we introduce EgoTask Translation (EgoT2), a unified learning framework that leverages the synergies among diverse egocentric video tasks to improve individual task performance. EgoT2 addresses the challenges of transfer learning and multi-task learning in egocentric video understanding. It consists of task-specific models for individual tasks and a task translator that models inter-task and inter-frame relations. We propose two designs: task-specific EgoT2 (EgoT2-s), which optimizes a primary task with the help of auxiliary tasks, and task-general EgoT2 (EgoT2-g), which supports task translation for multiple tasks simultaneously. EgoT2 preserves the peculiarities of each task and mitigates negative transfer. We evaluate EgoT2 on a diverse set of egocentric perception tasks from the Ego4D benchmark and achieve state-of-the-art performance in multiple challenges. Our results demonstrate the inherent task synergies and interpretability provided by EgoT2.