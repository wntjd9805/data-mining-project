This paper introduces a method for estimating the dense 3D scene using a single radar scan and a single camera image. The proposed approach learns a one-to-many mapping of correspondence between each radar point and the probable surfaces in the image. This allows for the mapping of ambiguous and noisy radar points to object surfaces in the image, enabling the recovery of dense depth information. The method also incorporates a gated fusion mechanism to adaptively modulate the trade-off between noisy radar depth and image information. Experimental results demonstrate that the proposed method outperforms existing approaches that use multiple image and radar frames, achieving state-of-the-art performance on the NuScenes benchmark. The contributions of this paper include the first approach to learn radar-to-camera correspondence using a single radar scan and a single camera image, the introduction of confidence scores for fusing radar and image modalities, and the use of a learned gated fusion mechanism.