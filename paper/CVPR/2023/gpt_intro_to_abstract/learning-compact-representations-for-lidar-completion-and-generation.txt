Building a robust 3D perception system is crucial for the advancement of self-driving vehicles. While LiDAR is the primary sensing modality for perceiving the environment, it has limitations in terms of sparsity and scalability. The sparsity problem arises from the decreasing point cloud density as the distance to the sensor increases. This difficulty is exacerbated in poor weather conditions or with fewer LiDAR beams. Increasing the number of beams is a costly solution. Furthermore, the high price of LiDARs limits their accessibility and hinders data collection for training and testing autonomous systems. Using LiDAR simulation suites is a potential workaround but has limitations. In this paper, we propose UltraLiDAR, a novel framework for LiDAR completion and generation. By learning a compact, discrete 3D representation of LiDAR point clouds, we can densify sparse point clouds and generate realistic driving scenes. We demonstrate the effectiveness of UltraLiDAR on sparse-to-dense LiDAR completion and LiDAR generation tasks and show significant improvements in the performance and generalization of detection models. Our results also match the statistics of ground truth data and are preferred by human participants over prior art methods. Our contributions include a LiDAR representation for downstream applications, a sparse-to-dense LiDAR completion pipeline, and an (un)conditional LiDAR generative model.