This paper introduces a novel approach to 3D-aware multi-class image-to-image (I2I) translation, specifically targeting the generation of view-consistent videos. Previous methods in 2D-I2I translation suffer from unrealistic shape and identity changes when changing viewpoints, leading to unnatural results, especially in video translation. The proposed approach leverages recent advancements in 3D-aware image synthesis, combining a multi-class 3D-aware generative model with a 3D-aware I2I translation model. The generative model synthesizes view-consistent 3D scenes, which are then used to initialize the I2I translation model. The authors also introduce several techniques, including a U-net-like adaptor network design, a hierarchical representation constraint, and a relative regularization loss, to address view-inconsistency issues. Extensive experiments demonstrate that the proposed method outperforms existing 2D-I2I systems in terms of temporal consistency. This work contributes to the exploration of 3D-aware multi-class I2I translation and provides valuable insights into improving view-consistency in such tasks.