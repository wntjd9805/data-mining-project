Deep generative models of images have made significant advancements in recent years, falling into two main categories: likelihood-based models and generative adversarial networks (GANs). Likelihood-based models, such as VAEs, flow-based models, diffusion models, and autoregressive models, aim to learn the full data distribution and detect overfitting. Autoregressive models, in particular, have gained attention for their modeling ability and scalability in image generation. These models follow a two-stage generation paradigm, where the first stage learns a codebook in the latent space for image reconstruction, and the second stage uses autoregressive models to generate images based on the learned codebook. However, existing codebook learning approaches do not consider the perceptual importance of different image regions, resulting in redundancy and degradation in generation quality. In this study, we propose a novel two-stage generation paradigm that leverages the mask mechanism to address these issues. Our proposed method, called Masked Quantization VAE (MQ-VAE), incorporates an adaptive mask module and adaptive de-mask module to selectively quantify and reconstruct important image regions. We also introduce Stackformer, a model that predicts code combinations and their corresponding positions, to account for the dynamic changes in important regions across different images. Experimental results demonstrate the effectiveness and efficiency of our approach, achieving improvements in generation quality and speed compared to existing autoregressive models.