Few-shot learning (FSL) is a promising approach for designing classifiers in scenarios where labeled data is limited. This paper focuses on transductive FSL, where the entire query set is accessible during evaluation. Many existing transductive FSL methods rely on distance-based predictions using prototypes for novel classes, making them susceptible to the hubness problem. The hubness problem occurs when certain exemplar points, known as hubs, appear among the nearest neighbors of many other points, resulting in low accuracy. Previous approaches have proposed embedding methods to improve FSL classifiers' performance, but only one directly addresses the hubness problem. In this paper, we propose two new embedding methods named noHub and noHub-S that reduce hubness while preserving the class structure in the representation space. Our methods optimize a tradeoff between local similarity preservation and uniformity on the hypersphere. Experimental results show that noHub and noHub-S outperform current state-of-the-art embedding approaches, improving the performance of various transductive FSL classifiers on multiple datasets and feature extractors.