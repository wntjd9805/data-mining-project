3D-consistent image synthesis from single-view 2D data has gained significant attention in the field of generative modeling. Existing approaches, such as GRAF and Pi-GAN, have introduced 3D inductive bias through neural radiance fields, allowing for geometry modeling and explicit camera control. However, these approaches struggle with complex scenes that contain multiple objects and intricate backgrounds. The varying quantity, diversity, and spatial arrangement of objects, along with mutual occlusions, pose significant challenges that existing generative models cannot handle. Efforts have been made towards 3D-aware scene synthesis, but current methods still have fundamental limitations. Generative Scene Networks (GSN) achieve scene synthesis at a large scale but lack object-level editing capabilities due to spatial entanglement. On the other hand, GIRAFFE supports object-level control through explicit compositing of object-centric radiance fields but performs poorly on datasets with multiple objects and complex backgrounds due to the lack of proper spatial priors.To address these limitations and achieve high-quality and controllable scene synthesis, we propose a novel 3D-aware generative model called DisCoScene. Our model leverages a layout prior, which is an abstract object-oriented scene representation in the form of object bounding boxes without semantic annotation. This layout prior acts as a lightweight supervision signal during training and enables user interaction during inference. With DisCoScene, users can generate and edit scenes by controlling the camera and the layout of objects' bounding boxes.To ensure spatial disentanglement between objects and the background, our model utilizes global-local discrimination during training. This discrimination mechanism attends to both the whole scene and individual objects, enforcing the disentanglement and enhancing the quality of scene synthesis. Additionally, we develop an efficient rendering pipeline tailored for the spatially-disentangled radiance fields, improving both training and inference stages.We evaluate our method on diverse datasets, including indoor and outdoor scenes. Qualitative and quantitative results demonstrate that DisCoScene outperforms existing baselines in terms of generation quality and editing capability. Notably, DisCoScene is the first method to achieve high-quality 3D-aware generation on challenging datasets like WAYMO while also enabling interactive object manipulation.