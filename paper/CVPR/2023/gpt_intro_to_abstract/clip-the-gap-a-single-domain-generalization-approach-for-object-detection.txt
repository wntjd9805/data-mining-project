Abstract:This paper addresses the problem of single domain generalization (SDG) in object detection, where the goal is to learn representations that can generalize to any target domain, even when only a single source dataset is available. We propose a novel approach that leverages a vision-language model, CLIP, to guide the training of an object detector for SDG. By exploiting the jointly learned visual and textual representations in CLIP, we perform semantic augmentations in feature space, increasing the diversity of the source data. We demonstrate the effectiveness of our approach on a practical driving dataset with diverse weather conditions. Our method outperforms existing SDG approaches and achieves state-of-the-art results. The implementation of our approach is publicly available.