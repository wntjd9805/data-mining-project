This paper introduces the concept of command-driven articulated object manipulation, which aims to enable machines to manipulate everyday objects under human instructions. The paper highlights the challenges in understanding objects and achieving user-controllable manipulation, such as interpreting user instructions and determining the corresponding object parts for manipulation actions. To address these challenges, the authors propose a learning-based framework called Command-driven articulation modeling (Cart), which combines object structure understanding, shape manipulation, and user-friendly commands. Cart utilizes structured templates, model segmentation prediction, and test-time state adaptation (TTSA) algorithm to improve the quality of object manipulation. The authors extensively evaluate Cart on articulated objects datasets and demonstrate its superior predictive performance and applicability to real-world scenarios. The contributions of this work include proposing a command-driven shape manipulation task, presenting the Cart framework, and showcasing its effectiveness in understanding object articulation and performing object manipulation.