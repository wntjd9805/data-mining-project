Animating humans with a novel view and expression sequence from a single image has various creative applications in computer science, such as talking head synthesis, augmented and virtual reality, image manipulation, and data augmentation for training deep models. Previous approaches to image animation used either 2D-based image generation models or 3D parametric models, but these suffered from artifacts, 3D inconsistencies, or unrealistic visuals. Neural Radiance Fields (NeRF) have emerged as a breakthrough approach for generating high-quality images of a scene in novel views. However, the original NeRF models only synthesized images of static scenes and required extensive multi-view data for training, limiting their application to novel view synthesis from a single image. NeRF-GANs have improved upon this by generating multi-view face images with single-shot data, even with controllable expressions. However, it is still challenging to generate 3D-consistent and identity-preserving images of real faces. Existing inversion methods either suffer from identity gaps or introduce artifacts. In this paper, we propose NeRFInvertor, a universal inversion method for NeRF-GAN models that achieves high-fidelity, 3D-consistent, and identity-preserving animation of real subjects from a single image. Our method fine-tunes the generator of NeRF-GANs using image space supervision and regularizations to overcome the limitations of existing inversion methods. Through experiments, we validate the effectiveness of our method in animating real face images with realistic, high-fidelity, and 3D consistent results. Our contributions include the proposed universal inversion method, the introduction of a novel geometric constraint using in-domain samples, and the demonstration of our method's effectiveness across different NeRF-GAN models and datasets.