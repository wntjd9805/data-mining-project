Image-text matching is a crucial task for multi-modal learning applications like cross-modal retrieval and text-to-image synthesis. The challenge lies in accurately and efficiently learning cross-modal embeddings and their similarities. Existing methods can be categorized into embedding-based and score-based matching approaches. Embedding-based methods have gained popularity due to their accuracy and efficiency. However, these methods only focus on fragment-level relation modeling and local feature interaction within one sample. They overlook instance-level relation modeling and global embedding interaction among different samples and modalities. This paper proposes a Hierarchical Relation Modeling framework (HREM) that captures both fragment-level and instance-level relations to learn holistic embeddings jointly. HREM incorporates a cross-embedding association graph and two relation interaction mechanisms to enhance embeddings. The proposed framework outperforms state-of-the-art methods for image-text retrieval on benchmarks such as Flickr30K and MS-COCO.