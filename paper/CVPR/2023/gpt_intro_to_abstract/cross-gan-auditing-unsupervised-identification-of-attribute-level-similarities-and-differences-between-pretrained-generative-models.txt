Generative Adversarial Networks (GANs) have become widely used in various commercial and scientific applications. As the use of GANs continues to grow, there is a need for investigative tools that can evaluate and differentiate different GAN models. Currently, the evaluation of GAN models is limited to comparing them against the dataset they were trained on using summary metrics. However, in real-world scenarios, comparing models trained on different datasets is not feasible using these summary metrics. This paper proposes a framework called cross-GAN auditing (xGA) that allows for the comparison of GAN models trained on different datasets. xGA identifies attribute similarities and differences between client GAN models and reference models. The audit identifies three sets of attributes: common attributes present in both models, novel attributes only encoded in the client model, and missing attributes present only in the reference model. xGA utilizes shared attributes to identify common attributes and leverages out-of-distribution attribute manipulations to discover novel and missing attributes. The framework is evaluated using various StyleGAN models and benchmark datasets, demonstrating its effectiveness in characterizing generative models. This paper presents the first cross-GAN auditing framework, introduces novel metrics for evaluating attribute-based GAN auditing approaches, and provides a suite of controlled experiments to evaluate cross-GAN auditing methods.