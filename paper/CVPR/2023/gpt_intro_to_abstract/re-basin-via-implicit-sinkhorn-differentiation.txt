Despite the success of deep learning (DL) in various application domains, the loss surfaces of neural networks (NNs) are not well understood. The number of saddle points and local optima can exponentially increase with the number of parameters, even for shallow NNs. The permutation symmetry of neurons in each layer allows the same function to be represented with different parameter values, leading to multiple minima in the loss landscape. Previous studies have shown that minima found by Stochastic Gradient Descent (SGD) can be linearly connected with no loss detriment, known as linear mode connectivity (LMC). This suggests that by considering permutation invariance, solutions can be teleported into a single basin with minimal loss barriers. However, efficiently searching for permutation symmetries to bring all solutions to one basin is challenging. Several approaches for matching units between NNs have been explored, including activation-based matching and weight alignment. However, these methods are either non-differentiable or computationally expensive, limiting their applicability to different objectives or domains. In this work, we propose a differentiable solution for the re-basin problem by relaxing the permutation matrix with the Sinkhorn operator. We use implicit differentiation to compute gradients efficiently, allowing for the optimization of any differentiable objective as a loss function. Our re-basin formulation has applications in model merging and incremental learning. We present a new continual learning algorithm that combines models trained on different domains, leveraging the LMC property observed in SGD-based solutions. Our contributions include solving the re-basin problem using implicit Sinkhorn differentiation, enabling better differentiable solutions, a powerful approach for incremental learning based on the Sinkhorn operator, and extensive experiments to validate our method's effectiveness in finding optimal permutations, achieving linear mode connectivity, and learning new domains while retaining knowledge from previous tasks.