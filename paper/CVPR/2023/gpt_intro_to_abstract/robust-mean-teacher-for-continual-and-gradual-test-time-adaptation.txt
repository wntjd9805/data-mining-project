Deep neural networks have achieved impressive performance in various tasks when training and test data come from the same distribution. However, in real-world scenarios, this assumption is frequently violated due to non-stationary environments. This can result in a drop in performance when a data shift occurs during test time. To address this issue, domain generalization aims to improve the robustness and generalization ability of models during training. However, existing approaches are often limited because they do not consider the wide range of potential unknown data shifts during training. Recent approaches have leveraged test samples encountered during model deployment to adapt the pre-trained model, a technique known as test-time adaptation (TTA). TTA can be performed either offline or online. While offline TTA assumes access to all test data at once, online TTA adapts the model on the fly using only the current test batch. Adapting batch normalization statistics during test-time is one effective method, but more sophisticated techniques update the model weights using self-training based approaches. However, most TTA methods have only been demonstrated for a single domain shift, which is unlikely in real-world applications. Continual test-time adaptation has been introduced to address this limitation, where the model is adapted to a sequence of domain shifts. However, adapting the model to long test sequences in non-stationary environments is challenging due to error accumulation. Resetting the model after each update prevents exploiting previously acquired knowledge, which is undesirable in gradual test-time adaptation settings. To overcome these challenges, this paper introduces a robust mean teacher (RMT) approach that utilizes a symmetric cross-entropy (SCE) loss for self-training instead of the commonly used cross-entropy (CE) loss. RMT also incorporates a multi-viewed contrastive loss to learn invariances in the input space. The framework performs well for both continual and gradual domain shifts, especially for easy-to-hard problems. Additionally, the paper addresses the source-free setting, where source data might not be available during test-time, and proposes an extension that considers the accessibility of source data. The contributions of this paper include motivating the use of SCE loss for TTA, presenting a framework for continual and gradual TTA that achieves state-of-the-art results on existing benchmarks, and addressing practical requirements such as the source-free setting and availability of source data.