Video object segmentation, also known as mask tracking, aims to segment the target object in a video given the annotation of the reference frame. However, acquiring dense annotations, particularly at the pixel level, is laborious. In this work, we investigate the feasibility of training a VOS model on sparsely annotated videos. We introduce the concept of N-shot, where N frames are annotated per training video. We evaluate a 2-shot STCN model on the combination of YouTube-VOS and DAVIS datasets and compare it with the native 2-shot STCN. Surprisingly, the 2-shot STCN still achieves decent results, demonstrating the practicality of 2-shot VOS. We also adopt the learning paradigm of semi-supervised learning to promote 2-shot VOS. The idea is to generate credible pseudo labels for unlabeled frames during training and optimize the model on the combination of labeled and pseudo-labeled data. We propose a two-phase approach where the first phase uses ground-truth for the first frame and the second phase lifts the restriction on the starting frame, allowing it to be either a labeled or pseudo-labeled frame. We store the generated pseudo labels in a pseudo-label bank and update them as training progresses. Our approach achieves similar results to its full set counterpart while accessing only a small amount of labeled data. Our contributions include demonstrating the feasibility of two-shot VOS, presenting a training paradigm to exploit information in unlabeled frames, and achieving competitive results with limited labeled data.