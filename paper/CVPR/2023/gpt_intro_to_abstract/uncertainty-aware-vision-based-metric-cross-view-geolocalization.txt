Autonomous driving systems require both a model of the vehicle's environment and its location relative to the model. These systems can either construct the model during runtime or create parts of it prior to runtime. Constructing high-definition maps in advance facilitates accurate localization but is expensive. Online methods create a model of the local environment in real-time using live sensor readings from lidar and/or cameras. Aerial images offer the potential to combine the benefits of both approaches, being affordable, globally available, and up-to-date. In this paper, we focus on matching sensor measurements of the vehicle against aerial images to determine its location and geo-location. Previous research in this area focuses on large search regions but lacks metric accuracy. We propose a transformer model that constructs a bird's eye view map of the vehicle environment and matches it with an aerial image to predict the vehicle's pose. Our model outperforms previous approaches and achieves high localization accuracy without the need for training data from the target region or fine-tuning. We provide multiple datasets and improved ground truth poses for evaluation and publish the source code online. Our contributions include a novel end-to-end trainable model, dataset collection and filtering, and improved performance in challenging cross-area and cross-vehicle settings.