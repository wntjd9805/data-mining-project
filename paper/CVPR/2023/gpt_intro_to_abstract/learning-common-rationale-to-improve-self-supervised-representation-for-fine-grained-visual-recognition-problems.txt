Self-supervised representation learning (SSL) has been proven effective for transferring learned representations to different downstream tasks. However, recent studies have highlighted a "coarse-grained bias" in SSL, which makes it less effective for fine-grained visual recognition (FGVR) tasks that require distinguishing visually similar subcategories of objects. This bias is rooted in the training objective of SSL, which focuses on minimizing a pretext task. Fine-grained discriminative patterns often reside on key object parts, making the features learned from SSL irrelevant for FGVR. Existing methods address this issue by incorporating pre-trained object part or saliency detectors, but these methods have limitations. In this paper, we propose a new approach that directly solves the problem using target domain data. We introduce an additional screening mechanism that filters out patterns irrelevant to FGVR in addition to the contrastive learning process of SSL. Surprisingly, we find that this screening mechanism can be learned from the GradCAM of the SSL loss through a simple "fitting and masking" procedure. Our method significantly improves the quality of learned features, as demonstrated by retrieval tasks and achieving state-of-the-art performance on FGVR.