Today's entertainment industry is investing heavily in content creation tasks, particularly in games and animated movies. However, the process of creating photos/videos that meet multiple constraints is time-consuming and requires manual labor. Conditional image generation methods, such as Stable Diffusion, Dall.E-2, and Imagen, have been developed to create photorealistic images using text prompts. These methods involve sampling points from a multi-dimensional space, which becomes more restricted as the number of conditions increases. Previous works have used generative models like VAEs and GANs to solve the conditional generation problem, but most of these methods only consider one constraint. GAN-based methods tend to outperform VAE-based methods in terms of image generation quality. Different strategies have been proposed for conditioning GANs, including text conditional GANs and image-level conditional GANs. One major challenge in training generative models for multimodal image synthesis is the lack of paired data containing multiple modalities. Most existing models restrict themselves to one or two modalities, while few works use more than two domain variant modalities. Recently, diffusion models have shown superior performance in image generation tasks due to their ability to perform exact sampling from complex distributions. These models offer flexibility in image manipulation and have been used for various low-level vision tasks. In this paper, we propose a solution to multimodal image generation problems using de-noising diffusion probabilistic models. We leverage the flexible property of diffusion models to design a scalable approach that does not require retraining with paired data across all modalities. We compare our proposed method with existing approaches and demonstrate its effectiveness in various applications. The main contributions of this paper are the diffusion-based solution for image generation under multimodal priors, the utilization of the flexible property of diffusion models to tackle the lack of paired data, and the scalability of our method by incorporating off-the-shelf models for additional constraints.