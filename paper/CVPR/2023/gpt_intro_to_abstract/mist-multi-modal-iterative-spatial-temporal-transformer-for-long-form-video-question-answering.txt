Video Question Answering (VideoQA) systems have the potential to assist users in solving everyday problems by understanding and seeking answers from long-form videos. However, as the duration of the video increases, unique challenges arise. These challenges include multi-event reasoning, where the systems need to perform complex temporal reasoning, and interactions among different granularities of visual concepts. Current vision-language methods excel at QA over images or short clips, but do not perform as well on long-form videos. In this paper, we propose a model called MIST (Modular Inference for long-form VideoQA) that tackles these challenges by decomposing dense joint spatial-temporal self-attention into a question-conditioned cascade segment and region selection module, along with spatial-temporal self-attention over multi-modal multi-grained features. Our experimental results on several VideoQA datasets with longer videos show that MIST achieves state-of-the-art performance, provides higher efficiency, and offers reasonable evidence for answering questions.