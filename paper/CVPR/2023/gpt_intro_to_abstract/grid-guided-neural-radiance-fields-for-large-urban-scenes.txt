Large urban scene modeling has gained significant attention in computer science research, particularly with the emergence of neural radiance fields (NeRF). NeRF-based methods have shown impressive results in modeling object-level scenes, but often struggle to capture fine details and complexity in large scenes. Grid-based representations offer faster rendering and scalability, but lack continuity compared to NeRF-based methods. In this paper, we propose a two-branch model architecture that integrates both grid-based and NeRF-based approaches. Our approach leverages the complementary strengths of each representation, using feature grids to fit local scene content and NeRF to provide global continuity and capture high-frequency details. We first pre-train the target scene with a feature grid to capture coarse geometry and appearance, which is then used to guide NeRF's point sampling and positional encodings. This allows NeRF to effectively capture finer details in a compressed sampling space. The feature grids are further optimized with gradients from the NeRF branch, resulting in more accurate rendering results. To reduce memory footprint, we adopt a compact factorization of the 3D feature grid, separating ground feature planes and a vertically shared feature vector. This reduces memory and enforces disentanglement of the learned feature grid, providing explicit and informative scene layouts. Our unified model and scene representation are shown to be effective through extensive experiments. Users can choose to use either the grid branch for faster rendering or the NeRF branch for more high-frequency details and spatial smoothness. Overall, our approach offers a comprehensive solution for reconstructing large urban scenes with implicit neural representations.