Image-to-video generation, or the synthesis of a realistic video given a single image and a specific condition, has garnered significant interest due to its potential applications in areas such as art generation, entertainment, and data augmentation for machine learning. Existing conditional image-to-video (cI2V) generation methods typically struggle with preserving both spatial details and temporal coherence in the generated frames. To address this issue, we propose a novel approach called latent flow diffusion models (LFDM), which synthesize a temporally-coherent flow sequence in the latent space to warp the given image based on the condition. Our LFDM model utilizes a two-stage training strategy, which includes training a latent flow auto-encoder in the first stage and a conditional 3D U-Net based diffusion model in the second stage. This disentangled training process facilitates easy adaptation to new domains. We conducted extensive experiments on multiple datasets, including videos of facial expression, human action, and gesture, demonstrating that our LFDM consistently outperforms previous state-of-the-art methods. Overall, our contributions in this paper include the proposal of LFDM, the development of a two-stage training strategy, and the achievement of superior performance compared to existing methods.