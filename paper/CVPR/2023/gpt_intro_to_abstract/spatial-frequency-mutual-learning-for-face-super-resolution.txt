Face super-resolution (FSR) is a technology that enhances the resolution of low-quality face images by generating corresponding high-resolution versions. These low-quality images negatively impact downstream tasks such as face recognition and face attribute analysis. Traditional FSR methods have employed techniques such as PCA and Bayesian approaches to improve the quality of face images, but they have limited representation abilities. The emergence of deep learning has revolutionized FSR research, with researchers developing various network frameworks to transform low-resolution face images to high-resolution ones.While existing FSR methods have improved performance, they still have limitations. Convolutional neural networks have a limited receptive field, making it challenging to model global facial structure. Transformer models have shown promise in achieving larger receptive fields, but they require extensive training data and computational resources. Furthermore, partition strategies may disrupt the structure of facial images.To address these limitations, this paper proposes a novel spatial-frequency mutual network (SFMNet) for FSR. SFMNet incorporates both spatial and frequency domains by using Fourier transform to decompose the image into amplitude and phase components. The frequency branch captures global facial structure, while the spatial branch extracts local facial features. A frequency-spatial interaction block (FSIB) is designed to fuse frequency and spatial information, enhancing the representation ability of the model. Additionally, a GAN-based model with spatial and frequency discriminators is developed to guide learning in both domains.The contributions of this work are three-fold. First, it introduces a spatial-frequency mutual network that explores the potential of spatial and frequency information for face super-resolution. Second, it incorporates a frequency-spatial interaction block to effectively utilize complementary information from both domains. Finally, experimental results on benchmark datasets demonstrate the superiority of the proposed method in terms of visual quality and quantitative metrics.