In this paper, we introduce the concept of VQ-VAE models, which utilize a discrete codebook of latent embeddings to reconstruct images. These models have gained popularity due to their scalability and versatility in various visual tasks. However, achieving a higher compression rate in VAE models often compromises the accuracy of image reconstruction. We identify several challenges that contribute to this gap, including spectral bias, neglect of alignment in the frequency domain, and the difficulty of reconstructing images from a single codebook embedding. To address these challenges, we propose the Frequency Augmented VAE (FA-VAE) model, which aims to improve reconstruction quality by aligning the frequency spectrums between the original and reconstructed images. We introduce Frequency Complement Modules (FCMs) that complement the decoder's features with missing frequencies. To guide the FCMs, we propose a Spectrum Loss (SL) that prioritizes learning lower-frequency features. However, we observe checkerboard patterns in the complemented decoder's features with SL. To overcome this issue, we introduce a Dynamic Spectrum Loss (DSL) that dynamically adjusts the range of low-frequency spectrum for optimal reconstruction. Additionally, we extend FA-VAE to the text-to-image generation task and propose the Cross-attention Autoregressive Transformer (CAT) model, which uses all token embeddings as a condition for more precise guidance. We conclude by highlighting the contributions of our work, including the introduction of FA-VAE, SL, DSL, and CAT.