Recently, vision-based perception methods have gained popularity in autonomous driving due to their higher signal-to-noise ratio and lower cost compared to LIDAR-based methods. Among these methods, Bird's-Eye-View (BEV) perception has become the mainstream approach. The BEV representation offers several advantages, including easier integration with other modalities, simplification of subsequent module development and deployment, and overcoming scale and occlusion difficulties. The performance of BEV perception relies on acquiring road and object feature representations effectively. Two existing categories of BEV perception pipelines are the late-interaction and middle-interaction pipelines. However, transforming pixel panel views to BEV using these strategies remains challenging. To address this challenge, we propose a novel paradigm: the early-interaction method, which offers distinct advantages compared to the existing strategies. Our proposed method integrates global contextual information and local details, allowing for the delivery of richer semantic information to the BEV space. Additionally, our method ensures that the information flow occurs bidirectionally, aligning features in the pixel panel view and BEV. We present a framework called BAE-Former that effectively aggregates multi-scale image features into a better BEV feature representation for map-view semantic segmentation. Our experiments on nuScenes and Lyft datasets demonstrate the efficiency and effectiveness of our proposed method, achieving state-of-the-art performance at real-time inference speed on BEV semantic segmentation.