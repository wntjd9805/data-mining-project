Deep learning has achieved great success in computer vision tasks when training and test data are sampled from the same distribution. However, in real-world applications, performance degradation usually occurs when training and test data are collected from different distributions, resulting in domain shift. Many works have been proposed to adapt the model at test time to address this problem. Test time adaptation (TTA) is a method that updates the parameters of the model during the test phase using only the pre-trained source model and unlabeled target data. In this paper, we address TTA as a representation revision problem. We propose to rectify the feature representations for the target domain by focusing on feature alignment and uniformity. We introduce the concepts of test time feature uniformity and test time feature alignment to improve the representations for the target domain. Test time feature uniformity aims to distribute representations of test images from different classes as uniformly as possible, while test time feature alignment encourages similar images to have similar representations. We propose Test Time Self-Distillation (TSD) to achieve feature uniformity and Memorized Spatial Local Clustering (MSLC) to achieve feature alignment. Additionally, we introduce the entropy filter and consistency filter to filter noisy labels and improve performance. Our experiments on domain generalization benchmarks and medical image segmentation benchmarks demonstrate that our proposed method outperforms existing approaches.