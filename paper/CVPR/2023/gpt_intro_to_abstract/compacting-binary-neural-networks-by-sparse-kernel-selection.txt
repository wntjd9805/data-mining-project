Deep Neural Networks (DNNs) are widely used in various applications but their large size and resource requirements make them unsuitable for deployment on resource-constrained embedded devices. Network quantization, specifically Binary Neural Networks (BNNs), has emerged as a promising approach to alleviate this issue by reducing memory footprint and inference speed. However, little attention has been given to the distribution and organization of the learnt binary kernels in BNNs. This paper proposes a method that exploits the clustering property of kernels in BNNs during training. By formulating the binary quantization process as a grouping task, the method selects the nearest codeword from a binary sub-codebook for each kernel. The problem is then converted to a permutation learning task using the Gumbel-Sinkhorn operation to generate a continuous approximation of the permutation matrix. The paper also introduces the Permutation Straight-Through Estimator (PSTE), which tunes the approximated permutation matrix while maintaining the binary property of the selected codewords. Experimental results on image classification and object detection demonstrate that the proposed method significantly reduces model size and computational burden. The method achieves better compression efficiency compared to existing techniques and enables the use of deeper and wider architectures while staying within the complexity budget of BNNs. The paper also highlights the differences between the proposed method and other approaches in terms of codeword representation and optimization. Overall, the proposed method improves the expressivity and performance of BNNs, making them more suitable for resource-constrained embedded devices.