Recently, there has been significant research on visually-grounded dialog systems, which are important in various real-world applications such as assisting visually impaired individuals. One popular testbed for studying these systems is Visual Dialog (VisDial), where a dialog agent must answer a sequence of image-grounded questions. Most previous approaches have trained dialog agents solely on VisDial data using supervised learning, while more recent studies have employed self-supervised pre-trained models such as BERT or ViLBERT and finetuned them on VisDial data. However, there is a need to expand the knowledge of dialog agents beyond what can be acquired through supervised learning or self-supervised pre-training. In this paper, we propose using semi-supervised learning (SSL) to address this challenge. We consider SSL for VisDial by generating synthetic conversations for unlabeled images and training the dialog agent with the synthetic data. However, there are two critical challenges in this approach: the complexity of the target output for VisDial and the presence of noise in the synthetic dialog datasets. To tackle these challenges, we introduce a new learning strategy called Generative Self-Training (GST), which generates multi-turn visual QA data and utilizes synthetic data for training. We also propose perplexity-based data selection and multimodal consistency regularization to effectively train the dialog agent with the noisy dialog data. Our experiments show that GST achieves state-of-the-art performance on the VisDial dataset, even when the human-annotated visual dialog data is scarce. Moreover, GST demonstrates robustness against visual and textual adversarial attacks. Overall, our contributions include introducing GST, achieving improved performance on the VisDial dataset, and validating the robustness of GST against adversarial attacks.