This paper introduces pointersect, a novel method for rendering point clouds. While point clouds are a common representation of surfaces, they are difficult to render directly as they do not occupy volume. Existing methods solve this by assigning volume-occupying shapes to each point, but determining the ideal shape for rendering is challenging. This paper proposes a solution that allows for direct ray tracing of point clouds by training a neural network to provide the necessary surface information for rendering. The proposed method takes into account the SE(3) equivariance of the problem and utilizes a transformer to learn the set function, ensuring invariance to the order of provided points. By restricting the input to nearby points, the method can be trained on a small number of meshes and applied to unseen point clouds. Experimental results demonstrate the effectiveness of pointersect in various applications, including scene rendering, relighting, inverse rendering, and ray tracing. Additionally, the method is applied to Lidar-scanned point clouds for novel-view synthesis and scene editing. The paper concludes with a call to examine the supplemental material and website for further results and demonstration videos.