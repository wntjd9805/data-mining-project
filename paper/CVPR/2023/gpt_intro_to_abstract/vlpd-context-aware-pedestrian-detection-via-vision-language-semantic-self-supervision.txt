This paper introduces a novel approach, called Vision-Language semantic self-supervision for Pedestrian Detection (VLPD), to address the challenges faced by pedestrian detection in urban scenarios. The authors observe that previous methods have not adequately investigated the contexts in urban scenarios, leading to unsatisfactory performance. To tackle this, they propose a vision-language semantic segmentation method (VLS) to model explicit semantic contexts. This method uses pseudo labels and cross-modal mapping to recognize the semantic classes without any annotations. The authors also propose a Prototypical Semantic Contrastive (PSC) learning method to better discriminate pedestrians and contexts. This method aggregates negative and positive prototypes to improve the detection's discrimination power. The integration of VLS and PSC in the VLPD approach achieves superior performance compared to previous methods on popular benchmarks, particularly on challenging subsets with small scale and occlusion. The contributions of this paper include the novel VLS method for explicit context modeling and the PSC method for better discrimination, leading to improved pedestrian detection performance in urban scenarios.