This paper introduces a novel framework called Text-grounded Contrastive Learning (TCL) for open-world semantic segmentation. Open-world semantic segmentation aims to identify arbitrary semantic concepts in the open world, beyond pre-defined categories. Existing methods tackle the challenge of learning arbitrary concepts by exploiting web-crawled image-text paired data, but the challenge of achieving precise localization without dense annotations remains. TCL addresses this challenge by reformulating the contrastive loss to be directly affected by the segmentation quality, enabling end-to-end training and improving region-text level alignment. The paper also presents a unified evaluation protocol and achieves state-of-the-art zero-shot segmentation performance on 8 benchmark datasets. The contributions of this work include the introduction of TCL, a novel framework for open-world segmentation, a unified evaluation protocol, and achieving new state-of-the-art performance compared to existing methods.