In this paper, we address the challenges of automatic font generation for vector fonts, particularly in the context of Chinese writing systems. While deep learning-based methods have shown promising results in vector font generation, existing approaches still suffer from distortions and suboptimal outlines. To overcome these issues, we propose DeepVecFont-v2, a Transformer-based encoder-decoder architecture that generates high-fidelity vector glyphs with compact and coordinated outlines. We introduce a relaxation representation for SVGs that separates the starting and ending points of drawing commands and merges them through an additional constraint. We also sample auxiliary points along BÂ´ezier curves to improve data alignment, and incorporate a self-refinement module to remove artifacts. Experimental results demonstrate the superiority of our method in generating complex and diverse vector fonts, as well as its ability to synthesize longer sequences. Our contributions include the development of a Transformer-based generative model, a relaxation representation for vector outlines, the introduction of auxiliary points for alignment, and a context-based self-refinement module. Extensive experiments validate the effectiveness of our approach in both English and Chinese font generation.