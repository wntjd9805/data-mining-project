Active 3D depth sensors are widely used in various applications such as augmented reality, navigation, and robotics. These sensors, like Lidar and Kinect V1, require vision algorithms to process the acquired data for tasks like 3D semantic understanding and object tracking. However, active depth sensors have some limitations, including high power consumption, limited sensing distance, and potential safety concerns due to strong light sources. In this paper, we propose an adaptive sensing method that addresses these limitations by redistributing the light energy and projecting the pattern only to the required regions. We demonstrate the advantages of our approach through theoretical analysis and implement two hardware prototypes. Experimental results confirm that our adaptive sensor outperforms existing sensing strategies in terms of power consumption, sensing distance, and eye-safety.