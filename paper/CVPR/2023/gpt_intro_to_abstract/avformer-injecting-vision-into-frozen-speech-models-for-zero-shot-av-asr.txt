Robustness in automatic speech recognition (ASR) systems is crucial for adapting to new and unconstrained domains. In multimodal video, the visual stream can provide valuable cues to enhance the robustness of ASR systems, especially when the audio is noisy. While existing works focus on lip motion, this research investigates the contribution of entire visual frames, which is particularly useful for videos in real-world situations where the mouth may not always be visible. However, building audiovisual datasets for training AV-ASR models is challenging, as existing datasets are small and require large models with both visual and audio encoders.On the other hand, there are large-scale audio-only models that have shown strong generalization across domains. This research aims to leverage the expertise and training time invested in these models by reusing their weights. Inspired by works that adapt frozen foundation models for multimodal tasks, such as injecting visual information into language models, this study seeks to do the same for AV-ASR using strong audio-only models. The proposed framework, called AVFormer, adds visual inputs to frozen ASR models using lightweight projection layers and trainable adaptors.The AVFormer model is trained on a small amount of weakly labeled video data with minimal additional training time and parameters. This approach aims to minimize domain shift and catastrophic forgetting that can occur during end-to-end fine-tuning. To ensure stability during adapter fine-tuning, a curriculum scheme is introduced during training. Experimental results demonstrate that the AVFormer model outperforms existing zero-shot methods on multiple AV-ASR benchmarks across different domains, while also maintaining decent performance on traditional audio-only speech recognition benchmarks.In conclusion, this research addresses the challenge of robustness in ASR systems by exploring the use of visual cues from entire frames in multimodal video. By leveraging existing audio-only models, the proposed AVFormer framework achieves improved performance in zero-shot AV-ASR, while preserving performance on audio-only speech recognition tasks.