Semantic segmentation is a fundamental task in computer vision that involves classifying each pixel in an image. Traditional semantic segmentation networks are trained on datasets where labels for all classes are available simultaneously. However, in practical applications, there is a need for networks to continuously learn new classes while old class labels are unavailable due to privacy or legal reasons. This problem, known as Class-Incremental Semantic Segmentation, leads to catastrophic forgetting if the old model is fine-tuned directly on new data. Existing methods for this task use fully-convolutional networks as the basic framework and employ knowledge distillation to preserve old knowledge. However, there are limitations to these methods, including the difficulty in capturing global context information and the inefficiency of adding convolutional layers for new classes in the decoder. Additionally, current knowledge distillation methods do not distinguish between old and new class regions, limiting the model's ability to learn discriminative features for new classes. To address these problems, we propose a new transformer-based framework called Incrementer, which utilizes a vision transformer as the encoder to extract global contextual features and employs a transformer decoder with class tokens for each class to generate segmentations. Our method only requires adding new class tokens for incremental learning, improving efficiency. We also propose a novel knowledge distillation scheme that focuses on the distillation of old-class features and a class deconfusion strategy to balance the learning of old and new classes. Experimental results on Pascal VOC and ADE20k datasets demonstrate that our Incrementer outperforms state-of-the-art methods in terms of accuracy.