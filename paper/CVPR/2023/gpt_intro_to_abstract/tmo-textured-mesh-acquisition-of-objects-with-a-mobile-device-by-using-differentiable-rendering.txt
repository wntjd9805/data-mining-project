The paper discusses the challenge of recovering the 3D geometry of objects and scenes in computer vision. It highlights the range of depth sensing technologies available, from specialized 3D scanners to smartphone-based LiDAR scanners. The use of smartphones, such as the iPhone 13 Pro, with their various sensors, including the LiDAR scanner, RGB camera, and visual-inertial odometry systems, offers the potential for high-resolution imaging and 3D capture. However, depth maps generated by smartphones are often noisy and have limited range, making them unsuitable for reconstructing objects with complex shapes. The paper proposes a practical method for capturing high-quality textured meshes of 3D objects using a smartphone, combining techniques such as Structure from Motion and multi-view stereo algorithms. The authors introduce a neural implicit representation for surface reconstruction and a new training method to improve performance. The paper also discusses the use of differential rendering to fine-tune texture maps and compares the proposed method to classical approaches in 3D reconstruction and texture mapping. The contributions of the paper include a unified framework for smartphone-based reconstruction, a depth filtering scheme, and the use of neural geometry representation for complex shape reconstruction and high-fidelity texture generation. The proposed method is evaluated using data collected from a smartphone application.