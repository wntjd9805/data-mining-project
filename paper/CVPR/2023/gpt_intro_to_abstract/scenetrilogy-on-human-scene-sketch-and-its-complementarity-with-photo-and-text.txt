Scene understanding is a fundamental aspect of computer vision. While object-level research has made significant progress, recent years have seen a shift towards scene-level tasks such as scene recognition, scene captioning, scene synthesis, and scene retrieval. Scene research has evolved from single modality approaches to multi-modality settings, particularly focusing on text and photo modalities. However, the inclusion of human scene-sketch as a complementary modality has not been extensively explored. In this paper, we investigate the usefulness of sketch in multi-modal scene understanding through two pilot studies: one comparing expressiveness between text and sketch for scene image retrieval, and another exploring subjectivity through a novel task of subjective captioning. The results demonstrate that sketch provides complementary information and offers more subjectivity compared to text. To fully exploit the complementarity of all three modalities, we propose a flexible joint embedding that supports optionality across modalities and tasks. This joint embedding is achieved through a three-way disentanglement approach, where each modality is separated into modality-specific and modality-agnostic components. We introduce a cross-attention mechanism to capture synergy across the modality-agnostic components and enable optionality across modalities. Our contributions include the inclusion of human scene-sketch in multi-modal scene understanding, the development of a flexible joint embedding that supports optionality, and the demonstration of various scene-related tasks that can be performed using the learned embedding.