Tracking by natural language specification is a task that aims to locate a target in a sequence of frames based on the specified state described in natural language. Compared to traditional tracking methods using bounding boxes, natural language specification provides a novel human-machine interaction approach for visual tracking. It offers advantages in terms of describing long-term target variations and providing clear semantics for assisting the tracker. However, this tracking approach has not been fully explored yet. Existing solutions for this task typically involve two steps: localizing the target based on the natural language description in the first frame, and tracking the localized target in subsequent frames. These solutions often treat the grounding and tracking steps as separate parts, lacking the connection between them. Additionally, many solutions use pre-trained models and fail to train the overall framework end-to-end. To address these limitations, we propose a joint visual grounding and tracking framework that unifies these two tasks and can be trained end-to-end. Our framework leverages a transformer-based multi-source relation modeling module to effectively model the relations between the input references and the test image, considering both the cross-modality and cross-time relations. We also introduce a semantics-guided temporal modeling module to improve the adaptability of our method to target appearance variations. Experimental results on multiple tracking and grounding datasets demonstrate the effectiveness of our approach.