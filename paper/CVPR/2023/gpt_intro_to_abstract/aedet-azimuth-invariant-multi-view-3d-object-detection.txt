In the field of autonomous driving, multi-view 3D object detection has been extensively researched due to its low assembly cost and high efficiency. Previous methods have relied on transferring image features to a Bird-Eye-View (BEV) perspective and processing them using convolutional backbones and detection heads. However, these methods ignore the inherent properties of BEV features and suffer from two limitations. First, the BEV representation of the same object in different azimuths is inconsistent due to the use of regular sampling grids that destroy the radial symmetry of BEV features. Second, the prediction targets for the same object in different azimuths are inconsistent, requiring the detection head to predict different targets. To address these limitations, we propose an Azimuth-equivariant Detector (AeDet) that models radial symmetry using an azimuth-equivariant convolution and an azimuth-equivariant anchor. We also introduce a camera-decoupled virtual depth to improve depth prediction. Our experiments show that AeDet significantly improves the accuracy of object orientation and velocity, outperforming recent multi-view object detectors.