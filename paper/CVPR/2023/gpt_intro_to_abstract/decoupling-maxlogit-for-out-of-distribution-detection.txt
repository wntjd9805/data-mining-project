This paper addresses the challenge of out-of-distribution (OOD) detection in machine learning models. The closed-world assumption, where all classes in the training phase are also present in the test phase, is not always applicable in real-world scenarios. OOD data, which does not belong to any training classes, poses a challenge for trustworthy model predictions. OOD detection algorithms rely on scoring functions to distinguish OOD samples. While logit-based methods like MaxLogit and MSP have been used, they are not state-of-the-art. This paper proposes a new scoring method called Decoupling MaxLogit (DML) that separates the cosine similarity and feature norm components of the logit-based method. DML overcomes the limitations of MaxLogit and achieves comparable performance with other state-of-the-art methods. The paper also provides insights into the key components that make these scoring methods effective, including the use of a cosine classifier and different training losses. These insights greatly improve the performance of existing OOD scoring methods. The proposed DML method is further enhanced with DML+ by changing the standard training. The effectiveness of DML+ is demonstrated through significant improvements in performance on CIFAR and ImageNet datasets.