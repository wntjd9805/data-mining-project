Deep neural networks (DNNs) have shown exceptional performance in computer vision tasks but are vulnerable to input perturbations, resulting in a decrease in accuracy. This trade-off between natural accuracy and adversarial robustness has been attributed to the existence of two types of features: moderately correlated and robust features, and weakly correlated and non-robust features. Efforts to improve this trade-off have been limited in their effectiveness, particularly on large-scale datasets like ImageNet. Recently, Vision Transformers (ViTs) have emerged as a promising alternative to CNNs, outperforming them in various tasks. However, existing works on ViTs predominantly focus on natural accuracy and lack attention to robustness improvement. Moreover, the robustness of naturally pretrained ViTs and ways to obtain a useful and reliable ViT have been overlooked. In this paper, we propose a method called TORA-ViT to enhance the robustness of pretrained ViTs by optimizing the accuracy and robustness adapters simultaneously and introducing a gated fusion module. Our experiments on ImageNet and various robust benchmarks demonstrate that TORA-ViTs effectively improve robustness while maintaining competitive natural accuracy. The most balanced setting of TORA-ViT achieves 83.7% accuracy on clean ImageNet and 54.7% accuracy under white-box attacks, as well as significant improvements on ImageNet variants.