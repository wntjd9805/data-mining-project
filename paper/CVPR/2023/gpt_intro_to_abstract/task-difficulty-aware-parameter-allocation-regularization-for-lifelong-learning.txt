This paper introduces the concept of lifelong learning in neural networks and discusses the issue of catastrophic forgetting, where the model's performance on previously learned tasks deteriorates rapidly after learning new ones. The authors propose a difficulty-aware method called Parameter Allocation & Regularization (PAR) to address this problem. PAR adapts its strategy based on the learning difficulty of each task, dividing tasks into groups and assigning dedicated expert models to each group. If a new task is related to existing groups, it is added to the corresponding group and learned via parameter regularization. Otherwise, a new group and expert are allocated for the task. The paper also discusses the challenges of measuring relatedness between tasks and the parameter explosion associated with allocation. To address these challenges, the authors propose a divergence estimation method based on prototype distance and an efficiency-improved architecture search. The contributions of this work include the PAR framework, the divergence estimation method, and the architecture search approach. Experimental results demonstrate the scalability and effectiveness of PAR in reducing model redundancy and improving performance. Ablation studies and visualizations further validate the components and reasoning behind PAR.