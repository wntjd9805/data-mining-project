Deep learning has made significant progress in various tasks by training large capacity networks on massive amounts of data. However, applying high-parameterized models in real-world scenarios with limited resources can be challenging. Knowledge distillation (KD) techniques aim to compress large-scale teacher models into compact and effective student models. One approach, online knowledge distillation (OKD), treats all networks as students and allows mutual learning from scratch. Existing OKD methods focus on enhancing students' generalization by designing sophisticated architectures, but they lack explicit constraints on generalization.In this paper, we propose a concise and effective OKD framework called online knowledge distillation with parameter hybridization (OKDPH) to promote flatter loss minima and achieve higher generalization. We use the theory of multi-model fusion to estimate the local curvature of the loss landscape by linearly combining the parameters of multiple students. This hybrid-weight model (HWM) reflects the upper and lower bounds of the loss region and guides students to converge in a more stable direction.To ensure the effectiveness of HWM, we restrict the differences between students through intermittent fusion operations. This process shortens the distance between students and pulls them in the same direction, while still maintaining diversity through different data augmentations. Our method obtains a lightweight parameter that performs well in various scenarios, integrating dark knowledge from multiple models while maintaining a compact architecture.Our contributions include the innovation of the HWM, which estimates the curvature of the loss landscape and measures generalization explicitly. We propose the OKDPH framework, which flexibly adapts to various network architectures without modifying peers' structures. Extensive experiments demonstrate that OKDPH greatly improves students' generalization compared to state-of-the-art OKD methods. Loss landscape visualization and stability analysis further confirm the robustness and low loss of our solution.In conclusion, our OKDPH framework effectively promotes flatter loss minima and higher generalization in OKD. It surpasses existing methods and can be applied in resource-constrained applications.