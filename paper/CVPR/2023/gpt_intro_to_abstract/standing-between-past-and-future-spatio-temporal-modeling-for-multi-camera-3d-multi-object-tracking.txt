This paper addresses the problem of object trajectory reasoning in 3D for autonomous navigation. While LiDAR-based approaches are limited by cost and reliability, the use of cameras alone to detect, track, and forecast object trajectories is critical. However, these tasks have traditionally been studied in isolation and combined in an ad-hoc manner. While 3D detection has received much attention, associating these detections over time has been mostly done independently. End-to-end detection and tracking approaches operate on neighboring frames and fail to integrate longer-term spatio-temporal cues. This paper argues that jointly optimizing the detection-tracking-prediction pipeline can significantly improve multi-object tracking, especially in a camera-based system. The proposed framework, named PF-Track, combines past and future reasoning for joint 3D object detection, tracking, and trajectory prediction. The framework leverages attention operations to capture object dynamics and interactions for track refinement and robust trajectory prediction. The evaluation of PF-Track on the nuScenes dataset demonstrates clear benefits in terms of decreased ID-Switches tracking compared to previous multi-camera 3D MOT methods. The contributions of this paper include an end-to-end framework for vision-only 3D MOT that utilizes object-level spatio-temporal reasoning, improved track quality through cross-attending to past features, a joint tracking and prediction pipeline, and establishment of new state-of-the-art results on the nuScenes dataset.