In recent years, self-supervised pre-training has achieved remarkable success in natural language processing (NLP), leading to numerous attempts to apply this approach in computer vision. Self-supervised pre-training involves learning a general visual representation by designing pretext tasks that do not require manual annotation. This approach has proven to be effective in high-level vision tasks but has not been well-established for low-level vision tasks due to their distinct characteristics and annotation methods. Existing pre-training strategies for low-level vision tasks lack generality and only provide marginal improvements. In this paper, we propose a novel pretraining paradigm tailored to low-level vision called the degradation autoencoder (DegAE). DegAE aims to disentangle and generate content-degradation representations by transferring the degradation of a reference image to an input image. We demonstrate that DegAE pretraining significantly improves the performance of representative backbone models on various low-level vision tasks, including image dehaze, derain, and motion deblur. Our approach bridges the gap between high-level and low-level vision tasks, potentially enhancing the performance of a wide range of low-level vision applications.