Indoor scene reconstruction and navigation is a crucial component for many computer science applications. One of the most challenging tasks in this field is the free-view-synthesis (FVS), which involves synthesizing high-quality views of a scene from any perspective, including both similar and significantly different views. However, FVS faces several difficulties, such as low-texture areas, complex geometry, illumination changes, and view imbalance. While existing techniques like NeRF and neural reconstruction methods have their strengths, they also have limitations in terms of interpolation and extrapolation abilities. In this paper, we propose a novel method that combines the strengths of NeRF and neural reconstruction techniques to improve the FVS performance for indoor scenes. We address the challenges of depth error and distribution ambiguity using robust depth loss and variance regularization techniques, respectively. Additionally, we adjust the weights of these losses based on view coverage sufficiency to mitigate the impact of view imbalance. Our experimental results on synthetic and real-world datasets demonstrate that our method significantly outperforms existing view synthesis methods, achieving high-fidelity extrapolation. Our contribution includes a novel approach for neural radiance fields to perform free-view synthesis, a robust depth loss to handle inaccuracies in reconstructed geometry, and a flexible variance loss with view coverage adjustment to improve rendering quality in low-texture and few-shot regions.