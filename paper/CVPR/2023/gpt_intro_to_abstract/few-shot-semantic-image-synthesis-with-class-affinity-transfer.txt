Image synthesis with deep generative models, such as GANs and VAEs, has made significant progress in recent years. These models can generate images conditioned on various inputs, such as class labels, text, bounding boxes, or seed images. In particular, semantic image synthesis, where images are generated based on a semantic map indicating desired class labels for each pixel, has been explored extensively. However, these approaches heavily rely on large datasets with pixel-precise label maps, which are costly to acquire. This high annotation cost motivates the development of transfer learning strategies to alleviate the annotation requirements. While transfer learning has been widely studied for classification tasks, it has received less attention in the context of image generation tasks. In this paper, we introduce CAT (Class Affinity Transfer), a finetuning procedure that leverages prior knowledge to establish pairwise relations between source and target classes and encodes them in a class affinity matrix. This matrix is used to make the source model compatible with the label space of the target domain, enabling further finetuning using the available target data. We integrate our transfer learning strategy into state-of-the-art adversarial and diffusion models and conduct experiments on multiple datasets, demonstrating that our approach improves over existing transfer methods and enables realistic synthesis from as few as 100 target images. Our contributions include the introduction of CAT, exploration of different methods to define class affinity, integration with state-of-the-art models, and excellent experimental transfer results.