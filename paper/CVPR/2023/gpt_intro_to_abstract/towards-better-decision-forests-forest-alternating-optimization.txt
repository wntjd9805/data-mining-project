Decision tree ensembles, also known as forests, have been widely recognized as highly accurate machine learning models for various tasks. They have been used in practical applications and consistently top the leaderboards in ML competitions. While forests require some hyperparameter tuning for optimal performance, this process is much easier compared to other models like neural networks. However, the training algorithm for forests seems outdated compared to recent numerical optimization techniques used in ML. Instead of optimizing a parametric model over a loss function and regularization terms, forests rely on two building blocks: learning individual trees using a greedy partitioning procedure and creating the ensemble using techniques like bagging and boosting. These building blocks do not define a global objective function and optimize it, resulting in larger models than necessary. The success of forests has been attributed to concepts like the diversity of base learners, but the underlying reasons remain unclear. The goal of this paper is not to explain why current forest algorithms work, but to apply solid optimization principles to learn forests and achieve even better performance. The authors propose a Forest Alternating Optimization (FAO) algorithm that can optimize a global objective function of all the parameters of a predetermined forest structure. This algorithm, which builds upon the Tree Alternating Optimization (TAO) algorithm, enables the selection of a loss function, regularization, and a parametric form for the forest, leading to improved accuracy and reduced forest size. Experimental results show that averaging multiple independent FAO forests can prevent overfitting and further enhance accuracy.