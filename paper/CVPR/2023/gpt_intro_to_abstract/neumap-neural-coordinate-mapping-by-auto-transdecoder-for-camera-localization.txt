Visual localization, which determines camera position and orientation based on image observations, is crucial for applications like VR/AR and self-driving cars. However, accurate visual localization remains challenging, particularly in the presence of large viewpoint and illumination changes. Additionally, the use of massive databases for keypoints with 3D coordinates and visual features poses a bottleneck in real-world applications. Existing visual localization techniques can be categorized into Feature Matching (FM) and Scene Coordinate Regression (SCR) methods. While FM methods achieve robust localization by leveraging learning-based feature extraction and correspondence matching, they require large maps, making them impractical for large-scale scenes. On the other hand, SCR methods regress a dense scene coordinate map but struggle with generalization and large viewpoint and illumination changes. In this paper, we propose a method called NeuMap that combines the benefits of compact scene representation from SCR methods and robust performance from FM methods. NeuMap uses a sparse set of robustly learned features to handle viewpoint and illumination changes and regresses the 3D scene coordinates using a compact map representation. We demonstrate the effectiveness of NeuMap on small and large-scale benchmarks, achieving significant compression without performance drop in small-scale datasets and outperforming state-of-the-art methods in large-scale datasets. Additionally, we showcase the ability of NeuMap to quickly adapt to new scenes through fine-tuning experiments.