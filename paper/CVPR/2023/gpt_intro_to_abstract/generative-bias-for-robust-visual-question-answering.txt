Visual Question Answering (VQA) is a challenging task that requires a model to understand and predict answers given an image and a question. However, VQA models tend to rely heavily on biases within the dataset and often predict similar answers regardless of the image. To address this issue, recent works have developed bias reduction techniques, including ensemble based methods. These methods introduce additional models to learn biases within each modality or dataset. However, existing ensemble based methods have limitations in representing biases and often rely on pre-computed label statistics or single modal branches. In this paper, we propose a novel stochastic bias model that directly learns the bias from the target model. We model the bias model as a Generative Adversarial Network (GAN) to mimic the target model's answer distribution. We use questions as the main bias modality and employ knowledge distillation to ensure the bias model closely resembles the target model. Our final bias model outperforms previous ensemble based debiasing methods by a significant margin. We conduct extensive experiments on various VQA datasets and architectures to demonstrate the effectiveness and robustness of our method. Our contributions include the proposal of GenB, a novel bias model for ensemble based debiasing, and the achievement of state-of-the-art performance on multiple VQA datasets without the need for external human annotations or dataset reshuffling.