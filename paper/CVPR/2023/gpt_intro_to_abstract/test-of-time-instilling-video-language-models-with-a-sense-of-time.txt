Self-supervised pretraining on multimodal web corpora has led to foundational models for images and videos, resulting in significant advancements in video-language tasks. However, it remains unclear if these models capture essential temporal properties of videos. Previous studies have suggested that existing video-language models lack time awareness, but it is uncertain whether this is due to the models themselves or the benchmarks used. In this paper, we investigate the time awareness of video-language models by examining their ability to understand temporal relations. We design a synthetic dataset to probe models for time understanding and find that existing models struggle to associate the time order across video and language. To address this issue, we propose TACT, a method for temporal adaptation that instills time awareness in video-language models without retraining from scratch. TACT utilizes artificial samples and a modified contrastive loss to learn time order consistency. We demonstrate the effectiveness of TACT in connecting time order in video and language on diverse real datasets. Additionally, we evaluate the TACT-adapted model on downstream tasks requiring different levels of time awareness and show improved zero-shot generalizability. Overall, our contributions include highlighting the lack of time awareness in existing video-language models, introducing the TACT method for temporal adaptation, and demonstrating its effectiveness in improving models' time awareness and zero-shot generalization capabilities.