The introduction of this computer science paper discusses the development of sixth-generation (6G) wireless networks that are designed to support sensing at the physical layer. This departure from previous networks aims to provide applications with sensing capabilities alongside communication functions. The dense cellular deployments in urban settings would enable improved radio coverage, allowing for various perception tasks such as obstacle detection and localization for autonomous driving, pedestrians, and drones. The paper focuses on the challenge of training perception models for radio signals, as they are difficult to label manually. The authors propose a self-supervised radio-visual learning approach, using cross-modal learning with vision to automatically generate radio self-labels and train a downstream localizer network. The paper presents a synthetic dataset and a contrastive radio-visual objective for label-free radio localization, along with evaluations that demonstrate the superior performance of their algorithm compared to state-of-the-art methods. The authors hope to inspire further research in this new cross-modal learning paradigm.