In recent years, there have been significant advancements in human-centric perceptions, including pose estimation, pedestrian detection, and person re-identification. These advancements have greatly contributed to the applications of visual models in various fields such as sports analysis, autonomous driving, and electronic retailing. While these different human-centric tasks have their own specific focus, they share a common structure of the human body and its attributes. This has led to attempts to train a shared neural network for multiple human-centric tasks. For example, human parsing has been trained alongside human keypoint detection, pedestrian attribute recognition, pedestrian detection, and person re-identification. These experiments have shown that training different human-centric tasks together can benefit each other. Building on these findings, this paper presents a unified model for human-centric perceptions (UniHCP) that combines five distinct tasks: pose estimation, semantic part segmentation, pedestrian detection, person re-identification, and person attribute recognition. UniHCP uses a simple and scalable architecture based on the vision transformer. It employs a shared encoder-decoder network to handle the diversity of input data and utilizes task-specific queries and a task-guided interpreter to generate different outputs for each task. By avoiding task-specific output heads, UniHCP minimizes task-specific parameters and allows for the reuse of backbone-encoded features across tasks. The model is pre-trained on a large collection of labeled human-centric datasets, and experimental results show that it achieves competitive performance compared to task-specialized models. Overall, UniHCP provides a unified solution for handling multiple human-centric tasks simultaneously, with the potential for fast adaptation to new tasks and reduced memory cost in large-scale multitask systems.