Dataset Distillation (DD) or Dataset Condensation is a popular research topic in the field of computer science. It aims to reduce the training cost by generating a small but informative synthetic set of training examples that can produce similar performance to models trained on the original, large-scale dataset. DD has been explored in various contexts, including federated learning, continual learning, neural architecture search, medical computing, and graph neural networks. However, the recursive computation involved in DD hinders its application to real-world large-scale model training, which involves thousands to millions of gradient descent steps. Several methods have been proposed to improve DD by introducing different techniques such as ridge regression loss and trajectory matching loss. However, the dataset distillation process itself is still expensive and time-consuming. Prior work on reducing the distillation cost has resulted in regression from state-of-the-art performance. In this paper, the authors propose a method to speed up the dataset distillation process while improving the testing performance. They focus on optimizing the synthetic set specifically for a targeted network, rather than generalization across different networks. The authors explore two research questions: how to design the candidate pool of models to learn synthetic data, and whether it is possible to learn a good synthetic set using only a few models. They propose using early-stage models and parameter perturbation as model augmentation techniques to accelerate the training speed of dataset distillation. The method achieves significant speedup and comparable performance to state-of-the-art DD methods.