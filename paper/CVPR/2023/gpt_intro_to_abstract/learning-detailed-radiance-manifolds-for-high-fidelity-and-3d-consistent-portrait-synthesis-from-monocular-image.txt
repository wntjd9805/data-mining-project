Synthesizing photorealistic portrait images from different viewpoints is crucial for applications like virtual avatar creation and online communication. While 2D Generative Adversarial Networks (GANs) have made it possible to generate high-quality portraits using monocular images, they still suffer from limitations such as multiview inconsistencies. 3D-aware GANs incorporating Neural Radiance Field (NeRF) have shown promising results in maintaining 3D consistency but face challenges in editing real image poses. Current methods rely on time-consuming optimization or encoder-based inversion, which may sacrifice fine details. In this paper, we propose GRAMInverter, an approach for high-fidelity and 3D-consistent novel view synthesis of monocular portraits. Our method builds upon the GRAM framework but addresses fidelity issues by combining the radiance manifolds with learned high-resolution detail manifolds. We introduce a detail manifolds reconstructor that employs manifold super-resolution to extract high-resolution details. Additionally, we replace the MLP-based radiance generator in GRAM with a more efficient StyleGAN2-based tri-plane generator. Experimental results demonstrate that GRAMInverter achieves superior pose control and 3D consistency while preserving fine details, outperforming previous methods. We believe our approach contributes to efficient 3D-aware content creation for real-world applications.