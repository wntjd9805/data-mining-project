Diffusion models have emerged recently as powerful deep generative models for high-quality image generation. They have found applications in various domains such as text-to-image generation, image-to-image generation, video generation, speech synthesis, and 3D synthesis. The backbone of diffusion models plays a central role in their development, with the CNN-based U-Net being a representative example. However, vision transformers (ViT) have shown promise in various vision tasks and raised the question of whether the reliance on CNN-based U-Net is necessary in diffusion models. In this paper, we propose a ViT-based architecture called U-ViT that treats all inputs, including time, condition, and noisy image patches, as tokens. Inspired by U-Net, U-ViT employs long skip connections between shallow and deep layers to capture low-level features important for pixel-level prediction in diffusion models. We also optionally add a 3x3 convolutional block before the output for better visual quality. We evaluate U-ViT in three popular tasks: unconditional image generation, class-conditional image generation, and text-to-image generation. Our results show that U-ViT performs comparably, if not superior, to a CNN-based U-Net of similar size in all tasks.Latent diffusion models with U-ViT achieve record-breaking scores in class-conditional image generation and text-to-image generation tasks, without using large external datasets during training. Our findings suggest that long skip connections are crucial for image diffusion models, while down/up-sampling operators in CNN-based U-Net are not always necessary. We believe that U-ViT can provide valuable insights for future research on diffusion model backbones and benefit generative modeling on large-scale cross-modality datasets.