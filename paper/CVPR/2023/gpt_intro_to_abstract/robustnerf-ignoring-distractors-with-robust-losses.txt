The ability to understand the structure of a 3D scene from 2D images is crucial in computer vision and has applications in various fields such as AR/VR, robotics, and photogrammetry. Neural fields have revolutionized this task by storing 3D representations within the weights of a neural network. These representations, known as Neural Radiance Fields (NeRF), can capture 3D scenes with photo-realistic accuracy. However, training NeRF models requires accurate camera calibration and assumes photometric consistency in the observed images. Distractors, anything that isn't persistent throughout the capture session, can negatively impact the quality of NeRF reconstructions. Existing approaches to handle distractors have limitations, such as the need for manual labeling or not generalizing to unexpected distractors. In this paper, we propose a method that models distractors as outliers in the NeRF optimization process. We analyze existing techniques, design a simplified and effective method, and evaluate its performance quantitatively and qualitatively on synthetic and real-world datasets. Our approach requires minimal hyper-parameter tuning and achieves state-of-the-art results.