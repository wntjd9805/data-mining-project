This paper addresses the problem of Few-shot Referring Relationship in videos, where the objective is to spatially and temporally localize subjects and objects that are related via a given predicate within a video. This problem has potential applications in cross-task video understanding and video retrieval. While referring relationships in images have been studied, referring relationships in videos pose additional challenges, such as understanding dynamic visual relationships. This paper aims to fill this research gap by proposing a solution for Few-shot Referring Relationship in videos. The paper formulates the problem as the minimization of an objective function defined over a T-partite random field, where T is the number of frames in the test video. The random field represents candidate bounding boxes for the subject and objects, and the objective function consists of frame-level potentials and visual relationship similarity potentials. The authors meta-train a relation network using a support set of videos and use belief propagation-based message passing to localize subject and object trajectories. The proposed approach outperforms related baselines on two public benchmarks. The contributions of this work include the novel problem setup, the formulation based on a random field, and aggregation techniques for enhanced relationship representations.