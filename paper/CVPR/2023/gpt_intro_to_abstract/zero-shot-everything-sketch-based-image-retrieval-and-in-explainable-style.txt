Abstract:Zero-shot sketch-based image retrieval (ZS-SBIR) is a challenging problem in sketch understanding due to the scarcity of human sketches compared to photos. Previous research efforts have mainly focused on ZS-SBIR to make sketch-based retrieval commercially viable. However, existing approaches often rely on external knowledge and lack explainability. In this paper, we propose a single model that can handle cross-category adaptation, ditch the external knowledge requirement, and provide explanations for its performance. Our approach is based on establishing local patch correspondences between sketches and photos and computing a distance score based on these correspondences. We introduce a transformer-based cross-modal network with novel designs for patch localization, patch-to-patch correspondence establishment, and matching score computation. Our approach achieves state-of-the-art performance in various ZS-SBIR settings and offers explainability through visualization of patch correspondences and synthesis of sketches to photos.