Image super-resolution (SR) is a fundamental problem in computer vision that aims to recover high-resolution (HR) images from degraded low-resolution (LR) inputs. In recent years, vision transformer (ViT) based frameworks have shown significant performance gains compared to convolutional neural networks (CNNs) in SR tasks. However, most existing techniques focus on improving large-scale ViT models, while the development of lightweight ViTs (less than 1M parameters) remains challenging. This paper addresses the limitations of lightweight ViT-based models and proposes a novel omni-dimension feature aggregation scheme called Omni Self-Attention (OSA). OSA enables comprehensive information propagation and interaction by incorporating both spatial and channel axis information in a simultaneous manner. The proposed method achieves higher-order receptive field information and improves contextual aggregation capabilities. Additionally, a multi-scale hierarchical aggregation block called Omni-Scale Aggregation Group (OSAG) is introduced to tailor the encoding of varying scales of texture patterns. OSAG includes local convolution, meso self-attention, and global self-attention, providing an omni-scale feature extraction capability. The proposed framework, called Omni-SR, achieves state-of-the-art performance in lightweight super-resolution tasks, with superior optimization properties and robustness compared to existing ViT-based frameworks. Extensive experiments demonstrate the effectiveness of the proposed approach on mainstream image super-resolution datasets.