This paper introduces the concept of 3D visual grounding (3D VG), which aims to identify target objects in point cloud scenes by analyzing descriptive query language. The authors highlight the challenges of 3D VG due to the sparseness of point clouds and the diversity of language descriptions. They discuss existing progress in improving point cloud features extraction, generating discriminative object candidates, and identifying spatial relationships. However, they identify two unexplored issues: imbalance and ambiguity. Imbalance occurs when object names exclude other properties, leading to imbalanced learning. Ambiguity arises when utterances refer to multiple objects or attributes, but the model's objective is to identify only the main object. To address these issues, the authors propose a decoupled and explicit strategy that parses text into semantic components and performs dense alignment to achieve fine-grained feature matching. Additionally, they introduce a challenging new task called grounding without object names (VG-w/o-ON) to test the model's comprehension without relying on names. The paper concludes with the contributions of the proposed text decoupling module, the supervised fine-grained visual-language feature fusion, and the achievement of state-of-the-art performance on two datasets.