We propose a method called Masked Image Consistency (MIC) to enhance unsupervised domain adaptation (UDA) for visual recognition tasks. UDA aims to adapt a network trained on a source dataset to perform well on a target dataset. However, UDA methods still struggle with classes that have similar visual appearances on the target domain due to the lack of ground truth supervision. To address this problem, MIC leverages spatial context relations as additional clues for robust visual recognition. We design a plug-in that masks out random patches of target images and trains the network to predict the semantic segmentation result of the entire image, including the masked-out parts. Pseudo-labels generated by an EMA teacher, which utilizes both context and local clues, are used as the training supervision. By masking out different parts of objects during training, the network learns to utilize various context clues, thereby improving robustness. MIC achieves significant and consistent performance improvements for different UDA methods and visual recognition tasks, setting a new state-of-the-art performance across multiple benchmarks. It can be easily integrated into various UDA methods, making it highly valuable in practice.