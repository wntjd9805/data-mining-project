Domain Adaptation (DA) is a machine learning scenario where the training and test data come from two related but distinct domains. Unsupervised DA (UDA) has been extensively studied, where the target domain lacks labeled data. In contrast, Semi-Supervised Domain Adaptation (SSDA) allows access to a few target labels, making it more applicable in real-world settings. The traditional strategy for SSDA, known as S+T, trains a model using source data and labeled target data with cross-entropy loss. However, this approach suffers from domain shift, leading to misalignment between the source and target distributions. To address this issue, recent algorithms explore different strategies to align the target distribution with the source distribution. SSL algorithms, such as entropy minimization, pseudo-labeling, and consistency regularization, have been applied in SSDA. However, these algorithms require the target data to closely match semantically similar source data in the feature space. If misalignment occurs, it becomes challenging to recover. To address this problem, we propose a source-adaptive paradigm, called the Source Label Adaptation (SLA) framework, which modifies the original source labels. We demonstrate the usefulness of the SLA framework when coupled with state-of-the-art SSDA algorithms by significantly improving their performance on benchmark datasets. This approach opens up a new direction for solving DA problems.