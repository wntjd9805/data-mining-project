Recent advancements in image synthesis and editing have been achieved through the use of diffusion models. These models have the ability to condition on various modalities such as texts, segmentation masks, and sketches. However, current explorations in this field are limited to using a single modality at a time, thus restricting the model's controllability. To address this issue, we propose Collaborative Diffusion, a framework that combines pre-trained uni-modal diffusion models for multi-modal face generation and editing without the need for re-training. The framework utilizes lateral connections between models driven by different modalities to leverage their complementary nature. We introduce the dynamic diffuser, which adaptsively predicts the spatial-temporal influence function for each pre-trained model, allowing for the suppression of irrelevant modalities and the enhancement of admissible modalities. Our framework not only enables multi-modal synthesis but also extends to multi-modal face editing with minimal modifications. We demonstrate the efficacy of our approach through qualitative and quantitative evaluations, showing superior image quality and condition consistency in both synthesis and editing tasks. Our contributions include the introduction of the Collaborative Diffusion framework, the proposal of the dynamic diffuser for selective modulation of modalities, and the extension of the framework to multi-modal face editing. Overall, our method provides a flexible and efficient approach to multi-modal image generation and editing.