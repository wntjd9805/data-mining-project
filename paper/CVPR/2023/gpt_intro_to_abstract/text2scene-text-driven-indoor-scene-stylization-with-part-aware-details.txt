Virtual spaces are becoming increasingly popular for metaverse, films, and games, creating a demand for realistic and high-quality 3D scenes with textures. Currently, manual creation of 3D assets and textures by skilled artists is not scalable enough to meet the diverse content needs of the industry. While existing methods utilize 3D database models or data-driven shape generation approaches, they often lack texture information or are limited to simple coloring. The challenge lies in adding consistent style to an entire scene while accounting for the boundaries of different materials. In this paper, we propose Text2Scene, a method that adds plausible texture details to 3D scenes without explicit part labels or large-scale data with complex texturing. Inspired by 3D shape and image datasets, we decompose the problem into sub-parts and process the entire scene using commodity memory and computation. We handle walls and individual objects separately, formulating the stylization of walls as texture retrieval and initializing objects with base colors. We then deduce the part-level relationship for stylization and refine it, ensuring the rendered images align with the input text within the joint embedding space of foundational models.Our approach follows a coarse-to-fine strategy, generating high-quality textures with clean part boundaries. We create segments of the input mesh aligned with low-level geometric cues and assign base colors per segment. Interestingly, large-scale image datasets assign similar colors to parts with similar textures, reflecting semantic context or symmetry. We add detailed texture to individual objects as additional perturbations on the assigned base colors, enforcing constraints on the image features of their projections.In summary, Text2Scene is a new method that can generate realistic scene textures with desired styles provided by text or an image. It adds detailed texture respecting the semantic part boundaries of individual objects, and can process the entire scene without a large amount of textured 3D scenes or extensive memory footprint. We believe this approach will enable everyday users to easily populate virtual scenes and experience next-generation technology with high-quality visual renderings.