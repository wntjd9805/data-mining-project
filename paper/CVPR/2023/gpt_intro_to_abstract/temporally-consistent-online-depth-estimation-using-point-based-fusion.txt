Depth reconstruction is a key problem in computer vision, with stereo matching and structure-from-motion being popular techniques. However, learning-based approaches have recently shown the best results. While the overall quality of reconstruction has improved, new areas of focus include monocular estimation, edge quality, and temporal consistency. Inconsistent depth can cause flickering and swimming artifacts in video applications. Video depth estimation remains challenging due to unpredictable errors and imperfections, especially in textureless and specular regions. Online reconstruction adds further complexity, as future frames are unknown and temporal consistency must be balanced with error correction. Dynamic objects also pose a challenge. Existing approaches attempt to address these issues through assumptions, recurrent architectures, or conditioning, but ultimately rely on the consistency of neural network output, which is difficult to achieve due to camera noise and aliasing. In this work, we propose the use of a global point cloud to enhance temporal consistency in online video depth estimation. We present a method to handle dynamic objects and update a potentially erroneous point cloud when future frames are not known. Our approach significantly improves temporal consistency without sacrificing spatial quality, as demonstrated through quantitative and qualitative results. Our contributions include proposing point cloud-based fusion, presenting a three-stage approach for consistency in online settings, and introducing a lightweight, image-space approach for dynamics estimation and depth fusion.