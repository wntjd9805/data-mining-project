Artistic style transfer is a technique that involves applying the style patterns of a reference image to a content image while preserving the content's semantic structure. The development of arbitrary style transfer methods has allowed for real-time style transfer using any style image without the need for model-specific training. However, the introduction of the Transformer model in arbitrary style transfer has led to a significant increase in model parameters, making training computationally expensive. In this paper, we propose a novel Transformer architecture specifically designed for artistic style transfer, aiming to alleviate the computational burden while maintaining stylization quality. We address the content distortion problem in Transformers and introduce learnable scale parameters as a solution. Additionally, we introduce a meta learning framework called Meta Style Transformer (Master) that allows for text-guided few-shot style transfer, reducing the training burden of previous per-text-per-model solutions. Experimental results demonstrate that our proposed model outperforms arbitrary-style-per-model methods and achieves comparable performance to per-style-per-model methods with significantly lower training cost.