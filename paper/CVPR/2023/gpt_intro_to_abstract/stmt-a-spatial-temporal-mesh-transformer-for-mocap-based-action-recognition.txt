Motion Capture (MoCap) is a widely used technique for digitally recording human movement in 3D space. It has applications in various research fields such as action recognition, tracking, pose estimation, imitation learning, and motion synthesis. MoCap is also important for enhancing human-robot interactions in practical scenarios. Skeleton representations have commonly been used to model MoCap sequences, but they have limitations such as sample variance and different numbers of body markers in different datasets. To address these limitations, a mesh-based action recognition method is proposed in this paper. However, classifying high-dimensional mesh sequences into different actions poses challenges due to the lack of vertex-level correspondence and the need for global understanding in the spatial-temporal domain. To overcome these challenges, the paper introduces a novel Spatial-Temporal Mesh Transformer (STMT) that leverages mesh connectivity information and uses a hierarchical transformer architecture to learn spatial-temporal associations. The model is further enhanced with masked vertex modeling and future frame prediction tasks to improve global interactions among vertex patches. The proposed model achieves state-of-the-art performance compared to skeleton-based and point-cloud-based models when evaluated on common MoCap benchmark datasets. The contributions of the paper include the introduction of a hierarchical transformer architecture for spatial-temporal mesh modeling, the design of effective and efficient pretext tasks for learning spatial-temporal global context, and the achievement of superior performance on MoCap benchmarks.