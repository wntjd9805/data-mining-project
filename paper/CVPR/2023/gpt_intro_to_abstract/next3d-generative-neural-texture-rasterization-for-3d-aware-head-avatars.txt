Animatable portrait synthesis is a crucial aspect in various applications like movie post-production, visual effects, augmented reality (AR), and virtual reality (VR) telepresence. The challenge lies in accurately modeling deformation and preserving identity while generating diverse high-fidelity portraits with full control over head pose, facial expressions, and gaze directions. Existing 2D generative models have limitations in shape distortion during large motion, and thus, incorporating 3D Morphable Face Models (3DMM) has been explored for better view consistency. However, these methods struggle with topological changes caused by non-facial components. This paper proposes a novel 3D GAN framework for unsupervised learning of generative, high-quality, and 3D-consistent facial avatars from unstructured 2D images. The model splits the head into dynamic and static parts and utilizes a novel representation called Generative Texture-Rasterized Triplanes for fine-grained expression control and flexibility of implicit volumetric representation. The method also addresses the lack of mouth interior modeling in 3DMM by introducing an efficient teeth synthesis module. Additionally, a deformation-aware discriminator is used to improve alignment with expected deformations. The contributions of this work include the presentation of an animatable 3D-aware GAN framework, the introduction of Generative Texture-Rasterized Triplanes as an efficient deformable 3D representation, and the potential to serve as a 3D prior for downstream applications of 3D-aware facial avatars. The model also expands the frontier of 3D stylization with high-quality out-of-domain facial avatars.