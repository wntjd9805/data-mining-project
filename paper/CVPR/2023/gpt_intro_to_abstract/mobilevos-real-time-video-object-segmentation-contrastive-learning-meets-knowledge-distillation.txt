Video object segmentation (VOS) is a crucial task in computer vision, with applications in video editing, autonomous driving, surveillance, and augmented reality. Semi-supervised VOS (SVOS), where only the initial frame is annotated, is particularly challenging due to the need to model motion and appearance changes under occlusion and drifts. SVOS approaches can be categorized into online fine-tuning, object flow, and offline matching, with memory-based matching methods showing promising results. However, these methods often suffer from a trade-off between memory usage and accuracy. In this paper, we address the problem of real-time SVOS on resource-constrained devices, specifically focusing on memory-based networks. We propose a novel approach that bridges the gap between infinite memory and finite memory networks using knowledge distillation. We introduce a pixel-wise representation distillation loss to transfer structural information from a large teacher network to a smaller student network. We also propose a boundary-aware sampling strategy to improve training convergence. Furthermore, we extend our approach to include a supervised pixel-wise contrastive representation objective. Experimental results on popular SVOS datasets demonstrate that our method is competitive with state-of-the-art while running significantly faster and having fewer parameters. Our model achieves real-time performance on mobile devices without complex architectural or memory design changes.