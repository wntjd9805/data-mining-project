Image segmentation plays a vital role in medical image analysis, and recent advancements in computer vision and deep learning have achieved expert-level performance. However, collecting and annotating training data for supervised machine learning methods is challenging due to the long-tailed distribution of real-world medical images. This distribution includes outliers that are difficult to train reliable models on. In safety-critical clinical applications, it is crucial for medical image segmentation models to detect and localize out-of-distribution (OOD) conditions. Previous studies on OOD localization have focused on finding lesions apart from normal cases or simulating OOD conditions, but the complex nature of real-world clinical scenarios makes it challenging to establish a direct relationship between image pixels and tumor types. To address this, we propose MaxQuery, a medical image semantic segmentation framework that extends Mask Transformers to localize OOD targets. We use learnable object queries to fit inlier cluster centers and detect OOD conditions based on the negative affinity with the cluster centers. We also introduce a query-distribution (QD) loss to improve the diversity of cluster assignments for OOD localization. We curate two real-world medical image datasets for pancreatic and liver tumors and demonstrate the robust performance of our framework in both segmentation and OOD localization tasks. Our proposed method outperforms previous OOD localization methods and improves upon inlier segmentation performance. Overall, our approach shows strong potential for clinical application and contributes to the field of near-OOD detection and localization in medical image segmentation.