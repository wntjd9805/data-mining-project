Adversarial examples, which can fool deep neural networks (DNNs) while being indistinguishable to human observers, pose significant security threats to machine learning systems. Understanding the origin of these adversarial examples is crucial in order to improve the reliability and safety of DNNs. Previous works have focused on analyzing associations between adversarial examples and target labels, but there is still a lack of consensus on the underlying causes of adversarial vulnerability. To address this, we propose the use of instrumental variable (IV) regression, a causal inference approach, to uncover the causal features that affect adversarial robustness. We introduce a data generating process (DGP) that models the existence of unknown confounders and utilizes an instrument to eliminate bias and estimate true causal relations. Our approach, called adversarial IV regression, combines a hypothesis model and a test function in a zero-sum optimization game to estimate causal features on adversarial examples. Through extensive analyses, we demonstrate that these estimated causal features are highly related to correct predictions for adversarial robustness. We also provide interpretability and discuss the potential of using these causal features to improve the robustness of defense networks against adversarial attacks.