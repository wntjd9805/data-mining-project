Person Re-Identification (ReID) is a computer vision task that involves matching person images in a query set to ones in a gallery set captured by non-overlapping cameras. This task has gained significant attention in various applications such as video surveillance, security, and persons analysis. Many existing ReID approaches primarily focus on visible-modality retrieval, which may not perform well under poor illumination conditions. To address this limitation, infrared cameras are commonly used in surveillance systems. However, matching infrared images to visible ones for ReID poses additional challenges due to inter-modality variation.In recent years, Visible-Infrared person Re-IDentification (VI-ReID) has been proposed to address the intra- and inter-modality variations between visible and infrared images. These approaches extract person representations from both modalities and learn a modality-invariant feature representation to eliminate the inter-modality variation. However, these global feature representations tend to focus only on the most discriminative part, ignoring the diverse parts that can help distinguish person identities.To enhance the discriminative power of person representations in VI-ReID, recent approaches have explored capturing diverse human body parts across different modalities. However, learning part detectors still leads to overfitting to specific parts and accumulates errors in the subsequent inter-modality alignment process, hindering generalization ability on unseen identities.On the other hand, data augmentation techniques that employ image mixture have been widely used to enlarge the training set and reduce overfitting. However, applying such techniques to part-based VI-ReID models is challenging due to limitations in generating ambiguous and unnatural patterns or local patches with only background or single human parts.In this paper, we propose a novel data augmentation technique called PartMix for VI-ReID. PartMix synthesizes part-aware augmented samples by mixing part descriptors. We randomly mix inter- and intra-modality part descriptors to generate positive and negative samples within the same and across different identities, and regularize the model through contrastive learning. An entropy-based mining strategy is also presented to mitigate the impact of unreliable samples.Experiments on several benchmarks demonstrate the effectiveness of our method. We also conduct an extensive ablation study to validate and analyze the components of our model. Overall, our proposed PartMix technique offers a promising solution for enhancing VI-ReID performance by leveraging part descriptors and data augmentation.