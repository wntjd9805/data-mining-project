This paper addresses the issue of performance degradation in deep learning methods when there is a distribution shift between training and test data. Unsupervised domain adaptation (UDA) is a common approach to narrow the distribution gap by utilizing unlabeled target domain data during training. However, UDA methods cannot guarantee performance on unknown target domains. This paper focuses on domain generalization (DG), which aims to learn a model from observed source domains that performs well on unseen target domains without re-training. Many DG methods aim to learn domain-invariant representations across source domains. Existing DG works primarily use convolutional neural networks (CNNs), but CNNs have a tendency to overfit source domains and perform poorly on unseen target domains due to their bias towards local representations. To tackle this, some approaches propose using transformer or MLP-like models, which can learn global representations with attention mechanisms. However, few studies have analyzed how the difference between MLP and CNN architectures affects model generalization in DG tasks. Additionally, these methods have high computational complexity and excessive network parameters, limiting their practical applications. In this paper, the authors investigate the generalization ability of MLP methods in the DG task and propose a lightweight MLP-based framework for DG. They analyze the differences between MLP and CNN methods using frequency analysis and observe that MLP methods are better at capturing global structure information, resulting in superior generalization to unseen target domains. The proposed framework utilizes learnable filters to remove structure-irrelevant information and a novel dynamic low-frequency spectrum transform (ALOFT) to suppress domain-specific features. Experimental results on standard domain generalization benchmarks demonstrate the effectiveness of the proposed method, achieving significant improvements with a small-sized network compared to state-of-the-art DG methods.