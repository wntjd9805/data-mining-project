This paper introduces MobileR2L, a real-time neural rendering model designed for mobile devices. The goal is to enable real-time interaction with users by leveraging the advancements in neural rendering techniques. The paper highlights the limitations of existing methods, such as slow rendering speed and high storage requirements, which restrict their applicability on mobile devices. To address these challenges, MobileR2L adopts a distillation procedure and utilizes a convolutional network as the backbone instead of the commonly used MLP. By rendering a light-field volume and upsampling it to the required resolution, MobileR2L achieves real-time inference speed and better rendering quality compared to existing methods like MobileNeRF. Additionally, MobileR2L significantly reduces the storage requirements, making it more suitable for resource-constrained devices. The proposed model has the potential for wide adoption in real-world applications, such as virtual try-on, where real-time interaction between mobile devices and users is crucial.