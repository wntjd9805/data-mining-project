3D object detection is a crucial task in various real-world applications, such as autonomous driving and robot navigation. Recent advances in deep learning have improved the performance of 3D object detection, but these methods require dense annotations of point clouds, which can be expensive and time-consuming to collect. Furthermore, there is a domain gap between different LiDAR sensors, as their data may have different densities and distributions. Existing unsupervised domain adaptation methods focus on reducing the domain gap caused by bias in object sizes, but neglect the domain gap induced by varying densities of point clouds. We propose a Density-insensitive Teacher-Student (DTS) framework to address this domain gap. Our framework pre-trains a density-insensitive object detector on a labeled source domain and fine-tunes it on an unlabeled target domain using a self-training strategy. We introduce a task-specific teacher-student framework to improve the density-insensitivity of the pre-trained detector by feeding it with samples of different densities. We also propose an Object Graph Alignment (OGA) module to enforce consistency in object attributes and relations between the teacher and student models. Experimental results show that our DTS framework outperforms state-of-the-art methods on three popular 3D object detection datasets.