The introduction of this computer science paper discusses the success of transformers in language models and how vision transformers (ViTs) have become popular in the computer vision field. The paper highlights attempts to extend ViTs to very large models with over a billion parameters. While ViTs have shown superior performance compared to convolutional neural networks (CNNs) in terms of parameters and data, the paper argues that CNN-based models can achieve comparable or even better performance than ViTs with similar design and scaling strategies. The paper introduces a new CNN-based foundation model called InternImage, which effectively scales to large parameter sizes and achieves comparable or better performance than state-of-the-art ViTs. The model incorporates a dynamic sparse convolution operator, adaptive spatial aggregation, and tailored basic block and scaling strategies. The proposed model is evaluated on various vision tasks and consistently outperforms prior CNN-based models and large-scale ViTs. On the COCO benchmark dataset, the best model achieves state-of-the-art performance with fewer parameters compared to other models.