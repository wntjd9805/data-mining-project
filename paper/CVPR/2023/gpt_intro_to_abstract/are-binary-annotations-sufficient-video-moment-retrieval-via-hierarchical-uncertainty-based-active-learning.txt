Video Moment Retrieval (VMR) is a crucial task in the field of video understanding, as it aims to localize the temporal region of a video based on a query description. This task has numerous downstream applications, including video question answering, dense video captioning, and video relation detection. Existing methods for VMR have focused on modeling cross-modal context in temporal, leading to significant performance improvements on well-annotated datasets. However, the process of dataset annotation for VMR is time-consuming and expensive, limiting the generalization ability of current models. In this paper, we propose a new active learning-based method to alleviate the burden of annotations in VMR. Our method explores the idea that not all frames in a video should be considered equally and that annotating complex video-query pairs first provides greater benefits. We describe the process as a student-teacher interaction, where the student asks a hard question, the teacher provides the answer as feedback, and the student learns and repeats the process. Our method also utilizes the uncertainty of each video to compress the number of videos that need annotation. We consider two aspects of uncertainty: the classification confidence of each frame and the distance of the frame from known labels. By combining these scores, we select the frame with the highest uncertainty. We train a fully-supervised VMR model using the labels provided by the expert in each round. Additionally, we seek to reduce the number of annotated videos at the sequence level by accumulating the frame-level uncertainty. Our method, named HUAL, reduces annotation costs while achieving comparable performance compared to fully supervised settings. Extensive experimental results on public datasets demonstrate the effectiveness of binary annotations for video moment retrieval and the competitiveness of our proposed method.