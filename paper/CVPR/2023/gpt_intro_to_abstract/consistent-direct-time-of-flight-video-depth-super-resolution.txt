On-device depth estimation plays a crucial role in various applications such as navigation, gaming, and augmented/virtual reality. Previous solutions have utilized different types of sensors like stereo/structured-light and indirect/direct time-of-flight (ToF) sensors. Direct ToF sensors have gained attention due to their high accuracy, compact form factor, and low power consumption. However, current direct ToF sensors suffer from low spatial resolutions, leading to spatial ambiguity in estimating high-resolution depth maps. Existing algorithms for RGB-guided depth completion and super-resolution either assume high-resolution spatial information or simplified image formation models, making them inadequate for the ill-posed direct ToF depth super-resolution task. Furthermore, these approaches typically focus on single-frame processing, ignoring temporal correlations and resulting in significant temporal jittering in depth estimations. In this paper, we propose a novel approach to address the spatial ambiguity in low-resolution direct ToF data using information aggregation between multiple frames in an RGB-direct ToF video and by leveraging histogram information. We design a deep-learning-based RGB-guided direct ToF video super-resolution framework that predicts high-resolution depth maps from a sequence of high-resolution RGB images and low-resolution direct ToF depth maps. Our approach significantly improves both prediction accuracy and temporal coherence compared to per-frame processing baselines. Additionally, we introduce a histogram video super-resolution network that further leverages histogram information. We also present a synthetic dataset, DyDToF, which contains dynamic indoor RGB-depth video sequences with diverse scenes and animations of dynamic objects. Our dataset enables comprehensive evaluations of our proposed algorithm and demonstrates significant improvements in accuracy and temporal coherence. Overall, the contributions of this paper are the introduction of RGB-guided direct ToF video depth super-resolution, the proposal of neural network-based algorithms to maximize the use of multi-frame videos and direct ToF histograms, and the creation of a synthetic dataset with physics-based direct ToF sensor simulations and diverse dynamic objects.