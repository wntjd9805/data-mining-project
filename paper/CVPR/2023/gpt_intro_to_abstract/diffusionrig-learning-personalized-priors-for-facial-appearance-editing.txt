It is a longstanding problem in computer vision and graphics to photorealistically change the lighting, expression, head pose, etc. of a portrait photo while preserving the personâ€™s identity and high-frequency facial characteristics. This paper addresses this problem by proposing a model called Diffusion-Rig that allows for 3D facial appearance editing, specifically focusing on relighting and head pose changes. The model combines the use of a 3D morphable face model and a diffusion model to achieve photorealistic edited images. It first extracts physical properties from single portrait photos using an off-the-shelf method, performs desired 3D edits in the 3D morphable model space, and then uses a diffusion model to map the edited physical buffers to photorealistic images. The model is trained using a two-stage strategy, where it first learns generic facial priors on a large-scale face dataset and then finetunes the model using a small dataset of around 20 images of a specific person of interest to learn personalized facial priors. The contributions of this work include the development of a deep learning model for 3D facial appearance editing, a method to drive portrait photo generation using diffusion models with 3D morphable face models, and a two-stage training strategy that enables editing while preserving identity and high-frequency details.