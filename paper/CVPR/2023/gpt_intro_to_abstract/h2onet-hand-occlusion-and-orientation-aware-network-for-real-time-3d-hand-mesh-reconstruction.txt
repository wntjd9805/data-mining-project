Estimating 3D hand meshes from RGB images is important for applications such as augmented reality and behavior understanding. However, achieving accurate, robust, and real-time reconstruction while handling severe occlusion remains challenging. Previous methods have focused on occlusion-robust features or multi-frame inputs, but they either ignore the influence of occlusion or struggle to effectively extract and fuse multi-frame features. In this paper, we propose H2ONet, a Hand-Occlusion-and-Orientation-aware Network, which aims to exploit non-occluding information from multiple frames to reconstruct the 3D hand mesh. H2ONet decouples hand mesh reconstruction into two tasks - reconstructing the hand mesh at canonical pose and regressing the hand orientation - to better fuse multi-frame features without considering hand orientation differences. Additionally, H2ONet incorporates finger-level and hand-level occlusion-aware feature fusion to handle self and object occlusions. Experimental results demonstrate that H2ONet achieves state-of-the-art performance in handling severe hand occlusions.