Image generation is a well-studied problem in computer vision, with various generative models proposed in recent years. Disentangling different aspects of generated images, such as semantic contents and styles, is crucial for image editing and style transfer. While previous studies have shown that generative adversarial networks (GANs) possess a strong disentanglement capability, it remains unknown whether diffusion models can also achieve this. In this paper, we investigate the disentanglement capability of stable diffusion models and propose a method to achieve disentangled image modifications. By partially changing the input text embedding, we can generate images with target styles without altering the semantic content. Our proposed algorithm, which involves optimizing mixing weights and utilizing perceptual and style matching losses, outperforms diffusion-model-based image-editing baselines without fine-tuning. Our findings shed light on diffusion models' capabilities and their potential applications in image editing tasks.