This paper introduces the problem of societal bias in computer vision, specifically in the context of image captioning models. The authors highlight the issue of bias amplification, where captioning models not only reproduce existing biases in training datasets but also amplify them in their generated sentences. The focus of the study is on gender bias and the authors propose a model-agnostic framework called LIBRA to mitigate bias amplification. LIBRA consists of two main modules: Biased Caption Synthesis (BCS) and Debiasing Caption Generator (DCG). BCS synthesizes biased captions with gender-context or context-gender biases, while DCG mitigates bias from synthesized captions. The framework is shown to reduce both types of gender biases in image captioning models, correcting gender misclassification caused by biased context and changing biased words towards each gender. The authors also discuss the evaluation of generated captions and highlight the importance of considering human-written captions as ground truth. Experimental results demonstrate the effectiveness of LIBRA in mitigating bias amplification in various image captioning models.