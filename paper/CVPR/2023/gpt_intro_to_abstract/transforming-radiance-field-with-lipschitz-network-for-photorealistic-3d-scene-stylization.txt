Photorealistic style transfer (PST) is a crucial task in visual content creation that aims to apply the color style of a reference image to other inputs such as images or videos. Deep learning has led to the development of sophisticated deep PST methods for practical usage. Recent advancements in 3D scene representation, specifically Neural Radiance Field (NeRF), have inspired researchers to explore the more challenging task of photorealistic 3D scene stylization. This task involves generating visually consistent and photorealistic stylized syntheses from arbitrary views, allowing for automatic modification of 3D scene appearance with different lighting, weather, or other effects. However, building an effective framework for photorealistic 3D scene stylization is not trivial due to the absence of a valid photorealistic style loss tailored for training NeRF.To overcome this limitation, the authors propose a framework called LipRF for photorealistic 3D scene stylization. LipRF involves two stages: training a radiance field to reconstruct the source 3D scene and learning a Lipschitz network to transform the appearance representation of the pre-trained NeRF to the stylized 3D scene with the guidance of style emulation using 2D PST. The authors demonstrate that Lipschitz multilayer perceptrons (MLPs) can be interpreted as an implicit regularization for safeguarding the 3D photography of stylized scenes. They also design adaptive regularization and gradual gradient aggregation techniques to optimize LipRF in a cost-efficient manner.The authors provide a thorough analysis of photorealistic 3D scene stylization and present LipRF as a concise and flexible framework for achieving this task. Their proposed approach is evaluated on both photorealistic 3D stylization and object appearance editing tasks to validate its effectiveness.