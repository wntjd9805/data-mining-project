3D object detection has achieved significant success in various fields such as autonomous driving and robotics. However, most existing methods rely on labeled 3D data, which is expensive and time-consuming to obtain. This hinders the utilization of large amounts of unlabeled data and limits the method's applicability in different scenes. In this paper, we propose a new framework called GD-MAE for pre-training in 3D object detection. GD-MAE simplifies the pre-training process by using a generative decoder to automatically expand the visible regions to the underlying masked areas. This eliminates the need for complex decoders and allows for flexible masking strategies. We introduce the Sparse Pyramid Transformer (SPT) as the multi-scale encoder, which utilizes sparse convolution and sparse transformer to handle the compact and regular representation of point clouds. The proposed GD-MAE framework achieves state-of-the-art detection results on various datasets. Our contributions include the introduction of a simpler MAE framework, the enabling of flexible masking strategies on point clouds, and the verification of the effectiveness of our model through extensive experiments.