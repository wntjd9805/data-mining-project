Training deep learning models often requires a large amount of labeled data, which can be expensive and time-consuming to collect and annotate. To address this problem, using synthetic data generated from 3D graphics has emerged as a promising solution. However, existing synthetic datasets often have a large domain gap from real-world data and are costly to generate. In this paper, we propose a new approach called Lift3D, which leverages 3D Generative Adversarial Networks (GANs) to synthesize 3D training data. Unlike previous 3D GANs that struggle with high-resolution synthesis and geometry-consistency, Lift3D achieves higher-resolution synthesis by inverting the generation pipeline from 2D-to-3D. We first generate multi-view images with pseudo pose annotation using a well-disentangled 2D GAN and then lift these images to 3D representation using a neural radiance field reconstruction. This allows us to generate high-quality images that are tightly aligned with the sampling label and can be synthesized in any resolution. Experimental results on image-based 3D object detection tasks demonstrate that Lift3D outperforms existing data augmentation methods and achieves promising results even without labeled data. Our contributions include the first exploration of using 3D GANs for synthesizing 3D training data and the development of the Lift3D framework for high-resolution synthesis of datasets with accurate 3D labels.