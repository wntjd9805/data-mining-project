Recent research has been successful in applying transformer models, specifically Self Attention (SA)-based models, to Natural Language Processing (NLP) tasks. However, the large model sizes, high computational consumption, and long training time of these models pose significant difficulties for hardware deployment. To address this issue, various compression and acceleration methods, including pruning, quantization, and neural architecture search, have been applied to vision transformer models. Among these methods, quantization has been proven to be effective and widely used. However, quantization inevitably leads to a performance drop due to the approximation error made by the quantizer. To mitigate this issue, Quantization-Aware Training (QAT) has been proposed but it requires high training cost and unstable retraining. Alternatively, Post-Training Quantization (PTQ) adjusts the design of the quantizer based on the full-precision pretrained model and sampled calibration data. This paper introduces a new method called NoisyQuant, which actively alters the distribution being quantized to reduce quantization error. NoisyQuant adds a fixed noisy bias to the activation before quantization, resulting in significant reduction of quantization error. The proposed method achieves performance improvement in the PTQ of vision transformer models, surpassing state-of-the-art mixed-precision and nonlinear PTQ methods. The paper also theoretically proves the feasibility of reducing quantization error for heavy-tailed distributions with a fixed additive noisy bias and presents experimental results demonstrating the effectiveness of NoisyQuant.