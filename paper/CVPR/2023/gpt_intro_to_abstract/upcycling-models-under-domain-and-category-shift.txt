Deep neural networks (DNNs) have shown great progress in various tasks, but they often struggle with generalizing to new domains and categories. Existing approaches for domain adaptation rely on access to labeled source data, which is not always available due to data privacy policies. Source-free Domain Adaptation (SFDA) has emerged as a promising solution, but it is limited to closed-set scenarios where the label space is the same across domains. In reality, we encounter various category shifts such as partial-set, open-set, and open-partial-set, making the strict assumption of identical label spaces unrealistic. To address this challenge, we propose Source-free Universal Domain Adaptation (SF-UniDA), which aims to adapt pre-trained closed-set models to handle domain and category shifts. We introduce the Global and Local Clustering (GLC) technique, which utilizes global one-vs-all clustering and local k-NN clustering algorithms to separate known and unknown data samples. Extensive experiments on multiple benchmarks demonstrate the superiority of GLC in handling various category shifts, achieving state-of-the-art performance even under stricter constraints. Our contributions include being the first to achieve SF-UniDA with only a pre-trained closed-set model, and proposing a generic GLC technique for adaptive clustering in SF-UniDA scenarios.