This paper introduces user-controllable image and video synthesis, focusing on the synthesis of 3D content. Existing image-to-image translation methods allow users to create and manipulate high-resolution images based on 2D input label maps, but they lack 3D structure reasoning. The goal of this work is to make conditional image synthesis 3D-aware, enabling 3D content generation, viewpoint manipulation, and attribute editing. However, synthesizing 3D content conditioned on user input poses challenges due to the lack of paired datasets and inconsistent 3D signals from multi-view user inputs. To overcome these challenges, the authors propose extending conditional generative models with 3D neural scene representations and encoding semantic information in 3D for cross-view editing. The proposed method is evaluated on the CelebAMask-HQ, AFHQ-cat, and shapenet-car datasets, outperforming 2D and 3D baselines. Additionally, the paper discusses the impact of design choices and demonstrates applications of the method, such as cross-view editing and explicit user control.