This paper introduces the problem of decoding visual information from functional Magnetic Resonance Imaging (fMRI) data using deep learning models. The authors highlight the challenges of recovering clear and meaningful images from fMRI data, due to the lack of fMRI-image pairs and biological guidance. Individual variability in brain representations further complicates this problem. The authors propose a framework called MinD-Vis, which combines Sparse-Coded Masked Brain Modeling (SC-MBM) with Double-Conditioned Latent Diffusion Model (DC-LDM) to improve the generation of visual stimuli from fMRI data. The framework utilizes large-scale representation learning and mimics the sparse coding of information in the brain. Several contributions are made, including the design of SC-MBM as an effective brain feature learner, the integration of DC-LDM for stronger decoding consistency and variance under the same semantics, and the generation of more plausible images with preserved semantic information. The proposed framework is evaluated quantitatively and qualitatively on multiple datasets, including a new dataset.