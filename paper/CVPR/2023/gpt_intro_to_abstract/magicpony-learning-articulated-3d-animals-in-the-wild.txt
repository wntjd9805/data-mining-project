In this paper, we present MagicPony, a new approach to learning 3D models of articulated object categories using single-view input images for training. Existing methods for reconstructing 3D shapes from images typically require ad-hoc data acquisition setups and significant manual effort. Learning a 3D prior from 2D images is a complex challenge that involves reconstructing the 2D training data in 3D. Our proposed approach leverages recent advancements in unsupervised representation learning, unsupervised image matching, efficient shape representations, and neural rendering. We introduce a novel auto-encoder architecture that can reconstruct the 3D shape, articulation, and texture of each object instance from a single image. We only require a 2D segmenter for the object category and a description of the topology and symmetry of its 3D skeleton for training, without any prior knowledge of specific cues or shapes. Our function exhibits strong generality, even reconstructing objects in abstract drawings. To address challenges in learning disentangled 3D representations from raw images, we tackle the problem of viewpoint estimation by fusing knowledge from a self-supervised visual transformer network and develop an efficient disambiguation scheme. We also address the issue of representing the 3D shape, appearance, and deformations by using a hybrid volumetric-mesh representation. Our contributions include a new 3D object learning framework that combines advances in unsupervised learning, 3D representations, and neural rendering, an effective mechanism for fusing self-supervised features, and an efficient multi-hypothesis viewpoint prediction scheme. Overall, our approach achieves better reconstruction results with less supervision.