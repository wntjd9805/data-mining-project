The task of image inpainting, filling missing pixels in defective regions, is widely applicable in image manipulation and object removal. Language-guided image inpainting has gained attention, allowing for controllable results with the guidance of text descriptions. Existing pre-trained models encode the entire image, resulting in receptive spreading and information loss in non-defective regions. To address this, we propose N ¨UWA-LIP, which incorporates a defect-free VQGAN and a multi-perspective sequence-to-sequence module. DF-VQGAN controls receptive spreading and retains information in non-defective regions, while MP-S2S enhances visual information from different perspectives. We evaluate N ¨UWA-LIP on three datasets and compare it with existing methods, demonstrating superior performance. Our contributions include the development of DF-VQGAN, MP-S2S, and open-domain datasets for language-guided image inpainting.