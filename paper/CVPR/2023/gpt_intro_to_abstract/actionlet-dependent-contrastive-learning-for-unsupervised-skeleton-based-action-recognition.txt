Skeletons, which represent human joints using 3D coordinate locations, have become popular in action recognition tasks due to their lightweight, privacy-preserving, and compact nature. While supervised skeleton-based methods have achieved impressive performance, they heavily rely on labeled training data, which is expensive to obtain. To address this issue, self-supervised learning has been introduced into skeleton-based action recognition, where unsupervised pretraining and downstream tasks are used. In this paper, we propose a new approach called ActCLR that focuses on motion regions, known as actionlets, to guide contrastive learning. By treating motion and static regions differently, we aim to preserve the rich action information in the motion regions and improve the consistency of contrastive learning. We introduce a motion-adaptive transformation strategy and a semantic-aware feature pooling method to enhance the model's performance in learning semantic consistency and extracting motion features. Our experimental results on the NTU RGB+D and PKUMMD datasets demonstrate the superiority of our method compared to state-of-the-art techniques in self-supervised learning. Our contributions include the introduction of unsupervised actionlets, the design of a motion-adaptive transformation strategy, and the utilization of semantic-aware feature pooling for motion feature extraction.