Learning a general perception model that can handle various modalities and tasks is a crucial step towards artificial general intelligence. Existing foundation models focus on learning a general representation encoder that can be fine-tuned to different tasks, but this approach is limited by the high marginal cost of adapting pre-trained models to diverse downstream tasks. In this paper, we propose Uni-Perceiver v2, a generalist model capable of handling major vision and vision-language tasks without task-specific fine-tuning. Uni-Perceiver v2 encodes images as general region proposals, allowing for more expressive localization modeling, and utilizes a Transformer-based language model for text encoding. The model is jointly trained on various tasks using a shared modality-agnostic Transformer network and a task-balanced gradient normalization technique. Uni-Perceiver v2 achieves competitive results on object detection, instance segmentation, image classification, image captioning, and image-text retrieval tasks. It outperforms existing generalist models in both versatility and performance, demonstrating its strong ability for general task modeling.