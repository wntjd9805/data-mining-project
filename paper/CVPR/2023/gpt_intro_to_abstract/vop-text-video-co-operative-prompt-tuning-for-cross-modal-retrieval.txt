This paper introduces the concept of prompt tuning as a method to address the challenges in transferability and generalizability in text-video cross-modal retrieval tasks. Prompt tuning involves freezing the backbone of the model and only tuning a few extra parameters before the input. The paper proposes the VoP (Text-Video Co-operative Prompt Tuning) method, which introduces tunable prompts in both textual and visual encoders. Unlike existing efforts, VoP inserts prompts in both encoders at every layer, achieving competitive performance with minimal parameter storage. Additionally, the paper designs three novel video prompts - position-specific, context-specific, and function-specific - to exploit essential video-specific information. Experimental results on multiple benchmark datasets demonstrate that VoP with video prompts outperforms full fine-tuning with fewer parameters, showcasing its effectiveness in text-video retrieval tasks. The contributions of this work are the introduction of VoP as a strong baseline, the development of video prompts conditioned on frame position, context, and layer function, and the demonstration of their effectiveness in enhancing VoP in text-video retrieval tasks.