Referring understanding, the integration of natural language processing into scene perception in computer vision, has gained significant attention in recent years. This approach aims to localize regions of interest in images or videos based on human language instructions, with applications in video editing and autonomous driving. Previous benchmarks, such as Flickr30k, ReferIt, and RefCOCO/+/g, have encouraged the development of image-based referring tasks. However, these benchmarks have limitations in handling multiple referent targets and temporal status variances, leading to inaccurate evaluations. To address these limitations, we propose a novel video understanding task called referring multi-object tracking (RMOT), where a language expression is used to ground semantically matched objects in a video. We also present a new benchmark dataset called Refer-KITTI, which offers high flexibility with referent objects, high temporal dynamics, and low labeling cost. To support the development of RMOT, we introduce an end-to-end differentiable framework called TransRMOT, which utilizes cross-modal reasoning and cross-frame conjunction to track multiple objects accurately. Our contributions include the introduction of the RMOT task, the creation of the Refer-KITTI benchmark dataset, and the development of the TransRMOT framework, which outperforms hand-crafted methods on the Refer-KITTI dataset.