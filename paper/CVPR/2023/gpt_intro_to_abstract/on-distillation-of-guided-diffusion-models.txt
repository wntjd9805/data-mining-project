This paper introduces a two-stage distillation approach to improve the sampling efficiency of classifier-free guided diffusion models. These models have shown state-of-the-art performance in various domains, but their low sampling efficiency has limited their practical application. The proposed approach involves using a single student model to match the combined output of the teacher models in the first stage, and then progressively distilling the model to a fewer-step model in the second stage. This allows for a trade-off between sample quality and diversity, and enables the model to handle different levels of guidance strength efficiently. The approach is applicable to both pixel-space and latent-space diffusion models, as demonstrated in experiments on ImageNet 64x64 and CIFAR-10 datasets. The distilled model achieves visually comparable sample quality to the teacher model using only 4 steps and comparable FID/IS scores with as few as 4 to 16 steps. The effectiveness of distillation for classifier-free diffusion models in both pixel-space and latent-space is demonstrated for the first time. The proposed framework is also applied to text-guided image inpainting and editing tasks, reducing the total number of sampling steps to as few as 2-4 steps, showcasing its potential in style-transfer and image-editing applications.