Imitation learning (IL) is widely used in various applications, but it often suffers from issues such as long training time, unstable training process, and suboptimal performance. Classical behavioral cloning (BC) methods face the covariate shift problem, while other IL methods such as inverse reinforcement learning (IRL) and adversarial imitation learning (AIL) have limitations due to the use of intermediate signals. In this paper, we propose a new IL approach called Imitation Learning as State Matching via Differentiable Physics (ILD), which utilizes a differentiable physics simulator (DPS) to recover expert behavior. ILD incorporates the environment dynamics as a physics prior into its computational graph during policy learning, avoiding the need for intermediate signals. We introduce a Chamfer-Î± distance for trajectory matching, which simplifies the optimization task and leads to better performance. Compared to IRL and AIL methods, ILD does not introduce additional signals and is more transferable to different environments. Experimental results on various tasks demonstrate that ILD achieves significant improvements in terms of convergence time, training stability, and final performance compared to existing methods.