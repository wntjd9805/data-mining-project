This paper introduces LayoutDM, a model for automatic generation of visually pleasing graphic layouts. The task of layout generation involves arranging elements with different attributes, taking into account the relationships between them. Existing approaches either use autoregressive models or develop specific inference strategies. In this paper, the authors propose using discrete state-space diffusion models for layout generation. They formulate the diffusion process for layout structure and train a denoising backbone network to infer complete layouts. Their model supports variable-length layout data and can incorporate complex layout constraints. LayoutDM overcomes issues with autoregressive models and achieves better variable-length generation. The authors evaluate LayoutDM on various layout generation tasks using large-scale datasets and show its superior performance compared to task-agnostic and task-specific baselines. They also conduct an ablation study to validate the effectiveness of their design choices. The contributions of this paper include the formulation of the discrete diffusion process for layout generation, the incorporation of complex layout constraints, and the empirical demonstration of LayoutDM's performance on different layout generation tasks.