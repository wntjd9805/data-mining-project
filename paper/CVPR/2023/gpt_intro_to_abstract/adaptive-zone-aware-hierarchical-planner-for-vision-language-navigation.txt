In recent years, there has been a growing interest in Embodied-AI (E-AI) research within the computer vision, natural language processing, and robotics communities. The goal of E-AI research is to develop intelligent agents that can interact with humans to complete tasks. This paper focuses on the Vision-Language Navigation (VLN) task, which involves embodied agents navigating in a photorealistic 3D environment based on natural language instructions. The instructions provided to the agent can be step-by-step or goal-oriented, with the latter being more practical but also more challenging. Goal-oriented instructions contain hierarchical information, where the agent needs to complete several sub-goals to achieve the overall goal. Additionally, the agent needs to partition the scene into zones and select the appropriate zone for each sub-goal. However, current state-of-the-art VLN methods do not explicitly consider this hierarchical planning nature, limiting their decision-making ability. To address this, the paper proposes an Adaptive Zone-aware Hierarchical Planner (AZHP) that models the navigation process as a hierarchical action-making process. The high-level action sets sub-goals and selects zones, while the low-level action executes specific navigation decisions within the selected zone. The paper introduces the Scene-aware Adaptive Zone Partition (SZP) method to divide the action map into zones and the Goal-oriented Zone Selection (GZS) method to select the appropriate zone based on the instruction. A State-Switcher Module (SSM) is also included to support asynchronous switching between high-level and low-level actions. To train the high-level action, a Hierarchical Reinforcement Learning (HRL) strategy is proposed, along with auxiliary losses and curriculum learning to improve learning robustness. The paper's contributions include the AZHP hierarchical navigation paradigm, the SZP and GZS methods, as well as the HRL strategy and auxiliary losses for constructing and learning the hierarchical planning policy. Experimental results demonstrate the effectiveness of the proposed method. The code is available for further exploration.