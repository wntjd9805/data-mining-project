Supervised semantic segmentation methods have made great strides in recent years, but they heavily rely on large datasets with pixel-level annotations. This labeling process is time-consuming and expensive, limiting their practical application with limited labeled data. To address this issue, semi-supervised semantic segmentation (SSS) has been proposed, which aims to train models on a smaller amount of labeled data combined with a larger amount of unlabeled data. Consistency regularization, the most common approach in SSS, incorporates unlabeled data into supervised learning by introducing prediction disagreement through data or model perturbations. Several studies have explored different data augmentations and model perturbations, as well as integrating auxiliary tasks and trainable modules to improve SSS performance. However, these methods often become more complex, requiring additional components and training procedures.In this paper, we propose AugSeg, a simple yet effective method for SSS that focuses on data perturbations to enhance performance. While previous studies have utilized auto data augmentations and cutmix-related transformations, we argue that these augmentations need to be adjusted specifically for semi-supervised training. We simplify the randomAug approach and design a highly random intensity-based augmentation that randomly selects different intensity-based augmentations and distortion strengths from a continuous space. Additionally, we address the issue of confirmation bias introduced by random copy-paste augmentation by adaptively injecting labeled information from confident predictions. AugSeg requires no extra operations to handle distribution issues and achieves state-of-the-art performance on various SSS benchmarks. Our contributions include breaking the trend of complex SSS designs with AugSeg, providing a simple baseline method, and demonstrating the effectiveness of our approach through extensive experiments and ablations studies.