The text-to-image generation has gained significant attention in recent years due to its ability to generate creative and realistic images from natural language descriptions. This technology has numerous potential benefits in areas such as multimedia editing, computer-aided design, and art creation. However, there are ethical concerns regarding its misuse, such as generating synthetic content with harmful stereotypes or violence. Existing content moderation filters are deployed to prevent the generation of such harmful content, but they remain susceptible to adaptive adversarial attacks. While deep neural networks have been shown to be vulnerable to adversarial examples, the adversarial attacks on text-to-image generators are not well explored. This paper addresses this gap by proposing RIATIG, a reliable and imperceptible adversarial attack against text-to-image models using natural examples. The proposed method formulates the generation of adversarial examples as an optimization problem and applies genetic-based optimization methods to solve it, resulting in more reliable and effective adversarial examples. Additionally, a new text mutation technique is proposed to improve the stealthiness of the generated adversarial text. Experimental results demonstrate that RIATIG outperforms state-of-the-art text-to-image adversarial attacks in terms of attack effectiveness and sample quality. This work contributes to the analysis of the adversarial robustness of text-to-image generation models, proposes reliable optimization methods for finding natural adversarial examples, and highlights the need for improving the robustness and security of text-to-image models.