Abstract:Deep learning is highly reliant on large amounts of data, which presents challenges in terms of both training and data storage. Dataset Condensation (DC) aims to condense large datasets into smaller ones while maintaining their validity for model training. Existing DC methods often rely on image synthesis to achieve dataset condensation and have shown improved performance compared to traditional coreset selection methods. However, these optimization-oriented methods are computationally expensive and not scalable to large datasets and models. Distribution matching (DM) proposes to match the output feature distributions of the real and condensed datasets extracted by randomly-initialized models, offering a more efficient and scalable approach. However, DM still lags behind optimization-oriented methods in terms of performance. In this paper, we analyze the shortcomings of DM and propose a set of remedies called improved distribution matching (IDM) to address these issues. IDM includes techniques such as partitioning and expansion augmentation, efficient and enriched model sampling, and class-aware distribution regularization. Experimental results demonstrate that IDM significantly improves upon DM and outperforms most optimization-oriented methods. Additionally, IDM is highly efficient and scalable, making it applicable to large datasets such as the ImageNet Subset.