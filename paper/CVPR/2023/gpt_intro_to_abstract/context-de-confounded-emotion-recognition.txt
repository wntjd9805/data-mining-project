Emotion recognition is crucial for various applications in fields like human-computer interaction, medical monitoring, and education. Prior works have focused on extracting emotion cues from various sources such as facial expressions, acoustic behaviors, and body postures, using deep learning algorithms. However, the performance of these subject-centered approaches is limited in natural and unconstrained environments. Recent studies have suggested that contextual information, including place category, attributes, objects, or actions of others, contributes to effective emotion cues. However, there is a severe bias in CAER datasets, where certain scene contexts are predominantly associated with positive or negative emotions. This bias misleads models and affects their performance. In this paper, we propose a causality-based bias mitigation strategy to address this issue. We formulate the CAER task using a causal graph and introduce a Contextual Causal Intervention Module (CCIM) to de-confound the context bias. CCIM is a plug-in module that can be integrated into existing CAER models to improve their performance. We evaluate the effectiveness of CCIM on three biased CAER datasets and demonstrate its superiority over existing baselines. Our contributions include the identification of the context bias as a confounder, the proposal of CCIM for context-deconfounded training, and achieving a new state-of-the-art in CAER.