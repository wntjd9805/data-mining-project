This paper addresses the problem of building a transferable visual system in computer vision (CV) that can easily adapt to various downstream tasks. The conventional approach involves training deep neural networks on large amounts of data to achieve generic visual representations. However, this approach has limitations in real-world settings. The authors propose a customization stage between pre-training and adaptation, where customization is implemented by leveraging retrieved external knowledge. This approach is inspired by how humans specialize in relevant subjects to master certain skills while maintaining basic skills. The authors explore the use of a large image-text corpus to acquire external knowledge for model customization, without the need for extra human annotation. They develop a multi-modal indexing system to retrieve relevant image-text pairs and propose a modularized learning strategy to update only additional trainable weights. The proposed customization strategy is demonstrated on various CV problems, including image classification, image-text retrieval, object detection, and semantic segmentation. The retrieved knowledge significantly improves the model's performance in zero-shot tasks and outperforms generic models in few/full-shot tasks. The authors provide a publicly available retrieval system, codebase, pre-trained models, and subsets of datasets for further research in leveraging internet data for customized visual recognition.