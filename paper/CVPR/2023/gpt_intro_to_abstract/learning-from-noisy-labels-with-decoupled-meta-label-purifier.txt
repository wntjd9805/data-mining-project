Deep learning has made significant progress in various recognition tasks, thanks to the availability of large-scale datasets with reliable annotations. However, collecting such datasets is time-consuming and expensive. Web crawling techniques provide easier access to labeled data, but the obtained samples often come with noisy labels, making them unsuitable for direct training of deep neural network models. To mitigate this problem, Learning with Noisy Label (LNL) methods have been proposed to identify and reduce the impact of noisy samples during training. However, these methods struggle in extremely noisy and complex scenarios where there is a lack of clean data to train a discriminative classifier. Label correction approaches have been proposed to revise noisy labels and augment clean training samples. Meta-learning based approaches have achieved state-of-the-art performance by using a small clean validation set and treating noisy labels as hyper-parameters. However, these approaches involve a computationally infeasible nested bi-level optimization problem. In this paper, we propose a Decoupled Meta Label Purifier (DMLP) method that separates the label purification process into representation learning and a non-nested meta label purifier. DMLP consists of a meta-learning based label purifier with two mutually reinforcing correcting processes, namely intrinsic primary correction (IPC) and extrinsic auxiliary correction (EAC). IPC purifies labels in a global sense, while EAC accelerates the purification process by training with updated labels from IPC. Our experimental results on synthetic and real-world datasets demonstrate the superiority of DMLP in robust learning with noisy labels. The contributions of this paper include the analysis of the necessity of decoupled optimization for label correction, the proposal of DMLP as a flexible multi-stage label purifier, and the demonstration of state-of-the-art results on various noisy datasets.