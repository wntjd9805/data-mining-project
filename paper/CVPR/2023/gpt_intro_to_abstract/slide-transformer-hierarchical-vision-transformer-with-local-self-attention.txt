Transformer models have gained significant attention in the field of natural language processing. More recently, researchers have started exploring the potential of Transformer models in vision tasks. However, adapting Transformer models to vision is a challenging task due to the computational complexity of self-attention with a global receptive field. Existing approaches have proposed limiting the global receptive field to smaller regions, but these approaches still suffer from limitations. A more natural and effective alternative is adopting local attention, which has the advantages of convolution with translation-equivariance and local inductive bias. Previous works have investigated applying local attention to convolution or Transformer models, but they either use inefficient methods or rely on CUDA kernels, restricting generalizability. In this paper, we present a novel local attention module, called Slide Attention, that can be efficiently integrated with various Vision Transformer models and hardware devices. We replace the inefficient Im2Col function with standard convolution operations, and introduce a deformed shifting module for increased flexibility. We empirically validate our module on various vision tasks and demonstrate consistent improvements over baselines. Our method also proves to be efficient on devices without CUDA support.