Video anomaly detection (VAD) has gained attention in academia and industry due to the prevalence of surveillance cameras in public places. The goal of VAD is to automatically detect abnormal events, reducing the workload of human monitors. Various VAD methods have been developed under different supervision settings, but learning from normal data is more practical and dominant. However, learning VAD models from normal data presents challenges. Some anomalies are scene-dependent, making it difficult to differentiate between normal and abnormal appearances or motions. Additionally, normal patterns are diverse, and dealing with rare but normal activities is challenging. Previous VAD methods mainly focus on frame-level learning or object-centric learning, but suffer from background bias or imbalance between normal and abnormal data. This work addresses these challenges by proposing a hierarchical semantic contrast method for scene-dependent anomaly detection. The proposed method uses pre-trained video parsing networks to group the appearance and activity of objects and background scenes into semantic categories. A scene-aware VAD model is learned by enforcing the encoded latent features to gather together based on their semantic categories. An object-centric feature decoder is designed to reduce background noise. Experimental results on public datasets and self-built datasets demonstrate the effectiveness of the proposed method in scene-independent and scene-dependent VAD.