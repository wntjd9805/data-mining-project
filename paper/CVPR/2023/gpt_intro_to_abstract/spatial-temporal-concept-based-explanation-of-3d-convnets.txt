With the rise of convolutional neural networks (CNNs) in computer vision tasks, there is a growing need for explainable artificial intelligence (XAI) to enhance the transparency and credibility of these models. While there are existing methods for explaining 2D image classification CNNs, very few studies have focused on interpreting 3D action recognition CNNs, primarily due to the computational complexity and rich spatial-temporal content of video data. In this paper, we propose a novel method called Spatial-temporal Concept-based Explanation (STCE) that extends the concept activation vectors (CAVs) approach to 3D action recognition CNNs. STCE segments videos into spatial-temporal supervoxels and groups similar supervoxels to form meaningful concepts. Our method assigns a score to each concept based on its contribution to network predictions, allowing us to answer important questions regarding which objects or motions are significant for a particular action recognition class. We validate the effectiveness of STCE on the Kinetics and KTH datasets, demonstrating that our method provides interpretable and human-understandable explanations for 3D action recognition CNNs.