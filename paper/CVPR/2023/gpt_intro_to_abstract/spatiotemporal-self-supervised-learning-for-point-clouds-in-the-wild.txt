Semantic segmentation from LiDAR point clouds is crucial for various applications, including self-driving vehicles. Current methods achieve this task using deep neural networks, but they require a large amount of annotated data, which is expensive to acquire. In this paper, we propose a self-supervised learning (SSL) framework that leverages unlabeled LiDAR data for training semantic segmentation networks. Our method does not rely on any external information such as pose, GPS, or IMU. We introduce a Point-to-Cluster (P2C) training paradigm that combines point-level and cluster-level representations to learn a structured point-level embedding space. Additionally, we incorporate cluster-level inter-frame self-supervised learning to capture temporal information in the SSL process. Our experiments demonstrate that our approach outperforms state-of-the-art SSL techniques for point cloud data, especially in outdoor scenes where data complexity and sparsity pose challenges. Our SSL approach effectively captures both spatial and temporal information, addressing the limitations of existing single-frame methods.