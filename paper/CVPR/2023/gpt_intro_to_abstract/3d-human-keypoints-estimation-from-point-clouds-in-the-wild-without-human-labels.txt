Human pose estimation in 3D is a significant problem in computer vision with various applications in AR/VR, AI-assisted healthcare, and autonomous driving. Perceiving human poses from sensor data is crucial for autonomous systems to understand their surrounding environment and make safe maneuvers. However, there is limited research on outdoor 3D keypoint detection using point cloud data due to the challenge of acquiring high-quality in-the-wild data with ground truth labels. Annotating 3D human keypoints on point clouds is expensive, time-consuming, and error-prone. Existing datasets with ground truth human poses are also limited in terms of quantity and diversity. Previous approaches have focused on weak supervision or leveraging signals from other modalities. This work proposes a novel method for learning 3D human keypoints from in-the-wild point clouds without manual labeled keypoints. The approach utilizes the observation that human skeletons are roughly centered within rigid body parts and that the movement of surface points should explain the movement of the skeleton. Novel unsupervised loss terms are designed to learn the locations of the 3D keypoints/skeleton within human point clouds. The proposed method first trains a regression model and a semantic segmentation model on synthetic data. Then, it is trained on the Waymo Open Dataset without 3D ground-truth annotation. Through unsupervised training, keypoint predictions are refined and the backbone learns from unannotated data. The contributions of this work include the presentation of a method for learning human 3D keypoints without manual annotations, the proposal of three effective unsupervised losses, and the demonstration of the usefulness of GC-KPL as unsupervised representation learning for human point clouds. The potential application of this approach is to improve human pose understanding in autonomous driving using a large amount of sensor data.