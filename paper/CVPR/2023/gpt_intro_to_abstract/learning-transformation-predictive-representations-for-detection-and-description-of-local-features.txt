Local visual descriptors are essential in computer vision applications such as camera calibration, 3D reconstruction, visual simultaneous localization and mapping (VSLAM), and image retrieval. These descriptors provide representation vectors for patches around keypoints and can be used to generate dense correspondences between images. The effectiveness of these descriptors relies on their representation, which has traditionally been trained using Siamese architecture and contrastive learning loss. Contrastive learning aims to "learn to compare" by distinguishing similar (positive) samples from dissimilar (negative) samples in an embedded space. Negative samples are introduced to maintain uniformity in the representation and prevent model collapse. However, existing methods for mining hard negatives increase computational and memory resources usage and may include false negatives that hinder accuracy. These false negatives represent instances with the same semantics as the anchor but are labeled as negatives. To address these challenges, we propose learning transformation predictive representations (TPR) for visual descriptors only with positive samples, avoiding collapsing solutions. Eliminating the use of negatives improves training efficiency and reduces computation load and memory usage. We also encourage the use of hard positives, corresponding pairs with large-scale transformations, to expose novel patterns. Directly applying contrastive learning on transformed images is ineffective, so we propose learning TPR with soft positive labels. We introduce a self-supervised curriculum learning module to generate controllable stronger positives with refined soft supervision. Our TPR with soft labels is trained on natural images in a fully self-supervised paradigm, making it easy to collect and scale up training datasets. Experimental results demonstrate that our method outperforms state-of-the-art techniques on image matching benchmarks and exhibits strong generalization capabilities in multiple downstream tasks. Our contributions include the proposal of TPR for joint local feature learning without negative samples, the use of self-supervised generation learning and curriculum learning to improve training with soft labels and stronger transformations, and the training of our pipeline using a self-supervised paradigm and natural images.