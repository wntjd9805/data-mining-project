Continual learning (CL) aims to mimic the ability of humans to learn from non-stationary data distributions without forgetting previously acquired knowledge. However, existing CL systems overlook the robustness against unforeseen data shifts during testing, resulting in decreased performance in the presence of corruptions in test images. This issue makes existing CL models unreliable in safety-critical applications. To address this, we propose MetaMix, a self-adaptive augmentation method within a meta-learning framework, which learns to mix augmentation operations tailored to evolving data distributions. MetaMix optimizes the performance of both seen and unseen corruptions and adapts the augmentation strategy automatically. We evaluate the corruption robustness of existing and proposed methods on new benchmark datasets, including split-CIFAR-10-C, split-CIFAR-100-C, and split-miniImageNet-C. Our experiments demonstrate the effectiveness of MetaMix in achieving corruption-robustness compared to state-of-the-art data-augmentation approaches. Our contributions include the first study on corruption-robustness in CL, the introduction of novel benchmarks, and the proposal of a versatile augmentation method that can be seamlessly integrated with existing CL methods.