Image retrieval plays a crucial role in various applications such as e-commerce search, face recognition, and landmark localization. With the advancements in deep learning, visual retrieval systems have evolved to encompass larger models and richer databases to provide better services. These systems include an embedding model that maps input images to high-dimensional vectors and a vector database that stores these embeddings for similarity search. However, when new models are deployed, the embeddings of query images extracted by these models are often incompatible with the existing database, requiring a time-consuming process called backfilling to rebuild the database. To address this issue, backward-compatible learning has been proposed to ensure compatibility between models, allowing the new model to directly replace the old one and update embeddings on-the-fly. This approach reduces resource requirements during backfilling but presents challenges in maintaining retrieval performance. In this paper, we propose an adversarial backward-compatible learning method that employs an elastic boundary loss to improve compatibility and discrimination. We also introduce a unified training and evaluation protocol and a new metric called PÎ²-score to comprehensively assess the performance of backward-compatible methods. Through extensive experiments, we demonstrate that our proposed method outperforms existing state-of-the-art methods on multiple image retrieval datasets. Overall, our work contributes to bridging the distribution gap between different models and improving compatibility and discrimination in visual search systems.