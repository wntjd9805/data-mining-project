Adapting pretrained vision-language models (VLMs) to the downstream visual question answering (VQA) task has become a popular approach in achieving state-of-the-art performance. However, finetuning the full model with millions or billions of parameters leads to high computation and storage costs, as well as over-fitting in low-resource learning scenarios. Parameter-efficient tuning methods have been proposed to address these challenges by updating only a small number of parameters in the pretrained models or the newly-added lightweight modules. While these methods reduce the number of tunable parameters, they still lag behind full finetuning in terms of performance. This paper explores more parameter-efficient tuning methods based on adapter-based approaches to outperform full finetuning in low-resource VQA. Two improvements are considered: (i) reducing parameter redundancy while maintaining adapter capacity, and (ii) reducing task-irrelevant redundancy while promoting task-relevant correlation in representations. The proposed MixPHM method achieves these improvements by employing redundancy-aware techniques and integrating multiple PHM-experts in a mixture-of-experts fashion. Extensive experiments conducted on three datasets demonstrate that MixPHM consistently outperforms full finetuning and existing parameter-efficient tuning methods, striking a better balance between performance and parameter efficiency. The contributions of this paper include the proposal of MixPHM as a redundancy-aware parameter-efficient tuning method, the introduction of redundancy regularization to reduce task-irrelevant redundancy and promote task-relevant correlation, and the demonstration of significant performance improvement over current methods.