This paper addresses the challenge of developing a blur-robust 3D hand mesh estimation framework, as blur significantly degrades the performance of 3D hand mesh estimation. While previous research has achieved promising results from single sharp images, there is a lack of consideration for blurry hands. The main reason for this is the absence of datasets that consist of blurry hand images with accurate 3D ground truth. To overcome this limitation, the authors present the BlurHand dataset, which provides natural blurry hand images with accurate 3D annotations. The dataset is synthesized from an existing video-based hand dataset, InterHand2.6M, using state-of-the-art blur synthesis techniques. The authors propose BlurHandNet, a method for recovering a 3D hand mesh sequence from a single blurry image. BlurHandNet incorporates useful temporal information from the blurry hand by using an Unfolder and a kinematic temporal Transformer (KT-Former). The Unfolder outputs hand features of three time steps, allowing the method to produce 3D meshes at both ends of the motion. The KT-Former enhances the temporal hand features using self-attention, enabling the model to consider both the kinematic structure and temporal relationship between the hands. Experimental results show that BlurHandNet achieves superior 3D hand mesh estimation performance on blurry hands. The proposed method provides a valuable contribution to the field, as blurry hands have been under-studied. The BlurHand dataset and BlurHandNet can be used as a basis for future research in accurate 3D hand mesh estimation from blurry hand images.