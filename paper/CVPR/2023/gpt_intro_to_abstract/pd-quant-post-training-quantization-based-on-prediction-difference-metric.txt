Neural networks have proven to be effective in various real-world applications, but their high memory and computation costs pose challenges when deployed on resource-limited devices. Network quantization is a technique that can compress neural networks by converting values from floating-point to low-bit format. There are two types of quantization: post-training quantization (PTQ) and quantization-aware training (QAT). PTQ requires fewer resources but may result in decreased prediction accuracy. Previous PTQ methods have demonstrated good accuracy with 8-bit or 4-bit quantization. Local metrics are commonly used to search for quantization scaling factors, but a gap between selected and optimal scaling factors has been observed. To address these issues, we propose PD-Quant, an effective PTQ method. PD-Quant improves the performance of PTQ on extremely low bit-width by using a metric that considers the global information from the prediction difference between the quantized and full-precision models. PD-Quant also adjusts the activations for calibration to mitigate overfitting. Experimental results show that PD-Quant achieves state-of-the-art performance in PTQ, significantly improving the prediction accuracy of quantized models. Our contributions include analyzing different metrics, proposing the use of prediction difference in PTQ, and introducing Distribution Correction (DC) to adjust activation distribution.