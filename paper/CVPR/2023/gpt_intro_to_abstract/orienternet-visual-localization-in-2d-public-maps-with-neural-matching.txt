This paper presents a novel approach, called OrienterNet, for localizing single images and image sequences using basic 2D maps. The algorithm estimates the position and heading (3-DoF pose) of a calibrated image in a 2D map, achieving sub-meter accuracy. The approach leverages planimetric maps from OpenStreetMap, which encode location and coarse shape of important objects but not their appearance or height. These maps are compact and can be stored on mobile devices, enabling on-device localization within large areas. The algorithm is probabilistic and can be fused with GPS priors or multiple views from a multi-camera rig or image sequences. The solution outperforms consumer-grade GPS sensors and reaches accuracy levels closer to traditional feature-based pipelines. OrienterNet is trained using a large-scale dataset of crowd-sourced images from cities worldwide. Experimental results demonstrate the superior performance of OrienterNet in driving scenarios and augmented reality use cases. This approach represents a significant advancement towards on-device localization for AR and robotics at a large scale.