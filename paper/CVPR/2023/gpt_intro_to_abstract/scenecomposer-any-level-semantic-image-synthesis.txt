Recently, deep generative models have made significant breakthroughs in generating high-quality images, attracting the attention of artists and designers. To make image generation more controllable, researchers have focused on conditional image synthesis using various types and levels of semantic input. However, existing models lack flexibility in supporting the full creative workflow and provide limited control over image composition. In this paper, we propose a new unified framework for image synthesis from semantic layouts at any combination of precision levels. Inspired by the workflow of artists and designers, we model a semantic layout as a set of semantic regions with free-form text descriptions, allowing users to control the shape precision. We introduce novel ideas, including a text feature map representation and a precision-encoded mask pyramid, to address the challenges in encoding open-domain layouts and varying precision. We also collect data from multiple sources to train our model and demonstrate its effectiveness in both text-to-image and layout-to-image generation with precision control. Our contributions include the development of a unified framework, novel modeling techniques, and a new real-world dataset, showcasing the potential of our model in creative workflows.