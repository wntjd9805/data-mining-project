Novel view synthesis is a challenging problem in computer vision, aiming to generate photo-realistic images from unseen viewpoints. Neural Radiance Fields (NeRF) has gained attention for its ability to model continuous scenes and achieve novel view synthesis. However, NeRF often fails to generalize to new scenes and datasets, necessitating research on improving generalization abilities. Previous works have focused on generalization to unseen scenes but rarely consider synthetic-to-real generalization, which is essential for practical applications with expensive data collection. In this paper, we investigate the effectiveness of synthetic data in NeRF training. We train NeRF models using a synthetic dataset and test them on a real indoor dataset. Surprisingly, we find that synthetic data leads to more artifacts but better fine-grained details in the rendered images. Additionally, models trained on synthetic data tend to predict sharper but less accurate volume densities. To address these issues, we propose geometry-aware contrastive learning, which improves the synthetic-to-real generalization ability by learning multi-view consistent features with geometric constraints. Our method outperforms existing NeRF-based novel view synthesis approaches in terms of high-quality rendering and preserving fine-grained details for unseen scenes. Moreover, our method also achieves better results in real-to-real scenarios. Overall, our contributions include investigating the effects of synthetic data on NeRF-based novel view synthesis, proposing geometry-aware contrastive learning, and achieving state-of-the-art results in both synthetic-to-real and real-to-real settings.