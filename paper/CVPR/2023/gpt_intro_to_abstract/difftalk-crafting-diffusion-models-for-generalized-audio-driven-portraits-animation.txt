Talking head synthesis aims to generate video portraits with given audio and has practical applications in animation, virtual avatars, online education, and video conferencing. Existing mainstream approaches, such as 2D-based methods using generative adversarial networks (GANs), produce competent results but suffer from stability issues and limited image quality. 3D-based methods perform better in generating higher-quality videos but heavily rely on identity-specific training, making them less practical. Some recent works improve model generalization, but fine-tuning on specific identities is still necessary. This paper proposes a crafted conditional Diffusion model for generalized Talking head synthesis (DiffTalk) to address the challenges of generation quality and model generalization simultaneously. Instead of using GANs, DiffTalk leverages Latent Diffusion Models and models the synthesis as an audio-driven denoising process. In addition to audio signals, reference face images and landmarks are incorporated as conditions to guide face identity and head pose for personality-aware video synthesis. This approach enables the learned model to naturally generalize across different identities without further fine-tuning. DiffTalk also allows for higher-resolution synthesis with minimal computational cost. Extensive experiments demonstrate that DiffTalk can synthesize high-quality talking videos for novel identities without additional fine-tuning, outperforming 2D-based methods in image quality and 3D-based methods in model generalization. The contributions of this paper include the proposal of a crafted conditional diffusion model, the incorporation of dual reference images for personality-aware synthesis, and the development of DiffTalk as a high-fidelity and generalized talking head synthesis method.