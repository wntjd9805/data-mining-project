Recent advancements in machine learning have demonstrated the effectiveness of combining large networks and big data. However, the scientific community is interested in understanding the underlying principles and limitations of these approaches. This paper focuses on the concept of Dataset Distillation, which aims to distill a large dataset into a small (synthetic) one that can yield comparable performance when used for training models. Previous methods have optimized the raw pixel values of the images in the synthetic dataset, but they face challenges related to cross-architecture generalization and the generation of visually noisy images when applied to larger-resolution datasets. To address these challenges, this work proposes Generative Latent Distillation (GLaD), which utilizes a deep generative prior by parameterizing the synthetic dataset in the intermediate feature space of generative models. GLaD acts as an add-on module that can improve the performance of existing and future methods of dataset distillation. The choice of generative model and latent space in GLaD has implications for the distillation results, with an intermediate latent space achieving a balance between realism and flexibility. Extensive experiments on CIFAR-10 and ImageNet subsets at various resolutions demonstrate the significant improvements in cross-architecture generalization achieved by integrating GLaD with three current distillation algorithms. Additionally, GLaD reduces high-frequency noise in high-resolution datasets and produces visually pleasing images with potential applications in art and design. Overall, GLaD provides a scalable approach to distilling large-scale datasets by leveraging generative models and differentiable parameterizations. The contributions of this work include the proposal of GLaD, the analysis and ablation studies highlighting the importance of a deep generative prior in addressing the challenges of dataset distillation, and the demonstration of GLaD's robustness to different types of generators.