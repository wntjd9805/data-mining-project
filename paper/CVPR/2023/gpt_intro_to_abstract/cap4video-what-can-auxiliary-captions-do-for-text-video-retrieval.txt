Text-video retrieval is an important task in video-language learning, and recent advancements in image-language pre-training have led researchers to focus on expanding pre-trained image-language models, such as CLIP, to tackle this task. The research path has evolved from global matching to fine-grained matching, and these studies have shown significant improvements in performance. CLIP offers powerful visual and textual representations that are pre-aligned, which reduces the challenge of cross-modal learning. Additionally, methods that fine-tune the pre-trained vision and text encoders using sparsely sampled frames have proven effective in learning cross-modal alignment. However, in real-life scenarios, online videos often come with related textual content, such as titles or tags, which can be used to describe the video content and improve the text-video retrieval task. One possible solution is to crawl video titles from the video website, but this method relies on annotations and can be unreliable. Another solution is to generate captions using zero-shot video caption models, which leverage knowledge-rich pre-trained models like ZeroCap. In this paper, we propose the Cap4Video learning framework, which utilizes captions in three ways: augmenting training data, leveraging feature interaction, and enhancing output scores. We also employ a two-stream architecture to reduce model bias. Our framework achieves state-of-the-art performance on four video benchmarks, demonstrating its effectiveness in enhancing text-video retrieval.