Deep Neural Networks (DNNs) have shown impressive results in computer vision tasks, but they require a large amount of labeled data. However, label quality is often hard to guarantee, and many benchmark datasets contain noisy labels. Various research approaches have been proposed to address the noisy-label problem, such as robust loss functions and sample screening. However, existing methods still have a significant performance gap compared to models trained with clean labels. This paper introduces the Gradient Switching Strategy (GSS) to prevent the continuous damage of mislabeled samples to model training. GSS assigns random gradient directions to cancel out the negative impact of mislabeled samples, especially for uncertain samples. In the training stage, different gradient directions are assigned probabilities based on the original noisy label, predictions, and randomness. For high-confidence samples, the model is optimized using their potential principal directions. Experimental results demonstrate that GSS effectively prevents the damage of mislabeled samples and improves accuracy compared to existing methods for high noise rates. The proposed GSS can be integrated into existing frameworks for noisy-label learning and achieve comparable performance to models trained with clean samples. The contributions of this paper include clarifying the continuous damage caused by mislabeled samples, proposing the GSS strategy, and providing theoretical analysis and experimental results to demonstrate its effectiveness.