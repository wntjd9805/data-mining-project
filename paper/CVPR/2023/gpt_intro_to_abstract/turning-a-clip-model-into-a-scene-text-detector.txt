This paper introduces a new method, TCM, for scene text detection by incorporating the CLIP model. The TCM framework utilizes cross-modal interaction and visual prompt learning to capture fine-grained information and align image and text embeddings. Compared to previous pretraining approaches, TCM does not require a pretraining process and can directly fine-tune for the text detection task. The advantages of TCM include easy integration with existing detectors, effective few-shot training capability, promising domain adaptation ability, and leveraging prior knowledge from the CLIP model. Experimental results show that TCM improves the performance of the baseline detector by an average of 22% using only 10% of labeled data. Additionally, TCM outperforms previous scene text pre-training methods.