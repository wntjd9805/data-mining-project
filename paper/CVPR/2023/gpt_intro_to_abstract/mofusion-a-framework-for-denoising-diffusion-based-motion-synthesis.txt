3D human motion synthesis is a challenging problem with various applications in computer vision, robotics, animation, and gaming. While recent works have made significant progress using reinforcement learning, deep generative models, and deterministic approaches, there are still open challenges in improving motion variability, realism, and synthesis fidelity.Conditional human motion synthesis aims to generate motions that align with user-specified conditioning signals while maintaining diversity. Current approaches often rely on generative techniques like conditional variational autoencoders (CVAE), normalizing flows, or generative adversarial networks (GANs), each with their own strengths and limitations.Inspired by the success of denoising diffusion probabilistic models (DDPM) in image and audio synthesis, this paper proposes MoFusion, a new approach for human motion synthesis using DDPM. The authors demonstrate the effectiveness of diffusion models for this task and introduce a lightweight 1D U-Net network for reverse diffusion to reduce inference times. They also incorporate domain-inspired kinematic losses into the diffusion framework using a time-varying weight schedule, a primary contribution of this work.The resulting MoFusion framework enables the synthesis of diverse, temporally and kinematically plausible, and semantically accurate human motions. The authors evaluate MoFusion on music-conditioned choreography generation and text-conditioned motion synthesis tasks. They show that MoFusion overcomes limitations of existing methods, such as repetitive motions in choreography generation and disambiguation issues in text-to-motion synthesis. Additionally, the diffusion framework allows for interactive editing of synthesized motion and enables applications like motion forecasting and in-betweening.The proposed MoFusion framework is evaluated on the AIST++ and HumanML3D datasets, and quantitative evaluations as well as a user study demonstrate the improvements in both sub-tasks. Overall, this work presents the first method for conditional 3D human motion synthesis using denoising diffusion models and showcases the potential of diffusion models in generating diverse and realistic human motions.