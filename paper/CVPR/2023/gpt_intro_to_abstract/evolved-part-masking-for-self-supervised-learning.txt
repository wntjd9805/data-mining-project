In recent years, there has been a significant increase in the demand for representation learning and data requirements in deep neural networks such as CNN and vision transformers. To address the need for labeled data, Masked Language Modeling (MLM) has been utilized for training natural language processing models through self-supervised learning. Inspired by this success, Masked Image Modeling (MIM) has been proposed as a method to pre-train vision models on unlabeled images. However, generating mask patterns for images presents unique challenges compared to sentences. Existing MIM methods use different criteria to generate mask patterns, including random masking of local regions, preservation of crucial cues, or complete masking of object regions. In this paper, we propose an evolved part masking approach that generates mask patterns based on the capability of the vision model being trained. We leverage an adaptive part partition module and a graph cut algorithm to generate mask patterns that evolve with the training stages. Our method incorporates contextual cues among image patches to enhance the training of vision models. We evaluate our approach on popular MIM architectures and observe significant performance improvements, particularly in semantic segmentation tasks. Compared to other self-supervised learning methods, our method achieves comparable or superior performance with fewer or similar training epochs. Our approach is an original effort in evolved part masking for self-supervised learning and does not rely on additional pre-trained models or annotations, ensuring training efficiency and enhancing the generalization ability of trained models.