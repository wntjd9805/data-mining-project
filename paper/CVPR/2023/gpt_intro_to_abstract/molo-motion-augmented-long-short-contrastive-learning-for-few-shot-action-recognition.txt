Recently, action recognition has made impressive progress and has shown potential in various application fields. However, the reliance on large amounts of manual data annotation hinders scalability to unseen categories. Few-shot action recognition, which aims to identify novel classes with limited labeled videos, is a promising direction to alleviate this issue. Most existing few-shot action recognition approaches adopt metric-based meta-learning strategies to map videos into a feature space and perform alignment metrics for query label prediction. However, these approaches have limitations in terms of the lack of global context awareness and the neglect of motion dynamics. Existing methods mainly focus on local frame-level alignment and do not explicitly involve essential global information. Additionally, they do not explore the rich motion cues between frames, resulting in suboptimal performance.To address these limitations, we propose a motion-augmented long-short contrastive learning (MoLo) method that jointly models global contextual information and motion dynamics. We integrate global context into the local matching process using a long-short contrastive objective, which enforces frame features to predict the global context of videos belonging to the same class. For motion compensation, we design a motion autodecoder to extract motion features between frame representations by reconstructing pixel motions. Our proposed MoLo method enables efficient and comprehensive exploitation of temporal contextual dependencies and motion cues for accurate few-shot action recognition.Our contributions can be summarized as follows: (1) We propose the MoLo method to better leverage global context and motion dynamics in few-shot action recognition. (2) We introduce a long-short contrastive objective to enhance the perception of comprehensive global information in local frame features and a motion autodecoder for explicit extraction of motion cues. (3) We conduct extensive experiments on multiple widely-used benchmarks, demonstrating that MoLo outperforms other advanced few-shot techniques and achieves state-of-the-art performance.Overall, our proposed MoLo method addresses the limitations of existing few-shot action recognition approaches by incorporating global context awareness and motion dynamics. Experimental results show the effectiveness of our method in achieving superior performance compared to baseline methods.