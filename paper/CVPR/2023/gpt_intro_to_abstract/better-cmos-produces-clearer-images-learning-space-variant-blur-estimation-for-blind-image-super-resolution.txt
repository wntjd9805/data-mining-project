This paper introduces the problem of blind image super-resolution (SR), which aims to reconstruct high-resolution (HR) images from low-resolution (LR) images with unknown degradations. The existing degradation models, such as bicubic downsampling and traditional degradation, are limited in their ability to accurately represent real-world blur scenarios. In this paper, the focus is on addressing the challenges posed by space-variant blur, which occurs due to factors like object motion and out-of-focus. A new degradation method and two corresponding datasets, NYUv2-BSR and Cityscapes-BSR, are proposed to support research on space-variant blur in the SR domain. Additionally, a novel model named Cross-MOdal fuSion network (CMOS) is introduced to improve the accuracy of blur estimation at the boundaries of different blur regions. The CMOS model incorporates semantic information to enhance feature learning and employs a feature Grouping Interactive Attention (GIA) module for effective interaction between blur and semantic features. Experimental results demonstrate that CMOS, combined with existing non-blind SR models, achieves state-of-the-art SR performance in both space-variant and space-invariant blur scenarios. The contributions of this work include the introduction of new datasets, the design of the CMOS model, and the development of the GIA module for feature interaction.