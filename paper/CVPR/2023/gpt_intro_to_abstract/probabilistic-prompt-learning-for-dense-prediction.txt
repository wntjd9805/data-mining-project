Dense predictions, such as semantic segmentation, instance segmentation, and object detection, are important computer vision problems that aim to generate detailed pixel-level predictions of a scene. The high cost of collecting dense annotations has led to the use of pre-training and fine-tuning approaches. However, these approaches suffer from scalability issues when there is a significant gap between the pre-trained and target tasks. Vision-language pre-trained models (VLM) have shown success in downstream tasks by leveraging the semantic relationship between text and image representations. Prompting techniques have been used to deploy VLM to these tasks, but manually designing task-specific prompts is laborious. To address this, learning-based prompting techniques have been introduced to automatically construct prompts based on VLM. However, these techniques are not fully compositional in dense prediction tasks due to semantic ambiguity and high uncertainty in visual representations. In this paper, we propose a Probabilistic Prompt Learning (PPL) that explores learning appropriate textual descriptions using visual cues in a probabilistic embedding space. We introduce a set of prompts that express class-agnostic attributes, and model each attribute distribution as a distinct normal distribution to effectively capture diverse and informative attributes. We also introduce a probabilistic pixel-text matching loss to handle high uncertainty. Our contributions include the novel PPL method, the introduction of a probabilistic pixel-text matching loss, and extensive experiments demonstrating the effectiveness of our approach on dense prediction tasks.