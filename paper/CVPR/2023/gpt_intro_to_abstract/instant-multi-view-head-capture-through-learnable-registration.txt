Capturing large datasets of 3D heads with varying facial expressions and poses is essential for realistic head avatar modeling. Traditionally, this involves a two-step process of capturing unstructured 3D scans and then performing non-rigid registration to unify the mesh topology. However, this approach has drawbacks, including the need for overlapping views, holes and noise in the scans, and the computational expense. Manual clean-up is often required to enhance the quality of the output meshes, but this is not feasible for large-scale captures. To address these challenges, we propose a more practical approach that directly predicts 3D heads from calibrated multi-view images. Our method, TEMPEH, achieves accurate and efficient 3D head estimation without the need for manual input. Existing methods for direct 3D face recovery from multi-view images have high computational costs and require careful parameter selection. Learning-based approaches using 3D morphable models have limitations in terms of quality and expressiveness. The recent ToFu method improves on these approaches but has its own limitations. TEMPEH builds on ToFu's framework but overcomes its limitations by optimizing weights and registering raw scans, inferring the entire head from images alone, accounting for surface visibility, and using a spatial transformer module for improved accuracy. TEMPEH is the first framework to accurately capture the entire head from multi-view images in near interactive rates. It is trained and evaluated on a dynamic 3D head dataset and is publicly available.