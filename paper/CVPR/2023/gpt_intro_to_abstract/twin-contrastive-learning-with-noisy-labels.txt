Deep neural networks have achieved remarkable performance in classification tasks, largely due to the availability of curated datasets with clean annotations. However, obtaining datasets with noisy annotations is much easier, from sources like online shopping websites, crowd-sourcing, or Wikipedia. Unfortunately, using such mislabeled data can significantly degrade the performance of deep neural networks. Thus, there has been a growing interest in training noise-robust classification networks. Existing methods mitigate the influence of noisy labels by proposing robust loss functions, reducing the weights of noisy labels, or correcting the noisy labels. Label correction methods have shown potential, but they typically require an iterative sample selection process. Contrastive learning has also been effective in handling noisy labels, but they only consider label noise within a small neighborhood. To address these issues, this paper presents TCL, a novel twin contrastive learning model that leverages label-free unsupervised representations and label-noisy annotations. The proposed method utilizes contrastive learning to learn discriminative image representations and constructs a Gaussian mixture model (GMM) over the representations. Unlike unsupervised GMM, TCL links the label-free GMM and label-noisy annotations by incorporating model predictions for updating the parameters of GMM. Furthermore, an out-of-distribution (OOD) label noise detection method is proposed, which models the data distribution and is robust to strong label noise. A bootstrap cross-supervision technique is introduced to reduce the impact of wrong labels, and contrastive learning and Mixup techniques are leveraged to enhance robustness and align representations with estimated labels. Experimental results demonstrate the superiority of the proposed method compared to existing state-of-the-art methods, particularly in extremely noisy scenarios with a 7.5% improvement. Overall, this paper contributes a novel approach for addressing label noise in deep neural networks.