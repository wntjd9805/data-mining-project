Face recognition has become a prevalent form of authentication in various biometric applications. However, recent research in adversarial machine learning has revealed that face recognition models based on deep neural networks are highly vulnerable to adversarial examples, which can lead to serious security problems. While there have been extensive attempts to develop adversarial attacks on face recognition models, most of these attacks are limited to the digital space and do not evaluate the robustness of practical systems. In this paper, we aim to develop practical and effective physical adversarial attacks on face recognition systems that can simultaneously deceive black-box recognition models and evade defensive mechanisms. We propose the use of adversarial textured 3D meshes, which can be 3D printed and applied to real human faces. We also leverage 3D Morphable Model (3DMM) to perform low-dimensional optimization and generate adversarial meshes that are constrained on the data manifold of realistic 3D faces. Our experimental results demonstrate the effectiveness of our method in misleading popular commercial recognition systems, including unlocking mobile phones and automated access control systems. Additionally, we propose our method as a reliable technique for evaluating the robustness of face recognition systems and as a data augmentation strategy to improve defensive abilities.