Visual search systems are widely used for identifying and retrieving nearest neighbors based on their distance to a query feature. These systems consist of multiple models deployed on different platforms, such as cloud, mobile, and smart cameras, which interact with each other through visual features. However, deploying a unified model that suits all platforms may result in resource waste and limited accuracy. To address this issue, we propose a Switchable representation learning Framework with Self-Compatibility (SFSC) that allows the deployment of different sub-models on different platforms to better utilize resources and achieve higher accuracy. SFSC achieves compatibility between any two sub-models, referred to as "self-compatibility", by constraining similar data processed by different models to be close in the feature space while dissimilar data are far apart. Existing research focuses on one-to-one compatibility, which limits achieving many-to-many compatibilities. In SFSC, we resolve conflicts between sub-models by considering the gradient magnitude and gradient direction. We estimate the uncertainty of different sub-models to weight the gradient, enabling optimization priorities. We also use gradient projection to avoid mutual interference and find a generic optimal direction for all sub-models. Our experiments demonstrate that SFSC outperforms existing methods on benchmark datasets, achieving 6% to 8% performance improvements compared to deploying a unified model.