This paper introduces a novel approach for learning an implicit representation of the physical imaging process of a camera. Most existing neural rendering methods assume that each pixel's RGB values are precisely the captured radiance field, neglecting factors such as aperture size, exposure time, and image processing. The authors propose a self-supervised framework that combines a blur generator module and a tone mapper module to model the camera imaging process. They also demonstrate the effectiveness of their approach in tasks such as all-in-focus imaging and HDR imaging. Experimental results show that their method outperforms baseline methods and can recover high-quality HDR images with fewer input images.