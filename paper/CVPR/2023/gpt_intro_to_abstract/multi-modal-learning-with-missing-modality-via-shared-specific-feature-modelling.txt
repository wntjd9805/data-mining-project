Recently, multi-modal learning has gained attention in computer vision and medical image analysis. This involves using audio, images, and short videos for model prediction in various applications. In the medical field, combining different modalities to improve diagnosis accuracy has become increasingly important. However, most existing multi-modal methods require the completeness of all modalities, which limits their applicability in real-world scenarios with missing modalities. To address this challenge, previous studies have proposed missing-modality multi-modal approaches, but they are specific to classification or segmentation tasks. In this paper, we propose a new approach called Shared-Specific Feature Modelling (ShaSpec) that can handle missing modalities in both training and testing, as well as dedicated and non-dedicated training. ShaSpec has a simpler and more effective architecture that can be easily adapted to classification and segmentation tasks. Our results on computer vision and medical imaging benchmarks demonstrate that ShaSpec achieves state-of-the-art performance, outperforming competing approaches in terms of segmentation accuracy for brain tumour detection.