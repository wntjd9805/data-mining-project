Research in automatic analysis of facial behavior has primarily focused on recognizing the six universal expressions and the neutral state. However, these basic expressions are limited in representing the complexity and subtlety of our daily affective displays. Compound expressions, which are combinations of basic expressions, provide a better representation of affective displays in everyday interactions. The design of systems capable of understanding the community perception of expressive attributes and affective displays is of increasing interest. Deep learning methods have greatly improved expression recognition performance, but they are limited by the lack of labeled data and the time-consuming and complex annotation process. While initial research focused on posed behavior in controlled conditions, progress is seen when datasets are collected in unconstrained conditions, referred to as in-the-wild. However, existing in-the-wild databases for compound expression recognition are small in size, imbalanced, static (containing only images), and lack proper training-validation-test set splits. To address these limitations, the authors collected and annotated the largest, diverse, in-the-wild audiovisual database, C-EXPR-DB, for 12 compound expressions. This database also includes annotations for continuous dimensions of valence-arousal, speech detection, facial landmarks and bounding boxes, action units, and facial attributes. The authors propose a novel methodology, C-EXPR-NET, which utilizes multi-task learning for compound expression recognition and action unit detection. They use the FACS manual for AU definitions and incorporate AU semantic descriptions as auxiliary information. For compound expression recognition, they use a multi-label formulation and apply a softmax activation and the Kullback-Leibler divergence as the loss function. The authors observed that while MTL improves compound expression recognition performance, it harms action unit detection performance. To address this, they propose a distribution matching approach based on task relatedness to alleviate negative transfer and further boost compound expression recognition performance. The authors conducted extensive experiments that demonstrate the superiority of C-EXPR-NET over state-of-the-art methods for compound expression recognition and basic expression recognition, regardless of whether the AU annotations are manual or automatic. Additionally, C-EXPR-NET effectively generalizes its knowledge to new emotion recognition contexts in a zero-shot manner.