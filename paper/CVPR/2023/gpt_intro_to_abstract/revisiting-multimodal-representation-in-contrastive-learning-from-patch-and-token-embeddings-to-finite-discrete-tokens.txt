The Contrastive Language-Image Pre-training (CLIP) framework has shown promising capabilities in learning powerful and transferable feature representations by aligning text and image information. However, CLIP does not consider that images and text captions convey information at different levels of granularity, which can hinder multimodal representation learning and lead to performance degradation. In this paper, we propose a new approach called Finite Discrete Tokens (FDT) based representations, which unifies the information granularities of images and texts. FDT is a set of learnable tokens that encode cross-modal shared semantic concepts. We present an overview of our method, where image and text representations are constructed based on the combinations of FDT shared between modalities. The encoders and FDT are trained using the InfoNCE loss to align the representations of matched image-text pairs. Our experiments demonstrate that our approach consistently enhances performance across different pre-training settings and downstream tasks. Our method also alleviates the model degradation problem and learns more comprehensive feature representations compared to CLIP. The learned FDT successfully capture and align visual-semantic concepts.