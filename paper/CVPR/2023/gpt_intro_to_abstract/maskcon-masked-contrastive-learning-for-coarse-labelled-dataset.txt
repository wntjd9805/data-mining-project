Supervised learning with deep neural networks has achieved remarkable success in computer vision tasks like image classification. However, these methods heavily rely on large-scale and high-quality annotated datasets, which are time-consuming and labor-intensive to produce. To overcome this reliance on human-localization, various learning frameworks have been proposed, such as self-supervised learning, semi-supervised learning, and learning with noisy data. In this work, we focus on the problem of reducing annotation effort by learning fine-grained representations with a coarsely-labelled dataset. Coarse labels are easier to obtain and are commonly used in specialized domains like medical pathology image recognition. While learning with coarse labels has been less investigated, recent studies have explored learning with coarse labels in the few-shot learning setting. Other related works combine fully supervised learning costs with self-supervised contrastive loss. In contrast, we propose a novel learning scheme called Masked Contrastive Learning (MaskCon) that leverages inter-sample relations and soft labels to estimate fine inter-sample relations. Our method significantly outperforms state-of-the-art approaches on various datasets, including CIFARtoy, CIFAR100, ImageNet-1K, Stanford Online Products, and Stanford Cars196.