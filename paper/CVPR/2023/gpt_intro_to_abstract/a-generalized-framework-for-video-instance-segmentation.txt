Video Instance Segmentation (VIS) is an important task in computer vision that involves identifying, segmenting, and tracking objects in videos. While many solutions have been proposed, recent online methods using image-based backbones have shown significant performance improvements. These findings challenge the common belief that end-to-end semi-online or offline approaches trained on longer video clips would better model long-range object relationships. In this paper, we propose a Generalized VIS framework called GenVIS, which aims to minimize the gap between training and inference in long videos. We introduce a novel training strategy that utilizes multiple clips to prioritize inter-clip tracking and improve temporal reasoning. Additionally, we propose a new learning criterion, called Unified Video Label Assignment (UVLA), that enables seamless association between consecutive clips, removing the need for heuristics during inference. Our approach outperforms previous methods on challenging long video benchmarks, demonstrating its effectiveness in handling complex real-world videos. We also propose the use of a memory mechanism to handle very long videos or streaming video scenarios, further bridging the gap between training and inference. GenVIS achieves state-of-the-art results and demonstrates strong generalization capabilities in online and semi-online settings.