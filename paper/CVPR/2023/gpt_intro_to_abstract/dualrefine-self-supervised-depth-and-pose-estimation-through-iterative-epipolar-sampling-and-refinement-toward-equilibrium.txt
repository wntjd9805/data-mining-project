Optimization of 3D point coordinates and camera poses in structure-from-motion (SfM) is crucial for various applications in robotics, autonomous driving, and AR/VR. However, traditional SfM techniques struggle with challenges like texture-less regions and dynamic objects. To address these challenges, deep learning models have been developed to predict depth from monocular images. These models can accurately predict depth without relying on geometric information. Recently, self-supervised training of depth and pose models has gained attention, as it eliminates the need for ground truth while maintaining precision comparable to supervised methods. These models synthesize neighboring images in a video sequence and ensure consistency between them. Incorporating multi-frame data improves depth predictions by integrating geometric information. In this context, the accuracy of matching costs computation is crucial, as demonstrated by DepthFormer. However, their approach had a large memory cost. Self-supervised multi-frame models, unlike stereo tasks, do not assume known camera poses and utilize learned pose estimates. Refining the pose estimates can improve matching costs and depth predictions. In this work, we propose a depth and pose refinement model trained in a self-supervised framework. Our iterative update module utilizes epipolar geometry and direct alignment to refine depth and pose estimates. This approach is memory efficient and improves depth estimates and global visual odometry results. Overall, our model demonstrates competitive performance compared to state-of-the-art models.