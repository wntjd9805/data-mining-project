Video human matting is a challenging task that involves estimating an accurate alpha matte to extract the foreground of a person from each frame of a video. Existing approaches for video matting face difficulties in maintaining spatial and temporal coherence. Some methods rely on image matting models but suffer from inconsistencies across frames, leading to flickering artifacts. Others employ dense trimaps or additional background images, but these solutions are either expensive or limited in their applicability.In this paper, we propose AdaM, a manual-free video matting method that addresses the limitations of previous approaches. AdaM leverages a data-driven estimation of foreground masks to effectively distinguish between foregrounds and backgrounds. Our network architecture and training scheme utilize both long-range and short-range spatial and motion cues for accurate alpha matte prediction.We evaluate AdaM on the VM and CRGNN benchmarks, and the results demonstrate its top-tier performance compared to existing methods such as MODNet and RVM. Our qualitative sample results showcase the superior quality of alpha mattes produced by AdaM on real video scenes.Overall, AdaM offers a promising solution for video human matting, combining the benefits of manual-free models with the ability to accurately handle complex and unconstrained videos.