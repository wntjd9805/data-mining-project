Neural rendering methods, such as Neural Radiance Fields (NeRFs), have emerged as a new modality for representing and reconstructing scenes, particularly for novel view synthesis. Efforts are being made to make NeRFs more efficient and accessible in use-cases with limited computational resources. However, there are obstacles to editing and manipulating NeRF scenes, such as the complex representation and the need to preserve 3D properties. Obtaining input segmentation masks is also challenging, as it is burdensome for users to annotate multiple images. In this paper, we propose an integrated method that takes multiview images of a scene, extracts a 3D mask with minimal user input, and fits a NeRF to generate a plausible appearance and geometry for the target object. Our approach combines interactive multiview segmentation and full 3D inpainting in a single framework. We leverage existing 3D-unaware models for segmentation and inpainting and transfer their outputs to 3D space in a view-consistent manner. Our framework starts from a small number of user-defined image points and initializes masks using a video-based model, then lifts them into a coherent 3D segmentation using semantic NeRF fitting. We also use a pretrained 2D inpainter to inpaint the multiview image set and a customized NeRF fitting process to reconstruct the 3D inpainted scene, considering perceptual losses and depth images. We provide extensive evaluations and introduce a benchmark dataset for comparing scene inpainting methods. Our contributions include a complete process for 3D scene manipulation, an extension of 2D segmentation models to the multiview case, a novel formulation for 3D inpainting in NeRFs, and a new dataset for 3D object removal evaluation with corresponding ground-truth images.