Deep neural networks have achieved impressive performance on various tasks, but their success often relies on a large amount of labeled training data, which is expensive and time-consuming to acquire. Semi-supervised learning (SSL) has emerged as a way to reduce the dependency on human annotation by leveraging unlabeled data. However, SSL assumes that the labeled and unlabeled data are collected from the same distribution, which is often not the case in real-world applications. Unlabeled data typically contains out-of-distribution (OOD) samples that are not seen in the labeled data, and training SSL models with these OOD samples can result in degraded performance. To address this issue, robust semi-supervised learning (Robust SSL) has been proposed to train classification models that perform stably even when the unlabeled set is corrupted by OOD samples. Typical methods focus on discarding OOD samples at the sample level, but they fail to consider the semantic-level pollution caused by the classification-useless semantics from OOD samples. In this paper, we propose an Out-of-distributed Semantic Pruning (OSP) method to address this problem and achieve effective robust SSL. Our approach consists of two main modules: an aliasing OOD matching module and a soft orthogonality regularization module. We evaluate the effectiveness of our approach on several benchmark datasets and demonstrate significant improvements compared to state-of-the-art alternatives. Our contributions include exploiting the OOD effects at the semantic level, developing an adaptive OOD matching module, and introducing a soft orthogonality regularization to enhance feature discrimination between ID and OOD samples. This work addresses the need for robust SSL algorithms that can handle unlabeled datasets containing OOD samples.