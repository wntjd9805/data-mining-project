Reconstructing 3D scenes from multi-view RGB images is a challenging task in computer vision with numerous applications. Existing methods estimate per-view depth maps, but struggle to recover the depth of texture-less planar areas. Recent data-driven methods alleviate this problem but require expensive 3D supervision or lack fine-grain details. Neural implicit representations have shown impressive results without 3D supervision, but struggle with texture-geometry ambiguities. Some approaches address this by introducing additional priors from trained models, but all rely on annotated datasets. In this work, we propose a novel neural 3D reconstruction framework called S3P, which utilizes a self-supervised super-plane constraint to guide the reconstruction process of planar regions. We group pixels with similar normal values into super-planes and apply a constraint to force consistency. An automatic filtering strategy removes outliers, and non-plane edge regions are masked out. Our method significantly improves the reconstruction quality of texture-less planar regions and can be extended to unsupervised plane reconstruction. Experimental results show that our method outperforms state-of-the-art supervised methods for plane reconstruction.