Object segmentation from videos is essential for various vision and robotics tasks. However, most existing methods rely on pixel-wise human annotations, which limit their practical applications. In this paper, we focus on learning object segmentation from entirely unlabeled videos. We draw inspiration from the Gestalt law of common fate, which suggests that objects that move at the same speed belong together. We categorize unsupervised video object segmentation (UVOS) methods into three main types. The first type uses motion segmentation methods that rely on a pretrained optical flow estimator to segment foreground objects based on motion signals. The second type utilizes motion-guided image segmentation methods, which use motion segmentation loss to guide appearance-based segmentation. The third type combines appearance segmentation and motion estimation methods, learning motion and segmentation simultaneously. However, common fate, while effective at binding parts of heterogeneous appearances into a single whole moving object, is not a reliable indicator of objectness. It fails to handle articulated objects and often results in excessive objectness for objects with reflections. To address these limitations, we propose a method that bootstraps full objectness from common fate in unlabeled videos. We allow different parts of an articulated object to assume slightly different speeds and rely on visual appearance grouping and statistical figure-ground relevance to detect objects from reflections. Our method consists of two stages: Stage 1 learns to discover objects from motion supervision with relaxed common fate, and Stage 2 refines the segmentation model based on image appearance. We also propose a novel unsupervised hyperparameter tuning approach that does not require any annotations. We evaluate the performance of our method on UVOS benchmarks and achieve state-of-the-art results, surpassing existing methods in terms of segmentation accuracy. Our method has several advantages, including the ability to segment stationary and articulated objects in single images and the capability to tune hyperparameters without external annotations.