This paper addresses the problem of inferring the geometric structure of a scene from a single image, which is crucial for various applications in computer vision. While previous approaches focused on predicting per-pixel depth values, recent advancements in deep learning have seen the rise of methods that can accurately estimate depth from a single image. However, these methods still have limitations, as they only model a single depth value per pixel and cannot reason about occlusions or objects behind other objects in the image. Additionally, novel view synthesis, which involves generating views of a scene from different perspectives, also faces challenges in terms of generalization to new scenes. This paper proposes a novel approach that tackles these limitations by generalizing the depth prediction formulation to a continuous density field. The proposed architecture consists of an encoder-decoder network that predicts a dense feature map from the input image, which is used to condition a density field inside the camera frustum. This density field can be evaluated at any spatial point using a multi-layer perceptron (MLP). The authors introduce three key novelties in their method: color sampling directly from input frames, shifting capacity to the feature extractor, and a new loss formulation called Behind the Scenes, which allows for reconstruction of novel views. The authors conduct experiments on various datasets to evaluate the performance of their approach in terms of 3D capture, depth estimation accuracy, and novel view synthesis. The results demonstrate the potential of the proposed method, achieving competitive performance and advancing the state-of-the-art in depth estimation and view synthesis. Ablation studies are also conducted to analyze the impact of design choices in the proposed approach.