Dense video captioning is a challenging task that involves both localizing and captioning events in untrimmed videos. Existing methods typically use two-stage approaches or task-specific components, and rely on manually annotated datasets of limited size. In this paper, we propose Vid2Seq, a video language model that predicts event captions and their temporal boundaries in a joint manner using a single sequence of tokens. To overcome the lack of large-scale training data, we leverage unlabeled narrated videos for pretraining. We demonstrate the effectiveness of Vid2Seq through extensive experiments and show that it achieves state-of-the-art performance on various dense video captioning benchmarks. Additionally, our model excels at generating paragraphs of text describing the video, even without using ground-truth event proposals. Vid2Seq also generalizes well to the task of video clip captioning and performs well in a few-shot dense video captioning setting. Overall, our contributions include the introduction of Vid2Seq, the effective use of transcribed speech and timestamps in unlabeled videos as weak supervision, and the improvement of state-of-the-art performance on multiple video captioning datasets.