Deep learning has achieved remarkable success in computer vision tasks and has emerged as a promising technique for research in multiple fields. However, Deep Neural Networks (DNNs) have been found to be highly vulnerable to adversarial examples, which are visually indistinguishable perturbations that can significantly disrupt DNN inference. These examples are hard to detect and pose a security threat to deep learning applications. Various defense methods, such as adversarial training, have been proposed to improve the robustness of DNNs. Adversarial training focuses on aligning the distribution between natural and adversarial examples to preserve consistency in DNN predictions. However, existing adversarial training methods suffer from a decrease in standard accuracy and limited data and model capacity. In this paper, we propose a novel adversarial training framework called Inverse Adversarial Training (IAT) that bridges the distribution gap between adversarial examples and the high-likelihood region of their belonging classes. We introduce an inverse procedure for generating adversarial examples that are more likely to be correctly classified by DNNs. We also design a class-specific inverse adversary generation method and incorporate a momentum mechanism for stable training. Our comprehensive experiments demonstrate the superiority of IAT compared to state-of-the-art adversarial training methods in terms of robustness and efficiency. Our method can be adapted to larger models and can also be combined with single-step adversarial training methods for enhanced robustness at a low cost.