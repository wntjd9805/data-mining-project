The paper introduces the concept of scene graph generation (SGG), which is a structured representation for describing image semantics using a graph-based approach. However, SGG in its current stage faces two challenges: the need for expensive manual annotation of ground-truth scene graphs and limitations in recognizing novel objects outside of training data. The paper proposes a solution by leveraging a pre-trained visual-semantic space (VSS) obtained through language-image pre-training. This allows for cheap acquisition of scene graph supervision and enables novel category prediction in SGG. The paper presents a novel SGG model called Visual-Semantic Space for Scene graph generation (VS3), which takes a raw image and a text prompt as inputs and uses the VSS to perform object detection and relation prediction. The proposed approach is validated through extensive experiments on the Visual Genome benchmark, achieving state-of-the-art performances in both supervised and language-supervised SGG, as well as closed-set and open-vocabulary SGG. The contributions of the paper include the use of a pre-trained VSS to address obstacles in SGG, the introduction of the VS3 model, and the validation of the approach through experiments.