Recently released models in the field of computer science, such as DALLÂ·E 2, Ima-gen, Parti, and Stable Diffusion, have achieved unprecedented levels of image quality and flexibility in image generation. However, these models rely on iterative inference, which incurs high computational costs. In contrast, Generative Adversarial Networks (GANs) can generate images efficiently through a single forward pass. While GANs have excelled at modeling single or multiple object classes, scaling them to complex datasets has remained challenging due to instabilities in the training process. This paper investigates whether GANs can be further scaled up and benefit from available resources. The authors experiment with StyleGAN2 and identify key issues, proposing techniques to stabilize training while increasing model capacity. These techniques include scaling the generator's capacity using a bank of filters, and incorporating self-attention and cross-attention mechanisms. The authors also reintroduce multi-scale training, which improves image-text alignment and low-frequency details of generated outputs. Through careful tuning, they achieve stable and scalable training of a one-billion-parameter GAN (GigaGAN) on large-scale datasets. The results demonstrate that GigaGAN can generate high-quality images at interactive speeds and ultra high-resolution. GigaGAN also outperforms other diffusion and autoregressive models in terms of practical advantages and controllable image synthesis capabilities. Overall, this work shows that GANs can still be a viable option for text-to-image synthesis and should be considered for future aggressive scaling.