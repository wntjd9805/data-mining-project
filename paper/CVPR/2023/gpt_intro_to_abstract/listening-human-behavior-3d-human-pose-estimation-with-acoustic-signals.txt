The ability to capture human behavior, specifically 3D poses, has numerous potential applications. However, current technologies such as conventional cameras and RF/WiFi measurements have limitations such as occlusion and restricted usage in specific environments. This paper proposes the use of low-level audio signals as an alternative solution, as they are not affected by poor lighting conditions and do not interfere with electronic systems. Acoustic signals, particularly ultrasonic waves, have longer wavelengths and are less occluded compared to visible light and RF/WiFi signals. Previous studies have used acoustic signals for cross-modal analysis and human sensing, but they rely on high-level semantics that raise privacy concerns. This paper introduces a new task, 3D human pose estimation using only low-level acoustic signals, and aims to answer three questions: whether low-level acoustic signals provide enough information for pose reconstruction, the minimum required hardware for the task, and effective inference algorithms. The proposed method utilizes a convolutional neural network (CNN) framework that takes multi-channel audio features as inputs and directly predicts 3D joint locations. The model accounts for occlusion by integrating phase features that represent time difference of arrival (TDOA) and learns to handle differences in physique with adversarial learning. To train the network, a dataset is created using an active acoustic-sensing system in both anechoic and noisy environments. The contributions of this paper include tackling the novel task of 3D human pose estimation using low-level audio signals, proposing a network architecture for direct mapping of acoustic features to poses, improving prediction accuracy with adversarial learning, providing guidelines for creating datasets, and demonstrating the effectiveness of the proposed method through extensive experimentation.