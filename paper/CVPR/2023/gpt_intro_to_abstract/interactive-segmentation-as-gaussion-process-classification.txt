Interactive segmentation (IS) has gained significant research interest due to its potential in reducing the cost of pixel-wise annotation. This paper focuses on click-based user interaction, where positive annotations are clicked on target objects while negative ones are clicked in background regions. Deep learning-based methods have shown promising success in IS, but they suffer from limitations such as lack of interaction between deep features of different pixels and insufficient theoretical support for proper activation and classification of clicked regions. To address these issues, this paper proposes a Gaussian process (GP)-based inference framework for IS. The IS task is reformulated as a pixel-level binary classification problem on each image, with clicks as training pixels and unclicked points as testing pixels. The GP classification model is constructed and solved using amortized variational inference and decoupling techniques. Deep kernel learning is incorporated for improved learning flexibility. The proposed framework, called GPCIS, achieves concise and clear interactive segmentation under a theoretically sound framework. The correlation between deep features of different pixels is captured by the GP posterior, allowing information from clicks to be propagated to the entire image for improved prediction. Extensive experiments demonstrate the superiority of GPCIS in segmentation quality and interaction efficiency, with consistent performance across different backbone segmentors.