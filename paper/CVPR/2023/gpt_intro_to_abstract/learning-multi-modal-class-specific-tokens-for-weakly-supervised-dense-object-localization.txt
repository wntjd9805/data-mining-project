This paper introduces a new method for weakly supervised dense object localization (WSDOL) in computer vision tasks. Current techniques rely on expensive pixel-level annotations, but this method explores the use of weak labels such as image-level or bounding-box level labels. The proposed method leverages the strong representations of visual concepts encoded by the pre-trained CLIP language model to guide dense object localization. A unified transformer framework is proposed, which includes multi-modal class-specific tokens that capture visual representations and rich language semantics. The method enhances class representations by incorporating sample-specific contextual information, both at the feature level and the loss level. Experimental results on benchmark datasets show that the proposed method achieves state-of-the-art results for weakly supervised semantic segmentation tasks.