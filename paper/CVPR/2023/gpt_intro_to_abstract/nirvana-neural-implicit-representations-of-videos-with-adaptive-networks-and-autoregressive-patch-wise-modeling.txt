In this paper, we introduce NIRVANA, a patch-wise autoregressive video Implicit Neural Representations (INR) framework that exploits spatial and temporal redundancies in videos to achieve high levels of encoding speedups with similar reconstruction quality and compression rates. We address the challenges posed by videos varying in spatial resolutions and temporal lengths by predicting patches instead of performing pixel-wise or whole-frame predictions. Our method takes in patch coordinates as inputs and outputs corresponding patch volumes, allowing adaptability to videos of different spatial resolutions without modifying the architecture. We also propose to train individual small models for each group of video frames in an autoregressive manner, with weights initialized from the previous frame group, to exploit temporal redundancies. To further improve compression gains, we employ entropy regularization for quantization and encode model weights for each video during training. We demonstrate the effectiveness of NIRVANA on the UVG dataset, achieving similar compression rates with 12 times the encoding speed of previous methods. We also show that our framework adapts to varying spatial and temporal scales, outperforming the baseline with smaller encoding times and scaling well with increasing number of GPUs. Our contributions include the development of NIRVANA and its patch-wise autoregressive approach, achieving high encoding speedups, adaptive bitrate compression, and superior reconstruction quality in videos.