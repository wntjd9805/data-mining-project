This paper introduces a new method for providing new grounding conditional inputs to pretrained text-to-image diffusion models. The existing large-scale text-to-image generation models are limited to being conditioned on text alone, which restricts the ability to precisely localize concepts or use reference images for control. The proposed method retains the text caption as input but enables other input modalities such as bounding boxes for grounding concepts, reference images, or part keypoints. The challenge is to preserve the vast concept knowledge in the pretrained models while learning to inject the new grounding information. To address this, the paper proposes the use of trainable gated Transformer layers that take in the new grounding input while freezing the original model weights. During training, the new grounding information is gradually fused into the pretrained model using a gated mechanism. The paper demonstrates that this design allows for flexibility in the sampling process during generation, leading to improved quality and controllability of the generated images. The experiments primarily focus on grounded text-to-image generation with bounding boxes, showing that the proposed model can generalize to unseen objects and outperforms a strong fully-supervised baseline on the LVIS dataset. The paper's contributions include the proposal of a new text-to-image generation method with grounding controllability, the achievement of open-world grounded text-to-image generation with bounding box inputs, and the improvement in zero-shot performance on layout-to-image tasks.