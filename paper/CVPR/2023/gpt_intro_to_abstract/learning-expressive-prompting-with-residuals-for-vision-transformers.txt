In recent years, scaling up neural networks has improved performance in various visual tasks. However, model adaptation is often necessary for optimal performance in specific downstream tasks. Traditional techniques like full-model finetuning have limitations, such as catastrophic forgetting and poor generalization with few training examples. Parameter efficient techniques, like partial-finetuning or adapters, have been developed to address these issues, but they are mostly designed for convolutional architectures. This paper explores adaptation techniques for large vision models based on the Transformer architecture, specifically focusing on visual prompt tuning (VPT). VPT introduces learnable tokens that interact with patch and class tokens to adapt transformers. However, VPT only allows partial interactions between prompts and the remaining tokens, and it requires a large number of inserted prompts, resulting in high computation costs. To overcome these limitations, this work proposes ExPRes, a new expressive prompt tuning method with residual tokens. ExPRes improves downstream performance by allowing the propagation of prompts through the encoder and modulating them with residual tokens at various layerwise computations. This enhances prompt capacity without significantly increasing computational costs. The effectiveness of ExPRes is empirically validated on fine-grained recognition and semantic segmentation tasks. It outperforms full-finetuning-based adaptation and state-of-the-art prompting approaches with fewer prompts, making it suitable for limited data settings. The proposed method also achieves competitive performance in few-shot semantic segmentation, outperforming strong adaptation baselines and achieving comparable results to language-assisted segmentation despite using significantly less annotated data.