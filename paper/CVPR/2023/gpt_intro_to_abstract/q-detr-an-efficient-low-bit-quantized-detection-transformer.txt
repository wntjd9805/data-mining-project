Abstract: Object detection with transformers (DETR) has gained popularity in the field of natural language processing (NLP) due to its success in training end-to-end detectors using transformer encoder-decoder architectures. Unlike previous approaches, DETR treats object detection as a direct set prediction problem, eliminating the need for post-processing procedures and hand-designed sample selection. However, DETR models often suffer from a large number of parameters and floating-point operations, leading to memory and computation consumption issues during inference. To address this challenge, network compression techniques, such as quantization, have been explored. Prior post-training quantization (PTQ) methods for DETR have shown limitations in terms of the model's performance when quantized to ultra-low bit representations. In this paper, we propose Q-DETR, the first quantization-aware training (QAT) framework for DETR, which overcomes the limitations of PTQ by fine-tuning the model on the training dataset. We also introduce a novel bi-level optimization distillation framework, referred to as DRD, which leverages the information bottleneck principle to effectively transfer knowledge from a real-valued teacher to a quantized student. Experimental results on the VOC dataset demonstrate the effectiveness of our approach, achieving significant performance improvements compared to existing quantized baselines. Overall, this paper provides a generic method for DETR quantization and offers insights into the challenges and opportunities in efficient object detection.