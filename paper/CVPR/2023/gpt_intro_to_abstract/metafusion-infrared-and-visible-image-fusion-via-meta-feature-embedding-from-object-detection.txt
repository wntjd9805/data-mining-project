Multi-modality sensor technology has led to the widespread use of multi-modality images, particularly infrared and visible images, due to their complementary information. Infrared images provide thermal structures but lack texture details, while visible images capture texture information but are affected by illumination. This has led to the development of methods for pixel-level infrared and visible image fusion (IVIF) and object detection (OD) to improve performance in high-level tasks. However, existing joint learning methods can be categorized into separate optimization and cascaded optimization, both of which have limitations in effectively incorporating OD into IVIF. To address this issue, we propose a meta-feature embedding network (MFE) to bridge the gap between IVIF and OD. Specifically, we introduce a joint learning framework called MetaFusion, which includes IVIF network (F), OD network (D), and MFE. MFE generates meta features to optimize F by simulating a meta learning process. We employ an inner update process to optimize F and MFE alternately, and an outer update process to optimize F with the guidance of fixed MFE. Additionally, we apply mutual promotion learning, where the improved F helps fine-tune D and the improved D provides better semantic information to optimize F. Our contributions are as follows: (1) We propose the MetaFusion framework for joint learning of IVIF and OD. (2) We design MFE to bridge the gap between F and D. (3) We introduce mutual promotion learning to enhance the performance of F and D. (4) Extensive experiments validate the effectiveness of our proposed method in image fusion and object detection.