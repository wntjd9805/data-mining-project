Abstract:Open-set fine-grained retrieval (OSFR) aims to build a generalized embedding space that can effectively handle visual discrepancies among unknown subcategories. Existing evaluators in OSFR typically follow a close-set learning setting, limiting the model to a predefined list of subcategories and hindering its generalization to unknown subcategories. Recent works have shown the potential of using the CLIP model, which can associate a wider range of visual concepts with their text descriptions, to alleviate this limitation. However, current prompt strategies are not tailored to capture fine-grained visual discrepancies. In this paper, we propose a novel prompting vision-language evaluator (PLEor) for OSFR, leveraging the CLIP model's ability to handle open-set visual concepts. PLEor incorporates a dual prompt scheme, consisting of a vision prompt and a text prompt, to highlight category-specific discrepancies. We also introduce an open-set knowledge transfer module to transfer the category-specific discrepancies from the CLIP model to the backbone network. Experimental results demonstrate that PLEor outperforms existing evaluators, achieving state-of-the-art results on three widely-used OSFR datasets. Our proposed approach improves retrieval accuracy by 8.0% on average. This work sheds light on the potential of prompt learning and demonstrates the effectiveness of leveraging the CLIP model for OSFR.