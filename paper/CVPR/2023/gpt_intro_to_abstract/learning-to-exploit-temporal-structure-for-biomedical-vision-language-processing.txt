Self-supervised learning from image-text pairs has led to the development of general-purpose vision-language models. However, incorporating temporal information in the medical domain poses a challenge due to the presence of prior imaging studies. In this paper, we propose a novel pre-training framework called BioViL-T that leverages the temporal relationship between samples for self-supervision in vision-language models. We introduce a multi-image encoder that handles missing inputs and incorporates longitudinal information without explicit image registration. Our approach achieves state-of-the-art performance in chest X-ray report generation, temporal image classification, and phrase grounding tasks. We also release a new multimodal benchmark dataset, MS-CXR-T, curated by an expert radiologist.