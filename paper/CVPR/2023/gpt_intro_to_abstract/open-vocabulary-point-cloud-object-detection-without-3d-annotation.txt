Current point-cloud detectors in computer vision are limited in their ability to detect objects from a diverse range of classes. This leads to poor generalization to unseen object classes that were not present in the training data. To address this issue, we propose a method called OV-3DET, which focuses on the problem of open-vocabulary point-cloud detection. This involves detecting 3D objects outside the training vocabulary, accompanied by arbitrary text descriptions. As large-scale point-cloud-caption data is currently unavailable, we leverage existing pre-trained models for images to learn general representations and relate them to text cues. Specifically, our approach involves a divide-and-conquer method in which the point-cloud detector first localizes the unknown objects and then names them based on text prompts. We use 2D pre-trained detectors to generate 2D bounding boxes or instance masks, which are then used to supervise the learning of 3D point-cloud detectors. We also propose a novel cross-modal contrastive learning method to connect point-cloud, image, and text modalities. Experimental results on two datasets demonstrate the effectiveness of our method, outperforming existing open-vocabulary and generazability-related methods. Ablation studies provide insights into the performance and workings of our proposed method.