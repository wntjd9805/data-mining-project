Video understanding is a significant research area in computer science, with various models being explored to model different aspects of videos. However, most existing methods focus on short-form videos and do not effectively handle long-form videos and their complex spatiotemporal dependencies. The recent vision transformer (ViT) has shown promise in modeling long-range dependencies, but its high computation and memory costs hinder its application to long-form videos. Other methods, such as ViS4mer, have attempted to address this challenge but have suboptimal performance for different video understanding tasks. In this paper, we propose a cost-efficient adaptive token selection module, called the S5 model, that selectively chooses informative image tokens for the Structured State-Spaces Sequence (S4) model, enabling learning of discriminative long-form video representations. Our approach avoids dense self-attention calculations and leverages S4 features in a gumble-softmax sampling based mask generator for token selection. To improve the robustness and temporal predictability of our S5 model, we introduce a novel long-short mask contrastive learning (LSMCL) approach for pre-training. Our experiments demonstrate that the S5 model, combined with LSMCL pre-training, achieves state-of-the-art performance on three challenging benchmarks for long-form video understanding, while reducing memory footprint. Our contributions include the S5 model, LSMCL pre-training, and their effectiveness for long-form video understanding.