Image diffusion models have shown promising results in various tasks, such as text-to-image generation, inpainting, object insertion, and personalization. However, their success in 3D generation and understanding is limited, mainly due to memory demands and the scarcity of 3D supervision data. In this paper, we propose RenderDiffusion, the first diffusion method for 3D content that is trained using only 2D images. We incorporate a latent 3D representation into the denoiser, allowing us to recover 3D objects without explicit 3D supervision. RenderDiffusion utilizes a triplane representation created by an encoder from noisy images, and a volumetric renderer to transform the 3D representation back into denoised 2D images. By avoiding the need for volumetric data and direct 3D supervision, RenderDiffusion achieves sharper generation and inference results compared to latent diffusion models. We evaluate RenderDiffusion on both in-the-wild and synthetic datasets, demonstrating its ability to generate plausible and diverse 3D-consistent scenes. Additionally, we show improved performance in monocular 3D reconstruction and inpainting tasks compared to a state-of-the-art method. Overall, our work introduces a denoising architecture with an explicit latent 3D representation, paving the way for training 3D-aware diffusion models solely from 2D images.