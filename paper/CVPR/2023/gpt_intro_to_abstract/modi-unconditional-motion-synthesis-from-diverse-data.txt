This paper introduces MoDi, an unconditional generative model for synthesizing diverse human motions. The challenge of learning motion priors from diverse, unstructured, and unlabeled datasets is addressed. Previous works focus on specific types of motion with limited diversity. MoDi is inspired by the StyleGAN architecture and utilizes a thorough study of potential operators, architectural variations, and a new building block to achieve state-of-the-art results. The results demonstrate that MoDi learns a structured latent space that enables clustering of semantically similar motions without supervision. Applications such as semantic editing, interpolation, and crowd animation are facilitated. An encoder architecture is also presented, allowing the inversion of unseen motions into MoDi's latent space. The encoding process is efficient and improves the projection of motions into the well-behaved regions of the latent space. Qualitative and quantitative evaluations on Mixamo and HumanAct12 datasets demonstrate the superiority of MoDi compared to state-of-the-art methods. Various applications, including motion fusion, denoising, and spatial editing, showcase the strength of the generative prior.