Dense depth estimation is crucial for various 3D vision tasks and self-driving applications. However, depth sensors often provide sparse depth maps with blank regions. In recent years, depth completion methods have leveraged RGB information to fill in the missing pixels. However, these methods fail to effectively explore the underlying 3D geometry and can result in over-smoothing at object boundaries. To address these issues, this paper proposes a cross-representation training scheme that exploits 3D representations for image-guided depth completion. The proposed framework includes an auxiliary LiDAR branch with a LiDAR encoder, cross-representation BEV decoder (CRBD), and point-voxel spatial propagation network (PV-SPN). The LiDAR scan is preprocessed and features are projected onto a Bird's-Eye View (BEV) space for fusion and completion. A point-voxel spatial propagation network refines the 3D geometric shapes. The LiDAR branch is only used during training, reducing computational burden in inference. Compared to previous fusion-based methods, the proposed framework offers generality, flexibility, and effectiveness. The main contributions include the BEV@DC method, the CRBD and PV-SPN modules, and achieving state-of-the-art results on benchmark datasets.