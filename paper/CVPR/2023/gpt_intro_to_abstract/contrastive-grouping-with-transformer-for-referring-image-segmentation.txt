This paper introduces Referring Image Segmentation (RIS), which aims to segment the target referent in an image based on a natural language expression. RIS has potential applications in human-robot interaction and image editing. Existing works in RIS utilize various fusion methods to integrate vision and language features, with recent improvements using the Transformer model. However, these methods have limitations in capturing object-level information and spatial priors, as well as modeling the differences between query vectors. To address these limitations, this paper proposes the Contrastive Grouping with Transformer (CGFormer) framework, which adopts an end-to-end mask classification framework and consists of the Group Transformer and Consecutive Decoder modules. The Group Transformer captures object-level information and achieves cross-modal reasoning, while the Consecutive Decoder performs cross-level reasoning and segmentation. Experimental results on standard benchmarks show that CGFormer outperforms state-of-the-art methods and demonstrates strong generalizability. This paper contributes to the advancements in RIS by explicitly addressing its inherent challenges and proposing an effective framework for referring image segmentation.