The paper introduces the concept of volumetric video, which captures dynamic scenes in 3D and allows users to view them from different angles. This technology has important applications in various fields such as video conferencing and sports broadcasting. However, the challenge lies in designing a representation for volumetric video that can meet the requirements of high-quality rendering, real-time processing, and efficient storage. The paper discusses traditional image-based rendering methods that use dense camera arrays to record scenes and synthesize novel views. These methods rely on multi-view video as the scene representation, but they suffer from high storage and transmission costs. Another approach is using RGB-D sensors to reconstruct textured meshes, but this is limited to specific environments. The authors propose a new representation called dynamic MLP maps for efficient view synthesis of dynamic scenes. Instead of using a single MLP network, each video frame is represented by a set of small MLP networks whose parameters are predicted by a 2D CNN decoder. This reduces the network evaluation cost and increases rendering speed. The proposed representation also leverages 2D CNNs instead of 3D CNNs, further reducing memory requirements. The authors evaluate their approach on dynamic scene datasets and demonstrate state-of-the-art performance in terms of rendering quality, speed, and storage efficiency. The contributions of this work include the dynamic MLP map representation, a new rendering pipeline for real-time dynamic scenes, and superior performance compared to existing methods.