Deep Neural Networks (DNNs), while widely used in systems like face recognition and object detection, are known to be vulnerable to adversarial examples â€“ inputs with slight perturbations that cause DNNs to produce incorrect outputs. This poses safety risks in real-world applications. Previous research has focused on crafting digital adversarial examples, but there is increasing interest in physical adversarial examples that deceive DNNs in the real world. In particular, hiding persons from DNN-based object detectors, especially when using non-rigid objects like clothes, is challenging. Most existing approaches involve printing patch-based adversarial examples on clothes, which can be easily noticed by human observers. Efforts have been made to make the patches more natural-looking, but they are only effective within a narrow range of viewing angles. Another approach involves designing texture-based adversarial examples, but it is difficult to replicate 3D textures on non-rigid objects without making them conspicuous to humans. In this paper, we propose a 3D modeling pipeline to create natural-looking adversarial clothes that can hide people at multiple viewing angles. We introduce adversarial camouflaged texture (Adv-CaT) patterns and apply them to clothes, leveraging camouflaged texture patterns commonly found in daily clothes. To make these patterns applicable to deformed and unseen 3D models, we propose a novel 3D augmentation method using topologically plausible projection (TopoProj) and thin plate spline (TPS) techniques. We optimize several Adv-CaT patterns to attack different object detection models and evaluate their effectiveness in evading detection and their naturalness compared to other adversarial clothes and daily clothes. Our experimental results demonstrate that our adversarial clothes can evade detectors at multiple viewing angles while maintaining a high naturalness score.