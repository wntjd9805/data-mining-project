Recent advances in digitizing humans for virtual and augmented reality applications have led to the development of 3D digital humans that can be rendered under novel views and controlled via face and body tracking. Additionally, research has focused on reproducing 3D objects from few input images without the need for control over expressions and poses. These developments offer significant advantages in real-world applications like video conferencing with holographic displays. This paper explores the reconstruction of volumetric scene representations for novel view synthesis from sparse camera views. The proposed approach, called DINER, leverages estimated dense depth maps to guide the image-based neural radiance field, providing strong prior information about visual opacity and improving sampling efficiency. DINER also enhances the extrapolation capabilities of image-based neural radiance fields by padding and positionally encoding the input images. The model is trained on various scenes and achieves superior results in terms of visual quality and viewpoint changes during novel view synthesis compared to existing methods. Experimental evaluations on FaceScape and DTU datasets demonstrate the effectiveness of DINER in reconstructing 3D scenes from limited source views. In summary, DINER contributes an effective approach for conditioning image-based neural radiance fields on depth maps, a depth-guided sampling strategy for increased efficiency, and a method to improve extrapolation capabilities through padding and positional encoding of source images.