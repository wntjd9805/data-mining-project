This paper introduces the concept of Scene Graph Generation (SGG) and its importance in complex visual understanding tasks. The paper identifies two main obstacles in current SGG research: the long-tail bias in datasets and the low-speed inference in practical applications. The paper discusses existing methods that aim to address these obstacles but conclude that they are unable to simultaneously balance the recall of common predicates and tail predicates. To overcome these limitations, the paper proposes a Contextual SGG (C-SGG) method that focuses on context descriptions to predict predicates, thereby reducing the reliance on visual features. The paper introduces the concept of context augmentation, which involves modifying images using software to generate diverse context descriptions. This allows for unbiased training, as each context description is unique. The C-SGG method has quadratic time complexity but is computationally efficient for each object pair. Additionally, the paper proposes a Context Guided Visual SGG (CV-SGG) method that combines visual and contextual features to further analyze high-confidence relationships and possible predicates. CV-SGG focuses on possible predicates from C-SGG, ignoring impossible predicates. The proposed methods are validated using the VG and PSG datasets, and the paper demonstrates that they achieve a balance between common predicates and tail predicates, while also enabling real-time SGG. The contributions of the paper include the proposal of context augmentation, the introduction of C-SGG and CV-SGG methods, and the demonstration of their advantages in addressing long-tail bias and inference speed.