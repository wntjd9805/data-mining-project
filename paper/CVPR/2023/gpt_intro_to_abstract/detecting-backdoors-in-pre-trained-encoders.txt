Self-supervised learning (SSL) has gained popularity in computer science due to its ability to achieve high performance without the need for labeled training data. SSL, particularly contrastive learning, has various applications such as similarity-based search, linear probe, and zero-shot classification. However, the reliance on large amounts of unlabeled data makes SSL computationally expensive for regular users, leading them to use pre-trained encoders provided by third parties. This production chain creates opportunities for adversaries to inject backdoors or conduct trojan attacks, compromising the model's integrity. While backdoor attacks have been studied in supervised learning scenarios, recent research has shown the feasibility of conducting such attacks in SSL. In this paper, we propose DECREE, a backdoor detection approach for pre-trained encoders in SSL. Unlike existing methods that focus on downstream classifiers, DECREE directly scans encoders to detect backdoors. We evaluate DECREE on a large number of encoders and demonstrate its effectiveness in detecting backdoors, even with limited access to the pre-training dataset. Our threat model assumes attackers can inject samples into the training set of vision encoders, and the defender has limited or no access to the pre-training dataset.