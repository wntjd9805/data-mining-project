This paper introduces the concept of Unseen Distribution Transformation (UDT) for Out-of-Distribution (OoD) prediction in machine learning. UDT aims to explicitly transform the testing distribution towards the training distribution in order to mitigate the distribution shift between them. Unlike previous approaches, UDT does not require access to the target distribution during training and instead trains a domain translator on the source distribution to transform any arbitrary target distribution. The proposed method, called Distribution Shift Inversion (DSI), uses a generative model, specifically the diffusion model, to perform the transformation. Experimental results demonstrate that UDT improves the performance of various OoD algorithms on benchmark datasets. The advantages of UDT include its ability to handle dynamically changing testing distributions, its capability to transform multiple distributions with a single model, and its independence from extra assumptions commonly used in OoD generalization algorithms. The proposed DSI method successfully operates without knowledge of the testing distribution, distinguishing it from previous approaches that rely on such information.