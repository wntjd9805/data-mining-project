In this paper, the authors address the problem of distinguishing between in-distribution and out-of-distribution samples in deep learning methods. They emphasize the need for deep neural networks to be able to recognize unknown samples, especially in high-stakes applications such as autonomous driving and medical image analysis. The authors discuss the challenge of over-confident predictions made by neural networks, even for out-of-distribution samples. They focus on the semantic shift scenario, where the goal is to detect inputs with semantic labels not present in the training set. Existing works for out-of-distribution detection rely on the predictive distribution, but integrating feature statistics for in-distribution data can improve performance. However, these methods often have practical constraints, such as requiring access to training data or internal feature activations. The authors propose GEN, a post-hoc method that uses the predictive distribution only, without the need for re-training or training data statistics. They show that GEN achieves better results in terms of AUROC and FPR95 compared to other post-hoc methods, particularly on large OOD datasets and challenging natural adversarial examples. This method has the potential to be used in more constrained model deployment scenarios.