In this paper, we focus on the problem of optimizing camera poses in NeRF (Neural Radiance Fields) and its variant GeNeRF (generalizable NeRFs) for novel view synthesis. While these methods demonstrate impressive results in encoding scene representation and rendering high-quality images, they still rely on accurate known camera poses as inputs, which can be expensive to acquire in real-world scenarios. Previous approaches obtain camera poses through Structure-from-Motion (SfM) techniques, which can be time-consuming and prone to failure in certain scenes. To address these issues, we propose DBARF, a deep neural network-based approach that jointly optimizes camera poses and GeNeRF. Our method does not require pre-computed camera poses and learns to correct relative camera poses using a deep pose optimizer. We also show that DBARF achieves better generalization across scenes compared to existing methods. Our contributions include an analysis of the challenges in optimizing camera poses with GeNeRFs, the introduction of DBARF for deep bundle adjusting camera poses, and experimental results demonstrating its effectiveness and generalization ability.