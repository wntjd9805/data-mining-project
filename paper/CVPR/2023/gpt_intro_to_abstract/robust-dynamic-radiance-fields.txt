Videos capture and preserve memorable moments, but traditional videos lack interactivity and the ability to view scenes from different angles. Dynamic view synthesis techniques aim to create photorealistic novel views of dynamic scenes, enabling free-viewpoint videos and interactive experiences. However, existing methods often require expensive multi-camera setups or precise camera poses estimated through complex systems, limiting their practicality. This paper introduces RoDynRF, an algorithm for reconstructing dynamic radiance fields from casual videos without the need for accurate camera poses. The algorithm optimizes camera poses and two radiance fields, modeling static and dynamic elements. A coarse-to-fine strategy and epipolar geometry are used to exclude moving pixels, deformation fields, time-dependent appearance models, and regularization losses for improved consistency. The algorithm is evaluated on multiple datasets and compared with existing methods, demonstrating its robustness, especially in challenging scenarios where traditional camera pose estimation systems fail. The contributions of this work include a space-time synthesis algorithm that does not require known camera poses, improved architecture designs, and auxiliary losses for robust camera pose estimation and dynamic radiance field reconstruction. The evaluations show the effectiveness of the proposed method in various challenging datasets.