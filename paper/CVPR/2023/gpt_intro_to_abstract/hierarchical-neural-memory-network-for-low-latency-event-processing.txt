This paper focuses on the importance of latency in vision applications such as autonomous vehicles and augmented reality. The use of event cameras, which have extremely low latency due to their unique principle of asynchronously recording intensity changes of individual pixels, is discussed. However, few works have dedicated low latency recognition models for event-based dense prediction tasks. Previous approaches either use standard CNN architectures with recurrent modules, resulting in similar latency levels as frame-based models, or employ Spiking Neural Networks (SNNs) with low accuracy or high latency. Therefore, the paper proposes a Hierarchical Neural Memory Network (HMNet) that encodes scene contents at different temporal scales and builds multi-level latent memories with variable operating rates. Additionally, an Event Sparse Cross Attention (ESCA) technique is introduced to minimize information loss when injecting sparse event streams into dense memory cells. The proposed HMNet is evaluated on various event-based vision tasks, demonstrating improved performance and reduced latency compared to existing methods.