Single image super-resolution (SR) is the task of reconstructing high-resolution (HR) images from their low-resolution (LR) counterparts. Deep neural networks (DNNs) have been successfully applied to this task by learning the inverse mapping from LR-HR sample pairs. However, most existing DNNs are designed for specific scaling factors and cannot handle arbitrary scaling factors. Implicit neural functions (INF) have been recently proposed to handle arbitrary resolution and enable continuous SR. These networks represent signals with evaluations of continuous functions parameterized by a multi-layer perceptron (MLP) and use encoder-based methods to share knowledge across instances. However, the point-wise behavior of MLP in spatial dimensions limits its performance, especially for high-frequency components. Neural operator is a neural network architecture used for efficient solvers of partial differential equations (PDE) in computational physics. It learns mappings between infinite-dimensional function spaces, allowing for continuous function evaluations. This architecture consists of three components: lifting, iterative kernel integral, and projection. The kernel integrals capture the global relationship of the underlying solution function and have shown promising results in various applications. In this paper, we propose the super-resolution neural operator (SRNO), a deep learning framework for resolving HR images from LR counterparts at arbitrary scales. SRNO treats LR-HR image pairs as continuous functions with different grid sizes and learns the mapping between these function spaces. The key characteristics of SRNO are the efficient implementation of the kernel integral using Galerkin-type attention and the dynamic latent basis update enabled by the multilayer attention architecture. Experimental results show that SRNO outperforms previous continuous SR methods in terms of both reconstruction accuracy and running time. In summary, the contributions of this paper are: 1. The proposal of the super-resolution neural operator method that enables continuous and zero-shot super-resolution.2. The development of an architecture for SRNO that explores common latent basis and refines instance-specific basis using the Galerkin-type attention mechanism.3. Experimental results demonstrating the superior performance and efficiency of SRNO compared to existing continuous SR methods.