In recent years, deep learning has been successfully applied to computer vision tasks, particularly in supervised models. However, self-supervised learning, which transforms unlabeled data into rich representations, has gained popularity. Obtaining state-of-the-art image encoders for self-supervised learning is a challenging task. Existing works have focused on model stealing attacks against supervised classifiers, leaving the vulnerability of unsupervised image encoders unexplored. In this paper, we pioneer the systematic investigation of model stealing attacks against image encoders trained by contrastive learning, a cutting-edge unsupervised representation learning strategy. We propose a contrastive-learning-based model stealing attack, called Cont-Steal, that enforces the surrogate embedding of an image to be close to its target embedding. Our evaluation demonstrates that Cont-Steal outperforms conventional attacks against encoders, achieving higher accuracy and being more query-efficient and dataset-independent. We also evaluate defense mechanisms against Cont-Steal, showing that most mechanisms are not effective in mitigating the attack. Our findings highlight the vulnerability of pre-trained encoders and call for attention to the protection of intellectual property in representation learning techniques.