The introduction of this computer science paper discusses the advancements in Computer Vision, particularly in the field of Convolutional Neural Networks (CNNs) and the emergence of Vision Transformers (ViTs) as a potential alternative. Previous studies have analyzed the similarities between ViTs and CNNs, but this work aims to understand how ViTs learn under different types of supervision, including self-supervised methods. The authors compare ViTs through three domains of analysis: How they process information through attention, What they learn to represent, and Why they are useful for various tasks. The authors uncover the local-global dual nature of ViTs, where each token can attend to any image region at any time, and the flexibility for global information integration. They also analyze the features of ViTs at different layers and assess their performance on local and global tasks. The contributions of this work include a detailed comparison of ViTs trained with different methods, a cross-cutting analysis of attention, features, and downstream tasks, and insights into ViT behavior. The authors summarize key observations about ViT behavior, such as the emergence of sparse repeating patterns in attention maps, the use of offset local attention heads, the processing of local and global information in different orders, and the differentiation of salient foreground objects. They also highlight the effectiveness of reconstruction-based self-supervised methods and the diversity in performance across different layers for downstream tasks. Overall, this work provides valuable insights into ViTs and guides future development and applications of ViT variants and training strategies.