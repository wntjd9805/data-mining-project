Robust panoptic 3D scene understanding models are crucial for various applications such as virtual reality, robot navigation, and self-driving. While significant progress has been made in 2D panoptic image understanding, the task of segmenting a 2D image into categorical "stuff" areas and individual "thing" instances, there are still limitations when it comes to tasks requiring coherency and consistency across multiple views. Single-image panoptic segmentation often results in view-specific imperfections and inconsistent classifications, and lacks the ability to track unique object identities across views.Previous works have attempted to address panoptic 3D scene understanding by leveraging Neural Radiance Fields (NeRFs) and gathering semantic scene data from multiple sources. However, these approaches either rely on expensive and time-consuming ground truth labeling or struggle to generalize beyond the training data due to the difference in scale between 2D and 3D datasets.In this paper, we propose Panoptic Lifting, a novel formulation that represents a static 3D scene as a panoptic radiance field. This formulation supports applications like novel panoptic view synthesis and scene editing, while maintaining robustness to diverse input data. Our model is trained using only 2D posed images and machine-generated panoptic segmentation masks, and can generate color, depth, semantics, and 3D-consistent instance information for novel views of the scene.We introduce lightweight output heads for learning semantic and instance fields, and use test-time augmentation on the 2D panoptic segmentation model to obtain confidence estimates for weighting the losses. We also employ specific techniques, such as bounded segmentation fields and stopping semantics-to-geometry gradients, to reduce inconsistent segmentations.In summary, our contributions include a novel approach to panoptic radiance field representation that models the radiance, semantic class, and instance ID for each point in the space for a scene by directly lifting machine-generated 2D panoptic labels. We also present a robust formulation to handle noise and inconsistencies in machine-generated labels, resulting in clean, coherent, and view-consistent panoptic segmentations from novel views, across diverse data.