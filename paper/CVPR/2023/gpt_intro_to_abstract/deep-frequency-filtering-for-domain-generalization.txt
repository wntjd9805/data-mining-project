Domain Generalization (DG) aims to overcome the assumption that training and testing data are identically and independently distributed. However, collecting sufficient training data from all possible domains can be costly and impractical. Therefore, learning generalizable feature representations is highly valuable. Recent research has shown that deep learning models prioritize capturing low-frequency components of training data while exploiting high-frequency components for trade-offs between robustness and accuracy. This observation suggests that different frequency components have different transferability across domains. In this paper, we propose Deep Frequency Filtering (DFF), a technique that enhances transferable frequency components and suppresses those not conducive to generalization. DFF utilizes Fast Fourier Transform (FFT) to obtain frequency representations of intermediate features and applies a spatial attention map to filter out components detrimental to generalization in an end-to-end manner. We compare task-wise filtering with instance-adaptive filtering and find the latter to be superior. Additionally, we incorporate DFF into a two-branch architecture called Fast Fourier Convolution (FFC) and demonstrate its effectiveness. Our contributions include the discovery of the significant enhancement of cross-domain generalization through a learnable filtering operation in the frequency domain, the proposal of the DFF module for learning generalizable features, and an empirical study that emphasizes the importance of instance-level adaptability in frequency-space filtering for domain generalization.