Video Language Grounding (VLG) is a fundamental problem in computer science that connects computer vision and natural language processing. It involves localizing video segments based on given natural language queries, bridging the gap between visual content and textual information. Early works in VLG focused on Video Sentence Grounding (VSG) which aimed to localize the most relevant moment with a single sentence query. Recently, Video Paragraph Grounding (VPG) has been introduced, requiring the localization of multiple events using a paragraph query consisting of temporally ordered sentences. Previous VPG works mainly explored correlations of events at the sentence level, neglecting the rich visual-textual correspondence at other semantic levels like words and paragraphs. To address this limitation, the authors propose a novel framework called Hierarchical Semantic Correspondence Network (HSCNet) for VPG. HSCNet leverages hierarchical semantic information from both the linguistic and visual perspectives. It uses a multi-level encoder-decoder architecture to align visual and textual semantics and progressively ground queries from coarse to fine temporal boundaries. The proposed HSCNet is evaluated on two benchmarks, demonstrating state-of-the-art results and surpassing previous approaches. This work contributes a novel hierarchical modeling framework for VPG, exploring hierarchical visual-textual semantic correspondence and grounding multiple levels of linguistic queries.