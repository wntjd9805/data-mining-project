Existing approaches for on-device training on edge devices are limited in their efficiency and practicality due to computational and memory complexity. This paper proposes a new research direction called gradient filtering to address these issues. Gradient filtering aims to reduce the computational complexity and memory consumption of back propagation through convolution layers. By creating a new gradient map with a special structure and fewer unique elements, the gradient propagation can be simplified, leading to cheaper operations and reduced memory requirements. The trade-off for this filtering process is a slight reduction in gradient precision, but experiments show that this does not significantly impact model accuracy. The proposed gradient filtering method is compared to other training acceleration techniques, demonstrating its effectiveness in reducing computation and memory costs while maintaining similar accuracy. The paper also discusses how this method can be easily deployed with optimized deep learning frameworks and presents evaluations on both resource-constrained edge devices and high-performance devices. The paper concludes by providing a summary of its main contributions and outlining the organization of the remaining sections.