This paper introduces StepFormer, a self-supervised approach for discovering and localizing key-steps in instructional videos. The challenge with using instructional videos is filtering out uninformative frames and focusing on the task-relevant segments. Previous work relies on supervision or a priori knowledge of instruction steps, limiting their applicability to large datasets. StepFormer addresses these challenges by using automated speech recognition as the only source of supervision and training on a large instructional video dataset. It employs a transformer decoder with learnable input queries to attend to informative video segments and enforce temporal order. Experimental results demonstrate that StepFormer outperforms weakly- and unsupervised baselines on three benchmark datasets. It also exhibits the ability to perform zero-shot key-step localization from a text prompt. The contributions of this paper are the novel self-supervised approach, the explicit modeling of temporal order, and the demonstration of improved performance on unsupervised step localization without finetuning.