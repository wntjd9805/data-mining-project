This paper introduces the concept of efficiently and effectively leveraging heterogeneous views for caption generation in vision-and-language tasks. The authors highlight the limitations of existing methods that encode input images using object detectors and propose encoding images from different pre-trained models and modalities to improve performance. They address the research question of how to leverage these views by considering factors such as computation, parameter count, and label efficiency. The authors propose a novel approach called HAAV (Hierarchical Aggregation of Augmented Views) that encodes each view independently using a shared transformer encoder, resulting in linear scalability and parameter efficiency. They also incorporate a contrastive loss to improve representation learning of heterogeneous views. Furthermore, they introduce a hierarchical decoder layer that aggregates within each view and across views to model view effectiveness and adaptively weigh them for caption generation. The paper demonstrates the effectiveness of HAAV through experiments on the MS-COCO image captioning benchmark and shows significant improvements over state-of-the-art methods. The authors also provide thorough ablations and analyses to validate the efficiency and effectiveness of their proposed method.