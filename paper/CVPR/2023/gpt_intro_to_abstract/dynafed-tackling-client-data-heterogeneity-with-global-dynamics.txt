Federated learning (FL) is a popular distributed training paradigm in computer science that aims to alleviate computational burden on the server and preserve clients' data privacy. However, in real-world applications, such as healthcare and bio-metrics, the client data often demonstrates heterogeneity, leading to slow convergence and performance drop. Previous approaches have focused on modifying the local training process or refining the global model in the server aggregation process, but they have limitations in terms of server control or external dataset requirements. In this paper, we propose a new approach called DYNAFED that extracts hidden information from the global model's trajectory to synthesize a pseudo dataset that approximates the global data distribution. This synthetic dataset is used to aid the aggregation process and alleviate the performance degradation caused by heterogeneous client data. Our approach has several advantages, including leveraging global data distribution knowledge without external datasets, generating informative data in early rounds of federated learning, and reducing computational overhead. Experimental results show that DYNAFED stabilizes training, boosts convergence, and achieves significant performance improvement under heterogeneity. We provide insights and detailed analysis into the working mechanisms of DYNAFED both experimentally and theoretically.