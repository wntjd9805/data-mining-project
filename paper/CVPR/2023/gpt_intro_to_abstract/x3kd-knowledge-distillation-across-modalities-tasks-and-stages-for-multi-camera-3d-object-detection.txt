3D object detection (3DOD) plays a crucial role in various real-world computer vision applications, especially in autonomous driving. Existing 3DOD approaches can be categorized based on their input modalities, such as camera images or LiDAR point clouds. While LiDAR-based 3DOD provides accurate 3D information, it is costly and not always available in commercially deployed vehicles. On the other hand, multi-camera 3DOD using low-cost monocular cameras is more feasible in current vehicle designs, but it lacks depth information and hence suffers from lower performance compared to LiDAR-based approaches.In this paper, we propose a knowledge distillation framework called X3KD for multi-camera 3DOD. X3KD leverages cross-modal and cross-task information by distilling knowledge from a LiDAR-based 3DOD model and an instance segmentation model into different stages of the multi-camera 3DOD pipeline. Specifically, we introduce three distillation techniques: feature distillation, adversarial training, and output distillation, to improve the feature representation and output supervision at different network stages.We also propose cross-task instance segmentation distillation (X-IS) for enhancing the PV feature extraction stage, which is crucial for optimal camera-based 3DOD. By transferring knowledge from a pre-trained instance segmentation teacher, we improve the extracted PV features. Furthermore, we demonstrate the effectiveness of X3KD on the nuScenes and Waymo datasets, outperforming previous approaches for multi-camera 3DOD.Additionally, we transfer X3KD to RADAR-based 3DOD and train it solely through knowledge distillation without using ground truth. Our extensive ablation studies provide a comprehensive evaluation of the effectiveness of knowledge distillation at different network stages for multi-camera 3DOD. Overall, our contributions include the introduction of the X3KD framework, the use of cross-modal KD and cross-task instance segmentation distillation, and the evaluation of X3KD on various datasets.