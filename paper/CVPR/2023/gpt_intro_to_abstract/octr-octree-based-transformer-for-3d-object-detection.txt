Abstract:The task of 3D object detection from point clouds is crucial in autonomous driving perception systems. Feature learning is challenging due to the sparse and disordered nature of the data, as well as occlusion and distance limitations. Grid-based and point-based methods have been proposed, but they struggle to capture necessary context information. Inspired by the success of Transformers in NLP, this paper introduces the Octree-based Transformer (OcTr) network for 3D object detection. OcTr employs an octree-based learnable sparse pattern to encode point clouds and uses self-attention to capture global context features. It also incorporates a hybrid positional embedding for better foreground perception. Experimental results on the Waymo Open Dataset and the KITTI dataset demonstrate the effectiveness of OcTr, achieving state-of-the-art performance with significant improvements on far objects. The proposed approach strikes a competitive balance between accuracy and efficiency in voxel-based 3D object detection.