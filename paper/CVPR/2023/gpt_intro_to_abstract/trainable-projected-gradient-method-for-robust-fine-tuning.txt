Improving out-of-distribution (OOD) robustness in deep learning is a crucial research topic, as it ensures that vision models can be trusted across various conditions beyond the training data. Pre-trained models have been effective in achieving OOD robustness, but fine-tuning them for downstream tasks often results in decreased generalization capability. This is due to unconstrained optimization on the new training data, which causes the model to over-fit and forget the pre-trained features. To address this issue, we propose a trainable projected gradient method (TPGM) that enforces different distance constraints for each layer. TPGM adopts trainable weight projection constraints, referred to as projection radii, and incorporates them in the forward pass of the main model to optimize. By learning the weight projection radii in a principled manner, TPGM preserves the generalization capability of the pre-trained model while fine-tuning. Our experiments on large-scale datasets demonstrate that TPGM outperforms existing approaches in terms of OOD generalization without sacrificing ID accuracy. Additionally, our theoretical analysis shows that the bi-level optimization formulation is essential for learning layer-specific constraints. Overall, our contributions include the proposal of TPGM, experimental validation on different datasets and architectures, and theoretical insights on the learning of different constraints for each layer.