In recent years, network compression techniques for Deep Neural Networks (DNNs) have been rapidly developing to achieve efficient inference in both edge devices and the cloud. One promising approach is quantization, which reduces computation complexity and memory footprint by representing weights and activations with low-bit integers. Traditional approaches for quantization-aware training (QAT) require long re-training and full training data, making it impractical in industry. On the other hand, post-training quantization (PTQ) is faster and lighter, but suffers from performance degradation at ultra low compression ratios. This is due to the accumulation of quantization noise throughout the network. To address this, various layer-wise block-wise reconstruction-based algorithms have been proposed, improving accuracy when quantizing weights to 4-bit. However, when compressing more complex models or using lower-bit quantization, there is still a significant accuracy gap compared to the original model. Lower bit-widths result in more rugged loss landscapes, causing the networks to be easily trapped into bad local minima. This makes optimizing low-bit models in PTQ very challenging. In this paper, we propose a novel approach called Bit-shrinking, which uses a sharpness term to estimate the degree of loss distortion caused by quantization noise. We design a self-adapted progressive quantization scheduler for PTQ, gradually shrinking the bit-width to limit the sharpness and find better local minima. Experimental results on ImageNet and COCO datasets demonstrate the superiority of Bit-shrinking, achieving state-of-the-art performance in quantized models.