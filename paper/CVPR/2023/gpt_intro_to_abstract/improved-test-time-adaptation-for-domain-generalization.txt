Abstract:Recent advancements in deep learning models have focused on training and testing data from the same domain and following the same distribution. However, this assumption does not always hold true in real-world scenarios, such as autonomous driving and object recognition tasks. This distribution shift between the source and target domains has led to poor performances and hindered the further application of deep learning techniques. To address this issue, domain generalization (DG) has received significant attention in the research community, aiming to generalize learned models to unseen target domains. While various approaches have been proposed, a recent study shows that most of these methods are inferior to the baseline empirical risk minimization (ERM) method. This is because existing methods primarily focus on decreasing the distribution shift through training data, neglecting the contributions from test samples. To alleviate this problem, a test-time training (TTT) technique has gained momentum, enabling dynamic tuning of the pretrained model with test samples. However, selecting an appropriate auxiliary TTT task and identifying reliable parameters to update are crucial for its success. In this paper, we propose two strategies to improve the TTT strategy for better domain generalization. First, we introduce a learnable consistency loss for test-time adaptation, which can be aligned with the main loss by adjusting its parameters. Second, we introduce new adaptive parameters and only update them during the test phase. We evaluate our method on various DG benchmarks and demonstrate its competitiveness against existing methods under rigorous settings. Overall, our work aims to ease the distribution shift problem and improve domain generalization through improved TTT strategies.