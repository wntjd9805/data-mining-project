Point clouds are widely used in 3D Computer Vision applications, but they often contain noise and lack connectivity information. Point cloud filtering or denoising is therefore an important research problem. Conventional methods rely on local information, such as point normals, but they have limitations in terms of accuracy and loss of geometric details. Recently, deep learning-based filtering methods have been proposed to overcome these limitations. Early methods required pre-processed 2D height maps, but with the development of direct point set convolution, more effective methods using feature encoders based on architectures like PointNet and PointNet++ have emerged. These methods can be categorized as resampling, probability, and displacement-based methods. However, existing displacement-based methods suffer from sensitivity to high noise or collapse of closely separated surfaces. Additionally, they do not consider true iterative filtering during training. To address these limitations, we propose a novel neural network architecture called IterationModule, which models the iterative filtering process internally. Each IterationModule represents a filtering iteration, allowing the network to learn the filtering relationship across iterations. We also introduce a novel loss function that promotes a gradual filtering process by minimizing the L2 norm between filtered displacements and the nearest point in a target point cloud at each iteration. Furthermore, we design a generalized patch-stitching method to improve efficiency. Experimental results demonstrate the advantages of our method compared to state-of-the-art methods on synthetic and real-world data.