This paper addresses the problem of evaluating the sensitivity of deep learning models to specific attributes without relying on labeled test data. Traditional methods of collecting and annotating large-scale datasets for evaluation are time-consuming and expensive, and often do not guarantee robustness or fairness. The paper proposes a model diagnosis technique using counterfactual explainability, which visualizes the sensitive factors in an input image that can influence a model's outputs. The approach leverages Contrastive Language-Image Pretraining (CLIP) and StyleGAN to generate counterfactual images based on user-defined text attributes. This eliminates the need for labeled test data and expert knowledge, making model diagnosis more accessible and cost-effective. The proposed method offers improvements such as an open-vocabulary setting, adaptability to new target models or attribute spaces, and increased distributional robustness against counterfactual images. Experimental results show that fine-tuning the target model with counterfactual images not only improves classification performance but also enhances distributional robustness.