Self-supervision is a promising approach for training deep neural networks without explicit supervision or labels. Denoising autoencoders have been effective for pre-training in NLP and learning visual representations from images. However, applying these techniques to images is challenging due to the high-frequency variations in pixels. To address this, previous methods have utilized intermediary codebooks and high masking ratios. In this paper, we propose enhancing image-based denoising autoencoders by explicitly incorporating scene-level information using a perceptual loss term. We also introduce adversarial learning with an adversarial discriminator to tie the features and improve representation learning. Our approach achieves lower reconstruction error and captures scene layout and object boundaries without explicit supervision or hand-engineered biases. Additionally, we show that techniques from generative modelling literature result in further improvements and performance gains across downstream tasks. Our contributions include introducing a technique for improved representations, setting new state-of-the-art for masked image modelling, demonstrating the use of pre-trained models for performance boost, and achieving qualitative improvements in object-centric representations and frozen feature performance compared to previous methods.