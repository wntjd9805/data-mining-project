Deep learning has made significant advancements in various areas such as architecture designs, optimization techniques, data augmentation, and learning strategies. However, deep learning applications often require a large amount of labeled data, which is time-consuming and costly to collect. Few-Shot Learning (FSL) has emerged as a solution to address this issue by learning with only a few training samples. FSL over out-of-distribution data poses a challenge in developing efficient algorithms that can perform well in cross-domain situations. Fine-tuning a pre-trained feature extractor with a few samples has shown promise in addressing this challenge. However, the fine-tuned models in FSL often suffer from imbalanced categorical performance. This imbalance leads to biased learning and can mislead the model. In this paper, we propose a method called TF-MP (Transductive Fine-tuning with Margin-based uncertainty and Probability regularization) to address the issue of imbalanced categorical performance in FSL. We introduce two solutions: per-sample loss weighting through Margin-based uncertainty and probability regularization. Our empirical experiments demonstrate that TF-MP effectively reduces the largest difference between per-class predictions and improves per-class accuracy. Furthermore, TF-MP shows robust performance boosts on Meta-Dataset, showcasing its potential for real-world applications. Our contributions include highlighting the importance of addressing imbalanced categorical performance in FSL, proposing TF-MP as a solution, and empirically verifying its effectiveness.