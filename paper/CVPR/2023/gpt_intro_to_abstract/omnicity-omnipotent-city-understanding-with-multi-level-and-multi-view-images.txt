This paper introduces the OmniCity dataset, a comprehensive city dataset that combines data from satellite and street views. Existing city datasets do not provide sufficient annotations for static objects such as buildings and roads. In contrast, remote sensing images offer dense spatial coverage and alignment with open maps and government datasets. The proposed annotation pipeline efficiently produces diverse street-level annotations by leveraging existing map labels and visual context. The OmniCity dataset contains over 100K annotated images from New York City, supporting tasks such as building footprint extraction, height estimation, and fine-grained building instance segmentation. Benchmark results and analysis demonstrate the dataset's potential for new methods and tasks in large-scale city understanding, reconstruction, and simulation. The contributions include the annotation pipeline, the OmniCity dataset itself, benchmark results, and discussions on its potential for future research.