Recent advances in coordinate-based neural representation, particularly neural radiance fields (NeRF), have shown remarkable performance in synthesizing high-quality images from novel viewpoints. However, the high computational costs have been a significant bottleneck. Several methods have been proposed to accelerate training and inference times, but they often have limitations such as requiring large-scale datasets or suffering from poor generalization performance. Another approach is to incorporate classical data structures into the NeRF framework, which has reduced training and inference time without compromising reconstruction quality. However, this approach increases the overall size due to dense and volumetric structures. To address these challenges, we propose compressing grid-based neural fields using frequency-based transformations. By leveraging existing compression techniques and employing the discrete wavelet transform, we achieve parameter efficiency while maintaining rendering quality. A trainable binary mask is used to filter out unnecessary coefficients, and standard compression algorithms such as run-length encoding and Huffman encoding are applied for further compression. Our method incurs negligible computational costs at test time and achieves state-of-the-art performance in novel view synthesis under a memory budget of 2 MB.