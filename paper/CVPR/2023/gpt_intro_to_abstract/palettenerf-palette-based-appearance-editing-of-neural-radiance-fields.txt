Neural Radiance Fields (NeRF) and its variants have gained attention for their ability to reconstruct 3D scenes from 2D images and synthesize photorealistic novel views. However, editing these volumetric representations is challenging due to the implicit encoding of scene appearance. Existing methods focus on recovering material properties or learning a latent code, but suffer from limitations in capacity, flexibility, and fine-grained editing. In this paper, we propose PaletteNeRF, inspired by color palette-based image editing methods. We model the radiance of each point using specular and diffuse components, and decompose the diffuse component into view-independent color bases shared across the scene. Through joint optimization, we modify the color bases to intuitively edit NeRF's appearance. We introduce regularizers to encourage sparsity and spatial coherence in the decomposition. Unlike previous methods, our approach produces globally coherent and 3D consistent recoloring results. We demonstrate superior performance in fine-grained local color editing while maintaining photorealism. Our contributions include the novel framework for editing NeRF, the robust optimization scheme, and the enabling of practical palette-based appearance editing.