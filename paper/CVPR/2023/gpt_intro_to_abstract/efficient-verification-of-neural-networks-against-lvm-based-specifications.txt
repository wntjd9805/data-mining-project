The deployment of neural networks in safety-critical applications requires assurance on their performance, particularly accuracy and robustness. Formal verification offers provable guarantees that a network meets a given specification. Specifications often include properties such as robustness to noise, geometric changes, and more. However, practical applications also require robustness against diverse changes in a scene that cannot be easily mathematically defined. Generative models can be used to encode these changes from data. While generative models are commonly used for network robustification and empirical evaluation, they are seldom used for verification due to the lack of mathematical guarantees on the completeness of the specifications they encode. In this paper, we propose a novel, invertible encoder-based pipeline for verifying latent space sets, which addresses the scalability of verification methods and the quality of reconstructions affecting the verification outcomes. We focus our analysis on pose and attribute variations in vision inference tasks but believe that the approach can be extended to other variations, domains, and tasks. The paper will further discuss the key notions for network verification, existing relevant work, and present and validate the proposed method in subsequent sections.