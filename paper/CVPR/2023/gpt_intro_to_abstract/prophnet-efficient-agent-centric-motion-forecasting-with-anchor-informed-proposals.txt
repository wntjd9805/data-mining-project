Predicting the future behaviors of road participants is crucial for autonomous driving systems to operate safely and comfortably. However, motion forecasting is challenging due to the interrelated modalities of input, stochastic and multimodal output, and limited computation resources. Previous methods handle this complexity with modality-specific designs. This paper introduces ProphNet, an efficient agent-centric motion forecasting model that integrates heterogeneous input and enhances multimodal output. The model combines agent history, agent relation, and road graph into the agent-centric scene representation (AcSR) through a modality-agnostic architecture. It uses a self-attention network to capture complex interactions and anchor-informed proposals to induce output multimodality. The proposed model achieves high efficiency in terms of architecture and inference, significantly reducing latency. It outperforms state-of-the-art methods in accuracy while maintaining low inference latency.