This paper introduces a holistic approach to audio-visual speech enhancement, focusing on improving the quality and intelligibility of corrupted speech in various types of distortion. The authors propose a universal model called ReVISE, which combines a pseudo audio-visual speech recognition model and a pseudo text-to-speech synthesis model. The model uses self-supervised speech units to encode speech content and does not require text supervision. The authors demonstrate the effectiveness of ReVISE on four types of corrupted speech, including denoising, separation, inpainting, and video-to-speech synthesis. Results show that ReVISE outperforms prior models in terms of high-quality video-to-speech synthesis and achieves comparable performance on denoising and separation tasks. The model also shows strong performance on real data from the EasyCom dataset, addressing the cocktail party problem.