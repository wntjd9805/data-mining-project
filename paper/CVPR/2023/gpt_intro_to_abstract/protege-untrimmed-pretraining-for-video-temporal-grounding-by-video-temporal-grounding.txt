This paper introduces the concept of video temporal grounding (VTG), which involves localizing a specific part of a long untrimmed video based on a natural language text query. VTG has applications in various fields, such as information retrieval and robotics. The primary challenge in VTG is the need for a detailed understanding of the spatio-temporal dynamics in the video. Existing methods for VTG rely on video backbones pretrained on trimmed videos, which creates a disconnect between the pretraining and the downstream VTG task. This paper proposes a novel approach called ProT´eG´e that formulates pretraining as a VTG task on untrimmed videos to improve grounding performance. The scarcity of large-scale annotated video grounding datasets is a challenge, but the HowTo100M and Youtube-8M datasets can be used for untrimmed pretraining. However, the utility of these datasets for grounding tasks is limited due to noise in the video-text correlations. To overcome this, this paper introduces a novel video-text similarity grounding module and an optimization objective that addresses the noisy correlations. ProT´eG´e is shown to outperform backbones pretrained on trimmed videos in standard datasets. The contributions of this work include the formulation of ProT´eG´e as a pretraining method for VTG, the incorporation of aggregated subtitles, the introduction of the Video-Text Similarity-based Grounding Module, and the proposal of a pretraining objective to leverage large-scale untrimmed video datasets with noisy video-text pairs. The experimental results validate the effectiveness of ProT´eG´e in improving VTG performance across multiple benchmarks.