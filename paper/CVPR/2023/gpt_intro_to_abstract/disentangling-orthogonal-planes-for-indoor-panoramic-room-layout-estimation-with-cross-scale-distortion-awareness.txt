Indoor panoramic layout estimation is the process of reconstructing 3D room layouts from omnidirectional images. Inferring 3D information from 2D images is a challenging problem, compounded by the severe distortions introduced by the 360Â° field-of-view of panoramas. In this paper, we propose a novel architecture that disentangles the orthogonal planes of the room to capture explicit geometric cues, leveraging the Manhattan World assumption that indoor scenes conform to flat planes. We decompose the widely used 1D representation into separate sequences for the horizontal and vertical planes, enabling the extraction of "clean" depth-relevant and height-relevant features. We also introduce an assembling mechanism that effectively fuses multi-scale features with distortion awareness, using attention to compute the distortion-relevant positions and cross-scale interaction to integrate shallow geometric structures and deep semantic features. Extensive experiments on popular datasets demonstrate the effectiveness of our proposed solution, outperforming state-of-the-art methods in terms of intersection over the union of 3D room layouts. Our contributions include the disentanglement of orthogonal planes, the distortion-aware assembling mechanism, and the improved performance on benchmark datasets.