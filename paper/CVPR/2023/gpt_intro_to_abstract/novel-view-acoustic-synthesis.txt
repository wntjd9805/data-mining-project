Replaying video recordings from a new viewpoint has numerous applications in cinematography, video enhancement, and virtual reality. However, there is a lack of research focused on synthesizing sound from a new acoustic viewpoint, limiting the emotional and cognitive impact of the experience. In this paper, we propose the novel task of novel-view acoustic synthesis (NVAS), which aims to synthesize the sound in a scene from a new acoustic viewpoint, given only visual and acoustic input from another source viewpoint. We highlight the challenges in NVAS, including the dynamic nature of sound, differences between visual and audio sensors, low spatial resolution in audio, and the difficulty of segmenting mixed sounds. We hypothesize that leveraging vision can improve sound synthesis through cues about space and geometry that affect sound. To validate this hypothesis, we propose a visually-guided acoustic synthesis network that analyzes audio and visual features and synthesizes audio at a target location. We also contribute two new datasets, Replay-NVAS and SoundSpaces-NVAS, to facilitate research on NVAS. Our experiments show that our model outperforms traditional signal processing approaches and learning-based baselines, demonstrating the potential of our approach in novel-view acoustic synthesis.