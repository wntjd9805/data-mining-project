The paper proposes a method to jointly optimize camera poses and a Neural Radiance Field (NeRF) from a sequence of RGB images with large camera motion. Previous methods have struggled with this task due to limited estimation of relative poses and shape-radiance ambiguity. The paper integrates monocular depth estimation into the system, addressing these challenges. Specifically, the authors optimize scale and shift parameters for each mono-depth map during NeRF training to transform them into undistorted multiview consistent depth maps. These depth maps are then used in a Chamfer Distance loss and a depth-based surface rendering loss to improve relative pose estimation. Experimental results show that the proposed method outperforms state-of-the-art methods in terms of novel view synthesis quality and camera trajectory accuracy.