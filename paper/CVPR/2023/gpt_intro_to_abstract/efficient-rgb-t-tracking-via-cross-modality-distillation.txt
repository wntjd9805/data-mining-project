RGB-T tracking is a research area that involves estimating the state of an object in each frame of an RGB-T video sequence. With the affordability of thermal infrared sensors, there is a growing interest in RGB-T tracking. Existing models typically use a two-stream structure to extract RGB and TIR features and then fuse them to exploit complementary information. However, these models are computationally expensive and have large model sizes. To address this, there are two solutions: using a single-stream feature extractor with fewer convolutional layers, or employing simpler multi-modal feature fusion modules. While these solutions reduce complexity, they also result in performance degradation. In this paper, we explore the use of knowledge distillation to shrink the size of RGB-T trackers without sacrificing performance. We propose a teacher-student framework called Cross-Modality Distillation (CMD) that guides efficient imitation in three stages: unimodal feature extraction, multi-modal feature fusion, and target state estimation. We design specific modules for modality-specific and modality-common feature distillation, as well as multi-path selection and response distillation. Experimental results show that our approach achieves state-of-the-art performance on challenging datasets while reducing the number of parameters and computational complexity.