Backdoor attacks pose a significant threat to applications of Deep Learning models, with various attack methods targeting different modalities and features. Existing defense methods, such as fine-tuning, distillation, and neuron pruning, have limitations in removing backdoors effectively. In this paper, we propose a novel backdoor removal technique called MEDIC that clones the benign functionalities of a pre-trained model to create a new sanitized model. MEDIC trains the clone model from scratch using a small set of clean samples, enforcing internal activation equivalence constraints. Additionally, it guides the cloning process using importance values to preclude backdoor behaviors. We evaluate MEDIC on nine different backdoor attacks and compare it against five state-of-the-art methods, showing that MEDIC significantly reduces the attack success rate with minimal benign accuracy degradation. Our contributions include the proposal of the importance-driven cloning method, theoretical analysis of its advantages, and empirical demonstration of its superiority over existing methods.