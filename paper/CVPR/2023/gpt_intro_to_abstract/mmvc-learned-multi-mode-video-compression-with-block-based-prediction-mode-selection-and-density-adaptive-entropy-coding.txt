Video has become the primary means of information and interaction in today's society, leading to a high demand for transmission bandwidth and storage space. Deep video codecs have been developed to leverage deep neural network models and predict future frames, reducing the amount of information that needs to be encoded and transmitted. This paper proposes a block-wise video compression scheme that utilizes content-driven mode selection to adaptively choose the appropriate encoding model for each block. The proposed method includes skip mode, optical flow conditioned feature prediction mode, feature propagation mode, and feature prediction mode. The transmitter generates a low-dimensional representation of each frame, and the block-wise difference between the previous and current frames represents the motion. Different prediction modes are evaluated at the block level to obtain residuals with high sparsity. Additionally, a residual channel removal strategy and density-adaptive entropy coding are proposed to improve compression efficiency. Extensive experiments and comparisons with existing methods demonstrate the effectiveness and superiority of the proposed approach.