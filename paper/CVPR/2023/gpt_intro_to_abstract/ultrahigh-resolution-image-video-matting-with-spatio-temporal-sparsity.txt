Ultrahigh resolution (UHR) matting is a crucial problem in computer science, particularly in applications such as gaming, TV/movie post-production, and image/video editing. However, consumer-level GPUs and mobile devices still have limited hardware resources, making it challenging to apply existing techniques such as guided filters or patch-based methods without introducing unwanted artifacts. Matting involves extracting a detailed alpha matte of a foreground object from an image/video, which can then be composited onto different backgrounds. Most state-of-the-art matting methods take the entire image as input, limiting their resolution due to memory restrictions. To handle UHR images, some approaches involve downsampling the input, resulting in blurry artifacts. Super-resolution methods have been proposed to recover missing details, but they can produce fuzzy artifacts when dealing with complex structures. Patch-based inference is another strategy, but it can lead to artifacts due to insufficient global context or heavy computation requirements with increasing patch size. In this paper, we propose a novel image/video matting framework called SparseMat to address UHR matting efficiently. Our method leverages low-resolution priors to generate spatio-temporal sparsity, which serves as a gate to activate pixels for processing using a sparse convolution module. This approach allows us to skip redundant computations and focus only on irregular and sparse regions surrounding the foreground object. Unlike traditional patch-based methods, our approach considers a more global perspective, producing high-quality alpha mattes for UHR images or videos in a single shot without downsampling or partitioning. We provide extensive experimental results on popular matting datasets, showcasing the superiority of SparseMat in handling general image/video matting tasks efficiently and accurately.