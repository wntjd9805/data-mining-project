Self-supervised methods, which pretrain a backbone with pretext tasks, have become increasingly effective in extracting useful latent representations. These self-supervised tasks, such as distinguishing positive and negative samples or restoring damaged images, have been shown to provide rich feature representations for improving the performance of downstream tasks. In the context of 3D point clouds, annotating data is challenging, motivating the development of self-supervised algorithms. Existing methods for self-supervised representation learning from point clouds require large amounts of data samples, making them computationally expensive. In this work, we propose ToThePoint, a framework for self-supervised pretraining of 3D point cloud features. We introduce a recycling mechanism to make use of discarded point cloud features and use them as a feature augmentation method for contrastive learning. Our approach achieves competitive results with significantly fewer training samples and training time compared to previous baselines. We demonstrate the effectiveness of our method on object classification, few-shot learning, and part segmentation tasks. Ablation studies are performed to analyze the effects of different loss terms on performance. Overall, our work contributes to the advancement of self-supervised learning in the context of 3D point clouds.