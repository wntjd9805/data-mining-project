This paper addresses the problem of assessing the visual quality of user-generated content (UGC) live videos in the context of compression artifacts. The authors first construct a large-scale UGC Live VQA database called TaoLive, consisting of 418 raw UGC live videos from the TaoBao live streaming platform and their corresponding 3,762 compressed videos at various bit rates. A subjective experiment is then conducted in a controlled environment, involving 44 participants and collecting a total of 165,528 subjective annotations. To measure the visual quality of UGC live videos, a multi-dimensional no-reference (NR) VQA model is proposed. This model leverages pretrained 2D-CNN, handcrafted distortion descriptors, and pretrained 3D-CNN for the extraction of semantic, distortion, and motion features, respectively. The extracted features are fused to obtain a video-level quality score. The effectiveness of the proposed method is demonstrated through extensive experimental results. The contributions of this paper include the construction of a large-scale UGC Live VQA database, the design and execution of a well-controlled subjective experiment, and the development of a multi-dimensional NR-VQA model for UGC live videos.