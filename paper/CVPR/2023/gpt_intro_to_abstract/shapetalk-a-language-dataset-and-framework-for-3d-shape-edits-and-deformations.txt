This paper introduces the concept of using natural language to create and adapt visual content in both 2D and 3D scenes. The goal is to democratize the content creation process and allow ordinary users to easily edit their content by describing their intent in words, which will then be translated into edits by AI-powered tools. While there have been successful visual language models for transforming text into 2D images, the need for automated tools in the 3D asset creation field is only starting to emerge. The specific task focused on in this work is modifying the shape of a 3D object according to natural language semantics. Operating directly in a 3D representation provides advantages for downstream tasks that require 3D awareness. The task of faithfully modifying a 3D object's geometry based on its class semantics is complex and has recently been a topic of interest. The proposed framework, ChangeIt3D, consists of three major components: the ShapeTalk dataset, a modular architecture for implementing edits on different 3D shape representations, and a set of evaluation metrics to measure the quality of the transformations. The ShapeTalk dataset contains over half a million discriminative utterances that link 3D shapes and free-form language, focusing on fine-grained differences between objects. To enable shape edits and deformations, the paper explores different generative models of shapes, including Point-Cloud Auto-Encoders, implicit neural methods, and Shape Gradient Fields. A network is trained on the ShapeTalk dataset to identify the target within a distractor-target pair, allowing the same network to guide edits directly in the latent spaces of generative models. Interestingly, it is found that the notion of parts can be learned from language alone, without explicit geometric part supervision. The paper also introduces evaluation metrics for measuring the success and quality of shape modifications. These metrics reflect the realism of the resulting shape, faithfulness to the language instructions, and avoidance of unnecessary changes. The availability of such metrics is crucial for further advancements in the field. In conclusion, this paper presents a large-scale multimodal dataset, ShapeTalk, that allows for detailed differentiation of shapes of common objects through referential language. It proposes a modular framework for language-based shape editing, supporting diverse 3D shape representations and implementing fine-grained edits guided by a 3D-aware neural-listening network. Additionally, intuitive evaluation metrics are introduced to assess the quality of shape edits and deformations.