Real-world 3D-aware editing with a single 2D image is a crucial application in computer graphics, including virtual reality (VR), augmented reality (AR), and immersive meetings. Recent advancements in 3D Generative Adversarial Networks (GANs) have enabled photo-realistic 3D-consistent image generation. However, existing methods struggle to achieve both high-quality reconstruction and 3D consistency simultaneously. While 2D GAN inversion methods can perform 3D-related attribute editing, they lack the underlying 3D representation, resulting in inconsistent generated views. Optimization-based inversion approaches on 3D-aware GANs provide high-fidelity reconstruction but suffer from artifacts in synthesized novel views. To address these challenges, we propose a 3D-aware inversion pipeline that optimizes reconstruction not only on the input image but also on a set of pseudo-multi-views, reducing ambiguity. Estimating pseudo views requires maintaining texture details and generating occluded parts plausibly. Our approach achieves high-fidelity reconstruction by utilizing input image textures for visible parts and a pretrained generator for inpainted regions. Our approach enables two types of editing: latent attribute editing and 3D-aware texture modification. By modifying the inverted latent code, we can control general attributes, while editing textures enables compelling 3D-consistent editing. Our extensive experiments demonstrate that our approach outperforms other baseline methods in terms of photorealism and faithfulness, achieving visually and geometrically consistent 3D-aware novel views.