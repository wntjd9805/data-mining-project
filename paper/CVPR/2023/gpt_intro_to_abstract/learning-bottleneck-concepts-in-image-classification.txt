In the field of explainable AI (XAI), understanding the behavior of deep neural networks (DNNs) is a significant challenge, especially in medical applications and identifying biases in DNNs. Previous research efforts have focused on the post-hoc paradigm for providing explanations by producing relevance maps. However, these maps only provide low-level relationships and lack explicit semantics. The concept-based framework offers a higher-level understanding by explaining decisions through a set of concepts found in the image. While some works have used concepts for interpretation, the link between the decision and concepts is not always clear. The concept bottleneck structure addresses this issue by using the presence/absence of concepts as image representation, making the decision strongly tied to the concepts. Designing a suitable set of concepts for a specific task is a major difficulty, with handcrafting and automated concept discovery being two approaches. This paper proposes the bottleneck concept learner (BotCL), which simultaneously discovers concepts and learns the classifier. BotCL optimizes concepts for the target image classification task without supervision and uses a slot attention-based mechanism to detect regions associated with each concept. The paper also introduces the use of self-supervision and additional constraints for better concept discovery. The contributions of the paper include demonstrating the importance of self-supervision for concept discovery and the effectiveness of additional constraints in improving concept quality.