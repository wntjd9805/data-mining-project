Semantic segmentation is essential in various computer vision applications, but training deep neural networks for this task requires a large amount of pixel-wise annotated data, which is costly and time-consuming. Domain adaptive semantic segmentation, which involves training on a labeled source domain and adapting to an unlabeled target domain, has emerged as a research area to address this issue. However, the performance of models often degrades due to the visual domain gap between the two domains. This paper proposes a novel framework called DiGA for domain adaptive semantic segmentation. The authors identify opportunities for improvement in both the warm-up and self-training stages of the training process. In the warm-up stage, instead of using adversarial training, they propose a pixel-wise symmetric knowledge distillation technique that enhances the model's domain generalizability. They also introduce a data augmentation technique called cross-domain mixture (CrDoMix) to further improve the warm-up model's performance. In the self-training stage, they propose bilateral-consensus pseudo-supervision to enable efficient self-training without the need for categorical thresholds. Experimental results show that their method outperforms prior approaches on popular benchmarks for domain adaptation in semantic segmentation.