Computer vision methods have made great strides in producing high-quality free-viewpoint renderings of static 3D scenes. However, the challenge of synthesizing novel views in dynamic scenes, such as those with people or pets, remains a difficult problem. Recent advancements in time-varying neural volumetric representations have shown promise in synthesizing novel views in space and time. However, these methods have limitations that prevent their application to real-world videos, such as scalability issues and difficulty in capturing complex object motion. In this paper, we propose a new approach that addresses these limitations and is capable of handling dynamic videos with long durations, unbounded scenes, uncontrolled camera trajectories, and fast and complex object motion. Our approach preserves the advantages of volumetric scene representations while significantly enhancing rendering fidelity for both static and dynamic scene content. We draw inspiration from aggregation-based methods for rendering static scenes, but adapt the approach to account for scene motion. We also address efficiency and robustness challenges in scaling up aggregation-based methods for dynamic scenes. To achieve temporal coherence, we introduce a new temporal photometric loss that operates in motion-adjusted ray space. Additionally, we propose a motion segmentation technique to factor the scene into static and dynamic components. Our approach outperforms state-of-the-art methods, producing highly detailed scene renderings with improved quality. We demonstrate the effectiveness of our method on dynamic scene benchmarks and in-the-wild videos, showcasing its applicability to real-world scenarios.