Action segmentation in untrimmed videos of complex human activities is the task of predicting the occurring action at each frame. While existing methods focus on classifying short video clips, action segmentation models need to understand the semantics of all action classes, their temporal boundaries, and contextual relations. However, few studies have investigated action segmentation with multiple data sources. This paper proposes ASPnet, a multi-source action segmentation model that exploits shared and complementary information from different data sources. ASPnet disentangles the latent space into modality-shared and modality-specific representations, allowing for more discriminative features and robust action recognition. The model learns shared feature spaces using Maximum Mean Discrepancy and integrates an attention bottleneck to capture long-range temporal dependencies. Evaluation results on benchmark datasets show improvements over unimodal baselines and different fusion strategies, with competitive or better performance than state-of-the-art models. ASPnet also demonstrates robustness to additive input noise and the ability to generalize well with an increasing number of inputs. Overall, this paper presents a novel approach to multi-source activity recognition for robust action segmentation.