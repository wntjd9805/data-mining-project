The paper introduces a new method, called SinGRAF, for generating realistic variations of a single 3D scene using a small number of photographs as input. Unlike existing 3D generative models that require 3D assets as input, SinGRAF only takes unposed images as input and outputs a generative model of the scene represented as a neural radiance field. The method builds on recent progress in unconditional 3D-aware GANs, training generative radiance fields from single-view images. The paper highlights the challenges in directly applying existing 3D GANs to the problem and presents SinGRAF as a solution for training a 3D generative radiance field for individual indoor 3D scenes. The core of the method involves continuous-scale patch-based adversarial training, where radiance fields are represented as triplane feature maps produced by a StyleGAN2 generator. The generated scenes are volume-rendered from randomly sampled cameras with varying fields of view, and a scale-aware discriminator is used to enforce realistic patch distributions across all sampled views. The paper shows that SinGRAF is able to create plausible 3D variations of a given scene trained only from a set of unposed 2D images, and it outperforms state-of-the-art 3D scene generation methods in terms of realism and diversity.