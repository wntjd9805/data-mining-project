Deep Neural Networks (DNNs) have gained popularity in various scientific fields, but their vulnerability to adversarial examples poses a high risk in real scenarios. Adversarial attacks can result in major changes to network outputs through tiny and imperceptible perturbations. These attacks can be categorized as white-box attacks, where the attacker has full knowledge of the victim model, or black-box attacks, where the attacker has no knowledge but can estimate perturbations through surrogate models or a large number of queries.Adversarial training is a popular technique for improving robustness in DNNs, but it may not be effective against advanced attack techniques. Randomization has shown promise in enhancing adversarial robustness, with noise injection preventing attackers from obtaining precise gradients of loss. However, the design of noises and the trade-offs between robustness and optimization difficulty are often ignored in randomized techniques.To address these challenges, this paper introduces randomness into DNNs using random projection filters. Random projection is a technique for dimension reduction that can preserve pairwise distances between data points. By partially replacing convolutional filters with random projection filters, the proposed scheme aims to achieve better trade-offs between noise injection and adversarial robustness. The effectiveness of the proposed Random Projection Filters (RPF) is validated through extensive empirical evaluations.Overall, this paper presents a novel approach to enhancing adversarial robustness in DNNs through the use of random projection filters. The proposed scheme offers improved trade-offs between noise injection and optimization difficulty, providing a more effective defense mechanism against adversarial attacks.