Deep Neural Networks (DNNs) have gained significant attention in recent years due to their wide range of applications. However, as DNNs become more complex, they require larger computational resources. To address this issue, research on neural network compression and acceleration has gained traction, with quantization being one of the popular methods. Quantization involves converting float network activations and weights to low-bit fixed points, which accelerates inference or training speed with minimal performance degradation. There are two main approaches to network quantization: quantization-aware training (QAT) and post-training quantization (PTQ). While PTQ is more efficient and does not require iterative quantization training, it often sacrifices accuracy, particularly in low-bit compact model quantization.Previous PTQ algorithms have attempted to mitigate the accuracy loss, but they fail to address the issue of oscillation during the reconstruction process. This paper addresses the oscillation problem in PTQ and its impact on the final performance. The authors propose the Mixed REConstruction Granularity (MRECG) method, which optimizes the modules where oscillation occurs. The MRECG method considers module capacity and loss metric and can be applied in both data-dependent and data-free scenarios. The authors demonstrate the effectiveness of their method in a wide range of compression tasks, achieving a top-1 accuracy of 58.49% in MobileNetV2 with 2/4 bit, surpassing the current state-of-the-art methods. They also confirm that the MRECG method eliminates oscillation in the reconstruction process, leading to a more stable network. Overall, this paper addresses the overlooked issue of oscillation in PTQ and presents an effective solution through the MRECG method.