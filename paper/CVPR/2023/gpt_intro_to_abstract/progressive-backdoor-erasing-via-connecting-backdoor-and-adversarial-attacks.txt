Deep neural networks (DNNs) have become widely used in safety-critical applications such as face recognition and autonomous driving, making the security of deep learning a paramount concern. DNNs are susceptible to potential threats in both their inference and training phases. Inference-time attacks, also known as adversarial attacks, aim to deceive a trained model by introducing small adversarial perturbations to produce incorrect predictions. On the other hand, training-time attacks, known as backdoor attacks, attempt to implant a backdoor into a model during the training phase. This causes the infected model to misclassify testing images as a target-label whenever a pre-defined trigger, such as a few pixels, is embedded in the images.In this paper, we demonstrate that there is a connection between backdoor attacks and adversarial attacks, challenging the conventional notion that they are distinct problems. We observe that planting a backdoor into a model significantly affects the model's adversarial examples. Specifically, for an infected model, its adversarial examples are highly likely to be predicted as the backdoor target-label. We show that this phenomenon persists across different target-labels, backdoor attack settings, and trigger embedding mechanisms.Based on our observations, we propose a novel backdoor defense method that leverages adversarial attack techniques. We find that the features of adversarial images from an infected model closely resemble the features of triggered images. This connection allows us to design a defense method that can defend against backdoor attacks without requiring a clean extra dataset. Our method, called Progressive Backdoor Erasing (PBE), alternates between purifying the infected model using adversarial examples and identifying clean images from the training data. This iterative process progressively filters poisoning training data, resulting in a purified model and clean data without the need for a clean extra dataset.We evaluate our approach and demonstrate its effectiveness in defending against modern backdoor attacks. Our method achieves state-of-the-art defensive performance, even in scenarios where a clean extra dataset is not available. This makes it the first work to successfully defend against backdoor attacks without relying on a clean extra dataset.