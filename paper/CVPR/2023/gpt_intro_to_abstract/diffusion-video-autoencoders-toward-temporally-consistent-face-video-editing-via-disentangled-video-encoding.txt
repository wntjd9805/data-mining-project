Face editing is a common task in computer vision with various applications and entertainment purposes. Recent advancements in Generative Adversarial Network (GAN) models have made it possible to manipulate face attributes by modifying latent features. However, GAN-based editing methods may not perfectly reconstruct encoded real images, especially when faces are occluded or decorated. Additionally, ensuring temporal consistency in video editing tasks is challenging. Existing methods rely on smoothness or the same editing step for all frames, which may produce inconsistent results. In this paper, we propose a diffusion video autoencoder framework that overcomes these limitations. Our model uses a diffusion-based approach for face video editing, enabling perfect reconstruction and direct editability of latent features. We decompose the video into time-invariant and per-frame variant features and manipulate a single invariant feature for consistent editing. We experimentally demonstrate the effectiveness of our framework in decomposing videos and providing temporally consistent manipulation. We explore attribute-based editing and propose a text-based editing method using CLIP loss. Our contributions include the development of diffusion video autoencoders, which enable temporally consistent editing, and the capability to edit exceptional cases, such as partially occluded faces. We also introduce a text-based identity editing method using CLIP loss for intermediate products of diffusion video autoencoders.