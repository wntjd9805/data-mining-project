Neural networks have been widely used in various domains, but recent studies have shown that they are susceptible to backdoor attacks. These attacks involve embedding a backdoor into the victim model by poisoning the training dataset, causing the model to behave wrongly on samples containing a specific trigger. This threat can have severe consequences for critical applications such as face authentication, malware detection, speech recognition, and autonomous driving. Researchers have proposed sophisticated attack techniques that improve stealthiness and robustness, but there is a trade-off between visual stealthiness and attack robustness. The existing strategies for stealthy backdoor attacks either rely on invisible triggers or natural triggers, but they have limitations in terms of vulnerability to image transformations and preprocessing-based defenses, and they cannot be applied in the data poisoning threat model. To overcome these limitations, we propose a novel poisoning-based backdoor attack called color backdoor. This attack exhibits both stealthiness and robustness by employing a uniform color space shift as the backdoor trigger. We use Local Interpretable Model-Agnostic Explanations (LIME) to explain the effectiveness of our attack, and we show through experiments that the color backdoor is resilient against preprocessing-based defenses and mainstream defenses. We use Particle Swarm Optimization (PSO) to find the optimal trigger and embed the color backdoor into the victim model during training with the poisoned dataset.