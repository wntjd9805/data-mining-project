Text spotting, the detection and recognition of text in natural scenes, has gained significant attention due to its wide-ranging applications, such as autonomous driving and intelligent navigation. The relationship between detection and recognition is a critical aspect in designing text spotting pipelines, impacting structure design, spotting performance, training efficiency, and annotation cost. Current end-to-end methods follow a detect-then-recognize pipeline, but they have limitations in terms of the need for additional connectors and the lack of synergy between the detection and recognition modules. Segmentation-based methods attempt to address these limitations, but they are sensitive to noise and require post-processing. The use of Transformer models has shown promising results for various computer vision tasks, including text spotting. However, existing methods based on Transformers lack efficient joint representation for scene text detection and recognition. In this paper, we propose a novel query form based on explicit point representations of text lines and introduce a new DETR-like baseline called DeepSolo. DeepSolo utilizes a single Transformer decoder and multiple simple prediction heads to solve text spotting efficiently. We also demonstrate experimentally that DeepSolo outperforms previous methods in terms of spotting accuracy, training efficiency, and annotation flexibility. Overall, this work contributes to advancements in the field of text spotting and offers a more efficient and accurate solution for text detection and recognition in natural scenes.