This paper introduces CLIP-Pixels Only (CLIPPO), a pure pixel-based model for multimodal learning of text and images. CLIPPO is a single Vision Transformer that processes visual input, text, or both together, all rendered as RGB images. Unlike other multimodal models, CLIPPO does not have modality-specific components and instead uses the same model parameters for all modalities. The model is trained using contrastive learning and performs similarly to CLIP-style models on tasks such as image classification and text/image retrieval. Surprisingly, CLIPPO can also perform complex language understanding tasks without explicit word-level losses. It outperforms classic NLP baselines on the GLUE benchmark and achieves good performance on VQA when rendering the image and text together. Pixel-based models have advantages over regular language models as they do not require pre-determining the vocabulary/tokenizer. Overall, CLIPPO offers a valuable step towards developing a single end-to-end model for multimodal learning.