Recently, pre-training large foundation models on massive amounts of data has emerged as a successful approach for learning generic representations in various data modalities. These pre-trained models can be easily adapted to different downstream tasks, resulting in excellent generalization capabilities. In the field of computer vision, efforts have been made to develop effective pre-trained models, particularly using masked autoencoding with the Transformer architecture. However, scaling up masked autoencoder pre-training for vision models, especially for videos, remains a challenge due to high computational costs and memory limitations. In this paper, we focus on scaling the video masked autoencoder (VideoMAE) and pushing its performance limits on various video downstream tasks. We investigate the scalability of VideoMAE in terms of model capacity and data size. To address the computational and memory limitations, we propose a dual masking strategy that not only masks the encoder but also masks the decoder based on the redundancy in video data. We also propose a progressive training pipeline that involves pre-training on a million-level unlabeled video dataset and post-pre-training on a labeled hybrid dataset. Our experiments demonstrate that this approach enables efficient training of a billion-parameter video transformer model, achieving state-of-the-art performance on action recognition, spatial action detection, and temporal action detection tasks.