Vision-language representation learning aims to optimize a joint embedding model for both image and text data, enabling the transfer of learned representations to solve various vision-language tasks. Recent advancements in large-scale image-text datasets and language models have spurred active research in this field. Self-supervision tasks such as image-text matching, masked language modeling, and image-text contrastive learning have been explored. However, during the pretraining stage, models are trained solely on image-caption pairs, lacking an understanding of various attributes observed in images. To address this limitation, this paper proposes three techniques: a soft masking technique on visual features, a focal version of the image-text contrastive learning loss to focus on hard examples, and multimodal data augmentations. These techniques aim to diversify observations and improve the performance of learned representations. Experimental results and performance evaluations on multiple downstream tasks demonstrate the effectiveness of the proposed approach.