3D Avatars play a crucial role in emerging applications like 3D games and the Metaverse, enhancing user engagement through personalization. While methods for generating and reconstructing 3D avatars have achieved impressive results, the ability to edit and customize these avatars remains unknown. In this paper, we propose a data-driven method for avatar creation and customization that enables partial transfer of geometric and appearance details between 3D assets, as well as authoring details via 2D-3D transfer. Our method ensures consistent local details in the avatars even when posed. Existing methods lack such capabilities, with generative models failing to control local details due to the entanglement of color and geometry in 2D supervision signals, and mesh-based representations having limited representational power for challenging geometry and flexible topologies. Inspired by recent neural 3D representations, we introduce a hybrid representation that combines consistent topologies with the representational power of neural fields. We propose a training pipeline that optimizes multi-subject feature codebooks and shared decoder weights through 3D reconstruction and 2D adversarial losses. Additionally, we present a large-scale dataset of high-quality 3D human scans and models for research purposes. Our contributions include a novel hybrid representation, a generative pipeline for avatar creation, and a high-quality dataset of 3D human scans.