This paper introduces a novel approach called Preﬁx Conditioning for unifying image-caption and image classification datasets during pre-training. The traditional use of supervised classification datasets for pre-training image representations is limited in its bias towards specific categories. On the other hand, web-scale image-caption datasets offer a wider variety of scene types and vocabularies, resulting in stronger generalization and remarkable performance on zero-shot image classification tasks. However, combining caption and classification datasets still presents challenges in resolving distribution differences and dataset biases. To address this, the authors propose learning dataset-specific language embeddings using Preﬁx Conditioning. This involves the use of different preﬁx tokens for each dataset to absorb the dataset bias, allowing the remaining tokens to focus on learning visual concepts. The approach is inspired by preﬁx or prompt tuning techniques but differs in its goal of unifying datasets and its application in vision language pre-training. Experimental results demonstrate that Preﬁx Conditioning improves zero-shot recognition performance and enables the model to extract language features more effectively. The contributions of this work include the first mechanism to use preﬁxes for conditioning the dataset source during vision language contrastive pre-training, as well as significant performance improvements on zero-shot evaluation.