Abstract: This paper explores the problem of multi-task learning in the context of heterogeneous fashion tasks in the Vision-and-Language (V+L) domain. Existing methods typically use a pre-trained generic V+L model and independently fine-tune it for each task, resulting in low parameter efficiency and lack of inter-task relatedness. To address these limitations, we propose a novel Fashion-focused Multi-task Efficient learning method for various Vision-and-Language based fashion tasks (FAME-ViL). FAME-ViL utilizes a task-versatile architecture on top of a pre-trained model and introduces lightweight Cross-Attention Adapters (XAA) to enable cross-modality interaction. We also introduce Task-Specific Adapters (TSA) to handle input/output format incompatibilities and a multi-teacher distillation scheme to mitigate overfitting and dataset imbalance problems. Our experiments on four diverse fashion tasks demonstrate that FAME-ViL outperforms existing single-task models while achieving significant parameter savings.