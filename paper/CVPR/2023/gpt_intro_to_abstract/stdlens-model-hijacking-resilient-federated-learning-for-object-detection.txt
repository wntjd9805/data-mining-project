Federated Learning (FL) has gained popularity in the field of object detection, particularly in the healthcare industry where privacy regulations are stringent. FL enables the training of a global object detection model using a distributed network of clients, each sharing only their model parameters with the central server while keeping their raw data private. However, this decentralized approach makes FL susceptible to model hijacking, where attackers aim to interfere with the training process and cause the model to misbehave. Model hijacking can be achieved through data poisoning, where compromised clients inject manipulated data into the training process, leading to the generation of malicious gradients.In this paper, we propose STDLens, a three-tier defense methodology against model hijacking attacks in FL-based object detection systems. Firstly, we analyze existing spatial signature-based defenses and demonstrate their ineffectiveness in protecting FL. Secondly, we introduce a pipeline that employs per-client spatio-temporal signature analysis to identify Trojaned gradients, track their contributors, revoke their subscriptions, and restore detection performance. Additionally, we introduce a density-based confidence inspection mechanism to manage spatio-temporal uncertainty and avoid purging benign clients contributing useful learning signals. Finally, we extend perception poisoning with adaptive techniques to analyze STDLens in countering advanced adversaries.Extensive experiments show that STDLens outperforms competitive defense methods in terms of defense precision, with a low false-positive rate. It demonstrates a significant reduction in the accuracy drop under attack, maintaining object detection accuracy comparable to the benign scenario. The proposed defense methodology provides a robust solution to mitigate the risks associated with model hijacking attacks in FL-based object detection systems.