Recognizing human actions from videos has numerous practical applications, including behavior understanding, medical assistive applications, AR/VR applications, and surveillance. Most previous research on action recognition has focused on appearance information, but recent methods have highlighted the advantages of using pose/skeleton information as a separate cue. However, annotating videos for skeleton-based action recognition is a challenging and time-consuming task. Self-supervised learning, which involves learning without labels, has been shown to have advantages such as improved transfer performance and robustness to domain shift and noisy labels. Inspired by these methodologies, this paper proposes a new approach for skeleton-based action recognition that does not rely on labels. The paper explores the use of hallucinated latent positives in a contrastive learning framework, aiming to reduce the dependence on hand-crafted data augmentations and improve training efficiency. The proposed approach introduces two new components: a prototype extraction method and a positive hallucination module. Experimental results on benchmark datasets demonstrate consistent improvements over state-of-the-art methods. The approach is flexible and can be used in both uni-modal and multi-modal training settings. The contributions of this paper include the proposal of the HaLP approach, the definition of an objective function for generating hard positives, and the consistent improvements achieved on benchmark datasets and tasks.