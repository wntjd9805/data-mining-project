This paper introduces a novel training approach for deep neural networks called co-training of submodels (cosub). The authors address the challenges of training deep neural architectures by utilizing residual connections and leveraging large datasets. They also discuss the limitations of traditional regularization techniques and propose the use of data augmentation strategies and self-supervised pre-training to address overparameterization. The paper presents the concept of co-distillation, where a pool of models supervise each other, and introduces a practical implementation of cosub by using layerwise dropout to instantiate submodels. The submodels serve as teachers to each other, providing additional supervision signal and ensuring consistency. The authors demonstrate the effectiveness of cosub by achieving state-of-the-art results in various tasks and architectures. They provide analyses, ablations, and share models and code for reproducibility.