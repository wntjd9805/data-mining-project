Recent advances in deep learning have allowed machines to tackle complex video-language tasks, including video question answering (VideoQA). However, existing VideoQA benchmarks have limitations and shortcomings in terms of visual diversity, biased answer distributions, and limited representation of fine-grained video semantics. To address these issues, we introduce ANetQA, a new benchmark that supports fine-grained compositional reasoning over complex web videos. ANetQA is built upon untrimmed long videos with fine-grained semantics and utilizes spatio-temporal scene graphs to represent objects, relationships, attributes, and actions in natural language. We generate diverse question templates that require fine-grained compositional reasoning. ANetQA consists of 1.4 billion unbalanced and 13.4 million balanced QA pairs, making it the largest VideoQA benchmark to date. We evaluate state-of-the-art VideoQA models on ANetQA and find that there is significant room for improvement, with the best model achieving 44.5% accuracy compared to human performance at 84.5%. The ANetQA benchmark is available for further research and development.