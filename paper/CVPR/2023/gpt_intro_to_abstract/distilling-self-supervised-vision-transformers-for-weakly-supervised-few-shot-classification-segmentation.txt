Few-shot learning is the task of learning to make predictions using a small number of examples, which is useful in applications where collecting large amounts of annotated data is expensive or impractical. In this paper, we focus on the problem of few-shot classification and segmentation (FS-CS), where we aim to predict the presence of each class and perform pixel-level segmentation based on limited supervision. Traditional few-shot learning methods rely on fully-annotated datasets, but in FS-CS, we explore the use of weakly-supervised learning with only image-level labels. To generate pseudo-ground truth segmentation masks, we leverage attention maps from a self-supervised vision transformer, and train an FS-CS learner on top of this transformer using the generated masks as supervision. We also propose a practical training setting where some images have both image-level and pixel-level annotations, and train an auxiliary mask enhancer to refine the attention maps into pseudo-ground truth masks. Finally, we introduce a Classification-Segmentation Transformer (CST) architecture that predicts classification and segmentation outputs through separate task heads, achieving improved performance in FS-CS compared to prior state-of-the-art methods. Our contributions include a powerful baseline for FS-CS using only image-level supervision, a learning setup for FS-CS with mixed supervision, and the design of CST for flexible tuning of the degree of supervision.