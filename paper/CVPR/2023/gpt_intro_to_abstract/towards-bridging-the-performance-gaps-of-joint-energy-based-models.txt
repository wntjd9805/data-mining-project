Deep neural networks (DNNs) have achieved impressive results in various learning tasks such as image classification and generation. Energy-based models (EBMs) have recently gained attention as they show promise in training generative models. However, training these models on complex high-dimensional data remains challenging due to the need for expensive MCMC sampling. Additionally, the accuracy and generation quality of models produced by EBMs still lag behind standard softmax classifiers and GAN-based approaches. In this paper, we propose several training techniques to address these limitations. We analyze the trained models under the lens of loss geometry and visualize their energy landscapes. We find that JEM, a popular EBM, converges to sharp local maxima, which hampers generalization. By incorporating sharpness-aware minimization (SAM) to promote smoothness, we improve image classification accuracy and generation quality. We also identify the negative impact of data augmentation and introduce separate data loaders for image classification and generation, leading to significant improvements in generation quality. Our extensive experiments on multiple datasets demonstrate that our proposed model, SADA-JEM, achieves state-of-the-art performance in both discriminative and generative tasks while surpassing JEM in terms of calibration, out-of-distribution detection, and adversarial robustness.