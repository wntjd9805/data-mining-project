Person re-identification (ReID) is an important task in computer vision that aims to identify pedestrians in images and videos. It is widely used in various fields such as intelligent video surveillance and intelligent security. Existing ReID research focuses on single-modal or cross-modal ReID, where retrieval is based on RGB images or different query modalities (e.g., infrared, text, or sketch). However, in real-world scenarios, it is often uncertain whether text or sketch information is available for a specific query, which poses a challenge for existing methods. To address this challenge, this paper proposes a modality-agnostic person re-identification model (UNIReID) that can handle uncertain query inputs. UNIReID combines cross-modal and multi-modal learning by using a dual-encoder for visual and textual feature extraction. It also incorporates a task-aware dynamic training strategy to balance learning among different retrieval tasks. Additionally, a cross-modality interaction is designed to align sketch and text feature representations for modality fusion. To support the study of modality-agnostic ReID, three multi-modal ReID datasets are constructed. Experimental results demonstrate that UNIReID outperforms existing methods in terms of accuracy and generalization capability.