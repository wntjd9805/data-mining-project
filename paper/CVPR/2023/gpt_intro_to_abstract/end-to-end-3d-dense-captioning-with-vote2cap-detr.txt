Abstract:The field of 3D learning has witnessed significant growth in recent years, with 3D dense captioning emerging as a challenging problem. This task involves localizing objects in a 3D scene and generating descriptive sentences for each object. Existing approaches typically adopt a two-stage pipeline, consisting of object detection followed by caption generation. However, this pipeline suffers from limitations such as high dependency on detection performance and reliance on hand-crafted components. To overcome these challenges, we propose Vote2Cap-DETR, a novel one-stage transformer-based model for 3D dense captioning. Our approach frames the problem as a set prediction task and introduces a vote query driven decoder for better object localization. Through extensive experiments on popular datasets, we demonstrate that our approach outperforms prior methods and achieves state-of-the-art performance. This research contributes to advancing the field of 3D vision and language tasks by showcasing the superiority of a fully transformer architecture with sophisticated components for 3D dense captioning.