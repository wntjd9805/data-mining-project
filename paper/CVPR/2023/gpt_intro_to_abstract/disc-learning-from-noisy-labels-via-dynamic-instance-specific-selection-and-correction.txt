Label noise is a common issue in image classification models, particularly when dealing with large-scale databases obtained through web-crawling, crowd-sourcing, or pre-trained models. Recent studies have shown that deep neural networks (DNNs) are susceptible to label noise and can fit to the entire dataset, including the noisy set. Additionally, DNNs exhibit a memorization effect, where they first memorize simple patterns and then more difficult patterns with noisy labels. Early-learning-based methods for label noise learning (LNL) have been proposed, including sample selection, label correction, and regularization. Sample selection methods utilize the losses or confidence of an early-stage DNN to select reliable instances for network updates, while label correction methods generate pseudo-labels to replace the original noisy ones. Regularization-based methods aim to design robust loss functions or augmentation techniques. However, these existing methods have limitations, such as the need for a predefined threshold, uniform thresholding across classes, or poor generalization under extremely noisy data.In this paper, we propose a Dynamic Instance-specific Selection and Correction (DISC) approach for LNL. DISC leverages a dynamic instance-specific threshold strategy that follows a memorization curriculum, selecting reliable instances and correcting noisy labels. The dynamic threshold strategy determines a threshold for each instance based on its memorization strength. DISC also employs weak and strong augmentations to produce two different views, considering the consistency and discrepancy between the views to divide the noisy data into reliable instances, hard instances, and recalibrated noisy labeled instances. Each subset is handled with different regularization strategies. The contributions of this paper include the observation and validation of the memorization strength of DNNs, the proposal of a dynamic threshold strategy, and the utilization of different subsets for noise contamination alleviation and improved utilization of the noisy dataset.