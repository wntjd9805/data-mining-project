Automatic real image editing is a challenging problem in generative models, especially when the target image is outside the training data distribution. Recent advancements in large-scale text-to-image models show promise in high-quality image generation with natural language guidance. However, these models still suffer from limitations, such as overfitting and lack of understanding of object geometry. In this paper, we propose SINE, a framework utilizing pre-trained text-to-image diffusion models for single image editing and content manipulation. Our approach addresses these limitations by introducing model-based classifier-free guidance and a patch-based fine-tuning strategy. Our method allows for creative editing of a single unique image to the targeted domain with preserved details in arbitrary resolution. Experimental results demonstrate the effectiveness of our approach in various editing tasks, including style transfer, content addition, and object manipulation. We believe that our method can significantly contribute to creative content creation and extend the possibilities of image editing.