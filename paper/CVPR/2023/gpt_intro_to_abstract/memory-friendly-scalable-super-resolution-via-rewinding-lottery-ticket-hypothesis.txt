The rise of deep learning has led to significant progress in single image super-resolution (SISR) methods. However, these methods often require substantial computational and memory resources, making them unsuitable for resource-limited devices. To address this limitation, lightweight SR methods have been developed, focusing on designing compact architectures. However, these lightweight models with fixed sizes are not flexible in applications and may require retraining and compression methods to match the resources of different platforms. This paper addresses the need for customizable models based on deployment resources and proposes a memory-friendly scalable deep SR model (MSSR) using the rewinding Lottery Ticket Hypothesis (LTH) approach. MSSR is capable of adapting to different memory resources without the need for loading or offloading different models. It achieves model compression and expansion through iterative pruning and rewinding LTH. Additionally, stochastic self-distillation (SSD) is used to improve the performance of the small-scale SR model. Experimental results demonstrate the effectiveness of MSSR in producing high-quality super-resolved images while accommodating resource constraints and outperforming state-of-the-art models.