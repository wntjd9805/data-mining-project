Image captioning is a fundamental task in the fields of computer vision and natural language processing, aiming to describe the semantics of an image in a natural sentence. State-of-the-art techniques often rely on an autoregressive approach, where an image encoder encodes visual content and a sentence decoder decodes the sequential sentence word-by-word. However, this approach has limitations, such as unidirectional message passing and high computational resources required for longer sentences. Non-autoregressive approaches have emerged as a solution but suffer from problems like word repetition and omissions due to under-exploited sequential dependencies. Diffusion models have shown promise for visual content generation, but their application to image captioning has not fully addressed the word repetition or omission issues. In this paper, we propose a new diffusion model-based non-autoregressive paradigm called Semantic-Conditional Diffusion Networks (SCD-Net). SCD-Net incorporates comprehensive semantic information from input images into the continuous diffusion process, guiding the learning of reverse state transitions. This helps to alleviate the omission of semantic words in the output sentence. Additionally, SCD-Net enables powerful self-critical sequence learning during the diffusion process to improve linguistic coherence and reduce word repetition. Technically, SCD-Net consists of cascaded Diffusion Transformer structures, which progressively enhance the output sentences. Each Diffusion Transformer retrieves semantically relevant words using a cross-modal retrieval model and incorporates them into the diffusion process to constrain reverse state transitions. We also introduce a new guided self-critical sequence learning strategy to transfer knowledge from autoregressive Transformer models to our non-autoregressive Diffusion Transformer. Our contributions include the design of SCD-Net's semantic-conditional diffusion process for image captioning, a novel approach for coupling continuous diffusion with guided self-critical sequence learning. We validate SCD-Net through extensive experiments on the COCO dataset, demonstrating its potential for improving visual-language alignment in image captioning tasks.