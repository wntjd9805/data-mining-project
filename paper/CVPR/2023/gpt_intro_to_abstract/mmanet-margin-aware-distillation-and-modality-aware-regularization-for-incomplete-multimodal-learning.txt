Multimodal learning has been successful in various computer vision tasks, but most existing approaches assume that models are trained and tested using the same modality data. However, due to factors such as device limitations, user privacy, and working conditions, obtaining complete modality data during inference is often costly or infeasible. This has led to interest in assisting incomplete or single modality inference using complete modality data during training. Previous solutions include reconstructing missing modalities from available ones, but this approach has high complexity as it requires building specific models for each modality combination. Recent studies have focused on learning a unified model for different modality combinations, but their performance is often sub-optimal compared to customized models for specific modalities. This is because existing unified models tend to focus on modality-invariant features and ignore modality-specific information, which can improve inference performance. Therefore, this paper aims to address two research questions: 1) Can a unified model consider both modality-invariant and specific information while maintaining robustness for incomplete modality input? and 2) How can the weak modality combination be effectively optimized in varying scenarios? To address these questions, the paper proposes a margin-aware distillation (MAD) method to guide the unified model to acquire complementary information and a regularization network with a memorization augmented regularization (MAR) algorithm to improve the representation ability for weak modality combinations. These methods are combined in a model and task agnostic framework called MMANet, which demonstrates effectiveness in multimodal classification and segmentation tasks through extensive experiments and comparisons.