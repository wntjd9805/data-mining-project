Video frame interpolation (VFI) is a crucial task in computer vision, with applications in video compression, novel-view rendering, and slow-motion video creation. Existing approaches for VFI can be categorized into two paradigms: mixed extraction of motion and appearance information, or separate extraction using additional modules. However, these paradigms have limitations in terms of design complexity, computational overhead, and capturing appearance correspondence between frames. To address these issues, we propose a unified operation of inter-frame attention for explicit extraction of both motion and appearance information. This approach allows us to enhance appearance features and acquire motion features simultaneously by reusing attention maps. Through a hierarchical stacking of this basic processing unit, we can obtain motion and appearance information. Our proposed method utilizes inter-frame attention to derive an attention map representing temporal correlation, which is used to aggregate appearance features and weight displacement for motion estimation. This results in the synthesis of intermediate frames using light networks. Compared to previous methods, our design preserves static structure information, allows generation of frames at any moment between input frames, and balances overall performance with inference speed. We adopt a hybrid architecture of CNN and Transformer, inspired by recent works, to efficiently extract motion and appearance features. This architecture overcomes the computational overhead of inter-frame attention at high-resolution input while preserving fine-grained information. Our proposed model achieves state-of-the-art performance on various datasets while being efficient compared to models with similar performance.