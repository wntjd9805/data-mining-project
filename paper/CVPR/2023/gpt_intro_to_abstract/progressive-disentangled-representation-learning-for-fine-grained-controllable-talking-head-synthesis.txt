The introduction of this computer science paper discusses the importance of talking head synthesis in creating realistic video avatars for various applications. Previous research has focused on either audio-driven methods for accurate lip motion synthesis or video-driven methods for transferring facial motions as a unity. However, the authors argue that a fine-grained and disentangled control over multiple facial motions is necessary to achieve lifelike talking heads. Existing methods have limitations in modifying specific factors or providing precise control over individual factors. In response to these challenges, the paper proposes a Progressive Disentangled Fine-Grained Controllable Talking Head (PD-FGC) method that allows for disentangled control over lip motion, head pose, eye gaze and blink, and emotional expression. The proposed method utilizes a progressive disentangled representation learning strategy to separate each motion factor based on their individual properties, including appearance and motion disentanglement, fine-grained motion disentanglement, and expression disentanglement. The contributions of the paper include the novel one-shot and fine-grained controllable talking head synthesis method, leveraging motion-specific contrastive learning and feature-level decorrelation, and achieving precise control over diverse facial motions.