In recent years, there has been growing research on the failure modes of deep vision models. Texture bias and simplicity bias have been identified as common issues, where image classifiers rely too heavily on textural cues and simple features, despite the presence of more predictive complex features. Additionally, evidence from psychology and neuroscience suggests that deep networks prioritize local features over global features, which differs from human behavior. Furthermore, there is a mismatch between the cognitive concepts implied by category labels in image datasets and the actual information available to models. In response to these challenges, the use of inductive biases and data augmentation methods has been proposed. This paper introduces ELEAS (Edge Learning for Shape sensitivity), a lightweight adversarial augmentation technique that aims to increase shape sensitivity in vision models. ELEAS encourages shape recognition by augmenting datasets with superpositions of images, where one image is processed to produce an edge map and the other is modified by shuffling image patches. Experimental results demonstrate that ELEAS significantly improves shape sensitivity and classifier robustness compared to previous approaches. The contributions of this paper include the proposal of ELEAS as a lightweight augmentation technique, the demonstration of increased shape sensitivity across various tasks, and the achievement of improved object classification accuracy and out-of-distribution robustness.