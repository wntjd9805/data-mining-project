Animating a static portrait image with speech audio is a challenging task with important applications in digital human creation and video conferences. Previous works have focused on generating lip motion, but recent works aim to generate a realistic talking face video with other related motions such as head pose. However, the quality of the generated videos is still unnatural and restricted by factors such as pose preference, mouth blur, identity modification, and distorted face. The connections between audio and different motions are complex, and previous methods struggle to disentangle head motion and expression, leading to distorted faces. Latent-based face animation methods also face challenges in synthesizing high-quality video. In this paper, we propose SadTalker, a stylized audio-driven talking head video generation system that utilizes the highly decoupled representation of a 3D facial model. We divide our task into two major components - generating realistic motion coefficients from audio and learning each motion individually. We design a novel audio to expression coefficient network and use a conditional VAE to model diverse and lifelike head motion. We drive the source image through a 3D-aware face render using explicit 3DMM coefficients and generate warping fields through unsupervised 3D keypoints. Each sub-network is trained individually, and our system can be inferred in an end-to-end style. Experimental results show that SadTalker outperforms existing methods in terms of motion synchronization and video quality. The main contributions of this paper are the introduction of SadTalker, the presentation of ExpNet and PoseVAE for learning realistic 3D motion coefficients, the proposal of a novel semantic-disentangled and 3D-aware face render, and the demonstration of state-of-the-art performance in motion synchronization and video quality.