Semantic segmentation of LiDAR point clouds plays a crucial role in enabling vehicles to perceive and navigate their 3D environment in autonomous driving tasks. Traditionally, LiDAR point clouds are projected onto a 2D surface and processed using convolutional neural networks (CNNs) designed for images. However, the recent success of vision transformers (ViTs) in image understanding tasks motivated us to explore their potential in LiDAR point cloud segmentation. In this paper, we propose a pure ViT-based architecture, called RangeViT, for LiDAR point cloud semantic segmentation. We leverage pre-trained ViT models on large-scale natural image datasets, despite the domain difference between images and point clouds. We also introduce a multi-layer convolutional stem and a convolutional decoder to compensate for the lack of inductive bias in ViTs. Our experiments demonstrate that RangeViT achieves state-of-the-art results among projection-based segmentation methods for LiDAR point clouds. Furthermore, our approach allows for the transfer of knowledge from pre-trained ViTs in the RGB image domain to enhance segmentation performance. Overall, this paper presents a novel and effective approach for semantic segmentation of LiDAR point clouds using vision transformers.