Vision-language representation learning from image-text pairs has gained significant attention for its potential in applications such as zero-shot classification and text-image retrieval. This paper introduces CLIP and other representative works that leverage massive amounts of image-text pairs to learn rich visual representations. While contrastive learning has shown promise in improving representation learning, a challenge in training CLIP-like models is how to increase batch size without exceeding GPU memory constraints. To address this, the authors propose DisCo-CLIP, a distributed solution that divides the contrastive loss computation into intra-GPU and inter-GPU parts, reducing memory consumption and enabling larger batch sizes. The proposed solution is proven to be mathematically equivalent to the original non-distributed contrastive loss while being more memory- and computation-efficient. The authors validate their contributions by demonstrating that training with a larger batch size can lead to improved contrastive learning model performance.