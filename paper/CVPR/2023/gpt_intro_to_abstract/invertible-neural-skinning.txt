The goal of this paper is to build animatable human representations from raw 3D scans and corresponding poses for realistic augmented or virtual reality experiences. Previous work in this area has focused on building parametric models or learning implicit 3D neural representations from data in canonical space. However, these methods have limitations in handling complex pose-varying deformations, especially for clothed humans. Additionally, the process of extracting a new mesh for each pose leads to a loss of correspondences between surfaces of the same subject across different poses.To address these limitations, this paper introduces an end-to-end learnable reposing pipeline called Invertible Neural Skinning (INS) that leverages Invertible Neural Networks (INN). INNs are bijection functions that can preserve exact correspondences while learning complex non-linear transforms. The proposed INS pipeline consists of a Pose-conditioned Invertible Network (PIN1) for learning pose-conditioned deformations, and two PINs placed around a differentiable Linear Blend Skinning (LBS) module to capture non-linear surface deformations of clothes. The canonical representation remains pose-free, allowing for efficient mesh extraction and reposing.Experimental results demonstrate that INS outperforms the previous state-of-the-art reposing method, SNARF, on clothed humans data. INS provides an absolute gain of roughly 1% compared to SNARF with pose-conditioning, and roughly 6% compared to SNARF without pose-conditioning. INS also shows competitive results on minimally clothed human data and is an order of magnitude faster at reposing long sequences. Ablation experiments confirm the effectiveness of the pose-conditioning formulation and showcase INS's ability to correct LBS artifacts.