This paper focuses on the challenge of accurately capturing human-object interaction using a single RGB camera, which is more convenient and user-friendly compared to existing methods that require expensive and complicated setup. Previous approaches using high-end systems or multi-view RGBD cameras have limitations in terms of cost and ease of use. The lack of depth information in monocular RGB images makes it difficult to reason about 3D human and object tracking, leading to temporally incoherent tracking and challenges with occlusions. Existing methods rely on hand-crafted heuristics or fixed depth assumptions, which are not accurate or robust. In this work, the authors propose a method that combines neural field predictions and model fitting to accurately track both human and object from monocular RGB videos. They condition the neural field reconstructions on per-frame SMPL estimates to improve translation and incorporate a transformer-based network to predict object pose under heavy occlusions. The proposed method is evaluated on benchmark datasets and outperforms the current state-of-the-art method. The authors also provide an ablation study to demonstrate the importance of SMPL conditioning and human/object pose prediction network. Overall, the contributions include the first method for joint tracking of human-object interaction from a single RGB camera, SMPL-T conditioned interaction fields, and a novel object pose prediction network. The paper concludes by highlighting the availability of code and pretrained models for further research in this direction.