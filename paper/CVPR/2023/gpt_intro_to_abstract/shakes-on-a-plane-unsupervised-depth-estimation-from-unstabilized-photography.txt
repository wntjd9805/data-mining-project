This paper introduces a novel approach for estimating high-quality object depth and camera motion using unstabilized long-burst photography. The method utilizes a neural RGB-D scene fitting approach to distill affine depth and camera pose estimates from 12-megapixel RAW frames and gyroscope data. Unlike previous methods, it does not require depth initialization or pose inputs, only a long-burst. The approach is formulated as an image synthesis task, similar to neural radiance methods, but incorporates explicit geometric projection through continuous depth and pose models. The paper also presents a smartphone data collection application for capturing the necessary data for the method, along with evaluations that demonstrate its superiority over existing depth estimation approaches. The accuracy of the reconstructed object geometries is validated through comparisons with high-precision structured light scans. Code, data, videos, and additional materials for the method are available on the project website.