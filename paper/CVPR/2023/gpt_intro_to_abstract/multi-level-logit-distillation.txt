This paper explores the challenge of deploying deep learning models in practical scenarios, such as mobile devices, due to their large model size. To address this challenge, Knowledge Distillation (KD) was introduced as a method to reduce model capacity. KD involves distilling the knowledge from a large teacher model to a smaller student model. Two main approaches in KD are logit distillation, which focuses on knowledge transfer at the logit level, and feature distillation, which utilizes intermediate layers in addition to logit outputs. The authors propose multi-level logit distillation, a method that improves upon logit distillation by conducting alignment at the instance, batch, and class levels. This approach makes better use of logit outputs by absorbing more information. The authors compare their method with previous approaches and introduce a prediction augmentation technique based on model calibration. Experimental results on mainstream benchmarks show that the proposed method outperforms previous logit distillation methods and achieves competitive performance compared to feature distillation methods, demonstrating the effectiveness of utilizing logit outputs in knowledge distillation.