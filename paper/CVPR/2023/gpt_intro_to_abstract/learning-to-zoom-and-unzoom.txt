Perception systems in various applications are often constrained by limited compute resources and strict real-time performance requirements. To overcome these constraints, downsampling techniques are commonly used during inference, but this results in the loss of valuable information. Learning to Zoom (LZ) has been proposed as a solution for nonuniform downsampling, where more salient regions of an image are sampled denser than others. However, adapting LZ to tasks with spatial labels, such as semantic segmentation and object detection, has proven to be challenging. In this paper, we introduce a general framework called Learning to Zoom and Unzoom (LZU) that allows for intelligent zooming, processing, and unzooming of an input image. LZU can be applied to any network that uses 2D spatial features to process spatial inputs without any modifications to the network or loss. We demonstrate the effectiveness of LZU on multiple tasks including object detection, semantic segmentation, and monocular 3D detection, and show that it outperforms uniform downsampling and previous works with minimal additional latency. Interestingly, our results also indicate that LZU can improve performance by intelligently upsampling, suggesting the limitations of current networks in handling small objects.