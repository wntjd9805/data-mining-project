Face swapping technology has gained significant interest from both the vision and graphics communities due to its wide range of applications. The goal of face swapping is to transfer the identity of a source face onto a target image or video frame while maintaining the attributes such as pose, expression, and background. Previous approaches have used 3D models or generative adversarial networks (GANs) to achieve face swapping but have encountered challenges such as artifacts and minor modifications to the target image. Diffusion-based models (DMs) have shown promise in generating high-resolution images with complex scenes, prompting the question of whether they can benefit face swapping. However, applying DMs to this task is nontrivial due to the lack of ground-truth data pairs and the computational expense of executing the autoencoder for a large number of steps. To address these challenges, this paper proposes the first diffusion model based face swapping framework, which formulates face swapping as a conditional inpainting task guided by identity features and facial landmarks. The diffusion model is trained to generate a face with the same identity as the source face, aligned with the target face. A midpoint estimation method is introduced to efficiently generate swapped faces with only 2 steps during training. The framework also allows for high controllability, with customizable landmarks and inpainting masks. Experimental results on various datasets demonstrate the effectiveness of the proposed model, outperforming previous methods in terms of ID retrieval and fidelity, while maintaining comparable results in pose and expression error. The framework also proves to be scalable and controllable, enabling higher-resolution face swapping and region-customizable face swapping. Overall, this paper presents a promising face swapping framework that distinguishes itself from existing methods, offering high fidelity, controllability, and scalability.