Sign languages are the primary languages used by deaf communities, and they have their own linguistic properties. However, sign languages are distinct from natural languages as they convey information through visual movements. This paper focuses on sign language recognition (SLR), which involves classifying isolated signs from videos into glosses. SLR has various applications, including sign spotting, sign video retrieval, sign language translation, and continuous sign language recognition.One challenge in SLR is the presence of visually indistinguishable signs called VISigns. VISigns have similar handshape and motion but different semantic meanings. Previous works have shown that vision neural networks are less effective in accurately recognizing VISigns. However, glosses, which are the labels of signs, provide semantic information that can be leveraged for sign recognition. This paper builds upon two findings related to VISigns:Finding 1: VISigns may have similar semantic meanings. Assigning hard labels to VISigns hinders training, so this paper proposes language-aware label smoothing. This approach generates soft labels for training samples and assigns higher values to negative terms with similar semantic meanings to the ground truth gloss.Finding 2: VISigns may have distinct semantic meanings. Although VISigns are visually challenging to classify, their glosses may have distinguishable semantic meanings. This paper introduces inter-modality mixup, which combines vision features and gloss features to maximize signs' separability in a latent space.The contributions of this paper include the incorporation of natural language modeling into SLR, the proposal of language-aware label smoothing and inter-modality mixup to leverage the linguistic properties of VISigns, and the presentation of a novel backbone network called VKNet for SLR. Experimental results show that the proposed method achieves state-of-the-art performance on SLR datasets.