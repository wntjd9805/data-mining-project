Action recognition is a crucial task in various fields such as video surveillance, human-computer interaction, sports analysis, and virtual reality. While most existing methods focus on modeling the spatio-temporal representations from RGB frames and/or optical flow, skeleton sequences have been proven to be robust against sensor noises and efficient in action recognition. Skeleton data, obtained through localization of 2D/3D coordinates of human body joints or pose estimation algorithms, exhibits simple structural connectivity and temporal continuity. However, existing graph-based models struggle to effectively handle temporal information and capture complex spatio-temporal motion dynamics. In this paper, we propose the use of hypergraphs with hyper-edges of different orders to represent skeleton data for action recognition. Our proposed model, the Multi-order Multi-mode Transformer (3Mformer), uses higher-order hyper-edges to capture various groups of body joints and their higher-order dynamics. We introduce two modules, Multi-order Pooling and Temporal block Pooling, to form coupled-mode tokens and perform weighted hyper-edge and temporal block aggregation. Experimental results demonstrate the superiority of our 3Mformer over existing GCN- and hypergraph-based models on multiple benchmark datasets.