Face parsing is the task of labeling pixels in a face image to identify different facial parts. This segmentation enables various applications such as face editing, e-beautification, face swapping, and face completion. Recent research has focused on using deep convolutional networks for face parsing, often incorporating conditional random fields (CRFs) for improved accuracy. Other approaches have explored the use of graph-based systems to model the relationship between facial components. Pretraining on human face captioning datasets, such as using Vision Transformers, has also shown promising results in face parsing. The current state-of-the-art model incorporates multi-task learning, graph convolutional networks (GCNs), and cyclic learning to achieve high performance. In this work, we propose a novel approach that leverages the consistency in human facial structures by modeling the face in a low-dimensional parametric space. We draw inspiration from implicit neural representation methods and modify existing models to map RGB face images into label space efficiently. Our proposed model achieves state-of-the-art results on face segmentation datasets with significantly fewer parameters and faster processing speeds compared to previous models.