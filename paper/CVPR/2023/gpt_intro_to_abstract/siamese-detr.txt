This paper introduces the concept of multi-view self-supervised learning for object detection with Transformers, specifically focusing on the DETR model. While DETR has shown impressive performance, it heavily relies on large-scale, well-annotated training data that may not be feasible in privacy-sensitive applications. Previous self-supervised learning approaches have mainly focused on generic representation learning, but it remains unclear how they can be effectively extended to task-specific Transformers modules in DETR. This work investigates the use of multi-view self-supervised learning for DETR pre-training and presents a Siamese self-supervised pre-training approach called Siamese DETR. Siamese DETR combines the Siamese network with the cross-attention mechanism in DETR and introduces two novel self-supervised pretext tasks specialized for multi-view detection pre-training. Experimental results show that Siamese DETR outperforms existing single-view pre-training approaches on benchmark datasets, demonstrating the effectiveness and versatility of the proposed designs.