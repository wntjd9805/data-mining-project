Talking face generation has made significant advancements in recent years, with literature categorized into audio-driven and video-driven methods. This work focuses on video-driven talking face generation, specifically in the context of video portrait editing. While previous methods have mainly focused on one-shot talking face generation, which involves driving a still portrait image with a video, only a few have explored the more challenging task of reenacting the portrait in a video with another talking video. One challenge in video portrait editing is disentangling pose and expression to avoid artifacts and inconsistencies. However, there is a lack of paired data for this task, making it difficult to separate the two factors. In this paper, a self-supervised disentanglement framework is proposed to decouple pose and expression without using 3D Morphable Models (3DMMs) or paired data. The framework includes a motion editing module, a pose generator, and an expression generator. The proposed bidirectional cyclic training strategy with well-designed constraints ensures the disentanglement of pose and expression. Experimental results demonstrate that the proposed method can control pose and expression independently and can be applied to general video editing. Overall, this work presents a novel approach to video-driven talking face generation with improved disentanglement capabilities.