The introduction of this computer science paper discusses the importance of end-to-end 3D perception in autonomous driving using multi-view cameras. It highlights the increasing attention towards bird's-eye-view (BEV) perception due to its unified representation for 3D location and scale and its compatibility with downstream tasks like motion planning. The paper categorizes existing BEV approaches into explicit and implicit representation methods based on their construction of a dense BEV feature map. The implicit methods encode global 3D information into 3D position embeddings, while the explicit methods lift 2D perspective-view features to 3D space. The paper then introduces the proposed CAPE (Camera-view position embedding) method, which eliminates the variances of view transformation by encoding ego-motion into the position embeddings. The paper highlights the contributions of CAPE, including its ability to handle different camera extrinsics, its temporal modeling with object queries and ego-motion, and its superior performance compared to other LiDAR-free methods.