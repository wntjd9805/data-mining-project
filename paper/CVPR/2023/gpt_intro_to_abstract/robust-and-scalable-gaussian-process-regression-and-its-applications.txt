Gaussian processes (GPs) are a popular non-parametric method for inferring latent functions and have various applications in fields such as biology and computer vision. However, GPs are sensitive to outliers in data, which limits their use in real-world scenarios. This paper addresses the problem of Gaussian process regression (GPR) with outlier contaminated data. Previous approaches have tried to replace the Gaussian likelihood with heavy-tailed distributions, but these have analytically intractable inference. In this paper, we propose a mixture likelihood model that uses a uniform distribution for outliers and a Gaussian distribution for inliers. We introduce a variational method that determines the modes of data (inlier or outlier) and hyperparameters by maximizing a lower bound to the marginal likelihood, reducing the risk of overfitting. To further improve efficiency, we integrate a sparse inducing variable approximation and use Stochastic Variational Inference (SVI) for stochastic optimization. Our robust GPR model is applied to feature matching and dense spatial gene expression imputation tasks, demonstrating its superior performance compared to existing methods. Our contributions include the formulation of a robust GPR model using variational learning, the integration of inducing variables and SVI for scalability, and the validation of our model through extensive experiments.