Image segmentation is a crucial problem in computer vision, encompassing semantic, instance, and panoptic segmentation. Traditional approaches involve specialized architectures for different tasks, but universal models often underperform in comparison. Vision Transformers have recently achieved significant success in various computer vision tasks, motivating the development of Mask2Former, a unified framework based on DETR's set prediction mechanism. However, Mask2Former still has limitations, lacking the layer-wise refinement proposed in deformable DETR. This leads to inconsistent predictions among layers and a low utilization rate of decoder queries. To address these issues, we draw inspiration from denoising training in object detection and propose a mask-piloted (MP) training approach. In MP training, we divide decoder queries into MP and matching parts, using class embeddings of GT categories and GT masks as attention masks to reconstruct accurate masks and labels. Our contributions include the development of a mask-piloted training approach, techniques for multi-layer training with noises, and label-guided training to improve segmentation performance. Additionally, we introduce novel metrics that measure the failure mode of Mask2Former and demonstrate the effectiveness of our method in alleviating inconsistent predictions. Our MP-Former outperforms Mask2Former on three datasets, achieving improved performance and faster training.