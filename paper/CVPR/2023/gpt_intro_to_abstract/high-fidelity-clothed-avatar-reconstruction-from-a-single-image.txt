Clothed avatar reconstruction plays a significant role in various applications such as video gaming, virtual try-on, and the movie industry. Traditional methods rely on expensive scanning devices or multi-camera studios, which are inflexible and not feasible in many scenarios. An alternative approach is to use depth sensors or RGB cameras to collect data for avatar creation. Monocular RGB reconstruction has been extensively studied, but existing methods often lack geometry details like cloth surfaces. In this paper, we propose an efficient method for creating high-fidelity clothed human avatars from a single image. Our method employs a learning-based canonical implicit model and an optimization-based normal refinement process. Unlike occupancy-based methods, we use a Signed Distance Function (SDF) to approximate the canonical human body surface, resulting in improved reconstruction accuracy. Additionally, we introduce a meta-learning-based hyper network for parameter initialization to accelerate the convergence of the normal refinement process. Extensive experiments on public datasets demonstrate that our method outperforms existing avatar reconstruction methods. Our contributions include a coarse-to-fine framework for efficient clothed avatar reconstruction, a novel canonical implicit regression model, and a pose dependent deformation learning process. Our method achieves high-quality 3D human reconstructions in both posed and canonical space from a single in-the-wild image.