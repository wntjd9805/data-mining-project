Mobile and Internet-of-Things (IoT) devices generate a massive amount of data that can be used to optimize services and improve user experience. Federated learning (FL) has emerged as a machine learning paradigm to address the privacy risks and logistic concerns of centralizing data in a server. However, existing FL algorithms assume similar resources across participating clients, which limits the involvement of clients with limited capacity. In this paper, we propose ScaleFL, a scalable and equitable FL framework that adapts the global model based on clients' computational resources. ScaleFL incorporates the concept of model size reduction to maintain a balance between complex and basic features. Additionally, we introduce self-distillation to enhance model update aggregation, improving knowledge flow among subnetworks. Experimental results demonstrate the advantages of ScaleFL in terms of model performance and inference speedup, making it a promising solution for FL tasks with system heterogeneity.