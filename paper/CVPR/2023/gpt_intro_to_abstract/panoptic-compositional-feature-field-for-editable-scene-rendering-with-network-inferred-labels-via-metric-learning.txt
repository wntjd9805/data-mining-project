Abstract:Virtually editing real-world scenes in mixed reality applications requires an effective 3D scene representation for photo-realistic view rendering and scene decomposition. Neural implicit representations, such as neural radiance field (NeRF), have shown impressive results in tasks like novel view synthesis and scene reconstruction. However, decomposing these representations into objects for scene editing is challenging due to the implicit encoding of the holistic scene. Existing object-compositional methods rely on ground truth instance annotations, which are costly to obtain in real-world practice. This paper proposes a novel panoptic compositional feature field (PCFF) that integrates deep metric learning into the learning of object-compositional representations. PCFF uses 2D network-inferred labels from panoptic segmentation networks instead of ground truth annotations, overcoming the challenge of 3D index inconsistency. The proposed method achieves superior performance in novel view synthesis and scene editing, leveraging network-inferred labels on real-world scenes.