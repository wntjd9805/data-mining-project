Learning representations for video correspondence is a fundamental problem in computer vision, with applications in optical flow estimation, video object segmentation, and key-point tracking. However, supervising such representations requires dense annotations, which are often unaffordable. As a result, most approaches rely on simulations or limited annotations, leading to poor generalization. Self-supervised feature learning has gained momentum as an alternative, with several pretext tasks designed for spatial or temporal feature learning. Spatial feature learning aims to handle appearance changes and deformations by providing video correspondence with discriminative and robust appearance cues. Contrastive models pretrained on image data have shown competitive performance in this regard. However, they struggle to recognize temporal patterns and can be misled by distractors with similar appearance. Temporal feature learning, on the other hand, focuses on learning temporal-repetitive features using a reconstruction task. These features are highly dependent on the consistency of pixels across the video and are easily influenced by temporal discontinuity caused by appearance changes. In this paper, we propose a novel spatial-then-temporal pretext task that combines spatial and temporal cues for video correspondence. We learn spatial features through contrastive learning and improve them by exploiting temporal repetition using frame reconstruction. However, there are challenges in implementing this approach, such as the need for down-sampling video frames and the degradation of previously learned discriminative features when training with only new data. To address these challenges, we propose a local correlation distillation loss to facilitate the learning of temporal features at different pyramid levels of the encoder. We also introduce a global correlation distillation loss to retain the spatial cues learned in the first step. Our main contributions include the proposal of a spatial-then-temporal pretext task, the introduction of local and global correlation distillation losses, and the verification of our approach in correspondence-related tasks. Our approach consistently outperforms previous self-supervised methods and is comparable to fully-supervised algorithms.