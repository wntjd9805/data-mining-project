Visual reasoning, the ability to reason and answer questions about visual scenes, is a challenging problem in artificial intelligence and computer vision. While existing datasets focus on 2D single-view images, they have limitations such as occlusion and inability to answer 3D-related questions. Human visual reasoning is based on underlying 3D representations, unlike traditional approaches that use 3D representations like point clouds. To address this, we propose a new task called 3D visual reasoning from multi-view images taken by an active exploration agent. We introduce a benchmark dataset, 3DMV-VQA, consisting of scenes and question-answering pairs. To efficiently obtain a compact visual representation, we use a neural-field model based on voxel grids. For concept learning, we propose to encode features from a pre-trained 2D vision-language model into the 3D representation. To answer questions, we introduce neural reasoning operators. Experimental results show the superiority of our proposed model, 3D-CLR. However, challenges remain in grounding small objects and separating close object instances. We provide an analysis of these challenges and suggest future directions. Our contributions include introducing the 3D visual reasoning task, creating the 3DMV-VQA benchmark, proposing the 3D-CLR model, and analyzing the challenges and future directions.