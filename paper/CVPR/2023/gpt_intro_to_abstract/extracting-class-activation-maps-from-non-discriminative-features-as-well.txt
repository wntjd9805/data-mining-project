In the field of computer science, extracting Class Activation Maps (CAM) is a crucial step in training semantic segmentation models when only image-level labels are available. The process involves a pipeline consisting of training a classification model, extracting CAM for each class to generate a seed mask, and using pseudo masks as labels to train a semantic segmentation model. However, conventional CAMs often suffer from poor coverage of foreground objects, where many object pixels are mistakenly recognized as background. This issue arises because CAMs are extracted from discriminative models that discard non-discriminative regions, leading to confusion between similar and co-occurring object classes. To address this problem, we propose a new method called Local Prototype CAM (LPCAM) that leverages non-discriminative local features and context features. LPCAM generates class activation maps with improved coverage on the complete object by clustering local features into prototypes and applying them to the feature map block. We evaluate LPCAM in various WSSS methods on popular semantic segmentation benchmarks and demonstrate its effectiveness in improving object coverage. Our contributions in this paper include the introduction of LPCAM and extensive evaluations of its performance in multiple WSSS methods.