Neural networks are commonly trained using a predefined loss function to capture complex behaviors from data. In image restoration tasks, selecting an appropriate loss function is necessary to constrain neural networks to a specific behavior. However, a fixed loss function is incapable of generating optimal results for all possible inputs, especially when balancing competing objectives. Existing approaches for controlling neural networks are limited to considering only two objectives and require the addition of new layers or parameters for each additional loss considered. Other approaches train the network conditioned on the true degradation parameter of the image and propose interacting with these parameters at inference time. However, this approach fails when operating in regimes unseen during training. In this paper, we propose a novel framework for reliably and consistently tuning model behavior at inference time. We introduce a parametric dynamic layer called tunable convolution, which consists of multiple kernels and biases that can be optimized using a parametric dynamic multi-loss with individual objectives. We establish an explicit link between the kernels and objectives using a shared set of parameters, allowing each objective to be disentangled into a different kernel. Our approach is capable of handling an arbitrary number of objectives and allows for predictable and intuitive tuning of the network behavior. Additionally, our tunable layer can be seamlessly integrated into existing neural networks with minimal computational cost. We validate our approach through extensive experiments in various image-to-image translation tasks, demonstrating its state-of-the-art performance in tunable inference.