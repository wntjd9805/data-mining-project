Deep neural networks have become increasingly prevalent in various fields, but their decision-making processes often lack transparency. Many explainability methods have been proposed to shed light on these decisions, but they are prone to biases and may produce incorrect explanations. Biased benchmarks and fidelity metrics further contribute to the inaccurate assessment of model behavior. Adversarial methods have been used to derive explanations that reflect the robustness of the model to perturbations, but they are computationally expensive. To address these issues, we present EVA (Explaining using Verified perturbation Analysis), a novel explainability method based on robustness analysis. EVA utilizes verified perturbation analysis to explore the perturbation space comprehensively and quantify the importance of input variables. Our contributions include the introduction of EVA, a scalable method for large vision models, extensive evaluation on multiple image datasets, and the generation of convincing class-specific explanations. We also study the effects of various verified perturbation analysis methods as hyperparameters in the generated explanations.