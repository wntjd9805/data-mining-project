This paper introduces the topic of image compression in the field of image processing. It highlights the importance of lossy image compression for efficient storage and transmission of rapidly increasing image data. The paper discusses various classical standards such as JPEG, WebP, and VVC, which have achieved impressive Rate-Distortion (RD) performance through the three-step process of transform, quantization, and entropy coding. However, recent advancements in end-to-end learned image compression (LIC) methods have outperformed classical standards, suggesting great potential for next-generation image compression techniques.The paper focuses on CNN-based and transformer-based LIC methods. CNN-based methods utilize the variational auto-encoder (VAE), while transformer-based methods leverage vision transformers. The advantages of CNN and transformers, such as local modeling and non-local information modeling respectively, are explored. The paper proposes an efficient parallel Transformer-CNN Mixture (TCM) block that combines the strengths of both CNN and transformers to improve RD performance.In addition to neural network type, the design of entropy models is also crucial in LIC. The paper discusses the use of hyper-priors and latent variables to convert the probability model of compact coding-symbols to a joint model. Various techniques such as masked convolutional layers and parallel channel-wise auto-regressive entropy models are mentioned.Furthermore, attention modules are introduced as a means to improve image compression by focusing on complex regions. However, many existing attention modules are time-consuming or only capture local information. To address this, the paper proposes a parameter-efficient swin-transformer-based attention module (SWAtten) with channel squeezing for the channel-wise entropy model. Additionally, the number of slices is reduced to balance running speed and RD performance.The contributions of the paper are summarized as follows: 1) the proposal of a LIC framework with parallel TCM blocks that incorporate the advantages of CNN and transformers while maintaining controllable complexity, 2) the design of a channel-wise auto-regressive entropy model using SWAtten module with channel squeezing, and 3) extensive experiments demonstrating state-of-the-art performance on three datasets, outperforming VVC in BD-rate.In conclusion, this paper presents a comprehensive overview of the current state of image compression techniques, highlighting recent advancements in LIC methods. The proposed TCM blocks and SWAtten module contribute to improving RD performance and show promise for future image compression techniques.