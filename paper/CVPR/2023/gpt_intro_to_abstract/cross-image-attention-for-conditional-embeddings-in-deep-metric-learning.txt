Deep metric learning (DML) aims to generate embeddings that not only capture semantic similarities among training samples but also transfer well to unseen classes. The ability to learn compact image representations that generalize and transfer in a zero-shot manner is crucial for various visual tasks. DML research has explored important aspects such as training sample mining, loss functions, and ensemble strategies. However, learning powerful embeddings is a challenging problem as it involves projecting a rich local feature encoding with comprehensive spatial information onto a compact vector that represents the entire image. This requires aggregating local details and capturing spatial relationships. Image similarity is also multi-modal, making it difficult to determine which local details to marginalize and which to preserve when only considering a single image. To address this, we propose conditioning the embedding on another image during training to focus attention on meaningful characteristics for subsequent comparisons. We utilize cross-attention blocks to project image feature encodings onto an embedding vector while conditioning on another image. This hierarchical approach divides the task of learning an embedding into smaller steps. Backpropagation from the similarity measure allows for direct updates to the image encoding and embeddings, improving the overall performance compared to classical DML training. During inference, we employ unconditional representations, resulting in a standard DML approach with no additional parameters or computational costs. Experimental evaluation demonstrates significant improvements in both conditional and unconditional embeddings compared to existing DML methods, with minimal computational overhead during training.