With the increasing accessibility of high-resolution images, the computational complexity of neural network models has also grown, making them less deployable for resource-constrained applications. Down-sampling the images to a lower resolution is a simple solution but results in a loss of fine details and impacts the model's performance, especially for small object detection tasks. To address this challenge, the concept of activation pruning is introduced, aiming to skip computations for less important image regions. However, translating activation sparsity into speedup on general-purpose hardware is not straightforward due to the introduction of unbalanced zeros and computing unit under-utilization. This paper revisits activation sparsity in the context of window-based 2D vision transformers (ViTs), specifically focusing on Swin Transformer. By executing FFNs (Feed-Forward Networks) and LNs (Layer Norms) at the window level, real speedup is achieved with window-level activation pruning. The researchers propose a non-uniform layerwise sparsity configuration using evolutionary search, where layers with larger computation and lower sensitivity are pruned more. They also introduce sparsity-aware adaptation to avoid expensive re-training and achieve optimal pruning ratios. The proposed Sparse-ViT achieves significant speedups in various visual perception tasks without sacrificing accuracy.