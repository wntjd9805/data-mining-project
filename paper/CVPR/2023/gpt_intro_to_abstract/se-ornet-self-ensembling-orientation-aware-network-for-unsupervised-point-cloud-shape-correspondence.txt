Abstract:LiDAR and depth cameras have become more affordable, making it easier to obtain 3D point cloud data. However, finding correspondence between two raw point clouds is challenging due to varying object orientations and ununified coordinate systems. Spectral-based methods and fully supervised deep learning methods have been proposed for shape correspondence, but they have limitations such as the need for connectivity information and expensive annotation. This paper presents Self-Ensembling Orientation-aware Network (SE-ORNet), an unsupervised deep learning framework for dense correspondence between point clouds. SE-ORNet integrates orientation modeling, consistent point cloud representations, and an adaptive domain discriminator. It also includes a lightweight Orientation Estimation Module that addresses the mismatching issue of symmetrical parts. Experimental results on human and animal benchmarks show that SE-ORNet achieves state-of-the-art performance and outperforms existing methods.