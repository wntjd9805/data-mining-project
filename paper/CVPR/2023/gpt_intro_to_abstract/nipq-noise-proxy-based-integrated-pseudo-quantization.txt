Neural network quantization is a popular technique for reducing memory usage and improving performance in network inference. However, quantization can lead to a degradation in network quality. To mitigate this issue, quantization-aware training (QAT) has been proposed, which trains networks with quantization operators. One common approach used in QAT is the straight-through estimator (STE), which approximates the gradient of the quantized data. However, STE-based QAT can suffer from instability and bias during training. Pseudo-quantization training (PQT) has been proposed as an alternative to address the instability of STE-based QAT, but existing PQT algorithms have limitations. In this paper, we present a novel PQT method called Noise proxy-based Integrated Pseudo-Quantization (NIPQ) that quantizes both activation and weight based on pseudo-quantization noise (PQN). NIPQ integrates truncation into the PQT framework, reducing quantization error and enabling activation quantization. We provide theoretical analysis and extensive experimental results to validate the effectiveness of NIPQ. Our method outperforms existing mixed-precision quantization schemes and achieves stable convergence without additional complexity or cost. Additionally, NIPQ improves the robustness of activation quantization and provides theoretical support for the convergence of PQT. Our work demonstrates the superiority of PQT-based pipelines over STE-based approaches.