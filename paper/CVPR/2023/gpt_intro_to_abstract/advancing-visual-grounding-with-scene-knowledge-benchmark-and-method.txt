Visual grounding (VG) has become a popular research focus in computer science, with applications in visual question answering and vision-and-language navigation. However, current VG benchmarks do not adequately evaluate the reasoning ability of models, as they only focus on simple vision-language alignment. This paper addresses this limitation by proposing a new benchmark, named SK-VG, which requires VG models to reason over scene knowledge. The benchmark consists of referring expressions and scene stories from real images, and includes easy, medium, and hard categories to provide a detailed evaluation. The authors propose two approaches, KeViLI and LeViLM, to enhance reasoning in SK-VG. Experimental results show that the proposed approaches perform well, but there is still room for improvement, particularly in the hard split. The contributions of this paper are the introduction of the challenging SK-VG task, the development of two approaches to enhance reasoning, and the demonstration of their effectiveness. This work can serve as a starting point for future research in the vision-and-language field.