3D sparse convolutional neural networks (CNNs) have been widely used in various 3D tasks, such as semantic segmentation and object detection. However, these networks have been recently challenged by transformer-based methods, especially in terms of building effective receptive fields. While global and local self-attention mechanisms in transformer models can capture context information from a large spatial scope, traditional 3D sparse CNNs are limited in this regard due to small kernel sizes and sparse features. Previous methods used in 2D CNNs to enlarge receptive fields, such as large kernel sizes, are not effective for 3D sparse CNNs. To address this issue, we propose a new approach called spatial-wise partition convolution, which is a form of group convolution that shares weights among spatially adjacent locations. This allows for larger kernel sizes while maintaining efficiency and reducing latency. We also introduce position embeddings to enhance the detail-capturing ability of large kernels. Our proposed spatial-wise large-kernel convolution (SW-LK Conv) outperforms plain convolution layers in existing 3D convolutional networks and achieves notable improvements in 3D semantic segmentation and object detection tasks. In object detection, our LargeKernel3D network achieves top performance on the nuScenes LIDAR leaderboard and scalability to large-scale Waymo 3D object detection. We visualize the effective receptive fields of plain 3D CNNs and our LargeKernel3D network, demonstrating that our approach effectively resolves the issue of limited receptive fields in traditional networks. Overall, our work demonstrates the effectiveness of large-kernel CNN designs in essential 3D visual tasks.