This paper introduces TranS4mer, an efficient end-to-end model for movie scene boundary detection. Unlike existing methods that use convolutional neural networks (CNNs) for short-range analysis, TranS4mer combines transformers for short-range modeling and structured state-space sequence (S4) operators for long-range modeling. The proposed model applies self-attention within each shot to reduce computational cost and utilizes the state-space operator to capture inter-shot interactions. Experimental results on three movie scene detection datasets show that TranS4mer outperforms prior approaches in terms of average precision (AP) and achieves faster computation with less GPU memory usage compared to pure Transformer-based models. Additionally, TranS4mer demonstrates strong performance in other long-range video understanding tasks such as movie clip classification and procedural activity recognition. Overall, the TranS4mer model provides an efficient and effective solution for movie scene detection and related video understanding tasks.