The task of image captioning, which involves generating natural language sentences to describe visual contents, has gained significant attention in recent years. Various methodological and architectural advancements have been made in this field, such as the use of self-attentive models and the incorporation of objects and tags. In addition to improving caption generation quality, there has also been a focus on developing effective evaluation metrics. Initially, translation metrics were used, but more recently, text-based and multimodal solutions have been proposed. The usage of cross-modal models, which can match visual and textual data, has been particularly successful in producing high-quality metrics. For example, the CLIP-Score, based on the CLIP model, has shown a strong correlation with human judgment. However, there are limitations to using large-scale models pre-trained on web-collected data, such as the lack of caption style and alignment issues with evaluation datasets. To address these limitations, this paper introduces a learnable metric called PAC-S, which combines the strengths of pre-training on web-collected data and cleaned data, while also incorporating additional positive samples from visual and textual generators. The proposed metric is trained using a positive-augmented contrastive learning approach and demonstrates improved alignment with human judgment compared to existing metrics. The metric is applied to evaluate both images and videos, in reference-based and reference-free settings, and is tested on various datasets. The results show that the proposed metric outperforms previous metrics in terms of correlation with human judgment, sensitivity to object hallucination, and performance compared to state-of-the-art caption generators. Overall, this paper presents a novel metric for image and video captioning, which leverages a positive-augmented training approach in a multimodal embedding space, resulting in superior performance and increased sensitivity to visual content.