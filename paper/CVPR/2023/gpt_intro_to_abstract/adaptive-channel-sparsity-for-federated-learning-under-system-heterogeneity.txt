This paper introduces Flado, a method that optimizes channel activation probabilities to sparsify client models with trajectory alignment towards the global trajectory in federated learning (FL). The authors highlight the limitations of existing fixed sparsity strategies in FL algorithms and propose an adaptive sparsity strategy that takes into account the heterogeneity of client data and computational capabilities. The advantages of Flado include the ease of implementation and leveraging existing models and hardware devices, as well as the low overhead for clients. Experimental results demonstrate that Flado significantly reduces the required floating-point operations for training while achieving improved communication/computation Pareto frontiers compared to competing FL approaches. Flado also shows superior convergence rates in the presence of high degrees of heterogeneity in data distributions and system capabilities, and is scalable to larger models and fractional client participation.