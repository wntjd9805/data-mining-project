Semantic segmentation is a fundamental computer vision task that aims to assign pixel-wise semantic labels to images. Previous deep learning methods for semantic segmentation have achieved outstanding results, but they require large amounts of accurately annotated data, which is costly and time-consuming. To address this issue, sparse annotations have been proposed, where only a few points or scribbles are labeled. However, sparse annotations lack sufficient supervision for accurate segmentation. Existing sparse annotation methods can be categorized into low-level regularization, pseudo supervision, and consistency learning, but each has its limitations in providing reliable supervision. In this paper, we propose a novel Adaptive Gaussian Mixture Model (AGMM) framework for sparse annotation semantic segmentation. The AGMM framework incorporates a Gaussian Mixture Model (GMM) branch into the segmentation branch to model the similarity between labeled and unlabeled pixels. The labeled pixels are used as centroids of Gaussian mixtures to represent the distribution of each class. By dynamically adapting the parameters of the GMM to input features, the AGMM framework achieves online self-supervision. The AGMM framework has several advantages. First, it can learn discriminative decision boundaries between different classes with limited supervision. Second, it enables end-to-end contrastive representation learning by pushing unlabeled pixels towards specific category-wise Gaussian mixtures. Finally, it leverages the reliable information from labeled pixels to generate GMM predictions for the unlabeled pixels, increasing the reliability of supervision. Experiments conducted on the PASCAL VOC 2012 and Cityscapes datasets demonstrate that our AGMM framework outperforms existing state-of-the-art sparse annotation semantic segmentation methods. Importantly, our AGMM framework does not require additional information for supervision, multi-stage training, or time-consuming post-processing.