Pre-training effective and general representations is crucial for the success of deep learning in computer vision. Currently, supervised classification on ImageNet has been the dominant pre-training task, demonstrating effectiveness across various vision tasks. However, recent advances in self-supervised pre-training, particularly masked image modeling (MIM), have shown promising results in computer vision tasks. This paper aims to investigate the key mechanisms and transferability of MIM models compared to supervised models.The authors begin by studying the attention maps of the pre-trained models. They find that MIM models exhibit a locality inductive bias, with attention heads aggregating near pixels based on the masking ratio and patch size. In contrast, supervised models focus locally at lower layers but more globally at higher layers. Moreover, the attention heads in MIM trained Transformer show greater diversity in aggregating different tokens across layers compared to supervised models, which lose diversity as layers go deeper. Fine-tuning experiments reveal that removing the last several layers benefits the supervised models but not the MIM models.Next, the authors examine the representation structures in MIM and supervised models using the Centered Kernel Alignment (CKA) similarity metric. Surprisingly, they find that MIM models exhibit high similarity in feature representations across different layers, while supervised models show varying representation structures across layers. Furthermore, experiments demonstrate that supervised models suffer more when pre-trained weights are randomly shuffled during fine-tuning.From an experimental perspective, the authors compare the fine-tuning performance of MIM and supervised models on three types of tasks: semantic understanding, geometric and motion, and combined tasks. For semantic understanding tasks, MIM models outperform supervised models on fine-grained classification datasets and datasets with different output categories. In geometric and motion tasks, MIM models show significant improvements in performance, achieving state-of-the-art results without additional techniques. For combined tasks, MIM models outperform supervised counterparts in object detection on COCO.Overall, MIM models exhibit improved performance on geometric/motion tasks with weak semantics or fine-grained classification tasks compared to supervised models. They also demonstrate competitive transfer performance in tasks where supervised models excel. This study highlights the potential of masked image modeling as a general-purpose pre-training approach in computer vision. The authors hope that their findings contribute to the understanding of pre-training models and inspire further research in this direction.