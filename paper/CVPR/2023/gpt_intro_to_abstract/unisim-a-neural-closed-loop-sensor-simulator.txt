This paper introduces UniSim, a closed-loop data-driven sensor simulation system for self-driving cars. The goal is to create a realistic and editable digital twin of the real world, allowing for modifications to existing actors in a scene, addition of new actors, and execution of new autonomy trajectories. UniSim reconstructs and renders multi-sensor data for novel views and scene configurations using a single recorded log. To handle the challenge of sparse and constrained observations, the authors propose enhancements to prior neural rendering approaches, including the use of multi-resolution voxel-based neural fields and convolutional networks for rendering feature maps. The experiments demonstrate that UniSim achieves state-of-the-art photorealism in simulating camera and LiDAR observations, reduces the domain gap in downstream autonomy tasks, and allows for closed-loop evaluation of autonomy systems in photorealistic safety-critical scenarios. Overall, UniSim enables safer and more efficient development of self-driving technology.