Large-scale pre-trained models (PTMs) have been highly successful in various domains and have received significant attention in the machine learning community. Parameter-Efficient Transfer Learning (PETL) methods have emerged to efficiently utilize PTMs by optimizing a small number of learnable parameters. Prompt-based approaches, such as Visual Prompt Tuning and visual prompting (VP), have been extensively studied for vision PTMs. However, existing PETL methods make unrealistic assumptions, assuming full parameter accessibility and requiring large memory capacity for backpropagation. In this paper, we propose a black-box visual prompting (BlackVIP) approach that enables parameter-efficient transfer learning of pre-trained black-box vision models. BlackVIP consists of two core components: pixel space input-dependent visual prompting and a stable zeroth-order optimization algorithm. We introduce a pixel-level prompt that covers the entire image view, reparameterize the prompt with a neural network, and optimize the reparameterized model instead of the prompt itself to reduce the number of parameters. Additionally, we adopt a zeroth-order optimization algorithm, SPSA-GC, which estimates the gradient of the target black-box model and corrects the initial estimates in a momentum-based look-ahead manner. Through extensive experiments on 16 datasets, we demonstrate the effectiveness of BlackVIP in adapting PTMs to downstream tasks without parameter access and with limited memory capacity. Our contributions include exploring input-dependent visual prompting in black-box settings, proposing the Coordinator for prompt reparameterization, introducing the SPSA-GC algorithm for zeroth-order optimization, and validating the effectiveness of BlackVIP on various tasks.