The problem of generating lip-synced videos based on conditional audio has important implications for digital human creation, audio dubbing, film-making, and entertainment. However, most existing methods focus on generating a whole dynamic talking head, which cannot seamlessly blend into an existing scene. This limitation makes these methods non-feasible for real-world scenarios such as audio dubbing. Previous approaches to seamless mouth modification take two different paths. Some studies pursue realistic results on person-specific settings, while others aim to build models that are not limited to specific individuals. However, both approaches come with their own challenges and limitations.To address these challenges and produce high-fidelity lip-synced results on any-length videos, this paper proposes a framework called StyleSync. This framework introduces modifications to a style-based generator, specifically designed for lip-syncing. The framework adopts a masked mouth modeling protocol and incorporates a Mask-based Spatial Information Encoding strategy, which encodes information from both the target and reference frames into a noise space.Furthermore, the framework enables personalized information preservation, such as speaking styles and mouth/jaw details. This is achieved through a Personalized Optimization scheme, which optimizes person-specific parameters based on a few seconds of the person's information.Extensive experiments demonstrate that the StyleSync framework outperforms previous state-of-the-art methods in terms of generating accurate and high-fidelity lip-synced results. The personalized optimization further enhances the generation quality. Overall, the contributions of this paper include the introduction of the StyleSync framework, the proposal of the Personalized Optimization procedure, and experimental evidence showcasing the superiority of the framework over previous methods.