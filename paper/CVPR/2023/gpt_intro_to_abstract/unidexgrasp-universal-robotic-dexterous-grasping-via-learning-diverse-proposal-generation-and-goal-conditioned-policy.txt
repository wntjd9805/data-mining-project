Robotic grasping is a fundamental capability for agents to interact with the environment and is necessary for object manipulation. While there have been advancements in grasping algorithms for parallel grippers, these grippers have limited dexterity, restricting their use to complex object manipulation. Dexterous grasping, on the other hand, offers a more diverse way to grasp objects, allowing for functional and fine-grained object manipulation. However, the high dimensionality of dexterous hands makes generating valid grasp poses and planning execution trajectories challenging. Previous works have tackled the grasping pose synthesis problem but assume ideal inputs. In this paper, we address the challenging task of learning universal dexterous grasping skills that can generalize to seen and unseen objects using depth observations and robot proprioception as input. We introduce a large-scale dataset containing over one million grasps for various object categories to evaluate universal dexterous grasping. Inspired by parallel grippers, we propose a two-stage approach, involving dexterous grasp proposal generation and goal-conditioned grasp execution. Our grasp proposal generation model leverages conditional generative models, including conditional normalizing flows and conditional diffusion models, to generate diverse grasp poses. To execute grasps, we learn a goal-conditioned policy using a teacher-student learning framework that distills an oracle teacher model's knowledge. We introduce innovations such as canonicalization and object curriculum to improve policy learning. Our experiments demonstrate the effectiveness of our approach, achieving high diversity and grasping quality in the proposal generation stage and outperforming baselines in the complete dexterous grasping pipeline with over 60% success rate. We will release our dataset and code to facilitate future research in this area.