Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, prompting the study of various protection strategies. Adversarial training and certified methods such as IBP and CROWN have been proposed to defend against attacks. However, these strategies only activate defense mechanisms after the attack has been crafted, giving an advantage to the attacker. In this paper, we propose a novel approach called Adversarial Augmentation Against Adversarial Attacks (A5) that preemptively certifies data and physical objects as robust against Man in the Middle (MitM) attacks. A5 achieves certified defense, guaranteeing protection against white, grey, and black box attacks. We evaluate different flavors of A5 on standard datasets and demonstrate its state-of-the-art certified protection against MitM attacks. Additionally, we apply A5 to the design of certifiably robust physical objects, using Optical Character Recognition (OCR) as an example. We provide visual inspection of robustified images to gain insights into the characteristics of non-attackable images. Our code is made available to replicate results and test A5 in different scenarios.