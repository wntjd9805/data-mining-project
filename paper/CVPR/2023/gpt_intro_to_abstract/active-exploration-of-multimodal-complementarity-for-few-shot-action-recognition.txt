Action recognition has made significant progress in recent years with the emergence of deep learning. However, existing deep methods for action recognition require a large amount of labeled videos, which can be expensive or even impossible to collect. This limitation hampers the effectiveness of supervised methods. To address this issue, researchers have begun to focus on few-shot action recognition (FSAR), which aims to classify unlabeled videos from novel action classes with only a few annotated samples. Existing FSAR methods can be categorized into data augmentation-based methods and alignment-based methods. While these methods have achieved remarkable performance, they mainly rely on limited unimodal data, which fails to capture the complex characteristics of human actions. In conventional action recognition, multiple modalities such as vision, optical flow, and audio are used to provide complementary information. However, the exploration of multimodal information in few-shot action recognition is still limited. In this paper, we propose an Active Multimodal Few-shot Action Recognition (AMFAR) framework inspired by active learning. This framework actively finds the most reliable modality for each query sample to improve few-shot reasoning. We use an Active Sample Selection (ASS) method to organize query samples based on the reliability of the modalities. We also introduce an Active Mutual Distillation (AMD) mechanism to transfer task-specific knowledge between modalities to enhance representation learning. In the meta-test phase, we adopt Adaptive Multimodal Inference (AMI) to fuse modality-specific results and prioritize the reliable modality. Experimental results on four challenging datasets show that our proposed method outperforms existing unimodal and multimodal methods.