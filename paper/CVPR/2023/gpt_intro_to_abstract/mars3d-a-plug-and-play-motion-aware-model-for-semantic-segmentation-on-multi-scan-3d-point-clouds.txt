3D semantic segmentation on multi-scan large-scale point clouds is a crucial task in computer vision, benefiting various autonomous systems applications. Unlike single-scan semantic segmentation, this task requires understanding both semantic categories and motion states of points based on multi-scan data. Previous research has focused on single-scan segmentation, but approaches for multi-scan point clouds often struggle to distinguish motion states accurately. This paper addresses the challenge of leveraging spatial-temporal information from multiple point cloud scans for better representation learning and motion state classification. The proposed Motion-aware Segmentation module for 3D multi-scan analysis (MarS3D) integrates seamlessly with existing single-scan semantic segmentation models. It introduces a Cross-Frame Feature Embedding module to enrich representation learning and a Motion-Aware Feature Learning module based on Bird's Eye View to effectively discriminate motion states. Extensive evaluations on SemanticKITTI and nuScenes datasets show that MarS3D consistently improves baseline approaches' performance with minimal increase in computational cost. The contributions of this work include the introduction of a plug-and-play module for large-scale multi-scan 3D semantic segmentation, the design of a Cross-Frame Feature Embedding module, and the implementation of a BEV-based Motion-Aware Feature Learning module. Experimental results demonstrate the superiority of MarS3D compared to baseline methods in terms of accuracy and efficiency.