In recent years, significant progress has been made in visual recognition using the vision transformer architecture as a network backbone and masked image modeling (MIM) for visual pre-training. Combining these techniques has resulted in a generalized pipeline that achieves state-of-the-art performance in various visual recognition tasks. However, a key issue in this pipeline is the transfer gap between upstream pre-training and downstream fine-tuning. This paper addresses this issue by proposing an integral pre-training framework that incorporates a feature pyramid into the pre-training stage and fine-tuning stage. Additionally, a new masked feature modeling (MFM) task is introduced to better pre-train the feature pyramid. The proposed method, named integrally pre-trained pyramid transformer networks (iTPNs), demonstrates improved accuracy in both reconstruction and recognition tasks. Experimental results show that iTPN outperforms existing methods in terms of downstream recognition accuracy on standard benchmarks. Furthermore, diagnostic experiments confirm the benefits of the proposed framework in terms of lower reconstruction error and faster convergence speed. The key contribution of this paper is the integral pre-training framework, which not only sets new state-of-the-art performance but also highlights the importance of unifying upstream pre-training and downstream fine-tuning to minimize the transfer gap.