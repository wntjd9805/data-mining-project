Neural fields have been increasingly used for 3D scene representation, demonstrating high quality due to their ability to parameterize scenes using compact neural networks. These neural fields have been successful in optimization tasks in 3D vision, including image-based and point cloud-based 3D reconstruction. One prominent method, NeRF, combines physically based light transport with neural fields to model the interaction between the scene and the camera. However, NeRF does not model the rays from the scene to the light source. In this paper, we propose a novel approach that optimizes a neural field by supervising these shadow rays, which are the rays from the scene to the light source. By recording the incoming radiance at the scene surface, we can infer the scene geometry and reconstruct a complete 3D shape. We address several challenges in supervising shadow rays using camera inputs, including determining ray positions and handling surface boundaries. We also propose a method to decompose the incoming radiance from RGB inputs to achieve shape reconstruction without shadow segmentation. We compare our approach with previous single-view reconstruction methods and demonstrate significant improvements in shape reconstruction. We also highlight the relationship between our method and NeRF, providing readers with a deeper understanding of neural scene representation. Our contributions include a framework for reconstructing neural scenes using light visibility, a shadow ray supervision scheme, and comparisons with previous works using either RGB or binary shadow inputs for scene reconstruction.