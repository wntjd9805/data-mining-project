Machine learning algorithms often assume that training and test data are independent and identically distributed (I.I.D.). However, this assumption is not always satisfied in real-world applications, leading to unreliable performance. Domain generalization (DG) focuses on the ability of models to generalize under distribution shift. Current DG benchmarks lack diversity and fail to simulate significant distribution shifts. To address these limitations, we propose a large-scale DG dataset called NICO++, which provides diverse domains and supports evaluation on multiple test domains. We also introduce novel metrics to quantify distribution shifts and analyze generalization bounds based on data construction. Furthermore, we investigate the problem of model selection and potential unfairness in comparisons caused by leveraging target data. Through extensive experiments, we demonstrate the superiority of NICO++ in evaluation capability and show that it helps alleviate the issue of unfair comparisons. NICO++ provides a comprehensive and fair benchmark for evaluating DG methods.