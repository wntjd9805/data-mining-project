This paper introduces a novel approach to person re-identification (Re-ID) by exploiting event streams captured by event information and guide cameras. The use of video data in Re-ID has gained attention due to its detailed spatial and temporal information. However, existing video-based Re-ID methods are limited in addressing motion blur, illumination variations, and occlusions. In this work, we propose a sparse-dense complementary learning network (SDCL) that combines features from dense video sequences and sparse event streams. We utilize a CNN-based backbone for frame-level feature aggregation and a deformable spiking neural network for processing event streams. We introduce a cross-feature alignment module to enhance representation capacity by leveraging both clear movement information from events and appearance cues from frames. Our experiments demonstrate the effectiveness of our proposed method in improving Re-ID performance compared to state-of-the-art methods. Contributions of this work include the introduction of event streams as a new modality for person Re-ID and the design of a deformable spiking neural network specifically tailored for event processing.