Traditional imaging methods focus on recovering information in line-of-sight scenarios, where there are no obstacles between the target and the camera. However, non-line-of-sight (NLOS) imaging aims to recover the hidden scene beyond the direct line of sight by using a diffuse relay surface to scatter the light from the scene. NLOS imaging has had significant applications in autonomous driving, disaster rescue, and medical diagnosis. The time-of-flight (ToF) based imaging scheme, which uses a laser source and a time-resolved single-photon avalanche diode detector, has been commonly used in NLOS imaging. Existing NLOS reconstruction algorithms have made progress but still face challenges such as texture loss, noise, and limited depth range. Recently, deep-learning-based methods have shown promising results in NLOS reconstruction but have limitations in handling complex scenes and generalization to different systems. Inspired by the success of transformers in computer vision tasks, we propose a transformer-based method for NLOS reconstruction called NLOST. Our method utilizes the powerful representation capabilities of transformers to capture local and global spatial-temporal correlations in 3D NLOS measurements. The proposed method consists of a feature extractor, two spatial-temporal self-attention encoders, and a spatial-temporal cross-attention decoder. We evaluate our method on synthetic and real-world datasets, including a self-built NLOS imaging system. Our method outperforms existing solutions in terms of reconstruction performance and generalization capabilities. Additionally, we provide a dataset of NLOS transient measurements captured with our system for future research in this field.