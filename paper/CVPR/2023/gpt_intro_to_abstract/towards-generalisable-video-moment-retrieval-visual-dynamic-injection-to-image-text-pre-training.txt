Video moment retrieval (VMR) is a critical task in computer science that aims to locate a specific moment in a video based on a natural language sentence. However, current methods lack the ability to correlate moments with text accurately. To address this challenge, we propose a method called Visual-Dynamic Injection (VDI) that leverages image-text pre-trained models to learn moment-text correlations. Our method incorporates visual and dynamic information from videos into text embeddings, emphasizing phrases that describe video changes. By doing so, our method improves the generalizability of VMR models to different scenes and vocabulary. Our contributions include being the first attempt to inject visual and dynamic information into image-text pre-training models for VMR, proposing a generic formulation called VDI that can be integrated into existing VMR models, and achieving state-of-the-art performances on standard VMR benchmark datasets. Our method demonstrates superior generalization abilities and has the potential to enhance image-text pre-training for fine-grained visual-textual comprehensions in video understanding tasks.