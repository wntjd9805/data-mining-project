The problem of recovering a human mesh from a single image is a challenging task in computer vision with various applications. Previous approaches have used convolutional neural networks and parametric models, but they are limited by memory cost and the use of coarse feature maps. This paper proposes a novel vertex-based transformer approach, called DeFormer, for 3D human mesh recovery. DeFormer leverages multi-scale feature maps and a dense mesh model using block sparse self-attention and multi-scale deformable cross attention. This allows for better 3D reconstruction and achieves state-of-the-art results on benchmark datasets. The contributions of this paper include the DeFormer architecture, body sparse self-attention, and deformable mesh cross attention.