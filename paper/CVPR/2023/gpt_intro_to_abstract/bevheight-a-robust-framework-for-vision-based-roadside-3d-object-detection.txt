The rise of autonomous driving vehicles has led to increased research on 3D perception tasks, particularly 3D object detection. While current approaches primarily rely on ego-vehicle sensors, there are limitations to this approach, such as occlusion of obstacles and restricted perception range. To address these challenges, researchers have begun exploring the use of intelligent roadside units, such as cameras, to improve perception capability. Two large-scale benchmark datasets have been created to facilitate research in this area. Recent studies have shown that using a bird's eye view (BEV) feature space can significantly enhance the performance of vision-centric systems compared to directly projecting 2D images into 3D space. However, existing camera-only methods that generate depth for each pixel encounter difficulties in bounding box regression optimization. This is because the depth measurements vary significantly with the change in distance between the car and the camera, as observed in roadside images. Additionally, the change in extrinsic parameters further affects the accuracy of depth prediction.In contrast, the height to the ground remains consistent regardless of the car-camera distance. Based on this observation, we propose a novel framework, called BEVHeight, that predicts per-pixel height instead of depth. Our method uses categorical height distribution to project contextual features to the appropriate height interval in a voxel space. A voxel pooling operation and a detection head are then applied to obtain final output detections. Additionally, we introduce a hyperparameter-adjustable height sampling strategy. Notably, our framework does not rely on explicit supervision like point clouds.We evaluate our approach on two popular roadside perception benchmarks and demonstrate superior performance compared to previous methods, achieving a 5% improvement. In realistic scenarios, where extrinsic parameters of roadside units may change, our method outperforms existing approaches by 26.88% when predicting height instead of depth. These findings highlight the robustness and effectiveness of our proposed BEVHeight framework.