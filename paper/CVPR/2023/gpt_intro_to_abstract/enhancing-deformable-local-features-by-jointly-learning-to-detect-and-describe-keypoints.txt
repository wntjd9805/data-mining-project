Finding pixel-wise correspondences between images depicting the same surface is a well-known challenge in computer vision. Existing correspondence algorithms typically assume a rigid world, ignoring the fact that many objects in the real world can deform in more complex ways than affine transformations. This limitation hinders the accurate tracking, retrieval, and monitoring of arbitrary deformable objects and surfaces, which are often required in various applications. While previous works have proposed deformation-aware methods, they mostly neglect the keypoint detection phase, limiting their applicability in challenging deformations. In this paper, we propose a new method called DALF (Deformation-Aware Local Features) that jointly learns to detect keypoints and extract descriptors, robust to deformations, viewpoint changes, and illumination variations. Our method significantly outperforms the state-of-the-art in feature matching for non-rigid deformations by 8%, demonstrating strong generalization capabilities. We leverage reinforcement learning combined with spatial transformers to capture deformations, and introduce a feature fusion approach that combines distinctive and invariant features from both the backbone and spatial transformer module. We also demonstrate state-of-the-art results in non-rigid local feature applications for deformable object retrieval and non-rigid 3D surface registration. Code and applications will be made publicly available.