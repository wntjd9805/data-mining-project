Neural network quantization has become a popular compression technique for hardware-friendly implementations. While quantizing linear and convolutional layer operands is common, modern algorithms aim for lossless quantization into fixed 8-bit integer values. At higher compression rates, mixed-precision is needed, with different layers requiring different precision levels. The selected precision may depend on the quantized operation or hardware at hand. However, finding the optimal precision for each matrix multiplication factor is challenging due to the exponentially scaling search space. Additionally, existing methods require multiple restarts of the searching process, leading to significant searching time. In this paper, we propose a novel One-Shot Mixed-Precision Search (One-Shot MPS) method that rapidly finds a diverse set of Pareto-front architectures. We derive the loss functions for the EdMIPS and DNAS methods and simplify and generalize the Bayesian Bits method. Our approach extends the supernet transformation with trainable functions that predict the bit width probability based on a hardware regularization parameter. We validate our method on popular models and demonstrate its superior performance in terms of searching time and Pareto-front architecture selection. This paper presents a theoretical derivation of existing state-of-the-art searching methods and introduces a novel approach to bit width prediction in the supernet, not described in existing literature.