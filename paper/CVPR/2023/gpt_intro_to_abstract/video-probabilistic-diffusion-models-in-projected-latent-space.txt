Recent advances in deep generative models have shown promising results in synthesizing realistic samples in various domains, including images, audio, 3D scenes, and natural languages. However, video synthesis remains a more challenging task due to the complexity and high-dimensionality of videos, which contain intricate spatiotemporal dynamics in high-resolution frames. To address this, researchers have explored the use of diffusion models for video synthesis, inspired by the success of these models in handling complex image datasets. While these video diffusion models have shown potential in modeling video distribution and achieving photorealistic results, they suffer from computational and memory inefficiencies due to the iterative processes required for synthesis. In contrast, latent diffusion models, which employ an autoencoder to learn a low-dimensional latent space for image synthesis, have demonstrated improved efficiency and generation results. However, the application of latent diffusion models to videos remains largely unexplored. In this paper, we propose a novel latent diffusion model for videos called the projected latent video diffusion model (PVDM). The PVDM consists of an autoencoder that factorizes the cubic array structure of videos into three 2D image-like latent vectors, capturing common contents and motion. We also introduce a diffusion model architecture based on 2D convolutions to model the video distribution. Our experiments on popular video datasets demonstrate that the PVDM achieves state-of-the-art results in terms of Inception score and Fr√©chet video distance, while also demonstrating memory and computation efficiency compared to existing video diffusion models. Overall, our work presents the first latent diffusion model designed specifically for video synthesis and can contribute to the development of efficient and high-quality video generation techniques under limited computational resources.