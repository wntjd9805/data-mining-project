The modular and segregated view of cortical color processing in the vision science community has led to the development of computational formulae for color difference (CD) assessment. However, the existing CD metrics built upon the CIELAB coordinate system have limitations in capturing perceptual non-uniformity. Recent evidence suggests that color and form are interdependent in color perception, and even the primary visual cortex plays a role in color perception. In this paper, we propose a data-driven approach to learn a deep CD metric for photographic images. Our approach is inspired by color perception in the visual cortex and satisfies properties of a proper metric, accuracy in predicting human perceptual CDs, and robustness to geometric distortions. We achieve these properties through a multi-scale autoregressive normalizing flow followed by Euclidean distance measure. Experimental results show that our proposed metric outperforms existing CD formulae and is more robust to geometric distortions. We also verify the perceptual uniformity of the learned color image representation.