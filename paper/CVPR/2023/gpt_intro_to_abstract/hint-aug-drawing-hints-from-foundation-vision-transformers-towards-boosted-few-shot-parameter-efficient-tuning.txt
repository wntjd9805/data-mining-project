Foundation vision transformers (FViTs) have demonstrated significant potential in various downstream tasks. Pretraining-then-tuning, which involves pretraining an FViT on a large-scale dataset and then tuning it using parameter-efficient methods, has become a new paradigm in deep learning. However, effectively tuning pretrained FViTs for real-world applications, especially under few-shot tuning scenarios, remains challenging. Although parameter-efficient tuning methods can mitigate overfitting, the data-hungry nature of FViTs limits their accuracy under data-limited scenarios. To address this, leveraging data augmentation techniques to increase data and feature diversity during few-shot tuning has been proposed. However, existing data augmentation techniques fail to boost model accuracy under these scenarios due to their random-based nature. In this paper, we propose the Hint-Aug framework, which integrates an Attentive Over-fitting Detector (AOD) and a Confusion-based Feature Infusion (CFI) module. AOD identifies overfitting samples and patches in the training dataset using attention maps, while CFI infuses pretrained FViTs' learned features into the training data. Extensive experiments on five datasets and three parameter-efficient tuning techniques validate the effectiveness of Hint-Aug, achieving higher accuracy compared to state-of-the-art data augmentation methods across different datasets and few-shot settings.