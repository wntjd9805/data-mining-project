Transformer models, known for their high model capacity and ability to capture long-range dependencies, have shown promising results in various tasks. The Vision Transformer (ViT) specifically, has demonstrated improved accuracy compared to state-of-the-art CNNs in image classification. However, the design principles of ViT, inherited from natural language processing tasks, may not be optimal for computer vision, leading to redundancy and inefficiency when scaled. This paper proposes an efficient ViT model by exploring parameter redistribution within and across ViT blocks. The authors analyze the importance and redundancy of different components in the model and perform global structural pruning to achieve an optimal architecture. The resulting NViT models achieve significant parameter reduction, FLOPs reduction, and speed up, while maintaining or improving accuracy compared to existing ViT compression methods. The authors also uncover the suboptimal parameter distribution rule in ViT and provide theoretical and empirical analysis to guide future efficient ViT architecture design. Overall, this work presents NViT as a hardware-friendly pruning algorithm, outperforming state-of-the-art compression methods, and provides insights into the design and optimization of Vision Transformer models.