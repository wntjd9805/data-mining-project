The demand for immersive animated content has grown significantly due to its applications in entertainment, metaverse, education, and augmented/virtual reality. However, the creation of animation content often involves tedious and labor-intensive processes such as modeling characters, rigging skeletons, and capturing motion from performers. These factors hinder the large-scale adoption of 3D content creation. This paper aims to address these challenges and democratize content creation by replacing expensive motion capture setups with a single monocular depth sensor and automatically generating character rigs for non-isometric objects. The authors propose a technique called Transfer4D, which enables the transfer of motion from a monocular depth video to a target virtual object without relying on a predefined template. They also introduce a novel unsupervised skeleton extraction algorithm to transfer motion from a single view incomplete mesh. The key contributions of this research are the development of the first intra-category motion transfer technique from a monocular depth video to a target virtual object and the proposal of an unsupervised skeleton extraction algorithm for motion transfer. Overall, this work aims to democratize motion-capture systems and reduce the burden on animators by simplifying rigging and automating motion transfer.