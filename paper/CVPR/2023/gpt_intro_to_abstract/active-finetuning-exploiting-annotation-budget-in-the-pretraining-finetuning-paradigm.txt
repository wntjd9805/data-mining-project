Recent advancements in deep learning have been highly dependent on the availability of large amounts of training data. However, manually annotating these large-scale datasets can be time-consuming and costly. To address this issue, a common approach is to pretrain models on a vast amount of unlabeled data and then fine-tune them on a smaller labeled subset. Previous research has focused on unsupervised pretraining and supervised fine-tuning. However, these approaches assume that the samples to be labeled are already known, which is often not the case in practice. To tackle this challenge, we introduce a new task called active fine-tuning, which focuses on selecting a small subset of samples from a large unlabeled data pool for annotation. We propose a novel method called ActiveFT that optimizes the selection process by finding a diverse subset that closely matches the distribution of the entire unlabeled pool. Unlike traditional active learning algorithms, our method can select all samples in a single-pass without iterative batch selections. We mathematically show that our optimization approach can reduce the Earth Mover's Distance (EMD) between the selected subset and unlabeled pool. Experimental results demonstrate the effectiveness of our method on various image classification and semantic segmentation tasks, outperforming baselines. Our contributions include identifying the gap in data selection for annotation and supervised fine-tuning, proposing the active fine-tuning task, and introducing the ActiveFT method for efficient and effective sample selection. The source code for our method will be made publicly available.