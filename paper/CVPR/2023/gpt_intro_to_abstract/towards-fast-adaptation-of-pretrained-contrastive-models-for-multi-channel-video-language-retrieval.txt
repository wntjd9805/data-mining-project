The increasing demand for AI agents capable of handling multi-channel video-language retrieval-style tasks has led to the development of approaches involving two rounds of pretraining. However, these approaches have limitations, such as requiring large amounts of data and computational resources for pretraining and being limited to strongly correlated video data. To address these limitations, this paper proposes a novel problem: fast adaptation of pretrained contrastive models on multi-channel video-language retrieval under limited resources. The paper explores potential model designs and identifies a simple and effective approach that leverages contrastive multimodal models and contrastive text models for fast adaptation. Extensive experiments on five video-language datasets demonstrate the effectiveness of the proposed approach without the need for large amounts of extra multimodal data samples. The proposed variant, Text Tokens + Text Transformer, achieves state-of-the-art performance and scales significantly better than other variants. The code for the proposed approach will be released to facilitate future research.