Motion retargeting, the process of mapping the motion of a source character to a target character while preserving plausibility, is a long-standing problem in computer vision and computer graphics. It has a wide range of applications in the game and animation industry and is crucial for digital avatars and metaverse technologies. In recent years, learning-based retargeting methods, particularly neural motion retargeting, have gained attention due to their advantages in intelligent perception and stable inference. However, existing methods still suffer from motion distortion and discontinuity issues. Inspired by the observation that artists manually modify motion to preserve semantics and avoid artifacts, we propose a Residual RETargeting network (R2ET) that mimics artist modifications. R2ET consists of a skeleton-aware module and a shape-aware module to address the differences in bone length ratio and body shape geometry between source and target characters. The skeleton-aware module aligns the semantics of the source and target motions using a normalized Distance Matrix. The shape-aware module ensures compatibility between the target character mesh and adjusted skeleton to avoid interpenetration and contact issues using Repulsive Distance Fields and Attractive Distance Fields. To balance between motion semantics preservation and interpenetration avoidance, we introduce a balancing gate. Our R2ET achieves motion semantics preservation, eliminates interpenetration, and contact-missing without post-processing. Extensive experiments demonstrate that R2ET outperforms existing methods in terms of qualitative and quantitative performance. Our contributions include the novel residual network structure, explicit motion semantics modeling using a normalized joint Distance Matrix, and the introduction of differentiable pose adjustment learning using Distance Fields.