This paper introduces SVGformer, a novel model architecture for learning effective representations of vector images in the SVG format. Unlike previous works that focused on bitmap images, SVGformer treats SVGs as sequential data and utilizes a transformer-based model to learn their compact and scale-invariant representation. The model addresses the challenges of handling the infinite token space and varying correlations between commands in SVGs. It incorporates geometric information through a geometric self-attention module and preserves the original continuous format of SVG data using a 1D convolutional embedding layer. The proposed SVGformer model achieves state-of-the-art performance in various downstream tasks, outperforming prior art by significant margins. This work is the first to explicitly consider vector geometric information and directly process SVG input in an end-to-end encoder-decoder fashion.