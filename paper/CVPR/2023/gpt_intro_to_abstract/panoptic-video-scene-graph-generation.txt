Scene graph generation has become a popular topic in the computer vision community, providing richer information in images by capturing both objects and their pairwise relationships. Recently, there has been a shift towards temporal, video-level scene graphs, which include the additional dimension of time and contribute to building more comprehensive visual perception systems. This paper introduces the concept of panoptic video scene graph generation (PVSG), where each node in the scene graph is grounded by a pixel-level segmentation mask. PVSG addresses the limitations of bounding box-based scene graphs by including both things and stuff classes, enabling a more comprehensive understanding of context. To facilitate research in this area, a high-quality PVSG dataset is proposed, consisting of 400 videos with fine panoptic segmentation and temporal scene graphs annotations. The paper also presents a two-stage framework for PVSG, exploring different design choices for feature extraction and scene graph generation. The contributions of this work include the identification of issues in scene graph generation, the introduction of the PVSG problem, the creation of a new dataset, and the proposal of new methods and benchmarking for future research.