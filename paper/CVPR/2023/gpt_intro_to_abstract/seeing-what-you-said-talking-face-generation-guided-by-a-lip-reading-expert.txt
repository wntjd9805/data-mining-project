Talking Face Generation (TFG) has gained considerable attention in both the industrial and research communities due to its practical applications in Human Robot Interaction (HRI). It aims to generate high-quality talking heads that are synchronized with input speech. The main challenges in TFG are lip-speech synchronization and visual quality. Several approaches have been proposed to address these challenges, including the use of auxiliary embedding networks, sync loss computation, and encoder-decoder structures. Blurry or unrealistic visual quality is also a major concern, and techniques such as skip connections and Generative Adversarial Nets (GAN) have been employed to improve the generation performance. However, the importance of reading intelligibility, especially for hearing-impaired users, has not been emphasized. Reading intelligibility refers to the ability to interpret text content from face videos through lip reading. Existing image quality and lip-speech synchronization techniques do not explicitly reflect reading intelligibility. In this paper, we propose a TalkLip net that focuses on reading intelligibility in TFG. We introduce a lip-reading expert that transcribes image sequences to text, penalizing the generator for incorrectly generated face images. To achieve reliable lip reading, we employ AV-Hubert, a self-supervised method with state-of-the-art performance in lip-reading and audio-visual speech recognition. We enhance lip-speech synchronization through contrastive learning between audio embeddings and visual context features, leveraging the lip-reading expert. Furthermore, we propose a new strategy to evaluate reading intelligibility and provide publicly available benchmark code. Extensive experiments demonstrate the feasibility and superiority of our approach in terms of reading intelligibility, visual quality, and lip-speech synchronization.