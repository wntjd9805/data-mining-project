The primary goal of this paper is to propose a novel meta-learning inspired strategy for updating the student network in the Adversarial Data-Free Knowledge Distillation (DFKD) setting. In DFKD, synthetic or pseudo samples are used to replace the original training data for knowledge transfer. Adversarial DFKD methods use a generator to create pseudo samples, and a Teacher-Student setup acts as a joint discriminator to penalize and update the generator parameters. However, the convergence of the student network is hindered due to distributional alterations over time, as the pseudo samples seem to be generated from different generator parameters. To address this issue, existing methods employ memory-based techniques to retain knowledge from previously encountered distributions. In this paper, the proposed approach treats the acquisition of new knowledge and retention of previous knowledge as meta-train and meta-test tasks, respectively. This strategy implicitly aligns the knowledge acquisition and retention tasks, resulting in improved student performance. Additionally, the proposed method is scalable and applicable in the absence of a validation set. Experimental results demonstrate the advantages of the proposed student update strategy compared to existing methods in terms of student learning evolution.