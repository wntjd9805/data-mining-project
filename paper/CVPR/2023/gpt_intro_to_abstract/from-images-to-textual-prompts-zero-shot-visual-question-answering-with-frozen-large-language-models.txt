Visual question answering (VQA) is an important task in computer vision and natural language processing, with numerous real-world applications. However, existing VQA systems heavily rely on human annotations, which are costly to obtain and introduce biases. To overcome these limitations, researchers have proposed zero-shot VQA methods that do not require ground-truth annotations. Recent approaches have leveraged large language models (LLMs) to achieve zero-shot VQA performance. However, integrating LLMs into VQA systems is challenging due to the modality and task disconnect between vision and language. In this paper, we propose a modular VQA system, Img2LLM, that uses off-the-shelf LLMs for zero-shot VQA. Img2LLM translates image content into synthetic question-answer pairs, bridging the modality and task disconnect. Our experimental results show that Img2LLM achieves comparable or superior zero-shot VQA performance to costly end-to-end trained models. We make the following contributions: (1) introducing Img2LLM as a plug-and-play module for zero-shot VQA, (2) enabling off-the-shelf LLMs for zero-shot VQA without costly training or specialized networks, and (3) demonstrating competitive performance of Img2LLM-equipped models compared to end-to-end trained models.