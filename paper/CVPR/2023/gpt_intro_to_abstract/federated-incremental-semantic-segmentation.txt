Federated learning (FL) is a decentralized training paradigm that allows for the learning of a global model across distributed local clients without accessing their private data. FL has been successful in semantic segmentation tasks by training on decentralized clients to alleviate the need for finely-labeled pixel annotations. However, existing FL-based semantic segmentation methods assume that the learned classes are static over time, which is impractical for real-world applications where new categories arrive continuously. This limitation leads to catastrophic forgetting and background shift, which degrade segmentation performance on old categories. To address these challenges, we propose a novel problem called Federated Incremental Semantic Segmentation (FISS). FISS aims to train a global incremental segmentation model while addressing catastrophic forgetting and background shift. We develop a Forgetting-Balanced Learning (FBL) model that incorporates adaptive class-balanced pseudo labeling, forgetting-balanced semantic compensation loss, forgetting-balanced relation consistency loss, and a task transition monitor to overcome intra-client and inter-client forgetting. Experimental results demonstrate the superiority of our model compared to existing methods. Our contributions include introducing FISS as a practical problem, proposing the FBL model for addressing heterogeneous forgetting, and designing mechanisms for handling intra-client and inter-client forgetting.