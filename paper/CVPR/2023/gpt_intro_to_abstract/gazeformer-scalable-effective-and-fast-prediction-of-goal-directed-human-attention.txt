Human gaze behavior prediction is a crucial computer vision problem with applications in various domains such as human-computer interaction, augmented/virtual reality systems, healthcare, visual display design, robotics, and education. Existing approaches for predicting gaze fixations during visual search have limitations, including the need for specific training data for each target category and the lack of scalability. To address these issues, we propose a new problem called ZeroGaze, which extends zero-shot learning to the gaze prediction problem. Under ZeroGaze, a model must predict the fixations a person makes while searching for a target category, even without training data for that target. We introduce Gazeformer, a novel multimodal transformer-based model that combines linguistic embeddings of target objects with image representations to predict scanpaths. Gazeformer does not require training a backbone detector for each target category and significantly improves inference speed. We demonstrate the effectiveness of Gazeformer in both the ZeroGaze and traditional settings, achieving state-of-the-art results and outperforming baselines. Our work contributes to the development of reliable, efficient, and effective gaze prediction models for use with edge devices.