In recent years, deep neural networks have achieved remarkable success in various applications. However, they often struggle to generalize to out-of-distribution data, which poses a domain discrepancy problem between the training and test data. This problem is particularly critical in domains like medical imaging and autonomous driving. To address this issue, there are two approaches: domain adaptation (DA) which transfers knowledge from a source domain to a target domain, and domain generalization (DG) which aims to learn a domain-agnostic feature representation using only source domain data. While DG typically relies on multi-source domains, there has been recent interest in single domain generalization. One solution is to use adversarial data augmentation, but these methods can be complex and computationally expensive. In this paper, we propose a progressive approach called Progressive Random Convolutions (Pro-RandConv) to improve style diversity while preserving object semantics in single domain generalization. Our approach involves recursively stacking small-scale random convolutions and employing a random convolution block with deformable offsets and affine transformation for texture and contrast diversity. We evaluate our method on single and multi DG benchmarks and demonstrate significant improvements in recognition performance compared to other methods.