Visual localization plays a crucial role in various applications such as autonomous driving and robotics. Traditional structure-based algorithms using handcrafted features have limitations in dealing with appearance changes and lack of semantic signals. In recent years, learning-based features have shown promising results but still suffer from limitations in selecting reliable keypoints. Advanced matchers based on sparse keypoints or dense pixels have achieved high accuracy but at the cost of high computational complexity. Some methods leverage semantics to improve accuracy, but they rely on additional segmentation networks and are prone to segmentation errors. In this paper, we propose a novel approach that implicitly incorporates semantics into the detection and description processes of a local feature model. We use explicit semantics as supervision during training and introduce a semantic-aware detection loss and a semantic-aware description loss. Our model produces semantic-aware features without the need for additional segmentation networks and is less sensitive to segmentation errors. Experimental results demonstrate that our approach outperforms previous methods in terms of accuracy and efficiency. We provide a detailed discussion of related works, describe our method, present experimental results, and conclude the paper.