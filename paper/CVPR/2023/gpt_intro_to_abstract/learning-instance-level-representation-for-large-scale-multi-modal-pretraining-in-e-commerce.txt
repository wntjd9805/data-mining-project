The paper introduces the concept of a general-purpose foundation model for E-commerce applications in computer science. It discusses the challenges of applying existing vision-language pretraining (VLP) methods to E-commerce, as the properties of natural and product images differ significantly. To address this, the paper presents a pretraining framework called ECLIP (E-commerce CLIP) that focuses on learning instance-level representations of products. ECLIP employs separate image and text encoders and introduces a decoder architecture that aggregates instance-centric representations without the need for manual annotation. The paper also proposes two pretext tasks, inter-product and intra-product multi-modal learning, to optimize the generated instance representations. Experimental results demonstrate the effectiveness of ECLIP in improving diverse real-world E-commerce tasks. Overall, the paper contributes a novel approach to multimodal representation learning in the E-commerce domain.