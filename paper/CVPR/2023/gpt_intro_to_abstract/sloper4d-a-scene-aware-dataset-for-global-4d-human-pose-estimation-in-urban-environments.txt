This paper introduces the problem of capturing fine-grained human poses in extra large-scale dynamic scenes and annotating detailed 3D representations. The authors discuss the limitations of existing datasets and propose a novel approach using wearable IMUs, LiDAR, and camera to capture human-scene interactions. They introduce a new dataset called SLOPER4D, which includes multi-modal capture data and rich annotations. The authors also propose a joint optimization method for obtaining accurate human motion representations. They conduct experiments to demonstrate the effectiveness of their approach and benchmark two HPE tasks on the SLOPER4D dataset. The contributions of this paper include the introduction of the first large-scale urban-level human pose dataset, the development of an effective joint optimization method, and the benchmarking of HPE tasks on the dataset.