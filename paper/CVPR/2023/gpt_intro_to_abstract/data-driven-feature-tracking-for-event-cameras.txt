Existing feature trackers for standard cameras are limited by the bandwidth-latency trade-off, which affects their performance under rapid movements. Low frame rates result in increased latency and difficulty in tracking features, while high frame rates increase bandwidth overhead and power consumption. Additionally, standard cameras suffer from motion blur in high-speed low-lit scenarios. Event cameras, on the other hand, offer high-temporal resolution, low power consumption, and minimal bandwidth, making them ideal for addressing the limitations of standard cameras. However, existing event-based trackers have poor tracking performance due to classical model assumptions and manual hand-tuning of parameters. In this paper, we propose a data-driven feature tracker for event cameras that combines the high-temporal resolution of event cameras with standard frames. Our method utilizes a neural network with a correlation volume and recurrent layers for tracking features. To improve tracking performance, we introduce a frame attention module that shares information across feature tracks. We train our tracker on synthetic optical flow data and finetune it using a self-supervision scheme based on 3D point triangulation. Our method outperforms state-of-the-art baselines on benchmark datasets and achieves faster inference without extensive parameter tuning. Furthermore, we demonstrate the combination of our method with a frame-based tracker for high-speed scenarios, highlighting the potential of sparingly triggering frames based on tracking quality for future applications.