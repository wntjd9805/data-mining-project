This paper addresses the problem of reconstructing 3D poses, shapes, and locations for hundreds of people in a large scene. While monocular human pose and shape estimation has been extensively studied, estimating global space locations together with poses and shapes for multiple people from a single image remains challenging due to depth ambiguity. Existing methods are limited to small scenes and cannot achieve consistent reconstructions in the global camera space. The coherence of reconstructed people with the outdoor scene, particularly with the ground, is rarely considered. To overcome these challenges, the authors propose Crowd3D, a framework for crowd reconstruction from a single large-scene image. They introduce an adaptive human-centric cropping scheme and a progressive ground-guided reconstruction network called Crowd3DNet to ensure consistent scale proportions and coherence with the scene. They also present the concept of Human-scene Virtual Interaction Point (HVIP) to convert the 3D crowd spatial localization problem into a 2D pixel localization problem. The authors construct a benchmark dataset called LargeCrowd, consisting of over 100K labeled humans in 733 gigapixel images, enabling training and evaluation on large-scene images with hundreds of people. Experimental results demonstrate the effectiveness and consistency of their method. Overall, the contributions of this paper include the proposal of the Crowd3D framework, the progressive reconstruction network with HVIP, and the construction of the LargeCrowd dataset.