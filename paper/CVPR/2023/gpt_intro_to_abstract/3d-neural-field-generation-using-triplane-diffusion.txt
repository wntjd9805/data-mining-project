This paper introduces a generative framework for 3D shape generation using 2D diffusion model backbones. The authors leverage the idea of learning to generate triplane representations, which encode 3D scenes as sets of axis-aligned 2D feature planes. The authors propose a two-step process, where a training set of 3D scenes is factorized into per-scene triplane features and a shared feature decoder. Then, a 2D diffusion model is trained on these triplanes, allowing for the generation of novel and diverse 3D scenes. The authors demonstrate that their approach outperforms state-of-the-art 3D generative adversarial networks (GANs) in terms of both fidelity and diversity. The use of triplanes as a multi-channel feature image representation enables the integration of existing 2D diffusion model backbones, improving the capture of local scene details. Overall, the proposed framework provides a scalable and efficient approach for 3D shape generation.