Convolutional neural networks (CNNs) have achieved remarkable success in recent years but suffer from high computational costs. To address this issue, various network compression methods have been proposed, such as network pruning, network decoupling, and network quantization. However, most of these methods rely on the availability of the entire training dataset, which may not be feasible in scenarios where data privacy or fast deployment is necessary. This paper focuses on the few-shot compression scenario, where only a few training samples are provided. Most previous works adopt filter-level pruning, but it fails to achieve a high acceleration ratio on real-world computing devices. In contrast, this paper advocates for block-level pruning, which offers a higher acceleration ratio and better accuracy recovery with a small training set. The authors propose a concept called recoverability to indicate which blocks to drop and develop a method to compute it efficiently. Based on this, they propose PRACTISE, a practical network acceleration algorithm that significantly outperforms previous few-shot pruning methods in terms of latency reduction and accuracy on ImageNet-1k. The paper contributes by highlighting the importance of the latency-accuracy tradeoff, demonstrating the effectiveness of block pruning, and introducing the concept of recoverability for better block selection. Extensive experiments showcase the superior performance of PRACTISE in both few-shot and extreme data-free scenarios, making it versatile and applicable to different network architectures.