Hand pose estimation is a crucial component for various applications such as sign language recognition and gesture-based interaction systems. However, obtaining accurate ground truth labels for training deep-learning-based hand pose estimation systems is challenging. One approach is to train models with synthetic data, but this often results in a domain gap and poor generalization to real-world settings. While more sophisticated synthesis can reduce this gap, there is still a noticeable performance drop. This paper addresses the cross-domain pose estimation problem and focuses on a semi-supervised setting, specifically learning from labeled synthetic data and unlabeled real-world data. Previous studies have mainly considered RGB synthetic images for pre-training. However, when generating synthetic data, it is relatively easy to render multiple data modalities, such as RGB images and depth maps. These modalities share common visual cues relevant for hand pose estimation. To exploit this information, the paper proposes a dual-modality network that incorporates both RGB images and depth maps. The network design includes an attention module that applies information learned from depth maps to the RGB image, producing attention-fused RGB features. This attention-fused information is then aligned with RGB features through multi-modal supervision. This approach limits the sensitivity of the RGB encoder to non-informative cues like background or irrelevant textures. In addition, intra-modal and inter-modal contrastive learning techniques are explored for multi-modal data, aiming to minimize feature discrepancies across modalities. After pre-training, a two-pass self-distillation procedure is employed to handle noisy pseudo-labels for fine-tuning. The first pass predicts the pose using the RGB input, while the second pass incorporates the predicted depth map to refine the pose estimation. This self-distillation process enhances the consistency between the two passes and prevents overfitting to noisy samples. In summary, the contributions of this paper are as follows: 1) the proposal of a dual-modality network that learns from RGB images and depth maps, with applicability to only RGB inputs for fine-tuning and inference; 2) the introduction of a supervised multi-modal contrastive learning method based on fused features to minimize feature discrepancies across modalities; 3) the development of a self-distillation procedure to exploit noisy pseudo-labels during fine-tuning without overfitting; and 4) significant improvements in state-of-the-art performance in 2D and 3D keypoint estimation.