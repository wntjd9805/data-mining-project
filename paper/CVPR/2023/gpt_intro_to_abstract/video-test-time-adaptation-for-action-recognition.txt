State-of-the-art neural architectures in action recognition have shown effectiveness, but they lack robustness to distribution shifts in test data. These shifts, caused by factors such as weather conditions, perturbations, and video processing changes, are difficult to avoid or account for. Test-Time-Adaptation (TTA) methods used in image classification do not suit action recognition well due to real-time requirements and vulnerability of videos to distribution shifts. Existing TTA algorithms do not effectively handle challenges like noise variations, motion blur, and compression artifacts in videos. To address these issues, we propose ViTTA, an online test-time-adaptation method for action recognition models. ViTTA leverages augmented views and enforces prediction consistency to improve adaptation efficacy. Extensive evaluations on popular action recognition benchmarks demonstrate that ViTTA outperforms existing TTA methods for image data and boosts the performance of state-of-the-art architectures. ViTTA is practical, requiring minimal latency, and can be seamlessly incorporated into existing systems without re-training. Our contributions include benchmarking existing TTA methods, adapting feature alignment approach, and proposing ViTTA as a video-specific adaptation technique for action recognition.