This paper addresses the issue of speech recognition in situations where both visual and audio inputs are corrupted. The authors analyze the robustness of previous models on three different input corruption scenarios: audio input corruption, visual input corruption, and audio-visual input corruption. They find that previous audio-visual speech recognition (AVSR) models are not robust to audio-visual input corruption and perform worse than uni-modal models. To address this issue, the authors propose a novel multimodal corruption modeling method and introduce the Audio-Visual Reliability Scoring module (AV-RelScore) that evaluates the reliability of each input modality. They demonstrate the effectiveness of their approach through comprehensive experiments on two large audio-visual datasets. The contributions of this work include the analysis of deep learning-based AVSR under multimodal input corruption, the proposal of an audio-visual corruption modeling method, and the introduction of the AV-RelScore module for robust speech recognition.