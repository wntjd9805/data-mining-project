This paper introduces CREPE, a benchmark for evaluating the compositionality abilities of vision-language models. Compositionality, the understanding that the meaning of the whole is determined by the meanings of its parts, is considered a key aspect of human intelligence. Previous evaluation methods for these models have focused on image-text retrieval, but they lack controlled sets of negatives and do not study retrieval performance with unseen compositional combinations or increased complexity. CREPE evaluates systematicity and productivity by utilizing large-scale image-caption datasets and introducing new test datasets. The experiments conducted on multiple architectures and training algorithms reveal that vision-language models struggle with compositionality. Key findings include a drop in performance between seen and unseen compositions, larger drops for models trained on specific datasets, degradation of retrieval performance with increased caption complexity, and no significant impact of training dataset size or model size on compositional reasoning. Additionally, models' zero-shot ImageNet classification accuracy correlates only with absolute retrieval performance, not with systematic generalization or productivity. Overall, this benchmark provides insights into the compositional abilities of vision-language models and can inform their further development.