Abstract: Vision transformers have achieved significant success in various tasks, but they still lack robustness against common image corruptions and adversarial perturbations. This vulnerability is suspected to be inherent in the self-attention mechanism, which relies on patch-based inputs and can become overly sensitive to corruptions or perturbations. We observe that transformers can be easily misled by adversarial perturbations on few patches, leading to misclassifications. However, generating adversarial perturbations can be computationally expensive, making it infeasible for large-scale datasets. Instead, we propose an efficient method of directly adding corruptions, such as random noise, to vulnerable patches. By occluding these patches with noise, we can expose the sensitivity issue and significantly reduce the model's confidence. To address this problem, we introduce the Reducing Sensitivity to Patch Corruptions (RSPC) training method. RSPC identifies vulnerable patches to construct patch-based corruptions and stabilizes the intermediate attention layers against them. Experimental results demonstrate that RSPC improves robustness against patch corruptions across different architectures and benchmark datasets. Furthermore, RSPC achieves a better tradeoff between accuracy and corruption robustness with lower training cost compared to adversarial training methods. Overall, this work highlights the need for a more robust attention mechanism in transformers and provides insights into improving their robustness against patch corruptions.