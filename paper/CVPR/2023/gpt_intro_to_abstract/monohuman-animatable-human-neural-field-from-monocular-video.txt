This paper introduces the problem of rendering a free-viewpoint photo-realistic view synthesis of a digital avatar with explicit pose control. Previous methods have limitations, such as the need for multi-view videos and controlled studios. To address this, the paper proposes a novel framework called MonoHuman, which aims to reconstruct and animate a photo-realistic avatar from a monocular video. The framework utilizes a Shared Bidirectional Deformation module to learn accurate and general deformation, as well as a Forward Correspondence Search module to improve rendering fidelity. The paper's main contributions include the proposal of MonoHuman as an approach for synthesizing novel pose sequences from a monocular video, the introduction of the Shared Bidirectional Deformation module for consistent deformation, and the demonstration of high-fidelity results through extensive experiments.