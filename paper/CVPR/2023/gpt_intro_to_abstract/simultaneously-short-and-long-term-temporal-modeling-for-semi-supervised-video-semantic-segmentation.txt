Deep neural networks have achieved state-of-the-art results in various vision tasks, but rely on large-scale labeled datasets for training. However, manual annotation of datasets is time-consuming and complex. This paper addresses the problem of training a video semantic segmentation model with limited annotations, where only the first frame of each video is labeled. The significance of this problem lies in the wide applications of video semantic segmentation and the cost-effectiveness of labeling one frame per video. Existing methods for this problem can be categorized into Pseudo Label and Feature Enhancement approaches. This work focuses on the latter, specifically designing innovative feature enhancement modules. Prior work mainly focuses on modeling short-term temporal correlations, neglecting the potential benefits of distant frames. This paper proposes a novel Simultaneously Short- and Long-Term Temporal Modeling method to capture temporal relationships from both adjacent and distant frames. Three components are designed for representation learning: a Spatial-Temporal Transformer to model short-term correlations, a Reference Frame Context Enhancement module to model long-term context, and a Global Category Context module to model global information. Experimental results show that the proposed method performs significantly better than existing methods, even under the partial annotation setting. This work contributes to closing the gap between dense annotations and one-frame-per-video annotations, which has practical applications.