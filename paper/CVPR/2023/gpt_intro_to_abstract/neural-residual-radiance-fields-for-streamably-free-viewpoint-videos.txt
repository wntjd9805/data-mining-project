Abstract:This paper introduces a novel neural modeling technique called the Residual Radiance Field (ReRF) for high-quality streaming and rendering of dynamic scenes. ReRF explicitly models the residual of the radiance field between adjacent timestamps in the spatial-temporal feature space. It employs a global tiny MLP to approximate the radiance output of the dynamic scene in a sequential manner. ReRF utilizes a compact motion grid and a sparse residual grid for each subsequent frame to compensate for errors and newly observed regions. This design allows ReRF to exploit feature similarities between adjacent frames without relying on a global canonical space. The paper presents a two-stage scheme for efficiently obtaining ReRF from RGB videos via sequential training, along with a ReRF-based codec that achieves high compression rates. Finally, a companion ReRF player is developed for online streaming of long-duration free-viewpoint videos of dynamic scenes. Overall, this work contributes to the support of streamable neural representations and enables high-quality free-viewpoint viewing experiences.