Implicit neural representations (INR) have emerged as a powerful method for parameterizing various signals such as 3D scenes, images, audio, and videos. While INR-based methods have shown success in image and video compression tasks, they are currently limited to encoding a single short video at a time. This hinders their potential applications in real-world scenarios where there is a need to represent and compress a large number of diverse videos. In this paper, we propose a novel implicit neural representation model called D-NeRV that is specifically designed to efficiently encode long or a large number of diverse videos. D-NeRV decouples each video clip into clip-specific visual content and motion information, which are modeled separately. We also introduce temporal reasoning to capture global temporal dependencies across frames and predict task-oriented flow as an intermediate output to reduce the complexity of memorizing pixel values. Experimental results demonstrate that D-NeRV outperforms state-of-the-art INR-based methods, traditional video compression approaches, and recent learning-based video compression methods in terms of video reconstruction, video compression, action recognition accuracy, and decoding speed. Overall, our proposed D-NeRV model enables the representation and compression of a large and diverse set of videos using a single neural network.