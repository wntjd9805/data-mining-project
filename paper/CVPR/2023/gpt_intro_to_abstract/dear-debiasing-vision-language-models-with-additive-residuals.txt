Deep learning-based vision-language models (VLMs) aim to unify text and visual data and reduce computing costs for computer vision and language tasks. However, these models often exhibit societal biases resulting from imbalanced training data and flawed training practices. In this paper, we propose a method called DEAR to reduce bias in VLMs by modifying the visual features of the models. We analyze the biases by computing cosine similarity between human images and specific text phrases, and evaluate the biases based on protected attributes such as gender and race. Our DEAR framework uses an additive residual-based debiasing technique to improve fairness in pre-trained VLMs. We demonstrate the effectiveness of our approach through quantitative analysis and qualitative evaluations on multiple datasets. Additionally, we introduce the PROTECTED ATTRIBUTE TAG ASSOCIATION dataset, which allows for nuanced reporting of biases in VLMs for race, age, and gender protected attributes. Our contributions include the DEAR framework and the PATA dataset, providing a simple and effective method for debiasing VLMs and a comprehensive evaluation platform for bias in VLMs.