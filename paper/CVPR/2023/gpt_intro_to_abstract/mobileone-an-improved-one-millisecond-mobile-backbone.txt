Efficient deep learning architectures for mobile devices have made significant progress in terms of decreasing FLOPs and parameter count while improving accuracy. However, these metrics may not accurately measure the efficiency of models in terms of latency. Metrics like FLOPs do not consider memory access cost and degree of parallelism, which can significantly impact inference latency. Additionally, parameter count is not well correlated with latency, as sharing parameters can lead to higher FLOPS but smaller model size. Moreover, parameter-less operations like skip-connections or branching can incur significant memory access costs. To address these challenges, our goal is to improve the latency cost of efficient architectures while maintaining or improving their accuracy. To identify architectural bottlenecks that affect on-device latency, we deployed neural networks on an iPhone12 using CoreML and benchmarked their latency costs. To alleviate optimization bottlenecks, we decoupled train-time and inference-time architectures by using a linearly over-parameterized model at train-time and re-parameterizing the linear structures at inference. We also dynamically relaxed regularization throughout training to prevent over-regularization of small models.Based on our findings, we designed a novel architecture called MobileOne, which achieves state-of-the-art accuracy within the efficient architecture family while significantly improving latency on the device. MobileOne introduces linear branches at train-time that get re-parameterized at inference, along with trivial over-parameterization branches for further improvements. At inference, our model has a simple feed-forward structure without any branches or skip-connections, resulting in lower memory access cost and the ability to incorporate wider layers for increased representation capacity. For example, MobileOne-S1 achieves better accuracy than MobileNet-V2 while having fewer parameters and lower latency.MobileOne outperforms other efficient models in terms of latency while maintaining or improving accuracy on various tasks including image classification, object detection, and semantic segmentation. Our contributions include the analysis of performance bottlenecks in activations and branching, the effects of train-time re-parameterizable branches and dynamic relaxation of regularization, and the generalization of our model to other tasks. We will release our trained networks and code for research purposes, as well as an iOS application code for benchmarking networks on iPhone devices.