This paper introduces the concept of task-guided top-down attention in computer vision, which has been lacking in current vision transformers. The authors propose a new Analysis-by-Synthesis Vision Transformer (AbSViT) model that incorporates a high-level prior to direct attention towards relevant objects in the image. The AbSViT model consists of a feedforward pathway and a feedback pathway, enabling top-down modulation of attention. The authors demonstrate that AbSViT outperforms existing models on various vision-language tasks and can also serve as a general backbone for tasks such as ImageNet classification and semantic segmentation. The object-centric representation produced by AbSViT also improves generalization to corrupted and adversarial images. This work encourages further exploration of task-guided attention designs and visual representation learning.