In the field of visual data analysis, increasing the resolution of images is often necessary for various purposes. However, many images have low resolutions due to technical reasons, such as in medical imaging, Earth observation, thermal surveillance, and robotics. In some cases, higher resolution images in a different modality can guide the super-resolution process by providing missing high-frequency content. This paper focuses on super-resolving depth images guided by RGB images, but the proposed framework is applicable to other sensor combinations. Previous research on guided super-resolution includes handcrafted methods and learning-based approaches, with recent works combining the two. Classical methods are often slow and memory-hungry, limited to low-level image properties, and lack contextual reasoning. Learning-based methods have improved the state of the art but struggle with sharp discontinuities and different image characteristics. This paper proposes a novel approach that combines diffusion-based optimization with deep feature learning. The method consists of anisotropic diffusion and a diffusion-adjustment loop, where diffusion weights are determined by a convolutional feature extractor. The feature extractor is trained end-to-end to fulfill the requirements of the optimization. The hybrid approach outperforms prior art on different datasets and upsampling factors. The proposed framework ensures adherence to the source depth values, crisp edges, and offers interpretability, while incorporating large-scale context information from deep learning. The contributions of this paper include the development of a hybrid framework, an implementation with constant memory demands and inference time, and setting a new state of the art for various datasets and upsampling factors.