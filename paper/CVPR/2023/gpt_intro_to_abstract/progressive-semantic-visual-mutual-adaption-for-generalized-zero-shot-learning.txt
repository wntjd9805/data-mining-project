Generalized Zero-Shot Learning (GZSL) is a challenging recognition task in computer vision that aims to recognize images belonging to both seen and unseen categories. This task requires knowledge transfer from the seen to unseen domains, which is achieved through auxiliary semantic information. Previous methods for GZSL have limitations in capturing fine-grained discriminative features and developing deep correspondence between visual and attribute features, leading to biased recognition of seen classes and ambiguous semantic representations.To address these limitations, we propose a progressive semantic-visual mutual adaptation (PSVMA) network. Our approach utilizes the powerful vision transformers (ViT) for visual embedding and extracts image patch features for interaction with semantic attributes. The PSVMA network consists of a dual semantic-visual transformer module (DSVTM), which includes an instance-motivated semantic encoder (IMSE) and a semantic-motivated instance decoder (SMID).In the IMSE module, we perform instance-aware semantic attention and attribute communication and activation to promote compactness between attributes. This adaptively converts sharing attributes into instance-centric semantic features and recasts unmatched semantic-visual pairs into matched ones, reducing semantic ambiguity. The SMID module explores cross-domain correspondences between visual patches and matched attributes, facilitating accurate semantic-visual interactions. Additionally, we introduce a novel debiasing loss to enhance knowledge transferability.Experimental results on common benchmarks demonstrate the effectiveness of our PSVMA approach, achieving superior performance compared to previous methods. Specifically, on the AwA2 benchmark, our method achieves a harmonic mean of 75.4%, outperforming competitive solutions by more than 2.3%. Overall, our contributions include the development of a PSVMA network that alleviates semantic ambiguity and strengthens feature transferability through progressive semantic-visual mutual adaptation, leading to improved recognition accuracy for both seen and unseen categories.