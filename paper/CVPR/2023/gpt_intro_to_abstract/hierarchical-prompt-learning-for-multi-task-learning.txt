Vision-language pre-training has shown promise in leveraging human language for a variety of recognition tasks. Vision-language models (VLMs) align image and text embeddings, encouraging similarity between matching pairs and pushing away unmatched pairs. During inference, task-relevant text can be used to query pre-trained VLMs for visual recognition. However, the performance of recognition tasks is significantly influenced by the provided task-relevant texts, which are often constructed manually or learned through prompt engineering. Existing approaches focus on adapting VLMs to individual tasks, but in realistic settings, models need to be adapted to multiple similar tasks, also known as multi-task learning. This paper explores how to adapt a pre-trained VLM to multiple target tasks through prompting. It is found that a combination of task-shared and task-individual prompts can enhance recognition by capturing both general and fine-grained content. The task-shared prompt acts as a regularization to avoid overfitting, while the task-individual prompt captures the specific characteristics of each task. To address the problem of negative transfer between dissimilar tasks, Hierarchical Prompt (HiPro) learning is proposed. HiPro constructs a hierarchical task tree to capture multi-grained shared information, with prompts learned at both task-individual and task-shared levels. Experimental results demonstrate the effectiveness of HiPro in improving recognition performance compared to existing prompt learning methods.