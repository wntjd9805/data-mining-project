This paper introduces a study on contrastive pre-training for improving image-text alignment in dual-encoder architectures. The authors revisit three important aspects of the contrastive pre-training pipeline - noise in datasets, model initialization, and contrastive training - and propose strategies that significantly enhance model performance on zero-shot benchmarks. They propose a scalable and effective approach called Complexity, Action and Text-spotting (CAT) filtering to select informative text-image pairs from noisy web-scale datasets. The authors also introduce concept distillation (CD) as an alternative approach to leveraging strong pre-trained vision models. Additionally, they address shortcomings in the training objective and demonstrate improvements using model-based importance sampling. The combined approach shows significant improvements over the baseline for dual-encoder architectures on a benchmark of 29 tasks. The authors also present an approach to maintaining performance in the transition from zero-shot to few-shot learning. Experimental results demonstrate the effectiveness of the proposed methods in various vision-language tasks.