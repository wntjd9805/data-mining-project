Persistent robots and virtual agents operating in human spaces such as homes should improve their performance over time by learning and adapting. Current work on language-guided agents in navigation or household tasks is episodic, erasing agent memory before each new instruction. However, physical robots have the advantage of building maps iteratively from visual observations, which facilitates long-term memory and navigation. In this paper, we propose Iterative Vision-and-Language Navigation (IVLN), where agents follow a sequence of language instructions to navigate indoor spaces. Each tour consists of episodes with target paths, and agents can utilize memory to enhance their understanding of future instructions. Through iterative exploration, irrelevant regions can be avoided, leading to rich semantic representations that can be reasoned about later. We investigate VLN scenarios in both discrete and continuous settings and introduce the IR2R and IR2R-CE benchmarks for studying persistent navigation agents. The discrete setting involves graph-based navigation with clear visual observations, while the continuous setting involves motion actions and noisy images of a 3D environment. We develop explicit mapping and implicit memory models for both settings and provide code and further details on our website.