This paper explores the shift in training paradigms for vision-and-language models, from manually annotated datasets to massive, uncurated datasets automatically crawled from the Internet. While the large amount of data has resulted in improved model performance, it also raises concerns about the potential for harmful and biased representations. The paper highlights the need for fairness protocols in dataset creation and model development to address societal bias. To contribute to the analysis and mitigation of bias, the authors annotate demographic and contextual attributes in the Google Conceptual Captions dataset and conduct experiments on various vision-and-language tasks. The findings reveal significant imbalances and representation gaps in the dataset, as well as evidence of performance variations across different demographic groups in downstream tasks.