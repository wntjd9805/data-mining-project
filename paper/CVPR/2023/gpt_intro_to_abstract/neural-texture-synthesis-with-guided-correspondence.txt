Example-based texture synthesis is a well-established field in vision and graphics, aiming to generate new textures that closely resemble a given exemplar. Classical approaches formulate texture synthesis as a Markov Random Field (MRF) problem and solve it by iteratively optimizing output patches to resemble their nearest neighbor in the input. While MRF optimization has been successful, recent research has explored the use of deep neural networks, either by matching deep features or training generative adversarial networks (GANs). In this paper, we propose a framework that combines the versatility and flexibility of MRF optimization with the power of deep neural networks. We introduce a Guided Correspondence Distance to search for the nearest neighbor for each output patch based on multi-layer deep features and define a Guided Correspondence loss to measure overall similarity based on corresponding patches. We update the output pixels using back-propagation. We discuss previous works that have explored MRF optimization combined with neural networks and deep learning-based approaches for texture synthesis. We also describe the classical MRF objective and the Expectation-Maximization algorithm used to solve it. We highlight the limitations of existing methods, such as poor patch diversity and blurry artifacts. To address these issues, we introduce the Guided Correspondence Distance and modify the conventional MRF energy to improve the sharpness of synthesized results. Our framework can be extended to various guided scenarios by adding corresponding penalties to the Guided Correspondence Distance. Experimental results demonstrate the effectiveness of our approach in both uncontrolled and controlled texture synthesis scenarios, achieving state-of-the-art visual quality. We also show that the Guided Correspondence loss can be used as a general textural loss for training feedforward networks and single-image editing. Existing statistic-based losses are inadequate for these tasks.