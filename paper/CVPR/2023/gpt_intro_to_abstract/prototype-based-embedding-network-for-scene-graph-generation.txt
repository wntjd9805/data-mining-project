Scene Graph Generation (SGG) is a foundational task in computer vision that involves detecting entities and predicting their relationships to generate a scene graph. This graph-based representation is useful for downstream tasks such as Visual Question Answering and Image Captioning. Existing SGG models employ object detection and contextual information exploration to enhance entity features and predict relations. However, these models often suffer from biased predictions, favoring common predicates over fine-grained ones. Various de-biasing frameworks have been proposed to address this issue, but they often sacrifice robust representations for head predicates. The main challenge lies in capturing compact and distinctive representations for relations, as existing methods struggle to learn accurate decision boundaries due to large intra-class variation and severe inter-class similarity. To tackle these challenges, we propose the Prototype-based Embedding Network (PE-Net) that models entities and predicates in a semantic space, leveraging category-inherent features and class labels to produce compact and distinguishable representations. We introduce the Prototype-guided Learning strategy and Prototype Regularization to facilitate efficient entity-predicate matching and alleviate semantic overlap between predicates. Experimental results on the Visual Genome and Open Images datasets demonstrate that our method achieves state-of-the-art performance in relation recognition for SGG.