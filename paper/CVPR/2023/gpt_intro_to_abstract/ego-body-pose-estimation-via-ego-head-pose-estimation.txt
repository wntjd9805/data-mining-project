Estimating 3D human motion from egocentric videos is crucial for VR/AR applications. However, this task is challenging due to the complex relationship between egocentric videos and full-body human motions, as well as the lack of a large-scale, diverse dataset. In this paper, we propose EgoEgo, a generalized and robust method for estimating full-body human motions from egocentric videos. Our approach decomposes the problem into two stages: head motion estimation and full-body motion estimation. We leverage the stabilizing ability of the head and use it as an intermediate representation for full-body motion estimation. This decomposition allows us to learn from large-scale, single-modality datasets, which are readily available. We address the head motion estimation problem by combining monocular SLAM methods with learned transformer-based models. In the second stage, we generate full-body motion based on a diffusion model conditioned on the predicted head pose. We also create a large-scale synthetic dataset with paired egocentric videos and 3D human motions for evaluation and future research. Our work contributes a decomposition paradigm, a hybrid approach for ego-head pose estimation, a conditional diffusion model for generating full-body poses, and a benchmark dataset. Experimental results show that our method outperforms existing baselines significantly.