In this paper, we propose a self-supervised approach called Video-Audio Separation through Text (VAST) to address the challenge of separating and localizing sound sources in videos based on natural language queries, without any supervision. We leverage large vision-language "foundation" models to provide pseudo-supervision for learning the alignment between the three modalities: audio, video, and natural language. Our approach preserves the transitive relationship between audio and natural language by using vision as an intermediary modality. We introduce two novel multi-modal alignment objectives to encourage the learned audio representations to encode the semantics of captions and infer the latent transitive relation between the modalities. We also formulate the problem as Multiple Instance Learning to perform audio separation at the video region level. Extensive evaluations on the SOLOS, MUSIC, and AudioSet datasets demonstrate the effectiveness of our approach, showing that VAST outperforms strongly-supervised state-of-the-art approaches without using labels during training. We also show that VAST learns to use language queries for audio separation, despite not being trained with ground-truth language supervision.