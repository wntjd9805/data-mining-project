This paper introduces a novel and effective Position-guided Text Prompt (PTP) paradigm for cross-modal model pre-training in the vision-and-language pre-training (VLP) context. The proposed PTP paradigm aims to address the position missing problem in end-to-end models while maintaining fast inference time for downstream tasks. Inspired by prompt learning methods, the PTP paradigm incorporates position-based co-referential markers in both image and text, transforming visual grounding into a fill-in-the-blank problem. This approach simplifies the learning of object information and enables strong visual grounding capabilities in VLP models. Experimental results demonstrate that the PTP paradigm outperforms existing methods, particularly in zero-shot settings. Furthermore, the PTP paradigm shows promising performance in tasks such as object position guided visual reasoning, visual question answering, and image captioning.