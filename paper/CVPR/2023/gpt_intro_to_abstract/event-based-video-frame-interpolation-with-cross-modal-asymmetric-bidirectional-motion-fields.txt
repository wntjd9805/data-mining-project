Video frame interpolation (VFI) aims to increase the temporal resolution of videos and is widely used in various fields. Deep learning-based VFI methods have shown promising performance improvements. However, existing methods often struggle to accurately estimate inter-frame motion fields, especially for complex motions. Event cameras, which capture blind motions with high temporal resolutions, have been explored for VFI but suffer from sparse and noisy data and sub-optimal approximation methods. To address these limitations, we propose a novel network called EIF-BiOFNet that directly estimates asymmetrical inter-frame motion fields by leveraging the characteristics of event and image data without approximation methods. Our proposed method achieves superior performance compared to previous methods. Moreover, we propose an Interactive Attention-based frame synthesis network to enhance long-range pixel correlation and introduce a new large-scale event-based VFI dataset called ERF-X170FPS with higher frame rate, resolution, and diverse scenes. Experimental results demonstrate the effectiveness of our proposed method, surpassing state-of-the-art methods on benchmark datasets, including our proposed dataset.