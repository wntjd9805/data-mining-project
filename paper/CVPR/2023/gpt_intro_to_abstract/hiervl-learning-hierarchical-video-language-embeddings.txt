Understanding human activity in video is a crucial problem in computer vision with numerous applications in augmented reality, robotics, and information retrieval. Despite the recent advances in the field, activity understanding in video still lags behind object understanding in images. This disparity is mainly due to the temporal nature of human activity, which requires a deeper understanding of the actor's intentions and the overall goal of the activity. In this paper, we propose a hierarchical video-language model, called HierVL, to capture both short-term actions and long-term intents in video. Unlike existing approaches that focus on matching short video segments with textual descriptions, HierVL incorporates both short-term and long-term context by integrating timestamped clip-level text descriptions and global text summaries. Through a two-layer contrastive learning framework, HierVL learns to generate video-text embeddings that capture the immediate observed actions as well as their contribution to the longer-term goal. Our model outperforms existing baselines and achieves state-of-the-art performance on multiple video benchmarks, demonstrating the effectiveness of our hierarchical approach in capturing temporal dependencies and intentions in video.