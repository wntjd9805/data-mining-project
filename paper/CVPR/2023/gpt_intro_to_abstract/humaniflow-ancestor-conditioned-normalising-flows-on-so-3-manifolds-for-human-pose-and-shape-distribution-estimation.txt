Estimating 3D human pose and shape from a single RGB image is a challenging task in computer vision. Due to depth ambiguity, occlusion, and truncation, multiple 3D solutions can correspond to a 2D observation. To address this, recent approaches have utilized deep neural networks to predict probability distributions over 3D pose and shape given the 2D input. This probabilistic approach offers advantages such as quantifying prediction uncertainty, sampling multiple plausible 3D solutions, and facilitating downstream tasks. However, existing probabilistic methods face a trade-off between accuracy, consistency, and diversity. Some methods prioritize accuracy and consistency but lack diversity, while others generate diverse samples that are not consistent with the 2D input. In this paper, we propose HuManiFlow, a method that aims to balance accuracy, consistency, and diversity in predicting distributions over pose and shape parameters. We use normalizing flows to construct expressive pose distributions and incorporate the manifold structure of the 3D body-part rotation group. Our method outperforms current approaches in terms of accuracy, input consistency, and diversity on benchmark datasets. It also provides interpretable uncertainty estimation due to occlusion, truncation, and depth ambiguities.