With the exponential growth of massive unlabeled data, there is increasing interest in mining and leveraging the "dark" knowledge present within it. A crucial step in achieving this is Novel Class Discovery (NCD), which aims to automatically recognize novel classes by partitioning unlabeled data into different clusters using knowledge learned from a labeled base class set. However, clustering without a prior is an ill-posed problem, making the base knowledge indispensable. Existing NCD methods assume a uniform class distribution in the unlabeled data, which is impractical. Clustering can still be ambiguous for features not removed by the base knowledge, leading to incorrect class assignments. To address this issue, a dynamic temperature technique is proposed to encourage sharper predicted distributions for less confident data. This technique adjusts the per-sample temperature to disambiguate less confident predictions.To estimate the class prior, the predicted novel class distributions are used as class assignments for training samples. The proportion of each class assignment is calculated, allowing the derivation of a new class prior that is beneficial for the next training iteration. By generating more accurate pseudo-labels, the reliability of the prior estimation for other classes is improved.The proposed BYOP (Bootstrap Your Own Priors) method is benchmarked against state-of-the-art methods on several standard datasets. BYOP outperforms these methods by large margins, demonstrating its effectiveness across different class distributions, including the conventionally balanced one.In summary, this paper presents a new challenging distribution-agnostic NCD task that relaxes the uniform class distribution assumption. The novel BYOP training paradigm is introduced to handle arbitrary unknown class distributions in NCD by iteratively estimating and utilizing the class prior. Extensive experiments validate the superiority of BYOP over current state-of-the-art methods in distribution-agnostic NCD.