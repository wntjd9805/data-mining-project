Understanding the surrounding scene is crucial for autonomous driving systems, and LiDAR sensors have become popular for perception due to their accurate distance information. LiDAR semantic segmentation (LSS) is a task that predicts semantic labels from LiDAR data. However, existing models do not consider the differences in data distribution between train and test domains, leading to performance deterioration in real-world applications with domain gaps. Two main domain gaps in LiDAR datasets arise from differences in sparsity and scene distribution. To address these challenges, we propose a domain generalization approach for LiDAR semantic segmentation (DGLSS). Our approach focuses on representation learning by considering sparsity and scene distribution differences. We simulate the unseen domain during the learning process by randomly subsampling LiDAR beams from the source domain and introduce sparsity invariant feature consistency (SIFC) and semantic correlation consistency (SCC) constraints. We also provide standardized training and evaluation settings for DGLSS and implement baseline methods for comparison. Experimental results show that our approach outperforms both unsupervised domain adaptation (UDA) and domain generalization (DG) baselines, demonstrating the effectiveness of our method in achieving high performance in unseen domains. Our contributions include proposing the first approach for domain generalization in LSS, building standardized settings for evaluation, and introducing novel constraints for generalizable representation learning.