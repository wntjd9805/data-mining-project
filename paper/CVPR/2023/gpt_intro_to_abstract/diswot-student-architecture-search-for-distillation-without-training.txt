Despite the success of Deep Neural Networks (DNNs) in visual recognition tasks, they suffer from heavy memory, computation, and power costs during model inference. Knowledge Distillation (KD) has been proposed as a solution to transfer knowledge from a high-capacity teacher model to a low-capacity student model, but existing methods are ineffective in improving distillation performance. To address this, we propose a training-free framework called DisWOT that leverages zero-cost metrics based on feature semantics and sample relations to select the optimal student architecture for distillation. DisWOT significantly improves accuracy and reduces training time compared to traditional approaches. We also introduce DisWOT as a new universal KD-based zero proxy for predicting model performance. Experimental results demonstrate the superiority of DisWOT on multiple datasets and search spaces. Our contributions in this area may aid future research in knowledge distillation and neural architecture search.