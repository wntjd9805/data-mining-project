Camouflage is a common defense tactic used by organisms to blend in with their surroundings. Camouflaged object detection (COD) aims to segment these camouflaged objects in a scene. In recent years, deep learning-based methods have been developed for COD, but they often struggle to explore global feature relations and provide accurate object localization and segmentation. Additionally, convolutional neural networks (CNNs) used in these methods have limited receptive fields and cannot effectively capture long-range dependencies. Vision transformers (ViT) have emerged as a potential solution to overcome these limitations, as they can efficiently model long-range dependencies with self-attention operations. However, existing ViT-based methods for COD have limitations in local feature modeling and feature aggregation in decoders. To address these issues, this paper proposes a novel transformer-based Feature Shrinkage Pyramid Network (FSPNet) for accurate and complete camouflaged object segmentation. The FSPNet incorporates a non-local token enhancement module to enhance local representations and a feature shrinkage decoder with adjacent interaction modules to aggregate object cues. Extensive experiments demonstrate that FSPNet outperforms existing state-of-the-art methods on multiple benchmark datasets for COD.