The goal of single image super-resolution (SR) is to reconstruct high-resolution (HR) images from low-resolution (LR) images. Many deep learning-based methods have been used in this field. However, there are two critical problems with existing methods. Firstly, the plain window self-attention (WSA) used in these methods has a limited receptive field, preventing the models from utilizing the information from neighboring windows. This leads to distorted images. Secondly, state-of-the-art SR networks require intensive computations, which is not ideal for real-world applications where reducing operations is crucial. To address these problems, this paper introduces the N-Gram context, which considers the interaction of neighbor local windows. This context is created by sliding-WSA to produce N-Gram context features before window partitioning. The N-Gram context expands the receptive field of WSA for recovery tasks and is inspired by N-Gram language models that consider the contextual information beyond individual words. The paper progresses in two tracks. Firstly, an efficient N-Gram Swin Transformer (NGswin) is proposed to address the problem of intensive operations. It consists of five components: a shallow module, hierarchical encoder stages with patch-merging, NSTBs (N-Gram Swin Transformer Blocks), SCDP bottleneck, and a small decoder stage. Experimental results demonstrate the efficiency and competitive performance of NGswin. Secondly, the N-Gram context is applied to other Swin-based SR models to improve their performance. SwinIR-NG, an improved version of SwinIR-light with N-Gram, achieves state-of-the-art results on lightweight SR. The contributions of this paper are twofold. Firstly, the N-Gram context is introduced to low-level vision with Transformers for the first time, enabling SR networks to expand their receptive field and recover degraded pixels. Secondly, an efficient SR network, NGswin, is proposed, which exploits hierarchical encoding, an asymmetrically small decoder, and SCDP bottleneck to achieve competitive performance. The N-Gram context also improves other Swin Transformer methods, as demonstrated by the improved performance of SwinIR-NG.