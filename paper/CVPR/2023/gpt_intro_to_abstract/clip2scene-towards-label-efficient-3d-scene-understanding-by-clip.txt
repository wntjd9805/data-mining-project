In this paper, we explore the potential of leveraging Contrastive Vision-Language Pre-training (CLIP) knowledge for 3D scene understanding. We propose a Semantic-driven Cross-modal Contrastive Learning framework, called CLIP2Scene, that utilizes CLIP's semantic and visual information to pre-train a 3D point cloud segmentation network. We address the optimization-conflict issue and temporal coherence problem present in previous cross-modal knowledge distillation methods. To overcome these challenges, we introduce Semantic Consistency Regularization and Spatial-Temporal Consistency Regularization. These regularization techniques make use of CLIP's text semantics and image pixel features to improve the representation learning and capture the rich inter-sweep correspondence in the multi-sweep point cloud. Our method achieves impressive results on annotation-free 3D semantic segmentation and outperforms other self-supervised methods when fine-tuning with annotated data. We conduct experiments on indoor and outdoor datasets, demonstrating the effectiveness of our approach in enhancing 3D scene understanding. The key contributions of our work include distilling CLIP knowledge to a 3D network, proposing a novel cross-modal contrastive learning framework, introducing semantic-guided spatial-temporal consistency regularization, and achieving promising results on annotation-free segmentation and label-efficient learning tasks.