Deep neural networks have achieved high task accuracy but lack interpretability, which is crucial for responsible use in high-risk settings. Concept-based interpretability methods have been proposed to bridge this gap by explaining models using semantic concepts. However, existing research overlooks factors such as the probe dataset used and the concepts employed in explanations. In this work, we analyze four representative concept-based methods for image classification models and conduct experiments using multiple probe datasets. We find that the choice of probe dataset significantly impacts explanations, and concepts used in explanations are often harder to learn than the target classes. Furthermore, human studies reveal that concept-based explanations with a large number of concepts are less effective and have no advantage over example-based explanations. These insights emphasize the need to carefully choose probe datasets and concepts for interpretability methods. We provide our analysis code to support the development and use of interpretability methods.