Co-salient object detection (CoSOD) aims to segment common salient objects in a group of relevant images. This task is crucial for downstream tasks such as object tracking, co-segmentation, and video co-localization. Existing CoSOD models rely on the group consensus assumption, assuming that all images in a group contain the same salient targets. While this assumption helps in modeling saliency, it limits the robustness of CoSOD models against images without a common object. State-of-the-art models often output false positive predictions for irrelevant images. To address this issue, we propose a learning framework called group exchange-masking (GEM). GEM involves exchanging images between two groups, introducing noisy images that do not contain the dominant co-salient object. These noisy images are labeled with a mask indicating the absence of a co-salient object. We design a dual-path image feature extraction module, consisting of a latent variable generator branch (LVGB) and a CoSOD Transformer Branch (CoSOD-TB), to model both group uncertainty and consensus. The outputs of these branches are concatenated and passed through a transformer-based decoder for co-saliency prediction. Experimental evaluations on benchmark datasets demonstrate the superiority of our proposed model compared to state-of-the-art methods, showing good robustness in handling noisy data without co-salient objects.