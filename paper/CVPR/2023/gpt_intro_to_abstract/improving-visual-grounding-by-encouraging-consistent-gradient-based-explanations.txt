Vision-language pretraining has enabled models to excel in various tasks such as visual question answering, image-text retrieval, and visual commonsense reasoning. However, the localization or grounding capabilities of these models can be further improved. This paper proposes a training objective called Attention Mask Consistency (AMC) to enhance the gradient-based explanations of vision-language models. The AMC objective optimizes the explanations to be consistent with human-provided region annotations, thereby improving the models' ability to produce accurate visual groundings. The formulation leverages gradient-based explanation maps obtained through methods like GradCAM. The proposed method, built upon the ALBEF model, achieves state-of-the-art performance in phrase grounding and referring expression comprehension benchmarks. The contributions of this work include the introduction of the AMC training objective, its effectiveness in enhancing grounding capabilities, and the achievement of state-of-the-art results in relevant benchmarks.