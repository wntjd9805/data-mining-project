Self-supervised learning has become a dominant paradigm for unsupervised visual representation learning. Augmentation-based self-supervision has been shown to produce powerful representations of unlabeled data, which can be used for supervised downstream tasks. However, a more suitable way to leverage self-supervision is by multi-tasking it with a custom (possibly supervised) objective. The computer vision community has explored the application of self-supervision in other sub-fields such as domain adaptation, novel class discovery, continual learning, and semi-supervised learning. Semi-supervised learning can benefit from advancements in unsupervised representation learning as it requires efficiently utilizing information from the unlabeled set to improve classification accuracy. Clustering-based self-supervised methods, such as DeepCluster v2, SwAV, and DINO, have been particularly successful in the self-supervised learning landscape. These methods learn representations by contrasting predicted cluster assignments of correlated views of the same image. In this paper, we propose a new framework for semi-supervised learning based on the observation that clustering-based methods can be adapted to a semi-supervised setting. We replace cluster prototypes with class prototypes learned with supervision and use the same loss function for labeled and unlabeled data. We introduce two instances of this framework: Suave and Daino, which are the semi-supervised counterparts of SwAV and DINO. These methods have favorable properties, including efficient representation learning, extraction of relevant information for semantic categories, and ease of implementation. Our proposed framework outperforms state-of-the-art approaches on both small and large-scale benchmarks, setting a new state-of-the-art in semi-supervised learning.