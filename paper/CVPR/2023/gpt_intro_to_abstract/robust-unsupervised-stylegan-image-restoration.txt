Image restoration is a challenging task in computer vision and image processing, aiming to recover a high quality image from a degraded input. Previous approaches have focused on solving specific restoration tasks using supervised learning with curated datasets, but this requires retraining large networks for each task. Unsupervised restoration methods have emerged, leveraging powerful generative models to recover clean images without task-specific training. In particular, StyleGAN has been found effective for unsupervised image restoration due to its elegant latent space design. However, existing methods often struggle to find a match between the degraded target image and the model distribution, leading to the need for additional regularization losses and careful tuning of hyperparameters. In this paper, we propose a robust unsupervised StyleGAN image restoration-by-inversion method that is independent of the type and intensity of degradations. Our approach utilizes a 3-phase progressive latent space extension, optimizing over the global latent space, individual layers of the generator, and individual filters. Additionally, we employ a conservative normalized gradient descent (NGD) optimizer, which maintains proximity to the initial point. This combination of optimization techniques eliminates the need for regularization losses and allows for a simple, consistent procedure across all tasks. We evaluate our method on various restoration tasks, including upsampling, inpainting, denoising, and deartifacting, at different degradation levels. Our approach achieves state-of-the-art results on most scenarios, even surpassing baselines optimized independently for each task. Furthermore, we demonstrate the effectiveness of our method under diverse and composed degradations. We develop a benchmark of synthetic image restoration tasks with controllable degradation levels, avoiding unrealistic assumptions. Comparative evaluations demonstrate the superiority of our unsupervised approach over existing unsupervised and diffusion-based methods. Overall, our method provides a robust and versatile solution to unsupervised image restoration, without the need for task-specific training or hyperparameter adjustment.