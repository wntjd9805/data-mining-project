The task of correspondence estimation between partially-overlapping point clouds is essential in various computer vision applications such as tracking, reconstruction, pose estimation, and 3D representation learning. This is typically done by encoding geometry into descriptors and establishing correspondences between frames based on the similarity of descriptors. The success of this task relies on the ability to handle rotations, as the same geometry can be observed from different views. Previous approaches have used handcrafted descriptors or deep neural models to achieve rotation invariance. However, deep point matchers often require augmented training to ensure invariance to rotations, leading to instability when facing rarely-seen rotations during inference. Other works have focused on intrinsic rotation invariance but sacrifice global context, resulting in less distinctive features. This paper introduces Rotation-Invariant Transformer (RoITr) for point cloud matching under arbitrary pose variations. It utilizes an attention mechanism with Point Pair Features (PPFs) as local coordinates to learn pure geometry regardless of poses. The proposed encoder-decoder architecture and global transformer enhance feature distinctiveness in a rotation-invariant fashion. The contributions of this work include the attention mechanism for pose-agnostic geometry description, the encoder-decoder architecture for rotation-invariant geometry encoding, and the global transformer with rotation-invariant cross-frame position awareness. Experimental results demonstrate the superiority of RoITr over existing methods in terms of efficiency and efficacy.