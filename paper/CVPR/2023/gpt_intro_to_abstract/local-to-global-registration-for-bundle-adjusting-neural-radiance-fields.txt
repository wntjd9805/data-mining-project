Recent advancements in neural fields have sparked renewed interest in visual computing problems, particularly in the use of coordinate-based neural networks to represent 2D images and 3D scenes. These neural networks are commonly warped to a global coordinate system using camera parameters obtained through methods like homography, structure from motion (SfM), or simultaneous localization and mapping (SLAM). However, the problem of simultaneously reconstructing neural fields from RGB images and registering camera frames poses a challenge. This paper proposes L2G-NeRF, a local-to-global registration approach that combines the benefits of both parametric and non-parametric methods. In the non-parametric stage, local transformations for each pixel are initialized using a deep network trained on photometric reconstruction errors. In the parametric stage, differentiable parameter estimation solvers are used to obtain a global alignment and apply a soft constraint to the local alignment. The contributions of this work include a strategy for local-to-global registration, differentiable parameter estimation solvers, and the agnostic nature of the proposed method towards different types of neural fields. L2G-NeRF shows promise in applications such as image reconstruction and novel view synthesis. The paper compares L2G-NeRF to explicit point clouds, highlighting the advantages of coordinate-based neural fields in terms of modeling view-dependent appearance and enabling downstream vision tasks. Furthermore, the paper discusses the significance of neural fields in solving visual computing problems and overcomes the limitations imposed by camera parameters. Finally, the paper mentions existing work on bundle-adjusting neural fields, both parametric and non-parametric, and suggests that the combination of these approaches with the local-to-global process in L2G-NeRF results in improved performance and noise resistance.