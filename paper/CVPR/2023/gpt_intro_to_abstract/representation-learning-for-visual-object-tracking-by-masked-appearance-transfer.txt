Visual object tracking relies heavily on the quality of visual representation. Deep representations have been successfully integrated into tracking algorithms, particularly in the siamese-based tracking methods. The siamese tracker assumes that different visual appearances of the same target can be embedded to have similar representations, enabling tracking through similarity matching. However, the effectiveness of representation learning methods for siamese tracking is still lacking. The common practice is to fine-tune ImageNet pre-trained representations, but they are learned for classification and may not yield good tracking performance. Most works focus on the design of the tracker neck and head, with representation learning driven by other tracker components. In this paper, we propose a representation learning method that decouples representation learning from tracker training. We employ a transformer-based autoencoder to learn discriminative visual features for object tracking, using a nontrivial learning objective. The learned representations are evaluated for target localization and box regression using a simple tracker with few parameters and no hyperparameters. Experimental results demonstrate the effectiveness and generalization ability of our proposed method, showing state-of-the-art performance in comparison to other trackers. Our contributions include proposing the masked appearance transfer (MAT) method for representation learning, designing a lightweight tracker for evaluation, and demonstrating the effectiveness of the proposed method through extensive experiments.