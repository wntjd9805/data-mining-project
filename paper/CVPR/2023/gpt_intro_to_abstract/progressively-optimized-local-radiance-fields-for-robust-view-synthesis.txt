Dense scene reconstruction for photorealistic view synthesis has many critical applications, such as VR/AR, video processing, and mapping. Radiance fields have been increasingly used to improve reconstruction fidelity, as they can model view-dependent appearance and intricate micro-details. However, when using a single handheld camera to capture large-scale scenes, there are challenges in estimating accurate camera trajectory and reconstructing radiance fields. Existing methods rely on separate techniques for camera pose estimation, which may fail in the handheld video setting. To address this, some approaches jointly optimize camera poses and radiance fields, but they struggle with long trajectories and tend to fall into local minima. This paper proposes a joint pose and radiance field estimation method that draws inspiration from incremental Structure-from-Motion algorithms and keyframe-based SLAM systems. The method progressively processes the video sequence using overlapping local radiance fields, estimating poses while updating the radiance fields. This enables the modeling of large-scale unbounded scenes with scalability, robustness, and increased sharpness. The method is validated on various datasets, including a new dataset specifically collected for evaluating the method. The contributions of this work include improved robustness through progressive estimation, the use of multiple overlapping radiance fields for visual quality improvement, and the introduction of a new dataset with challenging scenes. The limitations of this work include the lack of global bundle adjustment and loop closure, which are left as future research directions.