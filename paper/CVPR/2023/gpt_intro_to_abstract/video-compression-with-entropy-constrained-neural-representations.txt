Lossy video compression is an important optimization problem in computer science, where the goal is to minimize the number of bits required to represent a video while also minimizing any distortion caused by compression. Traditional video codecs have been used for this purpose, but recent research has focused on leveraging deep learning techniques for video compression. In particular, Encoder-Decoder neural networks have been used to transform groups of frames into latent representations, which are then quantized and stored/transmitted. Implicit Neural Representations (INRs) have also emerged as alternatives to dense grids for representing continuous signals, and have shown success in various applications. However, previous methods using INRs for video compression have treated fidelity and compression as separate tasks and have inefficient parameter allocation. In this paper, we propose a new compact convolutional architecture for video representation that outperforms previous works, and we introduce a theoretically motivated R-D formulation for video compression with INRs that jointly minimizes rate and distortion. We model the entropy of the neural network weights and use quantization-aware training to achieve better results. Our method improves on state-of-the-art video compression results and outperforms established methods.