This paper addresses the problem of video deblurring, aiming to restore clear videos from blurred ones captured by handheld devices. Unlike single image deblurring, video deblurring requires modeling both spatial and temporal information. Previous methods either use optical flow to model blur or employ deep convolutional neural networks (CNNs) to explore spatial and temporal information. However, these methods have limitations in terms of optimization problems, computational complexity, and the utilization of temporal information from non-local frames. To overcome these limitations, this paper proposes a lightweight deep CNN model called DSTNet, which consists of a channel-wise gated dynamic network (CWGDN) for spatial information exploration, a discriminative temporal feature fusion (DTFF) module for temporal information fusion, and a wavelet-based feature propagation method (WaveletFP) for propagating useful structures from long-range frames. The proposed network achieves better video deblurring performance compared to state-of-the-art methods, while being computationally efficient and not requiring additional alignment modules.