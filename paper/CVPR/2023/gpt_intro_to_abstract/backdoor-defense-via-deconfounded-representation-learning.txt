Recent studies have shown that deep neural networks (DNNs) are susceptible to backdoor attacks, where attackers inject stealthy backdoors into DNNs by poisoning a few training data points. These backdoors attach trigger patterns to benign images, causing the DNNs to misclassify them. The risk of backdoor attacks impedes the safe applications of DNNs in areas such as automatic driving and healthcare systems. In contrast, human cognitive systems are immune to such perturbations, as they are sensitive to causal relations rather than statistical associations. Therefore, it is crucial to leverage causal reasoning to analyze and mitigate the threats of backdoor attacks. In this paper, we focus on image classification tasks and propose a Causality-inspired Backdoor Defense (CBD) method to train backdoor-free models on poisoned datasets without requiring additional clean data. We construct a causal graph to model the generation process of backdoor data and identify the backdoor attack as a confounder that opens a spurious path between input images and predicted labels. CBD learns deconfounded representations by training two DNNs that focus on the spurious correlations and causal effects respectively. The first DNN captures the backdoor correlations, while the second clean model is trained to be independent of the first model in the hidden space. CBD achieves promising results in experiments with six representative backdoor attacks, significantly reducing the average backdoor attack success rates while maintaining clean accuracy. We also investigate an adaptive attack against CBD and demonstrate its robustness and resistance against such attacks.