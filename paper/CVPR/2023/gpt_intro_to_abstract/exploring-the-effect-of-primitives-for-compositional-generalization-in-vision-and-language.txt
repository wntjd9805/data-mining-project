Abstract:This paper addresses the concept of compositional generalization in vision-and-language (V&L) tasks. Compositional generalization refers to the ability of V&L methods to generalize to sentences with novel combinations of known words, mirroring the compositional properties of human cognition. The authors investigate the role of primitives, such as words, image regions, and video frames, in achieving compositional generalization. By modifying critical words in sentences, they demonstrate that existing V&L methods fail to establish the correct relationship between the primitives and the sample semantics, resulting in limited compositional generalization. To address this issue, the authors propose a self-supervised learning framework that generates numerous labeled samples to enhance the compositional generalization capability of existing V&L methods. Experimental results on temporal video grounding and visual question answering tasks show that the proposed framework improves the compositional generalization capability of existing methods. Overall, this research contributes to the understanding of compositional generalization and provides a practical solution to enhance the performance of V&L methods in this aspect.