This paper introduces the concept of using event cameras for object detection in time-critical scenarios. Event cameras are sensors that provide visual information at sub-millisecond latency by measuring changes in intensity at the time they occur. The advantages of event cameras include high dynamic range, motion blur robustness, and the ability to provide events continuously. The objective of this work is to design an approach that reduces processing latency while maintaining high performance. The paper explores different neural network designs, including dynamic graph neural networks and sparse neural networks, for event-based object detection. However, these approaches either require specialized hardware or have suboptimal performance. To address this, the paper proposes a new vision backbone design that combines ideas from conventional frame-based object detection with successful techniques from event-based vision. The design includes interleaved local and global self-attention, a convolutional layer for downsampling, and temporal recurrence using LSTM cells. The resulting neural network achieves competitive performance and higher efficiency compared to previous methods. The paper concludes by summarizing the contributions of the research, including improved object detection performance, a compact and composable stage design, and insights into effective data augmentation techniques.