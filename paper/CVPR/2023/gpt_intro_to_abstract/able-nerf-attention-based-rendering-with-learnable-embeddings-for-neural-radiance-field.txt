Neural Radiance Field (NeRF) is a widely used method for 3D scene representation, allowing for the generation of photo-realistic novel view images. However, NeRF struggles to accurately render colors of objects with view-dependent effects, such as translucent and reflective surfaces. This paper proposes a new approach called ABLE-NeRF, which addresses these limitations by reworking volumetric rendering as an attention-based framework and incorporating a memorization network using Learnable Embeddings (LE). ABLE-NeRF improves upon the existing Ref-NeRF model by producing accurate view-dependent effects and rendering colors of translucent objects more accurately. The paper demonstrates the superiority of ABLE-NeRF through quantitative and qualitative comparisons with Ref-NeRF on the Blender dataset. The technical contributions of this work are the use of transformers to model a physics-based volumetric rendering approach and the development of a memorization-based framework with LE to capture and render detailed view-dependent effects.