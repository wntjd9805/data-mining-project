Recently, deep neural networks (DNNs) have achieved impressive breakthroughs in computer vision and natural language processing tasks. However, the large number of parameters and computations required by DNNs limits their application on memory- and computation-constrained devices. Various approaches, such as pruning, quantization, and neural architecture search, have been proposed to compress and accelerate neural networks. Neural networks conventionally perform discrete representations and transformations, but their size cannot be adjusted without suffering performance degradation. This paper proposes Integral Neural Networks (INNs) which challenge the discrete representation of neural networks by exploring a continuous representation along the filters and channel dimensions. INNs employ the high-dimensional hypercube to present the weights of one layer as a continuous surface and define integral operators analogous to discrete operators in neural networks. INNs can be converted into tensor representation for the forward pass, and at the inference stage, can be discretized into networks of various sizes without the need for additional fine-tuning. Extensive experiments on image classification and super-resolution tasks show that INNs achieve the same performance as discrete DNN counterparts and preserve performance during structural pruning. INNs offer a solution for efficient network deployment in diverse conditions and hardware setups.