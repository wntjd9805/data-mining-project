Learning algorithms for image generation have made significant progress and are approaching human-level performance. However, video generation algorithms have not seen the same level of success and present a research opportunity. Generating high-quality videos with limited computational resources is a challenge due to the computational intensity of modeling videos. In addition, video synthesis requires generating frames with temporal consistency and realistic motions. This challenge becomes more severe as the number of moving elements in videos increases. Previous approaches have focused on reducing computational complexity but result in accumulated errors and inconsistent frames. Other methods use implicit neural representations but fail to capture diverse patterns of changing motions. This paper proposes a method for explicitly modeling and leveraging the diversity of motions for video generation. Inspired by motion segmentation methods, the authors formulate the notion of diverse motion modeling and introduce motion styles to control different levels of synthesis. They develop a motion network to generate time-dependent motion styles and a motion-style attention modulation mechanism to dynamically contribute to motion synthesis. The proposed approach outperforms previous methods and achieves state-of-the-art performance in unconditional video generation.