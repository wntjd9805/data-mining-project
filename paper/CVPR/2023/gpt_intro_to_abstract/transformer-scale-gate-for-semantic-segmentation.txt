Semantic segmentation is a challenging task in computer vision that involves segmenting all objects in an image and determining their categories. It serves as a foundation for higher-level tasks such as scene understanding, object recognition, and vision+language. Recent advancements in Vision Transformers have introduced encoder-decoder architectures for semantic segmentation, but most methods still struggle with accurately segmenting objects of varying sizes. Some approaches attempt to leverage multi-scale features, but they often fail to effectively select an appropriate scale for each image patch, which can impact segmentation performance. CNN-based methods have designed learnable models for optimal scale selection, but these models are complex and may cause overfitting. In this paper, we propose a novel approach called Transformer Scale Gate (TSG) that utilizes the inherent characteristics of Vision Transformers to guide the feature selection process for multi-scale segmentation. We observe that the self-attention module in the transformer encoder learns correlations among image patches, indicating that small-scale features may be preferred for patches correlated to many other patches, and vice versa. Similarly, the cross-attention module in the transformer decoder models correlations between patch-query pairs, implying that large-scale features are needed for patches correlated to multiple queries and small-scale features can avoid over-segmentation. Based on these observations, we design TSG to predict weights of multi-scale features for each image patch using correlation maps from self- and cross-attention modules. TSG is a simple and lightweight module that can be easily incorporated into diverse architectures. Furthermore, we extend TSG to TSGE and TSGD modules to optimize multi-scale features in the transformer encoder and decoder, respectively, through self-attention and cross-attention guidance. Experimental results on three semantic segmentation datasets demonstrate significant improvements in performance compared to a Swin Transformer baseline.To summarize, our contributions include the first exploration of utilizing inner properties of transformer attention maps for multi-scale feature selection in semantic segmentation, the design of TSG, and the introduction of TSGE and TSGD modules. Our proposed approach achieves substantial gains in semantic segmentation performance and offers a simple and efficient solution for multi-scale feature selection in Vision Transformers.