Our study focuses on the early learning dynamics of information fusion in deep networks. We argue that these dynamics are complex and fragile, similar to critical learning periods observed in biological systems. We show that shallow networks do not exhibit critical periods, but deep networks do, even in simple linear networks. We introduce a metric called "Relative Source Variance" to quantify the dependence of units in a representation on individual sources. We demonstrate that temporarily reducing the information in one source or breaking the correlation between sources can permanently change the overall amount of information in the learned representation. We propose that features inhibit each other in a competitive manner, but cross-modal reconstruction tasks can promote synergistic interaction between sources. We show that auxiliary cross-source reconstruction stabilizes learning dynamics and prevents critical periods. Our empirical analysis confirms the existence of critical learning periods for multi-source integration using state-of-the-art architectures. We suggest that pre-training on one modality and then adding additional pre-trained backbones fails to encode synergistic information, and training should be performed across modalities from the outset. We highlight the irrelevance of asymptotic analysis for deep network fusion and the need for separate considerations for deep networks as compared to shallow networks.