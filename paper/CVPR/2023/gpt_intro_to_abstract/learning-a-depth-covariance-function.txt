Inferring the 3D structure of the world from 2D images is an important task in computer vision. Recent research has focused on combining multi-view geometry with data-driven priors to improve the accuracy of geometry prediction. However, direct priors can sometimes be overconfident, leading to inaccurate 3D structure when multi-view geometry is well-defined. Designing a unified framework that combines both learning and optimization methods has been challenging, with previous attempts lacking flexibility and adaptability. In this paper, we propose a novel approach of learning a depth covariance function, which predicts the relationship between depths of different pixels in an RGB image. Our method utilizes a neural network to transform color information into features, and a Gaussian process models the prior distribution based on these features and a base kernel function. This approach allows for adaptive capacity and promotes locality, effectively modeling the geometric correlation between pairs of scene points. By learning this flexible prior, we can balance data-driven methods with test-time optimization for various geometric vision tasks. Our framework can be applied to depth completion, bundle adjustment, and monocular dense visual odometry. The proposed approach is illustrated through examples of bundle adjustment, dense depth prediction, and multi-view fusion. Our key contributions include the framework for learning the covariance function, its application to various tasks, and the ability to leverage the prior for inferring the desired 3D representation.