Generative adversarial networks (GANs) have made significant advancements in image synthesis through a two-player game between a generator (G) and a discriminator (D). The goal is for G to generate realistic data that can fool D, while D aims to distinguish between real and synthesized samples. However, the competition between G and D often favors D, as G's learning signals come solely from D. This allows D to dominate the game and limit the diversity and visual quality of the generated images. Many previous approaches have focused on improving the discriminator to address this issue. In this paper, we propose a new adversarial paradigm where G also acts as a referee to guide D. We encourage D to extract more information from each image by aligning its perspective with G's. We introduce a method called GLeaD, where D is given a generator-leading task. D extracts spatial and latent representations from real or synthesized images and feeds them into a frozen generator to reconstruct the original image. The difference between the input and reconstructed images is used to update the discriminator's parameters. Our method improves synthesis quality and fairness between G and D, as demonstrated through comprehensive experiments on various datasets. The effectiveness of GLeaD is evidenced by improvements in Frechet Inception Distance and Recall metrics. Ablation studies also show the benefits of incorporating both real and synthesized images in the generator-leading task. Additionally, experimental results reveal that our method enhances the spatial attention of D.