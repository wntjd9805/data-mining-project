Rotated detectors have shown superior performance for top view images, but the annotation required for training is more expensive compared to axis-aligned detectors. This limitation restricts the implementation of rotated detectors, as popular annotation tools do not support rotated bounding box annotations. In this paper, we propose a training scheme called Knowledge Combination to learn Rotated object detection. Our approach combines the task knowledge of a source dataset with stronger rotated annotation and the domain knowledge of the target dataset with weaker axis-aligned annotation. We believe that decoding to a more precise task on the target domain can be learned by co-optimizing with a strongly labeled source dataset. Our framework aims to maximize target domain knowledge while minimizing the negative impact of weaker labels. We conduct extensive experiments to demonstrate the effectiveness of our approach, showing that box orientation can be learned with no additional annotation cost. Our method consistently performs on par with fully supervised models on various datasets with different domain gaps. We believe that our approach will greatly increase the usage and impact of rotated object detectors, and the source code will be publicly available for the community.