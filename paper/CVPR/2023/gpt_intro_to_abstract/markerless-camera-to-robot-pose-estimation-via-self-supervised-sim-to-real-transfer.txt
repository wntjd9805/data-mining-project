This paper introduces a novel framework called CtRNet for image-based robot pose estimation. The majority of modern robotic automation relies on cameras for environmental information and uses position-based visual servoing (PBVS) to convert this information into the robot's frame of reference for manipulation. Traditionally, PBVS requires calibration between the camera and robot using fiducial markers. However, this approach is limited by its lack of flexibility and inability to handle real-world factors. To address this, the paper proposes a deep learning approach using keypoints and rendering-based methods. Keypoint-based methods offer fast inference speed, but their performance is limited by the sim-to-real gap. Rendering-based methods provide better accuracy but are impractical for real-time applications. CtRNet combines both approaches by using keypoints for inference and rendering-based methods for training. The framework includes a segmentation module for foreground segmentation and a keypoint detection module for pose estimation. The network is pretrained on synthetic data and then trained in a self-supervised manner using real-world data. The paper demonstrates the effectiveness of CtRNet in real-time robot pose estimation and highlights its high inference speed and performance comparable to rendering-based methods. The main contribution of this work is a scalable self-training pipeline for robot pose estimation, which utilizes unlimited real-world data without manual annotations.