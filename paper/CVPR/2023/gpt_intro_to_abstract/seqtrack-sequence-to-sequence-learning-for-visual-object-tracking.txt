Visual object tracking is a fundamental task in computer vision, aiming to estimate the position of a target object in a video sequence. Existing tracking approaches commonly adopt a divide-and-conquer strategy, decomposing the tracking problem into subtasks addressed by specific head networks. However, this strategy leads to a complicated tracking framework and the need for multiple learning loss functions. To address these issues, this paper proposes a new Sequence-to-sequence Tracking (SeqTrack) framework. SeqTrack models tracking as a sequence generation task, eliminating the need for complicated head networks and additional loss functions. It converts the bounding box values into a sequence of tokens and uses a transformer-based encoder-decoder model to generate this sequence token-by-token. The generation is performed in an autoregressive fashion, ensuring that each token value only depends on previously observed ones. Experiments demonstrate that SeqTrack achieves state-of-the-art performance on tracking benchmarks, outperforming existing methods while maintaining faster runtime. The contributions of this work are the proposal of the SeqTrack framework and the presentation of a new family of sequence tracking models that strike a balance between speed and accuracy.