Neural rendering has made significant progress in 3D reconstruction by leveraging the differentiability of neuron computation. This has attracted the attention of researchers from academia and industry due to its ability to bypass the expensive cost of high-fidelity 3D modeling. In this paper, we propose a one-shot talking face avatar (OTAvatar) that can generate realistic facial expressions while keeping in sync with driving signals like video and audio segments. Unlike previous methods that either focus on 2D expression animation or collapse under large pose variations, our approach utilizes volume rendering and a 3D generative model to ensure good 3D consistency and generalization to different identities. We also introduce a motion controller to decouple motion and identity in latent space, making the motion controllable and transferable to different identities. Our experiments demonstrate that our method can generate photo-realistic avatars with desired expressions and poses at 35FPS, showing promising results in terms of natural motion and 3D consistency on both 2D and 3D datasets. This work is the first attempt at one-shot 3D face reconstruction and motion-controllable rendering, making important contributions to the field of neural rendering.