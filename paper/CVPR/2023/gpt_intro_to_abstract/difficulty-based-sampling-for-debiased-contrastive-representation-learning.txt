Contrastive learning is a popular technique in computer science for learning representations that project samples from the same class to be closer to each other than samples from different classes. This method has shown superior performance in tasks such as object detection and natural language processing. Recent research has focused on developing sampling methods to draw high-quality positive and negative samples for training contrastive learning models. While positive samples can be easily generated through augmentation techniques, finding legitimate negative samples is more challenging. This paper introduces a debiased contrastive learning method that addresses the issues of hard vs. easy negatives and true vs. false negatives. The proposed method utilizes triplet loss to amplify biases in the representation space and measures the relative difficulty of samples to train a debiased model. Experimental results demonstrate that the learned representations achieve higher accuracy and reduced bias compared to existing methods in image and tabular data classification tasks.