Recent advancements in 2D and 3D image understanding have led to the development of powerful architectures such as ResNets and Vision Transformers. While these architectures have achieved success in 2D recognition and segmentation tasks, their focus has been on learning from 2D image data. However, current large-scale RGB-D datasets provide an opportunity to incorporate geometric and structural priors for more informed reasoning and efficient representation learning. While several methods have leveraged RGB-D datasets for contrasctive point discrimination and downstream 3D tasks, the reverse direction from 3D to 2D representation learning is less explored. In this paper, we propose a method called Mask3D that embeds 3D priors into 2D backbones by pre-training with single-view RGB-D data. Our approach involves a pretext reconstruction task to reconstruct the depth map by masking different random RGB and depth patches of an input frame. These masked inputs are encoded and decoded to reconstruct the dense depth map, thereby imbuing 3D priors into the RGB backbone. We demonstrate the effectiveness of Mask3D on various datasets and image understanding tasks such as semantic segmentation, instance segmentation, and object detection. Our approach achieves notable improvements on ScanNet, NYUv2, and Cityscapes datasets, highlighting the potential of incorporating 3D representation learning into powerful 2D backbones. The contributions of this paper are the introduction of a self-supervised pre-training approach that learns masked 3D priors for 2D image understanding tasks from single-view RGB-D data, and the demonstration of the effectiveness of this approach with the ViT architecture.