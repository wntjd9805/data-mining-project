This paper introduces a model-free approach for 3D interacting hand pose estimation from a single RGB image. The existing methods in this area can be categorized into model-based and model-free groups, with the former showing promising performance but requiring complex personalized model calibration. In this paper, we focus on the model-free approach and propose a method called A2J-Transformer, which extends the depth-based pose estimation method A2J to the RGB domain. A2J-Transformer addresses the limitations of A2J by introducing anchor point-wise adaptive multi-scale feature learning and 3D anchor point setup. The anchor points are evolved as learnable queries under the Transformer framework, enabling them to capture joints' global articulated clues and resist occlusion and confusing appearance patterns. Additionally, the queries are located within the 3D space to alleviate the ill-posed 2D to 3D hand pose lifting problem. Experimental results on the Interhand 2.6M dataset demonstrate that A2J-Transformer achieves state-of-the-art performance for 3D interacting hand pose estimation from a single RGB image, outperforming A2J and other existing methods. The paper contributes by extending A2J to the RGB domain, proposing a non-local self-attention mechanism with adaptive local feature learning, and addressing the 2D to 3D lifting problem using monocular RGB information.