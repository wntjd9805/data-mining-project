In this paper, we address the problem of achieving a large receptive field in 3D perception tasks. While self-attention methods like Transformer have shown success in achieving global relations in downstream vision tasks, we explore the potential of using large convolutional kernels to obtain wide-range information. We compare our approach, called LinK, with existing methods like RepLKNet and SLaK, which have achieved comparable results to Transformer-based methods. We argue that large kernel methods are more efficient in real applications due to the convolution operator's compatibility with existing chip architecture. We discuss the challenges in extending large kernel methods to 3D perception tasks, including the increase in time and space consumption. We introduce LargeKernel3D as a first step in achieving better metrics for 3D segmentation and detection tasks, but highlight the limitations of its kernel size. To address these limitations, we propose LinK, which utilizes a linear kernel-generating module to provide weights only for non-empty areas in the sparse 3D input. We also introduce a method to reuse pre-computed aggregation results in overlapped blocks, reducing computation complexity. Through extensive experiments on 3D detection and segmentation benchmarks, we demonstrate the effectiveness of LinK, achieving top performance in nuScenes 3D detection leaderboard and improving mIoU in the SemanticKITTI test split.