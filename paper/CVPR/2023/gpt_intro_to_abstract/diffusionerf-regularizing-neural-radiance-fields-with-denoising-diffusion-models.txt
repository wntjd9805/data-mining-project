This paper explores the use of neural radiance fields (NeRFs) for novel view synthesis and 3D reconstruction tasks. NeRFs are multi-layer perceptrons that predict density and color for a given 3D point and viewing direction, allowing differentiable rendering from arbitrary views. While NeRFs show impressive generalization capabilities, they suffer from under-constrained scene color and geometry fields, resulting in low-quality and physically implausible results. Previous approaches have proposed hand-crafted regularizers and learned priors, but none have learned a joint probability distribution of the scene geometry and color. In this paper, the authors propose leveraging denoising diffusion models (DDMs) as a learned prior over color and geometry. DDMs provide the gradient of the log-probability of the RGBD patch distribution, which can be back-propagated to NeRF networks during training to act as a regularizer. Experimental results demonstrate that the DDM gradient encourages NeRFs to fit density and color fields that are more physically plausible.