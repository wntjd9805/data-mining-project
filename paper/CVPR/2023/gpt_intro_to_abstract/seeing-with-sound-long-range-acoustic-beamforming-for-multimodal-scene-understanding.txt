This paper introduces the use of acoustic sensing as a complementary modality for autonomous mobile robots. While current sensing modalities rely on electromagnetic radiation-based sensors, such as camera, radar, and lidar, acoustic sensing overcomes limitations such as low reflectance and foggy conditions. The authors propose a neural acoustic beamforming method using a large multimodal dataset, which combines acoustic signals from a microphone array with visual sensors. They demonstrate the effectiveness of acoustic sensing in various real-world driving scenarios, including low-light and non-line-of-sight conditions. The paper presents a comprehensive dataset and validates the superiority of combined vision and acoustic detection over vision-only approaches. The authors introduce long-range acoustic beamforming as a complementary modality for automotive perception and propose a learned aperture expansion method for small aperture microphone arrays. The proposed method outperforms existing RGB-only and audio-only detection methods in challenging scenarios. The paper concludes by discussing the limitations of acoustic sensing for quieter traffic participants but highlights its potential for robust scene understanding in challenging situations.