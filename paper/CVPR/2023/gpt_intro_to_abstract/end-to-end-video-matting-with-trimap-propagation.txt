Image matting is a computer vision task that aims to estimate the alpha value for each pixel in an input image, representing the degree of transparency. Unlike general segmentation, which generates binary values, matting outputs values between 0 and 1, allowing for the representation of semi-transparent objects with precise details. In this paper, we focus on video matting, which aims to extract an alpha matte for each frame of a given video. This is crucial for applications such as background replacement in video conferencing and visual effects. However, performing image matting on each frame independently can result in severe flickering artifacts. To address this issue, we propose an end-to-end video matting model called FTP-VM (Fast Trimap Propagation - Video Matting), which incorporates spatial and temporal coherence to improve robustness. Additionally, we introduce a lightweight trimap fusion module to enhance efficiency and performance. We adapt the segmentation consistency loss from automotive segmentation to trimap segmentation in order to achieve satisfactory results. Our proposed method outperforms previous two-model methods in terms of speed, reaching a frame rate of 40 FPS on an NVIDIA RTX 2080Ti GPU, while maintaining competitive performance in various scenarios.