In this paper, we introduce PROTOCON, a novel method for addressing confirmation bias in label-scarce semi-supervised learning (SSL). SSL leverages unlabeled data to guide learning from a small amount of labeled data, providing a cost-effective alternative to human annotations. However, existing SSL methods that rely on confidence-based pseudo-labeling struggle in label-scarce settings as the model becomes overconfident about distinguishable classes faster than others, leading to collapse. PROTOCON addresses this limitation by complementing confidence with a label refinement strategy to encourage more accurate pseudo-labels. We propose a co-training framework where each image obtains two different labels - the model's softmax prediction and an aggregate pseudo-label based on the pseudo-labels of other images in its vicinity. To ensure the success of co-training, we employ a non-linear projection to map our encoder's representation into a different embedding space and define the neighborhood pseudo-label based on the vicinity in that space. Additionally, we design PROTOCON to be fully online, allowing scalability to large datasets at a low cost. We identify neighbors in the embedding space on-the-fly using online K-means clustering. Moreover, PROTOCON addresses the weak training signal issue faced by confidence-based methods in the initial training phase by adopting a self-supervised instance-consistency loss. We demonstrate PROTOCON's superior performance compared to state-of-the-art methods on multiple SSL benchmarks, including CIFAR, ImageNet, and DomainNet. PROTOCON achieves significant improvements in SSL accuracy and convergence speed with limited labeled data. Overall, our contributions include a memory-efficient method addressing confirmation bias, improved training dynamics and convergence, and state-of-the-art results on various SSL benchmarks.