Multi-label image classification is an important task due to the prevalence of web-crawled images with multiple objects or concepts. However, the annotation costs for this task can be significant, hindering the scalability of multi-label classification datasets. Partially annotated multi-label classification has emerged as a solution, where only a few categories are labeled for each training image to reduce annotation burden. One common approach is to assume unobserved labels as negative labels, but this assumption introduces label noise in the form of false negatives. In this paper, we investigate how false negative labels influence a multi-label classification model. Through control experiments, we observe that models trained with false negatives still highlight similar regions to models trained with full annotations, but with smaller attribution scores. To bridge this gap, we propose BoostLU, a simple piece-wise linear function that boosts the scores of highlighted regions in models trained with false negatives. By applying BoostLU during inference, we improve the test performance of the baseline method without additional training. When combined with methods that detect and modify false negatives during training, BoostLU achieves state-of-the-art performance on various datasets in the partially labeled setting. In summary, this paper analyzes the effect of false negatives in partially annotated multi-label classification, introduces BoostLU as a solution, and demonstrates its effectiveness in improving performance.