Video Anomaly Detection (VAD) is a crucial task for various real-world applications such as intelligent manufacturing, surveillance, and public security. Conventional fully-supervised VAD is impractical due to the expensive labeling cost required for diverse anomalies. On the other hand, unsupervised VAD often triggers false alarms. This paper focuses on Weakly Supervised VAD (WSVAD), where only video-level binary labels are available. The mainstream method, Multiple Instance Learning (MIL), has limitations as it is biased towards the simplest context shortcut in a video. This paper proposes a novel approach called Unbiased MIL (UMIL) that trains an unbiased anomaly detector by considering both confident abnormal/normal snippets and ambiguous ones. UMIL combines feature fine-tuning and detector learning into an end-to-end training scheme, leading to a more tailored feature representation for VAD. It also utilizes a fine-grained video partitioning strategy to preserve subtle anomaly information in video snippets. Experimental results on benchmark datasets demonstrate the effectiveness of UMIL, outperforming current state-of-the-art methods in terms of AUC. UMIL brings more than a 2% gain in AUC compared to the MIL baseline, highlighting its efficacy in video anomaly detection.