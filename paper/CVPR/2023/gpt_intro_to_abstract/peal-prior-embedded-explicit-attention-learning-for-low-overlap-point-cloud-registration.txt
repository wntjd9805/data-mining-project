Rigid point cloud registration is a challenging task in 3D vision and robotics, aiming to align two point clouds through an optimal rigid transformation. Recent advancements have shown that deep networks and keypoints-based methods are effective in achieving accurate point cloud registration. However, existing methods still face challenges in scenarios with ambiguous matching and low geometric discriminative patches. This paper proposes a Prior-embedded Explicit Attention Learning (PEAL) model to address these issues. PEAL divides superpoints into anchor and non-anchor ones based on an overlap prior and introduces a one-way attention mechanism to model correlations from non-anchor superpoints to anchor ones. This approach alleviates interference from non-anchor superpoints and improves the feature distinctiveness for registration. Furthermore, the paper introduces two models depending on the methods of obtaining a prior, and extensive experiments demonstrate the superiority of PEAL over state-of-the-art methods. The contributions of this work include the explicit injection of overlap prior into the Transformer framework, the one-way attention module, and a novel iterative pose refined fashion for low-overlap point cloud registration.