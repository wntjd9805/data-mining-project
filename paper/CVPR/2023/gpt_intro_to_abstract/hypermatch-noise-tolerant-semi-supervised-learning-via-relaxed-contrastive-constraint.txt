Abstract:Semi-supervised learning (SSL) has emerged as a promising approach for leveraging unlabeled data to improve model performance and reduce annotation costs. This is especially useful in applications that require large amounts of labeled data to achieve high performance. Modern SSL algorithms can be categorized into two main types: Pseudo Label-based and Consistency Regularization-based. However, these methods often rely on the accuracy of pseudo labels for effective class assignment, leading to challenges when dealing with noisy labels.To address this problem, we propose a novel approach called HyperMatch. We introduce a hyper-class assignment strategy that groups noisy samples into a hyper-class, which is the union of the top-K nearest classes. This relaxation of the traditional class assignment allows for better preservation of the ground truth class within the hyper-class. In conjunction with hyper-class assignment, we develop a Relaxed Contrastive Loss that aims to constrain the features of noisy samples to be closer to their corresponding hyper-class while increasing the distance from other classes. To effectively exploit both clean and noisy unlabeled data, we integrate predicted class probabilities and semantic similarities to produce unbiased per-sample class distributions. A Gaussian Mixture Model (GMM) is then fitted on these distributions to separate clean and noisy data. The proposed method, HyperMatch, can be easily applied to various SSL architectures, improving resilience in the presence of noisy data.Extensive experiments on SSL benchmarks demonstrate that HyperMatch achieves competitive performance and outperforms existing methods. Our analysis also reveals the effectiveness of HyperMatch in handling noisy pseudo labels. Overall, this work contributes to the development of enhanced contrastive learning methods and provides insights into mitigating confirmation bias in SSL.