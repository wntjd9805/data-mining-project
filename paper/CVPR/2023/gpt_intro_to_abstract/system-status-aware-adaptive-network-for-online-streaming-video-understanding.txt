Online video understanding is a crucial task in computer vision applications such as self-driving vehicles, security surveillance, streaming services, and human-computer interactions. The ability to make predictions for each video frame in real-time is essential for these applications, but it can be challenging to achieve low delay while maintaining high performance. Previous research has focused on reducing delay in deep neural networks but has not considered the dynamic conditions of the hardware platform. In practical scenarios, the computing resources available to the model can fluctuate, leading to longer inference times and potential safety issues. In this paper, we propose a novel System-status-aware Adaptive Network (SAN) that explicitly considers the system status of the host device and adjusts its computational complexity in real-time. SAN consists of a dynamic main module and a lightweight agent that learns a system-status-aware policy. This approach allows SAN to process video streams effectively and efficiently in a dynamic system environment. To further alleviate the training burden and enable deployment on different hardware platforms, we introduce a method called Meta Self-supervised Adaptation (MSA). MSA allows for quick adaptation of SAN to new deployment devices without the need for labeled training data. We demonstrate the effectiveness of our proposed method on online action recognition and pose estimation tasks, achieving low delays under fluctuating system loads without compromising prediction quality. Overall, our contributions include the development of SAN, the introduction of MSA for deployment-time adaptation, and empirical validation of our method's performance.