Recent advancements in neural rendering have allowed for highly realistic 3D reconstructions of various scenes, with particular interest in facial reconstructions. However, current solutions often struggle with effectively capturing and representing fine-grained and expression-dependent phenomena like wrinkles. Existing parametric models and approaches lack the ability to faithfully reproduce such details and require large datasets and significant computational resources. In this paper, we propose a more efficient solution called BlendFields, which extends a previous method called VolTeMorph to accurately render facial details learned from a sparse set of expressions. We draw inspiration from blend-shape correctives to address the mismatch between low-frequency deformations captured by the base model and the high-frequency nature of expression wrinkles. Our method utilizes local blending and differential property analysis to achieve realistic and controllable renderings. Compared to other methods, BlendFields offers improved data efficiency, applicability, and interpretability. We demonstrate the effectiveness of our approach through qualitative comparisons and empirical evaluations. Additionally, our model can also be applied beyond facial modeling, such as in the representation of wrinkles on deformable objects made of rubber.