Image fusion is a fundamental topic in image processing that aims to generate informative fused images by combining important information from source images. This paper focuses on two challenging sub-categories of multi-modality image fusion (MMIF): Infrared-Visible Image Fusion (IVF) and Medical Image Fusion (MIF). IVF targets fused images that preserve thermal radiation information from infrared images and detailed texture information from visible images. This fusion helps overcome the shortcomings of visible images being sensitive to illumination conditions and infrared images being noisy and low-resolution. Similarly, MIF aims to fuse multiple medical imaging modalities to reveal comprehensive information for diagnosis and treatment assistance. Existing methods for MMIF often use CNN-based feature extraction and reconstruction in an Auto-Encoder (AE) manner. However, these methods have shortcomings in terms of insufficient extraction of cross-modality features, limited ability to extract global information, and loss of high-frequency information during fusion. To address these challenges, this paper proposes the Correlation-Driven feature Decomposition Fusion (CDDFuse) model. CDDFuse utilizes a dual-branch encoder to extract modality-specific and modality-shared features and a decoder to reconstruct the fused image. The proposed model combines the advantages of local context extraction and computational efficiency in CNNs and the advantages of global attention and long-range dependency modeling in Transformers. The paper's contributions include:1. The proposal of a dual-branch Transformer-CNN framework for extracting and fusing global and local features, enhancing the representation of modality-specific and modality-shared features.2. Refinement of CNN and Transformer blocks to better adapt to the MMIF task, using Invertible Neural Network (INN) blocks for lossless information transmission and LT blocks for trade-offs between fusion quality and computational cost.3. The introduction of a correlation-driven decomposition loss function to enforce modality shared/specfic feature decomposition, improving the correlation of cross-modality base features while decorrelating detailed high-frequency features.4. Leading image fusion performance achieved for IVF and MIF, as well as the presentation of a unified measurement benchmark to demonstrate the facilitation of downstream MM object detection and semantic segmentation tasks by IVF fusion images.