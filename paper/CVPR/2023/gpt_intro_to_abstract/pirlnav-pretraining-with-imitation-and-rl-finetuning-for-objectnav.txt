This paper introduces a new approach called PIRLNav for pretraining with behavior cloning (BC) and finetuning with reinforcement learning (RL) for Object-Goal Navigation (OBJECTNAV). OBJECTNAV is a challenging task that requires agents to navigate to specific object categories in a new environment. The authors propose using BC-pretrained policies as a starting point for RL, which improves optimization and even enables RL with sparse rewards. The paper also explores the use of alternative sources of demonstrations, such as shortest paths and frontier exploration, and compares their performance to human demonstrations. Additionally, the authors analyze the scalability of RL with different dataset sizes and identify the failure modes of their OBJECTNAV policies. Overall, the PIRLNav approach achieves a state-of-the-art success rate of 65.0% on OBJECTNAV.