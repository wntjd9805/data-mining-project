Layouts play a crucial role in various applications, including magazine pages, advertising posters, and application interfaces. Designing layouts manually to meet aesthetic goals and constraints is time-consuming. To address this, layout generation aims to automatically generate design layouts based on user-specified attributes. Generative models like Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) have shown promise in layout generation but have limitations in terms of stability, distribution coverage, diversity, and sample quality. Diffusion models like the denoising diffusion probabilistic model (DDPM) have recently emerged as powerful generative models for high-fidelity image generation. However, applying diffusion models to layout generation poses unique challenges, such as the need to handle non-sequential data structures and incorporate attribute knowledge and element relationships. To address these challenges, we propose a Transformer-based Layout Diffusion Model (LayoutDM) for conditional layout generation. LayoutDM leverages the self-attention mechanism in transformer layers to capture high-level relationship information between elements and predict noise in layout data. Additionally, LayoutDM designs a conditional Layout Denoiser (cLayoutDenoiser) using a transformer architecture to generate layouts with desired attributes. Our experimental results demonstrate the superiority of LayoutDM over state-of-the-art methods in terms of visual quality and diversity on multiple layout datasets. This paper presents a novel approach to explore the potential of diffusion models for graphic layout generation, offering improved generation quality, diversity, distribution coverage, and scalability.