Deep neural networks (DNNs) have greatly advanced computer vision tasks such as image recognition and object detection. However, our understanding of their theoretical workings is lacking. Previous attempts to improve this have focused on designing principled architectures using kernel methods or fixed components. One such approach, Polynomial Nets (PNs), captures higher-order correlations without elementwise activation functions. However, the performance of PNs falls short of standard baselines like Residual Neural Networks (ResNet). To bridge this gap, we propose a class of regularized polynomial networks called R-PolyNets, which achieve performance on par with ResNet. Our study explores regularization schemes that improve the performance of PNs, and we introduce a new class of polynomial expansions called D-PolyNets. These expansions increase the total degree of polynomial expansion by concatenating lower-degree polynomials. We show that D-PolyNets are more expressive than previous polynomial expansions. Our contributions include the introduction of R-PolyNets, the proposal of D-PolyNets, and a thorough validation of their performance in image and audio recognition tasks.