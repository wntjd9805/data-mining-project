Convolutional neural networks (CNNs) have been successful in various vision tasks but struggle with generalizing to new domains due to the domain shift problem. Unsupervised domain adaptation (UDA) aims to solve this issue by transferring knowledge from a labeled source domain to an unlabeled target domain. Many existing solutions focus on category-level alignment, such as metric learning, adversarial training, and optimal transport. Some recent works explore the potential of Vision Transformer (ViT) for UDA. However, these approaches have limitations, particularly in cases where the domain gap is large. In this paper, we propose a new approach for UDA that bridges the source and target domains by constructing an intermediate domain using a ViT-based solution. We introduce a novel method called PMTrans (PatchMix Transformer) which interprets domain alignment as a min-max cross entropy game involving a feature extractor, a classifier, and a PatchMix module. The PatchMix module builds the intermediate domain by learning to sample patches from both domains using a learnable Beta distribution. We propose semi-supervised mixup losses to minimize the cross entropy between the intermediate domain and the source/target domain. We show through experiments on benchmark datasets that PMTrans outperforms existing ViT-based and CNN-based methods in UDA. Our contributions include the proposal of a novel ViT-based UDA framework, the introduction of the PatchMix module, and the demonstration of improved performance on benchmark datasets.