Generative Adversarial Networks (GANs) like StyleGAN2 can generate high-resolution images that are almost indistinguishable from real images. However, the ability to generate realistic images also raises concerns about potential misuse and misinformation. GAN-generated human faces are readily available and have been used to create fake identities online. Detecting GAN-generated images is an active research area, with various approaches using custom methods or generic CNN-based classifiers. These classifiers reveal consistent knowledge gaps left by discriminators during GAN training, which can be exploited by specialized classifiers to detect generated images. In this work, we modify the GAN training loss to deceive GAN-classifiers and examine the impact on training dynamics and output quality. We conduct experiments using both low-dimensional handwritten digits and high-dimensional human faces datasets. Our findings show that the artifact space exploited by classifiers is strongly correlated among different GAN generator instances. While the DCGAN struggles to generate high-quality outputs when faced with the need to fool classifiers, the StyleGAN2 generators easily fool the classifiers and move to a new artifact space. However, different classifier architectures learn different subsets of artifacts, indicating a strong dependence on the classifier's architecture. These insights into GAN optimization and classifier behavior contribute to better understanding and addressing the challenges posed by GAN-generated images.