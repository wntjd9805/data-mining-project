Deep neural networks (DNNs) have demonstrated impressive performance in various computer vision tasks such as image classification, object detection, and semantic segmentation. However, recent studies have shown that these deep models often provide overconfident predictions, leading to limited trust in their outputs, particularly in safety-critical applications. One of the underlying reasons for this miscalibration is the training with zero-entropy supervision, which inadvertently makes the models overconfident. Previous attempts to improve calibration include post-processing methods and train-time calibration methods but have primarily focused on classification tasks. The calibration of object detection models, which play a crucial role in safety-critical applications like self-driving vehicles, has not been extensively explored. Moreover, existing methods predominantly target in-domain predictions and lack robustness when faced with out-domain data. In this paper, we propose a novel train-time auxiliary loss formulation that aims to bridge the precision of object detection models with their predicted class confidence. Our approach leverages the count of true positives and false positives in a minibatch to construct a penalty for miscalibrated predictions. We conduct extensive experiments on both in-domain and out-domain scenarios, including the MS-COCO benchmark, and demonstrate that our train-time auxiliary loss significantly improves the calibration of a state-of-the-art vision-transformer based object detector under different scenarios.