Computer vision tasks, such as recognition, depth estimation, and edge detection, often have relationships that can benefit from shared features. However, some tasks, like tumor detection in medical images and face recognition, appear to be unrelated in terms of feature sharing. Multi-task learning (MTL) aims to model the relationships among tasks and build a unified model that can optimize experts for both cooperation and specialization. This paper introduces Mod-Squad, a new model that uses a Mixture of Experts (MoE) approach to create a modularized MTL system. This design allows experts to cooperate on tasks when helpful and specialize in specific tasks without interfering with each other. The paper addresses two challenges in MTL: gradient conflicts across tasks and designing architectures with high accuracy and computational efficiency. Mod-Squad integrates MoE layers into a vision transformer backbone network, dividing the model into groups of experts that can share or specialize in tasks. The paper introduces a novel loss function to induce a specific structure in the distribution of tasks to experts. Unlike previous MoE methods that apply a load-balancing loss, Mod-Squad leverages cooperation in some tasks and creates a subset of experts that learn specific features without interference. The paper demonstrates that the model converges to a state where most experts are specialized for particular tasks but are still balanced in activation frequency. This property allows for the extraction of compact sub-networks for each task without performance drop. The paper's contributions include the modular multi-task learner, optimizing the joint distribution between tasks and experts, effective and efficient multi-task learning at scale, and extracting small sets of experts as standalone models without sacrificing performance. Experimental results show that Mod-Squad achieves state-of-the-art performance on multi-task datasets while maintaining computational efficiency.