In recent years, there has been significant progress in studying cell functions and behaviors at the subcellular level. Fluorescence staining has become a popular technique for visualizing subcellular structures, but it is expensive, time-consuming, and can be detrimental to living cells. In this paper, we propose a deep learning task called subcellular structure prediction (SSP), which aims to predict the 3D fluorescent images of multiple subcellular structures from a 3D transmitted-light image. This task presents two challenges: partial labeling of images and the multi-scale nature of subcellular structures. Traditional approaches to SSP, such as Multi-Net and Multi-Head, have limitations in label-efficiency and scale-inflexibility. To address these issues, we introduce RepMode, an all-shared network that can dynamically organize its parameters with task-aware priors. RepMode is constructed using Mixture-of-Diverse-Experts (MoDE) blocks, which consist of task-agnostic experts with diverse configurations. Gating re-parameterization (GatRep) is employed to generate specialized parameters for each task. By doing so, RepMode fully utilizes partially labeled data and adapts to the multi-scale nature of SSP without manual intervention. Our experiments demonstrate that RepMode achieves state-of-the-art performance in SSP and can be easily extended to new tasks without degradation of performance. Overall, our contributions include the proposal of RepMode, the design of the MoDE block, and comprehensive experiments validating the effectiveness of RepMode in SSP.