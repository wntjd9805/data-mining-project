Deep neural networks have shown success in class-fixed classification problems, where object classes remain constant during training and testing. However, in practical scenarios, these models are often used in dynamic environments that require them to learn and identify new classes. This challenge, known as Class-Incremental Learning (CIL), is significant. Previous studies have attempted to address CIL using knowledge distillation and dynamic expandable networks, but challenges remain, including feature degradation, stability-plasticity dilemma, and computational resources. In this paper, we propose a new approach called Diverse Knowledge Transfer Transformer (DKT) that utilizes attention blocks and a duplex classifier. The attention blocks, GKAB and SKAB, transfer previous knowledge and mitigate feature degradation. The duplex classifier maintains stability on old categories while learning new categories. We also introduce a cluster-separation loss to encourage diverse task-specific knowledge. Extensive experiments on CIFAR100, ImageNet100, and ImageNet1000 benchmarks show that DKT outperforms other methods, achieving state-of-the-art performance with fewer parameters. Our key contributions include the DKT framework, the duplex classifier, the cluster-separation loss, and the experimental validation.