Analyzing and understanding human actions in videos is crucial for various applications, such as human-robot interaction and healthcare. Previous approaches have been successful in locating and analyzing activities in videos using action localization, segmentation, and recognition. However, these approaches rely on fully temporal supervision, which requires annotating the start and end frames of each action. As an alternative, timestamp-supervised temporal action segmentation (TSTAS) has been explored, where each action segment is annotated with only one frame in the untrimmed video.Most previous methods follow a two-step pipeline of initializing and refining the model using pseudo-labels generated from sparse single-timestamp annotations. However, these methods fail to capture the semantics of entire action segments due to label bias, including focus bias and representation bias. The initialization phase relies on the Naive approach, which focuses on frames distributed over segments of various action categories similar to the annotated frames. During the refinement phase, pseudo-labels generated from unlabeled frames suffer from representation bias, as single frames may not accurately represent the entire action segment.To address these issues, we propose a novel framework called Debiasing-TSTAS (D-TSTAS), which includes masked timestamp predictions and center-oriented timestamp expansion approaches. The Masked Timestamp Predictions (MTP) approach removes the model's dependency on annotated frames by masking input features, forcing the model to reconstruct the annotated frames and predict their action categories using contextual information. Additionally, the Center-oriented Timestamp Expansion (CTE) approach expands pseudo-timestamp groups by selecting semantically dissimilar center frames, capturing more semantics than single annotated timestamps.Our contributions include studying label bias in TSTAS and proposing the D-TSTAS framework to reduce focus and representation bias. The masked timestamp predictions approach alleviates dependencies on timestamps, while the center-oriented timestamp expansion progressively expands pseudo-timestamp groups to contain more semantic-rich motion representations. Experimental results on benchmark datasets show that D-TSTAS outperforms state-of-the-art TSTAS approaches and achieves competitive results compared to fully supervised approaches.