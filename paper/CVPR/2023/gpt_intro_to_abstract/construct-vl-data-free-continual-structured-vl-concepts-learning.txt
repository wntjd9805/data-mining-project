Large Vision-and-Language (VL) models have made significant advancements in zero-shot learning by being pre-trained on vast amounts of image-and-text pairs. However, there are limitations in the understanding of non-object textual content, known as Structured VL Concepts (SVLCs), which can lead to errors in various applications. To address this, we propose the Continual Data-Free Structured VL Concepts Learning (ConStruct-VL) benchmark and a novel data-free continual learning method called Adversarial Pseudo-Replay (APR). APR generates negative examples for past task models using adversarial attacks, reducing forgetting during training. We also introduce the Layered-LoRA (LaLo) architecture, which allows efficient invocation of past task models without additional memory cost. Our proposed approach shows significant improvements in accuracy and reduced forgetting compared to existing data-free continual learning methods.