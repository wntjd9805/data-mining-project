The task of saliency prediction involves estimating the most prominent area in a scenario by simulating human selective attention. This has become an important way to extract valuable information from large amounts of data, with applications in fields such as robotic camera control, video captioning, motion tracking, image quality evaluation, and video compression. Saliency prediction works have seen increasing attention in recent years, with studies focused on both Image Saliency Prediction (ISP) and Video Saliency Prediction (VSP). ISP aims to predict prominent areas in images by combining low-level heuristic characteristics with high-level semantic attributes, while VSP explores how to leverage spatio-temporal structure information in videos for the perception and identification of dynamic scenes.One area of interest in recent research is the use of audio information to enhance video understanding. Studies have shown that audio can significantly improve the prediction accuracy of video saliency by exploring audio-visual correspondence clues and combining spatio-temporal visual and auditory information. However, these approaches often rely on temporal consistency between visual and audio information, which may not hold in practical scenarios. Real-life videos often contain multiple sound sources, leading to temporal inconsistency between the visual and audio components.To address this challenge, this study proposes the Consistency-aware Audio-visual Saliency Prediction network (CASP-Net), inspired by neuroscience principles. CASP-Net explores the latent semantic correlations between cross-modal signals to minimize matching errors within multi-sensory data. It introduces a two-stream network to associate video frames with their corresponding sound source, enabling cross-modal interaction and achieving semantic similarities between audio and visual features. To enhance the consistency within audio and visual representations, a consistency-aware predictive coding module is designed. Finally, a saliency decoder is used to aggregate multi-scale audio-visual information and generate the final saliency map.The main contributions of this work include the proposal of a novel audio-visual saliency prediction model that considers the functionalities of audio-visual semantic interaction and consistent perception. It also introduces a consistency-aware predictive coding module to improve the consistency within audio and visual representations. The proposed method is evaluated on six audio-visual eye-tracking datasets and demonstrates superior performance compared to other state-of-the-art methods.