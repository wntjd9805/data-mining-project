This paper addresses the problem of automatically recovering a sequence of human meshes from a monocular video, which is important for various applications in computer graphics, AR/VR, and robotics. The current approach involves regressing the parameters of parametric human models using deep neural networks. However, effectively integrating these models with deep neural networks to improve estimation accuracy is still an open problem. The paper proposes a new approach called the Global-to-Local Transformer (GLoT) that decouples the modeling of long-term and short-term correlations in video-based 3D human pose and shape estimation. GLoT utilizes global and local transformers to capture inter-frame global-local contexts and intra-frame human mesh structure. Experimental results show that GLoT outperforms previous methods in terms of accuracy and efficiency while achieving the lowest model parameters. This work contributes to the field by introducing a novel approach for recovering human meshes from videos and improving the accuracy of estimation.