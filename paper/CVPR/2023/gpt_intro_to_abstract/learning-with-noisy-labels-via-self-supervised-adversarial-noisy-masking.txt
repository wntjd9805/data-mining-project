Deep learning has achieved great success but relies on expensive and time-consuming human-annotated labels. Cheaper methods like web crawling and machine-generated annotations result in numerous noisy samples. Previous research has shown that deep networks can overfit to noisy labels, leading to poor generalization performance. Various Learning with Noisy labels (LNL) approaches have been proposed, but they often fail to leverage the information of discarded samples or make assumptions that are not always met in real-world datasets. Regularization-based methods have been proposed to alleviate the side effects of label noise, but most of them are designed for fully-supervised learning tasks with correctly-labeled samples. In this paper, we propose a novel regularization-based method called Self-supervised Adversarial Noisy Masking (SANM) to address the LNL problem. Our method is inspired by the differences in activation maps between models trained with clean and noisy labels. We conduct experiments with different masking strategies and show that the performance gain is sensitive to the masking strategy. To address this, we propose a label quality guided adaptive masking strategy that modulates the image label and the mask ratio simultaneously. We also design an auxiliary decode branch for image reconstruction to provide noise-free self-supervised information. The proposed SANM method is flexible and can be used in conjunction with other LNL methods. We test it on synthetic and real-world noisy datasets and conduct ablation experiments to validate each design component. The key contributions of this work are the proposal of SANM to impose regularization for LNL, a label quality guided masking strategy, and a self-supervised mask reconstruction auxiliary task.