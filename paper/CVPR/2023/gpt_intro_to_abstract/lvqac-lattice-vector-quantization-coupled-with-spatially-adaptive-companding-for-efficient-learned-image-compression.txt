In recent years, image compression using Convolutional Neural Networks (CNN) has gained significant attention. These methods, operating under the paradigm of linear transform, quantization, and entropy coding, have shown promising results in terms of rate-distortion performance. However, the use of uniform scalar quantization in these CNN-based compression frameworks has been motivated by operational expediency rather than optimization. Adopting and optimizing vector quantizers (VQ) in the end-to-end CNN architecture design for compression is challenging due to its discrete nature and incompatibility with variational backpropagation. Previous methods have attempted to address this challenge by introducing soft-to-hard vector quantization schemes. In this paper, we propose a novel approach called Lattice Vector Quantization coupled with spatially Adaptive Companding (LVQAC) mapping to enhance the compression performance. LVQAC can better exploit inter-feature dependencies, making it more efficient than scalar uniform quantization. Furthermore, the incorporation of LVQAC with a checkerboard context model in the end-to-end CNN compression model improves rate-distortion performance without increasing model complexity. Experimental results demonstrate that LVQAC achieves similar performance to more expensive auto-regressive context models used in CNN-based compression methods.