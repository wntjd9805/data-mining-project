Recovering 3D human pose and shape from monocular input is a challenging problem in computer vision. Although deep neural networks have shown promise in this area, existing methods still struggle in complex real-world scenarios where people are often occluded and truncated. Most state-of-the-art approaches rely on pixel-aligned local evidence to achieve high accuracy in non-occlusion cases. However, these methods fail when mesh-image correspondences are unavailable due to occlusions. On the other hand, direct regression approaches that predict pose and shape parameters with neural networks are more robust to occlusions but suffer from image-mesh misalignment. In this paper, we propose a novel algorithm called NIKI that combines inverse kinematics and invertible neural networks to improve robustness to occlusions while maintaining pixel-aligned accuracy. NIKI models the bi-directional pose error and enforces zero-error boundary conditions to achieve mesh-image alignment and improve plausibility in occlusion scenarios. We benchmark NIKI on various datasets, including one with extremely challenging occlusions, and demonstrate its state-of-the-art performance in both occlusion and non-occlusion scenarios. Our contributions include the introduction of an error-aware inverse kinematics algorithm, decoupling of error information from plausible human poses, and outperforming previous approaches in both occlusion and non-occlusion benchmarks.