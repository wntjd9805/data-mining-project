Scene synthesis plays a crucial role in applications like virtual reality, augmented reality, computer graphics, and game development. Traditional scene synthesis approaches involve reconstructing scenes based on given observations like multi-view images or point clouds. With the increasing use of RGB/RGBD scanning devices, scene reconstruction from multi-view images has gained popularity. However, existing methods like Neural Radiance Fields (NeRFs) have limitations in generating or inferring missing parts of incomplete input scenes. While some studies have attempted to equip NeRFs with generative capabilities, their representation is typically limited, which hampers their ability to capture fine-grained details. In this paper, we introduce a novel task of generative scene synthesis from sparse RGBD views. Our goal is to learn across multiple scenes and enable scene synthesis from a sparse set of multi-view RGBD images. This task poses multiple challenges, such as preserving observed regions, hallucinating missing parts, eliminating additional computational costs during inference, ensuring 3D consistency, and maintaining scalability to scenes of variable scales.To address these challenges, we propose a method that combines straightforward reconstruction with view completion and diffusion models. We found that RGBD diffusion models simplify the training complexity by inpainting missing pixels while preserving known regions. We also employ back-projection, which eliminates the need for optimization and enhances test-time efficiency. To ensure consistency among multi-view images, we use an intermediate mesh representation. Additionally, we handle scenes of variable sizes by utilizing images with freely designated poses as input.Our proposed approach involves generating multi-view consistent RGBD views using an intermediate mesh, inpainting the images using a diffusion model, and transforming the RGBD views into 3D partial meshes via back-projection. This process is repeated until all test-time camera viewpoints are covered, resulting in a complete intermediate scene mesh. The final output is the mesh acquired from the last step.We evaluate our approach on the ScanNet dataset and demonstrate its superiority over existing solutions for scene synthesis from sparse RGBD inputs.