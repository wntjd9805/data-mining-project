In this paper, we address the problem of reconstructing a salient foreground object from multi-view images and automatically segmenting it from the background without any annotation. This task is important for scalable 3D content creation for virtual reality/augmented reality applications and has the potential to generate large-scale 2D and 3D object annotations for supervised-learning tasks. Traditional multi-view stereo and recent neural scene reconstruction methods have achieved impressive reconstruction quality but fail to identify objects and produce models that are coupled with the surrounding background. To overcome this limitation, we propose a two-stage framework for fully-automated 3D reconstruction of salient objects. In the first stage, we perform coarse decomposition to segment the foreground structure-from-motion (SfM) point cloud. We leverage the semantic features provided by a self-supervised 2D Vision Transformer to segment the salient foreground points with a point cloud segmentation Transformer. To train the Transformer, we devise a pseudo-ground-truth generation pipeline based on Normalized Cut. In the second stage, we reconstruct the foreground object geometry by learning an implicit neural scene representation within the estimated foreground bounding box. We use explicit regularization provided by the previously decomposed point cloud to guide the reconstruction. Experimental results on multiple datasets validate the effectiveness of our proposed framework in automatically recovering accurate 3D object models and high-quality segmentation masks even in the presence of cluttered backgrounds. Our contributions include the fully-automated framework, the coarse-to-fine pipeline for scene decomposition, the SfM point cloud segmentation Transformer, and the possibility of automatically creating object datasets with 3D models, bounding boxes, and segmentation masks.