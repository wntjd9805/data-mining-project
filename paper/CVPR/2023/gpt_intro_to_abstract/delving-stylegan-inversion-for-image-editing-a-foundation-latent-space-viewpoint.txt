StyleGAN has achieved significant success in image generation, particularly in attribute-based image editing. GAN inversion techniques, such as projecting an input image into the latent space, have enabled various image editing methods. However, finding the inversion space that allows for distortion-free editing has been a challenge. The widely explored inversion spaces include the latent space W+ and the feature space F. While W+ balances distortion and editability, F contains spatial image representation.In this paper, we propose a two-step design to improve the representation ability of the latent code in W+ and F. Firstly, we use a contrastive learning paradigm to align the image space and the foundation latent space W. This involves training two encoders to obtain feature representations of images and their corresponding latent codes, and aligning these features through contrastive learning. This approach improves upon existing studies by focusing on the alignment between the input image and its latent code in W.After discovering the proper latent code in W, we leverage a cross-attention encoder to transform it into representations in W+ and F. This enables improved editability and reconstruction ability. The cross-attention mechanism ensures that the transformed latent code closely aligns with the original code, preserving its editability. We demonstrate the effectiveness of our approach, named CLCAE, in achieving state-of-the-art performance in both reconstruction quality and editing capacity on benchmark datasets.In summary, our contributions include proposing a novel contrastive learning approach to align the image space and the foundation latent space of StyleGAN, and introducing a cross-attention encoder to improve the representations in W+ and F. Experimental results showcase the superiority of our CLCAE approach in terms of fidelity and editability.