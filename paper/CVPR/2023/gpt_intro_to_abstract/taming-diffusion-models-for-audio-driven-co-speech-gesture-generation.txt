Co-speech gestures are non-verbal behaviors that humans use during conversations to express their thoughts and enhance understanding. These gestures are important for animating virtual avatars in embodied AI. Previous research has focused on audio-driven co-speech gesture generation, synthesizing human upper body gestures aligned with speech audio. Early approaches treated this as a searching-and-connecting problem, while recent data-driven approaches have used deep neural networks to learn the mapping from audio to human skeletons. However, these approaches often result in dull or unreasonable poses. The recent development of diffusion probabilistic models provides a new perspective for realistic generation, but adapting them for co-speech gesture generation is challenging. Most existing diffusion models deal with static data and lack temporal coherence, and the commonly used denoising strategy leads to temporal inconsistency. To address these challenges, we propose a tailored Diffusion Co-Speech Gesture framework called DiffGesture. Our framework formulates the generation process as a diffusion-conditional process on skeleton and audio clips, gradually adding noise to the gestures and then denoising them using conditional context features from the audio. We also propose a Diffusion Audio-Gesture Transformer architecture to model long-term temporal dependencies, and a Diffusion Gesture Stabilizer module to eliminate temporal inconsistency. Our experiments on benchmark datasets demonstrate that our framework produces coherent and high-fidelity co-speech gestures with strong audio correlations, outperforming existing approaches. Our contributions include formalizing the diffusion and denoising process for gesture generation, introducing the Diffusion Audio-Gesture Transformer for handling sequential modalities, and proposing the Diffusion Gesture Stabilizer for improving temporal consistency.