The paper introduces a model for understanding and predicting the temporal ordering of action steps in procedural activities by learning from instructional videos and their narrations. The model combines a video representation, learned using a Transformer network, with a deep probabilistic model designed to capture temporal dependencies and variations among steps. The model is trained using only videos and their narrations from automatic speech recognition, without any manual annotations. The trained model supports zero-shot inference, including step recognition and future step forecasting, and allows for sampling multiple video representations for a missing action step. The model is evaluated on public benchmarks, demonstrating its effectiveness in step forecasting and classification tasks. The paper's contributions include leveraging video-and-language pre-training to capture the temporal ordering of action steps, the design of a deep probabilistic model using a diffusion process, and establishing new state-of-the-art results in step classification and forecasting tasks.