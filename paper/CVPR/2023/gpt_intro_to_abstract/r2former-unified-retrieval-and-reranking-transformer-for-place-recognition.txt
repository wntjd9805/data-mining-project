Abstract:Visual Place Recognition (VPR) plays a crucial role in localizing query images by matching them with reference images from known locations. In this paper, we propose a unified retrieval and reranking framework called R2Former, which employs transformers for efficient and effective VPR. Unlike previous methods that primarily use geometric verification for reranking, we utilize a data-driven module that learns task-relevant information from the correlation between tokens and attention information. Our framework achieves state-of-the-art performance on various VPR datasets while significantly reducing inference time and memory consumption. Furthermore, we compare the performance of vision transformer tokens with CNN local features, demonstrating their comparable performance for local matching and reranking. Overall, our contributions include the development of a unified framework, a novel transformer-based reranking module, and extensive experiments showcasing superior performance with improved efficiency for VPR applications.