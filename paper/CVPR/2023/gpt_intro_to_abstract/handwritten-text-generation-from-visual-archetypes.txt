This paper introduces an approach for styled handwritten text generation (HTG) that aims to mimic the calligraphic style of a specific writer. The practical applications of this research area include training data synthesis for personalized Handwritten Text Recognition (HTR) models and automatic generation of handwritten notes for physically impaired individuals. Existing approaches in HTG have focused on trajectory-based or image-based methods, with the latter being more suitable for offline generation. The authors propose a few-shot styled offline HTG task, where only a few example images of the writer's style are available. State-of-the-art approaches in this scenario typically use encoder-decoder generative models, such as Generative Adversarial Networks (GANs) or Transformer models, to generate styled text images. However, these models rely on a fixed charset represented by one-hot vectors, limiting the representation and generation of rare characters. To address this limitation, the authors propose a Visual Archetypes-based Transformer (VATr) approach that represents characters as continuous variables and uses them as query content vectors in a Transformer decoder. The approach leverages the visual archetypes of characters obtained from a large charset font and employs a pre-trained backbone for style representation, leading to strong style representations, especially for unseen styles. The effectiveness of the proposed approach is evaluated through extensive experimentation and comparison with existing generative models, demonstrating its ability to generate both common and rare characters in seen and unseen styles. The code and trained models for VATr are made publicly available.