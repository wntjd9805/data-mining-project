The ability to detect objects in the visual world is essential for computer vision and machine intelligence. However, current object detectors have limited vocabulary size due to manual annotations of regions and class labels. This limitation hinders their ability to adapt to new environments and handle fine-grained user queries. To overcome this, the open-vocabulary detection task has been proposed, which leverages abundant image-text pairs for training and ingesting text queries from users. Most existing works use image-text pre-training, but there is a lack of utilization of object/region information. In this paper, we present RO-ViT, a recipe for pretraining vision transformers specifically for open-vocabulary object detection. We introduce a novel positional embedding scheme called "Cropped Positional Embedding" that better matches the use of region crops in detection fine-tuning. We also replace the softmax cross entropy loss with focal loss in contrastive learning to learn from harder and more informative examples. Additionally, we utilize recent advances in novel object proposals to improve open-vocabulary detection fine-tuning. Our approach achieves state-of-the-art results on the LVIS open-vocabulary benchmark, surpassing existing approaches. We also outperform the best existing ViT-based approach by a significant margin on the COCO benchmark. Moreover, our model performs well on image-text retrieval benchmarks and outperforms strong baselines. We believe our findings will contribute to further research on open-vocabulary detection and have potential benefits for both region-level and image-level tasks.