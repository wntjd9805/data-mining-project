The introduction discusses the advancements in large-scale vision and language pre-training techniques, which have led to significant breakthroughs in object recognition. These techniques involve learning a mapping between image and text spaces to improve representational power and address the limitations of visual-only recognition methods. However, multi-modal models still face limitations related to sensitivity to class-prompt textual content and difficulty in adapting to new data. In this paper, the authors propose a solution that leverages textual inversion to adapt models to new data and improve label accuracy. This approach has several advantages, including maintaining open-set capabilities, preventing forgetting under sequential training, and improving interpretability. Experimental results demonstrate the effectiveness of this approach in improving model performance, adaptation, and interpretability, across multiple classification and detection datasets. The main contributions of this paper are a simple and data-efficient adaptation method for vision-language models, performance improvements in conjunction with prompt tuning, and insights into model interpretability.