Zero-Shot Learning (ZSL) aims to generalize an image classification model trained on seen classes to unseen classes using auxiliary information. Most existing works rely on fixed, human-labeled attributes, but these are expensive and difficult to scale. Unsupervised alternatives using word embeddings have limited information. Recent works have shown that text documents from internet sources like Wikipedia can provide valuable auxiliary information. However, these methods rely on a single source, which may not adequately represent all classes. Multiple sources of text documents can provide complementary information, but require additional annotation effort. In this paper, we propose using Large Language Models (LLM) trained on web-scale text to generate multiple text descriptions, or "views", of a class. We demonstrate that LLMs can act as a mixture of annotators and generate complementary information. Additionally, we introduce I2MVFormer, a transformer-based model that utilizes our memory-efficient summary modules to extract discriminative information from each view and learn a multi-view class embedding. Our contributions include the first study on using LLMs for generating auxiliary information in ZSL, the prompting strategy for extracting multiple descriptions, and the proposed I2MVFormer model. Experimental results on three benchmark datasets show that I2MVFormer achieves significant performance gains, establishing a new state-of-the-art in unsupervised class embeddings for ZSL.