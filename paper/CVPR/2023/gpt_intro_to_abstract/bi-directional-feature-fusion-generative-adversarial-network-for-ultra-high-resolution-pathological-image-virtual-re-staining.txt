Pathological examination is commonly used for cancer diagnosis, with hematoxylin-eosin (HE) staining being the most basic method. However, additional immunohistochemistry (IHC) staining is often needed for accurate diagnosis, despite being complex and expensive. To reduce costs and time, researchers have explored virtual restaining techniques by generating one type of staining image from another using computational methods. These techniques are usually done through unsupervised methods, similar to style transfer in natural images. However, pathological images have distinct characteristics such as higher resolution and the need for reliable results. Existing virtual restaining models address the issue of resolution by splitting whole-slide imaging (WSI) images into smaller patches for training and inference, resulting in color discrepancies between patches known as the square effect. In this paper, we propose a model called BFF-GAN that combines global and local features to address the square effect and bypass the memory constraint for ultra-high resolution images. This is achieved through a network architecture consisting of a global branch and a local branch, which perform feature fusion and use patch-wise attention and skip connections. We evaluate our model on private and public datasets, demonstrating its effectiveness in eliminating the square effect and generalizing to various datasets. Our contributions include proposing a solution for the square effect, introducing BFF-GAN as the first network for style transfer of ultra-high resolution images, and achieving impressive results with clinical significance.