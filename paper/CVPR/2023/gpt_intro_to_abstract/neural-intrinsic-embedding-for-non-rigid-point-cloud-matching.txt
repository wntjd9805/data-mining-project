Estimating non-rigidly aligned point clouds is crucial for various computer vision and graphics applications. Traditional methods use extrinsic or intrinsic approaches to model non-rigid deformations. Extrinsic methods approximate global deformations through local transformations but lack global structures. Intrinsic methods transform coordinates into an alternative representation using spectral embeddings but are computationally inefficient and sensitive to artifacts. This paper proposes a learning-based framework called Neural Intrinsic Embedding (NIE) to embed point clouds into a high-dimensional space. NIE learns embeddings that approximate geodesic distances on the underlying surface, satisfying the desiderata of intrinsic geometry awareness, computational efficiency, and robustness to point cloud artifacts. The NIE framework uses DGCNN as a backbone and addresses learning issues such as rank deficiency and sensitivity to point sampling density. The paper also introduces Neural Intrinsic Mapping (NIM), a weakly supervised learning framework for non-rigid point cloud matching, which replaces spectral embeddings with NIE and learns optimal features using a self-supervised loss. The proposed pipeline for weakly supervised non-rigid point cloud matching achieves comparable or better performance than competing baselines, shows sensible generalization, and is robust to artifacts.