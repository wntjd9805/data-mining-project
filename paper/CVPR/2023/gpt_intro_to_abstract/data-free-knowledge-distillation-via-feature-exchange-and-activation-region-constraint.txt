In this paper, we propose a novel data-free knowledge distillation (DFKD) method for training lightweight student models that can imitate the capabilities of pre-trained teacher models. Traditional DFKD methods assume that the teacher's training set is accessible to the student, but in real-world applications, there may be constraints on accessing the original training data, such as privacy concerns or copyright issues. Our DFKD method overcomes these constraints by generating synthetic data instead. Our method consists of two main components: synthetic data generation and constraint design between the student and teacher networks. We compare two types of synthetic data generation methods: noise image optimization-based methods and generative network-based methods. While the former can generate an infinite number of independent images, they are time-consuming and lack diversity. The latter can generate images much faster but have limited diversity. To enhance the diversity of the synthetic training data and improve knowledge transfer, we propose the use of channel-wise feature exchange (CFE) and multi-scale spatial activation region consistency (mSARC) constraints. CFE allows us to leverage the features of early synthetic images to generate more diverse training data. However, CFE also amplifies unwanted noise, so we introduce the mSARC constraint to address this issue. This constraint enables the student network to learn discriminative cues from similar regions used by the teacher network, overcoming the limitations of traditional KL divergence loss when applied to synthetic images. We evaluate our method on various datasets and compare it to state-of-the-art DFKD methods. Our approach demonstrates superior performance and achieves comparable results to models trained using original training data. We also evaluate our method on subsets of ImageNet, validating its efficacy in generating high-resolution synthetic images for distillation learning. Overall, our proposed DFKD method offers a solution to the challenges posed by accessing original training data and improves the performance of lightweight student models in knowledge distillation.