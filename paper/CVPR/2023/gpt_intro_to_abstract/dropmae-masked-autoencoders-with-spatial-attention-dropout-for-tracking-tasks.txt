Transformers have achieved significant success in various research domains, including natural language processing (NLP), computer vision, and audio generation. In NLP, masked autoencoding is commonly used to train large-scale transformers with billions of parameters. Inspired by the success of self-supervised learning in NLP, recent advancements in computer vision suggest that training large-scale vision transformers may follow a similar trajectory. A notable work in computer vision, MAE, reconstructs the input image from patches and has proven effective in tasks such as image classification and object detection.In the field of video object tracking (VOT), two recent works have explored using an MAE pre-trained ViT model as the tracking backbone, achieving state-of-the-art performance without complicated tracking pipelines. The success of these approaches lies in the robust pre-training weights learned by MAE on ImageNet. Additionally, it has been demonstrated that MAE unsupervised pre-training on ImageNet yields better results for VOT than supervised pre-training using class labels.This paper introduces DropMAE, which performs spatial-attention dropout in frame reconstruction to enhance effective temporal correspondence learning in videos. The authors conduct evaluations on competitive VOT and VOS benchmarks, achieving state-of-the-art performance without complicated online updating or memory mechanisms. The contributions of this work include being the first to investigate masked autoencoder video pre-training for temporal matching-based tasks, proposing DropMAE for adaptive spatial-attention dropout, and setting new state-of-the-art performance on various video tracking and segmentation benchmarks.