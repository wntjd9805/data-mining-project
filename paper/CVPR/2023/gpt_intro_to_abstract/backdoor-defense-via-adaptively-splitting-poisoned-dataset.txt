Backdoor attacks pose a significant threat to deep neural networks, particularly in safety-critical scenarios like autonomous driving. To mitigate these threats, backdoor defenses have been explored, with post-processing and training-time defenses being the two main approaches. However, the use of external data for training without security guarantees makes backdoor attacks feasible. In this paper, we introduce a unified framework for training-time backdoor defenses that involves splitting the poisoned dataset into clean and polluted data pools. We analyze two representative training-time defenses, ABL and DBD, and identify their limitations. To address these limitations, we propose an adaptively splitting dataset-based defense (ASD), which leverages meta-learning-inspired split to effectively separate clean and poisoned samples. We also utilize semi-supervised learning to leverage the semantic information of poisoned data without labels and avoid backdoor injection. Our comprehensive experimental results demonstrate the effectiveness and superiority of ASD compared to previous state-of-the-art backdoor defenses. Our contributions include the unified framework, the fast pool initialization method, and the adaptive updating of data pools using loss-guided split and meta-split.