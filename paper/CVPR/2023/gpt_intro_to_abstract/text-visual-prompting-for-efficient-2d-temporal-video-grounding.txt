Temporal video grounding (TVG) has seen significant advancements in recent years, with the use of fine-grained dense 3D visual features extracted by 3D convolutional neural networks (CNNs). However, the high cost of 3D feature extraction limits the practicality of these models. In this paper, we propose a novel text-visual prompting (TVP) framework for training TVG models using more efficient 2D visual features. Our framework leverages trainable 2D CNNs as the vision encoder to extract features from sparsely-sampled video frames and employs a universal set of frame-aware visual prompts and text prompts for end-to-end regression-based modeling. We demonstrate that our proposed framework achieves comparable performance to 3D TVG methods with significant inference time acceleration. We utilize input perturbation patterns called prompts in both visual inputs and textual features, optimizing them during training and applying them to all test-time videos during testing. Our framework combines transformer-based models with prompt learning to compensate for the loss of spatiotemporal information in 2D visual features. We also introduce the Temporal-Distance IoU (TDIoU) loss for training our framework. Our work stands out by focusing on boosting regression-based TVG methods using 2D CNNs as the vision encoder and utilizing prompts to enhance the utility of sparse visual features. Experimental results on the Charades-STA and ActivityNet Captions datasets demonstrate the effectiveness and efficiency of our proposed framework, outperforming existing 2D methods and achieving competitive performance compared to 3D TVG methods.