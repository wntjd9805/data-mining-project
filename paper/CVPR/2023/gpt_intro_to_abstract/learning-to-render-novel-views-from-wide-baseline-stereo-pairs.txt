The paper focuses on the problem of novel view synthesis, which involves rendering images of a scene from unseen camera viewpoints based on a set of image observations. Previous techniques have achieved near photorealistic results by exploring every part of the scene from multiple angles. However, this approach requires a large number of densely captured images. In contrast, this paper explores the possibility of synthesizing novel view images from a sparse set of observations. Specifically, the paper proposes a system that uses only a single wide-baseline stereo image pair as input. This sparse set of observations poses a challenge in computing 3D geometry and appearance, as many 3D points are only observed from one camera perspective. To address this, the paper introduces a multi-view vision transformer that incorporates camera pose information to reason about the scene geometry. The paper also presents an efficient differentiable renderer that allows for large-scale training by sampling points along epipolar lines in 2D and aggregating the sampled features using lightweight cross-attention layers. The proposed method achieves state-of-the-art results for novel view synthesis from sparse inputs, outperforming existing approaches.