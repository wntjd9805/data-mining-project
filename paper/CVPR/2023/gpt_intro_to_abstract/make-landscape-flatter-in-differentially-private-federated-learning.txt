Federated Learning (FL) allows distributed clients to train a shared model without sharing data, but privacy leakage is a major concern. Differential privacy (DP) has been introduced in FL to protect privacy, but it can cause severe performance degradation. To address this issue, this paper proposes DP-FedSAM, a novel scheme in DPFL that improves model performance by generating flat models using the SAM optimizer. This leads to better stability, higher generalization ability, and improved robustness to DP noise. Theoretical analysis provides a tighter convergence rate and privacy guarantees. Empirical experiments confirm the effectiveness of DP-FedSAM, achieving state-of-the-art performance compared to other DPFL methods.