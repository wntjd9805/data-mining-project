Language-supervised visual pre-training, such as CLIP, has proven to be a powerful approach for learning representations in computer vision. These pre-trained models demonstrate strong transferability, excel in text-to-image generation, and can improve various visual tasks. Unlike classical supervised learning, language supervision provides richer forms of supervision at multiple levels of granularity. However, large-scale training is necessary to leverage the capabilities of language-supervised models. In this paper, we introduce Fast Language-Image Pre-training (FLIP), a method that improves the efficiency of CLIP training. FLIP randomly removes a large portion of image patches during training, allowing for more sample pairs to be processed and compared in each step. This trade-off between careful examination of sample pairs and processing capacity significantly speeds up training without sacrificing accuracy. Experimental results demonstrate that FLIP reaches similar or higher accuracy compared to CLIP while training at a much faster wall-clock time. Furthermore, FLIP outperforms CLIP in various downstream tasks and shows potential for scaling in terms of model size and dataset size. We hope that our method and findings will inspire further research on scaling vision-language learning.