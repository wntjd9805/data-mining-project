This paper introduces a novel framework called VTacO for visual-tactile in-hand object reconstruction. The framework utilizes tactile images obtained from vision-based tactile sensors to enhance the perception of object geometry, particularly in cases of occlusion. The authors propose the use of the DIGIT tactile sensor and extract tactile and object features using neural networks. These features are then fused in the Winding Number Field to represent the object shape. The Marching Cubes algorithm is used to extract the object shape. The paper also discusses the determination of tactile sensor poses and the enhancement of pure visual information through local geometry refinement, deformation analysis, and hand-object contact state estimation. To train and evaluate the proposed framework, the authors synthesize training data using a tactile simulation environment called VT-Sim. They compare their method with a visual-only setting and a previous baseline method in terms of quantitative and qualitative improvements on object reconstruction. The generalization ability of the trained model to real-world data is also demonstrated. The contributions of this work include the VTacO and VTacOH frameworks for visual-tactile object reconstruction, the development of the VT-Sim simulation environment, and the validation of the trained models' generalization ability.