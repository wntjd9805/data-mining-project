Active speaker detection is an important task in various applications such as speaker diarization, speaker tracking, and automatic video editing. Despite research on this topic dating back several decades, progress has been hindered by the lack of reliable large-scale data. However, with the release of the AVA-ActiveSpeaker dataset, significant advancements have been made in this field, leveraging deep learning techniques. These advancements have improved the performance of active speaker detection but have also increased memory and computation requirements, making real-time processing challenging in scenarios with limited resources.In this study, we propose a lightweight end-to-end architecture for real-time active speaker detection. Our approach focuses on three aspects: (a) inputting a single candidate's face sequence with corresponding audio, (b) extracting spatial and temporal information separately using 2D and 1D convolutions, and (c) utilizing a gated recurrent unit (GRU) for cross-modal modeling instead of complex attention modules. We also design a novel loss function for training. Experimental results show that our proposed method significantly reduces model size and computational cost while achieving comparable performance to the state-of-the-art method. Our method also demonstrates good robustness in cross-dataset testing and can be deployed in real-time applications with inference times ranging from 0.1ms to 4.5ms.The main contributions of this study are: 1. The development of a lightweight end-to-end active speaker detection framework that effectively addresses the challenges of real-time processing by optimizing information input, feature extraction, and cross-modal modeling.2. Experimental results on the AVA-ActiveSpeaker dataset demonstrate that our proposed method achieves comparable performance to the state-of-the-art method while reducing model parameters by 95.6% and FLOPs by 76.9%.3. Ablation studies, cross-dataset testing, and qualitative analysis confirm the state-of-the-art performance and robustness of our proposed method.