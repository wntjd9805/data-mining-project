Recent advancements in face-swap technology have led to an increase in the creation of fake images and videos, resulting in the spread of fake news, malicious hoaxes, and the forging of judicial evidence. To tackle this issue, numerous deep-fake detection methods have been proposed to filter out manipulated media from online platforms. However, these methods often struggle to detect newly-developed deepfakes that differ from those in the training dataset. This paper aims to investigate the reason behind the decreased performance of binary classifiers in cross-dataset evaluations and proposes a solution to improve generalization. The authors identify the "Implicit Identity Leakage" phenomenon, where the identity representation of deepfake images is mistakenly learned during training, leading to false predictions. An analysis of this phenomenon is presented, followed by the introduction of an effective method called the ID-unaware Deepfake Detection Model. This model focuses on local artifact areas instead of relying on global identity information, reducing the influence of Implicit Identity Leakage. Extensive experiments verify the effectiveness of the proposed method, demonstrating its superior performance compared to existing approaches. In summary, this paper contributes by uncovering the Implicit Identity Leakage phenomenon, proposing a novel detection model, and providing empirical evidence of its efficacy.