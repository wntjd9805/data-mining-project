This paper introduces a neuro-symbolic method, called NS3D, to tackle the challenges of annotating 3D labels and understanding complex 3D grounded language in the task of referring expression comprehension (3D-REC). NS3D utilizes a combination of compositional structures and modular neural networks to achieve more accurate grounding of 3D objects and relations in a scene. It parses referring expressions into a neuro-symbolic program form using Codex, a language-to-code model, and decomposes the expressions into functional modules that perform object-level and relational grounding steps. NS3D extends prior neuro-symbolic approaches by incorporating high-arity programs for relation grounding over multiple objects. The proposed method demonstrates improved performance in resolving complex view-dependent referring expressions, increased data efficiency, better generalization to unseen object co-occurrences and scene types, and the ability to zero-shot transfer to novel reasoning tasks. Experimental results on the ReferIt3D benchmark show state-of-the-art accuracy and strong performance in data efficiency and generalization. Additionally, NS3D enables better interpretability and attribution to visual grounding successes and failures. Overall, this paper contributes a neuro-symbolic approach for grounding 3D objects and relations, with promising results in 3D-REC tasks.