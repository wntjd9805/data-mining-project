Visual Queries Localization is a task that involves retrieving objects from an 'episodic memory' in egocentric videos, such as VR headsets or AR glasses. Existing solutions to this problem rely on a 'Siam-detector' model trained on annotated response tracks but tested on whole video sequences. However, these models suffer from domain and task biases, as they are trained with well-posed objects in clear view and the query object is always available during training but absent during most test time. This leads to false positives on distractors and difficulty in detecting the target object. In this paper, we propose a conditioned contextual detector called CocoFormer to address these biases. CocoFormer uses a hypernetwork architecture and a transformer-based design to incorporate open-world visual queries and utilize context from other proposals in the detection reasoning process. Our model surpasses the Siam-RCNN model in performance and is more flexible in different applications.To tackle the domain bias, we sample proposal sets from both labeled and unlabeled video frames to collect data closer to the real distribution. We enlarge the positive query-frame training pairs and create negative query-frame training pairs that incorporate objects from background frames. By diversifying the object query and proposal sets and leveraging global scene information, we improve query object detection accuracy.Our experiments show that training with unlabeled frames and optimizing the conditional detector's architecture significantly improve detection accuracy. We evaluate our improved context-aware query object detector in both 2D and 3D configurations, achieving significant improvements in both settings. In the 2nd Ego4D challenge, our detector ranked first and second in the VQ2D and VQ3D tasks, respectively, and achieved state-of-the-art performance in Few-Shot Detection (FSD) on the COCO dataset.Overall, our proposed approach effectively addresses domain and task biases in visual query localization and demonstrates superior performance compared to existing methods.