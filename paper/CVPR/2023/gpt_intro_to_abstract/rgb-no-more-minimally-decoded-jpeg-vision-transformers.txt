In this paper, we propose the design of neural networks that can directly process frequency-domain representations of images. Compared to the traditional spatial-domain representation of RGB pixel values, this approach offers faster data loading by avoiding the expensive conversion process from frequency-domain to spatial-domain. We build upon previous work that modified standard CNN architectures to input frequency-domain (DCT) images and trained them to comparable accuracies. However, we improve upon these efforts by utilizing Vision Transformers (ViTs) instead of CNNs, which are more suitable for processing frequency-domain inputs. We show that ViTs can be easily adapted to DCT inputs by modifying the initial patch embedding layer. Furthermore, we address the challenge of data augmentation on frequency-domain images by directly augmenting DCT representations, eliminating the need for DCT to RGB conversions during training. We train ViT models on ImageNet dataset and achieve accuracy matching their RGB counterparts, while also achieving faster training iterations and inference. Additionally, we discuss the lack of studies on DCT augmentation during frequency-domain training and the importance of faster data loading to fully utilize the speed-ups offered by ViTs.