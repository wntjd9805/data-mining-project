Shape completion and reconstruction from scans is a critical task in 3D digitization for virtual and augmented reality applications. However, existing methods struggle with poor quality point clouds and limited ground truth shapes. To address this issue, we propose a new task called SCoDA, which focuses on transferring knowledge from synthetic domains to complete real scans. We introduce ScanSalon, an object-centric dataset consisting of real scans paired with manually annotated 3D models. The main challenge lies in the domain gap between synthetic and real point clouds, as real scans are often incomplete and have distinct characteristics from their simulated counterparts. To overcome these challenges, we propose a cross-domain feature fusion module that combines global and local features from synthetic and real domains to improve shape completion. Additionally, we develop a volume-consistent self-training framework to enhance the model's robustness to incomplete real scans. We evaluate our method on ScanSalon and compare it against existing baselines, demonstrating its effectiveness and superiority. Our contributions include the introduction of the SCoDA task, the creation of the ScanSalon dataset, the development of the cross-domain feature fusion module, and the proposal of the volume-consistent self-training framework.