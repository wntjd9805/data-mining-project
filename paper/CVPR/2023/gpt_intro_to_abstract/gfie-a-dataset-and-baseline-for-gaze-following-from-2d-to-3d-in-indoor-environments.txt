This paper introduces the concept of gaze-following, a human skill that emerges in infancy to understand the visual focus and intentions of other people. The detection of gaze targets, similar to how humans do, has great potential in various applications such as retail environment, driver distraction detection, action recognition, social relationship analysis, autism diagnosis, and human-aware robot navigation. While wearable eye-tracking devices and custom systems have been explored for monitoring gaze behavior, they have limitations in terms of calibration, cost, and applicability to constrained scenes. Therefore, recent works have focused on establishing datasets for inferring a person's gaze target from third-view images using deep-learning methods. However, existing datasets suffer from deficiencies in the way gaze data is gathered, including subjective annotations and labor-intensive manual processes. To address these problems, this paper proposes a novel system for building the GFIE dataset, which provides accurate annotations and clean training data recorded in natural environments. The system combines a laser rangefinder and an RGB-D camera Azure Kinect to guide the subject's gaze target through a laser spot while recording their activities. An image inpainting algorithm is employed to eliminate the laser spot, and the dataset includes RGB-D images, 2D/3D gaze targets, camera parameters, head bounding boxes, and 2D/3D eye locations. A baseline method is also introduced, utilizing the stereo field of view to estimate gaze targets and achieving excellent performance in both 2D images and 3D scenes. The contributions of this paper include the development of the GFIE dataset and the introduction of the stereo field of view in the baseline method.