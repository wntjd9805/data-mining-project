This paper addresses the problem of inferring the 6D pose parameters of objects in 3D space from a single RGB image, which is a fundamental task in 3D computer vision. The accurate estimation of pose parameters has various applications in fields such as AR/VR, autonomous driving, and robotic manipulation. Deep learning has significantly improved the accuracy and runtime of pose estimation methods, but it requires a large amount of labeled data for supervision, which is time-consuming and prone to errors. Most benchmarks in this field have only a few hundred images, which is insufficient for proper training of large models and leads to overfitting. To tackle these challenges, several approaches have been proposed, including cut-and-paste strategies, synthetic data generation, and the use of generative adversarial networks. Recently, there has been a new line of work that focuses on self-supervised pose estimation without relying on additional supervision sources. In this paper, we propose a novel self-supervised approach for 6D pose estimation that achieves state-of-the-art performance without requiring any additional supervision signals. Our method involves learning realistic textures of objects and conducting training on pixel-perfect labels and realistic appearance. We address the challenge of capturing accurate textures by leveraging synthetic data and utilizing adversarial training and regularisation techniques. Experimental results show that our approach outperforms recent state-of-the-art methods, even on difficult objects with little variance in appearance and geometry. We also demonstrate substantial generalization improvements towards unseen domains. Overall, our contributions include a new learning scheme that decomposes self-supervision for pose estimation into texture learning and pose learning, the introduction of a surfel-conditioned adversarial training loss and a synthetic texture regularisation term, and significant improvements over recent strong baselines with additional supervision signals.