Abstract:Self-supervised learning (SSL) has become essential in computer vision applications, particularly in the field of deep learning. Most SSL models utilize a multi-view, Siamese network architecture to learn visual representations. These models generate augmented versions of input images and pass them through a neural network, producing twin projected embeddings. The objective is to train the network so that the twin embeddings represent the same original image. However, collapsed embeddings, where all examples are projected to a single vector, pose a significant challenge in SSL. Contrastive approaches use loss terms to repel embeddings of different images from each other, but considering all possible negative pairs is infeasible, leading to the need for negative sampling. Non-contrastive SSL models, such as Barlow Twins and VICReg, penalize features with small variances and promote feature decorrelation to address collapsed embeddings. However, these models require extensive computation time as the regularizers are defined in terms of cross-correlation or covariance matrices. In this paper, we propose a relaxed decorrelating regularizer that can be computed efficiently using Fast Fourier Transform. Although this relaxation can result in undesirable local minima, we show that feature permutation effectively mitigates this issue. Our proposed method achieves competitive performance with Barlow Twins and VICReg in downstream tasks, while significantly reducing computation time and memory consumption.