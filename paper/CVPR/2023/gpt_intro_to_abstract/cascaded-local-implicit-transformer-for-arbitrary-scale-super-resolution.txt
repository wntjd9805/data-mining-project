Single Image Super-Resolution (SISR) is a challenging task in computer vision that aims to reconstruct high-resolution (HR) images from their low-resolution (LR) counterparts. Traditional approaches for SISR, known as 'fixed-scale SR', focus on upsampling LR images using separate deep neural networks for each scale factor. However, this approach limits the potential applications of SISR models. To overcome this limitation, recent research has focused on arbitrary-scale SR, which aims to upsample LR images continuously using a single model. These approaches replace traditional upsampling layers with local implicit image functions that map 2D coordinates and latent representations to RGB values.In this paper, we propose a novel approach called Local Implicit Transformer (LIT) for arbitrary-scale SR. LIT incorporates a Cross-Scale Local Attention Block (CSLAB) and a Local Frequency Encoding Block (LFEB) to improve feature correlation and capture contextual information. CSLAB generates attention maps based on interpolated latent vectors and key latent vectors sampled from a grid, while LFEB addresses the spectral bias problem by projecting relative coordinates into latent space. LIT also utilizes a decoder to generate RGB values from attention feature embeddings and frequency embeddings.To handle diverse scaling factors and achieve arbitrary-scale super-resolution, we introduce a cumulative training strategy that incrementally enhances the representative power of the local implicit image function. We also present Cascaded LIT (CLIT), which utilizes multi-scale feature embeddings to complement missing details during one-step upsampling. This combination of the cumulative training strategy and CLIT enables efficient and effective handling of arbitrary-scale SR tasks.Our contributions include the introduction of the LIT architecture with the local attention mechanism for arbitrary-scale SR, the development of a cumulative training strategy and the CLIT framework for large-scale upsampling, and comprehensive performance analyses of LIT and CLIT. Experimental results show that our proposed methods yield remarkable or comparable results across various benchmark datasets.The rest of the paper is organized as follows: Section 2 reviews related work, Section 3 describes the proposed LIT and CLIT frameworks, Section 4 presents experimental results, and Section 5 concludes the paper.