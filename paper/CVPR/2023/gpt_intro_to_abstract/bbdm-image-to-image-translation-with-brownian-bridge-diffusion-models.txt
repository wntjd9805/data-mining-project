Image-to-image translation is a key problem in computer vision and graphics, with applications ranging from style transfer to semantic image synthesis. One popular approach is conditional Generative Adversarial Networks (GANs), such as Pix2Pix, which learn the mapping between domains. However, these methods are difficult to train and often produce limited diversity in their translations. Diffusion models have shown promise in producing high-quality images but have not been extensively applied to image-to-image translation. In this paper, we propose a novel framework based on Brownian Bridge diffusion process for image-to-image translation. Unlike existing methods, our approach directly builds the mapping between input and output domains using a stochastic diffusion process, rather than conditional generation. We conduct the diffusion process in the same latent space as Latent Diffusion Model (LDM) for efficiency, but our method differs in the modeling of domain mapping. Experimental results demonstrate the competitive performance of our approach in various image-to-image translation tasks.