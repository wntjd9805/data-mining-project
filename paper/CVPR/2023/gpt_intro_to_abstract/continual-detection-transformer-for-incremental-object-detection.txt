Humans naturally learn incrementally, acquiring new knowledge without forgetting previous concepts. In contrast, machine learning suffers from catastrophic forgetting, where learning from non-i.i.d. data can overwrite previously acquired knowledge. This issue also affects object detection. The problem has been formalized in an incremental object detection (IOD) protocol, where trainers are limited in their access to past data. Current methods to address forgetting in other tasks like Knowledge Distillation (KD) and Exemplar Replay (ER) do not work well when directly applied to transformer-based object detection models. This paper introduces Continual Detection Transformer (CL-DETR), which solves the issues by proposing Detector Knowledge Distillation (DKD) and improving ER with a new calibration strategy. CL-DETR also addresses compatibility issues with the IOD protocol and achieves significant improvements in performance compared to baseline methods. The paper presents extensive experiments, ablation studies, and visualizations to support the proposed approach. Four main contributions are made: the DKD loss, a calibration strategy for ER, a revised IOD benchmark protocol, and extensive experiments on COCO 2017.