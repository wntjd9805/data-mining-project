Recent research in computer science has focused on the development of egocentric 3D human pose estimation methods using head- or body-mounted cameras. These methods allow for capturing a person's movements in a large space, unlike traditional pose estimation methods. The potential applications of egocentric pose estimation include xR technologies and mobile interaction applications. However, previous methods have suffered from issues such as body floating and body-environment penetration, which limit their effectiveness in scenarios involving human-object interaction and motion forecasting. To address these issues, we propose a scene-aware pose estimation framework that leverages the scene context to improve the accuracy and plausibility of 3D human body pose estimation. Our framework utilizes a fisheye camera mounted on the head to obtain wide-view scene context information. We train an egocentric depth estimator to predict the depth map of the surrounding scene, taking into account the occlusion caused by the human body. Additionally, we combine 2D pose features and scene depth in a voxel space and regress 3D body pose heatmaps using a V2V network. This approach allows for learning the relative position and potential interactions between human body joints and the surrounding environment, enabling the prediction of plausible poses. We have created synthetic and in-the-wild egocentric datasets with pose and scene geometry labels to evaluate our method. Our results demonstrate that our approach outperforms existing methods both quantitatively and qualitatively, producing accurate and plausible 3D poses even in challenging real-world scenes. In summary, our contributions include the first scene-aware egocentric pose estimation framework, synthetic and in-the-wild datasets, depth estimation and inpainting networks, and improved pose estimation considering the scene context.