This paper introduces a unified architecture called the streaming video model for handling both frame-based and sequence-based video understanding tasks in computer vision. The model consists of a temporal-aware spatial encoder and a task-related temporal decoder. The spatial encoder extracts temporal-aware spatial features for each video frame, leveraging additional information from past frames. The temporal decoder transfers frame-level features to task-specific outputs for sequence-based tasks. The model disentangles the frame-level feature extraction and clip-level feature fusion, allowing for more flexible use scenarios. The proposed streaming video model, implemented as the streaming video Transformer (S-ViT), incorporates self-attention within a frame to extract spatial information and cross-attention across frames to make the fused feature temporal-aware. S-ViT borrows ideas from triple 2D decomposition and restricts cross-attention to specific regions, reducing computational cost and enabling the handling of long-historical videos. The S-ViT model is evaluated on two downstream tasks: sequence-based action recognition and multiple object tracking (MOT). The results show that the S-ViT model achieves high performance on both tasks while reducing computation expenditure compared to frame-based architectures. The paper concludes that the proposed unified architecture is a step towards a universal video processing architecture.