Deep neural networks (DNNs) have shown remarkable progress in computer vision tasks. However, they are susceptible to adversarial examples, which are crafted to deceive these models without being perceptible to humans. Adversarial examples have been successful in real-world scenarios like autonomous driving and biometrics, even when the attacker does not know the target models. Hence, it is crucial to develop effective defense strategies against adversarial attacks.Several defense techniques have been proposed, and adversarial training is considered the most successful. Nevertheless, small perturbations in pixel-level can accumulate and significantly affect the output of the model in the intermediate feature space. To address this issue, recent methods have deactivated non-robust feature activations that cause mispredictions. However, these methods tend to neglect discriminative cues that might be present in these non-robust activations.This paper introduces a novel Feature Separation and Recalibration (FSR) module that aims to improve feature robustness. The FSR module separates the intermediate feature map into non-robust activations responsible for model mispredictions and robust activations that provide useful cues for correct predictions even under adversarial attacks. Unlike existing methods, the FSR module also recalibrates the non-robust activations to capture additional cues for correct model decisions. These cues can better guide the model and enhance its robustness.The effectiveness of the FSR module is demonstrated through experimental results on various white- and black-box attacks. The results show that the FSR module successfully restores discriminative cues in non-robust activations, leading to improved model performance. Additionally, the FSR module incurs only a small computational overhead, making it efficient to implement.