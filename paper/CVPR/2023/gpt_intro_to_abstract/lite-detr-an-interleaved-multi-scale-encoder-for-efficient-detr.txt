Object detection has seen significant advancements in recent years, particularly with the introduction of convolutional networks in classical detection models. However, there has been less focus on improving the encoder part of the DETR (DEtection TRansformer) model, which lacks multi-scale features necessary for detecting small objects. Existing approaches, such as Deformable DETR, introduce deformable attention algorithms to reduce self-attention complexity, but still suffer from high computational costs due to the large number of query tokens introduced from multi-scale features. In this paper, we propose Lite DETR, an efficient framework that uses fewer feature scales while maintaining important local details. We introduce an encoder block with deformable self-attention layers that can be added to any multi-scale DETR-based model, significantly reducing the computational cost by 62% to 78% while maintaining competitive performance. The encoder block updates high-level and low-level features in an interleaved way, improving the multi-scale feature pyramid. Additionally, we propose a key-aware deformable attention approach to enhance the lagged low-level feature update. Experimental results demonstrate that Lite DETR achieves a 60% reduction in detection head GFLOPs while maintaining 99% detection performance.