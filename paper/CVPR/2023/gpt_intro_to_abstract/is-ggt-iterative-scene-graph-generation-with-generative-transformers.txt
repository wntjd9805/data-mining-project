Graph-based visual representations are gaining popularity in various computer vision tasks due to their ability to encode visual, semantic, and temporal relationships. Scene graphs, which model the relationships between objects in a scene, have been proven useful in tasks such as visual question-answering and captioning. However, existing approaches to scene graph generation have limitations, such as ignoring the underlying semantic structure and high computational overhead. In this paper, we propose a two-stage generative approach called IS-GGT, which leverages generative graph models to sample the underlying interaction graph before reasoning over it for scene graph generation. This decoupling of graph generation and relationship modeling allows for more efficient reasoning and coherent relationship predictions. Experimental results on Visual Genome dataset demonstrate the effectiveness of our approach in achieving state-of-the-art performance while considering only a fraction of the possible pairwise edges. Overall, our contributions include a graph generative approach to scene graph generation, an iterative interaction graph generation, and contextualized relational reasoning using a transformer-based architecture.