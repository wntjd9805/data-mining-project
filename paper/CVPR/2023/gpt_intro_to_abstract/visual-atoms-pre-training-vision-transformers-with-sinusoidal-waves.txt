Vision transformers have had a significant impact on computer vision, with state-of-the-art models in various tasks now based on them. However, pre-training these models on large datasets like JFT-300M is challenging due to the enormous number of images. Self-supervised learning has emerged as a popular alternative, but collecting and managing large datasets remains a challenge. This limitation has led to the exploration of formula-driven supervised learning (FDSL) as a way to generate synthetic images for pre-training. Synthetic datasets offer advantages such as continuous improvement and reduced ethical concerns. Previous studies have shown the importance of contours in pre-training vision transformers, but they have only explored a limited design space. In this study, we propose a systematic investigation of contour-oriented synthetic images using circular harmonics to express contours as a superposition of sinusoidal waves onto ellipses. We create a new dataset called VisualAtom and show that pre-training vision transformers with VisualAtom leads to comparable accuracy to JFT-300M, using only a fraction of the images. We also outperform existing FDSL methods. In addition, we contribute by releasing the synthesized dataset, pre-trained models, and code for public use.