This paper introduces a novel approach for learning low level imaging properties from camera metadata in order to obtain a complete understanding of an image. The authors argue that while previous approaches in computer vision have focused on learning high level semantics, such as objects, from cross-modal associations, the properties of the camera that captured the image are also important for a comprehensive understanding. The proposed method involves training a joint embedding through contrastive learning that correlates image patches with camera metadata. The metadata, represented as Exchangeable Image File Format (EXIF) tags, is converted to a language-like representation and processed using a transformer. The authors demonstrate that their model can successfully estimate camera properties from images, and that it provides a useful representation for image forensics and camera calibration tasks. The learned features are evaluated on classification tasks, including estimating radial distortion parameters and distinguishing real and manipulated images, and shown to outperform alternative feature sets. The authors also show that their embeddings can be used to detect image splicing without labeled data, by identifying inconsistencies in patch embeddings and localizing spliced regions. Overall, the experiments confirm the effectiveness of camera metadata supervision for representation learning and the utility of image-metadata embeddings for various computer vision tasks.