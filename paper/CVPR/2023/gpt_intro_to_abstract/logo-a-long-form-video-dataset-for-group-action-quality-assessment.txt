Action quality assessment (AQA) is widely used in various real-world contexts, such as sports events, healthcare, art performances, and military parades. Despite several existing works achieving promising performances in simple scenarios, implementing AQA in more complex situations is still challenging. This difficulty arises due to the limitations of existing datasets, which lack richness in terms of scenario complexity and annotations.In this paper, we introduce a new multi-person long-form video dataset called LOGO (LOng-form GrOup) to address these limitations. LOGO contains videos with an average duration of 204.2 seconds and includes eight actors in each sample, making the scenes more complex compared to existing datasets. Moreover, LOGO provides fine-grained annotations, including frame-wise action type labels, temporal boundaries of actions, and formation labels to depict relations among actors.To bridge the gap between single-person short-sequence scenarios and multi-person long-sequence scenarios, we propose a plug-and-play module called GOAT (GrOup-aware ATtention). GOAT models the relations among actors in the spatial domain using a graph convolution network (GCN), while in the temporal domain, it learns the temporal features of long-term videos based on the spatial information in each clip.The contributions of this paper are threefold: (1) the construction of the LOGO dataset, which is the first multi-person long-form video dataset with longer average duration, more people, and richer annotations; (2) the development of the GOAT module, which effectively models group information and temporal contextual relations; and (3) the experimental validation of our approach, which outperforms existing methods in AQA and achieves state-of-the-art results.