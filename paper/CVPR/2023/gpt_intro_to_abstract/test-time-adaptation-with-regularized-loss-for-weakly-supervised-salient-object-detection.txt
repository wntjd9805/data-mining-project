The problem of overfitting in Convolutional Neural Networks (CNNs) is well-known in the field of computer science. Test-time adaptation has been proposed as a solution to this issue, where a model is trained on the training data and then fine-tuned during test time on a test image. However, test-time adaptation has only been used for a few applications where a suitable loss function can be designed.In this paper, we propose the first approach for test-time Salient Object Detection (SOD), which aims to identify image regions that attract human attention. Traditional training methods for SOD require pixel precise ground truth, which is challenging and time-consuming to obtain. Therefore, there is growing interest in weakly supervised SOD, where only image-level supervision is provided. Most weakly supervised SOD approaches rely on noisy pseudo labels generated from unsupervised methods, making them difficult to adapt for test-time adaptation.Our test-time adaptation approach is based on a previous method that does not require pseudo labels. We design a regularized loss function specifically tailored for test-time adaptation. However, one challenge with the regularized loss function is that it may result in an empty solution if the hyperparameters are not chosen correctly. To address this, we propose a method for setting hyperparameters specific to each test image in order to avoid empty solutions.Our approach involves training a base CNN using weakly supervised labels and then fine-tuning it on a small dataset created from the test image. We also utilize dense Conditional Random Field (CRF) post-processing to improve the results. Our experiments on standard benchmarks demonstrate that test-time adaptation significantly improves the performance of weakly supervised SOD, achieving state-of-the-art results.The remainder of the paper is organized as follows: related work is discussed in Section 2, the approach in [33] is explained in Section 3, our proposed approach is described in Section 4, and experimental results are presented in Section 5.