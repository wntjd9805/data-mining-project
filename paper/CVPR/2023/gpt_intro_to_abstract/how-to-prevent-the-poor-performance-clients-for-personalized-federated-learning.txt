Federated Learning (FL) is a popular research paradigm that trains a global learning model with distributed private datasets on multiple clients. However, when dealing with non-IID/heterogeneous datasets, learning a single global model may result in poor performance for individual clients. To address this, personalized federated learning (pFL) has been developed to provide customized local model solutions for each client. Existing pFL algorithms either use a global model or do not use a global model. While these algorithms have made accomplishments, they may lead some clients to have poor learning performance. This motivates the need for an effective strategy to prevent clients from falling into poor performance without degrading others. We propose a novel pFL strategy called Personalize Locally, Generalize Universally (PLGU) that aims to generalize universal learning for unbiased local adaptation while keeping the local personalized features. We tackle the challenge of generalizing universal information without local feature perturbation by developing a perturbation method called Layer-Wised Sharpness-Aware-Minimization (LWSAM). We embed the PLGU strategy with the perturbed universal generalization on both the two pFL schemes. For the scheme with a global model, we propose the PLGU-Layer Freezing (LF) algorithm. For the scheme without a global model, we propose the PLGU-GRep algorithm. We also extend the PLGU strategy to improve learning performance for poor clients in the pFedHN algorithm, called PLGU-GHN. We analyze the generalization bound and provide extensive experimental results that show the effectiveness of the PLGU-LF, PLGU-GRep, and PLGU-GHN algorithms in preventing poor client performance while outperforming the average learning performance.