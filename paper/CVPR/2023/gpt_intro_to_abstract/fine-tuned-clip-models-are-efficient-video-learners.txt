Pretrained vision-language (VL) models have demonstrated impressive performance in various downstream vision applications. However, adapting these models for video-based tasks is challenging due to limited availability of aligned video-text data and the complexity of videos. Previous approaches have used additional components for spatio-temporal modeling, but they require careful design efforts and can hinder the generalization capability of the models. In this paper, we investigate the impact of adapting VL models for videos and propose a simple fine-tuning approach that performs competitively to more complex methods. We also propose a two-stage "bridge and prompt" approach for adapting VL models in low-data regimes. Our experiments show that our proposed methods achieve impressive performance in various experimental settings and effectively bridge the modality gap between images and videos.