Recent advancements in self-supervised visual representation learning have achieved promising results in various vision downstream tasks using masked image modeling (MIM) methods. These methods, such as MAE and BEiT, utilize vision transformers. The success of pretraining paradigms has also been extended to the video domain, improving video transformers compared to supervised pretraining. Representative masked video modeling (MVM) works, including BEVT, VideoMAE, and ST-MAE, have demonstrated the effectiveness of pretraining video transformers through the reconstruction of low-level features. However, using low-level features as reconstruction targets often introduces noise, and the high redundancy in video data can lead to limited transfer performance. This paper introduces Masked Video Distillation (MVD), a two-stage masked video modeling approach that utilizes the high-level features of pretrained MIM and MVM models as masked prediction targets. The authors find that student models distilled with different teachers in MVD exhibit different properties on various video tasks, with image teachers performing better on spatial tasks and video teachers performing better on tasks that require temporal dynamics. By analyzing feature targets provided by image and video teachers, the authors observe that video teachers contain more temporal dynamics. Motivated by this observation, the paper proposes a spatial-temporal co-teaching strategy for MVD to leverage the advantages of both image and video teachers, resulting in strong performance on multiple video recognition benchmarks. Experimental results demonstrate that MVD significantly outperforms methods that utilize only one teacher model. The contributions of this paper include the discovery of using MIM and MVM models as teachers for improved video representation, the proposal of MVD with a co-teaching strategy, and the demonstration of state-of-the-art performance on standard video recognition benchmarks.