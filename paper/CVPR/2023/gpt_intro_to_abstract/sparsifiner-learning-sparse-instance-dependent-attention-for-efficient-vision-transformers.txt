Vision Transformers (ViTs) have become the dominant model for various vision tasks such as image classification, object detection, and semantic segmentation. However, scaling ViTs to handle a large number of tokens is challenging due to the quadratic computational complexity of multi-head self-attention (MHSA). To address this, previous works have used fixed attention patterns or token pruning methods to improve computational efficiency. However, these approaches have limitations in capturing the instance-dependent nature of semantic information and suffer from decreased accuracy. In this paper, we propose a novel method called Sparsifiner that predicts instance-dependent sparse attention patterns using low-rank connectivity patterns. This approach allows for more efficient utilization of the attention budget and improves attention pattern flexibility compared to fixed attention patterns or token pruning. We demonstrate that Sparsifiner achieves a better trade-off between accuracy and FLOPs (floating-point operations) compared to token sparsity methods. Additionally, we propose a knowledge distillation-based approach to train Sparsifiner from pretrained ViTs using a small number of training epochs. Our contributions include the introduction of Sparsifiner, an evaluation of its superior performance-accuracy trade-offs compared to token sparsity, and a training approach using knowledge distillation.