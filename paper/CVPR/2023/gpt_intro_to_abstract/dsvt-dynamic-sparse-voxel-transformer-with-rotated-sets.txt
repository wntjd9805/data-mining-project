3D perception is a challenging task in computer vision, with numerous applications in fields like autonomous driving systems and modern robotics. In this paper, we propose DSVT, a Transformer-based 3D backbone that can be applied to various 3D perception tasks for point cloud processing. Unlike the densely packed 2D images, 3D point clouds are sparse and irregularly distributed, making it difficult to directly apply traditional techniques. Previous methods utilize customized sparse operations, but they suffer from limitations such as intensive computation or limited representation capacity. Attention-based methods have been investigated to address these issues, but they either involve unnecessary computations or have non-competitive performance. We aim to build an efficient and deployment-friendly 3D Transformer backbone to overcome these challenges. We present two major modules: dynamic sparse window attention for efficient parallel computation of local windows with diverse sparsity and a learnable 3D pooling operation to downsample the feature map and better encode geometric information. Our approach achieves low latency and high modeling power without the need for customized CUDA operations. We also introduce an efficient deployment strategy and demonstrate superior performance compared to previous state-of-the-art methods on large-scale datasets.