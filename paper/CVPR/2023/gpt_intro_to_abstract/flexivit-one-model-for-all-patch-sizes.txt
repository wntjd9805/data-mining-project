The introduction discusses the shift from the dominant convolutional neural network (CNN) approach to the use of Vision Transformers (ViTs) in image processing. Patchification, a procedure in which images are divided into non-overlapping patches and computations are performed on tokens created from these patches, has become a key technique in ViT models. However, the role of patch size in ViT models has received little attention. This work aims to explore the impact of patch size on the compute and predictive performance of ViT models without changing model parameterization. The authors propose FlexiViT, a flexible ViT that can match or outperform standard fixed-patch ViTs across a wide range of patch sizes. They demonstrate the efficiency of FlexiViT models in various downstream tasks and show that the flexibility of the backbone, particularly in strong performance across patch sizes, is often preserved even after fine-tuning with a fixed patch size. Additionally, the authors discuss how the flexibility of patch size can be used to accelerate pre-training. They also analyze the representations of FlexiViT models and compare its effectiveness with alternative architectural approaches to controlling the performance-compute trade-off in ViT models.