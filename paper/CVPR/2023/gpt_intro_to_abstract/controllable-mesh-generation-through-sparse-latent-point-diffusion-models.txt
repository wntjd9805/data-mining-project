This computer science paper introduces a novel approach for generating meshes, which are widely used in computer graphics applications such as VR, AR, and games. The authors propose using point clouds as an intermediate representation of meshes, allowing for more efficient modeling and overcoming the challenges of irregular data structures and varying topologies. They utilize denoising diffusion probabilistic models (DDPMs) to learn the distribution of point clouds and employ Shape as Points (SAP) for reconstructing meshes from the generated point clouds. To ensure high-quality transformed meshes, they encode the point clouds to a sparse set of semantic latent points with attached features and train two DDPMs to learn the distribution of this latent space. Experimental results on the ShapeNet dataset demonstrate the superiority of their proposed approach in terms of visual quality and quantitative metrics. The main contributions of the work include the use of point clouds as an intermediate representation, the design of a point cloud autoencoder for encoding to a sparse set of latent points, and the ability to perform both unconditional and controllable point cloud generation based on the positions of the sparse latent points.