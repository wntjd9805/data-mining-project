Understanding indoor environments is crucial for various practical applications in computer vision, such as room reconstruction, robot navigation, and virtual reality. Traditional methods focused on modeling indoor scenes using perspective images, but with the advent of CNNs and omnidirectional photography, panorama images have become popular for indoor scene understanding. Panorama images provide a larger field-of-view and continuous geometric context compared to perspective images. However, modeling the holistic scene from equirectangular projection, a commonly used format for 360 indoor scene understanding, is challenging due to the distortion near zenith and nadir points. Recent approaches decompose panoramas into perspective patches, but this breaks the local continuity of gravity-aligned scenes and objects. To address this issue, we propose PanelNet, a novel network that represents panoramas as consecutive panels with corresponding global and local geometry. PanelNet tackles major 360 indoor understanding tasks such as depth estimation, semantic segmentation, and layout prediction. We design a geometry embedding network that encodes both local and global features of panels to reduce the negative effects of ERP distortion. Additionally, we introduce the Local2Global Transformer as a feature processor, which enhances continuity and accurately captures panel-wise context. Our framework outperforms existing methods in indoor depth estimation and achieves competitive results in other indoor scene understanding tasks. The contributions of our work include the novel panel representation, the panel geometry embedding network, and the Local2Global Transformer.