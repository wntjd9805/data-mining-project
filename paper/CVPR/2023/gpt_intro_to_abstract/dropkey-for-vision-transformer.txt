In recent years, the Vision Transformer (ViT) has shown great success in various vision tasks such as image recognition, object detection, and body shape estimation. While previous research has focused on patch division, architecture design, and task extension, the dropout technique, which plays a crucial role in achieving good generalizability, has been overlooked.Unlike Convolutional Neural Networks (CNNs) where dropout is used on the attention weights, ViT directly applies dropout from the original Transformer designed for Natural Language Processing. However, this vanilla design faces three major problems. Firstly, it breaks the probability distribution of attention weights, resulting in overfitting specific patterns locally. Secondly, the design is sensitive to the constant dropout ratio, leading to instability during training. Lastly, it ignores the structured characteristic of input patch grid, which could be beneficial for performance improvement.Motivated by these issues, we propose to analyze and improve the dropout technique in the self-attention layer of ViT. We focus on three core aspects: what to drop, how to schedule the drop ratio, and whether structured dropout is necessary. Instead of dropping attention weights, we suggest setting the Key as the dropout unit, which significantly influences the output. This introduces a novel dropout-before-softmax scheme that regularizes attention weights while maintaining their probability distribution, thereby penalizing weight peaks and enhancing model generalizability.To schedule the drop ratio, we propose a linear decreasing schedule that gradually decreases the drop ratio along the stack of self-attention layers. This helps to avoid overfitting to low-level features and preserve high-level semantics. Additionally, we explore two structured versions of dropout, but find that the structure trick used for CNNs is not essential for ViT.Based on our analysis, we present a new dropout method called DropKey, which utilizes the Key as the dropout unit and incorporates a decreasing schedule for the drop ratio. Through comprehensive experiments on different ViT architectures and vision tasks, we demonstrate the effectiveness of DropKey in improving performance in a general and effective way. This work contributes by providing a theoretical and experimental analysis of the dropout technique for self-attention layers in ViT, proposing a novel DropKey method, and achieving state-of-the-art results on various vision tasks with improved ViT architectures.