Semantic segmentation, which assigns semantic labels to every pixel of an image, has seen remarkable performance improvements with the development of CNNs and vision transformers. However, these advancements heavily rely on large quantities of pixel-level annotations and struggle to handle classes unseen in the training set. Few-shot Semantic Segmentation (FSS) has been proposed as a solution, using limited support annotations from unseen/novel classes to adapt models. However, FSS assumes that support and query images contain the same novel classes and only emphasizes segmentation of one novel class at a time. To address this limitation, Generalized Few-shot Semantic Segmentation (GFSS) has been introduced, aiming to simultaneously identify base and novel class pixels in a query image. GFSS consists of two training phases: base class learning and novel class updating, but suffers from performance degradation on base classes due to compromising the well-learned features of these classes in the fine-tuning process. In this paper, we propose a new framework called Projection onto Orthogonal Prototypes (POP) for GFSS. POP learns a series of orthogonal prototypes, where each prototype corresponds to a specific semantic class. By representing an image as a group of uncorrelated feature components, POP can learn and integrate new components for novel classes without affecting the ones learned for base classes. POP utilizes an encoder to extract feature maps and projects them onto prototypes, using the projection as a discriminative representation for each class. POP addresses three key aspects: freezing base prototypes during novel class learning, enforcing orthogonality between base and novel class prototypes, and dynamically representing background pixels in the context of "semantic shifting". Our proposed framework demonstrates superior performance compared to fine-tuning on two benchmarks (PASCAL-5i and COCO-20i), achieving improvements on both base and novel classes. This work contributes a novel approach for generalized few-shot semantic segmentation and provides insights into adapting models to novel classes without sacrificing well-learned features and dynamically representing background pixels.