This paper introduces a novel transfer strategy for few-shot learning in vision-language models (VLMs) to address the challenge of adapting pre-trained models to downstream tasks. The proposed strategy includes two key components: Selective Attack (SA) and Cross-Modal Distribution Alignment (CMDA). SA aims to reduce the overfitting of class-irrelevant image contents by generating a kernelized attention map to identify and perturb these areas. CMDA focuses on aligning the distributions of image and text representations by constructing Vision-Language Prototypes (VLPs) for each class and optimizing them to minimize the distance to the language prototypes. The effectiveness of SA and CMDA is illustrated through visualizations of image feature distributions and text feature distributions. Additionally, the paper introduces an augmentation strategy to increase the diversity of images and prompts, improving the overall performance of the model. Experimental results demonstrate that the proposed method outperforms existing approaches in few-shot learning on multiple benchmarks. Overall, this paper presents a comprehensive approach to enhance the capabilities of VLMs for downstream recognition tasks, addressing the challenges of overfitting and distribution misalignment.