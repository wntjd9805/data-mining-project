Generative models have shown great success in various applications, including text-to-image synthesis. This field has gained significant attention due to its ability to generate high-fidelity images based on given text descriptions. Recently, large pretrained autoregressive and diffusion models have shown impressive generative abilities in synthesizing complex scenes, surpassing previous text-to-image generative adversarial networks (GANs). However, these large models suffer from limitations such as high computing requirements, slow generation speed, and a lack of intuitive latent space. To address these limitations, we propose a novel text-to-image generation framework called Generative Adversarial CLIPs (GALIP). GALIP integrates the pretrained CLIP model in both the discriminator and generator, leveraging the complex scene understanding and domain generalization abilities of CLIP. The CLIP-based discriminator accurately assesses the quality of generated images by combining the frozen CLIP-ViT image encoder and a learnable mate-discriminator. Meanwhile, the CLIP-empowered generator utilizes the domain generalization ability of CLIP by mapping an implicit bridge domain to the target visual concepts through the frozen CLIP-ViT and a learnable mate-generator. By incorporating text-conditioned prompts for task adaptation, GALIP achieves efficient, fast, and more controllable text-to-image synthesis with high-quality results. Extensive experiments demonstrate that GALIP achieves comparable performance to large pretrained models but with significantly smaller computational costs.