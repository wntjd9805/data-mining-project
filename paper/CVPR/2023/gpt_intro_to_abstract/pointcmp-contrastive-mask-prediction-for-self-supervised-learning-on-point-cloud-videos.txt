In recent years, LiDARs have gained popularity in various applications such as autonomous vehicles and robots, providing rich geometric information through point clouds. While early research focused on static point clouds, there is now a growing interest in understanding point cloud videos. However, annotating point clouds is a time-consuming task, leading to an increased interest in self-supervised learning from point cloud videos. Despite the effectiveness of self-supervised learning methods on static point clouds and images, these methods face challenges when applied to point cloud videos. These challenges include the need for multiple-granularity information, the generation of suitable samples, and the leakage of location information. In this paper, we propose a self-supervised learning framework, called PointCMP, which integrates the learning of local and global spatio-temporal features and conducts learning at different granularities. To address the challenge of generating suitable samples, we introduce a mutual similarity based augmentation module. Additionally, we mitigate the leakage of location information by conducting token-level contrastive learning. Our experiments on benchmark datasets demonstrate the efficacy of PointCMP in understanding point cloud videos.