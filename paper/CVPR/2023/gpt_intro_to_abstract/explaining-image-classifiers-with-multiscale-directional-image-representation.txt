Modern image classifiers are often difficult to explain, and saliency maps have been commonly used to highlight important regions in an image for interpretation. However, previous approaches to generating saliency maps have been found to be unreliable and susceptible to explanation artifacts. Current practice heavily regularizes the explanation masks to be smooth, which can roughly localize the relevant image region, but often overlays irrelevant patterns on the relevant ones. In this paper, we propose a new mask explanation method called ShearletX that is able to effectively separate fine-detail patterns that are relevant to the classifier from nearby nuisance patterns that do not affect the classifier. ShearletX achieves this separation by optimizing a mask in the shearlet representation of an image, which efficiently encodes directional features and allows for the separation of relevant and irrelevant image parts. We provide theoretical and experimental evidence that ShearletX is resistant to explanation artifacts and demonstrate its effectiveness through new metrics for evaluating mask explanations. Our experimental results show that ShearletX outperforms previous approaches and is able to explain phenomena that were not explainable before. The source code for our experiments is publicly available.