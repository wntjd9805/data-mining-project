Recent advances in neural rendering have made it easier to reproduce virtual 3D objects from real-world objects. However, the scalability of the neural rendering approach is limited as it heavily relies on input images and cannot fully represent all possible variations of real and unreal objects. On the other hand, 3D generative adversarial network (GAN) models are more generalizable and extensible, allowing for easier configurations based on user intentions. This paper focuses on image manipulation in the latent space of 2D GANs, specifically exploring the manipulation of custom attributes of 3D objects synthesized using 3D-based GAN models. Existing works have not thoroughly explored fine-grained manipulation of custom attributes and mainly focus on 2D objects. Achieving view consistency during 3D image manipulation is crucial, and previous methods have shown inconsistencies in estimating attributes across viewpoints. To address this, the proposed model is based on a 3D-based GAN model equipped with an attribute quantifier and navigator. The quantifier estimates the quantity of attributes to be edited, and the navigator generates a manipulated image based on the quantified attribute. The attribute quantifier is trained on a small number of custom image samples, and the navigator is trained to generate and manipulate images corresponding to target custom attributes. The effectiveness of the proposed approach is evaluated in various attributes of 3D and 2D objects, confirming its qualitative and quantitative effectiveness.