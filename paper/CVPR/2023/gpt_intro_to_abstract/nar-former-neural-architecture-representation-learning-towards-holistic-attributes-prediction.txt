The current research and practical applications in deep neural network models have highlighted the need for neural architecture representation. This representation is crucial for tasks such as model accuracy prediction and network latency estimation. Existing models for neural architecture representation are often limited to specific applications and have drawbacks when applied to new tasks. This paper proposes a novel neural network encoding approach that utilizes a tokenized representation of the network's operation and topology information. This approach scales better and enables the use of transformer structures for network representation learning. A multi-stage fusion transformer model is designed to learn feature representations, and an information flow consistency augmentation technique is introduced to improve training efficiency. Extensive experiments validate the effectiveness of the proposed representation model in accuracy prediction, neural architecture search, and latency prediction tasks. Overall, the paper presents a simple yet effective approach to neural architecture representation that outperforms existing methods in various tasks.