The availability of vast amounts of "free" data on the Internet has been crucial for advances in deep learning and computer vision. However, this has also raised concerns about the unauthorized use of personal data for training commercial or malicious models. To address this issue, the concept of Unlearnable Examples (UEs) has been proposed, which involves adding protective noise to private data to make it unlearnable to machine learning models. Existing UE methods assume label-consistency, where the hacker and protector hold the same labels for protected data. However, this assumption is too idealistic, as hackers may collect unlearnable samples labeled for different tasks. To address this challenge, we propose a novel method called Unlearnable Clusters (UCs) that generates effective and transferable unlearnable examples under a label-agnostic setting. We demonstrate the limitations of existing UE methods and propose UCs as a solution, using cluster-wise perturbations. Additionally, we explore the selection of surrogate models to generate more effective and transferable UEs. We evaluate our approach in a black-box setting and compare it with existing UE methods against commercial machine learning platforms. Our results show the effectiveness of UCs in protecting private data and demonstrate the importance of addressing the label-agnostic setting for data protection.