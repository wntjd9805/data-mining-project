Recent developments in computer vision have shown the benefits of utilizing large-scale data for various computer vision tasks such as image retrieval, image recognition, and video recognition. However, there has been limited exploration in the object detection task due to the lack of a unified large-scale dataset. This paper aims to address this issue by proposing a method called "Detection Hub" that allows for training a single general object detector using multiple object detection datasets. The authors identify two main challenges in unifying these datasets: taxonomy difference and annotation inconsistency, both of which contribute to the domain gap issue. To tackle these challenges, the authors propose mapping the semantic category names of different datasets into a category-aligned embedding and dynamically adapting object queries based on the embedding. Additionally, the authors change the classification branch to vision-language alignment and adopt a region-to-word alignment loss instead of the traditional cross-entropy loss. By leveraging the linguistic properties of pre-trained language encoders and dataset-specific object query sets, the proposed method can overcome annotation inconsistencies and achieve better performance on multiple datasets. Experimental results demonstrate the effectiveness of the "Detection Hub" method on various standard object detection datasets as well as on a combination of 11 diverse datasets. The proposed method outperforms previous state-of-the-art approaches and showcases the potential for training a universal object detector for multiple datasets.