Sketch-based image retrieval (SBIR) is a popular area of computer vision research due to the expressiveness and flexibility provided by sketches. Traditionally, training deep neural photo-sketch encoders for SBIR requires datasets with matching photo-sketch pairs. However, acquiring sketches is laborious and time-consuming. This paper introduces a novel approach called Data-Free Sketch-Based Image Retrieval (DF-SBIR), which aims to train photo and sketch encoders for retrieval without any training data. This is achieved by leveraging pre-trained photo and sketch classification models. The problem is framed as data-free knowledge distillation, where the encoders are trained to match the predictions of the pre-trained models. Existing data-free knowledge distillation approaches have only been applied to single modality tasks, while SBIR is a cross-modal problem. This paper addresses this challenge by designing a unified, class-proxy based interface for teachers and students to interact. Additionally, a modality guidance network is introduced to ensure modality invariance in the encoder representations. The distribution estimation process is also designed to reconstruct class-aligned samples for metric learning. The proposed approach, termed CrossX-DFL, is evaluated on benchmark datasets and demonstrates competitive performance compared to the data-dependent setting.