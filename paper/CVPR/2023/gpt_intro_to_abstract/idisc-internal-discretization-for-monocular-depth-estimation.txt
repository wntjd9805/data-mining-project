Depth estimation is a crucial task in computer vision for understanding the geometric relations in a scene. It involves predicting the distance between the projection center and each pixel's corresponding 3D point. This task has direct significance in various downstream applications such as 3D modeling, robotics, and autonomous cars. However, depth estimation is an ill-posed problem due to its inherent scale ambiguity. In this paper, we propose a novel depth estimation model called iDisc, which does not impose any constraints on the final prediction. Instead, we design an Internal Discretization (ID) module that implicitly learns the underlying patterns or concepts in the scene, such as objects, planes, edges, and perspective relationships. This ID module consists of a continuous-to-discrete bottleneck, where the scene features are partitioned via learnable quantizers. We implement this bottleneck using attention-based operators, allowing for an end-to-end trainable architecture. Supervision is only applied to the final output, without any assumptions or regularization on the internal discrete representations (IDRs). We evaluate our iDisc model on multiple indoor and outdoor datasets, including two new benchmark datasets we propose for outdoor depth estimation. Our results demonstrate that iDisc consistently outperforms state-of-the-art methods and shows better transferability. Additionally, we apply iDisc to surface normal estimation and find that the proposed module is general enough to handle other dense prediction tasks. The contributions of this paper include the introduction of the ID module, which adeptly represents a scene by combining underlying patterns, and the establishment of new benchmark datasets for outdoor depth estimation.