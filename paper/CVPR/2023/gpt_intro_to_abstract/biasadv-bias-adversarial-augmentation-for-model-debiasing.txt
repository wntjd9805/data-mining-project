Real-world datasets often contain inherent biases, where certain visual attributes are correlated with class labels. This can lead to unintended shortcuts for neural networks trained on biased datasets, resulting in poor generalization in unbiased test environments. Conventional methods for addressing biases in datasets require explicit bias annotations or prior knowledge of the bias type, which can be expensive and limit universality. Recent research has utilized intentionally biased auxiliary models to train debiased models, focusing on re-weighting bias-conflicting samples identified by the auxiliary model. However, these methods often suffer from overfitting and degraded performance on bias-guiding samples. To overcome these limitations, data augmentation methods have been proposed, such as image-to-image translation and feature-level swapping. In this paper, we propose a simpler and more effective approach called Bias-Adversarial augmentation (BiasAdv) for generating bias-conflicting samples. BiasAdv utilizes an adversarial attack on the biased auxiliary model to generate adversarial images that alter the bias cue. These adversarial images are used as additional training data to improve the debiased model. Unlike previous methods, BiasAdv does not require complex image translation models or disentangled representations, making it applicable to any debiasing method. Experimental results show that BiasAdv significantly improves debiasing performance and model robustness, outperforming state-of-the-art results on various datasets. Ablation studies and analyses confirm the effectiveness of BiasAdv in learning generalizable representations and preventing overfitting without degrading performance on bias-guiding samples.