Deep neural networks (DNNs) are commonly used for vision tasks, but they are vulnerable to adversarial samples, which are carefully designed to mislead DNNs. Adversarial samples have negative effects on security-sensitive applications such as self-driving and medical diagnosis. To improve DNN model robustness against adversarial samples, it is necessary to understand DNN vulnerabilities and enhance attack algorithms. In the literature, there are white-box attacks, where attackers have access to the victim models, and black-box attacks, where attackers don't have access to the specifics of the victim models. Black-box attacks are more applicable to real-world systems and can be query-based or transfer-based. Transfer-based attacks are efficient as they craft adversarial samples with a local source model without querying the victim model. However, existing transfer-based attacks often have limited transferability due to overfitting to the local model. Existing solutions to improve transferability focus on optimization and generalization or use heuristics-based augmentation methods. These methods have limitations such as considering a single linear path or failing to constrain the image augmentation path length. To overcome these limitations, we propose the Path-Augmented Method (PAM), which augments images from multiple augmentation paths to improve transferability. We select representative path directions and settle the employed paths using a greedy search. We also train a Semantics Predictor to constrain the length of each augmentation path and avoid augmenting semantics-inconsistent images. Experimental results on the ImageNet dataset show that PAM outperforms state-of-the-art baselines in terms of attack success rates. Additionally, PAM combined with other compatible methods further improves performance. In summary, our contributions are discovering the limitations of existing attacks, proposing PAM, and conducting extensive experiments to validate its effectiveness.