Dynamic AI systems that have continual learning capabilities are expected to adapt to new classes while retaining knowledge of old classes. However, this can lead to a forgetting problem, where the training becomes dominated by new classes, resulting in a data imbalance between old and new classes. Previous approaches have attempted to address this issue by parameterizing and distilling exemplars or compressing exemplar images. In this paper, we propose a new approach based on image compression that downsamples only non-discriminative pixels while keeping the discriminative ones, thus increasing the quantity of exemplars without sacrificing their discriminativeness. To achieve selective and adaptive compression, we use the model's own attention mechanism to generate labels for discriminative pixels. We propose an adaptive version called class-incremental masking (CIM), which parameterizes a mask generation model and optimizes it in an end-to-end manner across all incremental phases. We optimize the CIM model together with the class-incremental learning (CIL) model in a global bilevel optimization problem and evaluate our approach on high-resolution benchmarks. Our experimental results show consistent and significant improvements compared to state-of-the-art methods.