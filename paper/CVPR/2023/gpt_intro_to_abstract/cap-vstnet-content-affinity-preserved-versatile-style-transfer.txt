This paper introduces a novel approach called CAP-VSTNet for photorealistic style transfer, which aims to reproduce a content image with the style from a reference image in a photorealistic way. The key challenge in style transfer is to achieve both clear content detail and consistent stylization in the transferred image. The existing methods typically use pre-trained VGG as the encoder, but this leads to content affinity loss. To address this issue, CAP-VSTNet uses a reversible framework consisting of a reversible residual network and an unbiased linear transform module based on Cholesky decomposition. This approach avoids content affinity information loss during forward and backward inference. However, directly using the reversible network results in redundant information accumulation, leading to content affinity loss and noticeable artifacts. To overcome this, CAP-VSTNet employs a channel refinement module to avoid redundant information accumulation. Additionally, the paper introduces cycle consistency loss to make the reversible network robust to numerical errors. The proposed CAP-VSTNet can be applied to various style transfer tasks, including photorealistic and artistic image/video style transfer. Extensive experiments demonstrate that CAP-VSTNet produces superior qualitative and quantitative results compared to state-of-the-art methods. It is also shown to perform stable video style transfer with minor modifications to the loss function.