This paper introduces the Multiscale Encoder Decoder Video Transformer (MED-VT), a novel approach that incorporates multiscale processing in both the encoding and decoding stages of video transformers. Previous works have focused on either multiscale transformer encoding or decoding, but not both simultaneously. Additionally, these previous approaches were mainly designed for single images and did not consider the structured prediction nature inherent in video tasks. In response to these gaps, MED-VT utilizes within and between-scale attention mechanisms during encoding to capture spatial, temporal, and integrated spatiotemporal information. At decoding, MED-VT introduces learnable coarse-to-fine queries to achieve precise target delineation, while enforcing temporal consistency among predicted masks through transductive learning. The utility of MED-VT is primarily demonstrated in the task of Automatic Video Object Segmentation (AVOS), where it successfully separates primary foreground objects from the background in videos without the need for supervision. AVOS is a challenging task with applications in autonomous driving and augmented reality. MED-VT addresses the challenges of AVOS by capturing appearance and motion information, guiding finer scale features for object delineation, and ensuring temporally consistent predictions. The paper also applies MED-VT to actor/action segmentation and provides an overview comparing MED-VT to other state-of-the-art multiscale video transformers.