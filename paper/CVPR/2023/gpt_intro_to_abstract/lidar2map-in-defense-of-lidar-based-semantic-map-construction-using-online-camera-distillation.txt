High-definition (HD) maps play a crucial role in navigation and path planning for autonomous driving. The recent focus has been on online construction of semantic HD maps using onboard LiDAR and cameras, which provides a compact representation of the environment around the vehicle. Existing methods for online semantic map construction can be categorized into camera-based, LiDAR-based, and Camera-LiDAR fusion methods. While camera-based methods leverage multi-view images with rich semantic information and achieve promising performance, LiDAR-based methods that utilize accurate 3D spatial information are less explored in this context. This paper introduces LiDAR2Map, an efficient framework for semantic map construction that fully utilizes the potentials of LiDAR-based models. The proposed framework includes a robust multi-scale bird's-eye view (BEV) feature decoder to improve the baseline performance. Additionally, a novel online Camera-to-LiDAR distillation scheme is introduced to leverage the semantic cues from the camera-based network, enhancing the performance of the LiDAR-based model. Experimental results on the challenging nuScenes benchmark demonstrate that LiDAR2Map outperforms conventional LiDAR-based methods and even achieves better performance than state-of-the-art camera-based methods. The contributions of this work include the efficient LiDAR2Map framework, the effective online distillation scheme, and comprehensive experiments showcasing the performance of LiDAR2Map in different settings.