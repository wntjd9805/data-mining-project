Abstract:With the increasing availability of multimedia data on the Internet, multimodal analysis is gaining attention in the field of computer science. However, learning from multimodal signals is still in its early stages and requires abundant labeled data, which is expensive and time-consuming to obtain. To address this issue, domain adaptation techniques have been proposed to learn models from labeled datasets that can be generalized to tasks without sufficient labeled data. In this paper, we focus on unsupervised domain adaptation (UDA) in multimodal scenarios where no samples in the target domain are annotated. Existing works often address the challenges of aligning source and target domains and leveraging multimodal information in two separate stages. However, these two issues are interconnected and their relationship should be considered. Unimodal domain adaptation methods are ineffective in multimodal tasks as they cannot preserve the relations between modalities. To overcome these challenges, we propose a One-Stage Alignment Network (OSAN) that unifies multimodal alignment and domain adaptation. Our method captures the relationship between domains and modalities, allowing for better performance in multimodal tasks. We introduce a TAL module to facilitate interactions between domains and modalities, and a DDG module to dynamically construct new domains based on the intrinsic structure of data distribution. Experimental results on different tasks demonstrate the effectiveness of our approach compared to supervised and UDA methods. Overall, our contributions lie in proposing a one-stage alignment network, developing modules for effective interaction between domains and modalities, and achieving better performance in multimodal analysis.