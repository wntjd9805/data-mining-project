Generating stylized fonts, particularly for glyph-rich scripts like Chinese and Korean, is a significant task. The style-content disentanglement paradigm has emerged as the popular approach for few-shot font generation (FFG) by separating font images into font-specific style codes and character-specific content features. However, existing methods that use global statistic features or fine-grained localized style representations have limitations in capturing the spatial transformations in font styles. In this paper, we draw inspiration from Neural Radiance Field (NeRF) in 3D view synthesis and propose a novel approach called Neural Transformation Field (NTF) for font generation. Instead of directly predicting pixel-level deformations, our method models font generation as a continuous transformation process by embedding desired spatial transformations into a neural transformation field. The NTF generates a set of intermediate transformations guided by estimated locations and corresponding transformation paths, and a font rendering formula accumulates these transformations into the target font image. We generalize the NTF into localized style representations, which show superior performance compared to universal style representations. Experimental results demonstrate that our proposed method outperforms state-of-the-art approaches in the few-shot font generation task, highlighting the effectiveness of our approach. In summary, our contributions include regarding font generation as a continuous transformation process, developing a differentiable font rendering procedure, and achieving superior performance in the FFG task.