Visual sentiment analysis (VSA) is an important computer vision task that aims to understand the emotions embedded in images. It has diverse applications ranging from opinion mining to entertainment assistance and business intelligence. Traditional VSA methods rely on hand-crafted features and classifiers, but they often fail to capture the high-level attributes required for accurate emotion recognition. With the advent of deep convolutional neural networks (DCNNs), there has been significant progress in VSA, as DCNNs excel in extracting high-level features. However, the performance of VSA heavily relies on the pre-trained models used. Due to the limited availability of annotated data for VSA, pre-training on large-scale datasets, such as ImageNet, has become a common practice to improve generalization. However, pre-training on ImageNet, which is designed for object classification, may not be optimal for VSA. In this paper, we argue that pre-trained models fail to capture sentiment-related initial states required for VSA and propose a novel pre-training method based on human visual sentiment perception mechanism. We split the pre-training process into three steps: stimuli taking, holistic organizing, and high-level perceiving, which mimic the chronology of human sentiment perception. We perform separate pre-training tasks for each step to extract sentiment-related features. To leverage the sentiment knowledge learned from pre-trained models, we propose an amalgamation strategy to effectively distillate their abilities into a single target model. We apply our method to various downstream VSA tasks and demonstrate significant performance improvements. Our contributions include the proposal of a sentiment-oriented pre-training method, the development of an amalgamation strategy to aggregate sentiment-discriminated knowledge, and extensive experiments demonstrating the effectiveness of our approach.