In this paper, we present MOOD (Masked Image Modeling for Out-of-Distribution Detection), a novel approach for detecting out-of-distribution (OOD) samples in visual recognition tasks. Unlike traditional methods that rely on outlier exposure or classification-based learning, MOOD leverages reconstruction-based methods to improve OOD detection performance. We introduce the masked image modeling (MIM) pretext task, where images are split into patches and a proportion of patches are randomly masked. The network then learns to predict the masked patches and reconstruct the original image using discrete VAE tokens as labels. This approach enables the model to learn from the intrinsic data distribution of images rather than relying on specific patterns among categories. Our extensive experiments demonstrate that MOOD outperforms the current state-of-the-art (SOTA) on various OOD detection tasks, including one-class OOD detection, multi-class OOD detection, near-distribution OOD detection, and even few-shot outlier exposure OOD detection. For example, MOOD significantly boosts the AUROC of current SOTA for one-class OOD detection by 5.7% to 94.9%, outperforms current SOTA for multi-class OOD detection by 3.0% and achieves an AUROC of 97.6%, surpasses the current SOTA for near-distribution OOD detection by 2.1% with an AUROC of 98.3%, and even defeats the current SOTA for few-shot outlier exposure OOD detection (which uses 10 OOD samples per class) with an accuracy of 99.41% without any OOD samples included in MOOD. Our findings highlight the effectiveness of reconstruction-based methods in improving OOD detection performance and showcase the potential of MOOD as a reliable visual recognition system for detecting and handling unknown OOD samples.