Temporal Action Localization (TAL) is an important problem in video understanding, which involves bounding semantic actions within start and end timestamps. TAL has various applications such as video-language grounding, moment retrieval, and video captioning. However, learning TAL approaches from long temporal scopes of videos with a large number of frames poses challenges due to the limited GPU memory. To address this issue, most TAL methods extract snippet-level features using a pre-trained video network in inference mode and then train a localizer on these features. This two-step strategy compromises the alignment and representation needs of TAL. In this paper, we propose a novel approach called Re2TAL to convert existing non-reversible video backbones into reversible ones. Our approach involves rewiring the architectural connections of a pre-trained backbone, which allows us to reuse the large compute resources invested in training the non-reversible backbones. We apply our technique to various representative video backbones and demonstrate that our reversible networks achieve similar performance with only minimal finetuning effort. Additionally, we propose an end-to-end TAL training approach using reversible video networks, which dramatically reduces GPU memory usage and boosts TAL performance compared to traditional feature-based approaches. With Re2TAL, we achieve state-of-the-art performance on ActivityNet-v1.3 and THUMOS-14 datasets.