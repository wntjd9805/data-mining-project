In recent years, end-to-end scene text spotting, which combines text detection and recognition in a single task, has gained attention due to its practical applications in fields such as visual navigation and document image understanding. This paper presents a novel approach to scene text spotting by formulating it as a sequence generation task. By treating the task as a language modeling problem conditioned on the image and text prompt, a sequence of discrete tokens can be generated using an auto-regressive transformer decoder. Previous studies have attempted to integrate text spotting into auto-regressive models, but there are challenges in directly applying the sequence generation approach to scene text spotting. One challenge is the various methods used to indicate the location and boundaries of text instances, each with its own trade-offs in terms of annotation costs and applicability. For example, a single central point or bounding box annotation may have low annotation costs but lack detailed shape information. To address this, the proposed method unifies various text detection formats, allowing for the extraction of arbitrary-shaped text areas. Additionally, the model can extract more texts than the decoder length permits by using a starting-point prompt, enabling the spotting of more texts than it has been trained on. Experimental results demonstrate that the proposed method achieves competitive performance on text spotting benchmarks and provides additional functionalities. The contributions of this work include the novel sequence generation-based approach for scene text spotting, the ability to extract texts in various detection formats using a single unified model, and the achievement of competitive performance on text spotting benchmarks.