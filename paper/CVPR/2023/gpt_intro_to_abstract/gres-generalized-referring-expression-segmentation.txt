This paper introduces the concept of Referring Expression Segmentation (RES), which involves finding and generating segmentation masks for target objects in images based on natural language expressions. The existing RES methods have limitations in terms of their constraints and lack of support for multi-target expressions. To address these limitations, the authors propose a new benchmark called Generalized Referring Expression Segmentation (GRES), which allows for expressions indicating any number of target objects. They also introduce a new dataset called gRefCOCO that complements the existing RefCOCO dataset with multi-target and no-target samples. Additionally, the authors propose a baseline method called ReLA for GRES, which explicitly models region-region interactions to handle the complexities of multi-target expressions. Extensive experiments and comparisons are conducted, demonstrating the effectiveness of the proposed method and pointing out new challenges in GRES. Overall, this paper presents a comprehensive approach to enhancing the flexibility and practicality of RES in real-world scenarios.