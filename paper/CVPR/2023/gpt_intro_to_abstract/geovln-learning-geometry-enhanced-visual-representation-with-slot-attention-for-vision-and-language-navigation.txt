In recent years, there has been significant progress in vision, robotics, and AI research, bringing us closer to the possibility of robots following human instructions to complete tasks. One fundamental problem in achieving this is enabling robots to make decisions based on natural language instructions and visual observations. This problem is known as Vision-and-Language Navigation (VLN). Existing approaches have used LSTM or Transformer models to process visual data, but they have limitations in incorporating geometry information, considering local spatial context, and building cross-modal representations. To address these limitations, we propose a novel framework called GeoVLN that utilizes slot attention and multimodal observations (RGB images, depth maps, and normal maps) to learn geometry-enhanced visual representations. We also introduce a multiway attention module to focus on informative visual observations based on different phrases in the instructions. Our experiments demonstrate the effectiveness and robustness of the GeoVLN framework in the Room-to-Room VLN task.