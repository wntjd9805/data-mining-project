Point cloud rendering is a useful technique in computer science for generating images from point clouds, which are commonly used in 3D visualization, navigation, and augmented reality. However, point clouds are often sparsely distributed in 3D scenes, resulting in unsatisfactory rendering quality with artifacts and missing details. Recent advancements in Neural Radiance Fields (NeRF) have enabled high-fidelity synthesis of novel views in 3D representation. However, most NeRF-based methods are scene-specific and require extensive training time, limiting their practical applications. In this paper, we propose Point2Pix, a novel point cloud renderer that can synthesize photo-realistic images from colored point clouds. Unlike other NeRF-based methods, Point2Pix does not require multi-view images or fine-tuning procedures for indoor scenes. We treat the point clouds as anchors for NeRF, allowing us to learn 3D point attributes from given locations. This enables supervised learning and improves performance. Additionally, we introduce a point-guided sampling strategy that focuses on the local areas of points in the point cloud, reducing the number of required samples while maintaining synthesis accuracy. We also propose Multi-scale Radiance Fields, which extract discriminative 3D features for any location in the scene. These features provide a 3D prior for subsequent applications, eliminating the need for finetuning. Our experimental results and ablation studies on indoor datasets demonstrate the effectiveness and generalization of Point2Pix. Our contributions include the development of Point2Pix as a method to render point clouds into photo-realistic images, an efficient ray sampling strategy and fusion decoder to accelerate the rendering process, and the introduction of Multi-scale Radiance Fields for extracting discriminative 3D features.