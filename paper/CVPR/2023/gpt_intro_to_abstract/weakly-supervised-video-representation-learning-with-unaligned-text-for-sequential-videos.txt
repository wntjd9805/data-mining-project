This paper introduces a novel approach for weakly supervised video representation learning in sequential videos with unaligned text. The goal is to develop a video understanding system that can learn knowledge from the open world in an embodied manner. Previous methods rely on annotations of temporal boundaries, which are difficult to obtain. The proposed approach leverages the rich information provided by text narrations accompanying sequential videos. A weakly supervised learning pipeline is proposed, which includes a global contrastive loss and a fine-grained contrastive learning loss to align frames with corresponding sentences. Pseudo-labels are generated based on the temporal relation of sentences, and an Info-NCE contrastive loss is calculated to guide the network in focusing on fine-grained action matching. Extensive experiments on video verification and text-to-video matching tasks demonstrate the effectiveness and generalization ability of the proposed method. The contributions of this work include the development of a novel weakly supervised video representation learning pipeline, the design of multiple granularity contrastive learning losses, and the demonstration of strong generalization ability to downstream tasks.