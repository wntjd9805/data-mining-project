This paper addresses the task of geometric matching, which involves identifying corresponding pixels in a source and support image for a 3D structure. Geometric matching is essential for various vision applications such as homography estimation, structure-from-motion, visual odometry estimation, and visual camera localization. Current methods for geometric matching can be categorized as sparse or dense, with sparse methods providing correspondence on sparse or semi-dense locations and dense methods estimating pixel-wise correspondence. Sparse methods assume a unique mapping between source and support frames, but they struggle with textureless surfaces and can result in ambiguous matching results. Dense methods, on the other hand, leverage fine-level local context and smoothness constraints to overcome these challenges and generally demonstrate better performance. However, the unique network components in geometric matching cannot benefit from pretraining tasks such as monocular pretraining for image classification and masked image modeling. To address this limitation, this paper introduces the concept of paired masked image modeling (pMIM) as a pretext task, where both the encoder and decoder of a dense geometric matching network are pretrained. The pretrained decoder is initialized to upsample the coarse-scale reconstruction to its original resolution, and a small number of randomly initialized components are pretrained using synthetic image pair augmentation. To further enhance the performance of dense geometric matching, a novel cross-frame global matching module (CFGM) is proposed. CFGM utilizes a correlation volume and positional embeddings to handle textureless local patches, and a homography loss is introduced to leverage the low-dimensional planar structure of textureless surfaces. The contributions of this paper are summarized as the introduction of pMIM for pretraining, the proposal of CFGM for robust matching, and the outperformance of existing methods on diverse datasets.