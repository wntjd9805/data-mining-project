Multi-task learning (MTL) has been instrumental in advancing autonomous driving and general vision tasks. In the context of autonomous driving, a robust vision perception system is crucial for providing critical information related to traffic participants, signals, lanes, and obstacles. Training these tasks independently is time-consuming and fails to exploit the relationships between them. Therefore, simultaneous multi-task learning is essential for improving data efficiency and reducing training and inference time. Previous works have attempted unified training on multiple tasks in autonomous driving, but they differ in task types and evaluation metrics, making it challenging to compare their performances. This paper focuses on heterogeneous multi-task learning in common autonomous driving scenarios, covering popular self-driving tasks such as object detection, semantic segmentation, drivable area segmentation, and lane detection. The study evaluates existing multi-task learning methods on a large-scale driving dataset and finds that task scheduling performs better than zeroing loss but worse than pseudo labeling. Uncertainty produces satisfactory results on most tasks, while MGDA performs well only on lane detection. To mitigate the negative transfer problem, the paper proposes the visual exemplar-driven task-prompting (VE-Prompt) framework. VE-Prompt leverages exemplars containing target object information to generate task-specific prompts, bridging transformer encoders and convolutional layers to build a hybrid multi-task architecture. Experimental results demonstrate that VE-Prompt outperforms multi-task baselines significantly. The main contributions of this work include an in-depth analysis of current multi-task learning approaches, the proposal of the VE-Prompt framework, and its computational efficiency and superior performance on all tasks.