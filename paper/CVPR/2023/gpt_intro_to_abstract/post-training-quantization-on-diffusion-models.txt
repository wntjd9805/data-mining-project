Recently, denoising diffusion generative models have been successful in various generative tasks such as images, audio, video, and graphs. These models have shown superior performance compared to Generative Adversarial Networks (GANs) in terms of quality and diversity. However, the long iterative process and high inference cost for generating samples make these models undesirable. To address this, researchers have proposed approaches to accelerate diffusion models, mainly focusing on sample trajectory learning for faster sampling strategies. This paper introduces a novel acceleration method called Post-Training for Diffusion Models (PTQ4DM), which incorporates network compression techniques to accelerate denoising diffusion models. Previous acceleration methods overlooked the cumbersome network for estimating noise in each iteration, while PTQ4DM addresses this issue by directly quantizing noise estimation networks in a post-training manner. Experimental results show that PTQ4DM can quantize pre-trained diffusion models to 8-bit without significant performance loss and can be easily integrated with other state-of-the-art diffusion model acceleration methods. Overall, this paper makes three contributions: it introduces PTQ into DM acceleration, investigates the performance drop induced by PTQ for DMs, and proposes PTQ4DM, which is the first work to investigate diffusion model acceleration from the perspective of training-free network compression.