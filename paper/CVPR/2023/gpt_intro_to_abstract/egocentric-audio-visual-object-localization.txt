The emergence of wearable devices has sparked interest in egocentric videos within the research community. These videos have applications in robotics, augmented/virtual reality, and healthcare. The computer vision community has made significant efforts to build benchmarks, establish new tasks, and develop frameworks for understanding egocentric videos. While existing works show promising results, fine-grained egocentric video understanding remains a challenging task. One such challenge is identifying the object emitting sound in a first-person recording, as demonstrated by the dynamic changes in visual and audio cues caused by head movement and occlusion. To tackle these challenges, this paper introduces a novel egocentric audio-visual object localization task and proposes a framework that integrates audio and visual information. The framework includes a geometry-aware temporal module to handle egomotion and a cascaded feature enhancement module to handle out-of-view sounds. The proposed approach is trained in a self-supervised manner and evaluated on a newly annotated Epic Sounding dataset. Experimental results demonstrate improved localization performance. The contributions of this work include the first systematic study on egocentric audio-visual sounding object localization, an effective approach to handle egomotion, a novel feature enhancement module, and a benchmark dataset for evaluating localization performance in egocentric videos.