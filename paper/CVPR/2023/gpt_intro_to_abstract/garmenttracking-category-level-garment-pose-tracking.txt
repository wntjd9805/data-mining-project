This paper introduces the task of category-level garment pose tracking and proposes a new dataset and method to address this problem. The authors extend the single-frame pose estimation setting to pose tracking in dynamic videos, specifically focusing on garment manipulation tasks. They highlight the challenges of extreme garment deformation and the lack of existing datasets for garment manipulation with complete pose annotations. To overcome these challenges, the authors develop a VR-based recording system called VR-Garment and create a large-scale dataset called VR-Folding. They also propose an end-to-end online tracking method called GarmentTracking, which involves three stages: NOCS predictor, NOCS refiner, and warp field mapper. The authors compare their method to a baseline approach and conduct ablation experiments to evaluate its efficacy. They also collect real-world data on garment manipulation and demonstrate the qualitative results of their method. The contributions of this paper include the VR-Garment recording system, the VR-Folding dataset, and the GarmentTracking framework, which can serve as a strong baseline for further research.