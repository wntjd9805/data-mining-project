Visual localization is a crucial task in various applications such as self-driving cars, drones, and augmented reality. Different algorithms represent the scene and estimate the camera pose differently. Most approaches use RGB(-D) images, but capturing and estimating camera poses for reconstruction can be difficult. Recent research has shown that modern local features are capable of matching real images with renderings of 3D models, even without explicit training. This suggests that ready-made 3D models downloaded from the internet can be used for scene representation in visual localization, simplifying the deployment of such applications. However, using internet models presents challenges such as fidelity of appearance and geometry. This work quantifies the relation between model inaccuracy and localization accuracy to inform AR application designers. Humans can establish correspondences between even coarse and untextured models, suggesting the possibility of teaching machines to do the same. The paper introduces the task of visual localization with 3D models downloaded from the internet, provides a benchmark with scenes and models of different fidelity levels, and presents experiments evaluating how fidelity affects localization performance. The results show room for improvement, especially for less realistic models, and a publicly available benchmark is provided to encourage research on algorithms for this challenging task.