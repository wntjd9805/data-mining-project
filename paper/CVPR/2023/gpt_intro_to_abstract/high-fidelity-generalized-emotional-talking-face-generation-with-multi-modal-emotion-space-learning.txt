This paper introduces the task of emotional audio-driven talking face generation, which involves driving a static portrait with given audio while maintaining synchronized lip movements and generating natural facial motion. While previous works have made progress in generating talking faces conditioned on emotion embedding, there are still critical issues to be addressed. These include exploring more semantic emotion embedding for better generalization, constructing multi-modal emotion sources for flexible and user-friendly emotion control, and designing a high-resolution identity-generalized generator. To tackle these challenges, this paper proposes an approach that supplements emotion styles in the text prompt, allowing flexible specification and precise reflection of unseen emotions. The approach also introduces an Aligned Multi-modal Emotion (AME) encoder to unify text, image, and audio emotion modalities into the same domain, enabling flexible emotion control using multi-modal inputs. The fixed CLIP text and image encoders are used to extract their embeddings, and a learned CLIP audio encoder guides the proper emotion representation of the given audio sequence in CLIP space.Furthermore, the paper proposes an Emotion-aware Audio-to-3DMM Convertor (EAC) that distills rich emotional semantics from AME and projects them to the facial structure using a Transformer. This allows the capture of longer-term audio context and the learning of correlated audio-emotion features for expression coefficient prediction. An intensity token is also introduced to control emotion intensity continuously.To generate high-resolution realistic faces, the paper proposes a style-based identity-generalized model called High-fidelity Emotional Face (HEF) generator. This model integrates appearance features, geometry information, and a style code in a hierarchical manner, allowing for efficient flow estimation and texture refinement.Overall, this paper makes three contributions: the AME that provides a unified multi-modal emotion space, the HEF generator that hierarchically learns facial deformation for high-resolution one-shot generation, and extensive experiments demonstrating the superiority of the proposed method for flexible and generalized emotion control and high-resolution talking face animation.