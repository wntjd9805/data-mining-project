In recent years, 3D generative models have been developed to extend 2D generative models for multi-view consistent and explicitly pose-controlled image synthesis. These models combine 2D CNN generators with 3D inductive bias from neural rendering, allowing for the efficient synthesis of high-resolution photorealistic images with view consistency and detailed 3D shapes. However, training these models is challenging as it requires a large set of images and information on the camera pose distribution. This limitation restricts these models to domains where camera parameters are annotated or off-the-shelf pose extractors are available. Existing text-guided domain adaptation methods have been developed for 2D generative models to overcome the data curation issue for the target domain. These methods leverage CLIP models pre-trained on image-text pairs to perform text-driven domain adaptation. However, these methods suffer from a loss of diversity in the generated samples due to the deterministic nature of CLIP text embeddings. In this paper, we propose a novel method called DATID-3D, which leverages text-to-image diffusion models to synthesize diverse pose-aware target images. We then apply our method to a state-of-the-art 3D generator, demonstrating superior quality, diversity, and text-image correspondence compared to existing 2D text-guided domain adaptation methods for 3D generative models. We also propose one-shot instance-selected adaptation and single-view manipulated 3D reconstruction to extend the usefulness of generative models.