This paper introduces the problem of Multispectral Video Semantic Segmentation (MVSS), which involves labeling each pixel in a video with category labels using both RGB and thermal images. Existing semantic segmentation networks primarily work with RGB images and may fail in adverse conditions. Thermal images provide complementary information and are relatively insensitive to lighting conditions, making them useful for semantic segmentation. However, existing RGBT segmentation methods are based on single images and lack mechanisms to account for temporal contexts in dynamic scenes. The authors address this limitation by proposing MVSS and curating a dataset called MVSeg, comprising synchronized RGB and thermal video sequences with fine-grained semantic annotations. They also introduce a baseline model, called MVNet, which utilizes a prototypical MVFuse module to attend to contextual multispectral video features and a novel MVRegulator loss to reduce cross-spectral modality difference. Experimental results on the MVSeg dataset demonstrate the significance of multispectral video data for semantic segmentation and validate the effectiveness of the MVNet model. The authors expect that the MVSeg dataset and MVNet baseline will facilitate further research in the field of MVSS.