Generative Adversarial Networks (GANs) have become an important technique for image generation tasks, but they often suffer from overfitting when trained on limited data. Previous approaches to address this issue include data augmentation and model regularization. In this paper, we propose a new approach based on knowledge distillation (KD) from vision-language models. We introduce KD-DLGAN, a knowledge-distillation based image generation framework that aims to mitigate discriminator overfitting and improve generation performance. We design two generative KD techniques, aggregated generative KD (AGKD) and correlated generative KD (CGKD), which distill knowledge from a powerful vision-language model called CLIP. AGKD aggregates features of real and fake samples, while CGKD enforces diverse correlations between images and texts. These techniques effectively mitigate discriminator overfitting and improve generation diversity. Our contributions are threefold: 1) we propose KD-DLGAN, the first framework to apply knowledge distillation in data-limited image generation; 2) we introduce AGKD and CGKD techniques that mitigate discriminator overfitting and improve generation performance; 3) extensive experiments demonstrate that KD-DLGAN achieves superior image generation and complements state-of-the-art approaches.