This paper introduces a novel method for 3D part segmentation using pretrained image-language models. The goal is to address the low-shot (zero- and few-shot) 3D part segmentation problem by leveraging the knowledge learned from large-scale image-text pairs. The proposed method integrates the GLIP model, which is pretrained on visual grounding and detection tasks, with 3D point cloud data and a text prompt containing part names. The GLIP model detects parts of interest in 2D views and outputs bounding box detections. A 3D voting and grouping module is then used to convert these 2D bounding boxes into 3D semantic and instance segmentation. The paper also introduces prompt tuning and multi-view feature aggregation techniques to improve the performance of the GLIP model. The proposed method is evaluated on a benchmark dataset and achieves excellent zero-shot and few-shot performance in 3D part segmentation. The contributions of this paper include the introduction of a novel 3D part segmentation method, the development of a 3D voting and grouping module, the utilization of prompt tuning and multi-view feature aggregation, and the creation of a benchmark dataset for low-shot and text-driven 3D part segmentation.