Visual grounding is a task in computer vision that aims to detect or segment specific objects based on natural language descriptions. Unlike general object detection or instance segmentation, visual grounding allows for more flexibility and purpose by allowing free-formed language descriptions to specify specific visual properties of the target object. Previous visual grounding approaches have typically followed general object detection frameworks and focused on cross-modal interaction modules. However, the visual backbone, which passively extracts visual features with fixed weights, has not been well explored.This paper introduces VG-LAW, an active perception Visual Grounding framework based on Language Adaptive Weights. VG-LAW dynamically adjusts the behavior of the visual backbone by injecting information from referring expressions into the weights. It first obtains language-adaptive weights through linguistic feature aggregation and weight generation processes. With the language-aware visual backbone, VG-LAW can extract expression-relevant visual features without manually modifying the visual backbone architecture. This eliminates the need for additional cross-modal fusion modules and results in a more streamlined network architecture.Additionally, this paper proposes a lightweight multi-task prediction head for referring expression comprehension and referring expression segmentation tasks based on the expression-relevant features extracted by VG-LAW. Experimental results on multiple datasets demonstrate the effectiveness of VG-LAW, achieving state-of-the-art performance. The contributions of this paper include proposing VG-LAW, which actively extracts expression-relevant visual features, and demonstrating the effectiveness of the framework and multi-task head on various datasets.