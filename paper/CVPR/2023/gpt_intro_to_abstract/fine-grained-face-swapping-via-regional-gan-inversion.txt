Face swapping is a technique used to transfer the identity information from one face to another while maintaining the other attributes of the target face. This technique has gained significant attention in the field of computer vision and graphics due to its potential applications in the entertainment and film production industry. The primary challenge in face swapping is preserving the unique facial characteristics of the source image. Existing methods rely on pre-trained 2D face recognition networks or 3D morphable face models to extract identity-related features, but these models are designed for classification rather than generation, leading to a loss of important visual details. Additionally, previous methods often result in "in-between" faces that do not faithfully preserve the source identity. Another challenge in face swapping is handling facial occlusion, such as hair occluding parts of the face. Existing approaches to handle occlusion suffer from issues such as blurry results or the reintroduction of target identity information. To address these challenges, this paper proposes a new perspective called "editing for swapping" (E4S) that disentangles shape and texture explicitly for better identity preservation. Instead of relying on global identity features, the paper utilizes local feature extraction through component masks. A mask-guided generator is then used to synthesize the final swapped face. The E4S framework also naturally handles facial occlusion by using masks provided by a face parsing network, eliminating the need for additional modules. The paper introduces a novel Regional GAN Inversion (RGI) method that disentangles shape and texture using a pre-trained StyleGAN. This new inversion latent space allows for the editing of each individual face component, enabling various applications beyond face swapping. Extensive experiments demonstrate the effectiveness of the proposed framework and RGI method in achieving high-fidelity face swapping with identity preservation and occlusion handling.