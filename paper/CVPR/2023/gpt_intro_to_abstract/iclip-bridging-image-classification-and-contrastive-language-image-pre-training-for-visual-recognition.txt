This paper introduces the iCLIP framework, which aims to combine image classification and contrastive language-image pre-training methods for better visual recognition and representation learning. The iCLIP framework utilizes a shared text encoder and cosine classifier, similar to the CLIP approach, to improve classification accuracy and transferability. Additionally, the framework incorporates external knowledge from dictionaries to enhance class names, addressing the issue of ambiguous and polysemous terms. Experimental results demonstrate that the iCLIP framework outperforms individual task learning and naive multi-task learning, achieving superior performance in in-domain/zero-shot classification and multi-modal retrieval tasks. The framework also demonstrates strong transferability on various downstream tasks, including semantic segmentation, long-tail detection, and action recognition. Overall, this paper contributes by combining image classification and CLIP approaches, adapting the image classification formulation, and introducing knowledge bases into computer vision problems.