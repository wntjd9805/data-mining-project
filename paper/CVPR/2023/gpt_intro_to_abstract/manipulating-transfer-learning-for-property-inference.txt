Transfer learning is a widely used method in deep learning models to enhance efficiency by reusing pretrained models. However, this centralized approach is vulnerable to security risks. In this paper, we focus on the risk of property inference in transfer learning, where an adversary aims to extract sensitive properties of the training distribution of a model. We consider a scenario where a malicious upstream trainer crafts a pretrained model to enable an inference attack on the downstream model, revealing precise and accurate information about the downstream training data. We develop methods to manipulate the upstream model training process to produce a model that increases property inference risk. Our experiments show that the manipulated models significantly outperform baseline models in inferring target properties. We also explore possible detection methods for these manipulated models and propose stealthy attacks that evade detection while maintaining attack effectiveness.