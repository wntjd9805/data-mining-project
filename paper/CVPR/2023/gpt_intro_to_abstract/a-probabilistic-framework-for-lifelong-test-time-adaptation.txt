Deep learning models have shown impressive performance on data from the same distribution as the training data. However, their performance significantly degrades when faced with test inputs from a different distribution. This issue can be especially challenging when there is no labeled target domain data available for fine-tuning the model, and unsupervised adaptation is required at test time. This scenario, known as test-time adaptation (TTA), poses several challenges including the lack of access to the source domain training data and the need to handle continually changing environments.Current approaches addressing TTA rely on techniques like self-training based pseudo-labeling or entropy minimization to improve performance under distribution shift during testing. However, these methods suffer from instability when the target test inputs come from a continually changing environment. Therefore, there is a need for continual test-time adaptation to handle long-term model adaptation while preserving knowledge about the source domain.Continual adaptation poses challenges such as noise and miscalibration in pseudo-labels due to the continually changing test distribution, leading to error accumulation and catastrophic forgetting. Existing TTA methods also do not consider model or predictive uncertainty, resulting in miscalibrated predictions.To address these challenges, we propose a principled, probabilistic framework for lifelong TTA. Our framework constructs a posterior distribution over the source model weights and a data-dependent prior, which incorporates knowledge of the source domain data. The learning objective includes a self-training-based cross-entropy loss with a regularizer term derived from the posterior. This regularizer helps handle the continual adaptation of the model while preventing catastrophic forgetting.Furthermore, our framework offers a probabilistic perspective and justification for the CoTTA approach. It also introduces a data-driven parameter restoration technique based on the Fisher Information Matrix (FIM) to handle error accumulation and catastrophic forgetting. Our method surpasses existing approaches in terms of accuracy and calibration during distribution shift.The main contributions of our work include the probabilistic formulation of the student-teacher training framework, the emergence of the student-teacher cross-entropy loss with a regularizer term, and the introduction of a data-driven parameter restoration technique based on FIM. Our proposed framework, termed Probabilistic lifElong Test-time Adaptation with seLf-Inspired from the training prior (PETAL), improves upon current TTA methods and achieves state-of-the-art results on various benchmarks.