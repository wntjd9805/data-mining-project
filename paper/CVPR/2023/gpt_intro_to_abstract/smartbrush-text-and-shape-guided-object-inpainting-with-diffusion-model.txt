This paper introduces a text and shape guided object inpainting diffusion model, which aims to fill missing areas in images conditioned on both a text description and the shape of the object to be inpainted. The model utilizes diffusion models, specifically Stable Diffusion, to generate high-quality images. However, existing methods suffer from text misalignment and mask misalignment, where the inpainted content does not precisely align with the text description or the given mask. To address these challenges, the paper proposes the use of precision factor input masks, where users can provide either detailed or coarse masks to guide the model. The model is trained to fill the entire mask accurately or insert the desired object within a rough bounding box. Additionally, a regularization loss is used to predict an instance mask of the generated object to preserve the background consistency. The model achieves state-of-the-art results on object inpainting tasks across multiple datasets and is preferred by users compared to other methods in terms of shape, text alignment, and realism. The paper also introduces a multi-task training strategy by jointly training object inpainting with text-to-image generation to improve performance and leverage more training data.