Vision-language pre-training (VLP) aims to learn multi-modal representations from image-text pairs. While previous approaches such as coarse-grained image-text matching have shown promising results, they suffer from modality bias and the under-utilization of unmasked tokens. This paper proposes a new method called EPIC (Leveraging Per Image-Token Consistency) to address these limitations. EPIC masks salient tokens in the image and generates inconsistent alternatives from a language model, forcing the model to refer to the visual modality for consistency determination. This allows for better vision-language associations and efficient use of unmasked tokens. The effectiveness of EPIC is demonstrated through experiments on various pre-training approaches, achieving significant improvements in downstream tasks such as image-text retrieval and visual reasoning. Additionally, EPIC enables better generalization of pre-training models in zero-shot image-text retrieval.