In this paper, we introduce the problem of long-term single-image view synthesis, which aims to generate a video sequence from a single image and a sequence of camera poses. Existing methods for single-image view synthesis are limited to small ranges of camera motion. To address this limitation, we propose a pose-guided diffusion model that leverages diffusion models for content creation and incorporates epipolar attention layers to ensure consistency and realism in the synthesized views. We evaluate our approach on real-world and synthetic datasets and demonstrate its ability to generate high-quality, consistent, and realistic long-term view synthesis results. Our contributions include the proposal of a pose-guided diffusion model and the validation of its effectiveness in generating realistic and consistent long-term view synthesis results.