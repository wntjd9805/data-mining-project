In this paper, we address the issue of requiring a large amount of 3D annotations for object detection and segmentation from LiDAR point clouds. To alleviate this problem, we explore learning representations from unlabeled point clouds using self-supervised methods. We find that existing self-supervised pretext tasks do not bring adequate improvements to downstream tasks, so we propose a new set of geometry aware prediction targets for point clouds. We introduce a self-supervised learning framework dedicated to point clouds, inspired by modeling and computational techniques in geometry processing. Our method includes a point cloud voxelizer, a feature encoder, a Transformer encoder, and a Transformer decoder to reconstruct original voxelized point clouds. We also predict point statistics and surface properties in parallel branches. We conduct experiments on large-scale point cloud datasets and demonstrate that our self-supervised pre-training with the proposed objectives significantly boosts the performance of 3D object detection. Our contributions include introducing geometry aware self-supervised objectives, achieving state-of-the-art performance on various downstream tasks, and conducting comprehensive ablation studies to understand the effectiveness of each module and learning objective in our approach.