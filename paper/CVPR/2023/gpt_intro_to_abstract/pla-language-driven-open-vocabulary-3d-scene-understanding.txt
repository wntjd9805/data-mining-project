The field of 3D scene understanding is crucial for various real-world applications such as robot manipulation, virtual reality, and human-machine interaction. While deep learning models have achieved great success in this area, they are limited to understanding only the semantic categories present in the annotated dataset they are trained on. This restricts their applicability in real-world scenarios with unseen categories. Additionally, the annotation costs for 3D datasets make it impractical to cover all real-world categories. Hence, there is a need to study open-vocabulary 3D scene understanding, which enables models to recognize and localize open-set classes beyond the labeled space of annotated datasets. Previous attempts have projected 3D data into 2D modalities, such as RGB images and depth maps, but they suffer from issues like high computation and memory costs, information loss, and subpar performance. In this paper, we propose a method that leverages pre-trained vision-language (VL) foundation models to build an explicit association between 3D and language for open-vocabulary understanding. We use textual descriptions obtained from easily-obtained image data aligned with 3D data to distill semantic-rich information. We also address the challenge of connecting objects with corresponding words in the caption by leveraging the 3D geometry to build hierarchical point-caption pairs. Our method, called Point-Language Association (PLA), is generic and applicable to various open-vocabulary 3D scene understanding tasks. Experimental results on ScanNet and S3IDS datasets demonstrate the effectiveness and transferability of our approach, surpassing baselines in both semantic and instance segmentation tasks. Our model also shows scalability and extensibility by benefiting from more advanced foundation models that provide higher-quality caption supervision.