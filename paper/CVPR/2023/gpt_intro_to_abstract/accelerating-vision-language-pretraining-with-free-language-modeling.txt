Abstract:Vision-language pretraining (VLP) has achieved impressive results on various vision-language tasks. One of the dominant pretraining objectives is masked language modeling (MLM), but it suffers from slow convergence and long training time. This paper proposes a new pretraining task called free language modeling (FLM) that allows for a 100% prediction rate and bidirectional contextualized representations. The FLM objective breaks the entanglement between corruption and prediction rates and introduces a flexible perception of bidirectional contexts. The paper presents an encode-corrupt-predict framework for VLP using FLM, which achieves competitive performance with MLM while reducing pretraining time by more than 50%. Experimental results on multiple vision-language tasks validate the effectiveness of FLM.