The robustness of deep learning models in handling real-world distribution shifts is vital for various vision applications, such as medicine, autonomous driving, conversational systems, robotics, and more. These distribution shifts can arise from changes in geographical location, background, lighting, camera models, object scale, orientations, and other environmental variations. However, these shifts can lead to model failures when deployed in real-world settings. While robustness has been extensively studied in image-based tasks, studying robustness in videos is critical for developing reliable systems for real-world deployment. In this paper, we conduct a large-scale analysis of the robustness of existing deep models for video action recognition against common real-world spatial and temporal distribution shifts. We propose four benchmark datasets and introduce various common perturbations to evaluate model robustness. We explore the effectiveness of temporal modeling, the robustness of models to real-world corruptions, the necessity of heavy architectures, and the robustness of transformer-based models. Our study reveals that transformer-based models outperform CNN models not only in terms of performance but also in robustness against distribution shifts. Furthermore, we observe that pretraining is more beneficial for transformers than CNN models in terms of robustness. We also analyze the role of data augmentations in model robustness and investigate their generalization to novel perturbations. Additionally, we introduce a real-world dataset with realistic distribution shifts to further understand the behavior of CNN and transformer-based models. This study provides important insights into video action recognition and paves the way for future research on video robustness and the development of more robust architectures for real-world deployment.