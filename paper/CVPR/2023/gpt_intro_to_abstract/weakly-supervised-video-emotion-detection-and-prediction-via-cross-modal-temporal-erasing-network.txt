Emotion analysis in user-generated videos (UGVs) has gained significant attention due to the increasing number of people expressing their opinions on social networks. Automatic predictions of video emotions have numerous applications such as online content filtering, attitude recognition, and customer behavior analysis. However, existing methods primarily focus on extracting keyframes from visual content, assuming that these frames hold the dominant information for conveying emotions. This approach has limitations as keyframes may fail to accurately represent intended emotions due to the subjective and ambiguous nature of human emotions. Additionally, keyframes may overlook important contextual information that could provide complementary cues for emotion recognition. To address these challenges, we propose a novel cross-modal temporal erasing network that considers both keyframes and contextual frames in a unified framework. Our method includes a temporal correlation learning module that discovers implicit correspondences between audio and visual segments, as well as a temporal erasing module that encourages the model to detect complementary information from context. Our contributions include introducing a weakly-supervised network that exploits keyframes and context for better video emotion analysis representation, and leveraging intra- and inter-modal relationships to provide localized information with only video-level annotations. We demonstrate the effectiveness of our approach through extensive experiments, achieving state-of-the-art results on three video emotion datasets.