Jakob von Uexk√ºll recognized the significant role that an organism's perceived environment (umwelt) plays in its life. This perception is shaped by the organism's interactions with the environment. Functional visual understanding and affordances, which determine the actions a scene allows an agent to perform, have been explored in psychology and vision research. However, there is still a lack of a comprehensive computational model for affordance perception. Recent years have seen a growing interest in data-driven computational models for affordance perception. While early approaches relied on intermediate semantic or 3D information, newer methods focus on direct perception of affordances. However, these methods are limited by specific dataset requirements, hampering their generalizability. To address this, we draw inspiration from large-scale generative models and propose a novel approach that explicitly predicts affordances by inpainting people into masked scenes. We train our model on videos of human activities, enabling it to learn scene affordances and necessary re-posing and harmonization. At inference time, the model can generate realistic people and scenes and perform partial human completion tasks. Our contributions include a self-supervised task formulation for learning affordances, a large-scale generative model for human insertion trained on millions of video clips, and the ability to prompt the model in various ways for person hallucination, scene hallucination, and interactive editing.