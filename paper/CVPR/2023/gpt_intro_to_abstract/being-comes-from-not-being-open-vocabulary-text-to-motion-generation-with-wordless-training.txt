Motion generation has become increasingly important in the fields of virtual reality, video games, and movies. In particular, text-conditional motion generation has the potential to greatly enhance user experiences by allowing virtual avatars to react to communication texts in real time. However, current text-to-motion approaches are limited in their ability to handle unseen open-vocabulary texts, as they are trained on paired text-motion data with limited types of annotations. In order to address this issue, recent works have utilized the zero-shot text-image alignment ability of pretrained models such as CLIP. However, these approaches still require paired data and struggle with texts that are dissimilar to the training texts. To fill the gap in offline open-vocabulary text-to-motion generation, we propose a method called OOHMG (Offline Open-vocabulary Human Motion Generation). Our approach tackles two main challenges: how to associate diverse texts and poses to supervise the pose generator, and how to obtain diverse texts as training inputs. To address the first challenge, we introduce TPA (Text-Pose Alignment), a large-scale text-pose alignment model based on CLIP. TPA efficiently measures the alignment between texts and 3D SMPL poses in the feature space, enabling the text-to-pose generator to learn to generate poses for texts through gradient descent. For the second challenge, we introduce wordless training, a novel training paradigm that samples random training inputs from the latent space of texts instead of collecting massive texts. Through extensive experiments, we demonstrate that OOHMG is able to efficiently and effectively generate motions for open-vocabulary texts, outperforming advanced baseline methods both qualitatively and quantitatively. Overall, our contributions include the proposal of the OOHMG framework, the development of the TPA alignment model, and the introduction of the wordless training mechanism to handle open-vocabulary texts.