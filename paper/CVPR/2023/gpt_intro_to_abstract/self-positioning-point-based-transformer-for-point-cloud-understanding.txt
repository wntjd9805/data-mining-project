This paper introduces a novel Transformer architecture called Self-Positioning point-based Transformer (SPoTr) for point clouds. Point clouds are widely used in applications such as autonomous driving and augmented reality, but their irregular structure makes it challenging to apply convolutional neural networks (CNNs) to them. Previous approaches have either transformed point clouds into regular structures or designed convolutions on the point space, but they have limited ability to capture long-range dependencies. To address this issue, the paper proposes SPoTr, which consists of two attention modules: local points attention (LPA) and self-positioning point-based attention (SPA). LPA learns local structures while SPA embraces global information by computing attention weights with a small set of self-positioning points. SPoTr achieves improved performance in shape classification and semantic segmentation tasks compared to other attention-based methods. The paper also demonstrates the effectiveness and interpretability of SPA through qualitative analyses. The contributions of this work include the design of SPoTr architecture, the introduction of SPA, the achievement of state-of-the-art performance on benchmark datasets, and the effectiveness of SPA in capturing global shape contexts.