Medical image segmentation is a crucial task in medical image analysis, and deep learning-based methods have significantly improved segmentation performance. However, the requirement for a large-scale, precisely labeled dataset is expensive and laborious. Additionally, different radiologists may provide different annotations for the same image, further complicating the process. This paper explores ways to alleviate the quantity and quality requirements of manual annotation in medical image segmentation. Two paradigms, weakly-supervised segmentation and semi-supervised segmentation, are commonly used to address this issue. Weakly-supervised methods utilize weak annotations, but they often struggle with fuzzy boundaries and increased computational burden. Semi-supervised methods, while successful, still require full 3D annotation for each labeled volume. This paper proposes a novel annotation method called orthogonal annotation, which only labels two slices in the orthogonal direction of a labeled volume. This approach forces the model to learn from complementary views while reducing the annotation costs. To incorporate orthogonal annotation, the authors propose a registration-based label propagation method and a Dense-Sparse Co-training (DeSCO) framework. DeSCO leverages dense pseudo labels and unlabeled volumes to gradually improve the model's performance. Experimental results on three public datasets show that the proposed barely-supervised method achieves comparable or even superior performance compared to fully annotated semi-supervised methods.