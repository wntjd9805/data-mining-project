Object detection has made significant progress with the advent of deep networks and the availability of large-scale annotated datasets. However, these models often struggle to generalize to visual domains that were not encountered during training. To address this, most existing works focus on unsupervised domain adaptation (UDA) methods that aim to minimize domain discrepancy by aligning feature distributions between the source and target domains. However, in practical scenarios, accessing source data for adaptation is often restricted due to various concerns. Therefore, it is more efficient to adapt the source-trained model without accessing the source data. This motivates the study of Source-Free Domain Adaptation (SFDA) for object detectors. SFDA is a more challenging setting than UDA as it does not have access to both labeled source and unlabeled target data during adaptation. Most SFDA methods use pseudo-labels generated by a source-trained model for training. However, challenges arise from noisy pseudo-labels and instances with inconsistent predictions. To tackle these challenges, we propose a mean-teacher framework for effective distillation of target domain knowledge and employ contrastive representation learning (CRL) methods to enhance target domain feature representations. Our proposed method utilizes the architecture of a detection model and a Graph Convolution Network (GCN) to model the inter-instance relations for generated proposals, which guide the contrastive learning process. Experimental results demonstrate that our method outperforms existing source-free domain adaptation methods and achieves competitive performance compared to unsupervised domain adaptation methods on multiple object detection benchmarks.