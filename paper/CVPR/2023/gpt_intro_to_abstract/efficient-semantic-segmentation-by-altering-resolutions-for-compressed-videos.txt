Video semantic segmentation (VSS) is an essential task in computer vision that aims to assign semantic labels to each pixel in a video sequence. Existing methods for VSS mainly focus on designing network architectures to improve computational efficiency. However, these methods often overlook the crucial factor of input resolution, which directly affects the computational cost. This paper proposes a novel framework called AR-Seg that leverages temporal correlation in videos and alters input resolution to achieve efficient VSS. AR-Seg uses a high-resolution branch for keyframes and a low-resolution branch for non-keyframes. To prevent accuracy degradation caused by downsampling, a Cross Resolution Feature Fusion (CReFF) module is introduced to fuse high-resolution keyframe features into low-resolution non-keyframe features. Additionally, a Feature Similarity Training (FST) strategy is proposed to guide the learning process of the fused features. Experimental results demonstrate that AR-Seg significantly reduces computational cost while maintaining high segmentation accuracy. This paper makes three main contributions: the AR-Seg framework for efficient VSS, the CReFF module for preventing accuracy loss, and the FST strategy for effective learning. The effectiveness of AR-Seg is showcased on the CamVid and Cityscapes datasets, where it reduces computational cost by nearly 70% without compromising accuracy.