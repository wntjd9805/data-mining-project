Deep learning methods have achieved impressive results in various tasks, but they often struggle with out-of-distribution data in practical applications due to domain shifts. To address this issue, domain generalization (DG) methods have been proposed to learn transferable knowledge from multiple source domains. However, these methods typically require large amounts of labeled source data, which is often unavailable in practical scenarios. In this paper, we focus on unsupervised domain generalization (UDG) and propose a method called DN2A to learn domain-invariant features in an unsupervised fashion. We first analyze the limitations of contrastive learning (CL) methods, which are commonly used for unsupervised learning, in the UDG scenario. We then introduce a novel semantic connectivity metric and propose techniques to suppress intra-domain connectivity and enhance intra-class connectivity. Our experiments demonstrate that DN2A outperforms state-of-the-art methods in terms of accuracy gains and requires fewer training samples compared to traditional pretraining methods. Overall, our work provides a promising approach for initializing models in the domain generalization problem.