Wearable cameras, such as smart glasses, have become increasingly popular. With advancements in technology, these cameras have improved in terms of battery capacity, sensor size, memory volume, and in-device processors. This has created a demand for real-time scene understanding algorithms to run efficiently on these devices. However, the unique nature of egocentric videos captured by wearable cameras presents challenges for object detection and tracking algorithms. The large ego motion caused by the wearer's head movements is unpredictable and unrelated to object motions. This makes it difficult for traditional multiple object tracking (MOT) algorithms to maintain performance, as they have to search in larger regions. Additionally, ego motion can cause distortion, blurriness, and occlusion of objects, further degrading the performance of appearance-based MOT approaches. In this paper, we propose a solution to address these challenges and improve the efficiency and reliability of object tracking on wearable cameras.