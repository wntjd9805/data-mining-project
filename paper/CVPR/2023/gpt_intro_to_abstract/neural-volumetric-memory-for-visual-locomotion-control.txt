This paper introduces a novel volumetric memory architecture for legged locomotion control, which aims to address the problem of partial observability in complex locomotion tasks. The architecture consists of a 2D to 3D feature volume encoder and a pose estimator, which together form a Neural Volumetric Memory (NVM). The NVM takes a sequence of depth images from an ego-centric camera and combines them into a coherent latent representation for locomotion control. The memory space is designed to be SE(3) equivariant to changes in camera pose, allowing for the integration of multiple timesteps into a coherent scene representation. The training pipeline involves a two-step teacher-student process, where the first stage produces a policy capable of robustly traversing difficult terrains. In the second stage, the visuomotor distillation stage, the ego-centric views are fed into the NVM and combined with a small policy network, which are trained end-to-end using behavior cloning. The proposed method is evaluated through comprehensive experiments and ablation studies in both simulation and the real world, demonstrating its superior performance compared to baselines. The results highlight the importance of modeling the 3D structure of the environment for effective locomotion control in complex tasks.