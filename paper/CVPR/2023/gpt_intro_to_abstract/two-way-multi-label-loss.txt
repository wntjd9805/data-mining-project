Deep neural networks have been widely used in supervised learning, where they are trained on annotated samples using back-propagation and a loss function. In supervised learning, classification tasks often involve assigning a single class label to each image sample. While this approach has been successful, it is limited in practical scenarios where images may contain multiple classification targets or visual attributes that are difficult to disentangle.To address the challenge of multi-label classification, the commonly used softmax loss is not suitable as it requires decomposing the problem into multiple binary classification tasks, which can lead to imbalanced class distributions. In contrast, the binary cross-entropy (BCE) loss can handle multi-label tasks, but it also suffers from the imbalance issue. Existing methods to address this issue involve weighting the loss based on class frequencies, but they are not always effective.In this paper, we propose a new loss function that effectively handles multiple labels per sample, similar to the softmax loss. By analyzing the intrinsic properties of the softmax loss, we formulate an efficient multi-label loss function that incorporates relative comparison between positive and negative classes. We introduce temperature on the logits to enhance the classification margin and further improve performance. Our proposed loss function is a generalization of the softmax loss, allowing losses to be computed at both the sample and class levels.Our contributions are three-fold: 1) we formulate a new loss function that handles multiple labels while enhancing the classification margin, 2) we propose a two-way approach for measuring multi-label loss, and 3) we thoroughly analyze the proposed loss function and demonstrate its competitive performance compared to other multi-label losses. Additionally, we show that our loss function can be used for single-label ImageNet training and provides transferable features.