Deep neural networks have shown great success in various visual recognition tasks. However, concerns have been raised regarding the trustworthiness of these models' decision-making process, particularly in medical imaging applications. One major issue is the presence of confounding behaviors in deep learning models, where irrelevant artifacts or bias in the input cues can affect the final predictions. These spurious correlations in the training distribution can make the models fragile when presented with novel testing samples. To address this, transparency in decision-making and human-guided bias correction are essential to increase reliability and trust in life-critical applications like cancer diagnosis. This paper aims to improve model transparency and develop a mechanism for human intervention during the model training process when confounding factors are observed. The proposed solution focuses on generating human-understandable textual concepts for model visualization, and it does not rely on prior knowledge about the confounding concepts within the dataset, making it more realistic and practical. The method involves learning confounding concepts based on spectral relevance analysis and concept activation vectors, allowing humans to provide feedback and remove unwanted bias and confounding behaviors. A curated dataset called ConfDerm is introduced for evaluating the model's trustworthiness under different confounding behaviors. The authors' contributions include the creation of the ConfDerm dataset, the identification of confounding artifacts in popular skin cancer datasets, the development of a human-in-the-loop framework for model explanation and behavior removal, and the demonstration of the method's superiority in performance improvement, artifact removal, and debiasing.