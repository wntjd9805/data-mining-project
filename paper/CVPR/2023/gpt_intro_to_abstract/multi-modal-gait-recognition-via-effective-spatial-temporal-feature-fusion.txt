Gait recognition, a biometric technology that identifies individuals based on their walking patterns, holds great potential for long-distance recognition systems. However, reliable gait recognition remains a challenge due to various complex factors that can affect its performance, such as clothing, carrying conditions, and cross-view issues. To address these challenges, different methods have been proposed, including appearance-based and model-based approaches. Appearance-based methods rely on silhouette images extracted from video frames and utilize convolutional neural networks to extract spatio-temporal features. Model-based methods, on the other hand, consider the underlying physical structure of the body and represent gait using skeletons obtained from pose estimation models. While silhouette-based and skeletons-based methods have their advantages, they also suffer from limitations that hinder further improvement. Silhouettes retain body shape information but can be affected by self-obscuring and clothing obscuration problems. Skeletons, on the other hand, preserve body structure information but ignore discriminative body shape information. By combining the complementary strengths of both modalities, a more comprehensive representation of gait can be achieved. Motivated by this, we propose a transformer-based gait recognition framework called MMGaitFormer that effectively fuses and aggregates spatial-temporal information from both silhouettes and skeletons. The framework consists of four main modules: silhouette and skeleton sequence extraction, independent encoding modules, spatial fusion module, and temporal fusion module. The spatial fusion module employs co-attention mechanisms to enable interactions between silhouettes and skeletons, while the temporal fusion module utilizes embedding modeling operations with Cycle Position Embedding for fine-aligned temporal modeling. Our proposed method achieves state-of-the-art performance on popular gait datasets, including a significant improvement (+11.2% accuracy) over single-modal methods on the challenging condition of walking in different clothes.