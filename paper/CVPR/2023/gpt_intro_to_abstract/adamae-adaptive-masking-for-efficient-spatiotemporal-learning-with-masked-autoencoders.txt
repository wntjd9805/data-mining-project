Self-supervised learning (SSL) is a framework in computer science that aims to learn transferable representations from unlabeled data for downstream applications. This is achieved through a two-stage process of pre-training on unlabeled data and fine-tuning on a specific task. Pre-training has been shown to improve performance, convergence speed, robustness, and reduce model overfitting. Two popular approaches for SSL are masked autoencoders (MAEs) and contrastive learning. MAEs patchify the input data, pass visible tokens through a Vision Transformer (ViT), and reconstruct masked patches using a shallow decoder transformer. Contrastive learning involves pulling together augmented views of the same input in the embedding space while pushing away embeddings of different inputs. MAEs have gained attention due to their high masking ratio and memory-efficient training. However, the success of MAEs depends on effective mask sampling techniques. Existing random patch, tube, and frame masking strategies do not consider the variability of token information and lead to inaccurate reconstructions. In this paper, we propose an adaptive sampling approach that selects tokens based on their spatiotemporal information. This approach optimizes both the MAE and an adaptive token sampling network, using an auxiliary loss inspired by the REINFORCE algorithm in reinforcement learning. Our empirical findings demonstrate that our approach samples more tokens from high spatiotemporal information regions, enabling high masking ratios and reducing GPU memory requirements. We compare our approach to existing techniques on the SSv2 dataset and show its superior performance. Our contributions include the proposal of AdaMAE, an adaptive and trainable token sampling strategy, empirical evidence of its effectiveness, efficiency in terms of performance and GPU memory, and outperformance of state-of-the-art methods on SSv2 and Kinetics-400 datasets.