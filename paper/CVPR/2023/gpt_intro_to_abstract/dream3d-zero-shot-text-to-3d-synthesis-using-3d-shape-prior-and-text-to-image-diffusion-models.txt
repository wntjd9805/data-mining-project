The field of text-to-3D synthesis aims to generate 3D content that aligns with input text, benefiting applications such as animations, games, and virtual reality. Although recent advancements in zero-shot text-to-image models have proven successful in generating diverse and high-quality images based on text prompts, extending this success to text-to-3D synthesis remains challenging due to the lack of paired text-3D datasets. Zero-shot text-to-3D synthesis, which eliminates the need for paired data, relies on powerful vision-language models like CLIP. Two main categories of this approach include CLIP-based generative models that utilize images as an intermediate bridge, and CLIP-guided 3D optimization methods that continuously optimize the CLIP similarity loss between text prompts and rendered images. However, these methods often fail to produce precise and accurate 3D structures that align with the input text, resulting in unconstrained and low-quality outputs. To address this issue, we propose a method that generates a high-quality 3D shape from the input text and uses it as a "3D shape prior" in the CLIP-guided 3D optimization process. By bridging the text and image modalities using a text-to-image diffusion model, we can synthesize high-quality 3D shapes that conform to the input text. To overcome the style domain gap between the synthesized images and shape renderings, we jointly optimize a learnable text prompt and fine-tune the text-to-image diffusion model. Our method, named "Dream3D," generates more accurate and high-quality 3D shapes while maintaining the creativity to produce diverse shape structures and textures. Compared to previous methods, Dream3D demonstrates better visual quality, higher shape accuracy, and the ability to generate imaginative 3D content.