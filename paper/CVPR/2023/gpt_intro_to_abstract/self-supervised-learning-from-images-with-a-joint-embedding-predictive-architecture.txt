This paper explores self-supervised learning approaches in computer vision, specifically invariance-based and generative methods. Invariance-based pretraining methods optimize an encoder to produce similar embeddings for different views of the same image using data augmentations. These methods can produce high-level semantic representations but introduce biases that may not be suitable for all tasks. Generative methods, on the other hand, learn to predict corrupted content in the input and can generalize to other modalities. However, these representations are typically of a lower semantic level. To address these limitations, the authors propose a joint-embedding predictive architecture (I-JEPA) that predicts missing information in an abstract representation space. Through empirical evaluation, I-JEPA is shown to outperform pixel-reconstruction methods, compete with view-invariant pretraining approaches, and achieve better performance on low-level vision tasks. Additionally, I-JEPA is scalable and efficient, requiring less compute for pretraining compared to other methods.