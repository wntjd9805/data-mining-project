Federated learning is a privacy-preserving paradigm that enables collaborative learning without compromising privacy. The main approach, FedAvg, aggregates parameters from participants and distributes the global model (averaged parameters) back for further training. However, a major challenge in federated learning is data heterogeneity, as the private data collected from different sources with diverse preferences often presents a non-iid distribution. This leads to inconsistent optimization towards the global direction, resulting in slow convergence and limited performance improvement of the averaged global model. Existing methods have primarily focused on addressing label skew, but another important data heterogeneous property in federated learning is domain shift, where private data is derived from various domains with distinct feature distributions.To address these issues, we propose Federated Prototype Learning (FPL) for federated learning with domain shift. FPL consists of two components: Cluster Prototypes Contrastive Learning (CPCL) and Unbiased Prototypes Consistent Regularization (UPCR). CPCL leverages cluster prototypes to construct contrastive learning, encouraging instance features to be more similar to cluster prototypes from the same class while separating them from prototypes with different semantics. This incorporates diverse domain knowledge and maintains a clear decision boundary, improving generalizability with discriminability. UPCR utilizes unbiased prototypes to provide a fair and stable convergence point, averaging cluster prototypes to acquire unbiased prototypes. The local instance is then minimized in feature-level distance with the corresponding unbiased prototype, avoiding bias towards dominant domains and ensuring stable performance on inferior domains.Our proposed FPL method addresses the limitation of existing methods, which lack sufficient global regularization signals to accurately depict diverse domain knowledge and are biased towards major domains. We conduct extensive experiments on Digits and OfÔ¨Åce Caltech tasks, along with ablative studies, to evaluate the efficacy of FPL. The results demonstrate the effectiveness of FPL and the importance of each module in improving performance in federated learning with domain shift.