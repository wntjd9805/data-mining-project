Image matting is an important computer vision task that involves predicting the opacity or transparency of objects in an image. This allows for precise separation of objects from their surrounding backgrounds. Many existing works have improved matting performance by using manual guidance in the form of a trimap, which requires pixel-level annotation of foreground, background, and unknown regions. However, this manual annotation process is time-consuming and limits the applicability of matting in practical applications such as image and video editing. Recently, there have been alternative methods proposed that reduce the burden of user guidance, such as trimap-free techniques, additional background images, scribbles, and user clicks. Among these, the mask-guided approach has shown a promising trade-off between performance and user interaction. This approach utilizes a coarse mask as guidance, which is easier to obtain either manually or from pre-trained segmentation models. The mask-guided matting model has demonstrated comparable or even better performance than trimap-based competitors on synthetic and real-world datasets. However, it still struggles to produce desirable results in complex real-world scenes. In this paper, we address the challenges of mask-guided matting in the wild, which refers to handling objects in their complex context, dealing with diverse categories of objects, and mitigating the limited data problem. We propose a learning framework that improves the generalization and robustness of the mask-guided matting model. Specifically, we investigate the reasons for the poor generalization of the previous model and propose an instance-wise learning objective to alleviate bias and improve object localization. We also leverage weakly supervised instance segmentation datasets to handle various categories of objects and propose a self-training framework to generate fine supervision signals. To evaluate the performance of the mask-guided matting model in the wild, we establish an evaluation protocol involving multiple sub-benchmarks and datasets. We design an in-the-wild extension of a synthetic benchmark, as well as use existing datasets to provide quantitative and qualitative results. Our contributions include the exploration of mask-guided matting in the wild, the development of a simple and effective learning framework, the design of an evaluation setup, and the initiation of extensions to video and panoptic matting.