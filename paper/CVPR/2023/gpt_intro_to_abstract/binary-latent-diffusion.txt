The goal of representation learning and generative models is to model the image distribution in order to generate high-quality novel samples efficiently. Various approaches such as generative adversarial networks, flow models, energy-based models, and diffusion models have been explored to represent and generate images. However, as the resolution increases, accurately regressing the pixel values becomes more difficult. This challenge is typically addressed using hierarchical model architectures or at a high cost. Additionally, while generative adversarial network models demonstrate outstanding image quality, they suffer from issues such as insufficient mode coverage and training instability.An alternative approach is to represent and generate images in a learned latent space. Latent diffusion performs denoising in a lower-dimensional latent feature space, reducing the cost of each denoising step. Variational autoencoders generate images without iterative steps but are limited by the static prior of the latent space, which can lead to posterior collapse. VQ-VAE introduces a vector-quantized latent space, where each image is represented as a sequence of indexes pointing to a vector in a learned codebook. This approach allows for a more flexible latent distribution without significantly increasing modeling complexity. However, using one codebook index to represent each image patch introduces a trade-off on the codebook size.In this research, we propose a compact and expressive representation of images in a binary latent space. Each image patch is represented as a binary vector, and the prior over the discrete binary latent codes is effectively modeled using a binary diffusion model tailored for the Bernoulli distribution. We model the bi-directional mappings between images and binary representations using a feed-forward autoencoder with a binary latent space. We then introduce how to generate novel samples by modeling the prior over binary latent codes of images using a binary latent diffusion. This approach reduces the need for precise regression and allows for more efficient sampling. We also introduce a progressive reparametrization technique and train the binary latent diffusion models to predict the "flipping probability" for improved training and sampling stability.We support our findings with experiments on multiple datasets for both conditional and unconditional image generation. Our method delivers remarkable image generation quality and diversity with more compact latent codes, larger image-to-latent resolution ratios, fewer sampling steps, and faster sampling speed. Examples of images generated using our proposed method are presented.