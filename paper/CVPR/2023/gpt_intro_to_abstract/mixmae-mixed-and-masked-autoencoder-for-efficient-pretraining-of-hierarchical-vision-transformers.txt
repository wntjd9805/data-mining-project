Utilizing unlabeled visual data in self-supervised manners to learn representations is intriguing but challenging. Pretraining with masked image modeling (MIM) has shown great success in learning visual representations for various downstream vision tasks. However, existing MIM approaches using the [MASK] symbol suffer from pretraining-finetuning inconsistency and inefficient computation. In contrast, MAE addresses these issues but is limited to vanilla ViT encoder. In this work, we propose MixMAE, a generalized pre-training method that combines the advantages of SimMIM and MAE while avoiding their limitations. MixMAE adopts a hierarchical ViT encoder and an encoder-decoder design to reconstruct mixed images and learn visual representations. MixMAE can be applied to different hierarchical ViTs and outperforms existing methods on various downstream tasks. It shows better pretraining efficiency compared to SimMIM.