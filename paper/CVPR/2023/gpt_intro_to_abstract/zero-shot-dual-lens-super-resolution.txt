Mobile devices with dual-lens camera systems offer the opportunity to capture the same scene with different field-of-views (FoVs) using wide-angle and telephoto lenses. This enables the acquisition of both low-resolution (LR) and high-resolution (HR) images within the overlapped FoV, which can be used to learn a super-resolution (SR) model. Previous works have explored reference-based SR models and self-supervised learning to enhance SR performance, but they often suffer from the limitations of assuming consistent degradation across images captured by the same device. In this paper, we propose ZeDuSR, a zero-shot learning solution for realistic SR on dual-lens devices that adapts to the image-specific degradation encountered during the acquisition process. To tackle the challenges of spatial misalignment and limited data, we introduce a degradation-invariant alignment method and a degradation-aware training strategy. Our experiments on both synthesized and real-world datasets demonstrate the superiority of ZeDuSR over existing single-image SR, reference-based SR, and dual-lens SR methods. The contributions of this paper include the proposal of a zero-shot learning solution, a degradation-invariant alignment method, a degradation-aware training strategy, and extensive experimental validation.