Video instance segmentation (VIS) aims to obtain pixel-level segmentation masks for objects in videos. Current VIS methods can be categorized into per-frame input based methods and per-clip input based methods. Per-frame methods segment objects frame by frame and associate instance masks across frames, while per-clip methods use spatio-temporal features to predict multi-frame instance masks. Per-clip methods have shown promising results on the YouTube-VIS dataset, but lag behind per-frame methods on the challenging OVIS dataset. To address this, we propose a per-clip VIS method called MDQE that improves query initialization and introduces an inter-instance mask repulsion loss. Our method achieves contrastive learning of instance embeddings and effectively segments hard instances in challenging videos. Experimental results on both OVIS and YouTube-VIS datasets demonstrate that MDQE achieves competitive performance compared to per-frame methods.