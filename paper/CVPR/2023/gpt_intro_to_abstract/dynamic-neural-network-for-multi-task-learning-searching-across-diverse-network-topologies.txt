Multi-task learning (MTL) has become a popular approach in computer science for improving the generalization performance of multiple tasks while reducing the number of network parameters. However, as the number of tasks increases, it becomes more challenging to learn shared representations and improper sharing can negatively impact task performance. To address this issue, researchers have explored separating shared and task-specific parameters in the network. Recent works have focused on dynamically controlling the ratio of shared parameters using a Dynamic Neural Network (DNN) and cell-based architecture search. However, these approaches may lead to performance degradation in heterogeneous MTL scenarios due to unbalanced task complexity. In this paper, we propose a new MTL framework that searches for optimized sub-network structures for each task across diverse network topologies. To accomplish this, we apply a Directed Acyclic Graph (DAG) to explore richer search spaces inspired by Neural Architecture Search (NAS) methods. However, MTL in the DAG search space presents scalability issues as the number of parameters and search time increase quadratically. To address this problem, we design a restricted DAG-based central network with read-in/read-out layers that allow for searching across diverse graph topologies while limiting the search space and search time. The read-in layer connects all the hidden states from the input state, while the read-out layer connects all the hidden states to the last feature layer, enabling various network topological representations.We then optimize the central network to have compact task-adaptive sub-networks using a three-stage training procedure. We propose a squeeze loss and a flow-based reduction algorithm to limit the number of parameters and prune the network based on information flow. The end result is a compact single network that serves as multiple task-specific networks with unique structures. Our experiments demonstrate that our framework successfully searches task-adaptive network topologies and outperforms state-of-the-art methods on common benchmark datasets for MTL. In summary, our contributions include: 1. Presenting an MTL framework that searches for task-adaptive structures and sharing patterns among tasks, achieving state-of-the-art performance.2. Introducing a DAG-based central network with diverse graph topologies in a reasonably restricted search space.3. Proposing a training procedure for constructing task-specific sub-networks in a single network.