The use of graphs and their spectral properties has been proven effective in various classical problems in computer science, such as clustering, classification, and dimensionality reduction. However, incorporating these spectral graph algorithms into deep learning frameworks poses challenges due to the need for eigenvalue computations and scalability limitations. To address these issues, a proposed method is to train deep neural networks that approximate the eigenspace by minimizing Rayleigh quotient type losses. Another approach is to compute the embedding analytically and use it as ground-truth to train the network in a supervised manner. However, this approach is computationally demanding and not scalable for large training sets. In this paper, a new method is introduced that learns the eigenspace directly in batches, providing scalability, immediate out of sample extension, high-quality approximation of the eigenspace, and robustness to changes in features and node affinities. This method serves as a spectral building block for deep learning algorithms, offering a low-dimensional embedding based on linear algebraic theory. Experimental results demonstrate the effectiveness and applicability of the proposed method.