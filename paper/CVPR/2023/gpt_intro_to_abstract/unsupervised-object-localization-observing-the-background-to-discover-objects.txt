Object localization, either through detection or segmentation, is crucial in safety-critical systems like self-driving cars. State-of-the-art methods train deep models on large labeled datasets, but this requires extensive annotation. To address this issue, alternative strategies such as semi-supervised, weakly-supervised, and active learning have been proposed. This paper focuses on the unsupervised object localization task, where objects are discovered in an image without any human annotation. Previous works have utilized hand-crafted features and inter-image information but struggle to scale to large datasets. Recent approaches leverage self-supervised features learned through pretext tasks, but these methods often make assumptions about what an object is, limiting the scope of object detection. In contrast, this paper takes a different approach by focusing on the concept of background and identifying objects based on the pixels that do not belong to the background. The proposed method, called FOUND, is cost-effective in terms of training and inference. Firstly, a rough estimate of the background mask is computed by mining a patch that likely represents the background. This is achieved by leveraging attention maps and selecting the least attended patch. The background mask is then refined by incorporating patches similar to the mined one. Additionally, a reweighting scheme based on attention sparsity is introduced to reduce the impact of noisy attention maps. In the second step, the complement of the background mask is utilized as an approximate estimation of object localization. To enhance this estimate, a single conv1 × 1 layer is trained on top of the frozen self-supervised transformer using the masks from the first step, an edge-preserving filter, and self-labelling. The proposed method achieves state-of-the-art results in saliency detection, unsupervised object discovery, and semantic segmentation retrieval tasks. The main contributions of this work include the upside-down approach to object discovery, the utilization of self-trained features for background concept discovery, the improvement of attention heads through weight-based sparsity integration, the lightweight model comprising only a single conv1 × 1 layer, and the demonstration of superior performance compared to competing methods.