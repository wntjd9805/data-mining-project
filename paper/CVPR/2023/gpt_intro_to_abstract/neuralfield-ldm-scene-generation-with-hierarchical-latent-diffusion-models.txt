There has been increasing interest in automating the generation of 3D real-world scenes for various applications such as virtual reality, game design, and digital twin creation. However, the manual process of designing 3D worlds is challenging and time-consuming, requiring expertise in 3D modeling and artistic talent. To address this issue, recent research has focused on automating 3D content creation using generative models that output individual object assets. While this is a step forward, automating the generation of real-world scenes remains an important open problem with numerous applications. In this work, we propose NeuralField-LDM (NF-LDM), a generative model capable of synthesizing complex real-world 3D scenes. NF-LDM is trained on a collection of posed camera images and depth measurements, offering a scalable way to synthesize 3D scenes. Existing approaches tackle similar problems but on less complex data. Our approach, NF-LDM, overcomes the limitation of capturing the entire scene in a single vector and instead utilizes a three-stage pipeline to model scenes. We learn an auto-encoder that encodes scenes into a neural field representation, and then employ latent diffusion models to generate novel 3D scenes. We demonstrate the capabilities of NF-LDM in various applications such as scene editing, birds-eye view conditional generation, and style adaptation. Additionally, we leverage score distillation to optimize the quality of generated neural fields, leveraging knowledge from state-of-the-art image diffusion models. Our contributions include the introduction of NF-LDM, achieving state-of-the-art scene generation results on challenging datasets, and extensions to semantic birds-eye view conditional generation, style modification, and 3D scene editing.