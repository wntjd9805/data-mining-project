Deep models in semantic segmentation have shown success due to large-scale datasets. However, popular datasets for segmentation, such as PAS-CAL VOC and Cityscapes, often have imbalanced distributions of samples across different categories. This poses a challenge for assigning labels to pixels accurately. Previous attempts to address this challenge have focused on balancing sample quantity or assigning higher loss weights to tail classes. However, performance degradation is still observed in tail categories. In this work, we propose a new approach called balancing logit variation (BLV) to improve long-tailed semantic segmentation. We argue that the features of tail classes are often squeezed into a narrow area in the feature space due to limited samples. To balance the feature distribution, we introduce logit variation during training by perturbing each predicted logit with random noise. This introduces a feature region for each instance, whose radius depends on the noise variance. We apply smaller variance to head classes and larger variance to tail classes to close the feature area gap between different categories. This approach acts as a special augmentation and is discarded during inference to ensure reliable predictions. We evaluate our method on fully supervised, semi-supervised, and unsupervised domain adaptation settings, consistently improving the baselines. Our method also works well with various state-of-the-art frameworks, demonstrating its strong generalizability.