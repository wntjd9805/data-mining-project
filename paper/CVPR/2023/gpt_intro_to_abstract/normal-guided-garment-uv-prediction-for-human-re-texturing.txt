Abstract:This paper presents a novel method for editing the appearance of garments in images or videos by considering their fine-grained geometric deformations. Traditional approaches to texture editing rely on accurate 3D reconstruction, which can be challenging due to limited data availability and generalization issues. On the other hand, direct texture mapping methods lack geometry details and are not suitable for editing garments. To overcome these limitations, we propose a method that combines the advantages of both approaches. Our method utilizes the 3D surface normals predicted from an image to impose the isometry property into the UV map estimation. By formulating a set of partial differential equations, we capture the geometric relationship between the UV map and surface normals, eliminating the need for 3D reconstruction and ground truth UV maps.The method takes an image or video, surface normal predictions, and dense optical flow as inputs, and outputs a geometry-aware UV map estimate. A multi-layer perceptron is employed to predict UV coordinates based on pixel locations in the image, while a predefined proxy UV map is used to disambiguate the choice of a reference coordinate frame. The isometry constraints are used as a loss function to optimize the UV map, while optical flow is utilized in videos to establish temporal coherence.The contributions of this paper include the formulation of a novel geometric relationship between surface normals and UV maps, a neural network design for predicting temporally coherent UV maps, and improved performance compared to existing re-texturing methods on a variety of real-world imagery. The proposed method opens up possibilities for realistic and coherent texture editing of garments in online clothing shops and other applications.