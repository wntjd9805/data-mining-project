Semi-supervised learning (SSL) aims to exploit unlabeled data to improve model performance when labeled data is limited. Several techniques, such as regularization and pseudo labeling, have been used in SSL frameworks. However, existing methods, such as FixMatch, heavily rely on a high threshold to produce accurate pseudo-labels, resulting in the exclusion of many unlabeled examples with ambiguous predictions. To address this limitation, we propose two novel approaches: Entropy Meaning Loss (EML) and Adaptive Negative Learning (ANL). EML enhances confidence on target classes by imposing additional supervision on non-target classes to prevent competition. ANL utilizes low-confidence examples without pseudo-labels by assigning negative pseudo-labels to classes ranked after a certain threshold. We integrate these approaches into a new framework called FullMatch and evaluate its performance on various datasets. Our results show that FullMatch outperforms FixMatch while maintaining similar training costs. Additionally, FullMatch can be easily adapted to other FixMatch-based algorithms and achieve further improvements. Overall, our contributions include the introduction of EML and ANL, the design of the FullMatch framework, and the demonstration of improved performance on multiple benchmarks.