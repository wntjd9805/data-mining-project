Representation learning that combines vision and language has gained significant attention and has various applications in cross-modal tasks such as text-video retrieval and video-question answering. The success of contrastive learning, like CLIP, has enabled networks to learn discriminative video-language representations by projecting video and text features into a common latent space based on semantic similarities. However, existing cross-modal contrastive approaches only consider global similarity and lack the ability to capture fine-grained interpretable information that reveals how cross-modal alignment is influenced by specific visual and textual elements. Manual labeling of these relationships is often unavailable, limiting the supervised learning of such representations. To address this limitation, we propose a novel approach called Hierarchical Banzhaf Interaction (HBI) that models cross-modal representation learning as a multivariate cooperative game. We formulate video frames and text words as players in a cooperative game and quantify the cooperation within a coalition using the Banzhaf Interaction index. By incorporating the Banzhaf Interaction, we can value the correspondence between video frames and text words for more sensitive and explainable cross-modal contrast. We introduce an adaptive token merge module to efficiently generate coalitions among game players and achieve hierarchical interaction at different levels of granularity. Experimental results on benchmark datasets demonstrate that our method outperforms existing approaches in text-video retrieval and video-question answering tasks. Additionally, our method can also serve as a visualization tool to enhance understanding of cross-modal interaction. Our contributions include proposing a novel proxy training objective based on Banzhaf Interaction, achieving state-of-the-art performance in text-video retrieval and video-question answering, and providing a visualization tool for interpreting cross-modal interactions.