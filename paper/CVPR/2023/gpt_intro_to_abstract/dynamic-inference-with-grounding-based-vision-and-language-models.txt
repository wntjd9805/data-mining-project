In recent years, significant progress has been made in the development of image and language models, thanks to the emergence of transformers for different modalities and large-scale pre-training paradigms. However, these models often come with a high number of parameters and computations, resulting in low run-time efficiency. This problem becomes even more pronounced in multimodal networks built on multiple transformer models. While existing methods such as pruning, knowledge distillation, and quantization can potentially reduce parameters, they often lead to significant performance drops and are not specifically designed for improving run-time speed. In this paper, we propose a dynamic inference approach with large image and language models to reduce run-time complexity while maintaining high accuracy. We first analyze the architectural choices for image and language models and build a vision and language model called ViTMDETR, inspired by MDETR. We identify the multi-head self attention (MSA) and feed forward network (FFN) blocks as computationally expensive modules in inference-time.To address these challenges, our D-ViTMDETR model dynamically prunes input tokens from multiple modalities and adaptively fuses vision tokens with text tokens to improve accuracy. We also skip computationally expensive MSA and FFN layers across the backbones and the multimodal network to enhance run-time efficiency. To learn dynamic policies, we use policy-gradients based reinforcement learning algorithm and distill the knowledge from ViTMDETR to optimize D-ViTMDETR.Our contributions include introducing the ViTMDETR model for vision and language tasks, proposing a novel method for dynamic token pruning and fusion using reinforcement learning, aligning representations and predictions of D-ViTMDETR with ViTMDETR for better optimization, and performing experiments on benchmarks for Referring Expression Comprehension (REC), Segmentation (RES), and Visual Question Answering (VQA) tasks. Our dynamic model, D-ViTMDETR, significantly improves the run-time efficiency of state-of-the-art models MDETR and GLIP, with up to 50% reduction in complexity and only a maximum of 0.3% decrease in accuracy.