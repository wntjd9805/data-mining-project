Video frame interpolation (VFI) is an important task in computer vision, with applications in video editing and novel view synthesis. Recent advances in deep neural networks have led to significant progress in VFI, with two main categories of approaches: optical-flow-based methods and kernel-regression-based algorithms. However, both categories have their limitations. Optical-flow-based methods struggle with modeling real-world motion and handling occlusion, while kernel-based methods have limited receptive fields and struggle with high-resolution frame interpolation and large motion. In this paper, we propose a novel texture consistency loss (TCL) to address the issue of motion ambiguity and improve the quality of interpolated frames. We also introduce a guided cross-scale pyramid alignment (GCSPA) module to leverage multi-scale information for more accurate motion compensation. Our experimental results demonstrate the effectiveness and efficiency of our proposed methods, with superior performance compared to state-of-the-art algorithms in frame interpolation and extrapolation tasks.