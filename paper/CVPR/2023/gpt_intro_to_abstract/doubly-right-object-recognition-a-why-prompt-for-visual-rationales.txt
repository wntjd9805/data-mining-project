Computer vision models have achieved high accuracy in object recognition tasks. However, these models often lack the ability to explain their predictions, which is important for building trustworthy systems, especially in human-machine interaction applications. Existing methods in interpretability have focused on understanding the features that contribute to the models' predictions, but their explanations are often imprecise, difficult to understand, and cannot be evaluated. Some approaches have explored verbal rationales but require manual collections of plausible rationales and are limited to small-scale datasets. Scalable methods for explainability have been developed in natural language processing (NLP) through prompting, where language models output descriptions of their reasoning. In this paper, we investigate whether visual representations can also explain their reasoning through visual chain-of-thought prompts. We first introduce a benchmark for doubly right object recognition, where models must predict both correct labels and correct rationales. We found that existing image-language pre-trained models often select the wrong rationales. To address this, we propose a framework to transfer chain-of-thought reasoning from NLP models to vision models. Our framework achieves significant improvements in doubly right performance on the benchmark. Our results also demonstrate zero-shot transfer to unseen tasks and datasets. We believe this doubly right recognition task is a promising direction for the visual recognition field. The data and code for our study are available online.