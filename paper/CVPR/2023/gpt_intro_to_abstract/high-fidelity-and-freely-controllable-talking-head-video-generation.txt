Talking head video generation, also known as face reenactment, is the process of synthesizing a realistic talking head video using a given source identity and target motion. This technique has applications in video conferencing, movie effects, and entertainment. With the advancement of deep learning and generative adversarial networks, significant progress has been made in talking head generation, face reenactment, and image animation. However, existing methods still face challenges such as unexpected deformations and distortions in the generated face, difficulties in manipulating movement-related information, and achieving smooth and natural movements in videos. To overcome these challenges, we propose the Pose and Expression Controllable Head model (PECHead) that enables high-fidelity face reenactment and full control over head pose and expression in talking head video generation. Our method combines learned landmarks and predeﬁned face landmarks to model overall head movement and detailed facial expression changes. We use a single image-based face reconstruction model to obtain physically reasonable face landmarks, reducing distortion during motion transfer. We introduce the use of learned sparse landmarks for global motion and predeﬁned dense landmarks for local motion, aligning them with the Motion-Aware Multi-Scale Feature Alignment module. By controlling different coefficients, we can manipulate head pose and expression. Additionally, we propose the Context Adaptation and Propagation module to improve the smoothness of the generated video.Our proposed method is evaluated on multiple talking head datasets and achieves state-of-the-art performance in generating high-fidelity face reenactment results and controllable talking head videos. We contribute by introducing a novel method that leverages head movements to control landmarks estimation, enabling free control over head pose and expression. We also incorporate learned and predeﬁned landmarks with the Motion-Aware Multi-Scale Feature Alignment module, and introduce the Context Adaptation and Propagation module to improve the smoothness and naturalness of the generated videos. Experimental results demonstrate the superiority of our framework in high-fidelity video face reenactment and freely controllable talking head generation.