Pseudo-LiDAR 3D detectors have made significant advancements in monocular 3D detection by improving depth perception through depth estimation networks and utilizing LiDAR-based detection architectures. Similarly, stereo 3D detectors have achieved accurate localization of 3D objects. However, the disparity between image-to-image generation for stereo views is much smaller compared to image-to-LiDAR generation.   With this in mind, we propose a Pseudo-Stereo 3D detection framework that introduces three innovative virtual view generation methods: image-level generation, feature-level generation, and feature-clone. These methods enable the detection of 3D objects from a single image. Our analysis of depth-aware learning reveals that the depth loss is effective only in feature-level virtual view generation, while the estimated depth map is effective in both image-level and feature-level generation within our framework.  To address the challenges posed by depth estimation errors and the resulting feature degradation, we introduce a disparity-wise dynamic convolution technique. This approach utilizes dynamic kernels sampled from the disparity feature map to adaptively filter features from a single image, thereby generating virtual image features.  As of the submission date (November 18, 2021), our Pseudo-Stereo 3D detection framework achieves the top ranking on car, pedestrian, and cyclist categories among monocular 3D detectors in theKITTI-3D benchmark. The code for our framework is publicly available at https://github.com/revisitq/Pseudo-Stereo-3D.