Temporal modeling is crucial for understanding videos. Although deep convolution-based architectures have been successful in recognizing videos on a large scale, recent research has revealed that they tend to focus on modeling short-term relationships and struggle to capture long-term temporal structures. As a result, they struggle to transfer their knowledge to new datasets and generalize well.   This study focuses on dynamic representation learning (DRL). We introduce the concept of dynamic score, which measures how much additional information a video network learns compared to a purely spatial student through knowledge distillation. We formulate DRL as an adversarial learning problem, where the video and spatial models compete, aiming to maximize the dynamic score of the learned spatiotemporal classifier.   To evaluate the quality of the learned video representations, we conduct transfer learning experiments on various problems related to action classification with many-shot and few-shot scenarios. The experimental results demonstrate that models trained with DRL outperform the baselines in dynamic modeling. These models exhibit higher transferability and better generalization capabilities when applied to novel domains and tasks.