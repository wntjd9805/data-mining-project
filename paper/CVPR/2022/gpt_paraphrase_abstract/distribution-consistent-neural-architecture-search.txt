Neural architecture search (NAS) has made significant advancements in automating deep network architecture designs. However, training each candidate architecture from scratch is computationally expensive. To address this, one-shot NAS approaches use weight-sharing to improve training efficiency. However, this introduces a weight coupling problem that reduces evaluation accuracy. Existing methods address this problem by shrinking the search space or using model distillation or few-shot training. In this paper, we propose a novel distribution consistent one-shot NAS algorithm. We investigate the weight coupling problem from a parameter distribution perspective and introduce a DistributionConsistent Constraint to measure weight sharing. Our strategy optimizes the supernet by inferring network weights and sharing states. This joint optimization reduces the discrepancy between inherited weights and those trained with a stand-alone model, leading to more accurate model evaluation and better search performance. We conduct extensive experiments on benchmark datasets and achieve superior performance compared to state-of-the-art NAS algorithms, demonstrating the effectiveness of our approach.