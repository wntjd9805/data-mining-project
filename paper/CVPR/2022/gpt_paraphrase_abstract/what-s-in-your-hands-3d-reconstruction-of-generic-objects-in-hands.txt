Our research focuses on reconstructing hand-held objects using a single RGB image. Unlike previous studies that assume known 3D templates and simplify the problem to 3D pose estimation, we aim to reconstruct generic hand-held objects without prior knowledge of their 3D templates. We have discovered that hand articulation provides valuable information about the shape of the object, and based on this insight, we propose a method that reconstructs the object by taking into account both the articulation and the visual input. To achieve this, we first estimate the hand pose using existing systems and then infer the shape of the object in a normalized hand-centric coordinate frame. We parameterize the object using signed distance, which is determined by an implicit network that incorporates information from both visual features and articulation-aware coordinates to process a query point. We conducted experiments on three datasets and consistently outperformed baseline methods, demonstrating our method's ability to reconstruct a wide range of objects. We also analyzed the advantages and robustness of explicit articulation conditioning and observed that it improves hand pose estimation through test-time optimization.In summary, our work focuses on reconstructing hand-held objects from a single RGB image by leveraging hand articulation as a key factor in determining the object shape. Our proposed approach outperforms existing methods, and we demonstrate its effectiveness through experiments on multiple datasets.