Scaling Visual Question Answering (VQA) for web searches requires advancements in visual representation learning, knowledge aggregation, and language generation. We present a challenging benchmark called WEBQA, which is difficult for large-scale models but easy for humans. WEBQA follows the process of asking a question, choosing sources to aggregate, and producing a language response, similar to how humans use the web. Current work assumes models can reason about knowledge in either images or text, but WEBQA includes a text-only QA task to ensure improved visual performance does not sacrifice language understanding. Our challenge is to create multimodal reasoning models that can answer questions regardless of the source modality, bringing us closer to digital assistants that can query both language and the visual online world. The answer format only provides the abstraction.