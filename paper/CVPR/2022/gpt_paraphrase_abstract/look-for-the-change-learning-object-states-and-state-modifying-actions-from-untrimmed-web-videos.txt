This paper focuses on the temporal localization of object states and their corresponding state-modifying actions in long uncurated videos. The goal is to achieve this localization with minimal supervision. The paper makes three main contributions. First, a self-supervised model is developed to learn state-modifying actions and object states together from a set of uncurated internet videos. The model is trained using the causal ordering signal, where the initial object state is followed by the manipulating action, which leads to the end state.Second, to address the issue of noisy uncurated training data, the model incorporates a noise adaptive weighting module. This module is supervised by a small number of annotated still images and helps filter out irrelevant videos during training, leading to more efficient learning.Third, a new dataset is collected, consisting of over 2600 hours of video and 34 thousand changes of object states. A portion of this dataset is manually annotated to validate the proposed approach. The experimental results show significant improvements over previous work in both action and object state recognition in videos.