The current perception datasets for autonomous driving focus primarily on the frontal view using sensors mounted on the vehicle, neglecting the importance of roadside perception tasks. However, data captured from roadside cameras offer advantages over frontal-view data and can contribute to a safer and more intelligent autonomous driving system. To advance roadside perception research, we introduce the Roadside Perception 3D dataset (Rope3D), which is the first dataset of its kind and provides a diverse and challenging collection of images and 3D objects. The dataset includes 50k images and over 1.5M 3D objects captured under various conditions, such as different camera setups, mounting positions, viewpoints, and environmental settings. We conduct thorough annotation and analysis of the data, and establish a new benchmark for 3D roadside perception with evaluation metrics and tools. Additionally, we propose adapting existing frontal-view monocular 3D object detection approaches to incorporate geometry constraints and address the inherent ambiguities caused by different sensors and viewpoints. The Rope3D dataset is accessible at https://thudair.baai.ac.cn/rope.