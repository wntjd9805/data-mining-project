Recent years have seen significant advancements in Vision-and-Language (V+L) pre-training models, which have achieved remarkable success on various multi-modal benchmarks. However, most existing models rely on pre-training with large sets of parallel image-text data, which can be expensive and time-consuming to collect compared to image-only or text-only data. In this study, we explore the concept of unsupervised Vision-and-Language pre-training (UVLP) as a means to learn cross-modal representations from non-parallel image and text datasets. Our research identifies two crucial factors that contribute to effective unsupervised V+L pre-training in the absence of parallel data: (i) the use of joint image-and-text input, and (ii) the alignment of image and text data, even when they are non-parallel. Based on these insights, we propose a novel curriculum for unsupervised V+L pre-training using non-parallel texts and images. This curriculum involves the creation of a weakly aligned image-text corpus through a retrieval-based approach, followed by a series of multi-granular alignment pre-training tasks, including region-to-tag, region-to-phrase, and image-to-sentence alignment. These tasks aim to bridge the gap between the modalities and enhance the representation learning process. Through a comprehensive ablation study, we demonstrate the effectiveness of each granularity in improving the pre-trained model's performance. Furthermore, we adapt our pre-trained model to a range of V+L downstream tasks, such as Visual Question Answering (VQA), NLVR2, Visual Entailment, and Ref-COCO+. Our model achieves state-of-the-art performance across all these tasks under the unsupervised setting.