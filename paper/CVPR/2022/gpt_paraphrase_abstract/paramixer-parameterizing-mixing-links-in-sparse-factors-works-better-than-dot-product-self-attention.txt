Self-Attention is a commonly used component in neural modeling for incorporating long-range data elements. Many self-attention neural networks rely on pairwise dot-products to determine attention coefficients. However, this approach becomes computationally expensive with a quadratic cost of O(N^2) for sequence length N. Although some approximation methods have been introduced to mitigate this issue, the dot-product approach is still limited by the low-rank constraint in attention matrix factorization.   To address this problem, we propose a new and efficient building block called Paramixer. Our method involves decomposing the interaction matrix into multiple sparse matrices, where the non-zero entries are parameterized by multilayer perceptrons (MLPs) using the data elements as input. The overall computational cost of Paramixer is significantly reduced to O(N log N), making it scalable. Additionally, all factorizing matrices in Paramixer are full-rank, eliminating the low-rank bottleneck.   We conducted experiments on synthetic and various real-world long sequential datasets, comparing Paramixer with several state-of-the-art attention networks. The results demonstrate that Paramixer outperforms other methods in most learning tasks.