This paper investigates whether an autonomous agent can navigate in a new environment without creating a map. Previous studies have shown that map-less neural models using CNNs and RNNs can achieve 100% success in PointGoal navigation tasks under ideal conditions. However, in a realistic setting with RGB-D sensing and actuation noise but without GPS+Compass, the success rate drops significantly. The main cause of this drop in performance is identified as the absence of GPS+Compass. It is found that an agent with perfect GPS+Compass can achieve 99.8% success in this scenario. This suggests that robust visual odometry is crucial for realistic PointNav. To test this hypothesis, the dataset size and model size are scaled up, and human-annotation-free data-augmentation techniques are developed to train neural models for visual odometry. As a result, the state-of-the-art performance on the Habitat Realistic PointNav Challenge is improved by 40% (relative) in terms of SPL and by 31% (relative) in terms of success. Although the dataset is not fully solved, these improvements, along with promising sim2real transfer results to a LoCoBot robot, provide evidence supporting the idea that explicit mapping may not be necessary for navigation, even in a realistic setting.