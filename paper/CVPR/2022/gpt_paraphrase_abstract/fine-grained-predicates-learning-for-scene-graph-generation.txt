The current performance of Scene Graph Generation (SGG) models is hindered by the difficulty in distinguishing between certain predicates, such as "woman-on/standing on/walking on" and "woman-near/looking at/in front of-child." Existing SGG models tend to predict head predicates, while re-balancing strategies focus on tail categories, but neither effectively handle these hard-to-distinguish predicates. To address this problem, we propose a method called Fine-Grained Predicates Learning (FGPL), inspired by fine-grained image classification. FGPL aims to differentiate among these challenging predicates in the SGG task. Our approach involves introducing a Predicate Lattice that helps SGG models identify fine-grained predicate pairs. Using the Predicate Lattice, we develop a Category Discriminating Loss and an Entity Discriminating Loss, both of which contribute to distinguishing fine-grained predicates while maintaining the ability to recognize others. This model-agnostic strategy significantly improves the performance of three benchmark models (Transformer, VCTree, and Motif) by 22.8%, 24.1%, and 21.7% in terms of Mean Recall (mR@100) on the Predicate Classification sub-task, respectively. On the Visual Genome dataset, our model also outperforms state-of-the-art methods by a large margin (6.1%, 4.6%, and 3.2% in terms of Mean Recall (mR@100)). The codes for our approach are publicly available.