Action recognition in computer vision has gained significant attention, leading to the development of various 3D-CNN based methods that address spatial and temporal dimensions in video action recognition. However, these methods have limitations in terms of robustness and generalization, particularly in understanding the impact of temporal ordering on recognition results. This study introduces a new framework called Direc-Former, which utilizes an end-to-end Transformer-based approach to address these limitations. The framework focuses on understanding the correct order of sequence actions using a Directed Attention mechanism. Additionally, the proposed approach incorporates conditional dependency in action sequence modeling, considering both orders and classes. Experimental results demonstrate that the Direc-Former consistently outperforms recent action recognition methods on standard benchmarks, including Jester, Kinetics-400, and Something-Something-V2.