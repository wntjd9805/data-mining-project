This paper explores the use of transformers in generative adversarial networks (GANs) for high-resolution image synthesis. The researchers propose a generator architecture called StyleSwin, which combines the Swin transformer model with a style-based approach. They highlight the importance of local attention in balancing computational efficiency and modeling capacity. To improve generation quality, they introduce double attention, which considers both local and shifted windows. Additionally, they demonstrate that preserving knowledge of absolute position, lost in window-based transformers, enhances generation quality. StyleSwin is scalable to high resolutions and effectively captures both coarse geometry and fine structures. However, block artifacts can occur during high-resolution synthesis due to local attention being performed in a block-wise manner. To address this, the researchers investigate different solutions and find that employing a wavelet discriminator effectively suppresses these artifacts. Extensive experiments show that StyleSwin outperforms previous transformer-based GANs, particularly at high resolutions. It achieves superior results on CelebA-HQ 1024 and comparable performance on FFHQ-1024 compared to StyleGAN. The researchers provide the code and pretrained models for StyleSwin on their GitHub repository.