The goal of Scene Graph Generation (SGG) is to encode visual contents in an image and parse them into a concise summary graph. However, current SGG approaches lack effective fusion between vision and language, leading to biased relationship predictions and impractical results. In this study, we propose a new Stacked Hybrid-Attention network as an encoder, which refines intra-modal information and facilitates inter-modal interaction. Additionally, we introduce a Group Collaborative Learning strategy to optimize the decoder. By using a group of classifiers specialized in different subsets of classes, we address the recognition limitations of individual classifiers in unbalanced datasets. Through cooperative optimization, we achieve unbiased SGG. Experimental results on VG and GQA datasets demonstrate that our approach not only sets a new state-of-the-art in unbiased metrics but also significantly outperforms two baseline methods. The code for our method is available at https://github.com/dongxingning/SHA-GCL-for-SGG.