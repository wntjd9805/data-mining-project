We propose a novel framework, CAT-Det, for multi-modal 3D object detection in autonomous driving using LiDAR point-clouds and RGB images. CAT-Det addresses the challenge of effectively utilizing these modalities by incorporating a two-stream structure comprising a Point-former branch, an Imageformer branch, and a Cross-Modal Transformer module. This framework enables the encoding of intra-modal and inter-modal long-range contexts to represent objects, thus leveraging multi-modal information for detection. Additionally, we introduce an effective data augmentation approach, One-way Multi-modal Data Augmentation (OMDA), which employs hierarchical contrastive learning at both the point and object levels. OMDA significantly improves accuracy by augmenting point-clouds without the need for generating paired samples of the two modalities. Experimental results on the KITTI benchmark demonstrate that CAT-Det achieves state-of-the-art performance, highlighting its effectiveness.