In recent times, video transformers have proven to be highly successful in video comprehension, surpassing the performance of CNNs. However, current video transformer models do not explicitly consider objects, despite the fact that objects play a crucial role in recognition. In this study, we introduce Object-Region Video Transformers (ORViT), which is an object-focused approach that extends video transformer layers with a block that directly integrates object representations. Our main idea is to merge object-centric representations from the early layers and propagate them into the transformer layers, thereby impacting the spatial and temporal representations throughout the network. The ORViT block consists of two object-level streams: appearance and dynamics. The appearance stream incorporates an "Object-Region Attention" module that applies self-attention over patches and object regions. This allows visual object regions to interact with uniform patch tokens and enhance them with contextualized object information. We also model object dynamics using a separate "Object-Dynamics Module" that captures trajectory interactions, and we demonstrate how to integrate these two streams. We evaluate our model on four tasks and five datasets, including compositional and few-shot action recognition, spatio-temporal action detection, and standard action recognition. Our results show significant performance improvements across all tasks and datasets, highlighting the value of incorporating object representations into a transformer architecture. For access to the code and pretrained models, please visit the project page at https://roeiherz.github.io/ORViT/.