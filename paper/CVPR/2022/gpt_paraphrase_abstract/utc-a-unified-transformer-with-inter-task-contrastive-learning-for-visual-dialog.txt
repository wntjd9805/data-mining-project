Visual Dialog is a task that involves answering questions based on both the history of the conversation and the content of an image. Current methods for solving this task either focus on ranking answers or generating them individually, without effectively capturing the relationship between the two tasks. There is a lack of research on a unified framework that can simultaneously learn to rank and generate answers in a single model. To address this gap, we propose a framework called UTC that uses contrastive learning to integrate discriminative and generative tasks in visual dialog. We introduce two contrastive losses, namely context contrastive loss and answer contrastive loss, to strengthen the interaction between the two tasks. These contrastive losses leverage dialog context and target answers as anchor points, providing different perspectives for representation learning. We evaluate our framework on the VisDial v1.0 dataset and demonstrate its superiority over existing methods in both discriminative and generative tasks. Our approach achieves a improvement of more than 2 absolute points on Recall@1 compared to previous state-of-the-art generative methods. The answer format produced by our framework is focused on abstraction.