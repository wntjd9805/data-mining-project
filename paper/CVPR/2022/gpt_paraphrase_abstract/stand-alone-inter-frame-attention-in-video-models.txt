Motion is a crucial aspect of videos and has played a significant role in the advancement of video understanding models. Current deep learning models utilize motion by employing spatio-temporal 3D convolutions, breaking down 3D convolutions into spatial and temporal convolutions, or implementing self-attention along the temporal dimension. These approaches assume that the feature maps from consecutive frames can be effectively combined. However, this assumption may not always hold true, especially for areas with substantial deformation.To address this limitation, we introduce a new technique called Stand-alone Inter-Frame Attention (SIFA). SIFA focuses on capturing deformation across frames to estimate local self-attention for each spatial location. This is achieved by modifying the deformable design through scaling the offset predictions using the difference between two frames. In SIFA, the current frame's spatial locations serve as queries, while the locally deformable neighbors in the next frame act as keys/values. Using stand-alone attention, SIFA measures the similarity between queries and keys to obtain weighted averages of the values for temporal aggregation.To demonstrate the effectiveness of SIFA, we integrate it into ConvNets and Vision Transformer, resulting in SIFA-Net and SIFA-Transformer, respectively. Extensive experiments conducted on four video datasets showcase the superiority of SIFA-Net and SIFA-Transformer as stronger backbone models. Notably, SIFA-Transformer achieves an impressive accuracy of 83.1% on the Kinetics-400 dataset.To facilitate further research and implementation, we have made the source code for SIFA available at https://github.com/FuchenUSTC/SIFA.