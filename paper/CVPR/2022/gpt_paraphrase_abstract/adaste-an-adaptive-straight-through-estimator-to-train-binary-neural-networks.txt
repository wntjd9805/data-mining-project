We present a novel algorithm for training deep neural networks (DNNs) with binary weights. Our approach treats the problem of training binary neural networks (BiNNs) as a bilevel optimization problem and introduces flexible relaxations to this problem. Our method shares similarities with existing techniques used to train BiNNs, such as the straight-through gradient estimator used in BinaryConnect. However, our proposed algorithm can be seen as an adaptive version of the straight-through estimator that acts like a linear mapping during the backward pass of error propagation under certain conditions. Experimental results demonstrate that our algorithm outperforms existing methods in terms of performance.