This paper introduces a novel approach called LiDAR Image Fusion Transformer (LIFT) for effectively utilizing sequential cross-sensor data in 3D object detection for autonomous driving. LIFT aims to model the interaction between LiDAR and camera data over time to enhance 3D perception. To achieve this, LIFT aligns the input sequential cross-sensor data and aggregates multi-frame multi-modal information. To reduce computational complexity, point clouds and images are projected onto bird-eye-view maps, enabling sparse grid-wise self-attention computation. Additionally, LIFT incorporates a data augmentation scheme that considers both cross-sensor and cross-time information. The proposed approach is evaluated on nuScenes and Waymo datasets, and LIFT outperforms state-of-the-art and strong baseline methods.