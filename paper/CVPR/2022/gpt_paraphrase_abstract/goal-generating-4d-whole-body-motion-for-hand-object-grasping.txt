The generation of realistic digital humans that move authentically is a widely studied area, but current methods often overlook the hands and head. While hands have been separately studied for generating static grasps, there is a need to simultaneously generate full-body motions and realistic hand grasps to synthesize virtual characters that interact with the world. This is a challenging task as the state space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and hand grasp must be physically plausible and coherent. Additionally, the head is involved as the avatar needs to look at the object it interacts with. In this study, we introduce GOAL, a method that addresses the problem of generating full-body, hand, and head motions of an avatar grasping an unknown object. GOAL takes a 3D object, its pose, and a starting 3D body pose and shape as input, and outputs a sequence of whole-body poses using two novel networks. The first network, GNet, generates a realistic whole-body grasp with accurate body, head, arm, and hand poses, as well as hand-object contact. The second network, MNet, generates the motion between the starting and goal pose, which includes walking towards the object with foot-ground contact, orienting the head towards it, reaching out, and grasping it with a realistic hand pose and hand-object contact. To achieve this, the networks utilize a representation that combines SMPL-X body parameters and 3D vertex offsets. We evaluate GOAL on the GRAB dataset both qualitatively and quantitatively, demonstrating its ability to generalize well to unseen objects and outperform baselines. A perceptual study also indicates that GOAL's generated motions approach the realism of GRAB's ground truth. Overall, GOAL represents a significant step towards generating realistic full-body object grasping motion. The models and code for GOAL are available at https://goal.is.tue.mpg.de.