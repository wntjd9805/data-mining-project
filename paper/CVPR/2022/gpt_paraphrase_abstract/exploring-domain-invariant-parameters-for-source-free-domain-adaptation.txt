Source-free domain adaptation (SFDA) has recently emerged as a crucial technique in privacy-preserving scenarios. Existing methods in SFDA primarily focus on learning domain-invariant representations solely from the target data, resulting in target-specific representations that do not fully address the problem of distribution shift across domains. In contrast, we propose a novel approach called Domain-Invariant Parameter Exploring (DIPE) that aims to explore the domain-invariant parameters of the well-trained source model. Our insight is that the domain-invariant representations are primarily determined by a subset of parameters in the deep source model. DIPE captures these domain-invariant parameters to generate domain-invariant representations. We develop a method to distinguish between domain-invariant and domain-specific parameters and introduce an effective update strategy based on the clustering correction technique. We also propose a target hypothesis to improve the adaptation process. Extensive experiments demonstrate that DIPE outperforms the current state-of-the-art models on various domain adaptation datasets.