This study focuses on the problem of continual semantic segmentation, which involves incorporating new classes into deep neural networks without forgetting previously learned knowledge. To address this issue, the authors propose a structural re-parameterization mechanism called representation compensation (RC) module. The RC module consists of two branches, one frozen and one trainable, that enable separate representation learning for old and new knowledge. Additionally, the authors introduce a pooled cube knowledge distillation strategy that enhances the model's plasticity and stability in both spatial and channel dimensions. The proposed method is evaluated on two challenging scenarios: continual class segmentation and continual domain segmentation. The experimental results demonstrate that the proposed method outperforms state-of-the-art approaches without any additional computational overhead or parameters during inference. The code for the method is available at https://github.com/zhangchbin/RCIL.