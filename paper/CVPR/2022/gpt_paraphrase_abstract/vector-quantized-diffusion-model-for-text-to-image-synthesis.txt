We propose a model called VQ-Diffusion for generating images from text. This model combines a vector quantized variational autoencoder (VQ-VAE) with a conditional variant of the Denoising Diffusion Probabilistic Model (DDPM) to create a robust and effective approach for text-to-image generation. Unlike existing methods, VQ-Diffusion eliminates biases and incorporates a mask-and-replace diffusion strategy to prevent error accumulation. Our experiments demonstrate that VQ-Diffusion outperforms conventional autoregressive (AR) models with similar parameter sizes, as well as previous GAN-based text-to-image methods, in terms of generating high-quality and complex images. Additionally, our method offers efficient image generation by utilizing reparameterization, which significantly reduces computational time compared to traditional AR methods. Specifically, VQ-Diffusion with reparameterization achieves image generation speeds that are fifteen times faster while maintaining superior image quality. The code and models for VQ-Diffusion are available at https://github.com/cientgu/VQ-Diffusion.