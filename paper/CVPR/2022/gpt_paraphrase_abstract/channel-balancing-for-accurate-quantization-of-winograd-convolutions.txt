The quantization of faster Winograd convolution algorithms often leads to a decrease in model quality. To address this issue, we propose a new class of Winograd algorithms that balance the filter and input channels in the Winograd domain. Unlike traditional Winograd convolutions, our approach scales the input tensor using special balancing coefficients to balance the ranges of input channels during the forward pass. This balancing makes it easier to quantize the inputs and filters of the Winograd convolution. As a result, our technique allows for the creation of models with quantized Winograd convolutions that have significantly higher quality compared to traditional quantized Winograd convolutions. We also introduce a direct algorithm for calculating the balancing coefficients, which eliminates the need for additional model training. By feeding a few data samples to the model without training, one can obtain post-training quantized balanced Winograd convolutions. Additionally, the balancing coefficients can be initialized using this algorithm and further trained as trainable variables during Winograd quantization-aware training to improve quality.