The lack of a benchmark dataset for multi-modal video similarity evaluation in video recommendation systems has hindered supervised training and accurate evaluation. To address this, we introduce the Tencent-MVSE dataset, the first benchmark dataset for this task. It includes video pairs similarity annotations and diverse metadata like Chinese title, automatic speech recognition (ASR) text, and human-annotated categories/tags. We present a basic approach using a multi-modal Transformer architecture for supervised evaluation. Additionally, we explore pre-training techniques to leverage unpaired data. The complete dataset and our baseline approach will be made publicly available to foster the advancement of multi-modal video similarity evaluation. The dataset can be accessed at https://tencent-mvse.github.io/.