This paper focuses on the classification of fine-grained, multi-step activities in long videos. Unlike traditional action classification, accurately categorizing these activities requires understanding the individual steps and their temporal dependencies. However, existing datasets lack segment labels for these steps due to the high cost of manually annotating long videos. To overcome this, the authors propose a method that automatically identifies steps in instructional videos using a textual knowledge base. By matching transcribed speech from the video to step descriptions in the knowledge base, video models can be trained to recognize these steps without manual supervision. The authors demonstrate that this approach achieves superior performance in recognizing procedural activities, step classification, step forecasting, and egocentric video classification.