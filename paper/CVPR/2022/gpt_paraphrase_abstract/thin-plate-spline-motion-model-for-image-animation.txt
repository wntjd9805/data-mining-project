This paper presents a new unsupervised motion transfer framework for animating static objects in images using a driving video. Existing unsupervised methods face challenges when there is a significant difference in pose between the objects in the source and driving images. To overcome this, the proposed framework utilizes thin-plate spline motion estimation to generate a more flexible optical flow, which warps the feature maps of the source image to match those of the driving image. Additionally, multi-resolution occlusion masks are used to realistically restore missing regions and improve feature fusion. The network modules are guided by auxiliary loss functions to ensure clear division of labor and generate high-quality images. The proposed method can animate various objects, such as talking faces, human bodies, and pixel animations. Experimental results show that the method outperforms state-of-the-art techniques on motion-related metrics, demonstrating visible improvements.