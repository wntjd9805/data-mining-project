Egocentric augmented reality devices, like wearable glasses, can passively capture visual data as a person explores a home environment. We have a vision where the person can communicate with an AI agent that powers such a device by asking questions, such as "where did you last see my keys?". To achieve this, the AI assistant needs to (1) create scene memories that contain detailed spatio-temporal information about the objects seen during the exploration and (2) understand the question and provide an answer based on the stored memory. To address these challenges, we introduce a new task called Episodic Memory Question Answering (EMQA). In this task, the egocentric AI assistant is given a video sequence of the exploration and a question, and it needs to accurately locate the answer within the video. We also provide a dataset of questions designed to test the agent's understanding of the spatio-temporal aspects of the exploration. Additionally, we propose a model for this task that represents the scene as a top-down semantic feature map and grounds the question in the map to find the answer. Our experimental results demonstrate that our episodic scene memory approach outperforms other solutions and is robust against noise in depth, pose, and camera movement. The answer format focuses on providing only the essential information.