Constructing a comprehensive data collection that includes all potential combinations of human actions and interacting objects is challenging due to the combinatorial nature of human-object interactions (HOI). This study aims to develop a transferable HOI detector capable of identifying unseen interactions. Existing HOI detectors typically treat interactions as discrete labels and train a classifier based on predetermined categories. However, this approach is inadequate for detecting interactions that fall outside of the predefined categories. In contrast, we consider independent HOI labels as the natural language supervision for interactions and embed them into a joint visual-and-text space to capture their correlations. To achieve this, we propose a novel HOI visual encoder that detects interacting humans and objects and maps them to a shared feature space for interaction recognition. Our visual encoder is implemented as a Vision Transformer with learnable HOI tokens and a sequence parser that generates unique HOI predictions. It leverages and distills transferable knowledge from the pretrained CLIP model to enable zero-shot interaction detection. Experimental results on two datasets, SWIG-HOI and HICO-DET, validate the effectiveness of our method in detecting both seen and unseen HOIs, demonstrating a significant improvement in mean Average Precision (mAP). The code for our proposed method is available at https://github.com/scwangdyd/promting_hoi.