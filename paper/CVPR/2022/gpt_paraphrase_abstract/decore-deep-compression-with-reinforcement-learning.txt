Deep learning has become a popular and powerful method for pattern recognition systems. However, the large number of parameters in deep neural networks makes them impractical for real-world applications due to memory and latency constraints. To address this, efficient network compression techniques are necessary. We introduce DECORE, an automated network compression approach using reinforcement learning. DECORE assigns an agent to each channel in the network and trains them to determine which neurons or channels to keep or remove. This method allows for faster training as each agent only has one parameter to learn. DECORE achieves state-of-the-art compression results on different network architectures and datasets. For example, on the ResNet-110 architecture, it achieves a 64.8% compression and 61.8% reduction in FLOPs without sacrificing accuracy on the CIFAR-10 dataset. It can also significantly reduce the size of regular architectures like the VGG network by up to 99% with only a small accuracy drop of 2.28%. Furthermore, on the ImageNet dataset, DECORE can compress the ResNet-50 architecture by 44.7% and reduce FLOPs by 42.3% with just a 0.69% drop in Top-5 accuracy compared to the uncompressed model. Additionally, DECORE can be used to search for compressed network architectures based on specific constraints such as memory and FLOPs.