Current methods for creating realistic and animatable 3D clothed avatars require either posed 3D scans or carefully controlled 2D images of users. In contrast, our objective is to develop a method that can generate avatars from unconstrained 2D images of people in various poses. Our approach involves estimating a detailed 3D surface from each image and then combining them to create an animatable avatar. We utilize implicit functions for capturing fine details like hair and clothes. However, existing methods struggle with different human poses and often produce flawed 3D surfaces with disconnected limbs, missing details, or unrealistic shapes. This issue arises from the use of global feature encoders that are sensitive to overall pose. To overcome this challenge, we propose a new method called ICON ("Implicit Clothed humans Obtained from Normals") that utilizes local features instead. ICON consists of two main modules that leverage the SMPL(-X) body model. The first module infers detailed clothed-human normals (front/back) based on the SMPL(-X) normals. The second module employs a visibility-aware implicit surface regressor to create an iso-surface representing the human occupancy field. At inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and refining the normals themselves. By reconstructing multiple frames of a subject in different poses, we can use a modified version of SCANimate to generate an animatable avatar. Evaluation on the AGORA and CAPE datasets demonstrates that ICON outperforms existing methods in reconstruction, even when trained with limited data. Furthermore, ICON exhibits greater robustness to out-of-distribution samples, such as poses/images captured in real-world scenarios and images with out-of-frame cropping. Our work represents a significant step towards robust 3D clothed human reconstruction from images taken in real-world settings. This advancement enables the creation of avatars directly from video, incorporating personalized pose-dependent cloth deformation. The models and code for our research are available at https://icon.is.tue.mpg.de.