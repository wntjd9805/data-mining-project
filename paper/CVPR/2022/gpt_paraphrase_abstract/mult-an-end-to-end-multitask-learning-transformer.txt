We introduce MulT, an end-to-end Multitask Learning framework based on the Swin transformer model. MulT enables simultaneous learning of multiple high-level vision tasks, such as depth estimation, semantic segmentation, reshading, surface normal estimation, 2D keypoint detection, and edge detection. By encoding the input image into a shared representation and utilizing task-specific transformer-based decoder heads, MulT achieves superior performance compared to both state-of-the-art multitask convolutional neural network models and single-task transformer models. The key aspect of our approach is the utilization of a shared attention mechanism that models dependencies across tasks. Through evaluations on multitask benchmarks, we demonstrate the advantages of sharing attention and the robustness and generalizability of our MulT model in various domains. For more information, visit our project website at https://ivrl.github.io/MulT/.