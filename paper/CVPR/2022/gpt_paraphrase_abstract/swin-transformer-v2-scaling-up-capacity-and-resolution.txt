We have developed methods to scale up the Swin Transformer model to accommodate 3 billion parameters and train it with high-resolution images up to 1,536Ã—1,536 pixels. This increased capacity and resolution have resulted in new performance records on several vision benchmarks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification.In our research, we addressed challenges related to training instability and the transfer of pre-trained models from low to high resolutions. To overcome these challenges, we introduced innovative techniques such as residual post normalization and scaled cosine attention to enhance the stability of large vision models. We also developed a log-spaced continuous position bias technique to effectively transfer models trained on low-resolution images to their high-resolution counterparts.Furthermore, we provide important implementation details that significantly reduce GPU memory consumption, enabling the training of large vision models using regular GPUs. By employing these techniques and utilizing self-supervised pre-training, we successfully trained a powerful 3 billion parameter Swin Transformer model and effectively applied it to various vision tasks involving high-resolution images or windows. Our approach achieved state-of-the-art accuracy on multiple benchmarks.For further information and access to our code, please visit https://github.com/microsoft/Swin-Transformer.