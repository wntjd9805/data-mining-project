Trajectory prediction is crucial for the safe planning and navigation of autonomous vehicles (AVs). However, little research has been done on analyzing the robustness of trajectory prediction and assessing whether worst-case predictions can still lead to safe planning. In order to address this gap, we investigate the adversarial robustness of trajectory prediction models by introducing a new adversarial attack that disrupts normal vehicle trajectories to maximize prediction errors. Our experiments, conducted on three models and three datasets, reveal that adversarial predictions lead to an increase in prediction errors of over 150%. Through case studies, we demonstrate that if an adversary drives a vehicle in close proximity to the target AV using an adversarial trajectory, the AV may produce inaccurate predictions and make unsafe driving decisions. To mitigate these risks, we explore potential techniques such as data augmentation and trajectory smoothing.