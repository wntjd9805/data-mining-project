As the use of powerful pre-trained vision-language models like CLIP increases, it is important to explore methods for adapting these models to different datasets. A recent approach called Context Optimization (CoOp) introduces the concept of prompt learning, which is commonly used in natural language processing, to the field of computer vision. CoOp converts context words in a prompt into learnable vectors and achieves significant improvements with only a small number of labeled images for training, surpassing the performance of manually tuned prompts. However, our research has identified a significant issue with CoOp: the learned context is not applicable to unseen classes within the same dataset, suggesting that CoOp is overfitting the classes observed during training. To address this problem, we propose Conditional Context Optimization (CoCoOp), an extension of CoOp that incorporates the learning of a lightweight neural network to generate an input-conditional token (vector) for each image. Unlike CoOp's fixed prompts, our dynamic prompts adapt to each individual instance and are less affected by changes in class distribution. Through extensive experiments, we demonstrate that CoCoOp achieves better generalization to unseen classes compared to CoOp, and even shows promising transferability across multiple datasets. Additionally, CoCoOp exhibits stronger performance in domain generalization. The code for CoCoOp is available at https://github.com/KaiyangZhou/CoOp.