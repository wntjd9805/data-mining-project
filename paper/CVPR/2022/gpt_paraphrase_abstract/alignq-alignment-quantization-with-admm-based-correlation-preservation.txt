Quantization is a technique used to compress neural networks and reduce inference time. However, existing methods overlook the distribution differences between training and testing data, leading to significant quantization errors during inference. To address this issue, we propose a new quantization scheme called Alignment Quantization with ADMM-based Correlation Preservation (AlignQ). AlignQ leverages the cumulative distribution function (CDF) to align the data and make it independently and identically distributed (i.i.d.), thereby minimizing quantization errors. Our theoretical analysis reveals that the quantization process alters data correlations and introduces errors. To preserve the relationship between data in the original space and aligned quantization space, we design an optimization process using the Alternating Direction Method of Multipliers (ADMM) optimization. This process minimizes differences in data correlations before and after alignment and quantization. We conduct experiments to visualize the non-i.i.d. nature of training and testing data in a benchmark. Additionally, we compare AlignQ with state-of-the-art methods using domain shift data. The experimental results demonstrate that AlignQ achieves significant performance improvements, particularly in low-bit models. The code for AlignQ is available at https://github.com/tinganchen/AlignQ.git.