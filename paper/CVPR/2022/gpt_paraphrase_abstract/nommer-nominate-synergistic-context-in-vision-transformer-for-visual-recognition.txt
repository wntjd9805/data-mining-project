In recent years, Vision Transformers (ViT) have shown great potential in computer vision tasks, thanks to their use of self-attention (SA). However, some approaches only apply SA within local patches, neglecting the importance of global contextual information. To address this limitation, global-local ViTs have been proposed, combining local and global context. However, this combination may lead to redundancy in visual data and fixed receptive fields. In this paper, we propose a new ViT architecture called NomMer, which dynamically selects the synergistic global-local context. Through our investigation, we identify the focused context information. NomMer achieves an 84.5% Top-1 classification accuracy on ImageNet with only 73M parameters and also performs well in object detection and semantic segmentation tasks. The code and models for NomMer are publicly available at https://github.com/TencentYoutuResearch/VisualRecognition-NomMer.