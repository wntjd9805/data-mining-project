Deep cross-modal hashing is an important tool for supervised multimodal search. However, most existing models are optimized using large, high-quality multimodal datasets with manually verified labels. In many scenarios, accurate labeling may not be available, resulting in datasets with low-quality annotations and label noise that negatively impact search performance. To address this challenge, we propose a robust cross-modal hashing framework that can handle noisy labels and correlate different modalities effectively. Our approach introduces a proxy-based contrastive (PC) loss, which reduces the gap between modalities and trains networks for different modalities jointly using small-loss samples selected based on the PC loss and a mutual quantization loss. The selection of small-loss samples helps guide the model training by choosing confident examples, while the mutual quantization loss maximizes agreement between modalities and improves the effectiveness of sample selection. We conducted experiments on three widely-used multimodal datasets, and our method outperformed existing state-of-the-art approaches significantly.