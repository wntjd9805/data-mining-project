Unsupervised learning is widely used in computer vision tasks, including visual object tracking. However, existing unsupervised tracking methods heavily rely on spatial supervision and struggle to track objects with significant variation over a long period. To address this limitation, we propose a new unsupervised tracking framework that leverages the self-supervision signals obtained by tracking a video in a forward-backward cycle. Our approach involves evolving a Siamese tracker by learning temporal correspondence in both the classification and regression branches.To ensure reliable template feature propagation in the forward propagation process, we introduce a consistency propagation transformation. Additionally, we identify a problem with conventional cycle training in the backward propagation process, which leads to an ill-posed penalty issue. To overcome this problem, we propose a differentiable region mask that selects features and implicitly penalizes tracking errors in intermediate frames.Furthermore, we address the issue of noisy labels during training by introducing a mask-guided loss reweighting strategy. This strategy assigns dynamic weights based on the quality of pseudo labels, thereby reducing the impact of noisy labels.We conducted extensive experiments to evaluate our tracker's performance. The results show that our approach outperforms previous unsupervised methods by a significant margin and performs on par with supervised methods on large-scale datasets such as TrackingNet and LaSOT.The code for our method is available at https://github.com/FlorinShum/ULAST.