Vision Transformers (ViT) have emerged as powerful models for computer vision tasks, offering the ability to capture long-range dependencies in data. However, the self-attention mechanism, a crucial component of transformer architectures, suffers from high latency and inefficient memory usage, making it less suitable for high-resolution images. To address these issues, hierarchical vision models have incorporated self-attention on non-interleaving windows, reducing complexity but limiting cross-window interaction and impacting model performance. In this paper, we propose a new shift-invariant local attention layer called query and attend (QnA), which aggregates input locally in an overlapping manner, similar to convolutions. QnA introduces learned queries for fast and efficient implementation. We validate the effectiveness of our layer by integrating it into a hierarchical vision transformer model, demonstrating improved speed and memory complexity while achieving comparable accuracy to state-of-the-art models. Importantly, our layer scales well with window size, requiring significantly less memory (up to x10) and being faster (up to x5) than existing methods. The code for our approach is publicly available at https://github.com/moabarar/qna.