This research paper investigates the issue of efficiency in visual transformers by identifying and eliminating redundant calculations within the given networks. While the transformer architecture has proven to be effective in achieving outstanding performance in various computer vision tasks, the computational cost associated with vision transformers remains a significant concern, similar to convolutional neural networks. The attention mechanism used in transformers aggregates patches in a layer-by-layer manner. To address this problem, a novel approach called patch slimming is proposed, which discards unnecessary patches in a top-down paradigm. The effective patches in the last layer are initially identified and then used to guide the patch selection process in previous layers. Each patch's impact on the final output feature is approximated, and patches with minimal impact are removed. Experimental results on benchmark datasets demonstrate that this method can substantially reduce the computational costs of vision transformers without compromising their performance. For instance, the proposed method achieves a reduction of over 45% in floating-point operations (FLOPs) for the ViT-Ti model, with only a minimal drop of 0.2% in top-1 accuracy on the ImageNet dataset.