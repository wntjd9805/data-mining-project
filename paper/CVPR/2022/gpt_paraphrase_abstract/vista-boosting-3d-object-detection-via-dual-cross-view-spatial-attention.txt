Accurate and reliable 3D object detection from LiDAR point clouds is crucial for autonomous driving. Existing multi-view methods have shown promise by utilizing information from both bird's eye view (BEV) and range view (RV), but their performance is limited due to a lack of global spatial context. In this paper, we propose a novel fusion module called Dual Cross-VIew SpaTial Atten-tion (VISTA) that adaptively fuses multi-view features in a global spatial context. VISTA replaces the standard attention module's multi-layer perceptron with a convolutional one, resulting in high-quality fused features for proposal prediction. We separate the classification and regression tasks in VISTA and apply an attention variance constraint to focus on specific targets. Our experiments on the nuScenes and Waymo benchmarks demonstrate the effectiveness of our approach. Our method achieves an overall mAP of 63.0% and an NDS of 69.8% on the nuScenes benchmark, surpassing all published methods by up to 24% in safety-crucial categories like cyclist.