Recent advancements in vision Transformers (ViTs) have resulted in a significant increase in computational requirements for training, highlighting the urgent need for efficient training methods for ViTs. One promising approach is progressive learning, where the model capacity gradually increases during training to enhance efficiency. This paper addresses the issue of efficient training for ViTs by customizing and automating progressive learning.  To begin, the authors develop a strong manual baseline for progressive learning of ViTs by introducing momentum growth (MoGrow) to mitigate the challenges posed by model growth. Building upon this, they propose automated progressive learning (AutoProg), a training scheme aimed at achieving lossless acceleration by dynamically adjusting the training overload. This is accomplished by adaptively determining when, where, and how much the model should grow during progressive learning.  The authors first relax the optimization of the growth schedule to a sub-network architecture optimization problem. They then propose a one-shot estimation approach using an elastic supernet to assess the performance of the sub-network. By recycling the parameters of the supernet, the searching overhead is minimized.  Extensive experiments on ImageNet using two representative ViT models, DeiT and VOLO, demonstrate the effectiveness of AutoProg in accelerating ViTs training by up to 85.1% without sacrificing performance.  Overall, this paper presents a practical and automated approach to efficiently train ViTs by leveraging progressive learning techniques. The proposed AutoProg scheme successfully accelerates training without compromising performance, making it a valuable contribution to the field of ViTs.