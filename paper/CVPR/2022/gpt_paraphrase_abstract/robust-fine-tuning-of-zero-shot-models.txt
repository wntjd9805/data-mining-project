Large pre-trained models like CLIP or ALIGN exhibit consistent accuracy across different data distributions when performing zero-shot inference, without the need for fine-tuning on specific datasets. However, existing fine-tuning methods enhance accuracy on a particular target distribution at the expense of reduced robustness to distribution shifts. To address this dilemma, we propose WiSE-FT, a simple yet effective approach that improves robustness while fine-tuning by ensembling the weights of both zero-shot and fine-tuned models.WiSE-FT outperforms standard fine-tuning by significantly boosting accuracy under distribution shift. On ImageNet and its derived distribution shifts, WiSE-FT achieves accuracy improvements ranging from 4 to 6 percentage points (pp) compared to previous methods, while also increasing ImageNet accuracy by 1.6 pp. Additionally, WiSE-FT demonstrates substantial robustness gains of 2 to 23 pp on six diverse distribution shifts, along with accuracy gains of 0.8 to 3.3 pp on commonly used transfer learning datasets when compared to standard fine-tuning. Notably, these improvements are achieved without any additional computational cost during fine-tuning or inference.