We present a solution to the challenging problem of guiding autonomous agents in unfamiliar environments based on language instructions. Our proposed method, called dual-scale graph transformer (DUET), addresses the need for both grounding language in visual scenes and exploring the environment to reach a target. To achieve this, we construct a topological map in real-time, enabling efficient global action planning. To balance the complexity of reasoning in a large action space and fine-grained language grounding, we employ graph transformers to dynamically combine fine-scale encoding of local observations with coarse-scale encoding on the global map. Our approach, DUET, surpasses existing methods on goal-oriented vision-and-language navigation benchmarks (REVERIE and SOON) and also improves the success rate on the fine-grained VLN benchmark R2R.