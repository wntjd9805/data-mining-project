Video summarization has become a popular topic in computer vision, but the lack of annotated data has hindered progress in this field. This study proposes a new approach to video summarization by utilizing samples from a related task, video moment localization, which has abundant training data. The researchers observe that annotated video moments can also indicate the important highlights of a video, similar to a video summary. They suggest that a video summary can be seen as a condensed, non-repetitive version of the video moments. Based on this insight, they introduce an importance Propagation-based collaborative Teaching Network (iPTNet) consisting of two modules for video summarization and moment localization. Each module generates an importance map for individual frames to identify keyframes or moments. To transfer samples between the tasks, they design an importance propagation module that converts the importance maps guided by summarization and localization. This enables the optimization of one task using data from the other task. They also address the issue of error amplification during joint training by implementing a collaborative teaching scheme that utilizes a cross-task mean teaching strategy for joint optimization and robust frame-level teaching signals. Experimental results on video summarization benchmarks demonstrate that iPTNet outperforms existing methods, offering a solution to overcome the scarcity of data in video summarization.