In this paper, we explore the concept of large kernel design in modern convolutional neural networks (CNNs) and its potential benefits. Drawing inspiration from recent advancements in vision transformers (ViTs), we propose that utilizing a few large convolutional kernels instead of a stack of small kernels can yield a more powerful paradigm. To facilitate the design of efficient and high-performance large-kernel CNNs, we put forth five guidelines, such as employing re-parameterized large depth-wise convolutions.Following these guidelines, we introduce RepLKNet, a CNN architecture that solely relies on large kernels with a size of 31×31, in contrast to the commonly used 3×3 kernels. RepLKNet significantly narrows the performance gap between CNNs and ViTs. It achieves comparable or even superior results compared to the Swin Transformer on ImageNet and other typical downstream tasks, all while maintaining lower latency.Moreover, RepLKNet exhibits excellent scalability when dealing with large datasets and models. It attains an impressive 87.8% top-1 accuracy on ImageNet and a 56.0% mIoU on ADE20K, making it highly competitive among state-of-the-art models of similar sizes. Our study also reveals that, unlike small-kernel CNNs, large-kernel CNNs possess significantly larger effective receptive fields and a stronger inclination towards shape bias rather than texture bias.For access to the code and models related to RepLKNet, interested readers can visit the following GitHub repository: https://github.com/megvii-research/RepLKNet.