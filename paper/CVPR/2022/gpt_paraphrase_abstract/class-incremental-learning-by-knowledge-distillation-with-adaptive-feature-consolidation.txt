We introduce a new approach to class incremental learning using deep neural networks, which can learn new tasks while preserving limited memory for storing previous examples. Our algorithm, based on knowledge distillation, allows for the maintenance of old model representations while adapting to new tasks. By estimating the relationship between representation changes and resulting loss increases, our method minimizes the bound of loss increases using representations. This approach takes advantage of the estimated importance of each feature map in a backbone model, allowing for restricted updates to important features for robustness while permitting changes in less critical features for flexibility. Despite the limited accessibility of data from previous tasks, this optimization strategy effectively addresses the catastrophic forgetting problem. Our experimental results demonstrate significant accuracy improvements compared to existing methods on standard datasets. The code for our algorithm is available.