The theoretical properties of model-agnostic meta-learning (MAML) with deep neural networks (DNNs) are not well understood due to the non-convexity of DNNs and the bi-level formulation of MAML. This paper proves that MAML with over-parameterized DNNs converges to global optima at a linear rate. The convergence analysis reveals that MAML with over-parameterized DNNs is equivalent to kernel regression using a new class of kernels called Meta Neural Tangent Kernels (MetaNTK). The paper introduces MetaNTK-NAS, a training-free neural architecture search (NAS) method for few-shot learning that utilizes MetaNTK to rank and select architectures. Experimental results on miniImageNet and tieredImageNet benchmarks demonstrate that MetaNTK-NAS performs comparably or better than state-of-the-art NAS methods for few-shot learning, with a speedup of over 100x. The efficiency of MetaNTK-NAS makes it practical for real-world tasks. The code for MetaNTK-NAS is available at github.com/YiteWang/MetaNTK-NAS.