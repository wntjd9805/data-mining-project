Previous research on action representation learning has primarily focused on designing architectures for extracting global representations from short video clips. However, practical applications such as video alignment require dense representations for long videos. In this paper, we propose a novel framework called Contrastive Action Representation Learning (CARL) that learns frame-wise action representations, specifically for long videos, in a self-supervised manner. Our approach utilizes a simple yet efficient video encoder that considers spatio-temporal context to extract frame-wise representations. Inspired by recent advancements in self-supervised learning, we introduce a Sequence Contrastive Loss (SCL) that operates on two correlated views obtained through spatio-temporal data augmentations. The SCL optimizes the embedding space by minimizing the KL-divergence between the sequence similarity of the augmented views and a prior Gaussian distribution of timestamp distance. Experimental results on the FineGym, PennAction, and Pouring datasets demonstrate that our method outperforms previous state-of-the-art approaches in fine-grained action classification. Surprisingly, even without training on paired videos, our approach also achieves outstanding performance in video alignment and fine-grained frame retrieval tasks. The code and models for our approach are available at the provided GitHub link. The figures in the paper illustrate the multiple applications of our frame-wise representation learning on different datasets, showcasing the invariance of the representations to appearance, viewpoint, and background.