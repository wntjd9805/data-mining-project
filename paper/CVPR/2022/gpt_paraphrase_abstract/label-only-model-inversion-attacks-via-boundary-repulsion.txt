Recent research indicates that advanced deep neural networks are susceptible to model inversion attacks, where an attacker can use a model to reconstruct private training data of a specific target class. Existing attacks require access to either the complete target model or the model's soft-labels. However, there is a lack of investigation into the more challenging but practical scenario where the attacker only has access to the model's predicted label, without a confidence measure. This paper introduces Boundary-Repelling Model Inversion (BREP-MI), an algorithm that can invert private training data solely using the predicted labels of the target model. The algorithm works by evaluating the model's predicted labels across a sphere and estimating the direction towards the centroid of the target class. The paper demonstrates the effectiveness of BREP-MI in reconstructing the semantics of private training data in face recognition tasks using various datasets and target model architectures. Comparisons with white-box and blackbox model inversion attacks reveal that BREP-MI, despite assuming less knowledge about the target model, outperforms the blackbox attack and achieves comparable results to the whitebox attack. The code for BREP-MI is available online.