Generating high-quality realistic images from text descriptions is a challenging task in the field of text-to-image Generative Adversarial Networks (GANs). However, existing GAN models have three main flaws. Firstly, the stacked architecture used in these models leads to entanglements between generators of different image scales. Secondly, previous studies often rely on additional networks for text-image semantic consistency, which limits the supervision capability of these networks. Lastly, the widely adopted cross-modal attention-based text-image fusion technique is limited to certain image scales due to computational constraints.To address these issues, we propose a simpler and more effective GAN model called DeepFusion GAN (DF-GAN). Our proposed model includes the following key components: 1. A novel one-stage text-to-image backbone that directly synthesizes high-resolution images without entanglements between different generators.2. A novel Target-Aware Discriminator that enhances text-image semantic consistency without the need for extra networks. This discriminator is composed of a Matching-Aware Gradient Penalty and One-Way Output.3. A novel deep text-image fusion block that deepens the fusion process, allowing for a comprehensive fusion of text and visual features.Compared to current state-of-the-art methods, DF-GAN is simpler yet more efficient in synthesizing realistic images that match the given text descriptions. Our proposed model achieves better performance on widely used datasets. For those interested, the code for DF-GAN is available at https://github.com/tobran/DF-GAN.