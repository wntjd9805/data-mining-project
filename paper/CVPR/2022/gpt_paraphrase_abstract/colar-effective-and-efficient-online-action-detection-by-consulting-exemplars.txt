Online action detection has become a popular research area, focusing on modeling historical dependencies and predicting future actions in videos for improved accuracy. However, current approaches lack category-level modeling and efficiency. To address this, this study proposes an exemplar-consultation mechanism that measures similarity between frames and exemplary frames, aggregating features based on similarity weights. This mechanism is efficient, requiring limited computations. By considering historical frames as exemplars, long-term dependencies are captured, while representative frames from a category serve as exemplars for category-level modeling. The proposed method achieves high performance on three benchmarks while using a lightweight architecture. Additionally, by using a spatio-temporal network for video frames, the method strikes a balance between effectiveness and efficiency. The code for this method is available at https://github.com/VividLe/Online-Action-Detection.