This paper introduces a multi-view approach to accurately estimate the 3D poses of real moving objects without using markers. The proposed method utilizes binocular data and requires synchronous and relatively fixed cameras that are calibrated. By optimizing the object pose based on an object-centered model and minimizing shape re-projection constraints in all views, the proposed approach achieves higher accuracy compared to single-view methods and even surpasses depth-based methods. A new benchmark dataset for monocular textureless 3D object tracking is constructed, containing 20 textureless objects, 22 scenes, 404 video sequences, and 126K images captured in real scenes. The annotation error in the dataset is guaranteed to be less than 2mm through theoretical analysis and validation experiments. The performance of state-of-the-art 3D object tracking methods is re-evaluated using this dataset, providing rankings in real scenes. The BCOT benchmark and code can be accessed at https://ar3dv.github.io/BCOT-Benchmark/.