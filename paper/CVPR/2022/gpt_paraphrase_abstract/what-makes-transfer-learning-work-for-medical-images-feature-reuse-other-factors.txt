Transfer learning is a widely used technique for transferring knowledge from one domain to another. In the field of medical imaging, the prevailing approach is to transfer knowledge from ImageNet, despite differences in the tasks and characteristics of the two domains. However, it is not clear what factors determine the usefulness and effectiveness of transfer learning in the medical domain. The traditional belief that features from the source domain can be reused has recently been questioned. To investigate this, we conducted a series of experiments on various medical image benchmark datasets. Our aim was to understand the relationship between transfer learning, data size, model capacity and inductive bias, as well as the distance between the source and target domains. Our findings indicate that transfer learning is generally beneficial, and we emphasize the crucial role of feature reuse in its success.