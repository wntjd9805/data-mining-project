Visual Emotion Analysis (VEA) is a growing field that focuses on understanding the emotions expressed in visual content. One of the main challenges in VEA is bridging the gap between the visual cues in an image and the actual emotion conveyed by the image. This gap becomes more difficult to bridge as emotions become more nuanced and detailed.Existing deep learning approaches attempt to bridge this gap by learning to distinguish between different emotions globally in a single step. However, these approaches overlook the hierarchical relationship between emotions at different levels and the variation in the levels of emotions to be classified.In this study, we propose a multi-level dependent attention network (MDAN) with two branches to address these limitations. The bottom-up branch learns emotions at the highest affective level while explicitly following the emotion hierarchy. This branch also predicts emotions at lower affective levels, thus preventing hierarchy violation.On the other hand, the top-down branch aims to disentangle the affective gap by mapping semantic levels to affective levels using a one-to-one mapping called Affective Semantic Mapping. At each semantic level, a local classifier is added to learn discrimination among emotions at the corresponding affective level.To integrate both global and local learning, we develop a unified deep framework that optimizes the model simultaneously. Additionally, we design two attention modules, namely the Multi-head CrossChannel Attention module and the Level-dependent ClassActivation Map module, to properly capture channel dependencies and spatial attention while disentangling the affective gap.Our proposed deep framework achieves state-of-the-art performance on six VEA benchmarks, surpassing existing methods significantly. For example, it achieves a 3.85% improvement in the WEBEmo dataset's 25-class classification accuracy.