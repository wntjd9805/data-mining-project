We introduce Point2Seq, a straightforward and efficient framework for detecting 3D objects from point clouds. Unlike previous methods that predict all attributes of 3D objects simultaneously, our approach models the relationships between these attributes, resulting in improved detection accuracy. We treat each 3D object as a sequence of words and reframe the detection task as decoding words from 3D scenes in an auto-regressive manner. To accomplish this, we propose a lightweight scene-to-sequence decoder that generates words based on features from the 3D scene and cues from preceding words. The predicted words form sequences that fully describe the 3D objects, and these sequences are matched to the corresponding ground truths using similarity-based sequence matching. Our approach is intuitive and can be easily integrated into existing 3D detection frameworks without significant computational overhead. Additionally, our sequential decoding technique effectively leverages information from complex 3D scenes with the help of preceding predicted words. Our method surpasses previous anchor- and center-based 3D object detection frameworks, achieving state-of-the-art performance on the challenging ONCE dataset and the Waymo OpenDataset. The code for our framework is available at https://github.com/ocNflag/point2seq.