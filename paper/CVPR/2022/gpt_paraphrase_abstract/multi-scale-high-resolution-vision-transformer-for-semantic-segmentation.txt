Vision Transformers (ViTs) have shown better performance than convolutional neural network (CNN)-based models in computer vision tasks. However, ViTs designed for image classification struggle with dense prediction tasks like semantic segmentation because they generate low-resolution representations. To address this, we propose HRViT, which enhances ViTs by integrating high-resolution multi-branch architectures to learn semantically-rich and spatially-precise multi-scale representations. We optimize HRViT's performance and efficiency through various branch-block co-optimization techniques. These include exploring different branch designs, reducing redundancy in linear layers, and enhancing the expressiveness of the attention block. Our evaluation on ADE20K and Cityscapes datasets demonstrates that HRViT achieves significant improvements in performance and efficiency compared to state-of-the-art MiT and CSWin backbones. Specifically, HRViT achieves 50.20% mIoU on ADE20K and 83.16% mIoU on Cityscapes, surpassing existing models with an average improvement of +1.78 mIoU, 28% parameter reduction, and 21% FLOPs reduction. These results highlight HRViT's potential as a strong vision backbone for semantic segmentation. Our code is publicly available for reference.