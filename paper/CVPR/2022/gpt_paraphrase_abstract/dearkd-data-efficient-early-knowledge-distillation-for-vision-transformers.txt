The use of transformers in computer vision has been successful due to their strong modeling capabilities with self-attention. However, their performance heavily relies on a large amount of training data. Therefore, there is a pressing need for a data-efficient solution for transformers. This study presents an early knowledge distillation framework called DearKD, which aims to improve the data efficiency of transformers. DearKD is a two-stage framework that first distills the biases from the early intermediate layers of a convolutional neural network (CNN), and then allows the transformer to train without distillation. Additionally, DearKD can also be applied to cases where no real images are available, using a boundary-preserving intra-divergence loss based on DeepInversion to bridge the performance gap with the full-data counterpart. Extensive experiments conducted on ImageNet, partial ImageNet, data-free settings, and other downstream tasks demonstrate the superiority of DearKD over its baselines and state-of-the-art methods.