Recent advancements in salient object detection (SOD) using deep neural networks have shown impressive results. However, existing SOD models designed for low-resolution images struggle to perform well on high-resolution images due to the mismatch between sampling depth and receptive field size. To address this issue, we propose a new framework called Pyramid Grafting Network (PGNet). PGNet combines a transformer and a CNN backbone to extract features independently from images of different resolutions. These features are then integrated by grafting the transformer branch's features onto the CNN branch's features. To enhance the CNN branch's ability to capture fine-grained details, we introduce an attention-based Cross-Model Grafting Module (CMGM). CMGM guides the CNN branch to holistically combine fragmented detailed information during the decoding process. Additionally, we introduce an Attention Guided Loss (AGL) to explicitly supervise the attention matrix generated by CMGM, enabling better interaction between the network and attention from different models. We also present a new Ultra-High-Resolution Saliency Detection dataset (UHRSD) comprising 5,920 images at 4K-8K resolutions, which is the largest dataset in terms of both quantity and resolution for high-resolution SOD research. Extensive experiments conducted on UHRSD and widely-used SOD datasets demonstrate that our method outperforms state-of-the-art approaches in terms of performance.