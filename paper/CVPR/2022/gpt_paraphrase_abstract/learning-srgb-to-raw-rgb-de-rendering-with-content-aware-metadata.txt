The majority of camera images are saved in the standard RGB (sRGB) format, which is non-linear and not ideal for computer vision tasks that rely on a direct relationship between pixel values and scene radiance. Linear raw-RGB sensor images are preferred for such applications, but they are not commonly saved due to storage limitations and lack of support from imaging applications. To address this issue, various "raw reconstruction" methods have been proposed, which utilize metadata from the raw-RGB image to convert the sRGB image back to its original format when necessary. However, existing methods use simple sampling strategies and global mapping, resulting in suboptimal de-rendering outcomes. This study introduces a new approach that improves de-rendering results by jointly learning sampling and reconstruction. The experiments demonstrate that the learned sampling technique can adapt to image content and produce superior raw reconstructions compared to existing methods. Additionally, an online fine-tuning strategy for the reconstruction network is described to further enhance the results.