We present a new task called visual acoustic matching, which involves altering an audio clip to resemble it was recorded in a specific environment. By using an image of the target environment and a waveform of the source audio, our aim is to synthesize the audio in a way that matches the room acoustics indicated by the visual characteristics and materials. To tackle this task, we propose a cross-modal transformer model that incorporates audio-visual attention to incorporate visual properties into the audio and generate realistic audio output. Additionally, we introduce a self-supervised training objective that can learn acoustic matching from unmodified audio in web videos. Our experiments demonstrate that our approach effectively transforms human speech to various real-world environments shown in images, surpassing both traditional acoustic matching methods and more supervised approaches.