The high cost of data annotations for human-centric perception in vision and graphics makes it necessary to have a versatile pre-training model that can be used for data-efficient downstream tasks. In response to this need, we propose a framework called HCMoCo (Human-Centric Multi-Modal Contrastive Learning) that takes advantage of the multi-modal nature of human data, such as RGB, depth, and 2D keypoints, to learn effective representations. Our objective faces two main challenges: dense pre-training for multi-modality data and efficient utilization of sparse human priors. To address these challenges, we introduce two novel learning targets: Dense Intra-sample Contrastive Learning and Sparse Structure-aware Contrastive Learning. These targets enable us to hierarchically learn a modal-invariant latent space with continuous and ordinal feature distribution and structure-aware semantic consistency. HCMoCo allows for pre-training on different modalities by combining diverse datasets, enabling the efficient use of existing task-specific human data. Extensive experiments on four downstream tasks involving different modalities demonstrate the effectiveness of HCMoCo, particularly in data-efficient settings where it achieves a 7.16% and 12% improvement in DensePose Estimation and Human Parsing, respectively. Furthermore, we showcase the versatility of HCMoCo by exploring cross-modality supervision and missing-modality inference, highlighting its strong ability in cross-modal association and reasoning. The codes can be found at the provided GitHub link.