Despite recent advancements in stereo matching networks, their performance is limited when faced with domain shifts and struggles to generalize well to unseen domains. This is due to the lack of consideration given to maintaining feature consistency between matching pixels. In order to address this issue, we propose a straightforward approach of pixel-wise contrastive learning across viewpoints. Our stereo contrastive feature loss function places explicit constraints on the consistency between learned features of matching pixels, which are observations of the same 3D points. To further enhance the preservation of stereo feature consistency across domains, we introduce a stereo selective whitening loss that removes stereo viewpoint-specific style information. Surprisingly, by improving the generalization of feature consistency between two viewpoints within the same scene, we also improve the stereo matching performance in unseen domains. Our method is versatile and can be easily integrated into existing stereo networks without requiring access to samples from the target domain. Through training on synthetic data and applying it to four real-world testing sets, our method outperforms several state-of-the-art networks. The code for our method is available online.