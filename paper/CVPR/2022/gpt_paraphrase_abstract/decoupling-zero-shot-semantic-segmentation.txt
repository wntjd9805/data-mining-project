Zero-shot semantic segmentation (ZS3) is a technique used to segment new categories that have not been seen during training. Previous approaches to ZS3 have treated it as a pixel-level zero-shot classification problem, relying on language models pre-trained solely with text. However, this formulation has limitations in integrating vision-language models that are pre-trained with image-text pairs and have shown promise in vision tasks.Drawing inspiration from the fact that humans often perform segment-level semantic labeling, we propose a new approach to ZS3 that consists of two sub-tasks: 1) a class-agnostic grouping task that groups pixels into segments, and 2) a zero-shot classification task applied to the segments. The first task does not require category information and can directly transfer to group pixels for unseen classes. The second task operates at the segment level and provides a natural way to leverage large-scale vision-language models pre-trained with image-text pairs (such as CLIP) for ZS3.Based on this decoupling formulation, we introduce ZegFormer, a simple and effective model for zero-shot semantic segmentation. ZegFormer outperforms previous methods on standard ZS3 benchmarks by significant margins, achieving a 22-point improvement in mean intersection over union (mIoU) for unseen classes on the PAS-CAL VOC dataset and a 3-point improvement on the COCO-Stuff dataset. The code for ZegFormer will be made available at https://github.com/dingjiansw101/ZegFormer.