Knowledge distillation has been found to be effective in improving neural architecture search (NAS). Mutual knowledge distillation (MKD) is a method where models generate knowledge to train each other and has shown promising results in various applications. However, existing MKD methods lack scrutiny as they allow a worse-performing model to train a better-performing model, which can lead to collective failures. To address this issue, we propose a performance-aware MKD (PAMKD) approach for NAS. In PAMKD, knowledge generated by model A is only allowed to train model B if A performs better than B. We introduce a three-level optimization framework to implement PAMKD, consisting of three learning stages: 1) each model independently trains an initial model, 2) the initial models are evaluated on a validation set and better-performing models train worse-performing models using knowledge distillation, and 3) architectures are updated by minimizing a validation loss. Experimental results on various datasets demonstrate the effectiveness of our proposed method.