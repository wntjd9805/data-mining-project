Fashion image retrieval is a difficult task that requires models to analyze both visual and textual information simultaneously. In this study, we propose a new model called FashionVLP that utilizes prior knowledge from large image-text corpora to improve fashion image retrieval. FashionVLP combines visual information from various levels of context to effectively capture fashion-related details. We encode queries using transformer layers and employ a unique attention-based approach to fuse target image features without using text or transformer layers. Our extensive experiments demonstrate that FashionVLP achieves state-of-the-art performance on benchmark datasets, particularly showing a significant 23% relative improvement on the challenging FashionIQ dataset, which includes complex natural language feedback.