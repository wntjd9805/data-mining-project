Amodal completion is a visual task that is easy for humans but challenging for computer vision algorithms. The goal is to identify object boundaries that are occluded and not visible. This task is especially difficult for deep neural networks because of the lack of available and annotated data. To address this, we approach amodal segmentation as a problem of generalization beyond the task and distribution. Instead of using a fully connected classifier, we replace it with a Bayesian generative model that utilizes neural network features. This model is trained using non-occluded images with bounding box annotations and class labels only. However, it is applied to generalize to object segmentation and to handle occluded objects. We show that Bayesian models can naturally generalize beyond the training task labels by learning a prior that captures the object's background context and shape. Additionally, by incorporating an outlier process, Bayesian models can further generalize to segment partially occluded objects and predict their amodal object boundaries. Our algorithm surpasses alternative methods, including those using annotated amodal segmentations during training, particularly in scenarios with a high degree of occlusion. The code for our algorithm is publicly available at https://github.com/YihongSun/Bayesian-Amodal.