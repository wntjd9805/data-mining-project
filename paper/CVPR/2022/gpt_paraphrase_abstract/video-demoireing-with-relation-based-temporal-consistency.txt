Moiré patterns, which cause color distortions, significantly degrade the quality of images and videos captured with digital cameras when filming a screen. As the demand for video capture increases, we have conducted a study on removing these undesirable moiré patterns in videos, a process known as video demoiréing. In order to achieve this, we have created a unique dataset specifically for hand-held video demoiréing, ensuring that the captured data is spatially and temporally aligned. Additionally, we have developed a baseline video demoiréing model that utilizes implicit feature space alignment and selective feature aggregation to make use of complementary information from neighboring frames, thereby improving the demoiréing at the frame level. Importantly, we have introduced a relation-based temporal consistency loss that encourages the model to learn temporal consistency priors directly from ground-truth reference videos. This helps in producing predictions that are temporally consistent and effectively maintains the quality of each frame. Extensive experiments have demonstrated the superiority of our model. The code for our project is available at https://daipengwa.github.io/VDmoire_ProjectPage/.