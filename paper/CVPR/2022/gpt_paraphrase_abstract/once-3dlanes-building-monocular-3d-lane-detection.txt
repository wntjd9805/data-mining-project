We introduce ONCE-3DLanes, an authentic autonomous driving dataset that includes 3D lane layout annotations. Traditional 2D lane detection methods struggle to perform well in autonomous driving tasks due to uneven road surfaces. Therefore, accurately predicting the 3D layout of lanes is crucial for effective and safe driving. However, existing 3D lane detection datasets are either unpublished or generated in simulated environments, which limits the progress in this field. To address these limitations, we propose a dataset annotation pipeline that leverages the relationship between point clouds and image pixels to automatically generate high-quality 3D lane locations from 2D lane annotations in 211K road scenes. Additionally, we present a novel method called SALAD, which does not require extrinsic information or anchor points, to regress the 3D coordinates of lanes in image view without converting the feature map into bird's-eye view. To promote further research on 3D lane detection, we provide a benchmark dataset and introduce a new evaluation metric. Through extensive experiments, we compare existing approaches with our proposed method. Our aim is to revive interest in 3D lane detection in real-world scenarios and inspire innovations in both academia and industry.