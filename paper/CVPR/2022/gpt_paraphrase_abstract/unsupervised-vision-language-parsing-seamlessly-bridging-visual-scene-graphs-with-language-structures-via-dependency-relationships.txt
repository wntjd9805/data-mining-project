Understanding how visual scenes and language descriptions are connected is crucial for achieving a comprehensive understanding of visual information. Previous studies have focused on separately constructing hierarchical structures for visual scenes (such as scene graphs) and natural languages (such as dependency trees), but little research has been done on building a joint vision-language (VL) structure. In this study, we propose a new task that aims to induce a joint VL structure in an unsupervised manner, specifically targeting the seamless integration of visual scene graphs and linguistic dependency trees. To address the lack of VL structural data, we create a new dataset called VLParse. Instead of relying on manual labeling, we propose an automatic alignment procedure followed by human refinement to generate high-quality structures. Additionally, we evaluate our dataset using a contrastive learning-based framework called VLGAE (Vision-Language Graph Autoencoder). Our model achieves superior performance on two related tasks: language grammar induction and VL phrase grounding. Experimental results demonstrate the effectiveness of both visual cues and dependency relationships in constructing detailed VL structures.