This study introduces a novel approach for jointly training 3D object detection and tracking using monocular videos. The proposed method incorporates a spatial-temporal information flow module that combines geometric and appearance features to predict similarity scores for objects across frames. The module utilizes self-attention to aggregate spatial information within a frame and cross-attention to consider the relationships and affinities between objects in the temporal domain. These affinities are supervised to estimate object trajectories and guide the flow of information between corresponding 3D objects. A temporal-consistency loss is also introduced to ensure smooth 3D trajectories in the world coordinate system. The proposed method, named Time3D, outperforms existing competitors on the nuScenes 3D tracking benchmark, achieving 21.4% AMOTA and 13.6% AMOTP, while running at 38 FPS. Additionally, Time3D achieves 31.2% mAP and 39.4% NDS on the nuScenes 3D detection benchmark.