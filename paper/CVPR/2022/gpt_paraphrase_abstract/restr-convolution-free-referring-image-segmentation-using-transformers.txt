This study introduces a convolution-free model called ReSTR for referring image segmentation, a task that involves describing a target object in natural language rather than using predefined classes. Existing methods for this task heavily rely on convolutional neural networks, which struggle to capture long-range dependencies between entities in language expressions and lack flexibility in modeling interactions between different modalities. ReSTR addresses these issues by utilizing transformers to extract features from both visual and linguistic modalities, enabling the capture of long-range dependencies within each modality. Additionally, ReSTR employs a self-attention encoder to fuse the features of the two modalities, allowing for flexible and adaptive interactions. The fused features are then passed to a segmentation module that adapts to the specific image and language expression. Evaluation on public benchmarks demonstrates that ReSTR outperforms all previous models in referring image segmentation.