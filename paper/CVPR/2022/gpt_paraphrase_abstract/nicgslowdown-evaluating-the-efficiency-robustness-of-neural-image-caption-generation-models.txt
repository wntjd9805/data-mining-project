Neural image caption generation (NICG) models have gained significant attention for their strong performance in visual understanding. While previous studies have focused on improving the accuracy of these models, their efficiency has been largely overlooked. However, real-world applications often require real-time feedback, making the efficiency of NICG models crucial. Recent research has indicated that the efficiency of NICG models can vary depending on the input, opening up a new avenue for potential attacks. Adversaries could manipulate inputs to cause NICG models to consume more computational resources. To investigate this efficiency-oriented threat, we propose a new attack method called NICGSlowDown. This approach aims to assess the efficiency robustness of NICG models. Our experimental results demonstrate that NICGSlowDown can generate images with imperceptible changes that significantly increase the latency of NICG models, up to 483.86%. We hope that this research will raise awareness within the research community about the importance of ensuring the efficiency robustness of NICG models.