Annotating images with detailed labels for each pixel is a time-consuming and expensive process. However, a recent method called DatasetGAN has shown promise in generating large labeled datasets using a generative adversarial network (GAN). This approach involves using a small set of manually labeled, GAN-generated images. In this study, we expand the capabilities of DatasetGAN to the scale of ImageNet, which is a dataset with a wide range of classes. We use samples from the class-conditional generative model BigGAN, which is trained on ImageNet, and manually annotate only 5 images per class for all 1,000 classes. By incorporating an effective feature segmentation architecture on top of BigGAN, we transform it into a labeled dataset generator. Additionally, we demonstrate that VQGAN can also serve as a dataset generator by utilizing the existing annotated data. We create a new benchmark for ImageNet by labeling a separate set of real images and evaluate the performance of segmentation in various scenarios. Through a comprehensive analysis, we show significant improvements in utilizing a large generated dataset to train different supervised and self-supervised models on pixel-wise tasks. Furthermore, we show that using our synthesized datasets for pre-training yields better results compared to standard ImageNet pre-training on multiple downstream datasets and tasks such as detection, segmentation, and evaluation on PASCAL-VOC, MS-COCO, Cityscapes, and chest X-ray. We plan to make our benchmark public and maintain a leaderboard for this challenging task.