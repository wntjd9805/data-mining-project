We propose a framework called Slimmable Domain Adaptation, which aims to improve cross-domain generalization by adapting the model architecture to different devices with varying resource limitations. Our framework utilizes a weight-sharing model bank that allows for sampling models of different capacities to balance accuracy and efficiency. The main challenge is to boost the adaptation performance of multiple models in the bank, which we address by developing a Stochastic EnsEmble Distillation method to leverage the complementary knowledge within the bank. To address the optimization conflict between inter-model interaction and intra-model adaptation, we enhance the existing bi-classifier domain confusion architecture with an Optimization-Separated Tri-Classifier counterpart. Additionally, we propose an Unsupervised Performance Evaluation Metric to facilitate architecture adaptation. Our framework outperforms other approaches by a large margin on multiple benchmarks, even when the computing complexity is reduced to 1/64. The code for our framework is available at https://github.com/HIK-LAB/SlimDA.