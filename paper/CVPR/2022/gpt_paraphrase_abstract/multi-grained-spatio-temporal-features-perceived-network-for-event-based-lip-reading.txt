This study introduces a new type of sensing device called event cameras for automatic lip-reading (ALR). Event cameras offer advantages over conventional cameras for ALR due to their higher temporal resolution, reduced redundant visual information, and lower power consumption. To recognize words from event data, the researchers propose a Multi-grained Spatio-Temporal Features Perceived Network (MSTP). This network architecture consists of multiple branches operating at different frame rates to learn different grained spatio-temporal features. The low frame rate branch captures spatial complete but temporal coarse features, while the high frame rate branch captures spatial coarse but temporal refinement features. A message flow module is used to integrate these features from different branches, resulting in more discriminative spatio-temporal features. Furthermore, the researchers present the first event-based lip-reading dataset (DVS-Lip) captured by an event camera. Experimental results demonstrate that the proposed model outperforms state-of-the-art event-based action recognition models and video-based lip-reading models.