Attention-based neural networks like the Vision Transformer (ViT) have achieved impressive results in computer vision tasks. Understanding the scaling properties of these models is crucial for designing future generations effectively. While scaling laws for Transformer language models are known, the scalability of Vision Transformers is still unknown. In this study, we investigate the scaling behavior of ViT models and data by scaling them up and down. We analyze the relationship between error rate, data, and compute. Additionally, we improve the architecture and training of ViT, reducing memory usage and improving model accuracy. Our findings show that we are able to train a ViT model with two billion parameters, which achieves a new state-of-the-art performance of 90.45% top-1 accuracy on ImageNet. Furthermore, the model performs well in few-shot transfer learning, achieving 84.86% top-1 accuracy on ImageNet with only 10 examples per class.