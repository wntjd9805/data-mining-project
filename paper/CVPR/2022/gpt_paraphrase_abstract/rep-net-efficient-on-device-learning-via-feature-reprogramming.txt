Transfer learning is a crucial approach for on-device machine learning, particularly for IoT/edge devices that collect and process large amounts of data. However, the limited memory capacity of these devices presents challenges for memory-efficient learning. Existing methods focus on reducing trainable parameters, but the bottleneck lies in activations rather than parameters. In this study, we propose a novel approach to transfer learning by reprogramming the intermediate features of a pre-trained model. We introduce a tiny Reprogramming Network (Rep-Net) that is trained directly from the new task input data while keeping the backbone model frozen. The Rep-Net model exchanges features with the backbone model using an activation connector, benefiting both models. Through extensive experiments, we demonstrate that Rep-Net achieves highly memory-efficient on-device reprogramming, outperforming state-of-the-art transfer learning methods in terms of low training memory and high accuracy. The code for Rep-Net is available at https://github.com/ASU-ESIC-FAN-Lab/RepNet.