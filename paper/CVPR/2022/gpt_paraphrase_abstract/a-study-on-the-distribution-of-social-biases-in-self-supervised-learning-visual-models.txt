Deep neural networks are highly effective at learning data patterns, but they can be influenced by irrelevant factors present in the training data. These factors include operational biases and social biases, which can lead to discriminatory and unethical outcomes in tasks that impact human processes. It is commonly believed that self-supervised learning (SSL) is a bias-free solution because it does not require labeled data. However, recent research has shown that popular SSL methods also incorporate biases. In this study, we examine the biases in various SSL visual models trained on ImageNet data using a dataset and method designed by experts in psychology. We find a correlation between the type of SSL model and the number of biases it incorporates. Interestingly, this number is not solely dependent on the model's accuracy and varies within the network. We conclude that a careful selection process for SSL models can reduce the number of social biases while maintaining high performance. The code for this study is available at https://github.com/vpulab/SB-SSL.