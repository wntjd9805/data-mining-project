Recently, there have been promising developments in memory-based approaches for semi-supervised video object segmentation. These approaches involve predicting object masks for each frame using a memory of previous masks that is frequently updated. However, instead of this per-frame inference, we propose an alternative approach that treats video object segmentation as clip-wise mask propagation.  In our per-clip inference scheme, we update the memory at intervals and process a set of consecutive frames (referred to as a clip) between these memory updates. This approach offers two potential benefits: improved accuracy through clip-level optimization and increased efficiency through parallel computation of multiple frames.  To achieve this, we introduce a new method specifically designed for per-clip inference. Firstly, we incorporate a clip-wise operation to refine features based on intra-clip correlation. Additionally, we utilize a progressive matching mechanism to efficiently pass information within each clip. By combining these two modules and implementing a per-clip based training approach, our network achieves state-of-the-art performance on benchmark datasets such as Youtube-VOS 2018/2019 val and DAVIS 2016/2017 val.  Furthermore, our model demonstrates a favorable speed-accuracy trade-off by varying the memory update intervals, providing flexibility in different scenarios. This is illustrated in Figure 1, where we compare the accuracy and frames per second (FPS) of our model with other state-of-the-art methods under different memory update intervals.  In conclusion, our proposed method for per-clip inference in video object segmentation shows promising results in terms of accuracy, efficiency, and flexibility.