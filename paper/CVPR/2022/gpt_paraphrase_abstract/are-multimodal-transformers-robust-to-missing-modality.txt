Multimodal data collected in the real world often have missing modalities, making it important to develop robust multimodal models. Transformer models have been successful in processing multimodal data, but previous research has focused only on architecture designs or pre-training strategies, without investigating whether Transformers are naturally robust against missing modal data. This paper presents the first comprehensive investigation into how Transformers behave in the presence of missing modal data. The findings show that Transformer models are sensitive to missing modalities, and different fusion strategies significantly impact their robustness. Surprisingly, the optimal fusion strategy is dataset-dependent, meaning there is no universal strategy that works in all cases. Based on these findings, a method is proposed to improve the robustness of Transformer models by automatically searching for an optimal fusion strategy for input data. Experimental validations on three benchmarks demonstrate the superior performance of the proposed method.