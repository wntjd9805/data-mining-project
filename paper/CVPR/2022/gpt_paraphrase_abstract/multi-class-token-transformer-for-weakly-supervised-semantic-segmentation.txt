In this paper, we present a novel framework for weakly supervised semantic segmentation (WSSS) that utilizes a transformer-based approach to learn class-specific object localization maps. We were inspired by the observation that the attended regions of the one-class token in a standard vision transformer can be used to create a class-agnostic localization map. Our goal was to investigate whether the transformer model could also capture class-specific attention to achieve more discriminative object localization by learning multiple class tokens within the transformer.To address this, we propose a Multi-class Token Transformer (MCTformer) that incorporates multiple class tokens to facilitate interactions between the class tokens and the patch tokens. By leveraging the class-to-patch attentions corresponding to different class tokens, the MCTformer successfully generates class-discriminative object localization maps. Additionally, we introduce the use of patch-level pairwise affinity, obtained from the patch-to-patch transformer attention, to further refine the localization maps.Through our experiments, we demonstrate that our proposed framework significantly outperforms the Class Activation Mapping (CAM) method in WSSS. We conducted experiments on the PASCAL VOC and MS COCO datasets, and the results highlight the importance of the class token in WSSS.Overall, our paper introduces a new transformer-based framework, the MCTformer, which effectively learns class-specific object localization maps for WSSS. The inclusion of multiple class tokens and the integration of patch-level pairwise affinity contribute to the improved performance of our approach. Our results demonstrate the superiority of our method over existing techniques and emphasize the significance of the class token in WSSS.