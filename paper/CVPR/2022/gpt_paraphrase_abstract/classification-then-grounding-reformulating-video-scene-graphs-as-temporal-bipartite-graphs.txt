Currently, Video Scene Graph Generation (VidSGG) models rely on proposal-based methods, which involve generating subject-object pairs as proposals and then classifying predicates for each proposal. However, this approach has three inherent drawbacks. Firstly, the ground-truth predicate labels for proposals are only partially correct. Secondly, it fails to capture the high-order relations among different predicate instances of the same subject-object pair. Lastly, the performance of VidSGG is limited by the quality of the proposals generated.   To address these issues, we propose a new framework for VidSGG called the classiﬁcation-then-grounding framework. This framework overcomes the aforementioned drawbacks associated with proposal-based methods. Additionally, we reformulate video scene graphs as temporal bipartite graphs, where entities and predicates are represented as nodes with time slots, and the edges represent semantic roles between these nodes. This reformulation maximizes the potential of our new framework.   Furthermore, we introduce a novel model called BIpartite Graph based SGG (BIG) within this framework. BIG consists of a classiﬁcation stage, which aims to classify the categories of all nodes and edges, and a grounding stage, which attempts to localize the temporal location of each relation instance. We have conducted extensive ablations on two VidSGG datasets to demonstrate the effectiveness of our framework and BIG. The code for our proposed model is available at https://github.com/Dawn-LX/VidSGG-BIG.