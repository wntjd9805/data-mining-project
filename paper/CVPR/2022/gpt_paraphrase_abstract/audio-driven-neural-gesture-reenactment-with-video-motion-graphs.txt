We have developed a method for reenacting videos with gestures that correspond to a specific speech audio. Our approach involves splitting and reassembling clips from a reference video using a video motion graph to ensure smooth transitions. To seamlessly blend different clips, we have created a pose-aware video blending network that synthesizes frames between them. Additionally, we have implemented an audio-based gesture searching algorithm to determine the optimal order of reenacted frames. Our system generates reenactments that are consistent with both the audio rhythms and speech content. Through quantitative and qualitative evaluations, as well as user studies, we have shown that our method produces higher quality and more consistent videos compared to previous approaches. The code and data for our project can be found on our GitHub page.