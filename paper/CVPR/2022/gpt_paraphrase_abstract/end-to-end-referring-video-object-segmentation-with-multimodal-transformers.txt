The referring video object segmentation (RVOS) task involves segmenting a specified object in a video based on textual references. Existing approaches for RVOS rely on complex pipelines to handle the multimodal nature of the task. In this paper, we propose a simpler approach called Multimodal Tracking Transformer (MTTR) that treats RVOS as a sequence prediction problem. MTTR utilizes recent advancements in computer vision and natural language processing to process video and text together using a single multimodal Transformer model. It is trainable end-to-end, does not have any text-related biases, and does not require additional post-processing steps for mask refinement. Our evaluation on standard benchmarks demonstrates that MTTR outperforms previous methods in terms of multiple metrics. Specifically, it achieves significant mAP gains on A2D-Sentences and JHMDB-Sentences datasets while processing 76 frames per second. We also achieve strong results on the Refer-YouTube-VOS dataset, which is a more challenging RVOS dataset that has received less attention from researchers. The code to reproduce our experiments is available at https://github.com/mttr2021/MTTR.