This paper introduces a perception architecture called Uni-Perceiver, which aims to improve the efficiency and effectiveness of machine learning models. Unlike current task-specific approaches, Uni-Perceiver processes multiple modalities and tasks simultaneously, allowing for more efficient collaboration between tasks and reducing the costs of developing perception models for new tasks. Uni-Perceiver utilizes a modality-agnostic Transformer encoder and modality-specific tokenizers to encode different inputs and targets into a unified representation space. It treats different perception tasks as the same formulation, finding the maximum likelihood target for each input based on their representations. The model is pre-trained on various uni-modal and multi-modal tasks and evaluated on downstream tasks, including novel tasks not seen during pre-training. The results demonstrate that the pre-trained model achieves reasonable performance even on novel tasks without any tuning. By conducting prompt tuning on a small amount of downstream task data, the performance can be further improved to approach or surpass state-of-the-art methods. Full-data fine-tuning achieves results on par with or better than state-of-the-art approaches. The code and pre-trained weights will be made available.