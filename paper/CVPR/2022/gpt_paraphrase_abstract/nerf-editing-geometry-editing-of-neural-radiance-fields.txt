Current methods in implicit neural rendering, specifically Neural RadianceField (NeRF), have shown promise in synthesizing novel views of a scene. However, these methods lack the ability to allow users to deform shapes within the scene. Although previous works have proposed techniques to modify the radiance field based on user constraints, these modifications are limited to color editing or object translation and rotation. This paper introduces a method that enables users to perform controllable shape deformations on the implicit representation of a scene and generate novel view images without the need for network re-training. The approach establishes a correspondence between the explicit mesh representation and the implicit neural representation of the scene. Users can utilize established mesh-based deformation methods to deform the mesh representation of the scene. The proposed method then leverages user edits from the mesh representation to bend the camera rays using a tetrahedra mesh as a proxy, resulting in rendered images of the edited scene. Extensive experiments demonstrate the effectiveness of this framework, showcasing its ability to achieve desired editing results on both synthetic and real scenes captured by users.