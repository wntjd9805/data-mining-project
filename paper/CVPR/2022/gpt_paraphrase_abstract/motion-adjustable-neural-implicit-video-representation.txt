We propose a method to generate temporally varying content using a single image-based Implicit Neural Representation (INR) model. By displacing the input sinusoidal patterns over time, we incorporate a phase-varying positional encoding module and a phase-shift generation module into the conventional INR model. The model is trained end-to-end on a video to jointly determine the phase-shift values at each frame and map the phase-shifted sinusoidal functions to the corresponding frame. Our experiments show that this model can learn to interpret phase-varying positional embeddings into time-varying content and capture meaningful temporal and motion information from the video. Manipulating the phase-shift vectors enables non-trivial temporal and motion editing effects such as temporal interpolation, motion magnification, motion smoothing, and video loop detection.