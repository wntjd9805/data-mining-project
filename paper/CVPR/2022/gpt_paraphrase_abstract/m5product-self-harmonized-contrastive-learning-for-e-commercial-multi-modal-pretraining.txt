Despite the lack of large-scale modality-diverse datasets, multi-modal pre-training has the potential to learn highly discriminative feature representations from different data modalities. To address this issue, we introduce M5Product, a large-scale multi-modal dataset derived from E-commerce. The dataset includes five modalities (image, text, table, video, and audio) and covers a wide range of categories and attributes. It is 500 times larger than the currently available datasets with similar modalities, and it also features incomplete modality pairs, noise, and a long-tailed distribution, making it more representative of real-world problems.To effectively leverage the diverse modalities in M5Product, we propose a novel pretraining framework called Self-harmonized ContrAstive Learning (SCALE). SCALE integrates the different modalities into a unified model through an adaptive feature fusion mechanism. This mechanism learns the importance of each modality directly from the modality embeddings and influences the inter-modality contrastive learning and masked tasks within a multi-modal transformer model.To evaluate the performance of existing multi-modal pre-training approaches, we conduct extensive experiments on four downstream tasks using the M5Product dataset. Our results demonstrate the superiority of the SCALE model and provide valuable insights into the significance of dataset scale and diversity.The M5Product dataset and the codes used in our experiments are available at [website].