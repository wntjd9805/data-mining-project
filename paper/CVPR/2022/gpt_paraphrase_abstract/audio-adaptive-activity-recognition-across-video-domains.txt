This paper aims to address the challenge of activity recognition under domain shift, which occurs when there are changes in scenery or camera viewpoint. Existing approaches mitigate the shift in activity appearance through adversarial training and self-supervised learning. However, instead of solely focusing on visual cues, we propose leveraging activity sounds for domain adaptation. Activity sounds have less variability across domains and can provide reliable indications of activities that are not occurring. To achieve this, we introduce an audio-adaptive encoder and associated learning methods that adjust the visual feature representation and handle shifts in the semantic distribution. Additionally, we propose an audio-infused recognizer that incorporates domain-invariant activity sounds to eliminate domain-specific features and improve recognition. This recognizer effectively models cross-modal interaction across domains. Furthermore, we introduce a new task called actor shift along with an audio-visual dataset to evaluate our method's performance in situations where activity appearance changes dramatically. We conduct experiments on this dataset, as well as on EPIC-Kitchens and CharadesEgo, which demonstrate the effectiveness of our approach. More information about our project can be found on our project page: https://xiaobai1217.github.io/DomainAdaptation.