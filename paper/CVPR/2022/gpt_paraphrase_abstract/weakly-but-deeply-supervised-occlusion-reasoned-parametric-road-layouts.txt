We present a new network that can generate occlusion-reasoned layouts in perspective space and bird's-eye-view (BEV) space using just a single RGB image of a complex road scene as input. Unlike previous methods that rely on dense supervision like semantic labels in perspective view, our network only requires human annotations for parametric attributes, which are easier and less ambiguous to obtain. To tackle this challenging task, our network incorporates modules with inductive biases that learn occlusion-reasoning, geometric transformation, and semantic abstraction. Each module can be supervised by appropriately transforming the parametric annotations. By making careful design choices and implementing deep supervision, we are able to achieve meaningful representations and accurate predictions. We evaluate our approach on two widely used datasets, KITTI and NuScenes, and demonstrate that our method outperforms existing approaches while requiring considerably less human supervision.