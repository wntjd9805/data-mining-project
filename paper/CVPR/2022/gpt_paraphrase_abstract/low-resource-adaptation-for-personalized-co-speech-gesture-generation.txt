We introduce a method called DiffGAN to personalize co-speech gesture generation models using a small amount of data. Traditional methods require large amounts of data for each speaker, but this is often impractical. DiffGAN can efficiently personalize gesture generation models from a high-resource source speaker to a target speaker with just 2 minutes of target training data. DiffGAN is unique in its ability to account for the shift in crossmodal grounding and the distribution shift in the output domain. We validate the effectiveness of our approach using a large-scale publicly available dataset through quantitative, qualitative, and user studies. Our methodology significantly outperforms previous approaches for adapting gesture generation to low-resource scenarios. The code and videos can be found at https://chahuja.com/diffgan.