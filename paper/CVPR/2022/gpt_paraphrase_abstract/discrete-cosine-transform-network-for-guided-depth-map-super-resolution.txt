We propose a novel Discrete Cosine Transform Network (DCTNet) for guided depth super-resolution (GDSR) in multi-modal image processing. GDSR reconstructs high-resolution (HR) depth maps from low-resolution ones collected under suboptimal conditions, using HRRGB images of the same scene. Our approach addresses challenges in interpreting the working mechanism, extracting cross-modal features, and avoiding RGB texture over-transfer. Firstly, the DCT module in our network reconstructs multi-channel HR depth features by using DCT to solve the channel-wise optimization problem derived from the image domain. This helps improve the quality of the depth maps. Secondly, we introduce a semi-coupled feature extraction module that utilizes shared convolutional kernels to extract common information and private kernels to extract modality-specific information. This allows for better extraction of relevant features from the input images.Thirdly, we employ an edge attention mechanism to highlight the contours that are informative for guided upsampling. This helps enhance the details and sharpness of the reconstructed depth maps.We have conducted extensive quantitative and qualitative evaluations, which demonstrate the effectiveness of our DCTNet. Our approach outperforms previous state-of-the-art methods, while also having a relatively small number of parameters. The code for our DCTNet is publicly available at https://github.com/Zhaozixiang1228/GDSR-DCTNet.