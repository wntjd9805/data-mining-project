The effectiveness of Contrastive Language Image Pretraining (CLIP) encoders in various visual tasks has been established. This study explores the potential of CLIP visual backbones in Embodied AI tasks. Simple baselines called EmbCLIP are developed without task-specific architectures, inductive biases, auxiliary tasks, or depth maps. Surprisingly, these improved baselines perform exceptionally well across different tasks and simulators. EmbCLIP outperforms the RoboTHOR ObjectNav leaderboard by a significant margin of 20 pts and surpasses the iTHOR 1-Phase Rearrangement leaderboard, which uses Active Neural Mapping. It also outperforms the winners of the 2021 Habitat ObjectNav Challenge and the 2019 Habitat PointNav Challenge. Evaluations demonstrate that CLIP's visual representations effectively capture semantic information for navigation-heavy tasks compared to ImageNet-pretrained backbones. Additionally, one of the baselines is extended to create an agent capable of zero-shot object navigation, enabling navigation to objects not used as targets during training. The code and models used in this study are available at https://github.com/allenai/embodied-clip.