Contrastive language-image pretraining (CLIP) has achieved impressive results in image classification. However, when directly applied to object detection, it performs poorly due to a lack of fine-grained alignment between image regions and text. To address this, we propose RegionCLIP, which extends CLIP to learn region-level visual representations and aligns them with textual concepts. When applied to object detection, our method outperforms the state of the art on COCO and LVIS datasets. Additionally, our approach supports zero-shot inference for object detection. Our code is available at https://github.com/microsoft/RegionCLIP.