Despite the progress made in deep neural networks, there are still challenges in learning deep representations due to limited data availability, such as imbalanced data, unseen distributions, and domain shifts. Existing methods that address these challenges focus on exploring sample relationships either from the input or the loss function, but they fail to utilize the internal structure of deep neural networks for learning with sample relationships. To overcome this limitation, we propose a method that enables deep neural networks to learn sample relationships from each mini-batch. This is achieved through the introduction of a batch transformer module called BatchFormer, which is applied to the batch dimension of each mini-batch to implicitly explore sample relationships during training. By doing so, our method facilitates collaboration between different samples, allowing the learning of tail classes to benefit from the head-class samples in long-tailed recognition tasks. Additionally, to mitigate the discrepancy between training and testing, we share the classifier with or without the BatchFormer during training, making it possible to remove the BatchFormer during testing. We conduct extensive experiments on various datasets and demonstrate that our proposed method achieves significant improvements in different data scarcity scenarios, including long-tailed recognition, compositional zero-shot learning, domain generalization, and contrastive learning. The code for our method is publicly available at https://github.com/zhihou7/BatchFormer.