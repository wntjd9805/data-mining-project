This paper aims to address the problem of detecting the gaze target in a single image. Existing methods either focus on 2D visual cues or use depth information in a limited way. In this study, we propose a method that explicitly models 3D geometry, even when only 2D annotations are available. We first obtain 3D point clouds of the scene using estimated depth and reference objects. Then, we identify the front-most points in all possible 3D directions of the person in the image. These points are then incorporated into our ESCNet model, which consists of geometry and scene parsing modules. The geometry module produces a heatmap that indicates the probability of each front-most point being the gaze target based on the estimated 3D gaze direction. The scene parsing module further considers contextual cues in the scene to improve detection results. We evaluate our approach on two publicly available datasets, GazeFollow and VideoAttentionTarget, and demonstrate its superior performance compared to existing methods. In fact, our method outperforms human performance in terms of AUC on the GazeFollow dataset. The code for our method is available at the following GitHub repository: https://github.com/bjj9/ESCNet.