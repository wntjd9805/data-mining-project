Most current metrics for video captioning rely on comparing the text of reference captions to candidate captions. However, these metrics have limitations, such as their inability to handle videos without references and their potential for biased evaluation due to the one-to-many relationship between video and text and the disregard for visual relevance. From a human evaluator's perspective, a high-quality caption should be consistent with the video provided, even if it is not necessarily similar to the reference in literal or semantic terms. Taking inspiration from human evaluation, we propose a new metric called EMScore (Embedding Matching-based score) for video captioning. This metric directly measures the similarity between the video and candidate captions, without the need for references. We leverage the advancements in large-scale pre-training models to extract visual and linguistic embeddings for computing EMScore. The metric combines matching scores at both coarse-grained (video and caption) and fine-grained (frames and words) levels, taking into account the overall understanding and detailed characteristics of the video. Additionally, EMScore can be extended to cases where human-labeled references are available, considering the potential information gain. We evaluate existing metrics using the VATEX-EVAL and ActivityNet-FOIL datasets. The VATEX-EVAL experiments demonstrate that EMScore has higher human correlation and lower dependence on references. The ActivityNet-FOIL experiment shows that EMScore effectively identifies "hallucinating" captions. We provide the code and datasets for further exploration.