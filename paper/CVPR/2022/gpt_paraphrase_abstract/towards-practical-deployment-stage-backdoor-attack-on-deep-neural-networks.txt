The AI security community aims to produce and deploy deep learning models securely and reliably for real-world applications. While data poisoning based backdoor attacks on deep neural networks (DNNs) in the training stage and corresponding defenses have been extensively explored, backdoor attacks in the deployment stage have received less attention. These attacks, which are more threatening in real-world scenarios when they occur on unprofessional users' devices, have been overlooked due to the impracticality of existing attack algorithms and the lack of real-world demonstrations. In this study, we investigate the realistic threat of deployment-stage backdoor attacks on DNNs using the adversarial weight attack paradigm. To address practicality, we propose a gray-box and physically realizable attack algorithm called subnet replacement attack (SRA), which only requires knowledge of the victim model's architecture and supports physical triggers. Through extensive simulations and experimental demonstrations, we show the effectiveness and practicality of the proposed attack algorithm and highlight the risk of a new type of computer virus that can stealthily inject backdoors into user devices' DNN models. Our study emphasizes the need for increased attention to the vulnerability of DNNs in the deployment stage.