Outside-knowledge visual question answering (OK-VQA) involves comprehending an image, utilizing relevant knowledge from the web, and processing the information to provide an answer. Previous approaches fuse the image and question in a multi-modal space, limiting further fusion with external knowledge. This paper proposes an alternative approach for OK-VQA, where the image is transformed into text, facilitating knowledge retrieval and generative question-answering in natural language. This approach leverages large knowledge bases and pre-trained language models. A Transform-Retrieve-Generate framework (TRiG) is introduced, which can be used with various image-to-text models and textual knowledge bases. Experimental results demonstrate that the TRiG framework surpasses all existing supervised methods by at least 11.1% in performance. The answer format focuses on providing only the abstraction.