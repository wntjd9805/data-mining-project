Current methods in visual grounding are limited by a single-stage process that evaluates and ranks pre-defined regions. These methods rely on the density and quality of candidate regions and cannot continuously optimize the located regions. To overcome these limitations, we propose a new approach called multi-modal dynamic graph transformer (M-DGT) that progressively optimizes visual semantic alignment. M-DGT uses a dynamic graph structure with regions as nodes and their semantic relations as edges. By making sustainable adjustments to the graph based on multi-modal information and graph features, M-DGT efficiently shrinks the graph to approach the ground truth regions. Experimental results on the Flickr30k Entities and RefCOCO datasets demonstrate that M-DGT outperforms existing methods in terms of accuracy and Intersect over Union (IOU) scores, especially when initialized with an average of 48 boxes. Additionally, incorporating M-DGT to optimize predicted regions from existing methods further improves their performance. The source codes for M-DGT are available at https://github.com/iQua/M-DGT.