Recent advancements in positional encodings have allowed for the training of a single adversarial network capable of generating images at different scales. However, existing methods are limited to discrete scales or struggle to maintain high perceptual quality for scales that were not explicitly trained on. To address this, we propose the development of scale-consistent positional encodings that remain invariant to transformations in the layers of our generator. This enables the generation of images at arbitrary scales, even those unseen during training. Additionally, we introduce innovative inter-scale augmentations and partial generation training into our pipeline to enhance the synthesis of consistent images across a range of scales. Our experimental results demonstrate competitive performance across various commonly used datasets for image synthesis.