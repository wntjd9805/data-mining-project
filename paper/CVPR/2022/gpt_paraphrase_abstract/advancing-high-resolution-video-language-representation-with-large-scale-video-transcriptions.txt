We investigate the combination of video and language pre-training to improve cross-modality learning and enhance various video and language tasks. Existing approaches either extract poor quality video features or learn limited text embeddings, without considering the benefits of high-resolution videos and diverse semantics in cross-modality learning. To address this, we propose a new pre-training model called HD-VILA (High-resolution and Diversified VIdeo-LAnguage) that is designed for multiple visual tasks. We create a large dataset with two unique characteristics: 1) it includes 371.5k hours of high-resolution (720p) videos, and 2) it covers 15 popular YouTube categories, ensuring a diverse range of content. Our HD-VILA model optimizes the joint learning of video and language through a hybrid Transformer, which captures rich spatiotemporal features, and a multimodal Transformer, which encourages interactions between video features and diverse texts. Through our pre-training model, we achieve state-of-the-art results in 10 video and language understanding tasks, as well as 2 novel text-to-visual generation tasks. For instance, we outperform previous models by 40.4% R@1 in the zero-shot MSR-VTT text-to-video retrieval task and by 55.4% in the high-resolution dataset LSMDC. Additionally, our learned video and language embeddings produce visually appealing and semantically relevant results in text-to-visual editing and super-resolution tasks.