MiniViT is a compression framework designed to reduce the number of parameters in Vision Transformer (ViT) models while maintaining their performance. ViT models have gained popularity in computer vision, but their large parameter size limits their usability on memory-constrained devices. To address this issue, MiniViT proposes weight multiplexing, where the weights of consecutive transformer blocks are shared and transformed to increase diversity. Additionally, knowledge transfer through weight distillation over self-attention is used to leverage information from larger ViT models. Experimental results demonstrate that MiniViT can reduce the size of Swin-B transformer by 48% while achieving a 1.0% increase in Top-1 accuracy on ImageNet. Furthermore, MiniViT can compress DeiT-B by 9.7 times using a single layer of parameters, reducing it from 86M to 9M parameters without significantly compromising performance. The transferability of MiniViT is also verified through performance evaluation on downstream benchmarks. The code and models for MiniViT are available for reference.