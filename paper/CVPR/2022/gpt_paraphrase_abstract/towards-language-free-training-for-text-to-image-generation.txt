This paper addresses the challenge of training text-to-image generation models, which typically require a large number of high-quality image-text pairs. Captioning the associated text descriptions is time-consuming and costly. The authors propose a novel approach that eliminates the need for text data by utilizing the well-aligned multi-modal semantic space of the pre-trained CLIP model. They generate text features from image features, effectively alleviating the requirement of text-conditioning. Extensive experiments demonstrate the effectiveness of their method, achieving state-of-the-art results in text-to-image generation tasks. Importantly, their language-free model outperforms existing models trained with full image-text pairs. Additionally, their approach can be used to fine-tune pre-trained models, reducing training time and cost. The authors' pre-trained model achieves competitive results in zero-shot text-to-image generation on the MS-COCO dataset, despite having only 1% of the model size and training data size compared to the large DALL-E model.