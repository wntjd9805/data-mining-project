Convolution has become a crucial component in smart applications, but its static and dynamic variations are not suitable for layout-specific tasks like face recognition and medical image segmentation. These applications typically have high spatial variance within images and low variance across different images. To address this, we propose an efficient translation variant convolution (TVConv) that considers layout awareness in visual processing. TVConv consists of affinity maps and a weight-generating block, which gracefully depict pixel-paired relationships and allow for explicit over-parameterization to improve training without sacrificing efficiency in inference. TVConv can easily be integrated into various network architectures. In experiments on face recognition, TVConv reduces computational cost by up to 3.1× and improves throughput by 2.3× compared to depthwise convolution, while maintaining high accuracy. Additionally, TVConv achieves a mean accuracy improvement of up to 4.21% for the same computation cost. We also demonstrate improved generalization performance in optic disc/cup segmentation, addressing the issue of limited data availability. The code for TVConv is available at https://github.com/JierunChen/TVConv.