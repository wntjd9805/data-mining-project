We present a method for selecting objects in neural volumetric 3D representations, such as multi-plane images (MPI) and neural radiance fields (NeRF). Our method uses user scribbles in one view to automatically generate a 3D segmentation of the desired object, which can be rendered in different views. To achieve this, we propose a new voxel feature embedding that combines the neural volumetric 3D representation and multi-view image features from all input views. To evaluate our method, we create a dataset of human-provided segmentation masks for objects in real-world multi-view scene captures. Our results demonstrate that our method performs better than strong baselines, including 2D and 3D segmentation approaches tailored to our task.