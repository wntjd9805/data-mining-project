The effectiveness of compressing multitask models has not been thoroughly studied due to the complexity of task interdependence. This paper introduces DiSparse, a novel pruning and sparse training scheme for multitask models. DiSparse treats each task independently and makes unanimous decisions among all tasks during parameter pruning and selection. Experimental results show that DiSparse outperforms popular sparse training and pruning methods in various configurations. Additionally, DiSparse offers a valuable tool for multitask learning. Surprisingly, DiSparse achieves better performance than dedicated multitask learning methods in some cases, despite its high model sparsity. The pruning masks generated by DiSparse reveal similar network architectures for each task even before training begins. The existence of a "water-shed" layer, where task relatedness sharply decreases, suggests that continued parameter sharing provides no benefits. The code and models for DiSparse are available at the provided GitHub link.