Vision transformers (ViTs) have become increasingly popular due to their perceived higher modeling capacity and representation flexibility compared to traditional convolutional networks. However, their potential has not been fully realized in practice, as learned ViTs often suffer from over-smoothening, resulting in redundant models. Recent efforts have attempted to address this issue by regularizing embedding similarity or reintroducing convolution-like structures. However, a comprehensive assessment of the extent of redundancy in ViTs and the potential benefits of mitigating it has been lacking. This paper systematically investigates redundancy at three levels: patch embedding, attention map, and weight space. Based on this analysis, the paper proposes a principle of diversity for training ViTs, introducing corresponding regularizers that encourage representation diversity and coverage at each level. This approach enables the capture of more discriminative information. Extensive experiments on ImageNet using various ViT backbones validate the effectiveness of these proposals, significantly reducing ViT redundancy and improving model generalization. For instance, our diversified DeiT achieves accuracy boosts of 0.70% to 1.76% on ImageNet with highly reduced similarity. The code for our approach is available at https://github.com/VITA-Group/Diverse-ViT.