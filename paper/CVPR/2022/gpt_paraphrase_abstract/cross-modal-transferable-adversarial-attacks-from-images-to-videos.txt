Recent research has shown that adversarial examples created for one white-box model can also be used to attack other black-box models. This transferability of attacks across models raises concerns about the security of real-world deep neural networks (DNNs). However, existing studies have mainly focused on investigating transferability between models that have the same type of input data. The possibility of transferring adversarial perturbations across different modalities has not been explored. This paper aims to investigate the transferability of adversarial perturbations between different modalities, specifically between white-box image models and black-box video models. The authors observe that the low-level feature space of images and video frames is similar, which motivates them to propose a simple yet effective attack method called Image To Video (I2V) attack. This attack generates adversarial frames by minimizing the cosine similarity between the features of pre-trained image models for adversarial and benign examples. These adversarial frames are then combined to perform black-box attacks on video recognition models. Extensive experiments demonstrate that I2V achieves high success rates in attacking different black-box video recognition models. For example, on datasets such as Kinetics-400 and UCF-101, I2V achieves average attack success rates of 77.88% and 65.68%, respectively. These findings highlight the feasibility of cross-modal adversarial attacks.