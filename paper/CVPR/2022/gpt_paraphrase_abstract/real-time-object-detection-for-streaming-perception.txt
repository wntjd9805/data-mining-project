This paper proposes a new approach called streaming perception for autonomous driving, which focuses on evaluating the latency and accuracy of video online perception. Unlike previous works that trade off between accuracy and speed, this approach suggests that real-time models should be able to predict the future to handle this problem effectively. The authors introduce a Dual-Flow Perception module (DFP) that captures the moving trend and basic detection features for streaming prediction. They also incorporate a Trend-Aware Loss (TAL) with adaptive weights for objects with different moving speeds. The authors demonstrate the effectiveness of their method by achieving competitive performance on the Argoverse-HD dataset, improving the average precision (AP) by 4.9% compared to a strong baseline. The code for their method is available at https://github.com/yancie-yjr/StreamYOLO.