This paper introduces SimMIM, a straightforward framework for masked image modeling. It simplifies existing approaches by eliminating the need for specialized designs like block-wise masking and tokenization through discrete VAE or clustering. The authors conduct a systematic study of the components in their framework to understand what contributes to effective representation learning. They find that simple designs yield strong performance: 1) randomly masking the input image with a moderately large patch size (e.g., 32) serves as a powerful pretext task; 2) directly regressing RGB values of raw pixels performs as well as more complex patch classification approaches; 3) a lightweight linear layer for the prediction head achieves comparable performance to heavier ones. By employing ViT-B, the proposed approach achieves 83.8% top-1 fine-tuning accuracy on ImageNet-1K, outperforming previous methods by 0.6%. When applied to the larger SwinV2-H model with approximately 650 million parameters, it attains 87.1% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. The authors also utilize this approach to tackle the data scarcity issue in large-scale model training. They successfully train a 3B model (SwinV2-G) to achieve state-of-the-art accuracy on four vision benchmarks, using 40 times less labeled data than previous practice (JFT-3B). The code for SimMIM is accessible at https://github.com/microsoft/SimMIM.