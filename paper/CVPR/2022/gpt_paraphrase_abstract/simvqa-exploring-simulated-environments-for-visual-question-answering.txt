Current research on Visual Question Answering (VQA) has focused on data augmentation techniques to improve generalization. These techniques involve manipulating images or modifying questions and answers in the dataset. While these methods have shown good performance, they are limited by the diversity of available images. In this study, we propose using synthetic computer-generated data to overcome this limitation and provide more varied scenarios. By generating data using 3D and physics simulation platforms, we can fully control the visual and language space. This allows us to expand and replace type-specific questions and answers without the risk of exposing sensitive or personal data present in real images.We evaluate the effectiveness of leveraging synthetic data for real-world VQA tasks. Additionally, we introduce Feature Swapping (F-SWAP), a technique where object-level features are randomly switched during training to enhance a VQA model's domain invariance. Our experiments demonstrate that F-SWAP improves VQA models on real images without compromising their accuracy in answering existing questions in the dataset.Overall, our work offers a comprehensive analysis of using synthetic data for VQA and expands existing hyper-realistic datasets for this purpose. The proposed F-SWAP technique provides a practical solution to enhance VQA models' performance on real images while maintaining their ability to answer questions accurately.