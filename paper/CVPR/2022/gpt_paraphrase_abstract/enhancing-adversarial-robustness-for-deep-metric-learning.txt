Improving the adversarial robustness of deep metric learning models is crucial for ensuring security. Current defenses against adversarial vulnerability often avoid min-max adversarial training to prevent model collapse from difficult examples, but this approach is inefficient. In contrast, we propose a method called HardnessManipulation which efficiently perturbs training triplets to a specified level of hardness, using a harder benign triplet or a pseudo-hardness function. This method is flexible, encompassing both regular training and min-max adversarial training. We also introduce Gradual Adversary, a family of pseudo-hardness functions that gradually increase the specified hardness level during training to strike a better balance between performance and robustness. Additionally, we incorporate an Intra-Class Structure loss term to improve model robustness and efficiency by considering the relationship between benign and adversarial examples. Extensive experiments demonstrate that our proposed method significantly outperforms state-of-the-art defenses in terms of robustness, training efficiency, and performance on benign examples.