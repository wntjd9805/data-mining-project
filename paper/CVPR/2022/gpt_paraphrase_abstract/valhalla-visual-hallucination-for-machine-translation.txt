In recent years, there has been a growing interest in improving machine translation systems by incorporating auxiliary inputs such as images. Although these methods have shown promising results compared to traditional text-only translation systems, they have a limitation of requiring paired text and image inputs during inference, which restricts their applicability in real-world scenarios. This paper presents VALHALLA, a visual hallucination framework that addresses this limitation by utilizing only the source sentences during inference and generating hallucinated visual representations for multi-modal machine translation. The framework employs an autoregressive hallucination transformer to predict a discrete visual representation based on the input text, and then combines this with the text representation to generate the target translation. The hallucination transformer is trained jointly with the translation transformer using backpropagation and cross-entropy losses. Additionally, a consistency loss is employed to ensure coherence between predictions using either ground-truth or hallucinated visual representations. Extensive experiments conducted on three standard translation datasets with various language pairs demonstrate the effectiveness of the proposed approach compared to both text-only baselines and state-of-the-art methods. More information about the project can be found at http://www.svcl.ucsd.edu/projects/valhalla.