In recent years, there have been significant advancements in image inpainting techniques. However, recovering corrupted images with both vivid textures and reasonable structures remains a challenging task. Some existing methods focus only on regular textures and often fail to preserve the overall structure of the image due to the limited receptive fields of convolutional neural networks (CNNs). On the other hand, attention-based models can capture long-range dependencies and aid in structure recovery, but they suffer from heavy computational requirements, especially when dealing with large image sizes.  To overcome these challenges, we propose a novel approach that incorporates an additional structure restorer to facilitate the image inpainting process. Our model employs a powerful attention-based transformer model in a fixed low-resolution sketch space to restore holistic image structures. This grayscale space can be easily upsampled to larger scales, ensuring the correct transmission of structural information. The integration of our structure restorer with other pretrained inpainting models is efficient, thanks to zero-initialized residual addition.  Additionally, we employ a masking positional encoding strategy to enhance the performance of our model when dealing with large irregular masks. Extensive experiments conducted on various datasets demonstrate the efficacy of our approach compared to other competing methods. We have also made our codes publicly available at https://github.com/DQiaole/ZITS_inpainting.