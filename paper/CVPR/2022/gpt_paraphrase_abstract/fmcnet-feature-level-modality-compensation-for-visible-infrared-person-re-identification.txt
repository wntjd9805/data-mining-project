Existing models for Visible-Infrared person Re-IDentification (VI-ReID) aim to reduce the discrepancy between visible and infrared images by generating missing modality-specific information. However, these models often produce low-quality images with color inconsistencies, which negatively impact VI-ReID performance. In this paper, we propose a novel approach called the Feature-level Modality Compensation Network (FMCNet) for VI-ReID. Instead of compensating missing modality-specific information at the image level, FMCNet focuses on the feature level by directly generating missing modality-specific features from existing modality-shared features. This allows our model to generate discriminative person-related modality-specific features while discarding non-discriminative ones, thereby improving VI-ReID performance. Our approach consists of three modules: a single-modality feature decomposition module that decomposes single-modality features into modality-specific and modality-shared ones, a feature-level modality compensation module that generates missing modality-specific features from existing modality-shared ones, and a shared-specific feature fusion module that combines existing and generated features for VI-ReID. The effectiveness of our proposed model is demonstrated on two benchmark datasets.