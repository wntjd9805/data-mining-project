This paper introduces VDTTS, a Visually-Driven Text-to-Speech model that incorporates video frames as an additional input to generate speech that matches the video. Unlike traditional TTS models, VDTTS produces speech with natural prosodic variations, such as pauses and pitch, that are synchronized to the video. Experimental results show that our model produces well-synchronized outputs, approaching the video-speech synchronization quality of the ground-truth, even on challenging benchmarks like "in-the-wild" content from VoxCeleb2. Supplementary demo videos showcasing the model's video-speech synchronization, robustness to speaker ID swapping, and prosody are available on the project page.