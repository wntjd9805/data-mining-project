This paper presents a solution to the challenge of efficiently processing high-resolution visual data using Transformers, which are neural networks known for their ability to refer to global features. While Transformers offer flexibility, their attention module introduces computational overhead that hinders their application to high-resolution visual data. To address this issue, the authors propose a specialized token called MSG (messenger) for each region. These MSG tokens enable flexible exchange of visual information between regions and reduce computational complexity. The authors integrate the MSG token into a multi-scale architecture called MSG-Transformer. Experimental results demonstrate that MSG-Transformer achieves competitive performance in image classification and object detection tasks while accelerating inference on both GPU and CPU. The code for MSG-Transformer is available at https://github.com/hustvl/MSG-Transformer.