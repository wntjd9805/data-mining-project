The ability to perceive and understand visual scenes goes beyond simply recognizing objects; it also involves identifying the relationship between objects. These relationships can be represented as triples of subject, predicate, and object, forming a scene graph that provides valuable information for visual comprehension. However, generating dynamic scene graphs from videos is more challenging than static scene graph generation due to the varying visual relationships caused by object motion. To address this, we propose a new pre-training approach based on the Transformer model, which explicitly models the temporal correlation of visual relationships across different frames to enhance dynamic scene graph generation. In the pre-training phase, our model predicts the visual relationships of the current frame by leveraging intra-frame spatial information from a spatial encoder and inter-frame temporal correlations from a progressive temporal encoder. In the fine-tuning phase, we utilize the spatial and progressive temporal encoders along with the information from the current frame to predict the visual relationships. Our method achieves state-of-the-art performance on the Action Genome dataset, as demonstrated through extensive experiments.