Deep neural networks often make confident but incorrect predictions when tested with outlier data that is significantly different from their training data. Deep generative models (DGMs) have been considered as a potential metric for outlier detection with unlabeled data, but previous studies have shown that DGM likelihoods can be unreliable and easily biased by simple transformations to the input data. In this study, we focus on outlier detection using varia-tional autoencoders (VAEs), which are one of the simplest types of DGMs. We propose new analytical and algorithmic approaches to address the biases in VAE likelihoods. These bias corrections are specific to each sample, computationally efficient, and can be computed for various decoder visible distributions. Additionally, we discover that using a common image preprocessing technique called contrast stretching further improves the effectiveness of bias correction in outlier detection. Our approach achieves state-of-the-art accuracies with nine grayscale and natural image datasets, and outperforms four recent competing approaches in terms of both speed and performance. In conclusion, our study demonstrates that simple remedies are sufficient to achieve robust outlier detection with VAEs.