We demonstrate the negative impact of exponential behavior on classical LDA and logistic regression, resulting in overconfident decisions. To address this issue, we propose a solution based on polynomiality. This approach intentionally produces random-level performance in the tails, outside the main body of the training data. Additionally, we introduce a novel technique called softRmax, which serves as an alternative to the standard softmax function used in modern neural networks. softRmax is derived by connecting the standard softmax to Gaussian class-conditional models, similar to LDA, and replacing them with a polynomial alternative. We highlight two key characteristics of softRmax: conservativeness and inherent gradient regularization. These properties provide robustness against adversarial attacks without the need for gradient obfuscation.