Integration of diverse and complex data, such as multiomics, is becoming increasingly important. Current multimodal classification algorithms aim to improve performance by utilizing the complementarity of different data types. However, these approaches are generally inadequate in providing reliable fusion of multiple modalities, particularly in safety-critical applications like medical diagnosis. To address this issue, we propose a novel multimodal classification algorithm called Multimodal Dynamics. This algorithm evaluates the informativeness of both the features and modalities for different samples, ensuring trustworthy integration of multiple data types. Our approach incorporates a sparse gating mechanism to capture the variation of within-modality features and utilizes the true class probability to assess the classification confidence of each modality. Additionally, we introduce a transparent fusion algorithm based on dynamic informativeness estimation. To the best of our knowledge, our work is the first to simultaneously model feature and modality variation for different samples, resulting in reliable fusion in multimodal classification. We conduct extensive experiments on multimodal medical classification datasets, which clearly demonstrate the superior performance and trustworthiness of our algorithm compared to state-of-the-art methods.