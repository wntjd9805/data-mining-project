We investigate the automatic creation of navigation instructions using 360-degree images of indoor routes. Current systems struggle with accurately identifying visual landmarks, leading to reliance on language biases and the creation of false objects. Our system, MARKY-MT5, addresses this issue by focusing on visual landmarks. It consists of a landmark detector in the first stage and a multimodal, multilingual, multitask encoder-decoder generator in the second stage. To train MARKY-MT5, we incorporate grounded landmark annotations onto the Room-across-Room (RxR) dataset. By utilizing text parsers, weak supervision from RxR's pose traces, and a multilingual image-text encoder trained on 1.8 billion images, we identify 971,000 English, Hindi, and Telugu landmark descriptions and associate them with specific regions in panoramas. In Room-to-Room evaluations, human navigators achieve a success rate (SR) of 71% when following MARKY-MT5's instructions, which is just slightly lower than their 75% SR when following human instructions and significantly higher than SRs achieved with other generators. Evaluations on longer and more diverse paths in the RxR dataset yield SRs of 61-64% across three languages. Generating high-quality navigation instructions in new environments is a crucial step towards developing conversational navigation tools and enabling large-scale training of instruction-following agents.