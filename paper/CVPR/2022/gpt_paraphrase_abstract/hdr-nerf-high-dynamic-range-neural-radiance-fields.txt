We introduce HDR-NeRF, a method for recovering an HDR radiance field from a set of LDR views with different exposures. With HDR-NeRF, we can generate both new HDR and LDR views under various exposures. Our approach models the simplified physical imaging process, which involves two implicit functions: a radiance field and a tone mapper. The radiance field encodes scene radiance, providing density and radiance values for rays based on their origin and direction. The tone mapper predicts the pixel value of a ray hitting the camera sensor by considering the radiance and exposure time. We employ volume rendering to project the output radiance, colors, and densities into HDR and LDR images, using only the input LDR images as supervision. To evaluate our method, we created a forward-facing HDR dataset. Experimental results on both synthetic and real-world scenes demonstrate that our approach can accurately control exposures and render views with a high dynamic range.