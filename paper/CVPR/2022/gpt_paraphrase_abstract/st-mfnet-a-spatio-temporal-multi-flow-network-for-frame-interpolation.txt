Video frame interpolation (VFI) is an active area of research with applications in computer vision, post-production, and video encoding. However, existing approaches struggle to achieve reliable results in sequences with large motions, occlusions, or dynamic textures. To address this challenge, we propose a novel deep learning method called ST-MFNet, which is based on a Spatio-Temporal Multi-Flow architecture. ST-MFNet utilizes a multi-scale multi-flow predictor to estimate intermediate flows, which are combined with conventional optical flows to capture both large and complex motions. Additionally, a 3D convolutional neural network (CNN) is employed to model content dynamics over an extended temporal window, enhancing interpolation performance for various textures. To further improve the perceptual quality of interpolation, ST-MFNet is trained within an ST-GAN framework originally developed for texture synthesis. Our approach has been extensively evaluated and compared with fourteen state-of-the-art VFI algorithms. The results demonstrate that ST-MFNet consistently outperforms these benchmarks on diverse test datasets, achieving significant gains up to 1.09dB in PSNR for cases involving large motions and dynamic textures. The source code for ST-MFNet is available at https://github.com/danielism97/ST-MFNet.