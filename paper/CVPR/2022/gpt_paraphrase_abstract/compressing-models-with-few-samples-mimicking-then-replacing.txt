Few-sample compression is a technique to compress a large and redundant model into a smaller and more compact one using only a few samples. However, directly fine-tuning models with limited samples can lead to overfitting and minimal learning. Previous methods address this issue by optimizing the compressed model layer by layer to match the outputs of the corresponding layers in the teacher model. This approach is cumbersome. In this paper, we introduce a new framework called Mimicking then Replacing (MiR) for few-sample compression. MiR first ensures that the pruned model produces the same features as the teacher model in the second-to-last layer. Then, it replaces the teacher's layers before the second-to-last layer with a well-tuned compact version. Unlike previous methods that reconstruct the network layer by layer, MiR optimizes the entire network holistically. This approach is simple, effective, unsupervised, and general. MiR significantly outperforms previous methods. The code for MiR is available at https://github.com/cjnjuwhy/MiR.