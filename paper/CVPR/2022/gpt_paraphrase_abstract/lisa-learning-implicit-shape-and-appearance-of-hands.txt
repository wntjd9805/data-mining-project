This paper introduces a versatile neural model called LISA, which aims to accurately represent human hands. LISA is capable of capturing precise hand shape and appearance, generalizing to different hand subjects, providing dense surface correspondences, reconstructing from images taken in various environments, and being easily animated. To train LISA, the model is optimized by minimizing shape and appearance losses using a large dataset of multi-view RGB image sequences annotated with coarse 3D poses of hand skeletons. The model predicts the color and signed distance of each hand bone independently for a 3D point in local hand coordinates, and then combines the per-bone predictions using the predicted skinning weights. The design of LISA allows for disentangling of shape, color, and pose representations, enabling fine control over selected hand parameters. Through experiments, it is demonstrated that LISA can accurately reconstruct dynamic hands from both monocular and multi-view sequences, achieving higher quality results compared to existing approaches. The project page for LISA can be found at https://www.iri.upc.edu/people/ecorona/lisa/. This work was conducted during an internship with Reality Labs, Meta.