The task of Visual Geo-localization (VG) involves determining the location where a photo was taken by comparing it to a database of known location images. To examine the effectiveness of existing techniques in a real-world city-wide VG application, we created a new dataset called San Francisco eXtra Large. This dataset covers an entire city and presents various challenging scenarios, making it 30 times larger than the previous largest dataset for visual geo-localization. Our findings indicate that current methods struggle to handle such large datasets. To address this issue, we developed a highly scalable training technique called CosPlace. Unlike the commonly used contrastive learning that requires expensive mining, CosPlace frames the training as a classification problem. We achieved state-of-the-art performance across multiple datasets and discovered that CosPlace is resilient to significant domain changes. Additionally, our research demonstrates that CosPlace requires approximately 80% less GPU memory during training and achieves better results with descriptors that are eight times smaller than the previous state-of-the-art. These advancements open the door for city-wide real-world visual geo-localization. The dataset, code, and trained models can be accessed for research purposes at https://github.com/gmberton/CosPlace.