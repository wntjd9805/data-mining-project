Articulated objects play a significant role in human life, and a comprehensive understanding of their various aspects can greatly benefit multiple research communities. However, current solutions for understanding articulated objects rely on synthetic object datasets with CAD models that lack physical properties. This limitation hinders the ability to generalize from simulations to real-world applications in visual and robotics tasks. To address this gap, we introduce AKB-48, a large-scale Articulated object Knowledge Base consisting of 2,037 real-world 3D articulated object models across 48 categories. Each object in AKB-48 is described using a knowledge graph called ArtiKG. To construct AKB-48, we propose a fast articulation knowledge modeling (FArM) pipeline that can generate ArtiKG for an articulated object in just 10-15 minutes. This pipeline significantly reduces the cost of object modeling in the real world. Leveraging our dataset, we also present AKBNet, an integrated pipeline for Category-level Visual Articulation Manipulation (C-VAM) tasks. Within C-VAM, we benchmark three sub-tasks: pose estimation, object reconstruction, and manipulation. We have made the dataset, codes, and models publicly available at https://liuliu66.github.io/AKB-48.