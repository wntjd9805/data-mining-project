Event-based cameras have unique capabilities for tracking due to their high temporal resolution and dynamic range. These cameras capture events asynchronously, encoding valuable temporal and spatial information. However, effectively extracting this information from events remains a challenge. In this study, we introduce STNet, a spiking transformer network designed for single object tracking. STNet dynamically extracts and combines information from both temporal and spatial domains. It incorporates a transformer module for global spatial information and a spiking neural network (SNN) module for temporal cues. The SNN module's spiking threshold is adjusted based on statistical cues from the spatial information, enhancing the robustness of SNN features. We introduce a novel cross-domain attention fusion algorithm to dynamically fuse the features from both branches. Extensive experiments on three event-based datasets (FE240hz, EED, and VisEvent) demonstrate that STNet outperforms existing state-of-the-art methods in terms of tracking accuracy and speed. The code and pre-trained models for STNet are available at https://github.com/Jee-King/CVPR2022_STNet.