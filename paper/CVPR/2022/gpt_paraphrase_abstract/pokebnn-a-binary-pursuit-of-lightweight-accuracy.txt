The aim of this study is to improve the quality of binary neural networks (BNNs) while reducing computational intensity. Previous models of BNNs have suffered from low quality, so the researchers propose a binary convolution block called Poke-Conv to address this issue. Poke-Conv incorporates techniques such as adding multiple residual paths and tuning the activation function. The researchers apply Poke-Conv to ResNet-50 and optimize the initial convolutional layer that is difficult to binarize, resulting in a network family called PokeBNN1. These techniques are chosen to improve both the accuracy and the cost of the network.   To enable joint optimization of cost and accuracy, the researchers introduce a cost metric called arithmetic computation effort (ACE), which takes into account hardware and energy considerations for quantized and binarized networks. They also identify the need to optimize a hyper-parameter that controls the binarization gradient approximation.   The researchers achieve a new state-of-the-art (SOTA) in terms of top-1 accuracy, CPU64 cost, ACE cost, and network size metrics. The previous SOTA, ReActNet-Adam, achieved a top-1 accuracy of 70.5% with an ACE of 7.9. However, a variant of PokeBNN achieves the same top-1 accuracy with an ACE of only 2.6, reducing the cost by more than 3 times. A larger PokeBNN achieves a top-1 accuracy of 75.6% with an ACE of 7.8, improving the accuracy by more than 5% without increasing the cost. The implementation of PokeBNN in JAX/Flax and reproduction instructions are openly available.