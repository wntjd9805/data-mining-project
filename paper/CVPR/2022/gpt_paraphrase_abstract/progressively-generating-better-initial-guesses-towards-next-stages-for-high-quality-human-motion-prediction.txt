This paper introduces a novel method for accurately predicting future human poses based on observed ones. The method utilizes the concept of an "initial guess" to improve the accuracy of the predictions. The proposed framework consists of two stages: an init-prediction network that computes the initial guess, and a formal-prediction network that predicts the target future poses based on the guess. Additionally, a multi-stage prediction framework is designed, where each stage predicts the initial guess for the next stage, resulting in improved performance. To accomplish the prediction task at each stage, a network comprising Spatial DenseGraph Convolutional Networks (S-DGCN) and Temporal Dense Graph Convolutional Networks (T-DGCN) is proposed. This allows for the extraction of spatiotemporal features over the entire pose sequence. Through the combination of these design choices, the proposed method surpasses previous approaches by significant margins in terms of performance. Specifically, it achieves a performance improvement of 6%-7% on Human3.6M, 5%-10% on CMU-MoCap, and 13%-16% on 3DPW datasets. The code for this method is available at https://github.com/705062791/PGBIG.