This paper addresses the limitations of existing works in the field of human-centric image generation, specifically in the area of garment transfer. While progress has been made, current approaches fail to accurately align garments with people in real-world imagery and suffer from degradation in fine texture details. The focus of this research is on virtual try-on in real-world scenes, with a particular emphasis on loose garments, challenging poses, and cluttered backgrounds. The authors propose a novel generative network called wFlow, which combines the advantages of pixel flow and vertex flow techniques to improve the authenticity and naturalness of garment transfer in diverse contexts. Unlike previous methods that require paired images for training, the authors construct a large-scale video dataset called Dance50k and employ self-supervised cross-frame training and online cycle optimization to reduce the laboriousness of dataset creation. The proposed Dance50k dataset covers a wide range of garments under dancing poses, enhancing the applicability of virtual dressing in real-world scenarios. The experiments conducted demonstrate the superiority of wFlow in generating realistic garment transfer results for in-the-wild images, without the need for expensive paired datasets.