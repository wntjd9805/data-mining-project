This paper introduces BodyMap, a new framework that enables the acquisition of high-definition, continuous dense correspondence between in-the-wild images of clothed humans and a 3D template model's surface. This dense correspondence includes fine details like hands and hair, as well as regions that are distant from the body surface, such as loose clothing. Previous methods either divided the 3D body into parts and unwrapped them into a 2D UV space, resulting in discontinuities along part seams, or used a single surface to represent the entire body without handling body details. To address these limitations, the authors propose a novel network architecture with Vision Transformers that learn fine-level features on a continuous body surface. BodyMap surpasses previous approaches by a significant margin on various metrics and datasets, including DensePose-COCO. Additionally, the authors demonstrate several applications of BodyMap, including multi-layer dense cloth correspondence, neural rendering with novel-view synthesis, and appearance swapping.