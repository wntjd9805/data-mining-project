Effectively utilizing the abundant amount of ego-centric navigation data found on the internet can enhance the capabilities of intelligent systems to adapt to different perspectives, platforms, environments, scenarios, and locations. However, it is challenging to directly utilize this large and diverse unlabeled data for complex 3D reasoning and planning tasks. As a result, researchers have mainly focused on using this data for auxiliary pixel- and image-level computer vision tasks that do not consider the ultimate navigational objective.To address this issue, we present SelfD, a framework for learning scalable driving by leveraging a large amount of online monocular images. Our main approach is to use iterative semi-supervised training when learning imitative agents from unlabeled data. To handle various viewpoints, scenes, and camera parameters, we train a model based on images that directly learns to plan in the Bird's Eye View (BEV) space. We then enhance the decision-making knowledge and robustness of the initially trained model using unlabeled data through self-training. Specifically, we propose a pseudo-labeling step that maximizes the use of diverse demonstration data through planning-based data augmentation.We utilize a large dataset of publicly available YouTube videos to train SelfD and thoroughly analyze its benefits in challenging navigation scenarios. Without the need for additional data collection or annotation efforts, SelfD consistently improves driving performance evaluation on nuScenes, Argoverse, Waymo, and CARLA by up to 24%.