Modern GANs are highly effective at generating diverse and high-quality images. However, when these GANs are transferred to small target datasets, such as those with only 10 samples, the generator often ends up replicating the training samples. Various methods have been proposed to address this issue of few-shot image generation, but there is a lack of a unified framework to analyze them. In our first contribution, we introduce a framework to analyze these existing methods during the adaptation process. Our analysis reveals that some methods overly focus on preserving diversity, which hinders the improvement in image quality. However, all methods ultimately achieve similar quality after convergence. Therefore, the most effective methods are those that can slow down the degradation of diversity. Additionally, our analysis shows that there is still room for further improvement in this aspect. Building on this analysis, our second contribution proposes a method called Dual Contrastive Learning (DCL) to slow down the degradation of diversity in the target generator during adaptation. DCL achieves this by maximizing mutual information (MI) and retaining the rich multi-level diversity information from the source domain in the target domain generator. We use a contrastive loss (CL) to perform MI maximization, leveraging the generator and discriminator as feature encoders to extract different multi-level features for computing CL. Extensive experiments on various public datasets demonstrate that our proposed DCL method leads to a slower degradation of diversity during adaptation, while also producing visually pleasing images and achieving state-of-the-art quantitative performance.