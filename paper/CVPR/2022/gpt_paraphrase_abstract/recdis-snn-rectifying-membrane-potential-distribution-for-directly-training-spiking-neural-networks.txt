The Spiking Neural Network (SNN) is a brain-inspired and event-driven system that mimics the synaptic activity of biological neurons. It transmits binary spike signals between network units when the membrane potential exceeds the firing threshold. This bio-mimetic mechanism of the SNN is energy-efficient due to its power sparsity and asynchronous operations on spike events. However, the distribution of membrane potential can shift with the propagation of binary spikes, causing degeneration, saturation, and gradient mismatch problems. These shifts are detrimental to network optimization and convergence and limit the performance and depth of the SNN. To address these issues, we propose a novel distribution loss called MPD-Loss, which rectifies the membrane potential distribution by explicitly penalizing the undesired shifts without introducing additional operations in the inference phase. Additionally, our method also mitigates the quantization error in SNNs, which is often overlooked in other approaches. Experimental results demonstrate that our proposed method enables the training of deeper, larger, and better-performing SNNs within fewer timesteps.