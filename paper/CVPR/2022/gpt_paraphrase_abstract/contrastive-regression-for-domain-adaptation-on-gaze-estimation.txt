Appearance-based Gaze Estimation relies on deep neural networks to predict gaze direction from monocular images, but accurate annotation is costly and burdensome. This limitation leads to a domain gap, which affects the performance of trained models on new domains. To address this issue, we propose Contrastive Regression Gaze Adaptation (CRGA), an unsupervised approach for generalizing gaze estimation to target domains. CRGA uses the Contrastive Domain Generalization (CDG) module to learn a stable representation from the source domain and the Contrastive Self-training Adaptation (CSA) module to learn from pseudo labels in the target domain. Both CDG and CSA utilize the Contrastive Regression (CR) loss, a novel contrastive loss for regression that brings features with similar gaze directions closer together and pushes features with different gaze directions farther apart. We evaluate CRGA on MPIIGAZE, RT-GENE, Gaze-Capture, and EyeDiap datasets, using ETH-XGAZE and Gaze-360 as source domains. The experimental results demonstrate that CRGA achieves significant performance improvement compared to baseline models and outperforms state-of-the-art domain adaptation approaches in gaze adaptation tasks.