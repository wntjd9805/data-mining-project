This study introduces FENeRF, a novel 3D-aware generator for portrait image generation. Existing methods in this field can be categorized into 2D GANs and 3D-aware GANs. While 2DGANs can generate high-quality portraits, they lack view consistency. On the other hand, 3D-aware GAN methods maintain view consistency but their generated images cannot be locally edited. To address these limitations, FENeRF proposes a solution that can produce view-consistent and locally-editable portrait images. The method utilizes two separate latent codes to generate facial semantics and texture within a spatial-aligned 3D volume with shared geometry. By leveraging this underlying 3D representation, FENeRF is able to simultaneously render boundary-aligned images and semantic masks, enabling the use of the semantic mask for editing the 3D volume through GAN inversion. The study also demonstrates that the 3D representation can be learned from commonly available monocular image and semantic mask pairs. Additionally, the joint learning of semantics and texture is shown to enhance the generation of more detailed geometry. Experimental results reveal that FENeRF outperforms state-of-the-art methods in various face editing tasks. The code for FENeRF is publicly available at https://github.com/MrTornado24/FENeRF.