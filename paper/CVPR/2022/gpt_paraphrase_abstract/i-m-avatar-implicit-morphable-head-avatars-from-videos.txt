We present IMavatar, a new approach to learning implicit head avatars from monocular videos. Traditional 3D morphable face models (3DMMs) offer control over expression but lack geometric and appearance details. Neural volumetric representations are realistic but challenging to animate and generalize to unseen expressions. To address these limitations, our method combines the fine-grained control of 3DMMs with the photorealism of neural volumetric representations. We use learned blendshapes and skinning fields to represent expression- and pose-related deformations, which can be applied to the canonical geometry and texture fields. By employing ray marching and iterative root-finding, we locate the surface intersection for each pixel. A key contribution is our novel analytical gradient formulation, enabling end-to-end training of IMavatars from videos. Our method outperforms state-of-the-art approaches in terms of geometry improvement and coverage of expression space. The code and data for our method are available at https://ait.ethz.ch/projects/2022/IMavatar/.