Client-wise data heterogeneity in federated learning (FL) poses challenges to effective training. The distribution of data on each client can vary significantly, impacting the convergence rate of FL. Recent studies have proposed active client selection strategies, but they overlook loss correlations between clients and provide only minimal improvements compared to uniform selection. This study introduces FedCor, an FL framework that utilizes a correlation-based client selection strategy to enhance convergence. The framework models loss correlations between clients using a Gaussian Process (GP) and develops a client selection strategy that reduces expected global loss in each round. Additionally, an efficient GP training method is devised to minimize communication overhead in the FL scenario by leveraging covariance stationarity. Experimental results demonstrate that FedCor outperforms state-of-the-art methods, improving convergence rates by 34% to 99% on FMNIST and 26% to 51% on CIFAR-10.