We present a new approach and dataset for estimating the gaze of a person in 3D while they are moving freely at a distance, typically in surveillance scenarios. In these situations, the eyes are often occluded or have low resolution, making them difficult to analyze. Existing methods for gaze estimation struggle or resort to approximations based on head pose, relying on clear, close-up views of the eyes. Our novel approach takes advantage of the natural coordination between gaze, head, and body movements. We formulate gaze estimation as a Bayesian prediction, using temporal estimates of head and body orientations that can be reliably determined from a distance. We develop separate neural networks to model the likelihoods of head and body orientations, as well as the conditional prior of gaze direction based on these orientations. These networks are then cascaded to output the 3D gaze direction. Additionally, we create a comprehensive dataset of surveillance videos annotated with 3D gaze directions in various indoor and outdoor scenes. Through experiments on this dataset and others, we demonstrate the accuracy of our method and show that gaze can be accurately estimated from typical surveillance distances even when the person's face is not visible to the camera.