Generating programs for 2D and 3D shapes is crucial for tasks like reverse engineering and editing. However, training models for this task is challenging due to the lack of paired data (shape-program pairs) in many domains, making supervised learning impractical. To address this, we can compromise the accuracy of either the program labels or the shape distribution to obtain paired data.  One approach is using wake-sleep methods, where samples from a generative model of shape programs are used to approximate the distribution of real shapes. Another approach is self-training, where shapes are passed through a recognition model that predicts programs as "pseudo-labels" for the shapes. However, both of these methods have limitations.  In this study, we propose a novel self-training variant specifically for program inference. In this approach, program pseudo-labels are paired with the executed output shapes, avoiding label mismatch at the cost of an approximate shape distribution. We refer to these different approaches as Pseudo-Labels or an Approximate Distribution (PLAD), and we propose a conceptual framework to group them together.  We evaluate the effectiveness of these techniques in multiple 2D and 3D shape program inference domains. Our results show that PLAD techniques outperform policy gradient reinforcement learning in terms of inferring more accurate shape programs and converging faster. Additionally, we find that combining updates from different PLAD methods within a single model leads to better performance than using any individual technique.  In summary, inferring programs for 2D and 3D shapes is important but challenging due to the lack of paired data. We propose PLAD techniques, which involve compromising either program labels or shape distribution, and demonstrate their effectiveness in various domains. Combining different PLAD methods further improves performance.