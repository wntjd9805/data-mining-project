In the field of video super-resolution (VSR), the diversity and complexity of real-world degradations present significant challenges in both inference and training. Mild degradations can be improved through long-term propagation, but severe in-the-wild degradations may be exaggerated, resulting in lower output quality. To address this issue, we propose the use of an image pre-cleaning stage to reduce noise and artifacts before propagation. Our method, RealBasicVSR, surpasses existing methods in terms of both quality and efficiency.In addition, real-world VSR models are typically trained with diverse degradations to enhance generalizability. However, this requires larger batch sizes to ensure a stable gradient, which increases computational burden and leads to speed-performance tradeoffs. To mitigate this tradeoff, we introduce a stochastic degradation scheme that reduces training time by up to 40% without sacrificing performance. We also investigate different training settings and find that using longer sequences instead of larger batches during training allows for more effective utilization of temporal information, leading to more stable performance during inference.To enable fair comparisons among VSR methods, we introduce the VideoLQ dataset, which contains a wide range of real-world low-quality video sequences with diverse textures and patterns. This dataset can serve as a common benchmark for evaluating VSR algorithms.We have made our code, models, and dataset publicly available at https://github.com/ckkelvinchan/RealBasicVSR to facilitate reproducibility and further research in the field.