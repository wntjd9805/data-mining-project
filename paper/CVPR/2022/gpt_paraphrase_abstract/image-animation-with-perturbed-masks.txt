We present a new method for animating images using a video as a guide. Our approach does not require knowledge of the object's structure or pose models, making it suitable for animating any type of object. Both the video and the image are only used during testing. Our method utilizes a shared mask generator to separate the foreground object from the background and capture its overall pose and shape. To control the identity of the output frame, we introduce perturbations to remove unwanted identity information from the video's mask. A mask-refinement module then replaces the driver's identity with the source image's identity. Using the source image as a condition, a multi-scale generator decodes the transformed mask to create a realistic animated image where the content of the source frame is animated based on the driving video's pose. Since there is a lack of fully supervised data, we train our method on reconstructing frames from the same video as the source image. Our method outperforms state-of-the-art methods on various benchmarks. The code and samples can be found at the following link: https://github.com/itsyoavshalev/Image-Animation-with-Perturbed-Masks.