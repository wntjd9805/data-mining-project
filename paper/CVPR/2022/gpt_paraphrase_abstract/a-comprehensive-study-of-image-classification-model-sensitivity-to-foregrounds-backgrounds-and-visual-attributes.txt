In order to better understand how models make predictions, additional annotations are required for image classification datasets. In this study, we collect segmentation masks and informative attributes for a subset of ImageNet samples, creating a dataset called RIVAL10. This dataset consists of approximately 26k instances across 10 classes. Using RIVAL10, we assess the sensitivity of various models to noise corruptions in foregrounds, backgrounds, and attributes. We analyze different architectures (ResNets, Transformers) and training procedures (CLIP, SimCLR, DeiT, Adversarial Training). Surprisingly, adversarial training in ResNets increases sensitivity to the background compared to the foreground, while contrastive training leads to lower relative foreground sensitivity in both transformers and ResNets. Interestingly, transformers demonstrate adaptive abilities by increasing relative foreground sensitivity as corruption levels rise. By employing saliency methods, we identify spurious features that contribute to background sensitivity and evaluate the alignment of saliency maps with foregrounds. Additionally, we quantitatively examine the attribution problem for neural features by comparing feature saliency with the ground-truth localization of semantic attributes.