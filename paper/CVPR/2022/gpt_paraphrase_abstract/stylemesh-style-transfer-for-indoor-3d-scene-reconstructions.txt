We have developed a method for applying style transfer to 3D mesh reconstructions of indoor scenes in virtual reality (VR) applications. Style transfer is typically done on 2D images, so applying it to a mesh presents challenges. When stylization is optimized for different poses, the patterns become stretched and inconsistent in size. While there are model-based 3D style transfer methods that can stylize from a sparse set of images, they require a network during inference. To overcome these limitations, we have developed a method that optimizes an explicit texture for the reconstructed mesh and stylizes it using all available input images. Our approach takes into account depth and surface normal information to ensure a uniform and consistent stylization for the entire scene. Our experiments demonstrate that our method produces sharp and detailed results for the entire scene without any view-dependent artifacts. Through extensive analysis, we show that our 3D-aware approach enables style transfer to be applied to the 3D domain of a mesh. Additionally, our method can render a stylized mesh in real-time using traditional rendering pipelines.