Despite previous efforts to improve domain adaptation ability under unsupervised or few-shot semi-supervised settings, active learning has gained attention for its practicality in transferring models with limited annotation resources on target data. However, most active learning methods do not handle the domain gap between data distributions, and some active domain adaptation methods require complex query functions that are susceptible to overfitting. In this study, we propose a concise yet effective active domain adaptation method called Select-by-Distinctive-Margin (SDM). SDM utilizes a maximum margin loss and a margin sampling algorithm for data selection, similar to a Support Vector Machine. The method stores hard examples around decision boundaries and exploits them to find informative and transferable data. Additionally, we introduce two variants of SDM: one adjusts the gradient from the margin loss adaptively, and the other enhances the selectivity of margin sampling by considering the gradient direction. We evaluate SDM using standard active learning settings and demonstrate its competitive results and scalability. The code for our algorithm is available at https://github.com/TencentYoutuResearch/ActiveLearning-SDM.