Federated learning (FL) is a collaborative training method that focuses on data privacy by training models on decentralized clients. However, current FL methods have a drawback as they assume the object classes in the framework remain fixed over time. This leads to significant catastrophic forgetting, where the global model forgets old classes, especially in real-world scenarios where clients continuously collect new classes and have limited storage. Furthermore, the participation of new clients with unseen classes worsens the catastrophic forgetting. To overcome these challenges, we propose a novel approach called Global-Local Forgetting for class-Compensation (GLFC). This incremental model aims to alleviate catastrophic forgetting from both local and global perspectives. To address local forgetting caused by class imbalance at the client level, we introduce a class-aware gradient compensation loss and a class-semantic relation distillation loss. These losses help balance the forgetting of old classes and maintain consistent inter-class relations across tasks. To tackle global forgetting resulting from non-i.i.d class imbalance across clients, we introduce a proxy server that selects the best old global model to assist in local relation distillation. Additionally, we develop a prototype gradient-based communication mechanism to ensure privacy protection. Our proposed model outperforms existing methods by 4.4% to 15.1% in terms of average accuracy on benchmark datasets. The code for our model is available at https://github.com/conditionWang/FCIL.