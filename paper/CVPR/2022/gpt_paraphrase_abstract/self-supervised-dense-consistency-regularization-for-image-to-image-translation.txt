Unsupervised image-to-image translation has received significant attention due to recent advancements in generative adversarial networks (GANs). This study introduces a simple yet effective regularization technique to enhance GAN-based image-to-image translation. In order to generate images with realistic local semantics and structures, we suggest an auxiliary self-supervision loss that enforces consistency at the pixel level in the overlapping region between two patches extracted from a single real image during the discriminator training of a GAN. Our experiments demonstrate that this dense consistency regularization substantially improves performance across various image-to-image translation scenarios. Additionally, when combined with instance-level regularization methods, it further enhances performance. Moreover, we validate that our proposed model is capable of capturing domain-specific characteristics more effectively, even with a limited amount of training data.