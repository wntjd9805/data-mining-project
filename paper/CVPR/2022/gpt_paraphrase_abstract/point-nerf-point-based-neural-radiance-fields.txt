Volumetric neural rendering methods such as NeRF are known for producing high-quality view synthesis results, but their reconstruction time is often too long due to being optimized per-scene. In contrast, deep multi-view stereo methods can quickly reconstruct scene geometry using direct network inference. Point-NeRF combines the strengths of these two approaches by utilizing neural 3D point clouds with associated neural features to model a radiance field. This allows for efficient rendering by aggregating neural point features near scene surfaces in a ray marching-based rendering pipeline. Additionally, Point-NeRF can be initialized using direct inference from a pre-trained deep network, resulting in a neural point cloud that can be fine-tuned to achieve superior visual quality compared to NeRF, all while reducing training time by a factor of 30. Point-NeRF can also be integrated with other 3D reconstruction methods and effectively handles errors and outliers through a unique pruning and growing mechanism.