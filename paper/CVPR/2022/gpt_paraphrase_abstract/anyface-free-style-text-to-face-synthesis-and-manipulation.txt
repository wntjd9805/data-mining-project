This abstract discusses the limitations of existing text-to-image synthesis methods and introduces a new approach called AnyFace. These existing methods are restricted to words in the training dataset and cannot effectively describe the wide variability of human faces. AnyFace, on the other hand, allows for free-style text-to-face synthesis, enabling a broader range of applications such as metaverse, social media, cosmetics, and forensics.AnyFace utilizes a two-stream framework for face image synthesis and manipulation based on arbitrary descriptions of the human face. One stream focuses on text-to-face generation, while the other stream handles face image reconstruction. The CLIP encoders are employed to extract facial text and image features, and a collaborative CrossModal Distillation (CMD) module is designed to align linguistic and visual features across the two streams. Additionally, a Diverse Triplet Loss (DT loss) is developed to enhance fine-grained features and improve facial diversity.Extensive experiments conducted on Multi-modal CelebA-HQ and CelebAText-HQ datasets demonstrate the significant advantages of AnyFace over existing methods. AnyFace achieves high-quality, high-resolution, and diverse face synthesis and manipulation results without any limitations on the number or content of input captions.