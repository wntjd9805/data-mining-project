Advancements in speech-driven animation techniques have made it possible to create realistic animations for virtual characters using only audio data. However, existing approaches often neglect the animation of the inner mouth, focusing primarily on facial and lip motion. This paper aims to address the issue of speech-driven inner mouth animation.Capturing the motion of the tongue and jaw for performance capture data solely from video is challenging because the inner mouth is only partially visible during speech. To overcome this limitation, we have developed a comprehensive dataset that focuses specifically on capturing the motion of the tongue, jaw, and lips. This dataset enables researchers to employ data-driven techniques to generate authentic inner mouth animation based on speech input.Furthermore, we propose a deep-learning method that utilizes accurate and adaptable speech-to-tongue and jaw animation. We evaluate various encoder-decoder network architectures and audio feature encoders to determine the most effective approach. Our findings indicate that recent self-supervised deep learning-based audio feature encoders exhibit robustness, generalizability to unseen speakers and content, and optimal performance for our task.To showcase the practical application of our approach, we present animations using high-quality parametric 3D face models driven by the landmarks generated through our speech-to-tongue animation method. This demonstrates how our method can be implemented in real-world scenarios.In summary, this paper introduces a novel approach to speech-driven inner mouth animation by utilizing a specialized dataset and deep-learning techniques. The proposed method proves to be accurate, generalizable, and practical, providing realistic animations for virtual characters.