The abstract discusses a problem in computer vision which involves inferring a 3D scene representation from a small number of images, allowing for the rendering of new views at interactive speeds. Previous approaches have focused on reconstructing specific 3D representations or using implicit representations, but these methods often require precise camera poses and long processing times for each new scene. In this study, the authors propose a method called the Scene Representation Transformer (SRT) that can process RGB images with or without poses, infer a "set-latent scene representation," and generate new views in a single pass. They achieve this by extending the Vision Transformer to handle sets of images, enabling the integration of global information and 3D reasoning. An efficient decoder transformer is used to parameterize the light field and render novel views based on the scene representation. The learning process is supervised end-to-end by minimizing a reconstruction error for the new views. The authors demonstrate that the SRT method outperforms recent baselines in terms of peak signal-to-noise ratio (PSNR) and speed on synthetic datasets, including a new dataset created for the study. They also show that SRT can scale to support interactive visualization and semantic segmentation of real-world outdoor environments using Street View imagery. The abstract concludes by listing the contributions of each author and providing contact information for correspondence. A figure is included to provide an overview of the SRT model.