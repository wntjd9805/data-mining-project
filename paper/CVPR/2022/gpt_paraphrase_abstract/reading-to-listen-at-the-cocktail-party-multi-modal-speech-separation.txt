This paper aims to improve speech separation and enhancement in environments with multiple speakers and background noise by utilizing various modalities. Previous studies have achieved promising results by considering either temporal or static visual cues such as lip movements or facial identity. In this paper, we introduce a unified framework that combines synchronous or asynchronous cues for multi-modal speech separation and enhancement. Our contributions include: (i) the development of a modern Transformer-based architecture specifically designed to integrate different modalities for solving the speech separation task in the raw waveform domain; (ii) the proposal of conditioning on the textual content of a sentence alone or in conjunction with visual information; (iii) the demonstration of our model's resilience to audio-visual synchronization offsets; and (iv) the achievement of state-of-the-art performance on the widely-used benchmark datasets LRS2 and LRS3.