We propose a Language-Bridged Duplex Transfer (LBDT) module that addresses the limitations of previous methods in referring video object segmentation. These methods either rely on 3DConvNets or incorporate additional 2D ConvNets as encoders, resulting in spatial misalignment or false distractions. Our LBDT module utilizes language as a bridge to facilitate explicit and adaptive spatial-temporal interaction earlier in the encoding phase. It achieves this through cross-modal attention between the temporal encoder, referring words, and the spatial encoder, allowing for the aggregation and transfer of language-relevant motion and appearance information. Additionally, we introduce a Bi-lateral Channel Activation (BCA) module in the decoding phase to further denoise and highlight spatial-temporal consistent features via channel-wise activation. Through extensive experiments, our method demonstrates new state-of-the-art performances on four popular benchmarks, with significant absolute AP gains on A2D Sentences and J-HMDB Sentences. Furthermore, our approach consumes approximately 7 times less computational overhead compared to existing methods.