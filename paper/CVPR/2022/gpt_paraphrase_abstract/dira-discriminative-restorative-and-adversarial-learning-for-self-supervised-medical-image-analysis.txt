Discriminative learning, restorative learning, and adversarial learning have individually shown benefits in self-supervised learning schemes in computer vision and medical imaging. However, their combined effects on each other have been overlooked in previous studies. We propose a framework called DiRA that integrates these three learning approaches to collaboratively extract complementary visual information from unlabeled medical images for fine-grained semantic representation learning. Our experiments demonstrate that DiRA encourages collaborative learning and produces more generalizable representations across different organs, diseases, and modalities. It outperforms fully supervised ImageNet models, increases robustness in small data scenarios, reduces annotation costs in medical imaging applications, and enables accurate lesion localization with only image-level annotations. Additionally, DiRA enhances state-of-the-art restorative approaches, indicating its potential as a general mechanism for united representation learning. The code and pretrained models for DiRA are available at https://github.com/JLiangLab/DiRA.