Supervised techniques dominate the field of monocular 3D human pose estimation, but they often struggle with unfamiliar data. To address this, we propose an unsupervised domain adaptation approach for learning 3D human pose. Our method, MRP-Net1, consists of a deep network with two output heads: one for model-free joint localization and one for model-based parametric regression. This design allows us to measure prediction uncertainty at both the pose and joint level. By training only on labeled synthetic samples, our adaptation process aims to minimize uncertainty for unlabeled target images and maximize uncertainty for extreme out-of-distribution backgrounds. Additionally, our approach can handle occlusion and truncation scenarios in in-the-wild images through joint-uncertainty expansion. We evaluate our method extensively and achieve state-of-the-art performance on benchmark datasets.