Recent studies on 3D semantic segmentation have explored the combination of images and point clouds by processing each modality separately and projecting 2D features onto 3D points. However, merging large-scale point clouds and images presents challenges such as establishing a mapping between points and pixels and aggregating features from multiple viewpoints. Existing methods rely on mesh reconstruction or specialized sensors to handle occlusions and use heuristics to select and combine available images. In contrast, our approach proposes an end-to-end trainable multi-view aggregation model that leverages the viewing conditions of 3D points to merge features from images taken at arbitrary positions. Our method can integrate standard 2D and 3D networks and outperforms both 3D models operating on colorized point clouds and hybrid 2D/3D networks without the need for colorization, meshing, or true depth maps. We achieve state-of-the-art results for large-scale indoor/outdoor semantic segmentation on the S3DIS dataset (with a mean Intersection over Union (mIoU) of 74.7% in a 6-fold validation) and on the KITTI-360 dataset (with a mIoU of 58.3%). Our complete pipeline is publicly available at https://github.com/drprojects/DeepViewAgg and only requires raw 3D scans and a set of images and poses.