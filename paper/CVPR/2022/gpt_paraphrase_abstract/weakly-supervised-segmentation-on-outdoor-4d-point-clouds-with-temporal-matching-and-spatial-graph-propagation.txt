Current methods for segmenting point clouds require a significant amount of annotated data, particularly for outdoor scenes. However, manually annotating outdoor point clouds is time-consuming and expensive. This paper explores how scene understanding can be achieved with limited annotated data. The approach involves treating 100 consecutive frames as a sequence and annotating only 0.1% of the points in the first frame of each sequence, resulting in a total annotation budget of 0.001%. To generate high-quality pseudo labels from these limited annotations, a novel temporal-spatial framework for effective weakly supervised learning is proposed. The framework consists of two modules: a matching module to propagate pseudo labels across frames and a graph propagation module to propagate pseudo label information across the entire point clouds in each frame. Experimental results on SemanticKITTI and SemanticPOSS datasets demonstrate that the weakly supervised two-stage framework is comparable to fully supervised methods when trained with only 0.001% annotations. Additionally, when the framework is evaluated with 0.005% initial annotations on SemanticKITTI, it achieves results close to a fully supervised backbone model.