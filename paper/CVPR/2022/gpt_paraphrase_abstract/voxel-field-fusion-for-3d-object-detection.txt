We introduce a straightforward yet effective framework called voxel field fusion for cross-modality 3D object detection. Our approach focuses on maintaining consistency across modalities by representing and merging augmented image features as a ray in the voxel field. We accomplish this by designing a learnable sampler that extracts important features from the image plane and projects them onto the voxel grid in a point-to-ray manner, ensuring that the feature representation remains consistent with spatial context. Additionally, we employ ray-wise fusion to combine features with supplemental context within the voxel field. To address variations in features caused by different modalities, we develop a mixed augmentor that aligns feature-variant transformations, bridging the modality gap in data augmentation. Through extensive benchmarking, we demonstrate that our framework consistently outperforms previous fusion-based methods on the KITTI and nuScenes datasets. The code for our framework is publicly available at https://github.com/dvlab-research/VFF.