There is an increasing gap in computer vision between large-scale models that achieve top performance and models that are practical and affordable. This paper aims to bridge this gap by finding a reliable and effective method for making state-of-the-art large-scale models affordable in practice. Our approach focuses on using knowledge distillation as a powerful tool to reduce the size of large models without sacrificing their performance. We discover that certain design choices significantly impact the effectiveness of distillation, which were not previously discussed in the literature. Through a comprehensive empirical study, we provide evidence to support our findings and demonstrate impressive results on various vision datasets. Specifically, we achieve a state-of-the-art ResNet-50 model for ImageNet with 82.8% top-1 accuracy.