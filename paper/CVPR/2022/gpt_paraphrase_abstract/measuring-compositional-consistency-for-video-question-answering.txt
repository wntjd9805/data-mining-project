Recently, it has been observed that current advanced models struggle to answer complex questions that require compositional reasoning in video question answering benchmarks. However, it is still unclear which specific types of compositional reasoning pose challenges for these models. Additionally, it is difficult to determine whether models arrive at answers through compositional reasoning or by exploiting biases in the data.To address these issues, this paper introduces a question decomposition engine that systematically breaks down a compositional question into a directed acyclic graph of sub-questions. The structure of the graph ensures that each parent question is composed of its child questions. The authors present AGQA-Decomp, a benchmark dataset that includes 2.3 million question graphs, with an average of 11.49 sub-questions per graph, and a total of 4.55 million new sub-questions.Using these question graphs, the authors evaluate the performance of three state-of-the-art models by introducing a set of novel metrics for assessing compositional consistency. The findings reveal that the models struggle to reason correctly through most compositions, often relying on incorrect reasoning to arrive at answers. This leads to frequent contradictions or high accuracies despite failures in intermediate reasoning steps.Furthermore, it is important to note that the answer format in this study only provides the abstraction of the answer.