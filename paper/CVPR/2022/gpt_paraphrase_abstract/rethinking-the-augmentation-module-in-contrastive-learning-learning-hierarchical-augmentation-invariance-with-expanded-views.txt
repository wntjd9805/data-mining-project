A data augmentation module is commonly used in contrastive learning to generate two different views of a given data example. However, the predetermined selection of augmentation types has two drawbacks. Firstly, it imposes specific representational invariances on the model, which can have varying positive and negative effects on downstream tasks. Treating all augmentation types equally during training leads to suboptimal representations for different tasks and limits the flexibility to choose augmentation types beforehand. Secondly, the strong data augmentations used in traditional contrastive learning methods may introduce excessive invariance, causing the loss of fine-grained information crucial for certain downstream tasks.  This paper proposes a general approach to address these issues by considering "where" and "what" to contrast within a contrastive learning framework. Instead of uniformly learning representational invariances in the backbone, we suggest learning different augmentation invariances at different depths of the model based on the importance of each augmentation. Additionally, we propose expanding the contrast content with augmentation embeddings to mitigate the negative effects of strong data augmentations. Experimental results using various baseline methods demonstrate that our approach enables the learning of superior representations for classification, detection, and segmentation tasks.