Fully unsupervised 3D representation learning has become popular due to its benefits in data collection. One successful approach involves a viewpoint-aware technique that learns an image distribution using generative models like GANs, while also generating different view images using 3D-aware models such as NeRFs. However, this approach requires a large number of images with various views for training, making it challenging to apply to datasets with limited viewpoints. As an alternative, an aperture rendering GAN (AR-GAN) was proposed, which uses a defocus cue. However, AR-GAN is a CNN-based model that represents defocus independently from a viewpoint change, despite the high correlation between the two factors, which affects its performance. To address this issue, we propose an aperture rendering NeRF (AR-NeRF) that can integrate viewpoint and defocus cues in a unified manner by representing both factors within a common ray-tracing framework. Additionally, to learn defocus-aware and defocus-independent representations in a disentangled manner, we introduce aperture randomized training, where we learn to generate images by independently randomizing the aperture size and latent codes. We conducted experiments applying AR-NeRF to various natural image datasets, including flowers, birds, and faces, and the results demonstrate the effectiveness of AR-NeRF for unsupervised learning of depth and defocus effects.