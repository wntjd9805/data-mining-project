Unsupervised video representation learning has achieved impressive results in recent years. However, most current methods are focused on video classification and may not perform well for temporal localization tasks. To address this issue, we propose a self-supervised pretext task called PseudoAction Localization (PAL) to pretrain feature encoders for Temporal Action Localization tasks (UP-TAL). PAL involves randomly selecting temporal regions from one video and pasting them at different positions in two other videos. The goal is to align the features of these pasted pseudo action regions and maximize their agreement. PAL introduces a temporally dense and scale-aware contrastive learning paradigm, which better adapts to downstream TAL tasks. Extensive experiments demonstrate that PAL effectively improves the performance of existing TAL methods using large-scale unlabeled video data. Our codes and models will be publicly available at https://github.com/zhang-can/UP-TAL.