The CLIP model's learning objective for vision-language tasks is not effective in handling the noisy many-to-many correspondences in image captioning datasets gathered from the web. This inefficiency is due to both computational and data-related issues. To overcome this challenge, we propose a new training framework that utilizes cross-modal contrastive learning, progressive self-distillation, and soft image-text alignments. With this approach, our model can learn robust representations more efficiently from noisy data. Our model employs self-distillation to generate soft-alignment targets dynamically within each minibatch, using a subset of images and captions. These targets are then used to update the model's parameters. Through extensive evaluation on 14 benchmark datasets, we consistently outperform the CLIP model in various settings, including zero-shot classification, linear probe transfer, and image-text retrieval. Notably, our method achieves these improvements without incurring additional computational costs.Additionally, we conducted an analysis using an ImageNet-based robustness test-bed and found that our method exhibits better effective robustness compared to both ImageNet-trained models and the original CLIP model. Lastly, by pretraining with datasets of varying sizes spanning two orders of magnitude, we observed that our improvements over CLIP tend to scale with the number of training examples.