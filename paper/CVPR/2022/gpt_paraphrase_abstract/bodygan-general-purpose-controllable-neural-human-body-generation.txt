Recent advancements in generative adversarial networks (GANs) have offered potential solutions for creating realistic human images. However, existing methods struggle to allow explicit and individual control over multiple factors, such as poses, body shapes, and skin colors. This limitation arises from the reliance on a single pose/appearance model, which fails to disentangle various poses and appearances in human images. Moreover, this unimodal approach often leads to significant issues in the generated images, such as color distortion and unrealistic textures. To address these challenges, this research introduces a multi-factor conditioned method called BodyGAN. The proposed BodyGAN aims to capture the characteristics of the human body from different perspectives. Firstly, it adopts a pose encoding branch consisting of three hybrid subnetworks to generate representations based on semantic segmentation, 3D surface, and key points of the human body. Secondly, an appearance encoding branch is employed to obtain appearance information of the human body parts based on the segmentation results. Lastly, the outputs of these two branches are represented by user-editable condition maps, which are then processed by a generator to predict the synthesized image. Consequently, BodyGAN achieves a fine-grained disentanglement of pose, body shape, and appearance, enabling explicit and effective control of synthesis with diverse conditions. Extensive experiments conducted on multiple datasets and a comprehensive user study demonstrate that BodyGAN achieves state-of-the-art performance.