Dealing with label noise in Deep Neural Networks (DNNs) is a practical yet challenging task. Current methods focus on filtering low-confidence samples or mining information from them, but they fail to ensure robust generalization due to the neglect of useful information in noisy data. To address this, we propose a new method called LaCoL (Latent Contrastive Learning) that leverages negative correlations from noisy data. We filter samples using weakly-augmented data and apply classification loss on strongly-augmented selected samples to maintain training diversity. In metric space, we use weakly-supervised contrastive learning to uncover negative correlations in noisy data. Additionally, we introduce a cross-space similarity consistency regularization to minimize the gap between label space and metric space. Extensive experiments demonstrate the superiority of our approach compared to existing state-of-the-art methods.