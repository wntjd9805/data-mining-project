This research paper addresses the issue of text-based video segmentation, which involves segmenting a target object in a video based on a descriptive sentence. Previous work in this area has neglected the importance of incorporating motion information from optical flow maps along with appearance and linguistic modalities. To address this gap, the authors propose a method that fuses and aligns appearance, motion, and linguistic features to achieve accurate segmentation. They introduce a multi-modal video transformer that can fuse and aggregate multi-modal and temporal features between frames. Additionally, they develop a language-guided feature fusion module that progressively combines appearance and motion features at each feature level with guidance from linguistic features. To bridge the semantic gap between features from different modalities, the authors propose a multi-modal alignment loss. The effectiveness and generalization ability of the proposed method are demonstrated through extensive experiments on A2D Sentences and J-HMDB Sentences datasets, where it outperforms state-of-the-art methods.