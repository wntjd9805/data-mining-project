Monocular 3D object detection often lacks accurate depth recovery due to the ill-posed nature of 2D-3D projection. While deep neural networks (DNNs) can provide depth estimation from learned features, they often ignore pixel-level cues. To address this, we propose a new approach that combines the powerful feature representation of DNNs with pixel-level geometric constraints. We reformulate monocular object depth estimation as a progressive refinement problem and introduce a joint semantic and geometric cost volume to model depth errors. Initially, neural networks are used to learn object position, dimension, and normalized 3D coordinates. Using the estimated object depth, we reproject dense coordinates and corresponding object features onto the image space, constructing a cost volume that incorporates semantic and geometric errors. This cost volume is then input into a refinement network, where a combination of direct depth supervision and regularization techniques is used to improve the accuracy of the depth estimation. By effectively mitigating depth errors through our refinement framework, our approach achieves state-of-the-art results on both the KITTI and Waymo datasets.