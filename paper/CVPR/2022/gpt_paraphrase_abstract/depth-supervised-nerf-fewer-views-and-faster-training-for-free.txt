Neural RadianceField (NeRF) often fails to accurately fit geometries when given limited input views. This is because traditional volumetric rendering does not consider the fact that most of a scene is empty space and opaque surfaces. To address this issue, we propose DS-NeRF (Depth-supervised Neural RadianceFields), which utilizes depth supervision to learn radiance fields. We take advantage of the depth information obtained from structure-from-motion (SFM) algorithms used in NeRF pipelines. By incorporating depth uncertainty, we add a loss that encourages the terminating depth of a ray to match a given 3D keypoint. This "free" depth supervision improves the quality of rendered images with fewer training views and accelerates training by 2-3 times. Additionally, we demonstrate that our depth supervision loss is compatible with other NeRF methods, highlighting the cost-effectiveness and ease of use of depth as a supervisory signal. Furthermore, DS-NeRF can also be applied with other types of depth supervision, such as scanned depth sensors and RGB-D reconstruction outputs.