The growing interest in video-language research has led to the creation of large-scale datasets for machine learning. However, little attention has been given to evaluating the suitability of these datasets for the task of grounding video-language. Recent studies have uncovered significant limitations in these datasets, indicating that state-of-the-art techniques often overfit due to hidden biases in the data. To address this issue, we introduce MAD (Movie Audio Descriptions), a new benchmark that focuses on collecting and aligning audio descriptions from mainstream movies instead of augmenting existing video datasets with text annotations. MAD consists of over 384,000 natural language sentences grounded in more than 1,200 hours of videos. It significantly reduces the biases found in current video-language grounding datasets. MAD's unique collection approach presents a more challenging version of video-language grounding, where short temporal moments need to be accurately linked to longer videos lasting up to three hours. We have made MAD's data and baselines code available at https://github.com/Soldelli/MAD.