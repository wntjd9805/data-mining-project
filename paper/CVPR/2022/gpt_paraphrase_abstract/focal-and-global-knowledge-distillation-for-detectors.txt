Knowledge distillation has been successfully applied to image classification tasks, but it has proven to be challenging for object detection. This paper introduces the Focal and Global Distillation (FGD) method to address this issue. The authors highlight that in object detection, the features of the teacher (pre-trained model) and student (target model) exhibit significant variations in different areas, particularly in the foreground and background. To overcome this, FGD consists of two components: focal distillation and global distillation. Focal distillation separates the foreground and background, directing the student model to focus on the critical pixels and channels of the teacher model. On the other hand, global distillation rebuilds the relationship between different pixels and transfers this information from the teacher to the student model, compensating for any missing global information in focal distillation. Notably, FGD only requires the loss calculation on the feature map, making it applicable to various object detectors. The authors conducted experiments using different detectors with various backbones, and the results demonstrate that the student detector achieves significant mean average precision (mAP) improvement. For instance, when applied to RetinaNet, FasterRCNN, RepPoints, and Mask RCNN with a ResNet-50 backbone, FGD achieves mAP values of 40.7%, 42.0%, 42.0%, and 42.1% on the COCO2017 dataset, respectively. These results are notably higher (3.3, 3.6, 3.4, and 2.9 points) compared to the baseline models. The code for FGD is available at https://github.com/yzd-v/FGD.