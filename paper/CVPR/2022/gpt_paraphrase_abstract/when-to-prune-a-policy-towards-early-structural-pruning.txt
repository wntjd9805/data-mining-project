Pruning is a technique that reduces the memory usage and time complexity of neural networks. Traditionally, pruning has focused on optimizing inference but has overlooked the computational cost during training. Recently, there has been exploration of pruning at the initialization stage to reduce training costs, but this has led to a noticeable decrease in performance. In this study, we propose a method that combines the benefits of both post-training and pre-training pruning. Instead of pruning at initialization, our method utilizes dense training for a few epochs to guide the architecture and constantly evaluates the importance of neurons to identify dominant sub-networks. This allows us to push conventional pruning earlier into the training process. To achieve early pruning, we introduce an Early Pruning Indicator (EPI) that detects when the architecture of a sub-network stabilizes and triggers pruning. Through extensive experiments on ImageNet, we demonstrate that EPI enables quick identification of early training epochs suitable for pruning, offering the same efficacy as a computationally expensive grid-search approach. Our method improves top-1 accuracy by 1.4% compared to state-of-the-art pruning techniques and reduces GPU training cost by 2.4 times. This establishes a new efficiency-accuracy boundary for network pruning during training.