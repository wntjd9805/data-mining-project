Large-scale pre-training is crucial for computer vision tasks, but it is not efficient to pre-train all model architectures on large-scale datasets. This study explores an alternative strategy called Knowledge Distillation as Efficient Pre-training (KDEP) to transfer learned feature representation from pre-trained models to new student models for future tasks. Existing Knowledge Distillation methods are not suitable for pre-training because they distill logits that are discarded during transfer. To address this, the study proposes a feature-based KD method with non-parametric feature dimension alignment. The proposed method achieves comparable performance to supervised pre-training in multiple downstream tasks and datasets, while requiring significantly less data and pre-training time. The code is available at https://github.com/CVMI-Lab/KDEP.