Videos are typically recorded as discrete consecutive frames, but due to the high storage cost, they are often stored in low resolution and frame rate. Recent research has focused on Space-Time Video Super-Resolution (STVSR) to improve the quality of these videos. However, most existing methods only support a fixed up-sampling scale, limiting their flexibility and applications. To address this limitation, we propose a new approach called Video Implicit Neural Representation (VideoINR). Instead of using discrete representations, VideoINR uses learned implicit neural representations that can be decoded to videos of any spatial resolution and frame rate. We demonstrate that VideoINR achieves competitive performance compared to state-of-the-art STVSR methods on common up-sampling scales and outperforms previous works on continuous and out-of-training-distribution scales. More information and access to the code can be found on our project page (here) and GitHub repository (https://github.com/Picsart-AI-Research/VideoINR-Continuous-Space-Time-Super-Resolution).