Hyperbolic Neural Networks (HNNs) have the ability to embed hierarchies, which sets them apart from Euclidean Neural Networks (ENNs). By lifting Euclidean features into hyperbolic space, HNNs have shown superior performance in classifying datasets with known semantic hierarchies. However, HNNs struggle to perform as well as ENNs on standard benchmarks that lack clear hierarchies, limiting their practical applicability. This issue arises due to vanishing gradients during backpropagation in HNNs, caused by their hybrid architecture that connects Euclidean features to a hyperbolic classifier. To address this, we propose a simple solution of clipping the magnitude of Euclidean features during HNN training. Our experiments demonstrate that these clipped HNNs become super-hyperbolic classifiers. They consistently outperform HNNs on hierarchical data, and achieve comparable performance to ENNs on popular benchmarks such as MNIST, CIFAR10, CIFAR100, and ImageNet. Additionally, the clipped HNNs exhibit better adversarial robustness and out-of-distribution detection.