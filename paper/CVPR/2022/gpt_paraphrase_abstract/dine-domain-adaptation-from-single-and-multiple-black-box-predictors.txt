Unsupervised domain adaptation (UDA) is a technique used to transfer knowledge from labeled datasets to unlabeled datasets in order to alleviate the burden of labeling. However, existing methods often require access to the raw source data and use data-dependent alignment approaches, which can raise privacy concerns. Some recent studies have attempted to address this issue by utilizing well-trained white-box models, but this approach may still leak raw data. This paper introduces a practical and interesting approach to UDA, where only black-box source models (i.e., only network predictions) are available during adaptation in the target domain. The proposed method, called DIstill and fine-tuNE (DINE), consists of two steps. First, DINE distills knowledge from the source predictor to a customized target model, and then it fine-tunes the distilled model to better fit the target domain. DINE does not require neural networks to be identical across domains, making it suitable for adaptation on low-resource devices. Empirical results on three UDA scenarios demonstrate that DINE achieves highly competitive performance compared to state-of-the-art data-dependent approaches. The code for DINE is available at https://github.com/tim-learn/DINE/.