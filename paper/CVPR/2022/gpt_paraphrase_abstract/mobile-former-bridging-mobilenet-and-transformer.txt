We introduce Mobile-Former, a parallel design that combines the strengths of MobileNet and transformer models. The structure includes a two-way bridge that allows for the fusion of local and global features. Unlike previous vision transformer approaches, our Mobile-Former utilizes a transformer with very few tokens, reducing computational costs. Additionally, we propose a lightweight cross-attention mechanism to model the bridge, making Mobile-Former both efficient and powerful in representation. In ImageNet classification, Mobile-Former outperforms MobileNetV3 in the low FLOP range, achieving a 77.9% top-1 accuracy at 294M FLOPs, which is a 1.3% improvement with 17% fewer computations. In object detection with the RetinaNet framework, Mobile-Former achieves an 8.6 AP improvement compared to MobileNetV3. Furthermore, we replace the backbone, encoder, and decoder in DETR with Mobile-Former, resulting in an efficient end-to-end detector that outperforms DETR by 1.3 AP while reducing computational cost by 52% and parameter usage by 36%. The code for Mobile-Former is available at https://github.com/aaboys/mobileformer.