This study focuses on text-guided image manipulation in real-world scenarios, specifically on the entity level. Existing methods in this field are limited to modifying the appearance of images or editing objects in simple scenarios, which is not practical. The proposed framework, called ManiTrans, addresses three key requirements: editing entities according to text descriptions, preserving text-irrelevant regions, and seamlessly integrating manipulated entities into images. It utilizes a transformer-based approach with a two-stage image synthesis method. The framework includes a semantic alignment module to identify the regions for manipulation and a semantic loss to align the relationship between vision and language. Extensive experiments on real datasets validate the effectiveness of the proposed method in distinguishing relevant and irrelevant image regions.