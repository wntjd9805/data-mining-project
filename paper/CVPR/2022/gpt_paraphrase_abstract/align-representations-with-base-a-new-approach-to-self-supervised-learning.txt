Symmetric contrastive learning methods currently face issues such as collapses and quadratic complexity in objectives. To address these problems, a new approach called ARB (Align Representations with Base) is proposed. Unlike existing methods that maximize mutual information between two views, ARB introduces intermediate variables at the feature level and maximizes consistency between these variables and representations of each view. The intermediate variables are determined as the nearest group of base vectors to the representations. ARB offers several advantages compared to other symmetric approaches. Firstly, it does not require negative pairs, resulting in a linear complexity for the overall objective function. Secondly, it reduces feature redundancy and enhances the information density of training samples. Lastly, ARB demonstrates greater robustness to output dimension size, achieving over 28% higher Top-1 accuracy on ImageNet-100 compared to previous feature-wise approaches under low-dimension settings.