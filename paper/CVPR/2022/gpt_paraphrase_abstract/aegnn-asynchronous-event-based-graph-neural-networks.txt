The current top-performing learning algorithms for event cameras convert events into dense representations and process them using standard convolutional neural networks (CNNs). However, this approach discards the sparsity and high temporal resolution of events, resulting in high computational burden and latency. To address this issue, recent studies have utilized Graph Neural Networks (GNNs) to process events as sparse spatio-temporal graphs. Building upon this trend, we introduce Asynchronous, Event-based Graph Neural Networks (AEGNNs), a novel event-processing paradigm that extends GNNs to handle events as evolving spatio-temporal graphs. AEGNNs employ efficient update rules that minimize recomputation of network activations to only the nodes affected by each new event, thereby significantly reducing computation and latency for event-by-event processing. AEGNNs can be trained on synchronous inputs and converted to efficient asynchronous networks during testing. We extensively validate our method on object classification and detection tasks, demonstrating up to a 200-fold reduction in computational complexity (FLOPs) compared to state-of-the-art asynchronous methods, while achieving similar or even better performance. This reduction in computation directly translates to an 8-fold reduction in computational latency compared to standard GNNs, enabling low-latency event-based processing. For additional multimedia material, including videos and code, please visit our project page at https://uzh-rpg.github.io/aegnn/.