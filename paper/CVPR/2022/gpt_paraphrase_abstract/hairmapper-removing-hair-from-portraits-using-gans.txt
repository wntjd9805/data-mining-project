We propose a dataset and a baseline method for removing hair from portrait images using generative adversarial networks (GANs). Our approach involves training a fully connected network called HairMapper to identify the hair removal direction in the latent space of StyleGAN during the training stage. We introduce a new separation boundary and diffuse method to generate paired training data for both males and females. Our method demonstrates natural handling of portrait images with variations in gender and age. Through extensive experiments and user studies, we show that our method outperforms state-of-the-art techniques. We also showcase its applications in hair design and 3D face reconstruction.