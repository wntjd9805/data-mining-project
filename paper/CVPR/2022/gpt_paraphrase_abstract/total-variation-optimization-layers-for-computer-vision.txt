Optimization within deep-net layers has become a novel approach in layer design. However, applying these layers to computer vision tasks presents two main challenges: determining which optimization problem within a layer is beneficial, and ensuring efficient computation within the layer. To address the first challenge, we propose using total variation (TV) minimization as a layer for computer vision, inspired by its success in image processing. We hypothesize that TV as a layer can provide valuable inductive bias for deep-nets. We test this hypothesis on five computer vision tasks, including image classification, weakly supervised object localization, edge-preserving smoothing, edge detection, and image denoising, and achieve improved performance compared to existing baselines. To tackle the second challenge, we developed a GPU-based projected-Newton method, which is significantly faster (37Ã—) than current solutions.