Long-tailed learning poses a challenge as networks trained on the same settings yield remarkably different results, highlighting the uncertainty in this field. To address this issue, we propose Nested Collaborative Learning (NCL), a solution that involves the collaborative learning of multiple experts. NCL comprises two main components: Nested Individual Learning (NIL) and Nested Balanced Online Distillation (NBOD). NIL focuses on the supervised learning of each expert individually, while NBOD facilitates knowledge transfer among these experts. Both NIL and NBOD adopt a nested approach, considering not only all categories but also selectively targeting hard categories for more comprehensive representation learning. To identify these hard categories, we introduce the concept of Hard Category Mining (HCM), which involves selecting negative categories with high predicted scores. By nesting the learning process, NCL enables the network to capture both global and robust features as well as intricate distinguishing abilities. Additionally, we leverage self-supervision to further enhance the features. Extensive experiments demonstrate the superiority of our method, outperforming state-of-the-art approaches whether using a single model or an ensemble. The code for our method is available at https://github.com/Bazinga699/NCL.