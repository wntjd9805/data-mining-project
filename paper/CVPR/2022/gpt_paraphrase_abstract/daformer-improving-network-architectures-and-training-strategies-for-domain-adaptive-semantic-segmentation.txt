Training models for semantic segmentation on real-world images can be expensive due to the need for pixel-wise annotations. To address this issue, researchers have explored using synthetic data for training and then adapting the models to real images without annotations, a process known as unsupervised domain adaptation (UDA). However, most existing UDA methods rely on outdated network architectures. In this study, we evaluate different network architectures for UDA and demonstrate the potential of Transformers for semantic segmentation in UDA. Based on our findings, we propose a novel UDA method called DAFormer, which incorporates a Transformer encoder and a context-aware feature fusion decoder. DAFormer also incorporates three training strategies to improve stability and prevent overfitting to the source domain. These strategies include rare class sampling to mitigate confirmation bias, a Thing-Class ImageNet Feature Distance, and a learning rate warmup. Our experiments show that DAFormer outperforms existing methods, achieving a mean intersection over union (mIoU) improvement of 10.8 for the GTA→Cityscapes dataset and 5.4 for the Synthia→Cityscapes dataset. Additionally, DAFormer shows promising results in accurately segmenting challenging classes such as trains, buses, and trucks. The implementation of DAFormer is publicly available at the provided GitHub link.