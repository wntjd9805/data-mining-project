Data mixing, such as Mixup, Cutmix, and ResizeMix, plays a crucial role in enhancing recognition models. This study focuses on investigating the effectiveness of data mixing in the self-supervised context. Recognizing that mixed images derived from the same source images are inherently connected, we introduce SDMP (Simple DataMixing Prior) to capture this straightforward yet essential prior. SDMP positions these mixed images as additional positive pairs to facilitate self-supervised representation learning. Our experiments demonstrate that SDMP enables data mixing to improve the accuracy and out-of-distribution robustness of self-supervised learning frameworks like MoCo. Notably, SDMP is the first method to effectively enhance the performance of Vision Transformers in the self-supervised setting using data mixing. The code for SDMP is publicly available at https://github.com/OliverRensu/SDMP.