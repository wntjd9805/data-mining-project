Achieving better results with fewer labeling costs is still a difficult task. This paper introduces a novel active learning framework that incorporates contrastive learning into the recently proposed one-bit supervision. One-bit supervision refers to a simple query that asks whether the model's prediction is correct or not, and it is more efficient than previous active learning methods that require accurate labels for queried samples. The authors argue that this one-bit information aligns with the goal of contrastive loss, which aims to bring positive pairs closer and push negative samples apart. To achieve this goal, they develop an uncertainty metric to actively select samples for query. These samples are then divided into different branches based on the query results. A Yes query represents positive pairs of the queried category for contrastive pulling, while a No query represents hard negative pairs for contrastive repelling. Additionally, they propose a negative loss that penalizes negative samples that are far from the incorrectly predicted class, optimizing hard negatives for the corresponding category. The proposed method, called ObCP, offers a more effective active learning framework, and experiments on various benchmarks demonstrate its superiority.