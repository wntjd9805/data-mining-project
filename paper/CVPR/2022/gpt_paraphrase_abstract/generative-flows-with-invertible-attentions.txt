This paper introduces two types of invertible attention mechanisms for flow-based generative models. The mechanisms, called map-based and transformer-based attentions, are used to learn long-range data dependencies in both unconditional and conditional generative flows. A masked scheme is employed to ensure that the attention modules have tractable Jacobian determinants, allowing for seamless integration within the flow-based models. The proposed attention mechanisms improve the efficiency of generative flows and achieve favorable results compared to state-of-the-art models in multiple image synthesis tasks.