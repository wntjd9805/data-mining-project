This study introduces a transformer-based image inversion and editing model for pretrained StyleGAN. Unlike existing methods, this model enables reliable reconstruction and flexible editing simultaneously. It utilizes a CNN encoder to provide multi-scale image features as keys and values, and treats the style code for different generator layers as queries. Initially, the model initializes query tokens as learnable parameters and maps them into the W+ space. It then employs multi-stage alternate self-and cross-attention to update the queries for inverting the input by the generator. Additionally, the inverted code allows for reference- and label-based attribute editing through a pretrained latent classifier, resulting in high-quality image-to-image translation with flexibility. Extensive experiments demonstrate superior performance in both inversion and editing tasks within StyleGAN. The source code is available at https://github.com/sapphire497/style-transformer.