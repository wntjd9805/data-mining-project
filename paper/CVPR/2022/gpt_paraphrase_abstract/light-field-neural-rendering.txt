This abstract discusses the limitations of traditional light field rendering methods and introduces a new model that combines the strengths of both classical rendering and geometric reconstruction approaches. The proposed model operates on a four-dimensional representation of the light field, allowing for accurate representation of view-dependent effects. By enforcing geometric constraints during training and inference, the model is able to learn the scene geometry from a sparse set of views. The model utilizes a two-stage transformer-based approach to aggregate features and produce the color of a target ray. Experimental results show that the proposed model outperforms existing methods on various datasets, particularly on scenes with significant view-dependent variations. More information, including code and results, can be accessed at light-field-neural-rendering.github.io.