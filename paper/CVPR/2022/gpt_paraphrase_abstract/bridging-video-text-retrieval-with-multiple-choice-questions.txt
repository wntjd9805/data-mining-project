Recently, there has been a growing interest in pre-training models to learn transferable representations for video-text retrieval. Existing approaches either use separate encoders for efficiency or a joint encoder for better video-text interaction but sacrifice efficiency. In this study, we propose a novel method called Multiple Choice Questions (MCQ) that enables fine-grained video-text interactions while maintaining high efficiency. We train a parametric module called BridgeFormer to answer questions constructed from text features using video features. By leveraging the semantics of text, we create questions that allow the video encoder to capture more regional content and temporal dynamics. This establishes proper semantic associations between local video-text features. The BridgeFormer module can be removed for downstream retrieval, resulting in a flexible and efficient model with only two encoders. Our method outperforms state-of-the-art techniques on popular text-to-video retrieval tasks across multiple datasets, including large-scale datasets like HowTo100M. We also demonstrate superior performance in zero-shot action recognition, which can be framed as video-to-text retrieval. Additionally, our method achieves competitive results with shorter pre-training videos on single-modality downstream tasks such as action recognition with linear evaluation. The answer format of our method provides only the abstraction, making it a versatile approach for various retrieval tasks.