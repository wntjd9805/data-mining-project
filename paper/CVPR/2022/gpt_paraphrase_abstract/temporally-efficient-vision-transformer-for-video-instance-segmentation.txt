The Temporally Efficient Vision Transformer (TeViT) is a novel approach for video instance segmentation (VIS) that effectively models temporal information in video clips. Unlike previous transformer-based methods, TeViT is nearly convolution-free and consists of a transformer backbone and a query-based video instance segmentation head. The backbone stage introduces a parameter-free messenger shift mechanism for early fusion of temporal context. In the head stages, a parameter-shared spatiotemporal query interaction mechanism is proposed to establish a one-to-one correspondence between video instances and queries. This allows TeViT to fully leverage both frame-level and instance-level temporal context information, resulting in strong temporal modeling capability with minimal additional computational cost. TeViT achieves state-of-the-art results on three widely used VIS benchmarks (YouTube-VIS-2019, YouTube-VIS-2021, and OVIS) while maintaining high inference speed, such as 46.6 AP with 68.9 FPS on YouTube-VIS-2019. The code for TeViT is available at https://github.com/hustvl/TeViT.