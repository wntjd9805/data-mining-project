Previous methods for RGB-D-based motion recognition have achieved good results but still face challenges in optimization, information redundancy, and interaction between modalities. To address these issues, we propose a method that decouples and recouples spatiotemporal representation. This involves learning high-quality features independently in spatial and temporal dimensions, establishing stronger space-time dependency, and capturing cross-modal information using a Cross-modal Adaptive Posterior Fusion mechanism. Our approach outperforms state-of-the-art methods on four motion datasets, demonstrating the effectiveness of our design. The code for our method is available at the given GitHub link.