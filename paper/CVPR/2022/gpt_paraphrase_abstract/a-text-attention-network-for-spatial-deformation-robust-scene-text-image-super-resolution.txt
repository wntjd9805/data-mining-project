Scene text image super-resolution aims to enhance the resolution and legibility of text in low-resolution images. While deep convolutional neural networks (CNNs) have made significant progress in this area, they still struggle to reconstruct high-resolution images for text that is spatially deformed, such as rotated or curve-shaped text. This is because current CNN-based methods rely on locality-based operations that are ineffective in handling deformations. To address this issue, we propose a CNN-based Text ATTention network (TATT). Our approach involves extracting the semantics of the text using a text recognition module, which serves as text prior information. We also introduce a novel transformer-based module that utilizes a global attention mechanism to incorporate the semantic guidance of the text prior during the text reconstruction process. Furthermore, we introduce a text structure consistency loss to refine the visual appearance of the reconstructed text by enforcing structural consistency between regular and deformed texts. Experimental results on the TextZoom dataset demonstrate that TATT achieves state-of-the-art performance in terms of PSNR/SSIM metrics, while also significantly improving recognition accuracy in downstream text recognition tasks, especially for text instances with multiple orientations and curved shapes. The code for TATT is available at https://github.com/mjq11302010044/TATT.