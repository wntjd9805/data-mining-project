Cutting-edge models for vision and vision-and-language tasks currently rely on extensive pretraining that combines visual and linguistic information. However, these models typically focus on either cross-modal or multi-modal tasks, and often only specialize in specific modalities or tasks. To address this limitation, we propose the use of a single comprehensive model that encompasses all modalitiesâ€”a true vision and language foundation model. This model should excel in vision tasks, language tasks, and tasks that involve both vision and language. We introduce FLAVA, a model that embodies this concept, and demonstrate its remarkable performance across a diverse set of 35 tasks that encompass the aforementioned modalities.