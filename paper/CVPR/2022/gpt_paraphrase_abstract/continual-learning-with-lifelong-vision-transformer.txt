This study introduces a new framework called Lifelong VisionTransformer (LVT) that addresses the issue of catastrophic forgetting in continual learning. While existing methods focus on convolutional neural networks (CNNs), LVT utilizes the potential of powerful vision transformers. LVT incorporates an inter-task attention mechanism that retains information from previous tasks and slows down the shift of attention to the current task. It also implements a dual-classifier structure to prevent catastrophic interference and balance the integration of new and previous knowledge. Additionally, a confidence-aware memory update strategy is developed to enhance retention of previous tasks. Experimental results demonstrate that LVT outperforms other methods on continual learning benchmarks using fewer parameters.