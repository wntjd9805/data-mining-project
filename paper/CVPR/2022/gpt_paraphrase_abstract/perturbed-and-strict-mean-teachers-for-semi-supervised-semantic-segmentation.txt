Consistency learning in semi-supervised semantic segmentation has achieved impressive results by using perturbations in input images, features, or networks. However, inaccurate predictions of unlabelled training images can significantly impact this approach. There are two main consequences of these inaccurate predictions. Firstly, training based on the "strict" cross-entropy (CE) loss can easily overfit prediction mistakes, leading to confirmation bias. Secondly, applying perturbations to these inaccurate predictions can use potentially erroneous predictions as training signals, thereby degrading consistency learning.   This paper proposes novel extensions to the mean-teacher (MT) model to address the issue of prediction accuracy in consistency learning methods. The extensions include introducing a new auxiliary teacher and replacing MT's mean square error (MSE) with a stricter confidence-weighted cross-entropy (Conf-CE) loss. By accurately predicting labels, this model enables the use of challenging combinations of network, input data, and feature perturbations to enhance the generalization of consistency learning. Notably, the feature perturbations include a new adversarial perturbation.   The results obtained from public benchmarks demonstrate that our approach surpasses the previous state-of-the-art (SOTA) methods in the field. The code for our method is available at https://github.com/yyliu01/PS-MT.