This study focuses on improving the effectiveness of deep-learning based Sign Language Production (SLP) models. Current models produce limited and under-articulated skeleton pose sequences from a constrained vocabulary of signs. To address this limitation, the researchers propose a method called "learning to co-articulate between dictionary signs" which allows for the generation of smooth and realistic signing sequences for a wide range of topics. They introduce a novel Frame Selection Network (FS-NET) to improve the alignment of dictionary signs with continuous signing sequences. They also propose SIGNGAN, a pose-conditioned human synthesis model, to generate photo-realistic sign language videos directly from skeleton pose. The researchers develop a new keypoint-based loss function to enhance the quality of synthesized hand images. The SLP model is evaluated on a large-scale corpus called meineDGS (mDGS), and the results demonstrate that the FS-NET approach improves the co-articulation of interpolated dictionary signs. Furthermore, SIGNGAN outperforms other baseline methods in terms of quantitative metrics, human perceptual studies, and comprehension by native deaf signers.