This study addresses the issue of Model Inversion (MI) attacks on Split Federated Learning (SFL), a distributed training method where clients send intermediate activations instead of raw data to a central server. While this reduces computational load for clients, it also enables the server to reconstruct raw data from the activations. Existing approaches to protect SFL focus on inference and neglect attacks during training. To address this gap, the authors propose ResSFL, a MI-resistant Split Federated Learning Framework. ResSFL utilizes attacker-aware training to derive a resistant feature extractor, which is then used to initialize the client-side model before standard SFL training. This approach reduces the computational complexity of client-side adversarial training and the vulnerability to early-stage attacks. The proposed framework successfully mitigates MI attacks on a VGG-11 model trained on the CIFAR-100 dataset, achieving a high reconstruction Mean-Square-Error of 0.050 compared to the baseline system's 0.005. The framework achieves 67.5% accuracy with minimal computation overhead. The code for ResSFL is available at: https://github.com/zlijingtao/ResSFL.