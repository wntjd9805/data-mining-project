Recent advancements have shown that large-scale pre-training using contrastive image-text pairs can serve as a promising alternative for acquiring high-quality visual representation learning through natural language guidance. This new approach, benefiting from a wider range of supervision, demonstrates impressive adaptability to downstream classification tasks and datasets. However, the process of transferring the knowledge gained from image-text pairs to more intricate dense prediction tasks has received little attention. In this study, we introduce a novel framework for dense prediction that implicitly and explicitly utilizes the pre-trained knowledge from CLIP. Specifically, we convert the initial image-text matching problem in CLIP into a pixel-text matching problem and employ pixel-text score maps to guide the learning process of dense prediction models. Additionally, by incorporating contextual information from the image to prompt the language model, we enhance our model's ability to effectively exploit the pre-trained knowledge. Our approach is not limited to specific models or visual backbones, as it can be applied to any dense prediction systems and various pre-trained visual backbones, including both CLIP models and ImageNet pre-trained models. Through extensive experiments, we demonstrate the superior performance of our method in semantic segmentation, object detection, and instance segmentation tasks. The source code is available at https://github.com/raoyongming/DenseCLIP.