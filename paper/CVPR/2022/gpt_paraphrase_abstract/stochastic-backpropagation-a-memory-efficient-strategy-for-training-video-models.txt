We present a memory-efficient method called Stochastic Backpropagation (SBP) for training deep neural networks on videos. Our method takes advantage of the redundancy in video data to reduce the memory requirements during training. By randomly and independently removing backward paths for each network layer in each training step, SBP eliminates the need to cache activation values, resulting in significant GPU memory savings. The amount of backward paths to be dropped can be controlled using an adjustable keep-ratio. Experimental results demonstrate that SBP can be applied to various video tasks, achieving up to 80.0% GPU memory savings and 10% training speedup with less than 1% accuracy loss in action recognition and temporal action detection.