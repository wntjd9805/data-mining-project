Open-vocabulary instance segmentation is an important task that involves segmenting new classes without the need for mask annotations, which reduces the need for extensive human supervision. Previous approaches in this field typically pretrain a model on captioned images that cover a wide range of new classes and then fine-tune it on a smaller set of base classes with mask annotations. However, relying solely on the high-level textual information learned from caption pre-training is not sufficient for accurately encoding the detailed information required for pixel-wise segmentation.  To address this limitation, we propose a cross-modal pseudo-labeling framework. This framework generates training pseudo masks by aligning the semantic information of words in captions with the visual features of object masks in images. By leveraging the word semantics in captions, our framework is able to label novel classes and self-train a student model. To tackle the potential noise in pseudo masks, we introduce a robust student model that selectively distills mask knowledge by estimating the level of noise in the masks. This approach helps mitigate the adverse impact of noisy pseudo masks.  Through extensive experiments, we demonstrate the effectiveness of our framework. Compared to the state-of-the-art, our approach achieves a significant improvement in mean average precision (mAP) scores of 4.5% on the MS-COCO dataset and 5.1% on the large-scale Open Images & ConceptualCaptions datasets.  In summary, our proposed framework for open-vocabulary instance segmentation addresses the limitations of previous approaches by incorporating cross-modal pseudo-labeling and a robust student model. This results in improved segmentation accuracy, as demonstrated by our experiments on various datasets.