This study focuses on the integration of visual and auditory cues for object localization in 3D virtual environments. The researchers introduce a scenario where an object is dropped in a room, and a robot agent equipped with a camera and microphone must determine the object's identity and location by combining audio and visual signals with knowledge of physics. To facilitate this research, the researchers have created a large dataset called the Fallen Objects dataset, which consists of 8000 instances of 30 different object categories in 64 rooms. The dataset utilizes the ThreeDWorld Platform, which can simulate realistic physics-based impact sounds and interactions between objects. To address this challenge, the researchers develop baseline embodied agent models based on imitation learning, reinforcement learning, and modular planning, and conduct a thorough analysis of the task's difficulties. The dataset is publicly available for further research.