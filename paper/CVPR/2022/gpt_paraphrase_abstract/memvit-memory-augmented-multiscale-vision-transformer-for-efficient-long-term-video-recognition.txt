Current video recognition systems are effective at analyzing short clips or snapshots, but they struggle to connect and reason across longer periods of time. Most existing video architectures face computational and memory limitations, restricting their ability to process more than a few seconds of video. To overcome this challenge, we propose a novel approach in this paper. Instead of attempting to process a larger number of frames simultaneously like existing methods, we suggest processing videos in an online manner while caching memory at each iteration. By utilizing this memory, the model can access prior context for long-term modeling at a minimal cost. Our proposed solution, MeMViT (Memory-augmented Multiscale Vision Transformer), extends temporal support by a factor of 30 compared to existing models, with only a 4.5% increase in computation. In contrast, traditional methods require over 3,000% more compute to achieve similar results. MeMViT consistently delivers significant improvements in recognition accuracy across various scenarios. It achieves state-of-the-art performance on datasets such as AVA, EPIC-Kitchens-100 action classification, and action anticipation. We will make the code and models publicly accessible.