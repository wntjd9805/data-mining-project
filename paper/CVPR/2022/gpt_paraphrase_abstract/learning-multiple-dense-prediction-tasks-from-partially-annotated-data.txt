This paper introduces a method for efficiently learning multiple dense prediction tasks using partially annotated data. While most existing methods rely on expensive labelled datasets, our approach, called multi-task partially-supervised learning, leverages task relations to supervise the learning process when data is only partially annotated. We achieve this by mapping task pairs to a joint pairwise task-space, allowing for information sharing between tasks in a computationally efficient manner. This approach avoids learning trivial cross-task relations and retains high-level information about the input image. Through rigorous experimentation, we demonstrate that our method effectively exploits unlabelled tasks and outperforms existing semi-supervised learning approaches on three standard benchmarks.