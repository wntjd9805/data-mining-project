We investigate the issue of Vision-and-Language Navigation (VLN). Most existing methods for VLN train end-to-end using unstructured memory or cross-modal attention. However, we propose a novel approach that emphasizes the importance of explicit spatial representations in strengthening the association between language and vision. Our approach involves a cross-modal map learning model that predicts top-down semantics on an egocentric map for both observed and unobserved regions, and then determines a path towards the goal using a set of way-points. Language plays a crucial role in these predictions through cross-modal attention mechanisms. Through experimentation, we validate the hypothesis that language-based navigation can be successfully accomplished with the aid of a map, and demonstrate competitive performance on the VLN-CE benchmark.