The progress in understanding 3D objects has been limited by the use of shape datasets that are manually standardized with consistent position and orientation. This makes it difficult to apply these methods to shapes found in real-world scenarios, such as those from internet model collections or depth sensors. To address this, we propose ConDor, a self-supervised method that learns to standardize the orientation and position of 3D point clouds. ConDor is built upon Tensor Field Networks (TFNs), which are 3D networks that are permutation- and rotation-equivariant, as well as translation-invariant. During inference, ConDor can take an unseen 3D point cloud in any pose and output a standardized pose. During training, ConDor uses self-supervision to learn the standardized pose from a collection of non-standardized 3D point clouds. Additionally, ConDor has the capability to co-segment object parts consistently without any supervision. Our approach outperforms existing methods, as demonstrated by extensive quantitative results on four new metrics. Furthermore, ConDor enables new applications such as operating on depth images and transferring annotations.