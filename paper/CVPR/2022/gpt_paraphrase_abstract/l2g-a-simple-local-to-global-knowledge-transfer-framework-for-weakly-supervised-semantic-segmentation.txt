This paper introduces L2G, an online local-to-global knowledge transfer framework for mining high-quality object attention in weakly supervised semantic segmentation. The authors propose that using local patches instead of the entire input image allows classification models to detect object regions with more detail. The framework consists of two steps: first, a local classification network extracts attentions from multiple randomly cropped local patches of the input image. Then, a global network learns complementary attention knowledge by analyzing multiple local attention maps. By capturing rich object detail knowledge from a global perspective, the framework generates high-quality attention maps that can be used as pseudo annotations for semantic segmentation networks. Experimental results on PASCAL VOC 2012 and MS COCO 2014 datasets demonstrate that the proposed method achieves state-of-the-art performance, with mIoU scores of 72.1% and 44.2%, respectively. The code for the framework is available at https://github.com/PengtaoJiang/L2G.