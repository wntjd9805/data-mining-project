Spatial-Temporal Video Super-Resolution (ST-VSR) aims to generate high-resolution (HR) and high frame rate (HFR) videos. Previous methods have tackled this task using a two-stage approach, combining Spatial Video Super-Resolution (S-VSR) and Temporal Video Super-Resolution (T-VSR) independently, without considering their reciprocal relations. However, there are important connections between these two sub-tasks that have been overlooked. Firstly, T-VSR can benefit from S-VSR by utilizing the temporal correlations to enhance the spatial detail representation. Secondly, S-VSR can benefit from T-VSR by using the abundant spatial information to refine the temporal prediction.To address these issues, we propose a one-stage approach called Cycle-projected Mutual learning network (CycMu-Net) for ST-VSR. CycMu-Net leverages the mutual learning between S-VSR and T-VSR to fully exploit the spatial-temporal correlations. We achieve this by iteratively projecting the features up and down, allowing for comprehensive fusion and distillation of both spatial and temporal information. This mutual information helps in achieving high-quality video reconstruction.We conduct extensive experiments on benchmark datasets to evaluate the performance of CycMu-Net. Additionally, we compare it with separate S-VSR and T-VSR methods, demonstrating that our proposed approach outperforms state-of-the-art techniques. To facilitate further research, we have made the source code of CycMu-Net publicly available at: https://github.com/hhhhhumengshun/CycMuNet.