This paper introduces the MUFIN technique, which is designed for extreme classification (XC) tasks that involve millions of labels and utilize visual and textual descriptors for both data-points and labels. The paper demonstrates the applications of MUFIN in product-to-product recommendation and bid query prediction tasks involving millions of products. While existing multi-modal methods often rely solely on embedding-based approaches, XC methods utilize classifier architectures to achieve higher accuracies, but mainly focus on text-based categorization tasks. MUFIN aims to bridge this gap by redefining multi-modal categorization as an XC problem with millions of labels. This poses two main challenges: developing multi-modal architectures that can generate expressive embeddings for accurate categorization across millions of labels, and designing training and inference routines that scale logarithmically with the number of labels. To address these challenges, MUFIN proposes an architecture based on cross-modal attention and trains it in a modular manner using pre-training and positive and negative mining techniques. The paper also introduces new datasets, such as MM-AmazonTitles-300K, which contains over 300K products with titles and multiple images, and a dataset with over 4 million labels curated from Bing search engine click logs. Experimental results show that MUFIN achieves at least 3% higher accuracy compared to leading text-based, image-based, and multi-modal techniques. The paper includes a figure illustrating the need for accurate multi-modal retrieval in the product-to-product recommendation task, showcasing how MUFIN successfully retrieves visually similar and thematically related products.