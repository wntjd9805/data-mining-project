Deep network architectures face challenges in continually learning new tasks without forgetting previous tasks. One approach to address this issue is using dynamic architectures that expand the parameters, effectively reducing catastrophic forgetting. However, existing methods have limitations such as requiring a task identifier at test-time, complex parameter tuning, and limited information sharing across tasks. Consequently, these methods struggle to scale to a large number of tasks without significant overhead.To overcome these limitations, we propose a transformer architecture based on a dedicated encoder/decoder framework. Notably, the encoder and decoder are shared across all tasks. By dynamically expanding special tokens, we tailor each forward pass of the decoder network to a specific task distribution. Our approach can scale efficiently to a large number of tasks with minimal memory and time overhead, thanks to precise control over parameter expansion. Additionally, our strategy does not require any hyperparameter tuning to manage network expansion.Our model achieves excellent results on CIFAR100 and outperforms existing methods on the large-scale ImageNet100 and ImageNet1000 datasets while utilizing fewer parameters than comparable dynamic frameworks.