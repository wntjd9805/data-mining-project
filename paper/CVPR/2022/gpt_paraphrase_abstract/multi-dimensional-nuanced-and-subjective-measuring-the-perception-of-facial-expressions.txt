We have developed a methodology for collecting and modeling multidimensional modulated expression annotations from human annotators. Our research shows that different individuals perceive expressions in a face picture differently, leading us to design a model that can represent both intensity and ambiguity. Through empirical exploration, we have determined that six principal expression dimensions are sufficient to capture the perception of facial expressions. To validate our approach, we gathered multidimensional modulated expression annotations for 1,000 images from the popular ExpW in-the-wild dataset. Furthermore, we utilized these annotations to evaluate the performance of four publicly available algorithms for automated facial expression prediction. This demonstrates the effectiveness of our improved measurement technique.