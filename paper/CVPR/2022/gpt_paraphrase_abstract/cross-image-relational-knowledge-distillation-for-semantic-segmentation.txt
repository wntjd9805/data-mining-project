This study introduces a new approach called Cross-Image Relational Knowledge Distillation (CIRKD) for semantic segmentation. Current methods in knowledge distillation focus on guiding the student to mimic the teacher's structured information at the individual data sample level. However, they overlook the importance of global semantic relations among pixels across different images. CIRKD aims to transfer structured pixel-to-pixel and pixel-to-region relations between entire images. The underlying idea is that a well-structured feature space can be constructed by a good teacher network based on global pixel dependencies. By mimicking these structured semantic relations from the teacher, the student network improves its segmentation performance. The effectiveness of CIRKD is demonstrated through experiments on Cityscapes, CamVid, and PascalVOC datasets, outperforming state-of-the-art distillation methods. The source code for implementing CIRKD is available at https://github.com/winycg/CIRKD.