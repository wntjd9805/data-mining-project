The Sinkhorn operator has gained popularity in computer vision and related fields due to its easy integration into deep learning frameworks. To efficiently train neural networks, we propose an algorithm that obtains analytical gradients of a Sinkhorn layer through implicit differentiation. Unlike previous work, our framework is based on the most general formulation of the Sinkhorn operator, allowing for any type of loss function and differentiating both the target capacities and cost matrices jointly. We also provide error bounds for approximate inputs. Our algorithm improves the stability and accuracy of gradients compared to automatic differentiation and is more computationally efficient, especially when GPU memory is limited.