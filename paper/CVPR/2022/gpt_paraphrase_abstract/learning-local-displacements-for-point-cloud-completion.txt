We present a new method for completing objects and semantic scenes using a partial 3D point cloud scan. Our approach utilizes three unique layers within an encoder-decoder structure, designed specifically for this task. The first layer extracts features by matching point features to pre-trained local descriptors. To prevent the loss of individual descriptors during operations like max-pooling, we propose an alternative neighbor-pooling operation that selects feature vectors with the highest activations. In the decoder, we modify our feature extraction to increase the output dimension during up-sampling. While our model already achieves competitive results compared to the state of the art, we also introduce a second model that incorporates our layers into a transformer architecture to enhance the versatility of our approach for processing point clouds. We evaluate both architectures on object and indoor scene completion tasks and achieve state-of-the-art performance.