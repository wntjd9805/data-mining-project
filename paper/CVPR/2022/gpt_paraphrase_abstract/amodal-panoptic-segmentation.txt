Humans possess a remarkable ability to perceive objects as complete entities, even when certain parts are hidden from view. This capacity for amodal perception serves as the foundation for our understanding of the world around us. In order to equip robots with this capability, we introduce a new task called amodal panoptic segmentation. The objective of this task is to simultaneously predict the semantic segmentation labels for visible portions of "stuff" classes and the instance segmentation labels for both visible and occluded portions of "thing" classes at the pixel level. To promote research in this area, we enhance two existing benchmark datasets with pixel-level amodal panoptic segmentation labels, which we make publicly accessible as KITTI-360-APS and BDD100K-APS. We present several strong baseline methods, along with the amodal panoptic quality (APQ) and amodal parsing coverage (APC) metrics, to evaluate performance in a meaningful way. Additionally, we propose a novel network called APSNet for amodal panoptic segmentation, which explicitly models the intricate relationships between occluders and occludes. Through extensive experiments, we demonstrate that APSNet achieves state-of-the-art performance on both benchmarks, highlighting the practicality of amodal recognition. The datasets can be accessed at http://amodal-panoptic.cs.uni-freiburg.de.