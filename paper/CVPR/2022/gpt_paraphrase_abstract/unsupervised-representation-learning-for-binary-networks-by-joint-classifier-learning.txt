Self-supervised learning has been successful in unsupervised learning with large floating point networks. However, these networks are not easily deployable to edge devices. To address this issue and accelerate the deployment of models with unsupervised representation learning to resource-limited devices, we propose a self-supervised learning method for binary networks using a moving target network. Our approach involves training a randomly initialized classifier, connected to a pretrained floating point feature extractor, alongside a binary network. We also introduce a feature similarity loss, dynamic loss balancing, and modified multi-stage training to enhance accuracy. Our method, called BURN, is evaluated on five downstream tasks using seven datasets. The results demonstrate that BURN outperforms existing self-supervised baselines for binary networks and occasionally surpasses supervised pretraining. The code for our method is available at https://github.com/naver-ai/burn.