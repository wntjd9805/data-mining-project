Backdoor attacks aim to manipulate a subject model by adding a trigger to inputs. These triggers can be injected through malicious training or naturally occur. Finding the backdoor trigger is crucial for both attacking and defending against such attacks. Currently, the most popular method for trigger inversion is optimization. However, existing methods focus on finding the smallest trigger that can flip a set of input samples uniformly by minimizing a mask. The mask determines which pixels need to be changed. In this study, we propose a new optimization method that directly minimizes changes to individual pixels without using a mask. Our experiments demonstrate that compared to existing methods, our approach generates triggers that require fewer changes to input pixels, have a higher success rate in attacks, and are more robust. This makes them more desirable for real-world attacks and more effective for defense. Additionally, our method is more cost-effective.