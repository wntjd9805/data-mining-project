We introduce HandLer, a novel convolutional architecture that can simultaneously detect and track hands in unrestricted videos. HandLer builds upon the Cascade-RCNN model by incorporating three additional stages. The first stage, Forward Propagation, propagates features from the previous frame to the current frame based on the motion of previously detected hands. The second stage, Detection and Backward Regression, utilizes the output of the forward propagation to detect hands in the current frame and determine their relative position in the previous frame. The third stage employs a pre-existing human pose method to connect fragmented hand tracklets. We train the forward propagation, backward regression, and detection stages in an end-to-end manner along with the other components of Cascade-RCNN. Additionally, we introduce YouTube-Hand, a large-scale dataset consisting of unconstrained videos annotated with hand locations and trajectories, which we use to train and evaluate HandLer. Experimental results on this dataset and other benchmarks demonstrate that HandLer achieves significantly better performance than existing state-of-the-art tracking algorithms. The code and data for HandLer are available at https://vision.cs.stonybrook.edu/Ëœmingzhen/handler/.