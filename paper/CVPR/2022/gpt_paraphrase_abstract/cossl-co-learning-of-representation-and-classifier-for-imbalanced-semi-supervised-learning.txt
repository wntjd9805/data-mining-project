The current standard approach to semi-supervised learning (SSL) using balanced datasets has been successful in utilizing unlabeled data effectively. However, when facing realistic imbalanced datasets, known as imbalanced SSL, standard SSL methods tend to perform poorly. In this study, we introduce a novel co-learning framework called CoSSL, which separates the learning of representations and classifiers but tightly integrates them. To address the issue of imbalanced data, we propose a technique called Tail-class Feature Enhancement (TFE) for classifier learning. Additionally, we highlight the limitations of the current evaluation protocol for imbalanced SSL, which only considers balanced test sets and lacks practicality in real-world scenarios. To overcome this, we conduct a thorough evaluation under various shifted test distributions. Through experimental results, we demonstrate that our approach outperforms other methods across a wide range of shifted distributions, achieving state-of-the-art performance on benchmark datasets such as CIFAR-10, CIFAR-100, ImageNet, and Food-101. We will make our code publicly available.