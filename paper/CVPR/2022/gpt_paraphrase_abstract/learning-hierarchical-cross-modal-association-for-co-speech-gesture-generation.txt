Creating virtual avatars that generate body and gesture movements consistent with speech has long been a challenge. Current methods synthesize pose movements holistically, without capturing the finer details of co-speech gestures. To address this, we propose a new framework called Hierarchical Audio-to-Gesture (HA2G) that leverages the hierarchical nature of speech semantics and human gestures. HA2G includes a Hierarchical Audio Learner that extracts audio representations at different semantic levels, and a Hierarchical Pose Inferer that gradually renders the full human pose. We also introduce a contrastive learning strategy to improve audio representations through audio-text alignment. Extensive experiments and human evaluation show that our approach produces realistic co-speech gestures that surpass previous methods. More information can be found on our project page: https://alvinliu0.github.io/projects/HA2G.