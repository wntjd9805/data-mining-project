Transfer learning has gained popularity as an efficient way to train models by leveraging pre-trained models instead of starting from scratch, especially in computer vision tasks where it has proven to be a strong baseline. With the rise of model repositories like TensorFlow Hub, practitioners and researchers now have access to a wide range of models for various downstream tasks. However, as these repositories continue to grow rapidly, it becomes crucial to effectively select the most suitable model for a specific task.To address this problem, we formalize it using the concept of regret and explore two main strategies: task-agnostic and task-aware search. Task-agnostic strategies involve ranking models based on their performance on a benchmark dataset like ImageNet, while task-aware strategies evaluate models using linear or kNN methods. Our extensive empirical study reveals that both strategies can lead to high regret.To overcome this, we propose a hybrid search strategy that is simple yet computationally efficient. This approach outperforms existing methods, providing better results. We demonstrate the practical benefits of our solution by applying it to 19 diverse vision tasks.In summary, transfer learning has emerged as a valuable approach for training models efficiently. However, with the increasing availability of model repositories, selecting the most appropriate model for a specific task becomes crucial. Our study highlights the limitations of current strategies and introduces a hybrid search strategy that significantly improves performance across various vision tasks.