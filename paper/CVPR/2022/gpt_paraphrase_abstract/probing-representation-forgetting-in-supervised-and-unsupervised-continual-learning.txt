Continual Learning (CL) research primarily focuses on addressing the problem of catastrophic forgetting in neural networks, which refers to the sudden loss of previously learned knowledge when the task or data distribution being trained on changes. In supervised learning scenarios, this forgetting is typically measured by evaluating the decline in performance on old tasks due to changes in the model's representation. However, a model's representation can change without actually losing knowledge about previous tasks. In this study, we explore the concept of representation forgetting, which is observed by comparing the performance of an optimal linear classifier before and after introducing a new task. By using this metric, we reevaluate several standard continual learning benchmarks and find that models trained without explicit control for forgetting often experience minimal representation forgetting. In fact, they can sometimes perform comparably to methods that explicitly address forgetting, particularly when faced with longer sequences of tasks. Additionally, we demonstrate that representation forgetting provides new insights into the impact of model capacity and loss function selection in continual learning. Based on our findings, we propose a simple yet effective approach: continuously train representations using standard supervised contrastive learning while constructing prototypes of class samples when queried about old samples.