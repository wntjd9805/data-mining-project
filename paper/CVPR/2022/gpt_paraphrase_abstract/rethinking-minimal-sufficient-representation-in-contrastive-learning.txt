Contrastive learning, which involves comparing different views of data, has been highly successful in self-supervised representation learning. The representations learned through this approach have proven to be valuable in various downstream tasks. However, since the supervision for one view is derived from the other view, contrastive learning tends to capture only the shared information and discard the non-shared information between views. This can be problematic because not all task-relevant information may be shared between views, leading to a degradation in performance.  To address this issue, we argue that the non-shared task-relevant information cannot be ignored. We theoretically demonstrate that the minimal sufficient representation obtained through contrastive learning is not enough for downstream tasks, resulting in performance degradation. This highlights a new problem with contrastive learning models, as they can overfit to the shared information between views.  In order to mitigate this problem, we propose a solution that involves increasing the mutual information between the representation and input. This is done as a regularization technique to approximately introduce more task-relevant information. Importantly, we cannot utilize any information from downstream tasks during training. Through extensive experiments, we validate our analysis and demonstrate the effectiveness of our approach. Our method significantly improves the performance of several classic contrastive learning models in downstream tasks. The code for our method is publicly available at https://github.com/Haoqing-Wang/InfoCL.