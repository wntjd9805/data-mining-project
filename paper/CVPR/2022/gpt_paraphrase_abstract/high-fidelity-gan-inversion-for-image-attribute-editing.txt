We introduce a new framework for generative adversarial network (GAN) inversion that allows for attribute editing while preserving image-specific details such as background, appearance, and illumination. Previous methods have struggled to maintain high-fidelity details in reconstructed and edited images when using a low bit-rate latent code. Increasing the size of the latent code improves accuracy but sacrifices editability. To address this, we propose a distortion consultation approach called distortion consultation inversion (DCI) that uses a distortion map as a reference for high-fidelity reconstruction. The distortion map is projected to a high-rate latent map, which enhances the basic low-rate latent code with more details through consultation fusion. To enable high-fidelity editing, we introduce an adaptive distortion alignment (ADA) module with a self-supervised training scheme that bridges the gap between edited and inversion images. Our experiments in the face and car domains demonstrate significant improvements in both inversion and editing quality. More information about our project can be found at https://tengfei-wang.github.io/HFGI/.