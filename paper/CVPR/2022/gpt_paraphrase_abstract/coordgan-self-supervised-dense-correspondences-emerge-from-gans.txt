Recent research has shown that Generative Adversarial Networks (GANs) are capable of generating images that exhibit smooth variations in semantically meaningful latent directions, such as pose, expression, and layout. However, little is known about how GANs explicitly extract pixel-level correspondences across images. This study introduces CoordGAN, a disentangled GAN that learns a dense correspondence map for each generated image. The correspondence maps are represented as coordinate frames that are transformed from a canonical coordinate frame. By finding correspondences, we locate the same coordinate in different correspondence maps. CoordGAN samples a transformation to represent the structure of a synthesized instance, while a separate texture branch handles appearance details independent of the structure. The proposed approach is also capable of extracting dense correspondence maps for real images by incorporating an encoder. The quality of the learned dense correspondences is quantitatively demonstrated through segmentation mask transfer on multiple datasets. Additionally, the generator in CoordGAN achieves superior structure and texture disentanglement compared to existing methods. The project page for more information can be found at https://jitengmu.github.io/CoordGAN/.