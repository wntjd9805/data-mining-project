The interpretation of visual scenes with people has advanced with the availability of large-scale video action datasets. However, recognizing human actions and social interactions in a real-world environment with multiple people and unbalanced action labels captured from a mobile robot platform remains challenging due to the lack of a comprehensive dataset. This paper introduces JRDB-Act, an extension of JRDB, which captures human daily-life actions in a university campus setting. JRDB-Act is densely annotated with atomic actions, providing a large-scale spatio-temporal action detection dataset. Each human bounding box is labeled with pose-based and optional interaction-based action labels, as well as social group annotations. The dataset also includes annotators' confidence levels for reliable evaluation strategies. To showcase the utility of these annotations, an end-to-end trainable pipeline is developed for individual action and social group detection. The data and evaluation code will be publicly available at https://jrdb.erc.monash.edu/.