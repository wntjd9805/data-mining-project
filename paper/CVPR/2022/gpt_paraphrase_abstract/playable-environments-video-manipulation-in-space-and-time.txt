We introduce Playable Environments, a novel approach for generating and manipulating interactive videos in space and time. By using a single image during inference, our framework allows users to move objects in 3D and generate a video by providing a sequence of desired actions. These actions are learned in an unsupervised manner, and users can control the camera to achieve their desired viewpoint. Our method creates an environment state for each frame, which can be manipulated using our proposed action module and then rendered back into the image space using volumetric rendering. To support diverse object appearances, we enhance neural radiance fields with style-based modulation. Our approach is trained on a collection of monocular videos, only requiring estimated camera parameters and 2D object locations. Additionally, we introduce two large-scale video datasets with significant camera movements to establish a challenging benchmark. Through our experiments, we demonstrate that playable environments enable various creative applications that were not achievable with previous video synthesis methods, including playable 3D video generation, stylization, and manipulation.