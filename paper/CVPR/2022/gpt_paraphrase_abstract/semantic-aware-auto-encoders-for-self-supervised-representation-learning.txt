The resurgence of unsupervised learning can be attributed to the progress of self-supervised learning, which includes generative (G) and discriminative (D) models. While D models are commonly used in computer vision, they can be complex and less general than G models. In this study, we propose using a classical auto-encoder (AE) as a G model to improve self-supervised learning in computer vision tasks. We introduce a novel AE that learns semantic-aware representation through cross-view image reconstruction. To optimize this AE, we propose a semantic aligner that aligns the hidden code using geometric transformation knowledge. These techniques enhance the learning ability of the AE and enable self-supervised learning with G models. Experimental results on various benchmarks demonstrate the effectiveness of our methods. The code is available at https://github.com/wanggrun/Semantic-Aware-AE.