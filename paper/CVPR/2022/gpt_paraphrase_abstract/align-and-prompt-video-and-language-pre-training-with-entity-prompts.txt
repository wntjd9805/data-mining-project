We present a new framework called Align and Prompt (ALPRO) for video-and-language pre-training. Previous methods have used standard transformer-based multimodal encoders to capture cross-modal interactions but have not fully addressed the misalignment between video and text features. Additionally, fine-grained visual-language alignment typically relies on object detectors, which have limitations in vocabulary and computational cost. ALPRO overcomes these challenges by operating on sparsely-sampled video frames and achieving more effective cross-modal alignment without explicit object detectors. It introduces a video-text contrastive (VTC) loss to align video-text features at the instance level, improving cross-modal modeling. ALPRO also introduces a visually-grounded pre-training task called prompting entity modeling (PEM) that learns fine-grained alignment between visual regions and text entities in a self-supervised manner. The proposed framework is pre-trained on large webly-sourced video-text pairs using the VTC and PEM losses, as well as standard losses of masked language modeling (MLM) and video-text matching (VTM). Experimental results demonstrate that ALPRO achieves state-of-the-art performance on text-video retrieval and video question answering (videoQA) tasks, surpassing previous approaches by a significant margin. The implementation and pre-trained models are publicly available at https://github.com/salesforce/ALPRO.