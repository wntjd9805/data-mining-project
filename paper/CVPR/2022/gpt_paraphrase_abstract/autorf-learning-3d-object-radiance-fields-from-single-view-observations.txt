We present AutoRF, a novel approach to learning neural 3D object representations using only a single view of each object in the training set. Unlike previous works that rely on multiple views, explicit priors, or pixel-perfect annotations, our method aims to learn a normalized, object-centric representation that disentangles shape, appearance, and pose. This encoding provides compact and generalizable information about the object, allowing for the synthesis of novel views. We also enhance reconstruction quality by optimizing shape and appearance codes during testing. Our experiments demonstrate that our approach performs well on unseen objects across various challenging real-world datasets. More details can be found on our project page at https://sirwyver.github.io/AutoRF/.