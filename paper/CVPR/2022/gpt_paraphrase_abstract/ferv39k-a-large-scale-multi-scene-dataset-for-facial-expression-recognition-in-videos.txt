The current benchmarks for facial expression recognition (FER) mainly focus on static images, and there is a lack of datasets for FER in videos. This raises questions about the performance of existing methods in real-world application-oriented scenes. To address this gap, we have created a large-scale multi-scene dataset called FERV39k. We discuss the important factors involved in constructing this dataset, including the multi-scene hierarchy and expression classes, the generation of candidate video clips, and the trusted manual labeling process. We have selected four scenarios with 22 scenes, annotated 86k samples obtained from 4k videos, and built 38,935 video clips labeled with seven classic expressions. We also provide experiment benchmarks on four baseline frameworks and analyze their performance across different scenes, as well as highlight challenges for future research. Additionally, we conduct ablation studies to investigate key components of dynamic facial expression recognition (DFER). The baseline framework and our project are available on https://github.com/wangyanckxx/FERV39k.