Hand and object reconstruction for augmented and virtual reality applications require explicit physical constraints and known objects. However, these limitations restrict the potential domains of application. To overcome this, our algorithm utilizes a collaborative learning strategy where two branches of deep networks learn from each other. This allows us to infer the shapes and physical interaction of hands and potentially unknown objects. To address the instability of the optimization problem, we employ attention-guided graph convolution to focus on mutual occlusion and an unsupervised associative loss for information transfer between the branches. Our framework surpasses state-of-the-art accuracy in 3D pose estimation and successfully recovers dense 3D hand and object shapes, as demonstrated through experiments on four widely-used benchmarks. Each component of our approach contributes significantly, as evidenced by the ablation study.