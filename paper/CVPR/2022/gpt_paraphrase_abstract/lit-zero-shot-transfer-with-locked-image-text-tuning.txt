This study introduces a method called contrastive-tuning, which combines contrastive training with pre-trained image and text models to align them effectively. Through empirical research, it is discovered that using locked pre-trained image models with unlocked text models yields the best results. This approach, referred to as Locked-image Tuning (LiT), enables a text model to extract meaningful representations from a pre-trained image model for new tasks. By implementing LiT, a model can perform zero-shot transfer to new vision tasks like image classification or retrieval. LiT is versatile and works well with various pre-training methods and architectures, including ResNet, Vision Transformers, and MLP-Mixer, across different image-text datasets. When using the transformer-based pre-trained ViT-g/14 model, the LiT model achieves an 84.5% accuracy in zero-shot transfer on the ImageNet test set and an 81.1% accuracy on the challenging out-of-distribution ObjectNet test set.