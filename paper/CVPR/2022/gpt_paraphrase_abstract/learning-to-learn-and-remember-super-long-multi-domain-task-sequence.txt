Catastrophic forgetting is a common problem when learning from non-stationary data distribution. This issue becomes even more challenging in sequential domain meta-learning (SDML), where learning occurs on a sequence of domains. In this study, we propose a meta optimizer approach to address the problem of catastrophic forgetting in SDML. Initially, we apply the meta optimizer to domain-aware meta-learning, where we have knowledge of domain labels and boundaries during the learning process. By dynamically freezing the network and incorporating it with the meta optimizer, taking into account the domain nature during meta training, we aim to mitigate catastrophic forgetting. Furthermore, we extend the meta optimizer to domain-agnostic meta-learning, where domain labels and boundaries are unknown during learning. To handle this, we introduce a domain shift detection technique to identify latent domain changes and equip the meta optimizer with this information. Our proposed meta optimizer is versatile and can be integrated with existing meta-learning algorithms. To evaluate our approach, we create a large-scale benchmark comprising 10 diverse domains and a lengthy task sequence of 100K tasks. Through extensive experiments on this benchmark, we demonstrate the effectiveness of our proposed method, significantly outperforming current strong baselines.