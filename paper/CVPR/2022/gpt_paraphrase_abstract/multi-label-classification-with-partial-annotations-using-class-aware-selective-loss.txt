Large-scale multi-label classification datasets often have partial annotations, where only a few labels are annotated per sample. The handling of missing labels can impact the accuracy of the model. In this study, we examine the problem of partial labeling and propose a solution based on two key ideas. Firstly, unannotated labels should be treated selectively based on the overall class distribution in the dataset and the likelihood of a specific label for a given sample. We suggest estimating the class distribution using a dedicated temporary model, which outperforms a naive estimation using the partial annotations. Secondly, during the training of the target model, we prioritize the contribution of annotated labels over unannotated labels by using an asymmetric loss. Our approach achieves state-of-the-art results on the OpenImages dataset, with a mean average precision (mAP) of 87.3 on V6. Furthermore, experiments on the LVIS and simulated-COCO datasets demonstrate the effectiveness of our approach. The code for our approach is available at https://github.com/Alibaba-MIIL/PartialLabelingCSL.