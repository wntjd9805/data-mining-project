Self-supervised learning has emerged as a promising approach for extracting powerful visual representations without the need for human annotations. Different methods have been proposed to tackle self-supervised learning from various perspectives. Contrastive learning methods, such as MoCo and SimCLR, use positive and negative samples to guide the training direction. Asymmetric network methods, like BYOL and Sim-Siam, eliminate negative samples by introducing a predictor network and the stop-gradient operation. Feature decorrelation methods, such as Barlow Twins and VICReg, aim to reduce redundancy between feature dimensions. These methods have different loss functions and yield varying accuracy results due to the use of different networks and techniques.In this study, we demonstrate that these methods can be unified into a single form by analyzing their gradients. Instead of comparing their loss functions, we derive a unified formula through gradient analysis. We also conduct extensive and fair experiments to compare their performances. Interestingly, we find that there is little difference between these methods, and the use of a momentum encoder is the key factor for improving performance. Based on this unified framework, we propose UniGrad, a simple but effective gradient form for self-supervised learning. UniGrad does not require a memory bank or a predictor network, yet it achieves state-of-the-art performance and can easily incorporate other training strategies. Our experiments on linear evaluation and various downstream tasks also demonstrate the effectiveness of UniGrad. We will release the code for further research.In Figure 1, we provide an overview of the three typical types of self-supervised learning methods (contrastive learning, asymmetric network methods, and feature decorrelation methods) and our proposed UniGrad. We observe that these methods share a similar gradient structure composed of positive and negative gradients, analogous to positive and negative samples in contrastive learning. Although some methods do not explicitly use negative samples, we highlight the source of negative gradient in each method.In conclusion, our study reveals the similarities among different self-supervised learning methods and presents a unified framework. We introduce UniGrad as a simple yet effective gradient form that achieves state-of-the-art performance. Our experiments validate the effectiveness of UniGrad in various evaluation tasks.