This study focuses on the BERT pretraining of video transformers, following the successful application of BERT pretraining to image transformers. The researchers propose a new approach called BEVT, which separates video representation learning into spatial representation learning and temporal dynamics learning. BEVT first performs masked image modeling on image data and then combines masked image modeling with masked video modeling on video data. This design is based on two observations: firstly, transformers trained on image datasets can provide spatial priors that facilitate the learning of video transformers, which can be computationally intensive to train from scratch; and secondly, the discriminative clues required for accurate predictions vary between different videos due to significant variations within and between classes. Extensive experiments are conducted on three challenging video benchmarks, and BEVT achieves promising results. On the Kinetics 400 benchmark, which heavily relies on discriminative spatial representations, BEVT achieves comparable results to strong supervised baselines. On the Something-Something-V2 and Diving 48 benchmarks, which involve videos relying on temporal dynamics, BEVT outperforms alternative baselines by clear margins and achieves state-of-the-art performance with 71.4% and 87.2% Top-1 accuracy, respectively. The code for BEVT is available at https://github.com/xyzforever/BEVT.