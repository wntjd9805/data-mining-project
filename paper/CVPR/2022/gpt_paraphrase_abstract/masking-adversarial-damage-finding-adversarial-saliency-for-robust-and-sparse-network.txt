Adversarial examples create reliability issues and potential security risks in deep neural networks. Adversarial training has been studied to enhance robustness, but it requires high computational resources and memory. To address this, we propose a new method called Masking Adversarial Damage (MAD) that uses second-order information of adversarial loss. MAD accurately identifies which model parameters can be pruned without compromising adversarial robustness. Additionally, we find that the initial layer parameters are particularly sensitive to adversarial examples and that compressed feature representations still contain meaningful information. Through extensive experiments on public datasets, we demonstrate that MAD effectively prunes adversarially trained networks without sacrificing robustness and outperforms previous adversarial pruning methods.