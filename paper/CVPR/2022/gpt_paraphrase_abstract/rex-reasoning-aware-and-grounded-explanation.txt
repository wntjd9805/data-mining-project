Trustworthy AI systems require both effectiveness and interpretability. However, recent studies in visual reasoning focus more on improving accuracy than explaining decision-making. Consequently, these systems often rely on biases rather than genuine reasoning, and lack the ability to provide explanations by considering information from both visual and textual data. This paper aims to address these issues from three perspectives. Firstly, it introduces a new type of multi-modal explanations that progressively explain decisions by grounding keywords in images. A functional program is developed to execute different reasoning steps, resulting in a dataset of 1,040,830 multi-modal explanations. Secondly, the paper highlights the importance of tightly coupling components across visual and textual modalities to improve explanation generation. A novel method is proposed that explicitly models the correspondence between words and regions of interest, enhancing interpretability and reasoning performance. Lastly, extensive analyses are conducted using the new data and method to evaluate the effectiveness of the explanations under various settings, such as multi-task learning and transfer learning. The code and data are available at https://github.com/szzexpoi/rex.