This paper focuses on enhancing the representation learning of human actions in videos using large-scale synthetic videos. The authors introduce an automatic video synthesis pipeline based on a photorealistic video game to generate a dataset called GATA, which contains 8.1 million action clips across 28,000 action classes. They then propose a contrastive learning framework for human motion representation learning using this dataset, which improves performance on various video datasets for action recognition. The authors also explore a domain adaptation method to address the gap between synthetic and realistic data. They conduct extensive analyses to demonstrate the effectiveness of the proposed dataset in enhancing human motion representation learning.