Abductive reasoning, which aims to find the most plausible explanation for partial observations, is commonly used in human reasoning but has not been explored much in computer vision research. This paper introduces a new task and dataset called Visual Abductive Reasoning (VAR) to evaluate the abductive reasoning capabilities of AI systems in everyday visual scenarios. The goal is for these systems to not only describe what they observe but also infer the hypothesis that best explains the visual evidence. To achieve this, the authors propose a baseline model called REASONER (causal-and-cascaded reasoning Transformer) that utilizes a contextualized directional position embedding strategy to capture the causal structure of the observations. The model includes multiple decoders that generate and refine the premise and hypothesis sentences, with prediction scores guiding the flow of information between sentences during reasoning. Experimental results on the VAR dataset demonstrate that REASONER outperforms several well-known video-language models but still lags behind human performance. This research is expected to inspire further studies in the field of reasoning beyond simple observation.