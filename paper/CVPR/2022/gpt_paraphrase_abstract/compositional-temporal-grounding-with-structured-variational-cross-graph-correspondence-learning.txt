Temporal grounding in videos involves identifying a specific video segment that corresponds to a given query sentence. This task has gained attention due to its ability to go beyond predefined activity classes and accommodate the diverse semantics of natural language descriptions. The compositionality principle in linguistics allows for the creation of new semantics by combining existing words in novel ways. However, current temporal grounding datasets do not evaluate the compositional generalizability of models. To address this, we introduce a new Compositional Temporal Grounding task and create two dataset splits, Charades-CG and ActivityNet-CG. Evaluating state-of-the-art methods on these datasets, we find that existing models struggle to handle queries with novel word combinations. To overcome this challenge, we propose a varia-tional cross-graph reasoning framework that decomposes video and language into multiple structured hierarchies and learns semantic correspondence between them. Our experiments demonstrate the superior compositional generalizability of our approach. The repository for this work can be found at https://github.com/YYJMJC/Compositional-Temporal-Grounding.