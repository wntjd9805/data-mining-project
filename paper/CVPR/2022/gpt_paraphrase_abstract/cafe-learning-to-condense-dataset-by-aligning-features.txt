Dataset condensation involves reducing the effort required for network training by converting a large training set into a smaller synthetic one. Current methods rely on matching the gradients between real and synthetic data batches to learn the synthetic data. However, these gradient-based methods often overfit to a biased set of samples that produce dominant gradients and lack a global supervision of data distribution. In this paper, we propose a novel approach called CAFE (Condense dataset by Aligning FEatures) that aims to preserve the distribution of real features and the discriminant power of the resulting synthetic set. Our approach aligns features from real and synthetic data at different scales while considering the classification of real samples. Additionally, we introduce a dynamic bi-level optimization to prevent overfitting or underfitting. Extensive experiments on various datasets demonstrate that CAFE outperforms state-of-the-art methods, achieving a performance gain of up to 11% on the SVHN dataset. We also provide analysis and visualizations to support the effectiveness and necessity of our proposed approach.