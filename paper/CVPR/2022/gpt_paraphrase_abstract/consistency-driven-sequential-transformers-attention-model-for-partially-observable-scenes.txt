This paper presents a Sequential Transformers Attention Model (STAM) that is capable of predicting informative glimpse locations in an image based on past glimpses, without the need to observe the entire scene. This is particularly useful in applications such as aerial imaging where time and resource constraints limit the ability to capture complete scenes. The agent is developed using DeiT-distilled and trained using a one-step actor-critic algorithm. To improve classification performance, a novel training objective is introduced, which enforces consistency between the class distribution predicted by a teacher model from a complete image and the class distribution predicted by the agent using glimpses. When the agent only senses 4% of the image area, the inclusion of the proposed consistency loss in the training objective leads to a 3% and 8% increase in accuracy on the ImageNet and fMoW datasets respectively. Furthermore, the agent outperforms previous state-of-the-art models by observing significantly fewer pixels in glimpses (27% and 42% less on ImageNet and fMoW respectively).