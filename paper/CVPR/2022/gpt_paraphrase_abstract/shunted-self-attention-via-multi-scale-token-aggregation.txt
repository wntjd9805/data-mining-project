Recent Vision Transformer (ViT) models have shown promising results in various computer vision tasks by effectively modeling long-range dependencies of image patches or tokens using self-attention. However, these models typically assign similar receptive fields to each token feature within each layer, limiting their ability to capture multi-scale features. This limitation leads to decreased performance when dealing with images containing multiple objects of different scales. To overcome this issue, we propose a new strategy called shunted self-attention (SSA) that enables ViTs to model attentions at hybrid scales per attention layer.The main idea behind SSA is to introduce tokens with different receptive field sizes: before computing the self-attention matrix, some tokens are merged to represent larger object features, while others are kept to preserve fine-grained features. This merging scheme allows the self-attention to learn relationships between objects of varying sizes while reducing the number of tokens and computational cost. Extensive experiments across different tasks demonstrate the superiority of SSA.In particular, the SSA-based transformer achieves an 84.0% Top-1 accuracy, outperforming the state-of-the-art Focal Transformer on ImageNet with only half the model size and computation cost. It also surpasses the Focal Transformer by 1.3 mAP on COCO and 2.9 mIOU on ADE20K under similar parameter and computation cost conditions. We have released the code for our approach at https://github.com/OliverRensu/Shunted-Transformer.