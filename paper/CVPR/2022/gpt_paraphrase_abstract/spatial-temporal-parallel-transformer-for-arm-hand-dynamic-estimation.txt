We present an approach for estimating the movements of the arm and hand from monocular video by leveraging the relationship between the two. While there have been advancements in monocular full human motion capture, accurately capturing arm twists and hand gestures in uncontrolled videos still poses a challenge. To address this, we propose a solution based on the observation that arm poses and hand gestures are highly correlated in real-life scenarios. Our approach, the Spatial-Temporal Parallel Arm-Hand Motion Transformer (PAHMT), is designed to exploit this correlation and inter-frame information to simultaneously predict the dynamics of the arm and hand. We also introduce new loss functions to encourage smooth and accurate estimations. Additionally, we create a motion capture dataset of 200K frames of hand gestures to train our model. By combining a 2D hand pose estimation model and a 3D human pose estimation model, our method can generate plausible arm and hand movements from monocular video. Extensive evaluations demonstrate that our approach outperforms previous state-of-the-art methods and remains robust in challenging scenarios.