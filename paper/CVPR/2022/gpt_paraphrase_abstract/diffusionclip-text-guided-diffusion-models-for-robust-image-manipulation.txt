Recently, the combination of GAN inversion methods and Contrastive Language-Image Pretraining (CLIP) has allowed for image manipulation guided by text prompts. However, these methods still struggle with diverse real images due to limited GAN inversion capability. Specifically, they have difficulty reconstructing images with new poses, views, and variable content compared to the training data. They may also alter object identity or produce unwanted image artifacts. To address these challenges, we propose a new method called DiffusionCLIP that uses diffusion models for text-driven image manipulation. Leveraging the full inversion capability and high-quality image generation of diffusion models, our method successfully performs zero-shot image manipulation even between unseen domains. We also introduce a novel noise combination method that enables straightforward multi-attribute manipulation. Through extensive experiments and human evaluation, we demonstrate that our methods outperform existing baselines in terms of robustness and manipulation performance. The code for our method is available at https://github.com/gwang-kim/DiffusionCLIP.git.