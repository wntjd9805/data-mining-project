Previous methods for disentangled image manipulation rely heavily on manual annotation and are limited to a predetermined set of manipulations. This paper introduces a new framework called Predict, Prevent, and Evaluate (PPE) that requires less manual annotation and can be applied to a wide range of manipulations. The framework leverages the CLIP vision-language model to predict potentially entangled attributes based on a given text command. An entanglement loss is then introduced during training to prevent entanglements. Additionally, a new evaluation metric is proposed to assess the effectiveness of the disentangled image manipulation. The framework is validated through experiments on face editing tasks, demonstrating superior quantitative and qualitative results compared to the StyleCLIP baseline. The code for the framework is available on GitHub.