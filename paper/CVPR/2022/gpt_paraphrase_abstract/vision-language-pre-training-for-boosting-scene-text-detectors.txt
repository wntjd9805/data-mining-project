Vision-language joint representation learning has been successful in different scenarios. This paper focuses on applying this approach to scene text detection, a task that involves the interaction between vision and language. The goal is to improve the performance of scene text detectors by learning contextualized joint representations through vision-language pre-training. To achieve this, a pre-training architecture is proposed with an image encoder, a text encoder, and a cross-modal encoder. Three pretext tasks are used: image-text contrastive learning, masked language modeling, and word-in-image prediction. The pre-trained model produces more informative representations with richer semantics, which can enhance existing scene text detectors like EAST and PSENet. Extensive experiments on standard benchmarks show that this paradigm significantly improves the performance of various text detectors, surpassing previous pre-training approaches. The code and pre-trained models will be made publicly available.