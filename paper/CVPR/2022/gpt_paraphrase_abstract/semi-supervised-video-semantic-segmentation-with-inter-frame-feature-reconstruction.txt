In real-world scenarios, semantic segmentation faces the challenge of having limited pixel-level labels due to the high cost of human labor, despite the availability of a large volume of video data. Existing semi-supervised methods attempt to utilize unlabeled data in model training, but they treat videos as independent images. To address this limitation and better explore semi-supervised segmentation with video data, this paper introduces a novel approach. The authors observe that there is a significant overfitting issue between labeled and unlabeled frames within a training video, despite their similarities in style and content. This phenomenon, known as inner-video overfitting, leads to inferior performance. To overcome this challenge, the authors propose a new technique called inter-frame feature reconstruction (IFR). IFR leverages ground-truth labels to supervise model training and focuses on the relevance of different frames within a video. During training, IFR narrows the feature distributions between labeled and unlabeled frames, effectively alleviating the inner-video overfitting problem. The proposed method is evaluated on Cityscapes and CamVid datasets, and the results demonstrate its superiority over previous state-of-the-art methods. The code for this approach is available at https://github.com/jfzhuang/IFR.