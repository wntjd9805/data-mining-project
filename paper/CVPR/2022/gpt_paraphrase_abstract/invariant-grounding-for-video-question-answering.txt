Video Question Answering (VideoQA) involves answering questions about a video by understanding the connections between visual scenes and linguistic meaning. Existing VideoQA models use empirical risk minimization (ERM) to learn correlations between video-question pairs and answers. However, ERM can be problematic as it relies on superficial correlations and overlooks the causal effect of question-critical scenes. This leads to unreliable reasoning in VideoQA models. In this study, we propose a new learning framework called Invariant Grounding for VideoQA (IGV) that focuses on invariant grounding to eliminate spurious correlations. IGV forces the VideoQA models to prioritize the question-critical scene, which improves reasoning ability by avoiding negative influences from spurious correlations. Experiments on three benchmark datasets demonstrate that IGV outperforms existing baselines in terms of accuracy, visual explainability, and generalization ability. The code for IGV is available at https://github.com/yl3800/IGV.