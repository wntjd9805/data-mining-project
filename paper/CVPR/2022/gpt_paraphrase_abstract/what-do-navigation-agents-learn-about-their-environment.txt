Current visual navigation agents are typically complex deep learning models that lack interpretability. The actions and learned skills of these agents are not easily understood. Previous research has focused on interpreting deep learning models, but little attention has been given to interpreting embodied AI systems, which require reasoning about the environment's structure, target characteristics, and action outcomes. In this paper, we present the Interpretability System for Embodied Agents (iSEE) for navigation agents with Point Goal and Object Goal tasks. iSEE investigates the dynamic representations generated by these agents to identify information about the agent and the environment. Through iSEE, we gain valuable insights into navigation agents, such as their ability to encode accessible locations, target visibility, progress from initial spawn location, and the significant impact of masking critical neurons on agent behavior.