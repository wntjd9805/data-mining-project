Blind face restoration aims to recover high-quality face images from unknown degradations. This study presents a method called RestoreFormer that utilizes fully-spatial attentions to model contextual information, surpassing previous works that rely on local operators. RestoreFormer offers several advantages over existing approaches. Firstly, it incorporates a multi-head cross-attention layer, unlike conventional multi-head self-attention used in previous Vision Transformers (ViTs), enabling it to learn fully-spatial interactions between corrupted queries and high-quality key-value pairs. Secondly, RestoreFormer utilizes key-value pairs sampled from a reconstruction-oriented high-quality dictionary, which contains rich facial features specifically designed for face reconstruction. This leads to superior restoration results. Thirdly, RestoreFormer outperforms state-of-the-art methods on synthetic and real-world datasets, producing images with improved visual quality. The code for RestoreFormer is available at https://github.com/wzhouxiff/RestoreFormer.git.