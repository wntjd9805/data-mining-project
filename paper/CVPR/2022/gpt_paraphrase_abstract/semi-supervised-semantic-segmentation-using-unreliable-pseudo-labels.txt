The main focus of semi-supervised semantic segmentation is assigning appropriate pseudo-labels to unlabeled image pixels. Typically, highly confident predictions are selected as pseudo ground-truth, but this approach disregards many pixels that are considered unreliable. We propose that every pixel is important for model training, even if its prediction is ambiguous. While an unreliable prediction may be confused among the top classes, it should still be confident about not belonging to the remaining classes. Therefore, we can confidently treat such pixels as negative samples for the least likely categories. Building on this idea, we have developed a pipeline that effectively utilizes unlabeled data. Specifically, we separate reliable and unreliable pixels based on prediction entropy, place each unreliable pixel in a category-specific queue as negative samples, and train the model using all candidate pixels. We adjust the threshold for the reliable-unreliable partition adaptively as the training progresses and predictions become more accurate. Experimental results on various benchmarks and training settings demonstrate the superiority of our approach compared to existing state-of-the-art alternatives.