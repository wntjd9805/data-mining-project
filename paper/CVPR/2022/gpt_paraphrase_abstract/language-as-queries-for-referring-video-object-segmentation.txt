This study focuses on referring video object segmentation (R-VOS), which involves segmenting a target object in all frames of a video based on a given language expression. The researchers propose a new framework called ReferFormer, which is built on the Transformer model. The language expression is treated as queries, allowing the model to attend to the relevant regions in the video frames. By conditioning a small set of object queries on the language, the model is able to find the referred objects accurately. These queries are transformed into dynamic kernels, which capture important object-level information and generate segmentation masks from feature maps. Object tracking is achieved by linking corresponding queries across frames, simplifying the pipeline. The end-to-end ReferFormer framework differs significantly from previous methods. Experimental results on various datasets demonstrate the effectiveness of ReferFormer, surpassing the state-of-the-art performance on Ref-Youtube-VOS by 8.4 points using a ResNet-50 backbone and achieving the best performance among existing methods using the Video-Swin-Base backbone. The results also outperform previous methods by a large margin on A2D-Sentences and JHMDB-Sentences datasets. The code for ReferFormer is publicly available at the given GitHub link.