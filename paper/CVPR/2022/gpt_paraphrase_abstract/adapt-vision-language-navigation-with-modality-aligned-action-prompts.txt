This study addresses the challenging task of Vision-Language Navigation (VLN), where an embodied agent needs to align instructions with actions in complex visual environments. Existing VLN agents primarily learn the instruction-path data, lacking exploration of action-level alignment within multi-modal inputs. To address this, the authors propose a method called modAlity-aligneD Action PrompTs (ADAPT) that provides the agent with action prompts for explicit learning of action-level modality alignment to improve navigation success. An action prompt consists of an image sub-prompt and a text sub-prompt, representing a single-view observation and a corresponding action phrase, respectively. During navigation, the agent retrieves the instruction-related action prompt set from a pre-built base, encodes the prompts, and combines them with the original instruction feature for action prediction using a multi-layer transformer. To ensure high-quality prompts, the Contrastive Language-Image Pretraining (CLIP) model is used to align language and image modalities effectively. Additionally, a modality alignment loss and a sequential consistency loss are introduced to enhance prompt alignment and guide the agent's sequential focus. Experimental results on benchmark datasets demonstrate that ADAPT outperforms state-of-the-art methods in VLN tasks.