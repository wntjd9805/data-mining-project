We present a method for accurately estimating the 3D poses of two hands in close interaction from a single color image. This is a challenging task due to occlusions and confusion between joints. Existing approaches address this problem by regressing a heatmap for each joint, which involves simultaneously localizing and recognizing the joints. In our approach, called "Keypoint Transformer," we separate these tasks by using a CNN to localize joints as 2D keypoints and then employ self-attention to associate them with the corresponding hand joint. This architecture achieves state-of-the-art performance with fewer model parameters on the InterHand2.6M dataset. We also demonstrate that our method can be extended to estimate the 3D pose of an object manipulated by one or two hands with high accuracy. Additionally, we have created a new dataset of over 75,000 fully annotated 3D images of two hands manipulating an object, which will be publicly available.