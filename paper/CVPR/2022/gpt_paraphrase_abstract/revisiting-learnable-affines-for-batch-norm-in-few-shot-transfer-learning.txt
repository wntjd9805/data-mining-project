Batch normalization is a widely used technique in computer vision models, including those used in few-shot learning. In convolutional neural networks, batch normalization layers consist of a normalization step, followed by a shift and scale of the normalized features using trainable affine parameters γ and β for each channel. These affine parameters were initially introduced to preserve the expressive capabilities of the model after normalization. However, this study demonstrates that these parameters have a negative impact on the performance of common few-shot transfer tasks. The researchers conducted experiments using various methods on popular benchmarks such as miniImageNet, cross-domain few-shot learning (CD-FSL), and META-DATASET. The results consistently showed that CNNs with batch normalization layers without affine parameters performed better, especially in scenarios with significant domain shifts in few-shot transfer settings. In contrast to conventional practices in few-shot transfer learning where the affine parameters are kept fixed during adaptation, this study suggests that fine-tuning these parameters can lead to improved performance.