This paper conducts a comprehensive study on the robustness of deep learning-based rain removal methods against adversarial attacks. The study reveals that when images/videos are highly degraded, rain removal methods are more vulnerable to adversarial attacks as small distortions become less noticeable. The paper presents an empirical evaluation of various methods at different levels of attacks and with different losses/targets. The evaluation is done from the perspective of human perception and machine analysis tasks. Key modules in existing methods are systematically evaluated for their robustness against adversarial attacks. Based on the insights gained from the analysis, a more robust deraining method is constructed by integrating these effective modules. The paper also examines various types of adversarial attacks specific to deraining problems and their effects on both human and machine vision tasks. These attacks include rain region attacks, which add perturbations only in the rain regions to make them less visible, and object-sensitive attacks, which add perturbations near given objects. The code for the study is available at https://github.com/yuyi-sd/Robust_Rain_Removal.