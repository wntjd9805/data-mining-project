Various methods have been suggested to detect out-of-distribution (OOD) instances by modifying models, input examples, training sets, and optimization objectives. However, we propose a different approach. We believe that standard off-the-shelf models already contain enough information about the training set distribution, which can be utilized for reliable OOD detection. To validate our hypothesis, we conducted an empirical study that measured the mean activations of model outputs for both OOD and in-distribution (ID) mini-batches. Surprisingly, we found that the activation means of OOD mini-batches consistently deviate more from those of the training data. Moreover, the activation means of the training data can be efficiently computed offline or obtained from batch normalization layers without any additional cost. Based on this observation, we introduce a novel metric called Neural Mean Discrepancy (NMD), which compares the neural means of input examples and training data. Leveraging the simplicity of NMD, we propose an efficient OOD detector that calculates neural means through a standard forward pass followed by a lightweight classifier. Our extensive experiments demonstrate that NMD outperforms state-of-the-art OOD methods in terms of both detection accuracy and computational cost across multiple datasets and model architectures.