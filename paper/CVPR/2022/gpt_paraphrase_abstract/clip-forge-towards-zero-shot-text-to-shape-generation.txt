Text-to-shape generation is a challenging task due to the lack of paired text and shape data on a large scale. However, it has the potential to revolutionize the way we imagine and create objects. In this study, we propose a simple yet effective method called CLIP-Forge to address this issue. CLIP-Forge utilizes a two-stage training process, relying on an unlabeled shape dataset and a pre-trained image-text network like CLIP. This approach has several advantages, including the avoidance of time-consuming inference time optimization and the ability to generate multiple shapes based on a given text. We showcase the promising zero-shot generalization capabilities of the CLIP-Forge model through qualitative and quantitative evaluations. Moreover, extensive comparative evaluations are conducted to gain a deeper understanding of its behavior.