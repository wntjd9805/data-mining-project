We expand the capabilities of neural 3D representations to offer users more intuitive and understandable control options beyond just changing camera views. By using a small number of mask annotations in the training images, users can indicate which part of the scene they want to control. Our approach involves treating these attributes as latent variables that the neural network can predict based on the scene encoding. This enables a few-shot learning framework, where the attributes can be automatically discovered by the system when no annotations are provided. We apply our method to different scenes with various types of controllable attributes, such as controlling expressions on human faces or manipulating the movement of inanimate objects. Our results showcase, for the first time to our knowledge, the ability to render scenes with novel views and attributes from a single video.