This paper introduces a novel method for training video transformers without the need for labeled data. The proposed approach generates different spatiotemporal views of a given video, with varying sizes and frame rates. The objective of the self-supervised training is to match the features of these views, making them invariant to spatiotemporal variations in actions. Notably, this approach is the first to eliminate the reliance on negative samples or dedicated memory banks in Self-supervised Video Transformer (SVT). Additionally, the flexibility of Transformer models allows for slow-fast video processing within a single architecture, achieved through dynamically adjusted positional encoding. Furthermore, the proposed method supports long-term relationship modeling across spatiotemporal dimensions. Experimental results demonstrate the effectiveness of the approach on four action recognition benchmarks (Kinetics-400, UCF-101, HMDB-51, and SSv2), with the added benefit of faster convergence using small batch sizes. The code for this method is available at https://git.io/J1juJ.