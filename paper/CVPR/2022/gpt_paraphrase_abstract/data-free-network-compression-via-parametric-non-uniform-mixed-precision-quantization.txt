Deep Neural Networks (DNNs) with a large number of parameters and high storage requirements face limitations when applied to memory-limited devices. To address this, network quantization has been used to compress DNNs. However, existing quantization methods often rely on the training dataset and fine-tuning, which is not feasible in confidential scenarios due to privacy and security concerns. To overcome this, we propose PNMQ, a novel data-free compression method. PNMQ utilizes Parametric Non-uniform Mixed precision Quantization to generate a quantized network. In the compression stage, an optimal non-uniform quantization grid is calculated for each layer to minimize quantization errors. The user can specify the desired compression ratio, which is used by the PNMQ algorithm to select the bitwidths of layers. This approach eliminates the need for model retraining or complex computations, enabling efficient implementation of network compression on edge devices. Extensive experiments on computer vision tasks demonstrate that PNMQ outperforms other state-of-the-art network compression methods.