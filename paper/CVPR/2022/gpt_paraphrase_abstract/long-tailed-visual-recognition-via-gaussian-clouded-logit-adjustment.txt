Deep neural networks have achieved great success in classifying balanced data, but they struggle with long-tailed data. When trained using vanilla methods and cross-entropy loss, the head classes with more instances tend to compress the spatial distribution of the tail classes, making it difficult to classify samples from those classes. Additionally, the original cross-entropy loss has a limited ability to propagate gradients, as the gradient in softmax form quickly approaches zero with increasing logit difference, a phenomenon known as softmax saturation. Although unfavorable for balanced data, this saturation can be leveraged to adjust the validity of samples in long-tailed data, addressing the distorted embedding space characteristic of such problems. In this paper, we propose a method called Gaussian clouded logit adjustment, which perturbs the logits of different classes using Gaussian noise with varying amplitudes. We refer to the amplitude as the cloud size and assign relatively large cloud sizes to tail classes. By reducing softmax saturation, the large cloud size makes tail class samples more active and expands the embedding space. To mitigate classifier bias, we introduce the class-based effective number sampling strategy and suggest re-training the classifier. Extensive experiments on benchmark datasets validate the effectiveness of our proposed method, and the source code is available at https://github.com/Keke921/GCLLoss.