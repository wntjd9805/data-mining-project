This paper proposes a solution to generating dynamic 3D facial expressions using a neutral 3D face and an expression label. The solution involves addressing two sub-problems: modeling the temporal dynamics of expressions and deforming the neutral mesh to obtain the expressive counterpart. The temporal evolution of expressions is represented using the motion of a sparse set of 3D landmarks, which are learned through training a manifold-valued GAN called Motion3DGAN. To encode the expression-induced deformation and separate it from identity information, the generated motion is represented as per-frame displacement from a neutral configuration. To generate the expressive meshes, a Sparse2Dense mesh Decoder (S2D-Dec) is trained to map the landmark displacements to dense, per-vertex displacement. This allows for learning how the motion of a sparse set of landmarks influences the deformation of the overall face surface independently from identity. Experimental results on the CoMA and D3DFACS datasets demonstrate significant improvements in dynamic expression generation and mesh reconstruction compared to previous solutions, while also showing good generalization to unseen data. The code and models for this solution are available at the provided GitHub link. Figure 1 illustrates the process of 3D dynamic facial expression generation: a GAN generates the motion of 3D landmarks based on an expression label and noise, and a decoder expands the animation from the landmarks to a dense mesh while preserving the identity of a neutral 3D face.