Event cameras are innovative sensors that capture changes in pixel-level intensity asynchronously, resulting in minimal motion blur and high temporal resolution. Compared to traditional cameras, event cameras require less power and memory. These unique characteristics make them well-suited for various real-world applications, including egocentric action recognition on wearable devices, where fast camera movement and limited power pose challenges for traditional vision sensors. However, the potential of event cameras in such applications has been largely overlooked in the growing field of event-based vision. This paper aims to demonstrate the value of event data for egocentric action recognition by introducing N-EPIC-Kitchens, the first event-based camera extension of the large-scale EPIC-Kitchens dataset. Two strategies are proposed: (i) directly processing event-camera data with traditional video-processing architectures (E2(GO)), and (ii) using event-data to distill optical flow information (E2(GO)MO). Results from our benchmark indicate that event data performs comparably to RGB and optical flow, without the need for additional flow computation during deployment. Furthermore, using event data improves performance by up to 4% compared to relying solely on RGB information. The N-EPIC-Kitchens dataset can be accessed at https://github.com/EgocentricVision/N-EPIC-Kitchens.