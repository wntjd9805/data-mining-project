This abstract discusses the use of diffusion models (DMs) in image synthesis and their limitations in terms of computational resources and inference speed. The authors propose a method to address these limitations by applying DMs in the latent space of pretrained autoencoders. This approach allows for a near-optimal balance between complexity reduction and detail preservation, resulting in improved visual fidelity. Additionally, the authors introduce cross-attention layers to make DMs more flexible for conditioning inputs like text or bounding boxes, enabling high-resolution synthesis in a convolutional manner. The proposed latent diffusion models (LDMs) achieve state-of-the-art results in image inpainting, class-conditional image synthesis, unconditional image generation, text-to-image synthesis, and super-resolution tasks, while significantly reducing computational requirements compared to pixel-based DMs.