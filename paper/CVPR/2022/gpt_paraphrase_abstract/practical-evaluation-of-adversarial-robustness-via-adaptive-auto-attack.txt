The lack of practical evaluation methods has hindered the progress of defense models against adversarial attacks. Evaluation aims to determine the minimum level of robustness of defense models given a limited number of iterations and a test dataset. An ideal evaluation method should be convenient, efficient, and reliable. In order to address these challenges, we propose an evaluation method called Adaptive Auto Attack (A3) that is parameter-free and improves efficiency and reliability through a test-time-training approach.  Our method is based on the observation that adversarial examples targeting a specific defense model exhibit regular patterns in their starting points. To accelerate the evaluation process, we introduce an Adaptive Direction Initialization strategy. Additionally, to approach the lower bound of robustness within the given number of iterations, we propose an online statistics-based discarding strategy that automatically identifies and discards difficult-to-attack images.  We conducted extensive experiments on nearly 50 widely-used defense models and our A3 method proved to be effective. Compared to existing methods, our approach consumed significantly fewer iterations, achieving an average speedup of 10 times. Despite the reduced number of iterations, our method consistently achieved lower robust accuracy. Notably, our method secured the first place in the CVPR 2021 White-box Adversarial Attacks on Defense Models competition. The code for our method is available at the following GitHub repository: https://github.com/liuye6666/adaptive-auto-attack.