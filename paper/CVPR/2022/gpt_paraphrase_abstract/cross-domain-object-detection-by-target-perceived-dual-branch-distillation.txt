Cross domain object detection is a challenging task due to differences in data distributions and lack of annotations in the target domain. Existing approaches only address one of these difficulties. To tackle this problem, we propose the Target-perceived Dual-branch Distillation (TDD) framework. This framework integrates detection branches from both source and target domains in a teacher-student learning scheme to reduce domain shift and provide reliable supervision. We introduce a Target Proposal Perceiver that enhances the source detector to perceive objects in the target domain using target proposal contexts. Additionally, we employ a Dual Branch Self Distillation strategy for model training, which combines object knowledge from different domains through self-distillation in two branches. Our TDD outperforms state-of-the-art methods in cross domain object detection benchmarks. We will release the codes and models for further research.