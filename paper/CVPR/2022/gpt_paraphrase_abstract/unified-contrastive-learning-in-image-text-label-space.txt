We propose a new approach called Unified Contrastive Learning (UniCL) that combines supervised learning and language-image pretraining to improve visual recognition. By integrating human-annotated image-label data and webly-crawled image-text pairs, we create a common image-text-label space. UniCL utilizes a single learning objective to effectively learn semantically rich and discriminative representations. Our experiments demonstrate that UniCL outperforms both language-image contrastive learning and supervised learning methods in zero-shot recognition, linear-probing, fully finetuning, and transfer learning scenarios. In zero-shot recognition benchmarks, UniCL achieves gains of up to 9.2% and 14.5% compared to the language-image contrastive learning and supervised learning methods, respectively. In the linear probe setting, UniCL also improves performance by 7.3% and 3.4% over the two methods, respectively. Furthermore, our study shows that UniCL performs competitively with supervised learning methods on pure image-label data across multiple image classification datasets and vision backbones. The code for UniCL is available at: https://github.com/microsoft/UniCL.