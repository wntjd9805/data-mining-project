To address the increasing demand for high-quality 3D surfaces in display technology, this article proposes a multimodal-driven deep neural network for 3D surface super-resolution. Traditional methods struggle to obtain detailed surfaces and geometry textures at a low cost due to the complex nature of 3D object data. The proposed network overcomes this challenge by simultaneously leveraging texture, depth, and normal modalities to restore fine-grained surface details and preserve geometry structures. To enhance the utilization of cross-modal information, a two-bridge normal method with a transformer structure is employed for feature alignment, along with an affine transform module for multimodal feature fusion. The effectiveness of the proposed method is demonstrated through extensive experiments on both public and newly constructed photometric stereo datasets, where it outperforms nine competitive schemes in delivering promising surface geometry details.