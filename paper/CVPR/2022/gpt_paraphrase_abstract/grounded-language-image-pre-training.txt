This study introduces a model called grounded language-image pre-training (GLIP) that aims to learn visual representations that are object-level, language-aware, and semantic-rich. GLIP combines object detection and phrase grounding in its pre-training process, which brings two advantages. Firstly, it allows GLIP to learn from both detection and grounding data, improving both tasks and establishing a strong grounding model. Secondly, GLIP can utilize a large number of image-text pairs to generate grounding boxes in a self-training manner, resulting in semantic-rich representations. In the experiments conducted, GLIP is pre-trained on 27 million grounding data, consisting of 3 million human-annotated pairs and 24 million web-crawled pairs. The learned representations demonstrate impressive zero-shot and few-shot transferability to various object-level recognition tasks. For instance, when directly evaluated on COCO and LVIS datasets without any exposure to COCO images during pre-training, GLIP achieves 49.8 AP and 26.9 AP, respectively, outperforming several supervised baselines. After fine-tuning on COCO, GLIP achieves 60.8 AP on the validation set and 61.5 AP on the test-dev set, surpassing the state-of-the-art. Additionally, when applied to 13 downstream object detection tasks, a 1-shot GLIP performs comparably to a fully-supervised Dynamic Head. The code for GLIP will be made available at https://github.com/microsoft/GLIP.