This paper presents a new approach to Audiovisual Event (AVE) localization, which involves jointly localizing events by analyzing audio and visual information. The challenge in unconstrained videos is that both types of information can be inconsistent or affected by background noise. To address this issue, the paper proposes a novel cross-modal background suppression network for AVE tasks. This network operates at both the time and event levels to improve localization performance by suppressing asynchronous audiovisual background frames and reducing redundant noise.At the time-level, the background suppression scheme focuses the audio and visual modalities on relevant information in the temporal dimension, disregarding segments that the other modality considers as background. This helps to refine the localization process. At the event-level, the background suppression scheme utilizes class activation sequences predicted by both audio and visual modalities to control the final event category prediction. This effectively suppresses noise events that may occur accidentally in a single modality.Additionally, the paper introduces a cross-modal gated attention scheme to extract relevant visual regions from complex scenes. This scheme leverages both global visual and audio signals to identify the most informative regions.Extensive experiments demonstrate that the proposed method significantly outperforms state-of-the-art approaches in both supervised and weakly supervised AVE settings. This highlights the effectiveness of the cross-modal background suppression network in improving AVE localization.