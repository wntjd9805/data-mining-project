This study focuses on online class-incremental continual learning, which involves learning new classes from a continuous data stream while retaining knowledge of old classes. Existing methods that use replay-based techniques have shown promise but have limitations in terms of sample selection and exploration of semantic information. Therefore, this paper proposes a new framework that addresses these limitations. The framework includes a gradient-based sample selection strategy that prioritizes stored samples whose gradients are most affected by new incoming samples. This strategy allows for more effective updates to the neural network. Additionally, the study explores the semantic information between different views of training images by maximizing their mutual information, which improves classification accuracy. The proposed method outperforms existing approaches in various benchmark datasets. The code for the method is available on GitHub.