Defocus deblurring is a challenging task because defocus blur varies spatially. While deep learning has shown promise in solving image restoration problems, accurate training data for defocus deblurring is difficult to collect. Naive two-shot capturing cannot achieve pixel-wise correspondence between defocused and all-in-focus image pairs. Synthetic aperture of light fields is suggested as a more reliable way to generate accurate image pairs. However, the defocus blur generated from light field data differs from that of traditional digital camera images. This paper proposes a novel deep defocus deblurring network that leverages the strengths and overcomes the shortcomings of light fields. The network is first trained on a light field-generated dataset for highly accurate image correspondence. Then, it is fine-tuned using feature loss on a dataset collected by the two-shot method to reduce differences between the defocus blur in the two domains. This strategy proves highly effective and achieves state-of-the-art performance on multiple test sets. Extensive ablation studies analyze the effect of each network module on the final performance.