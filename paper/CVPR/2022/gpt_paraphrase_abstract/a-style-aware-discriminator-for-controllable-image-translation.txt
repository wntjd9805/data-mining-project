Current image-to-image translation methods lack control over the output domain and struggle to interpolate between different domains, resulting in unrealistic outcomes. This limitation stems from the absence of semantic distance consideration in labels. To address this issue, we introduce a style-aware discriminator that acts as both a critic and a style encoder, offering conditions for translation. Through prototype-based self-supervised learning, the style-aware discriminator learns a manageable style space and guides the generator simultaneously. Experimental results on multiple datasets demonstrate the superiority of our proposed model compared to current state-of-the-art image-to-image translation methods. Unlike existing approaches, our method supports diverse applications such as style interpolation, content transplantation, and local image translation. The code for our model is available at github.com/kunheek/style-aware-discriminator.