We present a method called Behavioral Keypoint Discovery (B-KinD) that can learn the posture and structure of agents by analyzing unlabeled behavioral videos. Our approach utilizes an encoder-decoder architecture with a geometric bottleneck to reconstruct the spatiotemporal difference between frames in the video. Instead of relying on manual annotations, B-KinD focuses solely on areas of movement in the video, allowing it to directly analyze input videos. We conducted experiments on various agent types, including mice, flies, humans, jellyfish, and trees, which demonstrated the versatility of our method. Moreover, the keypoints discovered by B-KinD represent meaningful body parts and achieve state-of-the-art performance in keypoint regression among self-supervised methods. Remarkably, B-KinD also performs comparably to supervised keypoints on downstream tasks like behavior classification, indicating that our method can significantly reduce the costs associated with model training when compared to supervised approaches.