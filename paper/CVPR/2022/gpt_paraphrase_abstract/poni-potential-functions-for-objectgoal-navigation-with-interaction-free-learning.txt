We present PONI (Potential functions for ObjectGoalNavigation with Interaction-free learning), a modular approach to ObjectGoal navigation (ObjectNav) that separates the skills of "where to look?" for an object and "how to navigate to (x, y)?". Our key insight is that the "where to look?" skill can be treated as a perception problem and learned without interacting with the environment. To address this, we propose a network that predicts two complementary potential functions based on a semantic map and uses them to determine where to look for an unseen object. We train the potential function network using supervised learning on a passive dataset of top-down semantic maps and integrate it into a modular framework for ObjectNav. Our experiments on Gibson and Matterport3D datasets show that our method achieves state-of-the-art performance in ObjectNav while requiring significantly less computational resources for training (up to 1,600Ã— less). We provide the code and pre-trained models for reproducibility.