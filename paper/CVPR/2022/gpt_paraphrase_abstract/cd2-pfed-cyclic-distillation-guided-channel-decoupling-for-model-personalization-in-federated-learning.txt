Federated learning (FL) is a distributed learning approach that allows multiple clients to collaborate and learn a shared global model. However, dealing with heterogeneous data clients is still a challenge, as the different data distributions often hinder the global model from performing well on each client. In this paper, we present CD2-pFed, a new framework called Cyclic Distillation-guided Channel Decoupling, which personalizes the global model in FL under various data heterogeneity settings. Unlike previous methods that personalize the model at the layer level to handle non-IID data across clients, we introduce channel-wise assignment for model personalization, known as channel decoupling. To enhance collaboration between private and shared weights, we propose a cyclic distillation scheme that enforces consistent regularization between local and global model representations during the federation. Guided by this cyclical distillation, our channel decoupling framework achieves more accurate and generalized results for different types of heterogeneity, such as feature skew, label distribution skew, and concept shift. We conduct comprehensive experiments on four benchmark tasks, including natural image and medical image analysis, and demonstrate the consistent effectiveness of our method through both local and external validations.