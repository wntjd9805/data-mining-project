Neural Architecture Search (NAS) has become increasingly popular for automatically designing deep neural networks. Differential NAS methods like DARTS have gained attention due to their search efficiency. However, they face two main challenges: performance collapse and poor generalization. To address these issues, we propose a simple yet effective regularization technique called Beta-Decay. This method imposes constraints on the values and variances of activated architecture parameters to prevent them from becoming too large. We provide a detailed theoretical analysis of how and why Beta-Decay works. Experimental results on NAS-Bench-201 demonstrate that our approach helps stabilize the search process and improves the transferability of the resulting networks across different datasets. Our search scheme also exhibits a notable characteristic of being less reliant on training time and data. We validate the effectiveness of our method through comprehensive experiments on various search spaces and datasets. The code for our approach is available at https://github.com/Sunshine-Ye/Beta-DARTS.