We introduce a novel system called NPBG++ that addresses the task of novel view synthesis (NVS) by achieving realistic rendering while minimizing scene fitting time. Our approach efficiently utilizes multiview observations and the point cloud of a static scene to predict a neural descriptor for each point. This improves upon the existing Neural Point-Based Graphics (NPBG) pipeline in several important ways. By predicting descriptors in a single pass through the source images, we eliminate the need for per-scene optimization. Additionally, our method allows for view-dependent neural descriptors that are better suited for scenes with non-Lambertian effects. Through comparisons, we demonstrate that our proposed system outperforms previous NVS approaches in terms of fitting and rendering runtimes, while maintaining similar image quality. For more details, please visit our project page at https://rakhimovv.github.io/npbgpp/.