Learning to synthesize data has become a promising approach in zero-shot quantization (ZSQ), where neural networks are represented by low-bit integers without accessing real data. However, existing methods in this field fail to maintain the intra-class heterogeneity observed in real data, resulting in limited performance improvement. To address this issue, we propose a novel zero-shot quantization method called IntraQ. Our approach includes a local object reinforcement technique that places target objects at various scales and positions in synthetic images. We also introduce a marginal distance constraint to ensure that class-related features are distributed in a coarse area. Additionally, we devise a soft inception loss that incorporates a soft prior label to prevent over-fitting of synthetic images to a specific object. Our IntraQ method effectively retains the intra-class heterogeneity in synthetic images and achieves state-of-the-art performance. For instance, when quantizing all layers of MobileNetV1 to 4-bit, IntraQ achieves a 9.17% increase in top-1 accuracy on ImageNet compared to advanced ZSQ methods. The code for IntraQ is available at https://github.com/zysxmu/IntraQ.