Transfer learning from pre-trained models on large-scale datasets is crucial for various computer vision tasks. However, recent research has demonstrated that commonly used datasets like ImageNet have weak labels, as images containing multiple object classes are assigned only a single label. This labeling ambiguity introduces bias in models, leading to a single prediction and potentially suppressing classes that co-occur in the data. Inspired by language emergence studies, we propose a novel approach called multi-label iterated learning (MILe) to address this issue. MILe leverages the inductive biases of multi-label learning from single labels by employing an iterated learning framework. It is a straightforward yet highly effective procedure that constructs a multi-label description of the image by propagating binary predictions through successive generations of teacher and student networks, with a learning bottleneck. Our experiments demonstrate that MILe consistently outperforms standard training procedures, even when fine-tuning from self-supervised weights, as evidenced by improved ImageNet accuracy and ReaL F1 score. Furthermore, MILe proves to be effective in reducing label noise, achieving state-of-the-art performance on real-world large-scale noisy data such as WebVision. Additionally, MILe demonstrates improved performance in class incremental settings like IIRC and exhibits robustness to distribution shifts. The code for implementing MILe is available at: https://github.com/rajeswar18/MILe.