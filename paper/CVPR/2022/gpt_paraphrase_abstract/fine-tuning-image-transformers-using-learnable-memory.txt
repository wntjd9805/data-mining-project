This paper suggests enhancing Vision Transformer models by incorporating learnable memory tokens. By introducing these tokens at each layer, the model can effectively adapt to new tasks using minimal parameters, while also maintaining its performance on previously learned tasks if desired. This approach significantly improves accuracy compared to conventional fine-tuning methods that only focus on the model's head, and it performs slightly below the more costly full fine-tuning. Additionally, the paper introduces an attention-masking technique that allows the model to handle new tasks by reusing computations. This setup enables efficient parameter usage and allows the model to perform both old and new tasks within a single inference at a low additional cost.