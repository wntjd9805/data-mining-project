We introduce a new multimodal architecture called Layout-Aware Transformer (LaTr) for SceneText Visual Question Answering (STVQA). STVQA requires models to reason over different modalities, and we investigate the impact of each modality. We find that the language module, especially when enriched with layout information, plays a crucial role. Taking this into account, we propose a pre-training scheme that only requires text and spatial cues. We demonstrate that pre-training on scanned documents has advantages over using natural images, despite the domain gap. Scanned documents are easily accessible, contain dense text, and have various layouts, which helps the model learn spatial cues by combining language and layout information. Compared to existing approaches, our method allows vocabulary-free decoding and generalizes well beyond the training vocabulary. Additionally, LaTr improves robustness to OCR errors, a common cause of failure in STVQA. By utilizing a vision transformer, we eliminate the need for an external object detector. LaTr surpasses state-of-the-art STVQA methods on multiple datasets, achieving a +7.6% accuracy improvement on TextVQA, +10.8% on ST-VQA, and +4.0% on OCR-VQA. The answer format outputs only the abstraction.