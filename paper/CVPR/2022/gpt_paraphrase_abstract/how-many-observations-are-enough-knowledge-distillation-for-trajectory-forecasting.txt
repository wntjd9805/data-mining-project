Accurate prediction of human positions is a crucial task for modern video-surveillance systems. Currently, the most advanced models rely on a history of past tracked locations to predict future positions. However, this approach overlooks important factors in real-world scenarios. In crowded scenes, errors in detection and tracking can accumulate, resulting in tracking drifts. This means that the model is fed with corrupted and noisy data, which significantly affects its prediction performance.  To address this issue, we aim to achieve accurate predictions using only a few input observations. By doing so, we can reduce the risks associated with automatic perception. Our approach involves a novel distillation strategy that transfers knowledge from a teacher network to a student network. The student network is fed with just two observations, but with the guidance from the teacher network, it can perform on par with state-of-the-art methods that require more observations.  Furthermore, extensive experiments on commonly used trajectory forecasting datasets demonstrate that our student network exhibits better generalization to unseen scenarios. Overall, our approach improves the accuracy of human position prediction in video-surveillance systems while minimizing the amount of input data required.