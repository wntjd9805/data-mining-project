Creating generative models that are aware of 3D space is a challenging task that connects the 2D image domain with the physical 3D world. Recent efforts have combined a Generative Adversarial Network (GAN) with a Neural Radiance Field (NeRF) as a 3D prior. However, the NeRF's implicit function has limited awareness of the global structure, hindering the generator's ability to capture it effectively. Additionally, NeRF relies on volume rendering, which can be computationally expensive and hampers the generation of high-resolution results, making optimization more difficult. To address these issues, we propose VolumeGAN, a novel framework for high-fidelity 3D-aware image synthesis. VolumeGAN explicitly learns both a structural representation and a textural representation. We first learn a feature volume to depict the underlying structure, which is then transformed into a feature field resembling NeRF's approach. The feature field is further condensed into a 2D feature map, acting as the textural representation, and combined with a neural renderer for generating appearance. This framework allows for independent control over shape and appearance. Further details and the project page can be found at https://genforce.github.io/volumegan.