Computer vision systems currently have specific purposes and are designed and trained for predefined tasks. Adapting these systems to new tasks is difficult and often requires significant changes to the network architecture or training process. To address this, we aim to develop general purpose vision systems that can learn and perform a variety of tasks without any modifications to the architecture or learning process. This paper introduces GPV-1, a task-agnostic vision-language architecture capable of learning and performing tasks involving image input and text/bounding box output, such as classification, localization, visual question answering, and captioning. We also propose evaluations to assess the architecture's generality, skill-concept transfer, and learning efficiency, which can guide future research on general purpose vision. Our experiments demonstrate that GPV-1 is effective across multiple tasks, utilizes shared concept knowledge, achieves zero-shot performance in the Referring Expressions task, and further improves performance with minimal training samples.