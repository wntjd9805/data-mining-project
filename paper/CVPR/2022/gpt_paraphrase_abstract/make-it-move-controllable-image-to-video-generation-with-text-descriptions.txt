Proposed is a novel video generation task called Text-Image-to-Video generation (TI2V) that aims to generate controllable videos based on user intentions. This task involves generating videos from a static image and a text description while addressing challenges related to aligning appearance and motion from different modalities and handling uncertainty in text descriptions. To overcome these challenges, a Motion Anchor-based video Generator (MAGE) is introduced, which utilizes a motion anchor structure to store aligned representation of appearance and motion. The MAGE model incorporates explicit condition and implicit randomness to model uncertainty and increase diversity. By utilizing three-dimensional axial transformers, the motion anchor interacts with the given image to recursively generate next frames with controllability and diversity. To evaluate the effectiveness of MAGE and TI2V, two new video-text paired datasets based on MNIST and CATER are created. Experimental results on these datasets demonstrate the effectiveness of MAGE and the promising potential of the TI2V task. The datasets can be accessed at https://github.com/Youncy-Hu/MAGE.