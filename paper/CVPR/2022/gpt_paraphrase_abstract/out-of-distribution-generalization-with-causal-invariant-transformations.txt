In practical applications, it is crucial to have a model that performs well on data outside of its training distribution. Causality has recently emerged as a powerful approach to address this issue, as it relies on the invariant causal mechanism that exists across different domains. However, existing methods either assume a linear causal feature or require a large number of diverse training domains, which is often impractical. In this study, we propose a novel approach that does not explicitly recover the causal feature but instead focuses on modifying the non-causal feature while preserving the causal part. This can be achieved by either prior knowledge or learning from the training data in a multi-domain scenario. We theoretically demonstrate that if all these transformations are available, we can learn an optimal model for all domains using only single domain data. However, since obtaining a complete set of causal invariant transformations may be challenging, we show that knowing a subset of them is sufficient. Based on these findings, we propose a regularized training procedure to enhance the model's ability to generalize to out-of-distribution data. Extensive experiments on both synthetic and real datasets validate the effectiveness of our algorithm, even when only a few causal invariant transformations are known.