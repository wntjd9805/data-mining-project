We introduce a novel framework utilizing a cross-modal Transformer to address zero-shot action recognition (ZSAR) by jointly encoding video data and text labels. Our approach incorporates visual-semantic associations during the learning process, enabling visual representations to be learned alongside semantic representations in a shared knowledge space. This promotes the discriminative nature of learned visual embeddings while maintaining semantic consistency. To tackle zero-shot inference, we propose a straightforward semantic transfer scheme that combines semantic relatedness information between seen and unseen classes to generate composite unseen visual prototypes. This preserves and leverages the discriminative features within the visual structure, mitigating common challenges in zero-shot learning such as information loss, semantic gap, and the hubness problem. Notably, our model achieves significant improvements in ZSAR compared to existing methods on benchmark datasets including UCF101, HMDB51, and ActivityNet, without the need for pre-training on additional datasets. We will provide the code for reproducibility.