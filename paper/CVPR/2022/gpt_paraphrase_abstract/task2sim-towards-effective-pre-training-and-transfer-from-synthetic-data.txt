This paper explores the use of synthetic data generated by graphics simulators for pre-training models in computer vision tasks. The authors find that different configurations of simulation parameters are beneficial for different downstream tasks, indicating the need for tailored synthetic pre-training data. To address this, they propose Task2Sim, a model that maps downstream task representations to optimal simulation parameters for generating synthetic pre-training data. Task2Sim is trained on a set of "seen" tasks and can then predict the best simulation parameters for novel "unseen" tasks without additional training. Extensive experiments with 20 diverse downstream tasks demonstrate that Task2Sim's task-adaptive pre-training data yields significantly improved performance compared to non-adaptive approaches on both seen and unseen tasks. It even shows competitive performance compared to pre-training on real images from Imagenet.