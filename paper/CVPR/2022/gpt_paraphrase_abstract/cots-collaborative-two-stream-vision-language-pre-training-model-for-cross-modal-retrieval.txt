Large-scale pre-training models have shown impressive results in image-text retrieval. However, these models suffer from low inference efficiency due to heavy attention layers. Recently, two-stream methods like CLIP and ALIGN have emerged as efficient alternatives, but they only focus on instance-level alignment between the image and text streams, leaving room for improvement. To address these limitations, we propose a new vision-language pre-training model called COTS, which enhances cross-modal interaction for image-text retrieval.In addition to instance-level alignment achieved through momentum contrastive learning, our COTS model incorporates two additional levels of cross-modal interaction. Firstly, we introduce token-level interaction by devising a masked vision-language modeling (MVLM) learning objective. This objective does not rely on a cross-stream network module and instead applies a variational autoencoder to the visual encoder, generating visual tokens for each image. Secondly, we implement task-level interaction by introducing a KL-alignment learning objective between text-to-image and image-to-text retrieval tasks. This objective computes the probability distribution per task using negative queues in momentum contrastive learning.Through fair comparisons, our COTS model achieves the highest performance among all two-stream methods while maintaining comparable performance to the latest single-stream methods. Furthermore, our model demonstrates a significantly faster inference speed, being 10,800 times faster than the single-stream methods. Notably, our COTS model can also be applied to text-to-video retrieval and achieves state-of-the-art results on the widely-used MSR-VTT dataset.