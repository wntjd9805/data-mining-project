Training machine learning models with a focus on difficult aspects of the data has been shown to improve their generalization, particularly in scenarios where robustness across distributions is important. Previous research has explored this concept of "hard-to-learn" either in terms of the samples or the features. This paper proposes a simple approach that combines both dimensions, emphasizing the worst-case scenarios in both the sample and feature dimensions. This method, called W2D (Worst-case along Two Dimensions), is shown to be effective through empirical experiments on standard benchmarks.