This paper introduces TokenFusion, a method specifically designed for transformer-based vision tasks that involve multiple modalities of data. While previous adaptations of transformers have focused on single-modal vision tasks, this method aims to improve performance by effectively fusing multiple modalities. TokenFusion achieves this by dynamically identifying uninformative tokens and replacing them with projected and aggregated inter-modal features. Additionally, residual positional alignment is utilized to explicitly utilize inter-modal alignments after fusion. The proposed design enables the transformer to learn correlations among multimodal features while preserving the integrity of the single-modal transformer architecture. Extensive experiments conducted on various homogeneous and heterogeneous modalities demonstrate that TokenFusion outperforms state-of-the-art methods in three typical vision tasks: multimodal image-to-image translation, RGB-depth semantic segmentation, and 3D object detection using point cloud and images. The code for TokenFusion will be made publicly available.