Monocular 3D object detection in autonomous driving is a difficult task. Some current methods use depth information from a depth estimator to aid in 3D detection, but this leads to increased computational load and limited accuracy due to imprecise depth priors. To address this issue, we propose MonoDTR, a new depth-aware transformer network for monocular 3D object detection. MonoDTR consists of two main components: (1) the Depth-Aware Feature Enhancement (DFE) module, which learns depth-aware features with auxiliary supervision without adding extra computation, and (2) the Depth-Aware Transformer (DTR) module, which integrates context- and depth-aware features on a global scale. Additionally, we introduce a novel depth positional encoding (DPE) to inject depth positional hints into transformers instead of using traditional pixel-wise positional encodings. Our depth-aware modules can be easily incorporated into existing image-based monocular 3D object detectors to enhance performance. Extensive experiments on the KITTI dataset show that our approach surpasses previous state-of-the-art monocular-based methods and achieves real-time detection. The code for our method is available at https://github.com/kuanchihhuang/MonoDTR.