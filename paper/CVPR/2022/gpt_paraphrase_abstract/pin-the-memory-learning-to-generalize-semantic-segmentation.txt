Deep neural networks have made significant advancements in semantic segmentation. However, models trained on one domain often struggle to perform well in new and challenging domains, highlighting the need for improved generalization capabilities. In this study, we introduce a novel approach to domain generalization in semantic segmentation using a meta-learning framework. Our method utilizes a categorical memory to store the conceptual knowledge of semantic classes, which remains consistent across different domains. By repeatedly training memory-guided networks and conducting virtual tests, we aim to learn how to memorize domain-agnostic and distinct class information, while also providing an externally established memory as a guide to reduce representation ambiguity in test data from unseen domains. Additionally, we propose memory divergence and feature cohesion losses to facilitate category-aware domain generalization by encouraging the learning of memory reading and updating processes. We conducted extensive experiments on various benchmarks for semantic segmentation, which demonstrated the superior generalization capabilities of our method compared to state-of-the-art approaches.