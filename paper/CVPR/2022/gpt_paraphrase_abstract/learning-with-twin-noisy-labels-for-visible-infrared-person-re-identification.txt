This paper addresses the problem of TwinNoise Labels (TNL) in visible-infrared person re-identification (VI-ReID), which refers to the presence of noisy annotation and correspondence. The complexity of data collection and annotation, such as poor recognizability in the infrared modality, leads to incorrect identity annotations. These wrongly annotated data not only contaminate the correspondence within a single modality but also affect cross-modal correspondence. To tackle the TNL problem, the authors propose a novel method called DuAlly Robust Training (DART) for robust VI-ReID. DART first calculates the clean confidence of annotations using the memorization effect of deep neural networks. Then, the method rectifies the noisy correspondence based on the estimated confidence and divides the data into four groups for further utilization. Finally, DART employs a dually robust loss, which consists of a soft identification loss and an adaptive quadruplet loss, to achieve robustness against noisy annotation and correspondence. The effectiveness of the proposed method is validated through extensive experiments on the SYSU-MM01 and RegDB datasets, comparing it with five state-of-the-art methods. The code for the proposed method is available at https://github.com/XLearning-SCU/2022-CVPR-DART.