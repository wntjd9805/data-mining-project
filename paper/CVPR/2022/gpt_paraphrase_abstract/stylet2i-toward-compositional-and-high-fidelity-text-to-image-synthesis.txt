Previous methods for text-to-image synthesis have limitations in generalizing to unseen or underrepresented attribute compositions in input text. This lack of compositionality can have negative effects on robustness and fairness, such as the inability to generate face images of underrepresented demographic groups. To address this issue, we propose a new framework called StyleT2I. Our approach includes a CLIP-guided Contrastive Loss that improves the distinction between different compositions in sentences. Additionally, we introduce a Semantic Matching Loss and a Spatial Constraint to identify latent directions for attribute manipulation, resulting in better disentangled representations of attributes. With the identified latent directions, we propose Compositional Attribute Adjustment to enhance the compositionality of image synthesis. Furthermore, we utilize â„“2-norm regularization of latent directions to balance image-text alignment and image fidelity. To evaluate the compositionality of text-to-image synthesis models, we create a new dataset split and evaluation metric. Experimental results demonstrate that StyleT2I surpasses previous approaches in terms of consistency between input text and synthesized images, as well as achieving higher fidelity.