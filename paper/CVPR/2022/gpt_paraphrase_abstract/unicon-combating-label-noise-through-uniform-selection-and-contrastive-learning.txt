Supervised deep learning methods rely on annotated data, leading to the presence of label noise. This noise negatively affects the generalization performance of deep neural networks. To address this issue, recent methods have implemented sample selection mechanisms to choose a potentially clean subset of data. However, our analysis reveals that these current selection methods tend to favor easier classes while disregarding harder ones, resulting in a class imbalance in the selected clean set and a decline in performance under high label noise. In this study, we propose UNICON, a straightforward yet effective sample selection method that is robust to high label noise. To address the disproportionate selection of easy and hard samples, we introduce a uniform selection mechanism based on Jensen-Shannon divergence, which does not require probabilistic modeling or hyperparameter tuning. Additionally, we utilize contrastive learning to further combat the memorization of noisy labels. Through extensive experimentation on various benchmark datasets, we demonstrate the effectiveness of UNICON, achieving an 11.4% improvement over the current state-of-the-art on the CIFAR100 dataset with a 90% noise rate. Our code is publicly available.