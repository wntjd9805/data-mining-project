This paper investigates the possibility of identifying an optimal sub-model within a vision transformer and introduces a framework called ViT-Slim for slimming down the vision transformer. The framework allows for the search of a sub-structure from the original model across different dimensions, such as input tokens, MHSA, and MLP modules, while still achieving state-of-the-art performance. Our approach is based on a learnable and unified sparsity constraint with predefined factors that capture the overall importance in the continuous search space of various dimensions. The search process is highly efficient, thanks to a single-shot training scheme. For example, using DeiT-S, ViT-Slim requires only around 43 GPU hours for the search process, and the resulting structure can have diverse dimensionalities in different modules. After that, a budget threshold is applied based on the desired trade-off between accuracy and FLOPs (floating-point operations) on the target devices, followed by a re-training process to obtain the final model. Extensive experiments demonstrate that ViT-Slim can compress up to 40% of parameters and 40% of FLOPs in different vision transformers, while improving accuracy by approximately 0.6% on the ImageNet dataset. Furthermore, our searched models show advantages on various downstream datasets. The code for ViT-Slim is available at https://github.com/Arnav0400/ViT-Slim.