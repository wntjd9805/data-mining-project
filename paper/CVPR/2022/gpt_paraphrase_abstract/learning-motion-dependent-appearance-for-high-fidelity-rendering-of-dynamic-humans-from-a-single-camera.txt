The appearance of dressed humans undergoes a complex geometric transformation influenced by both static poses and their dynamics. Existing human rendering methods often neglect the effect of motion on clothing, resulting in physically unrealistic motion. The challenge lies in learning the dynamics of appearance, which requires a large amount of observations. To address this, we propose a compact motion representation that enforces equivariance, meaning that the representation transforms in the same way as the pose. We develop an equivariant encoder that generates a generalizable representation using spatial and temporal derivatives of the 3D body surface. This representation is then decoded by a compositional multi-task decoder, which produces high-quality, time-varying appearance. Our experiments demonstrate that our method can generate coherent videos of dynamic humans, even for unseen body poses and novel viewpoints using only a single-view video.