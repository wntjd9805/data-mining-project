Generative Adversarial Networks (GANs) have revolutionized image generation, but generating content with specific attributes remains a challenge. Annotating datasets with desired attributes is time-consuming, so it is important to introduce control into unsupervised generative models. In this study, we focus on controllable image generation using well-trained GANs. We find that the intermediate layers of the generator form clusters that separate data based on semantically meaningful attributes like hair color and pose. By conditioning on these clusters, we can control the semantic class of the generated image. Our approach allows sampling from each cluster using Implicit Maximum Likelihood Estimation (IMLE). We demonstrate the effectiveness of our method on various datasets including faces, animals, and objects. Our results show that our approach can condition image generation on attributes like gender, pose, and hair style for faces, as well as various features for different object classes.