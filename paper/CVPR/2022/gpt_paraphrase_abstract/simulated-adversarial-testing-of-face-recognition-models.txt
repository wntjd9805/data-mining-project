Most machine learning models are typically evaluated on fixed datasets, which may not provide a comprehensive understanding of the model's strengths and weaknesses. These weaknesses often become apparent when the model is deployed in real-world scenarios. The consequences of such failures can range from financial losses to loss of time or even loss of life in critical applications. To address this issue, we propose a framework that utilizes simulators to finely control and explore the semantic image manifold, using interpretable parameters. Our framework aims to test machine learning algorithms in an adversarial manner, identifying weaknesses in the model before it is deployed in critical scenarios. We apply this framework to a face recognition system and demonstrate that simulated samples can reveal weaknesses in models trained on real data. By employing our method, we are able to identify adversarial synthetic faces that deceive contemporary face recognition models. This highlights the presence of weaknesses in these models that are not captured by commonly used validation datasets. We propose that these types of adversarial examples are not isolated incidents, but rather exist within connected regions in the latent space of the simulator. We present a method to identify these adversarial regions, which differs from the traditional approach of identifying individual adversarial points commonly found in the literature on adversarial examples.