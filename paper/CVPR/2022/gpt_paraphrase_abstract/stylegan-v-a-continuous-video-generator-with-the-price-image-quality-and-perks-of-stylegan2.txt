Most video synthesis frameworks treat videos as discrete events, but this work proposes a continuous-time video generator that views videos as continuous signals. The approach involves designing continuous motion representations using positional embeddings and training on sparse videos with as few as 2 frames per clip. Instead of using traditional image and video discriminators, a holistic discriminator is developed that concatenates frames' features to aggregate temporal information. This reduces training costs and provides a richer learning signal to the generator. The model is built on StyleGAN2 and achieves similar image quality with just a 5% increase in training cost. Additionally, the latent space of the model allows for spatial manipulations that can be propagated over time. Unlike previous methods that struggle to generate even 64 frames at a fixed rate, this model can generate arbitrarily long videos at arbitrary high frame rates. The model is evaluated on several video synthesis benchmarks and outperforms the closest competitor by an average of 30% in terms of metrics. The project website can be found at https://universome.github.io/stylegan-v.