CWTFederated learning is a research approach that allows multiple organizations to collaboratively train machine learning models without sharing their private data. However, there are still challenges in this field, such as the lack of convergence and the risk of catastrophic forgetting when dealing with different devices. This study investigates the use of self-attention-based architectures, specifically Transformers, to address these challenges and enhance federated learning with heterogeneous data. The researchers conducted extensive experiments using various neural architectures, federated algorithms, real-world benchmarks, and different data splits. The results demonstrate that replacing convolutional networks with Transformers significantly reduces catastrophic forgetting, speeds up convergence, and produces better global models, particularly in the context of heterogeneous data. The code and pretrained models used in this study have been made publicly available to encourage further exploration of robust architectures for federated learning.