This study focuses on the task of dataset distillation, which involves creating a smaller dataset that can produce a model with similar accuracy to one trained on a full dataset. The authors propose a new approach that optimizes the distilled data to guide networks towards a comparable state as those trained on real data. The method involves training a network on the distilled data for multiple iterations and optimizing the data based on the difference between the parameters of the synthetically trained network and those trained on real data. To handle large-scale datasets, the authors pre-compute and store training trajectories of expert networks trained on the real dataset. The proposed method outperforms existing methods and also enables distillation of higher-resolution visual data.