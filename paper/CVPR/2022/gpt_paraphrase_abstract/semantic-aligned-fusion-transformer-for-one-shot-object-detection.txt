The performance of current approaches in one-shot object detection, which aims to detect novel objects with limited data, is often unsatisfactory. This is attributed to inappropriate correlation methods that do not consider spatial structures and scale variances. To address this issue, we propose a simple yet effective architecture called Semantic-aligned Fusion Transformer (SaFT) that leverages the attention mechanism. SaFT includes a vertical fusion module (VFM) for enhancing cross-scale semantics and a horizontal fusion module (HFM) for fusing features from different samples. These modules enable semantic-aligned associations by broadening the vision of each feature point. Extensive experiments on multiple benchmarks demonstrate the superiority of our framework. Even without fine-tuning on novel classes, our approach significantly improves the performance of one-stage baselines, pushing state-of-the-art results to a higher level.