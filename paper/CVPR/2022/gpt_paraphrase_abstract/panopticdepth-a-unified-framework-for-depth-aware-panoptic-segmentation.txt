This paper introduces a comprehensive framework for depth-aware panoptic segmentation (DPS), which aims to reconstruct a 3D scene with instance-level semantics from a single image. Previous approaches have simply added a dense depth regression head to panoptic segmentation networks, resulting in two separate task branches. However, this disregards the potential benefits of the interplay between these tasks, leading to suboptimal depth maps and the failure to leverage instance-level semantic cues to improve depth accuracy. To overcome these limitations, we propose a unified framework for DPS by employing dynamic convolution techniques in both the panoptic segmentation and depth prediction tasks. Instead of predicting depth for all pixels simultaneously, our approach generates instance-specific kernels to predict depth and segmentation masks for each instance. Additionally, by incorporating an instance-wise depth estimation scheme, we introduce a new depth loss that utilizes instance-level depth cues to supervise the depth learning process. Through extensive experiments on Cityscapes-DPS and SemKITTI-DPS datasets, we demonstrate the effectiveness and potential of our method. We believe that our unified solution for DPS can bring about a new paradigm in this field. The source code for our method is available at https://github.com/NaiyuGao/PanopticDepth.