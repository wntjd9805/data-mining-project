Image segmentation is a crucial task in combining vision and language, aiming to identify and extract specific objects from an image based on natural language descriptions. The main challenge lies in effectively utilizing the referring expression to highlight relevant positions in the image. Previous methods have used a vision-language decoder to merge features extracted from separate vision and language encoders. However, in this study, we propose a different approach that demonstrates significantly improved cross-modal alignments by fusing linguistic and visual features at intermediate layers of a vision Transformer encoder network. By integrating these features early on, we can leverage the power of Transformer encoders to capture useful multi-modal context, leading to accurate segmentation results with a lightweight mask predictor. Our method outperforms previous state-of-the-art techniques on RefCOCO, RefCOCO+, and G-Ref datasets by a large margin. Figure 1 illustrates the task of referring image segmentation, showcasing the difference between our proposed method and the previous approach. Our method directly incorporates linguistic information into visual features at intermediate levels, allowing for joint exploitation of vision-language cues and eliminating the need for a complex cross-modal decoder.