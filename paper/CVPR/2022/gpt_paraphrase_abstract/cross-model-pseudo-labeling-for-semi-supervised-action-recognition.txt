Semi-supervised action recognition is a challenging task due to the high cost of data annotation. One common approach is to assign pseudo-labels to unlabeled data, obtained from training a model on labeled data and using confident predictions. This study proposes a more effective pseudo-labeling scheme called Cross-Model Pseudo-Labeling (CMPL). It introduces a lightweight auxiliary network in addition to the primary backbone, where both models predict pseudo-labels for each other. These models learn complementary representations from the same video clips due to their different structural biases, benefiting from cross-model predictions as supervision. Experimental results using different data partition protocols demonstrate the significant improvement of CMPL over existing alternatives. For instance, CMPL achieves 17.6% and 25.1% Top-1 accuracy on Kinetics-400 and UCF-101 using only RGB modality and 1% labeled data, outperforming the baseline model, FixMatch, by 9.0% and 10.3% respectively.