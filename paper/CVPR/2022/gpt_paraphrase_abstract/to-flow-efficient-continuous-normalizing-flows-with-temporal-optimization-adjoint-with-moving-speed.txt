Continuous normalizing flows (CNFs) are a technique that create reversible mappings between complex distributions and Gaussian distributions using neural ordinary differential equations (neural ODEs). However, applying CNFs to large datasets has been challenging due to the increasing complexity of training neural ODEs. Recent research has introduced the use of optimal transport theory to regulate the dynamics of the ODE and accelerate training. This paper proposes a new approach called temporal optimization, which optimizes the evolutionary time for forward propagation during neural ODE training. By optimizing the network weights of the CNF in conjunction with the evolutionary time using coordinate descent, the proposed approach ensures stability of the evolution through temporal regularization. Experimental results demonstrate that this approach significantly speeds up training without sacrificing performance compared to baseline models.