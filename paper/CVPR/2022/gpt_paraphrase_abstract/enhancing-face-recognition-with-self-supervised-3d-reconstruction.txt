Face recognition has made significant progress in recent years, but it still faces challenges in dealing with variations in illumination and pose. To address this, we propose a method that enhances face recognition by incorporating self-supervised 3D reconstruction. This approach focuses on extracting identity-related depth and albedo information, while disregarding pose and illumination information that is not relevant to identity. Inspired by the physical model of image formation, we improve the face recognition network by introducing a 3D face reconstruction loss with two auxiliary networks. One network estimates pose and illumination from the input face image, while the other network decodes canonical depth and albedo from the intermediate features of the face recognition network. The entire network is trained end-to-end, combining classic face identification loss with the loss of 3D face reconstruction. This self-supervised reconstruction acts as a regularization that enables the recognition network to understand faces in a 3D view and encode more intrinsic information about facial depth and albedo. Extensive experiments on face recognition benchmarks demonstrate that our method outperforms state-of-the-art approaches without requiring additional annotations or computations. Additionally, the learned representations generalize well to other face-related tasks, such as facial attribute recognition with limited labeled data.