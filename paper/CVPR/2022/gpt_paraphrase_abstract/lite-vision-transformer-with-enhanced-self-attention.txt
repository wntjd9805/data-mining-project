Current lightweight vision transformer models still struggle with inconsistent and incorrect predictions in local regions. We believe that their self-attention mechanism is limited in shallower and thinner networks. To address this issue, we propose a new lightweight transformer network called Lite Vision Transformer (LVT). LVT incorporates two enhanced self-attention mechanisms to improve model performance for mobile deployment.For low-level features, we introduce Convolutional Self-Attention (CSA). Unlike previous methods that merge convolution and self-attention, CSA integrates local self-attention into the convolution process using a 3Ã—3 kernel. This enriches low-level features in the initial stage of LVT.For high-level features, we propose Recursive Atrous Self-Attention (RASA). RASA leverages multi-scale context during similarity map calculation and employs a recursive mechanism to enhance representation capability with minimal additional parameters.We demonstrate the superiority of LVT through experiments on ImageNet recognition, ADE20K semantic segmentation, and COCO panoptic segmentation. The code for LVT is publicly available.