The task of finding connections between images is difficult, especially when there are significant changes in appearance due to different perspectives or variations within the same category. This study introduces a powerful semantic image matching system called TransforMatcher, which is based on the successful use of transformer networks in visual tasks. Unlike existing methods that use convolution or attention, TransforMatcher employs global match-to-match attention to accurately locate matches and dynamically refine them. To handle a large number of matches in a dense correlation map, a lightweight attention architecture is developed to consider global match-to-match interactions. Additionally, a multi-channel correlation map is used for refinement, treating the scores at different levels as features rather than a single score to fully utilize the richer layer-wise semantics. Experimental results show that TransforMatcher achieves state-of-the-art performance on the SPair-71k dataset and performs comparably to existing methods on the PF-PASCAL dataset.