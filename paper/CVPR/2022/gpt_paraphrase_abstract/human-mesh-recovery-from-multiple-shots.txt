Videos from edited media, such as movies, contain a diverse range of visuals and human interactions over a long period of time. However, these videos pose challenges due to sudden changes in scenes and close-up shots of actors that are heavily truncated. These challenges make it difficult to apply existing methods for understanding 3D human movements. In this study, we address these limitations by recognizing that although shot changes may cause discontinuity between frames, the 3D structure of the scene changes smoothly. This insight allows us to consider frames before and after the shot change as a multi-view signal, which provides strong clues to reconstruct the 3D positions of the actors. We propose a multi-shot optimization framework that leverages this insight, resulting in improved 3D reconstruction and the ability to extract pseudo-ground truth 3D human mesh sequences. We consider this data as valuable supervision for models that can recover human mesh from movies, both from single images and videos. To handle missing observations caused by shot changes, we introduce a transformer-based temporal encoder. Through extensive experiments, we demonstrate the significance of our insight and proposed models. Our developed tools facilitate the processing and analysis of 3D content from a wide range of edited media, which can be beneficial for various downstream applications. The code, models, and data are accessible at the following link: https://geopavlakos.github.io/multishot/. Figure 2 illustrates the concept of multi-shot reasoning, where frames before and after a shot change contribute to reconstructing the 3D pose of humans, particularly in cases of close-up and heavily truncated images. Each person is reconstructed independently.