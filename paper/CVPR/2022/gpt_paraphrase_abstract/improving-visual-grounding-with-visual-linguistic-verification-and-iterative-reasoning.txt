This study addresses the task of visual grounding, which involves locating a target indicated by a natural language expression. Current methods in this field rely on generic object detection frameworks, using pre-generated proposals or anchors and combining them with text embeddings to locate the target. However, this approach may not fully leverage the visual context and attribute information in the text query, leading to limited performance.   To overcome this limitation, the researchers propose a transformer-based framework for accurate visual grounding. This framework establishes text-conditioned discriminative features and incorporates multi-stage cross-modal reasoning. A visual-linguistic verification module is developed to focus on regions relevant to the textual descriptions and suppress unrelated areas. Additionally, a language-guided feature encoder is designed to enhance the distinctiveness of the target object by aggregating its visual contexts.   To retrieve the target from the encoded visual features, a multi-stage cross-modal decoder is introduced. This decoder iteratively speculates on the correlations between the image and text, improving the accuracy of target localization. The proposed components are extensively evaluated on five widely used datasets, demonstrating their effectiveness and achieving state-of-the-art performance.