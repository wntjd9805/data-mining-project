Predicting future motion based on past motion sequences is a crucial problem in computer vision, with applications in autonomous driving and robotics. Previous studies have shown that Graph Convolutional Networks (GCN) are effective in modeling the relationship between different joints. However, due to the diverse nature of human motion data, accurately capturing the cross-dependency of spatio-temporal relationships is challenging. The decoupled modeling strategy used in previous approaches may exacerbate the issue of insufficient generalization. To address these limitations, we propose the Spatio-Temporal Gating-Adjacency GCN (GAGCN) method. GAGCN utilizes gating networks to improve the generalization of GCN by incorporating trainable adaptive adjacency matrices, which blend candidate spatio-temporal adjacency matrices. Additionally, GAGCN balances the weights of spatio-temporal modeling and integrates decoupled spatio-temporal features to address the cross-dependency of space and time. Our extensive experiments on Human 3.6M, AMASS, and 3DPW datasets demonstrate that GAGCN achieves state-of-the-art performance in both short-term and long-term motion predictions.