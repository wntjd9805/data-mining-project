Most deep-learning-based models for continuous sign language recognition (CSLR) consist of three modules: visual, sequential, and alignment. However, training these backbones with a single connectionist temporal classification loss is challenging. To address this issue, we propose two auxiliary constraints to improve the CSLR backbones in terms of consistency. The first constraint focuses on enhancing the visual module, which often suffers from insufficient training. We introduce a keypoint-guided spatial attention module to the visual module, forcing it to prioritize informative regions, thereby improving spatial attention consistency. However, enhancing the visual module alone may not fully exploit the backbone's potential. Therefore, we propose a sentence embedding consistency constraint between the output features of the visual and sequential modules, as they represent the same sentence. This constraint enhances the representation power of both features. Experimental results using three representative backbones confirm the effectiveness of these constraints. Notably, when using a transformer-based backbone, our model achieves state-of-the-art or competitive performance on three benchmarks: PHOENIX-2014, PHOENIX-2014-T, and CSL.