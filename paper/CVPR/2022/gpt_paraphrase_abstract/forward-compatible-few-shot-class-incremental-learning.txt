In our rapidly changing world, new classes often emerge, such as new users in an authentication system. Machine learning models need to be able to recognize these new classes while still retaining knowledge of old ones. This becomes particularly challenging when there is limited data available for the new classes, a situation known as few-shot class-incremental learning (FSCIL). Current approaches to incremental learning focus on making the updated model similar to the old one. However, we propose a different approach called ForwArd Compatible Training (FACT) for FSCIL. FACT takes a prospective approach, aiming to prepare the model for future updates. We achieve forward compatibility by reserving embedding space for future new classes. Specifically, we assign virtual prototypes to compress the embedding of known classes and set aside space for new ones. Additionally, we anticipate possible new classes and prepare for the updating process. The virtual prototypes enable the model to easily incorporate future updates, acting as proxies scattered within the embedding space to enhance classification during inference. FACT effectively integrates new classes with forward compatibility, while also preventing the model from forgetting old ones. Through extensive experiments, we demonstrate that FACT outperforms existing methods. The code for FACT is available at: https://github.com/zhoudw-zdw/CVPR22-Fact.