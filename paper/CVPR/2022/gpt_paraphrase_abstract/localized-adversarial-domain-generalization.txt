Deep learning methods often struggle to handle shifts in domain that they have not been trained on, leading to poor generalization to unseen domains. To address this issue, domain generalization (DG) has gained research attention by focusing on improving the model's ability to generalize to out-of-distribution data. Adversarial domain generalization is a commonly used approach in DG, but it has limitations. Firstly, it struggles to align features effectively, resulting in a lack of mixing between local neighborhoods across domains. Secondly, it can suffer from feature space collapse, which can negatively impact generalization performance. To overcome these limitations, we propose a novel method called localized adversarial domain generalization with space compactness maintenance (LADG). LADG introduces two main contributions. Firstly, it utilizes an adversarial localized classifier as the domain discriminator, in conjunction with a principled primary branch. This creates a min-max game where the featurizer aims to generate locally mixed domains. This approach helps to effectively align features across different domains. Secondly, LADG incorporates a coding-rate loss to mitigate feature space collapse. By doing so, it ensures that the feature space maintains its compactness, thereby improving generalization performance. To validate the effectiveness of our proposed method, we conducted comprehensive experiments on the Wilds DG benchmark. Our results demonstrate that LADG outperforms leading competitors on most datasets, highlighting its superiority in domain generalization tasks.