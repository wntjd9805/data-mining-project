Recently, transformers and modality encoders have been used in multi-modal detectors to achieve impressive results in visual object detection with text queries. However, these models are large and computationally intensive, making it challenging to deploy them on mobile devices with limited hardware resources. In this study, we propose a lightweight modulated detector called Lite-MDETR, which enables efficient multi-modal understanding on mobile devices. We introduce a new approach called Dictionary-Lookup-Transformations (DLT) to replace Linear Transformations (LT) in multi-modal detectors. DLT factorizes the weights of LT into smaller dictionaries, indices, and coefficients. This conversion reduces the computational complexity of the linear projection from weights to efficient linear projection with dictionaries, lookups, and scalings using indices and coefficients. DLT can be applied to any pre-trained multi-modal detector, eliminating the need for expensive training from scratch. To address the challenge of training DLT due to non-differentiable indexes, we convert the index and coefficient into a sparse matrix, train it during the fine-tuning phase, and convert it back during the inference phase. Our experiments on various tasks such as phrase grounding, referring expression comprehension, segmentation, and visual question answering demonstrate that Lite-MDETR achieves similar accuracy to previous multi-modal detectors while reducing the model size by up to approximately 4.1 times.