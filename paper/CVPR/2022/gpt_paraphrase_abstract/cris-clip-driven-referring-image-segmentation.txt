This study focuses on the challenge of aligning text and image features in referring image segmentation, where the goal is to segment a referent using a natural language expression. Previous approaches have used pretrained models to facilitate learning but have failed to consider the multi-modal corresponding information. Building on the recent development in Contrastive Language-Image Pretraining (CLIP), this paper introduces an end-to-end framework called CRIS for referring image segmentation. CRIS utilizes vision-language decoding and contrastive learning to effectively transfer multi-modal knowledge and align text with pixel-level features. A vision-language decoder is designed to propagate fine-grained semantic information from textual representations to pixel-level activations, ensuring consistency between the two modalities. Additionally, text-to-pixel contrastive learning is employed to enforce similarity between text features and related pixel-level features while dissimilarity to irrelevant ones. Experimental results on three benchmark datasets demonstrate that the proposed framework outperforms state-of-the-art methods without requiring any post-processing.