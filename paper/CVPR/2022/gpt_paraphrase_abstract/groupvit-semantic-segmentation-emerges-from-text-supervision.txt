This paper introduces a novel approach to visual scene understanding by incorporating grouping and recognition mechanisms into deep learning systems. Traditionally, grouping of image regions is achieved through top-down supervision, but the proposed method brings back the grouping mechanism within deep networks. The proposed model, called GroupViT, surpasses the regular grid structure representation and learns to group image regions into larger arbitrary-shaped segments. GroupViT is trained with a text encoder on a large-scale image-text dataset using contrastive losses. Remarkably, GroupViT achieves impressive results in semantic segmentation without pixel-level annotations or fine-tuning. Specifically, it achieves a zero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 dataset and 22.4% mIoU on the PASCAL Context dataset, outperforming state-of-the-art transfer-learning methods that require more supervision. The code for GroupViT is made available on GitHub at https://github.com/NVlabs/GroupViT.