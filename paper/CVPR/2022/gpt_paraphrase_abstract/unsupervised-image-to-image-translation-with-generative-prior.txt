The abstract discusses the challenges of unsupervised image-to-image translation, which involves learning the translation between two visual domains without paired data. Despite recent progress in this field, it is still difficult to create mappings between complex domains with significant visual differences. To address this issue, the authors propose a new framework called Generative Prior-guided UNsupervised Image-to-image Translation (GP-UNIT), which aims to improve the quality and applicability of the translation algorithm.The key idea behind GP-UNIT is to utilize the generative prior obtained from pre-trained class-conditional GANs, such as BigGAN, to learn content correspondences across different domains. The authors introduce a novel coarse-to-fine scheme, in which they first distill the generative prior to capture a robust coarse-level content representation. This representation helps link objects at a more abstract semantic level. In addition, fine-level content features are adaptively learned to establish accurate multi-level content correspondences.The effectiveness of the GP-UNIT framework is demonstrated through extensive experiments. The authors show that their approach outperforms state-of-the-art methods in terms of robustness, high-quality translations, and diversity. Furthermore, GP-UNIT is shown to be effective even for challenging and distant domains. The code for GP-UNIT is made available on GitHub.In summary, the authors propose a novel framework, GP-UNIT, for unsupervised image-to-image translation. By leveraging generative priors and employing a coarse-to-fine scheme, GP-UNIT achieves superior performance compared to existing methods.