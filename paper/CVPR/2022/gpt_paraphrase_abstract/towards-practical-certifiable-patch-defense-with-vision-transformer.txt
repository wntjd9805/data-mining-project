Patch attacks are a serious threat to image classifiers, as they can manipulate pixels in a continuous region to induce misclassification. Existing defenses against patch attacks sacrifice the accuracy of classifiers and only provide low certified accuracy on small datasets. This limits their practical application. To address this, we propose incorporating the Vision Transformer (ViT) into the Derandomized Smoothing (DS) framework. We introduce a progressive smoothed image modeling task to train ViT, which enables it to capture local context while preserving global semantic information. We also innovate the global self-attention structure of ViT by using isolated band unit self-attention for efficient inference and deployment. Our method achieves a certified accuracy of 41.70% and a clean accuracy of 78.58% on ImageNet under 2% area patch attacks. This is a significant improvement compared to previous methods. Extensive experiments demonstrate that our approach achieves state-of-the-art performance on CIFAR-10 and ImageNet datasets, while maintaining efficient inference.