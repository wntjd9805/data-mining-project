This paper tackles the issue of cross-dataset generalization in 3D human pose estimation models. When testing a pretrained model on a new dataset, there is a significant drop in performance. Existing methods have focused on improving the diversity of training data, but we argue that diversity alone is not enough. It is crucial to adapt the characteristics of the training data to match those of the new dataset, including camera viewpoint, position, human actions, and body size. To address this, we propose an end-to-end framework called AdaptPose, which generates synthetic 3D human motions from a source dataset and uses them to fine-tune a 3D pose estimator. AdaptPose employs an adversarial training scheme, where a generator generates a sequence of 3D poses and a camera orientation based on a source 3D pose. These generated poses are then projected to a novel view. Remarkably, AdaptPose can learn to create synthetic 3D poses from the target dataset without any 3D labels or camera information, only being trained on 2D poses. In experiments using the Human3.6M, MPI-INF-3DHP, 3DPW, and Ski-Pose datasets, our method outperforms previous approaches in cross-dataset evaluations by 14% and previous semi-supervised learning methods that utilize partial 3D annotations by 16%.