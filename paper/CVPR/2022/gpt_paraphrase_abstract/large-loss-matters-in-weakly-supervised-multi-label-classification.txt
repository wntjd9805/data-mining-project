The task of weakly supervised multi-label classification (WSML) involves learning to classify images with only partial labels, reducing the need for extensive annotation. This study approaches WSML by treating unobserved labels as negative labels, transforming the task into a noisy multi-label classification problem. The authors discover that the memorization effect, previously observed in a noisy multi-class setting, also applies to multi-label classification. This means that the model first learns to represent clean labels and then memorizes the noisy labels. Based on this observation, the authors propose new methods for WSML that identify and correct large loss samples to prevent the model from memorizing noisy labels. These methods outperform existing state-of-the-art WSML techniques on various datasets, including Pascal VOC 2012, MS COCO, NUSWIDE, CUB, and OpenImages V3. Additional analyses confirm the effectiveness of the proposed methodology, highlighting the importance of handling large loss samples in weakly supervised multi-label classification. The code for the proposed methods is available at https://github.com/snucml/LargeLossMatters.