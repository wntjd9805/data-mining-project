Conditional Neural Processes (CNPs) are a type of neural network that incorporates probabilistic inference to approximate functions of Stochastic Processes in meta-learning scenarios. In order to handle non-i.i.d function instantiations, CNPs are optimized for predicting observations within individual instantiations and adapting meta-representations across different instantiations within a generative reconstruction pipeline. However, when dealing with high-dimensional and noisy function observation distributions, it can be challenging to reconcile these two objectives. To address this limitation, we propose enhancing CNPs by incorporating noise contrastive estimation, which learns distributional matching objectives to improve the robustness of generative models. This is achieved by aligning predictions with encoded ground-truth observations and decoupling meta-representation adaptation from generative reconstruction. Two auxiliary contrastive branches, namely in-instantiation temporal contrastive learning (TCL) and cross-instantiation function contrastive learning (FCL), are introduced to facilitate local predictive alignment and global function consistency. Our empirical results demonstrate that TCL captures high-level abstractions of observations, while FCL aids in identifying underlying functions, resulting in more efficient representations. When evaluating function distribution reconstruction and parameter identification across various dimensions and time-series, our model outperforms other variants of CNPs.