The current approach to image-level weakly supervised semantic segmentation (WSSS) using Class Activation Maps (CAMs) has made significant progress. However, there is still a large gap between the supervision provided for classification and segmentation, which hinders the model's ability to generate accurate and complete pseudo masks for segmentation. To address this issue, we propose a weakly-supervised pixel-to-prototype contrast method that provides pixel-level supervisory signals to narrow the supervision gap. Our method incorporates two intuitive priors and is applied across different views and within a single view of an image. This approach enforces cross-view feature semantic consistency regularization and helps achieve compactness or dispersion of the feature space within and between classes. Importantly, our method can be seamlessly integrated into existing WSSS models without modifying the base networks or increasing the inference burden. Extensive experiments demonstrate the effectiveness of our method, as it consistently improves upon two strong baselines by a significant margin. For example, when applied to SEAM, our method improves the initial seed mean intersection over union (mIoU) on the PASCAL VOC 2012 dataset from 55.4% to 61.5%. Furthermore, with the inclusion of our method, the segmentation mIoU of EPS is increased from 70.8% to 73.6%, achieving a new state-of-the-art performance.