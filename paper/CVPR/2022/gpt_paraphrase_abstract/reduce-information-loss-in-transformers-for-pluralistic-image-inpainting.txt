In recent advancements in pluralistic image inpainting, transformers have been successful. However, existing transformer-based solutions suffer from information loss due to two main reasons. Firstly, they downsample the input image to lower resolutions for efficiency purposes, resulting in information loss and misalignment at the boundaries of masked regions. Secondly, they quantize the RGB pixels into a smaller number of quantized pixels, using their indices as tokens for the transformer. Although an additional CNN network is used to upsample and refine the low-resolution results, it is challenging to recover the lost information. To address these issues, we propose a new transformer-based framework called "PUT". Instead of downsampling, we introduce a patch-based auto-encoder called P-VQVAE, which converts the masked image into non-overlapping patch tokens, allowing for efficient computation while preserving input information. To eliminate quantization-induced information loss, we incorporate an Un-Quantized Transformer (UQ-Transformer) that directly takes features from the P-VQVAE encoder without quantization, using the quantized tokens solely as prediction targets. Extensive experiments demonstrate that PUT outperforms state-of-the-art methods in terms of image fidelity, particularly for large masked regions and complex large-scale datasets.