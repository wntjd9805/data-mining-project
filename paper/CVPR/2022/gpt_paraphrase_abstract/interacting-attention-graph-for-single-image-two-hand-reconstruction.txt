This paper introduces IntagHand, the first graph convolution based network for reconstructing two interacting hands from a single RGB image. The authors address the challenges of occlusion and interaction in two-hand reconstruction by incorporating two novel attention-based modules. The first module, called pyramid image feature attention (PIFA), uses multiresolution features to achieve vertex-to-image alignment. The second module, known as cross hand attention (CHA), encodes the coherence of interacting hands by establishing dense cross-attention between hand vertices. The proposed model outperforms existing methods for two-hand reconstruction on the InterHand2.6M benchmark. Ablation studies confirm the effectiveness of both PIFA and CHA modules in improving reconstruction accuracy. The generalization ability of the network is demonstrated through results on in-the-wild images and live video streams. The code for IntagHand is available at https://github.com/Dw1010/IntagHand.