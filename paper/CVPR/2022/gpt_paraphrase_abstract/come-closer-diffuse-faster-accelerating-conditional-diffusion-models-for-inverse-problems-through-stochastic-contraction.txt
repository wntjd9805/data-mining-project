Diffusion models have gained significant attention for their effectiveness as generative models and their ability to solve inverse problems. However, a major drawback of these models is their slow sampling process, which typically requires thousands of iterations to generate images from pure Gaussian noise. In this study, we propose a new approach that eliminates the need to start from Gaussian noise and instead utilizes a single forward diffusion with better initialization. This significantly reduces the number of sampling steps in the reverse conditional diffusion process. We support our findings with the contraction theory of stochastic difference equations, which explains the effectiveness of our conditional diffusion strategy. We introduce a new sampling strategy called Come-Closer-Diffuse-Faster (CCDF), which not only accelerates the sampling process but also suggests a novel way to combine feed-forward neural network approaches with diffusion models for solving inverse problems. Our experimental results in super-resolution, image inpainting, and compressed sensing MRI demonstrate that our method achieves state-of-the-art reconstruction performance with significantly fewer sampling steps. This research was supported by grants from the Institute of Information & Communications Technology Planning & Evaluation (IITP) and the National Research Foundation (NRF) of Korea.