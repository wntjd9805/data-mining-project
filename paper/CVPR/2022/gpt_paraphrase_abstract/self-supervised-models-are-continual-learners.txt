This study explores the limitations of self-supervised models in Continual Learning (CL) scenarios, where data is presented sequentially. While self-supervised models have shown promise in offline training on unlabeled data, their performance suffers in a CL setting. To address this issue, the authors propose converting self-supervised loss functions into distillation mechanisms for CL. This involves adding a predictor network that maps current representations to their past states. The authors develop a framework for Continual self-supervised visual representation Learning that enhances the quality of learned representations, is compatible with state-of-the-art self-supervised objectives, and requires minimal hyperparameter tuning. The effectiveness of the approach is demonstrated through empirical experiments involving six popular self-supervised models in various CL settings. The code for implementing the proposed framework is available on GitHub at github.com/DonkeyShot21/cassle.