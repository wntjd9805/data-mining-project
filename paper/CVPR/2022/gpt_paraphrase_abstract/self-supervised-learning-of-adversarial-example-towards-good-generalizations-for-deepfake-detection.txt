Recent studies have shown promising results in detecting deepfake videos when the training and testing data come from the same dataset. However, the challenge arises when trying to detect forgeries created using unseen methods during training. This study aims to address this issue by proposing a generalizable deepfake detection approach based on the principle that a reliable representation should be sensitive to various types of forgeries. To achieve this, the researchers suggest enriching the diversity of forgeries by synthesizing augmented forgeries using different configurations and enhancing the sensitivity to forgeries by training the model to predict these configurations. To effectively explore the vast space of forgery augmentation, an adversarial training strategy is employed to dynamically synthesize the most challenging forgeries for the current model. Extensive experiments demonstrate the effectiveness of these proposed strategies, surpassing the performance of current state-of-the-art methods. The code for this study is available at https://github.com/liangchen527/SLADD.