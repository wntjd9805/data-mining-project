This paper addresses the issue of texture representation for 3D shapes in the context of texture transfer and synthesis. Existing methods either use spherical texture maps, which can cause distortions, or continuous texture fields, which lack details. The authors argue that representing textures with images and linking them to a 3D mesh through UV mapping is preferable, as synthesizing 2D images is a well-studied problem. They propose AUV-Net, a model that learns to embed 3D surfaces into a 2D aligned UV space, ensuring that textures are aligned across objects for easy synthesis by generative models. The alignment is learned in an unsupervised manner using a texture alignment module inspired by traditional linear subspace learning. The resulting UV mapping and aligned texture representations enable various applications, such as texture transfer, synthesis, and single view 3D reconstruction with textures. The effectiveness of the proposed method is demonstrated through experiments on multiple datasets.