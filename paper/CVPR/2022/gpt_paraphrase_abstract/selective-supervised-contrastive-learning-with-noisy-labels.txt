This paper introduces selective-supervised contrastive learning (Sel-CL) as a method to learn robust representations and handle noisy labels in deep networks. While deep networks have the ability to embed data into latent representations and complete tasks effectively, this typically relies on high-quality annotated labels, which can be expensive to obtain. Alternatively, using noisy labels is more affordable but results in corrupted representations and poor generalization performance. To address this issue, Sel-CL extends supervised contrastive learning (Sup-CL), a powerful representation learning technique that is compromised by noisy labels. The problem with Sup-CL is that it operates in a pair-wise manner, and noisy labels lead to the creation of misleading pairs for representation learning. Sel-CL tackles this problem directly by selecting confident pairs from the noisy ones for Sup-CL, without requiring knowledge of the noise rates. The selection process involves measuring the agreement between learned representations and given labels to identify confident examples, which are then used to build confident pairs. The representation similarity distribution within these confident pairs is leveraged to identify more confident pairs from the noisy ones. All the obtained confident pairs are finally used for Sup-CL to enhance representations. Experiments conducted on multiple noisy datasets demonstrate the robustness of the learned representations using the proposed Sel-CL method, achieving state-of-the-art performance. The source codes for Sel-CL are available at https://github.com/ShikunLi/Sel-CL.