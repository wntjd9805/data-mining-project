This paper presents a new denoising training method that speeds up the training process of DETR (DEtection TRansformer) and provides a deeper understanding of the slow convergence issue faced by similar methods. The authors identify the instability of bipartite graph matching as the cause of slow convergence, which leads to inconsistent optimization goals in the early stages of training. To overcome this problem, the proposed method introduces ground-truth bounding boxes with added noise into the Transformer decoder and trains the model to reconstruct the original boxes. This approach effectively reduces the difficulty of bipartite graph matching and accelerates convergence. The method can be easily integrated into any DETR-like methods with minimal code modifications, resulting in a significant improvement. The authors' DN-DETR achieves remarkable results (+1.9AP) compared to the baseline under the same settings, achieving the best performance (AP 43.4 and 48.6 with 12 and 50 epochs of training, respectively) among DETR-like methods using a ResNet-50 backbone. DN-DETR achieves comparable performance to the baseline with only 50% of the training epochs. The code for the method is available at https://github.com/FengLi-ust/DN-DETR.