This paper introduces a novel two-branch fusion network for depth completion in indoor environments. The raw depth image captured by the depth sensor often contains missing values, which hinders downstream vision tasks. Existing methods can generate accurate dense depth maps from sparse depth maps, but they struggle with large contiguous regions of missing depth values. In this study, the first branch of our network uses an encoder-decoder structure to predict local dense depth values from the raw depth map, aided by guidance information from the RGB image. The second branch employs an RGB-depth fusion GAN to transfer the RGB image into a fine-grained textured depth map. Adaptive fusion modules and a confidence fusion head are used to integrate the features from both branches and produce the final depth map. The proposed method is evaluated on NYU-Depth V2 and SUN RGB-D datasets, demonstrating improved depth completion performance, especially in realistic indoor environments with the assistance of the pseudo depth map.