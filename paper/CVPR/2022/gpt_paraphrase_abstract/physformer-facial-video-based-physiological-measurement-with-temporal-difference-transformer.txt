Remote photoplethysmography (rPPG) is a non-contact method for measuring heart activities and physiological signals from facial video. While recent deep learning approaches have focused on extracting subtle rPPG clues using convolutional neural networks with limited spatio-temporal receptive fields, they have overlooked the importance of long-range spatio-temporal perception and interaction in rPPG modeling. In this study, we introduce PhysFormer, an end-to-end video transformer-based architecture that dynamically aggregates both local and global spatio-temporal features to enhance rPPG representation. The temporal difference transformers, a key component of PhysFormer, enhance the quasi-periodic rPPG features by incorporating temporal difference-guided global attention and refining the local spatio-temporal representation to mitigate interference. Additionally, we propose label distribution learning and a curriculum learning-inspired dynamic constraint in the frequency domain to provide comprehensive supervision and alleviate overfitting. We evaluate our approach on four benchmark datasets and demonstrate superior performance in both intra- and cross-dataset testing. Notably, unlike most transformer networks that require pretraining on large-scale datasets, PhysFormer can be easily trained from scratch on rPPG datasets, positioning it as a promising transformer baseline for the rPPG community. The source code is available at https://github.com/ZitongYu/PhysFormer.