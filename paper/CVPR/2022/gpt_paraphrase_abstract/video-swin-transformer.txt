The video recognition field is shifting from using CNNs to Transformers, with pure Transformer architectures achieving top accuracy. These architectures connect patches globally across spatial and temporal dimensions. However, this paper proposes a different approach that incorporates a bias towards locality in video Transformers. This bias leads to a better trade-off between speed and accuracy compared to previous methods that use global self-attention, even with spatial-temporal factorization. The proposed video architecture is based on the Swin Transformer designed for images, while still utilizing the benefits of pre-trained image models. This approach achieves state-of-the-art accuracy on various video recognition benchmarks, including action recognition and temporal modeling. Notably, it achieves high accuracy with significantly less pre-training data and a smaller model size.