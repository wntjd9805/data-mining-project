We propose a new approach to unsupervised anomaly detection using knowledge distillation (KD). Previous studies using similar architectures for the teacher and student models limited the diversity of anomalous representations. To address this, our novel teacher-student (T-S) model consists of a teacher encoder and a student decoder, with a "reverse distillation" paradigm. Instead of raw images, the student network takes the teacher's one-class embedding as input and aims to restore the teacher's multi-scale representations. Our knowledge distillation starts from high-level presentations to low-level features. We also introduce a trainable one-class bottleneck embedding (OCBE) module in our T-S model, which preserves essential information on normal patterns while disregarding anomaly perturbations. Our method outperforms state-of-the-art approaches on anomaly detection benchmarks, demonstrating its effectiveness and generalizability.