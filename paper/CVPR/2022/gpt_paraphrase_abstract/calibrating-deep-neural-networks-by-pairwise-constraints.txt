Deep neural networks (DNNs) are known to produce inaccurate estimates of class-posterior probabilities. This is believed to be due to the limited calibration supervision provided by the cross-entropy loss, which focuses primarily on the probability of the true class and neglects the rest. To address this issue, we explore the idea of utilizing each example to supervise all classes, realizing that the calibration of a C-way classification problem is equivalent to the calibration of C(C-1)/2 pairwise binary classification problems derived from it. Based on this hypothesis, we propose a method called calibration by pairwise constraints (CPC) that improves DNN calibration by providing calibration supervision to all binary problems. The CPC method involves two types of binary calibration constraints and can be implemented with minimal complexity increase during cross-entropy training. Experimental evaluations conducted on various datasets and DNN architectures demonstrate that the proposed CPC method achieves state-of-the-art calibration performance.