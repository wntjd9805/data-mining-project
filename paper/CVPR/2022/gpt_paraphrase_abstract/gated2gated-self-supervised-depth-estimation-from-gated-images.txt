Gated cameras have the potential to replace LiDAR sensors for high-resolution 3D depth scanning in challenging weather conditions. Unlike pulsed LiDAR sensors, which record depth directly through photon time-of-flight, gated imagers capture depth information by encoding it in the relative intensity of a few gated slices. Current methods for decoding depth from these measurements require synchronized and calibrated LiDAR, which limits their widespread use and application beyond automotive scenarios. To address this limitation, we propose a self-supervised depth estimation method that utilizes gated intensity profiles and temporal consistency for training. Our model is trained end-to-end using gated video sequences and does not rely on LiDAR or RGB data. It learns to estimate absolute depth values by disentangling the scene albedo, depth, and ambient light from the input slices, and reconstructs the slices using a cyclic loss. We leverage the temporal consistency between frames and neighboring gated slices to estimate depth in areas with shadows and reflections. Experimental validation demonstrates that our approach outperforms existing supervised and self-supervised depth estimation methods based on monocular RGB and stereo images, as well as supervised methods based on gated images. The code for our method is available at https://github.com/princeton-computational-imaging/Gated2Gated.