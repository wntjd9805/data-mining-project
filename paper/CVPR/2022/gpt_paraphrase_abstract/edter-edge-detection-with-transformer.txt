Convolutional neural networks have made significant advancements in edge detection by incorporating context and semantic features. However, as their receptive fields increase, they tend to suppress local details. In contrast, vision transformers have demonstrated the ability to capture long-range dependencies effectively. Motivated by this, we propose a novel transformer-based edge detector called EDTER (Edge Detection TransformER), which leverages both global image context information and local cues to extract clear and precise object boundaries and meaningful edges.EDTER operates in two stages. In Stage I, a global transformer encoder is employed to capture long-range global context within coarse-grained image patches. In Stage II, a local transformer encoder works on fine-grained patches to uncover short-range local cues. Each transformer encoder is followed by a Bi-directional Multi-Level Aggregation decoder, carefully designed to generate high-resolution features. The global context and local cues are then combined using a Feature Fusion Module and fed into a decision head for edge prediction.Extensive experiments conducted on BSDS500, NYUDv2, and Multicue datasets demonstrate the superior performance of EDTER compared to state-of-the-art methods. The source code for implementing EDTER is available at https://github.com/MengyangPu/EDTER.