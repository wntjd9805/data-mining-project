This study focuses on object-guided text-to-image synthesis, which involves generating images from natural language descriptions. Existing frameworks for this task have two main issues: complex structure and error propagation. To address these problems, the authors propose a new approach called the object-guided joint-decoding module. This module simultaneously generates the image and its corresponding layout, eliminating the need for a two-step process. The authors introduce the joint-decoding transformer, which models the joint probability of image tokens and layout tokens. By incorporating layout tokens as observed data, the model can better capture the complex scene. Additionally, a novel Layout-VQGAN is employed for layout encoding and decoding, providing more information about the scene. To enrich the language-related details, a detail-enhanced module is introduced based on the understanding that visual details may be lost during compression and the joint-decoding transformer may have limited generating capacity. Experimental results demonstrate that the proposed approach is competitive with previous models and capable of generating diverse and high-quality objects based on given layouts.