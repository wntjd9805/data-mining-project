Recently, there have been successful applications of Contrastive Vision-Language Pre-training (CLIP) in zero-shot and few-shot learning for 2D visual recognition. CLIP learns to match images with their corresponding texts in open-vocabulary settings. However, it is unclear whether CLIP, which is pre-trained using large-scale image-text pairs in 2D, can be applied to 3D recognition. In this study, we propose a method called PointCLIP that aligns CLIP-encoded point clouds with 3D category texts. We encode a point cloud by projecting it onto multi-view depth maps and aggregate the view-wise zero-shot prediction in an end-to-end manner, enabling efficient knowledge transfer from 2D to 3D. To improve feature extraction and adaptively fuse 3D few-shot knowledge into CLIP pre-trained in 2D, we design an inter-view adapter. By fine-tuning the adapter under few-shot settings, we significantly improve the performance of PointCLIP. Additionally, we find that PointCLIP and classical 3D-supervised networks have complementary knowledge. Through simple ensemble during inference, PointCLIP outperforms state-of-the-art 3D networks. Therefore, PointCLIP shows promise as an effective method for understanding 3D point clouds with limited data and minimal resource cost. We conduct comprehensive experiments on Model-Net10, ModelNet40, and ScanObjectNN datasets to demonstrate the effectiveness of PointCLIP. The code is available at https://github.com/ZrrSkywalker/PointCLIP.