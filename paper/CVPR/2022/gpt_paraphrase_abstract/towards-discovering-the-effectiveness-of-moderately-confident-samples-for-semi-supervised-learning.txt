Semi-supervised learning (SSL) has been extensively researched in order to address vision tasks in scenarios where data efficiency is crucial. SSL aims to develop an effective classification model by utilizing a small number of labeled data along with a large amount of unlabeled data. Recent advancements in SSL have combined various techniques such as self-training and consistency regularization to achieve this goal. Typically, a confidence filter (CF) is employed to select reliable unlabeled samples with high prediction confidence. However, it is unclear whether moderately confident samples are useless and how to effectively select useful samples for model optimization. To address these issues, we propose a novel framework called Taylor expansion inspired filtration (TEIF). TEIF allows for the inclusion of moderately confident samples that exhibit similar features or gradients to those averaged over labeled and highly confident unlabeled data. This framework facilitates stable and new information-induced network updates, resulting in improved generalization. Two novel filters are derived from the TEIF framework, each with its own explanatory perspective. The first is the gradient synchronization filter (GSF), which enhances the optimization dynamic of fully-supervised learning. GSF selects samples whose gradients are similar to the majority gradients of their respective classes. The second is the prototype proximity filter (PPF), which involves including more prototypical samples in training to learn better semantic representations. PPF selects samples that are located near class-wise prototypes. These filters can be integrated into SSL methods alongside CF. We conducted experiments using the state-of-the-art Fix-Match as the baseline, and the results on popular SSL benchmarks demonstrate that our approach achieves a new state-of-the-art performance level.