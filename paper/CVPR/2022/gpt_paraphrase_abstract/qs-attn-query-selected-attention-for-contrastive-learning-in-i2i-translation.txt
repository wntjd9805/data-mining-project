Unpaired image-to-image (I2I) translation often requires maximizing the mutual information between the source and translated images across different domains to preserve the source content and prevent unnecessary modifications. The previous use of self-supervised contrastive learning in I2I has been successful. By ensuring that features from the same location are closer than those from different locations, it implicitly ensures that the result retains content from the source. However, previous methods use features from random locations to impose this constraint, which may not be appropriate as some locations contain less information about the source domain. Additionally, the feature itself does not reflect its relation with other features. This study addresses these issues by intentionally selecting significant anchor points for contrastive learning. A query-selected attention (QS-Attn) module is designed to compare feature distances in the source domain and generate an attention matrix with a probability distribution for each row. Queries are then selected based on their significance measurement computed from the distribution and used as anchors for contrastive loss. Simultaneously, the reduced attention matrix is used to route features in both domains, ensuring that source relations are maintained in the synthesis. The proposed method is validated on three different I2I datasets, demonstrating improved image quality without the addition of learnable parameters. The overall structure of the model involves translating the source domain image into a target domain image using a generator, extracting features from both images using an encoder, selecting significant features using the QS-Attn module to establish the contrastive loss, and utilizing a discriminator to construct the adversarial loss. The abstraction is available in a format that only includes the essential information.