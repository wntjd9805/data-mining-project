Image outpainting aims to extend an input image in a semantically consistent manner beyond its existing content. Unlike inpainting, which fills in missing pixels based on neighboring pixels, outpainting allows for more diverse approaches since it is less constrained by the surrounding pixels. Current image outpainting methods often generate repetitive structures and textures by replicating the available content in the input image, treating the problem as a conditional image-to-image translation task.   In this study, we approach the problem by inverting generative adversarial networks. Our generator generates micro-patches based on their joint latent code and their positions in the image. To achieve outpainting, we seek multiple latent codes that not only recover available patches but also generate diverse outpainting through patch-based generation. This approach results in richer structure and content in the outpainted regions. Furthermore, our formulation allows for outpainting that is conditioned on categorical input, providing flexible user controls.   Extensive experiments demonstrate that our proposed method outperforms existing in- and outpainting methods, showcasing higher visual quality and diversity.