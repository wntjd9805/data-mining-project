Automatic video summarization aims to create a condensed version of a longer video while preserving its major content and events. There is a growing interest in incorporating user queries into video summarization, known as query-driven video summarization. This approach generates a concise summary of the original video based on the user's query, typically in the form of text input. However, this method faces two inherent challenges. Firstly, the text query may not fully capture the diverse and precise needs of the user. Secondly, once the summaries are generated, the user is unable to make any edits, despite the fact that their needs may require subtle adjustments through interactive means. To address these challenges, we propose IntentVizor, an interactive video summarization framework guided by multi-modality queries that go beyond just text. These queries can include video snippets, allowing for a more comprehensive representation of the user's needs. We define these multi-modality queries as user "intents," which are not only interpretable and editable but also provide a better quantification of the user's requirements. In this paper, we use a set of proposed intents to represent the user query and develop a novel interactive visual analytic interface. Through this interface, users can interactively control and adjust these mixed-initiative intents to obtain a more satisfying video summary. Additionally, to enhance the quality of video summarization through improved video understanding, we introduce a novel approach called Granularity-Scalable Ego-Graph Convolutional Networks (GSE-GCN). We evaluate our framework on two benchmark datasets and compare it with state-of-the-art methods, demonstrating its effectiveness. The code and dataset for IntentVizor are available at https://github.com/jnzs1836/intent-vizor.