We examine the challenging problem of incremental few-shot object detection (iFSD). Previous studies have explored hypernetwork-based approaches for continuous and finetune-free iFSD, but with limited success. We investigate the design choices of these methods and make several key improvements to create a more accurate and flexible framework called Sylph. Our approach leverages a pretrained base detector for class-agnostic localization on a large-scale dataset, decoupling object classification from localization. Contrary to previous results, we demonstrate that finetune-free iFSD can be highly effective with a carefully designed class-conditional hypernetwork, especially when there is a large number of base categories with abundant data available for meta-training. These results are particularly significant as our approach offers practical advantages such as sequential learning of new classes without additional training, detecting both novel and seen classes in a single pass, and no forgetting of previously seen classes. We evaluate our model on both COCO and LVIS datasets, achieving up to 17% average precision (AP) on the long-tail rare classes in LVIS, highlighting the potential of hypernetwork-based iFSD.