Group activity recognition is the task of comprehending the actions performed by a group of individuals in a video. Current models for this task are often impractical as they require precise bounding box labels for actors during testing or depend on pre-existing object detectors. To address this issue, we propose a new model for group activity recognition that does not rely on bounding box labels or object detectors. Our model, based on Transformer, localizes and encodes partial contexts of a group activity by utilizing the attention mechanism. It represents a video clip as a collection of partial context embeddings. These embeddings are then aggregated to create a single group representation that captures the entire context of an activity and the temporal evolution of each partial context. Our method achieves exceptional performance on two benchmarks, Volleyball and NBA datasets, surpassing both the state-of-the-art models trained with the same level of supervision and some existing models that rely on stronger supervision.