We introduce a highly efficient model for map-view semantic segmentation using multiple cameras. Our model employs a cross-view attention mechanism to implicitly learn how to map individual camera views to a standardized map-view representation. By incorporating positional embeddings based on intrinsic and extrinsic calibration, our model can learn the mapping between different views without explicitly modeling it geometrically. The architecture includes a convolutional image encoder for each view and cross-view transformer layers to generate a map-view semantic segmentation. Our model is straightforward, easily parallelizable, and capable of real-time processing. It achieves state-of-the-art performance on the nuScenes dataset, with inference speeds that are four times faster compared to existing methods. The source code for our model is available at https://github.com/bradyz/cross_view_transformers.