Dynamic model pruning allows for the inference of a different sub-network for each input sample during deployment. However, current dynamic methods rely on learning a continuous channel gating through regularization by inducing sparsity loss, which introduces complexity in balancing different losses. Additionally, regularization based methods lack transparent tradeoff hyper-parameter selection for computational budget realization. Our contribution is two-fold. Firstly, we decouple task and pruning losses. Secondly, we propose a simple hyperparameter selection that enables estimation of FLOPs reduction before training. Inspired by the Hebbian theory in Neuroscience, we predict a mask to process k filters in a layer based on the activation of its previous layer. We frame this problem as a self-supervised binary classification problem, training each mask predictor module to predict if the log-likelihood for each filter in the current layer belongs to the top-k activated filters. The value of k is dynamically estimated for each input based on a novel criterion using the mass of heatmaps. We conducted experiments on various neural architectures, including VGG, ResNet, and MobileNet, using CIFAR and ImageNet datasets. On CIFAR, we achieved similar accuracy to state-of-the-art methods with 15% and 24% higher FLOPs reduction. Similarly, in ImageNet, we obtained a lower drop in accuracy with up to 13% improvement in FLOPs reduction.