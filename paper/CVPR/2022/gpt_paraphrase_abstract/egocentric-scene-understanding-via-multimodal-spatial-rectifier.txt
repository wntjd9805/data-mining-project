This paper focuses on the problem of understanding egocentric scenes, specifically predicting depths and surface normals from egocentric images. Egocentric scene understanding is challenging due to non-canonical viewpoints caused by large head movements and the presence of dynamic foreground objects like hands. Existing models trained on upright images of static scenes do not perform well in this scenario. To address this, the authors propose a multimodal spatial rectifier that stabilizes egocentric images to a set of reference directions, allowing for a coherent visual representation. Unlike a unimodal spatial rectifier that can cause excessive perspective warp, the multimodal spatial rectifier learns from multiple directions to minimize this effect. To learn visual representations of dynamic foreground objects, a new dataset called EDINA (Egocentric Depth on everyday INdoor Activities) is introduced, consisting of over 500K synchronized RGBD frames and gravity directions. Using the multimodal spatial rectifier and the EDINA dataset, the proposed method significantly outperforms baseline models not only on the EDINA dataset but also on other popular egocentric datasets like First Person Hand Action (FPHA) and EPIC-KITCHENS.