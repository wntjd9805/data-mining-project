Generative transformers have gained popularity in computer vision for generating high-quality images. However, current models treat images as token sequences and decode them sequentially, which is not optimal or efficient. This paper introduces a new approach called MaskGIT, which uses a bidirectional transformer decoder. During training, MaskGIT predicts randomly masked tokens by considering tokens in all directions. During inference, the model generates all tokens simultaneously and iteratively refines the image based on previous generations. Experimental results show that MaskGIT outperforms existing transformer models on the ImageNet dataset and speeds up decoding by up to 48 times. Additionally, MaskGIT can be easily adapted for various image editing tasks like in-painting, extrapolation, and manipulation. More information can be found on the project page: masked-generative-image-transformer.github.io.