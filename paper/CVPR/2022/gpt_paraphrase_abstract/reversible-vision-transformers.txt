We introduce Reversible Vision Transformers, an architecture design that allows for efficient memory usage in visual recognition tasks. This design separates the GPU memory usage from the model depth, enabling memory-efficient scaling of transformer architectures. We modify two popular models, Vision Transformer and Multiscale Vision Transformers, to reversible versions and extensively evaluate their performance in image classification, object detection, and video classification tasks, considering different model sizes. The results show that Reversible Vision Transformers achieve a significant reduction in memory usage, up to 15.5 times, while maintaining the same model complexity, parameters, and accuracy. This demonstrates the potential of reversible vision transformers as efficient backbones for training with limited resources. Additionally, we find that the additional computational burden of recomputing activations is outweighed by the benefits for deeper models, as their throughput can increase by up to 3.9 times compared to their non-reversible counterparts. The code and models used in this study are publicly available at https://github.com/facebookresearch/mvit.