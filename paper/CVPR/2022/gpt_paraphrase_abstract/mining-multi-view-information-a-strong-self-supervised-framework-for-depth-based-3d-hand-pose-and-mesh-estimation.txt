This study focuses on the problem of cross-view information fusion in self-supervised 3D hand pose estimation using depth images. Previous methods rely on hand-crafted rules to generate pseudo labels from multi-view estimations for training. However, these methods overlook the valuable semantic information in each view and fail to consider the complex dependencies between different regions across views. In order to address these limitations, the authors propose a cross-view fusion network that effectively utilizes and adaptively combines multi-view information. This is achieved by encoding diverse semantic information into compact nodes and applying graph convolution to model the dependencies and interactions between nodes across views. The proposed framework enables accurate estimation of 3D hand pose and hand mesh through self-supervision. Additionally, a pseudo multi-view training strategy is introduced to extend the framework to scenarios where only single-view training data is available. Experimental results on the NYU dataset demonstrate that the proposed method surpasses previous self-supervised approaches by 17.5% and 30.3% in multi-view and single-view scenarios, respectively. Moreover, the framework achieves results comparable to strongly supervised methods.