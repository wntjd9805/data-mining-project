Current neural style transfer methods require users to have reference style images in order to transfer texture information from the style images to the content images. However, there are practical situations where users do not have reference style images but still want to transfer styles by simply imagining them. To address this issue, we propose a new framework that allows style transfer "without" a style image, but instead only requires a text description of the desired style. By leveraging the pre-trained text-image embedding model of CLIP, we showcase the ability to modulate the style of content images using just a single text condition. Our approach involves a patch-wise text-image matching loss with multiview augmentations to achieve realistic texture transfer. Through extensive experiments, we have successfully demonstrated image style transfer with realistic textures that accurately reflect the semantic query texts.