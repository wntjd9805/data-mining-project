Video super-resolution (VSR) is a technique used to enhance the quality of low-resolution (LR) video frames by generating high-resolution (HR) frames. While some progress has been made in this field, there are still challenges in effectively utilizing temporal dependency in video sequences. Existing approaches typically align and aggregate frames from a limited number of adjacent frames, resulting in unsatisfactory outcomes. In this paper, we propose a new method called Trajectory-aware Transformer for Video Super-Resolution (TTVSR) to address this issue. Our approach involves formulating video frames into pre-aligned trajectories, composed of continuous visual tokens. Self-attention is then applied only to relevant visual tokens along these spatio-temporal trajectories. This design significantly reduces computational costs and allows Transformers to model long-range features. To address scale-changing problems in long-range videos, we also introduce a cross-scale feature tokenization module. Extensive evaluations on four popular video super-resolution benchmarks demonstrate the superiority of our proposed TTVSR over state-of-the-art models. The code and pre-trained models are available for download at https://github.com/researchmm/TTVSR.