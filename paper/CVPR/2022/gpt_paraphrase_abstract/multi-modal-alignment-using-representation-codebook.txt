Aligning signals from different modalities is crucial for vision-language representation learning, as it impacts the performance of subsequent stages like cross-modality fusion. However, directly aligning image and text at the instance level is challenging, especially when features are still evolving during training. To address this, we propose a higher-level alignment approach using cluster representation. We consider image and text as two "views" of the same entity and encode them into a joint vision-language coding space defined by a cluster center dictionary (codebook). By contrasting positive and negative samples based on their cluster assignments and optimizing the cluster centers, we achieve alignment. Additionally, we employ a teacher-student distillation paradigm to enhance the learning process by having a momentum teacher guide the student learning of the other view. We conducted evaluations on popular vision-language benchmarks and achieved state-of-the-art results on zero-shot cross-modality retrieval, while remaining competitive on various other transfer tasks.