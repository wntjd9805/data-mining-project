Recently, self-attention mechanisms have demonstrated impressive performance in various natural language processing (NLP) and computer vision (CV) tasks. These mechanisms are effective in capturing sequential characteristics and deriving global information. In this study, we aim to enhance the learning of subtle feature embeddings for recognizing fine-grained objects, such as different bird species or person identities, by extending self-attention modules. To achieve this, we propose a dual cross-attention learning (DCAL) algorithm that works in conjunction with self-attention learning. Firstly, we introduce the global-local cross-attention (GLCA), which strengthens the interactions between global images and local high-response regions. By doing so, GLCA reinforces spatial-wise discriminative clues that aid in recognition. Secondly, we propose the pair-wise cross-attention (PWCA), which establishes interactions between pairs of images. PWCA regularizes the attention learning of an image by treating another image as a distractor. During inference, PWCA is removed. Through DCAL, we observe a reduction in misleading attentions and a diffusion of attention response, leading to the discovery of more complementary parts for recognition.We extensively evaluate DCAL on tasks involving fine-grained visual categorization and object re-identification. The experiments demonstrate that DCAL performs comparably to state-of-the-art methods and consistently improves multiple self-attention baselines. For example, on MSMT17, DCAL surpasses DeiT-Tiny and ViT-Base by 2.8% and 2.4% mean average precision (mAP), respectively.