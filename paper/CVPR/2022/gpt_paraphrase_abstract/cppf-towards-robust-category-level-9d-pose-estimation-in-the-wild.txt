This paper addresses the issue of estimating category-level 9D poses in real-world environments using a single RGB-D frame. The use of supervised data for training pose estimation models is problematic as it is time-consuming, prone to errors, and lacks generalization to unseen scenarios. Additionally, category-level pose estimation requires the ability to generalize to unseen objects during testing, which is a challenging task. To overcome these challenges, the authors propose a novel approach called Category-level Point Pair Features (CPPF) voting method, inspired by traditional point pair features. The CPPF method aims to achieve accurate, robust, and generalizable 9D pose estimation in wild environments. To ensure robust pose estimation, the authors sample multiple point pairs on an object and predict SE(3)-invariant voting statistics for each pair, including object centers, orientations, and scales. They also introduce a coarse-to-fine voting algorithm to eliminate noisy point pair samples and generate final predictions. To address false positives in the orientation voting process, they introduce an auxiliary binary disambiguating classification task for each sampled point pair. In order to detect objects in real-world environments, the authors design a sim-to-real pipeline that is trained primarily on synthetic point clouds. However, when objects have ambiguous poses in terms of geometry, color information is leveraged to disambiguate these poses. The results of their method on standard benchmarks demonstrate its competitiveness with current state-of-the-art methods that use real-world training data. Furthermore, extensive experiments show that their method is robust to noise and performs well in extremely challenging scenarios. The authors have made their code available on GitHub for further exploration.