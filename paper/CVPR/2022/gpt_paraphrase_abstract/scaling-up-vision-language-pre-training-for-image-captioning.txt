The recent development of vision-language pre-training (VLP) has greatly improved performance in image captioning. Scale has been identified as a crucial factor in this progress. However, current research primarily focuses on pre-training transformers of moderate sizes on a limited number of images. This paper introduces LEMON iMage captiONer, which conducts an empirical study on the scaling behavior of VLP for image captioning. The study utilizes the VinVL model as a reference, varying the size of the transformer from small to large. The experiment also involves a large dataset of 200 million image-text pairs gathered from the web. Through extensive analysis, the performance trend is examined as both the model size and pre-training data size increase. Different training methods are compared, particularly for training on large-scale noisy data. LEMON achieves state-of-the-art results on prominent image captioning benchmarks and demonstrates the ability to generate captions for uncommon visual concepts in a zero-shot manner.