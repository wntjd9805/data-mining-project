This paper introduces a new problem called novel-view scene layout generation, which aims to generate a plausible scene layout for a specific viewpoint. Previous approaches focused on generating novel-view images in the image space, but often resulted in ghosting and blurry artifacts. In this paper, the authors propose a deep model that captures contextualized object representation by modeling the object context transformation in the scene. This contextualized object representation is crucial for generating consistent scene layouts in terms of geometry and semantics. The experiments demonstrate that the proposed model outperforms several strong baselines both qualitatively and quantitatively for indoor and outdoor scenes. Additionally, the model enables various applications such as novel-view image synthesis, novel-view image editing, and amodal object estimation.