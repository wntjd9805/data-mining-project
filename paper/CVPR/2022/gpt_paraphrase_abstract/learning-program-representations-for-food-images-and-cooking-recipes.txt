This paper aims to create a meaningful and detailed representation for instructional procedures, such as cooking recipes, by using cooking programs. These programs offer a structured representation of the task, capturing the semantics of cooking and the sequential relationships between actions. The authors propose a model that learns a joint embedding of recipes and food images, generating a program sequence from this embedding. The effectiveness of this approach is validated through crowd-sourced programs, which demonstrate improved cross-modal retrieval and recognition results compared to traditional methods. Additionally, the authors show that food images can be generated by manipulating programs using a GAN's latent code. The code, data, and models are available online.