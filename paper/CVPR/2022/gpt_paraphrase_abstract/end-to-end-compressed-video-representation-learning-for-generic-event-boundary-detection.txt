This study introduces a novel approach for event boundary detection in generic videos. Existing methods require decoding the video frames before inputting them into the network, which is computationally intensive and requires a large amount of storage space. In contrast, the proposed method utilizes compressed video representation learning, utilizing information such as RGB, motion vectors, residuals, and the internal group of pictures (GOP) structure without fully decoding the video. The method involves extracting features from the I-frames using ConvNets and computing feature representations of the P-frames based on motion vectors, residuals, and representations of their dependent I-frames using a lightweight spatial-channel compressed encoder. A temporal contrastive module is employed to determine the event boundaries of video sequences. To improve annotation accuracy and training efficiency, the ground-truth event boundaries are preprocessed using a Gaussian kernel. Experimental results on the Kinetics-GEBD dataset demonstrate that the proposed method achieves comparable performance to state-of-the-art methods while running 4.5 times faster.