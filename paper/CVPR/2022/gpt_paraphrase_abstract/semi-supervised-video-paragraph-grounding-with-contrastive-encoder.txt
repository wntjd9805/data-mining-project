Video events grounding aims to retrieve relevant moments from an untrimmed video based on a given natural language query. Previous works have mainly focused on Video Sentence Grounding (VSG), which localizes moments using sentence queries. Recently, the task has been extended to Video Paragraph Grounding (VPG), which retrieves multiple events using a paragraph query. However, existing VPG methods may not perform well in terms of context modeling and heavily rely on video-paragraph annotations. To address this issue, we propose a new VPG method called Semi-supervised Video-Paragraph TRansformer (SVPTR), which effectively utilizes contextual information in paragraphs and reduces the dependency on annotated data. Our SVPTR method consists of two key components: (1) a base model called VPTR, which learns video-paragraph alignment using contrastive encoders and addresses the lack of sentence-level contextual interactions, and (2) a semi-supervised learning framework with multimodal feature perturbations that reduces the need for annotated training data. We evaluate our model on three widely-used video grounding datasets: ActivityNet-Caption, Charades-CD-OOD, and TACoS. The experimental results demonstrate that our SVPTR method achieves state-of-the-art performance on all datasets. Even with fewer annotations, it can still achieve competitive results compared to recent VPG methods.