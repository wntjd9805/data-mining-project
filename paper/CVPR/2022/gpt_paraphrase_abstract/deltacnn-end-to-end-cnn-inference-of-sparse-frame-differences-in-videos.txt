Convolutional neural networks (CNNs) are computationally intensive for real-time video processing. However, much of the information in consecutive video frames remains unchanged. To address this, researchers have explored reducing computational redundancy by skipping identical image regions and truncating insignificant pixel updates. However, implementing this approach has proven challenging due to sparse updates impacting computational consistency and memory access coherence. In this study, we introduce DeltaCNN, a sparse CNN framework that enables efficient video inference by allowing sparse frame-by-frame updates. Our framework includes sparse implementations for all typical CNN layers and ensures that feature updates are propagated end-to-end without accumulating errors over time. Notably, DeltaCNN outperforms the dense reference cuDNN in practical settings, achieving speedups of up to 7x with only minor differences in accuracy. Our CUDA kernels and PyTorch extensions for DeltaCNN can be accessed at https://github.com/facebookresearch/DeltaCNN.