Vision transformers have been successful in various computer vision tasks. However, existing methods treat all regions of an image equally when generating vision tokens, which may not be ideal for human-centric tasks where certain areas require more detailed representation. To address this issue, we propose a new Vision Transformer called Token Clustering Transformer (TCFormer). TCFormer utilizes progressive clustering to merge tokens from different locations with flexible shapes and sizes. This allows the tokens to focus on important areas and adapt their shapes to fit semantic concepts, resulting in a finer resolution for regions with critical details. Extensive experiments demonstrate that TCFormer consistently outperforms other models on challenging human-centric tasks and datasets. In particular, it achieves superior performance in whole-body pose estimation on COCO-WholeBody and 3D human mesh reconstruction on 3DPW. The code for TCFormer is available at https://github.com/zengwang430521/TCFormer.git. Figure 1 illustrates a comparison between vision tokens generated by standard grids and TCFormer. The tokens generated by TCFormer dynamically distribute more densely on the human body, while larger areas of the background are represented by a single token. Tokens with fine spatial sizes are used for regions containing important details, such as the face area.