The effectiveness of mixup-based augmentation for training models, particularly Vision Transformers (ViTs), has been established to prevent overfitting. However, previous methods assume that the ratio of targets in linear interpolation should match the ratio in input interpolation. This can lead to a situation where there is no valid object in a mixed image due to random augmentation, but the label space still shows a response. To address this discrepancy, we propose TransMix, which mixes labels based on attention maps of ViTs. The confidence of a label increases if the corresponding input image receives higher attention weights. TransMix is a simple method that can be easily implemented without introducing additional parameters or FLOPs to ViT-based models. Experimental results demonstrate that our approach consistently improves various ViT-based models in ImageNet classification. Moreover, pre-training ViTs with TransMix leads to better transferability in semantic segmentation, object detection, and instance segmentation. TransMix also exhibits robustness across four different benchmarks. The code for TransMix is publicly available at https://github.com/Beckschen/TransMix.