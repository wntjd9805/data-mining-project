This study investigates the potential use of diffusion probabilistic models (DPMs) for representation learning in image generation. While DPMs have achieved impressive results comparable to generative adversarial networks (GANs), they lack semantic meaning in their latent variables, limiting their usefulness for other tasks. The goal of this research is to extract a meaningful and decodable representation of input images using DPMs through an autoencoding process. The approach involves employing a learnable encoder to discover high-level semantics and a DPM as the decoder to capture stochastic variations. The proposed method can encode images into a two-part latent code, where the first part has semantic meaning and linearity, while the second part captures stochastic details for near-exact reconstruction. This capability enables challenging applications that are currently problematic for GAN-based methods, such as attribute manipulation on real images. Additionally, the two-level encoding improves denoising efficiency and facilitates various downstream tasks, including few-shot conditional sampling. For more information, please visit our website: https://Diff-AE.github.io/.