Current state-of-the-art Generative Adversarial Networks (GANs) used for image-to-image translation are often inefficient and require large amounts of memory due to their numerous parameters. This paper addresses this issue by examining GAN performance from a frequency perspective. It is found that smaller GANs struggle to generate high-quality high frequency information. To overcome this limitation, a novel knowledge distillation technique called wavelet knowledge distillation is proposed. Instead of directly distilling the generated images, this method decomposes the images into different frequency bands using discrete wavelet transformation and only distills the high frequency bands. This allows the studentGAN to focus more on learning high frequency information. Experimental results demonstrate that our approach achieves a compression ratio of 7.08× and an acceleration of 6.80× on CycleGAN without significant performance degradation. Furthermore, the study reveals that compressing the discriminators can enhance the performance of the compressed generators.