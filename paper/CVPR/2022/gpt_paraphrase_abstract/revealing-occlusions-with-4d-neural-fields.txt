To enable computer vision systems to function effectively in dynamic situations, they must possess the ability to comprehend and reason about object permanence. In this study, we present a framework that facilitates the learning of 4D visual representations from monocu-lar RGB-D video, enabling the persistence of objects even when obstructed by occlusions. Our approach involves encoding point clouds into a continuous representation, allowing the model to consider the spatiotemporal context and resolve occlusions. Through experiments conducted on two large video datasets, which are made publicly available, we demonstrate that our representation successfully reveals occlusions for various tasks without requiring any architectural modifications. Visualizations further illustrate that the attention mechanism automatically learns to track occluded objects. Given that our approach can be trained end-to-end and easily adapted, we anticipate its utility in addressing occlusions across numerous video understanding tasks. The necessary data, code, and models can be accessed at occlusions.cs.columbia.edu.