Test-time adaptation refers to the process of adapting a trained model from a source domain to a target domain without using any source data. In this study, we propose a new approach to enhance target feature learning using self-supervised contrastive learning. We also introduce an online pseudo labeling scheme with refinement to improve the accuracy of the pseudo labels. Our method combines contrastive learning with pseudo labeling by creating positive and negative pairs, similar to the MoCo method, but with a source-initialized encoder. We exclude same-class negative pairs based on the pseudo labels. Additionally, we generate pseudo labels online and refine them by using soft voting among their nearest neighbors in the target feature space, which is facilitated by maintaining a memory queue. Our method, called AdaContrast, achieves state-of-the-art performance on major benchmarks and offers several advantages compared to existing approaches, such as memory efficiency, insensitivity to hyperparameters, and improved model calibration. The code for our method is available at https://github.com/DianCh/AdaContrast.