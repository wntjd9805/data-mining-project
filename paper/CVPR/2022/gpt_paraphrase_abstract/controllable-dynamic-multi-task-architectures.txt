This study introduces a controllable multi-task network that can dynamically adjust its architecture and weights to prioritize specific tasks and meet resource limitations. Unlike existing dynamic multi-task approaches, which only adjust weights within a fixed architecture, our method allows for flexible control over total computational cost and better matching of user-preferred task importance. We propose training two hypernetworks using task affinity and a novel branching regularized loss to predict tree-structured models with adapted weights based on input preferences. Experimental results on three multi-task benchmarks demonstrate the effectiveness of our approach. More information can be found on our project page at https://www.nec-labs.com/Ëœmas/DYMU.