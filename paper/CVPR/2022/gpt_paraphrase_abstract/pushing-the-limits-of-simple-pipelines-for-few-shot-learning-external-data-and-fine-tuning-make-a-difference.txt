Few-shot learning (FSL) is a current and significant issue in computer vision, which has prompted extensive research into various methods, ranging from complex meta-learning techniques to basic transfer learning approaches. We aim to advance a straightforward yet effective approach for practical few-shot image classification. In doing so, we examine FSL from a neural architecture standpoint and employ a three-stage pipeline involving pre-training on external data, meta-training with labeled few-shot tasks, and task-specific fine-tuning on unseen tasks. We address the following inquiries: 1) The benefits of pre-training on external data for FSL, 2) How to leverage state-of-the-art transformer architectures, and 3) The most effective utilization of fine-tuning. Ultimately, we demonstrate that a simple transformer-based pipeline achieves surprisingly impressive performance on standard benchmarks like Mini-ImageNet, CIFAR-FS, CDFSL, and Meta-Dataset. Our code can be accessed at https://hushell.github.io/pmf.