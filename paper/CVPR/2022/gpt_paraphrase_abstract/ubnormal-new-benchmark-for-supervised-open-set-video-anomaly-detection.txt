Detecting abnormal events in videos is typically approached as a one-class classification task, where the training videos only contain normal events, while the test videos include both normal and abnormal events. This creates an open-set problem for anomaly detection. However, some studies treat anomaly detection as action recognition, which is a closed-set scenario that does not evaluate the system's ability to detect new types of anomalies. To address this, we introduce UBnormal, a new benchmark for supervised open-set video anomaly detection. Unlike existing datasets, UBnormal includes pixel-level annotations of abnormal events during training, allowing for the use of fully-supervised learning methods for abnormal event detection. To maintain the open-set formulation, we ensure that our training and test collections of videos include separate sets of anomaly types. UBnormal is the first benchmark that enables a fair comparison between one-class open-set models and supervised closed-set models for video anomaly detection. Our experiments demonstrate that UBnormal can improve the performance of a state-of-the-art anomaly detection framework on the Avenue and ShanghaiTech datasets. The UBnormal benchmark is freely available at https://github.com/lilygeorgescu/UBnormal.