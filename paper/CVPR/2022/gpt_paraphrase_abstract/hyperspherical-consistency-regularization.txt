Contrastive learning has recently been widely applied in various semi-supervised fields. The common approach is to train supervised and unsupervised learning together using a shared feature encoder. However, this approach suffers from bias in the classifier. This study explores the relationship between self-supervised learning and supervised learning, and investigates how self-supervised learning can improve robust and data-efficient deep learning. The proposed method, called hyperspherical consistency regularization (HCR), addresses the bias issue by using feature-dependent information to regularize the classifier. HCR projects logits and feature projections onto hyperspheres and ensures that data points on hyperspheres have similar structures. This is achieved by minimizing the binary cross entropy of pairwise distances' similarity metrics. Extensive experiments on semi-supervised and weakly-supervised learning demonstrate the effectiveness of HCR, showing superior performance compared to other methods.