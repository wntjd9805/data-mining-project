We present Displacement Aware Relation Module (DisARM), a new module for improving the performance of 3D object detection in point cloud scenes. Our key idea is that extracting the most important contextual information is crucial for detection when the target is incomplete or lacks features. We have found that relations between proposals offer a good representation of the context. However, considering relations between all object or patch proposals is inefficient, and a combination of local and global relations can introduce noise that hinders training. Instead, we have discovered that training with relations only between the most representative anchors can significantly enhance detection performance. Effective anchors should be semantic-aware, unambiguous, and able to describe the entire layout of a scene without redundancy. To identify these anchors, we first use a preliminary relation anchor module with an objectness-aware sampling approach. Then, we introduce a displacement-based module to weigh the importance of relations for better utilization of contextual information. This lightweight relation module greatly improves object instance detection accuracy when integrated into state-of-the-art detectors. Our method achieves state-of-the-art performance on the SUN RGB-D and Scan-Net V2 benchmarks, as demonstrated through evaluations on real-world scenes. The code and models are publicly available at https://github.com/YaraDuan/DisARM.