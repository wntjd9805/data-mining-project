Asymmetric image retrieval is a useful solution for scenarios with limited resources, where a small model is used for querying and a large model is used for the database server. However, existing approaches have limitations such as lacking feature coherence or making strong assumptions, such as the need for labeled datasets or classifiers from the large model. These limitations restrict their practical application. To address this, we propose a flexible contextual similarity distillation framework to improve the small query model and ensure its output features are compatible with those of the large gallery model, which is crucial for asymmetric retrieval. Our approach involves training the small model with a new contextual similarity consistency constraint, without the need for data labels. During the training, the small model preserves the contextual similarity between each training image and its neighbors using the features extracted by the large model. This simple constraint ensures both first-order feature vector preservation and second-order ranking list preservation. Extensive experiments demonstrate that our proposed method outperforms state-of-the-art techniques on the Revisited Oxford and Paris datasets.