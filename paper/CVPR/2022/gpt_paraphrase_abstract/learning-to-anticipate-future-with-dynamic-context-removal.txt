Anticipating future events is a crucial aspect of intelligent systems and embodied AI. However, this task is highly challenging due to the uncertainty of future events and the requirement for reasoning abilities. Previous methods in this field have primarily focused on the design of the model architecture, neglecting the importance of training the anticipation model with an appropriate learning policy. To address this gap, we introduce a new training scheme called Dynamic Context Removal (DCR), which dynamically adjusts the visibility of observed future events during the learning process. This approach follows a curriculum learning process similar to human learning, gradually removing the context of events to increase the difficulty of anticipation until reaching the final target. Our learning scheme is versatile and can be easily integrated with various reasoning models, such as transformer and LSTM, offering advantages in both effectiveness and efficiency. Through extensive experiments, we demonstrate that our proposed method achieves state-of-the-art performance on four widely-used benchmarks. Our code and models are publicly available on GitHub at https://github.com/AllenXuuu/DCR.