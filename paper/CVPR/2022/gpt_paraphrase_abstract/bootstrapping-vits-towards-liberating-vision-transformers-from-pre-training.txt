Vision Transformers (ViTs) have emerged as a powerful alternative to convolutional neural networks (CNNs) in computer vision. While ViTs outperform CNNs in data-rich scenarios, they tend to overfit on small datasets and require time-consuming pre-training. This paper proposes a solution by reintroducing CNN's inductive biases to ViTs while maintaining their network architectures. The approach involves designing an agentCNN based on ViTs with inductive biases and implementing a bootstrapping training algorithm that optimizes both the agent and ViT through weight sharing. The ViT learns the inductive biases from the agent's intermediate features. Experimental results on CIFAR-10/100 and ImageNet-1k datasets with limited training data demonstrate that the inductive biases enable ViTs to converge faster and outperform traditional CNNs with fewer parameters. The code for this work is publicly available at https://github.com/zhfeing/Bootstrapping-ViTs-pytorch.