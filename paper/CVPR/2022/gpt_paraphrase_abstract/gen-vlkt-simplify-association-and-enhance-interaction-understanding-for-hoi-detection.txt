This paper focuses on the task of Human-Object Interaction (HOI) detection, which involves two core problems: human-object association and interaction understanding. The authors identify the limitations of conventional query-driven HOI detectors and propose solutions to address them. Regarding human-object association, previous two-branch methods require complex and costly post-matching, while single-branch methods overlook the differences in features between different tasks. To overcome these drawbacks, the authors propose a Guided-Embedding Network (GEN) that utilizes a two-branch pipeline without post-matching. GEN includes an instance decoder that detects humans and objects using two independent query sets, as well as a position Guided Embedding (p-GE) that identifies pairs of humans and objects in the same position.In terms of interaction understanding, previous methods struggle with long-tailed distribution and zero-shot discovery. To tackle these challenges, the authors introduce a Visual-Linguistic Knowledge Transfer (VLKT) training strategy that leverages a visual-linguistic pre-trained model called CLIP. This strategy involves extracting text embeddings for all labels using CLIP to initialize the classifier and using a mimic loss to minimize the visual feature distance between GEN and CLIP.The proposed GEN-VLKT approach surpasses the state of the art by a significant margin on multiple datasets, achieving a +5.05 mAP improvement on HICO-Det. The source codes for the proposed method are available at https://github.com/YueLiao/gen-vlkt.