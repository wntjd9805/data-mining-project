Augmented reality devices have the potential to enhance human perception and enable assistive functionalities in complex conversational environments. However, effectively capturing the audio-visual context necessary for understanding social interactions is challenging. This is because detecting and localizing voice activities in egocentric conditions, such as head motion, difficult viewing angles, occlusions, visual clutter, audio noise, and bad lighting, pose significant difficulties. Existing active speaker detection methods do not yield satisfactory results under these conditions. To address this problem, we propose a new approach that utilizes both video and multi-channel microphone array audio. Our novel end-to-end deep learning method achieves robust voice activity detection and localization. Unlike previous methods, our approach can localize active speakers from all possible directions on the sphere, even outside the camera's field of view. Additionally, it can simultaneously detect the device wearer's own voice activity. Experimental results demonstrate that our proposed method outperforms existing approaches, operates in real time, and is resilient to noise and clutter.