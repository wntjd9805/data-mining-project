The problem of detecting human-object interactions involves localizing and classifying interactions between humans and objects. Existing approaches either use a single decoder for predicting interaction triplets or employ two parallel decoders to detect individual objects and interactions separately and then combine them. In contrast, we propose a Disentangled Transformer that separates the prediction of triplets into human-object pair detection and interaction classification. Our motivation is that accurately detecting human-object instances and classifying interactions requires learning representations that focus on different regions. To achieve this, we disentangle both the encoder and decoder to facilitate learning of the two sub-tasks. To combine the predictions of the disentangled decoders, we generate a unified representation for HOI triplets with a base decoder and use it as input features for each disentangled decoder. Extensive experiments demonstrate that our method significantly outperforms previous approaches on two public HOI benchmarks. The code for our method will be made available.