Learning how to autonomously assemble shapes is an important skill for robots in various applications. Most existing methods for part assembly focus on correctly aligning semantic parts to recreate a whole object. However, we propose a different approach by interpreting assembly literally as mating geometric parts together to achieve a tight fit. This allows us to generalize across different categories and scales. In this paper, we introduce a new task called pairwise 3D geometric shape mating and present NeuralShape Mating (NSM) as a solution. NSM learns to reason about the fit between two object parts based on their point clouds and predicts a pair of 3D poses that tightly mate them together. To make NSM more robust to imperfect point cloud observations, we also incorporate an implicit shape reconstruction task during training. To train NSM, we develop a self-supervised data collection pipeline that generates pairwise shape mating data by randomly cutting object meshes into two parts. This results in a dataset with 200K shape mating pairs, encompassing various object meshes and diverse cut types. We train NSM on this dataset and evaluate its performance against several point cloud registration methods and a baseline part assembly approach. Through extensive experiments and ablation studies, we demonstrate the effectiveness of our proposed algorithm. Additional materials can be found at neural-shape-mating.github.io.