We introduce a new approach called SD-SSMDE (self-distillation based self-supervised monocular depth estimation) for improving the accuracy of depth estimation from monocular images. In the first step, our network is trained in a self-supervised manner using high-resolution images and a photometric loss. This allows the network to learn depth estimation without the need for ground truth labels. In the second step, we use the trained network to generate pseudo depth labels for all the images in the training set. To enhance the quality of the depth estimates, we re-train the network using a scale invariant logarithmic loss supervised by the pseudo labels. We address the challenge of scale ambiguity and inter-frame scale consistency by incorporating an automatically computed scale in our depth labels. Additionally, we propose a filtering scheme based on 3D consistency between consecutive views to remove noisy depth values. Our experiments demonstrate that each component of our approach, as well as the overall self-supervised learning framework, significantly improve the accuracy of depth estimation compared to a baseline method. We achieve state-of-the-art results on the KITTI and Cityscapes datasets. The accompanying figure illustrates the improvement in depth estimation achieved by our method, with reduced error in specific areas of the image.