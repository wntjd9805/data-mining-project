The proposed PointTracking TRansformer (PTTR) is a method for efficient and accurate 3D object tracking in point cloud sequences. It utilizes transformer operations and introduces three novel designs to improve tracking results.  Firstly, instead of randomly sampling points, PTTR incorporates Relation-Aware Sampling to retain relevant points during subsampling, ensuring the preservation of important information from template point clouds.  Secondly, PTTR introduces a Point Relation Transformer (PRT) consisting of a self-attention and a cross-attention module. The self-attention operation captures long-range dependencies within the point features of the search area and template, enhancing their encoded representations. The cross-attention module matches the two sets of point features, generating coarse tracking results.  Thirdly, PTTR employs a Prediction Refinement Module to refine the coarse tracking results and obtain a final prediction.  To evaluate the performance of PTTR, a large-scale point cloud single object tracking benchmark is created based on the Waymo Open Dataset. Extensive experiments demonstrate that PTTR outperforms existing methods in terms of both accuracy and efficiency.  The code for PTTR is publicly available at https://github.com/Jasonkks/PTTR.