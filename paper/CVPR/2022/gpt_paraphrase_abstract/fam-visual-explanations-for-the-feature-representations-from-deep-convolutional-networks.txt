In recent years, there has been increasing focus on understanding the internal workings of representation models. Traditional methods fall short in fully explaining feature representations, especially when dealing with images that do not fit into any specific category. In such cases, relying on existing classes or similarities with other images fails to provide a comprehensive and reliable visual explanation. To address this issue, we introduce a new visual explanation paradigm called Feature Activation Mapping (FAM). Under this paradigm, we develop two approaches, Grad-FAM and Score-FAM, for visualizing feature representations. Unlike previous methods, FAM identifies the regions of images that contribute the most to the feature vector itself. Through extensive subjective and objective experiments and evaluations, we demonstrate that Score-FAM offers the most promising interpretable visual explanations for feature representations in Person Re-Identification. Moreover, FAM can also be applied to analyze other vision tasks, such as self-supervised representation learning.