Deep image matting techniques have made significant progress in benchmark performance. However, their robustness, particularly with regards to trimaps and generalization across different domains, has not been adequately explored. Existing approaches either refine trimaps or adapt algorithms to real-world images through additional data augmentation, but none consider both factors simultaneously. Moreover, these data augmentation techniques often lead to performance deterioration on benchmarks. To address these limitations, we propose a novel image matting method called RMat, which achieves higher robustness through multilevel context assembling and strong data augmentation tailored for matting. Our approach builds a strong matting framework by incorporating transformer blocks in the encoder to capture global information and leveraging convolution layers and a low-level feature assembling attention block in the decoder to focus on details. We then analyze current data augmentation strategies and introduce simple yet effective techniques to enhance the baseline model and improve generalizability. Our proposed method surpasses previous approaches by achieving state-of-the-art results on the Composition-1k benchmark with a smaller model size, demonstrating a 11% improvement in SAD and a 27% improvement in Grad. Furthermore, our method exhibits robust generalization on other benchmarks, real-world images, and varying coarse-to-fine trimaps, as demonstrated through extensive experiments.