In recent times, significant advancements have been made in both long-tailed recognition and object tracking as separate fields. The TAO benchmark was introduced to combine these two areas and better represent real-world scenarios through long-tailed object tracking. However, existing solutions have primarily focused on using detectors that perform well in long-tailed distributions and then applying tracking algorithms to combine the detections into tracklets. Unfortunately, these approaches neglect temporal changes in scenes, resulting in inconsistent classification results and overall low performance.  In this study, we propose a novel approach to enhance the accuracy of classifying tracklets by utilizing a set classifier that aggregates information from multiple viewpoints within a tracklet. This method not only addresses the limitations of previous approaches but also takes into account the sparse annotations commonly found in videos. We introduce tracklet augmentation techniques that maximize data efficiency in such cases.  Our set classifier is designed to be easily integrated with existing object trackers, providing a plug-and-play solution. By applying our method to QDTrack with ResNet-101 as the base model, we achieve state-of-the-art performance on the TAO benchmark, with TrackAP50 scores of 19.9% and 15.7% on the validation and test sets, respectively. We provide the code for our approach, which can be accessed through the provided link.  Overall, our study presents a novel approach that significantly improves the performance of long-tailed object tracking by introducing a set classifier and addressing the challenges posed by temporal changes and sparse annotations in videos.