Training quantised neural networks (QNNs) is a challenging task as it involves optimizing non-differentiable functions. To address this issue, the straight-through estimator (STE) is commonly used, which utilizes different functions for inference and gradient computation. Various variants of STE have been proposed to improve the task accuracy of trained QNNs. In this study, we analyze these STE variants and investigate their impact on QNN training. We observe that most of these variants can be interpreted as stochastic regularizations of stair functions. Building on this interpretation, we examine QNNs that combine multiple regularizations and find that a synchronized smoothing of each layer is necessary to ensure convergence to the target discontinuous function. Based on these theoretical insights, we propose a new algorithm called additive noise annealing (ANA) for training QNNs, which encompasses standard STE and its variants as special cases. When evaluating ANA on the CIFAR-10 image classification benchmark, we discover that the proper synchronization of different STE variants used in a network has a significant impact on task accuracy, rather than the specific shape of the regularizations. This finding aligns with our theoretical results. QNNs have gained attention in the field of tiny machine learning (TinyML) as they enable the deployment of DNNs on resource-constrained devices. Different techniques have been developed to enhance the efficiency of DNNs in terms of accuracy-per-parameter or accuracy-per-operation. These techniques can be categorized as topological optimizations, which focus on modifying the structure of DNNs, and hardware-related optimizations, which exploit the characteristics of the deployment platform. QNNs, in particular, utilize reduced-precision integer operands to meet storage requirements and leverage the optimized support for integer arithmetic on embedded and edge platforms. However, compared to floating-point counterparts, QNNs often suffer from decreased task accuracy. Various strategies have been proposed to address this issue, such as modifying the QNN's topology, incorporating different data representations and precisions within the network, and learning the shape of the stair functions. Nonetheless, propagating gradients through the discontinuous functions that represent quantized operands remains a critical challenge in QNN training.