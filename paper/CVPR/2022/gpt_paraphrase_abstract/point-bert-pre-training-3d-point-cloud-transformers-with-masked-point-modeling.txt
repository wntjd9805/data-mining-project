We introduce Point-BERT, a novel approach for training Transformers to handle 3D point clouds, building upon the concept of BERT. Inspired by BERT, we propose a Masked Point Modeling (MPM) task to pre-train point cloud Transformers. Our method involves dividing a point cloud into local point patches and using a discrete Variational AutoEncoder (dVAE) to generate meaningful local point tokens. We then randomly mask some patches of the input point clouds and feed them into the Transformers. The objective of pre-training is to recover the original point tokens at the masked locations, guided by the point tokens obtained from the Tokenizer. Extensive experiments demonstrate that our BERT-style pre-training significantly enhances the performance of standard point cloud Transformers. With our pre-training approach, we achieve 93.8% accuracy on ModelNet40 and 83.1% accuracy on the most challenging setting of ScanObjectNN, surpassing hand-designed point cloud models with fewer manual interventions. Furthermore, we show that the representations learned by Point-BERT generalize well to new tasks and domains, outperforming the current state-of-the-art in few-shot point cloud classification. The code and pre-trained models can be found at https://github.com/lulutang0608/Point-BERT.