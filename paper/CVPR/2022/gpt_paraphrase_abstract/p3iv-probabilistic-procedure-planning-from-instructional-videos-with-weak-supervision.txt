This paper investigates the problem of procedure planning in instructional videos, where an agent needs to generate a sequence of actions to achieve a desired goal from a given start state. Previous research in learning procedure planning from instructional videos has relied on annotating intermediate visual observations, which is costly and time-consuming. In contrast, we propose a weakly supervised approach that learns from natural language instructions, eliminating the need for expensive temporal video annotations. Our model utilizes a transformer with a memory module to map the start and goal observations to a sequence of plausible actions. Additionally, we incorporate a probabilistic generative module to account for the inherent uncertainty in procedure planning, which has been largely overlooked in previous work. We evaluate our model on three datasets and demonstrate that our weakly supervised approach surpasses previous fully supervised state-of-the-art models across multiple metrics.