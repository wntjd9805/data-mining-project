This paper introduces a Multi-View Transformer (MVT) approach for 3D visual grounding, which aims to associate natural language descriptions with specific objects in a 3D scene represented by point clouds. Unlike previous works that focused on specific views, the proposed method projects the 3D scene into a multi-view space, allowing for simultaneous modeling and aggregation of position information from different views. This multi-view space enables the network to learn a more robust multi-modal representation for 3D visual grounding, eliminating the reliance on specific views. Experimental results demonstrate that the MVT approach outperforms existing methods, achieving significant improvements in performance. In particular, on the Nr3D and Sr3D datasets, the proposed method surpasses the best competitor by 11.2% and 7.1%, respectively. It even outperforms recent work using 2D assistance by 5.9% and 6.6%. The code for the proposed method is publicly available at https://github.com/sega-hsj/MVT-3DVG.