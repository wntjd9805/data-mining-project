Adversarial attacks are a method of manipulating images to deceive deep neural networks into producing incorrect classification results. To defend against these attacks, a context-consistency check can be imposed, where objects detected in an image must align with the surrounding context. We propose a novel approach to generate context-consistent adversarial attacks that can bypass the context-consistency check of black-box object detectors operating on complex, natural scenes. Unlike previous attacks that require repeated attempts and are susceptible to detection, our method operates in a "zero-query" setting, meaning the attacker has no knowledge of the victim system's classification decisions.Our approach involves deriving multiple attack plans that assign incorrect labels to victim objects while maintaining contextual consistency. We introduce a new data structure called the perturbation success probability matrix, which allows us to filter and select the attack plan most likely to succeed. This final attack plan is implemented using a perturbation-bounded adversarial attack algorithm. To evaluate the effectiveness of our zero-query attack, we compare it to a few-query scheme that repeatedly checks if the victim system is deceived. We also compare it to state-of-the-art context-agnostic attacks. Against a context-aware defense, our zero-query approach achieves a significantly higher fooling rate compared to context-agnostic attacks and even outperforms the few-query scheme with up to three rounds of checking.In summary, we propose a novel method for generating context-consistent adversarial attacks that can evade black-box object detectors in complex, natural scenes. Our zero-query approach outperforms existing methods in terms of fooling rate, providing a more effective means of attacking context-aware defenses.