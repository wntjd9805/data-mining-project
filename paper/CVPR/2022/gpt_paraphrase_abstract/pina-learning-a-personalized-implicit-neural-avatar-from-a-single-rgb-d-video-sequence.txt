We introduce a new approach called Personalized Implicit Neural Avatars (PINA) for creating virtual copies of individuals using a short RGB-D sequence. PINA enables non-experts to generate detailed and personalized virtual avatars with realistic clothing deformations. Unlike existing methods, PINA does not rely on complete scans or pre-learned models from large datasets. However, learning a complete avatar in this scenario is challenging due to limited and noisy depth observations, as well as partial visibility of the body in each frame. To overcome these challenges, we propose a method that learns the shape and non-rigid deformations using a pose-conditioned implicit surface and a deformation field in a canonical space. By fusing all the partial observations into a single canonical representation, we address the issue of incompleteness. The fusion process is formulated as a global optimization problem involving pose, shape, and skinning parameters. Our method is capable of learning neural avatars from real RGB-D sequences with noise for various individuals and clothing styles. Furthermore, these avatars can be animated using unseen motion sequences.