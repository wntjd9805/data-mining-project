Class-imbalanced datasets are common in real-world data, where a few classes have a large number of instances and many classes have very few instances. Neural classification models often struggle to perform well on minority classes in such imbalanced datasets. Existing methods address this issue by rebalancing the data distribution at the class level, assigning higher weights to minority classes and lower weights to majority classes during training. However, we have observed that even the majority classes contain difficult instances to learn. By reducing the weights of majority classes, these difficult instances become even harder to learn and consequently harm overall performance. To overcome this problem, we propose a new strategy that rebalances at the instance level. This strategy dynamically adjusts the sampling probabilities of instances based on their difficulty. Instance difficulty is measured using the learning speed, inspired by the human learning process where easier instances are learned faster. We have theoretically proven the correctness and convergence of our resampling algorithm. Empirical experiments have shown that our method outperforms state-of-the-art rebalancing methods on class-imbalanced datasets.