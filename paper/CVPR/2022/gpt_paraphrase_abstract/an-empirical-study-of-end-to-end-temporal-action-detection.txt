Temporal action detection (TAD) is a challenging task in video understanding, where the goal is to predict both the semantic label and the temporal interval of each action instance in an untrimmed video. Many existing methods use a head-only learning approach, where the video encoder is trained for action classification and only the detection head is optimized for TAD. This paper presents an empirical study of end-to-end learning for TAD, evaluating its advantages over head-only learning and exploring the efficiency-accuracy trade-off. The study shows that end-to-end learning can improve performance by up to 11%. The paper also investigates the impact of various design choices on TAD performance and speed, including the detection head, video encoder, and resolution of input videos. Based on the findings, a mid-resolution baseline detector is proposed, achieving state-of-the-art performance while running over 4 times faster. The authors hope that this paper will serve as a guide for end-to-end learning and inspire future research in this field. The code and models used in the study are available at https://github.com/xlliu7/E2E-TAD.