The role of contextual information is crucial in semantic segmentation, especially in the case of video segmentation where it includes both static and motional contexts. Static contexts refer to the static content in a video clip, while motional contexts correspond to the moving content. While static contexts have been effectively utilized in image segmentation by learning multi-scale and global/long-range features, the study of motional contexts in video segmentation has been limited. There is a lack of research on simultaneously learning static and motional contexts, despite their high correlation and complementarity. To address this issue, our proposed technique called Coarse-to-Fine Feature Mining (CFFM) aims to learn a unified representation of both static and motional contexts. CFFM consists of two parts: coarse-to-fine feature assembling and cross-frame feature mining. The former operation prepares the data for further processing, facilitating joint learning of static and motional contexts. The latter operation extracts useful information/contexts from sequential frames to enhance the video contexts of the features in the target frame. The enhanced features can then be directly used for the final prediction. Experimental results on popular benchmarks demonstrate that our proposed CFFM technique outperforms state-of-the-art methods for video semantic segmentation. We have made our implementation available at https://github.com/GuoleiSun/VSS-CFFM.