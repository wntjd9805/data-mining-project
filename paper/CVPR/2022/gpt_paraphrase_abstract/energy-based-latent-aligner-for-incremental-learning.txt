Deep learning models often forget their previous knowledge when learning new tasks incrementally. This happens because the updates made to the model's parameters for the new tasks might not align well with the updates needed for the older tasks. This mismatch in the latent representations leads to forgetting. In this study, we introduce ELI (Energy-based Latent Aligner for Incremental Learning), which learns an energy manifold for the latent representations. This manifold ensures that the latent representations of previous tasks have low energy values, while the current task's latent representations have high energy values. By countering the representational shift that occurs during incremental learning, ELI provides implicit regularization that can be easily incorporated into existing incremental learning methodologies. We evaluate ELI extensively on CIFAR-100, ImageNet subset, ImageNet 1k, and Pascal VOC datasets. The results consistently show improvement when ELI is added to three popular class-incremental learning methodologies, across various incremental settings. Additionally, when integrated with the state-of-the-art incremental object detector, ELI increases detection accuracy by more than 5%, demonstrating its effectiveness and complementarity to existing approaches. The code for ELI is available at https://github.com/JosephKJ/ELI.