This study focuses on developing a 3D generative model that can disentangle the geometry and appearance variations in a scene using a novel non-rigid deformable scene formulation. The model learns a canonical 3D volume and its deformations jointly during training. This formulation improves the separation between the 3D scene and the camera viewpoints using a pose regularization loss. The model also enables the computation of dense correspondences between generated images and real images, allowing for editing of real images.