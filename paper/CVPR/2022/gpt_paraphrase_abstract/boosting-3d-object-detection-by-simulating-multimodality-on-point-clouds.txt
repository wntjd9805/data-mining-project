This research introduces a novel method to enhance the performance of a single-modality (LiDAR) 3D object detector by training it to simulate features and responses of a multi-modality (LiDAR-image) detector. The proposed approach only requires LiDAR-image data during the training phase of the single-modality detector, and once adequately trained, it only relies on LiDAR data during inference. To implement this approach, a new framework is designed, which includes response distillation to prioritize important response samples and minimize background samples, sparse-voxel distillation to learn voxel semantics and relationships from the identified important voxels, fine-grained voxel-to-point distillation to better capture the features of small and distant objects, and instance distillation to further improve the consistency of deep features. Experimental evaluations on the nuScenes dataset demonstrate that our approach surpasses the performance of all state-of-the-art LiDAR-only 3D detectors and even outperforms the baseline LiDAR-image detector in terms of the key NDS metric. Our approach successfully reduces the gap in mean average precision (mAP) between single-modality and multi-modality detectors by approximately 72%.