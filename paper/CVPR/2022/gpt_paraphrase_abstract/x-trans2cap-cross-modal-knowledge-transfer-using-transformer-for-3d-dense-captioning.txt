This study focuses on improving the accuracy of 3D dense captioning, which involves describing individual objects in 3D scenes using natural language. Previous approaches have only utilized single modal information, such as point clouds, leading to inaccurate descriptions. While aggregating 2D features into point clouds could be helpful, it adds computational complexity, particularly during inference. To address this, the researchers propose X-Trans2Cap, a cross-modal knowledge transfer method using Transformer for 3D dense captioning. X-Trans2Cap enhances the performance of single-modal 3D captioning through knowledge distillation using a teacher-student framework. During training, the teacher network utilizes auxiliary 2D information to guide the student network, which only takes point clouds as input, using feature consistency constraints. Through a well-designed cross-modal feature fusion module and feature alignment in training, X-Trans2Cap effectively incorporates appearance information from 2D images. As a result, more accurate captions can be generated using only point clouds during inference. The researchers conducted qualitative and quantitative evaluations, which demonstrated that X-Trans2Cap significantly outperforms previous state-of-the-art methods, achieving improvements of approximately +21 and +16 CIDEr points on the ScanRefer and Nr3D datasets, respectively.