Paraphrased:  The accurate bridging of global context interactions is crucial for achieving high-quality image completion when dealing with large masks. Previous approaches using deep or large receptive field (RF) convolutions have been limited by the dominance of nearby interactions, which may lead to subpar results. In this study, we propose a different approach by treating image completion as a directionless sequence-to-sequence prediction task. We employ a transformer to directly capture long-range dependencies, while using a restrictive CNN with small and non-overlapping RFs to represent tokens with weights. This allows the transformer to explicitly model the long-range visible context relations without being influenced by neighboring tokens. Additionally, we introduce a novel attention-aware layer (AAL) to enhance the consistency between visible and generated regions by leveraging distantly related high-frequency features. Extensive experiments demonstrate that our method outperforms state-of-the-art techniques on multiple datasets. The code for our approach is available at https://github.com/lyndonzheng/TFill.