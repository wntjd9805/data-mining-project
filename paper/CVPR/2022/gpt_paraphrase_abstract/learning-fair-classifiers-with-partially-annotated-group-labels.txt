Fairness-aware learning methods are important for ensuring fairness in machine learning models, but most of these methods require fully annotated demographic group labels, which is unrealistic and costly for real-world applications. In this paper, we propose a more practical approach called Algorithmic Group Fairness with Partially annotated Group labels (Fair-PG). We find that existing methods for achieving group fairness perform poorly under Fair-PG compared to vanilla training with full target labels. To address this issue, we introduce a Confidence-based Group Label assignment (CGL) strategy that uses an auxiliary group classifier to assign pseudo group labels, with random labels assigned to low confident samples. Theoretical analysis shows that our method design is better than the vanilla pseudo-labeling strategy in terms of fairness criteria. Empirical results on benchmark datasets demonstrate that combining CGL with state-of-the-art fairness-aware in-processing methods improves both target accuracies and fairness metrics compared to baselines. Additionally, we show that CGL allows for natural augmentation of group-labeled datasets with external target label-only datasets, resulting in improvements in both accuracy and fairness. The code for our method is available at https://github.com/naver-ai/cgl_fairness.