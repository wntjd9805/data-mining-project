The nonuniform quantization strategy is more effective than the uniform strategy in compressing neural networks due to its superior representational capacity. However, existing nonuniform quantization methods often neglect the complex projection process involved in implementing nonuniformly quantized weights/activations, resulting in significant time and space overhead in hardware deployment. To address this issue, we propose a method called Nonuniform-to-Uniform Quantization (N2UQ) that maintains the strong representation ability of nonuniform methods while being hardware-friendly and efficient like uniform quantization during model inference. Our approach involves learning flexible input thresholds that are unevenly spaced to better match the underlying distribution, while quantizing these real-valued inputs into evenly spaced output levels. To train the quantized network with learnable input thresholds, we introduce a generalized straight-through estimator (G-STE) to calculate the backward derivative with respect to threshold parameters, which are otherwise computationally intractable. Additionally, we incorporate entropy preserving regularization to further minimize information loss in weight quantization. Despite the constraint of imposing uniformly quantized weights and activations, our N2UQ outperforms state-of-the-art nonuniform quantization methods by 0.5 to 1.7% on the ImageNet dataset, showcasing the contribution of our N2UQ design. The code and models for our method are available at the following GitHub repository: https://github.com/liuzechun/Nonuniform-to-Uniform-Quantization.