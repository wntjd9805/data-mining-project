Transformers have become widely used in vision tasks due to their effectiveness and scalability, not just in natural language processing. In particular, vector quantized variational autoencoders (VQ-VAEs) are commonly used to convert RGB images into sequences of feature vectors. To enhance the connection between images and text, a new architecture called L-Verse is proposed, which combines a feature-augmented variational autoencoder (AugVAE) and a bidirectional auto-regressive transformer (BiART) for both image-to-text and text-to-image generation. The AugVAE achieves superior reconstruction performance on the ImageNet1K validation set and proves to be robust to unseen images. Unlike other models, BiART can differentiate between an image or text used as a conditional reference and a generation target. L-Verse can directly generate text from images or images from text without the need for fine-tuning or additional object detection frameworks. Through quantitative and qualitative experiments on MS-COCOCaptions, L-Verse demonstrates impressive results compared to previous methods in both image-to-text and text-to-image generation. Additionally, the scalability of the L-Verse architecture is evaluated on Conceptual Captions, and initial results of bidirectional vision-language representation learning in a general domain are presented.