We introduce a technique called pyramid adversarial training (PyramidAT) to enhance the overall performance of Vision Transformer (ViT). Aggressive data augmentation, including adversarial training, is crucial for ViT's strong generalization capabilities. However, previous studies have shown that adversarial training often leads to poor clean accuracy. PyramidAT addresses this issue by pairing it with a "matched" Dropout and stochastic depth regularization, which ensures that the same configuration is applied to both clean and adversarial samples. Similar to AdvProp's impact on CNNs, PyramidAT breaks the trade-off between in-distribution accuracy and out-of-distribution robustness for ViT and related architectures. When trained solely on ImageNet-1K data, our approach achieves a 1.82% absolute improvement in clean accuracy for the ViT-B model on ImageNet, while also enhancing performance on various ImageNet robustness metrics by absolute numbers ranging from 1.76% to 15.68%. We achieve new state-of-the-art results on ImageNet-C (41.42 mCE), ImageNet-R (53.92%), and ImageNet-Sketch (41.04%) without using additional data, solely relying on the ViT-B/16 backbone and our pyramid adversarial training technique. Our code is publicly accessible at pyramidat.github.io.