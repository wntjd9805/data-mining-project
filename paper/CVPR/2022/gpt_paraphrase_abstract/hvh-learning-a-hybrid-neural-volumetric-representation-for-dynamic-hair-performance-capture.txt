This paper addresses the challenges of capturing and rendering realistic hair, which is essential for creating believable avatars. The authors propose a novel approach to representing hair using thousands of primitives, which can be rendered efficiently using neural rendering techniques. To track hair accurately, the authors introduce a new method that operates at the strand level and uses guide hairs and classic techniques to expand the representation. To improve temporal consistency and generalization, the authors optimize the 3D scene flow using multiview optical flow and volumetric raymarching. The proposed method can generate realistic renders for recorded multi-view sequences and can also handle new hair configurations by providing new control signals. The authors compare their approach with existing methods in viewpoint synthesis and drivable animation and achieve state-of-the-art results.