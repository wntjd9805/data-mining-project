This abstract discusses a new approach to video captioning called SWINBERT, which is an end-to-end transformer-based model. Unlike traditional methods that use offline-extracted dense video features, SWINBERT takes video frame patches directly as inputs and generates natural language descriptions. The model uses a video transformer to encode spatial-temporal representations, allowing it to adapt to variable video lengths without specific design for different frame rates. The researchers demonstrate that densely sampled video frames can significantly improve video captioning performance compared to sparsely sampled frames used in previous video-and-language understanding tasks. They also propose learning a sparse attention mask to avoid redundancy in consecutive frames and optimize task-specific performance. Extensive experiments on five video captioning datasets show that SWINBERT outperforms previous methods by a large margin. The learned sparse attention masks not only push the performance limit but can also be transferred to different video lengths and datasets. The code for SWINBERT is available at https://github.com/microsoft/SwinBERT.