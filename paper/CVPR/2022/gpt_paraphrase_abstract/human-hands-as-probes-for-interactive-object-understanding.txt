This paper focuses on the long-standing goal of computer vision: interactive object understanding. The authors propose a solution by studying human hands in egocentric videos captured in real-world environments. They argue that observing the actions performed by human hands and the objects they interact with can provide valuable data and necessary guidance. By paying attention to hands, the researchers are able to locate and stabilize active objects, enabling effective learning and identification of interaction points. Analyzing the movements and gestures of hands also helps determine what actions can be performed on objects and how. To validate their approach, the authors conduct experiments using the EPIC-KITCHENS dataset. They successfully develop state-sensitive features and identify object affordances, such as regions of interaction and potential grasps, solely by observing hands in egocentric videos.