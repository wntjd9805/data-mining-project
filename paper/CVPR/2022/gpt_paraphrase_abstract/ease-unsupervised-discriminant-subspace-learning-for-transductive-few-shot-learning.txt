Few-shot learning (FSL) has gained considerable attention for its ability to adapt to new classes. While there have been various techniques proposed for FSL, most of them focus on improving FSL backbones. Some approaches also aim to enhance the adaptability of these backbones to novel classes by learning on top of the generated features. In this study, we introduce an unsupervised discriminant subspace learning method called EASE, which enhances transductive few-shot learning performance by learning a linear projection onto a subspace constructed from the features of the support set and the unlabeled query set during test time. The proposed EASE method efficiently solves the similarity matrix and dissimilarity matrix generation based on the structure prior using SVD. Additionally, we present a constrained Wasserstein Mean Shift clustering method called SIAMESE, which extends Sinkhorn K-means by incorporating labeled support samples. SIAMESE utilizes the features obtained from EASE to estimate class centers and query predictions. By applying these two steps on the mini-ImageNet, tiered-ImageNet, CIFAR-FS, CUB, and OpenMIC benchmarks, we observe a significant performance boost in transductive FSL and semi-supervised FSL.