Human-annotated attributes are valuable for zero-shot learning, but the process is time-consuming and requires expert supervision. Unsupervised semantic embeddings, such as word embeddings, allow for knowledge transfer between classes. However, word embeddings do not always capture visual similarities accurately and lead to subpar zero-shot performance. In this study, we propose a method to discover semantic embeddings that contain discriminative visual properties for zero-shot learning without human annotation. Our model divides a set of images from known classes into clusters of local image regions based on visual similarity and enhances their class discrimination and semantic relatedness. To associate these clusters with unseen classes, we leverage external knowledge, such as word embeddings, and introduce a novel class relation discovery module. Through both quantitative and qualitative evaluation, we demonstrate that our model discovers semantic embeddings that accurately represent the visual properties of both known and unknown classes. Additionally, we show significant performance improvements over word embeddings using our visually-grounded semantic embeddings across various zero-shot learning models on three benchmark datasets. The code for our model is available at https://github.com/wenjiaXu/VGSE.