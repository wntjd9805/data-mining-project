The success of vision-language representation learning is often attributed to the alignment of images and text using contrastive losses. However, this alignment strategy overlooks the potential within each modality, leading to subpar representations. This issue becomes more pronounced with noisy pre-training data. To address this, we propose triple contrastive learning (TCL) for vision-language pre-training. TCL combines cross-modal alignment with intra-modal self-supervision to improve representation learning. Additionally, TCL maximizes the mutual information between local regions and global summaries of image/text inputs to leverage localized and structural information. Our work is the first to consider local structure information for multi-modality representation learning. Experimental evaluations demonstrate the competitiveness of our approach, achieving state-of-the-art performance on various vision-language tasks.