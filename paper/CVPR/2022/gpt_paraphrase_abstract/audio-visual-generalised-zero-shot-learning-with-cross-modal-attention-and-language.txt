Learning to classify video data from unseen classes is a challenging task known as video-based zero-shot learning. This study proposes using the natural alignment between audio and visual modalities in video data to improve the learning of discriminative multi-modal representations. Specifically, the authors focus on audio-visual zero-shot learning, a relatively underexplored task, and introduce a method that utilizes cross-modal attention and textual label embeddings to transfer knowledge from seen to unseen classes. To make the learning setting more realistic, the authors include all training classes as distractors during test time. To evaluate their method, the authors introduce a (generalized) zero-shot learning benchmark on three audio-visual datasets, VGGSound, UCF, and ActivityNet, ensuring that the unseen test classes are not present in the supervised training dataset. Through comparisons with other relevant methods, the proposed AVCA model demonstrates state-of-the-art performance on all three datasets. The code and data for this study are available at https://github.com/ExplainableML/AVCA-GZSL.