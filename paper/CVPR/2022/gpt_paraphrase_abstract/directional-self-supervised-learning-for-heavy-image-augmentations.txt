This paper introduces a new approach called directional self-supervised learning (DSSL) for improving image representation learning. While there are many augmentation techniques available, only a few have proven to be effective for self-supervised learning. DSSL addresses this limitation by incorporating a wider range of augmentations. It achieves this by applying heavy augmentations to views that have been lightly augmented using standard techniques, resulting in harder views (HV) with greater deviation from the original image. Unlike previous methods that treat all augmented views equally, DSSL treats them as a partially ordered set, considering the directions of the views (SV↔SV, SV←HV). It then utilizes a directional objective function that takes into account the relationships between the views. DSSL can be easily implemented with minimal code and is compatible with popular self-supervised learning frameworks. Experimental results on CIFAR and ImageNet datasets demonstrate that DSSL consistently improves various baseline models and is compatible with a wider range of augmentations. The code for DSSL is available at: https://github.com/Yif-Yang/DSSL.