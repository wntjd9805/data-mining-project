Knowledge distillation is commonly used to enhance the performance of small neural networks. Previous approaches have focused on regressing the teacher's representation features to the student's in a one-to-one spatial matching manner. However, this overlooks the fact that the semantic information at the same spatial location can differ due to architectural variations. This undermines the assumption of one-to-one distillation. To address this, we propose a new approach called one-to-all spatial matching knowledge distillation. In this approach, each pixel of the teacher feature is distilled to all spatial locations of the student features based on their similarity, which is determined by a target-aware transformer. Our approach outperforms state-of-the-art methods by a significant margin on various computer vision benchmarks, including ImageNet, Pascal VOC, and COCOStuff10k. The code for our approach is available at https://github.com/sihaoevery/TaT.