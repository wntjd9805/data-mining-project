Federated semi-supervised learning (FSSL) aims to train a global model using fully-labeled and fully-unlabeled clients or partially labeled clients. Current approaches work well when clients have independent and identically distributed (IID) data, but struggle in a more practical FSSL setting where the data is non-IID. In this paper, we propose a new method called RSCFed (Random Sampling Consensus Federated learning) that addresses the uneven reliability of models from different types of clients. Our approach involves randomly sub-sampling clients to create sub-consensus models, which are then aggregated to form the global model. We also introduce a novel distance-reweighted model aggregation technique to improve the robustness of the sub-consensus models. Experimental results demonstrate that our method outperforms existing methods on benchmark datasets containing natural and medical images. The code for our method is available at https://github.com/XMed-Lab/RSCFed.