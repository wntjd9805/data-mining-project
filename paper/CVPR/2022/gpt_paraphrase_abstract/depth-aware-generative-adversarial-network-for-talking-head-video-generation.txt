The current methods for generating synthetic human face videos heavily rely on 2D representations learned from input images, such as appearance and motion. However, the use of dense 3D facial geometry, such as pixel-wise depth, is crucial for generating accurate 3D face structures and distinguishing background noise. Unfortunately, obtaining dense 3D geometry annotations for videos is expensive and typically not available for this task. In this study, we present a self-supervised face-depth learning method that automatically recovers dense 3D facial geometry from face videos without the need for expensive 3D annotation data. We use the learned dense depth maps to estimate sparse facial keypoints that capture critical head movements. Additionally, we leverage the depth information to learn 3D-aware cross-modal attention, which guides the generation of motion fields for warping source image representations. These contributions form a novel depth-aware generative adversarial network (DaGAN) for talking head generation. Our experiments demonstrate that our proposed method can generate highly realistic faces and achieve significant results on unseen human faces.