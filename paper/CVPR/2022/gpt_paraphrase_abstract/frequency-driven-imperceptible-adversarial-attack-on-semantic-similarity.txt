Current research on adversarial attacks has demonstrated the vulnerability of learning-based classifiers to carefully crafted perturbations. However, existing attack methods have limitations in their ability to generalize across datasets, as they rely on a classification layer with a predefined set of categories. Additionally, these methods often generate perturbations that are easily detectable by the human visual system. To address these issues, we propose a new algorithm that attacks semantic similarity in feature representations. This approach allows us to deceive classifiers without being limited to a specific dataset. To ensure imperceptibility, we introduce a low-frequency constraint that confines perturbations to high-frequency components, thus ensuring perceptual similarity between adversarial examples and original images. Through extensive experiments on three datasets and three online platforms, we demonstrate that our attack can produce misleading and transferable adversarial examples across different architectures and datasets. Furthermore, visualization results and quantitative performance metrics indicate that our algorithm generates more imperceptible perturbations compared to state-of-the-art methods. The code for our algorithm is available at https://github.com/LinQinLiang/SSAH-adversarial-attack.