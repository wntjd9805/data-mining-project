Visual grounding is an important aspect of multimodal reasoning systems as it establishes a precise connection between vision and natural language. Existing approaches in this field utilize query-agnostic visual backbones to extract visual features independently, disregarding the query information. However, we argue that the visual features obtained from these backbones do not align well with the features required for multimodal reasoning. This inconsistency arises due to disparities between pre-training tasks and visual grounding. Additionally, training the visual backbone end-to-end within the visual grounding framework does not fully resolve this issue because the backbones lack query-specificity. To address this problem, we propose a novel approach called Query-modulated Refinement Network (QRNet). QRNet tackles the inconsistency problem by adjusting intermediate features in the visual backbone using a unique Query-aware Dynamic Attention (QD-ATT) mechanism and query-aware multiscale fusion. The QD-ATT dynamically calculates query-dependent visual attention at both the spatial and channel levels of the feature maps generated by the visual backbone. We integrate QRNet into an end-to-end visual grounding framework and conduct extensive experiments to evaluate its performance. The results demonstrate that our proposed method surpasses state-of-the-art techniques across five widely used datasets. To facilitate further research, we have made our code available at https://github.com/LukeForeverYoung/QRNet.