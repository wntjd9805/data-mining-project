This abstract discusses the limitations of existing methods in text-to-image synthesis (T2I) and proposes a novel framework called Semantic-Spatial Aware GAN to address these limitations. The existing methods use conditional generative adversarial networks (GANs) to generate images based on text descriptions. However, these methods often fail to produce images where individual regions or parts are recognizable or consistent with the words in the sentence. To overcome this problem, the proposed framework introduces a Semantic-Spatial Aware block. This block learns a semantic-adaptive transformation that effectively combines text features and image features. It also learns a semantic mask in a weakly-supervised manner to guide the transformation spatially. The experiments conducted on COCO and CUB bird datasets demonstrate that the proposed method outperforms the state-of-the-art approaches in terms of visual fidelity and alignment with input text description. The code for this framework is available at https://github.com/wtliao/text2image.