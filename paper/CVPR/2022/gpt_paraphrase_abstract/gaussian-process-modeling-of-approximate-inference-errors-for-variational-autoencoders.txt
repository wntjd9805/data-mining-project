The variational autoencoder (VAE) is a successful generative model that uses an amortized inference network for test time inference. However, this approach often results in degraded accuracy in posterior approximation compared to instance-wise variational optimization. Semi-amortized approaches have attempted to mitigate this issue, but they suffer from computational overhead during inference. In this paper, we propose a different solution by using a random inference model that models the mean and variance functions of the variational posterior as random Gaussian processes (GP). We view the deviation of the VAE's amortized posterior distribution as random noise, allowing us to treat the approximation error as uncertainty in a principled GP manner. Our model quantifies the difficulty in posterior approximation using a Gaussian variational density and performs inference with a single feed forward pass, which is faster than semi-amortized methods. We demonstrate that our approach achieves higher test data likelihood than current state-of-the-art methods on various benchmark datasets.