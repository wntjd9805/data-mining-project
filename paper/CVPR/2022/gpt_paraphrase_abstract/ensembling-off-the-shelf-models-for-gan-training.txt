The rise of large-scale training has led to the development of numerous powerful visual recognition models. However, generative models like GANs have typically been trained without supervision from scratch. This study explores the possibility of utilizing the collective knowledge from a vast collection of pretrained vision models to enhance GAN training. The question arises as to which models should be chosen and how they can be most effectively employed.The research demonstrates that pretrained computer vision models can significantly enhance performance when used in a group of discriminators. Importantly, the specific subset of models selected greatly impacts the results. To address this, the authors propose an effective selection mechanism that evaluates the linear separability between real and fake samples in pretrained model embeddings. The most accurate model is chosen and progressively added to the discriminator ensemble. Interestingly, this method proves beneficial in both scenarios with limited data and large-scale datasets.The findings reveal that with only 10k training samples, our method achieves a Fr√©chet Inception Distance (FID) on LSUN CAT equivalent to StyleGAN2 trained on 1.6M images. On the complete dataset, our approach improves FID by 1.5 to 2 for the cat, church, and horse categories of LSUN.