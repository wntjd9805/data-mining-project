Generalized zero-shot learning (GZSL) involves training a classifier on known classes to recognize objects from both known and unknown classes. However, the absence of training samples for the unknown classes tends to bias the classifier towards the known classes. To address this issue, previous models have proposed generating visual features for the unknown classes. However, these features lack discriminative ability as they are generated in the visual feature space. To overcome this limitation, some methods have focused on finding a better embedding space for classifier training. These methods prioritize the inter-class relationships of known classes, which results in an embedding space that is biased towards known classes and not conducive to recognizing unknown classes.In this paper, we propose a method called Intra-Class Compactness Enhancement (ICCE) for GZSL. Our method promotes intra-class compactness and inter-class separability in both the embedding space and visual feature space for both known and unknown classes. By emphasizing the intra-class relationships while maintaining inter-class structures, we improve the ability to distinguish between different classes and achieve better generalization. Specifically, we introduce a Self-Distillation Embedding (SDE) module and a Semantic-Visual Contrastive Generation (SVCG) module. The SDE module enhances intra-class compactness in the embedding space, while the SVCG module accomplishes this in the visual feature space.Experimental results demonstrate that our ICCE method outperforms state-of-the-art approaches on four datasets and achieves competitive results on the remaining dataset.