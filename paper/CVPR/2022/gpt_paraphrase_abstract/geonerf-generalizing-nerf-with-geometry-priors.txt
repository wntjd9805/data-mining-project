We introduce GeoNeRF, a novel approach for generating photorealistic images from new viewpoints using neural radiance fields. Our method consists of two main components: a geometry reasoner and a renderer. The geometry reasoner constructs cascaded cost volumes based on nearby source views, while the renderer utilizes a Transformer-based attention mechanism and the cost volumes to infer geometry and appearance, ultimately generating detailed images through volume rendering techniques. This architecture enables advanced occlusion reasoning by leveraging information from consistent source views. Additionally, our method can be easily fine-tuned for specific scenes and achieves comparable results to scene-specific neural rendering methods at a significantly reduced computational cost. Experimental results demonstrate that GeoNeRF outperforms existing generalizable neural rendering models on both synthetic and real datasets. Furthermore, we propose an alternative model that adapts to RGBD images by modifying the geometry reasoner to leverage depth information commonly available from depth sensors. The implementation code for GeoNeRF can be accessed at https://www.idiap.ch/paper/geonerf.