The need for efficient searching of sign language videos has been recognized as a valuable application of sign language technology. However, there has been limited research on searching videos using more than just keywords. To address this gap, we introduce the task of sign language retrieval using free-form textual queries. The objective is to find the sign language video that best matches a written query. We propose a solution by learning cross-modal embeddings on the How2Sign dataset, which is a large-scale collection of American Sign Language videos. We identify that the quality of the sign video embedding is a key factor in the system's performance, but there is a lack of labeled training data. To overcome this limitation, we present SPOT-ALIGN, a framework that combines iterative sign spotting and feature alignment to increase the availability of training data. We demonstrate the effectiveness of SPOT-ALIGN in improving both sign recognition and video retrieval tasks.