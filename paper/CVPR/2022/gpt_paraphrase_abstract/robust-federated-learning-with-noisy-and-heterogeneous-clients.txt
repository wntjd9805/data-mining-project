This paper addresses the challenge of modeling heterogeneous federated learning, where each client designs its own model. Existing algorithms fail to effectively address the noise present in local clients due to annotation difficulty and free-riding participant issues. The paper presents a new solution called RHFL (Robust Heterogeneous Federated Learning) that tackles the problem of label noise and performs federated learning simultaneously. RHFL has three key features: (1) It aligns model feedback between heterogeneous models using public data, eliminating the need for additional shared global models for collaboration. (2) It employs a robust noise-tolerant loss function to mitigate the negative effects of internal label noise. (3) It introduces a client confidence re-weighting scheme to handle challenging noisy feedback from other participants, assigning adaptive weights to each client during the collaborative learning stage. Extensive experiments demonstrate the effectiveness of RHFL in reducing the negative effects of different noise rates/types in both model homogeneous and heterogeneous federated learning settings, consistently outperforming existing methods.