We present a novel approach for detecting out-of-distribution (OoD) samples using deep hybrid models (DHMs). These models capture the joint density of features and labels in a single forward pass. By decomposing the joint density into three sources of uncertainty, our method effectively identifies samples that are semantically different from the training data. To ensure scalability, we incorporate weight normalization during training, allowing us to utilize state-of-the-art deep neural network architectures for modeling and inferring probability distributions. Our approach offers an efficient, versatile framework for estimating predictive uncertainty, supported by promising results and theoretical justification. Remarkably, we achieve 100% accuracy in OoD detection tasks across vision and language datasets, including challenging dataset pairs like CIFAR-10 vs. SVHN and CIFAR-100 vs. CIFAR-10. This work represents a significant step towards deploying deep neural networks in real-world applications that require safety assurance.