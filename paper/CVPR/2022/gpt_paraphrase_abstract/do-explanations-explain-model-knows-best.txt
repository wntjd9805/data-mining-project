The contribution of input features to a neural network's output is still unknown. Various methods have been proposed to explain this phenomenon. However, these methods often point to different important features, leading to the question of which explanation to trust. To address this, we propose a framework that evaluates these explanations using the neural network itself. This framework uses the network to generate input features that produce a specific output behavior. With these generated features, we design controlled experiments to assess whether an explanation method aligns with a particular principle. Therefore, we introduce an empirical framework for evaluating explanation methods based on axioms. We apply this framework to evaluate popular and promising explanation solutions, allowing us to uncover their properties and limitations. Overall, this framework serves as a valuable tool to uncover strengths and weaknesses in current and future explanation methods.