The objective of image-to-video person re-identification is to identify a specific pedestrian from a collection of videos based on an image query. Current approaches treat this task as a cross-modality retrieval problem, where they attempt to learn latent embeddings from both image and video modalities. However, these methods are less effective and efficient due to the large gap between the two modalities and the redundant feature learning using all video frames. In this study, we propose a new approach called Temporal Complementarity-Guided Reinforcement Learning (TCRL) for image-to-video person re-identification. Instead of considering it as a cross-modality retrieval task, we view it as a point-to-set matching problem similar to human decision-making. TCRL utilizes deep reinforcement learning to sequentially determine the number of frames to select from the video gallery, accumulating temporal complementary information based on the query image to achieve a balance between efficiency and accuracy. The TCRL approach formulates the point-to-set matching procedure as a Markov decision process, where a sequential judgement agent evaluates the uncertainty between the query image and historical frames at each time step, deciding whether enough complementary clues have been gathered for a judgment or if additional frames are required. Additionally, TCRL incorporates a sequential feature extraction module with complementary residual detectors to dynamically suppress redundant salient regions and extract diverse complementary clues from the selected frames, enhancing the representation at the frame level. Extensive experiments demonstrate the superiority of our proposed TCRL method.