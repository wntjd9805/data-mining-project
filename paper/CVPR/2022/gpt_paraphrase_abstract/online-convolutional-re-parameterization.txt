The use of structural re-parameterization has become increasingly popular in computer vision tasks. Its purpose is to enhance the performance of deep models without adding any additional time during inference. However, these models often require complex training-time blocks to achieve high accuracy, resulting in significant training costs. In this study, we propose a two-stage pipeline called online convolutional re-parameterization (OREPA) to address this issue. OREPA compresses the intricate training-time block into a single convolution, effectively reducing training overhead. To optimize the online blocks, we introduce a linear scaling layer. Additionally, we explore more effective re-parameterization components. Compared to existing re-parameterization models, OREPA reduces training-time memory costs by approximately 70% and accelerates training speed by about 2 times. Moreover, when equipped with OREPA, the models outperform previous methods on ImageNet by up to 0.6%. We also conduct experiments on object detection and semantic segmentation, consistently demonstrating improvements in downstream tasks. The code for OREPA is available at https://github.com/JUGGHM/OREPA_CVPR2022.