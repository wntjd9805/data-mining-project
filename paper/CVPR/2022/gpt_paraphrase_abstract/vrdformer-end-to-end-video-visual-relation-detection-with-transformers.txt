The paper introduces a new framework called VRDFormer, which aims to improve the understanding of visual relations in videos. Existing methods for video visual relation detection typically use a multi-stage framework, which has limitations in capturing long-term spatio-temporal contexts and is inefficient. In contrast, VRDFormer adopts a transformer-based approach that integrates and unifies these stages. The model utilizes query-based methods to generate relation instances in an autoregressive manner. It incorporates static and recurrent queries to facilitate efficient object pair tracking with spatio-temporal contexts. The model is trained jointly with object pair detection and relation classification. Experimental results on two benchmark datasets, ImageNet-VidVRD and VidOR, demonstrate that VRDFormer outperforms existing methods in relation detection and tagging tasks. The code for VRDFormer is publicly available on GitHub.