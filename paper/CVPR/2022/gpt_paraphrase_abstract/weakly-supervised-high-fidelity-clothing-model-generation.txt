The rise of online economics has increased the need for generating images of models wearing clothes to showcase new products and boost sales. However, the high cost of proprietary model images poses a challenge for existing virtual try-on methods, as they typically require large amounts of paired model and clothing images for training. To address this issue, we introduce a cost-effective and scalable approach called DeepGenerative Projection (DGP) that operates in a weakly-supervised manner. Our method aims to replicate the process by which humans predict the effect of wearing clothes, relying on unsupervised imagination based on life experience rather than learned computational rules. We utilize a pretrained StyleGAN to capture the practical experience of wearing clothes. Experimental results demonstrate that projecting the approximate alignment of clothing and body onto the StyleGAN space produces realistic images of models wearing clothes. When compared to several state-of-the-art supervised methods, our DGP outperforms them in generating model images of clothing, as shown in Figure 1. This includes accurately reconstructing patterns and achieving high levels of realism.