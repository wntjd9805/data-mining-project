In recent years, significant advancements have been made in the field of semantic image synthesis. However, the challenge of generating photo-realistic images with intricate details remains. Previous approaches have primarily focused on utilizing semantic maps, which only capture object-level layouts in an image. To enhance the generation of object details, it is crucial to consider fine-grained part-level semantic layouts, which can be inferred from an object's shape. To address this, we propose a Shape-aware Position Descriptor (SPD) that encodes the object's shape into each pixel's positional feature. Additionally, we introduce a Semantic-shape Adaptive Feature Modulation (SAFM) block that combines the given semantic map with our positional features to produce adaptively modulated features. Through extensive experiments, we demonstrate that our proposed SPD and SAFM significantly enhance the generation of objects with intricate details. Furthermore, our method outperforms state-of-the-art approaches based on both quantitative and qualitative evaluations. Interested individuals can access the source code and model at SAFM.