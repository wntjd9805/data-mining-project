Neural network quantization is a technique used to reduce the bit-widths of weights and activations in order to improve memory and computational efficiency. Existing methods typically use pre-defined functions with learnable parameters to build the quantizer, but these complex quantizers introduce significant computational overhead during inference. This paper proposes a simpler approach by formulating the quantization process as a lookup operation and learning lookup tables as quantizers. The lookup tables can be trained with the network in an end-to-end manner and have very minimal additional computational cost. Experimental results demonstrate that networks using these lookup tables achieve state-of-the-art performance on various tasks such as image classification, image super-resolution, and point cloud classification.