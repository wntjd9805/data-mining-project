Recently, a technique called Sharpness-Aware Minimization (SAM) has been successful in improving the performance of training large-scale models like vision transformers by connecting the geometry of the loss landscape and generalization. However, SAM's update rule requires two sequential gradient computations at each step, which can significantly increase computational overhead. In this paper, we propose a new algorithm called LookSAM, which reduces the additional training cost of SAM by only periodically calculating the inner gradient ascent. Empirical results show that LookSAM achieves similar accuracy gains as SAM but with much faster computation - it has a comparable complexity to first-order optimizers like SGD or Adam. To further evaluate LookSAM's performance and scalability, we introduce a layer-wise modification and conduct experiments in a scenario with large-batch training, which is more likely to converge to sharp local minima. Using the proposed algorithms, we are the first to successfully scale up the batch size for training Vision Transformers, achieving competitive performance by training ViTs from scratch in minutes with a batch size of 64k. The code for LookSAM is available at the following link: [insert link].