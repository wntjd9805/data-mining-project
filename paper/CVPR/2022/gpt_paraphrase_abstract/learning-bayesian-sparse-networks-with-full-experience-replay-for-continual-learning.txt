Continual Learning (CL) methods aim to prevent machine learning models from forgetting previously learned tasks while learning new ones. Existing approaches use various techniques like knowledge distillation and regularization, but they still suffer from interference between tasks, resulting in catastrophic forgetting. To address this issue, we propose Sparse Neural Network for Continual Learning (SNCL), which activates and selects only sparse neurons for learning current and past tasks, leaving more parameter space for future tasks. This reduces interference between parameters for different tasks. SNCL incorporates variational Bayesian sparsity priors on neuron activations in all layers, and utilizes Full Experience Replay (FER) to supervise the learning of sparse activations. A loss-aware reservoir-sampling strategy is developed to maintain the memory buffer. SNCL is applicable to different network structures and task boundaries. Experimental results on various datasets demonstrate that SNCL achieves state-of-the-art performance in mitigating forgetting.