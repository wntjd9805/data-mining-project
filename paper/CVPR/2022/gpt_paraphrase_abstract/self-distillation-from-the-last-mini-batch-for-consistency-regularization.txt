Knowledge distillation (KD) is a regularization technique that enhances generalization ability by using soft targets from a pre-trained teacher network or an ensemble of student networks. However, existing KD methods that employ complex models are time-consuming and computationally expensive. To address this, self KD methods have been proposed, but they either require architectural modifications or are difficult to parallelize. In this study, we propose a more efficient and reliable self-distillation framework called Self-Distillation from Last Mini-Batch (DLB). Our approach involves rearranging the sampling process by splitting each mini-batch, with one half coinciding with the previous iteration and the other half coinciding with the upcoming iteration. The former half mini-batch then distills soft targets generated in the previous iteration. This mechanism improves training stability, consistency, and robustness to label noise without requiring additional memory or model modifications. Experimental results on three classification benchmarks demonstrate that our approach consistently outperforms state-of-the-art self-distillation methods with different network architectures. Furthermore, our method is compatible with augmentation strategies, leading to additional performance improvements. The code for our approach is available at https://github.com/Meta-knowledge-Lab/DLB.