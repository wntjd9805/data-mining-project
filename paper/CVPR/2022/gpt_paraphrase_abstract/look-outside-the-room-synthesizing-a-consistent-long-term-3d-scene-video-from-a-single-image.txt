Recent advancements in 3D deep learning and rendering techniques have led to increased interest in novel view synthesis from a single image. However, existing methods are limited in their ability to synthesize new views with significant camera motions. In this paper, we propose a novel approach that addresses this limitation by synthesizing a consistent long-term video using a single scene image and a trajectory of large camera motions. Our approach incorporates an autoregressive Transformer to sequentially model multiple frames, considering the relationships between frames and the corresponding cameras to predict the next frame. To enhance learning and ensure consistency among generated frames, we introduce a locality constraint based on the input cameras, guiding self-attention across a large number of patches in both space and time. Our method surpasses state-of-the-art view synthesis techniques, particularly when synthesizing long-term future views in indoor 3D scenes. More information can be found on our project page at https://xrenaa.github.io/look-outside-room/.