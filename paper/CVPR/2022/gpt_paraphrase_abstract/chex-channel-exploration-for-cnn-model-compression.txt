Channel pruning is a widely used technique to reduce the computational and memory requirements of deep convolutional neural networks. However, current pruning methods have limitations in that they are restricted to the pruning process only and require a fully pre-trained large model. These limitations can lead to sub-optimal model quality and excessive memory and training costs. To address these issues, we propose a new methodology called ChannelExploration (CHEX). Instead of solely relying on pruning, we suggest repeatedly pruning and regrowing channels during the training process to avoid prematurely pruning important channels. We use a column subset selection (CSS) formulation to tackle the channel pruning problem within each layer, and we allow for dynamic reallocation of channel numbers across all layers under a global channel sparsity constraint. Furthermore, all the exploration is done in a single training session from scratch without the need for a pre-trained large model. Our experimental results demonstrate that CHEX effectively reduces the FLOPs (floating-point operations) of various CNN architectures across different computer vision tasks, such as image classification, object detection, instance segmentation, and 3D vision. For instance, our compressed ResNet-50 model achieves 76% top-1 accuracy on the ImageNet dataset with only 25% FLOPs of the original ResNet-50 model, surpassing previous state-of-the-art channel pruning methods. The checkpoints and code for CHEX are available at [link].