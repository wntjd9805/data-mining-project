Recent advancements in deep learning have heavily relied on the use of large datasets with labels to train models with high capacity. However, the process of collecting such datasets in a time- and cost-efficient manner often leads to the introduction of label noise. In this study, we propose a technique for learning from noisy labels that capitalizes on the similarities between training examples in the feature space. Our method encourages the prediction for each example to be similar to its closest neighbors. Unlike other training algorithms that employ multiple models or distinct stages, our approach involves a simple regularization term. It can be interpreted as an inductive version of the classical transductive label propagation algorithm. We extensively evaluate our method on various datasets, including both synthetic (CIFAR-10, CIFAR-100) and realistic (mini-WebVision, WebVision, Clothing1M, mini-ImageNet-Red) noise scenarios. Our results demonstrate competitive or state-of-the-art accuracies across all datasets.