We investigate a less explored scenario in class-incremental learning (CIL) where a strong model is pre-trained on a large number of base classes. We propose a two-stage training approach: feature augmentation and fusion. In feature augmentation, we fine-tune a cloned part of the backbone on the novel data. In fusion, we combine the base and novel classifiers into a unified classifier. Experimental results on the ImageNet dataset demonstrate that our method outperforms state-of-the-art CIL methods by a significant margin, achieving a 10% increase in overall accuracy. We also analyze practical CIL scenarios, including base-novel overlap with distribution shift, and show that our method is robust and generalizes well across all analyzed settings.