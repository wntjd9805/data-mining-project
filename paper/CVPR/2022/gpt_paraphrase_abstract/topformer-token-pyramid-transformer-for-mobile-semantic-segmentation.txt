This study introduces a mobile-friendly architecture called TokenPyramid Vision Transformer (TopFormer) to address the computational challenges of vision transformers (ViTs) in dense prediction tasks like semantic segmentation on mobile devices. TopFormer utilizes Tokens from various scales to generate scale-aware semantic features, which are then integrated into the corresponding tokens to enhance representation. Experimental results show that our approach outperforms CNN- and ViT-based networks in multiple semantic segmentation datasets, achieving a balance between accuracy and latency. On the ADE20K dataset, TopFormer achieves 5% higher accuracy in mIoU compared to MobileNetV3, while maintaining lower latency on ARM-based mobile devices. Additionally, the smaller version of TopFormer achieves real-time inference with competitive results on ARM-based mobile devices. The code and models can be accessed at: https://github.com/hustvl/TopFormer.