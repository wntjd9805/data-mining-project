Motion style transfer is a popular technique used to enhance character animation. Traditionally, motion style transfer algorithms are designed for offline use, where motions are processed in segments. However, for online animation applications like real-time avatar animation from motion capture, motions need to be processed as a continuous stream with minimal delay. In this study, we present a flexible and high-quality motion style transfer method specifically designed for this online setting. We introduce a new model called Style-ERD, which utilizes an Encoder-Recurrent-Decoder structure to stylize motions in real-time. Additionally, we propose a novel discriminator that combines feature attention and temporal attention. Our method is capable of stylizing motions into multiple target styles using a single model. Despite targeting online settings, our approach surpasses previous offline methods in terms of motion realism and style expression, while also significantly improving runtime efficiency.