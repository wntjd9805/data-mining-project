Video-language pre-training has recently achieved significant success in retrieval tasks by utilizing large-scale datasets and strong transformer networks. However, current video-language transformer models lack explicit fine-grained semantic alignment. In this study, we introduce Object-aware Transformers, an object-centric approach that extends the video-language transformer by incorporating object representations. Our key idea is to utilize bounding boxes and object tags to guide the training process. We evaluate our model on three standard video-text matching sub-tasks using four widely used benchmarks. Additionally, we provide in-depth analysis and detailed ablation studies on our proposed method. Our results demonstrate clear performance improvement across all tasks and datasets, highlighting the benefits of integrating object representations into a video-language architecture. The code for our model is available at https://github.com/FingerRec/OA-Transformer.