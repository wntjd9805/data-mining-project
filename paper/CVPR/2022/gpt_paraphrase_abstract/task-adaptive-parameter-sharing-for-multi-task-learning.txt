Adapting pre-trained models for various tasks is a common practice in machine learning. However, fine-tuning different models for each task can be memory-intensive. To address this issue and efficiently learn multiple tasks, we propose Task Adaptive Parameter Sharing (TAPS). TAPS is a straightforward approach that modifies only a small subset of layers in the base model to adapt it to a new task. By doing so, TAPS minimizes resource usage, prevents catastrophic forgetting, and avoids competition between tasks. TAPS solves an optimization problem to determine the shared layers and the task-specific weights. Additionally, it incorporates a sparsity penalty to encourage weight sharing with the base model. Compared to other methods, TAPS achieves high accuracy on target tasks while introducing a small number of task-specific parameters. Furthermore, TAPS is architecture-agnostic and requires minimal changes to the training scheme. We evaluate TAPS on various fine-tuning tasks and architectures (ResNet, DenseNet, ViT) and demonstrate that it achieves state-of-the-art performance while being easy to implement.