We propose a novel approach to reconstructing the depth of dynamic objects from videos. While previous methods have focused on improving monocular depth estimators without considering restrictions on the deformation of dynamic parts, we believe that constraining deformations is essential for accurate 3D reconstruction. Our approach, called Keypoint Transporter, fits a dynamic point cloud to the video data using Sinkhorn's algorithm to associate 3D points with 2D pixels. We also use a differentiable point renderer to ensure the compatibility of 3D deformations with measured optical flow. By modeling the overall deformation of the object throughout the entire video, Keypoint Transporter can better constrain the reconstruction process, reducing ambiguity. Compared to weaker deformation models, our approach achieves superior or comparable reconstruction quality to prior methods while being faster and relying on a pre-trained monocular depth estimator network. We evaluate our method on new synthetic datasets and demonstrate qualitative results on real-world videos of pets.