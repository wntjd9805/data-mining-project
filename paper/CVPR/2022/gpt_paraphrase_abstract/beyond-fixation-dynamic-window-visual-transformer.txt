There has been a recent increase in interest in visual transformers that aim to reduce computational costs by limiting the calculation of self-attention within a local window. However, most current approaches use a fixed single-scale window without considering the impact of window size on model performance. This limitation hinders the modeling potential of window-based models for capturing multi-scale information. This paper introduces a new method called Dynamic Window Vision Transformer (DW-ViT) that addresses this issue. DW-ViT employs a dynamic window strategy that surpasses the fixed single window approach. It utilizes different window sizes assigned to different head groups of window multi-head self-attention to capture multi-scale information. The information from these windows is dynamically fused using varying weights assigned to each multi-scale branch. The performance of DW-ViT is evaluated on three datasets (ImageNet-1K, ADE20K, and COCO) and compared to state-of-the-art methods. The results show that DW-ViT achieves the best performance compared to other methods, including the current state-of-the-art Swin Transformers. Moreover, DW-ViT demonstrates good scalability and can be easily integrated into any window-based visual transformers.