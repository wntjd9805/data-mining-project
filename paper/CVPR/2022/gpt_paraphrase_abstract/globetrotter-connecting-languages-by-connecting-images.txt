Machine translation between multiple languages is a difficult task due to the lack of supervision across all language pairs. However, we propose a novel approach that leverages the consistent visual appearance of the world across languages. Instead of relying on parallel corpora or topological properties, our method utilizes visual observations to bridge the language gap. Our model aligns text segments from different languages only when the associated images are similar and each image is well-aligned with its textual description. We have trained our model from scratch on a comprehensive dataset consisting of text in over fifty languages and corresponding images. Experimental results demonstrate that our approach surpasses previous methods in unsupervised word and sentence translation using retrieval techniques. The code, models, and data are accessible at globetrotter.cs.columbia.edu.