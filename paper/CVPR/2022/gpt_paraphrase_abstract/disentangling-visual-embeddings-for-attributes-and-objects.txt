We investigate the issue of compositional zero-shot learning in object-attribute recognition. Previous studies have relied on visual features extracted from a backbone network that was pre-trained for object classification. However, these features fail to capture the subtle distinctions associated with attributes. To address this limitation, these studies incorporate supervision from the linguistic space, utilizing pre-trained word embeddings to improve the recognition of attribute-object pairs. In contrast, our approach shifts the focus back to the visual space and introduces a novel architecture capable of separating attribute and object features. We utilize visual decomposed features to generate representative embeddings for both familiar and novel compositions, thereby enhancing the regularization of our model. Through extensive experiments on three datasets (MIT-States, UT-Zappos, and a new benchmark based on VAW), we demonstrate that our method significantly outperforms existing approaches. The code, models, and dataset splits can be accessed at https://github.com/nirat1606/OADis.