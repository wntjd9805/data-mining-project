Multimodal learning, which integrates different senses, is crucial for a comprehensive understanding of the world. While multiple input modalities are expected to enhance model performance, we have found that they are not fully utilized, even when a multimodal model outperforms its unimodal counterpart. This paper highlights the issue of optimization imbalance in existing multimodal discriminative models, where a uniform objective is designed for all modalities. In some scenarios, a dominant modality such as sound or vision can hinder the optimization of other modalities. To address this, we propose on-the-fly gradient modulation, which adaptively controls the optimization of each modality by monitoring their contribution to the learning objective. Additionally, we introduce dynamic Gaussian noise to prevent any potential decline in generalization caused by gradient modulation. Our approach yields significant improvements over common fusion methods in various multimodal tasks and can also enhance existing multimodal methods. This demonstrates the effectiveness and versatility of our simple strategy. The source code for our approach is available at https://github.com/GeWu-Lab/OGM-GE_CVPR2022.