We present a new self-supervised learning framework for medical image analysis inspired by the success of Vision Transformers (ViTs) in global and local representation learning. Our framework, called Swin UNETR, is a 3D transformer-based model with a hierarchical encoder designed for self-supervised pre-training. We also introduce tailored proxy tasks that focus on learning the underlying patterns of human anatomy. To evaluate the effectiveness of our approach, we pre-trained the model using 5,050 publicly available computed tomography (CT) images from various body organs. We further fine-tuned the pre-trained models on the Beyond the Cranial Vault (BTCV) Segmentation Challenge and the Medical Segmentation Decathlon (MSD) dataset, specifically for segmenting 13 abdominal organs. Our model currently achieves state-of-the-art performance on the public test leaderboards of both the MSD1 and BTCV 2 datasets. The code for our model is available at https://monai.io/research/swin-unetr.