Transformers have shown great potential in computer vision tasks, with their attention-based token mixer module believed to contribute significantly to their performance. However, recent studies have demonstrated that the attention module can be replaced by spatial MLPs while still achieving good results. This suggests that the overall architecture of transformers, rather than the specific token mixer module, is more crucial for model performance. To investigate this, we replace the attention module with a simple spatial pooling operator in transformers, resulting in a model called PoolFormer. Surprisingly, PoolFormer achieves competitive performance on various computer vision tasks, such as ImageNet-1K, surpassing well-tuned vision transformer/MLP-like baselines with fewer parameters and computational operations. The success of PoolFormer supports our hypothesis and prompts the introduction of "MetaFormer," a general architecture abstracted from transformers without specifying the token mixer. Extensive experiments suggest that MetaFormer plays a vital role in achieving superior results for transformer and MLP-like models in vision tasks. This work calls for future research focused on improving MetaFormer rather than token mixer modules. Additionally, PoolFormer can serve as a starting point for designing future MetaFormer architectures.