In recent studies on video prediction, the focus has been on passive forecasting and low-level action-conditional prediction, neglecting the learning of interactions between agents and objects. This paper introduces a new task called semantic action-conditional video prediction, which involves using semantic action labels to describe these interactions. This task can be seen as an inverse problem of action recognition. The main challenge of this task is effectively incorporating semantic action information into the model. Drawing inspiration from the concept of Mixture of Experts, this paper proposes a novel video prediction model called Modular Action Concept Network (MAC). In MAC, each abstract label is represented by a combination of different visual concept learners. The proposed method is evaluated on three datasets, including two synthetic datasets and one real-world dataset. The experimental results demonstrate that MAC can accurately generate future frames based on given instructions without the need for bounding boxes. The trained model also shows out-of-distribution generalization, quick adaptation to new object categories, and the ability to utilize its learned features for object detection, indicating progress towards higher-level cognitive abilities. Additional visualizations can be found at http://www.pair.toronto.edu/mac/.