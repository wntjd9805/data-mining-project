This paper proposes a method to enhance the performance of self-supervised learning (SSL) in learning visual representations from unlabeled images. The current state-of-the-art SSL methods lack the advantage of utilizing the architectural benefits of the underlying neural network. The focus of this paper is on Vision Transformers (ViTs), which have shown superiority over convolutional networks in various visual tasks. ViTs process patch-level representations internally by taking a sequence of disjoint patches from an image. Inspired by this characteristic, the authors introduce a simple yet effective visual pretext task called SelfPatch to improve patch-level representations. The SelfPatch task enforces invariance among patches and their neighbors, considering similar neighboring patches as positive samples. This approach enables ViTs to learn more semantically meaningful relations among patches without using human-annotated labels. The authors demonstrate that SelfPatch significantly enhances the performance of existing SSL methods in object detection and semantic segmentation tasks. For example, SelfPatch improves the performance of the self-supervised ViT, DINO, by achieving +1.3 average precision (AP) on COCO object detection, +1.2 AP on COCO instance segmentation, and +2.9 mean intersection over union (mIoU) on ADE20K semantic segmentation.