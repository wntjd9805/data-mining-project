This paper focuses on the problem of Cloth-Changing person re-identification (CC-ReID), which involves matching the same person across different locations over a long duration. The goal is to handle this problem efficiently and accurately using only a single image for identification, which is useful for surveillance applications. The proposed approach, named GI-ReID, incorporates gait recognition as an auxiliary task to learn cloth-agnostic representations. This is achieved through a two-stream architecture consisting of an image ReID-Stream and an auxiliary gait recognition stream (Gait-Stream). The Gait-Stream encourages the ReID-Stream to capture cloth-invariant biometric motion features during training, while a Gait Sequence Prediction (GSP) module is designed to extract temporal continuous motion cues from a single image. A semantics consistency constraint is enforced between the two streams for effective knowledge regularization. Experimental results on multiple benchmarks show that GI-ReID outperforms state-of-the-art methods in image-based Cloth-Changing ReID.