This paper presents a novel approach to Unsupervised Domain Generalization (UDG) that allows for generalized representation learning across different visual domains without any training supervision in the source or target domains. The approach involves self-supervised learning of a Bridge Across Domains (BrAD), which acts as an auxiliary bridge domain with semantic-preserving visual mappings from each training domain. The BrAD and its mappings are learned together with a contrastive self-supervised representation model that aligns the domains to their BrAD-projections, thereby promoting semantic alignment between all domains. The proposed method utilizes an edge-regularized BrAD and demonstrates significant improvements across various benchmarks and tasks, including UDG, Few-shot UDA, and unsupervised generalization across multi-domain datasets, even on unseen domains and classes.