This study addresses the need for machines to acquire knowledge on how humans manipulate objects from two perspectives: understanding object affordances and learning human interactions based on those affordances. However, current databases lack a comprehensive understanding of these perspectives. To address this, the researchers propose OakInk, a multi-modal and rich-annotated knowledge repository for visual and cognitive understanding of hand-object interactions. They collect 1,800 household objects and annotate their affordances to create the first knowledge base, Oak. They also record rich human interactions with 100 selected objects in Oak. Then, they transfer these interactions to their virtual counterparts using a novel method called Tink, creating the second knowledge base, Ink. Overall, OakInk includes 50,000 distinct affordance-aware and intent-oriented hand-object interactions. The researchers evaluate OakInk on pose estimation and grasp generation tasks and propose two practical applications: intent-based interaction generation and handover generation. The dataset and source code are publicly available at www.oakink.net.