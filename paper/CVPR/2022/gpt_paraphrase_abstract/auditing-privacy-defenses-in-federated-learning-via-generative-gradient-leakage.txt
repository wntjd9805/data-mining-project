The Federated Learning (FL) framework allows multiple clients to participate in a learning task while preserving privacy by not sharing their private data. However, recent studies have shown that private information can still be leaked through shared gradient information. To enhance privacy protection, defense mechanisms have been proposed, such as using additive noise or gradient compression. This study validates that private training data can still be leaked, even with these defenses, through a new type of leakage called Generative Gradient Leakage (GGL). Unlike existing methods that rely solely on gradient information, this method utilizes the latent space of generative adversarial networks (GAN) trained on public image datasets to compensate for information loss during gradient degradation. To tackle nonlinearity introduced by the gradient operator and GAN model, various gradient-free optimization methods are explored, demonstrating their superiority in reconstructing high-quality images from gradients compared to gradient-based optimizers. The proposed method aims to serve as a tool for empirically measuring privacy leakage, facilitating the development of more robust defense mechanisms.