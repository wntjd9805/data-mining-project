Lidars and cameras are essential sensors for 3D detection in autonomous driving. While existing methods simply add camera features to raw lidar point clouds and feed them into 3D detection models, our study demonstrates that fusing deep lidar features with camera features instead of raw points can improve performance. However, aligning the transformed features from both modalities is a challenge. In this paper, we propose two new techniques: InverseAug, which reverses geometric-related augmentations to achieve accurate alignment, and LearnableAlign, which uses cross-attention to capture correlations between image and lidar features during fusion. Using these techniques, we develop a set of multi-modal 3D detection models called DeepFusion, which outperforms previous methods. Our models achieve state-of-the-art performance on the Waymo Open Dataset and demonstrate robustness against input corruptions and out-of-distribution data. The code will be publicly available at the provided GitHub link. Our method fuses modalities at the deep feature level, in contrast to previous methods that decorate lidar points with camera features at the input level. To address the alignment challenge, we introduce InverseAug and LearnableAlign, which are techniques for achieving accurate alignment and feature-level alignment, respectively.