Domain generalization algorithms aim to develop models that can effectively generalize to unseen domains by utilizing training data from multiple domains. However, recent benchmarks have revealed that most existing algorithms do not outperform basic benchmarks. Additionally, the current evaluation methods fail to uncover the factors contributing to this poor performance. To address this issue, we propose an evaluation framework for domain generalization algorithms that allows for the decomposition of errors into different components, each capturing distinct aspects of generalization. Building upon the prevalent use of domain-invariant representation learning algorithms, we extend the framework to encompass various types of failures in achieving invariance. Our analysis demonstrates that the primary contributor to the generalization error varies depending on the method, dataset, regularization strength, and training duration. We identify two problems associated with the strategy of learning domain-invariant representations. Firstly, on the Colored MNIST dataset, most domain generalization algorithms fail because they only achieve domain-invariance on the training domains. Secondly, on the Camelyon-17 dataset, domain-invariance diminishes the quality of representations on unseen domains. As an alternative direction, we propose focusing on optimizing the classifier on top of a rich representation as a promising approach.