There has been a significant increase in the use of quantitative evaluation in recent video inpainting research. However, the content used to assess performance has not been given enough attention. Factors such as camera and background scene motion affect the difficulty of the inpainting task and impact different methods differently. Existing evaluation methods do not account for these factors, resulting in limited understanding of inpainting failure modes. To bridge this gap, we introduce the Diagnostic Evaluation of Video Inpainting on Landscapes (DEVIL) benchmark. This benchmark consists of two components: (i) a new dataset of videos and masks labeled with various key inpainting failure modes, and (ii) an evaluation scheme that selects specific slices of the dataset based on fixed content attributes and rates performance on reconstruction, realism, and temporal consistency. By identifying systematic changes in performance caused by specific content characteristics, our challenging benchmark provides deeper analysis of video inpainting methods and serves as an invaluable diagnostic tool for the field. The code and data for DEVIL are available at github.com/MichiganCOG/devil.