In recent years, there has been increasing interest in using the human skeleton as a representation of human action. Many methods for action recognition based on skeletons utilize Graph Convolutional Networks (GCNs) to extract features. However, these GCN-based methods have limitations in terms of robustness, interoperability, and scalability. To address these limitations, we propose a new approach called PoseConv3D for skeleton-based action recognition. Instead of using a graph sequence, PoseConv3D relies on a 3D heatmap volume as the base representation of human skeletons. Compared to GCN-based methods, PoseConv3D is more effective in learning spatiotemporal features, more robust against pose estimation noises, and generalizes better in cross-dataset settings. Additionally, PoseConv3D can handle multiple-person scenarios without incurring additional computation costs. The hierarchical features of PoseConv3D can be easily integrated with other modalities at early fusion stages, allowing for improved performance. Our approach achieves state-of-the-art results on five out of six standard skeleton-based action recognition benchmarks. When fused with other modalities, it achieves state-of-the-art performance on all eight multi-modality action recognition benchmarks. We have made the code available at https://github.com/kennymckormick/pyskl.