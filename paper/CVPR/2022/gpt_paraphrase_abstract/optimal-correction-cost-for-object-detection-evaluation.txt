Mean Average Precision (mAP) is the primary metric used to evaluate object detection models. However, mAP is based on ranked instance retrieval and may not be suitable for all downstream tasks. To address this limitation, we propose a new evaluation measure called Optimal Correction Cost (OC-cost) that assesses detection accuracy at the image level. OC-cost calculates the cost of correcting detections to ground truths by solving an optimal transportation problem. Unlike mAP, OC-cost properly penalizes false positive and false negative detections and treats every image in the dataset equally. Our experimental results demonstrate that OC-cost better aligns with human preference compared to mAP for a single image. Additionally, detectors' rankings based on OC-cost are more consistent across different data splits than mAP. Our intention is not to replace mAP but to provide an additional tool for evaluating detectors from a different perspective. We conduct a series of experiments to compare and contrast mAP and OC-cost, aiming to assist future researchers and developers in selecting the most appropriate evaluation metric.