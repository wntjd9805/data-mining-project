This paper introduces a new approach to hair editing in computer vision and graphics. Existing methods often require sketches or masks for editing, which can be cumbersome for users. To address this, the proposed method allows users to manipulate hair attributes individually or collectively using text descriptions or reference images. The method encodes both the image and text conditions in a shared embedding space and utilizes the Contrastive Language-Image Pre-Training (CLIP) model for hair editing. The framework is designed to disentangle hair attributes and achieves high-quality editing results with accuracy, visual realism, and preservation of irrelevant attributes. Extensive experiments validate the effectiveness of the approach.