Neuromorphic cameras, also known as event cameras, imitate human vision by detecting changes in scene intensity rather than capturing entire frames like conventional cameras. These changes, known as events, can be numerous when the scene or the camera is in rapid motion. However, this rapid movement often leads to missed or overridden events when creating a tensor for machine learning purposes. To address this issue, we propose a method to focus on the dense events and generate a concise yet detailed event representation for depth estimation. Our approach involves learning a model using events from both past and future, but only utilizing past data during inference along with predicted future events. Initially, we estimate depth solely based on events, but we also suggest incorporating images and events through a hierarchical network that combines event and intensity information for improved depth estimation. Through experiments conducted in challenging real-world scenarios, we demonstrate that our method surpasses previous techniques even with low computational requirements. The code for our method is available at: https://github.com/yonseivnl/se-cff.