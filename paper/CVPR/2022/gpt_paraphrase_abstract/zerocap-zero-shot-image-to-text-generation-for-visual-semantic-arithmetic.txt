Recent models for matching text to images have been successful in providing accurate scores for matching and zero-shot tasks. However, these models are unable to generate captions for images. In this study, we adapt these models to generate descriptive text for images during inference, without any additional training or tuning. We achieve this by combining a visual-semantic model with a large language model, leveraging the knowledge from both models. The resulting captions are less restrictive compared to supervised captioning methods. Additionally, as a zero-shot learning method, our approach is highly flexible and can perform image arithmetic, where inputs can be either images or text, and the output is a sentence. This allows for novel high-level vision capabilities, such as comparing two images or solving visual analogy tests. The code for our approach is available at the following GitHub link: https://github.com/YoadTew/zero-shot-image-to-text.