Vision transformers have shown impressive performance on various tasks, but they require significant computational resources. This paper introduces AdaViT, an adaptive computation framework that dynamically determines which patches, self-attention heads, and transformer blocks to use for each input image. By optimizing the decision-making process alongside the transformer backbone, AdaViT significantly improves inference efficiency without sacrificing accuracy. Experimental results on ImageNet demonstrate over a 2Ã— improvement in efficiency compared to state-of-the-art vision transformers, with only a 0.8% decrease in accuracy. The learned usage policies reveal insights into the redundancy present in vision transformers. The code for AdaViT is available at https://github.com/MengLcool/AdaViT.