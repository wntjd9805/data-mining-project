Backdoor attacks pose a serious security threat to deep learning models as they can manipulate the behavior of predictions in the testing phase by providing users with a model trained on poisoned data. These backdoored models appear normal when processing clean images but can produce incorrect predictions when the input contains a specific trigger pattern. Existing backdoor attacks focus on defining triggers that are imperceptible in the input space, without considering the abnormality of these triggers' latent representations in the poisoned model. These attacks are vulnerable to backdoor detection algorithms and visual inspection. To address these limitations, we propose a new and stealthy backdoor attack called DEFEAT. This attack poisons clean data by incorporating adaptive imperceptible perturbations and constrains the latent representation during the training process to enhance the stealthiness and resistance of our attack against defense algorithms. We perform extensive experiments on various image classifiers using real-world datasets to demonstrate the effectiveness of our attack. Our results show that DEFEAT can withstand state-of-the-art defenses, successfully deceive victim models without compromising their utility, and maintain practical stealthiness in image data.