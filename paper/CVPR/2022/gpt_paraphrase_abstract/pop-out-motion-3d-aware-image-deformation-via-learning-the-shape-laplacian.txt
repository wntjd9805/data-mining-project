We propose a framework that allows for the deformation of objects in a 2D image based on their 3D spatial representation. Current methods for 3D-aware image manipulation are limited in that they either only change the overall scene information or depth, or they are restricted to manipulating specific object categories. In this paper, we introduce a 3D-aware image deformation method that overcomes these limitations by offering minimal restrictions on shape category and deformation type. While our framework utilizes 2D-to-3D reconstruction, we argue that reconstruction alone is insufficient for achieving realistic deformations as it is prone to topological errors. Therefore, we suggest a supervised learning-based approach that predicts the shape Laplacian of the underlying volume of a 3D reconstruction represented as a point cloud. By calculating the deformation energy using the predicted shape Laplacian and user-defined deformation handles (such as keypoints), we can obtain bounded biharmonic weights that accurately model handle-based image deformation. Our experimental results demonstrate the effectiveness of our approach in deforming 2D character and clothed human images. Additionally, we provide quantitative evidence showing that our method produces more accurate deformation weights compared to alternative methods such as mesh reconstruction and point cloud Laplacian methods.