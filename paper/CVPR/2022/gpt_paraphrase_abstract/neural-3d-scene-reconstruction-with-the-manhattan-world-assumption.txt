This paper focuses on the challenge of reconstructing 3D indoor scenes using multiple-view images. While previous works have achieved impressive results on textured objects, they struggle with low-textured planar regions commonly found in indoor scenes. One potential solution is to incorporate planar constraints into depth map estimation in multi-view stereo-based methods. However, existing approaches lack efficiency and multi-view consistency in per-view plane estimation and depth optimization. In this study, we propose integrating planar constraints into implicit neural representation-based reconstruction methods. Specifically, we utilize a multilayer perceptron (MLP) network to represent the signed distance function as the scene geometry. By leveraging the Manhattan-world assumption, we employ planar constraints to regularize the geometry in floor and wall regions predicted by a 2D semantic segmentation network. To address inaccurate segmentation, we encode the semantics of 3D points with another MLP and introduce a novel loss function that jointly optimizes scene geometry and semantics in 3D space. Experimental evaluations on the ScanNet and 7-Scenes datasets demonstrate that our approach significantly outperforms previous methods in terms of 3D reconstruction quality. The code and supplementary materials can be accessed at https://zju3dv.github.io/manhattan_sdf.