Adversarial examples can deceive black-box models, and targeted attacks that exploit transferability have gained significant attention for their practical use. To enhance the success rate of transferring adversarial examples, it is crucial to avoid overfitting to the source model. Image augmentation is a popular approach for achieving this, but previous methods have been limited by their use of simple image transformations like resizing, which restricts input diversity. To overcome this limitation, we propose the object-based diverse input (ODI) method. This technique involves creating an adversarial image on a 3D object and ensuring that the rendered image is classified as the target class by the model. Our inspiration for this method comes from humans' ability to perceive images printed on 3D objects. If the image is clear enough, humans can recognize its content under various viewing conditions. Similarly, if an adversarial example appears to be the target class to the model, the model should also classify the rendered image of the 3D object as the target class. The ODI method enhances input diversity by utilizing multiple source objects and randomizing viewing conditions. In our experiments using the ImageNet-Compatible dataset, we found that this method significantly improves the average success rate of targeted attacks, increasing it from 28.3% to 47.0% compared to state-of-the-art techniques. We also demonstrate the effectiveness of the ODI method in creating adversarial examples for face verification tasks, where it shows superior performance improvement. Our code for implementing the ODI method is available at https://github.com/dreamflake/ODI.