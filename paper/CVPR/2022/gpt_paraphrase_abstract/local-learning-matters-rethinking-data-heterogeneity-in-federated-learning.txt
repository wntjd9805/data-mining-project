Federated learning (FL) is a promising approach for distributed learning with a network of clients, but the non-IID nature of data distribution among clients makes optimization challenging. Many FL algorithms try to address data heterogeneity by introducing proximal terms, but these can be computationally expensive. Instead, we propose focusing on local learning generality to tackle data heterogeneity in FL. Through a systematic study, we find that standard regularization methods are effective in mitigating data heterogeneity. Based on this finding, we introduce a simple and efficient method called FedAlign to overcome data heterogeneity and the limitations of previous approaches. FedAlign achieves competitive accuracy with state-of-the-art FL methods while minimizing computation and memory overhead. The code for FedAlign is available at https://github.com/mmendiet/FedAlign.