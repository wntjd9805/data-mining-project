Many recent self-supervised frameworks for visual representation learning are based on Siamese networks, which consist of two parallel encoders. However, these networks often have practical asymmetry due to various mechanisms implemented to break the symmetry. In this study, we investigate the importance of asymmetry by explicitly differentiating the two encoders as source and target encodings. Our key finding is that maintaining a relatively lower variance in the target encodings compared to the source encodings generally improves learning. We support this empirically through five case studies involving different variance-oriented designs, which align with our preliminary theoretical analysis. Additionally, we observe that the benefits of asymmetric designs extend to longer training schedules, multiple frameworks, and newer backbones. By combining several asymmetric designs, we achieve state-of-the-art accuracy on ImageNet linear probing and competitive results on downstream transfer tasks. We hope that our exploration of asymmetry in Siamese representation learning will inspire further research in this area.