Transformers have high model capacity and are suitable for object detection. However, they are not as effective as CNN-based detectors. This is due to several reasons: (a) Cross-attention is used for both classification and bounding-box regression tasks, (b) Transformer's decoder poorly initializes content queries, and (c) Self-attention does not consider certain prior knowledge that could improve inductive bias. To address these limitations, we propose three contributions. Firstly, we introduce a new model called Detection Split Transformer (DE-STR) that separates cross-attention into two branches, one for classification and the other for box regression. Secondly, we utilize a mini-detector to initialize the content queries in the decoder using classification and regression embeddings from the mini-detector's respective heads. Lastly, we enhance self-attention in the decoder to consider pairs of adjacent object queries. Our experiments on the MS-COCO dataset demonstrate that DE-STR outperforms DETR and its successors.