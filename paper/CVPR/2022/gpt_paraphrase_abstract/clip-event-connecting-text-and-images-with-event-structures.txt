Vision-language pretraining models have been successful in multimedia applications by understanding the connections between images and text. However, these models often overlook the alignment of events and their corresponding argument structures. To address this, we propose a contrastive learning framework that encourages vision-language models to comprehend events and their associated roles by utilizing text extraction techniques and employing multiple prompt functions. We also introduce an event graph alignment loss based on optimal transport to capture event argument structures. Additionally, we curate a large dataset with rich event information for pretraining, which serves as a challenging benchmark for evaluating the understanding of complex sentences. Our experiments demonstrate that our zero-shot CLIP-Event outperforms the current supervised model in argument extraction on Multimedia Event Extraction, achieving a significant 5% absolute F-score gain in event extraction and notable improvements in various downstream tasks under zero-shot conditions.