Deep learning-based approaches in appearance-based gaze estimation have shown impressive performance. However, the task of generalizing these algorithms to unseen environments is still challenging due to limited data and absence of target labels. This paper introduces a new approach called Rotation-enhanced Unsupervised Domain Adaptation (RUDA) for gaze estimation. The authors discover the rotation-consistency property in gaze estimation and propose the use of 'sub-labels' for unsupervised domain adaptation. The original images are rotated at different angles during training, and domain adaptation is performed under the constraint of rotation consistency. In the target domain, images are assigned with sub-labels derived from relative rotation angles instead of real labels. A novel distribution loss is then introduced to facilitate domain adaptation using these sub-labels. The RUDA framework is evaluated on four cross-domain gaze estimation tasks, showing significant performance improvements ranging from 12.2% to 30.5% compared to baseline methods. The proposed framework also has potential applications in other computer vision tasks with physical constraints.