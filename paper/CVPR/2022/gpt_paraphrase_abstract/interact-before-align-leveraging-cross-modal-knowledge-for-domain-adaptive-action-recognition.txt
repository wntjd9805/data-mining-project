Unsupervised domain adaptive video action recognition is a challenging task that involves recognizing actions in a target domain using a model trained with out-of-domain annotations. To tackle this challenge, researchers have explored the use of multi-modal inputs such as RGB, Flow, and Audio. Previous approaches have either aligned each modality individually or learned representations through cross-modal self-supervision. However, we propose a different approach that leverages cross-modal interaction to effectively align the domains. By allowing modalities to supplement missing transferable information, cross-modal knowledge interaction takes advantage of the complementary nature of different modalities. Additionally, our model highlights the most transferable aspects of data by finding a consensus among all modalities. We introduce two modules in our model: the first module exchanges complementary transferable information across modalities in the semantic space, and the second module identifies the most transferable spatial region based on cross-modal consensus. Extensive experiments on benchmark datasets, including the challenging EPIC-Kitchens-100, demonstrate that our proposed method outperforms state-of-the-art approaches.