We introduce Block-NeRF, a variation of Neural Radiance Fields (NeRF) designed to handle large-scale environments. Our research demonstrates that to effectively render city-scale scenes spanning multiple blocks, it is essential to decompose the scene into individually trained NeRFs. This decomposition allows for rendering scalability and updates on a per-block basis, while decoupling rendering time from scene size.   To address challenges associated with capturing data over extended periods and under varying environmental conditions, we implement architectural modifications to enhance the robustness of NeRF. We incorporate appearance embeddings, learned pose refinement, and controllable exposure into each NeRF instance. Additionally, we develop a method for aligning appearance across adjacent NeRFs to seamlessly combine them.  Leveraging these advancements, we construct a grid of Block-NeRFs using a dataset of 2.8 million images, resulting in the most extensive neural scene representation created thus far. This representation has the capability to render an entire neighborhood in San Francisco.