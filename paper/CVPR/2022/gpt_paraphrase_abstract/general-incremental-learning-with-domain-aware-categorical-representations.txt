Continual learning is a crucial challenge in achieving human-level intelligence in real-world applications. It involves an agent continuously accumulating knowledge in response to streaming data/tasks. This study focuses on an incremental learning problem that has received little attention so far. In this problem, both the class distribution and the class-specific domain distribution change over time.Aside from the usual challenges faced in class incremental learning, this setting also encounters two additional difficulties. The first is the intra-class stability-plasticity dilemma, which refers to the need for a balance between preserving existing knowledge and acquiring new knowledge within the same class. The second challenge is the intra-class domain imbalance, which involves dealing with imbalanced data within and across classes.To tackle these issues, a new domain-aware continual learning method is proposed based on the EM framework. This method introduces a flexible class representation using the von Mises-Fisher mixture model to capture the intra-class structure. It employs an expansion-and-reduction strategy to dynamically adjust the number of components based on the complexity of the class. Additionally, a bi-level balanced memory is designed to address data imbalances within and across classes. This memory, combined with a distillation loss, helps achieve a better trade-off between inter- and intra-class stability-plasticity.To evaluate the effectiveness of the proposed method, extensive experiments are conducted on three benchmarks: iDigits, iDomainNet, and iCIFAR-20. The results consistently demonstrate the superiority of the approach, as it outperforms previous methods by a significant margin.