We present a new method for generating coherent UV coordinates for loose clothing. Our approach is not limited by human body outlines and can accurately represent loose garments and hair. We have developed a differentiable pipeline that learns the UV mapping between a series of RGB inputs and textures using UV coordinates. Instead of treating each frame's UV coordinates separately, our method connects all UV coordinates through feature matching to ensure temporal stability. We then train a generative model that balances spatial quality and temporal stability using supervised and unsupervised losses in both UV and image spaces. Our experiments demonstrate that our trained models produce high-quality UV coordinates and can generalize to new poses. Once our model infers a sequence of UV coordinates, it can be used to create new looks and modify visual styles flexibly. Compared to existing methods, our approach significantly reduces the computational workload required to animate new outfits.