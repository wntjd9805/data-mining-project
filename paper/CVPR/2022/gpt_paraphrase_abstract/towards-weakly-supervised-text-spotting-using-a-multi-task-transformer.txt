Text spotting methods that integrate text detection and recognition have gained attention in recent literature. Existing approaches typically have separate branches for detection and recognition, requiring precise annotations for both tasks. However, we propose a transformer-based approach called TextTranSpotter (TTS) that can be trained using both fully- and weakly-supervised settings. Our method learns a single latent representation per word detection and employs a novel loss function based on the Hungarian loss, eliminating the need for costly localization annotations. Remarkably, our weakly-supervised approach achieves competitive performance with previous fully-supervised methods by training solely on text transcription annotations from actual data. Moreover, when trained in a fully-supervised manner, TextTranSpotter outperforms state-of-the-art results on various benchmarks.