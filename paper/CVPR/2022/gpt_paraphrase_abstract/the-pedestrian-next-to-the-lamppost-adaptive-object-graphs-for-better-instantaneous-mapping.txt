A popular technique for autonomous control and navigation is estimating a semantically segmented birdâ€™s-eye-view (BEV) map from a single image. However, there is an increase in localization error as the distance from the camera increases. This increase in error is expected because localization becomes harder at a distance. However, the drop in performance is also due to the cues used by current texture-based models, specifically object-ground intersections like shadows, which become sparse and uncertain for distant objects. To address these limitations, we propose a graph neural network that learns the spatial relationship between objects in a scene. This network predicts BEV objects from a monocular image by reasoning about an object within the context of other objects. Our approach achieves state-of-the-art performance in BEV estimation from monocular images across three large-scale datasets, including a 50% relative improvement for objects on nuScenes.