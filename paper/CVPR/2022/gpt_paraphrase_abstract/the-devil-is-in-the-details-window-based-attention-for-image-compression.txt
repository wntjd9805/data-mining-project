Learned image compression methods have shown better rate-distortion performance compared to traditional image compression standards. However, existing models based on Convolutional Neural Networks (CNNs) have a drawback in capturing local redundancy, particularly non-repetitive textures, which negatively impacts the quality of reconstruction. To address this issue, we propose combining the local-aware attention mechanism with global-related feature learning, inspired by recent advancements in Vision Transformer (ViT) and Swin Transformer. In this study, we extensively investigate the effects of various attention mechanisms for learning local features and introduce a window-based local attention block that is straightforward yet effective. This window-based attention can be easily integrated into both CNN and Transformer models, enhancing their performance. Furthermore, we introduce a novel Symmetrical Transformer (STF) framework that utilizes absolute transformer blocks in the down-sampling encoder and up-sampling decoder. Extensive experiments demonstrate that our proposed method is highly effective and surpasses state-of-the-art techniques in image compression. The code for our method is publicly available at the provided GitHub link (https://github.com/Googolxx/STF).