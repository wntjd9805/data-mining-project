We utilize neural radiance fields (NeRFs) to construct interactive 3D environments from extensive visual captures, such as those obtained from drones, which can cover large areas including buildings and multiple city blocks. However, working with such large-scale scenes presents several challenges. Firstly, we need to model thousands of images with varying lighting conditions, each capturing only a small portion of the scene. Secondly, the models required for this task are too large to be trained on a single GPU. Lastly, there are significant obstacles in achieving fast rendering to enable interactive fly-throughs.To tackle these challenges, we begin by examining the visibility statistics of large-scale scenes. This analysis motivates us to adopt a sparse network structure, where parameters are specialized for different regions of the scene. Additionally, we introduce a simple geometric clustering algorithm to implement data parallelism, dividing the training images (or pixels) into different NeRF sub-modules that can be trained simultaneously. We evaluate our approach using existing datasets (Quad 6k and UrbanScene3D) as well as our own drone footage, resulting in a 3x improvement in training speed and a 12% increase in PSNR (Peak Signal-to-Noise Ratio) accuracy. Furthermore, we assess the performance of recent NeRF fast renderers on top of Mega-NeRF and propose a novel method that leverages temporal coherence. Our technique achieves a 40x acceleration in rendering speed compared to traditional NeRF rendering, while still maintaining a PSNR quality within 0.8 dB. This surpasses the fidelity provided by existing fast renderers.