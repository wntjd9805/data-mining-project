Anticipating future actions in videos is crucial for real-world agents. Humans consider long-term relationships over the entire sequence of actions, including both past and potential future actions. To address this, we present Future Transformer (FUTR), an attention-based model that predicts minutes-long sequences of future actions. FUTR utilizes global attention across all input frames and output tokens, enabling parallel decoding and improving accuracy and efficiency for long-term anticipation. We evaluate FUTR on two benchmark datasets, Breakfast and 50 Salads, and achieve state-of-the-art performance.