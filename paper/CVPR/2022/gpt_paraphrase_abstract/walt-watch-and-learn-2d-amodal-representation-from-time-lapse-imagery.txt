Current methods for detecting, segmenting, and tracking objects struggle when faced with severe occlusions in busy urban environments. The scarcity of labeled real data on occlusions, coupled with the domain gap in synthetic data, makes it challenging to explicitly model and learn occlusions. This study proposes a novel approach that combines both real and synthetic data to automatically supervise occlusions using time-lapse imagery from stationary webcams observing street intersections over extended periods. The researchers introduce a new dataset called Watch and Learn Time-lapse (WALT), comprising 12 cameras capturing urban environments throughout a year. By leveraging this real data, they are able to mine a large collection of unoccluded objects and composite them in the same views to generate occlusions. This longitudinal self-supervision enables an amodal network to learn representations of object-occluder-occluded layers. The researchers also demonstrate a method to expedite the discovery of unoccluded objects and establish a relationship between the confidence in this discovery and the training accuracy and speed for occluded objects. After several days of watching and autonomous learning, this approach exhibits significant improvements in detecting and segmenting occluded people and vehicles compared to human-supervised amodal approaches.