Knowledge distillation (KD) is a widely-used technique that uses large networks to improve the performance of smaller models. However, existing KD methods often require the student model to mimic the teacher model exactly, which can be problematic for low-capacity models. In this study, we propose a novel Evaluation-oriented KD method (EKD) for deep face recognition that aims to reduce the performance gap between the teacher and student models during training. We use commonly used evaluation metrics in face recognition, such as False Positive Rate (FPR) and True Positive Rate (TPR), as performance indicators. We identify the critical pair relations that cause the difference in TPR and FPR between the teacher and student models, and then constrain the student model to approximate these relations using a rank-based loss function. This approach allows for more flexibility in the student model with lower capacity. Extensive experiments on popular benchmarks show that our EKD method outperforms state-of-the-art competitors. The illustration in Figure 1 demonstrates the critical relations of samples, where different colors represent different models (Teacher T in blue and Student S in green), and different shapes represent samples of different subjects. The numbers indicate the cosine similarities of samples. The relation of the 1st and 3rd samples is the only one that falls on different sides of the threshold in the teacher and student models, causing the TPR difference. To achieve the same TPR as the teacher, the student model with limited capacity should focus on the critical relation (in red) of the 1st and 3rd samples. Similarly, for negative pairs, the relation of the 1st and 5th samples leads to the FPR difference and should be given more attention.