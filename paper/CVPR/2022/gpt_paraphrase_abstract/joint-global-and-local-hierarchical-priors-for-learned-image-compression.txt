In recent years, learned image compression methods have proven to be more effective than traditional hand-crafted approaches, including BPG. The success of these methods can be attributed to the use of learned entropy models, which estimate the probability distribution of the quantized latent representation. Most current learned entropy models are based on convolutional neural networks (CNNs), but these models have a limitation in capturing long-range dependencies due to their local connectivity nature. This limitation can be a significant obstacle in image compression, as reducing spatial redundancy is crucial. To address this issue, we introduce a new entropy model called Information Transformer (Informer) that leverages both global and local information in a content-dependent manner using an attention mechanism. Our experiments demonstrate that Informer significantly improves rate-distortion performance compared to state-of-the-art methods on the Kodak and Tecnick datasets, without the computational complexity problem. Our source code for Informer is publicly available at https://github.com/naver-ai/informer.