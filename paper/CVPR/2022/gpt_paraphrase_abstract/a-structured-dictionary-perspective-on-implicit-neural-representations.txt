Recent advancements in implicit neural representations (INRs) have shown great potential in signal representation compared to traditional discrete representations. However, the underlying mechanism of how INRs represent signals is not well understood. To address this, we propose a novel unified perspective to analyze INRs theoretically. By integrating concepts from harmonic analysis and deep learning theory, we demonstrate that most INR families can be viewed as structured signal dictionaries. These dictionaries consist of integer harmonics derived from a set of initial mapping frequencies. This inherent structure allows INRs to efficiently represent signals with increasing frequencies by using a linearly growing number of parameters with depth.Additionally, we investigate the inductive bias of INRs by leveraging recent findings on the empirical neural tangent kernel (NTK). We establish that the eigenfunctions of the NTK can be interpreted as dictionary atoms, whose inner product with the target signal influences the quality of signal reconstruction. Interestingly, we discover that meta-learning has a similar effect on the NTK as dictionary learning, where dictionary atoms are formed by combining examples encountered during meta-training. This finding sheds light on the reshaping impact of meta-learning on the NTK.Our findings not only contribute to the design and optimization of novel INR architectures but also hold relevance for the broader deep learning theory community.