Recently, there has been a trend in training models on large amounts of unlabeled data using self-supervised learning (SSL) and fine-tuning them on various downstream tasks. However, most SSL methods lack the ability to adapt to different downstream scenarios, such as different data domains, vision tasks, and computational constraints. Neural architecture search (NAS) has been widely used to address these issues, but it is not feasible to apply NAS to SSL since there are no labels or metrics available for model selection.   In this paper, we propose a NAS approach called DATA that is specifically designed for SSL. DATA provides Domain-Aware and Task-Aware pre-training by training a supernet that represents a diverse set of networks without labels. We also introduce a flexible searching mechanism that is compatible with SSL, allowing for the selection of networks with different computational costs for various downstream vision tasks and data domains, even without explicit metrics.   By applying our method, instantiated with MoCo v2, we achieve promising results across a wide range of computational costs on downstream tasks such as image classification, object detection, and semantic segmentation. DATA is orthogonal to existing SSL methods and allows for customization based on specific downstream needs. We validate the generalizability of our approach through extensive experiments on other SSL methods. The code for DATA is available at https://github.com/GAIA-vision/GAIA-ssl.  Figure 1 illustrates the working principle of DATA. We first construct a supernet consisting of multiple subnets and train them simultaneously using self-supervised learning. Then, we propose an unsupervised searching method that enables domain-aware and task-aware model selection without the need for labels. This mechanism allows self-supervised models to adapt to various scenarios, including point, edge, and cloud, and to different vision tasks such as image classification, object detection, and segmentation. The network architectures shown in the figure are visualized using the software PlotNeuralNet.