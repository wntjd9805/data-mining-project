Modern handheld devices are capable of capturing burst image sequences in quick succession. However, each individual frame in the burst sequence often suffers from various degradations and misalignments caused by camera shake and object motions. The objective of Burst Image Restoration is to effectively combine complementary information from multiple burst frames in order to generate high-quality outputs. To achieve this, we propose a novel approach that focuses on exchanging information between burst frames to filter out degradations while preserving and enhancing the actual scene details.  Our key idea is to create a set of pseudo-burst features that combine information from all input burst frames to seamlessly exchange information. However, the creation of pseudo-burst features requires proper alignment of individual burst frames to account for inter-frame movements. Therefore, our approach begins by extracting pre-processed features from each burst frame and matching them using an edge-boosting burst alignment module. The pseudo-burst features are then created and enriched using multi-scale contextual information.  The final step of our approach is to adaptively aggregate information from the pseudo-burst features to progressively increase resolution in multiple stages while merging the pseudo-burst features. In comparison to existing methods that typically employ a late fusion scheme with single-stage upsampling, our approach performs favorably and achieves state-of-the-art results in burst super-resolution, burst low-light image enhancement, and burst denoising tasks.  The source code and pre-trained models for our approach are available at https://github. com/akshaydudhane16/BIPNet.