Existing methods for counting repetitive actions in videos focus on short videos, making it difficult to handle longer videos in realistic scenarios. This lack of generalization capability is mainly due to the absence of long video datasets. To address this gap, we introduce a new large-scale dataset that includes videos of various lengths and realistic situations where action interruptions or inconsistencies occur. In addition to counting annotations, our dataset also provides fine-grained annotations of action cycles. With 1,451 videos and approximately 20,000 annotations, this dataset presents a more challenging task for repetitive action counting in realistic scenarios. To improve the performance and efficiency of repetitive action counting, we propose a method that encodes multi-scale temporal correlation using transformers. This approach considers both performance and efficiency. Furthermore, leveraging the fine-grained annotation of action cycles, we introduce a density map regression-based method to predict the action period. This method achieves better performance and interpretability compared to existing approaches. Our proposed method outperforms state-of-the-art methods on all datasets and achieves superior performance on an unseen dataset without fine-tuning. The dataset and code are publicly available.