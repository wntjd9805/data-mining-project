Semantic segmentation of point cloud data is a crucial task for applications like autonomous driving. Recent advancements in point cloud segmentation have focused on local aggregation operators and point sampling methods. However, unlike image segmentation, there has been limited exploration of the fundamental issue of scale and how different scales should interact and merge. This study aims to efficiently and effectively integrate features at varying scales and stages in a point cloud segmentation network. The commonly used encoder-decoder architecture is expanded upon by introducing scale pyramid architectures that facilitate the flow of information both laterally and vertically across scales. Additionally, a cross-scale attention feature learning block is incorporated to enhance multi-scale feature fusion throughout the network. This design of multi-scale processing and fusion significantly improves accuracy without significantly increasing computation. When implemented on the KPConv network, consistent enhancements are observed across various datasets, with state-of-the-art performance achieved on NPM3D and S3DIS. Furthermore, the pyramid architecture is versatile and can be applied to other network designs, as demonstrated by similar improvements over RandLANet.