We present UNIST, the initial deep neural implicit model for general-purpose shape-to-shape translation in 2D and 3D domains without paired data. Unlike existing methods that use point clouds, our model utilizes autoencoding implicit fields. Additionally, our translation network is trained on a latent grid representation that combines the advantages of latent-space processing and position awareness. This allows our model to not only achieve significant shape transformations but also preserve spatial features and fine local details for natural shape translations. Our model can learn both style-preserving content alteration and content-preserving style transfer using the same network architecture and input domain pairs. We demonstrate the versatility and quality of our translation results by comparing them to established baselines. The code for our model is available at https://qiminchen.github.io/unist/.