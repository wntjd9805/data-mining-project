In order for deep learning systems to function effectively in real-world high-stakes environments, they need to be able to handle various types of noise that can affect their performance. Data-end defense has emerged as a practical solution to improve the robustness of these systems by manipulating the input data instead of modifying the models themselves. However, existing data-end defenses have shown limited ability to generalize against different types of noise and have difficulty transferring across multiple models. To overcome these challenges, we propose a defensive patch generation framework that enhances the utilization of both local and global features in deep learning models. To address the issue of generalization against diverse noises, we incorporate class-specific identifiable patterns into local patches, allowing the defensive patches to preserve more recognizable features for specific classes. This enables the models to achieve better recognition performance in the presence of noise. Additionally, to improve transferability across multiple models, we guide the defensive patches to capture more global feature correlations within a class. This enables the patches to activate shared global perceptions in the models, facilitating better transfer of knowledge between different models. Our defensive patches have the potential to significantly enhance the robustness of deep learning applications in practical scenarios, as they can be easily applied to target objects. Extensive experiments demonstrate that our approach outperforms existing methods by a large margin, achieving more than 20% improvement in accuracy for both adversarial attacks and corruption robustness, both in digital and physical environments.