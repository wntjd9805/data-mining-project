This paper introduces DualStyleGAN, a novel approach for high-resolution portrait style transfer that allows for flexible control of dual styles in the original face domain and the extended artistic portrait domain. Unlike StyleGAN, DualStyleGAN utilizes an intrinsic style path and a new extrinsic style path to characterize the content and style of a portrait, respectively. The extrinsic style path is designed to modulate both color and complex structural styles in order to accurately mimic the style example. Additionally, the paper presents a progressive fine-tuning scheme to smoothly transform the generative space of the model to the target domain, despite the modifications made to the network architecture. Experimental results demonstrate that DualStyleGAN outperforms state-of-the-art methods in terms of high-quality portrait style transfer and flexible style control. The code for DualStyleGAN is available at https://github.com/williamyang1991/DualStyleGAN.