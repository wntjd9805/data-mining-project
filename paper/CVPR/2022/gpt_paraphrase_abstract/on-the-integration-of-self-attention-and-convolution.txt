This paper explores the relationship between convolution and self-attention techniques in representation learning. The authors demonstrate that these two approaches share a common underlying operation, with the bulk of computations being performed in the same way. They show that a traditional convolution can be decomposed into individual 1x1 convolutions followed by shift and summation operations. Similarly, the projections of queries, keys, and values in self-attention can be interpreted as multiple 1x1 convolutions followed by attention weight computation and value aggregation. This observation leads to the proposal of a mixed model, called ACmix, that combines the benefits of both self-attention and convolution while minimizing computational overhead. Extensive experiments on image recognition and downstream tasks demonstrate that ACmix outperforms competitive baselines consistently. The code and pre-trained models will be made available on GitHub and Gitee.