Referring video object segmentation (RVOS) is a complex task that involves understanding both the semantic information of video content and language queries to predict objects. However, current methods use multi-modal fusion at a frame-based spatial level, which leads to vision-language mismatching and poor segmentation results. To overcome this limitation, we propose a new approach that learns multi-level representations by considering the inherent structure of the video content. This approach generates discriminative visual embeddings at different levels of granularity, including long-temporal information at the video level, spatial semantics at the frame level, and enhanced object-aware features at the object level. By combining these embeddings with dynamic alignment techniques, our model produces a robust representation for accurate video object segmentation. Experimental results on Refer-DAVIS17 and Refer-YouTube-VOS datasets demonstrate that our model outperforms others in both segmentation accuracy and inference speed.