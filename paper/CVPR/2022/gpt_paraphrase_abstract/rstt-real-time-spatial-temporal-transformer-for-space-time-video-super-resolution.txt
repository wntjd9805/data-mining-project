Space-time video super-resolution (STVSR) refers to the process of enhancing videos with both low frame rate and low resolution to produce high-frame-rate and high-resolution versions. Existing methods that use Convolutional Neural Networks (CNN) have achieved visually satisfactory results but suffer from slow inference speed due to their complex architectures. To address this issue, we propose a spatial-temporal transformer that combines spatial and temporal super-resolution modules into a single model. Unlike CNN-based methods, our approach does not explicitly separate temporal interpolations and spatial super-resolutions, but instead utilizes a single end-to-end transformer architecture. We create a reusable dictionary using encoders based on the input low frame rate and low-resolution frames, which is then used in the decoder to synthesize the high-frame-rate and high-resolution frames. In comparison to the state-of-the-art TMNet, our network is 60% smaller in terms of parameters and 80% faster in terms of frame rate without sacrificing much performance. The source code for our approach is available at https://github.com/llmpass/RSTT.