Concerns regarding morality, security, and privacy have arisen due to various facial manipulation techniques. While current face forgery classifiers show promise in detecting fake images, they are susceptible to adversarial examples that introduce imperceptible perturbations on the pixels. Additionally, many face forgery detectors rely on the frequency diversity between real and fake faces as a crucial clue. To address these issues, we propose a frequency adversarial attack method against face forgery detectors. Instead of injecting adversarial perturbations into the spatial domain, we utilize discrete cosine transform (DCT) on input images and introduce a fusion module to capture the salient region of the adversary in the frequency domain. Unlike existing spatial domain attacks (e.g. FGSM, PGD), our method is more imperceptible to human observers and does not degrade the visual quality of the original images. Furthermore, we introduce a hybrid adversarial attack that leverages both the spatial and frequency domains, inspired by the concept of meta-learning. Extensive experiments demonstrate that our proposed method effectively fools both spatial-based detectors and state-of-the-art frequency-based detectors. Additionally, the frequency attack enhances the transferability across face forgery detectors, making it effective as a black-box attack.