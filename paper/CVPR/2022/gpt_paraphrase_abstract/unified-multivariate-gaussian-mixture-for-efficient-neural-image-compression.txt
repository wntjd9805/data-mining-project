Variational image compression requires accurately modeling latent variables with priors and hyperpriors to balance rate and distortion. Current methods use univariate priors and process each variable separately, disregarding inter-correlations and intra-correlations observed in vectorized latent variables. By identifying these redundancies, we propose a novel vectorized prior approach. This involves a multivariate Gaussian mixture with estimated means and covariances. We use probabilistic vector quantization to effectively approximate means and solve the remaining covariances through cascaded estimation without context models. To reduce complexity, we extend codebooks to multi-codebooks. Our extensive experiments on benchmark datasets demonstrate that our model outperforms state-of-the-art methods in terms of rate-distortion performance and achieves an impressive 3.18Ã— compression speedup. This enables real-time, high-quality variational image compression. Our source code is publicly available at https://github.com/xiaosu-zhu/McQuic.