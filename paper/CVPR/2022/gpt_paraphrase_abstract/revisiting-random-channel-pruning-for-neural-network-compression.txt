Channel pruning is a technique used to speed up neural network inference. Many algorithms have been developed to address this issue, but there is a lack of a benchmark to directly compare these algorithms due to their complexity and custom settings. A fair benchmark is crucial for the further advancement of channel pruning. Recent studies have shown that the channel configurations discovered by pruning algorithms are as important as the pre-trained weights. This highlights the role of channel pruning in searching for optimal channel configurations. In this study, we propose using random search to determine the channel configuration of pruned models. This approach provides a new method for comparing different pruning methods based on how they perform compared to random pruning. We demonstrate that this simple strategy works well in comparison to other channel pruning methods. Interestingly, we find that there are no clear winners among different channel importance evaluation methods under this setting. This suggests a need for further research on advanced channel configuration searching methods. The code for our approach will be made available at https://github.com/ofsoundof/random_channel_pruning.