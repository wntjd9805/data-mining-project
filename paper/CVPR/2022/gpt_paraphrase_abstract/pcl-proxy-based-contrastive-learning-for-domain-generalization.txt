The paper discusses the problem of domain generalization, which involves training a model to generalize to unseen target domains by using different source domains. One potential solution is contrastive learning, which aims to learn domain-invariant representations by considering semantic relations among sample pairs from different domains. The conventional approach of pulling positive sample pairs closer and pushing negative pairs apart is found to be ineffective for domain generalization. This is because aligning positive pairs hinders model generalization due to large distribution gaps between domains. To overcome this issue, the paper proposes a novel proxy-based contrastive learning method. This method replaces sample-to-sample relations with proxy-to-sample relations, which effectively addresses the positive alignment problem. Experimental results on four standard benchmarks demonstrate the effectiveness of the proposed method. Additionally, the method also performs well in a more complex scenario where there are no pre-trained models from ImageNet available.