This paper presents a framework for accurately reading analog clocks in natural images or videos. The framework has several key contributions. Firstly, it introduces a scalable pipeline that generates synthetic clocks, reducing the need for labor-intensive annotations. Secondly, it proposes a clock recognition architecture based on spatial transformer networks (STN), which is trained end-to-end for clock alignment and recognition. The study demonstrates that the model trained on the synthetic dataset performs well on real clocks, emphasizing the effectiveness of a Sim2Real training approach. Thirdly, to bridge the gap between simulation and real data, the researchers utilize the uniformity of time to generate reliable pseudo-labels for unannotated clock videos. Training on these videos yields further improvements without requiring any manual annotations. Lastly, the paper introduces three benchmark datasets with comprehensive annotations for time, accurate to the minute, based on COCO, Open Images, and The Clock movie. Overall, this framework offers a robust solution for reading analog clocks in various visual contexts.