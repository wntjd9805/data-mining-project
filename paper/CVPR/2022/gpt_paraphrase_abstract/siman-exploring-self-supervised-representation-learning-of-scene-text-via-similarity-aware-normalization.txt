Recently, the field of scene text recognition has shown a growing interest in self-supervised representation learning. Unlike previous approaches that have relied on contrastive learning, we propose a novel perspective by formulating the representation learning process in a generative manner. We observe that neighboring image patches within a text line often share similar styles, such as strokes, textures, and colors. Based on this observation, we introduce the Similarity-Aware Normalization (SimAN) module, which augments an image patch and uses its neighboring patch as a guide to restore its original style. The SimAN module identifies different patterns and aligns corresponding styles from the guiding patch, enabling the network to learn representations that can distinguish complex patterns like messy strokes and cluttered backgrounds. Experimental results demonstrate that SimAN significantly enhances representation quality and achieves promising performance. Surprisingly, our self-supervised generative network also exhibits remarkable potential in data synthesis, text image editing, and font interpolation. This suggests that SimAN has a wide range of practical applications.