We present a novel framework called Bailando for driving 3D characters to dance in accordance with music. This task is challenging due to the constraints imposed by choreography norms, which limit the spatial possibilities for poses. Furthermore, the generated dance sequence must also maintain temporal coherence with different music genres. To address these challenges, our framework consists of two components: a choreographic memory and an actor-critic Generative Pre-trained Transformer (GPT).   The choreographic memory learns to summarize meaningful dancing units from a 3D pose sequence into a quantized codebook. This allows dance generation to be performed on high-quality units that adhere to choreography standards and spatial constraints. The GPT, trained using an actor-critic-based reinforcement learning scheme, composes these units into a coherent dance that aligns with the music beats. To achieve synchronized alignment between diverse motion tempos and music beats, we introduce a newly-designed beat-align reward function.  Our proposed framework achieves state-of-the-art performance both qualitatively and quantitatively, as demonstrated by extensive experiments on a standard benchmark. Notably, the learned choreographic memory is capable of discovering human-interpretable dancing-style poses in an unsupervised manner. We provide access to the code and a video demo of our framework at https://github.com/lisiyao21/Bailando/.