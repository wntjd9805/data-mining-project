Traditional vision models typically use structured features to perform tasks such as semantic segmentation. However, in this study, we propose a new approach that treats an image as a collection of learnable regions, each with flexible geometry and consistent meaning. To capture the context between regions, we utilize a Transformer model to encode the regions using multi-layer self-attention. These encoded region embeddings serve as representations for specific regions. Instead of using a decoder, we perform semantic segmentation by making per-region predictions based on the encoded region embeddings using a single linear classifier. Our proposed model, called RegProxy, operates solely at the region level, abandoning the common Cartesian feature layout. Consequently, RegProxy achieves a strong balance between performance and efficiency compared to traditional dense prediction methods. For instance, on the ADE20K dataset, the smaller RegProxy-S/16 model outperforms the best CNN model while using only 25% of the parameters and 4% of the computation. Additionally, the larger RegProxy-L/16 model achieves a 2.1% improvement in mean intersection over union (mIoU) compared to the state-of-the-art, with fewer resources. The codes and models for our approach are available at https://github.com/YiF-Zhang/RegionProxy.