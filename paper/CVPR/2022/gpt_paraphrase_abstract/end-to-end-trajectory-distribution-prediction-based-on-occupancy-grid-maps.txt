This paper aims to predict the future trajectory distribution of a moving agent in the real world using social scene images and historical trajectories. However, this is a challenging task because the true distribution is unknown and unobservable, and only one sample can be used for model learning, which can introduce bias. Previous approaches focus on predicting diverse trajectories to cover all modes of the distribution, but they often overlook precision and give too much credit to unrealistic predictions. To address this issue, we propose learning the distribution using symmetric cross-entropy and occupancy grid maps as an explicit approximation to the ground-truth distribution. This approach effectively penalizes unlikely predictions. We present an inverse reinforcement learning-based framework that learns to plan using an approximate value iteration network in an end-to-end manner. Additionally, we use a differentiable Transformer-based network to generate a small set of representative trajectories based on the predicted distribution, leveraging its attention mechanism to model trajectory relations. In experiments, our method achieves state-of-the-art performance on the Stanford Drone Dataset and Intersection Drone Dataset.