Using RGB-D images for accurate 6D pose estimation is becoming more viable as RGB-D sensors become more affordable. Current methods typically use separate backbones for extracting features from RGB and depth images, as well as a fusion network for combining these features. This separation is due to the "projection breakdown" problem, where spatial transformations applied to the depth image, such as resizing or cropping, break the relationship between pixel values and their corresponding UV coordinates. This results in a loss of the preserved 3D structure. To address this issue, we propose a method called Uni6D that takes both RGB-D images and UV data as input. Our method utilizes a unified CNN framework with a single backbone based on Mask R-CNN. It includes two additional heads: one for directly predicting the 6D pose and another for guiding the network to map visible points to their coordinates in the 3D model. This approach achieves comparable accuracy to state-of-the-art methods while being 7.2 times faster in terms of inference speed on the YCB-Video dataset. By introducing UV data, we are able to overcome the projection breakdown problem and improve the performance of 6D pose estimation with various spatial transformations.