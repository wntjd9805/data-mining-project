A major challenge in machine learning models is their ability to generalize to data that falls outside of the distribution they were trained on. This is often due to the presence of spurious correlations. To address this challenge, we introduce a constrained optimization approach called Disentanglement-constrained Domain Generalization (DDG). We simplify this complex optimization problem by using a finite-dimensional parameterization and empirical approximation. We also provide a theoretical analysis of the deviations that occur from the original problem due to these transformations. Building on this transformation, we propose a primal-dual algorithm that simultaneously disentangles representations and enables generalization across domains. Unlike traditional approaches that rely on domain adversarial training and domain labels, DDG learns semantic and variation encoders for disentanglement, allowing for flexible manipulation and augmentation of training data. DDG aims to learn intrinsic representations of semantic concepts that remain invariant to nuisance factors and can be applied to various domains. Our extensive experiments on popular benchmarks demonstrate that DDG achieves competitive performance on out-of-distribution data and reveals interpretable salient structures within the data.