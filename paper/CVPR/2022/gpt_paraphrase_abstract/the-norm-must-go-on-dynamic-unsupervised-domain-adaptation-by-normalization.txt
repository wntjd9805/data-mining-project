Domain adaptation is essential for adapting a learned model to new scenarios, such as changes in data distributions. However, current approaches typically require a significant amount of labeled or unlabeled data from the shifted domain, which can be challenging in fields that require continuous dynamic adaptation or suffer from data scarcity, such as autonomous driving in challenging weather conditions. To overcome this issue and enable continuous adaptation to distribution shifts, we propose a method called Dynamic Unsupervised Adaptation (DUA). DUA achieves this by continuously adapting the statistics of the batch normalization layers, thereby modifying the feature representations of the model. Our experiments demonstrate that by sequentially adapting a model using only a fraction of the unlabeled data, we can achieve significant performance improvements. Remarkably, even with less than 1% of unlabeled data from the target domain, DUA achieves competitive results compared to strong baselines. Furthermore, our approach incurs minimal computational overhead compared to previous methods. The simplicity and effectiveness of DUA make it applicable to any architecture that utilizes batch normalization. We validate the utility of DUA by evaluating its performance on various domain adaptation datasets and tasks, including object recognition, digit recognition, and object detection.