The abstract discusses the use of neuromorphic vision sensors, which are bio-inspired imaging systems that detect asynchronous changes in brightness called "events." Current methods for reconstructing images from these events rely on artificial neural networks or hand-crafted techniques. This paper presents a new approach using a deep spiking neural network architecture, which operates with binary spikes distributed over time. The authors propose a framework called EVSNN, which utilizes specific types of neurons to reconstruct event-based videos. They also introduce a hybrid framework called PA-EVSNN that utilizes the temporal information stored in the membrane potential of spiking neurons. Experimental results show that these models achieve comparable performance to ANN-based models on various datasets and are more computationally efficient in terms of energy consumption. The code and pretrained models are available for further exploration.