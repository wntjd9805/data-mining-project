The security of AI systems, particularly in the field of medical imaging, has become a significant concern. Developing a secure medical image analysis (MIA) system requires studying potential backdoor attacks (BAs), which can embed hidden malicious behaviors into the system. However, creating a unified BA method for various MIA systems is challenging due to the diversity of imaging modalities and analysis tasks. Existing BA methods designed for natural image classification models are inadequate for attacking dense prediction models used in MIA. To address this issue, we propose a novel Frequency-Injection based Backdoor Attack method (FIBA) that can deliver attacks in various MIA tasks. FIBA utilizes a trigger function in the frequency domain to inject low-frequency information from a trigger image into the poisoned image by combining their spectral amplitudes linearly. This method preserves the semantics of the poisoned image pixels, enabling attacks on both classification and dense prediction models. We conducted experiments on three MIA benchmarks, namely ISIC-2019 for skin lesion classification, KiTS-19 for kidney tumor segmentation, and EAD-2019 for endoscopic artifact detection, to validate the effectiveness of FIBA. The results demonstrate the superiority of FIBA over state-of-the-art methods in attacking MIA models and bypassing their defenses.