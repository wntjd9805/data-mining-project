This study addresses the issue of backfilling in visual retrieval systems, which involves recomputing features for each data item when updating the embedding model. Backward compatible training (BCT) was previously proposed as a solution to avoid the costly backfilling process. However, BCT can negatively impact the performance of the new model. To overcome this limitation, the authors propose a new learning paradigm called forward compatible training (FCT). In FCT, the training of the old model prepares for future versions of the model by incorporating side-information as an auxiliary feature for each sample. This side-information facilitates future updates of the model. Additionally, a forward transformation from old to new embeddings is combined with the side-information to create a powerful and flexible framework for model compatibility. Unlike BCT, the training of the new model remains unmodified, preserving its accuracy. The authors demonstrate that FCT significantly improves retrieval accuracy compared to BCT across various datasets, including ImageNet-1k (+18.1%), Places-365 (+5.4%), and VGG-Face2 (+8.3%). FCT also achieves model compatibility even when the new and old models are trained on different datasets, losses, and architectures.