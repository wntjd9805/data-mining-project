TransFusion is a robust solution for fusing LiDAR and camera sensors in autonomous driving. Unlike existing fusion methods, TransFusion addresses the challenges posed by inferior image conditions such as bad illumination and sensor misalignment. It achieves this by incorporating a soft-association mechanism that allows for more flexibility in handling these conditions. TransFusion consists of convolutional backbones and a detection head based on a transformer decoder. The decoder predicts initial bounding boxes from a LiDAR point cloud using a sparse set of object queries. The second decoder layer then adaptively fuses the object queries with useful image features, taking into account both spatial and contextual relationships. This adaptive fusion strategy is enabled by the attention mechanism of the transformer, which determines where and what information should be extracted from the image. To handle objects that are difficult to detect in point clouds, TransFusion incorporates an image-guided query initialization strategy. This strategy improves the detection capabilities of the model. TransFusion achieves state-of-the-art performance on large-scale datasets and demonstrates robustness against degraded image quality and calibration errors. It has been extended to the 3D tracking task and achieved first place in the nuScenes tracking leaderboard, showcasing its effectiveness and generalization capability. Code for the TransFusion method is also available for public use.