Creating an animatable human avatar wearing loose clothing from RGB videos is difficult due to challenges in motion modeling. To overcome this problem, we propose a new representation based on neural scene rendering techniques. Our representation involves structured local radiance fields anchored to nodes on a human body template. These radiance fields allow for flexible shape and appearance modeling and separate cloth deformations into skeleton motions, node translations, and dynamic detail variations within each field. To learn this representation and enable pose generalization, we use a conditional generative latent space to learn node translations and detail variations from RGB data. Our method can automatically generate animatable human avatars with various types of clothing without the need for subject-specific templates. It also produces realistic images with dynamic details for novel poses. Experimental results demonstrate that our method outperforms existing techniques both qualitatively and quantitatively.