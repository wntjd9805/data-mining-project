The goal of knowledge distillation is to compress a complex teacher model into a simpler student model while maintaining good performance. Previous approaches have used intricate knowledge representations, making model development and interpretation more challenging. However, we demonstrate through empirical evidence that a simple knowledge distillation technique can significantly reduce the performance gap between teacher and student models. Our method involves reusing the discriminative classifier from the teacher model for student inference and training a student encoder using feature alignment with a single â„“2 loss. By ensuring perfect alignment of extracted features, the student model achieves the same performance as the teacher model. We also introduce an additional projector to align the student encoder with the teacher classifier, making our technique applicable to different teacher and student architectures. Extensive experiments confirm that our approach achieves state-of-the-art results, albeit with a slight decrease in compression ratio due to the added projector.