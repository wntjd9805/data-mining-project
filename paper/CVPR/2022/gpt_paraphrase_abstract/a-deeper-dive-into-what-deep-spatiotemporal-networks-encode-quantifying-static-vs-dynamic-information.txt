Deep spatiotemporal models are widely used in computer vision tasks like action recognition and video object segmentation. However, there is a lack of understanding regarding the information captured by these models in their intermediate representations. Specifically, there is no quantitative method for evaluating the bias towards static visual appearance compared to dynamic information (motion) in these models. In this study, we propose a novel approach to quantify the static and dynamic biases in any spatiotemporal model. We evaluate the effectiveness of our approach by analyzing two well-known tasks: action recognition and video object segmentation. Our findings reveal three main points: (1) Most spatiotemporal models exhibit a bias towards static information, although certain two-stream architectures with cross-connections achieve a better balance between static and dynamic information. (2) Some datasets that are commonly assumed to be biased towards dynamics are actually biased towards static information. (3) Individual units (channels) within an architecture can exhibit bias towards static, dynamic, or a combination of both.