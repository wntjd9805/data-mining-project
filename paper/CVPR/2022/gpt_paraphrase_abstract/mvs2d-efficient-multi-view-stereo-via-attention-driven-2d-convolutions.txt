Deep learning has had a significant impact on multi-view stereo systems. Current approaches involve constructing a cost volume and using multiple 3D convolution operations to determine the depth of each pixel in the input image. While these end-to-end learning methods have improved accuracy on public benchmarks, they are computationally slow. To address this issue, we propose MVS2D, an efficient multi-view stereo algorithm that seamlessly integrates multi-view constraints into single-view networks using an attention mechanism. Unlike other methods, MVS2D only utilizes 2D convolutions, making it at least twice as fast as comparable approaches. Additionally, our algorithm produces accurate depth estimations and 3D reconstructions, achieving state-of-the-art results on challenging benchmarks such as ScanNet, SUN3D, RGBD, and the DTU dataset. Furthermore, our algorithm outperforms all other algorithms when dealing with imprecise camera poses. We have made our code available at https://github.com/zhenpeiyang/MVS2D.