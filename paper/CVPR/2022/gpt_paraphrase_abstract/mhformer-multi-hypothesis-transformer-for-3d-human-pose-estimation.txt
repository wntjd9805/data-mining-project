Creating accurate 3D human pose estimations from monocular videos is a difficult task due to challenges like depth ambiguity and self-occlusion. Previous approaches have focused on addressing these issues by considering spatial and temporal relationships. However, they neglect the fact that this is an inverse problem with multiple possible solutions or hypotheses. To overcome this limitation, we propose the Multi-Hypothesis Transformer (MHFormer), which learns spatio-temporal representations of multiple plausible pose hypotheses. The approach involves three stages: (i) generating multiple initial hypothesis representations, (ii) modeling communication between these hypotheses to merge them into a converged representation and then partitioning it into diverged hypotheses, and (iii) learning cross-hypothesis communication and aggregating the features of these hypotheses to synthesize the final 3D pose. This process enhances the representation and significantly improves the accuracy of the synthesized pose. Extensive experiments on challenging datasets (Human3.6M and MPI-INF-3DHP) demonstrate that MHFormer achieves state-of-the-art results. It outperforms previous methods by a substantial margin of 3% on Human3.6M. The code and models for MHFormer are available at https://github.com/Vegetebird/MHFormer.