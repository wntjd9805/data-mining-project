Action detection is a difficult task, particularly in datasets of untrimmed videos with dense labels. These datasets contain complex temporal relationships, including overlapping or simultaneous actions. To accurately detect actions in these complex scenarios, it is important to efficiently capture both short-term and long-term temporal information. In this study, we introduce a new network called MS-TCT1, which combines convolutional and transformer layers for action detection. MS-TCT1 consists of three main components: a Temporal Encoder module that analyzes global and local temporal relations at different resolutions, a Temporal Scale Mixer module that effectively combines features from multiple scales to create a unified representation, and a Classification module that learns the position of each action instance in relation to a center point in time and predicts frame-level classification scores. We conducted experiments on challenging datasets like Charades, TSU, and MultiTHUMOS, and our results demonstrate that our proposed method outperforms the current state-of-the-art methods on all three datasets.