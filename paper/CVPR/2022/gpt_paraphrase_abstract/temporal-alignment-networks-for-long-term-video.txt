This paper presents a temporal alignment network that processes long-term video sequences and associated text sentences. The main objectives are to determine if a sentence can be aligned with the video and, if so, to determine its alignment. The challenge lies in training the network using large-scale datasets like HowTo100M, which contain noisy text sentences that are only weakly aligned to the relevant video segments. In addition to proposing the alignment network, the paper makes four contributions. Firstly, a novel co-training method is described, which allows for denoising and training on raw instructional videos without the need for manual annotation, despite the presence of considerable noise. Secondly, a 10-hour subset of HowTo100M is manually curated to benchmark the alignment performance. This subset consists of 80 videos with sparse temporal descriptions. The proposed model, trained on HowTo100M, outperforms strong baselines (CLIP, MIL-NCE) by a significant margin on this alignment dataset. Thirdly, the trained model is applied in zero-shot settings to various downstream video understanding tasks. This includes achieving state-of-the-art results in text-video retrieval on YouCook2 and weakly supervised video action segmentation on Breakfast-Action. Finally, the automatically-aligned annotations from HowTo100M are used to finetune the backbone model, resulting in improved performance on downstream action recognition tasks.