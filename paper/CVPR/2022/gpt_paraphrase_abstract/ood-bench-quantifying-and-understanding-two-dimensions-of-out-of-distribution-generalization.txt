Deep learning has been highly successful in handling independent and identically distributed (i.i.d.) data. However, neural networks often struggle when faced with out-of-distribution (OoD) data, where the training and test data come from different distributions. Although several algorithms have been proposed for OoD generalization, our understanding of the data used to train and evaluate these algorithms has remained limited. In this study, we identify and measure two distinct types of distribution shifts that are commonly observed in various datasets. Through extensive experiments, we compare OoD generalization algorithms using two groups of benchmarks, each dominated by one of the distribution shifts. This allows us to uncover the strengths and limitations of these algorithms on different shifts. By connecting existing datasets and algorithms from unrelated research areas, we provide a comprehensive overview that can serve as a foundation for future OoD generalization research. The code for our study is available at https://github.com/ynysjtu/ood_bench.