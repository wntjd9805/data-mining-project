Recent advancements in few-shot learning have led to the development of a more practical cross-domain scenario, where the source and target datasets belong to different domains. However, the domain gap and disjoint label spaces between these datasets result in limited shared knowledge. As a result, it is more beneficial to explore information from the target domain rather than focusing solely on elaborate training strategies on the source domain, which is common in existing methods. To address this challenge, we propose a novel approach that starts with a generic representation pre-trained using cross-entropy loss and a distance-based classifier. We also leverage an image retrieval perspective to employ a re-ranking process, which involves calibrating a target distance matrix by identifying the k-reciprocal neighbors within the task. Since the pre-trained representation may be biased towards the source, we construct a non-linear subspace to reduce task-irrelevant features while preserving transferable discriminative information through a hyperbolic tangent transformation. The calibrated distance within this target-aware non-linear subspace complements the pre-trained representation. To incorporate this distance calibration information into the pre-trained representation, we employ a Kullback-Leibler divergence loss to gradually guide the model towards a calibrated distance-based distribution. Through extensive evaluations on eight target domains, we demonstrate that our target ranking calibration process can enhance conventional distance-based classifiers in few-shot learning.