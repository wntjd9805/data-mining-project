We introduce a new transformer-based architecture for global multi-object tracking. Our approach involves using a sequence of frames as input and generating trajectories for all objects. The key component of our architecture is a global tracking transformer that operates on objects across all frames. This transformer encodes object features from each frame and utilizes trajectory queries to group them into trajectories. By using trajectory queries, which are object features from a single frame, our approach naturally produces distinct trajectories. Importantly, our global tracking transformer does not require intermediate pairwise grouping or combinatorial association and can be trained together with an object detector. On the widely used MOT17 benchmark, our approach achieves competitive performance, with 75.3 MOTA and 59.1 HOTA. Furthermore, our framework seamlessly integrates with state-of-the-art large-vocabulary detectors, enabling tracking of various objects. Experimental results on the challenging TAO dataset demonstrate consistent improvements over baselines that rely on pairwise association, outperforming existing work by a significant 7.7 tracking mAP. The code for our approach can be found at https://github.com/xingyizhou/GTR.