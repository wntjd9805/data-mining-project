Unsupervised methods have shown promise in estimating depth from single images. However, they require training data that is captured in scenes without moving objects. Recent methods have increased their model parameters to improve accuracy. This paper proposes an unsupervised learning framework that predicts monocular depth and complete 3D motion, including moving objects and camera motion. To improve depth inference without increasing model parameters, recurrent modulation units are used to adaptively fuse encoder and decoder features. Multiple sets of filters are used for residual upsampling, allowing for the learning of edge-preserving filters and improving performance. A warping-based network is used to estimate motion fields of moving objects without relying on semantic priors. This allows for the use of general videos in unsupervised learning and breaks the requirement of scene rigidity. The motion fields are regularized with an outlier-aware training loss. Despite using only a single image at test time and having 2.97M parameters, the proposed depth model achieves state-of-the-art results on the KITTI and Cityscapes benchmarks.