Auxiliary loss is an additional loss used to optimize the learning process of neural networks. When calculating the auxiliary loss for semantic segmentation, it is necessary to ensure that the size of the feature maps matches the ground truth. Previous studies have either used down-sampling or up-sampling techniques to adjust the resolution between the feature map and the ground truth. However, these methods result in information loss. To address this issue, we propose a new pooling technique called Class Probability Preserving (CPP) pooling, which reduces information loss during down-sampling. We evaluate our approach on various datasets and demonstrate its superiority when combined with auxiliary losses based on seven popular segmentation models. Additionally, we introduce See-ThroughNetwork (SeeThroughNet), which incorporates an improved multi-scale attention-coupled decoder structure to maximize the benefits of CPP pooling. SeeThroughNet achieves state-of-the-art results in semantic understanding of urban street scenes, ranking first on the Cityscapes benchmark.