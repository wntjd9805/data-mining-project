Self-supervised learning (SSL) has made significant advancements in narrowing the gap with supervised learning. However, current SSL methods face challenges in accurately aligning projected embeddings and eliminating object-irrelevant nuisances and spatial misalignment. In this study, we propose a new approach called Learning Where to Learn (LEWEL) to address these issues. LEWEL adaptively aggregates spatial information of features by reinterpreting the projection head in SSL as a per-pixel projection. We predict a set of spatial alignment maps from the original features using this weight-sharing projection head. By aggregating the features with spatial weighting based on these alignment maps, we obtain a spectrum of aligned embeddings. The adaptive alignment provided by LEWEL leads to significant improvements in image-level prediction and dense prediction. Experimental results demonstrate that LEWEL outperforms state-of-the-art SSL methods such as MoCov2 and BYOL on tasks including ImageNet linear/semi-supervised classification, Pascal VOC semantic segmentation, and object detection.