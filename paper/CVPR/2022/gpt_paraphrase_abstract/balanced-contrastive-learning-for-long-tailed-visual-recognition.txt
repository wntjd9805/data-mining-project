Representation learning for imbalanced data is a challenging task as most real-world datasets have a long-tailed distribution, where a few majority categories dominate the data while minority categories have limited samples. Existing classification models that minimize cross-entropy struggle to effectively represent and classify these tail classes. While methods for learning unbiased classifiers have been extensively studied, approaches for handling imbalanced data are not well-explored.  This paper focuses on representation learning for imbalanced data. Recently, supervised contrastive learning (SCL) has shown promising performance on balanced data. However, through theoretical analysis, it is found that SCL fails to form a regular simplex, which is an ideal geometric configuration for representation learning, when applied to long-tailed data. To address this limitation and enhance the performance of long-tailed visual recognition, a novel loss called balanced contrastive learning (BCL) is proposed.  BCL introduces two improvements over SCL: class-averaging, which balances the gradient contribution of negative classes, and class-complement, which ensures that all classes appear in every mini-batch. By satisfying the condition of forming a regular simplex and assisting in the optimization of cross-entropy, BCL improves the representation learning process. The proposed two-branch framework, equipped with BCL, achieves competitive performance on long-tailed benchmark datasets, including CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist2018.