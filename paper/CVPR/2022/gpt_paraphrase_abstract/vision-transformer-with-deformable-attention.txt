Transformer models have been successful in various vision tasks due to their large receptive field, which gives them higher representation power compared to convolutional neural network (CNN) models. However, simply increasing the receptive field poses challenges. Dense attention, as used in models like Vision Transformer (ViT), leads to excessive memory and computational costs, and can be influenced by irrelevant regions outside the region of interest. On the other hand, sparse attention, as used in models like PVT or SwinTransformer, is data agnostic and may limit the ability to model long-range relations.To address these concerns, we propose a novel deformable self-attention module that selects the positions of key and value pairs in a data-dependent manner. This approach allows the self-attention module to focus on relevant regions and capture more informative features. Building upon this, we introduce the Deformable Attention Transformer (DAT), a versatile backbone model with deformable attention for both image classification and dense prediction tasks. Through extensive experiments, we demonstrate that our models consistently achieve improved results on comprehensive benchmarks. The code for our models is available at https://github.com/LeapLabTHU/DAT.