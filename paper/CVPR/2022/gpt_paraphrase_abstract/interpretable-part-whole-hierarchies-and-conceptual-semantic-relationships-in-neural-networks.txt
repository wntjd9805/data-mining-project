Deep neural networks have achieved remarkable performance in various tasks, surpassing human experts. However, a notable drawback of current neural architectures is the lack of understanding and interpretability in the network's response to inputs. This issue arises due to the vast number of variables and non-linearities in neural models, which are often treated as black boxes. This lack of interpretability poses a challenge in critical domains like autonomous driving, security, medicine, and healthcare, as it hampers trust and reliability despite the system's accurate performance.Moreover, relying solely on a single metric, such as classification accuracy, is insufficient for comprehensive evaluation in real-world scenarios. This paper aims to address the interpretability issue in neural networks and introduces a novel framework called Agglomerator. This framework utilizes visual cues to generate part-whole hierarchies and organizes input distributions based on the conceptual-semantic hierarchical structure between classes. The proposed method is evaluated on popular datasets, including SmallNORB, MNIST, FashionMNIST, CIFAR-10, and CIFAR-100, and demonstrates improved interpretability compared to other state-of-the-art approaches.