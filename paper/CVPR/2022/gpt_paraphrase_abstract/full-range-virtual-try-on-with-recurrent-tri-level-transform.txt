Virtual try-on technology has made significant advancements in transferring clothing images onto reference people. However, current methods are limited to standard clothes and overlook the complexity and variety of non-standard clothes. In this study, we propose a principled framework called Recurrent Tri-Level Transform (RT-VTON) that enables virtual try-on for both standard and non-standard clothes. Our framework is based on two key insights. First, semantics transfer requires a gradual feature transformation on three levels of clothing representations: clothes code, pose code, and parsing code. Second, geometry transfer necessitates a regularized image deformation that balances rigidity and flexibility. To achieve this, we use recurrent refinement of tri-level feature codes with local gated attention and non-local correspondence learning to predict the semantics of the "after-try-on" person. We then employ a semi-rigid deformation technique to align the clothing image with the predicted semantics, preserving local warping similarity. Finally, a canonical try-on synthesizer combines all the processed information to generate the final image of the clothed person. Our extensive experiments on standard benchmarks and user studies demonstrate that RT-VTON achieves state-of-the-art performance both quantitatively and qualitatively. Notably, it produces compelling results for a wide range of non-standard clothes. For more information about our project, please visit our project page: https://lzqhardworker.github.io/RT-VTON/.