The importance of prioritizing fairness in artificial intelligence (AI) systems, particularly in societal applications such as hiring and risk assessment, cannot be overstated. While previous efforts have focused on addressing biases in training data or incorporating fairness principles during training, our proposed approach takes a more flexible approach. We introduce fairness-aware adversarial perturbation (FAAP), which perturbs input data to hide fairness-related attributes in deployed models without modifying their parameters or structures. To achieve this, we train a discriminator to identify fairness-related attributes based on latent representations from deployed models, and a perturbation generator to ensure that perturbed inputs do not reveal any fairness-related features. Extensive experiments demonstrate the effectiveness and superior performance of FAAP, even on real-world commercial deployments that are inaccessible in terms of model parameters. This highlights the potential for black-box adaptation and the transferability of FAAP.