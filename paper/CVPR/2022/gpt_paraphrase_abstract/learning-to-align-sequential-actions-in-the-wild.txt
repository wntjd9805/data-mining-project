Current methods for aligning sequential actions in videos rely on deep networks that find correspondences across frames. However, these methods have limitations as they either focus on frame-to-frame mapping without considering temporal information or assume a monotonic alignment between video pairs, disregarding variations in the order of actions. Consequently, these methods cannot handle real-world scenarios involving background frames or videos with non-monotonic sequences of actions. In this study, we propose a novel approach to address these limitations and align sequential actions in diverse temporal variations. Our approach enforces temporal priors on the optimal transport matrix, allowing for temporal consistency while accommodating variations in action order. Additionally, our model can handle background frames that should not be aligned. Experimental results on four benchmark datasets demonstrate that our approach consistently outperforms the existing state-of-the-art methods for self-supervised sequential action representation learning. The code for our approach is publicly available at https://github.com/weizheliu/VAVA.