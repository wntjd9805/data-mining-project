Class Incremental Learning (CIL) is a method of learning a classifier in a phased manner, where each phase only provides data for a subset of classes. While previous research has focused on mitigating forgetting in later phases, we have found that improving CIL in the initial phase is also promising. Through experiments, we demonstrate that encouraging the CIL Learner to produce similar representations as a model trained on all classes significantly enhances CIL performance. This insight led us to investigate the difference between a naively-trained initial-phase model and an oracle model. We discovered that the number of training classes affects the distribution of data representations, with fewer training classes resulting in representations that lie in a long and narrow region, while more training classes lead to more uniformly scattered representations. Based on this observation, we propose Class-wise Decorrelation (CwD) as a regularization technique to make representations of each class scatter more uniformly, mimicking the model trained with all classes. CwD is easy to implement and can be integrated into existing methods. Extensive experiments on benchmark datasets show that CwD consistently and significantly improves the performance of state-of-the-art methods by approximately 1% to 3%. The code for CwD is available at https://github.com/Yujun-Shi/CwD.