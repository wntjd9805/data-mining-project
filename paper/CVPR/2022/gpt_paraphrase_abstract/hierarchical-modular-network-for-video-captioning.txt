Video captioning is the process of generating natural language descriptions for video content, with representation learning being a crucial component. Current methods in this field are primarily developed using supervised learning, comparing the generated captions word by word with the ground-truth text, but they do not fully utilize linguistic semantics. To address this limitation, we propose a hierarchical modular network that connects video representations and linguistic semantics at three levels before generating captions.   The first level, called the Entity level, focuses on identifying objects that are likely to be mentioned in the captions. The second level, known as the Predicate level, learns actions based on the highlighted objects and is supervised by the predicates in the captions. The third level, the Sentence level, learns the overall semantic representation and is supervised by the entire caption. Each level is implemented using a separate module.   Extensive experiments demonstrate that our proposed method outperforms state-of-the-art models on two widely-used benchmark datasets, MSVD and MSR-VTT, achieving CIDEr scores of 104.0% and 51.5% respectively. We will make the code available at https://github.com/MarcusNerva/HMN.