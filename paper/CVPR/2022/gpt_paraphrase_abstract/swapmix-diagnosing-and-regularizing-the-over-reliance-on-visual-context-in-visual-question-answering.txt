The robustness of current Visual Question Answering (VQA) models has been called into question, as they tend to rely too heavily on irrelevant objects in the image. To address this issue, we propose a new approach called SwapMix, which perturbs the visual context by swapping features of irrelevant objects with features from other objects in the dataset. By using SwapMix, we were able to change the answers to over 45% of the questions for a representative VQA model. We also found that the reliance on visual context depends on the quality of visual representations, as models trained with perfect sight showed less over-reliance. SwapMix can also be used as a data augmentation strategy during training to regulate the over-reliance on context. We tested SwapMix on two representative VQA models, MCAN and LXMERT, and found that it effectively diagnosed model robustness and helped reduce the reliance on visual context. Our experiments were conducted on the GQA dataset and the code for our method is available on GitHub.