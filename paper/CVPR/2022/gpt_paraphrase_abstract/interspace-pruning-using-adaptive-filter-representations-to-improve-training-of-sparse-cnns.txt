Unstructured pruning is a technique commonly used to reduce the memory usage of convolutional neural networks (CNNs) during training and inference. CNNs consist of parameters organized in filters. Standard unstructured pruning (SP) reduces the memory footprint by setting certain filter elements to zero, creating a fixed subspace that limits the filter's capabilities. However, this approach introduces a strong bias, especially when pruning is applied before or during training.To address this issue, we propose interspace pruning (IP), a general tool that enhances existing pruning methods. IP represents filters in a dynamic interspace using linear combinations of an adaptive filter basis (FB). By setting the coefficients of the FB to zero while jointly training the un-pruned coefficients, IP achieves superior performance compared to SP. Our mathematical analysis supports the superiority of IP and demonstrates that it outperforms all tested state-of-the-art unstructured pruning methods.IP particularly excels in challenging scenarios, such as pruning for ImageNet or achieving high sparsity levels, while maintaining the same runtime and parameter costs as SP. We further show that the advancements of IP are attributed to its improved trainability and superior generalization ability.