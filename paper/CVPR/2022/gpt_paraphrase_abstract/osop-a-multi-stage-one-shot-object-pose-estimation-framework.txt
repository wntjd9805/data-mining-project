We introduce a new approach for object detection and 6 DoF pose estimation without the need for training on specific objects. During testing, our method takes a target image and a textured 3D model as input. The key idea is to represent the 3D model using multiple 2D templates rendered from different viewpoints. This allows for direct dense feature extraction and matching using CNNs. The object is initially localized in 2D, followed by estimation of its approximate viewpoint and prediction of dense 2D-3D correspondences. The final pose is calculated using PnP. We evaluate our method on various datasets, including LineMOD, Occlusion, Homebrewed, YCB-V, and TLESS, and achieve highly competitive performance compared to state-of-the-art methods trained on synthetic data. Notably, our method does not require training on the object models used for testing.