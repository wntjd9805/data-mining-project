This paper introduces a new approach to meta-learning that aims to improve the performance of meta-learners by simultaneously optimizing the architecture and meta-weights. Unlike existing methods that use a two-stage strategy, this approach proposes progressive connection consolidation, where layers are fixed in order of their weight values. By jointly searching architectures and training meta-weights on fixed layers, the mutual impact between architecture and meta-weights is maintained. Additionally, a more effective rule for co-optimization called Connection-Adaptive Meta-learning (CAML) is proposed to enhance the generalization performance of the searched meta-learner. Through extensive experiments, it is shown that this method achieves state-of-the-art performance with significantly reduced computational cost, demonstrating its effectiveness and efficiency.