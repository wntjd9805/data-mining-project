Semi-supervised learning (SSL) is a technique that improves models by using a large amount of easily accessible unlabeled data alongside a small number of labeled data, which can be expensive to obtain. Most current SSL studies focus on scenarios where there are sufficient labeled samples available, typically tens to hundreds of samples for each class, but this still requires significant labeling costs.   In this research, we address the challenge of SSL in situations with extremely limited labeled samples, specifically only 1 or 2 labeled samples per class. Existing methods struggle to learn in such scenarios. To overcome this, we propose a propagation regularizer that effectively and efficiently learns from these scarce labeled samples by suppressing confirmation bias.   Additionally, we propose a model selection method based on our propagation regularizer to make realistic model choices when a validation dataset is not available.   Our proposed methods achieve significant improvements over existing approaches. With just one labeled sample per class, our methods achieve accuracies of 70.9%, 30.3%, and 78.9% on CIFAR-10, CIFAR-100, and SVHN datasets, respectively. These results represent improvements ranging from 8.9% to 120.2%. Furthermore, our methods also perform well on a higher resolution dataset, STL-10.