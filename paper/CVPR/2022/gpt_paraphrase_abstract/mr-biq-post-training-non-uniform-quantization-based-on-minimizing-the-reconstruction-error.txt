We introduce Mr.BiQ, a new post-training non-uniform quantization method that enables low bit-width quantization for Transformer models. Unlike previous methods, Mr.BiQ treats quantization parameters as directly and jointly learnable parameters during optimization, rather than optimizing full-precision weights first. We evaluate the effectiveness of Mr.BiQ on various models, including convolutional neural networks and Transformer models. Experimental results demonstrate that Mr.BiQ significantly improves accuracy, with improvements of up to 5.35 p.p. in CNNs, up to 4.23 p.p. in Vision Transformers, and up to 3.37 p.p. in Transformers for NLP when using a weight bit-width of 2.