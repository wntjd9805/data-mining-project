Generating 3D human motions from text poses a challenging task. The motions generated must not only be diverse but also accurately represent the content described in the text. To address this, we adopt a two-stage approach: text2length sampling and text2motion generation. In text2length sampling, we sample motion lengths based on the input text using a learned distribution function. Then, our text2motion module employs a temporal variational autoencoder to synthesize a wide range of human motions with the sampled lengths. Instead of working directly with pose sequences, we propose using motion snippet code as our internal motion representation. This code captures local semantic motion contexts and has been empirically proven to enhance the generation of realistic motions that faithfully reflect the input text. Furthermore, we construct a large-scale dataset called HumanML3D, which consists of 14,616 motion clips and 44,970 text descriptions.