This paper introduces NeuralHOFusion, a neural-based method for efficiently capturing and rendering human-object interactions in 4D using sparse RGBD sensors. The proposed approach combines traditional non-rigid fusion techniques with recent advancements in neural implicit modeling and blending. The captured humans and objects are disentangled in a layer-wise manner to enable detailed geometry modeling even in complex scenarios with occlusions. The authors also propose a robust object tracking pipeline that incorporates a neural implicit inference scheme and non-rigid key-volume fusion for accurate geometry generation. Additionally, a layer-wise human-object texture rendering scheme is introduced, which combines volumetric and image-based rendering to produce photo-realistic results in both spatial and temporal domains. Extensive experiments demonstrate the effectiveness and efficiency of the proposed approach in synthesizing high-quality free-view results in complex human-object interactions.