The existing semantic segmentation solutions, regardless of their network designs and mask decoding strategies, can be classified as learning class prototypes. However, this parametric approach has limitations. To address this, we propose a nonparametric alternative that uses non-learnable prototypes. Instead of learning a single weight/query vector for each class, our model represents each class as a set of non-learnable prototypes based on the mean features of training pixels. We achieve dense prediction through nonparametric nearest prototype retrieval, allowing us to optimize the arrangement between embedded pixels and prototypes. Our model can handle any number of classes with a fixed number of learnable parameters. We demonstrate the effectiveness of our approach on various datasets and in situations with a large vocabulary. We hope that this study will prompt a reconsideration of the current design of semantic segmentation models.