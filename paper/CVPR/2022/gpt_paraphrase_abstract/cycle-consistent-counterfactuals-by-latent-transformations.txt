CounterFactual (CF) visual explanations aim to find images that resemble the query image but result in a different outcome for a vision system. However, existing methods have limitations such as requiring optimization during inference or the need for joint training with a generative adversarial model, making them impractical and time-consuming. In this study, we propose a new approach called Cycle-Consistent Counterfactuals by Latent Transformations (C3LT). Our method learns a latent transformation that generates visual counterfactuals by navigating through the latent space of generative models. By leveraging cycle consistency between the query and counterfactual latent representations, our training process is able to find better solutions. C3LT can be easily integrated into any state-of-the-art pretrained generative network, enabling the generation of high-quality and interpretable counterfactual images, even at high resolutions like those in ImageNet. We not only evaluate our method using established metrics for evaluating CF explanations but also introduce a novel metric specifically designed to assess the quality of the generated counterfactual examples. Through an extensive set of experiments, we validate the effectiveness of our approach.