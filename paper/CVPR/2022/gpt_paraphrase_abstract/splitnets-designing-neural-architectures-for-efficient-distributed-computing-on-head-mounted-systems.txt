We develop deep neural networks (DNNs) and their corresponding network splits to distribute the workload of DNNs between camera sensors and a centralized aggregator on head-mounted devices. This is done to meet system performance goals in terms of accuracy and latency, given the limitations of the hardware resources available. To achieve an optimal balance between computation, communication, and performance, we introduce a framework called SplitNets, which simultaneously conducts model design, splitting, and communication reduction. We also expand this framework to include multi-view systems, allowing for the fusion of inputs from multiple camera sensors with optimal performance and system efficiency. We demonstrate the effectiveness of SplitNets in both single-view systems using ImageNet and multi-view systems using 3D classification. Our results show that the SplitNets framework achieves state-of-the-art performance and system latency compared to existing approaches.