The rise of Vision Transformers (ViTs) marked a significant advancement in visual recognition, surpassing Convolutional Neural Networks (ConvNets) as the leading model for image classification. However, ViTs faced challenges when applied to broader computer vision tasks like object detection and semantic segmentation. Hierarchical Transformers, such as Swin Transformers, reintroduced ConvNet principles, enabling Transformers to serve as a versatile vision backbone with impressive performance across various vision tasks. Nevertheless, the success of hybrid approaches is mostly attributed to the superiority of Transformers rather than the inherent biases of convolutions. This study reevaluates the design possibilities and explores the capabilities of a pure ConvNet. By gradually adapting a standard ResNet to the design of a vision Transformer, the research identifies key components that contribute to the performance disparity. The outcome is a family of ConvNet models called ConvNeXt, created solely from standard ConvNet modules. ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% top-1 accuracy on ImageNet and surpassing Swin Transformers on COCO detection and ADE20K segmentation. Moreover, ConvNeXts retain the simplicity and efficiency of standard ConvNets.