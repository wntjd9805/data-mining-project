We introduce Depthwise Quantization (DQ), a technique that applies quantization to a sub-tensor of Deep Neural Networks (DNNs) along the feature axis. This feature decomposition allows for a significant increase in representation capacity while only incurring a linear increase in memory and parameter cost. The advantage of DQ is that it can be seamlessly integrated into existing encoder-decoder frameworks without modifying the DNN architecture.   To demonstrate the effectiveness of DQ, we incorporate it into Hierarchical Auto-Encoders and train the model end-to-end on image feature representation. By analyzing the cross-correlation between spatial and channel features, we propose decomposing the image feature representation along the channel axis. The depthwise operator exhibits improved performance due to the enhanced representation capacity achieved through implicit feature decoupling.  We evaluate the performance of DQ on the likelihood estimation task and find that it surpasses the previous state-of-the-art on CIFAR-10, ImageNet-32, and ImageNet-64 datasets. Moreover, we progressively train a single hierarchical model with increasing image sizes, resulting in 69% fewer parameters and faster convergence compared to previous approaches.