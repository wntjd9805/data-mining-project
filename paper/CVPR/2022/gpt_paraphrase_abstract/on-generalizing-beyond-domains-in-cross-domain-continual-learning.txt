Humans are able to learn new tasks in different conditions, but deep neural networks often forget previously learned knowledge when learning a new task. Many existing methods focus on preventing this forgetting, assuming that the training and testing data have similar distributions. However, in this research, we consider a more realistic scenario of continual learning under domain shifts, where the model needs to generalize to unseen domains. To address this, we incorporate class similarity metrics as learning parameters to encourage the learning of semantically meaningful features. These metrics are computed using Mahalanobis similarity. The backbone representation and these additional parameters are learned seamlessly in an end-to-end manner. Additionally, we propose an approach using the exponential moving average of parameters for better knowledge distillation. Our experiments show that existing continual learning algorithms struggle with the forgetting issue under multiple distributions, while our approach achieves accuracy improvements of up to 10% on challenging datasets like DomainNet and OfÔ¨ÅceHome.