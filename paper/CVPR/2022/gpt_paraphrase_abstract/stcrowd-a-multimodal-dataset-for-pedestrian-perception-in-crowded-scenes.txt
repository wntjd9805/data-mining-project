Detecting and tracking pedestrians accurately in 3D space is a challenging task due to the wide range of rotations, poses, and scales. This difficulty is amplified in dense crowds with significant occlusions. However, existing benchmarks either offer only 2D annotations or have limited 3D annotations with low-density pedestrian distribution, making the development of reliable pedestrian perception systems for crowded scenes problematic. To address this issue and enable better evaluation of pedestrian perception algorithms in crowded scenarios, we introduce a large-scale multimodal dataset called STCrowd. STCrowd includes a total of 219K pedestrian instances, with an average of 20 persons per frame, and exhibits varying levels of occlusion. The dataset provides synchronized LiDAR point clouds and camera images, along with their corresponding 3D labels and joint IDs. STCrowd can be utilized for various tasks, such as LiDAR-only, image-only, and sensor-fusion-based pedestrian detection and tracking. We also offer baseline results for most of these tasks. Furthermore, considering the sparse global distribution and density-varying local distribution of pedestrians, we propose a novel method called Density-aware Hierarchical Heatmap Aggregation (DHA) to enhance pedestrian perception in crowded scenes. Extensive experiments demonstrate that our new method achieves state-of-the-art performance for pedestrian detection across different datasets. The STCrowd dataset and our proposed DHA method can be accessed at https://github.com/4DVLab/STCrowd.git.