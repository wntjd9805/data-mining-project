The problem of few-shot classification involves the development of a model that can adapt to new classes with limited labeled samples. Current approaches involve pre-training a feature extractor and fine-tuning for episodic meta-learning, or using spatial features to learn pixel-level correspondence while training a classifier. However, these methods have shown only marginal improvements. This paper introduces a novel strategy inspired by the transformer self-attention mechanism. The proposed approach involves a shared module that cross-attends and re-weights discriminative features for few-shot classification. By computing attention scores between features, the module produces an attention pooled representation that is added to the original representation, resulting in re-weighted features that enhance metric-based meta-learning. Experimental results on public benchmarks demonstrate that our approach outperforms state-of-the-art methods by 3%âˆ¼5%.