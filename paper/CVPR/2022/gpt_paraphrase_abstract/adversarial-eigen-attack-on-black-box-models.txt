Black-box adversarial attacks are challenging due to the lack of information about the targeted model and limitations on query budget. To enhance attack efficiency, researchers have used gradient information from a white-box substitute model trained on an additional dataset. However, we address a more practical scenario where a pre-trained white-box model with network parameters is provided without extra training data. To overcome the model mismatch issue, we propose EigenBA, a novel algorithm that combines gradient-based white-box methods with zeroth-order optimization in black-box settings. We theoretically demonstrate that the optimal perturbation directions at each step are strongly linked to the right singular vectors of the Jacobian matrix of the pre-trained white-box model. Extensive experiments conducted on ImageNet, CIFAR-10, and WebVision datasets validate that EigenBA consistently outperforms state-of-the-art approaches in terms of success rate and attack efficiency.