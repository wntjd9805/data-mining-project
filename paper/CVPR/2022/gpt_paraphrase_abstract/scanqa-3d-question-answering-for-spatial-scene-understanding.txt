We introduce a novel task called 3D question answering (3D-QA), which involves understanding spatial information in a 3D scene and answering questions about it. Unlike traditional visual question answering tasks that focus on 2D images, 3D-QA requires models to comprehend object alignment, directions, and localization based on textual questions. To address this challenge, we propose a baseline model called ScanQA1 that combines 3D object proposals and encoded sentence embeddings to learn a fused descriptor. This descriptor helps correlate language expressions with the geometric features of the 3D scan, enabling the regression of 3D bounding boxes to identify the objects described in the questions. We collect a dataset called ScanQA, consisting of over 41k question-answer pairs from 800 indoor scenes in the ScanNet dataset. This dataset is the first large-scale effort to perform object-grounded question answering in 3D environments. The answer format provided by our model focuses on abstraction.