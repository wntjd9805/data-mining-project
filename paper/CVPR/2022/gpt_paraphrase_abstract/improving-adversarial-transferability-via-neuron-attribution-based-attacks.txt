Deep neural networks (DNNs) are susceptible to adversarial examples, necessitating the development of effective attack algorithms for security-sensitive applications. In the black-box setting where the target model's specifics are unknown, feature-level transfer-based attacks contaminate intermediate feature outputs of local models and utilize the resulting adversarial samples to attack the target model directly. These attacks show promise in creating more transferable adversarial samples due to feature transferability. However, existing feature-level attacks often employ inaccurate estimations of neuron importance, which hinders their transferability. To address this issue, this paper introduces the Neuron Attribution-based Attack (NAA), which conducts feature-level attacks with more accurate estimations of neuron importance. The approach involves attributing a model's output to each neuron in a middle layer and using an approximation scheme to reduce computation overhead. Neurons are then weighted based on their attribution results to launch feature-level attacks. Extensive experiments demonstrate the superiority of the proposed approach over state-of-the-art benchmarks. The code for the approach can be accessed at: https://github.com/jpzhang1810/NAA.