We have developed a new pretraining framework called MultimodalVideo Generative Pretraining (MV-GPT) that addresses the limitations of existing video and language pretraining frameworks. Unlike these frameworks, MV-GPT is capable of generating sentences and is specifically designed for generative tasks like multimodal video captioning.In our framework, we simultaneously train a multimodal video encoder and a sentence decoder. This is in contrast to recent frameworks that only focus on one aspect of the task. To overcome the challenge of unlabelled videos lacking captions, we utilize the future utterance as an additional source of text. We propose a bidirectional generation objective, where we generate future utterances based on the present multimodal context, and also generate the present utterance based on future observations.By employing this objective, we train an end-to-end encoder-decoder model that can generate captions directly from raw pixels and transcribed speech. Our model achieves state-of-the-art performance in multimodal video captioning on four commonly used benchmarks. Additionally, it also performs exceptionally well in other video understanding tasks such as VideoQA, video retrieval, and action classification.