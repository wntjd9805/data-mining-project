Despite the prevalence of data-driven methods, video inpainting remains a challenging task in filling unknown areas of video frames with plausible and coherent content. Transformer-based architectures have shown promise for this task, but they still suffer from issues such as blurry content and spatial-temporal inconsistency over long periods. To address these limitations, we propose a novel approach called Discrete Latent Transformer (DLFormer) that reformulates video inpainting tasks into the discrete latent space instead of the previous continuous feature space.Our method begins by learning a compact discrete codebook and an autoencoder to represent the target video. Using these representative discrete codes, the subsequent discrete latent transformer can infer proper codes for unknown areas through a self-attention mechanism. This enables the transformer to generate fine-grained content with long-term spatial-temporal consistency. Additionally, we incorporate a temporal aggregation block among adjacent frames to explicitly enforce short-term consistency and reduce visual jitters.We have conducted comprehensive quantitative and qualitative evaluations to demonstrate the superiority of our method over other state-of-the-art approaches. Our approach excels in reconstructing visually plausible and spatial-temporal coherent content with fine-grained details. The code for our method is available at https://github.com/JingjingRenabc/dlformer.