Understanding hand-object interactions involves identifying the active object being manipulated by the human hand. To accurately locate the active object, we propose a pixel-wise voting function that uses information from each image pixel to determine the bounding box. This function takes an initial bounding box as input and generates an improved bounding box based on the majority vote of pixels inside the box. We refer to the collection of bounding boxes generated during this process as the Relational Box Field. Since one round of voting is often insufficient to accurately localize the active object, we repeatedly apply the voting function to refine the bounding box's location. However, repeated application of the voting function can cause a data distribution shift. To address this issue, we use reinforcement learning (RL) to learn the voting function parameters. Our experiments on two large-scale datasets demonstrate that RL significantly improves the performance of the voting function compared to standard supervised learning. The project page, containing code and visualizations, can be accessed at https://fuqichen1998.github.io/SequentialVotingDet/.