Current approaches in semi-supervised object detection (SS-OD) primarily rely on self-training, where a teacher model generates pseudo-labels on unlabeled data. While these methods have shown some success, the scarcity of labeled data poses challenges in object detection. Our analysis of these methods, based on empirical experiments, reveals that they overlook the abundance of False Negative samples and the imprecise localization. Additionally, the significant variance in object sizes and class imbalance further hampers their performance. To address these challenges, we propose a novel approach called Scale-Equivalent Distillation (SED), which is a simple yet effective end-to-end knowledge distillation framework that is robust to large object size variance and class imbalance. SED offers several advantages over previous methods: (1) it incorporates a consistency regularization to handle the issue of large scale variance, (2) it mitigates noise from False Negative samples and imprecise localization, and (3) it employs a re-weighting strategy to implicitly prioritize potential foreground regions in unlabeled data and reduce the impact of class imbalance. Extensive experiments demonstrate that SED consistently outperforms state-of-the-art methods on various datasets, achieving substantial improvements. For instance, when utilizing 5% and 10% labeled data on the MS-COCO dataset, SED surpasses the performance of the supervised counterpart by more than 10 mAP.