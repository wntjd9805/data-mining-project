This study introduces a new method for representing scenes in real-time novel view synthesis using multiple semitransparent layers with scene-adapted geometry. The approach consists of two stages: the first stage generates the geometry of a few data-adaptive layers from a stereo pair of views, and the second stage infers the color and transparency values for these layers. Both stages are connected through a differentiable renderer and are trained end-to-end. The experiments demonstrate that this approach outperforms the use of regularly spaced layers without adaptation to scene geometry and also outperforms the IBRNet system based on implicit geometry representation.