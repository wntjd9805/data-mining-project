Estimating the risk level of adversarial examples is crucial for safely deploying machine learning models in real-world settings. One commonly used method for physical-world attacks is the "sticker-pasting" strategy, but it has limitations such as difficulties in accessing the target or printing with valid colors. A new type of non-invasive attack has emerged, using optical tools like laser beams and projectors to cast perturbations onto the target. However, the added optical patterns are artificial and easily noticeable by humans. In this study, we explore a new type of optical adversarial examples that use shadows, a natural phenomenon, to achieve a more stealthy and naturalistic physical-world attack under the black-box setting. We extensively evaluate the effectiveness of this attack in both simulated and real-world environments. Our experimental results on traffic sign recognition show that our algorithm can effectively generate adversarial examples, achieving success rates of 98.23% and 90.47% on the LISA and GTSRB test sets respectively. In real-world scenarios, our attack continuously misleads a moving camera over 95% of the time. We also discuss the limitations and defense mechanisms against this attack.