Magnetic resonance imaging (MRI) allows for the generation of multi-contrast images of anatomical structures, which can be utilized in multi-contrast super-resolution (SR) techniques. Compared to SR reconstruction using a single-contrast, multi-contrast SR reconstruction shows promise in producing higher quality SR images by utilizing diverse and complementary information from different imaging modalities. However, current methods have two limitations: (1) they do not consider that multi-contrast features at different scales contain different anatomical details, resulting in ineffective matching and fusion of these features for improved reconstruction; and (2) they lack the ability to capture long-range dependencies, which are crucial for regions with complex anatomical structures. In this study, we propose a new network, named McMRSR, to address these limitations comprehensively. Our network incorporates innovative Transformer-based multi-scale contextual matching and aggregation techniques. Firstly, Transformers are employed to model long-range dependencies in both the reference and target images. Next, we introduce a novel multi-scale contextual matching method to capture corresponding contexts from reference features at different scales. Additionally, we propose a multi-scale aggregation mechanism to gradually and interactively aggregate the matched features from different scales for the reconstruction of the target SR MR image. Extensive experiments demonstrate that our network outperforms existing approaches and exhibits great potential for clinical applications. The source code is available at https://github.com/XAIMI-Lab/McMRSR.