This study demonstrates the scalability and effectiveness of masked autoencoders (MAE) as self-supervised learners in computer vision. The MAE approach involves randomly masking patches of an input image and reconstructing the missing pixels. The key components of our approach are an asymmetric encoder-decoder architecture and a high proportion of masking (e.g., 75%) for a meaningful self-supervisory task. These design choices enable efficient training of large models, resulting in improved accuracy. Our scalable approach allows for learning high-capacity models that generalize well, with a vanilla ViT-Huge model achieving the highest accuracy (87.8%) among methods using only ImageNet-1K data. Furthermore, the transfer performance in downstream tasks surpasses supervised pre-training and exhibits promising scaling behavior.