We introduce a novel technique called prompt distribution learning, which allows a pre-trained vision-language model to adapt effectively to different recognition tasks. Our approach not only learns unbiased prompts from a limited number of examples, but also captures the distribution of diverse prompts to handle various visual representations. By doing so, we generate high-quality task-specific content that aids in recognition. To achieve prompt distribution learning, we propose an efficient method that focuses on learning the output embeddings of prompts rather than the input embeddings. This enables us to effectively model the prompts using a Gaussian distribution and derive a surrogate loss for efficient training. Through extensive experiments on 12 datasets, we consistently and significantly outperform existing methods. For instance, with just one sample per category, our approach improves the average result by 9.1% compared to manually crafted prompts.