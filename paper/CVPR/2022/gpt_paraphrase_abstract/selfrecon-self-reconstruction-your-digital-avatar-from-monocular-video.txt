We introduce SelfRecon, a novel approach to reconstructing clothed human bodies using a single rotating video. Our method combines implicit and explicit representations to achieve consistent and accurate geometries. Explicit methods rely on a predefined template mesh, which is challenging to obtain for specific individuals and limits reconstruction accuracy and clothing types. In contrast, implicit representations offer arbitrary topology and high-fidelity geometry shapes but struggle with integrating multi-frame information for coherent registration. To address these limitations, we propose a hybrid approach that leverages the strengths of both representations. We employ a differential mask loss on the explicit mesh to capture the overall shape and refine the details using differentiable neural rendering on the implicit surface. Additionally, we periodically update the explicit mesh to accommodate changes in topology and introduce a consistency loss to align both representations. Our method, SelfRecon, outperforms existing techniques by generating high-fidelity surfaces for clothed humans through self-supervised optimization. Extensive experiments on real monocular videos demonstrate its effectiveness. The source code can be found at https://github.com/jby1993/SelfReconCode.