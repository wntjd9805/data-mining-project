This study focuses on 3D scene stylization, which involves generating stylized images of a scene from different viewpoints using a set of style examples. Traditional methods for image or video stylization cannot achieve consistency when applied to 3D scenes. However, recent advancements in neural radiance fields (NeRF) have allowed for the consistent representation of 3D scenes. To address the domain gap between 2D style examples and NeRF, the authors propose a mutual learning framework for 3D scene stylization. This framework combines a 2D image stylization network with NeRF to leverage the stylization ability of the 2D network and the 3D consistency of NeRF. The authors first pre-train a standard NeRF and then replace its color prediction module with a style network to obtain a stylized NeRF. They introduce a consistency loss to transfer spatial consistency knowledge from NeRF to the 2D stylization network. A mimic loss is also introduced to facilitate mutual learning between the NeRF style module and the 2D stylization decoder. To handle ambiguities in 2D stylization results, learnable latent codes conditioned on the style are introduced. Experimental results demonstrate that the proposed method outperforms existing approaches in terms of visual quality and long-range consistency.