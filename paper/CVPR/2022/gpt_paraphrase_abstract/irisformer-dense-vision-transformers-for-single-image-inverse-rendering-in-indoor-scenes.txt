The appearance of indoor scenes can vary significantly due to complex interactions between objects, materials, and lighting. In order to understand and recreate these scenes, inverse rendering techniques aim to recover shape, material, and lighting information from images. This paper proposes the use of transformer architectures, specifically a dense vision transformer called IRISformer, to address the challenges in single-image inverse rendering. By leveraging the long-range attention learned by transformers, IRISformer demonstrates excellent performance in both single-task and multi-task reasoning required for inverse rendering. The proposed transformer architecture can simultaneously estimate depths, normals, spatially-varying albedo, roughness, and lighting from a single image of an indoor scene. Extensive evaluations on benchmark datasets show that the proposed approach achieves state-of-the-art results in each of these tasks, enabling applications such as object insertion and material editing with greater photorealism compared to previous methods. The code and data used in this work are publicly available.