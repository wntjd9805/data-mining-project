The importance of visual appearance in understanding images for cross-modal retrieval is well recognized. However, the inclusion of scene text information can also provide valuable insights into the visual semantics of an image. Existing cross-modal retrieval methods often overlook the usage of scene text information, and directly incorporating this information can lead to a decrease in performance in scenarios where scene text is absent. To address this issue, we propose a comprehensive transformer-based architecture called ViSTA (Vision and Scene Text Aggregation) that can unify cross-modal retrieval scenarios. ViSTA employs transformer blocks to encode image patches and fuse scene text embedding, enabling the learning of an aggregated visual representation for cross-modal retrieval. To overcome the modality missing problem of scene text, we introduce a novel fusion token-based transformer aggregation approach that facilitates the exchange of necessary scene text information only through the fusion token and focuses on the most important features in each modality. Additionally, to enhance the visual modality, we introduce dual contrastive learning losses that embed both image-text pairs and fusion-text pairs into a shared cross-modal space. Compared to existing methods, ViSTA allows for the aggregation of relevant scene text semantics with visual appearance, leading to improved results in both scene text free and scene text aware scenarios. Experimental results demonstrate that ViSTA outperforms other methods by at least 8.4% at Recall@1 for scene text aware retrieval tasks. Furthermore, compared to state-of-the-art scene text free retrieval methods, ViSTA achieves better accuracy on datasets such as Flicker30K and MSCOCO while being at least three times faster during the inference stage, confirming the effectiveness of our proposed framework.