This paper proposes a method called Shapley-NAS for neural architecture search. The method aims to evaluate the contribution of different operations in the search for optimal architectures. The existing differentiable architecture search (DARTS) approach optimizes architecture parameters using gradient descent, but fails to accurately reflect the importance of operations to task performance. To address this, the proposed method uses the Shapley value to measure the direct influence of operations on validation accuracy. The Shapley value is used to quantify the marginal contributions of supernet components by considering all possible combinations. The method iteratively optimizes supernet weights and updates architecture parameters based on the evaluation of operation contributions using the Shapley value. The computation of the Shapley value is approximated using a Monte-Carlo sampling based algorithm with early truncation, and the momentum update mechanism is used to reduce sampling process fluctuations. Experimental results on various datasets and search spaces demonstrate that Shapley-NAS outperforms existing methods with a lower search cost. The code for the proposed method is available at https://github.com/Euphoria16/Shapley-NAS.git.