This paper addresses the issue of few-shot class incremental learning (FSCIL), which involves learning new classes with only a few samples per class. Existing methods for FSCIL primarily focus on incremental steps during testing, but their learning objectives are not directly aligned with the objective of incrementally learning new classes. This misalignment leads to suboptimal performance during evaluation. To overcome this issue, we propose a bi-level optimization approach based on meta-learning. We sample sequences of incremental tasks from base classes for training, simulating the evaluation protocol. Each task is learned using a meta-objective that enables fast adaptation without forgetting. Additionally, we introduce bi-directional guided modulation, which automatically adjusts activations to reduce catastrophic forgetting. Our extensive experiments demonstrate that our proposed method surpasses baseline approaches and achieves state-of-the-art results on CI-FAR100, MiniImageNet, and CUB200 datasets.