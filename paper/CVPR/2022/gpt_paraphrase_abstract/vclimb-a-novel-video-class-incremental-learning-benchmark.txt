The video domain has not been extensively explored in terms of continual learning (CL). Existing works in this area often have imbalanced class distributions or use inappropriate datasets. To address this gap, we introduce vCLIMB, a new benchmark for video continual learning. vCLIMB serves as a standardized test-bed for analyzing the issue of catastrophic forgetting in deep models when applied to video CL. Unlike previous studies, our focus is on class incremental continual learning, where models are trained on a sequence of distinct tasks and the number of classes is evenly distributed across these tasks. Through comprehensive evaluations using vCLIMB, we identify two unique challenges specific to video data. Firstly, the selection of instances to store in episodic memory is done at the frame level. Secondly, the effectiveness of frame sampling strategies is influenced by untrimmed training data. To overcome these challenges, we propose a temporal consistency regularization method that can be added to memory-based continual learning approaches. Our approach significantly improves the baseline performance, achieving up to a 24% improvement on the untrimmed continual learning task. The code for our benchmark is available at https://vclimb.netlify.app/.