Machine learning applications often struggle due to the lack of annotated data. To address this issue, we propose a method called VisualGPT that leverages a large pre-trained language model (PLM) to learn from small amounts of multimodal data. The key to effectively utilizing the pre-trained model is finding the right balance between visual input and prior linguistic knowledge. VisualGPT achieves this by using a unique self-resurrecting encoder-decoder attention mechanism that allows quick adaptation of the PLM with limited in-domain image-text data. This mechanism produces sparse activations, preventing accidental overwriting of linguistic knowledge. In experiments, VisualGPT outperforms the best baseline by up to 10.0% CIDEr on MS COCO and 17.9% CIDEr on Conceptual Captions when trained on only 0.1%, 0.5%, and 1% of the respective training sets. Additionally, VisualGPT achieves state-of-the-art results on IU X-ray, a medical report generation dataset. Our code is freely available at https://github.com/Vision-CAIR/VisualGPT.