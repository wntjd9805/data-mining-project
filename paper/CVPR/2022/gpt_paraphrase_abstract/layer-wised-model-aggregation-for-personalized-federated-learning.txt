Personalized Federated Learning (pFL) is a technique that captures common knowledge from distributed data and supports customized models for different clients. However, existing pFL methods have not considered the impact of individual layers on model aggregation, leading to slow convergence and inadequate personalization for non-IID datasets. In this paper, we propose a new pFL framework called Layer-wised Personalized Federated Learning (pFedLA) that addresses these issues. pFedLA utilizes a hyper-network on the server side to determine the importance of each layer for different clients. It also introduces a parameterized mechanism to update layer-wise aggregation weights, allowing for accurate model personalization by exploiting inter-user similarity. Extensive experiments demonstrate that our proposed methods outperform state-of-the-art pFL methods in terms of performance.