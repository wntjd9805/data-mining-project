We present GuideFormer, a transformer-based architecture for dense depth completion. Unlike existing methods that use static convolutional neural networks (CNNs), GuideFormer captures the dynamic nature of input contexts. It employs separate transformer branches to process sparse depth and color guidance images, extracting hierarchical and complementary token representations. Each branch consists of self-attention blocks with key design features tailored for the task. We also introduce a guided-attention mechanism for effective token fusion, explicitly modeling information flow between the branches and capturing inter-modal dependencies. This enables GuideFormer to leverage visual dependencies and recover precise depth values with fine details. We evaluate our approach on the KITTI dataset and perform extensive ablation studies, demonstrating its superiority over state-of-the-art methods.