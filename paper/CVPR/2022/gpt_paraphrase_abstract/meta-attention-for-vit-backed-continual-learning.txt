Continual learning has been extensively studied in the field of computer vision, primarily focusing on convolutional neural networks (CNNs). However, with the emergence of vision transformers (ViTs) in computer vision, there is a growing trend towards utilizing ViTs instead of CNNs. Unfortunately, applying CNN-based continual learning methods directly to ViTs can result in significant performance degradation. To address this issue, this paper investigates ViT-backed continual learning to achieve better performance using the advancements of ViTs. Inspired by mask-based continual learning methods in CNNs, where a mask is learned for each task to adapt the pre-trained CNN, the authors propose a novel approach called MEta-ATtention (MEAT). MEAT utilizes attention to self-attention and adapts a pre-trained ViT to new tasks without sacrificing performance on previously learned tasks. Unlike previous mask-based methods, such as Piggyback, MEAT only masks a portion of the ViT's parameters, taking advantage of the characteristics of ViTs. This makes MEAT more efficient and effective, with less overhead and higher accuracy.Extensive experiments demonstrate that MEAT outperforms state-of-the-art CNN-based counterparts, achieving absolute boosts in accuracy ranging from 4.0% to 6.0%. The code for MEAT is publicly available at https://github.com/zju-vipa/MEAT-TIL.