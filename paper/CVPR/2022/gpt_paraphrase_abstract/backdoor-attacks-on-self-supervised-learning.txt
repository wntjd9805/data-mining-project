Recent advancements in self-supervised learning have been driven by the availability of large-scale unlabeled data, enabling the development of methods that learn comprehensive visual representations. These methods, such as MoCo, BYOL, and MSF, employ an inductive bias that assumes random augmentations, like random crops, applied to an image should yield similar embeddings. However, we demonstrate that these methods are susceptible to backdoor attacks, where an attacker introduces a trigger (an image patch chosen by the attacker) into a small portion of the unlabeled data. Although the model performs well on clean test images, the attacker can manipulate the model's decisions by presenting the trigger during testing. While backdoor attacks have been extensively studied in supervised learning, our study is the first to investigate them in the context of self-supervised learning. The practicality of backdoor attacks is greater in self-supervised learning due to the difficulty of inspecting and removing poisoned data given the large amount of unlabeled data used. Our targeted attack reveals that the attacker can generate numerous false positives for the target category by utilizing the trigger at test time. To counteract this, we propose a defense method based on knowledge distillation that effectively neutralizes the attack. For reference, our code is available at https://github.com/UMBCvision/SSL-Backdoor.