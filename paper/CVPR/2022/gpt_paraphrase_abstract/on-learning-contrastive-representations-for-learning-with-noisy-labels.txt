Deep neural networks have the ability to remember noisy labels easily using a softmax cross entropy loss. Previous studies have tried to solve this problem by incorporating a noise-robust loss function into the cross entropy loss. However, this approach only partially solves the issue as the cross entropy loss is still not robust enough. In order to address this problem, we focus on learning robust contrastive representations of data that make it difficult for the classifier to memorize the label noise under the cross entropy loss. We propose a new contrastive regularization function to learn these representations, which prevents the label noise from dominating the representation learning process. Through theoretical analysis, we demonstrate that the learned representations retain information related to true labels while discarding information related to corrupted labels. Additionally, our theoretical findings indicate that the learned representations are resilient to label noise. We validate the effectiveness of our method through experiments on benchmark datasets.