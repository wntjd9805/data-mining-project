Recent advancements in Vision Transformer (ViT) and its improved variations have demonstrated that self-attention-based networks outperform traditional Convolutional Neural Networks (CNNs) in most visual tasks. However, existing ViTs primarily focus on standard accuracy and computation cost, neglecting the investigation of their impact on model robustness and generalization. In this study, we systematically evaluate the components of ViTs to assess their influence on robustness against adversarial examples, common corruptions, and distribution shifts. Our findings reveal that certain components can be detrimental to robustness. To address this, we propose a new vision transformer called Robust Vision Transformer (RVT) by incorporating robust components as building blocks. RVT exhibits superior performance with strong robustness. Building upon our evaluation results, we introduce two new plug-and-play techniques, namely position-aware attention scaling and patch-wise augmentation, to further enhance our RVT, denoted as RVT∗. Experimental results on ImageNet and six robustness benchmarks demonstrate that RVT outperforms previous ViTs and state-of-the-art CNNs in terms of robustness and generalization. Additionally, RVT-S∗ achieves the top rank on multiple robustness leaderboards, including ImageNet-C, ImageNet-Sketch, and ImageNet-R.