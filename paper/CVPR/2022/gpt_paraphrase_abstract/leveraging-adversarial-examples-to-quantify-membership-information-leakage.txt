Measuring the privacy level of machine learning models is a major challenge due to the potential privacy threat posed by the use of personal data for training. One common method to assess privacy risks is identifying training data based on a trained model. In this study, we propose a new approach to address membership inference in pattern recognition models by utilizing information from adversarial examples. Our strategy involves measuring the magnitude of perturbation needed to create an adversarial example, as we believe this reflects the likelihood of belonging to the training data. Through extensive numerical experiments on multivariate data and various state-of-the-art target models, we demonstrate that our method performs comparably or even better than existing techniques, all without requiring additional training samples.