We aim to address the problem of localizing each object word in a sentence that describes a visual input, using weak supervision from image- or video-caption pairs. Existing approaches utilize region proposals and ground words based on attention coefficients from captioning models. However, these methods have limitations as they do not consider the word being localized when computing attention coefficients. To overcome this, we propose a novel approach called Grounded Visual Description Conditional Variational Autoencoder (GVD-CVAE). Our method introduces a discrete random variable to model word-to-region alignment and learns its posterior distribution given the full sentence. Through experiments on challenging image and video datasets, such as Flickr30k Entities, YouCook2, and ActivityNet Entities, we demonstrate that our conditional generative model, GVD-CVAE, outperforms soft-attention-based baselines in grounding.