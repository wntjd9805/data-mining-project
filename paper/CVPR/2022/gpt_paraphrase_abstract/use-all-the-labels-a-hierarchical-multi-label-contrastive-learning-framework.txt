Current contrastive learning methods focus on using a single supervisory signal to learn representations, which limits their effectiveness on new data and downstream tasks. This paper introduces a hierarchical multi-label representation learning framework that can utilize all available labels and maintain the hierarchical relationship between classes. The framework incorporates novel hierarchy-preserving losses that combine a hierarchical penalty with the contrastive loss to enforce the hierarchy constraint. This data-driven loss function automatically adapts to different multi-label structures. Experimental results on various datasets demonstrate that our relationship-preserving embedding performs well on different tasks and outperforms both supervised and self-supervised approaches. The code for this framework is available at https://github.com/salesforce/hierarchicalContrastiveLearning.