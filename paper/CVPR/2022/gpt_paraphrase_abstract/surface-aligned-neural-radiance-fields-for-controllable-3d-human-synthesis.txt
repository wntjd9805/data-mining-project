We present a novel technique for reconstructing 3D human models with control labels using sparse multi-view RGB videos. Our method utilizes neural scene representation on mesh surface points and signed distances from the human body mesh surface. We address the problem of indistinguishability that occurs when mapping a 3D point to its nearest surface point on a mesh for learning surface-aligned neural scene representation. To resolve this issue, we propose a barycentric interpolation with modified vertex normals to project a point onto the mesh surface. Our experiments on the ZJU-MoCap and Human3.6M datasets demonstrate that our approach outperforms existing methods in synthesizing novel views and poses. Additionally, we show that our method easily allows for controlling body shape and clothes. More information can be found on our project page: https://pfnet-research.github.io/surface-aligned-nerf/.