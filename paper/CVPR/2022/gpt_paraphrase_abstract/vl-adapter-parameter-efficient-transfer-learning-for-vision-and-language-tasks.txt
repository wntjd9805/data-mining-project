In recent times, there have been significant advancements in vision-and-language (V&L) tasks and pure language tasks through the fine-tuning of language models that were pre-trained on large text corpora. However, fine-tuning the entire parameter set of these pre-trained models has become impractical due to the rapid growth in model size. This paper introduces adapter-based parameter-efficient transfer learning techniques for V&L models like VL-BART and VL-T5. The effectiveness of these methods is evaluated in a unified multi-task setup using image-text and video-text benchmarks. Four diverse V&L datasets (VQAv2, GQA, NLVR2, and MSCOCO image captioning) are used for image-text tasks, while TVQA, How2QA, TVC, and YC2C are used for video-text tasks. Through careful training and extensive experiments, three popular adapter-based methods (Adapter, Hyperformer, Compacter) are compared against standard full fine-tuning and the recently proposed prompt-tuning approach. Additionally, the efficiency and performance of adapters are enhanced by sharing their weights to gain knowledge across tasks. The results demonstrate that training the adapter with weight-sharing technique (4.18% of total parameters for image-text tasks and 3.39% for video-text tasks) can achieve performance similar to fine-tuning the entire model. Finally, a comprehensive analysis is presented, including the combination of adapter and task-specific prompts, and the impact of V&L pre-training on adapters.