Current stereo matching techniques face challenges such as restricted searching space, occluded regions, and large image sizes. On the other hand, single image depth estimation can overcome these challenges by utilizing monocular cues extracted from a single image. However, the absence of stereoscopic relationship makes monocular predictions less reliable, especially in dynamic or cluttered environments. To address these issues in both scenarios, we propose a self-supervised binocular depth estimation method inspired by the optic chiasm, called Chi-Transformer. This method utilizes a vision transformer (ViT) with a gated positional cross-attention (GPCA) layer to enable feature-sensitive pattern retrieval between views while retaining extensive context information through self-attentions. The monocular cues from a single view are then rectified using a blending layer with the retrieved pattern pairs. This design is analogous to the optic chiasm structure in the human visual system. Our experiments demonstrate that the Chi-Transformer architecture outperforms state-of-the-art self-supervised stereo approaches by 11% and can be applied to both rectilinear and non-rectilinear (e.g., fisheye) images.