As humans and autonomous systems increasingly share the same space, there is a growing need for sophisticated cooperative interactions between robots and humans. To achieve this, it is important to not only consider a human's center of gravity position but also their detailed motion. However, many algorithms overlook the multimodal nature of humans and do not account for uncertainty in their motion predictions. To address this, we propose Motron, a graph-structured model that utilizes probabilistic methods to capture the multimodality of human motion. This model is capable of generating deterministic maximum-likelihood motions for each mode, along with corresponding confidence values. Our goal is to integrate this model seamlessly into the robotic planning-control-interaction loop, providing physically feasible human motions while maintaining computational efficiency. We have tested our model on various challenging real-world motion forecasting datasets and have found it to outperform a wide range of generative and variational methods. Additionally, our model can also provide state-of-the-art single-output motions if needed, all while utilizing significantly less computational power than existing algorithms.