This paper focuses on addressing the performance and computational cost gaps between vision transformers and convolutional neural networks (CNNs) in image recognition tasks. The authors propose a new hybrid network called CMTs that combines the strengths of transformers and CNNs. By leveraging transformers to capture long-range dependencies and CNNs to extract local information, the CMTs models achieve a better trade-off between accuracy and efficiency compared to previous transformer-based and CNN-based models. The CMT-S model, in particular, achieves an impressive top-1 accuracy of 83.5% on ImageNet while being significantly smaller in terms of FLOPs compared to existing models like DeiT and EfficientNet. The CMT-S model also demonstrates strong generalization on other datasets such as CIFAR10, CIFAR100, Flowers, and COCO with reduced computational cost. Overall, the proposed CMTs models offer improved performance and efficiency in image recognition tasks compared to both transformers and CNNs.