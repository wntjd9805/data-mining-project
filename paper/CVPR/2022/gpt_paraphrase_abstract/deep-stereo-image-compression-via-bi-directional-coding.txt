This abstract discusses the limitations of current learning-based stereo compression methods and introduces a new approach called BCSIC-Net. Unlike existing methods, BCSIC-Net takes a bi-directional approach to compressing stereo images. It includes a novel bi-directional contextual transform module that reduces redundancy between views by performing a nonlinear transform based on the inter-view context. Additionally, BCSIC-Net utilizes a bi-directional conditional entropy model that uses inter-view correspondence as a conditional prior to improve coding efficiency. Experimental results on the InStereo2K and KITTI datasets show that BCSIC-Net effectively reduces inter-view redundancy and outperforms existing methods.