Training advanced vision models has become extremely expensive for researchers and practitioners. To make these models more accessible and reusable, it is crucial to focus on adapting them for various downstream scenarios. One practical approach is online test-time adaptation, where training data is not accessible, labeled data from the test distribution is unavailable, and adaptation can only occur during testing on a limited number of samples. In this study, we explore how test-time adaptation methods perform on different pretrained models in real-world scenarios, expanding their original evaluation. Our findings indicate that these methods only work well in specific experimental setups and can fail dramatically if their hyperparameters are not selected for the same scenario being tested. Recognizing the uncertainty surrounding test conditions, we propose a "conservative" approach that addresses this issue with a Laplacian Adjusted Maximum-likelihood Estimation (LAME) objective. Instead of adapting the model's parameters, our method adjusts its output and solves the objective using an efficient concave-convex procedure. Compared to existing methods, our approach achieves higher average accuracy across scenarios, while being faster and requiring less memory. The code for our approach is available at https://github.com/fiveai/LAME.