This paper addresses the issue of cross-modality person re-identification (re-ID) by reducing the discrepancy between different modalities. Cross-modality re-ID involves matching query and gallery images from different modalities. The current deep classification baseline uses a shared proxy, which acts as a bridge between the modalities. However, we find that this approach has a high tolerance for the modality gap. To overcome this, we propose a Memory-Augmented Unidirectional Metric (MAUM) learning method that consists of two novel designs: unidirectional metrics and memory-based augmentation. MAUM first learns modality-specific proxies independently for each modality. Then, it uses these learned proxies as references to bring the features in the counterpart modality closer together. These unidirectional metrics alleviate the relay effect and improve cross-modality association. To further enhance cross-modality association, the learned proxies are stored in memory banks to increase reference diversity. Importantly, we demonstrate that MAUM improves cross-modality re-ID under modality-balanced settings and provides extra robustness against the modality-imbalance problem. Our experiments on SYSU-MM01 and RegDB datasets show that MAUM outperforms state-of-the-art methods. The code for MAUM will be made available.