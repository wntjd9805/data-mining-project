This paper presents a computationally efficient regression framework for estimating the 6D pose of rigid objects using a single RGB-D image. The framework is capable of handling symmetric objects and is designed with a simple architecture that extracts point-wise features from the RGB-D data using a fully convolutional network called XYZNet. It directly regresses the 6D pose without requiring any post refinement.When dealing with symmetric objects, there can be multiple ground-truth poses for a single object, leading to estimation ambiguity. To address this problem, the authors propose a symmetry-invariant pose distance metric called average (maximum) grouped primitives distance (A(M)GPD). The A(M)GPD loss ensures that the regression network converges to the correct state, where all minima in the A(M)GPD loss surface correspond to the correct poses.The proposed framework is evaluated on the YCB-Video and T-LESS datasets, and extensive experiments demonstrate its superior performance in terms of top accuracy and low computational cost. The code for the framework is available at the provided GitHub link.Figure 1 illustrates a comparison between the A(M)GPD and ADD-S metrics. The X-axis represents the rotation angle of the object (ranging from 0° to 360°), while the Y-axis represents the calculated distance. The initial pose is set as the ground truth. The A(M)GPD curve shows that all minima are mapped to correct poses, whereas the ADD-S curve shows that several minima point to incorrect poses.Overall, this paper introduces a computation efficient regression framework for estimating the 6D pose of rigid objects, specifically addressing the challenges posed by symmetric objects. The framework achieves high accuracy and low computational cost, as demonstrated through experiments on benchmark datasets.