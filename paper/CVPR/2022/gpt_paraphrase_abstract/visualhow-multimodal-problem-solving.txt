Advancements in computer vision (CV) and natural language processing (NLP) have allowed the development of intelligent systems that can describe visual content and answer questions pertaining to it. However, current methods still struggle to understand and provide step-by-step guidance for real-life problems. To address this, we introduce VisualHow, a research initiative that aims to comprehend real-life problems and derive their solutions by integrating multiple modalities. We create a new dataset comprising 20,028 real-life problems with corresponding solutions consisting of 102,933 steps. Each step includes both visual illustrations and textual descriptions to guide problem-solving. Additionally, we provide annotations for multimodal attention, which highlight important components across modalities, and solution graphs, which represent the steps in a structured manner. These data and annotations enable the development of a range of vision-language tasks for solving real-life problems. Through extensive experiments with different models, we demonstrate the effectiveness of our approach for training and testing these tasks. There is ample room for improvement by learning effective attention mechanisms. The dataset and models are publicly available at https://github.com/formidify/VisualHow.