The modeling of objects in prior work on multisensory object-centric learning has been unrealistic. While the OBJECTFOLDER 1.0 dataset introduced virtual objects with sensory data, it was limited in scale and quality, making it difficult to generalize to real-world scenarios. To address these limitations, we present OBJECTFOLDER 2.0, a large-scale dataset of common household objects in the form of implicit neural representations. This new dataset improves upon OBJECTFOLDER 1.0 in three key ways. Firstly, it contains ten times more objects and renders them much faster. Secondly, the multisensory rendering quality for visual, acoustic, and tactile modalities is significantly enhanced. Lastly, models trained on virtual objects from OBJECTFOLDER 2.0 successfully transfer to real-world objects in challenging tasks such as object scale estimation, contact localization, and shape reconstruction. OBJECTFOLDER 2.0 provides a valuable resource for advancing multisensory learning in computer vision and robotics. The dataset can be accessed at https://github.com/rhgao/ObjectFolder.