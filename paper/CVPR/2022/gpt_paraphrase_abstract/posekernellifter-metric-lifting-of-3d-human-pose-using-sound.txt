Reconstructing the metric 3D pose of a person from a single view image is a challenging problem due to the lack of precise distance measurements. Existing methods address this issue by reconstructing the pose up to scale, but applications like virtual telepresence and robotics require metric scale reconstruction. This paper proposes using audio signals recorded alongside the image to provide complementary information for metric pose reconstruction. The authors introduce a time-invariant transfer function called pose kernel, which captures the interactions between audio signals and the body to extract metric information about the pose. The pose kernel's envelope correlates with the 3D pose, its time response indicates the distance to the microphone, and it is invariant to changes in scene geometry. To implement this approach, a multi-stage 3D CNN is designed to fuse audio and visual signals and learn to reconstruct the metric 3D pose. The authors demonstrate that their multi-modal method achieves accurate metric reconstruction in real-world scenes, surpassing state-of-the-art approaches such as parametric mesh regression and depth regression.