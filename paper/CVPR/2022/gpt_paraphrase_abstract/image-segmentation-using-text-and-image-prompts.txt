We present a system that can produce image segmentations based on arbitrary prompts during testing, without the need for re-training on new datasets. This system utilizes a unified model, trained once, to handle three different segmentation tasks: referring expression segmentation, zero-shot segmentation, and one-shot segmentation. To achieve this, we enhance the CLIP model with a transformer-based decoder for dense prediction. By training on an extended version of the PhraseCut dataset, our system can generate binary segmentation maps for images using either free-text prompts or additional images as queries. We examine various forms of image-based prompts in detail to understand their effectiveness. This hybrid input approach allows for dynamic adaptation to not only the aforementioned segmentation tasks but also any binary segmentation task that can be formulated with a text or image query. Furthermore, our system demonstrates good performance when confronted with generalized queries involving affordances or properties. The code for our system is available at https://eckerlab.org/code/clipseg.