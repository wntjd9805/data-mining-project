Current methods for Image Captioning (IC) predict words based on visual information and partially generated sentences. However, the generated descriptions often become irrelevant due to insufficient visual information. To address this, we propose Dual Information Flow Network (DIFNet1) that utilizes segmentation features as an additional visual information source. This enhances the contribution of visual information for prediction. To maximize the use of both information flows, we introduce Iterative Independent Layer Normalization (IILN), an effective feature fusion module that condenses relevant inputs while retaining modality-specific information. Our experiments demonstrate that our method improves the reliance on visual information, leading to more visually-focused word predictions. As a result, our approach achieves new state-of-the-art performance on the MSCOCO dataset, with a CIDEr score of 136.2 on the COCO Karpathy test split.