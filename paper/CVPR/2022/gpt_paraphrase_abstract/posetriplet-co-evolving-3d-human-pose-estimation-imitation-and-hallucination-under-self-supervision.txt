This study presents a new self-supervised approach for 3D human pose estimation, aiming to improve results in real-world scenarios with unseen poses. The proposed approach generates 2D-3D pose pairs to enhance supervision by introducing a reinforcement-learning-based imitator. This imitator is learned alongside a pose estimator and a pose hallucinator, forming two loops during training to strengthen each other. The pose estimator converts a 2D pose sequence to a low-fidelity 3D output, which is then refined by the imitator to enforce physical constraints. The refined 3D poses are further utilized by the hallucinator to generate diverse data, which is strengthened by the imitator and used to train the pose estimator. This co-evolution scheme allows training a pose estimator without relying on any given 3D data. Experimental results on various benchmarks demonstrate that the proposed approach outperforms the state of the art and achieves 89.1% 3D PCK on MPI-INF-3DHP under self-supervised cross-dataset evaluation setup, improving upon the previous best self-supervised method by 8.6%.