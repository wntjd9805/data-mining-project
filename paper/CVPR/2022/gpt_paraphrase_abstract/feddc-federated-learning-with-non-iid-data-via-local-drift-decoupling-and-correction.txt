Federated learning (FL) is a technique that enables multiple clients to collectively train a high-performance global model without sharing their private data. However, a major challenge in FL is the significant statistical heterogeneity among the local data distributions of the clients, resulting in inconsistent optimized local models. To address this dilemma, we propose a novel FL algorithm called FedDC, which incorporates local drift decoupling and correction. FedDC introduces lightweight modifications in the local training phase, where each client uses an auxiliary local drift variable to track the difference between its local model parameter and the global model parameters. The key concept of FedDC is to leverage this learned local drift variable to bridge the gap and ensure parameter-level consistency. Experimental results and analysis demonstrate that FedDC leads to faster convergence and superior performance in various image classification tasks, while remaining robust in scenarios involving partial participation, non-iid data, and heterogeneous clients.