The current Voice Cloning (VC) tasks are effective in converting written text into spoken words with a desired voice based on a reference audio. However, these tasks do not adequately capture scenarios that require emotions consistent with movie plots, such as movie dubbing. To address this gap, we introduce a new task called Visual Voice Cloning (V2C), which aims to convert text into speech with both a desired voice specified by a reference audio and a desired emotion specified by a reference video. To support research in this area, we create a dataset called V2C-Animation, consisting of 10,217 animated movie clips spanning various genres and emotions. Additionally, we propose a strong baseline using state-of-the-art VC techniques. We also develop evaluation metrics, called MCD-DTW-SL, to assess the similarity between ground-truth and synthesized speeches. Our extensive experiments demonstrate that even the best VC methods struggle to generate satisfactory speeches for the V2C task. We believe that our proposed task, dataset, and evaluation metric will facilitate research in voice cloning and the broader vision-and-language community. The source code and dataset are available at https://github.com/chenqi008/V2C.