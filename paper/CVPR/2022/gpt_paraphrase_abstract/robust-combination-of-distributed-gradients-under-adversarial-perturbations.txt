In scenarios where distributed learning is conducted using gradient descent, the server collects gradients from multiple clients to optimize learning objectives. However, due to variations in data collection and learning environments, some clients may provide incorrect gradients, possibly due to adversarial data or gradient perturbations. Additionally, the server may not know the identities of these affected clients due to privacy and security concerns. Simply aggregating these gradients can lead to misleading results in the learning process. To address this issue, we propose a new server-side learning algorithm that robustly combines gradients. Our algorithm incorporates the local gradients into a manifold of normalized gradients and refines their combinations through a simulated diffusion process. The resulting algorithm is computationally efficient and implemented as a weighted gradient averaging algorithm. Experimental results on various classification and regression benchmark datasets demonstrate that our algorithm achieves significant performance improvements compared to existing robust gradient combination algorithms and the baseline uniform gradient averaging algorithm.