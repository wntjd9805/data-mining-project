We have created a dataset of 1000 video sequences featuring human portraits recorded in real-life, uncontrolled conditions. The recordings were made using a handheld smartphone and a high-quality depth camera. The dataset includes footage of 200 individuals in various poses and locations, and its main purpose is to bridge the gap between raw measurements obtained from smartphones and downstream applications such as state estimation, 3D reconstruction, and view synthesis. The data collection process involved using the smartphone's camera and Inertial Measurement Unit (IMU), as well as an external Azure Kinect DK depth camera software that was synchronized with sub-millisecond precision to the smartphone system. The smartphone flash was utilized to provide additional lighting during recording. We also provide an accurate mask of the foreground person and evaluate its impact on camera alignment accuracy. To evaluate the dataset, we compared multiple state-of-the-art camera alignment methods using a Motion Capture system. We offer a benchmark for smartphone visual-inertial performance in portrait capturing, presenting results for various methods and encouraging the utilization of the provided trajectories in view synthesis and 3D reconstruction tasks.