This paper introduces a new approach to self-supervised monocular depth estimation using multi-frame depth estimation techniques. The proposed method utilizes feature matching and a transformer architecture for cost volume generation. Depth-discretized epipolar sampling is employed for selecting matching candidates, and a series of self- and cross-attention layers are used to refine predictions by enhancing the matching probability between pixel features. The refined cost volume is then decoded into depth estimates. The entire pipeline is trained end-to-end from videos using a photometric objective. Experimental results on the KITTI and DDAD datasets demonstrate that the proposed DepthFormer architecture achieves state-of-the-art performance in self-supervised monocular depth estimation, and even performs competitively against highly specialized supervised single-frame architectures. Additionally, it is shown that the learned cross-attention network allows for the transferability of representations across datasets, thereby increasing the effectiveness of pre-training strategies. The project page for the proposed method can be found at https://sites.google.com/tri.global/depthformer.