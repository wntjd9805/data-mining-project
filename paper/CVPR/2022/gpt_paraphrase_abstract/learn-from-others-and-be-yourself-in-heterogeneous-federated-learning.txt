Federated learning is a distributed learning approach that involves collaborative and local updating on private data. However, it faces challenges such as heterogeneity and catastrophic forgetting. Heterogeneity arises from non-i.i.d data and different architectures, leading to performance degradation and communication barriers. Local updating can result in overfitting and forgetting previously acquired knowledge. To address these challenges, we propose FCCL, which leverages unlabeled public data for communication and constructs a cross-correlation matrix to learn a generalizable representation. Additionally, FCCL utilizes knowledge distillation in local updating to provide inter and intra-domain information without compromising privacy. Empirical results on image classification tasks demonstrate the effectiveness and efficiency of our method.