Differentiable rendering is an essential operation in modern computer vision, as it allows for the use of inverse graphics approaches in machine learning frameworks. However, existing shape representations such as voxels, point clouds, and meshes have limitations in terms of geometric fidelity and topological constraints. On the other hand, implicit representations like occupancy, distance, or radiance fields preserve more fidelity but suffer from complex and inefficient rendering processes that limit scalability. In this study, we propose a new shape representation called Directed Distance Fields (DDFs) that combines the advantages of both explicit and implicit representations. DDFs map an oriented point to surface visibility and depth, enabling fast differentiable rendering and extraction of surface geometry. They can also be easily composed and allow for the extraction of classical unsigned distance fields. We introduce probabilistic DDFs (PDDFs) to model inherent discontinuities in the field. We demonstrate the effectiveness of our method in fitting single shapes, unpaired 3D-aware generative image modeling, and single-image 3D reconstruction tasks. Our approach achieves strong performance using simple architectural components, showcasing the versatility of our representation.