Limited datasets capturing the relationship between vision, language, and emotions have hindered the understanding of the emotional aspect of human intelligence. To address this, the ArtEmis dataset was recently introduced, consisting of a large-scale collection of emotional reactions to images accompanied by language explanations. However, it was found that there was a significant bias towards frequently occurring emotions, leading to less accurate descriptions of under-represented emotions by trained neural models. Attempts to collect new data in the same manner did not effectively mitigate this bias. To tackle this issue, a contrastive data collection approach was proposed to balance the ArtEmis dataset by pairing similar images with contrasting emotions (one positive and one negative). Through this approach, a new dataset called ArtEmis v2.0 was created by combining 260,533 instances from the proposed method with the original ArtEmis dataset. ArtEmis v2.0 features a balanced distribution of emotions and provides more detailed explanations related to the associated artwork. Experimental results demonstrate that neural models trained on the new dataset exhibit improved performance in terms of CIDEr and METEOR evaluation metrics, with a 20% and 7% increase, respectively, compared to the biased dataset. Furthermore, the performance of neural models across all emotion categories, especially under-represented emotions, is significantly enhanced. The collected dataset and code can be accessed at https://artemisdataset-v2.org.