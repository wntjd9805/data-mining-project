The visual relationship recognition (VRR) task is challenging due to the long-tail distribution and large vocabulary of visual relationships. To address this, this paper introduces RelTransformer, a method that utilizes an attention mechanism to model the message-passing flow. RelTransformer represents each image as a fully-connected scene graph and organizes the scene into relation-triplet and global-scene contexts. Through self-attention, messages are directly passed from each element in the contexts to the target relation. Additionally, a learnable memory is incorporated to enhance the representation of long-tail relations. Experimental results demonstrate the effectiveness of RelTransformer, outperforming other models on long-tail VRR benchmarks with skewed distributions. The model also achieves strong results on the VG200 relation detection task. The code for RelTransformer is available at https://github.com/Vision-CAIR/RelTransformer.