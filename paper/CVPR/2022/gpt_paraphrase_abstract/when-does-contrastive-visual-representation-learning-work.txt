Recent advancements in self-supervised representation learning have significantly narrowed the gap between supervised and unsupervised learning in ImageNet classification. However, the field still lacks widely accepted guidelines for replicating this success on different datasets. To address this, we conducted a study on contrastive self-supervised learning using four diverse large-scale datasets. By analyzing data quantity, data domain, data quality, and task granularity, we gained new insights into the essential requirements for effective self-supervised learning. Our findings highlight several key observations: (i) the benefits of additional pretraining data beyond 500k images are limited, (ii) incorporating pretraining images from another domain does not improve the generality of representations, (iii) corrupted pretraining images have varying impacts on supervised and self-supervised pretraining, and (iv) contrastive learning falls behind supervised learning in fine-grained visual classification tasks. These results contribute to the understanding of self-supervised learning and lay the groundwork for future research in this area.