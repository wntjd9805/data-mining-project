We focus on the task of generating controllable person images, where the goal is to recreate a human image with specific body pose and appearance based on a reference image. To achieve this, we propose a method that leverages the structured nature of person images by extracting and distributing semantic entities from the reference image. Our approach utilizes a neural texture extraction and distribution operation, which involves extracting semantic neural textures from the reference image and distributing them based on the spatial distributions learned from target poses. By training our model to predict human images in various poses, we encourage it to extract disentangled and expressive neural textures that represent different semantic entities' appearances. This disentangled representation allows for explicit control over appearance, as the neural textures from different reference images can be combined to manipulate the interested areas' appearance. Experimental comparisons demonstrate the effectiveness of our proposed model. The code for our method is available at https://github.com/RenYurui/Neural-Texture-Extraction-Distribution.