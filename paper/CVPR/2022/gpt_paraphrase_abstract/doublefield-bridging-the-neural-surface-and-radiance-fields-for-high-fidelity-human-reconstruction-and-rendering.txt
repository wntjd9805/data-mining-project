We present DoubleField, an innovative framework that combines the advantages of surface field and radiance field for accurate human reconstruction and rendering. This framework connects the surface field and radiance field through a shared feature embedding and a surface-guided sampling strategy. Additionally, we introduce a view-to-view transformer that integrates multi-view features and learns view-dependent features directly from high-resolution inputs. By utilizing DoubleField and the view-to-view transformer, our approach significantly enhances the quality of both geometry and appearance reconstruction, while enabling direct inference, scene-specific high-resolution fine-tuning, and fast rendering. We validate the effectiveness of DoubleField through quantitative evaluations on multiple datasets and qualitative results from a real-world sparse multi-view system. These results demonstrate the superior capability of DoubleField in achieving high-quality human model reconstruction and photo-realistic free-viewpoint human rendering. The data and source code will be publicly available for research purposes.