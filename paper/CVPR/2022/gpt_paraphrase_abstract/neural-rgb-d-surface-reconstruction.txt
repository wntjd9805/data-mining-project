Obtaining accurate 3D reconstructions of room-scale scenes is crucial for various applications in augmented reality (AR) or virtual reality (VR), including teleconferencing, virtual measuring, virtual room planning, and robotics. Current methods that use neural radiance fields (NeRFs) for volume-based view synthesis show promise in reproducing the appearance of objects or scenes but do not reconstruct the actual surface. The use of densities in the volumetric representation of the surface leads to artifacts when extracting the surface using Marching Cubes. To address this, we propose representing the surface using an implicit function, specifically a truncated signed distance function. We demonstrate how to incorporate this representation into the NeRF framework and utilize depth measurements from a commodity RGB-D sensor like a Kinect. Additionally, we introduce a technique for refining pose and camera parameters, which enhances the overall quality of the reconstruction. Unlike other approaches that focus on novel view synthesis, our method enables the generation of high-quality, metrically accurate 3D reconstructions.