This paper focuses on the task of Audio-Visual Question Answering (AVQA), which involves answering questions about visual objects, sounds, and their associations in videos. This task requires a comprehensive understanding of multimodal information and spatio-temporal reasoning. To facilitate the study of AVQA, the authors introduce a large-scale dataset called MUSIC-AVQA, which consists of over 45,000 question-answer pairs covering various question templates and modalities. The authors develop several baselines and propose a spatio-temporal grounded audio-visual network for the AVQA problem. Their experimental results show that AVQA benefits from multisensory perception and their model outperforms previous approaches in terms of audio-only, video-only, and AVQA tasks. The authors believe that their dataset can serve as a valuable testbed for evaluating and advancing audio-visual scene understanding and spatio-temporal reasoning. The code and dataset can be found at http://gewu-lab.github.io/MUSIC-AVQA/.