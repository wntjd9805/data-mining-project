Domain generalization (DG) aims to improve the ability of models trained on a set of known domains to perform well on unseen target domains. Current DG methods heavily rely on labeled data, which can be expensive or unavailable. To address this issue, we propose exploring the use of unsupervised learning to help deep models generalize across domains.In this study, we focus on a new problem called unsupervised domain generalization (UDG), which aims to learn generalizable models using unlabeled data. We also investigate the impact of pre-training on DG. In UDG, models are first pre-trained using unlabeled data from different source domains, and then trained using labeled data from the source domains. Finally, the models are tested on unseen target domains.To handle the challenges posed by the heterogeneity within unlabeled pretraining data and the distribution shifts between source and target data, we introduce a method called Domain-Aware Representation LearnING (DARLING). Surprisingly, we find that DARLING not only addresses the scarcity of labeled data but also enhances the generalization ability of models even when labeled data is insufficient. DARLING demonstrates superior or comparable performance to the ImageNet pretraining protocol, even when the available data is unlabeled and much smaller in quantity compared to ImageNet. This finding suggests that leveraging large-scale unlabeled data may offer opportunities for improving generalization.Overall, our research explores the use of unsupervised learning in the context of domain generalization. We propose the UDG problem and introduce DARLING as a method to address the challenges associated with unlabeled data and distribution shifts. Our findings demonstrate the potential of DARLING as a pretraining approach and highlight the benefits of utilizing large-scale unlabeled data for improving generalization.