The long-tailed recognition problem arises when dealing with data that follows long-tailed class distributions. Naive training methods result in models that are biased towards common classes, with higher accuracy for these classes. To address this issue, it is important to balance various aspects including data distribution, training losses, and gradients in learning. In this study, we explore weight balancing as an orthogonal approach to address this problem. We observe that naively trained classifiers have artificially larger weights for common classes, due to the abundance of training data available for them compared to rare classes. We investigate three techniques for weight balancing: L2-normalization, weight decay, and MaxNorm. L2-normalization perfectly balances per-class weights to have a unit norm, but this hard constraint may hinder the learning of better classifiers for each class. On the other hand, weight decay penalizes larger weights more heavily, resulting in small balanced weights. The MaxNorm constraint encourages the growth of small weights within a norm ball, but caps all the weights by the radius.Our extensive study demonstrates that both weight decay and MaxNorm help in learning balanced weights and significantly improve the accuracy of long-tailed recognition. Surprisingly, weight decay, which has been underexplored in this context, outperforms prior work. Therefore, we propose a two-stage training paradigm for long-tailed recognition. In the first stage, we learn features using the cross-entropy loss by tuning weight decay. In the second stage, we learn classifiers using a class-balanced loss by tuning weight decay and MaxNorm. This approach achieves state-of-the-art accuracy on five standard benchmarks, serving as a future baseline for long-tailed recognition.