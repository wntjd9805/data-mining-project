Real-time 3D reconstruction using RGBD data often suffers from inaccurate motion estimation between frames, which is exacerbated in single-view systems due to occlusions. To address this, we propose OcclusionFusion, a novel method that calculates occlusion-aware 3D motion for guiding the reconstruction process. Our technique involves estimating the motion of visible regions and using temporal information to infer the motion of occluded regions through a graph neural network with LSTM. Additionally, we compute the confidence of the estimated motion using a probabilistic model, which helps mitigate unreliable motions and improves tracking robustness. Our experimental results, obtained from public datasets and our own recorded data, demonstrate that our method significantly outperforms existing real-time single-view approaches. By reducing motion errors, our technique can handle long and challenging motion sequences. For sequence results, please visit our project page: https://wenbin-lin.github.io/OcclusionFusion.