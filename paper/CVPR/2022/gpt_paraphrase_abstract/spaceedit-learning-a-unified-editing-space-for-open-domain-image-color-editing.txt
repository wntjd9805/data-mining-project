In this study, we propose a unified model for open-domain image editing that focuses on color and tone adjustment while preserving the original content and structure of the images. We draw inspiration from the success of large pretrained models such as BERT, Style-GAN, and CLIP in knowledge transfer and generalization across different tasks within their respective domains. Instead of relying on traditional photo editing software that operates in an operation space defined by parameters like contrast, brightness, and color curve, our model learns a more semantic and intuitive editing space. This editing space is easier to manipulate and provides a unified framework for image-to-image translation.Our model consists of an image encoder and decoder and is trained on pairs of before-and-after edited images to generate multimodal outputs. By inverting image pairs into latent codes within the learned editing space, our model can be applied to various downstream editing tasks. These tasks include language-guided image editing, personalized editing, editing-style clustering, and retrieval. To evaluate the effectiveness of our model, we conduct extensive experiments to explore the unique properties of the editing space. The results demonstrate the superior performance of our model in the aforementioned editing tasks. Overall, our unified model for open-domain image editing offers a more efficient and intuitive approach to color and tone adjustment, with potential applications in various image editing scenarios.