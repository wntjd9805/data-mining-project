To address the challenge of establishing space-time correspondence in various video modeling tasks, such as optical flow and multiple object tracking, different approaches are typically used in each space. This study aims to bridge this gap by extending the contrastive random walk formulation to densely connected, pixel-level space-time graphs. The key contribution is the introduction of hierarchy into the search problem by computing the transition matrix between frames in a coarse-to-fine manner, forming a multiscale contrastive random walk over time. This approach provides a unified technique for self-supervised learning of optical flow, keypoint tracking, and video object segmentation. Experimental results show that the unified model performs competitively with strong self-supervised approaches specific to each task.