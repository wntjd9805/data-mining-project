Recently, there has been an increasing interest in multi-modal learning from video data, which allows for the training of meaningful embeddings without the need for human annotation. This enables tasks such as zero-shot retrieval and action localization. In this study, we introduce a multi-modal fusion transformer that is agnostic to the specific modality of the input data. The transformer is designed to exchange information between different modalities, such as video, audio, and text, and integrate them into a fused representation within a shared multi-modal embedding space. To train the system, we propose using a combinatorial loss that considers all possible combinations of input modalities, including single modalities and pairs of modalities. This approach avoids the use of additional encodings such as position or modality encoding. The resulting model can process and fuse any number of input modalities during testing. Furthermore, the inherent properties of the transformer allow it to handle inputs of varying lengths. We evaluate our approach by training the model on the HowTo100M dataset and testing it on four challenging benchmark datasets. The results demonstrate that our model achieves state-of-the-art performance in zero-shot video retrieval and zero-shot video action localization. The code for our work is also available.