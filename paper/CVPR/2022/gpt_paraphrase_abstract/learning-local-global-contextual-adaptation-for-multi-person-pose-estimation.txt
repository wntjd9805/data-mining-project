This study focuses on multi-person pose estimation using a bottom-up approach. By recognizing that the localization issue of the center-offset formulation can be addressed through a local-window search scheme in ideal situations, we propose a new method called LOGO-CAP. LOGO-CAP utilizes the LOcal-GlObal Contextual Adaptation for humanPose by learning keypoint attraction maps (KAMs) from local keypoints expansion maps (KEMs) in small local windows. These KAMs are then used as dynamic convolutional kernels on keypoints-focused global heatmaps to achieve accurate multi-person pose estimation. Our method is trainable end-to-end and provides near real-time inference speed in a single forward pass. It achieves state-of-the-art performance on the COCO keypoint benchmark for bottom-up human pose estimation and outperforms previous methods by a large margin on the challenging OCHuman dataset.