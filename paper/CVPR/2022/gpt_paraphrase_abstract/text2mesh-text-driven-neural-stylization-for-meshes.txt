This work presents a method called Text2Mesh for intuitively controlling the style of 3D objects. The framework uses a disentangled representation of a 3D object, combining a fixed mesh input with a neural stylefield network (NSF) to predict color and local geometric details based on a target text prompt. The similarity between the text prompt and the stylized mesh is measured using CLIP. Text2Mesh does not require a pre-trained generative model or a specialized 3D mesh dataset, and it can handle low-quality meshes with arbitrary genus without the need for UV parameterization. The technique is capable of synthesizing various styles across different 3D meshes, and the code and results are available on the project webpage.