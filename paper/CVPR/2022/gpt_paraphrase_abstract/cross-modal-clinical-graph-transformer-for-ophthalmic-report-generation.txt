The potential of using data-driven neural networks to automatically generate ophthalmic reports in clinical practice is significant. However, previous methods have neglected the incorporation of prior clinical knowledge, which is essential for accurate reporting. In order to address this issue, we propose a Cross-modal clinicalGraph Transformer (CGT) for ophthalmic report generation (ORG). Our approach involves injecting clinical relation triples into visual features as prior knowledge to guide the decoding process. However, there are two common issues with incorporating knowledge: 1) existing biomedical knowledge bases may not align well with the specific context and language of the report, limiting their usefulness, and 2) incorporating too much knowledge can disrupt the correct interpretation of visual features. To overcome these limitations, we have developed an automatic information extraction scheme based on natural language processing to directly obtain clinical entities and relations from training reports in the ophthalmic domain. Our CGT model first restores a sub-graph from the clinical graph using ophthalmic images and injects the restored triples into visual features. To mitigate the impact of knowledge, we use a visible matrix during the encoding process. Finally, reports are generated using a Transformer decoder based on the encoded cross-modal features. Extensive experiments on the FFA-IR benchmark dataset demonstrate that our proposed CGT model surpasses previous benchmark methods and achieves state-of-the-art performance.