The primary focus of addressing clothes-changing person re-identification (re-id) is to extract features that are not related to clothing, such as face, hairstyle, body shape, and gait. Although current research primarily concentrates on modeling body shape using various information sources like silhouettes and sketches, they do not fully utilize clothes-irrelevant information. In this study, we propose a method called Clothes-based Adversarial Loss (CAL), which extracts clothes-irrelevant features from original RGB images by penalizing the predictive power of re-id models concerning clothes. Extensive experiments demonstrate that CAL, using RGB images alone, outperforms all existing methods on widely-used benchmarks for clothes-changing person re-id. Additionally, videos offer richer appearance and temporal information, which can be utilized to model appropriate spatiotemporal patterns for assisting in clothes-changing re-id. However, there is currently no publicly available dataset for clothes-changing video re-id. Therefore, we contribute a new dataset named CCVID and demonstrate the potential for improvement in modeling spatiotemporal information. The code and new dataset can be accessed at: https://github.com/guxinqian/Simple-CCReID.