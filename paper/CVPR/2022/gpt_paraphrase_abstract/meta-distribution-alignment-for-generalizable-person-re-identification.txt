Domain Generalizable (DG) person ReID is a challenging task that involves training a model on source domains and achieving good performance on target domains. Current methods focus on learning domain-invariant features from source domains, assuming that these features will also be irrelevant in target domains. However, they overlook the target domain information that is not available during training. To address this limitation, we propose a novel approach called Meta Distribution Alignment (MDA) that enables the sharing of similar distributions in a test-time-training manner. In our method, we introduce an intermediate latent space constrained to a known prior distribution to handle the complexity of high-dimensional features. We map the source domain data to this latent space and then reconstruct it back. Additionally, we incorporate a meta-learning strategy to facilitate generalization and support fast adaptation. To further reduce the discrepancy between source and target domains, we propose a test-time adaptive updating strategy based on the latent space. This strategy efficiently adapts the model to unseen domains using only a few samples.Extensive experiments demonstrate the effectiveness of our model, outperforming state-of-the-art methods by up to 5.2% R-1 on average in large-scale scenarios and 4.7% R-1 in the single-source domain generalization ReID benchmark. We have made our source code publicly available at https://github.com/haoni0812/MDA.git.