Modelling interactions between humans and objects in natural environments is crucial for various applications, such as gaming, virtual and mixed reality, human behavior analysis, and human-robot collaboration. However, this task is challenging due to the need for generalization across numerous objects, scenes, and human actions. Unfortunately, there is currently no dataset available that fulfills these requirements. Additionally, acquiring this data in diverse natural environments is not feasible using 4D scanners and marker-based capture systems.  To address this gap, we introduce the BEHAVE dataset, which is the first dataset to provide full-body human-object interaction data. This dataset includes multi-view RGBD frames, along with corresponding 3D SMPL and object fits, and annotations for the contacts between humans and objects. Our data collection process involved recording approximately 15,000 frames at 5 different locations, with 8 subjects engaging in a wide range of interactions with 20 common objects.  We leverage this dataset to develop a model that can simultaneously track humans and objects in natural environments using a portable multi-camera setup. Our approach involves predicting correspondences between the human and the object, which are then used to determine human-object contacts during interactions. Importantly, our model captures these interactions in 3D, specifically as surface contacts.  For researchers and practitioners interested in utilizing our dataset and code, they can access them through our website: http://virtualhumans.mpi-inf.mpg.de/behave.