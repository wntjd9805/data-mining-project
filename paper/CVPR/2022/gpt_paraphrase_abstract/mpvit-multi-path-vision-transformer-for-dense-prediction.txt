The abstract discusses the importance of multi-scale feature representation in dense computer vision tasks like object detection and segmentation. While Convolutional Neural Networks (CNNs) have been widely used for such tasks, Vision Transformers (ViTs) have emerged as a potential alternative. The abstract introduces a new approach called Multi-Path Vision Transformer (MPViT) that explores multi-scale patch embedding and a multi-path structure. MPViT embeds features of different scales simultaneously using overlapping convolutional patch embedding. These features are then independently processed through Transformer encoders via multiple paths and aggregated to enable fine and coarse feature representations at the same level. The MPViTs consistently outperform state-of-the-art Vision Transformers in various computer vision tasks such as ImageNet classification, object detection, instance segmentation, and semantic segmentation.