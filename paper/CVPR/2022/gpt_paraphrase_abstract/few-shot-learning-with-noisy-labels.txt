Few-shot learning (FSL) methods often assume that support sets used for training on new classes are clean and accurately labeled. However, this assumption is often unrealistic as support sets, even if small, can contain mislabeled samples. It is crucial for FSL methods to be robust to label noise in order to be practical, yet this problem has been largely unexplored. To address mislabeled samples in FSL settings, we propose several technical contributions. Firstly, we introduce simple but effective feature aggregation methods that improve the prototypes used by the popular FSL technique, ProtoNet. Secondly, we present a novel model called TraNFS (Transformer for Noisy Few-Shot Learning), which leverages the attention mechanism of a transformer to weigh mislabeled samples against correctly labeled samples. Lastly, we extensively evaluate these methods on noisy versions of MiniImageNet and TieredImageNet datasets. Our results demonstrate that TraNFS performs comparably to leading FSL methods when trained on clean support sets, but significantly outperforms them in the presence of label noise.