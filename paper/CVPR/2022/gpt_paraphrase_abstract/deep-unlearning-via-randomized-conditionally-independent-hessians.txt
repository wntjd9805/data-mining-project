Recent legislation has sparked interest in the concept of machine un-learning, which involves removing specific training samples from a predictive model as if they were never part of the training dataset. There are various reasons why unlearning may be necessary, such as dealing with corrupted or adversarial data, or addressing updated privacy requirements from users. While deleting the closest original sample can be effective for models that require no training like k-NN, this approach is not suitable for models that learn more complex representations. Existing ideas that rely on optimization-based updates have scalability issues when it comes to the model dimension. This is because they involve inverting the Hessian matrix of the loss function, which can be computationally expensive.To overcome these challenges, we propose a variant of a new conditional independence coefficient called L-CODEC. This coefficient helps identify a subset of the model parameters that have the highest semantic overlap on an individual sample level. The advantage of our approach is that it completely avoids the need to invert a potentially large matrix. By using a Markov blanket selection, we believe that L-CODEC is also suitable for deep un-learning and other applications in computer vision. Compared to alternative methods, L-CODEC enables approximate unlearning in scenarios that would otherwise be impractical. This includes vision models used for face recognition, person re-identification, and NLP models that may require unlearning samples for exclusion.For those interested in implementing our approach, the code is available at https://github.com/vsingh-group/LCODEC-deep-unlearning.