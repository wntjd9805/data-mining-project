In order to overcome the limitations of current video interpolation methods that heavily rely on deep convolution neural networks, we propose a new framework based on Transformers. This framework addresses the issues of content-agnostic kernel weights and restricted receptive field by introducing content-aware aggregation weights and considering long-range dependencies through self-attention operations. To mitigate the computational cost of global self-attention, we introduce local attention into video interpolation and extend it to the spatial-temporal domain. Additionally, we propose a space-time separation strategy to reduce memory usage and improve performance. To fully leverage the capabilities of Transformers, we develop a multi-scale frame synthesis scheme. Extensive experiments demonstrate that our proposed model outperforms state-of-the-art methods both quantitatively and qualitatively on various benchmark datasets. The code and models of our proposed framework are publicly available at https://github.com/zhshi0816/Video-Frame-Interpolation-Transformer.