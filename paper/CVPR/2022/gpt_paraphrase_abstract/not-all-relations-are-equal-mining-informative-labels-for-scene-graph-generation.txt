Scene graph generation (SGG) is important for understanding scenes as it captures various interactions between objects. However, existing SGG methods trained on all relations lack complex reasoning due to biases in the training data. Learning from generic spatial relations instead of informative ones limits generalization. To address this, we propose a novel SGG training framework that leverages relation labels based on their informativeness. Our model-agnostic approach fills in missing informative relations for less informative samples and trains a SGG model on the imputed labels and existing annotations. We demonstrate that this approach improves the performance of state-of-the-art SGG methods on the Visual Genome benchmark, achieving significant improvements in multiple metrics. Moreover, our framework also shows considerable improvements in a more challenging zero-shot setting for unseen triplets.