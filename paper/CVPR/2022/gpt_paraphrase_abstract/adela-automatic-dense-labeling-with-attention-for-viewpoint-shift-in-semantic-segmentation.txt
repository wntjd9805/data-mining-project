We present a novel approach to address the decrease in performance in semantic segmentation when there are changes in viewpoint within multi-camera systems. Although we have access to temporally paired images, the annotations may only be abundant for a few typical views. Existing methods attempt to mitigate the performance drop by aligning domains in a shared space and assuming the transferability of the mapping from the aligned space to the output. However, the introduction of new content due to viewpoint changes may render such a space ineffective for alignment, resulting in negative adaptation. Instead of aligning image statistics between domains, our method utilizes an attention-based view transformation network trained solely on color images to generate semantic images for the target. Despite the lack of supervision, the view transformation network can still generalize to semantic images due to the induced "information transport" bias. Additionally, to handle uncertainties in converting semantic images to semantic labels, we treat the view transformation network as a functional representation of an unknown mapping implied by the color images and propose functional label hallucination to generate pseudo-labels with uncertainties in the target domains. Our method surpasses baselines that rely on state-of-the-art correspondence estimation and view synthesis methods. Furthermore, it outperforms the current unsupervised domain adaptation methods that use self-training and adversarial domain alignments. We will make our code and dataset publicly available.