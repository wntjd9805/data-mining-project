Traditional object detectors are not suitable for incremental learning, as fine-tuning them with only new data leads to catastrophic forgetting. To address this issue, knowledge distillation is used to mitigate catastrophic forgetting. Previous research in Incremental Object Detection (IOD) has focused on distilling knowledge from the combination of features and responses, but has overlooked the information contained in responses. In this paper, we propose a response-based incremental distillation method called Elastic Response Distillation (ERD). ERD focuses on learning responses elastically from the classification head and regression head of the detector. Our method transfers category knowledge and enables the student detector to retain localization information during incremental learning. Additionally, we evaluate the quality of all locations and provide valuable responses using the Elastic Response Selection (ERS) strategy. We also highlight that different responses should be assigned different levels of importance during incremental distillation. Extensive experiments on the MS COCO dataset demonstrate that our method achieves state-of-the-art results, significantly reducing the performance gap compared to full training. The code for our method is available at https://github.com/Hi-FT/ERD.