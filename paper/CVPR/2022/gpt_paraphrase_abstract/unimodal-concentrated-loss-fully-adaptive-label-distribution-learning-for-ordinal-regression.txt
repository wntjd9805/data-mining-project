Learning from a label distribution has shown promising results in ordinal regression tasks such as facial age and head pose estimation. Adaptive label distribution learning (ALDL) has gained attention for its theoretical superiority, but it has not outperformed methods assuming fixed label distributions. This paper argues that existing ALDL algorithms do not fully utilize the intrinsic properties of ordinal regression. The authors propose three principles for learning an adaptive label distribution: the highest probability should correspond to the ground truth, the probabilities of neighboring labels should decrease with increasing distance from the ground truth (unimodal distribution), and the label distribution should vary with sample changes and instances with the same label. Based on these principles, they introduce a novel loss function called unimodal-concentrated loss for fully adaptive label distribution learning. The loss function ensures a unimodal distribution by constraining it using a learning to rank strategy and integrates estimation error and variance to maximize the distribution at the ground truth and account for predicting uncertainty. Experimental results on age and head pose estimation tasks demonstrate the superiority of the proposed loss function compared to existing ones.