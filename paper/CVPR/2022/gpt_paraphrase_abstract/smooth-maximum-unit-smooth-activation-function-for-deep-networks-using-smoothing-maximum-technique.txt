Deep learning researchers are constantly seeking new activation functions to enhance neural network performance. Selecting the right activation function can greatly improve network performance and training dynamics. While Rectified Linear Unit (ReLU) is a popular choice due to its simplicity, it has its limitations. This paper introduces two new activation functions, Smooth Maximum Unit (SMU) and SMU-1, which are based on approximations of the maximum function. These functions can effectively approximate ReLU, Leaky ReLU, and the more general Maxout family, with GELU being a specific case of SMU. By replacing ReLU with SMU, the Top-1 classification accuracy on the CIFAR100 dataset improves by 6.22%, 3.39%, 3.51%, and 3.08% for the ShuffleNet V2, PreActResNet-50, ResNet-50, and SeNet-50 models respectively. Experimental evaluations demonstrate that SMU and SMU-1 outperform widely used activation functions in various deep learning tasks such as image classification, object detection, semantic segmentation, and machine translation.