Neural networks have made it possible to interpret 3D point clouds more effectively. However, dealing with large-scale 3D scenes is still a challenging task. Existing methods divide these scenes into smaller regions and combine local predictions, but this approach requires additional preprocessing and post-processing stages, and it may also result in lower-quality predictions due to the local perspective. This paper introduces the Fast Point Transformer, which includes a lightweight self-attention layer. Our method encodes continuous 3D coordinates and uses a voxel hashing-based architecture to enhance computational efficiency. We demonstrate the effectiveness of our approach in 3D semantic segmentation and 3D detection. Our method achieves competitive accuracy compared to the best voxel-based method, while also offering 129 times faster inference time than the state-of-the-art Point Transformer. This improvement comes with a reasonable trade-off in accuracy for 3D semantic segmentation on the S3DIS dataset.