Quantization is a technique used to reduce the memory and computational requirements of deep neural networks by representing weights and activations as low-bit values. However, existing quantization methods use a static bit-width for all samples in a dataset, which may not be optimal for diverse natural images. In this paper, we propose a dynamic quantization approach that individually quantizes each image and determines its optimal bit-width using a lightweight bit-controller trained alongside the neural network. During inference, the controller generates the appropriate bit-widths based on the image's content, allocating lower bits for simpler textures to reduce computational complexity. Experimental results on benchmark datasets demonstrate that our dynamic quantization method achieves state-of-the-art performance in terms of accuracy and computational complexity. The code for our method is available at https://github.com/huawei-noah/Efficient-Computing and https://gitee.com/mindspore/models/tree/master/research/cv/DynamicQuant.