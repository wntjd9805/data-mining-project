This paper focuses on the emerging topic of explaining the generalization characteristics of deep learning in advanced machine learning. The authors aim to address the question of whether it is possible to analyze intermediate layers of a deep neural network to determine the quality of learning in each layer. To achieve this, they propose new metrics that measure redundant information in a network's layers using a low-rank factorization framework. Additionally, they introduce a complexity measure that is closely correlated with the generalization performance of a given optimizer, network, and dataset. These metrics are then used to enhance the Stochastic Gradient Descent (SGD) optimizer by adaptively adjusting the learning rate in each layer, resulting in improved generalization performance. The augmented SGD, referred to as RMSGD, requires minimal computational overhead and outperforms state-of-the-art methods across various applications, architectures, and datasets.