The growing interest in data representation learning without labels is due to its ability to eliminate the need for human annotation. This type of learning has recently been extended to bimodal data, specifically sound and image, which are closely related to human senses. However, existing methods for sound and image representation learning require a large number of paired data, making it challenging to ensure their effectiveness in weakly paired conditions where paired bimodal data is lacking. Cognitive studies suggest that the human brain can enhance cognitive functions for a certain modality by receiving input from other modalities, even if they are not directly paired. In light of this observation, we propose a new problem: how to enhance a specific modal representation using unpaired modal data. To address this issue, we introduce a novel bimodal associative memory (BMA-Memory) with key-value switching. This memory allows for the construction of sound-image associations using a small amount of paired bimodal data and the enhancement of these associations using a large amount of unpaired data. Through this associative learning approach, it is possible to strengthen the representation of a particular modality (e.g., sound) using other unpaired modal data (e.g., images).