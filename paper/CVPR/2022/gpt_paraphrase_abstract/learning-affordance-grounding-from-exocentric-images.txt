This paper introduces the concept of affordance grounding, which involves localizing action possibility regions in objects. The challenge lies in establishing a clear connection between object parts and interactive affordances due to their diversity. Humans have the ability to transform various exocentric interactions into invariant egocentric affordances to overcome this diversity. The paper proposes a task of affordance grounding from an exocentric view, where the goal is to learn affordance knowledge from exocentric human-object interactions and transfer it to egocentric object images using only affordance labels as supervision. To achieve this, a cross-view knowledge transfer framework is developed, which extracts affordance-specific features from exocentric interactions and enhances the perception of affordance regions by preserving affordance correlation. The paper introduces an Affordance Invariance Mining module to extract specific clues by minimizing differences in exocentric images caused by interaction habits. Additionally, an Affordance Co-relation Preserving strategy is presented to align the correlation matrix of predicted results between the two views, enabling the perception and localization of affordance. The authors construct a dataset called AGD20K, consisting of over 20K images labeled with 36 affordance categories, to evaluate their method. Experimental results demonstrate that their approach outperforms representative models in terms of objective metrics and visual quality. The code for their method is available on GitHub at github.com/lhc1224/Cross-View-AG.