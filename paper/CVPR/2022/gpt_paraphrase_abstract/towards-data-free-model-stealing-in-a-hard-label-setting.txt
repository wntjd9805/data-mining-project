Machine learning models deployed as a service (MLaaS) can be vulnerable to model stealing attacks, where an adversary tries to steal the model even within a restricted access framework. While previous attacks have shown that near-perfect clones can be created using softmax predictions of the classification network, most APIs only provide access to the top-1 labels. In this study, we prove that it is possible to steal machine learning models by only accessing the top-1 predictions (Hard Label setting), even without access to model gradients (Black-Box setting) or the training dataset (Data-Free setting), and with a limited query budget. We propose a new framework based on Generative Adversarial Networks (GAN) that trains the student and generator simultaneously to effectively steal the model. We address the challenge of the hard label setting by using gradients of the clone network as a substitute for the victim's gradients. To overcome the high query costs associated with the Data-Free setting, we utilize publicly available datasets as a weak image prior, even if they are unrelated to the target dataset. Furthermore, we demonstrate that state-of-the-art results can be achieved within a low query budget using synthetically created samples, even in the absence of such prior data. We are the first to show the scalability of model stealing in a restricted access setting on a dataset with 100 different classes.