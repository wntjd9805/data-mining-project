We present Neural Point Light Fields, a method for representing scenes using a sparse point cloud that contains a light field. By combining differentiable volume rendering with learned implicit density representations, we can generate photorealistic images of new views for small scenes. However, traditional neural volumetric rendering methods are limited to small scenes with the same objects projected to multiple training views, as they require dense sampling of the scene representation. To overcome this limitation, we transform sparse point clouds into neural implicit light fields, which allows us to effectively represent large scenes with only one radiance evaluation per ray. These point light fields are dependent on the ray direction and local point feature neighborhood, enabling us to interpolate the light field based on training images without dense object coverage and parallax. We evaluate our method for synthesizing novel views in large driving scenarios, where we successfully generate realistic views that existing implicit approaches fail to represent. Our results demonstrate that Neural Point Light Fields enable the prediction of videos along previously unexplored trajectories, which was previously only possible through explicit scene modeling.