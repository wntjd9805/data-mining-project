We propose Face-Former, a Transformer-based autoregressive model for speech-driven 3D facial animation. This addresses the challenges posed by complex facial geometry and limited 3D audio-visual data. Previous approaches have focused on phoneme-level features in short audio windows, leading to inaccurate lip movements. To overcome this limitation, our model encodes long-term audio context and predicts a sequence of animated 3D face meshes. To address data scarcity, we incorporate self-supervised pre-trained speech representations. Additionally, we introduce two biased attention mechanisms tailored for this task: biased cross-modal multi-head attention and biased causal multi-head self-attention with periodic positional encoding. The former aligns audio and motion modalities effectively, while the latter enables generalization to longer audio sequences. Extensive experiments and a user study demonstrate that our approach outperforms existing state-of-the-art methods. The code and video can be found at: https://evelynfan.github.io/audio2face/.