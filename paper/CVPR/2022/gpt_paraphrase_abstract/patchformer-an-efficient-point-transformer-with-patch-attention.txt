The point cloud learning community is shifting from using CNNs to Transformers for modeling, as pure Transformer architectures have shown high accuracy on major learning benchmarks. However, existing point Transformers are computationally expensive due to the need for generating a large attention map, which has quadratic complexity in terms of input size. To address this issue, we propose Patch ATtention (PAT), which learns a smaller set of bases to compute attention maps. By summing these bases, PAT captures global shape context and achieves linear complexity. We also introduce a lightweight Multi-Scale aTtention (MST) block to enable attentions among features of different scales. Our neural architecture, called PatchFormer, combines PAT and MST into a joint framework for point cloud learning. Extensive experiments demonstrate that our network achieves comparable accuracy to previous point Transformers but with a 9.2Ã— speed-up.