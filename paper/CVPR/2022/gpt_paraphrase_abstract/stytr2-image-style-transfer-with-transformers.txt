This study introduces a transformer-based approach called StyTr2 for image style transfer, which aims to render images with artistic features while preserving the original content. Traditional methods in neural style transfer struggle with biased content representation due to the difficulty in extracting and maintaining global information from input images, which is caused by the locality in convolutional neural networks (CNNs). To overcome this issue, StyTr2 considers long-range dependencies in input images. It consists of two transformer encoders that generate domain-specific sequences for content and style, respectively. These sequences are then passed through a multi-layer transformer decoder to stylize the content sequence based on the style sequence. The study also addresses the limitations of existing positional encoding methods and proposes a content-aware positional encoding (CAPE) that is scale-invariant and better suited for image style transfer tasks. The effectiveness of StyTr2 is demonstrated through qualitative and quantitative experiments, which show its superiority over state-of-the-art CNN-based and flow-based approaches. The code and models for StyTr2 are publicly available at https://github.com/diyiiyiii/StyTR-2.