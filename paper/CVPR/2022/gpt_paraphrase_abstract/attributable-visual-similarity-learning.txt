This paper introduces the concept of Attributable Visual Similarity Learning (AVSL) as a framework for accurately measuring and explaining the similarity between images. Many existing methods for similarity learning lack explainability as they map each sample to a single point in an embedding space using a distance metric. Inspired by how humans perceive semantic similarity, we propose a generalized similarity learning approach that represents the similarity between images using a graph and infers overall similarity based on this representation. Additionally, we establish a framework for bottom-up similarity construction and top-down similarity inference to determine similarity based on semantic hierarchy consistency. We first identify unreliable higher-level similarity nodes and then correct them using adjacent lower-level similarity nodes, preserving traces for similarity attribution. Extensive experiments on multiple datasets demonstrate significant improvements over existing deep similarity learning methods and validate the interpretability of our framework.