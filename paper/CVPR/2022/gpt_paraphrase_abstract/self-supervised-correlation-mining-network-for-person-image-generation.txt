The objective of person image generation is to perform non-rigid de-formation on source images, typically requiring un-aligned data pairs for training. Recent self-supervised methods have shown promise in this task by combining disentangled representations for self-reconstruction. However, these methods do not effectively utilize the spatial correlation between the disentangled features. In this study, we propose a Self-supervised Correlation Mining Network (SCM-Net) that rearranges the source images in the feature space using two collaborative modules: Decomposed Style Encoder (DSE) and Correlation Mining Module (CMM). The DSE creates unaligned pairs at the feature level, while the CMM establishes a spatial correlation field for feature rearrangement. A translation module is then used to transform the rearranged features into realistic results. Additionally, we introduce a graph-based Body Structure Retaining Loss (BSR Loss) to improve the fidelity of cross-scale pose transformation by preserving reasonable body structures during half body to full body generation. Extensive experiments on the DeepFashion dataset demonstrate the superiority of our method compared to other supervised and unsupervised approaches. Furthermore, our method shows satisfactory results in face generation, showcasing its versatility in other deformation tasks.