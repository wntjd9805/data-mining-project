Recent self-supervised contrastive learning methods have seen significant advancements by utilizing the Siamese structure to minimize distances between positive pairs. However, the effectiveness of such methods heavily relies on the design of high-quality contrastive pairs. Previous approaches have often used random sampling to create different image crops, neglecting the semantic information that can impact the quality of views.To address this limitation, we propose ContrastiveCrop, a technique that generates improved crops for Siamese representation learning. We introduce a semantic-aware object localization strategy that guides the training process in a fully unsupervised manner. This strategy enables the generation of contrastive views that minimize false positives, such as distinguishing between objects and backgrounds. Additionally, we observe that views with similar appearances do not contribute significantly to Siamese model training. As a result, we design a center-suppressed sampling technique that increases the diversity of crops.Remarkably, our method considers positive pairs for contrastive learning without incurring significant extra training overhead. It can be easily integrated into different frameworks as a plug-and-play module. When applied to SimCLR, MoCo, BYOL, and SimSiam, ContrastiveCrop consistently improves classification accuracy on CIFAR-10, CIFAR-100, Tiny ImageNet, and STL-10 by 0.4% to 2.0%. Furthermore, we achieve superior results in downstream detection and segmentation tasks when pre-trained on ImageNet-1K.In summary, our proposed ContrastiveCrop technique enhances Siamese representation learning by generating better crops through semantic-aware object localization and center-suppressed sampling. It offers improved performance across various datasets and tasks, making it a valuable addition to self-supervised contrastive learning methods.