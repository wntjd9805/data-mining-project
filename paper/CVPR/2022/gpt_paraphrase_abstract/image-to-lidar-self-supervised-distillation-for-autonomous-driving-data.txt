Autonomous driving requires the segmentation and detection of objects in sparse Lidar point clouds. However, annotating 3D Lidar data for these tasks is time-consuming and expensive. To address this issue, we propose a self-supervised pre-training method for 3D perception models in autonomous driving. By utilizing synchronized and calibrated image and Lidar sensors, we distill self-supervised pre-trained image representations into 3D models, eliminating the need for point cloud or image annotations. Our method utilizes superpixels to pool 3D point features and 2D pixel features in visually similar regions. We then train a 3D network to match these pooled features. The use of superpixels has several advantages: it creates a more meaningful contrastive task, ensures equal weight for all regions, and reduces noise from incorrect matching due to occlusions. Extensive experiments on autonomous driving datasets demonstrate the effectiveness of our image-to-Lidar distillation strategy in producing 3D representations that transfer well to semantic segmentation and object detection tasks.