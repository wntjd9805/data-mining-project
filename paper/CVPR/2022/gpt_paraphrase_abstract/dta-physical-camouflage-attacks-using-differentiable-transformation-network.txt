Previous studies have proposed adversarial camouflage as a method to hide objects in the physical world by applying camouflage patterns on 3D object surfaces. However, existing techniques using neural renderers have limitations in representing real-world transformations due to a lack of control over scene parameters. In this paper, we introduce the Differen-tiable Transformation Attack (DTA) framework, which generates a robust physical adversarial pattern on a target object to camouflage it against object detection models with a wide range of transformations. Our approach utilizes the Differentiable Transformation Network (DTN), which learns the expected transformation of a rendered object when the texture is changed while preserving the original properties of the target object. By offering differentiability, our attack framework combines the advantages of legacy photo-realistic renderers and white-box access. Experimental results demonstrate that our camouflaged 3D vehicles can successfully evade state-of-the-art object detection models in a photo-realistic environment. Additionally, we showcase the applicability and transferability of our method to the real world through a demonstration on a scaled Tesla Model 3.