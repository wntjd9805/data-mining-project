The use of optimization-based tracking methods, which incorporate a target model prediction module, has been highly successful in providing global reasoning through objective function minimization. However, this approach restricts the expressiveness of the tracking network due to its reliance on domain-specific knowledge. To address this limitation, we propose a tracker architecture that utilizes a Transformer-based model prediction module. Transformers, with their ability to capture global relations without strong biases, enable the learning of more powerful target models. We also extend the model predictor to estimate a second set of weights for precise bounding box regression. Our resulting tracker, named ToMP, leverages both training and test frame information to predict all weights transductively. We train ToMP end-to-end and evaluate its performance on various tracking datasets. Notably, ToMP achieves state-of-the-art results on three benchmarks, with an AUC of 68.5% on the challenging LaSOT dataset. The code and trained models for ToMP are available at https://github.com/visionml/pytracking.