Considerable progress has been made in visual captioning using pre-trained features and fixed object detectors. However, a major limitation of these methods is that the output of the model is solely based on the object detector's outputs. This assumption is unrealistic, particularly when the detector is applied to different datasets.   To address this issue, we propose augmenting the captioning model with an auxiliary input that represents missing information, such as object relationships. To accomplish this, we leverage attributes and relationships mined from the Visual Genome dataset and condition the captioning model on them. We also utilize a multi-modal pre-trained model called CLIP to retrieve contextual descriptions, which we demonstrate to be crucial for our approach.  Additionally, we observe that object detector models, when frozen, lack the necessary richness to properly ground the captioning model. Thus, we suggest conditioning both the detector and description outputs on the image, which leads to improved grounding.   We validate our method on image captioning and conduct comprehensive analyses of each component, including the importance of the pre-trained multi-modal model. The results show significant improvements over the current state of the art, with an increase of 7.5% in CIDEr and 1.3% in BLEU-4 metrics.