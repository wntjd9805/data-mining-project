Mixup is a widely-used technique for data augmentation that involves interpolating between examples in the input or feature space and their corresponding target labels. However, the best way to interpolate images using mixup is not well-defined. Previous methods have involved overlaying or cutting-and-pasting multiple objects into one image, but this requires careful selection of regions. Another approach has been to connect mixup with autoencoders, as autoencoders can generate images that continuously deform into each other. However, these generated images are often of low quality.In this study, we propose a new approach to mixup called AlignMixup, which focuses on the deformation perspective. In AlignMixup, we geometrically align two images in the feature space. By establishing correspondences between the images, we can interpolate between two sets of features while preserving the spatial locations of one set. This results in a mixture that retains the geometry or pose of one image and the appearance or texture of the other. We also demonstrate that an autoencoder can still improve representation learning under mixup, even without the classifier seeing the decoded images.Our experiments show that AlignMixup outperforms state-of-the-art mixup methods on five different benchmark datasets. We have made the code for AlignMixup available at https://github.com/shashankvkt/AlignMixup_CVPR22.git.