The fusion of different sensor modalities, such as camera, Li-dar, and Radar, is commonly used in autonomous vehicles to improve detection accuracy and ensure robust perception even in adverse weather conditions and sensor failures. Radar, in particular, performs well in extreme weather conditions that can significantly impair the performance of camera and Li-dar sensors. Some recent studies have developed vehicle detection methods that combine Li-dar and Radar signals, known as MVD-Net. However, these models assume that both sensors are always error-free, which can lead to catastrophic failures if one of the sensors is unavailable or faulty. To address this issue, we propose a new approach called Self-Training Multimodal Vehicle Detection Network (ST-MVDNet). Our method utilizes a Teacher-Student mutual learning framework and a simulated sensor noise model to augment the data for Li-dar and Radar. By enforcing output consistency between the Teacher and Student networks and introducing missing modalities during training, our model is able to handle missing sensor data and improve the Teacher model. Through experiments, we demonstrate that our learning framework is effective in handling missing sensor data during inference and achieves state-of-the-art performance (5% improvement) on the Oxford Radar Robotcar dataset in various evaluation settings.