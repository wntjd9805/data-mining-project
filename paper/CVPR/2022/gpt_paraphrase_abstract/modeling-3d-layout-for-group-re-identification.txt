Group re-identification (GReID) aims to correctly identify groups with the same members across different cameras. The main challenge lies in dealing with variations in group membership and layout. Existing approaches incorporate layout modeling based on appearance features to achieve robust group representations, but they introduce layout ambiguity by only considering the 2D layout on the imaging plane. In this paper, we propose a solution to overcome these limitations by introducing 3D layout modeling. We introduce a novel 3D transformer (3DT) that reconstructs the relative 3D layout relationship among group members. This involves sampling and quantifying a series of layout tokens along three dimensions, and selecting the corresponding tokens as layout features for each member. To address the problem of data shortages and poor annotations, we create a synthetic GReID dataset called City1M. This dataset includes 1.84 million images, 45,000 individuals, and 11,500 groups, all with 3D annotations. To our knowledge, 3DT is the first approach to tackle GReID from a 3D perspective, and City1M is currently the largest dataset available for this task. Several experiments demonstrate the superiority of our 3DT approach and the usefulness of the City1M dataset. Our project, including the dataset, has been made publicly available at https://github.com/LinlyAC/City1M-dataset.