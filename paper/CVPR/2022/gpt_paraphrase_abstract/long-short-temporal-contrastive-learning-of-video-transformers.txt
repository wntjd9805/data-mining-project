This paper explores the effectiveness of self-supervised pretraining for video transformers, which have emerged as a competitive alternative to 3D CNNs for video understanding. These models typically require supervised pretraining on large-scale image datasets, but this study shows that self-supervised pretraining on video-only datasets can achieve comparable or even better results. The authors propose a learning procedure called Long-Short Temporal Contrastive Learning (LSTCL) that forces the model to match long-term and short-term views of the same video, enabling the video transformers to learn effective clip-level representations. The approach is validated using three different self-supervised contrastive learning frameworks (MoCo v3, BYOL, SimSiam) and two distinct video-transformer architectures. Through a comprehensive ablation study, the authors demonstrate that LSTCL achieves competitive performance on multiple video benchmarks and can serve as a viable alternative to supervised image-based pretraining.