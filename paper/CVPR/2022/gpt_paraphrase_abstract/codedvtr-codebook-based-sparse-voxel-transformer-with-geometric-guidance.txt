Transformers have shown promising results in 2D vision tasks but they struggle with generalization and require extensive pre-training and sophisticated training techniques. Applying transformers to 3D tasks is even more challenging due to the irregular data structure and limited data scale. To address these issues, we propose Cod-edVTR, a Codebook-based Voxel Transformer. This approach enhances data efficiency and generalization for 3D sparse voxel transformers. We introduce codebook-based attention that projects an attention space into a learnable subspace represented by "prototypes" in a codebook. This regularization technique improves attention learning and generalization. Additionally, we propose geometry-aware self-attention that incorporates geometric information (geometric patterns and density) to guide attention learning. Cod-edVTR can be integrated into existing sparse convolution-based methods and consistently improves performance in indoor and outdoor 3D semantic segmentation tasks.