A new approach called Dual-task Pose Transformer Network (DPTN) is proposed for Pose Guided Person Image Generation (PGPIG). The existing methods for this task often fail to capture realistic texture mapping. To overcome this limitation, DPTN introduces an auxiliary task called source-to-source task and utilizes the correlation between the two tasks to improve the performance of PGPIG. DPTN consists of a Siamese structure with a source-to-source self-reconstruction branch and a transformation branch for source-to-target generation. By sharing weights between these branches, the knowledge learned from the source-to-source task can effectively assist the source-to-target learning process. Additionally, a Pose Transformer Module (PTM) is introduced to establish correlation between features from the dual tasks, enabling fine-grained mapping of pixels between the source and target images. This enhances the transmission of source textures and improves the details of the generated target images. Experimental results demonstrate that DPTN outperforms existing methods in terms of both PSNR and LPIPS metrics. Furthermore, DPTN has significantly fewer parameters compared to other approaches. The code for DPTN is available at the provided GitHub link.