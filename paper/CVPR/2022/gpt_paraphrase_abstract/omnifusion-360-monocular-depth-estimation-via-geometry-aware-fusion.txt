A major challenge in using deep-learning methods with omnidirectional images is the distortion caused by the spherical shape. This distortion leads to the loss of important information in tasks such as depth estimation. To address this issue, we present a pipeline called OmniFusion for monocular depth estimation in 360 images. Our pipeline transforms the distorted 360 image into less-distorted perspective patches using tangent images. We then use a CNN to make patch-wise predictions, which are merged to obtain the final output. To address the discrepancy between patch-wise predictions, we propose a framework that combines 3D geometric features with 2D image features, and uses a self-attention-based transformer architecture for global information aggregation. We also introduce an iterative depth refinement mechanism based on more accurate geometric features. Experimental results demonstrate that our method effectively mitigates distortion and achieves state-of-the-art performance on various benchmark datasets for 360 monocular depth estimation. The code for our method is available at https://github.com/yuyanli0831/OmniFusion.