Semi-supervised learning (SSL) has shown promising results, but it relies on the assumption that labeled and unlabeled data have the same class distribution, which is often not true in real-world scenarios. This mismatch can lead to biased pseudo-labels and performance degradation. To address this, we propose a new SSL framework called Distribution Consistency SSL (DC-SSL) that rectifies pseudo-labels from a distribution perspective. The key idea is to estimate a reference class distribution (RCD) that serves as a surrogate for the true class distribution of unlabeled data, and then improve the pseudo-labels by gradually aligning the predicted class distribution (PCD) with the RCD. We achieve this by revisiting the Exponentially Moving Average (EMA) model and using it to iteratively improve the estimation of RCD through a momentum-update scheme during training. Additionally, we propose two strategies for rectifying pseudo-label predictions based on RCD, including a training-free scheme and a training-based alternative that generates more accurate and reliable predictions. We evaluate DC-SSL on multiple SSL benchmarks and show significant performance improvement compared to other methods in both matched- and mismatched-distribution scenarios.