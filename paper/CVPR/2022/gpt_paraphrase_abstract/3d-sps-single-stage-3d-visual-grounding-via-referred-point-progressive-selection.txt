The goal of 3D visual grounding is to locate a specific object in a 3D point cloud scene based on a language description. Previous approaches have used a two-stage process, involving language-agnostic object detection and cross-modal matching. However, this approach has limitations due to its isolated architecture. The detector must sample keypoints from the point clouds to generate object proposals, but sparse proposals may miss the target and dense proposals may confuse the matching model. Additionally, the language-agnostic detection stage may only sample a small proportion of keypoints, leading to poor target prediction.  To address these limitations, we propose a method called 3D Single-Stage Referred Point Progressive Selection (3D-SPS). This method progressively selects keypoints guided by language information to directly locate the target object. We introduce a Description-aware Keypoint Sampling (DKS) module to focus on points relevant to the language description, which are important clues for grounding. Furthermore, we develop a Target-oriented Progressive Mining (TPM) module to concentrate on the points of the target, utilizing progressive modeling of intra-modal relations and inter-modal target mining. By bridging the gap between detection and matching, 3D-SPS achieves target localization in a single stage.  Experimental results on the ScanRefer and Nr3D/Sr3D datasets demonstrate that 3D-SPS achieves state-of-the-art performance.