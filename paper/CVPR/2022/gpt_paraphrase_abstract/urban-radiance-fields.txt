This study aims to achieve 3D reconstruction and novel view synthesis using data obtained from scanning platforms commonly used for mapping urban outdoor environments, such as Street View. By analyzing a sequence of RGB images and lidar sweeps captured by cameras and scanners moving through the scene, we develop a model that allows us to extract 3D surfaces and generate new RGB images. To enhance the existing Neural Radiance Fields approach, we introduce new techniques for utilizing asynchronously captured lidar data, addressing exposure variations in captured images, and using predicted image segmentations to supervise densities on skyward-facing rays. Through experiments on Street View data, we demonstrate that each of these three extensions significantly improves performance. Our system produces state-of-the-art 3D surface reconstructions and generates higher quality novel views compared to traditional methods like COLMAP and recent neural representations like Mip-NeRF.