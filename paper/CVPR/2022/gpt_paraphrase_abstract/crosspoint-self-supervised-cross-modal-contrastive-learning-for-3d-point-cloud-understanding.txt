Manual annotation of large-scale point cloud datasets for tasks such as 3D object classification, segmentation, and detection is often time-consuming due to the irregular structure of point clouds. To address this challenge, self-supervised learning, which does not require human labeling, shows promise. In the real world, humans are capable of transferring visual concepts learned from 2D images to understand the 3D world. Building upon this observation, we propose CrossPoint, a straightforward cross-modal contrastive learning approach that learns transferable 3D point cloud representations. Our method establishes a correspondence between 3D and 2D modalities by maximizing agreement between point clouds and the corresponding rendered 2D images in an invariant space, while also encouraging invariance to transformations in the point cloud modality. By jointly training on both modalities, our approach generates a rich learning signal for self-supervised learning. Experimental results demonstrate that our approach outperforms previous unsupervised learning methods in various downstream tasks, including 3D object classification and segmentation. Ablation studies confirm the effectiveness of our approach in improving point cloud understanding. Code and pretrained models are available at the following link: https://github.com/MohamedAfham/CrossPoint.