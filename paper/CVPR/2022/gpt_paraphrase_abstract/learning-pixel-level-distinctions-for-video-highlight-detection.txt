Video highlight detection aims to select the most engaging parts of a long video. While existing methods focus on modeling relationships between video segments to assign highlight scores, they overlook the contextual dependency within individual segments. To address this, we propose learning pixel-level distinctions to enhance video highlight detection. This approach determines whether each pixel in a video belongs to an interesting section. By modeling fine-level distinctions, we can exploit the temporal and spatial relations within the video's content. Additionally, learning pixel-level distinctions provides insight into what content within a highlight segment is appealing to viewers. Our proposed framework utilizes an encoder-decoder network, leveraging 3D convolutional neural networks to capture temporal context and incorporating visual saliency for spatial distinction. Our framework demonstrates superior performance on three public benchmarks, validating its effectiveness in video highlight detection.