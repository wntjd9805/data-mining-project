This study explores the task of One-Shot image Generation (OSG), which involves generating images of novel categories using a generation network trained on a base dataset with only one sample per category. Existing methods for feature transfer in one-shot image generation typically learn reusable features implicitly through pre-training tasks. However, these methods are prone to overfitting. In this paper, we propose a novel model that explicitly learns and memorizes reusable features to assist in synthesizing images of novel categories. Our algorithm decomposes image features into Category-Related (CR) and Category-Independent (CI) features. The model learns to memorize class-independent CI features, which are then used by our feature hallucination component to generate images of the target novel categories. Our model is validated on various benchmarks, and extensive experiments demonstrate its effectiveness in improving OSG performance and generating diverse and compelling samples.