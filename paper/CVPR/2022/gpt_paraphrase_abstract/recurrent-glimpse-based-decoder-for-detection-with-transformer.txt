Although the popular detection model Transformer (DETR) requires a long training period to optimize and achieve good performance, we propose a solution using Region-of-Interest (RoI) based detection refinement. We introduce a novel approach called REcurrentGlimpse-based decOder (REGO), which employs a multi-stage recurrent processing structure to improve the accuracy of DETR's attention on foreground objects. REGO extracts glimpse features from RoIs with enlarged bounding box areas and uses a glimpse-based decoder to refine the detection results. REGO can be easily integrated into existing DETR variants without disrupting the training and inference pipelines. Our experiments demonstrate that REGO significantly improves the performance of DETR models, reducing the required training epochs while achieving comparable results. For example, REGO helps Deformable DETR achieve 44.8 AP on the MSCOCO dataset with only 36 training epochs, compared to 500 and 50 epochs required by the original DETR and Deformable DETR, respectively. Additionally, REGO consistently boosts the performance of different DETR detectors by up to 7% relative gain at the same training epoch setting. The code for REGO is available at https://github.com/zhechen/Deformable-DETR-REGO.