Current self-supervised pretraining techniques for object detection mainly focus on pretraining the backbone of the object detector, neglecting crucial aspects of the detection architecture. In contrast, we propose a novel self-supervised method called DETReg, which pretrains the entire object detection network, including the object localization and embedding components. During the pretraining process, DETReg predicts object localizations to match those generated by an unsupervised region proposal generator. Additionally, it aligns the feature embeddings with embeddings from a self-supervised image encoder. We implement DETReg using the DETR family of detectors and demonstrate its superiority compared to competitive baselines when fine-tuned on popular benchmarks such as COCO, PASCAL VOC, and Airbus Ship. In scenarios with limited data, including semi-supervised and few-shot learning settings, DETReg achieves many state-of-the-art results. For instance, on COCO, we observe a significant +6.0 AP improvement for 10-shot detection and over 2 AP improvements when training with just 1% of the labels.