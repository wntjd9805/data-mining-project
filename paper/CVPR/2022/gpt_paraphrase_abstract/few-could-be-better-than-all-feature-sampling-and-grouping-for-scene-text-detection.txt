Transformer-based methods have shown promising advancements in object detection by eliminating post-processing steps like non-maximum suppression (NMS) and enhancing deep representations. However, these methods struggle with scene text detection due to the wide range of scales and aspect ratios. This paper introduces a straightforward yet effective transformer-based architecture for scene text detection. Unlike previous approaches that learn holistic representations of scene text, our method focuses on a few representative features, minimizing background interference and reducing computational costs. Initially, we select representative features at all scales that are closely associated with foreground text. Then, we employ a transformer to model the relationships between these sampled features, effectively grouping them. Each feature group corresponds to a text instance, allowing for easy bounding box extraction without the need for post-processing. By utilizing a basic feature pyramid network for feature extraction, our approach consistently achieves top results on various popular datasets for scene text detection.