Generalized zero-shot learning (GZSL) aims to identify samples from categories that may not have been encountered during training. However, treating unseen classes as seen ones or vice versa often leads to poor performance in GZSL. Therefore, a challenging yet effective solution for GZSL is to distinguish between seen and unseen domains. In this paper, we propose a novel approach that utilizes both visual and semantic modalities to differentiate between seen and unseen categories. Our method involves the use of two variational autoencoders to generate latent representations for the visual and semantic modalities in a shared latent space. We align the latent representations of both modalities using the Wasserstein distance and reconstruct the two modalities using each other's representations. To learn a clearer boundary between seen and unseen classes, we introduce a two-stage training strategy that leverages the semantic descriptions of both seen and unseen categories and determines a threshold to separate visual samples from seen and unseen categories. Finally, we employ a seen expert and an unseen expert for the final classification. Extensive experiments on five widely used benchmarks demonstrate that our proposed method significantly improves GZSL results. For instance, when separating domains, our method correctly recognizes over 99% of the samples and improves the final classification accuracy from 72.6% to 82.9% on AWA1.