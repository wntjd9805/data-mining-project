Learning discriminative representation from the complex spatio-temporal dynamic space is crucial for video recognition. Previous methods have focused on using a single type of context to calibrate feature channels, which limits their applicability to diverse video activities. To address this problem, we propose a lightweight feature refinement method called group contextualization (GC). GC decomposes feature channels into groups and separately refines them with different axial contexts in parallel. We introduce a family of efficient element-wise calibrators, ECal-G/S/T/L, which aggregate information dynamics from other axes to contextualize feature channel groups. The GC module can be easily integrated into existing video networks, resulting in consistent performance improvements across different networks. By embedding feature channels with four different types of contexts in parallel, our approach enhances the resilience of the learnt representation to diverse activity types. Empirical results show that GC significantly improves the performance of 2D-CNN models (such as TSN and TSM) on videos with rich temporal variations, achieving performance comparable to state-of-the-art video networks. The code for our method is available at https://github.com/haoyanbin918/Group-Contextualization.