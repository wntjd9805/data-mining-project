This paper presents a new and practical mechanism for verifying if a suspect model has been stolen from a victim model through model extraction attacks. The authors propose using Universal Adversarial Perturbations (UAPs) to characterize the decision boundary of a deep neural network (DNN) model. They observe that piracy models' subspaces are more similar to the victim model's subspace compared to non-piracy models. Based on this insight, the authors introduce a UAP fingerprinting method for DNN models. They train an encoder using contrastive learning, which takes fingerprints as inputs and outputs a similarity score. Extensive studies demonstrate that this framework can detect model intellectual property breaches with a confidence level of over 99.99% using only 20 fingerprints of the suspect model. The method also exhibits good generalizability across different model architectures and remains robust against post-modifications made to stolen models.