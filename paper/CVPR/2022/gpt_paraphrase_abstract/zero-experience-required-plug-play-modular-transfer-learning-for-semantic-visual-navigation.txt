In visual navigation reinforcement learning, it is common to create and train a separate model for each new task, requiring extensive task-specific interactions in 3D environments. However, this approach is costly and time-consuming, as a large number of interactions are necessary for the model to perform well in different tasks. Additionally, this process must be repeated whenever there is a change in task type or goal modality. To address these issues, we propose a unified approach to visual navigation using a novel modular transfer learning model. Our model effectively leverages experience from a single source task and applies it to multiple target tasks, such as ObjectNav, Room-Nav, and ViewNav, with different goal modalities including images, sketches, audio, and labels. Furthermore, our model enables zero-shot experience learning, allowing it to solve target tasks without any task-specific interactive training. We conducted experiments using various photorealistic datasets and challenging tasks, and our results demonstrate that our approach learns more quickly, generalizes better, and outperforms state-of-the-art models by a significant margin. For more information, please visit our project page at https://vision.cs.utexas.edu/projects/zsel/.