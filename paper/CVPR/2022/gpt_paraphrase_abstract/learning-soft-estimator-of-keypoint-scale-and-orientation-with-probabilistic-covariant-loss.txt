This paper presents a solution to the challenge of estimating keypoint scale and orientation in order to extract invariant features in the presence of significant geometric changes. Existing self-supervised learning-based estimators typically predict a single scalar for scale or orientation, known as hard estimators. However, hard estimators struggle with local patches that contain structures from different objects or multiple edges. To address this issue, the authors propose a Soft Self-Supervised Estimator (S3Esti) that learns to predict multiple scales and orientations.S3Esti incorporates three key factors. First, the estimator is designed to predict discrete distributions of scales and orientations. The elements with high confidence in these distributions are selected as the final scales and orientations. Second, a probabilistic covariant loss function is introduced to enhance the consistency of the scale and orientation distributions across different transformations. Third, an optimization algorithm is developed to minimize the loss function, and its convergence is theoretically proven.When combined with various keypoint extraction models, S3Esti demonstrates significant improvements in image matching tasks under substantial viewpoint changes, with an average accuracy increase of over 50%. In 3D reconstruction tasks, S3Esti reduces the reprojection error by more than 10% and enhances the number of registered images. The authors have also released the code for S3Esti.In summary, this paper introduces S3Esti, a Soft Self-Supervised Estimator that addresses the limitations of hard estimators by predicting multiple scales and orientations. The proposed method shows promising results in image matching and 3D reconstruction tasks, offering improvements in accuracy and reprojection error.