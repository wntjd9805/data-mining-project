Surface reconstruction from point clouds is an important task in 3D computer vision. Current methods use large datasets to learn local context priors represented as neural network-based signed distance functions (SDFs), with parameters encoding the local contexts. These methods match the local reconstruction target by searching for the best match in the local prior space at the given query location. However, this approach struggles to generalize to unseen target regions. To address this issue, we propose Predictive Context Priors, which involve learning Predictive Queries for each specific point cloud at inference time. We initially train a local context prior using a large point cloud dataset. During inference, we specialize the local context prior into our Predictive Context Prior by learning Predictive Queries, which predict adjusted spatial query locations as displacements of the original locations. This allows us to search the learned local context prior over the entire prior space, improving generalizability. Our method does not require ground truth signed distances, normals, or additional procedures for signed distance fusion across overlapping regions. Experimental results on surface reconstruction for single shapes and complex scenes demonstrate significant improvements over the state-of-the-art methods according to widely used benchmarks.