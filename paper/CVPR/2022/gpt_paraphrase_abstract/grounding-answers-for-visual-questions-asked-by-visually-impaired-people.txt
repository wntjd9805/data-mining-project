Visual question answering involves answering questions about images. The VizWiz-VQA-Grounding dataset is introduced as the first dataset that grounds answers to visual questions asked by individuals with visual impairments. A comparison is made between this dataset and five other VQA-Grounding datasets, highlighting both similarities and differences. The performance of state-of-the-art (SOTA) VQA and VQA-Grounding models is evaluated, revealing that these algorithms often struggle to identify the correct visual evidence for the answer. Particularly challenging scenarios include when the visual evidence is only a small part of the image, when the image is of higher quality, and when the question requires text recognition skills. The dataset, evaluation server, and leaderboard can be accessed at the provided link. The answer format exclusively includes the abstraction.