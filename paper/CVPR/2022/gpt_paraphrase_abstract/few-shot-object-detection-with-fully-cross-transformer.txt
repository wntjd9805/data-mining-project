In recent years, there has been a growing interest in few-shot object detection (FSOD), which aims to detect new objects with limited training examples. Previous approaches have used metric-learning based methods with a two-branch siamese network to calculate similarity between image regions and few-shot examples for detection. However, these methods only focused on the detection head, neglecting the potential benefits of interaction between the branches in the feature extraction process.Building on recent advancements in vision transformers and vision-language transformers, we propose a new model called Fully Cross-Transformer (FCT) for FSOD. Our model incorporates cross-transformer into both the feature backbone and detection head. We introduce asymmetric-batched cross-attention to aggregate key information from the two branches with different batch sizes. This enables multi-level interactions and improves the few-shot similarity learning between the branches.To evaluate the effectiveness of our model, we conducted comprehensive experiments on both PAS-CAL VOC and MSCOCO FSOD benchmarks. The results demonstrate that our model outperforms existing approaches in detecting novel objects using limited training examples.