Vision Transformers (ViTs) have been successful in capturing image representations but have mainly been studied for low-resolution images. However, in computational pathology, whole-slide images (WSIs) can be extremely large and exhibit a hierarchical structure of visual tokens across varying resolutions. To address this, we propose a new ViT architecture called the Hierarchical Image Pyramid Transformer (HIPT). HIPT leverages the hierarchical structure inherent in WSIs by using two levels of self-supervised learning to learn high-resolution image representations. We pretrained HIPT on a large dataset of gigapixel WSIs and benchmarked its performance on various slide-level tasks. Our results show that HIPT with hierarchical pretraining outperforms current state-of-the-art methods for cancer subtyping and survival prediction. Additionally, we demonstrate that self-supervised ViTs can effectively model the hierarchical structure of phenotypes in the tumor microenvironment.