This study investigates whether facial geometry can be inferred from a person's voice. Previous research on this topic has focused on converting voices into face images using image synthesis techniques, which involves predicting attributes that voices cannot convey, such as facial textures and hairstyles. Instead, this study focuses solely on reconstructing 3D faces to analyze the geometric aspects, which are more closely related to human physiology. The authors propose a framework called Cross-Modal Perceptionist, which includes both supervised and unsupervised learning approaches. They create a dataset called Voxceleb-3D that contains paired voices and face meshes, enabling supervised learning. Additionally, they employ a knowledge distillation mechanism to explore the ability to infer face geometry from voices without paired voice and 3D face data, when 3D face scans are limited. The core question is divided into four parts, and the authors conduct visual and numerical analyses to address these components. The findings of this study align with existing knowledge from physiology and neuroscience regarding the correlation between voices and facial structures. The work contributes to the development of explainable cross-modal learning methods in the context of human perception. For more information, please refer to the project page.