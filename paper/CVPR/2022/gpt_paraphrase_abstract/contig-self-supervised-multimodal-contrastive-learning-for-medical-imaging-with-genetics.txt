The high costs associated with annotating medical data hinder the application of advanced deep learning techniques in clinically relevant scenarios. To address this issue, we propose ContIG, a self-supervised learning method that can effectively learn from large sets of unlabeled medical images and genetic data. Our approach aligns these different modalities in the feature space using a contrastive loss. Importantly, our method is designed to handle varying modalities across individuals by integrating multiple modalities within the same model. We demonstrate that our approach outperforms existing self-supervised methods on various downstream benchmark tasks. Additionally, we leverage gradient-based explainability algorithms to gain insights into the learned associations between images and genetic modalities. Finally, we conduct genome-wide association studies on the features learned by our models, revealing interesting relationships between images and genetic data.