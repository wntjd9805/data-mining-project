What factors differentiate video tasks from single-image tasks? This question is explored in the context of video and language tasks, building upon recent advancements in self-supervised image-language models. The atemporal probe (ATP) is introduced as a novel model for video-language analysis, offering a stronger measure of accuracy for multimodal models that rely on image-level comprehension. Through the application of ATP to standard video and language tasks such as video question answering and text-to-video retrieval, the limitations and potential of existing video-language benchmarks are examined. Surprisingly, it is found that understanding temporal events is often unnecessary for achieving strong performance, even when compared to state-of-the-art video-language models designed to assess deeper video-level comprehension. The ATP model also proves beneficial in enhancing both video-language dataset and model design. A technique is proposed for utilizing ATP to better distinguish subsets of the dataset that contain more temporally challenging data, thereby improving the effectiveness of benchmarking for causal and temporal comprehension. Additionally, the integration of ATP into full video-level temporal models is shown to enhance efficiency and achieve state-of-the-art accuracy.