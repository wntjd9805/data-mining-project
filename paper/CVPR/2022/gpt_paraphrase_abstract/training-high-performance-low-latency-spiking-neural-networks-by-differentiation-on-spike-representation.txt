The use of Spiking Neural Networks (SNNs) on neuromorphic hardware has shown promise in achieving energy-efficient AI models. However, efficiently training SNNs has been a challenge due to their non-differentiability. Existing methods either have high latency or do not perform as well as Artificial Neural Networks (ANNs). To address this, we propose the Differentiation on Spike Representation (DSR) method, which achieves high performance comparable to ANNs while maintaining low latency. The DSR method encodes spike trains into spike representation using firing rate coding. By viewing the spiking dynamics as sub-differentiable mappings, we leverage gradients to train SNNs and overcome the non-differentiability issue. We also analyze the error in representing the mapping and propose training the spike threshold in each layer and introducing a new hyperparameter for the neural models to reduce this error. The DSR method achieves state-of-the-art SNN performance with low latency on various datasets such as CIFAR-10, CIFAR-100, ImageNet, and DVS-CIFAR10.