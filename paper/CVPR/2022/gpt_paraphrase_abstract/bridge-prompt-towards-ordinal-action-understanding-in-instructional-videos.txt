Action recognition models have shown promise in accurately classifying human actions in short video clips. However, these models often fail to consider the contextual relationships between adjacent actions, which are crucial for understanding longer videos. In this study, we propose a framework called Bridge-Prompt (Br-Prompt) that addresses this limitation by modeling the semantics across adjacent actions. Our approach involves transforming individual action labels into text prompts that bridge the gap between individual action semantics. These text prompts are then used to supervise the training of both the text encoder and the video encoder using a contrastive approach. By integrating out-of-context and contextual information from a series of ordinal actions in instructional videos, our framework enhances the vision encoder's ability to perform downstream tasks such as action segmentation and human activity recognition. We evaluate the performance of our approach on three video datasets (Georgia Tech Egocentric Activities, 50Salads, and the Breakfast dataset) and achieve state-of-the-art results on multiple benchmarks. The code for our framework is available at: https://github.com/ttlmh/Bridge-Prompt.