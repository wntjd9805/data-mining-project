Transfer learning is a well-known method where pre-trained models on large datasets are adapted to perform well on specialized datasets. Typically, more accurate models on the large dataset also achieve better results on the specialized dataset. This study focuses on convolutional neural networks (CNNs) trained on the ImageNet dataset, which have been pruned by compressing their connections. The researchers investigate the transfer performance of these pruned models using various pruning methods, including magnitude-based, second-order, re-growth, lottery-ticket, and regularization approaches, across twelve standard transfer tasks. Surprisingly, the study reveals that sparse models can achieve comparable or even better transfer performance than dense models, even at high sparsities. Additionally, using sparse models can significantly speed up both inference and training processes. Furthermore, the researchers observe and analyze significant differences in the behavior of different pruning methods. The code for this study is available at the provided GitHub link.