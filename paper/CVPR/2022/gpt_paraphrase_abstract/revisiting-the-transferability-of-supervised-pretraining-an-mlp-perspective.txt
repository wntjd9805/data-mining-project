This study examines the transferability gap between unsupervised and supervised pre-training methods in visual learning. While previous research has focused on the effectiveness of multilayer perceptron (MLP) in unsupervised image classification on the same dataset, this paper highlights the importance of the MLP projector in achieving better transfer performance for unsupervised pre-training. By adding an MLP projector before the classifier in supervised pre-training, the study aims to narrow the transferability gap. The analysis reveals that the MLP projector helps preserve intra-class variation, reduce feature distribution distance, and minimize feature redundancy. Extensive experiments on public benchmarks demonstrate that the inclusion of an MLP projector significantly improves the transferability of supervised pre-training, resulting in comparable or even superior performance to unsupervised pre-training.