This paper introduces PETR, a novel end-to-end multi-person pose estimation framework that utilizes transformers. Unlike current methods that separate body joint localization and association, PETR treats pose estimation as a hierarchical set prediction problem. It eliminates the need for hand-crafted modules such as RoI cropping, NMS, and grouping post-processing. PETR learns multiple pose queries to directly reason a set of full-body poses, and then utilizes a joint decoder to refine the poses by exploring the kinematic relations between body joints. The attention mechanism in PETR allows it to adaptively focus on the most relevant features for target keypoints, overcoming the challenge of feature misalignment in pose estimation and significantly improving performance. Extensive experiments on MS COCO and CrowdPose benchmarks demonstrate that PETR outperforms state-of-the-art approaches in terms of both accuracy and efficiency. The code and models for PETR are available at https://github.com/hikvision-research/opera.