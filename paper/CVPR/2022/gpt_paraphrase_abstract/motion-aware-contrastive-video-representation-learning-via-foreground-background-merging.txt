Current self-supervised video representation learning methods often use contrastive loss to improve video representation learning. However, a common issue known as background bias arises when the model focuses too much on the static background and fails to capture motion information. This bias leads to weaker generalization ability and poorer performance on action recognition tasks. To address this bias, we propose a method called Foreground-background Merging (FAME). FAME deliberately combines the moving foreground region of one video with the static background of others. We achieve this without using any pre-trained detector by extracting the moving foreground using frame difference and color statistics, and shuffling the background regions among the videos. By ensuring semantic consistency between the original clips and the fused ones, the model can focus more on motion patterns and reduce the bias towards the background shortcut. Extensive experiments demonstrate that FAME effectively mitigates background cheating and achieves state-of-the-art performance on UCF101, HMDB51, and Diving48 datasets for downstream tasks. The code and configurations for FAME are available at https://github.com/Mark12Ding/FAME.