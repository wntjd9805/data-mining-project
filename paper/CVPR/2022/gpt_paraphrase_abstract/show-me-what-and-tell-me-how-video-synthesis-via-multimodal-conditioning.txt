This study addresses the limitations of using a single modality in conditional video synthesis methods. It highlights the challenges of generating specific motion trajectories based on image conditioning alone, as it lacks motion information. On the other hand, language information can describe desired motion but does not precisely define video content. To overcome these limitations, the researchers propose a multimodal video generation framework that incorporates both text and images as joint or separate inputs. They utilize quantized representations for videos and employ a bidirectional transformer with multiple modalities to predict a discrete video representation. The framework introduces a new video token trained with self-learning and an improved mask-prediction algorithm for sampling video tokens, thereby enhancing video quality and consistency. Text augmentation is also introduced to improve the robustness of textual representation and diversity of generated videos. The framework can incorporate various visual modalities such as segmentation masks, drawings, and partially occluded images, and is capable of generating longer sequences than those used for training. Moreover, the model can extract visual information based on text prompts and generate corresponding videos. The researchers evaluate their approach on three public datasets and a newly collected dataset labeled with facial attributes, achieving state-of-the-art generation results across all four datasets.