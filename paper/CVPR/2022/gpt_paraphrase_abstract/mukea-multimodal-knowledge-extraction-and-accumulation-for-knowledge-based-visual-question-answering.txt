Knowledge-based visual question answering (VQA) requires the ability to connect external knowledge with visual scenes for comprehensive understanding. However, existing solutions have limitations in capturing relevant knowledge as they rely solely on text-based knowledge bases that lack complex multimodal information necessary for visual comprehension. The construction of vision-relevant and explainable multimodal knowledge for VQA has received little attention. To address this gap, our proposed model, MuKEA, represents multimodal knowledge using an explicit triplet that correlates visual objects and factual answers through implicit relationships. To bridge the gap between different data types, we introduce three objective losses to learn triplet representations from various perspectives, including embedding structure, topological relations, and semantic space. Through a pre-training and fine-tuning learning strategy, our model progressively accumulates both generic and domain-specific multimodal knowledge to improve answer prediction. Our approach outperforms the current state-of-the-art methods by 3.35% and 6.08% on two challenging datasets, OK-VQA and KRVQA, which require extensive knowledge. Experimental results demonstrate the complementary benefits of combining multimodal knowledge with existing knowledge bases and highlight the superiority of our end-to-end framework compared to existing pipeline methods. The code for our model is available at https://github.com/AndersonStra/MuKEA.