The rise of vision-language navigation (VLN) has led to significant progress in instruction following, where a follower navigates environments based on instructions. However, less attention has been given to the task of instruction generation, where a speaker learns to generate descriptions for navigation routes. Current VLN methods typically train the speaker independently and use it as a data augmentation tool for the follower, without considering the interplay between the two tasks.To address this, we propose a simultaneous learning approach that leverages the inherent correlations between instruction following and generation. In our approach, the follower evaluates whether the instruction generated by the speaker accurately explains the original navigation route, and vice versa. This cycle-consistent learning scheme does not require aligned instruction-path pairs and can also be applied to unlabeled paths, which are sampled without paired instructions. Additionally, we introduce a new agent called the creator, which generates counterfactual environments by significantly altering current scenes while preserving novel items crucial for executing the original instructions. This allows us to synthesize more informative training scenes, resulting in a powerful VLN learning system. Through extensive experiments on a standard benchmark, we demonstrate that our approach enhances the performance of various follower models and produces precise navigation instructions.