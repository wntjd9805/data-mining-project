We present a novel end-to-end learning-based solution for near-field Photometric Stereo (PS), specifically designed for reconstructing large immobile objects. Our method is efficient, generating a mesh from high-resolution images in just one second on a regular GPU. This speed opens up possibilities for various AR/VR applications. Existing approaches rely on optimization combined with a far-field PS network operating on pixels or small patches. However, these methods are slow, memory-intensive, and prone to noise and calibration errors. To overcome these limitations, we propose a recursive multi-resolution approach to estimate surface normal and depth maps for the entire image at each step. The predicted depth map from each scale is then used to calculate per-pixel lighting for the next scale. This design significantly improves the speed and accuracy of our approach compared to the state-of-the-art near-field PS reconstruction technique, which relies on iterative optimization.