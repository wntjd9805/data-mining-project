We introduce a method called Class-aware Contrastive Semi-Supervised Learning (CCSSL) to address the limitations of pseudo-label-based semi-supervised learning (SSL). While SSL has been successful in utilizing raw data, its training procedure is prone to confirmation bias due to noise in self-generated artificial labels. Additionally, the model's judgment becomes less reliable when applied to real-world scenarios with out-of-distribution data. CCSSL serves as a drop-in helper to enhance the quality of pseudo-labels and improve the model's robustness in real-world settings.  Unlike traditional approaches that treat real-world data as a single set, our method separately handles two types of data: reliable in-distribution data and noisy out-of-distribution data. For the former, we employ class-wise clustering to blend the data into downstream tasks. For the latter, we use image-wise contrastive techniques to improve generalization. Additionally, we apply target re-weighting to emphasize clean label learning while reducing the impact of noisy labels.  Despite its simplicity, CCSSL outperforms state-of-the-art SSL methods on standard datasets such as CIFAR100 and STL10. On the real-world dataset Semi-iNat 2021, our method improves FixMatch by 9.80% and CoMatch by 3.18%. The code for CCSSL is available at https://github.com/TencentYoutuResearch/Classification-SemiCLS.