Temporal modeling is an essential aspect of video super-resolution. Many existing methods use optical flow or deformable convolution to compensate for motion explicitly. However, these techniques can increase model complexity and may fail when dealing with occlusion or complex motion, leading to distortions and artifacts. In this study, we propose a new approach that focuses on explicit temporal difference modeling in both low-resolution (LR) and high-resolution (HR) spaces. Instead of directly inputting consecutive frames into a video super-resolution (VSR) model, we calculate the temporal difference between frames and divide the pixels into two subsets based on the level of difference. These subsets are then processed separately using two branches with different receptive fields to extract complementary information more effectively. Additionally, we not only extract spatial residual features but also compute the difference between consecutive frames in the high-frequency domain to enhance the super-resolution outcome. This allows the model to utilize intermediate SR results from both past and future frames to refine the current SR output. By caching the differences at different time steps, information from frames further in the past can be propagated to the current frame for further improvement. Our experiments on various video super-resolution benchmark datasets demonstrate the effectiveness of our proposed method, which outperforms state-of-the-art methods in terms of performance.