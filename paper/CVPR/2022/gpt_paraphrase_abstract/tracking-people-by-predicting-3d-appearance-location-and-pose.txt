We propose a method for tracking individuals in videos using monocular vision and predicting their future 3D representations. Our approach involves lifting people to 3D from a single frame, capturing information about their 3D pose, location, and appearance. As we track individuals, we gather 3D observations over time and create temporal models for each attribute. These models enable us to predict the future state of the tracklet, including 3D appearance, location, and pose. To associate tracklets with future frames, we compute the similarity between predicted states and single frame observations using a probabilistic approach. We solve the association problem using Hungarian matching and update the corresponding tracklets accordingly. Our approach achieves state-of-the-art results on various benchmarks. Code and models can be found at: https://brjathu.github.io/PHALP.