In this study, we tackle the difficulties of annotating large-scale datasets for supervised video shadow detection methods. Directly applying a model trained on labeled images to video frames can result in high generalization error and temporal inconsistency. To address these challenges, we propose a framework called Spatio-Temporal Interpolation Consistency Training (STICT). This framework involves incorporating unlabeled video frames along with labeled images into the training process of an image shadow detection network.To improve generalization and encourage temporal consistency in the pixel-wise classification task, we introduce two new interpolation schemes: spatial interpolation and temporal interpolation. We then establish consistency constraints for both spatial and temporal interpolation. Additionally, we develop a Scale-Aware Network (SANet) to leverage multi-scale shadow knowledge in images. To minimize discrepancies among predictions at different scales, we introduce a scale-consistency constraint.To validate our approach, we conduct extensive experiments on the ViSha dataset and a self-annotated dataset. The results demonstrate that our approach outperforms most existing supervised, semi-supervised, and unsupervised image/video shadow detection methods, as well as other methods in related tasks. Importantly, our approach achieves superior performance even without video labels.The code and dataset used in this study are available at https://github.com/yihong-97/STICT. Figure 1 showcases the shadow maps produced by our image shadow detection network, SANet, when trained solely on labeled images and when trained on both labeled images and unlabeled videos using the STICT framework.Overall, our proposed STICT framework, along with the SANet model, offers a promising solution for addressing the challenges of large-scale dataset annotation and improving video shadow detection performance.