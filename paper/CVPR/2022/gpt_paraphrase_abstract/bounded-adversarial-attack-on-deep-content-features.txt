We propose a new method of adversarial attack that targets individual neurons in a deep layer by manipulating their activation values. Previous attempts to enforce a fixed value or percentage bound on neuron activation have been unsuccessful and have resulted in noisy samples. This is because the perceptual variation caused by a fixed value bound differs across neurons and even within the same neuron. To address this issue, we introduce a novel approach that uses a distribution quantile bound and a polynomial barrier loss function. With this method, a fixed quantile bound is translated into multiple value bounds for each neuron, based on the distributions of their activations and the current activation value of the given input. This allows for precise regulation of content feature mutations while controlling perceptual variations. Our evaluation on ImageNet and multiple model architectures demonstrates the effectiveness of our attack. Compared to seven other recent adversarial attacks in both the pixel space and feature space, our attack achieves the best trade-off between attack success rate and imperceptibility.