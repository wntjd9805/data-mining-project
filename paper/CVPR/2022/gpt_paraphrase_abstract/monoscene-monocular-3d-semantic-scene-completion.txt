MonoScene presents a novel framework called 3D Semantic Scene Completion (SSC) that can infer the dense geometry and semantics of a scene from a single monocular RGB image. In contrast to existing SSC approaches that rely on 2.5 or 3D input, our framework tackles the challenging task of reconstructing a 3D scene from a 2D image while simultaneously inferring its semantics. Our approach utilizes a combination of 2D and 3D UNets, connected by a unique 2D-3D feature projection inspired by optics. Additionally, we introduce a 3D context relation prior to enforce spatio-semantic consistency. To further improve our framework, we propose global scene and local frustums losses. Experimental results demonstrate that our approach surpasses existing methods on various metrics and datasets, and it is capable of generating realistic scenes even beyond the camera's field of view. The code and trained models for our framework are publicly available at https://github.com/cv-rits/MonoScene.