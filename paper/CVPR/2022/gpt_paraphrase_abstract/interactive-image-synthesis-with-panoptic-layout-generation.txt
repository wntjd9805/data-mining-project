Generating interactive images that accurately reflect user-guided input is a difficult task, especially when users want to have control over the structure of the scene. While layout-based image synthesis methods have made significant progress, they often require precise inputs like accurately placed bounding boxes. However, in interactive settings, these bounding boxes may be constantly changing, resulting in missing regions in the constructed semantic layouts and undesirable artifacts in the generated images. To address this challenge, we propose the Panoptic Layout Generative Adversarial Network (PLGAN). The PLGAN utilizes panoptic theory, which distinguishes between "stuff" (with amorphous boundaries) and "things" (with well-defined shapes) in order to construct separate stuff and instance layouts that are later fused into panoptic layouts. The stuff layouts can fill in the missing regions left out by the instance layouts. We compare our PLGAN with state-of-the-art layout-based models on various datasets and demonstrate its advantages both visually and quantitatively using metrics such as inception score, Fr√©chet inception distance, classification accuracy score, and coverage. The code for our PLGAN is available at https://github.com/wb-finalking/PLGAN.