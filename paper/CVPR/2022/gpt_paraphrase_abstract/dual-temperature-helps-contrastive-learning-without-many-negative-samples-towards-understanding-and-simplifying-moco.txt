Contrastive learning methods, such as MoCo, typically require a large number of negative samples. However, dictionary-free frameworks struggle with limited negative sample sizes due to their mini-batch size. To address this issue, many CL frameworks, including MoCo, have adopted a dynamic dictionary. In this study, we analyze the size and consistency of MoCo's momentum-based queue dictionary. We discover that the InfoNCE loss used in MoCo implicitly attracts anchors to their corresponding positive samples with varying penalties, which necessitates a large dictionary. Based on our findings, we propose simplified frameworks, Sim-MoCo and SimCo, that remove the dictionary and momentum from MoCo. These simplified frameworks outperform MoCo v2 significantly when utilizing InfoNCE with a dual temperature. Additionally, our work helps bridge the gap between contrastive learning and non-contrastive learning frameworks, contributing to a more unified understanding of these two mainstream frameworks in self-supervised learning. The code for our frameworks is available at: https://bit.ly/3LkQbaT.