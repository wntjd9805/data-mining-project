Exemplar-based image translation involves creating realistic translations between two different domains by establishing dense correspondences between a conditional input and an exemplar. Previous methods have implicitly built cross-domain correspondences by minimizing feature-wise distances, but this approach may not effectively reduce the domain gap, resulting in sub-optimal translations. To address this, we propose the Marginal Contrastive Learning Network (MCL-Net), which uses contrastive learning to learn domain-invariant features for realistic image translation. We introduce a novel marginal contrastive loss that explicitly guides the establishment of dense correspondences. However, relying solely on domain-invariant semantics for correspondence building may negatively impact texture patterns and degrade texture generation. To overcome this, we introduce the Self-Correlation Map (SCM), which incorporates scene structures as auxiliary information to significantly improve correspondences. Our method consistently outperforms state-of-the-art techniques in both quantitative and qualitative experiments on various image translation tasks.