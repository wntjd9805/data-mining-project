This paper presents a new approach to visual place recognition called TransVPR, which is based on vision Transformers. Visual place recognition is a difficult task in applications like autonomous driving and mobile robot localization due to distracting elements in complex scenes. To address this problem, TransVPR integrates information from task-relevant regions into image representations. The self-attention operation in Transformers is utilized to naturally aggregate task-relevant features. Multiple levels of the Transformer generate attentions that focus on different regions of interest, which are combined to create a global image representation. Additionally, key-patch descriptors are generated from the filtered output tokens of the Transformer layers using a fused attention mask. These descriptors are then used for spatial matching to re-rank the candidates retrieved by the global image features. The entire model can be trained end-to-end with a single objective and image-level supervision. TransVPR achieves state-of-the-art performance on various real-world benchmarks while maintaining low computational time and storage requirements.