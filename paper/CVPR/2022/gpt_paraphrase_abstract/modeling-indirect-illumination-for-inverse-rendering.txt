Recent advancements in implicit neural representations and differentiable rendering have made it possible to simultaneously determine the geometry and materials of an object using multi-view RGB images captured under unknown static lighting conditions. However, previous methods have often overlooked the modeling of indirect illumination due to the computational complexity associated with expensive recursive path tracing. In this paper, we propose a novel approach that efficiently recovers spatially-varying indirect illumination. Our key insight is that the neural radiance field learned from input images can be used to derive indirect illumination, instead of estimating it jointly with direct illumination and materials. By accurately modeling indirect illumination and the visibility of direct illumination, we can successfully recover albedo that is free from interreflection and shadows. Our experimental results, using both synthetic and real data, demonstrate the superior performance of our approach compared to previous methods. Additionally, our approach has the capability to synthesize realistic renderings from novel viewpoints and illumination conditions. The code and data for our approach are publicly available at https://zju3dv.github.io/invrender/.