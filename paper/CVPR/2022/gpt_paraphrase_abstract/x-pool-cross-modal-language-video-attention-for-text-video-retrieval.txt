The objective of text-video retrieval is to develop a similarity function that ranks relevant text-video pairs higher than irrelevant pairs. However, videos contain a broader range of information compared to texts, which typically capture specific sections of videos that are semantically similar to certain frames. Existing methods often aggregate entire videos without considering the text, leading to the inclusion of irrelevant visual information. To address this issue, we propose X-Pool, a cross-modal attention model that allows the text to focus on the most semantically similar frames. Our model utilizes scaled dot product attention to enable the text to attend to relevant frames and generates an aggregated video representation based on the text's attention weights. We evaluate our approach on three benchmark datasets and achieve state-of-the-art results, with up to a 12% improvement in Recall@1. These findings emphasize the importance of considering both text and video to extract relevant visual cues. The full code and demo can be accessed at layer6ai-labs.github.io/xpool/.