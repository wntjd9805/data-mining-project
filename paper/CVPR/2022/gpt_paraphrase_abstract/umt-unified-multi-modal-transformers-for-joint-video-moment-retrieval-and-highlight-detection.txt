In the current era of video content explosion, there is a growing need to find relevant moments and highlights in videos using natural language queries. While some related tasks have been studied, the joint optimization of moment retrieval and highlight detection is a relatively new research topic. This paper introduces a unified framework called Unified Multi-modal Transformers (UMT), which can efficiently handle both joint optimization and individual problem-solving. UMT is the first scheme to integrate multi-modal learning (visual-audio) for moment retrieval, using a unique query generator and query decoder to treat moment retrieval as a keypoint detection problem. Extensive comparisons and ablation studies on various datasets demonstrate the effectiveness, superiority, and flexibility of the proposed method. The source code and pre-trained models for UMT are available at https://github.com/TencentARC/UMT.