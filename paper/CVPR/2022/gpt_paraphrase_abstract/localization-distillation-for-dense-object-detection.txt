Knowledge distillation (KD) has proven to be effective in training compact models for object detection. Previous KD methods for object detection have mainly focused on imitating deep features within specific regions, rather than mimicking classification logit. This is because mimicking classification logit is inefficient in capturing localization information and does not lead to significant improvements. In this paper, we propose a novel approach called localization distillation (LD) that reformulates the knowledge distillation process to efficiently transfer localization knowledge from teacher models to student models. We also introduce the concept of valuable localization regions, which selectively distill semantic and localization knowledge for specific regions. By combining these two components, we demonstrate that mimicking classification logit can outperform feature imitation, and that localization knowledge distillation is more important and efficient than semantic knowledge for training object detectors. Our distillation scheme is simple yet effective, and can be easily applied to different dense object detectors. Experimental results show that our LD method can improve the average precision (AP) score of GFocal-ResNet-50 from 40.1 to 42.1 on the COCO benchmark, without sacrificing inference speed. The source code and pretrained models are publicly available at https://github.com/HikariTJU/LD.