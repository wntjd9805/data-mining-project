Creating a new font library for scripts with a large number of glyphs is a time-consuming and labor-intensive process. To address this issue, few-shot font generation techniques have been developed, which require only a small number of glyph references without the need for fine-tuning during testing. However, existing methods for few-shot font generation have limitations in capturing content-independent style representations and modeling complex Chinese font styles. To overcome these limitations, we propose a self-supervised cross-modality pre-training strategy and a cross-modality transformer-based encoder that considers both the glyph image and corresponding stroke labels. The pre-trained encoder effectively captures correlations between different modalities and scales, enabling the disentanglement of content and style and modeling of style representations at stroke, component, and character levels. Our method outperforms state-of-the-art methods in transferring styles of all scales and achieves the lowest rate of bad cases in the few-shot font generation task, requiring only one reference glyph.