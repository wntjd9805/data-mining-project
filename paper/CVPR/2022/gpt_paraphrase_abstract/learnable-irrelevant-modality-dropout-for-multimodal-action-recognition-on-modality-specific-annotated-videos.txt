Current multimodal methods for action recognition in video datasets rely on modality fusion or cross-modality attention. However, effectively utilizing the audio modality in vision-specific annotated videos poses a significant challenge. To address this challenge, we propose a new audio-visual framework that leverages the audio modality in any solely vision-specific annotated dataset. We create a semantic audio-video label dictionary (SAVLD) using language models such as BERT, which maps each video label to its most K-relevant audio labels. This SAVLD acts as a bridge between audio and video datasets. During the training phase, we use SAVLD and a pretrained audio multi-label model to estimate the relevance of the audio-visual modality. We also introduce a learnable irrelevant modality dropout (IMD) technique to remove the irrelevant audio modality and fuse only the relevant modalities. Additionally, we propose a new two-stream video Transformer model for efficient modeling of the visual modalities. Our framework has been evaluated on various vision-specific annotated datasets, including Kinetics400 and UCF-101, and has shown superior performance compared to other action recognition methods.