Training a supernet in one-shot NAS methods is challenging due to the large search space. To improve the supernet's evaluation ability, a greedy strategy is commonly used to sample good paths and bias the supernet towards them. However, this approach is inefficient as identifying good paths accurately is difficult, and the sampled paths are scattered across the entire search space. In this study, we introduce an explicit path filter that captures path characteristics and filters out weak paths, allowing for a more efficient and greedy search on a reduced space. We utilize the fact that weak paths outnumber good paths and use the label "weak paths" with more confidence in multi-path sampling. We adopt a positive and unlabeled learning paradigm to train the path filter and encourage path embedding for better path/operation representation, enhancing the filter's identification capacity. Additionally, we aggregate similar operations with similar embeddings to further shrink the search space, improving efficiency and accuracy. Extensive experiments confirm the effectiveness of our proposed method, GreedyNASv2. For instance, GreedyNASv2-L achieves 81.1% Top-1 accuracy on the ImageNet dataset, surpassing the strong baselines set by ResNet-50.