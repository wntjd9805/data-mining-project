This paper introduces a novel framework called cross-architecture contrastive learning (CACL) for self-supervised video representation learning. The CACL framework combines a 3D CNN and a video transformer to generate diverse positive pairs for contrastive learning, allowing the model to learn strong representations from meaningful pairs. Additionally, the paper proposes a temporal self-supervised learning module that predicts the Edit distance between two video sequences in the temporal order. This module helps the model learn a rich temporal representation that complements the video-level representation learned by CACL. The proposed method is evaluated on video retrieval and action recognition tasks using UCF101 and HMDB51 datasets. The results demonstrate excellent performance, surpassing state-of-the-art methods like VideoMoCo and MoCo+BE by a significant margin.