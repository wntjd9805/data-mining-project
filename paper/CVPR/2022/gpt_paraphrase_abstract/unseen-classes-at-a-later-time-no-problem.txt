Recent advancements in limited supervision learning have sparked interest in developing models capable of recognizing new classes during testing, known as generalized zero-shot learning (GZSL). GZSL methods typically assume knowledge of all classes beforehand, with or without labeled data. However, practical scenarios require models that can adapt and handle the dynamic addition of new seen and unseen classes in real-time, referred to as continual generalized zero-shot learning (CGZSL). Retraining and reusing conventional GZSL methods sequentially has been proposed as a solution, but it suffers from catastrophic forgetting, resulting in suboptimal generalization performance. Previous efforts to address CGZSL have been limited by differences in settings, practicality, data splits, and protocols, hindering fair comparisons and a clear path forward. To address these limitations, we firstly consolidate the various CGZSL setting variants and introduce a new Online-CGZSL setting that is more practical and flexible. Secondly, we propose a unified feature-generative framework for CGZSL that utilizes bi-directional incremental alignment to dynamically adapt to the addition of new classes, with or without labeled data, that are introduced over time in any of these CGZSL settings. Through comprehensive experiments and analysis on five benchmark datasets, and comparison with baseline methods, we consistently outperform existing approaches, particularly in the more practical Online setting.