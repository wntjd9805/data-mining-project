This study presents a probabilistic model called Uncertainty-Guided Probabilistic Transformer (UGPT) for recognizing complex actions. The model utilizes the self-attention mechanism of a Transformer to capture the intricate and long-term dynamics of these actions. By incorporating the distribution of attention scores, the deterministic Transformer is transformed into a probabilistic Transformer to quantify prediction uncertainty. This uncertainty is leveraged to enhance both training and inference processes. The authors propose a novel training strategy using a majority model and a minority model based on epistemic uncertainty. During inference, the prediction is made jointly by both models using a dynamic fusion approach. The effectiveness of the proposed method is demonstrated on benchmark datasets, including BreakfastActions, MultiTHUMOS, and Charades, showing state-of-the-art performance even with limited data.