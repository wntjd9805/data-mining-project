Current instance segmentation methods typically require densely annotated images for training. This poses a significant challenge, particularly in the field of biomedical imaging where domain expertise is often necessary for annotation and there is a lack of large publicly available datasets for pre-training. To overcome this bottleneck, we propose a proposal-free segmentation approach that utilizes non-spatial embeddings to extract individual instances in a differentiable manner, leveraging the structure of the learned embedding space. This allows the segmentation loss to be directly applied to instances, enabling the entire pipeline to be trained either in a fully or weakly supervised manner. We specifically address the difficult case of positive-unlabeled supervision, introducing a novel self-supervised consistency loss for the unlabeled parts of the training data. We evaluate our proposed method on 2D and 3D segmentation problems in various microscopy modalities, as well as on the Cityscapes and CVPPP instance segmentation benchmarks. Our approach achieves state-of-the-art results on the CVPPP benchmark and demonstrates promising performance in the other evaluation scenarios.