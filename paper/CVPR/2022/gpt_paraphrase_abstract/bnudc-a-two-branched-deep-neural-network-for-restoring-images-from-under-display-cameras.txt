We present a novel approach to address the degradation of images captured by under-display cameras (UDCs) caused by the screen in front of them. Our model considers two main factors contributing to this degradation: diffraction by the pixel grid, which reduces high-frequency image details, and diffuse intensity and color changes resulting from the multiple thin-film layers in an OLED, which affect low-frequency components. To tackle these issues, we propose a deep neural network with two branches, each specifically designed to reverse one type of degradation. Our approach surpasses the performance of existing methods that attempt to restore UDC images using a single forward network for both types of degradation.   Additionally, we introduce an affine transform connection to replace the commonly used skip connection in most existing deep neural networks for UDC image restoration. By confining the solution space to linear transforms, we mitigate the blurring caused by convolution. Moreover, we eliminate any significant color shift in the training images through inverse color filtering.   To evaluate our approach, we train our network on three datasets of UDC images and compare its performance against existing methods using measures of distortion and perceived image quality. Our network outperforms the existing methods in both aspects.