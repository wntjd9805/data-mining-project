We introduce a highly parallel approach using a Lagrange decomposition method to solve 0-1 integer linear programs in structured prediction. Our method involves an innovative iterative update scheme for solving the Lagrangean dual problem, as well as a perturbation technique for decoding primal solutions. We utilize binary decision diagrams (BDDs) to represent subproblems, following the approach described in [40]. Our primal and dual algorithms require minimal synchronization between subproblems, and optimization over BDDs involves only basic operations without complex control flow. This enables us to leverage the parallelism provided by GPUs for all aspects of our method. We conduct experiments on various combinatorial problems, including MAP inference for Markov Random Fields, quadratic assignment, and cell tracking for developmental biology. Our GPU implementation significantly reduces the running times compared to the algorithms in [40], often achieving an order of magnitude improvement. In fact, our method performs on par with or outperforms some state-of-the-art specialized heuristics, while remaining problem-agnostic. Our implementation can be found at https://github.com/LPMP/BDD.