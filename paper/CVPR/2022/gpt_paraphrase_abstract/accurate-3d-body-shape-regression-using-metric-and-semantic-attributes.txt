Despite the rapid progress in regressing 3D human meshes from images, the accuracy of estimated body shapes often fails to capture the true human form, which is a significant problem as accurate body shape is crucial for many applications. The main reason for the gap in accuracy between body shape and pose is the lack of available data. While labeling 2D joints is relatively straightforward and helps constrain 3D pose, labeling 3D body shape is more challenging. To address this issue, we leverage two sources of information: internet images of diverse fashion models along with a small set of anthropometric measurements, and linguistic shape attributes for a wide range of 3D body meshes and model images. By combining these datasets, we can infer dense 3D shape. We employ the anthropometric measurements and linguistic shape attributes in innovative ways to train a neural network called SHAPY, which can regress 3D human pose and shape from an RGB image. We evaluate the performance of SHAPY on existing benchmarks but note that they lack significant body shape variation, ground-truth shape, or clothing variation. To overcome this limitation, we create a new dataset called HBW (Human Bodies in the Wild) that includes photos with ground-truth 3D body scans for evaluating 3D human shape estimation. On this new benchmark, SHAPY outperforms state-of-the-art methods in the task of 3D body shape estimation. This study demonstrates for the first time that 3D body shape regression from images can be trained using easily obtainable anthropometric measurements and linguistic shape attributes. The SHAPY model and data are accessible at shapy.is.tue.mpg.de.