Learning joint representations for video and text typically requires a large amount of manually annotated video data, which can be impractical. An alternative approach is to use a large-scale uncurated video dataset called HowTo100M, but learning joint embeddings in a self-supervised manner is still challenging due to ambiguity and non-sequential alignment. In this paper, we propose a novel self-supervised framework called VT-TWINS that uses a variant of Dynamic Time Warping (DTW) to capture significant information from noisy and weakly correlated data. We observe that standard DTW cannot handle weakly correlated data and only considers the globally optimal alignment path. To address this, we develop a differentiable DTW that incorporates local information with weak temporal alignment. Additionally, our model applies a contrastive learning scheme to learn feature representations on weakly correlated data. Extensive experiments show that VT-TWINS achieves significant improvements in multi-modal representation learning and outperforms various challenging downstream tasks. The code for VT-TWINS is available at https://github.com/mlvlab/VT-TWINS.