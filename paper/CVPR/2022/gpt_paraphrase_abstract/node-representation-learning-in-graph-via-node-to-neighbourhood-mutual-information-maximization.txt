This study focuses on the importance of gaining contextual information from the neighborhood in order to learn informative node representations in graphs. The authors propose a self-supervised node representation learning strategy that maximizes the mutual information between hidden representations of nodes and their neighborhood. This strategy is justified by its connection to graph smoothing. The framework is optimized through a surrogate contrastive loss, where positive selection plays a crucial role in the quality and efficiency of representation learning. To improve positive selection, the authors propose a topology-aware positive sampling strategy that considers the structural dependencies between nodes when sampling positives from the neighborhood. This strategy allows for upfront positive selection and avoids expensive neighborhood aggregation in cases where only one positive is sampled. The proposed methods show promising performance on various node classification datasets. Additionally, applying the loss function to MLP-based node encoders can significantly improve efficiency compared to existing solutions. The authors provide their codes and supplementary materials on GitHub.