This study addresses the challenging task of generating gestures that align with speech in order to effectively convey information and evoke empathy in the audience. Current approaches primarily focus on synchronizing gestures with speech rhythms, but struggle to capture the semantics and explicitly model semantic gestures. To tackle this issue, the authors propose a new method called Semantic Energized Generation (SEEG) for generating semantic-aware gestures. SEEG consists of two modules: the DEcoupled Mining module (DEM) and the Semantic Energizing Module (SEM). DEM separates semantic-irrelevant information from the inputs and individually mines information for beat and semantic gestures. SEM is responsible for semantic learning and generating semantic gestures. In addition to measuring representational similarity, SEM also requires the predictions to express the same semantics as the ground truth. To enhance semantic awareness, SEM incorporates a semantic prompter that provides supervision to the predictions. The experimental results, evaluated using three metrics on different benchmarks, demonstrate that SEEG effectively mines semantic cues and generates semantic gestures. SEEG outperforms other methods in all semantic-aware evaluations across various datasets. Qualitative evaluations further confirm the superior semantic expressiveness of SEEG. The code for SEEG is available at https://github.com/akira-l/SEEG.