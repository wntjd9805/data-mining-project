Significant advancements have been made in recent years in the development of improved image captioning models. However, most existing models rely on a separate object detector to extract regional features. A recent trend in vision-language studies is moving towards detector-free approaches that utilize grid representations for more flexible training and faster inference. However, this trend has primarily focused on image understanding tasks and has received less attention in the context of caption generation. This paper aims to address this gap by proposing a detector-free image captioning model called ViTCAP, which is based on a pure vision transformer and utilizes grid representations without extracting regional features. To enhance performance, a novel Concept Token Network (CTN) is introduced, which predicts semantic concepts and incorporates them into the end-to-end captioning process. The CTN is built on a vision transformer and predicts concept tokens through a classification task, providing valuable semantic information for the captioning task. Compared to previous detector-based models, ViTCAP offers simplified architectures while achieving competitive performance on challenging image captioning datasets. For instance, ViTCAP achieves CIDEr scores of 138.1 on COCO-caption Karpathy-split, 93.8 on nocaps, and 108.6 on Google-CC captioning datasets.