Joint embeddings have become the leading approach for cross-modal retrieval, thanks to large-scale training datasets, improvements in neural architecture design, and efficient inference. However, these embeddings suffer from a common issue known as the "hub-ness problem," where a small number of gallery embeddings are the closest neighbors to multiple queries. To address this problem, we introduce a framework called Querybank Normalization (QB-NORM), inspired by techniques used in natural language processing. QB-NORM effectively re-normalizes query similarities to account for hubs in the embedding space, improving retrieval performance without the need for retraining. Unlike previous methods, QB-NORM does not require access to test set queries during its operation. Additionally, within the QB-NORM framework, we propose a new similarity normalization method called the Dynamic Inverted Softmax, which proves to be more robust than existing approaches. We demonstrate the effectiveness of QB-NORM across various cross-modal retrieval models and benchmarks, consistently surpassing state-of-the-art baselines. Code for QB-NORM is available at https://vladbogo.github.io/QB-Norm/.