Recently, there have been several proposed multimodal networks for understanding visually-rich documents (VRDU). These networks integrate visual and layout information with text embeddings, utilizing transformers. However, most existing approaches incorporate sequence information through position embeddings, which can be problematic due to improper reading orders obtained from OCR tools. In this paper, we introduce a robust layout-aware multimodal network called XYLayoutLM. This network captures and utilizes rich layout information from proper reading orders generated by our Augmented XY Cut method. Additionally, we propose a Dilated Conditional Position Encoding module to handle input sequences of varying lengths. This module extracts local layout information from both textual and visual modalities while generating position embeddings. Experimental results demonstrate that our XYLayoutLM achieves competitive performance on document understanding tasks.