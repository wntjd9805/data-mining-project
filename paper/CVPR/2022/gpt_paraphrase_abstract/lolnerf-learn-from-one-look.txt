We propose a method for learning a 3D model using neural radiance fields, which can generate realistic images from single views of each object. While generating realistic images is now easier, producing corresponding 3D structures that allow for rendering from different views remains challenging. Unlike existing methods, our approach does not require multi-view data. Instead, we reconstruct multiple images aligned to a standard pose using a single network conditioned on a shared latent space. This enables us to learn a space of radiance fields that capture both shape and appearance for a specific object class. We validate our method by training models on datasets with only one view per object, without depth or geometry information. Our experiments demonstrate that our approach achieves state-of-the-art results in synthesizing novel views and high-quality monocular depth prediction. Please visit https://lolnerf.github.io for more information.