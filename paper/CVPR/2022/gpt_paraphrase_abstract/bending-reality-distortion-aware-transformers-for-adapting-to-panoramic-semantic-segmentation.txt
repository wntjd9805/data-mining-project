Panoramic images provide comprehensive information about the surrounding space and are valuable for scene understanding. However, training robust panoramic segmentation models requires a large number of expensive pixel-wise annotations, which are mostly available for narrow-angle, pinhole-camera images. These annotations are sub-optimal for training panoramic models due to distortions and differences in image-feature distribution. To address this issue, we propose a model called Trans4PASS that combines object deformations and panoramic image distortions in the Deformable Patch Embedding (DPE) and Deformable MLP (DMLP) components. We also introduce Mutual Prototypical Adaptation (MPA) to align shared semantics in pinhole and panoramic feature embeddings for unsupervised domain adaptation. Our Trans4PASS model with MPA achieves comparable performance to fully-supervised state-of-the-art models on the indoor Stanford2D3D dataset, reducing the need for over 1,400 labeled panoramas. On the outdoor DensePASS dataset, our model sets a new state-of-the-art with a 14.39% improvement in mean Intersection over Union (mIoU), reaching 56.38%.