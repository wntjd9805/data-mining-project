Video transformers have become a popular alternative to convolutional networks for action classification. However, most existing video transformers use fixed attention schemes, either relying on global space-time attention or predefined strategies to compare patches within and across frames. These fixed attention schemes are computationally expensive and fail to capture the motion dynamics present in videos. To address these limitations, we propose the Deformable Video Transformer (DVT). DVT dynamically predicts a small subset of video patches to attend to for each query location based on motion information. This allows the model to determine where to focus its attention in the video by considering correspondences across frames. Importantly, these motion-based correspondences are obtained at zero-cost from information stored in the compressed format of the video. Our deformable attention mechanism is optimized directly for classification performance, eliminating the need for suboptimal hand-designed attention strategies. We evaluate our approach on four large-scale video benchmarks (Kinetics-400, Something-Something-V2, EPIC-KITCHENS, and Diving-48). Our experimental results demonstrate that compared to existing video transformers, our model achieves higher accuracy at the same or lower computational cost. Additionally, our model achieves state-of-the-art results on all four datasets.