Temporal sentence grounding is the task of identifying the most relevant moment in a video that corresponds to a given natural language query. Traditionally, this task has involved manually labeling the exact time boundaries, which is time-consuming and subjective. As a result, researchers have turned to weakly-supervised methods that require less manual effort. However, existing weakly-supervised methods often generate proposals using sliding windows, which are not tailored to the specific content of the video and produce low-quality results. Additionally, these methods train their models to distinguish positive visual-language pairs from randomly collected negative pairs from other videos, without considering the presence of highly confusing video segments within the same video.To address these limitations, we propose a new method called Contrastive Proposal Learning (CPL). This approach uses multiple learnable Gaussian functions to generate both positive and negative proposals within the same video, capturing the multiple events that occur in a long video. We also introduce a strategy for mining controllable easy to hard negative proposals within the same video, which facilitates model optimization and enables CPL to effectively differentiate highly confusing scenes. Experimental results on the Charades-STA and ActivityNet Captions datasets demonstrate that our method achieves state-of-the-art performance.For more details and access to the code and models, please visit our GitHub repository at https://github.com/minghangz/cpl.