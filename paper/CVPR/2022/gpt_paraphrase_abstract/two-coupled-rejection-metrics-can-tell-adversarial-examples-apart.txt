Classifying adversarial examples accurately is a difficult task when deploying machine learning models. Even the most advanced models struggle to achieve a robust test accuracy of more than 67% on CIFAR-10, as reported in RobustBench. To enhance the models' robustness, a complementary approach is to introduce a rejection option. This allows the model to refrain from making predictions on uncertain inputs, which can be determined by measuring confidence. Our research reveals that confidence, along with a rectified confidence (R-Con), can serve as two linked rejection metrics. These metrics have the ability to accurately differentiate between wrongly classified and correctly classified inputs. This finding suggests the potential of using coupling strategies to improve the detection and rejection of adversarial examples. We assess the effectiveness of our rectified rejection (RR) module on CIFAR-10, CIFAR-10-C, and CIFAR-100 datasets, against various attacks including adaptive ones. Our results demonstrate that the RR module, when integrated with different adversarial training frameworks, enhances robustness with minimal additional computational requirements.