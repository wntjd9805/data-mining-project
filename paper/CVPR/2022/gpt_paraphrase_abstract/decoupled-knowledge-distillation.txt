State-of-the-art distillation techniques primarily focus on extracting deep features from intermediate layers, neglecting the importance of logit distillation. In order to present a fresh perspective on logit distillation, we redefine the classical KD loss into two components: target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). Through empirical investigation, we demonstrate the significance of these two components. TCKD transfers knowledge regarding the difficulty of training samples, while NCKD is the main factor contributing to the effectiveness of logit distillation. Additionally, we discover that the classical KD loss is a coupled formulation that hampers the effectiveness of NCKD and limits the flexibility in balancing these two components. To address these limitations, we propose a new approach called Decoupled Knowledge Distillation (DKD), which allows TCKD and NCKD to play their roles more efficiently and flexibly. Compared to complex feature-based methods, our DKD achieves comparable or even superior results, and demonstrates better training efficiency on CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object detection tasks. This paper highlights the significant potential of logit distillation and aims to provide valuable insights for future research. The code for our approach can be accessed at https://github.com/megvii-research/mdistiller.