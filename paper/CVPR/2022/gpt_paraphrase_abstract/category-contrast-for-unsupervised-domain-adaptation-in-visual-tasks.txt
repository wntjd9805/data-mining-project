This study investigates the concept of instance contrastive learning in unsupervised domain adaptation (UDA) and introduces a new technique called Category Contrast (CaCo) that incorporates semantic priors into instance discrimination for visual UDA tasks. By treating instance contrastive learning as a dictionary look-up operation, a semantics-aware dictionary is constructed using samples from both the source and target domains. Each target sample is assigned a pseudo category label based on the category priors of source samples. This enables category contrastive learning between target queries and the category-level dictionary, resulting in category-discriminative yet domain-invariant feature representations. Samples of the same category are brought closer together, while those of different categories are pushed apart. Extensive UDA experiments across multiple visual tasks demonstrate that CaCo outperforms state-of-the-art methods. The experiments also highlight the complementary nature of CaCo with existing UDA methods and its generalizability to other learning setups such as unsupervised model adaptation, open-/partial-set adaptation, etc.