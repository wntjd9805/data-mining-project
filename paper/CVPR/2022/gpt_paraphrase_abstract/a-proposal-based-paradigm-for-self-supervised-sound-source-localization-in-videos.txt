Humans have the ability to recognize the location and characteristics of sound by combining visual and auditory cues. Existing methods for replicating this cross-modal perception in machines use interpolation operations to generate maps that locate the sound source. However, these map-based approaches only provide a rough and indirect description of the sound source. In this paper, we propose a new paradigm that directly performs semantic object-level localization of sound sources without the need for manual annotations. Our approach incorporates a global response map as an unsupervised spatial constraint to determine the accuracy of the proposed sound source locations. By filtering out instances that correspond to unrelated regions, our proposal-based method simplifies the sound source localization task into a Multiple Instance Learning (MIL) problem. Our method outperforms several baselines on multiple datasets, achieving state-of-the-art performance.