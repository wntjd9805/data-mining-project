We introduce HumanNeRF, a neural representation that allows for high-quality synthesis of dynamic humans from sparse video inputs. Unlike previous methods, HumanNeRF does not require dense multi-view inputs or costly training for each frame. Instead, it utilizes an aggregated pixel-alignment feature and a pose embedded non-rigid deformation field to handle dynamic motions. The initial version of HumanNeRF can already generate reasonable renderings of unseen subjects and camera settings. To further enhance the rendering quality, we incorporate in-hour scene-specific fine-tuning and an appearance blending module, combining the advantages of neural volumetric rendering and neural texture blending. Our extensive experiments on various multi-view dynamic human datasets demonstrate the effectiveness of our approach in producing photo-realistic free-view humans under challenging motions and with very sparse camera view inputs.