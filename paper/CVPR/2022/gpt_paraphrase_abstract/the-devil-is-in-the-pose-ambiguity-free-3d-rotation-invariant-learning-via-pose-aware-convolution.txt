Recent advancements in 3D deep learning methods have focused on incorporating rotation invariance (RI) by replacing 3D coordinates with RI features as input. However, a challenge arises in restoring the lost global information caused by these input features. Existing approaches address this issue by introducing additional blocks or complex global representations, which are time-consuming and inefficient. In this study, we identify that the loss of global information stems from a previously unexplored problem of pose information loss. Conventional convolution layers fail to capture the relative poses between RI features, thereby hindering hierarchical aggregation of global information in deep networks. To tackle this problem, we propose a novel technique called Pose-aware Rotation Invariant Convolution (PaRI-Conv), which dynamically adapts its kernels based on the relative poses. In each PaRI-Conv layer, we design a lightweight Augmented Point Pair Feature (APPF) to encode the RI relative pose information comprehensively. Furthermore, we introduce a factorized dynamic kernel that reduces computational and memory burden by decomposing it into a shared basis matrix and a pose-aware diagonal matrix, which can be learned from the APPF. Extensive experiments conducted on shape classification and part segmentation tasks demonstrate that our PaRI-Conv outperforms existing RI methods while being more compact and efficient.