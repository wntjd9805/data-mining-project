In the field of autonomous driving, the tracking of single objects in 3D LiDAR point clouds is crucial. Current approaches rely on appearance matching using the Siamese paradigm. However, LiDAR point clouds lack texture and are often incomplete, which makes effective appearance matching difficult. Additionally, previous methods have overlooked the importance of motion clues in tracking targets.   This study introduces a new perspective for 3D object tracking called the motion-centric paradigm. Based on this paradigm, a two-stage tracker called M2-Track is proposed. In the first stage, M2-Track uses motion transformation to localize the target in consecutive frames. In the second stage, it refines the target box by utilizing motion-assisted shape completion. Extensive experiments demonstrate that M2-Track outperforms previous state-of-the-art methods on three large-scale datasets, achieving precision gains of approximately 8%, 17%, and 22% on KITTI, NuScenes, and Waymo Open Dataset, respectively. The tracker runs at a speed of 57 frames per second.   Further analysis confirms the effectiveness of each component of M2-Track and highlights the promising potential of combining the motion-centric paradigm with appearance matching. The code for M2-Track is available at https://github.com/Ghostish/Open3DSOT.