Federated Learning (FL) is a new type of distributed learning that prioritizes privacy. However, FL faces challenges due to data heterogeneity, resulting in slow convergence and decreased performance. Existing methods address this issue by limiting local model updates on the client side, but they ignore the negative impact of direct global model aggregation. To overcome this, we propose a data-free knowledge distillation method called FedFTG. This method fine-tunes the global model on the server, alleviating the problem of direct model aggregation. FedFTG achieves this by exploring the input space of local models using a generator and transferring the knowledge from local models to the global model. We also introduce a hard sample mining scheme to improve knowledge distillation during training. Additionally, we implement customized label sampling and class-level ensemble techniques to maximize knowledge utilization and reduce distribution discrepancies across clients. Extensive experiments demonstrate that our FedFTG outperforms the current state-of-the-art FL algorithms and can be effectively integrated with FedAvg, FedProx, FedDyn, and SCAFFOLD.