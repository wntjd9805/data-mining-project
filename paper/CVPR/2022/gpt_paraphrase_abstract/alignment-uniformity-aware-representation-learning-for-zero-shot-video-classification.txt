This paper introduces a novel framework for zero-shot video classification that aims to improve model generalizability. Existing methods focus on aligning visual-semantic representations within seen classes, which limits their ability to generalize to unseen classes. In contrast, our approach preserves alignment and uniformity properties for representations on both seen and unseen classes.To achieve this, we propose a supervised contrastive loss that simultaneously aligns visual-semantic features and encourages uniform distribution of learned features. Unlike previous methods that only consider alignment, we also emphasize uniformity to maintain the maximal information of existing features. This improves the likelihood that unobserved features will be similar to observed data.Furthermore, we synthesize features of unseen classes by introducing a class generator that interpolates and extrapolates features from seen classes. This allows us to expand the model's capabilities to classify unseen classes.To assess the generalizability of our model, we introduce two new metrics: closeness and dispersion. These metrics quantify the alignment and uniformity properties and provide a measure of the model's ability to generalize.Experimental results demonstrate that our method outperforms the state-of-the-art with relative improvements of 28.1% on UCF101 and 27.0% on HMDB51 datasets. We have made our code available for further exploration.