Detecting keypoints in images is crucial for computer vision tasks, and the orientation and scale of these keypoints are significant for their description and matching. However, current learning-based methods using translation-equivariant CNNs often struggle to detect reliable keypoints in the presence of geometric variations. In order to address this issue, we propose a self-supervised learning framework that utilizes rotation-equivariant CNNs to detect robust oriented keypoints. Our approach involves training a histogram-based orientation map using a dense orientation alignment loss, which is computed from image pairs generated by synthetic transformations. Through evaluation on image matching and camera pose estimation benchmarks, our method outperforms previous approaches in terms of keypoint detection accuracy.