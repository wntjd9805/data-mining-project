In recent studies, the importance of understanding the meaning of scene text in fine-grained image classification has been established. However, current methods mainly focus on the literal meaning of the text, which may not always be relevant to the objects or scenes depicted. To address this limitation, we propose an end-to-end trainable network that uncovers implicit contextual knowledge within scene text images and enhances the semantics and correlation to improve image representation for fine-grained classification. Unlike existing approaches, our model integrates three modalities: visual feature extraction, text semantics extraction, and the incorporation of background knowledge to enhance fine-grained image classification. We utilize KnowBert, a tool for retrieving relevant knowledge, to improve semantic representation and combine it with image features for classification purposes. Experimental results on two benchmark datasets, Con-Text and Drink Bottle, demonstrate that our method surpasses the current state-of-the-art by 3.72% mAP and 5.39% mAP, respectively. To further validate the effectiveness of our approach, we introduce a new dataset for crowd activity recognition evaluation. The source code and dataset for this work can be found in the provided repository.