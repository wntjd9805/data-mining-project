Previous research has primarily focused on bilingual sign language translation (BSLT), which has limitations in building efficient multilingual sign language translation systems. To address this issue, we propose the multilingual sign language translation (MSLT) task, which aims to use a single model to translate between multiple sign languages and spoken languages. Our model, MLSLT, is the first of its kind and incorporates two innovative dynamic routing mechanisms to control the extent of parameter sharing between different languages. The intra-layer language-specific routing mechanism controls the flow of data through shared parameters and language-specific parameters at the token level using a soft gate within each layer. The inter-layer language-specific routing mechanism controls and learns the data flow path of different languages at the language level using a soft gate between layers. To evaluate the performance of MLSLT, we have collected the Spreadthesign-Ten (SP-10) dataset, the first publicly available multilingual sign language understanding dataset, which includes up to 100 language pairs such as CSL→en and GSG→zh. Experimental results demonstrate that MLSLT outperforms both the baseline MSLT model and the combination of multiple BSLT models in many cases. Furthermore, we have explored zero-shot translation in sign language and found that our model achieves comparable performance to the supervised BSLT model for certain language pairs. For additional information and access to the dataset, please visit https://mlslt.github.io/.