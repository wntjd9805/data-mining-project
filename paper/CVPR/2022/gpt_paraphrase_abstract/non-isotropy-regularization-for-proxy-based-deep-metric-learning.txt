Deep Metric Learning (DML) is a technique that aims to learn representation spaces where semantic relations can be easily expressed through predefined distance metrics. The best-performing DML approaches often use class proxies as substitutes for samples to improve convergence and generalization. However, these proxy methods only optimize for the distances between samples and proxies, which can result in locally isotropic sample distributions. This can lead to the loss of crucial semantic context due to difficulties in resolving local structures and relationships between samples within the same class.To address this issue, we propose a solution called non-isotropy regularization (NIR) for proxy-based Deep Metric Learning. By utilizing Normalizing Flows, we enforce the unique translatability of samples from their respective class proxies. This allows us to explicitly induce a non-isotropic distribution of samples around each proxy, which leads to better optimization for local structures. Our extensive experiments demonstrate the consistent generalization benefits of NIR, achieving competitive and state-of-the-art performance on standard benchmarks such as CUB200-2011, Cars196, and Stanford Online Products.Furthermore, we find that NIR retains or even improves upon the superior convergence properties of proxy-based methods, making it highly attractive for practical usage. The code for implementing NIR is available at github.com/ExplainableML/NonIsotropicProxyDML.