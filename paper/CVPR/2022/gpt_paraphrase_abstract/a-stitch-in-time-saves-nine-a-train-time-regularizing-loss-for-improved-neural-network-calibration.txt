Deep Neural Networks (DNNs) often make overly confident errors, which poses challenges in safety-critical applications. Current calibration techniques improve the confidence of predicted labels but fail to calibrate the confidence of non-max classes, such as top-2 or top-5. Additionally, these techniques typically learn hyperparameters post-hoc, which limits their applicability to image-specific calibration or tasks involving domain shift and dense prediction. To address these issues, this paper proposes a novel auxiliary loss function called Multi-class Difference in Confidence and Accuracy (MDCA) to directly produce calibrated DNN models during training. MDCA can be used together with task-specific loss functions and has been shown to improve calibration in image classification and segmentation tasks, as measured by Expected Calibration Error (ECE) and Static Calibration Error (SCE). For example, on the CIFAR 100 dataset, MDCA achieves an ECE (SCE) score of 0.72 (1.60), compared to the state-of-the-art (SOTA) score of 1.90 (1.71). Under domain shift, a ResNet-18 model trained with MDCA on the PACS dataset achieves an average ECE (SCE) score of 19.7 (9.7) across all domains, outperforming the SOTA score of 24.2 (11.8). In segmentation tasks, MDCA reduces calibration error by two times on the PASCAL-VOC dataset compared to the use of Focal Loss. Furthermore, MDCA training also improves calibration on imbalanced data and natural language classification tasks.