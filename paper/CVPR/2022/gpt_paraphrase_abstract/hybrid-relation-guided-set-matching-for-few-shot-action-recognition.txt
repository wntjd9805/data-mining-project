Current few-shot action recognition methods achieve impressive performance by learning discriminative features for each video through episodic training and utilizing various temporal alignment strategies. However, these methods have limitations. Firstly, they focus on learning individual features without considering the entire task, which may result in the loss of relevant information in the current episode. Secondly, the alignment strategies employed may fail to accurately align misaligned instances. To address these limitations, we propose a novel approach called Hybrid Relation guided Set Matching (HyRSM). HyRSM incorporates two key components: a hybrid relation module and a set matching metric. The hybrid relation module aims to learn task-specific embeddings by fully utilizing associated relations within and across videos in an episode. Leveraging the task-specific features, we redefine the distance measure between query and support videos as a set matching problem. Additionally, we design a bidirectional Mean Hausdorff Metric to enhance the model's ability to handle misaligned instances. This approach allows HyRSM to be highly informative and flexible in predicting query categories in few-shot settings. We evaluate the performance of HyRSM on six challenging benchmarks and the experimental results demonstrate its superiority over state-of-the-art methods by a significant margin. For more information about the project, please visit our project page: https://hyrsm-cvpr2022.github.io/.