Unsupervised Domain Adaptation (UDA) is a challenging task that involves using a labeled source domain to solve tasks on a related unlabeled target domain. This becomes especially difficult when there is a large difference between the two domains. In this paper, we propose a novel solution called SSRT (Safe Self-Refinement for Transformer-based domain adaptation) that improves UDA in two ways.First, we incorporate a transformer backbone into SSRT, inspired by the success of vision transformers in various vision tasks. By combining this transformer with simple adversarial adaptation, we achieve better results than the best reported Convolutional Neural Network (CNN)-based methods on the challenging DomainNet benchmark. This demonstrates the strong transferability of the feature representation provided by SSRT.Second, to address the risk of model collapse and enhance knowledge transfer between domains with significant gaps, we introduce a Safe Self-Refinement strategy. In this strategy, SSRT utilizes predictions from perturbed target domain data to refine the model. Since vision transformers have a large model capacity and predictions in challenging tasks can be noisy, we design a safe training mechanism that adaptively adjusts the learning configuration.We extensively evaluate SSRT on multiple widely tested UDA benchmarks, and it consistently achieves the best performance. For example, SSRT achieves an accuracy of 85.43% on the Office-Home dataset, 88.76% on VisDA-2017, and 45.2% on DomainNet.In summary, our proposed SSRT method for UDA combines a transformer backbone with adversarial adaptation and employs a Safe Self-Refinement strategy to mitigate the risk of model collapse. Through extensive evaluations, we demonstrate its superior performance on various UDA benchmarks.