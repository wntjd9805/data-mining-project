Sketch-based image manipulation is a task that involves modifying an image based on user input sketches. Existing methods typically treat this task as a conditional inpainting problem, where users must draw an additional mask to indicate the region to be modified. This approach simplifies data preparation and model design but complicates user interaction and ignores useful information in the masked regions. In this study, we propose a new approach called mask-free local image manipulation, which only requires user sketches and utilizes the entire original image. Our model predicts the target modification region and encodes it into a style vector, which is used by a generator to synthesize new image content based on the sketch. The manipulated image is then produced by blending the generator output into the modification region of the original image. We train our model in a self-supervised manner by learning the reconstruction of an image region from the style vector and sketch. Our method offers simpler and more intuitive workflows for sketch-based image manipulation and yields better results compared to previous approaches.