Learning the geometry, motion, and appearance of object classes is crucial for various computer vision problems. However, most existing approaches have focused on static objects, leaving dynamic objects with controllable articulation underexplored. In this study, we propose a new method for learning a representation of articulated objects' geometry, appearance, and motion using only a set of color images as input. Through self-supervised learning, our approach develops shape, appearance, and articulation codes that allow independent control of these semantic dimensions. Importantly, our model is trained end-to-end without the need for any articulation annotations. Experimental results demonstrate that our method performs well for different types of joints, including revolute and prismatic joints, as well as various combinations of these joints. Compared to state-of-the-art approaches that rely on direct 3D supervision and do not output appearance, our approach achieves more accurate geometry and appearance reconstruction from 2D observations alone. Additionally, our representation enables a wide range of applications, such as few-shot reconstruction, generating novel articulations, and synthesizing novel views. More information about the project can be found at our project page: https://weify627.github.io/nasam/.