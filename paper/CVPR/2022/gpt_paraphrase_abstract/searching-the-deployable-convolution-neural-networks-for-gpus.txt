This study aims to simplify the customization of Convolutional Neural Networks (CNN) for practical use in Deep Learning (DL) by introducing a model hub that contains optimized models ranked by their inference latency using Neural Architecture Search (NAS). A distributed NAS system was developed to search a unique search space comprising factors that significantly impact latency and accuracy. The optimized models, named GPUNet, outperform EfficientNet-X and FBNetV3 in both speed and accuracy on GPU, establishing a new state-of-the-art (SOTA) Pareto frontier. GPUNet is twice as fast as EfficientNet-X and FBNetV3, achieving superior accuracy within 1ms. The performance of GPUNet was also validated in detection tasks, consistently outperforming EfficientNet-X and FBNetV3 in terms of latency and accuracy on COCO detection tasks. These results confirm the effectiveness and versatility of the NAS system in handling various design tasks. By utilizing this NAS system, GPUNet can be adapted to different latency requirements, allowing DL practitioners to directly deploy the models in diverse scenarios.