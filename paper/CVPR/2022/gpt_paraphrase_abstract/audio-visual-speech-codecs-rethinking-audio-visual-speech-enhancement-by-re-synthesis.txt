Audio-visual speech enhancement methods are more accurate than audio-only methods because facial actions provide significant information about speech content. However, current approaches still struggle to produce clean and realistic speech without noise artifacts and unnatural distortions in challenging acoustic environments. This paper presents a new framework for audio-visual speech enhancement in high-fidelity telecommunications for AR/VR. Our approach utilizes audio-visual speech cues to generate codes for a neural speech codec, enabling efficient synthesis of clean and realistic speech from noisy signals. We specifically focus on developing personalized models for individual speakers, considering the importance of speaker-specific cues in speech. Our approach is evaluated on a new audio-visual speech dataset collected in a large vocabulary setting, as well as existing datasets. The results demonstrate superior performance compared to baseline speech enhancement methods, both in terms of quantitative metrics and human evaluation studies. For qualitative results, please refer to the supplemental video.