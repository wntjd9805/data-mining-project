This paper aims to develop effective lip reading models for speech recognition in silent videos. While previous studies have focused on adapting existing automatic speech recognition techniques, we address the specific challenges of lip reading and propose tailored solutions. Our contributions include introducing an attention-based pooling mechanism to aggregate visual speech representations, utilizing sub-word units for improved modeling of task ambiguities, and proposing a Visual Speech Detection (VSD) model trained on top of the lip reading network. By training on public datasets, our approach achieves state-of-the-art results on the challenging LRS2 and LRS3 benchmarks, even outperforming models trained on large-scale industrial datasets with a significantly smaller amount of data. Our best model achieves an unprecedented 22.6% word error rate on the LRS2 dataset, narrowing the performance gap between lip reading and automatic speech recognition. Additionally, our VSD model surpasses visual-only baselines and outperforms several recent audio-visual methods on the AVA-ActiveSpeaker benchmark.