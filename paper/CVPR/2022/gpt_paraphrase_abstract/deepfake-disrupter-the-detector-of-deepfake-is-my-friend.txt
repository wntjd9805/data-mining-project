In recent years, there has been significant progress in the development of face manipulation systems known as DeepFakes, which are based on Deep Neural Networks (DNNs). However, if not properly controlled, DeepFakes pose a real threat to both celebrities and ordinary individuals. One precautionary measure is to introduce disruptions to the source inputs, making the DeepFake results appear distorted to the human eye. However, previous methods have not explored whether these disrupted images can still deceive DeepFake detectors. This is crucial in applications where DeepFake detectors are used to distinguish between DeepFake data and real data, as manual examination of large amounts of data is costly. We argue that DeepFake detectors may not have the same perspective as human eyes and could still be fooled by disrupted data. Additionally, existing disruption methods rely on time-consuming iteration-based perturbation generation algorithms. To address these issues, we propose a new DeepFake disruption algorithm called "DeepFakeDisrupter". By training a perturbation generator, we can add imperceptible disruptions to source images without the need for backpropagation updates. The resulting DeepFake images not only appear unrealistic to the human eye but can also be easily identified by DeepFake detectors. Experimental results demonstrate that our trained perturbations can increase the F1-score of various DeepFake detectors by 10-20% when applied to fake images generated by Star-GAN.