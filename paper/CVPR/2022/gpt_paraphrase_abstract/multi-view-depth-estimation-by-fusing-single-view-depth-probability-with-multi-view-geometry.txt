Multi-view depth estimation methods often require the calculation of a multi-view cost-volume, which results in high memory usage and slow inference. Additionally, these methods can struggle with texture-less surfaces, reflective surfaces, and moving objects. In contrast, single-view depth estimation methods are often more reliable in these challenging scenarios. To address these issues, we propose MaGNet, a new framework that combines single-view depth probability with multi-view geometry to enhance the accuracy, robustness, and efficiency of multi-view depth estimation.In MaGNet, for each frame, we estimate a pixel-wise Gaussian distribution to represent the single-view depth probability. Using this distribution, we sample depth candidates for each pixel, enabling the network to achieve higher accuracy by evaluating fewer candidates. Furthermore, we introduce depth consistency weighting to the multi-view matching score, ensuring that the multi-view depth aligns with the single-view predictions. The proposed method achieves state-of-the-art performance on ScanNet, 7-Scenes, and KITTI datasets.Qualitative evaluation confirms that our method is more robust against challenging artifacts such as texture-less/reflective surfaces and moving objects. We provide our code and model weights on GitHub for easy access.