Deep learning has revolutionized the field of multimodal tasks, which involve combining multiple types of input data. While multimodal models have shown promise in solving various problems, their complexity also makes them susceptible to attacks. One type of attack, known as a Backdoor or Trojan attack, involves embedding a malicious secret behavior into a network. This behavior is triggered when a specific input, specified by the attacker, is added. In this study, we introduce a new type of attack called Dual-Key Multimodal Backdoors, which exploits the complex fusion mechanisms used by state-of-the-art multimodal networks. Instead of using a single trigger, this attack embeds a trigger in each of the input modalities and activates the malicious behavior only when both triggers are present. We conducted an extensive study of this attack on the Visual Question Answering (VQA) task using different network architectures and visual feature backbones. Embedding backdoors in VQA models poses a challenge because most models use visual features from a pretrained object detector. This presents difficulties for the attacker as the detector can distort or ignore the visual trigger, resulting in backdoors that heavily rely on the language trigger. To address this issue, we propose a visual trigger optimization strategy specifically designed for pretrained object detectors. Using this method, we were able to create Dual-Key Backdoors with a success rate of over 98% while only poisoning 1% of the training data. To facilitate research in defending against multimodal backdoors, we have released a large collection of clean and trojan VQA models called TrojVQA. This dataset will enable further exploration and development of defense mechanisms against these types of attacks.