Human action recognition using skeleton-based data is a valuable tool for understanding human behavior. However, most studies have focused on encoding the skeleton itself, neglecting the importance of embedding this information into the latent representations of human action. This paper introduces InfoGCN, a learning framework for action recognition that combines a novel learning objective and an encoding method. The proposed learning objective is based on an information bottleneck, aiming to guide the model in learning compact yet informative representations. To capture the context-dependent intrinsic topology of human action, the framework utilizes attention-based graph convolution. Additionally, a multi-modal representation of the skeleton is presented, incorporating the relative positions of joints to provide complementary spatial information. InfoGCN outperforms existing state-of-the-art methods on various skeleton-based action recognition benchmarks, achieving accuracies of 93.0% on NTU RGB+D 60 cross-subject split, 89.8% on NTU RGB+D 120 cross-subject split, and 97.0% on NW-UCLA.