We propose a method for predicting future hand-object interactions based on egocentric videos. Instead of predicting action labels or pixels, our approach directly predicts the trajectory of hand motion and the future contact points on the next active object. This representation provides a clear description of future interactions. To accomplish this task, we develop an automated method to collect trajectory and contact point labels using large-scale data. We then train an Object-Centric Transformer (OCT) model using this data for prediction. Our model utilizes the self-attention mechanism in Transformers to reason about hand and object interactions. OCT also incorporates a probabilistic framework to handle uncertainty in prediction by sampling future trajectories and contact points. We conduct experiments on the Epic-Kitchens-55, Epic-Kitchens-100, and EGTEAGaze+ datasets, demonstrating that OCT outperforms state-of-the-art approaches by a significant margin. Further information about our project can be found on our project page: https://stevenlsw.github.io/hoi-forecast.