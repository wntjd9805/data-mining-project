Contrastive learning has emerged as a promising approach in the field of self-supervised learning, offering comparable performance to supervised learning with minimal fine-tuning. However, achieving high accuracy requires the use of wide and large networks, which results in significant computational requirements and limits the practicality of self-supervised learning. In order to address this issue, recent dynamic pruning algorithms for supervised learning have utilized auxiliary salience predictors to effectively reduce computation by eliminating insignificant features or channels. However, applying these salience predictors directly to contrastive learning from scratch proves to be challenging. To overcome this obstacle, we propose a novel dynamic pruning algorithm called contrastive dual gating (CDG), which selectively skips uninformative features during contrastive learning without compromising network trainability. We demonstrate the superiority of CDG by implementing it with ResNet models on CIFAR-10, CIFAR-100, and ImageNet-100 datasets. Compared to other state-of-the-art dynamic pruning algorithms for self-supervised learning, our implementation of CDG achieves up to a 15% increase in accuracy for the CIFAR-10 dataset, while also reducing computation requirements.