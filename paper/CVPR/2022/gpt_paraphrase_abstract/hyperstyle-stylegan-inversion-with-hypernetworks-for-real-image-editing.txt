The problem of converting real images into StyleGAN's latent space has been extensively studied. However, applying existing methods to real-world scenarios is still a challenge due to the trade-off between reconstruction accuracy and semantic control. Recent research suggests fine-tuning the generator to improve this trade-off by incorporating the target image into editable regions of the latent space. However, this fine-tuning approach is not practical for widespread use as it requires lengthy training for each new image. To address this, we introduce HyperStyle, a hypernetwork that modulates StyleGAN's weights to faithfully represent a given image in editable regions of the latent space. By carefully designing the network, we reduce the number of parameters to be in line with existing encoders, while achieving reconstructions comparable to optimization techniques in near real-time. Additionally, we demonstrate HyperStyle's effectiveness in various applications beyond image inversion, including editing out-of-domain images that were not part of the training set. The code for HyperStyle is available on our project page: https://yuval-alaluf.github.io/hyperstyle/.