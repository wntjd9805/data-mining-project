Fine-tuning pre-trained models for downstream tasks is a common practice in deep learning. However, these models are typically limited to being fine-tuned using data from a specific modality. For example, visual models like DenseNet cannot directly process textual data as input. This limitation prevents large pre-trained models like DenseNet or BERT from effectively leveraging multimodal information, which is an emerging trend in deep learning. This study focuses on addressing this limitation by proposing a method called Multimodal Information Injection Plug-in (MI2P). This plug-in is attached to different layers of unimodal models such as DenseNet and BERT. MI2P enables the integration of information from other modalities into these unimodal models. Specifically, MI2P performs cross-modal feature transformation by learning the detailed correlations between visual and textual features. With the proposed MI2P unit, language information can be injected into the visual backbone by attending to word-wise textual features across different visual channels. Similarly, visual information can be injected into the language backbone by attending to channel-wise visual features across different textual words. This approach expands the capabilities of pre-trained unimodal models to process multimodal data without requiring changes to the network structures.