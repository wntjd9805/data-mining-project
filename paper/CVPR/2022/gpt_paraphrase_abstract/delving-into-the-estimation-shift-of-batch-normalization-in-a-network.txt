Batch normalization (BN) is a significant technique in deep learning that normalizes activation using mini-batch statistics during training and population statistics during inference. This study focuses on investigating the estimation of population statistics in BN. The authors introduce the concept of estimation shift magnitude to measure the difference between estimated and expected population statistics. They observe that this estimation shift can accumulate due to the stack of BN in a network, leading to detrimental effects on test performance. To address this issue, they propose a batch-free normalization (BFN) technique that prevents the accumulation of estimation shift. Based on this observation, they design XBNBlock, which replaces one BN with BFN in the bottleneck block of residual-style networks. Experimental results on ImageNet and COCO benchmarks demonstrate that XBNBlock consistently improves the performance of various architectures, such as ResNet and ResNeXt, by a significant margin. Additionally, XBNBlock appears to be more robust to distribution shift.