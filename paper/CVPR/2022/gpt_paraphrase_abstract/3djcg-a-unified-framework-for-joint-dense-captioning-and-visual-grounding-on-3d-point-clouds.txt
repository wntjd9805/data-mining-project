This study proposes a unified framework to address the 3D captioning task and the 3D grounding task simultaneously. The framework includes shared task-agnostic modules and lightweight task-specific modules. The shared modules aim to learn precise object locations, attribute features, and complex object relations, benefiting both captioning and visual grounding. The task-specific modules are designed to solve each task by treating it as a proxy task for the other. Experiments on three 3D vision and language datasets demonstrate that the joint training framework significantly improves the performance of both captioning and grounding tasks, surpassing the current state-of-the-art.