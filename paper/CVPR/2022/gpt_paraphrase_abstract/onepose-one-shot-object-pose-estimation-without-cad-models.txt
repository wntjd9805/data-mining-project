We introduce a novel approach called OnePose for estimating the pose of objects. Unlike existing methods, OnePose does not require CAD models and can handle objects from any category without the need for specific network training. Our method leverages the concept of visual localization and constructs a sparse SfM (Structure from Motion) model using a simple RGB video scan of the object. By employing a generic feature matching network, this model is then aligned with new query images. To address the slow runtime of existing visual localization techniques, we propose a graph attention network that directly matches 2D interest points in the query image with the 3D points in the SfM model, resulting in efficient and reliable pose estimation. Additionally, we incorporate a feature-based pose tracker, enabling OnePose to accurately detect and track the 6D poses of everyday household objects in real-time. To evaluate our approach, we have curated a large-scale dataset consisting of 450 sequences involving 150 objects. The project page, located at https://zju3dv.github.io/onepose/, provides access to the code and data.