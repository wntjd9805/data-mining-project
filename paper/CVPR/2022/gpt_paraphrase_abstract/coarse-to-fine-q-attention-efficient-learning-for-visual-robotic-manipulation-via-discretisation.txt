We propose a method for discretizing continuous robotics domains using discrete reinforcement learning. This method, called the coarse-to-fine discretisation method, replaces unstable and data-inefficient actor-critic methods with discrete reinforcement learning approaches. Our approach builds upon the ARM algorithm, which uses a discrete next-best pose agent with coarse-to-fine Q-attention. By learning which parts of a voxelised scene to focus on, the coarse-to-fine Q-attention enables a near-lossless discretisation of the translation space. This allows us to employ a discrete action, deep Q-learning method. Our experimental results demonstrate that our new coarse-to-fine algorithm achieves state-of-the-art performance on challenging sparsely rewarded RLBench vision-based robotics tasks. Furthermore, it can train real-world policies from scratch in just a few minutes and with only a few demonstrations.