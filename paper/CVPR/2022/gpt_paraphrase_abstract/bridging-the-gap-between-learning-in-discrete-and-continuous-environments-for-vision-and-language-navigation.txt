Most existing research in vision-and-language navigation (VLN) focuses on either discrete or continuous environments, training agents that cannot transfer their skills across both types. Training agents to navigate in continuous spaces, which closely resemble real-world environments, is significantly more challenging than training them in discrete spaces. However, recent advancements in discrete VLN are difficult to apply to continuous VLN due to the domain gap between the two setups. Discrete navigation assumes prior knowledge of the environment's connectivity graph, allowing the agent to simplify navigation by jumping from one node to another using high-level actions based on an image of a navigable direction. To bridge the gap between discrete and continuous navigation, we propose a predictor that generates a set of candidate waypoints during navigation. This allows agents designed with high-level actions to be transferred and trained in continuous environments. We refine the connectivity graph of Matterport3D to fit the continuous Habitat-Matterport3D and train the waypoints predictor using the refined graphs to produce accessible waypoints at each time step. Additionally, we demonstrate that augmenting the predicted waypoints during training can enhance the agent's generalization ability by diversifying the views and paths. Through extensive experiments, we show that agents navigating in continuous environments with predicted waypoints outperform agents using low-level actions, reducing the absolute gap between discrete and continuous navigation by 11.76% for the Cross-Modal Matching Agent and 18.24% for the VLNÅ“BERT in terms of Success Weighted by Path Length (SPL). Our agents, trained with a simple imitation learning objective, achieve state-of-the-art results on the testing environments of the R2R-CE and RxR-CE datasets, surpassing previous methods by a significant margin.