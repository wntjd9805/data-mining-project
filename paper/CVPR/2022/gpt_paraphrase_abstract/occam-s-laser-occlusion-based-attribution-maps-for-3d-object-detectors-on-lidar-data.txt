This study focuses on the explainability of 3D object detection models in LiDAR point clouds, an area that has received little attention. The paper proposes a method to generate attribution maps for the detected objects, which can help in understanding the behavior of these models. The attribution maps indicate the importance of each 3D point in predicting specific objects. Importantly, the proposed method works with black-box models, meaning that no prior knowledge of the model's architecture or access to its internals is required. The method uses an efficient perturbation-based approach that estimates the importance of each point by testing the model with randomly generated subsets of the input point cloud. The sub-sampling strategy takes into consideration the unique characteristics of LiDAR data, such as depth-dependent point density. The evaluation of the attribution maps demonstrates their interpretability and high informativeness. Additionally, the paper compares the attribution maps of recent 3D object detection architectures, providing insights into their decision-making processes.