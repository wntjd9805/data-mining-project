The aim of video virtual try-on is to accurately fit clothes to a person in a video while maintaining consistency across frames. While image virtual try-on has made progress, it often leads to inconsistencies in videos. Limited research has been done on video-based virtual try-on, and the results have not been visually pleasing or temporally coherent. Additionally, there are two challenges: accurately warping clothes when occlusions occur and generating clothes and non-target body parts in harmony with the background. To address these challenges, we propose a new framework called ClothFormer, which successfully synthesizes realistic, harmonious, and spatio-temporal consistent results in complex environments. ClothFormer consists of three modules: an anti-occlusion warping module, an appearance-flow tracking module, and a dual-stream transformer. Through experiments, we show that our method outperforms existing methods in terms of video quality.