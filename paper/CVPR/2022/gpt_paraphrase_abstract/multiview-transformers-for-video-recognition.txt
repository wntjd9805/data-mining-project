Video understanding requires reasoning at various spatiotemporal resolutions, ranging from fine-grained motions to longer-term events. While transformer architectures have made significant advancements in this area, they have not explicitly addressed the modeling of different spatiotemporal resolutions. In order to fill this gap, we propose MultiviewTransformers for Video Recognition (MTV). Our approach utilizes separate encoders to represent different views of the input video, and incorporates lateral connections to facilitate information fusion across these views. We conduct comprehensive experiments to evaluate our model, demonstrating that MTV consistently outperforms single-view counterparts in terms of accuracy and computational cost across different model sizes. Additionally, we achieve state-of-the-art performance on six widely-used datasets, and further improve results through large-scale pretraining. For those interested, we provide access to our code and checkpoints at: https://github.com/google-research/scenic.