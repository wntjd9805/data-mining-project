Neural networks trained using SGD have a simplicity bias, where they prefer linearly-predictive features over complex ones, leading to a lack of robustness in out of distribution (OOD) scenarios. This bias is due to the fact that statistical artifacts are often simpler than the mechanisms required to learn complex tasks. However, we have found a way to mitigate this bias and improve OOD generalization. By training a set of similar models with a penalty on the alignment of their input gradients, we encourage the learning of more complex predictive patterns. OOD generalization requires additional information beyond i.i.d. examples, such as multiple training environments or counterfactual examples. Our approach addresses this requirement by deferring it to an independent model selection stage. We have achieved state-of-the-art results in visual recognition on biased data and generalization across visual domains using this method. This is the first approach to evade the simplicity bias, emphasizing the need for a better understanding and control of inductive biases in deep learning.