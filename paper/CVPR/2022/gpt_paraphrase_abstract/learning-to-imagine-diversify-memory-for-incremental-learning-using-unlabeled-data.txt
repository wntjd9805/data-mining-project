Deep neural networks (DNN) face the issue of catastrophic forgetting when learning incrementally, which severely limits their applications. While existing methods attempt to mitigate this problem by storing a few samples called "exemplars" from each task, the limited number of exemplars fails to retain enough task-specific information, resulting in persistent forgetting. To address this limitation, we propose a solution that leverages unlabeled data to "imagine" diverse counterparts of the given exemplars, incorporating abundant semantic-irrelevant information. Our approach involves a learnable feature generator that adaptively generates diverse counterparts of exemplars based on both semantic information from exemplars and semantically-irrelevant information from unlabeled data. We introduce semantic contrastive learning to ensure that the generated samples are semantically consistent with exemplars, and we employ semantic-decoupling contrastive learning to encourage diversity among the generated samples. By effectively preventing forgetting during the learning of new tasks, our method incurs no additional inference cost and outperforms state-of-the-art approaches on two benchmark datasets: CIFAR-100 and ImageNet-Subset.