Explainable models for visual question answering (VQA) have been developed using neural modules and query-based knowledge incorporation. However, existing reasoning methods often fail to generate effective queries or incorporate external knowledge, resulting in suboptimal results. To address this issue, we propose a general approach called Query and Attention Augmentation. This approach enhances neural module networks by jointly reasoning about visual and external knowledge. In order to consider both knowledge sources during reasoning, we parse the input question into a functional program and augment the queries using a novel reinforcement learning method. Additionally, we direct augmented attention to both visual and external knowledge based on intermediate reasoning results. Through extensive experiments on multiple VQA datasets, our method outperforms state-of-the-art models in terms of performance, explainability, and generalizability when answering questions that require different extents of knowledge. The source code for our method is available at https://github.com/SuperJohnZhang/QAA. The answer format outputs only the abstraction.