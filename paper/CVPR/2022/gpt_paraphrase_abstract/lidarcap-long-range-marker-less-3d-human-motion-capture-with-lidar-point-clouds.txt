We introduce LiDARHuman26M, a novel human motion capture dataset captured using LiDAR technology at longer ranges to address the limitations of existing short-range datasets. Our dataset includes ground truth human motions obtained from an IMU system and synchronized RGB images. Additionally, we present a robust baseline method called LiDARCap for human motion capture from LiDAR point cloud data. Our approach utilizes PointNet++ to extract feature representations from points and leverages inverse kinematics solver and SMPL optimizer to estimate the pose by aggregating temporally encoded features hierarchically. Through quantitative and qualitative experiments, we demonstrate that our method outperforms techniques that solely rely on RGB images. Ablation experiments highlight the challenging nature of our dataset, motivating further research. Furthermore, evaluation on the KITTI Dataset and the WaymoOpen Dataset showcases the generalizability of our method across different LiDAR sensor settings.