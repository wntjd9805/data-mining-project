Efficient image captioning is crucial in real-world scenarios, requiring both accuracy and speed. While Transformer-based models have improved captioning performance, they are computationally expensive. One approach to reduce time complexity is to exit prediction early in decoding layers, but this is challenging in image captioning. Shallow layers lack the necessary high-level semantic and cross-modal fusion information for accurate prediction, and internal classifiers may make unreliable exiting decisions. To address these issues, we propose the DeeCap framework for efficient image captioning. It dynamically selects appropriate-sized decoding layers from a global perspective to exit early. The key to successful early exiting is an imitation learning mechanism that predicts deep layer activation using shallow layer features. By integrating imitation learning into the image captioning architecture, the imitated deep layer representation compensates for the missing actual deep layers during early exiting, resulting in a significant reduction in computational cost with minimal loss of accuracy. Experiments on the MS COCO and Flickr30k datasets demonstrate that DeeCap achieves competitive performance with a 4Ã— speed-up. The code for DeeCap is available at: https://github.com/feizc/DeeCap.