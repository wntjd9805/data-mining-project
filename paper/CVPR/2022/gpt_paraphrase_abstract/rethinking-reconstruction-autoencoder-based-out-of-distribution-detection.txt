Reconstruction autoencoder-based methods are effective in detecting out-of-distribution samples that are different from the classifier's training data. These methods use input reconstruction error to determine if a sample is novel or normal. We propose a quadruplet domain translation approach that focuses on measuring conditional data uncertainty. Our approach aims to compress the autoencoder's latent space while maintaining its ability to reconstruct data. To enhance the original methods, we introduce strategies such as semantic reconstruction, data certainty decomposition, and normalized L2 distance. These strategies significantly improve performance on various benchmarks, achieving a FPR@95%TPR of 0.2% on CIFAR-100 vs. TinyImagenet-crop with Wide-ResNet. Importantly, our method does not require additional data, complex structures, lengthy processes, or compromise the classification accuracy of known classes.