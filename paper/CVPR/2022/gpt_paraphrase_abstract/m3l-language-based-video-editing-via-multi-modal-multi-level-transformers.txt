This paper introduces the concept of language-based video editing (LBVE), which aims to improve accessibility to video editing tools by allowing users to edit videos through natural language instructions. LBVE focuses on preserving the scenario of the source video while presenting semantic changes in the target video based on the given instruction. To achieve this, a Multi-Modal Multi-Level Transformer (M3L) is proposed, which dynamically learns the connection between video perception and language semantic at different levels. Three new datasets are created for evaluation, including diagnostic and human-labeled text datasets. Experimental results demonstrate the effectiveness of M3L in video editing and suggest that LBVE can pave the way for further research in the field of vision-and-language.