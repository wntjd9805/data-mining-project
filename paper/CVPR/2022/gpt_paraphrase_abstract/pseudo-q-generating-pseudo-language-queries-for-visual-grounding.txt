Visual grounding, which involves identifying the location of objects in images based on natural language queries, is a crucial aspect of visual language understanding. Current approaches to this task rely on deep learning and require expensive manual annotations of image-query or patch-query pairs. To address this issue, we propose a new method called Pseudo-Q, which automatically generates pseudo language queries for supervised training. Our method uses a pre-trained object detector to identify visual objects in unlabeled images and then generates language queries for these objects in an unsupervised manner using a pseudo-query generation module. We also introduce a task-related query prompt module that specifically tailors the generated pseudo language queries for visual grounding tasks. Additionally, we develop a visual-language model with a multi-level cross-modality attention mechanism to capture the contextual relationships between images and language queries. Extensive experiments demonstrate two significant benefits of our method: (1) it significantly reduces the cost of human annotation, achieving a 31% reduction on the RefCOCO dataset without compromising the performance of the fully supervised approach, and (2) it achieves superior or comparable performance compared to state-of-the-art weakly-supervised visual grounding methods on all five datasets we experimented with. The code for our method is available at https://github.com/LeapLabTHU/Pseudo-Q.