Recent advancements in self-supervised learning have led to the development of effective techniques for image representation learning. However, these techniques have mostly focused on learning at the image level and have not been beneficial for tasks such as unsupervised image segmentation, which require diverse spatial representations. The challenge lies in learning dense representations in an unsupervised context, as it is unclear how to guide the model to learn representations that correspond to different object categories. In this paper, we propose that self-supervised learning of object parts can address this issue. Object parts are independent of specific object definitions but can be combined to form objects. To achieve this, we leverage the capabilities of Vision Transformer, which can attend to objects, and combine it with a task of spatially dense clustering to fine-tune the spatial tokens. Our method outperforms the current state-of-the-art on three semantic segmentation benchmarks by a significant margin, demonstrating the versatility of our representations across different object definitions. Furthermore, we extend this approach to fully unsupervised segmentation, where no label information is used even during testing. We demonstrate that a simple method of automatically merging discovered object parts based on community detection leads to significant improvements in performance.