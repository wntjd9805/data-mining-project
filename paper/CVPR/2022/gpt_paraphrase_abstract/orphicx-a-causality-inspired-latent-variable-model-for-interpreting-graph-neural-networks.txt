This paper introduces a novel framework called OrphicX, which aims to generate causal explanations for graph neural networks (GNNs) by leveraging learned latent causal factors. The framework consists of a distinct generative model and an objective function that encourages the model to produce explanations that are causal, concise, and accurate. The proposed approach achieves this by isolating the causal factors in the latent space of graphs through maximizing information flow measurements. The paper also provides a theoretical analysis of the cause-effect relationships in the causal graph, identifies node attributes as confounders in GNN predictions, and addresses this confounding effect using the backdoor adjustment formula. Importantly, OrphicX is compatible with any GNNs and does not require knowledge of the prediction process used by the target GNN. It also does not rely on the linear-independence assumption of the explained features or prior knowledge of the graph learning tasks. The authors demonstrate the effectiveness of OrphicX through a proof-of-concept on classification problems using graph data, specifically analyzing explanatory subgraphs generated for molecular graphs (Mutag) and evaluating the explanation performance based on frequently occurring subgraph patterns. The empirical results show that OrphicX outperforms alternative methods in effectively identifying causal semantics for generating causal explanations.