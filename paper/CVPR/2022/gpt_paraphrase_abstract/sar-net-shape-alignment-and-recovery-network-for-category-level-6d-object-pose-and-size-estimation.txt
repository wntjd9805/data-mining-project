This paper introduces a method called Category-level 6D Object Pose and Size Estimation (COPSE) for estimating the pose and size of an object from a single scene image. The method does not require external pose-annotated training data and relies on shape information primarily from the depth channel. The main idea is to align the shape of each object instance with its corresponding category-level template shape and utilize symmetric correspondence to estimate a coarse 3D object shape. The point cloud of the category-level template shape is deformed to align with the observed instance point cloud, representing its 3D rotation. Symmetric point cloud prediction is used to model the symmetric correspondence. By combining the observed and symmetric point clouds, a coarse object shape is reconstructed, enabling estimation of the object center and size in 3D. Extensive experiments on the category-level NOCS benchmark demonstrate that our lightweight model performs competitively compared to state-of-the-art approaches that require labeled real-world images. We also validate the effectiveness of our model by deploying it on a physical Baxter robot for grasping tasks on unseen instances with known categories. The code and pre-trained models are available on the project webpage.