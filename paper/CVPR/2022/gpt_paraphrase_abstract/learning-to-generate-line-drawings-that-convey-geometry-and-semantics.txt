This paper presents a novel approach to generate line drawings from photographs without relying on paired datasets. Existing methods often require high quality paired datasets, which have limitations in terms of subject domain and data collection. Although unsupervised image-to-image translation techniques have made progress, they still struggle to generate convincing line drawings. To address this, we propose incorporating the understanding that line drawings encode scene information, conveying 3D shape and semantic meaning. We formulate a set of objectives and train an image translation model to convert photographs into line drawings. Our approach includes a geometry loss that predicts depth from line drawing features and a semantic loss that matches the features of a line drawing with its corresponding photograph using CLIP features. Our method outperforms current state-of-the-art methods for unpaired image translation and line drawing generation when applied to various photographs.