Abstract
In this paper we present VDTTS, a Visually-Driven Text-to-Speech model. Motivated by dubbing, VDTTS takes ad-vantage of video frames as an additional input alongside text, and generates speech that matches the video signal.
We demonstrate how this allows VDTTS to, unlike plain TTS models, generate speech that not only has prosodic varia-tions like natural pauses and pitch, but is also synchronized to the input video.
Experimentally, we show our model produces well-synchronized outputs, approaching the video-speech syn-chronization quality of the ground-truth, on several chal-lenging benchmarks including “in-the-wild” content from
VoxCeleb2.
Supplementary demo videos demonstrating video-speech synchronization, robustness to speaker ID swapping, and prosody, presented at the project page.1 1.

Introduction
Post-sync, or dubbing (in the ﬁlm industry), is the pro-cess of re-recording dialogue by the original actor in a con-trolled environment after the ﬁlming process to improve au-dio quality. Sometimes, a replacement actor is used instead of the original actor when a different voice is desired such as Darth Vader’s character in Star Wars [1].
Work in the area of automatic audio-visual dubbing of-ten approaches the problem of generating content with syn-chronized video and speech by (1) applying a text-to-speech (TTS) system to produce audio from text, then (2) modify-ing the frames so that the face matches the audio [2]. The second part of this approach is particularly difﬁcult, as it requires generation of photorealistic video across arbitrary
ﬁlming conditions. 1Project page: http://google-research.github.io/lingvo-lab/vdtts
Figure 1. Given a text and video frames of a speaker, VDTTS generates speech with prosody that matches the video signal.
In contrast, we extend the TTS setting to input not only text, but also facial video frames, producing speech that matches the facial movements of the input video. The re-sult is audio that is not only synchronized to the video but also retains the original prosody, including pauses and pitch changes that can be inferred from the video signal, provid-ing a key piece in producing high-quality dubbed videos.
In this work, we present VDTTS, a visually-driven TTS model. Given text and corresponding video frames of a speaker speaking, our model is trained to generate the cor-responding speech (see Fig. 1). As opposed to standard vi-sual speech recognition models, which focus on the mouth region [3], we provide the full face to avoid potentially excluding information pertinent to the speaker’s delivery.
This gives the model enough information to generate speech which not only matches the video but also recovers aspects of prosody, such as timing and emotion. Despite not being explicitly trained to generate speech that is synchronized to the input video, the learned model still does so.
Our model is comprised of four main components. Text and video encoders process the inputs, followed by a multi-source attention mechanism that connects these to a decoder that produces mel-spectrograms. A vocoder then produces waveforms from the mel-spectrograms.
We evaluate the performance of our method on GRID [4]
as well as on challenging in-the-wild videos from Vox-Celeb2 [5]. To validate our design choices and training pro-cess, we also present an ablation study of key components of our method, model architecture, and training procedure.
Demo videos are available on the project page,1 demonstrating video-speech synchronization, robustness to speaker ID swapping, and prosody. We encourage readers to take a look.
Our main contributions are that we:
• present and evaluate a novel visual TTS model, trained on a wide variety of open-domain YouTube videos;
• show it achieves state-of-the-art video-speech synchro-nization on GRID and VoxCeleb2 when presented with arbitrary unseen speakers; and
• demonstrate that our method recovers aspects of prosody such as pauses and pitch while producing nat-ural, human-like speech. 2.