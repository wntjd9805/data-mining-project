Abstract
Unsupervised semantic segmentation aims to discover groupings within and across images that capture object-and view-invariance of a category without external supervi-sion. Grouping naturally has levels of granularity, creating ambiguity in unsupervised segmentation. Existing methods avoid this ambiguity and treat it as a factor outside model-ing, whereas we embrace it and desire hierarchical group-ing consistency for unsupervised segmentation.
We approach unsupervised segmentation as a pixel-wise feature learning problem. Our idea is that a good represen-tation shall reveal not just a particular level of grouping, but any level of grouping in a consistent and predictable man-ner. We enforce spatial consistency of grouping and boot-strap feature learning with co-segmentation among multiple views of the same image, and enforce semantic consistency across the grouping hierarchy with clustering transformers between coarse- and fine-grained features.
We deliver the first data-driven unsupervised hierarchi-cal semantic segmentation method called Hierarchical Seg-ment Grouping (HSG). Capturing visual similarity and sta-tistical co-occurrences, HSG also outperforms existing un-supervised segmentation methods by a large margin on five major object- and scene-centric benchmarks. 1.

Introduction
Semantic segmentation requires figuring out the seman-tic category for each pixel in an image. Learning such a seg-menter from unlabeled data is particularly challenging, as neither pixel groupings nor semantic categories are known.
If pixel groupings are known, semantic segmentation is reduced to an unsupervised image (segment) recognition problem, to which contrast learning methods [9, 20, 59, 62] could apply, on computed segments instead of images.
If semantic categories are known, semantic segmentation is reduced to a weakly supervised segmentation problem with coarse annotations of image-level tags; pixel labeling can be predicted from image classifiers [32, 34]. image
Revisit [56] SegSort [26]
Our HSG
Figure 1. We develop an unsupervised semantic segmentation method by embracing the ambiguity of grouping granularity and desiring hierarchical grouping consistency for unsupervised seg-mentation. Top: We formulate it as a pixel-wise feature learning problem, such that a good feature must be able to best reveal any level of grouping in a consistent and predictable manner. We boot-strap feature learning from multiview cosegmentation and enforce grouping consistency with clustering transformers. Bottom: Our method can not only deliver hierarchical semantic segmentation, but also outperform the state-of-the-art unsupervised segmentation methods by a large margin. Shown are sample Cityscapes results.
The fundamental task of unsupervised semantic segmen-tation is grouping, not semantics in terms of naming, which is unimportant other than the convenience of tagging seg-ments in the same or different groups. The challenge of unsupervised semantic segmentation is to discover group-ings within and across images that capture object- and view-invariance of a category without external supervision, so that (Fig. 1): 1) A baby’s face and body are parts of a whole
in the same image; 2) The whole baby is separated from the rest of the image; 3) A baby instance is more similar to another baby instance than to a cat instance, despite their different poses, illuminations, and backgrounds.
Several representative approaches have been proposed for tackling this challenge under different assumptions.
• Visual similarity: SegSort [26] first partitions each im-age into segments based on contour cues and then by segment-wise contrastive learning discovers clusters of visually similar segments. However, semantics by visual similarity is far too restrictive: A semantic whole is of-ten made up of visually dissimilar parts. Parts of body such as head and torso look very different; it is not their visual similarity but their spatial adjacency and statistical co-occurrence that bind them together.
• Spatial stability: IIC [29] maximizes the mutual infor-mation between clusterings from two views of the same image related by a known spatial transformation, enforc-ing stable clustering while assuming that a fixed number of clusters are equally likely within an image. It works best for coarse and balanced texture segmentation and has major trouble scaling up with the scene complexity.
• Image-wise feature learning: [56, 60] train representa-tions on object-centric datasets with multiscale cropping to sharpen the representation within the image. These methods do not work well on scene-centric datasets where an image has more than one dominant semantic class.
Grouping as well as semantics naturally have different levels of granularity: A hand is an articulated configuration of a palm and five fingers, likewise a person of a head, a torso, two arms, and two legs. Such an inherent grouping hierarchy poses a major challenge: Which level should an unsupervised segmentation method target at and what is the basis for such a determination? Existing methods avoid this ambiguity and treat it as either a factor outside the segmen-tation modeling, or an aspect of secondary concern.
Our key insight is that the inherent hierarchical organi-zation of visual scenes is not a nuisance for scene parsing, but a universal property that we can exploit and desire for unsupervised segmentation. This idea has previously led to a general image segmenter that handles texture and il-lusory contours through edges entirely without any explicit characterization of texture or curvilinearity [65]. We now advance the concept to data-driven representation learning:
A good representation shall reveal not just a particular level of grouping, but any level of grouping in a consistent and predictable manner across different levels of granularity.
We approach unsupervised semantic segmentation as an unsupervised pixel-wise feature learning problem. Our ob-jective is to best produce a consistent hierarchical segmen-tation for each image in the entire dataset based entirely on hierarchical clusterings in the feature space (Fig. 1). Specif-ically, given the pixel-wise feature, we perform hierarchical groupings within and across images and their transformed versions (i.e.,views). In turn, groupings at each level impose a desire on how the feature should be improved to maximize the discrimination among different groups.
Our model has two novel technical components: 1)
Multiview cosegmentation is to not only enforce spatial consistency between segmentations across views, but also bootstrap feature learning from visual similarity and co-occurrences in a simpler clean setting; 2) Clustering trans-formers are used to enforce semantic consistency across different levels of the feature grouping hierarchy.
To summarize, our work makes three contributions. 1. We deliver the first unsupervised hierarchical seman-tic segmentation method that can produce parts and wholes in a data-driven manner from an arbitrary collec-tion of images, whether they come from object-centric or scene-centric datasets. 2. We are the first to embrace the ambiguity of grouping granularity and exploit the inherent grouping hierarchy of visual scenes to learn a pixel-wise feature representa-tion for unsupervised segmentation. It can thus discover semantics based on not only visual similarity but also statistical co-occurrences. 3. We outperform existing unsupervised (hierarchical) semantic segmentation methods by a large margin on not only object-centric but also scene-centric datasets. 2.