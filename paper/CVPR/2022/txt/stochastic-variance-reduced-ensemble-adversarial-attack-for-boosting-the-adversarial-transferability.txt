Abstract
The black-box adversarial attack has attracted impres-sive attention for its practical use in the field of deep learn-ing security. Meanwhile, it is very challenging as there is no access to the network architecture or internal weights of the target model. Based on the hypothesis that if an ex-ample remains adversarial for multiple models, then it is more likely to transfer the attack capability to other mod-els, the ensemble-based adversarial attack methods are effi-cient and widely used for black-box attacks. However, ways of ensemble attack are rather less investigated, and exist-ing ensemble attacks simply fuse the outputs of all the mod-els evenly. In this work, we treat the iterative ensemble at-tack as a stochastic gradient descent optimization process, in which the variance of the gradients on different models may lead to poor local optima. To this end, we propose a novel attack method called the stochastic variance reduced ensemble (SVRE) attack, which could reduce the gradient variance of the ensemble models and take full advantage of the ensemble attack. Empirical results on the standard Ima-geNet dataset demonstrate that the proposed method could boost the adversarial transferability and outperforms ex-isting ensemble attacks significantly. Code is available at https://github.com/JHL-HUST/SVRE. 1.

Introduction
Deep neural networks (DNNs) have shown impressive performance on various computer vision tasks. However, recent researches have shown that DNNs are strikingly vul-nerable to adversarial examples crafted by adding human-imperceptible perturbations [7, 23, 28]. Moreover, adversar-ial examples are known to be transferable that the exam-ples crafted for one model can also mislead other black-box models [17, 20, 22]. Generating adversarial examples, i.e.,
*The first two authors contribute equally.
†Corresponding author. adversarial attack, has drawn enormous attention since it can help evaluate the robustness of different models [2, 29] and improve their robustness by adversarial training [7, 19].
Various adversarial attack methods have been pro-posed, including the optimization-based methods such as box-constrained L-BFGS [28] and Carlini & Wagner’s method [2], the gradient-based methods such as Fast Gradi-ent Sign Method [7] and its iterative variants [13,19], etc. In general, these adversarial attack methods can achieve high attack success rates in the white-box setting [2], where the attacker can access the complete information of the target model, including the model architecture and gradient infor-mation. However, these methods often exhibit low attack success rates in the black-box setting [3], where the attacker can not access the information of the target model. In this case, the attacker either utilizes the transferability of adver-sarial examples to fool the black-box model, or attacks di-rectly based on a small amount of queries on the output of the black-box model.
In recent years, a number of methods have been proposed to enhance the transferability of adversarial examples so as to improve the attack success rates in the black-box setting, including the gradient optimization attacks [3, 16, 31], in-put transformation attacks [4, 16, 35], and model ensemble attacks [3, 17]. Among these methods, the model ensemble attacks are efficient and have been broadly adopted in boost-ing the black-box attack performance [5, 16, 35]. However, as compared to the other two categories that have been ex-plored in depth, the category of model ensemble attack is rather less investigated.
In this work, we observe that the existing model ensem-ble attack methods simply fuse the outputs of all models directly but ignore the variance of the gradients on differ-ent models, which may limit the potential capability of the model ensemble attacks. Due to the inherent difference of the model architectures, the optimization paths of the mod-els may differ widely, indicating that there exists consid-erable difference on the variance of the gradient directions among the possible models. Such variance may cause the
optimization direction of the ensemble attack to be less ac-curate. As a result, the attack capability of the transferred adversarial examples is rather limited.
To address this issue, we propose a novel method called the stochastic variance reduced ensemble (SVRE) attack to enhance the adversarial transferability of ensemble attacks.
Our method is inspired by the stochastic variance reduced gradient (SVRG) method [12] designed for stochastic opti-mization, which has an outer loop that maintains an average gradient on a batch of data and an inner loop that randomly draws an instance from the batch and calculates an unbi-ased estimate of gradient based on the variance reduction.
In our method, we regard the ensemble models as the batch of data for the outer loop and randomly draw a model at each iteration of the inner loop. Taking the benign image as the initial adversarial example, the outer loop calculates the average gradient on the batch of models, and copies the current example to the inner loop, then the inner loop con-ducts multiple iterations of inner adversarial example up-dates. At each inner iteration, SVRE calculates the current gradient on a randomly picked model, tuned by the gradi-ent bias of the outer adversarial example on this randomly picked model and on the ensemble model. At the end of the inner loop, the outer adversarial example is updated using the tuned gradient of the newest inner adversarial example.
In this way, SVRE can obtain a more accurate gradi-ent update at the outer loop to escape from poor local op-tima such that the crafted adversarial example would not
“overfit” the ensemble model. Hence, the crafted adver-sarial example is expected to have higher transferability to other unknown models. To our knowledge, this is the first work to investigate the limitation of existing ensemble at-tack through the lens of gradient variance on multiple mod-els. Extensive experiments on the ImageNet dataset demon-strate that SVRE consistently outperforms the vanilla en-semble model attack in the black-box setting. 2.