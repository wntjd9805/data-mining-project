Abstract
Establishing correspondences between images remains a challenging task, especially under large appearance changes due to different viewpoints or intra-class varia-tions. In this work, we introduce a strong semantic image matching learner, dubbed TransforMatcher, which builds on the success of transformer networks in vision domains. Un-like existing convolution- or attention-based schemes for correspondence, TransforMatcher performs global match-to-match attention for precise match localization and dy-namic reﬁnement. To handle a large number of matches in a dense correlation map, we develop a light-weight attention architecture to consider the global match-to-match interac-tions. We also propose to utilize a multi-channel correla-tion map for reﬁnement, treating the multi-level scores as features instead of a single score to fully exploit the richer layer-wise semantics. In experiments, TransforMatcher sets a new state of the art on SPair-71k while performing on par with existing SOTA methods on the PF-PASCAL dataset. 1.

Introduction
Establishing correspondences between images is a fun-damental task in computer vision, and is used for a wide range of problems including 3D reconstruction, visual lo-calization and object recognition [11]. With the recent advances of deep neural networks, many learning-based keypoint extractors and feature descriptors were intro-duced [7,10,41,51,53], showing signiﬁcantly improved per-formances over their traditional counterparts [1, 6, 32, 33].
More recently, dense feature matching methods - which use all extracted features for matching - have shown impres-sive performances despite higher computation complexi-ties [29, 34, 45]. However, establishing reliable correspon-dences between images under the presence of intra-class variations i.e., different instances of the same category, re-mains a critical challenge for semantic visual correspon-dence [3, 12–14, 16–18, 20, 30, 30, 34, 36, 38, 42, 43, 45, 53].
The idea of applying high-dimensional convolutional
Figure 1. Patch-to-patch vs. Match-to-match attention. Patch-to-patch attention considers each position in a 2D feature map as an individual element, while match-to-match attention considers every match in pair-wise correlations as an individual element. layers on the 4D feature correlation map was ﬁrst proposed in NCNet [45], which proposes that unique matches will support the nearby ambiguous matches. Among the vari-ous methods proposed for establishing semantic correspon-dences, NCNet and its follow-up methods have shown im-pressive results [16, 27, 34, 44, 45]. These methods evidence that considering the match-to-match consensus by utilizing the full set of dense correspondences represented by the 4D correlation map is effective in establishing robust and accu-rate semantic correspondences. However, the convolution-based methods suffer from inherent limitations of local and static transformations; performing the same local transfor-mation over all spatial positions of the input.
While convolutional neural networks have been the de-facto standard for visual correspondence, transformer net-works have recently shown promising results in the com-puter vision domain. The success of transformer networks can be largely attributed to their dynamic feature transform unlike stationary convolutional layers, and the non-local in-teractions between input elements which enable easy scal-Figure 2. Conceptual difference between recent methods and ours. Convolution-based matching methods [16, 34, 35, 45] (left),
Cost Aggregation Transformers [3] (middle), and ours (right). ability to attend to global contexts. For example,ViT [9] attains excellent results compared to convolutional base-lines on the task of image recognition with fewer train-ing computational resources; Segmenter [47] outperforms convolution-based methods by modeling global context al-ready at the ﬁrst layer and throughout the network. These pioneering work show that transformer layers are attractive alternatives to convolutional layers in vision models.
Inspired by the effectiveness of match-to-match con-sensus consideration and transformer networks, we pro-pose a novel semantic matching pipeline, dubbed Trans-forMatcher. Speciﬁcally, we introduce match-to-match at-tention, a self-attention based mechanism to consider the global match-to-match interactions by leveraging the 4D correlation maps computed from features of images to match. Considering the global match-wise interactions allows to capture long-range relevance across matches, and incorporates geometric consistency between distant matches in a dynamic manner especially under challeng-ing appearance variations. This is achieved by considering each spatial entry of the 4D correlation map (i.e. a match) as an individual element for attention, which differs from
LoFTR [49] or CoTR [19] which consider the patch-to-patch relations within or across 2D feature maps through self- or cross-attention. Figure 1 visualizes the comparison between patch-to-patch and match-to-match attention.
Our contributions can be summarized as follows:
• We propose the TransforMatcher, a novel image matching pipeline built on transformer networks for dynamic match-to-match interactions at a global scale,
• To the best of our knowledge, we are the ﬁrst to model the global interactions between the full set of dense correspondences using a self-attention mecha-nism within feasible computational constraints,
• We leverage multi-level correlation scores to be used as features, improving over using a single score,
• We demonstrate state-of-the-art or on-par perfor-mances on standard benchmarks of category-level matching - SPair-71k and PF-PASCAL. 2.