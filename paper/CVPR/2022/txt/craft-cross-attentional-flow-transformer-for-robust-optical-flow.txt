Abstract
Sintel (Clean)
Sintel (Final)
Optical ﬂow estimation aims to ﬁnd the 2D motion ﬁeld by identifying corresponding pixels between two images.
Despite the tremendous progress of deep learning-based optical ﬂow methods, it remains a challenge to accurately estimate large displacements with motion blur. This is mainly because the correlation volume, the basis of pixel matching, is computed as the dot product of the convolu-tional features of the two images. The locality of convo-lutional features makes the computed correlations suscep-tible to various noises. On large displacements with mo-tion blur, noisy correlations could cause severe errors in the estimated ﬂow. To overcome this challenge, we pro-pose a new architecture “CRoss-Attentional Flow Trans-former” (CRAFT), aiming to revitalize the correlation vol-ume computation. In CRAFT, a Semantic Smoothing Trans-former layer transforms the features of one frame, mak-ing them more global and semantically stable.
In addi-tion, the dot-product correlations are replaced with trans-former Cross-Frame Attention. This layer ﬁlters out feature noises through the Query and Key projections, and com-putes more accurate correlations. On Sintel (Final) and
KITTI (foreground) benchmarks, CRAFT has achieved new state-of-the-art performance. Moreover, to test the robust-ness of different models on large motions, we designed an image shifting attack that shifts input images to generate large artiﬁcial motions. Under this attack, CRAFT per-forms much more robustly than two representative meth-ods, RAFT and GMA. The code of CRAFT is is available at https://github.com/askerlee/craft. 1.

Introduction
Optical ﬂow estimates pixel-wise 2D motions between two consecutive video frames by matching corresponding
*Equal contribution.
Frame 1
Frame 2
RAFT
GMA
CRAFT
Figure 1. The optical ﬂow ﬁelds estimated by RAFT, GMA and
CRAFT on two frames from Sintel test set, in which a dragon is chasing a chicken. On the Clean pass, all the three methods per-form similarly. On the Final pass, as the area enclosed in the red rectangle has large motions (80 ∼ 100 pixels) with motion blur,
RAFT and GMA only identiﬁed part of the motions. Nonetheless,
CRAFT still performs well. pixels. It is a fundamental computer vision task with broad applications in action recognition [31, 34, 37], video seg-mentation [43, 45], video frame interpolation [17], medical image registration [28], representation learning [10,41], au-tonomous driving [26], and robot navigation [5].
In recent years, deep learning based methods have ad-vanced optical ﬂow estimation tremendously [7, 13, 18, 30, 36, 38, 42, 47]. Although newest methods are very accurate on benchmark data, under certain conditions, such as large displacements with motion blur [9], ﬂow errors could still be large. It spurs us to dig deeper to identify the root causes.
Most of these methods perform optical ﬂow estimation based on a correlation volume (also known as a cost vol-ume), which stores the pairwise similarity between each pixel in Frame 1 and another in Frame 2. Given the cor-relation volume, subsequent modules try to match the two images, with an aim of maximizing the overall correlations between matched regions. The current paradigm computes the pairwise pixel similarity as the dot product of two con-volutional feature vectors. Due to the locality and rigid weights of convolution, limited contextual information is incorporated into pixel features, and the computed correla-tions suffer from a high level of randomness, such that most of the high correlation values are spurious matches (Figure 6). Noises in the correlations increase with noises in the in-put images, such as loss of texture, lighting variations and motion blur. Naturally, noisy correlations may lead to un-successful image matching and inaccurate output ﬂow (Fig-ure 1). This problem becomes more prominent when there are large displacements. Reducing noisy correlations can lead to substantial improvements of ﬂow estimation [11,46].
Recent years have witnessed the widespread adoption of transformers for computer vision tasks [4, 6]. An impor-tant advantage of Vision Transformers (ViTs) over convolu-tion is that, transformer features better encode global con-text, by attending to pixels with dynamic weights based on their contents. For the optical ﬂow task, useful informa-tion can propagate from clear areas to blurry areas, or from non-occluded areas to occluded areas [18], to improve the
ﬂow estimation of the latter. A recent study [29] suggests that, ViTs are low-pass ﬁlters that do spatial smoothing of feature maps.
Intuitively, after transformer self-attention, similar feature vectors take weighted sums of each other, smoothing out irregularities and high-frequency noises.
Inspired by the feature denoising property of ViTs, we propose “CRoss-Attentional Flow Transformer” (CRAFT), a novel architecture for optical ﬂow estimation. With two novel components, CRAFT revitalizes the computation of the correlation volume. First, a semantic smoothing trans-former layer fuses the features of one image, making them more global and semantically smoother. Second, a cross-frame attention layer replaces the dot-product operator for correlation computation. It provides an additional level of feature ﬁltering through the Query and Key projections, so that the computed correlations are more accurate.
We performed extensive evaluations of CRAFT on com-mon optical ﬂow benchmarks. On Sintel (Final) and KITTI (foreground) benchmarks, CRAFT has achieved new state-of-the-art (SOTA) performance. In addition, to test the ro-bustness of different models on large motions, we designed an image shifting attack that shifts input images to generate large artiﬁcial motions. As the motion magnitude increases,
CRAFT performs robustly, while two representative meth-ods, RAFT and GMA, deteriorate severely. 2.