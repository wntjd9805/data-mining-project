Abstract
Today’s VidSGG models are all proposal-based methods, i.e., they ﬁrst generate numerous paired subject-object snip-pets as proposals, and then conduct predicate classiﬁcation for each proposal. In this paper, we argue that this prevalent proposal-based framework has three inherent drawbacks: 1) The ground-truth predicate labels for proposals are par-tially correct. 2) They break the high-order relations among different predicate instances of a same subject-object pair. 3) VidSGG performance is upper-bounded by the quality of the proposals. To this end, we propose a new classiﬁcation-then-grounding framework for VidSGG, which can avoid all the three overlooked drawbacks. Meanwhile, under this framework, we reformulate the video scene graphs as tem-poral bipartite graphs, where the entities and predicates are two types of nodes with time slots, and the edges denote dif-ferent semantic roles between these nodes. This formulation takes full advantage of our new framework. Accordingly, we further propose a novel BIpartite Graph based SGG model:
BIG. It consists of a classiﬁcation stage and a grounding stage, where the former aims to classify the categories of all the nodes and the edges, and the latter tries to localize the temporal location of each relation instance. Extensive ablations on two VidSGG datasets have attested to the ef-fectiveness of our framework and BIG. Code is available at https://github.com/Dawn-LX/VidSGG-BIG. 1.

Introduction
To bridge the gap between vision and other modalities (e.g., language), a surge of interests in our community start to convert the vision data into graph-structured representa-tions, called scene graphs [20]. Scene graphs are visually-grounded graphs, where the nodes and edges represent ob-ject instances (or entities) and their pairwise visual relations (predicates), respectively. Due to the inherent interpretabil-∗Long Chen is the corresponding author. This work started when LC at
Zhejiang University, and YN at Nanyang Technological University.
Figure 1. (a): The pipeline of proposal-based framework. Given a video, it ﬁrst generates numerous proposals (with different time slots), and then conducts predicate classiﬁcation for each proposal. (b): The pipeline of classiﬁcation-then-grounding framework. It
ﬁrst conducts predicate classiﬁcation based on the whole tracklet pair, and then grounds all the predicted relation instances. ity, scene graphs have been widely used in numerous down-stream tasks to help boost model performance, e.g., caption-ing [6, 10, 47], grounding [8, 23], and QA [9, 11, 19, 26].
Video Scene Graph Generation (VidSGG) has achieved signiﬁcant progress over the recent years. Currently, almost all existing VidSGG models are proposal-based1. Speciﬁ-cally, they can be categorized into two groups: 1) Segment-proposal based: They ﬁrst cut the video into short segments and detect object tracklets in each segment to compose seg-ment proposals, then classify predicates in each proposal and merge all predicted relation triplets (i.e., (cid:104)subject, predicate, object(cid:105)) among adjacent segments [28,32, 33]. However, they fail to exploit the long-term context in the video (or tracklets) due to the limits of short segments. 2) Tracklet-proposal based: They directly detect tracklets in 1We use proposals to represent paired subject-object tracklet segments.
the whole video and generate tracklet proposals by sliding-windows [22] or conﬁdence splitting [13], and then conduct predicate classiﬁcation for each proposal.
Although these proposal-based methods have dominated the performance on VidSGG datasets, it is worth noting that this prevalent framework has three inherent drawbacks: 1. The ground-truth predicate labels for proposals are partially correct. By “partially”, we mean that the ground-truth predicate labels sometimes are WRONG. Speciﬁcally, following the IoU-based strategy in object detection, exist-ing proposal-based models all assign predicate labels to pro-posals based on the volume IoU (vIoU). This strategy natu-rally discards some “ground-truth” predicates if their vIoUs are less than the threshold. As shown in Figure 1(a), two re-lations behind and towards happen simultaneously on multiple frames inside both proposala and proposalb, but the assigned predicate label for proposala is only behind (and towards for proposalb)2. Meanwhile, once a predi-cate label is assigned to the proposal, they assume this rela-tion should last for the whole proposal (i.e., it happens in all the frames of the proposal). Obviously, one of the negative impacts of this issue is that the ground-truth labels for two highly-overlapped proposals (proposala/b) may be totally different, and this inconsistency hurts the model training. 2. They break the high-order relations among different predicate instances of a same subject-object pair. Due to the nature of videos, there are always multiple relations hap-pening between a same subject-object pair, and these rela-tions can serve as critical context (or inductive bias) to bene-ﬁt the predictions of other relations. For example, behind, towards, and away always happen sequentially between dog and child. Instead, proposal-based methods explic-itly break these high-order relations by pre-cutting tracklets, and classify predicates independently in each proposal3. 3. VidSGG performance is upper-bounded by the qual-ity of the proposals. The VidSGG performance is sensitive to the heuristic rules for proposal generation (e.g., the sizes or number of proposals). Meanwhile, to achieve higher re-calls, they always generate excessive proposals, which sig-niﬁcantly increases the computation complexity.
In this paper, we propose a classiﬁcation-then-grounding framework for VidSGG, which can avoid all the mentioned drawbacks in proposal-based methods. Speciﬁcally, we ﬁrst conduct predicate classiﬁcation based on the whole track-lets, and then ground each predicted predicate instance (Fig-ure 1(b)). Compared to proposal-based methods, we regard all the relations happen between the two tracklets as ground-truth predicate labels (e.g., behind, towards, away, and in-front-of are all ground-truth predicates for dog and 2For proposala, its vIoU with predicate towards < 0.5 and its vIoU with predicate behind > 0.5. The situation is opposite for proposalb. 3Although a few proposal-based models start to resort to some context modeling techniques to remedy this weakness, we claim that the proposal-based framework itself overlooks and breaks these high-order relations.
Figure 2. Left: A video example and its ground-truth visual rela-tion triplets. Right: The corresponding temporal bipartite graph.
Comparisons with existing formulation are left in appendix. child). Our framework not only provides more accurate ground-truth predicate labels, but also preserves the ability to utilize high-order relations among predicates. Moreover, it avoids superﬂuous proposals and heuristic rules.
Under this framework, we propose to reformulate video scene graphs as temporal bipartite graphs, where the entities and predicates are two types of nodes with time slots, and the edges denote different semantic roles (i.e., subject and object) between these nodes (Figure 2). Each entity node is an object tracklet, and its time slot is the temporal range of this tracklet. Each predicate node is a set of rela-tion instances between two entities with the same predicate category, where each time slot denotes the temporal range of each relation instance (e.g., predicate node towards in
Figure 2 has two time slots). Thus, each entity node can be linked with multiple predicate nodes to represent multiple relations involved, and each predicate node can be linked with at most one entity node for each role. This formulation can not only be easily extended to more general relations with more semantic roles [50], but also avoid exhaustively enumerating all entity pairs for predicate prediction.
Accordingly, we propose a BIpartite Graph based model
BIG, which consists of a classiﬁcation stage and a ground-ing stage. Speciﬁcally, the former aims to classify the cate-gories of all the nodes and edges, and the latter tries to local-ize the temporal location of each relation instance. For the classiﬁcation stage, it is a Transformer-based model, where the inputs for the encoder and decoder are tracklet features and learnable predicate embeddings, respectively. To distin-guish different semantic roles, we also propose a role-aware cross-attention which introduces role-wise distinctions into predicate embeddings. For the grounding stage, we regard the triplet categories of each predicate node as a language query (e.g., (cid:104)dog, towards, child(cid:105) in Figure 2), and ground this language query in the video. Since each relation category may happen multiple times between two tracklets, we design a multi-instance grounding head at this stage.
We evaluate models on two challenging VidSGG bench-marks: VidVRD [32] and VidOR [30]. Extensive ablations and results have demonstrated the effectiveness of our new
classiﬁcation-then-grounding framework and BIG model.
In summary, we make three contributions in this paper: 1. We propose a new classiﬁcation-then-grounding frame-work for VidSGG. It avoids three inherent drawbacks of the existing proposal-based framework. 2. We reformulate video scene graphs as temporal bipartite graphs, and take full advantage of the new framework. 3. We propose a novel model BIG, which achieves state-of-the-art performance on two VidSGG datasets. 2.