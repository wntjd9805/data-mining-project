Abstract
Few-shot learning (FSL) is an important and topical prob-lem in computer vision that has motivated extensive research into numerous methods spanning from sophisticated meta-learning methods to simple transfer learning baselines. We seek to push the limits of a simple-but-effective pipeline for real-world few-shot image classification in practice. To this end, we explore few-shot learning from the perspective of neural architecture, as well as a three stage pipeline of pre-training on external data, meta-training with labelled few-shot tasks, and task-specific fine-tuning on unseen tasks.
We investigate questions such as: 1 How pre-training on external data benefits FSL? 2 How state of the art trans-former architectures can be exploited? and 3 How to best exploit fine-tuning? Ultimately, we show that a sim-ple transformer-based pipeline yields surprisingly good per-formance on standard benchmarks such as Mini-ImageNet,
CIFAR-FS, CDFSL and Meta-Dataset. Our code is available at https://hushell.github.io/pmf. 1.

Introduction
Mainstream supervised deep learning achieves excellent results in applications where huge annotated datasets are available. However, this assumption is not met in many ap-plications where data (e.g., rare categories), or the cost of human annotation are prohibitive bottlenecks. This has moti-vated a large and growing set of research in few-shot learning (FSL), which aims to emulate the human ability to learn new concepts from few training examples. The FSL challenge has proven fertile ground for developing and testing a vast array of sophisticated research ideas spanning metric learn-ing [55, 57], gradient-based meta-learning [29], program induction [40], differentiable optimization layers [41], hy-pernetworks [9], neural optimizers [50], transductive label propagation [51], neural loss learning [4], Bayesian neural priors [66] and more [64]. But how much practical progress
*Equal contributions.
Figure 1. How does pre-training and architecture affect few-shot learning? Learning from a few shots can be achieved by a) meta-learning [62, 66] and b) transfer learning from self-supervised foundation models pre-trained on large-scale external data [18, 49].
While the majority of FSL community focuses on the former, we show that the latter can be more effective because it enables the use of stronger architectures such as vision transformer (ViT) [25] – and can be combined with simple meta-learners such as ProtoNet. The figure shows results aggregated from dozens of studies from the past 5 years of FSL research and the result of ProtoNet + ViT backbone
+ contrastive language-image pretraining (CLIP) [49] (yellow star).
To emphasize the importance of pre-training, ProtoNet + randomly initialized ViT (blue square) is also compared. have we made based on all these technical advances?
A few studies [19, 20, 23, 47, 59, 63] have investigated whether simpler baselines can offer comparable performance to sophisticated state of the art few-shot learners. While there is no conclusive answer, due to on-going developments in both sophisticated learners [66] and simple baselines, there is a trend that simple approaches often perform surprisingly well compared to sophisticated counterparts. Their simplic-ity and efficacy leads these simple methods to be taken up in many practical applications of few-shot learning from
medical data analysis [11] to electronic engineering [39].
We follow this line of enquiry, but go further in inves-tigating previously under-studied factors that influence the performance of simple few-shot pipelines. In particular we start with a simple ProtoNet [55]-like pipeline, and investi-gate three practically important design choices: pre-training data, neural architecture, and fine-tuning in meta-test.
Source data While FSL addresses the small data regime, in reality FSL research is almost always about algorithms to transfer knowledge from large scale source tasks (aka meta-train) to small scale target tasks (aka meta-test). Existing literature almost always controls the source data, in order to carefully compare the impact of different knowledge transfer mechanisms of interest from hyper-networks [9] to gradient-based meta-learners [29]. While this is helpful to drive research on sophisticated algorithms, it does not answer the question of how choice of source data impacts performance?
This question has been studied in other areas of vision and pattern recognition [10, 31, 56], but not for FSL. This is un-helpful for consumers of computer vision FSL research, who would be interested to know how much a simple change of source data can improve their applications? Especially since freely available large datasets already exist [21, 58], and ex-ploiting more external source data is easier in practice than implementing sophisticated state-of-the-art meta-learners.
To this end we investigate the impact of unsupervised pre-training on external data – a workflow recently termed as exploiting a foundation model [10] – on FSL tasks. This small change has substantial impact compared to 5 years of
FSL research (Figure 1). Although this may violate defini-tions of the FSL problem that strictly prescribe the source set, the efficacy of the approach may prompt reflection on whether this is the best problem definition to focus on.
Neural architecture
Similarly to the situation with source data, FSL studies often control neural architecture to a hand-ful of small networks such as CNN-4-64 and ResNet-12.
This is partly to enable fair comparison of FSL algorithms, but this particular suite of networks is also a consequence of the small size of the source datasets used for training in common benchmarks such as miniImageNet. Thus the archi-tectures commonly studied in FSL are somewhat out-of-date with regard to state-of-the-art computer vision. We there-fore ask to what extent state-of-the-art architectures such as vision transformers [25] can benefit few-shot performance, especially in conjunction with larger pre-training datasets?
Fine-tuning
The many studies in the FSL literature are somewhat divided in whether they advocate [29,50,61] some kind of fine-tuning during model deployment (aka meta-test) for individual tasks, or whether a fixed feature representa-tion should be sufficient [41, 55, 63]. We also investigate this issue, and suggest that fine-tuning is necessary for de-ploying foundation models to out-of-distribution tasks. We also introduce an algorithmic improvement to fine-tuning by
Figure 2. Overview – A schematic of the simple pre-training, meta-training, fine-tuning pipeline that we consider. Following the red arrows, the pipeline turns a pre-trained feature backbone into a task-specific one. automating the learning rate selection via validation, which leads to a more performant pipeline for cross-domain FSL.
In summary, we advance few-shot learning by studying design choices of a simple pipeline [55] (Figure 2), rather than developing new algorithms. We answer questions in-cluding: How does pre-training impact FSL? Can recent transformer architectures be adapted to FSL? and How to best exploit fine-tuning? Based on this analysis we demon-strate a new baseline for FSL that surpasses state-of-the-art performance, while being simple and easy to implement. 2.