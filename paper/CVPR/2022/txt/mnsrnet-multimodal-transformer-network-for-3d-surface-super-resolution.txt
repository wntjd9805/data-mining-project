Abstract
With the rapid development of display technology, it has become an urgent need to obtain realistic 3D surfaces with as high-quality as possible. Due to the unstructured and irregular nature of 3D object data, it is usually difficult to obtain high-quality surface details and geometry tex-tures at a low cost.
In this article, we propose an effec-tive multimodal-driven deep neural network to perform 3D surface super-resolution in 2D normal domain, which is simple, accurate, and robust to the above difficulty. To leverage the multimodal information from different perspec-tives, we jointly consider the texture, depth, and normal modalities to simultaneously restore fine-grained surface details as well as preserve geometry structures. To better utilize the cross-modality information, we explore a two-bridge normal method with a transformer structure for fea-ture alignment, and investigate an affine transform mod-ule for fusing multimodal features. Extensive experimen-tal results on public and our newly constructed photometric stereo dataset demonstrate that the proposed method deliv-ers promising surface geometry details compared with nine competitive schemes. 1.

Introduction
With the increasing improvements of the capability and demand in the sensing and analyzing of real-world objects, more and more 3D vision-based applications require the in-put of high-quality object surface [11, 43]. However, most current 3D acquisition devices do not provide high-quality 3D data. In view of this practical difficulty, it is desirable to develop low-cost computer vision methods to enhance the acquisition quality for 3D data collectors.
Intuitively, the most straightforward way to improve the quality of an acquired 3D surface data is to directly perform
*This work was supported in part by the National Natural Science
Foundation of China (No. 61902251 and No. 61701310), in part by
Natural Science Foundation of Shenzhen City (No. 20200805200145001 and No.
JCYJ20180305124209486), and in part by Natural Science
Foundation of Guangdong Province (No. 2019A1515010961 and No. 2021A1515011877). (Corresponding author: Miaohui Wang)
Figure 1. Illustration of the proposed multimodal transformer framework for 3D surface super-resolution. The texture, depth, and normal modalities are jointly investigated to perform 3D sur-face super-resolution in 2D domain. the up-sampling operation in 3D domain. The existing stud-ies can be classified as voxel-based, point cloud-based, and mesh-based methods according to the representation of a 3D surface. 1) The voxel-based methods [6] have been used in 3D surface processing for many years, which commonly have high requirements for equipment and computation. 2)
Point cloud is the most simple way to represent a 3D object, which has been directly up-sampled [26, 40, 44] based on a special convolutional neural network (CNN) structure [29].
Due to the intrinsic irregularity of point cloud, it is diffi-cult to achieve dense and high-quality 3D surface enhance-ment results. 3) Mesh-based methods, as the most wildly-used 3D representation, have been studied based on mesh subdivision and vertex interpolation [3]. With the devel-opment of deep neural networks, mesh-based CNN struc-tures [12, 14, 32] have inspired several data-driven methods for the up-sampling operation on mesh-based 3D surfaces
[24]. Nevertheless, these traditional schemes can only opti-mize some mathematical properties of the mesh data, while learning-based methods face the problem of large amounts of insufficient data.
Due to the aforementioned difficulties in improving sur-face quality in 3D domain, some preliminary investigations have aimed to enhance the surface quality in 2D domain. By
representing 3D surface in 2D domain using normals and displacements in the field of physical cloth enhancement
[19], the related 3D surface has been indirectly up-sampled through 2D image super-resolution (SR) algorithms [45].
This kind of strategy can avoid a high computational com-plexity, which is also benefited from well developed 2D image SR techniques. However, these existing methods in 2D domain usually only explore a single modality, which is lack of utilizing the multimodal attributes of 3D objects to further improve the performance of up-sampling.
Inspired by the above discussions, we present a multi-modal transformer network for 3D surface super-resolution by jointly considering the texture, depth, and normal modal-ities as shown in Fig. 1. More specifically, the texture, depth, and normal data are obtained from a low-resolution 3D object surface. Then, the texture and depth modalities are firstly aligned by a transformer network to the normal modality, and the related side features are fused into the main SR backbone network. Finally, a fine-grained 3D ob-ject surface is reconstructed by the enhanced normal map.
To sum up, there are three main contributions compared with the previous approaches:
• To better utilize the modality information acquired by camera sensors, we investigate a novel multimodal-driven surface super-resolution network (denoted as
“MNSRNet”) to fuse the texture and depth modalities so as to enhance a 3D object surface in 2D domain.
• To capture the auxiliary modality information more easily, the original texture photographs are divided into hierarchical texture representations in multimodal pre-processing stage (MPS). Further, we design a new cross-modality transformer alignment (cmTA) module to align auxiliary modality information, and explore a cross-modality affine fusion (cmAF) module based on affine transform mechanism to fuse the intermediate features.
• Due to the lack of multimodality training data, we have also established a new photometric stereo dataset1 which consists of 400 objects. Extensive experimental results on public and our newly constructed datasets demonstrate that the proposed method achieves supe-rior performance compared with 9 competitive meth-ods. 2.