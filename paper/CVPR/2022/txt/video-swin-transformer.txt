Abstract
The vision community is witnessing a modeling shift from
CNNs to Transformers, where pure Transformer architec-tures have attained top accuracy on the major video recog-nition benchmarks. These video models are all built on
Transformer layers that globally connect patches across the spatial and temporal dimensions. In this paper, we instead advocate an inductive bias of locality in video Transform-ers, which leads to a better speed-accuracy trade-off com-pared to previous approaches which compute self-attention globally even with spatial-temporal factorization. The lo-cality of the proposed video architecture is realized by adapting the Swin Transformer designed for the image do-main, while continuing to leverage the power of pre-trained image models. Our approach achieves state-of-the-art ac-curacy on a broad range of video recognition benchmarks, including on action recognition (84.9 top-1 accuracy on
Kinetics-400 and 85.9 top-1 accuracy on Kinetics-600 with
∼20× less pre-training data and ∼3× smaller model size) and temporal modeling (69.6 top-1 accuracy on Something-Something v2). 1.

Introduction
Convolution-based backbone architectures have long dominated visual modeling in computer vision [14, 18, 22, 24, 32, 33]. However, a modeling shift is currently un-derway on backbone architectures for image classification, from Convolutional Neural Networks (CNNs) to Trans-formers [8, 28, 34]. This trend began with the introduction of Vision Transformer (ViT) [8, 34], which globally models spatial relationships on non-overlapping image patches with the standard Transformer encoder [38]. The great success of ViT on images has led to investigation of Transformer-based architectures for video-based recognition tasks [1, 3].
Previously for convolutional models, backbone architec-* Equal Contribution. † Equal Advising. The work is done when Ze
Liu, Jia Ning and Yixuan Wei are interns at Microsoft Research Asia. tures for video were adapted from those for images sim-ply by extending the modeling through the temporal axis.
For example, 3D convolution [35] is a direct extension of 2D convolution for joint spatial and temporal modeling at the operator level. As joint spatiotemporal modeling is not economical or easy to optimize, factorization of the spa-tial and temporal domains was proposed to achieve a bet-ter speed-accuracy tradeoff [30, 41]. In the initial attempts at Transformer-based video recognition, a factorization ap-proach is also employed, via a factorized encoder [1] or factorized self-attention [1, 3]. This has been shown to greatly reduce model size without a substantial drop in per-formance.
In this paper, we present a pure-transformer backbone ar-chitecture for video recognition that is found to surpass the factorized models in efficiency. It achieves this by taking advantage of the inherent spatiotemporal locality of videos, in which pixels that are closer to each other in spatiotem-poral distance are more likely to be correlated. Because of this property, full spatiotemporal self-attention can be well-approximated by self-attention computed locally, at a sig-nificant saving in computation and model size.
We implement this approach through a spatiotemporal adaptation of Swin Transformer [28], which was recently introduced as a general-purpose vision backbone for image understanding. Swin Transformer incorporates inductive bias for spatial locality, as well as for hierarchy and trans-lation invariance. Our model, called Video Swin Trans-former, strictly follows the hierarchical structure of the orig-inal Swin Transformer, but extends the scope of local atten-tion computation from only the spatial domain to the spa-tiotemporal domain. As the local attention is computed on non-overlapping windows, the shifted window mechanism of the original Swin Transformer is also reformulated to process spatiotemporal input.
As our architecture is adapted from Swin Transformer, it can readily be initialized with a strong model pre-trained on a large-scale image dataset. With a model pre-trained on
ImageNet-21K, we interestingly find that the learning rate of the backbone architecture needs to be smaller (e.g. 0.1×)
than that of the head, which is randomly initialized. As a result, the backbone forgets the pre-trained parameters and data slowly while fitting the new video input, leading to bet-ter generalization. This observation suggests a direction for further study on how to better utilize pre-trained weights.
The proposed approach shows strong performance on the video recognition tasks of action recognition on Kinetics-400/Kinetics-600 and temporal modeling on Something-Something v2 (abbreviated as SSv2). For video action recognition, its 84.9% top-1 accuracy on Kinetics-400 and 85.9% top-1 accuracy on Kinetics-600 slightly surpasses the previous state-of-the-art results (ViViT [1]) both by
+0.1 points, with a smaller model size (200.0M params for
Swin-L vs. 647.5M params for ViViT-H) and a smaller pre-training dataset (ImageNet-21K vs. JFT-300M). For tempo-ral modeling on SSv2, it obtains 69.6% top-1 accuracy, an improvement of +0.9 points over previous state-of-the-art (MViT [9]). 2.