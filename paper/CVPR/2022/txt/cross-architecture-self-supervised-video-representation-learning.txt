Abstract
In this paper, we present a new cross-architecture con-trastive learning (CACL) framework for self-supervised video representation learning. CACL consists of a 3D CNN and a video transformer which are used in parallel to gen-erate diverse positive pairs for contrastive learning. This allows the model to learn strong representations from such diverse yet meaningful pairs. Furthermore, we introduce a temporal self-supervised learning module able to predict an
Edit distance explicitly between two video sequences in the temporal order. This enables the model to learn a rich tem-poral representation that compensates strongly to the video-level representation learned by the CACL. We evaluate our method on the tasks of video retrieval and action recogni-tion on UCF101 and HMDB51 datasets, where our method achieves excellent performance, surpassing the state-of-the-art methods such as VideoMoCo [23] and MoCo+BE [34] by a large margin. 1.

Introduction
Video representation learning is a fundamental task for video understanding, as it plays an important role on various tasks, such as action recognition [25, 30, 36, 37, 44], video retrieval [39, 43], and video temporal detection [18, 27, 46].
Recent efforts have been devoted to improving its perfor-mance by using deep neural networks in a supervised learn-ing manner, which often requires a large-scale video dataset with very expensive human annotations, such as Sports-1M [1], Kinetics [12], HACS [45], and MultiSports [17].
The large annotation cost inevitably limits the potential of deep networks on learning video representation. Therefore, it is of great importance to improve this task by leveraging unlabeled videos which are easily accessible at a large scale.
Recently, self-supervised learning has made significant progress on learning strong image representation, by con-structing various supervised learning signals from images
*Equal contribution.
†Corresponding author. themselves.
It has also been extended to video domain, where contrastive learning has been widely-applied. For example, in recent works such as [23, 28, 35] contrastive learning was introduced to capture the discrimination be-tween two video instances, which enables it to learn an invariant representation within each video instance. How-ever, the contrastive learning mainly focuses on learning a global spatio-temporal representation of videos in these ap-proaches, while it is difficult to capture meaningful tempo-ral details which often provide important cues for discrimi-nating different video instances, e.g., human actions. There-fore, different from learning image representation, mod-elling temporal information is critical to video representa-tion. In this work, we present a new self-supervised video representation method able to perform both video-level con-trastive learning and temporal modelling simultaneously in a unique framework, as shown in Figure 1.
The supervised signal of learning temporal information can be created by exploring the sequential nature of videos, allowing for performing self-supervised learning. Recent methods, such as pace predictions [11, 41] and playback speeds perception [4, 42], followed this line of research by creating a pretext task that implements self-supervised tem-poral predictions. In this work, we introduce a new self-supervised temporal learning by predicting an approximate
Edit distance between a video (i.e. a sequence of frames) and its temporal shuffle. This allows us to explicitly mea-sure the degree of temporal difference quantitatively in Edit distance, setting it apart from the existing self-supervised methods which are often limited to estimate a rough dif-ference of two videos in the temporal domain. For exam-ple, they often created a pretext task to predict whether two video sequences are in the same pace or playback speeds, but ignore details in such temporal difference.
While most self-supervised contrastive learning methods generate positive pairs using various data augmentations which provide different views of an instance, we develop a new method able to learn stronger representation from di-verse architectures via contrastive learning. The family of
3D CNNs has achieved remarkable performance in various video tasks, including C3D [30], R3D [8], R(2+1)D [31], etc. They are capable of capturing local dependencies in the temporal domain due to the intrinsic property (i.e., convolu-tions) of CNNs. But the effective receptive fields of CNNs might limit their ability to modelling long-range dependen-cies. On the other hand, such long-range dependencies can be naturally captured by the transformer architecture [32] using a self-attention mechanism, where each token is able to learn an attention to the whole sequence, and thus en-codes meaningful context information into video represen-tations. Moreover, the inductive biases of CNNs may limit their performance when trained on sufficiently large data, while this limitation may not happen in the transformers due to the dynamic weighting of self-attention [7].
We argue that modelling both local and global dependen-cies are essential for video understanding; and the inductive biases of CNNs and the capacity of transformers can com-pensate strongly to each other. In this work, we present a new cross-architecture contrastive learning (CACL) frame-work for self-supervised video representation learning. Our
CACL is able to learn from diverse yet more meaning-ful constrastive pairs generated by a 3D CNN and a video transformer. We demonstrate that a video transformer can strongly enhance the video representation generated by a 3D CNN. It produces rich high-level contextual features and encourages the 3D CNN to capture more detailed informa-tion. This allows the two architectures to work collabora-tively, which is the key to boost the performance. Our main contributions can be summarized as follows: – We design a new cross-architecture contrastive learn-ing (CACL) framework for self-supervised video represen-tation learning. CACL uses a 3DCNN and a Transformer to collaboratively generate diverse yet meaningful positive pairs, which allow for more effective contrastive represen-tation learning. – We introduce a new self-supervised temporal learning method by explicitly measuring an edit distance between a video and its temporal self-shuffle. This helps to learn rich temporal information complementary to the representation learned from our CACL. – We verify our method on two downstream video tasks: action recognition and video retrieval. Experimental re-sults on UCF101 [26] and HMDB51 [14] show that the pro-posed CACL can significantly outperform the state-of-the-art methods, such as VideoMoCo [23] and MoCo+BE [34]. 2.