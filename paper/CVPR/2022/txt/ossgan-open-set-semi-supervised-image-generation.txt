Abstract at https://github.com/raven38/OSSGAN .
We introduce a challenging training scheme of condi-tional GANs, called open-set semi-supervised image gen-eration, where the training dataset consists of two parts: (i) labeled data and (ii) unlabeled data with samples belong-ing to one of the labeled data classes, namely, a closed-set, and samples not belonging to any of the labeled data classes, namely, an open-set. Unlike the existing semi-supervised image generation task, where unlabeled data only contain closed-set samples, our task is more general and lowers the data collection cost in practice by allow-ing open-set samples to appear. Thanks to entropy regu-larization, the classiﬁer that is trained on labeled data is able to quantify sample-wise importance to the training of cGAN as conﬁdence, allowing us to use all samples in un-labeled data. We design OSSGAN, which provides decision clues to the discriminator on the basis of whether an unla-beled image belongs to one or none of the classes of inter-est, smoothly integrating labeled and unlabeled data during training. The results of experiments on Tiny ImageNet and
ImageNet show notable improvements over supervised Big-GAN and semi-supervised methods. Our code is available 1.

Introduction
The outstanding performance of the SoTA conditional generative adversarial networks (cGANs) [1,12,23] is heav-ily reliant on having access to a vast amount of labeled data during training (Fig. 1a). This dependence necessitates sig-niﬁcant efforts to label the data and limits the applications of cGANs in real-world scenarios. Reducing the reliance on labeled data in training cGANs is thus deemed necessary.
Semi-supervised image generation [4, 9, 10, 19, 20] al-lows the appearance of both labeled and unlabeled data dur-ing training, with the unlabeled data primarily containing within classes of interest (closed-set samples) (Fig. 1b). De-spite the advances, the unlabeled data assumption is at odds with the fact that the majority of unlabeled data is outside of classes of interest (open-set samples), and ensuring that un-labeled data do not contain open-set samples is often costly and prone to error. In fact, in [4, 9, 10, 19, 20], even open-set samples are classiﬁed into classes that appear in labeled data, resulting in cGAN performance deterioration.
We go beyond semi-supervised image generation by al-lowing the use of unlabeled data gathered miscellaneously to reduce the effort of labeling and introduce a novel task of open-set semi-supervised image generation (Fig. 1c). Un-labeled data contain open-set samples, and the conditional generator should produce images that are indistinguishable from real ones even when trained on both labeled and unla-beled data. Unlike the conventional semi-supervised fash-ion, the task allows unlabeled data to have the category set mismatched with labeled data, reducing the required label-ing effort. Our task is a signiﬁcant step towards real-world data, which contains labeled data and unlabeled data (both closed-set and open-set samples), lowering the data con-struction cost and expanding the range of real-world appli-cations of cGANs.
To address our new task, we design Open-Set Semi-supervised GAN (OSSGAN). We simultaneously train cGAN and a classiﬁer that assigns labels to unlabeled data. By incorporating entropy regularization into the cross-entropy loss, the classiﬁer quantiﬁes the conﬁdence of the prediction to enable the discriminator to use unlabeled data, including open-set samples, smoothly. Consequently, OS-SGAN allows the natural integration of unlabeled data into cGANs without explicitly eliminating open-set samples.
The results of empirical experiments demonstrate that
OSSGAN effectively utilizes unlabeled data, including open-set samples. More importantly, we achieve better per-formance in terms of FID and other metrics against strong supervised and semi-supervised baselines. Notably, our method achieves a performance comparable to that of Big-GAN [1], which has up to ﬁve times as many labeled sam-ples as OSSGAN. Furthermore, the experiments with dif-ferent degrees of an open-set sample ratio show that the proposed method is robust to miscellaneous data. Quali-tative experimental results also reveal the superiority of our method. Our contributions are summarized as follows.
• We propose a novel open-set semi-supervised image generation task, which is based on a relaxed assump-tion in the case of building a dataset at a reasonable cost.
• We design OSSGAN, thanks to entropy regularization, smoothly using closed- and open-set samples in unla-beled data in cGAN training.
• We demonstrate the superiority of the proposed method over baselines on several benchmarks with limited labeled data in terms of quantitative metrics such as FID. Our qualitative experiments also show that our method achieves better generation quality. many types of conditions such as class label [12], text de-scription [24], or another image [7]. For well-constructed datasets, [1, 13, 23] are proposed to improve quality, ﬁ-delity, and training stability. Among them, self-attention
GAN [23] and BigGAN [1] outperform other GANs with hundreds of classes. The progress in network architectures, optimization algorithms, and the quality and quantity of datasets support high-ﬁdelity image generation. We aim to achieve high-ﬁdelity image generation without a well-constructed dataset.
Image generation with data constraints is aimed at im-proving the generation quality without using enormous amounts of data. Collecting a large labeled dataset re-quires a tremendous annotation cost. To achieve better per-formance within limited resources (i.e., time and money), several studies [6, 8, 18, 25] contribute to the data-efﬁcient aspect of cGANs. Another line of studies [2, 10] focus-ing on the fact that unlabeled images are easier to col-lect than labeled images employs semi-supervised or un-supervised fashion. For semi-supervised image generation,
[19, 20] employ an unconditional discriminator for unla-beled data, and [4, 9] take pseudo labels by employing a classiﬁer. Unlabeled data have been efﬁciently utilized in self-supervised learning [10]. A different aspect in the study of semi-supervised learning and GAN concerns semi-supervised recognition tasks that employ GAN for generat-ing pseudo samples [3,17]. Unsupervised image generation frees us from tedious annotation labor. However, unsuper-vised methods do not control generated outcomes as semi-supervised methods do. In this study, we utilize unlabeled data gathered miscellaneously for training cGANs to reduce the data construction cost, which will further broaden their range of application.
Open-set semi-supervised recognition has the same ob-jective as our method but addresses a totally different task.
The goal of the recognition task is to build a model dis-tinguishing open-set samples using a dataset consisting of labeled data with only closed-set classes and unlabeled data with both closed- and open-set classes. The joint optimiza-tion of classiﬁcation and open-set sample detection mod-els [22] and the consistent regularization with data augmen-tation [11] are used to tackle the problem. In contrast to recognition paradigms aimed at separating explicitly open-and closed-set samples, our generation task does not neces-sarily require explicit separation of these samples. Instead of applying a method for detecting open-set samples, we investigate a method of utilizing open-set samples. 3. Open-set semi-supervised image generation 2.