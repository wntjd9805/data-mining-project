Abstract
Self-supervised learning (SSL) has made enormous progress and largely narrowed the gap with the supervised ones, where the representation learning is mainly guided by a projection into an embedding space. During the projec-tion, current methods simply adopt uniform aggregation of pixels for embedding; however, this risks involving object-irrelevant nuisances and spatial misalignment for differ-ent augmentations.
In this paper, we present a new ap-proach, Learning Where to Learn (LEWEL), to adaptively aggregate spatial information of features, so that the pro-jected embeddings could be exactly aligned and thus guide the feature learning better. Concretely, we reinterpret the projection head in SSL as a per-pixel projection and pre-dict a set of spatial alignment maps from the original fea-tures by this weight-sharing projection head. A spectrum of aligned embeddings is thus obtained by aggregating the fea-tures with spatial weighting according to these alignment maps. As a result of this adaptive alignment, we observe substantial improvements on both image-level prediction and dense prediction at the same time: LEWEL improves
MoCov2 [15] by 1.6%/1.3%/0.5%/0.4% points, improves
BYOL [14] by 1.3%/1.3%/0.7%/0.6% points, on ImageNet linear/semi-supervised classification, Pascal VOC semantic segmentation, and object detection, respectively.† 1.

Introduction
In recent years, self-supervised learning (SSL) [7,12,14, 15, 29, 30, 42] has attained tremendous attention due to its impressive ability to learn good representations from large volume of unlabeled data. Among them, the state-of-the-art instance discrimination approaches [7, 14, 15, 30, 42] en-courage the representation learning with image-level invari-ance to a set of random data transformations, e.g., random cropping and color distortions. These methods even ex-hibit superior performance over their supervised counter-*Corresponding author.
†Code: https://t.ly/ZI0A. parts for various downstream tasks, such as object detec-tion [11,25] and semantic segmentation [11]. There remain, however, several important issues unresolved. Two of them are mainly attributed to the rigorous invariance to random cropping because it would risk introducing more irrelevant nuisance (e.g., background information) and spatial mis-alignment of objects for augmentations. Though, for SSL, random cropping might be the most effective data augmen-tation option [7] and a good degree of spatial misalignment is beneficial [38], it remains unclear how to choose the opti-mal degree of misalignment. Furthermore, the involved nui-sance will hinder the discrimination ability of image-level representations while the misalignment discards some im-portant spatial information of objects.
Several recent literatures have dedicated to alleviating these issues by involving some localization priors of down-stream tasks in advance. For example, the works of [40, 44] explored pixel-level consistency between two augmented views, while some other works proposed to match the rep-resentation of a set of pre-defined bounding-boxes [35, 43] or pre-computed masks [19] between the two views. De-spite the improved performance on dense prediction tasks, these methods still suffer from several drawbacks, e.g., they rely on the prior from a specific downstream task and fail to generalize to other tasks. Specifically, there is an undesir-able trend that these methods perform worse on the classifi-cation task than their instance discrimination counterparts, since they are delicatedly tailored for dense predictions and emphasis on the local feature learning.
We argue that a good self-supervised representation learning algorithm should not leverage task-specific priors but learn local representations spontaneously.
In this pa-per, we present a new self-supervised learning approach,
Learning Where to Learn (LEWEL) in a pure end-to-end manner. We first regard the spatial aggregation for embed-dings of existing SSL methods, e.g., by the global aver-age pooling (GAP), as summations over all spatial pixels weighted by a set of alignment maps. This formulation sug-gests that we can explicitly control where to learn in SSL by manipulating the alignment maps. Moreover, in contrast to
head, thus the aligned and global embeddings can be re-ciprocal to each other, and, as a result, boost the learned representations. 3. LEWEL brings substantial improvements over prior arts on both image-level prediction and dense prediction. We perform extensive evaluation on linear/semi-supervised classification, semantic segmentation, and object detec-tion tasks, using ImageNet-1K [36], Pascal VOC [11], and MS-COCO [25] benchmarks. Experimental re-sults suggest that LEWEL is able to improve the strong baselines MoCov2 [15] and BYOL [14] under all set-tings. Specifically, LEWEL improves MoCov2 [15] by 1.6%/1.3%/0.5%/0.4% points, improves BYOL [14] by 1.3%/1.3%/0.7%/0.6% points, on ImageNet linear/semi-supervised classification, Pascal VOC semantic segmen-tation, and object detection, respectively. 2.