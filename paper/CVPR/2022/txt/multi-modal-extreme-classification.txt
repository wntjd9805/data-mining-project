Abstract
This paper develops the MUFIN technique for extreme classification (XC) tasks with millions of labels where data-points and labels are endowed with visual and textual de-scriptors. Applications of MUFIN to product-to-product recommendation and bid query prediction over several mil-lions of products are presented. Contemporary multi-modal methods frequently rely on purely embedding-based meth-ods. On the other hand, XC methods utilize classifier ar-chitectures to offer superior accuracies than embedding-only methods but mostly focus on text-based categorization tasks. MUFIN bridges this gap by reformulating multi-modal categorization as an XC problem with several mil-lions of labels. This presents the twin challenges of devel-oping multi-modal architectures that can offer embeddings sufficiently expressive to allow accurate categorization over millions of labels; and training and inference routines that scale logarithmically in the number of labels. MUFIN de-velops an architecture based on cross-modal attention and trains it in a modular fashion using pre-training and posi-tive and negative mining. A novel product-to-product rec-ommendation dataset MM-AmazonTitles-300K containing over 300K products was curated from publicly available amazon.com listings with each product endowed with a title and multiple images. On the MM-AmazonTitles-300K and
Polyvore datasets, and a dataset with over 4 million labels curated from click logs of the Bing search engine, MUFIN offered at least 3% higher accuracy than leading text-based, image-based and multi-modal techniques.
*Equal contribution. Author names appear in alphabetical order.
Figure 1. Predictions on the MM-AmazonTitles-300K product-to-product recommendation task illustrate the need for accurate multi-modal retrieval. For a decorative motorcycle-shaped alarm clock as the query product, multi-modal retrieval using MUFIN was able to retrieve visually similar products such as a motorcycle-shaped pencil holder as well as visually dissimilar but related prod-ucts such as a motorcycle themed ashtray. Recovery using the vi-sual modality alone ignored thematically linked products, instead recovering mostly motorcycle-shaped products. Textual recovery on the other hand fixated on the word “motorcycle” and started re-covering accessories for actual motorcycles.
1.

Introduction
Extreme Classification (XC). The goal of extreme multi-label classification is to develop architectures to annotate datapoints with the most relevant subset of labels from an extremely large set of labels. For instance, given a prod-uct purchased by a user, we may wish to recommend to the user, the subset (i.e. one or more) of the most related prod-ucts from an extremely large inventory of products. In this example, the purchased product is the datapoint and each product in the inventory becomes a potential label for that datapoint. Note that multi-label classification generalizes multi-class classification where the objective is to predict a single mutually exclusive label for a given datapoint. An ex-ample of a multi-class problem would be to assign a product to a single exclusive category in a product taxonomy.
Multi-modal XC. An interesting XC application arises when datapoints and labels are endowed with both visual and textual descriptors. Example uses cases include (1) Product-to-product recommendation [27] with products being represented using their titles and one or more images. (2) Bid-query prediction [5] where an advertisement with visual and textual descriptions has to be tagged with the list of user queries most likely to lead to a click on that ad. (3) Identifying compatible outfits where each outfit is de-scribed using multiple images and a textual caption [35].
Challenges in Multi-modal XC. Existing multi-modal methods [10, 32, 34, 35] are often embeddings-only i.e. categorization is done entirely using embeddings of dat-apoints and categories obtained from some neural archi-tecture. However, XC research has shown that training classifiers alongside embedding architectures can offer im-proved results [3, 5, 39]. However, existing XC research fo-cuses mostly on text-based categorization. Bridging this gap requires architectures that offer multi-modal embed-dings sufficiently expressive to perform categorization over millions of classes. Also required are routines that can train classifiers over millions of classes and still offer pre-dictions in milliseconds as demanded by real-time applica-tions [3, 5, 12]. This is usually possible only if training and inference scale logarithmically with the number of labels.
Contributions. The MUFIN method targets XC tasks with millions of labels where both datapoints and labels can be endowed with visual and textual descriptors. (1) MUFIN melds a novel embedding architecture and a novel classifier architecture. The former uses multi-modal attention whereas the latter uses datapoint-label cross atten-tion and high-capacity one-vs-all classifiers. (2) MUFIN training scales to tasks with several millions of labels by using pre-training and hard-positive and hard-negative mining. MUFIN offers predictions within 3-4 mil-liseconds per test point even on tasks with millions of labels. (3) This paper releases the MM-AmazonTitles-300K product-to-product recommendation dataset curated from publicly available amazon.com listings with over 300K products each having a title and multiple images. (4) MUFIN offers at least 3% higher accuracy than leading text-only, image-only and multi-modal methods on several tasks (MM-AmazonTitles-300K, A2Q-4M) including zero-shot tasks (Polyvore) indicating the superiority of not just
MUFIN’s classifiers but its embedding model as well. 2.