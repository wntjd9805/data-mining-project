Abstract
Knowledge distillation has shown great effectiveness for improving neural architecture search (NAS). Mutual knowledge distillation (MKD), where a group of mod-els mutually generate knowledge to train each other, has achieved promising results in many applications. In existing
MKD methods, mutual knowledge distillation is performed between models without scrutiny: a worse-performing model is allowed to generate knowledge to train a better-performing model, which may lead to collective failures.
To address this problem, we propose a performance-aware
MKD (PAMKD) approach for NAS, where knowledge gen-erated by model A is allowed to train model B only if the performance of A is better than B. We propose a three-level optimization framework to formulate PAMKD, where three learning stages are performed end-to-end: 1) each model trains an initial model independently; 2) the initial mod-els are evaluated on a validation set and better-performing models generate knowledge to train worse-performing mod-els; 3) architectures are updated by minimizing a validation loss. Experimental results on a variety of datasets demon-strate that our method is effective. 1.

Introduction
Neural architecture search (NAS) [34, 45, 64], which aims to automatically search for high-performance neu-ral architectures, has attracted much research attention re-cently. Many NAS works [17, 26, 29, 35, 49, 57] propose to leverage knowledge distillation (KD) [3, 21, 52] to improve the quality of searched architectures, by transferring knowl-edge from human-designed architectures to auto-searched architectures [29, 35, 57], enabling multi-fidelity evaluation of architectures [49], alleviating model capacity gap [26],
In KD, a teacher model generates knowledge such etc. as pseudo-labels [21] and a student model is trained using these knowledge. Among various studies on KD, mutual
KD [1, 4, 27, 28, 37, 51, 55, 62], where a group of models mutually perform KD (i.e., each model generates pseudo-labels to train other models), has shown promising results.
Mutual KD can help models converge to a more robust min-ima [62], can achieve better generalization to test data [28], can learn multi-scale representations to boost prediction ac-curacy [55], etc.
In existing mutual KD works, knowledge distillation is performed between any pair of models without scrutiny,
If a model A is which may lead to collective failure. not performing well, its generated knowledge is not accu-rate. Trained using these low-quality knowledge, the perfor-mance of the rest models is degraded, which renders their knowledge L to have low-quality as well. Updated using
L, model A becomes worse, which further worsens the rest models. This vicious circle renders all models to fail col-lectively (empirical justification is in Fig. 1).
In this paper, we aim to address this problem, by propos-ing a performance-aware mutual KD approach, for improv-ing NAS. In our method, performance scrutiny is performed before transferring knowledge: a learner A is allowed to generate knowledge to train another learner B only if the performance of A is better than B. By doing this, the risk of collective failure can be greatly reduced (empirical justi-fication is in Fig. 1), because a poorly-performing learner is prohibited from generating knowledge.
Existing mutual KD methods [1, 4, 27, 28, 37, 51, 55, 62] are not amenable for performance scrutiny. In these meth-ods, the same model weights are used for measuring per-formance and are trained at the same time, which will lead to a degenerated solution: all models have the same perfor-mance and no KD will be conducted (empirical justification is in Table 6). To address this problem, we propose to learn two sets of model weights sequentially for each learner, use one set of them for measuring performance and generat-ing knowledge, and then train the other set using gener-ated knowledge. The two sets of weights are learned se-quentially at different stages instead of simultaneously at the same stage, which can avoid the degenerated solution of
existing methods (empirical justification is in Table 6).
Our method is formulated as a three-level optimization problem, consisting of three learning stages performed end-to-end. In the first stage, each learner k independently trains a predictive model Vk. In the second stage, for each pair of learners k and j, their models Vk and Vj are evaluated
If the performance of Vk is bet-on a validation dataset. ter than Vj, then Vk generates knowledge which is used to train another model Wj of learner j.
In the third stage, models trained in the second stage are further validated and their architectures are updated by minimizing valida-tion losses. Chen et al. [4] learn attentional weights to con-trol how much knowledge is allowed to transfer from one model to another. Attentional weights and model param-eters are learned jointly on a training dataset, which may lead to overfitting. In contrast, our method measures perfor-mance (during scrutiny) on a validation set and trains mod-els on a training set, which can greatly reduce the risk of overfitting.
Another problem of existing KD methods [1, 4, 27, 28, 37, 51, 55, 58, 62] is that they mutually transfer knowledge on individual examples [1, 4, 27, 28, 51, 55, 62] or on low-order triplets [58], without considering the higher-order (e.g. ≥ 4) relationship between examples, which there-fore may not be able to capture complex structure of the en-tire dataset (empirical justification is in Fig. 2). To address this problem, we propose a new group-wise relative sim-ilarity (GRS) based approach to transfer knowledge from model j to k, where learner j uses its model to determine which group of data instances have larger mutual similar-ities, and learner k trains its model by fitting these GRS relationships. These relationships capture high-order (≥ 4) nonlinear manifold structure [46] in the dataset, which can facilitate more effective knowledge transfer between learn-ers (empirical justification is in Fig. 2).
The major contributions of this paper are:
• We propose a performance-aware mutual knowledge dis-tillation (PAMKD) method for improving neural architec-ture search. In PAMKD, a model A is allowed to generate knowledge to train another model B only if the perfor-mance of A is better than B, which can address the col-lective failure problem of existing MKD methods. Our framework consists of three learning stages which are performed end-to-end: 1) each learner trains a prelimi-nary model; 2) learners conduct performance-aware mu-tual knowledge distillation; 3) architectures are updated by minimizing validation losses.
• We propose a group-wise relative similarity based knowl-edge transfer approach which can capture high-order re-lationships among data instances.
• Experiments on several datasets show the effectiveness of our method. 2.