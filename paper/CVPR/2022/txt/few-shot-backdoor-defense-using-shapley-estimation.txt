Abstract
Deep neural networks have achieved impressive perfor-mance in a variety of tasks over the last decade, such as au-tonomous driving, face recognition, and medical diagnosis.
However, prior works show that deep neural networks are easily manipulated into specific, attacker-decided behaviors in the inference stage by backdoor attacks which inject ma-licious small hidden triggers into model training, raising serious security threats. To determine the triggered neurons and protect against backdoor attacks, we exploit Shapley value and develop a new approach called Shapley Prun-ing (ShapPruning) that successfully mitigates backdoor at-tacks from models in a data-insufficient situation (1 image per class or even free of data). Considering the interaction between neurons, ShapPruning identifies the few infected neurons (under 1% of all neurons) and manages to protect the model’s structure and accuracy after pruning as many infected neurons as possible. To accelerate ShapPruning, we further propose discarding threshold and ϵ-greedy strat-egy to accelerate Shapley estimation, making it possible to repair poisoned models with only several minutes. Exper-iments demonstrate the effectiveness and robustness of our method against various attacks and tasks compared to ex-isting methods. 1.

Introduction
Over the past years, Deep Neural Networks (DNNs) play a great role in machine learning and are applied in many critical domains such as face recognition [39], image gen-eration [11, 12], autonomous driving [7], and medical diag-nosis [22, 45]. However, because of a lack of transparency and interpretability [21, 27, 44], DNNs are easy to be ma-*This work was done when he was research intern at JDEA
†Corresponding author nipulated by an adversary into attacker-decided behaviors and make serious mistakes in security-related areas, causing serious threats and concerns. For example, it has been ob-served that adding deliberate and small distortion to the im-ages in inference stage(i.e., adversarial examples) can cause misclassification in neural network classifiers [15].
Backdoor attacks, on the other hand, are a different type of attack, making use of opacity and overfitting of DNNs to create a maliciously trained network which achieves state-of-the-art performance on normal samples but behaves badly on specific attacker-chosen inputs. Gu et al. [16] demonstrates that, compared with adversarial examples, backdoor attacks can cause wrong predictions in models with much smaller distortion. Meanwhile, for black-box models like DNNs, it is difficult to identify the backdoor, and we can only use the test dataset to judge whether they are poisoned. Thus, the backdoor attack is more impercep-tible and dangerous [1,8]. Furthermore, as training on cloud or directly using the third-party trained models becomes more common today [47], backdoor attacks have more ac-cess to the models’ training procedure. Thus, it is much easier for them to inject triggers into models in recent years.
The vulnerabilities to backdoor attacks raise concerns about the security of DNNs [24], and many defense meth-ods have been proposed, trying to mitigate backdoor from the models e.g. Fine Pruning [25], Neural Cleanse [41],
GangSweep [48] etc. However, these methods need a rela-tively large amount of clean data (e.g. 10% of training data required in Neural Cleanse), and can’t locate poisoned neu-rons accurately (e.g. pruning 70% of all neurons in Fine
Pruning). To determine the poisoned neurons and mitigate backdoor, we introduce Shapley value and propose a Shap-Pruning framework to guide detecting the attacked neurons, which successfully mitigates backdoor in the given models.
Shapley value is a concept from game theory and is used to allocate worth to cooperative players [2, 13, 36]. We use
Shapley value to attribute the overall backdoor behavior to
Figure 1. Shapley Pruning Framework. Our framework consists of four components, trigger and data reverse, detection, Shapley estimation, backdoor mitigation, and can effectively remove backdoor in models. each neuron and find neurons with the largest Shapley value which are the most responsible for backdoor behavior in models. Compared to prior work, our ShapPruning method can handle the data-insufficient situation and needs only a tiny amount of data (e.g. one image per class or even free of clean data) and prunes a very small number of neurons (about 1% of all the neurons), to maintain a good classifi-cation accuracy (under 1% accuracy decline in most cases) and clean backdoor clearly.
Our contributions are summarized as follows:
• We introduce Shapley value into the backdoor area and propose a backdoor mitigation method called Shapley
Pruning which can locate and prune poisoned neurons accurately with the reversed trigger.
• We also propose discarding threshold and ϵ-greedy to accelerate Shapley value’s estimation, which yields a more accurate estimation with much less time.
• Our method considers the relationship between neu-rons and locates the attacked neurons accurately with few images. As a result, it can prune only 1% of all neurons to recover the model with a small accuracy de-crease (accuracy declines 0.1% in the GTSRB dataset and the attack success rate drops to 0.4%). Moreover, our method is robust in different situations.
• We utilize information in model’s batch normalization layer and propose a data-free backdoor cleanse method with mixture-mode ShapPruning. 2.