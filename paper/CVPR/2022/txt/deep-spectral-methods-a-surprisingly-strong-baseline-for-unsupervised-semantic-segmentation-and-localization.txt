Abstract
Unsupervised localization and segmentation are long-standing computer vision challenges that involve decom-posing an image into semantically meaningful segments without any labeled data. These tasks are particularly in-teresting in an unsupervised setting due to the difficulty and cost of obtaining dense image annotations, but existing un-supervised approaches struggle with complex scenes con-taining multiple objects. Differently from existing methods, which are purely based on deep learning, we take inspira-tion from traditional spectral segmentation methods by re-framing image decomposition as a graph partitioning prob-lem. Specifically, we examine the eigenvectors of the Lapla-cian of a feature affinity matrix from self-supervised net-works. We find that these eigenvectors already decompose an image into meaningful segments, and can be readily used to localize objects in a scene. Furthermore, by clus-tering the features associated with these segments across a dataset, we can obtain well-delineated, nameable re-gions, i.e. semantic segmentations. Experiments on complex datasets (PASCAL VOC, MS-COCO) demonstrate that our simple spectral method outperforms the state-of-the-art in unsupervised localization and segmentation by a significant margin. Furthermore, our method can be readily used for a variety of complex image editing tasks, such as background removal and compositing. 1 1.

Introduction
Well-established computer vision tasks such as localiza-tion and segmentation are aimed at understanding the struc-ture of images at a fine level of detail. The modern approach to these tasks, which consists of training deep neural net-works in an end-to-end fashion, has shown strong perfor-mance given large quantities of human-labeled data. How-1Project Page: https://lukemelas.github.io/deep-spectral-segmentation/
Figure 1. Deep Spectral Methods. We present a simple approach based on spectral methods that decomposes an image using the eigenvectors of a Laplacian matrix constructed from a combination of color information and unsupervised deep features. The method surpasses the state of the art in unsupervised image segmentation and object localization while also being significantly simpler. ever, obtaining labeled data for these dense tasks can be difficult and expensive. Whereas vast quantities of (weak) image labels and descriptions may be obtained from the In-ternet [43, 55, 63], dense image annotations cannot be eas-ily sourced from it, and creating them manually is a labor-intensive process. Moreover, in many specialized fields such as medical imaging, where detection and segmentation tasks are particularly important, data labeling must be man-ually performed by a domain expert. As a result, performing dense vision tasks without labeled data is an important open problem.
Numerous existing methods perform dense visual tasks with weak annotations, such as image-level labels, captions, spoken narratives, scribbles, and points [9,36,46,47]. How-ever, fully-unsupervised dense image understanding re-mains challenging and under-explored. Current approaches involve clustering dense features (e.g., IIC [41]), contrastive learning with saliency masks (e.g., MaskContrast [74]), and
GAN-based object discovery (e.g., ReDo [13]). Others seg-ment images of a given object category into a number of semantic regions (parts) [20, 22, 37]. However, these ap-proaches tend to struggle with complex images, and most of them can only identify a single object per image.
In this paper, we take inspiration from image segmenta-tion methods from the pre-deep learning era, which framed the segmentation problem as one of graph partitioning.
Whereas existing unsupervised segmentation methods are based primarily on deep learning, we show the benefits of combining deep learning with traditional graph-theoretic methods.
Our method first utilizes a self-supervised network to ex-tract dense features corresponding to image patches. We then construct a weighted graph over patches, where edge weights give the semantic affinity of pairs of patches, and we consider the eigendecomposition of this graphâ€™s Lapla-cian matrix. We find that without imposing any additional structure, the eigenvectors of the Laplacian of this graph directly correspond to semantically meaningful image re-gions. Notably, the eigenvector with the smallest nonzero eigenvalue generally corresponds to the most prominent ob-ject in the scene. We show that, surprisingly, simply ex-tracting bounding boxes or masks from this eigenvector sur-passes the current state of the art on unsupervised object localization/segmentation across numerous benchmarks.
Next, we propose a pipeline for semantic segmentation.
We first convert the eigensegments into discrete image re-gions by thresholding and associate each region with a se-mantic feature vector from the network. We consider all image regions in a large dataset of images and jointly clus-ter these regions, yielding semantic (pseudo-)labels that are consistent across the dataset. Lastly, we perform self-training using these labels to refine our results, and we evaluate against the ground truth segmentations. Different from prior self-supervised semantic segmentation methods, our method performs well on complex images without fine-tuning. Importantly, while recent GAN-based and saliency-based methods are limited to finding a single semantic re-gion per image, our method can segment multiple semantic regions and outperforms prior methods on PASCAL VOC 2012, re-surfacing spectral methods as a strong baseline for future work.
Finally, we show that a slight variant of our method is well-suited to the task of soft image decomposition (i.e., im-age matting), or breaking down an (RGB) image into mul-tiple (RGB-A) layers with soft boundaries. This decompo-sition dramatically simplifies real-world image editing and compositing tasks such as background replacement. 2.