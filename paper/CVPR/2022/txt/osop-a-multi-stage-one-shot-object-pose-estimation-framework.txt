Abstract
We present a novel one-shot method for object detection and 6 DoF pose estimation, that does not require training on target objects. At test time, it takes as input a target im-age and a textured 3D query model. The core idea is to represent a 3D model with a number of 2D templates ren-dered from different viewpoints. This enables CNN-based direct dense feature extraction and matching. The object is
ﬁrst localized in 2D, then its approximate viewpoint is es-timated, followed by dense 2D-3D correspondence predic-tion. The ﬁnal pose is computed with PnP. We evaluate the method on LineMOD, Occlusion, Homebrewed, YCB-V and
TLESS datasets and report very competitive performance in comparison to the state-of-the-art methods trained on syn-thetic data, even though our method is not trained on the object models used for testing. 1.

Introduction
The rapid development of high quality 6 DoF pose es-timation methods is underway. According to the BOP challenge [14], which combines publicly available 6 DoF pose estimation datasets and offers standardized evaluation and comparison procedures, the ﬁeld is dominated by deep learning methods [2, 12, 16, 19, 21, 22, 22, 24–27, 36, 37, 47, 49–52, 59]. The methods’ performance is, however, lim-ited by the availability of labeled training data. Accurate 6 DoF pose annotation of real data is a complicated and time-consuming process [20], that must be repeated man-ually for each new object. This severely limits the practi-cal applicability of 6 DoF pose estimation methods. Addi-tionally, labels can exhibit imperfection [4]. As synthetic data preparation tools improved, more methods shifted to training on synthetically rendered images. This greatly simpliﬁes the preparation of data for new objects. These time-consuming and computationally-intensive data render-ing and model training steps, on the other hand, must be repeated for each new target object of interest.
While one-shot object detection, i.e., detection of novel (a) (c) (b) (d)
Figure 1. Qualitative evaluation of the proposed method on an object from the Homebrewed dataset [20]. a) an input image, cropped only for visualization purposes, with a comparison of the ground truth (green cuboid) and estimated (blue cuboid) poses; b) predicted one-shot segmentation ; c) a matched template; and d) predicted correspondences as color-coded NOCS correspon-dences. objects not seen during training, appears to yield promising results for conventional 2D detection, its extensions to pose estimation were very limited. There are very few related works that attempt to generalize to new objects. They pri-marily focus on objects from the same category [6, 31, 55], objects with very similar geometry [38,39], rely on partially training on target objects [52], or limit the task to viewpoint estimation [57]. We extend the one-shot object detection ideas to estimate the full 6 DoF pose. Our method is trained only once and then automatically generalizes to new objects without training on them, obviating the need for synthetic or real data preparation and training for new objects.
The 4-stage pipeline of the proposed approach is visual-ized in Figure 2. The input to the method is a test image
Figure 2. Pipeline of the proposed detector. 1) One-shot object localization conditioned on the 3D model. 2) Initial viewpoint estimation by template matching. 3) Dense 2D-2D matching between the image patch and the matched template. 4) 6 DoF pose estimation with
PnP+RANSAC or Kabsch+RANSAC. The proposed pose estimation pipeline generalizes well to new target objects not seen duing training. and a textured 3D model of the target object of interest.
Inspired by OS2D [35], the method relies on dense slid-ing window-based feature correlation between the object, represented with a sparse set of 2D templates obtained by rendering the object from various viewpoints, and the input test image. In the ﬁrst stage, one-shot object segmentation is performed. The detected object is matched to a database of object renderings in the second stage to perform initial
In the third stage, a network esti-viewpoint estimation. mates dense 2D-2D correspondences between pixels of the input image patch and the matched template, whose pose is known. This provides us with 2D-3D correspondences be-tween the pixels of the input image and the 3D model. This enables 6 DoF pose estimation using PnP [23] or Kabsch [1] with RANSAC [8] in the last stage.
The evaluation of our approach on ﬁve datasets (LineMOD, Occlusion, HomebrewDB, YCB-V and
TLESS) proves that it fully generalizes to new objects and scenes not seen during training. Our key contributions include: 1) The ﬁrst RGB-based one-shot pose estimation pipeline that truly scales to new objects without training on them. This results in a considerable time reduction required for synthetic data generation and retraining. 2) A novel architecture and a novel attention mechanism for one-shot semantic segmentation; 3) An architecture for dense 2D-2D matching that enables 2D-3D correspondence transfer from a template with known 6 DoF pose to a target image with unknown pose. 2.