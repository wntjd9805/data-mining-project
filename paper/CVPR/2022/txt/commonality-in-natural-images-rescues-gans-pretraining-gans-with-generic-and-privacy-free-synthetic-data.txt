Abstract
Transfer learning for GANs successfully improves gen-eration performance under low-shot regimes. However, ex-isting studies show that the pretrained model using a sin-gle benchmark dataset is not generalized to various target datasets. More importantly, the pretrained model can be vulnerable to copyright or privacy risks as membership in-ference attack advances. To resolve both issues, we pro-pose an effective and unbiased data synthesizer, namely
Primitives-PS, inspired by the generic characteristics of natural images. Specifically, we utilize 1) the generic statistics on the frequency magnitude spectrum, 2) the el-ementary shape (i.e., image composition via elementary shapes) for representing the structure information, and 3) the existence of saliency as prior. Since our synthesizer only considers the generic properties of natural images, the single model pretrained on our dataset can be consistently transferred to various target datasets, and even outperforms the previous methods pretrained with the natural images in terms of Fr´echet inception distance. Extensive analysis, ab-lation study, and evaluations demonstrate that each com-ponent of our data synthesizer is effective, and provide in-sights on the desirable nature of the pretrained model for the transferability of GANs. 1.

Introduction
Generative adversarial networks (GANs) [13] are a pow-erful generative model that can synthesize complex data by learning the implicit density distribution with adversar-ial training. Thanks to the impressive generation quality, particularly in image generation tasks [4, 23, 30], GANs have been widely used in various downstream tasks in computer vision, such as data augmentation [9], super-resolution [25,54], image translation [1,10], and image syn-thesis with primitive representation [27, 37]. Despite the re-markable quality, GANs require at least several thousand,
†Hyunjung Shim is a corresponding author. (a) PinkNoise (b) Primitives (c) Primitives-S (d) Primitives-PS
Figure 1. Visualization of our synthetic datasets. We visualize four variants of our synthetic datasets and Primitives-PS is finally chosen for the best performance. Example images are resized in three different scales. mostly several hundred thousand images for training. This requirement for data collection is often infeasible in prac-tical applications (e.g., many pictures of a treasure, endan-gered species, or the medical images of rare disease).
The idea of transfer learning has been recently intro-duced to GAN training [31, 49] for resolving the real-world generation problem. Following the common practice, the general framework of GAN transfer learning 1) pretrains
GANs on a publicly available large-scale source dataset (e.g., FFHQ and ImageNet) and then 2) finetunes GANs with a relatively small target dataset. As a result, develop-ing GANs with transfer learning clearly improves the gen-eration quality and diversity over the models trained from scratch only with the target dataset.
Unfortunately, the effectiveness of transfer learning for
GANs highly depends on how similar the source dataset is to the target dataset. According to TransferGAN [49], transfer learning can achieve the best performance when the source shares common characteristics with the target. For example, when LFW [21] is the target dataset, the best per-formance is achieved with the source dataset of CelebA [28] as both are face datasets. For Flower [33] or Kitchens [53], utilizing CelebA as the source dataset does not significantly improve the performance. Thus, it is required to search the best source dataset for a given target dataset by measuring the similarity between two datasets (e.g., FID score). Be-cause exploring the best source dataset and then acquiring its pretrained model is ad-hoc, the search result does not guarantee the best pair for transfer learning [49]. Moreover, none of the existing source datasets can sufficiently fit the target dataset in real-world applications.
Other than the performance issue, we argue that the cur-rent pretrained models can be vulnerable to copyright (see the supplementary 7 for potential copyright issues of large-scale datasets) and privacy issues [58]. Even for public benchmark datasets, employing them for commercial pur-poses is not always permitted. For examples, ImageNet-1K having 1M images, the copyright issue might not be feasible to handle. When targeting the commercial use of a dataset, the developer should negotiate with the author of each sam-ple. For this reason, one might compose her own dataset via web crawling, but filtering out the copyrighted samples is practically difficult. Besides, unresolved copyright and privacy issues might cause legal issues [42].
Recent studies [8,15,18] also show that the deep genera-tive models are vulnerable to membership inference attacks, implying that privacy issues still remains beyond the copy-right issues. An adversary can reconstruct a face even with-out additional prior information [55]. That is, we can reveal individual training samples by attacking the trained model.
As the network capacity of GANs increases rapidly to im-prove performance, the risk of memorization also grows quickly. Memorization effects make GANs more vulner-able to membership inference attacks [7]. Since we con-sider transfer learning, someone might argue that the mem-bership inference on the source (e.g., pretraining) dataset should not be a critical issue. However, Zou et al. [58] re-ported that the membership inference of the source dataset could be conducted even after the transfer learning (see the supplementary 7 for empirical evidence).
In this work, we dive into tackling the two undiscov-ered but critical issues of transfer learning for GANs: 1) the lack of generalization for the pretrained model and 2) the copyright or privacy issue of the pretraining dataset. To this end, we devise a synthetic data generation strategy for acquiring pretrained GANs. Since our pretrained model is newly computed with a synthetic dataset, it is inherently free from copyright and privacy issues. Besides, the learned features of existing pretrained models encode the induc-tive bias of a training dataset, exhibiting lower transferabil-ity [52]. Learned from this lesson, we ensure that our syn-thetic data should be unbiased to any datasets and free from expert knowledge or specific domain prior.
Towards this goal, we adopt the generic property of the natural images in the frequency spectrum and struc-ture. We develop our data generation strategy, namely
Primitives-PS, inspired by the analysis and observa-tions on natural images from previous studies [29, 36, 44].
Our design philosophy is built upon three aspects: 1) con-sidering the power spectrum distribution of the natural im-ages as in Figure 1(a), 2) reflecting the structural property of the natural images as illustrated in Figure 1(b), and 3) utilizing the existence of saliency in images (Figure 1(c) shows the synthetic data generated by applying both 2) and 3).) Finally, we combine all three aspects and develop our final data synthesizer Primitives-PS, as visualized in
Figure 1(d). We pretrain GANs using the synthetic dataset generated by our data synthesizer. Then, the effectiveness of the proposed method is evaluated by repurposing the pre-trained model to various low-shot datasets.
Extensive evaluations and analysis confirm that this sin-gle pretrained network 1) can be effectively transferred to various low-shot datasets and 2) improve the generation performance and the convergence time. Interestingly, the model pretrained with our dataset outperforms the model pretrained with the natural images when transferred to sev-eral datasets. Our empirical study shows that the bias from a specific dataset for pretraining GANs is harmful to the gen-eralization performance of transfer learning. Finally, our analysis of learned filters provides insight into what makes the pretrained model transferable. The code is available at https://github.com/FriedRonaldo/Primitives-PS. 2.