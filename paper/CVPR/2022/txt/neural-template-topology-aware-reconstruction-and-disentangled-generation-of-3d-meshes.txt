Abstract
This paper introduces a novel framework called DT-Net for 3D mesh reconstruction and generation via Dis-entangled Topology. Beyond previous works, we learn a topology-aware neural template specific to each input then deform the template to reconstruct a detailed mesh while preserving the learned topology. One key insight is to de-couple the complex mesh reconstruction into two sub-tasks: topology formulation and shape deformation. Thanks to the decoupling, DT-Net implicitly learns a disentangled repre-sentation for the topology and shape in the latent space.
Hence, it can enable novel disentangled controls for sup-porting various shape generation applications, e.g., remix the topologies of 3D objects, that are not achievable by previous reconstruction works. Extensive experimental re-sults demonstrate that our method1 is able to produce high-quality meshes, particularly with diverse topologies, as compared with the state-of-the-art methods. 1.

Introduction
Polygonal meshes, as a compact 3D shape representa-tion, are widely used in many applications, such as model-ing, rendering, and animation. In recent years, generative modeling and reconstruction of 3D meshes has received in-creasing interest and we may also guide the generative pro-cess by using various forms of input, e.g., images [22,43,63] and point sets [10, 14, 25]. Yet, typical challenges remainâ€” how to deal with the diverse topologies of 3D meshes, and also how to effectively provide high-level controls for new shape generation, e.g., in a topology-aware manner.
To directly reconstruct a 3D mesh, one popular scheme is to learn to deform the vertices of an initial template [5, 37, 43, 52, 56, 62, 63], e.g., a manually-defined skeleton or a universal sphere, into the target mesh. However, the topolo-gies of the final reconstructed meshes are typically limited by the template model. To address this, other works learn to cover a 3D mesh with planar or curved patches [22,64]; yet, the visual quality is often tampered due to the patch mis-*Joint first authors 1Code available at https://github.com/edward1997104/
Neural-Template.
Figure 1. Our DT-Net learns to construct a topology-aware neural template (b) adapted to the input (a) and then deform it towards an accurate 3D mesh while preserving the initial (learned) topology.
This decoupled design enables a disentangled latent representa-tion of topology (ZT ) and shape (ZS), promoting controllable 3D mesh generation, e.g., remixing codes for object re-synthesis. alignment, so the resulting meshes often have rough surface appearance. While other 3D representations such as vox-els [20, 61, 65, 66, 71, 72], point clouds [2, 16, 28], and im-plicit functions [3, 21, 39, 44, 55] have been explored, these representations typically require conversions to meshes via a post-processing step for supporting visual applications.
Another drawback is that most works focus on capturing the mesh geometry directly in a single step, without pro-viding high-level interpretability, e.g., structure or topology.
So, it is particularly hard to control the mesh generation process. Some recent works tried to address this shortcom-ing by generating objects using parts and parts composi-tion, e.g., in terms of voxels [68], point clouds [40, 67], and meshes [17]. While the approach allows certain part-aware generation, these works highly rely on the availability and the quality of the extra parts annotations.
In this paper, we present a novel framework, namely
DT-Net, for 3D mesh reconstruction and generation via disentangled topology (DT). Distinctively, DT-Net enables the reconstruction of high-quality 3D meshes with diverse topologies, well-adapting to the input, e.g., images or vox-els. Also, our novel design facilitates controllability in the generative process, since DT-Net implicitly learns a disen-tangled latent representation for the topology and shape.
Therefore, we can achieve disentangled mesh generations with separate topology and shape manipulations.
Figure 1 illustrates the pipeline of DT-Net. Beyond pre-vious works, we learn a topology-aware neural template (e.g., genus of chairs) that fits each input then deform the template to reconstruct a detailed mesh. A key insight be-hind our design is that we decouple the mesh reconstruction into two sub-tasks: (i) topology formation for adapting var-ious topologies; and (ii) shape deformation for reconstruct-ing accurate objects while respecting their initial topologies.
Our decoupling scheme eases the learning process and ac-counts for the topology, while enhancing the reconstruction quality and enriching mesh generation with diverse topolo-gies. Another important design is that we extract a topology code (blue) and a shape code (red) from the input, to guide the learning of the two decoupled sub-tasks, respectively.
By doing so, two key aspects of 3D objects, topology and shape, can be jointly learned to ensure the reconstruction plausibility, while being disentangled in the latent space, for enabling novel disentangled controls in the mesh generation process; see Figure 1 (right). Please refer to Section 3.2 for further elaborations on our framework.
Method-wise, we design an end-to-end framework with the topology-learning module to first learn to produce a topology-aware neural template composed of convexes. To decouple topology learning and shape learning, we learn a family of invertible maps [23, 73] to maintain the topology between the neural template and the final reconstructed ob-ject. Also, we propose to use a dual (implicit and explicit) representation for the neural template, so it can be train-able via the implicit functions and extractable as polygonal meshes at the inference. Importantly, our approach can di-rectly learn the topology-aware neural template without in-termediate topology annotations, while well-aligning it with an inversely-deformed version of the ground-truth mesh.
Both quantitative and qualitative results show that DT-Net enables the reconstruction of high-quality meshes with diverse topologies, performing favorably over the state of the arts. Further, our method supports various genera-tive applications via disentangled controls, which cannot be achieved by existing reconstruction-based methods. 2.