Abstract
Following language instructions to navigate in unseen environments is a challenging problem for autonomous em-bodied agents. The agent not only needs to ground lan-guages in visual scenes, but also should explore the envi-ronment to reach its target.
In this work, we propose a dual-scale graph transformer (DUET) for joint long-term action planning and fine-grained cross-modal understand-ing. We build a topological map on-the-fly to enable ef-ficient exploration in global action space. To balance the complexity of large action space reasoning and fine-grained language grounding, we dynamically combine a fine-scale encoding over local observations and a coarse-scale encod-ing on a global map via graph transformers. The proposed approach, DUET, significantly outperforms state-of-the-art methods on goal-oriented vision-and-language navigation (VLN) benchmarks REVERIE and SOON. It also improves the success rate on the fine-grained VLN benchmark R2R. 1.

Introduction
Autonomous navigation is an essential ability for intel-ligent embodied agents. Given the convenience of natu-ral language for human-machine interaction, autonomous agents should also be able to understand and act accord-ing to human instructions. Towards this goal, Vision-and-Language Navigation (VLN) [1] is a challenging problem that has attracted a lot of recent research [2–9]. VLN re-quires an agent to follow language instructions and to navi-gate in unseen environments to reach a target location. Ini-tial approaches to VLN [2–4] use fine-grained instructions providing step-by-step navigation guidance such as “Walk out of the bedroom. Turn right and walk down the hallway.
At the end of the hallway turn left. Walk in front of the couch and stop”. This fine-grained VLN task enables grounding of detailed instructions but is less practical due to the need of step-by-step guidance. A more convenient interaction with agents can be achieved by goal-oriented instructions [7, 8] such as “Go into the living room and water the plant on the table”. This task, however, is more challenging as it re-quires both the grounding of rooms and objects as well as the efficient exploration of environments to reach the target.
In order to efficiently explore new areas, or correct pre-vious decisions, an agent should keep track of already executed instructions and visited locations in its mem-ory. Many existing VLN approaches [2, 10–14] implement
memory using recurrent architectures, e.g. LSTM, and con-dense navigation history in a fixed-size vector. Arguably, such an implicit memory mechanism can be inefficient to store and utilize previous experience with a rich space-time structure. A few recent approaches [15, 16] propose to explicitly store previous observations and actions, and to model long-range dependencies for action prediction via transformers [17]. However, these models only allow for local actions, i.e., moving to neighboring locations. As a result, an agent has to run its navigation model N times to backtrack N steps, which increases instability and compute.
A potential solution is to build a map [18] that explicitly keeps track of all visited and navigable locations observed so far. The map allows an agent to make efficient long-term navigation plans. For example, the agent is able to select a long-term goal from all navigable locations in the map, and then uses the map to calculate a shortest path to the goal. Topological maps have been explored by previous
VLN works [8, 19, 20]. These methods, however, still fall short in two aspects. Firstly, they rely on recurrent architec-tures to track the navigation state as shown in the middle of
Figure 2, which can greatly hinder the long-term reasoning ability for exploration. Secondly, each node in topologi-cal maps is typically represented by condensed visual fea-tures. Such coarse representations reduce complexity but may lack details to ground fine-grained object and scene descriptions in instructions.
Our approach addresses both of these shortcomings, the first one based on a transformer architecture and the second one with a dual-scale action planning approach. We propose a Dual-scale graph Transformer (DUET) with topological maps. As illustrated in Figure 1, our model consists of two modules: topological mapping and global action plan-ning. In topological mapping, we construct a topological map over time by adding newly observed locations to the map and updating visual representations of nodes. Then at each step, the global action planning module predicts a next location in the map or a stop action. To balance fine-grained language grounding and reasoning over large graphs, we propose to dynamically fuse action predictions from dual scales: a fine-scale representation of the current location
In particu-and a coarse-scale representation of the map. lar, we use transformers to capture cross-modal vision-and-language relations, and improve the map encoding by in-troducing the knowledge of graph topology into transform-ers. We pretrain the model with behavior cloning and aux-iliary tasks, and propose a pseudo interactive demonstra-tor to further improve policy learning. DUET significantly outperforms state-of-the-art methods on goal-oriented VLN benchmarks REVERIE and SOON. It also improves success rate on fine-grained VLN benchmark R2R. In summary, the contributions of our work are three-fold:
• We propose a dual-scale graph transformer (DUET)
] 5 1
[
] 0 2
, 9 1
, 8
[
Figure 2. Method comparison. HAMT [15] stores navigation and visual memories to capture long-range dependency in action pre-diction, but is limited to a local action space. Graph-based ap-proaches [8, 19, 20] use topological maps to support a global ac-tion space, but suffer from a recurrent navigation memory and a coarse-scale visual representation. Our DUET model overcomes previous limitations with a dual-scale encoding over the map. with topological maps for VLN. It combines coarse-scale map encoding and fine-scale encoding of the cur-rent location for efficient planning of global actions.
• We employ graph transformers to encode the topolog-ical map and to learn cross-modal relations with the instruction, so that action prediction can rely on a long-range navigation memory.
• DUET achieves state of the art on goal-oriented VLN benchmarks, with more than 20% improvement on success rate (SR) on the challenging REVERIE and
SOON datasets.
It also generalizes to fine-grained
VLN task, i.e., increasing SR on R2R dataset by 4%. 2.