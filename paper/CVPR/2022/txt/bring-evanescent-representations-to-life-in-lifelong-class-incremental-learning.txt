Abstract
In Class Incremental Learning (CIL), a classiﬁcation model is progressively trained at each incremental step on an evolving dataset of new classes, while at the same time, it is required to preserve knowledge of all the classes ob-served so far. Prototypical representations can be lever-aged to model feature distribution for the past data and in-ject information of former classes in later incremental steps without resorting to stored exemplars. However, if not up-dated, those representations become increasingly outdated as the incremental learning progresses with new classes. To address the aforementioned problems, we propose a frame-work which aims to (i) model the semantic drift by learn-ing the relationship between representations of past and novel classes among incremental steps, and (ii) estimate the feature drift, deﬁned as the evolution of the represen-tations learned by models at each incremental step. Se-mantic and feature drifts are then jointly exploited to infer up-to-date representations of past classes (evanescent rep-resentations), and thereby infuse past knowledge into incre-mental training. We experimentally evaluate our framework achieving exemplar-free SotA results on multiple bench-marks. In the ablation study, we investigate nontrivial rela-tionships between evanescent representations and models. 1.

Introduction
Continual learning (also called lifelong learning) refers to the ability to continuously learn and adapt to new envi-ronments, exploiting knowledge gained from the past for solving novel tasks. Though being a common human trait, lifelong learning methods are hardly deployed in practical systems. As a matter of fact, learning models are usually constrained to well-deﬁned and narrow tasks, where they can achieve remarkable performance. Nonetheless, when training a model on a continuous stream of tasks, the catas-trophic forgetting arises; new information acquired by the
*Researched during internship at Samsung Research UK
Figure 1. In CIL, training models on new classes causes represen-tations of past categories to constantly change. Yet, unavailability of data of former classes prevents from tracking their evolution in feature spaces, leading to evanescence of their representations and, in turn, to catastrophic forgetting. We propose to model represen-tation drift on a semantic level (i.e., relationship among novel and past classes) and on a feature level (i.e., the combined evolution of features learned by a classiﬁcation model), and exploit it to infer up-to-date representations of past classes. By injecting old-class knowledge into the learning process, we counteract forgetting. model tends to erase what has been experienced so far.
Continual learning has been extensively studied in a class incremental fashion [6, 22, 23]. In class incremental learning (CIL), a model is employed with sequential tasks, where classes to be learned progressively change (Fig. 1).
For each incremental training task and step t, the training set is composed of images belonging to the current class set
Ct, whereas past semantic categories Cold (cid:44) {Ct(cid:48)}t−1 t(cid:48)=1 lack any training sample. The goal of the model is to maximise the generalisation (classiﬁcation) accuracy on all the classes observed up to the current step. Yet, the change of distribu-tion of training data Dt in the form of semantic drift (i.e., due to change of experienced class set Ct) leads to forget-ting, where bias towards new data causes past information to be gradually erased and learned representations to be con-stantly updated (i.e., feature drift) focusing on new tasks.
Contributions of this work to overcome the aforemen-tioned limitations can be summarised as follows:
• To expound the forgetting phenomena in CIL, we ex-plore dynamics of incrementally learned classiﬁers using a probabilistic approach. Our investigation (Fig. 1) sug-gests that a source is the evanescence of representations
Fold learned using old classes Cold and unavailability of their distribution p(Fold) in incremental steps (Sec. 3).
• To revive the evanescent representations (ERs), we devise a framework which enables to model different types of representation drifts modularly. In the framework (Fig. 1, 2 and 3), we ﬁrst deﬁne the change of feature represen-tations by feature drift (i.e., due to constantly evolving feature representations of different patterns learned from data in CIL) and propose an effective method to model it (Sec. 4.1). Next, we deﬁne the change of representations of classes by semantic drift (i.e., due to the change of se-mantic categories learned at different incremental steps) and propose an effective method to model it (Sec. 4.2).
• We propose to train semantic and feature drift models together with feature learning and classiﬁcation models.
The proposed method integrates learning and inference in training: it is used to estimate distributions p(Fold) to be exploited for preserving knowledge of Cold while learning new representations for Ct, ∀t (Sec. 5).
• In the experimental analyses, our proposed methods outperform SotA exemplar-free competitors on various benchmarks. We also provide a detailed ablation study of geometric and statistical properties of drift models (Sec. 6). Our experimental results explicate the nontrivial rela-tionships between accuracy of models and distribution of evanescent and revived representations in CIL (Sect. 6.2). 2.