Abstract
We present prompt distribution learning for effectively adapting a pre-trained vision-language model to address downstream recognition tasks. Our method not only learns low-bias prompts from a few samples but also captures the distribution of diverse prompts to handle the varying visual representations. In this way, we provide high-quality task-related content for facilitating recognition. This prompt dis-tribution learning is realized by an efficient approach that learns the output embeddings of prompts instead of the in-put embeddings. Thus, we can employ a Gaussian distribu-tion to model them effectively and derive a surrogate loss for efficient training. Extensive experiments on 12 datasets demonstrate that our method consistently and significantly outperforms existing methods. For example, with 1 sam-ple per category, it relatively improves the average result by 9.1% compared to human-crafted prompts. 1.

Introduction
Recent progress in vision-language models (VLMs), e.g., CLIP [30] and ALIGN [16], provides a promising opportunity to explicitly leverage human language for ad-dressing downstream recognition tasks efficiently. VLMs learn aligned embeddings of image and text via contrastive learning [4, 13, 39], encouraging the representations of an image and its language description to be similar.
In the downstream task, providing the task-related content, i.e., the category descriptions, can significantly benefit the recognition via the pre-trained VLM, even to perform zero-shot inference without training samples [30].
Leveraging the language, VLMs convert the prior knowl-edge from humans into exploitable representations to ad-dress downstream tasks. The recognition performance of such methods is highly sensitive to the form of the provided content. However, it is still a challenging problem to deter-*This work was done during an internship in Huawei Noah’s Ark Lab.
†Corresponding author
Figure 1. Comparison with existing prompt-based methods of leveraging VLM, i.e., hand-crafted prompts (zero-shot CLIP [30]) and prompt tuning (CoOp [48]), and the linear probing. We report the average results on 12 downstream datasets with various train-ing samples. Our method ProDA consistently and substantially outperforms the previous prompt learning approaches. mine the optimal text descriptions.
VLMs [16, 30] construct category descriptions with the hand-crafted prompt templates. A default prompt is “a photo of a {class}.”, which works well for generic object recognition (e.g., on ImageNet [7] and STL-10 [6]).
However, it is difficult to handle fine-grained object recog-nition. On the flower dataset (Oxford Flowers 102 [27]), a better choice of prompt is “a photo of a {class}, a type of flower.” [30].
In this case, the prompt word “flower” indicates the context of the current task, thus providing the more precise description.
From this perspective, the provided text should be adapted to the task-defined context, i.e., low bias to the vi-sual representations of the target task. However, manually designing inevitably introduces artificial bias and could be sub-optimal for the target task. Thus, customizing suitable prompts for different recognition tasks relies on repetitive and time-consuming attempts by experts and also requires a large validation set for prompt selection [30].
(a) Input embeddings (b) Output embeddings
Figure 2. The t-SNE [40] visualization of the descriptions for 50 random categories on ImageNet. The descriptions of each category are generated by 80 hand-crafted prompts presented by CLIP [30]. For clarity, we randomly select 10 categories and highlight them with different colors. Other categories are in gray. (a) The input embeddings of the text encoder, which are obtained by feeding the raw text into the embedding layer. Various descriptions within a category are scattered in the space, resulting in difficulty representing their distribution. (b) The output embeddings of the text encoder about category descriptions. Relying on the capability of the text encoder, the output embeddings of the descriptions within a category are close to each other, allowing them to be modeled with a simple distribution. (Best viewed in color.)
Another challenge arises from the diversity of visual content. Due to inherent factors such as pose, deforma-tion, and lighting condition, there exists significant diver-sity among various examples within a category [43]. This intra-class variance prevents a prompt from sufficiently de-scribing visual variation. The prompts are desirable to be diverse and informative, allowing to handle the variance of visual representations. Existing work [30] ensembles 80 hand-crafted prompts to predict the categories on ImageNet including “a photo of a small {class}.”,
[7],
“a photo of a large {class}.”, etc. However, it still has the limitation of manual design, requiring cumber-some efforts to select an appropriate but potentially sub-optimal collection of prompts.
We present PROmpt Distribution leArning (ProDA) as a way for automatically learning diverse prompts from data, which can effectively adapt the pre-trained VLM to down-stream recognition tasks. As a data-driven approach, ProDA learns the soft prompts* from a few downstream samples, discovering the task-related content with less bias than man-ual design. Moreover, rather than learning one soft prompt
[48], our ProDA estimates a distribution over diverse and informative prompts to capture the variance of visual repre-sentations. In this way, our approach obtains better general-ization to various and unknown samples (Fig. 1). Besides, we explicitly differentiate prompts on both construction and semantics to further improve their diversity.
Given the purpose of learning the prompt distribution, the challenge is how to preform the learning efficiently.
Considering the soft prompt is a sequence of tokens (each token is represented by a vector), precise modeling relies on
*Soft prompts, also known as continuous prompts, represent the (word) embeddings of the raw (discrete) prompts. a complicated sequence generation model [3, 38], requiring a large number of target samples for training. In addition, the random nature of prompts leads to the weights of a clas-sification model for the target task being random variables, resulting in the exact computation of the classification loss being intractable (discussed in Sec. 3.2).
To address the problem, we adopt an efficient solution which learns the distribution of the output embeddings of the prompts (with class names), i.e., the weights of the tar-get classifier, instead of learning the distribution of the in-put embeddings of the prompts. The underlying intuition is that, although the various descriptions within a category are significantly different in the raw text (or low-level em-beddings) (Fig. 2a), the high-level embeddings of them are usually adjacent (Fig. 2b), which can be modeled using a simple distribution, such as the multivariate Gaussian dis-tribution in our paper. Moreover, based on the Gaussian distribution assumption, we propose a surrogate objective, an upper bound of the original optimization objective, for effective training, avoiding the intractable calculation.
We conduct large-scale experiments on 12 datasets to demonstrate the effectiveness of our method, which has a consistent and significant improvement over existing base-lines. For example, ProDA with 1 sample per category rela-tively improves the average result by 9.1% compared to the human-crafted prompts. 2.