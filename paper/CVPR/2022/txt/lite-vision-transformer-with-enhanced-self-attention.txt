Abstract
Despite the impressive representation capacity of vision transformer models, current light-weight vision transformer models still suffer from inconsistent and incorrect dense predictions at local regions. We suspect that the power of their self-attention mechanism is limited in shallower and thinner networks. We propose Lite Vision Transformer (LVT), a novel light-weight transformer network with two enhanced self-attention mechanisms to improve the model performances for mobile deployment. For the low-level fea-tures, we introduce Convolutional Self-Attention (CSA). Un-like previous approaches of merging convolution and self-attention, CSA introduces local self-attention into the con-volution within a kernel of size 3×3 to enrich low-level fea-tures in the first stage of LVT. For the high-level features, we propose Recursive Atrous Self-Attention (RASA), which uti-lizes the multi-scale context when calculating the similarity map and a recursive mechanism to increase the represen-tation capability with marginal extra parameter cost. The superiority of LVT is demonstrated on ImageNet recogni-tion, ADE20K semantic segmentation, and COCO panoptic segmentation. The code is made publicly available1. 1.

Introduction
Transformer-based architectures have achieved remark-able success most recently, they demonstrated superior per-formances on a variety of vision tasks, including visual recognition [65], object detection [38, 56], semantic seg-mentation [10, 60] and etc [32, 54, 55].
Inspired by the success of the self-attention module in the Natural Language Processing (NLP) community [53],
Dosovitskiy [18] first propose a transformer-based network for computer vision, where the key idea is to split the image into patches so that it can be linearly embedded with posi-tional embedding. To reduce the computational complex-ity introduced by [18], Swin-Transformer [38] upgrades
*Work done while an intern at Adobe. 1https://github.com/Chenglin-Yang/LVT
Image
MobileNetV2
PVTv2-B0
LVT
Figure 1. Mobile COCO panoptic segmentation. The model needs to recognize, localize, and segment both objects and stuffs at the same time. All the methods have less than 5.5M parameters with same training and testing process under panoptic FPN framework.
The only difference is the choice of encoder architecture. Lite
Vision Transformer (LVT) leads to the best results with significant improvement over the accuracy and coherency of the labels. the architecture by limiting the computational cost of self-attention with local non-overlapping windows. Addition-ally, the hierarchical feature representations are introduced to leverage features from different scales for better repre-sentation capability. On the other hand, PVT [56, 57] pro-poses spatial-reduction attention (SRA) to reduce the com-putational cost. It also removes the positional embedding by inserting depth-wise convolution into the feed forward network (FFN) that follows the self-attention layer. Both
Swin-Transformer and PVT have demonstrated their effec-tiveness for downstream vision tasks. However, when scal-ing down the model to a mobile friendly size, there is also a significant performance degradation.
In this work, we focus on designing a light yet effec-tive vision transformer for mobile applications [47]. More specifically, we introduce a Lite Vision Transformer (LVT) backbone with two novel self-attention layers to pursue both
the performance and compactness. LVT follows a standard four-stage structure [21, 38, 57] but has similar parameter size to existing mobile networks such as MobileNetV2 [47] and PVTv2-B0 [56].
Our first improvement of self-attention is named Con-volutional Self-Attention (CSA). The self-attention lay-ers [1, 24, 44, 58, 66] are the essential components in vision transformer, as self-attention captures both short- and long-range visual dependencies. However, identifying the local-ity is another significant key to success in vision tasks. For example, the convolution layer is a better layer to process low-level features [16]. Prior arts have been proposed to combine convolution and self-attention with the global re-ceptive field [16, 59]. Instead, we introduce the local self-attention into the convolution within the kernel of size 3×3.
CSA is proposed and used in the first stage of LVT. As a re-sult of CSA, LVT has better generalization ability as it en-riches the low-level features over existing transformer mod-els. As shown in Fig 1, compared to PVTv2-B0 [56], LVT is able to generate more coherent labels in local regions.
On the other hand, the performances of lite models are still limited by the parameter number and model depth [60].
We further propose to increase the representation capac-ity of lite transformers by Recursive Atrous Self-Attention (RASA) layers. As shown in Fig 1, LVT results have bet-ter semantic correctness due to such effective representa-tions. Specifically, RASA incorporates two components with weight sharing mechanisms. The first one is Atrous
Self-Attention (ASA). It utilizes the multi-scale context with a single kernel when calculating the similarities be-tween the query and key. The second one is the recursion pipeline. Following standard recursive network [19,28], we formalize RASA as a recursive module with ASA as the activation function. It increases the network depth without introducing additional parameters.
Experiments are performed on ImageNet [46] classifica-tion, ADE20K [69] semantic segmentation and COCO [36] panoptic segmentation to evaluate the performance of LVT as a generalized vision model backbone. Our main contri-butions are summarized in the following:
• We propose Convolutional Self-Attention (CSA). Un-like previous methods of merging global self-attention with convolution, CSA integrates local self-attention into the convolution kernel of size 3 × 3.
It is pro-posed to process low-level features by including both dynamic kernels and learnable filters.
• We propose Recursive Atrous Self-Attention (RASA).
It comprises two parts. The first part is Atrous Self-Attention (ASA) that captures the multi-scale context in the calculation of similarity map in self-attention.
The other part is the recursive formulation with ASA as the activation function. RASA is proposed to in-crease the representation ability with marginal extra parameter cost.
• We propose Lite Vision Transformer (LVT) as a light-weight transformer backbone for vision models. LVT contains four stages and adopts CSA and RASA in the first and last three stages, respectively. The su-perior performances of LVT are demonstrated in Im-ageNet recognition, ADE20K semantic segmentation, and COCO panoptic segmentation. 2.