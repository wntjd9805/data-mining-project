Abstract
Stage 1
One of the most pressing challenges for the detection of face-manipulated videos is generalising to forgery methods not seen during training while remaining effective under common corruptions such as compression.
In this paper, we examine whether we can tackle this issue by harnessing videos of real talking faces, which contain rich information on natural facial appearance and behaviour and are read-ily available in large quantities online. Our method, termed
RealForensics, consists of two stages. First, we exploit the natural correspondence between the visual and auditory modalities in real videos to learn, in a self-supervised cross-modal manner, temporally dense video representations that capture factors such as facial movements, expression, and identity. Second, we use these learned representations as targets to be predicted by our forgery detector along with the usual binary forgery classiﬁcation task; this encourages it to base its real/fake decision on said factors. We show that our method achieves state-of-the-art performance on cross-manipulation generalisation and robustness experi-ments, and examine the factors that contribute to its per-formance. Our results suggest that leveraging natural and unlabelled videos is a promising direction for the develop-ment of more robust face forgery detectors. 1.

Introduction
Automatic face manipulation methods can realistically change someone’s appearance or expression without requir-ing substantial human expertise or effort [34, 57, 62, 67, 89].
This technology’s potential social harm has spurred consid-erable research efforts to detect forgery content [3, 22, 33, 41, 46, 47, 49, 58, 63, 76, 87, 106, 110, 112].
It is known that although deep learning-based detectors can achieve high accuracy on in-distribution data, perfor-mance often plummets on videos generated using novel ma-nipulation methods (i.e., not seen during training) [17, 32, 49, 63, 67, 100, 112].
†Corresponding author. real  samples video/audio cross-modal self-supervised learning temporally  dense video  represen representations fake  samples video video
Stage 2 target  target  prediction prediction multi-task face  forgery detection labels forgery  detection
Figure 1. Overview of our two-stage method. First, we learn temporally dense video representations in a self-supervised way, by exploiting the correspondence between the visual and auditory modalities of real videos. Second, the network is presented with real and fake data and is tasked with performing face forgery de-tection while simultaneously predicting, for the real videos, the representations learned in stage 1. We use many more real than fake samples, as the former are more easily acquired. that
Various frame-based methods (i.e., take a sin-gle frame as input) have been proposed to tackle cross-manipulation generalisation, including using data augmen-tation [100], truncating classiﬁers [17], using 3D decompo-sition [112], amplifying multi-band frequencies [74], and targeting the blending boundary between the background and the altered face [63]. Nevertheless, many still signif-icantly underperform on novel forgery types or focus on low-level cues which can easily be corrupted by common perturbations like compression [49].
It is reasonable to believe that incorporating the tempo-ral dimension can improve performance, especially since many synthesis methods do not take into account tempo-ral consistency during the generation process [89]. How-ever, as with frame-based methods, naively training deep networks on videos can lead to overﬁtting to the seen forgeries [49, 92, 108]. To counteract this, LipForensics
[49] pre-trains on a large-scale lipreading dataset and then freezes part of the network to prevent it from focusing on
It achieves strong performance in cross-low-level cues.
the detector to focus on the aforementioned cues when clas-sifying the samples and, as a result, alleviates overﬁtting.
Our contributions are as follows: (1) We present a novel two-stage detection approach that uses large amounts of nat-ural talking faces for strong generalisation and robustness performance; this opens up the avenue for future forgery detection works to exploit the ubiquitous real videos online. (2) We propose, for the ﬁrst stage, a non-contrastive self-supervised framework that learns temporally dense repre-sentations, and we validate its design for our task through ablations. (3) We achieve state-of-the-art performance in experiments that test cross-manipulation generalisation and robustness to common corruptions, and highlight the factors responsible for our method’s performance. 2.