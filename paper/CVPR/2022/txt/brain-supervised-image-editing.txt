Abstract 1.

Introduction
Despite recent advances in deep neural models for se-mantic image editing, present approaches are dependent on explicit human input. Previous work assumes the availabil-ity of manually curated datasets for supervised learning, while for unsupervised approaches the human inspection of discovered components is required to identify those which modify worthwhile semantic features. Here, we present a novel alternative: the utilization of brain responses as a su-pervision signal for learning semantic feature representa-tions. Participants (N=30) in a neurophysiological experi-ment were shown artificially generated faces and instructed to look for a particular semantic feature, such as “old” or “smiling”, while their brain responses were recorded via electroencephalography (EEG). Using supervision sig-nals inferred from these responses, semantic features within the latent space of a generative adversarial network (GAN) were learned and then used to edit semantic features of new images. We show that implicit brain supervision achieves comparable semantic image editing performance to explicit manual labeling. This work demonstrates the feasibility of utilizing implicit human reactions recorded via brain-computer interfaces for semantic image editing and inter-pretation.
Semantic editing of images has recently become possi-ble by utilizing models that allow for the smooth manipu-lation of image representations. However, semantic editing requires real-world conceptualizations of semantic informa-tion to be captured by the underlying models to achieve convincing results. Due to their high performance in mod-eling highly complex features, the most popular techniques involve various approaches built on generative neural net-works [28, 2, 11, 42, 41, 27, 32, 16, 7], although other neu-ral architectures [4, 29] have also shown promise.
Recent work has demonstrated that Generative Adver-sarial Networks (GANs) [15] encode human-interpretable representations of semantic concepts [14, 42, 16, 7, 41], which partially explains their performance in semantic edit-ing tasks. However, GANs suffer from a lack of direct in-terpretability of their latent representations and do not di-rectly allow accurate semantic control. That is, semantic representations are encoded in a continuous space, but due to their high-dimensional and multivariate representation, mapping from latent features to salient semantic image fea-tures is non-trivial. Because of this, identifying and trans-lating learned semantic representations into a usable form remains an unsolved problem.
Supervised approaches, such as conditional GANs, do
allow specific features to be controlled, but they do so us-ing extensive manual labor, as they require appropriately labeled data to be available during model training. These approaches are also influenced by the subjective opinions of the labelers themselves. As GANs are typically trained using anywhere from thousands to millions of examples, the crowdsourced labeling of such datasets for specific seman-tic features to match the personal interests of an individual is unrealistic.
Unsupervised approaches typically involve identifying components within the latent GAN space [7, 16]. Human assessment is then required to filter through these discov-ered components to determine what is and what is not se-mantically relevant [7]. While such approaches allow for discovery and control of semantic features, it is by no means guaranteed to find features that are highly subjective or per-sonal, such as faces that an individual finds attractive or scenery that evokes particular emotions, moods, or mem-ories.
Supervised or unsupervised, all methods of semantic editing and how they are assessed are fundamentally in-formed by the natural human ability to assess semantic rel-evance and saliency. In other words, they need human judg-ment of what semantic information is present and how no-ticeable it is. However, the present approaches are fairly limited as they require manual human involvement to per-form. While replacing human judgments is not advisable, how these judgments are collected could be significantly improved.
Here, we propose a novel alternative: brain-supervised semantic editing. By obtaining human judgments from nat-ural, immediate responses recorded from the brain while an individual perceives visual stimuli, we demonstrate that it is possible to model semantic features of the latent space using implicit feedback from the brain. Unlike conventional su-pervised methods, the brain-supervised approach acquires relevant labeling information much more rapidly, does not require labels to be available at the time of training the GAN models, and is not limited to features discoverable by ex-ploratory methods such as those used in unsupervised ap-proaches.
In detail, we ask the following research questions:
RQ1: Can brain responses be used as supervision signals for semantic image editing?
RQ2: How does brain-supervised semantic editing per-form compared with editing informed by manual la-bels?
We show that semantically meaningful decision bound-aries within the latent GAN space can be learned using implicit feedback from the brain and that transformations using these decision boundaries offer similar performance as those produced by decision boundaries trained from ex-plicit manually provided labels. More generally, we demon-strate an intriguing new paradigm: utilizing the natural hu-man ability to detect and assess salient semantic informa-tion within images using signals recorded directly from the brain. This offers a new methodology for semantic image understanding and processing. 2.