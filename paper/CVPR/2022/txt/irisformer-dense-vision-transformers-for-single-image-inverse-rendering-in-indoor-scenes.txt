Abstract
Indoor scenes exhibit signiﬁcant appearance variations due to myriad interactions between arbitrarily diverse object shapes, spatially-changing materials, and complex light-ing. Shadows, highlights, and inter-reﬂections caused by visible and invisible light sources require reasoning about long-range interactions for inverse rendering, which seeks to recover the components of image formation, namely, shape, material, and lighting. In this work, our intuition is that the long-range attention learned by transformer architec-tures is ideally suited to solve longstanding challenges in single-image inverse rendering. We demonstrate with a spe-ciﬁc instantiation of a dense vision transformer, IRISformer, that excels at both single-task and multi-task reasoning re-quired for inverse rendering. Speciﬁcally, we propose a transformer architecture to simultaneously estimate depths, normals, spatially-varying albedo, roughness and lighting from a single image of an indoor scene. Our extensive evalu-ations on benchmark datasets demonstrate state-of-the-art results on each of the above tasks, enabling applications like object insertion and material editing in a single uncon-strained real image, with greater photorealism than prior works. Code and data are publicly released.1 1.

Introduction
Inverse rendering has long been of great interest to the computer vision community owing to its promise to decom-pose a scene into the intrinsic factors of shape, complex spatially-varying lighting, and material, thereby enabling downstream tasks of virtual object insertion, material edit-ing, and relighting. The problem is particularly challenging for indoor scenes, where complex appearances stem from multiple interactions among the above intrinsic factors, such as shadows, specularities, and interreﬂections.
Recent advances in inverse rendering has led to the emer-1https://github.com/ViLab-UCSD/IRISformer
Figure 1. Given a single real world image, IRISformer simulta-neously infers material (albedo and roughness), geometry (depth and normals), and spatially-varying lighting of the scene. The estimation enables virtual object insertion where we demonstrate high-quality photorealistic renderings in challenging lighting con-ditions compared to previous work [19]. The learned attention is also visualized for selected patches, indicating beneﬁts of global attention to reason about distant interactions (see text for details). gence of numerous works undertaking either some speciﬁc aspects of this challenge (geometry [8,24], albedo [4, 20,33], lighting [9, 11, 18, 36]), or joint estimation [2, 19, 33, 39].
However, the task of scene decomposition can be extremely ill-posed due to the inherent ambiguity between complex lighting, geometry, and material which jointly govern im-age formation in indoor scenes. For example, high-intensity pixel values can be explained by either specular or light-colored material, particular local geometry, bright lighting, or by a combination of all those factors. The problem is espe-cially severe with only a single image as input, where prior knowledge is necessary to disambiguate among all possible intrinsic decompositions that explain the image. Classical methods leverage strong heuristic priors in an optimization objective [2, 3], which may not always hold for real world scenes with complex geometry or lighting conditions.
The widespread use of convolutional neural networks (CNN) and large-scale datasets for scene decomposition [4, 23, 33, 34] promotes supervised training of end-to-end multi-task models [19,33] for joint estimation. CNN-based models have demonstrated impressive progress on inverse rendering of real world images [19, 33, 36, 39]. Nonetheless, receptive
ﬁelds in CNN architectures remain largely local throughout the consecutive layers, limiting the ability to capture long-range interactions between scene elements. As shown in
Fig. 1, CNN-based approaches fail to handle scenes where strong shadows or highlights abound due to complex light transport. This indicates that long-range dependencies across the image space must be exploited to provide globally coher-ent predictions in inverse rendering. Recently, vision trans-formers [7, 43] (ViT) have emerged for multiple computer vision tasks, beneﬁting from global reasoning via spatial attention mechanisms. In particular, dense vision transform-ers [25, 28, 38] are well-suited for dense prediction, which we posit can beneﬁt inverse rendering.
In this paper, we propose to leverage vision transformers to better account for complex light transport in inverse ren-dering. Consider Fig. 1 as an example, where we compare our proposed transformer-based approach, i.e. IRISformer (Transformer for Inverse Rendering in Indoor Scenes), with a CNN-based prior state-of-the-art [19]. Note the improve-ment in material consistency and geometry of the ﬂoor where complex lighting governs appearance; as a result of which, the leftmost sphere is properly reﬂected on the ﬂoor. Addi-tionally, IRISformer better captures global ambient lighting so that the third sphere from left is better illuminated. We also visualize the heatmaps of four patch locations shown by colored squares from selected transformer layers and heads (warmer colors indicate higher attention). By attending to large global regions with semantic meaning, the transformer can better disambiguate geometry material and lighting (yel-low). Long-range interactions among such regions can help reason about inter-reﬂections (green), directional highlights (red), or shadows (blue). as well as the long-range attention to/within those homogeneous regions, the model manages to better resolve the albedo-lighting ambiguity, and making more consistent estimations.
We demonstrate that by the insightful design of single-task and multi-task models for inverse rendering with dense vision transformers, we can achieve state-of-the-art, high-quality, and globally coherent BRDF, geometry, and light-ing prediction. In addition, downstream tasks like object insertion and material editing greatly beneﬁt from our im-provements, especially in scenarios of complex highlights or shadows. We achieve state-of-the-art results on all sub-tasks on real world datasets of IIW [4] and NYUv2 [34], as well as object insertion tasks compared to prior works.
Our contributions are threefold. (1) We propose the ﬁrst dense vision transformer-based framework for inverse ren-dering in a multi-task setting. (2) We demonstrate that ap-propriate design choices lead to better handling of global interactions between scene components, leading to better disambiguation of shape, material and lighting. (3) We demonstrate state-of-the-art results on all tasks and enable high-quality applications in augmented reality. 2.