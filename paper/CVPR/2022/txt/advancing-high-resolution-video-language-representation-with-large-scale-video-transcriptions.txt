Abstract
We study joint video and language (VL) pre-training to enable cross-modality learning and benefit plentiful down-stream VL tasks. Existing works either extract low-quality video features or learn limited text embedding, while ne-glecting that high-resolution videos and diversified se-mantics can significantly improve cross-modality learn-ing.
In this paper, we propose a novel High-resolution and Diversified VIdeo-LAnguage pre-training model (HD-VILA) for many visual tasks.
In particular, we collect a large dataset with two distinct properties: 1) the first high-resolution dataset including 371.5k hours of 720p videos, and 2) the most diversified dataset covering 15 popular
YouTube categories. To enable VL pre-training, we jointly optimize the HD-VILA model by a hybrid Transformer that learns rich spatiotemporal features, and a multimodal
Transformer that enforces interactions of the learned video features with diversified texts. Our pre-training model achieves new state-of-the-art results in 10 VL understand-ing tasks and 2 more novel text-to-visual generation tasks.
For example, we outperform SOTA models with relative increases of 40.4% R@1 in zero-shot MSR-VTT text-to-video retrieval task, and 55.4% in high-resolution dataset
LSMDC. The learned VL embedding is also effective in gen-erating visually pleasing and semantically relevant results in text-to-visual editing and super-resolution tasks. 1.

Introduction
Recent years we have witnessed an increasing number of videos with the popularity of appealing video websites and mobile apps (e.g., YouTube, TikTok). As the rapid de-velopment of smartphone cameras, device storage, and 5G
*Equal contribution in alphabetical order. This work was performed when Hongwei Xue, Tiankai Hang, Yanhong Zeng and Yuchong Sun were visiting Microsoft Research Asia as research interns. Corresponding au-thors: Bei Liu, Huan Yang, Jianlong Fu. networks, high-quality video creation, and diverse content sharing like travel, sports, and music become a new fash-ion. Therefore, the capability of video analytic and joint high-level understanding with language play a key role in many video tasks, such as video search [3,39], video recom-mendation [6], and video editing [38,48]. To facilitate video understanding, we study joint video and language (VL) pre-training, which is a new paradigm in both natural language processing [8] and computer vision [19, 52].
Existing video-language understanding models are highly limited in either scale or scope of video-language datasets. Early datasets (e.g., MSR-VTT [53], DiDeMo [2]) consist of videos and descriptions that are manually anno-tated by humans. The heavy and expensive annotation cost limits the scale of data. Moreover, datasets with only de-scriptive sentences are limited in complexity and variability that largely hinders generalization power. Recently, several datasets [3, 37] are proposed by transcriptions along with videos using ASR (automatic speech recognition), so that the data scale can be greatly enlarged. One most represen-tative work is HowTo100M [37] which consists of million-scale instructional videos. However, there are still large gaps between these video datasets and real-scenario videos in terms of video quality and semantic diversity.
To tackle the above limitations, we propose the HD-VILA-100M dataset (i.e., High-resolution and Diversified
VIdeo and LAnguage) which covers a wide range of video categories and benefits a plenty of VL tasks, such as text-to-video retrieval [39] and video QA [27]. This dataset has the following key properties: (1) Large: we have collected one of the largest video-language datasets, which consists of 100M video clip and sentence pairs from 3.3 million videos with 371.5K hours in total (2.8× video hour and 8× av-erage sentence length than HowTo100M [37]). (2) High resolution: all the videos are 720p which is much higher quality than existing datasets that are mostly 240p or 360p. (3) Diverse and balanced: we cover a wide range of topics from the YouTube, with 15 popular categories (e.g., sports, music, autos). Meanwhile, we ensure a balanced video clip
Figure 1. Examples of video clips and ASR generated transcriptions in the proposed HD-VILA-100M dataset. We present six samples (four frames for each), with diverse video categories covering HowTo & Style, People & Blog, Sports, Travel & Event, Pets & Animals,
Film & Animation. Relevant words from auto-generated video transcriptions are manually highlighted in red. [Best viewed in Color] number in each category to ease underfit problem.
To enable video-language pre-training, effective video representation is essential. Due to computational limita-tions (e.g., memory), previous works either 1) adopt simple frame-based encoders and turn to end-to-end visual encod-ing and multimodal fusion [27], or 2) choose advanced spa-tiotemporal encoders [5, 49], while having to do visual en-coding and multimodal fusion step-by-step. Few works can learn joint spatiotemporal video representation with end-to-end video-language pre-training.
In this paper, we propose to utilize hybrid image se-quence that consists of few high-resolution (HR) frames and more low-resolution (LR) neighbor frames for multi-ple video learning tasks. Such a design enables end-to-end training with high-resolution spatiotemporal video repre-sentation. To achieve this goal, we tackle two questions: (1) Which HR and LR frames should be sampled? (2) How to learn spatiotemporal features with the hybrid image se-quences? For the first problem, we randomly sample HR frames from a video clip to ensure the robustness of learned video features. LR frames are uniformly sampled from its surroundings considering that neighboring frames contain similar spatial information and are critical to temporal fea-ture learning. Second, we propose to encode HR and LR frames separately while mapping HR feature to a joint em-bedding space with LR features by a hybrid Transformer.
Such design ensures the spatiotemporal representation of videos to cover both HR and LR frames in a learnable way.
The learned spatiotemporal feature is further combined with detailed spatial features, followed by a multimodal Trans-former that learns to optimize video and language embed-ding in an end-to-end manner.
Our contributions are summarized as follows: 1) We use automatic video transcriptions to build to-date the largest high-resolution and diversified video-language dataset; 2)
We propose a novel pre-training framework to learn spa-tiotemporal information for video representation from hy-brid image sequences that consist of HR and LR frames; 3) Extensive experiments verify the effectiveness of the learned cross-modality embedding in 10 video understand-ing and 2 text-to-visual generation tasks. The dataset, model and code are released 1. 2.