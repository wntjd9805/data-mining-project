Abstract
Appearance-based gaze estimation aims to predict the 3D eye gaze direction from a single image. While recent deep learning-based approaches have demonstrated excel-lent performance, they usually assume one calibrated face in each input image and cannot output multi-person gaze in real time. However, simultaneous gaze estimation for mul-tiple people in the wild is necessary for real-world appli-cations. In this paper, we propose the first one-stage end-to-end gaze estimation method, GazeOnce, which is capa-ble of simultaneously predicting gaze directions for multiple faces (>10) in an image. In addition, we design a sophisti-cated data generation pipeline and propose a new dataset,
MPSGaze, which contains full images of multiple people with 3D gaze ground truth. Experimental results demon-strate that our unified framework not only offers a faster speed, but also provides a lower gaze estimation error com-pared with state-of-the-art methods. This technique can be useful in real-time applications with multiple users. 1.

Introduction
Eye gaze is one of the important channels in revealing human intentions. It has been adopted for a wide range of applications such as human-computer interaction [23], vir-tual/augmented reality [3, 26], medical diagnostics [4], and surveillance systems [17]. To estimate the gaze direction, various systems have been developed. However, fast and accurate calculation of gaze direction in a large range of en-vironment remains challenging.
With the development of deep learning, appearance-based gaze estimation has attracted more and more atten-tion, i.e., gaze estimation using face images captured by common cameras. The main drawbacks of existing meth-ods are: 1) they usually only support the gaze estimation for a single person, while multi-person with different head
*Corresponding author.
This work is partially supported by the National Natural Science Foun-dation of China (NSFC) under Grant 61972012.
This work is done during M. Zhang’s internship at Pengcheng Lab.
Figure 1. Compared with previous appearance-based gaze estima-tion methods [6, 30, 32], our method is the only one that can main-tain the real-time speed as the number of faces increases in the input image. Consider the average gaze accuracy across different face resolutions, our method also achieves the best performance.
The experiment setting is the same as Tab. 3. poses have been less explored; 2) they need to pre-process the images, i.e., cropping and calibration of the face images, resulting in a longer computation time. Fig. 2 illustrates the typical flow of existing systems. It first extracts face ROI us-ing a face detector, calibrates each face using the detected facial landmarks, and then the normalized face is fed into the gaze direction estimation system. It can be seen that the system errors accumulated after these steps. Moreover, their computational complexity is proportional to the number of people in the image, and they normally cannot operate in real time when there are more than 5 faces in each frame, as shown in Fig. 1.
In this paper, we reframe the multi-person gaze esti-mation as a single-stage regression task, which directly maps image pixels to multiple gaze directions. Specifically, we propose the first one-stage gaze estimation method,
Figure 2. Comparison between existing appearance-based gaze estimation (AGE) methods and our method. AGE methods usually conduct localization, normalization, and gaze estimation for each face one by one. We present the first one-stage method to simultaneously estimate gaze directions for multiple people in one pass. i.e., GazeOnce, which estimates all human gaze directions within one pass. The proposed method not only estimates gaze directions but also predicts auxiliary face information including bounding box and facial landmarks. In addition, we carefully design a projection-based self-supervision loss for 3D gaze estimation.
Another difficulty to overcome is about the dataset.
Appearance-based gaze estimation relies on high-quality datasets with face images and ground truth gaze directions.
However, obtaining gaze ground truth is very challenging.
Many gaze datasets have been released [30, 32], while they usually record data for each single person in a strictly con-trolled environment, leading to limited image styles and body poses. On the other hand, manual annotation of 3D gaze directions is time-consuming and error-prone.
In order to train our GazeOnce method, a new high qual-ity dataset is needed with multiple people and their gaze ground truth in every image. To this end, we propose a sophisticated gaze swapping method for generating a high-quality multi-person gaze dataset. The proposed MPSGaze dataset has no restrictions on the number of people and scenes, and is also easily extensible. This makes the training and evaluation of multi-person gaze estimation possible.
Based on the proposed dataset, our method not only achieves real-time multi-person gaze estimation, but also outperforms state-of-the-art methods in terms of estimation error and running time, as shown in Fig. 1.
In summary, our main contributions are as follows:
• We propose the first one-stage gaze estimation method, i.e., GazeOnce, which can estimate multi-user gaze di-rections simultaneously in a single image. In addition, we design a projection-based self-supervised strategy that can further improve the gaze accuracy.
• We provide a new gaze dataset MPSGaze, which en-ables one-stage gaze estimation training and evalua-tion. This dataset is generated by a sophisticated swap-gaze procedure to produce full images of multi-person with their gaze ground truth.
• Our method outperforms state-of-the-art methods in terms of gaze accuracy and speed, especially in the cases of a large number of faces. 2.