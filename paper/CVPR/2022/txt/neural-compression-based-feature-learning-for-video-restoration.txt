Abstract
How to efficiently utilize the temporal features is crucial, yet challenging, for video restoration. The temporal fea-tures usually contain various noisy and uncorrelated infor-mation, and they may interfere with the restoration of the current frame. This paper proposes learning noise-robust feature representations to help video restoration. We are in-spired by that the neural codec is a natural denoiser.
In neural codec, the noisy and uncorrelated contents which are hard to predict but cost lots of bits are more inclined to be discarded for bitrate saving. Therefore, we design a neural compression module to filter the noise and keep the most useful information in features for video restora-tion. To achieve robustness to noise, our compression mod-ule adopts a spatial-channel-wise quantization mechanism to adaptively determine the quantization step size for each position in the latent. Experiments show that our method can significantly boost the performance on video denoising, where we obtain 0.13 dB improvement over BasicVSR++ with only 0.23x FLOPs. Meanwhile, our method also ob-tains SOTA results on video deraining and dehazing. 1.

Introduction
Video restoration aims to recover the high-quality video from the degraded input. Typical degradation includes var-ious noises, rain, haze, etc. It has a wide range of appli-cations, but this problem is still under-explored. Different from image restoration that focuses on the intrinsic propri-eties in single image [44], video restoration relies more on extracting and utilizing temporal features for better quality.
Recent video restoration methods mainly focus on net-work structure design for better extracting temporal fea-tures. For example, RViDeNet [43] and EDVR [36] use deformable convolution to align the features of neighboring frames. BasicVSR [7] designs a bi-directional feature prop-agation network. BasicVSR++ [8] introduces the second-order grid propagation network structure and flow-guided
*This work was done when Cong Huang was an intern at Microsoft
Research Asia.
Figure 1. (a) Previous framework without temporal feature refine-ment. (b) Our framework via neural compression-based feature learning. (c) t-SNE [35] visualization. cgt t are sampled from clean video (hypersmooth, Set8 [34]). For ct and ˜ct, we add different ad-ditive white Gaussian noises (same noise σ but different noise ran-dom seeds) to the same input video to sample these feature points.
It shows that ˜ct are more robust to noise and get closer to cgt t . (d)
Performance comparison on video denoising (Set8, noise σ = 50). deformable alignment network. However, these methods directly use the extracted temporal features without any re-finement. The temporal features usually contain lots of noisy and irrelevant information, which interferes with the restoration of the current frame. In this paper, we take video denoising as a case study and explore how to utilize the ex-tracted temporal features efficiently.
We propose a novel neural compression-based solution to refine the features and learn noise-robust feature repre-sentations. From the perspective of neural codec, the noisy data usually contains lots of high-frequency and is hard to predict. To save the bitrate, codec prefers to discard these noisy and uncorrelated contents. This motivates us to design a neural compression module to purify the temporal features and filter the noisy information therein for video restoration.
To achieve robustness to noise, namely let the representa-tions of the noise-perturbed data be mapped to the same quantized representation with the clean data with high prob-ability, the quantization step needs to be properly set. How-ever, most existing neural compression frameworks only support fixed quantization step size. This cannot meet our purpose and even harms the inherent textures. To solve this problem, we design an adaptive quantization mechanism at spatial-channel-wise for our compression module, where the quantization step is learned by our prior model. Our quantization mechanism can adaptively purify the features with different content characteristics. During the training, the cross-entropy loss is used to guide the learning of the compression module and helps preserve the most useful in-formation.
Fig. 1 shows the framework comparison. From the t-SNE [35] visualization shown in Fig. 1 (c), we find, via our neural compression-based feature learning, the features are more robust to noise and get closer to the features generated from the clean video. Fig. 1 (d) is the performance compar-ison. We observe that, empowered by the noise-robust fea-ture representations, our framework significantly improves the restoration quality, when compared with prior state-of-the-art (SOTA) methods. The major contributions of this paper are summarized as follows:
• We propose a novel neural compression-based feature learning for video restoration. After processed by our com-pression module, the features are more robust to noise and then improve the restoration quality.
• To achieve robustness to noise and adaptively purify the features with different content characteristics, we de-sign a learnable quantization mechanism at spatial-channel-wise.
• To further boost the performance, we also design an attention module to help the feature learning, and a motion vector refinement module to improve the discontinuous mo-tion vector estimated from noisy video.
• We propose a lightweight framework. Compared with previous SOTA methods, our method achieves a better quality-complexity trade-off on video denoising, deraining, and dehazing. 2.