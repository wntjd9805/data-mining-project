Abstract
Free-form inpainting is the task of adding new content to an image in the regions specified by an arbitrary bi-nary mask. Most existing approaches train for a certain distribution of masks, which limits their generalization ca-pabilities to unseen mask types. Furthermore, training with pixel-wise and perceptual losses often leads to simple tex-tural extensions towards the missing areas instead of se-mantically meaningful generation.
In this work, we pro-pose RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting approach that is applicable to even extreme masks. We employ a pretrained unconditional
DDPM as the generative prior. To condition the genera-tion process, we only alter the reverse diffusion iterations by sampling the unmasked regions using the given image infor-mation. Since this technique does not modify or condition the original DDPM network itself, the model produces high-quality and diverse output images for any inpainting form.
We validate our method for both faces and general-purpose image inpainting using standard and extreme masks. Re-Paint outperforms state-of-the-art Autoregressive, and GAN approaches for at least five out of six mask distributions.
Github Repository: git.io/RePaint 1.

Introduction
Image Inpainting, also known as Image Completion, aims at filling missing regions within an image. Such in-painted regions need to harmonize with the rest of the im-age and be semantically reasonable. Inpainting approaches thus require strong generative capabilities. To this end, current State-of-the-Art approaches [20, 39, 47, 50] rely on
GANs [8] or Autoregressive Modeling [32, 41, 48]. More-over, inpainting methods need to handle various forms of masks such as thin or thick brushes, squares, or even ex-treme masks where the vast majority of the image is miss-ing. This is highly challenging since existing approaches train with a certain mask distribution, which can lead to poor generalization to novel mask types. In this work, we investigate an alternative generative approach for inpaint-ing, aiming to design an approach that requires no mask-specific training.
Denoising Diffusion Probabilistic Models (DDPM) is an emerging alternative paradigm for generative mod-elling [12, 37]. Recently, Dhariwal and Nichol [7] demon-strated that DDPM can even outperform the state-of-the-art
GAN-based method [4] for image synthesis.
In essence, the DDPM is trained to iteratively denoise the image by re-versing a diffusion process. Starting from randomly sam-pled noise, the DDPM is then iteratively applied for a cer-tain number of steps, which yields the final image sam-ple. While founded in principled probabilistic modeling,
DDPMs have been shown to generate diverse and high-quality images [7, 12, 27].
We propose RePaint: an inpainting method that solely leverages an off-the-shelf unconditionally trained DDPM.
Specifically, instead of learning a mask-conditional gener-ative model, we condition the generation process by sam-pling from the given pixels during the reverse diffusion it-erations. Remarkably, our model is therefore not trained for the inpainting task itself. This has two important ad-vantages. First, it allows our network to generalize to any mask during inference. Second, it enables our network to learn more semantic generation capabilities since it has a powerful DDPM image synthesis prior (Figure 1).
Although the standard DDPM sampling strategy pro-duces matching textures, the inpainting is often semanti-cally incorrect. Therefore, we introduce an improved de-noising strategy that resamples (RePaint) iterations to better condition the image. Notably, instead of slowing down the diffusion process [7], our approach goes forward and back-ward in diffusion time, producing remarkable semantically meaningful images. Our approach allows the network to effectively harmonize the generated image information dur-ing the entire inference process, leading to a more effective conditioning on the given image information.
We perform experiments on CelebA-HQ [21] and Im-ageNet [35], and compare with other State-of-the-Art in-painting approaches. Our approach generalizes better and has overall more semantically meaningful inpainted re-gions. 2.