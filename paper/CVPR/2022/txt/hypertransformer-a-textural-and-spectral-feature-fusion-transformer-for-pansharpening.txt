Abstract
Pansharpening aims to fuse a registered high-resolution panchromatic image (PAN) with a low-resolution hyper-spectral image (LR-HSI) to generate an enhanced HSI with high spectral and spatial resolution. Existing pansharp-ening approaches neglect using an attention mechanism to transfer HR texture features from PAN to LR-HSI fea-tures, resulting in spatial and spectral distortions. In this paper, we present a novel attention mechanism for pan-sharpening called HyperTransformer, in which features of
LR-HSI and PAN are formulated as queries and keys in a transformer, respectively. HyperTransformer consists of three main modules, namely two separate feature extrac-tors for PAN and HSI, a multi-head feature soft-attention module, and a spatial-spectral feature fusion module. Such a network improves both spatial and spectral quality mea-sures of the pansharpened HSI by learning cross-feature space dependencies and long-range details of PAN and
LR-HSI. Furthermore, HyperTransformer can be utilized across multiple spatial scales at the backbone for obtain-ing improved performance. Extensive experiments con-ducted on three widely used datasets demonstrate that Hy-perTransformer achieves significant improvement over the state-of-the-art methods on both spatial and spectral qual-ity measures. Implementation code and pre-trained weights can be accessed at https://github.com/wgcban/
HyperTransformer. 1.

Introduction
Hyperspectral (HS) pansharpening aims to spatially en-hance Low-Resolution Hyperspectral Images (LR-HSIs) by transferring textural (spatial) details from better spatial res-olution panchromatic (PAN) images, while preserving the spectral characteristics of LR-HSIs [28, 48]. The recent advancements in HS pansharpening greatly improve the amount of spectral and textural details in HSIs, which is in-deed a crucial pre-processing for many remote sensing ap-Figure 1: How our HyperTransformer differs from exist-ing pansharpening architectures. Traditional pansharpening methods simply concatenate PAN (p) and LR-HSI (y) in (a) image domain [58, 22] or (b) feature domain [14, 41, 45] to learn the mapping function from LR-HSI to pansharp-ened HSI (x). In contrast, (c) our HyperTransformer uti-lizes feature representations of LR-HSI, PAN↓↑, and PAN as Queries (Q), Keys (K), and Values (V) in an attention mechanism to transfer most relevant HR textural features to spectral features of LR-HSI from a backbone network. The output of HyperTransformer is an enhanced version of the feature representation of y. ↑ and ↓ denote bicubic upsam-pling and down-sampling, respectively. plications to accurately and rapidly identify the underlying phenomena that would otherwise be difficult to see from
LR-HSIs. HS pansharpening can be beneficial in a broad range of remote sensing tasks such as unmixing [8], change detection [37, 5], object recognition [31], scene interpreta-tion [21], and classification [26, 6].
The early research on HS pansharpening employed com-ponent substitution (CS) [10, 17, 19], multi-resolution anal-ysis (MRA) [29, 9], Bayesian [4, 15], and variational
[32, 12, 50] methods to transform spatial details from PAN image to LR-HSI. However, these traditional pansharpen-ing approaches often result in spatial and spectral distor-tions due to improper modeling of prior knowledge, inac-cessibility of sensor characteristics, the mismatch between prior assumptions with the problem [57] (such as linear spectral mixture assumption [54] and the sparsity assump-tion [60]), and reliance on hand-crafted features such as dic-tionary [60, 61] with limited representation ability.
Recently, deep convolutional neural networks (Con-vNets) have also been introduced for HS pansharpening due to their excellent ability to learn proper image features.
However, state-of-the-art (SOTA) approaches often adopt straightforward ways to transfer textural and spectral details from PAN image to LR-HSI. For example, Lee et al. [23],
Zheng et al. [58] and Bandara et al. [7] adopted a network shown in Figure 1-(a) as the backbone to learn the mapping function from the concatenation of up-sampled LR-HSI and
PAN to the pansharpened HSI. However, we argue that the concatenation of PAN image along with hundreds of LR spectral bands makes textural and spectral feature fusion difficult, and inefficient. In addition, it could result in high spectral and spatial distortions in pansharpened HSI due to the inappropriate mixing of textural-spectral details. In con-trast to the image-domain concatenation, researchers have also investigated feature-domain concatenation of PAN and
LR-HSI as shown in Figure 1-(b). In this approach, two sep-arate ConvNets are utilized to extract HR textural patterns from PAN, and spectral properties from LR-HSI [41, 14].
However, still the mixing process of textural and spectral details is just the addition without any appropriate guid-ance/attention over features. We argue that the above ap-proaches do not effectively utilize the cross-feature space dependency between LR-HSI and PAN, and the long-range details of PAN during the textural-spectral mixing process.
Instead, they completely rely upon the succeeding convo-lutional operations to propagate relevant textural-spectral features through the network. Although the convolution operation with sufficient depth is able to fuse the textural-spectral features appropriately to some extent, it is not in-tended to adjust each pixel value based on global (long-range) spectral-spatial details of the feature maps, but to adjust values of the small spatial regions together by em-ploying the convolution kernel, which is not accurate and appropriate specially in HS pansharpening.
Motivated by a recent work on image super-resolution [47], we propose a novel textural-spectral feature fusion transformer called HyperTransformer for HS pansharpening that addresses the aforementioned issues of conventional pansharpening approaches as depicted in
Figure 1-(c).
In contrast to conventional pansharpening approaches, our HyperTransformer utilizes an attention mechanism to extract cross-feature space dependency between PAN and LR-HSI features, and finds texturally advanced and more spectrally similar features for LR-HSI before fusion, which greatly helps to obtain pansharpened
HSI with simultaneously high spectral and spatial qual-ities. Formally, our HyperTransformer consists of four interconnected modules, namely two feature extraction modules for PAN and LR-HSI called FE-PAN and FE-HSI, the attention mechanism, and textural-spectral feature fusion module (TSFF). Our HyperTransformer begins by transforming PAN and LR-HSI to their respective feature space by employing FE-PAN and FE-HSI, respectively.
We then utilize LR-HSI, PAN↓↑, and PAN features as queries (Q), keys (K), and values (V) in an attention mechanism to compute texturally advanced and spectrally similar feature representations for LR-HSI features. The computed texturally advanced feature maps are then mixed with LR-HSI features from a backbone network which constitutes the pansharpened HSI. Furthermore, to obtain visually appealing pansharpened HSIs, we also introduce two new loss terms to the HS pansharpening, namely perceptual loss and transfer-perceptual loss in addition to the widely adopted L1 loss. In summary, this paper makes the following contributions:
• We propose a novel transformer network called Hy-perTransformer for HS pansharpening which achieves significant improvements over SOTA approaches. To the best of our knowledge, we are one of the first to introduce fusion transformer architecture for HS pan-sharpening.
• We propose a novel multi-scale feature fusion strategy for HS pansharpening which enables our network to effectively capture multi-scale long-range details and cross-feature space dependencies of PAN and LR-HSI by employing HyperTransformers at different scales of the backbone network.
• We also introduce two novel loss functions for HS pan-sharpening, namely synthesized perceptual loss and transfer perceptual loss which enables our Hyper-Transformer to learn more powerful feature represen-tations of PAN and LR-HSI. pansharpening approaches. Classical 2.