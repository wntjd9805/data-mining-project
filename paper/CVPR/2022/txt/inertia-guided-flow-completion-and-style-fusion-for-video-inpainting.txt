Abstract page is at https://github.com/hitachinsk/ISVI.
Physical objects have inertia, which resists changes in the velocity and motion direction. Inspired by this, we in-troduce inertia prior that optical flow, which reflects ob-ject motion in a local temporal window, keeps unchanged in the adjacent preceding or subsequent frame. We pro-pose a flow completion network to align and aggregate flow features from the consecutive flow sequences based on the inertia prior. The corrupted flows are completed un-der the supervision of customized losses on reconstruction, flow smoothness, and consistent ternary census transform.
The completed flows with high fidelity give rise to signifi-cant improvement on the video inpainting quality. Never-theless, the existing flow-guided cross-frame warping meth-ods fail to consider the lightening and sharpness variation across video frames, which leads to spatial incoherence af-ter warping from other frames. To alleviate such prob-lem, we propose the Adaptive Style Fusion Network (ASFN), which utilizes the style information extracted from the valid regions to guide the gradient refinement in the warped re-gions. Moreover, we design a data simulation pipeline to re-duce the training difficulty of ASFN. Extensive experiments show the superiority of our method against the state-of-the-art methods quantitatively and qualitatively. The project
This work is supported by the Natural Science Foundation of China under Grant 62036005 and Grant 62022075, and by the Fundamental Re-search Funds for the Central Universities under Grant WK3490000006. K.
Zhang performed this work at Microsoft Research Asia. 1.

Introduction
Video inpainting aims at filling-in the corrupted regions across video frames to maintain the visual coherence of the restored video [2]. It has wide application scenarios, such as object removal, watermark removal, video retargeting, etc. Different from image inpainting [15, 31, 32, 49], video inpainting highly depends on the utilization of the comple-mentary content across video frames to synthesize video frames with high visual quality.
Over the past two decades, researchers have committed significant efforts to video inpainting [10, 27, 36, 42, 45].
In recent years, a number of deep learning-based video in-painting methods are proposed, and they can be classified into two categories. The first category [5, 6, 12, 18, 20, 21, 29, 42, 54, 57] synthesizes the pixels in the video frames di-rectly, while the second category [8, 48] completes the op-tical flows to guide the warping procedure from the valid regions to fill in the corrupted regions. We refer these two categories to pixel-based methods and flow-based methods, respectively. Compared with pixel-based methods, flow-based methods are capable of maintaining high-frequency details in the inpainted video frames, because they mainly rely on warping video frames rather than synthesizing the pixels. Therefore, flow-based methods could achieve more visual pleasing results against the pixel-based rivals [38].
Similar to frames, consecutive optical flows are also cor-related. The fully utilization of the context provided by the flows nearby is crucial for accurate flow completion.
DFGVI [48] directly concatenates the consecutive flows for target flow completion and lacks of insightful modeling on the motion correlation between the flows.
The existing flow-based methods suffer from inaccurate flow completion, which results in erroneous warping and in-painting performance degradation, observable as the seams and ghosting shown in Fig. 1b and 1c. Moreover, the style (including lightening and sharpness) across different video frames is not exactly the same, which causes spatial in-coherence between the valid regions and the warped re-gions (the corrupted regions filled with the warped content).
Although FGVC [8] has introduced gradient warping and
Poisson blending [33] to obtain seamless fusion, such strat-egy is inadequate to deal with the style difference between each frames.
For more effective flow context utilization, we introduce the inertia prior for accurate flow completion in a local tem-poral sequence. Inertia is the resistance of any physical ob-ject to any change in its speed or direction of motion. In a local temporal window, inertia guarantees strong coherence of optical flows. Therefore, We align the features from con-secutive optical flows under the inertia prior and generate richer temporal context representation, which empowers ac-curate flow completion. We refer this flow completion net-work as Inertia-Guided Flow Completion (IGFC) Network.
We also introduce the smoothness loss and the ternary cen-sus transform (TCT) loss to supervise the completion of op-tical flows with respect to their intrinsic properties.
To amend the spatial incoherence caused by style varia-tion across different video frames after flow-guided warp-ing, we design Adaptive Style Fusion Network (ASFN) to optimize the warped gradients in the warped regions under the guidance of the gradients in the valid regions. ASFN is a lightweight network with several adaptive style fusion (ASF) modules. In each ASF module, the mean and stan-dard deviation of the valid regions and the warped regions are extracted and fused to correct the style in the warped regions. Experimental results demonstrate the effectiveness of ASFN in style correction for better spatial coherence.
For the training of ASFN, we design a data simulation pipeline to ease the cost on data preparation and enable separative training scheme. Besides, our method achieves memory-efficient inference and is capable to tackle videos up to 4K.
The contributions of this work can be summarized as:
• We introduce the inertia prior to model the inherent correlation within optical flow sequences, and pro-pose the flow completion network (IGFC) with inertia-guided flow feature alignment and aggregation for high-quality flow completion.
• We propose the Adaptive Style Fusion Network (ASFN) to refine the warped gradients in the warped regions to alleviate the spatial incoherence caused by style variation across different video frames.
• We establish a data simulation pipeline for ASFN training, which degrades the data preparation cost sig-nificantly for more efficient training. 2.