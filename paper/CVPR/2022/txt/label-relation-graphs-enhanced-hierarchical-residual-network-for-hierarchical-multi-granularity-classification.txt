Abstract
Hierarchical multi-granularity classification (HMC) as-signs hierarchical multi-granularity labels to each object and focuses on encoding the label hierarchy, e.g., [“Al-batross”, “Laysan Albatross”] from coarse-to-fine levels.
However, the definition of what is fine-grained is subjec-tive, and the image quality may affect the identification.
Thus, samples could be observed at any level of the hier-archy, e.g., [“Albatross”] or [“Albatross”, “Laysan Alba-tross”], and examples discerned at coarse categories are often neglected in the conventional setting of HMC. In this paper, we study the HMC problem in which objects are la-beled at any level of the hierarchy. The essential designs of the proposed method are derived from two motivations: (1) learning with objects labeled at various levels should trans-fer hierarchical knowledge between levels; (2) lower-level classes should inherit attributes related to upper-level su-perclasses. The proposed combinatorial loss maximizes the marginal probability of the observed ground truth label by aggregating information from related labels defined in the tree hierarchy. If the observed label is at the leaf level, the combinatorial loss further imposes the multi-class cross-entropy loss to increase the weight of fine-grained classi-fication loss. Considering the hierarchical feature interac-tion, we propose a hierarchical residual network (HRN), in which granularity-specific features from parent levels act-ing as residual connections are added to features of chil-dren levels. Experiments on three commonly used datasets demonstrate the effectiveness of our approach compared to the state-of-the-art HMC approaches. The code will be available at https://github.com/MonsterZhZh/HRN. 1.

Introduction
Traditional single-granularity classification usually as-signs a single label to a given object from a set of mu-*Yuntao Qian is the corresponding author. (a) Differences in domain knowledge and interference from the image occlusion. (b) Large variations of image resolutions.
Figure 1. Different objects can be discerned at various levels in the label hierarchy due to differences in domain knowledge or image quality such as occlusion or resolution. tually exclusive class labels. For instance, FGVC aims at distinguishing objects from different subordinate-level cat-egories within a given object category, e.g., subcategories of birds [31], cars [16], aircraft [20]. However, the definition of what is fine-grained is subjective, and the image quality may affect the identification, as illustrated in Fig. 1. A bird can be discerned as Albatross or Laysan Albatross due to differences in domain knowledge. Moreover, a bird expert recognizes a bird as Albatross rather than Black-footed Al-batross because of the occlusion of key parts. Airborne or satellite image resolutions often have large variations, caus-ing objects to be recognized at different levels. These chal-lenges increase the difficulty of constructing a dataset for single-granularity classification, while images annotated as coarse categories are also overlooked.
Compared to single-granularity classification, a more preferable solution is to employ hierarchical multi-granularity labels to describe an object, which provides more flexible options for annotators with different knowl-edge backgrounds [4]. HMC [28] aims to exploit hierarchi-cal multi-granularity labels and embeds the label hierarchy
in loss function or network architecture. Whereas conven-tional HMC usually evaluates each sample with complete hierarchical labels from the coarsest to the finest granular-ity. A more robust HMC model should effectively utilize examples observed at various levels in the hierarchy, e.g., making use of bird images annotated as [“Albatross”] and
[“Albatross”, “Laysan Albatross”].
In this paper, we study the HMC problem in which sam-ples are labeled at any level of the hierarchy. We factor-ize this problem into two aspects: (1) how to effectively use instances labeled at different levels; (2) how to perform hierarchical feature interaction in the network architecture.
For the first problem, we adopt a tree hierarchy that defines two kinds of semantic relationships between labels: parent-child correlations between levels and mutual exclusion at the same level. Inspired by the work of [7], if an instance is discerned at a label in the hierarchy, we maximize its marginal probability in the probability space constrained by the tree hierarchy. Such marginalization enjoys two bene-fits: learning with the coarse-level label could impact de-cisions of fine-grained subclasses while learning with the fine-level label aids the prediction of coarse-grained super-classes. Moreover, if the ground truth label is observed at the leaf level, we further impose the multi-class cross-entropy loss to enhance the discriminative power among fine-grained categories.
Another critical issue is to design appropriate hierarchi-cal feature interaction that reflects the label hierarchy. A distinct characteristic of hierarchical categories is that from coarse-to-fine levels, fine-level classes not only have unique attributes but also inherit attributes related to coarse-level superclasses. Based on this property, we propose a hierar-chical residual network (HRN) illustrated in Fig. 2. We first set up granularity-specific layers to disentangle hierarchi-cal features from the trunk network. Then, these hierarchi-cal features interact via residual connections [12–14,29,34], i.e., features from parent levels acting as skip connections are added to features of children levels. In summary, we aim to tackle two challenges in HMC: (1) exploiting samples la-beled at different levels; (2) designing the suitable experi-mental setting for this scenario. Accordingly, we propose a hierarchical loss on HRN and introduce class relabeling, image degeneration, and two evaluation metrics [30] in the experimental setting. Experiments on three commonly used
FGVC datasets demonstrate the advantages of our approach compared to the state-of-the-art HMC approaches. 2.