Abstract
Label-to-image translation models generate images from semantic label maps. Existing models depend on large vol-umes of pixel-level annotated samples. When given new training samples annotated with novel semantic classes, the models should be trained from scratch with both learned and new classes. This hinders their practical applications and motivates us to introduce an incremental learning strat-egy to the label-to-image translation scenario. In this pa-per, we introduce a few-shot incremental learning method for label-to-image translation.
It learns new classes one by one from a few samples of each class. We propose to adopt semantically-adaptive convolution ﬁlters and nor-malization. When incrementally trained on a novel seman-tic class, the model only learns a few extra parameters of class-speciﬁc modulation. Such design avoids catastrophic forgetting of already-learned semantic classes and enables label-to-image translation of scenes with increasingly rich content. Furthermore, to facilitate few-shot learning, we propose a modulation transfer strategy for better initializa-tion. Extensive experiments show that our method outper-forms existing related methods in most cases and achieves zero forgetting. 1.

Introduction
In this work, we consider the task of generating images from semantic label maps. Semantic maps depict the lay-outs and semantic classes of images, which can be regarded as human doodles. Existing works have made great progress in generation spatial alignment [36], diversity [45], ﬁne de-tails [47], and style controls [62]. Nevertheless, these ap-proaches still suffer from two issues. Firstly, they require a vast quantity of labeled data in training. However, manu-ally labeling data is a costly and complicated process, and thus semantically ﬁne-annotated data is often expensive to acquire. Secondly, existing label-to-image translation mod-*Corresponding author
Figure 1. An illustration of our proposed FILIT. FILIT is ca-pable of continually learning novel semantic classes from a few samples, without forgetting old semantic classes. In this example, the model starts with a version that can depict images consisting of the sky, mountain, tree, dirt, and river (FILIT0). When given a few unseen images labeled with a new semantic class “cloud”, the model incrementally learns to depict clouds without forget-ting learned classes (FILIT1). Similarly, given other new images labeled with “elephant”, the model incrementally learns to gener-ate elephants by the river (FILIT2). Therefore, as incrementally learning on new labeled images, FILIT can generate scenes with increasingly rich content. Best viewed magniﬁed on the screen. els require that training samples of all classes are prepared beforehand and learned at once. However, in practice, a trained translation model is often expected to perform new image generation tasks by learning novel semantic classes.
A naive approach to achieve this is to retrain the model on all the old and new data, which is both time-consuming and computationally expensive. Therefore, it is necessary to equip a label-to-image translation model with the ability to learn new semantic classes ﬂexibly without retraining.
Humans can incrementally learn a large number of different tasks without forgetting already-acquired knowl-edge [31]. To imitate the human learning process, incre-mental learning [35] has been proposed.
It aims at con-tinuously updating a trained model with samples from new tasks. A long-standing problem in incremental learning is how to prevent “catastrophic forgetting” [10, 30]. To ad-dress this problem, methods based on regularization [1, 24, 43, 54], rehearsal [4, 27, 38] and expansion [23, 53] are pro-posed. To alleviate the dependency on the amount of data for new tasks, advanced methods are proposed to solve few-shot incremental learning problems [11, 21, 29, 61]. No-tably, recent efforts have demonstrated that generative mod-els could also incrementally learn a sequence of datasets to generate different images. The method of memory re-play [50] treats generated data from previous tasks as parts of training samples in new tasks. LifelongGAN [57] em-ploys knowledge distillation for conditional image genera-tion. PiggybackGAN [55] factorizes previously learned ﬁl-ters into a set of piggyback ﬁlters to perform new tasks.
Inspired by the above works, we propose a Few-shot
Incremental learning method for Label-to-Image Transla-tion (FILIT). It enables a pre-trained translation model to learn novel semantic classes from a few samples incre-mentally (Fig. 1). To achieve this, we adopt semantically-adaptive normalization and convolution ﬁlters in the genera-tor (Sec. 3.2), which customizes convolution ﬁlters and nor-malization for each pixel of input feature according to the pixel’s semantic class. When learning a new task, the model only learns a few modulation parameters for base convolu-tion and normalization (Sec. 3.3). In addition, we propose a modulation transfer strategy to accelerate the convergence of modulation parameters for a new class (Sec. 3.4).
Experimental results show FILIT effectively learns new classes and successfully achieves zero forgetting of learned classes (Sec. 5.1). Ablation studies exemplify the efﬁcacy of the semantically-adaptive design and transfer strategy (Sec. 5.2), and the required number of the extra parameters is low (Sec. 5.3). Further experiments show that a trained
FILIT model can even continually learn semantic classes from datasets in other domains (Sec. 5.4).
Real label-to-image applications require massive classes to train the model, but it is infeasible to collect data of all classes at once. With FILIT, we provide a set of classes for basic creation with the pre-trained model and allow users to incrementally add new classes with a few labeled images.
Such design places a low data annotation burden on users.
Our contributions are three-fold: 1) We present a few-shot incremental learning method for label-to-image translation.
It enables the ﬂexible addition of novel classes. To the best of our knowledge, we are the ﬁrst to target this problem. 2)
We propose to adopt semantically-adaptive ﬁlters and nor-malization in the model. The model learns new classes with only a few extra parameters and avoids forgetting. 3) We propose a modulation transfer strategy to accelerate the con-vergence of incremental learning. 2.