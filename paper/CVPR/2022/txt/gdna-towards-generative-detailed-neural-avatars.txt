Abstract 1.

Introduction
To make 3D human avatars widely available, we must be able to generate a variety of 3D virtual humans with varied identities and shapes in arbitrary poses. This task is chal-lenging due to the diversity of clothed body shapes, their complex articulations, and the resulting rich, yet stochas-tic geometric detail in clothing. Hence, current methods that represent 3D people do not provide a full generative model of people in clothing. In this paper, we propose a novel method that learns to generate detailed 3D shapes of people in a variety of garments with corresponding skin-ning weights. Speciﬁcally, we devise a multi-subject for-ward skinning module that is learned from only a few posed, un-rigged scans per subject. To capture the stochastic na-ture of high-frequency details in garments, we leverage an adversarial loss formulation that encourages the model to capture the underlying statistics. We provide empirical evi-dence that this leads to realistic generation of local details such as wrinkles. We show that our model is able to gen-erate natural human avatars wearing diverse and detailed clothing. Furthermore, we show that our method can be used on the task of ﬁtting human models to raw scans, out-performing the previous state-of-the-art.
The ability to easily create diverse high-quality virtual humans with full control over their pose has many applica-tions in movie production, games, VR/AR, architecture, and computer vision. While modern computer graphics tech-niques achieve photorealism, they typically require a lot of expertise and extensive manual effort. Our goal is to make 3D human avatars widely accessible by learning a gener-ative model of people. Towards this goal, we propose the
ﬁrst method that can generate 1) diverse 3D virtual humans with 2) various identities and shapes, appearing in 3) differ-ent clothing styles and poses, with 4) realistic and stochastic high-frequency details such as wrinkles in garments.
Generative modeling of 3D rigid objects has recently seen rapid progress, fueled by continuous and resolution-independent neural 3D representations [13, 40, 43, 48, 54].
However, modeling clothed humans and their articulation is more difﬁcult due to the complex interaction of garments, their topology, and pose-driven deformations. Recent work leverages neural implicit surfaces to learn high-quality ar-ticulated avatars for a single subject [12, 15, 52, 58] but these methods are not generative, i.e. they cannot synthe-size novel human identities and shapes. Generative mod-els of clothing exist that augment SMPL by predicting dis-placements from the body mesh (CAPE [36]), or by drap-ing an implicit garment representation on a T-posed body (SMPLicit [14]), and relying on SMPL’s learned skinning for reposing. We show empirically that holistic modeling of identity, shape, articulation and clothing leads to higher
ﬁdelity generation and animation of virtual humans and to higher accuracy in ﬁtting to 3D scans.
Taking a step towards fully generative modeling of detailed neural avatars, we propose gDNA, a method that synthesizes 3D surfaces of novel human shapes, with con-trol over the clothing style and pose, and that produces re-alistic high-frequency details of the garments. To leverage raw (posed) 3D scans, we build a multi-subject implicit gen-erative representation. We build upon SNARF [12], a recent method for learning single-subject articulation-dependent effects that has been shown to generalize well to unseen poses. SNARF [12] requires many poses of a single sub-ject for training. In contrast, our multi-subject method can be learned from very few posed scans (1-3) of many differ-ent subjects. This is achieved via the addition of a latent space for the conditional generation of shape and skinning weights for clothed humans. Furthermore, a learned warp-ing ﬁeld yields accurate deformations, using the same skin-ning ﬁeld, independent of body size.
Clothing wrinkles are produced by an underlying stochastic process. To capture these effects, we propose a method that learns the underlying statistics of 3D cloth-ing details via an adversarial loss. Previous mesh-based approaches formulate this in UV-space [28], which is not directly applicable to implicit surfaces due to the lack of mesh connectivity. To learn high-frequency details, we ﬁrst predict a 3D normal ﬁeld, conditioned on the coarse shape features. To backpropagate the adversarial loss to the 3D normal ﬁeld we establish 3D-2D correspondences by aug-menting forward skinning with an implicit surface renderer.
We show that adversarial training leads to signiﬁcantly im-proved ﬁdelity of 3D geometric details, see Fig. 9.
Trained from posed scans only, we demonstrate the ﬁrst method that can generate a large variety of 3D clothed hu-man shapes with detailed wrinkles under pose control. The generated samples can be reposed via the learned skinning weights. We evaluate gDNA quantitatively, qualitatively, and through a perceptual study; gDNA strongly outper-forms baselines. Furthermore, we show that gDNA can be used for ﬁtting and re-animation of 3D scans, outperform-ing the state of the art (SOTA). In summary, we contribute:
• The ﬁrst method to generate a large variety of animat-able 3D human shapes in detailed garments; that
• learns from raw posed 3D scans without requiring canonical shapes, detailed surface registration, or man-ually deﬁned skinning weights, and
• a technique to signiﬁcantly improve the geometric de-tail in clothing deformation, based on recovering the underlying statistics of cloth deformation. 2.