Abstract
We present a novel Transformer-based network architec-ture for instance-aware image-to-image translation, dubbed
InstaFormer, to effectively integrate global- and instance-level information. By considering extracted content fea-tures from an image as tokens, our networks discover global consensus of content features by considering context infor-mation through a self-attention module in Transformers.
By augmenting such tokens with an instance-level feature extracted from the content feature with respect to bound-ing box information, our framework is capable of learn-ing an interaction between object instances and the global image, thus boosting the instance-awareness. We replace layer normalization (LayerNorm) in standard Transformers with adaptive instance normalization (AdaIN) to enable a multi-modal translation with style codes.
In addition, to improve the instance-awareness and translation quality at object regions, we present an instance-level content con-trastive loss defined between input and translated image.
We conduct experiments to demonstrate the effectiveness of our InstaFormer over the latest methods and provide exten-sive ablation studies. 1.

Introduction
For a decade, image-to-image translation (I2I), aiming at translating an image in one domain (i.e., source) to another domain (i.e., target), has been popularly studied, to the point of being deployed in numerous applications, such as style transfer [14, 21], super-resolution [11, 30], inpainting [24, 44], or colorization [62, 63].
In particular, most recent works have focused on de-signing better disentangled representation to learn a multi-modal translation from unpaired training data [22, 33, 43].
While they have demonstrated promising results, most of these methods only consider the translation on an whole im-age, and do not account for the fact that an image often con-tain many object instances of various sizes, thus showing
*Corresponding author
Figure 1. Results of InstaFormer for instance-aware image-to-image translation. Our InstaFormer effectively considers global-and instance-level information with Transformers, which enables high quality instance-level translation. the limited performance at content-rich scene translation, e.g., driving scene, which is critical for some downstream tasks, such as domain adaptive object detection [3], that re-quire well-translated object instances.
To address the aforementioned issues, some methods [3, 27, 49] seek to explicitly consider an object instance in an image within deep convolutional neural networks (CNNs).
This trend was initiated by instance-aware I2I (INIT) [49], which treats the object instance and global image sepa-rately. Following this [49], some variants were proposed, e.g., jointly learning translation networks and object de-tection networks, called detection-based unsupervised I2I (DUNIT) [3], or using an external memory module, called memory-guided unsupervised I2I (MGUIT) [27]. While these methods improve an instance-awareness to some ex-tent, they inherit limitation of CNN-based architectures [3, 27,49], e.g., local receptive fields or limited encoding of re-lationships or interactions between pixels or patches within an image which are critical in differentiating an object in-stance from an whole image and boosting its translation.
To tackle these limitations, for the first time, we present to utilize Transformer [53] architecture within I2I networks that effectively integrates global- and instance-level infor-mation present in an image, dubbed InstaFormer. We follow common disentangled representation approaches [22, 33] to extract both content and style vectors. By consider-ing extracted content features from an image as tokens,
our Transformer-based aggregator mixes them to discover global consensus by considering global context information through a self-attention module, thus boosting the instance-awareness during translation. In addition, by augmenting such tokens by an instance-level feature extracted from the global content feature with respect to bounding box infor-mation, our framework is able to learn an interaction be-tween not only object instance and global image, but also different instances, followed by a position embedding tech-nique to consider both global- and instance-level patches at once, which helps the networks to better focus on the object instance regions. We also replace layer normalization (Lay-erNorm) [1] in Transformers with adaptive instance nor-malization (AdaIN) [21] to facilitate a multi-modal trans-lation with extracted or random style vectors. Since aggre-gating raw content and style vectors directly with Trans-formers requires extremely large computation [12, 29], we further propose to apply a convolutional patch embedding and deconvolutional module at the beginning and end of our Transformer-based aggregator. In addition, to improve the instance-awareness and quality of translation images at object regions, we present an instance-level content con-trastive loss defined between input and translated images.
In experiments, we demonstrate our framework on sev-eral benchmarks [8,15,49] that contain content-rich scenes.
Experimental results on various benchmarks prove the ef-fectiveness of the proposed model over the latest methods for instance-aware I2I. We also provide an ablation study to validate and analyze components in our model. 2.