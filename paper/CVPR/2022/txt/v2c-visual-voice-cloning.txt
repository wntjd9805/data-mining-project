Abstract
Existing Voice Cloning (VC) tasks aim to convert a para-graph text to a speech with desired voice speciﬁed by a ref-erence audio. This has signiﬁcantly boosted the develop-ment of artiﬁcial speech applications. However, there also exist many scenarios that cannot be well reﬂected by these
VC tasks, such as movie dubbing, which requires the speech to be with emotions consistent with the movie plots. To ﬁll this gap, in this work we propose a new task named Vi-sual Voice Cloning (V2C), which seeks to convert a para-graph of text to a speech with both desired voice speci-ﬁed by a reference audio and desired emotion speciﬁed by a reference video. To facilitate research in this ﬁeld, we construct a dataset, V2C-Animation, and propose a strong baseline based on existing state-of-the-art (SoTA) VC tech-niques. Our dataset contains 10,217 animated movie clips covering a large variety of genres (e.g., Comedy, Fantasy) and emotions (e.g., happy, sad). We further design a set of evaluation metrics, named MCD-DTW-SL, which help eval-uate the similarity between ground-truth speeches and the synthesised ones. Extensive experimental results show that even SoTA VC methods cannot generate satisfying speeches for our V2C task. We hope the proposed new task together with the constructed dataset and evaluation metric will fa-cilitate the research in the ﬁeld of voice cloning and broader vision-and-language community. Source code and dataset will be released in https://github.com/chenqi008/V2C. 1.

Introduction
Voice Cloning (VC) [2, 9, 20, 30] aims to convert a para-graph of text to speech with the desired voice from a refer-ence audio. However, there exist many applications in the real world that require the generated speeches not only us-ing a template voice but also being with rich emotions (e.g., angry, happy, and sad), such as movie dubbing. This is be-*Partial of the work was performed when Qi Chen was an intern at
Pazhou Lab.
†Corresponding authors.
Text
Elsa: Do you want to  build a snowman?
Reference
Audio content
M o d e l voice
Speech (a) Voice Cloning (VC)
Text
Elsa: Do you want to  build a snowman? content
Reference
Audio
Reference
Video voice
M o d e l emotion
…
Speech (b) Visual Voice Cloning (V2C)
Figure 1. (a) Voice Cloning (VC) vs. (b) Visual Voice Cloning (V2C). Given an input triplet (i.e., subtitle/text, reference audio, and target video), our V2C task seeks to convert the text into a speech, which should be with the voice of reference audio and the emotion derived from reference video. Note that the reference au-dio only provides an expected voice while its content is irrelevant. yond the scope of conventional VC tasks (Figure 1(a)), as no extra guiding information can be used to generate de-sired tones and rhythms. Considering that we humans ac-complish the movie dubbing task with the most reference from visual observations (e.g., watching the movie to grasp the emotion of characters), we propose an extension task of
VC, namely Visual Voice Cloning (V2C).
An example of the proposed V2C task is shown in Fig-ure 1(b). Unlike the conventional VC task, which con-verts text to speech only aided by a reference audio, our
V2C task takes a triplet (text/subtitle, reference audio, ref-erence video) as input and expects a resulting speech with the same voice but varying emotions derived from the ref-erence video. The text/subtitle is the content that the gen-erated speech needs to cover. The reference audio includes a pre-recorded voice of the target speaker from a different
clip. And we aim to generate a speech with the voice in the reference audio and the character’s visual emotion from the reference video, speaking the content in the given text.
The new task poses several novel challenges. First, the conventional Voice Cloning (VC) methods [2, 6, 9, 20, 30] cannot well solve the V2C task as they focus only on how to convert the input text to speech with the voice/tone ex-hibited in the reference audio, without considering the emo-tion and context of the new speech. However, in our V2C task (e.g., movie dubbing) the voice emotion is crucial for generating human-like speech. Second, in our V2C task, the voice emotion should be derived from the reference video rather than the reference audio from an irrelevant clip.
Taking movie dubbing as an example, it requires humans to grasp the emotions of characters by watching the cor-responding movie clips and observing their performances (e.g., facial expressions or actions). Although several im-proved VC methods [37,47] also try to inject the voice emo-tion into their generated speech, they capture both emotion and voice from the reference audio, which cannot satisfy the requirements of V2C task. In our V2C task, an ideal method should be able to disentangle voice and emotion from the reference audios and the reference videos, respectively.
As there is no off-the-shelf dataset suitable for the V2C task, we collect the ﬁrst V2C-Animation dataset to facilitate the research in this ﬁeld. It comprises 10,217 video clips with audios and subtitles, covering 26 animated movies with 153 characters (i.e., speakers) in total. Our V2C dataset covers three modalities (i.e., text, audio and video) unlike the existing text-to-speech datasets [19, 31, 49, 51] or movie description datasets [36,41], which only focus on text and audio, or text and video. Besides, we also provide emo-tion annotation (e.g., happy or sad) for each audio and video clip like [14]. To alleviate the impact from background mu-sic, we only extract the sound channel of the centre speaker, which mainly focuses on the sound of the speaking char-acter. In this way, we ensure that all the audio clips only contain the sound from speaking characters.
To address the above challenges of V2C task, based on the widely used Text-to-Speech (TTS) framework (i.e.,
FastSpeech2 [34]), we propose a new method called Visual
Voice Cloning Network (V2C-Net), considering the emo-tion information derived from the reference video frames.
Moreover, based on MCD [24], we design an evaluation metric, called MCD-DTW weighted by Speech Length (MCD-DTW-SL), seeking to evaluate the generated speech effectively and automatically.
In summary, our contribution include:
• We propose a new task, namely Visual Voice Cloning (V2C). Given a triplet (i.e., text/subtitle, reference au-dio and reference video), the task seeks to convert the text into a speech with voice and emotion derived from reference audio and reference video, respectively.
• We collect the ﬁrst V2C-Animation dataset, consisting of 26 animated movies, 153 characters, 10,217 video clips with aligned audios and subtitles, covering three modalities (text, audio, video) and speakers’ emotion.
• We design a new method, called Visual Voice Cloning
Network (V2C-Net). Besides, to evaluate the gener-ated speech automatically, we provide an advanced au-tomatic evaluation metric, named MCD-DTW-SL. 2.