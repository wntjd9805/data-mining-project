Abstract
We scrutinise an important observation plaguing scene-level sketch research – that a significant portion of scene sketches are “partial”. A quick pilot study reveals: (i) a scene sketch does not necessarily contain all objects in the corresponding photo, due to the subjective holistic interpre-tation of scenes, (ii) there exists significant empty (white) re-gions as a result of object-level abstraction, and as a result, (iii) existing scene-level fine-grained sketch-based image retrieval methods collapse as scene sketches become more partial. To solve this “partial” problem, we advocate for a simple set-based approach using optimal transport (OT) to model cross-modal region associativity in a partially-aware fashion. Importantly, we improve upon OT to further ac-count for holistic partialness by comparing intra-modal ad-jacency matrices. Our proposed method is not only robust to partial scene-sketches but also yields state-of-the-art per-formance on existing datasets. 1.

Introduction
The prevailing nature of touch-screen devices has trig-gered significant research progress on sketches [6, 16, 24, 36, 44]. The field has predominately focused on object-level sketches to date [21, 25, 59], studying its abstract-ness [32], creativeness [25], and applications such as im-age retrieval [9, 78], and 3D synthesis/editing [29]. It was not until very recently that research efforts has undertaken a shift to scene-level analysis [37, 44, 90]. Compared to object-level sketches [21, 59], scene sketches exhibit ab-straction not only on individual objects, but also on global scene configurations. Fig. 1(a) offers some examples where randomly selected sketches are overlapped on top of their corresponding photos.
We start with an important observation plaguing scene-level sketch research – that a significant portion of scene sketches are “partial”. This partialness happens on two
*Interned with SketchX (b) (a) (c)
Figure 1. (a): Scene sketches exhibit abstraction on global scene configuration as shown by overlapping sketches on top of their corresponding photos. (b): Existing scene-level FG-SBIR meth-ods collapse as scene sketches become more partial. (c): There are significant empty (white) regions. Also, the sketch of a sheep in the scene might correspond to that in the centre of photo. This calls for a solution modelling region-wise associativity. fronts (i) a scene sketch does not necessarily contain all ob-jects in the corresponding photo, due to subjective interpre-tation of scenes, i.e., holistically partial, and (ii) there exists significant empty (white) regions as a result of object-level abstraction, i.e., locally partial. This is verifiable through a quick pilot study on existing scene sketch dataset [24]: (i) scene sketches include an average of 49.7% of the objects present in photos. (ii) On average, only 13.0% of the area in any scene sketch is occupied by its individual objects (the other 87.0% being empty regions).
In this paper, we specifically tackle this “partial” prob-lem in the context of scene-level fine-grained sketch-based image retrieval (Scene-Level FG-SBIR). We first confirm the prevalence of this problem in existing Scene-Level FG-SBIR models, where we conduct an empirical study by measuring the retrieval accuracy by progressively masking out individual objects in a scene sketch. Fig.1(b) shows that popular models [24, 90] collapse as scene sketches become more partial, calling for a solution that is robust towards par-tial input, and in turn yields state-of-the-art performance.
Global Average Pooling used by most existing FG-SBIR methods [4, 5] is clearly not up for the job since it is no-torious for losing spatial scene configuration information.
A naive alternative is computing distances between pairs of local features from corresponding regions in sketch and photo. This however is sub-optimal since sketch and photo do not follow strict region-wise alignment (see Fig. 1). Al-ternatively, one can compute soft attentions independently in each domain [64], yet this largely ignores the cross-modal gap between sketch and photo. Using cross-modal co-attention [58, 72] sounds a viable option but is otherwise intractable for practical applications1. A close contender to our approach is using graph-based matching [15, 49] of sketch and photo regions such as Liu et al. [44]. However, graph-based approaches have two common problems: (a) they dictate bounding-box annotations which are not always available (e.g., on [90]). (b) optimal graph construction (or sketching) strategy can be overly complicated [15, 19].
The key to solving this “partial” problem lies with mod-elling cross-modal region associativity. Crucially, such as-sociativity needs to happen in a partially-aware fashion.
This is because most sketch regions being empty will not be matched to any part of the photo. Partial graph matching is possible [19,68] but again not without resorting to expen-sive bounding-box annotations and complex scene graph construction procedures. Instead, as our first contribution, we advocate for the use of classic transportation theory (e.g., optimal transport (OT) [11]) to model this region as-sociativity. Set-based approaches are a great fit because (i) they do not require any explicit data annotation, and (ii) nat-urally tackle this partial matching problem [15, 80]2.
While using OT can already model region associativity, it does not yet account for holistic partialness, i.e., differ-ences in scene configurations. This dictates a holistic mech-anism that accounts for spatial object relationships within each modal. Thus, as our second contribution, we improve upon OT by capturing intra-modal scene configurations for either modality in their respective region adjacency ma-trix [86].
It follows that during cross-modal comparison, we compute the differences between these two matrices and obtain a scalar for each corresponding region to use along-side OT. Simply dotting together the intra-modal adjacency matrices is however not ideal since it ignores the local par-tialness of sketches which results in lots of near-zero en-tries in the adjacency matrix, ultimately leading to an overly sparse cross-modal matrix when dotted. Instead, we per-form a weighted comparison by computing the cosine dis-tance of two region-pairs, each pair taken from sketch and photo modality respectively.
In summary, our contributions are: (i) We show a sig-nificant portion of scene-level sketches are “partial”, both holistically and locally. (ii) The prevalence of this problem in existing FG-SBIR models is confirmed by showing how popular models [44, 90] collapse as scene sketches become 1See Appendix for further discussion 2Please refer to [15] for a detailed proof. more partial. (iii) We propose a simple solution to this “par-tial” problem by modelling partially-aware cross-modal re-gion associativity using the classic transportation problem (optimal transport (OT) [11]). (iv) We improve upon OT by capturing one-to-one intra-modal spatial relationships for partial scene sketches. (v) Our method is not only robust to partial input scene sketches but also yields state-of-the-art performance on existing scene sketch datasets. 2.