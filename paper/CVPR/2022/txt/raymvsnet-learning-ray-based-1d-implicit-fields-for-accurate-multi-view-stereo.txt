Abstract
Camera ray
Feature aggregation 0 mm 5 mm
Learning-based multi-view stereo (MVS) has by far cen-tered around 3D convolution on cost volumes. Due to the high computation and memory consumption of 3D CNN, the resolution of output depth is often considerably limited.
Different from most existing works dedicated to adaptive re-ﬁnement of cost volumes, we opt to directly optimize the depth value along each camera ray, mimicking the range (depth) ﬁnding of a laser scanner. This reduces the MVS problem to ray-based depth optimization which is much more light-weight than full cost volume optimization.
In particular, we propose RayMVSNet which learns sequen-tial prediction of a 1D implicit ﬁeld along each camera ray with the zero-crossing point indicating scene depth. This sequential modeling, conducted based on transformer fea-tures, essentially learns the epipolar line search in tradi-tional multi-view stereo. We also devise a multi-task learn-ing for better optimization convergence and depth accuracy.
Our method ranks top on both the DTU and the Tanks &
Temples datasets over all previous learning-based methods, achieving overall reconstruction score of 0.33mm on DTU and f-score of 59.48% on Tanks & Temples. 1.

Introduction
Learning-based multi-view stereo has gained a surge of attention recently since the seminal work of MVSNet [42].
The core idea of MVSNet and many followup works is to construct a 3D cost volume in the frustum of the reference view through warping the image features of several source views onto a set of fronto-parallel sweeping planes at hy-pothesized depths. 3D convolutions are then conducted on the cost volume to extract 3D geometric features and regress the ﬁnal depth map of the reference view.
Most existing methods are limited to low-resolution cost volume since 3D CNN is generally computation and mem-ory consuming. Several recent works proposed to upsample or reﬁne cost volume aiming at increasing the resolution of output depth maps [8, 13, 41]. Such reﬁnement, however,
*Joint ﬁrst authors
†Corresponding author: kevin.kai.xu@gmail.com
Source image
Baseline
Reference image 0 mm 5 mm
Source image
Ours
Figure 1. RayMVSNet performs multi-view stereo via predict-ing 1D implicit ﬁelds on a camera ray basis. The sequential pre-diction of 1D ﬁeld is light-weight and the monotonicity of ray-based distance ﬁeld around surface-crossing points facilitates ro-bust learning, leading to more accurate depth estimation than the purely cost-volume-based baselines such as MVSNet [42]. still needs to trade off between depth and spatial (image) resolutions. For example, CasMVSNet [13] opts to narrow down the range of depth hypothesis to allow high-res depth estimation, matching the spatial resolution of input RGB. 3D convolution is then naturally conﬁned within the narrow band, thus degrading the efﬁcacy of 3D feature learning.
In fact, depth map is view-dependent although cost vol-ume is not. Since the target is depth map, reﬁning the cost volume seems neither economic nor necessary. There could be a large portion of the cost volume invisible to the view point. In this work, we advocate direct optimizing the depth value along each camera ray, mimicking the range (depth)
ﬁnding of a laser scanner. This allows us to convert the
MVS problem into ray-based depth optimization which is, individually, a much more light-weight task than full cost volume optimization. We formulate the “range ﬁnding” of each camera ray as learning a 1D implicit ﬁeld along the ray whose zero-crossing point indicates the scene depth along that ray (Figure 1). To do so, we propose RayMVSNet which learns sequential modeling of multi-view features along camera rays based on recurrent neural networks.
Technically, we propose two critical designs to facilitate learning accurate ray-based 1D implicit ﬁelds. Firstly, the sequential prediction of 1D implicit ﬁeld along a camera ray is essentially conducting an epipolar line search [2] for
cross-view feature matching whose optimum corresponds to the point of ray-surface intersection. To learn this line search, we propose Epipolar Transformer. Given a camera ray of the reference view, it learns the matching correlation of the pixel-wise 2D features of each source view based on attention mechanism. The transformer features of all views, together with (low-res) cost volume features, are then con-catenated and fed into an LSTM [15] for implicit ﬁeld re-gression. Figure 3 visualizes how epipolar transformer se-lects reliable matching features from different views.
Secondly, we conﬁne the sequential modeling for each camera ray within a ﬁxed-length range centered around the hypothesized surface-crossing point given by the vanilla
MVSNet. This makes the output 1D implicit ﬁeld along each ray monotonous, which is normalized to [−1, 1]. Such restriction and normalization lead to signiﬁcant reduction of learning complexity and improvement of result quality. We devise two learning tasks: 1) sequential prediction of signed distance at a sequence of points sampled in the ﬁxed-length range and 2) regression of the zero-crossing position on the ray. A carefully designed loss function correlates the two tasks. Such multi-task learning approach yields highly ac-curate estimation of per-ray surface-crossing points.
Learning view-dependent implicit ﬁelds has been well-exploited in neural radiance ﬁelds (NeRF) [25] with great success. Recently, NeRF was combined with MVSNet for better generality [4]. Albeit sharing conceptual similarity, our work is a completely different from NeRF. First, NeRF (including MVSNeRF [4]) is designed for novel view syn-thesis, a different task from MVS. Second, the radiance ﬁeld in NeRF is deﬁned and learned in continuous 3D space and camera rays are used only in the volume rendering stage. In our RayMVSNet, on the other hand, we explicitly learn 1D implicit ﬁelds on a camera ray basis.
RayMVSNet ranks top on both the DTU and the Tanks
It
& Temples datasets over all learning-based methods. achieves overall reconstruction score of 0.33mm on DTU, and f-score of 59.48% on Tanks & Temples. Notably, since all rays share weights for the LSTM and the epipolar trans-former, the RayMVSNet model is light weight. Moreover, the computation for each ray is highly parallelizable.
Our work makes the following contributions:
• A novel formulation of deep MVS as learning ray-based 1D implicit ﬁelds.
• An epipolar transformer designed to learn cross-view feature correlation with attention mechanism.
• A multi-task learning approach to sequential modeling and prediction of 1D implicit ﬁelds based on LSTM.
• A challenging test set focusing on regions with specu-lar reﬂection, shadow or occlusion based on the DTU dataset [1] and associated extensive evaluations. 2.