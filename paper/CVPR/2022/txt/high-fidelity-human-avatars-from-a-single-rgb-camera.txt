Abstract
In this paper, we propose a coarse-to-fine framework to reconstruct a personalized high-fidelity human avatar from a monocular video. To deal with the misalignment problem caused by the changed poses and shapes in dif-ferent frames, we design a dynamic surface network to re-cover pose-dependent surface deformations, which help to decouple the shape and texture of the person. To cope with the complexity of textures and generate photo-realistic results, we propose a reference-based neural rendering network and exploit a bottom-up sharpening-guided fine-tuning strategy to obtain detailed textures. Our frame-work also enables photo-realistic novel view/pose syn-thesis and shape editing applications. Experimental re-sults on both the public dataset and our collected dataset demonstrate that our method outperforms the state-of-the-art methods. The code and dataset will be available at http://cic.tju.edu.cn/faculty/likun/projects/HF-Avatar. 1.

Introduction
Automatic generation of personalized human avatars has a wide range of applications in virtual/augmented reality, virtual try-on, entertainment and gaming. Especially tech-nologies using a single RGB camera will enable To C (cus-tomer) applications instead of To B (business).
High-quality human models can be reconstructed with expensive 3D scanners [1], multi-view studios with con-trolled lighting [10], or depth cameras [6, 8, 53]. These systems are usually costly or using non-consumer devices, leading to restricted applications. Therefore, avatar acqui-sition from a single RGB camera is the most practical but challenging. Some methods [38, 39, 51] based on implicit representations reconstruct both geometry and texture from a single image, which can handle arbitrary topology but cannot support animation. Moreover, the reconstructed un-seen regions tend to be smooth due to the limited obser-vation. Therefore, many work proposed to reconstruct an
*Corresponding author
Figure 1. Given a self-captured RGB video, the state-of-the-art method [2] fails to produce seamless and reasonable texture maps.
To address this, we propose a coarse-to-fine framework with dy-namic surface deformation and reference-based neural rendering, which can generate seamless and sharp texture maps. avatar from an RGB video. Alldieck et al. [2–4] proposed to generalize visual hull methods to monocular videos of people in motion, which optimized a fixed displacement for each vertex across the video. Although this method is com-putationally cheap and memory-saving, such a single off-set shared across all the frames is unreasonable, because the pose and geometry of the person change with the mov-ing. Besides the reconstructed geometry, a high-quality tex-ture map is also an essential component for a personalized avatar. Alldieck et al. [4] proposed to get a full texture map by calculating the median of unwrapped texture maps, which leads to a coarse texture map due to direct averag-ing. To obtain a sharp texture map, they further proposed to solve the texture stitching based on graph cut [2, 3]. How-ever, all the above methods suffer from either blurred tex-tures or texture artifacts/mistakes, due to the intrinsic com-plexity of textures and unreasonable processing (shown in
the middle of Fig. 1).
To address the above problems, in this paper, we propose a coarse-to-fine framework which consists of a dynamic sur-face network and a reference-based neural rendering net-work, to generate a fully-textured high-fidelity avatar from a monocular video where the person is rotating in front of the camera. The geometry of the person will change con-tinuously when the person is moving. This leads to the misalignment among different frames of the video. To deal with the misalignment problem, we design a dynamic sur-face network to recover pose-dependent surface deforma-tions, which help to decouple the shape and texture of the person. We learn to optimize both geometry and texture by the photometric constraint, which guides the vertices to be close to the right positions and relieves the misalignment of geometry.
Based on the dynamic surface network, we obtain a coarse texture map. However, texture is extremely complex: it resides in high dimensional space and is difficult to rep-resent. Therefore, to cope with the complexity of textures and generate photo-realistic results, we propose a reference-based neural rendering network and exploit a bottom-up sharpening-guided fine-tuning strategy to obtain detailed textures. The neural rendering network fuses observations into a joint representation whose results are used as supervi-sion to optimize the texture map which avoids the direct av-eraging of textures and adds more texture details. Besides, we propose to map the supervisions into a new space by enhancing its high-frequency information, which improves the clarity and fidelity of texture maps. Our framework can reconstruct high-fidelity personalized avatars and generate photo-realistic results of novel view/pose synthesis, which is compatible with traditional graphics pipeline. Experi-mental results on both the public dataset and our collected dataset demonstrate that our method outperforms the state-of-the-art methods. An example is given in Fig. 1.
The main contributions are summarized as follows:
• We propose a coarse-to-fine framework which com-bines neural texture with dynamic surface deformation to generate a fully-textured avatar from a monocular video captured by the users themselves.
• We propose a dynamic surface network to model the pose-dependent surface deformations of a moving per-son, which deals with the misalignment problem and disentangles the shape and texture of the person.
• We propose a reference-based neural rendering net-work and exploit a bottom-up sharpening-guided fine-tuning strategy, which fuses all the observations into a consistent representation and enables to generate the detailed texture map. 2.