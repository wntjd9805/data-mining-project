Abstract
Deep networks have strong capacities of embedding data into latent representations and ﬁnishing following tasks.
However, the capacities largely come from high-quality an-notated labels, which are expensive to collect. Noisy la-bels are more affordable, but result in corrupted represen-tations, leading to poor generalization performance. To learn robust representations and handle noisy labels, we propose selective-supervised contrastive learning (Sel-CL) in this paper. Speciﬁcally, Sel-CL extend supervised con-trastive learning (Sup-CL), which is powerful in represen-tation learning, but is degraded when there are noisy labels.
Sel-CL tackles the direct cause of the problem of Sup-CL.
That is, as Sup-CL works in a pair-wise manner, noisy pairs built by noisy labels mislead representation learning. To alleviate the issue, we select conﬁdent pairs out of noisy ones for Sup-CL without knowing noise rates. In the selec-tion process, by measuring the agreement between learned representations and given labels, we ﬁrst identify conﬁdent examples that are exploited to build conﬁdent pairs. Then, the representation similarity distribution in the built con-ﬁdent pairs is exploited to identify more conﬁdent pairs out of noisy pairs. All obtained conﬁdent pairs are ﬁnally used for Sup-CL to enhance representations. Experiments on multiple noisy datasets demonstrate the robustness of the learned representations by our method, following the state-of-the-art performance. Source codes are available at https://github.com/ShikunLi/Sel-CL. 1.

Introduction
Deep networks are powerful in various tasks, e.g., im-age recognition [19, 62], object detection [58], visual track-ing [11] and text matching [3]. The power is largely at-tributed to the collection of large-scale datasets with high-quality annotated labels. In supervised learning, with the
*Shiming Ge is the corresponding author.
Figure 1. Left: learning a classiﬁer with ideal representations in-duced by clean labels; Right: learning a classiﬁer with corrupted representations caused by noisy labels. Circles represent the repre-sentations of positive examples while triangles represent the rep-resentations of negative examples. When the representations are corrupted by noisy labels, the decision boundary of the classiﬁer will be largely changed. Therefore, the learned classiﬁer in this case cannot generalize well on test examples. data (i.e., the instance and label pairs) in such datasets, deep networks ﬁrst learn ideal latent representations of the in-stances and then complete following tasks with the repre-sentations [14, 57]. However, it is extremely expensive to obtain large-scale high-quality annotated labels. Alterna-tively, we can collect labels based on web search and user tags [36, 55]. These labels are cheap but inevitably noisy.
Noisy labels impair the generalization performance of deep networks [15, 60].
It is because, supervised by the datasets with noisy labels, the mislabeled data provide in-correct signals when inducing latent representations for the instances. The corrupted representations then cause inac-curate decisions for following tasks and hurt generaliza-tion [29, 52]. For example, as shown in Fig. 1, the cor-rupted representations result in an inprecise classiﬁcation boundary. Therefore, it is crucial to induce robust latent representations of instances for learning with noisy labels, which is also our focus in this paper.
Recent works [7, 8, 13, 18, 65] show that, working in a pair-wise manner, contrastive learning (CL) methods can bring good latent representations to help following tasks.
Based on whether supervised information is provided, the
CL methods can be grouped into supervised contrastive learning (Sup-CL) [27] and unsupervised contrastive learn-ing (Uns-CL) [7, 8, 18].
It has been shown that Sup-CL can exploit the supervised information to learn better rep-resentations than Uns-CL, but relies on the quality of su-pervised information [41].
If the supervised information is corrupted by noisy labels, built pairs by training exam-ples are noisy, following corrupted representations learned by Sup-CL. Motivated by this phenomenon, prior meth-ods use general-purpose techniques in tackling noisy labels for robust representation learning with Sup-CL, e.g., intro-ducing regularization [41] or generating pseudo-labels [32].
Although these methods can work ﬁne in some cases, the general-purpose techniques fail to consider the remarkable pair-wise characteristic of Sup-CL in strengthening repre-sentation learning. The achieved performance by them is thus argued to be sub-optimal.
In this paper, we propose selective-supervised con-trastive learning (Sel-CL) to address the above issue. Sel-CL can make use of the pair-wise characteristic to learn ro-bust latent representations. The core idea of Sel-CL is (1) select conﬁdent pairs out of noisy pairs; (2) employ the con-ﬁdent pairs to learn robust latent representations. Note that it is hard to identify conﬁdent pairs directly for representa-tion learning. The main reason is that we always need to set a threshold with the noise rate for precise identiﬁcation, e.g., see [15, 16, 26]. Nevertheless, it is difﬁcult to estimate the noise rate of noisy pairs. To handle this problem, we propose to ﬁrst employ conﬁdent examples [16, 41], which is much easier to identiﬁed, to build a reliable set of conﬁ-dent pairs at each epoch. Then, based on the representation similarity distribution of conﬁdent pairs in this set, we set a dynamic threshold to selected more conﬁdent pairs out of all noisy pairs. By this pair-wise selection, we can make better use of not only the pairs whose class labels are correct, but also the pairs whose class labels are incorrect, but the ex-amples in them are misclassiﬁed to the same class. All se-lected conﬁdent pairs are utilized to enhance representation learning with Sup-CL. As the selected conﬁdent pairs are less noisy, the learned representations with this selective-supervised paradigm will be more robust, naturally follow-ing promising generalization.
The main contributions of this paper are summarized as three aspects: 1) We propose selective-supervised con-trastive learning with noisy labels, which can obtain robust pre-trained representations by effectively selecting conﬁ-dent pairs for performing Sup-CL. 2) Without knowing the noise rate of pairs, our approach selects the pairs built by identiﬁed conﬁdent examples, and the pairs built by the ex-amples with high representation similarities. It fulﬁls a pos-itive cycle, where better conﬁdent pairs result in better rep-resentations and better representations will identify better conﬁdent pairs. 3) We conduct experiments on synthetic and real-world noisy datasets, which clearly demonstrate our approach achieves better performance compared with the state-of-the-art methods. Comprehensive ablation stud-ies and discussions are also provided. 2.