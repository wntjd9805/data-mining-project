Abstract
Class-incremental learning (CIL) has been widely stud-ied under the setting of starting from a small number of classes (base classes). Instead, we explore an understud-ied real-world setting of CIL that starts with a strong model pre-trained on a large number of base classes. We hypoth-esize that a strong base model can provide a good repre-sentation for novel classes and incremental learning can be done with small adaptations. We propose a 2-stage train-ing scheme, i) feature augmentation – cloning part of the backbone and ﬁne-tuning it on the novel data, and ii) fusion – combining the base and novel classiﬁers into a uniﬁed classiﬁer. Experiments show that the proposed method sig-niﬁcantly outperforms state-of-the-art CIL methods on the large-scale ImageNet dataset (e.g. +10% overall accuracy than the best). We also propose and analyze understudied practical CIL scenarios, such as base-novel overlap with distribution shift. Our proposed method is robust and gen-eralizes to all analyzed CIL settings. 1.

Introduction
As deep classiﬁers become more popular for real-world applications, the need for incrementally learning novel classes (novel data) becomes more prevalent. Training a classiﬁer with both old and novel data is not optimal when old data can become unavailable over time [6, 13, 15, 18, 19, 23, 24, 32]. Fewer old data leads to a high im-balance between the old and novel data, and simply ﬁne-tuning the model causes catastrophic forgetting for the old classes [15].
Class-incremental learning (CIL) methods [5,6,8,11,23, 29,34] learn to categorize more and more classes over time.
However, they typically start their incremental training with a small number of base classes (e.g. only 50), and add an
In many equally small number of new classes at a time. practical scenarios, having a large number of base classes can be a more useful starting point for building an applica-Figure 1. We study the problem of CIL in the setting where there are a large number of base classes. Classes between splits can have overlaps, and data can be sampled from the same or different distributions (e.g. different styles, poses). tion. For example, a strong model could have been devel-oped to identify different dog breeds, and a small set of ad-ditional breeds needs to be added for model update. More-over, base and novel classes may overlap but may distribute differently, such as a guitar class present in both base and novel classes, with only acoustic guitars in base samples while electric guitars in novel ones.
Some CIL methods use a static model and typically
ﬁne-tune the existing parameters with some constraints im-posed on parameter changes [3, 13, 32], gradients [17], fea-tures [8, 11], or activations [6, 15, 23]. These methods mod-ify the well-trained network weights and risk performance degradation. On the other hand, methods based on dynamic models learn separate parameters for novel tasks, either by expanding the model [24], or introducing a parameter gat-ing function [18,19]. However, almost all dynamic methods focus on task-incremental learning (TIL). TIL assumes that which task a sample belongs to is known at inference time, and different tasks are inferred individually. This assump-tion is not realistic if the application needs to distinguish between base and novel classes. A recent work DER [30]
uses a dynamic model for CIL, where it duplicates the entire backbone for novel data and prunes the model.
Current CIL approaches may not be optimal when a large number of base classes (e.g. 800) is used to pre-train a strong model. We hypothesize that the well-trained back-bone is capable of extracting representative features for the novel data and freezing it partially while learning a small adaptation branch for novel data works better than ﬁne-tuning the whole backbone. We show in a preliminary study that ﬁne-tuning fewer layer blocks outperforms full ﬁne-tuning when using a strong pre-trained model.
Hence, we propose a 2-stage training scheme for CIL starting with a large number of base classes: i) duplicat-ing part of the backbone as the adaptation module and ﬁne-tuning it on the novel data, and ii) combining all the inde-pendently trained base and novel classiﬁers into a uniﬁed classiﬁer at each incremental step. Towards this, we pro-pose a score fusion network that enables knowledge trans-fer between base and novel classes by combining the logits.
While the optimal adaptation module size may depend on the old and novel data discrepancy, we show that our score fusion generalizes to different adaptation module sizes.
Most CIL research [6, 8, 11, 23, 29, 34] only consider the scenario where the base and novel label sets are disjoint.
Rainbow Memory [5] (RM) is the only exception where label sets are identical among tasks but class frequencies differ.
In this work, we explore a more general setting where some novel classes can overlap with base classes, po-tentially with a different distribution (e.g., different styles, poses), as shown in Figure 1. We show how our score fu-sion can handle overlapping classes by using a knowledge pooler to combine their base and novel logits.
In summary, we provide three contributions:
• We propose a 2-stage CIL training strategy. In stage-I, instead of tuning the base network and risking catas-trophic forgetting, we replicate part of the network and
ﬁne-tune the extra branch on novel data. We show that as we start with a strong pre-trained base network, this approach outperforms state-of-the-art CIL methods.
• We propose a new score fusion algorithm for stage-II, where we unify the classiﬁers for base and novel data into one by consolidating their output logits.
• We generalize CIL to a broader and more challenging scenario: base and novel classes can partially overlap, and the overlap classes may have changed distributions (e.g. new style). We show that our method is robust and generalizes to this new scenario. 2.