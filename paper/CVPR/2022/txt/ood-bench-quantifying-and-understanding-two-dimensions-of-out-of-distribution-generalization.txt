Abstract
Deep learning has achieved tremendous success with in-dependent and identically distributed (i.i.d.) data. However, the performance of neural networks often degenerates dras-tically when encountering out-of-distribution (OoD) data, i.e., when training and test data are sampled from different distributions. While a plethora of algorithms have been proposed for OoD generalization, our understanding of the data used to train and evaluate these algorithms re-mains stagnant.
In this work, we ﬁrst identify and mea-sure two distinct kinds of distribution shifts that are ubiq-uitous in various datasets. Next, through extensive experi-ments, we compare OoD generalization algorithms across two groups of benchmarks, each dominated by one of the distribution shifts, revealing their strengths on one shift as well as limitations on the other shift. Overall, we position existing datasets and algorithms from different research ar-eas seemingly unconnected into the same coherent picture.
It may serve as a foothold that can be resorted to by fu-ture OoD generalization research. Our code is available at https://github.com/ynysjtu/ood_bench. 1.

Introduction
Deep learning has been widely adopted in various ap-plications of computer vision [32] and natural language processing [24] with great success, under the implicit as-sumption that the training and test data are drawn from the same distribution, which is known as the independent and identically distributed (i.i.d.) assumption. While neural net-works often exhibit super-human generalization performance
*Nanyang Ye and Kaican Li are the joint ﬁrst authors. Nanyang Ye is the corresponding author.
†Work is done during Haoyue and Runpeng’s internships at Shanghai
Jiao Tong University. on the training distribution, they can be susceptible to minute changes in the test distribution [74, 88]. This is problematic because sometimes true underlying data distributions are sig-niﬁcantly underrepresented or misrepresented by the limited training data at hand. In the real world, such mismatches are commonly observed [28, 42], and have led to signiﬁcant per-formance drops in many deep learning algorithms [11,44,55].
As a result, the reliability of current learning systems is sub-stantially undermined in critical applications such as medical imaging [4, 20], autonomous driving [7, 22, 56, 80, 92], and security systems [37].
Out-of-Distribution (OoD) Generalization, the task of generalizing under such distribution shifts, has been frag-mentarily researched in different areas, such as Domain Gen-eralization (DG) [17, 59, 95, 106], Causal Inference [67, 69], and Stable Learning [104]. In the setting of OoD gener-alization, models usually have access to multiple training datasets of the same task collected in different environments.
The goal of OoD generalization algorithms is to learn from these different but related training environments and then extrapolate to unseen test environments [8, 82]. Driven by this motivation, numerous algorithms have been proposed over the years [106], each claimed to have surpassed all its precedents on a particular genre of benchmarks. How-ever, a recent work [31] suggests that the progress made by these algorithms might have been overestimated—most of the advanced learning algorithms tailor-made for OoD generalization are still on par with the classic Empirical Risk
Minimization (ERM) [90].
In this work, we provide a quantiﬁcation for the dis-tribution shift exhibited in OoD datasets from different research areas and evaluate the effectiveness of OoD generalization algorithms on these datasets, revealing a possible reason as to why these algorithms appear to be no much better than ERM, which is left unexplained in previous work [31]. We ﬁnd that incumbent datasets ex-PACS
Camelyon
ImageNet-V2
NICO
Colored MNIST g n i n i a r
T t s e
T
Diversity shift
?
Correlation shift
Figure 1. Examples of image classiﬁcation datasets demonstrating different kinds of distribution shifts. While it is clear that the datasets at both ends exhibit apparent distribution shifts, in the middle, it is hard to distinguish the differences in distribution between the training dataset and the test dataset (e.g., ImageNet [23] and ImageNet-V2 [74]), which represent a large body of realistic OoD datasets. This motivates us to quantify the distribution shifts in these OoD datasets. hibiting distribution shifts can be generally divided into two categories of different characteristics, whereas the majority of the algorithms are only able to surpass ERM in at most one of the categories. We hypothesize that the phenomenon is due to the inﬂuence of two distinct kinds of distribution shift, namely diversity shift and correlation shift, while pre-existing literature often focuses on merely one of them. The delineation of diversity and correlation shift provides us with a uniﬁed picture for understanding distribution shifts. Based on the ﬁndings and analysis in this work, we make three recommendations for future OoD generalization research:
• Evaluate OoD generalization algorithms comprehen-sively on two types of datasets, one dominated by diver-sity shift and the other dominated by correlation shift.
We provide a method to estimate the strength of these two distribution shifts on any labeled dataset.
• Investigate the nature of distribution shift in OoD prob-lems before designing algorithms since the optimal treatment for different kinds of distribution shift may be different.
• Design large-scale datasets that more subtly capture real-world distribution shifts as imperceptible distribu-tion shifts can also be conspicuous to neural networks. 2. Diversity Shift and Correlation Shift
Normally, datasets such as VLCS [89] and PACS [46] that consist of multiple domains are used to train and evaluate DG models. In these datasets, each domain represents a certain spectrum of diversity in data.1 During experiments, these domains are further grouped into training and test domains, leading to the diversity shift. Although extensive research 1An implicit assumption of DG over the years is that each domain is distinct from one another, which is a major difference between DG and
OoD generalization as the latter considers a more general setting. efforts have been dedicated to the datasets dominated by diversity shift, it is not until recently that [9] draws atten-tion to another challenging generalization problem stemmed from spurious correlations. Colored MNIST, a variant of
MNIST [45], is constructed by coloring the digits with either red or green to highlight the problem. The colored digits are arranged into training and test environments such that the labels and colors are strongly correlated, but the correlation
ﬂips across the environments, creating correlation shift.
As shown in Figure 1, diversity and correlation shift are of clearly different nature. At the extremes, discrepancies between training and test environments become so appar-ent, causing great troubles for algorithms trying to gener-alize [9, 46]. Interestingly, in some real-world cases such as ImageNet versus ImageNet-V2 where the discrepancy is virtually imperceptible, neural networks are still unable to generalize satisfactorily, of which the reason is not fully understood [74]. In Figure 3, our estimation of diversity and correlation shift shed some light on the issue—there is a non-trivial degree of correlation shift between the original
ImageNet and the variant. Besides, other OoD datasets have also shown varying degrees of diversity and correlation shift. 2X is assigned with a label y
In the setting of supervised learning,
Formal Analysis. every input x by some 2Y
ﬁxed labeling rule f :
. The inner mechanism of f
X!Y usually depends on a particular set of features
Z1, whereas the rest of the features
Z2 are not causal to prediction. For example, we assign the label “airplane” to the image of an airplane regardless of its color or whether it is landed or
ﬂying. The causal graph in Figure 2a depicts the interplay among the underlying random variables of our model: the input variable X is determined by the latent variables Z1 and Z2, whereas the target variable Y is determined by Z1 alone. Similar graphs can be found in [2, 53, 57, 63].
Given a labeled dataset, consider its training environment
Z1
X
Y
Z2 (a) Causal graph depicting the causal in-ﬂuence among the concerned variables. (b) Illustration of diversity shift and correlation shift. Diversity shift amounts to half of the summed area of the
, where every integrand can be seen as colored regions in the left ﬁgure. Correlation shift is an integral over q(z2). the summed heights of the colored bars in the right ﬁgure then weighted by the square root of p(z2)
T
·
Figure 2. Explanatory illustrations for diversity and correlation shift. Diversity shift is deﬁned by the support set’s difference of the latent environment’s distribution while correlation shift is deﬁned by the probability density function’s difference on the same support set. 2Y
Etr and
Ete as distributions with probabil-Etr and test environment ity functions p and q respectively. For ease of exposition, we assume no label shift [10] across the environments, i.e.
.2 Without loss of generality, p(y) = q(y) for every y we further assume that
Ete share the same labeling rule f , complementing the causal graph. To put it in the language of causality [66], it means that the direct cause of Y (which is Z1) is observable in both environments and the causal mechanism that Z1 exerts on Y is stable at all times. Formally, it dictates the following property for every z 2Z 1: p(z) 2Y
The existence of such invariant features makes OoD general-ization possible. On the other hand, the presence of z 2Z 2 possessing the opposite property, z) = q(y
: p(y q(z)
^ 8
= 0 z). (1) y
·
|
| p(z) q(z) = 0 y
: p(y z)
= q(y z), (2)
|
·
|
_ 9 2Y makes OoD generalization challenging. From (2), we can
Z2 consists of two kinds of features. Intuitively, see that diversity shift stems from the ﬁrst kind of features in
Z2 since the diversity of data is embodied by novel features not shared by the environments; whereas correlation shift is caused by the second kind of features in
Z2 which is spuriously correlated with some y. Based on this intuition, we partition
Z2 into two subsets, p(z) z
{ z
{ 2Z 2 | 2Z 2 |
:=
:= p(z)
S
T q(z) = 0
}
= 0
} q(z)
,
,
·
· (3) that are respectively responsible for diversity shift and cor-relation shift between the environments. We then deﬁne the quantiﬁcation formula of the two shifts as follows: 2As a side note, by this assumption we do not ignore the existence of label shift in datasets. In practice, datasets with label shift can be made to satisfy this assumption by techniques such as sample reweighting.
Deﬁnition 1 (Diversity Shift and Correlation Shift). Given the two sets of features deﬁned in (3), the proposed
T quantiﬁcation formula of diversity shift and correlation shift between two data distributions p and q is given by and
S
Ddiv(p, q) :=
Dcor(p, q) := 1 2 1 2 where we assume
Y p(z) q(z) dz,
|
 
|
ZS p(z) q(z)
·
ZT p to be discrete. p(y z)
|
  q(y
| z) dz,
 
 
Xy 2Y  
 
Figure 2b illustrates the above deﬁnition when z is unidi-mensional. It can be proved that Ddiv and Dcor are always bounded within [0, 1] (see Proposition 1 in Appendix B).
In particular, the square root in the formulation of corre-lation shift serves as a coefﬁcient regulating the integrand because features that hardly appear in either environment should have a small contribution to the correlation shift over-all. Nevertheless, we are aware that these are not the only viable formulations, yet they produce intuitively reasonable and numerically stable results even when estimated by a simple method described next.
Practical estimation. Given a dataset sampled from
Etr
Ete, a neural and another dataset (of equal size) sampled from network is ﬁrst trained to discriminate the environments.
The network consists of a feature extractor g :
X!F and a classiﬁer h : is some
. The mapping induces two joint learned representation of distributions over
, one for each environment, with probability functions denoted by ˆp and ˆq. For every example from either
Ete, the network tries to tell which environment the example is actually sampled from, in order to minimize the following objective:
F⇥Y !
X
X⇥Y⇥F
Etr or
[0, 1], where
F
E(x,y)
⇠Etr `(ˆex,y, 0) + E(x,y)
⇠Ete`(ˆex,y, 1), (4) 6 6 6
details of our method including pseudo codes of the whole procedure.
We have also shown that in theory the extracted features would converge to a unique solution as the network width grows to inﬁnity using Neural Tangent Kernel [39]. It sug-gests that as long as the network has sufﬁcient capacity, we can always obtain similar results within a small error bound.
To empirically verify this, we have also experimented with different network architectures which demonstrates the sta-bility of our estimation (see Appendix E).
The results in Figure 3 are obtained by the aforemen-tioned method. Most of the existing OoD datasets lie over or near the axes, dominated by one kind of shift. For datasets under unknown distribution shift such as ImageNet-A [35],
ImageNet-R [34], and ImageNet-V2, our method success-fully decomposes the shift into the two dimensions of di-versity and correlation, and therefore one may choose the appropriate algorithms based on the estimation. As shown by our benchmark results in the next section, such choices might be crucial as most OoD generalization algorithms do not perform equally well on two groups of datasets, one dominated by diversity shift and the other dominated by correlation shift. 3. Experiment
Previously, we have numerically positioned OoD datasets in the two dimensions of distribution shift.
In this sec-tion, we run algorithms on these datasets to reveal the two-dimensional trend for existing datasets and algorithms. All experiments are conducted on Pytorch 1.4 with Tesla V100
GPUs. Our code for the following benchmark experiments is modiﬁed from the DomainBed [31] code suite. 3.1. Benchmark
In our experiment, datasets are chosen to cover
Datasets. as much variety from different OoD research areas as pos-sible. As mentioned earlier, the datasets demonstrated two-dimensional properties shown by their estimated diversity and correlation shift. The following datasets are dominated by diversity shift: PACS [46], OfﬁceHome [91], Terra
Incognita [14], and Camelyon17-WILDS [42]. On the other hand, our benchmark also include three datasets domi-nated by correlation shift: Colored MNIST [9], NICO [33], and a modiﬁed version of CelebA [54]. See Appendix G for more detailed descriptions of the above datasets.
For PACS, OfﬁceHome, and Terra Incognita, we train multiple models in every run with each treating one of the domains as the test environment and the rest of the domains as the training environments since it is common practice for DG datasets. The ﬁnal accuracy is the mean accuracy over all such splits. For other datasets, the training and test environments are ﬁxed. A reason is that the leave-one-domain-out evaluation scheme would destroy the designated
Figure 3. Estimation of diversity and correlation shift in various datasets. For ImageNet variants, the estimates are computed with respect to the original ImageNet. See Appendix F for the results in numeric form with error bars. where ˆex,y = h(g(x), y) is the predicted environment and
` is some loss function. The objective forces g to extract those features whose joint distribution with Y varies across the environments so that h could make reasonably accurate predictions. This is formalized by the theorem below. max p(x), q(x)
Theorem 1. The classiﬁcation accuracy of a network trained to discriminate two different environments is bounded above by 1
, as the data size tends to inﬁnity. 2
}
This optimal performance is attained only when the following condition holds: for every x that is not i.i.d. in the two environments, i.e. p(x) such that ˆp(y, z)
= q(x), there exists some y
= ˆq(y, z) where z = g(x). 2X 2Y
{
R
X
·
T
S
F
F and is partitioned by whether ˆp(z) features tion (KDE) [65,76] to estimate ˆp and ˆq over
The proof is provided in Appendix B. After obtaining the extracted by g, we use Kernel Density Estima-. Subsequently,
ˆq(z) is close to zero, in
F correspondence to
, into two sets of features that are responsible for diversity and correlation shift respectively.
The integrals in Deﬁnition 1 are then approximated by Monte
Carlo Integration under importance sampling [61]. A caveat in Dcor(p, q) is that z) in evaluating the term
  the conditional probabilities are computationally intractable for z is continuous. Instead, the term is computed by the following equivalent formula as an application of Bayes’ theorem: p(y q(y z)
|
|
|
|
ˆp(y)
ˆp(z
·
ˆp(z) y)
|
ˆq(y) y)
ˆq(z
·
ˆq(z)
|
 
, (5) where ˆp(z
| for every y y) can be approximated individually again by KDE. See Appendix C for more
|
 
 
  y) and ˆq(z
  2Y
 
 
 
  6 6
Algorithm
PACS
OfﬁceHome
TerraInc
Camelyon17
Average Ranking score
RSC [38]
MMD [48]
SagNet [60]
ERM [90]
IGA [43]
CORAL [85]
IRM [9]
VREx [44]
GroupDRO [79]
ERDG [105]
DANN [27]
MTL [16]
Mixup [101]
ANDMask [64]
ARM [103]
MLDG [47]
Average 0.4" 0.2" 0.4" 0.0 0.4# 0.6" 0.3# 0.1" 0.3# 0.5# 0.4# 0.4# 0.6# 0.0# 0.4# 0.4# 82.8 81.7 81.6 81.5 80.9 81.6 81.1 81.8 80.4 80.5 81.1 81.2 79.8 79.5 81.0 73.0 80.7
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
± 0.4# 0.1" 0.4# 0.2 0.2" 0.3" 0.2# 0.1 0.2 0.4# 0.6# 0.2# 0.5 0.3# 0.2 0.2# 62.9 63.8 62.7 63.3 63.6 63.8 63.0 63.5 63.2 63.0 62.9 62.9 63.3 62.0 63.2 52.4 62.5
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
± 0.5" 0.4# 0.7 0.9 0.8# 0.7# 1.8 0.7# 1.1# 1.2# 0.2# 0.6# 0.3# 1.4# 0.7# 2.0# 43.6 38.3 42.3 42.6 41.3 38.3 42.0 40.7 36.8 41.3 39.5 38.9 39.8 39.8 39.4 27.4 39.8
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
± 0.2" 0.4" 0.2" 0.1 0.1" 0.3# 0.4" 0.3# 0.2" 0.2" 0.0" 0.1" 0.3 0.1" 0.6# 0.4# 94.9 94.9 95.0 94.7 95.1 94.2 95.0 94.1 95.2 95.5 94.9 95.0 94.6 95.3 93.5 91.2 94.6
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
± 71.1 69.7 70.4 70.5 70.2 69.5 70.3 70.0 68.9 70.1 69.6 69.5 69.4 69.2 69.3 61.0 69.4
+2
+2
+1 0 0 0
-1
-1
-1
-2
-2
-2
-2
-2
-3
-4 –
Table 1. Performance of ERM and OoD generalization algorithms on datasets dominated by diversity shift. Every symbol of -1, and every symbol score for each algorithm. denotes a score denotes a score of +1; otherwise the score is 0. Adding up the scores across all datasets produces the ranking
"
# training/test splits of these datasets. For more details about dataset statistics and environment splits, see Appendix G.
Algorithms. We have selected Empirical Risk Minimiza-tion (ERM) [90] and several representative algorithms from different OoD research areas for our benchmark: Group
Distributionally Robust Optimization (GroupDRO) [79],
Inter-domain Mixup (Mixup) [100, 101], Meta-Learning for
Domain Generalization (MLDG) [47], Domain-Adversarial
Neural Networks (DANN) [27], Deep Correlation Alignment (CORAL) [85], Maximum Mean Discrepancy (MMD) [48],
Invariant Risk Minimization (IRM) [9], Variance Risk
Extrapolation (VREx) [44], Adaptive Risk Minimization (ARM) [103], Marginal Transfer Learning (MTL) [16],
Style-Agnostic Networks (SagNet) [60], Representation Self
Challenging (RSC) [38], Learning Explanations that are
Hard to Vary (ANDMask) [64], Out-of-Distribution Gen-eralization with Maximal Invariant Predictor (IGA) [43], and Entropy Regularization for Domain Generalization (ERDG) [105].
Model selection methods. As there is still no consensus on what model selection methods should be used in OoD generalization research [31], appropriate selection methods are chosen for each dataset in our study. To be consistent with existing lines of work [19,38,44,46,60], models trained on PACS, OfﬁceHome, and Terra Incognita are selected by training-domain validation. As for Camelyon17-WILDS and NICO, OoD validation is adopted in respect of [42] and [12]. The two remaining datasets, Colored MNIST and
CelebA, use test-domain validation which has been seen in [1, 9, 44, 70]. Another reason for using test-domain vali-dation is that it may be improper to apply training-domain validation to datasets dominated by correlation shift since under the inﬂuence of spurious correlations, achieving exces-sively high accuracy in the training environments often leads to low accuracy in novel test environments. More detailed explanations of these model selection methods are provided in Appendix H.
Implementation details. Unlike DomainBed, we use a simpler model, ResNet-18 [32], for all algorithms and datasets excluding Colored MNIST, as it is the common practice in previous works [19, 25, 38, 60, 105]. Moreover, we believe smaller models could enlarge the gaps in OoD generalization performance among the algorithms, as larger models are generally more robust to OoD data [34] and thus the performance is easier to saturate on small datasets. The
ResNet-18 is pretrained on ImageNet and then ﬁnetuned on each dataset with only one exception—NICO, which con-tains photos of animals and vehicles largely overlapped with
ImageNet classes. For simplicity, we continue to use a two-layer perceptron following [9, 44, 70] for Colored MNIST.
Our experiments further differs from DomainBed in several minor aspects. First, we do not freeze any batch normaliza-tion layer in ResNet-18, nor do we use any dropout, to be consistent with most of prior works in DG. Second, we use a larger portion (90%) of data from training environments for
Algorithm
Colored MNIST CelebA
NICO
Average Prev score Ranking score
VREx [44]
GroupDRO [79]
ERM [90]
IRM [9]
MTL [16]
ERDG [105]
ARM [103]
MMD [48]
RSC [38]
IGA [43]
CORAL [85]
Mixup [101]
MLDG [47]
SagNet [60]
ANDMask [64]
DANN [27]
Average 1.9" 0.2" 0.9 2.4" 0.1 1.3" 1.8" 0.1" 1.5# 0.5 0.5 1.8# 1.1" 0.7 1.4# 0.8# 56.3 32.5 29.9 60.2 29.3 31.6 34.6 50.7 28.6 29.7 30.0 27.6 32.7 30.5 27.2 24.5 34.5
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
± 0.2 1.1 0.6 1.2# 0.7 0.2# 0.7 0.5# 0.2# 0.7# 0.5# 0.5 1.3# 1.4# 0.2# 0.4# 87.3 87.5 87.2 85.4 87.0 84.5 86.6 86.0 85.9 86.2 86.3 87.5 85.4 85.8 86.2 86.0 86.4
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
± 2.3 0.4 1.6 2.1 0.8 1.9 0.2# 1.2# 1.9" 0.1 1.0 1.5 2.4# 0.7# 0.8 1.7# 71.5 71.0 72.1 73.3 70.6 72.7 67.3 68.9 74.3 71.0 70.8 72.5 66.6 69.8 71.2 69.4 70.8
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
± 71.7 63.7 63.1 73.0 62.3 62.9 62.8 68.5 61.4 62.3 61.5 60.6 56.6 62.0 61.5 59.7 63.7
-1
-1 0
-1
-2
-2
-3
+2
+2 0
-1
-2
-4
+1
-2
-2 –
+1
+1 0 0 0 0 0
-1
-1
-1
-1
-1
-1
-2
-2
-3 –
Table 2. Performance of ERM and OoD generalization algorithms on datasets dominated by correlation shift. Every symbol score of -1, and every symbol ranking score for each algorithm. Prev scores are the scores of corresponding algorithms in Table 1. denotes a denotes a score of +1; otherwise the score is 0. Adding up the scores across all datasets produces the
"
# training and the rest for validation. Third, we use a slightly different data augmentation scheme following [19].
Finally, we adopt the following hyperparameter search protocol, the same as in DomainBed: a 20-times random search is conducted for every pair of dataset and algorithm, and then the search process is repeated for another two ran-dom series of hyperparameter combinations, weight initial-ization, and dataset splits. Altogether, the three series yield the three best accuracies over which a mean and standard error bar is computed for every dataset-algorithm pair. See
Appendix J for the hyperparameter search space for every individual algorithm.
Results. The benchmark results are shown in Tab. 1 and
Tab. 2. In addition to mean accuracy and standard error bar, we report a ranking score for each algorithm with respect to ERM. For every dataset-algorithm pair, depending on whether the attained accuracy is lower than, within, or higher than the standard error bar of ERM accuracy on the same dataset, we assign score -1, 0, +1 to the pair. Adding up the scores across all datasets produces the ranking score for each algorithm. We underline that the ranking score does not indicate whether an algorithm is deﬁnitely better or worse than the other algorithms. It only reﬂects a relative degree of robustness against diversity and correlation shift.
From Tab. 1 and Tab. 2, we observe that none of the
OoD generalization algorithms achieves consistently bet-ter performance than ERM on both OoD directions. For example, on the datasets dominated by diversity shift, the ranking scores of RSC, MMD, and SagNet are higher than
ERM, whereas on the datasets dominated by correlation shift, their scores are lower. Conversely, the algorithms (VREx and GroupDRO) that outperform ERM in Tab. 2 are worse than ERM on datasets of the other kind. This supports our view that OoD generalization algorithms should be evalu-ated on datasets embodying both diversity and correlation shift. Such a comprehensive evaluation is of great impor-tance because real-world data could be tainted by both kinds of distribution shift, e.g., the ImageNet variants in Figure 3.
In the toy case of Colored MNIST, several algorithms are superior to ERM, however, in the more realistic and com-plicated cases of CelebA and NICO, none of the algorithms surpasses ERM by a large margin. Hence, we argue that con-temporary OoD generalization algorithms are by large still vulnerable to spurious correlations. In particular, IRM that achieves the best accuracy on Colored MNIST among all algorithms, fails to surpass ERM on the other two datasets.
It is in line with the theoretical results discovered by [77]:
IRM does not improve over ERM unless the test data are sufﬁciently similar to the training distribution. Besides, we have also done some experiments on ImageNet-V2, and the result again supports our argument (see Appendix I).
Due to inevitable noises and other changing factors in the chosen datasets and training process, whether there is any compelling pattern in the results across the datasets domi-nated by the same kind of distribution shift is unclear. So, it is important to point out that the magnitude of diversity and correlation shift does not indicate the absolute level of difﬁ-(a) Estimation of correlation shift under varying
⇢tr and ⇢te. Digits are in red and green only. (b) Estimation of diversity shift under varying
µtr and µte while ﬁxing ⇢tr = 0.1, ⇢te = 0.9 and  tr =  te = 0.1. (c) Estimation of correlation shift under varying
µtr and µte while ﬁxing ⇢tr = 0.1, ⇢te = 0.9 and  tr =  te = 0.25.
Figure 4. Estimation of diversity and correlation shift under varying color distribution in Colored MNIST. Another color, blue, uncorrelated with the labeled classes, is added onto the digits to create diversity shift. The intensity of blue is sampled from a truncated Gaussian distribution for every image. Assuming only one training and one test environment, ⇢tr and ⇢te stand for the correlation between red/green and the digits; µtr and µte stand for the mean intensities of blue;  tr and  te stand for the standard deviations. culty for generalization. Instead, it represents a likelihood that certain algorithms will perform better than some other algorithms under the same kind of distribution shift. 3.2. Further Study
In this section, we conduct further experiments to check the reliability of our estimation method for diversity and correlation shift and compare our method against other exist-ing metrics for measuring the non-i.i.d. property of datasets, demonstrating the robustness of our estimation method and the signiﬁcance of diversity and correlation shift.
Sanity check and numerical stability. To validate the robustness of our estimation method, we check whether it can produce stable results that faithfully reﬂect the expected trend as we manipulate the color distribution of Colored
MNIST. For simplicity, only one training environment is assumed. To start with, we manipulate the correlation coefﬁ-cients ⇢tr and ⇢te between digits and colors in constructing the dataset. From Figure 4a, we can observe that when ⇢tr and ⇢te have similar values, the estimated correlation shift is negligible. It aligns well with our deﬁnition of correlation shift that measures the distribution difference of features present in both environments. As for examining on the esti-mation of diversity shift, another color, blue, is introduced in the dataset. The intensity (between 0 and 1) of blue added onto each digit is sampled from truncated Gaussian distribu-tions with means µtr, µte and standard deviations  tr,  te for training and test environment respectively. Meanwhile, the intensity of red and green is subtracted by the same amount.
From Figure 4b, we observe that as the difference in color varies between red/green and blue, the estimate of diversity shift varies accordingly (at the corners). Lastly, we investi-gate the behavior in the estimation of correlation shift while keeping the correlation coefﬁcients ﬁxed and manipulating
µtr and µte that controls diversity shift. Figure 4c shows a trade-off between diversity and correlation shift, as implied by their deﬁnitions. Experiments in every grid cell are con-ducted only once, so the heatmaps also reﬂect the variance in our estimation, which can be compensated by averaging over multiple runs.
Comparison with other measures of distribution shift.
We also compare OoD-Bench with other measures of distri-bution shift. The results on the variants of Colored MNIST are shown in Tab. 3. We empirically show that general met-rics for measuring the discrepancy between distributions, such as EMD [78] and MMD [30], are not very informative.
Speciﬁcally, EMD and MMD are insensitive to the correla-tion shift in the datasets, while EMD is also insensitive to the diversity shift. Although NI [33] can produce compar-ative results on correlation shift, it is still unidimensional like EMD and MMD, not discerning the two kinds of dis-tribution shift present in the datasets. In comparison, our method provides more stable and interpretable results. As ⇢tr an ⇢te gradually become close, the estimated correlation shift reduces to zero. On the other hand, the estimated diversity shift remains constant zero until the last scenario where our method again produces the expected answer. 4.