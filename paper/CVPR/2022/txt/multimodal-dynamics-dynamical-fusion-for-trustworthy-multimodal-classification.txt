Abstract
Integration of heterogeneous and high-dimensional data (e.g., multiomics) is becoming increasingly important. Ex-isting multimodal classification algorithms mainly focus on improving performance by exploiting the complemen-tarity from different modalities. However, conventional approaches are basically weak in providing trustworthy multimodal fusion, especially for safety-critical applica-tions (e.g., medical diagnosis). For this issue, we pro-pose a novel trustworthy multimodal classification algo-rithm termed Multimodal Dynamics, which dynamically evaluates both the feature-level and modality-level infor-mativeness for different samples and thus trustworthily in-tegrates multiple modalities. Specifically, a sparse gat-ing is introduced to capture the information variation of each within-modality feature and the true class probability is employed to assess the classification confidence of each modality. Then a transparent fusion algorithm based on the dynamical informativeness estimation strategy is induced.
To the best of our knowledge, this is the first work to jointly model both feature and modality variation for different sam-ples to provide trustworthy fusion in multi-modal classifica-tion. Extensive experiments are conducted on multimodal medical classification datasets. In these experiments, supe-rior performance and trustworthiness of our algorithm are clearly validated compared to the state-of-the-art methods. 1.

Introduction
Multimodal learning has achieved impressive success in a wide spectrum of applications (e.g., medical-diagnosis
[16, 52]), which improves the performance by exploring the complementary information from different modalities.
Representative multimodal methods typically integrate dif-∗ Equal contribution. † Corresponding authors. ‡ Supported by 2021
Tencent Rhino-Bird Research Elite Training Program. ferent modalities into a unified representation with power-ful neural networks [29, 34, 45, 61, 63, 64, 71, 72, 74]. De-spite encouraging progress, traditional multimodal models are still unreliable due to the limitation of existing fusion strategies. As a result, existing multimodal learning also challenges itself in deployment for safety-critical applica-tions (e.g., computer-aided diagnosis). This inspires us to utilize multimodal information in a more elegant way to produce trustworthy multimodal fusion.
For multimodal learning, traditional methods mainly fo-cus on obtaining a common or joint representation by ex-ploring the correlated and complementary information be-tween different modalities with powerful neural networks
[8, 65]. Some existing multimodal methods obtain a joint representation by simply concatenating the features ob-tained from different modalities [26,32]. Then a neural net-work is employed to explore the joint representation. Be-sides, joint representations can be obtained through care-fully designed objective functions [3, 4, 27, 63] and neural network architectures [6, 38, 62]. Although effective, these methods are weak in dynamically perceiving the informa-tiveness of each feature and modality for different samples, which could enhance the trustworthiness (including stabil-ity and explainablity) in multimodal classification. In multi-modal medical data, as shown in Fig. 1, uninformative fea-tures and modalities widely exist due to the unsatisfactory data collection (e.g., inherent noise in multiomics data [7], uneven quality of histopathological images for different pa-tients [68] and tabular data with complex missing patterns and feature noise [70]). This motivates us to evaluate the in-formativeness of each feature and each modality of different samples, and conduct a dynamical multimodal fusion.
In this work, we propose a novel algorithm termed Multi-modal Dynamics for trustworthy multimodal classification, which models the feature and modality informativeness to promote the fusion stability and explainablity. Specifically, we introduce a sparse gating strategy to dynamically ob-tain the informative features for different samples, and the modality confidence is introduced to dynamically evalu-(a) Dynamics in multimodal data. (b) Proposed multimodal learning paradigm.
Figure 1. (a) Illustration of feature and modality dynamics in multimodal data. For one modality, the informativeness of different features may vary with the samples. Meanwhile, the informativeness of different modalities may also change for different samples. (b) To capture the dynamics, multimodal dynamics paradigm is proposed, where feature and modality informativeness is dynamically evaluated to pro-mote multimodal fusion. ate the informativeness of different modalities for different samples. Accordingly, a unified multimodal fusion frame-work is introduced to dynamically fuse informative features and modalities, and to reduce the influence from noisy fea-tures and modalities, endowing the model with robustness for dynamic variation of quality for features and modalities, and trustworthiness for final decision. For clarification, the contributions of our method could be summarized as fol-lows: (i) We propose a dynamical multimodal fusion strat-egy, which models both the feature-level and modality-level dynamicities to provide a trustworthy multimodal fusion.
To the best of our knowledge, the proposed method is the first work to exploit the feature-level and modality-level dy-namicities for trustworthy multimodal fusion. (ii) We intro-duce effective mechanisms, i.e., sparse gating and true class probability approximation to dynamically estimate the dy-namicity of each feature and modality, which are coopera-tive for the optimal prediction. (iii) We conduct experiments on four multimodal medical classification datasets and the experimental results demonstrate significant improvement against state-of-the-art methods. Qualitative experiments also validate the trustworthiness and interpretability in mod-eling the multimodal dynamicity. 1 2.