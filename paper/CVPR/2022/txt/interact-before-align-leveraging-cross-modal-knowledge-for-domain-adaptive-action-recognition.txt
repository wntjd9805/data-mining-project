Abstract
Unsupervised domain adaptive video action recognition aims to recognize actions of a target domain using a model trained with only out-of-domain (source) annotations. The inherent complexity of videos makes this task challenging but also provides ground for leveraging multi-modal in-puts (e.g., RGB, Flow, Audio). Most previous works utilize the multi-modal information by either aligning each modal-ity individually or learning representation via cross-modal self-supervision. Different from previous works, we find that the cross-domain alignment can be more effectively done by using cross-modal interaction first. Cross-modal knowledge interaction allows other modalities to supplement missing transferable information because of the cross-modal com-plementarity. Also, the most transferable aspects of data can be highlighted using cross-modal consensus.
In this work, we present a novel model that jointly con-siders these two characteristics for domain adaptive action recognition. We achieve this by implementing two modules, where the first module exchanges complementary transfer-able information across modalities through the semantic space, and the second module finds the most transferable spatial region based on the consensus of all modalities. Ex-tensive experiments validate that our proposed method can significantly outperform state-of-the-art methods on multi-ple benchmark datasets, including the complex fine-grained dataset EPIC-Kitchens-100. 1.

Introduction
Unsupervised domain adaptation (UDA) models aim at learning features on the source dataset that can also be used on the target dataset. Due to its potential in reducing the necessity of large-scale labeling, UDA has been extensively explored for tasks such as image recognition [33,49,52,58], semantic segmentation [3, 64] and object detection [5, 8].
With one additional temporal dimension, video data is
*Corresponding author.
Figure 1. Different from existing UDA works that directly align the multi-modal inputs (a), we find that it is more effective to first enhance the transferability of each modality by cross-modal inter-action, and then perform cross-domain alignment (b). more complex than image data, and the domain gap not only resides in the appearance difference of environments but also in the motion variance when different people per-form the same action. This prevents the direct application of image-based domain adaptation methods on the domain adaptive action recognition task [6, 20]. One direction to address this complexity is to use additional modality in-formation (e.g. optical flow, audio). Other than directly combining multi-modal inputs [38], recent works add self-supervised modality alignment to implicitly learn properties of source and target data [24,36,47]. However, since the ob-jectives of cross-modal alignment and cross-domain align-ment are not perfectly consistent, simultaneously aligning modality and aligning domain can distract the learning tar-get, i.e., minimizing the domain discrepancy.
Due to different characteristics, the transferability (i.e., invariance of feature across domains) of each modality lies in different and complementary perspectives. For example, for an action “wash cup” on the target domain, since the sound of water is similar across domains, the audio modal-ity is more transferable to determine the verb “wash” of the action. Meanwhile, although RGB cannot perform as good as audio when recognizing the verb on the target domain, it can well recognize the noun “cup” on the target domain based on its domain-transferable appearance knowledge. If
these two modalities can interact with each other and ex-change their unique domain-transferable knowledge, both of them can enhance their transferability and finally de-termine the action “wash cup” accurately. Based on this observation, we leverage this cross-modal complementar-ity and propose a Mutual Complementarity (MC) module that allows each modality to refine its feature by absorbing the transferable knowledge from other modalities, thus the transferability of all modalities can be enhanced.
Another aspect brought by multiple modalities is the cross-modal consensus. Since domain shift is often ac-companied by changes of the scenario background, find-ing and focusing on more transferable foreground objects is critical. Rather than applying spatial attention like pre-vious works [27, 55] which introduce additional parame-ters that also suffer from domain gap, we instead use a parameter-free correlation-based spatial consensus opera-tion. Leveraging multi-modal features, we find and empha-size the transferable regions which share consensus among different modalities by developing a cross-modal Spatial
Consensus (SC) module. Compared with spatial attention, our proposed consensus operation is proved in the experi-ments to be more suitable for domain adaptation.
We conduct experiments on the standard UCF-HMDB dataset and EPIC-Kitchens-55 dataset. Our experiments demonstrate that with cross-modal knowledge interaction, our proposed method can outperform state-of-the-art meth-ods significantly. We also show that our method can bring remarkable enhancement on the EPIC-Kitchens-100 dataset that contains challenging fine-grained actions.
Our contributions can be summarized as follows:
• We propose a novel model to enhance multi-modality fea-tures for domain adaptive action recognition. To our best knowledge, this is the first work to consider cross-modal interaction for increasing the feature transferability across domains.
• We propose to use correlation-based operation to evaluate the transferability of spatial locations, which is proved to be simple and effective compared with spatial attention in the context of domain adaptation.
• Our proposed model achieves state-of-the-art perfor-mance on multiple datasets, including the challenging
EPIC-Kitchens-100 dataset with fine-grained actions. 2.