Abstract 1.

Introduction
Neural radiance fields (NeRF) encode a scene into a neural representation that enables photo-realistic rendering of novel views. However, a successful reconstruction from
RGB images requires a large number of input views taken under static conditions — typically up to a few hundred images for room-size scenes. Our method aims to synthe-size novel views of whole rooms from an order of magnitude fewer images. To this end, we leverage dense depth priors in order to constrain the NeRF optimization. First, we take ad-vantage of the sparse depth data that is freely available from the structure from motion (SfM) preprocessing step used to estimate camera poses. Second, we use depth completion to convert these sparse points into dense depth maps and uncertainty estimates, which are used to guide NeRF op-timization. Our method enables data-efficient novel view synthesis on challenging indoor scenes, using as few as 18 images for an entire scene.
Synthesizing realistic views from varying viewpoints is essential for interactions between humans and virtual envi-ronments, hence it is of key importance for many virtual reality applications. The novel view synthesis task is es-pecially relevant for indoor scenes, where it enables vir-tual navigation through buildings, tourist destinations, or game environments. When scaling up such applications, it is preferable to minimize the amount of input data required to store and process, as well as its acquisition time and cost.
In addition, a static scene requirement is increasingly diffi-cult to fulfill for a longer capture duration. Thus, our goal is novel view synthesis at room-scale using few input views.
NeRF [21] represents the radiance field and density dis-tribution of a scene as a multi-layer perceptron (MLP) and uses volume rendering to synthesize output images. This approach creates impressive, photo-realistic novel views of scenes with complex geometry and appearance. Unfor-tunately, applying NeRF to real-world, room-scale scenes given only tens of images does not produce desirable re-sults (Fig. 1) for the following reason: NeRF purely relies on RGB values to determine correspondences between in-put images. As a result, high visual quality is only achieved by NeRF when it is given enough images to overcome the inherent ambiguity of the correspondence problem. Real-world indoor scenes have characteristics that further com-plicate the ambiguity challenge: First, in contrast to an
“outside-in” viewing scenario of images taken around a cen-tral object, views of rooms represent an “inside-out” view-ing scenario, in which the same number of images will ex-hibit significantly less overlap with each other. Second, in-door scenes often have large areas with minimal texture, such as white walls. Third, real-world data often has in-consistent color values across views, e.g., due to white bal-ance or lens shading artifacts. These characteristics of in-door scenes are likewise challenging for SfM, leading to very sparse SfM reconstructions, often with severe outliers.
Our idea is to use this noisy and incomplete depth data and from it produce a complete dense map alongside a per-pixel uncertainty estimate of those depths, thereby increasing its value for NeRF — especially in textureless, rarely observed, or color-inconsistent areas.
We propose a method that guides the NeRF optimiza-tion with dense depth priors, without the need for additional depth input (e.g., from an RGB-D sensor) of the scene. In-stead, we take advantage of the sparse reconstruction that is freely available as a byproduct of running SfM to com-pute camera pose parameters. Specifically, we complete the sparse depth maps with a network that estimates uncertainty along with depth. Taking uncertainty into account, we use the resulting dense depth to constrain the optimization and to guide the scene sampling. We evaluate the effective-ness of our method on complete rooms from the Matter-port3D [2] and ScanNet [6] datasets, using only a handful of input images. We show that our approach improves over recent and concurrent work that uses sparse depth from SfM or multi-view stereo (MVS) in NeRF [10, 26].
In summary, we demonstrate that dense depth priors with uncertainty estimates enable novel view synthesis with
NeRF on room-size scenes using only 18–36 images, en-abled by the following contributions: 1. A data-efficient approach to novel view synthesis on real-world scenes at room-scale. 2. An approach to enhance noisy sparse depth input from
SfM to support the NeRF optimization. 3. A technique for accounting for variable uncertainty when guiding NeRF with depth information. 2.