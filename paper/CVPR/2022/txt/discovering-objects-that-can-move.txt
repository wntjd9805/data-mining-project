Abstract
This paper studies the problem of object discovery – sep-arating objects from the background without manual labels.
Existing approaches utilize appearance cues, such as color, texture, and location, to group pixels into object-like regions.
However, by relying on appearance alone, these methods fail to separate objects from the background in cluttered scenes.
This is a fundamental limitation since the definition of an object is inherently ambiguous and context-dependent. To re-solve this ambiguity, we choose to focus on dynamic objects – entities that can move independently in the world. We then scale the recent auto-encoder based frameworks for unsuper-vised object discovery from toy synthetic images to complex real-world scenes. To this end, we simplify their architec-ture, and augment the resulting model with a weak learning signal from general motion segmentation algorithms. Our experiments demonstrate that, despite only capturing a small subset of the objects that move, this signal is enough to gener-alize to segment both moving and static instances of dynamic objects. We show that our model scales to a newly collected, photo-realistic synthetic dataset with street driving scenar-ios. Additionally, we leverage ground truth segmentation and flow annotations in this dataset for thorough ablation and evaluation. Finally, our experiments on the real-world
KITTI benchmark demonstrate that the proposed approach outperforms both heuristic- and learning-based methods by capitalizing on motion cues. 1.

Introduction
Objects are the key building blocks of perception [31, 50].
We understand the world not in terms of pixels, surfaces, or entire scenes, but rather in terms of individual objects and their combinations. Object-centric representation makes tractable higher-level cognitive abilities such as casual rea-soning, planning, etc., and is crucial for generalization and adaptation [5, 60]. In computer vision, progress has been achieved in object recognition recently [9, 24, 46], but these
*Equal contribution
†Work done during an internship at TRI
Figure 1. A sample from the TRI-PD dataset with: (a) motion segmentation from [14], top-10 segments produced by (b) our ap-proach, (c) heuristic-based MCG [3], and (d) learning-based SlotAt-tention [38]. Our method uses noisy, sparse motion segmentation to learn to separate both moving and static instances of dynamic objects from the background, whereas others cannot resolve the object definition ambiguity based on appearance alone. approaches rely on large amounts of expensive manual la-bels, and only cover a fixed vocabulary of object categories.
Discovering objects and their extent in data – in a manner that generalizes across domains – remains an open problem.
What makes this task especially challenging is that the notion of an object is inherently ambiguous and context-dependent. Consider a car in Figure 1: its left door and the handle on that door can be treated as individual objects, or parts of the whole. It is thus not surprising that existing approaches that attempt to automatically separate objects from the background based on appearance struggle beyond controlled scenarios. In particular, classical methods that use graph-based inference tend to over- or under-segment the objects [3, 18] (Figure 1, bottom left). More recent learning-based methods model object discovery with struc-tured generative networks, often leveraging iterative infer-ence in the bottleneck of an auto-encoder [8, 16, 22, 37, 38].
While promising results have been demonstrated, these ap-proaches are typically restricted to toy images with colored geometric shapes on a plain background, and completely fail on realistic scenes (Figure 1, bottom right).
We posit that while the ambiguity of the object definition
is not resolvable in the static image world without direct su-pervision, it has a natural resolution in the dynamic world of videos. Concretely, we choose to focus on dynamic objects, which we define as entities that are capable of moving inde-pendently in the world. Independent object motion is a strong grouping cue, which has been shown to drive object learning in animal perception [13,49]. In computer vision, there exists a long line of works on motion segmentation that automat-ically separate moving objects from the background based on optical flow [7, 14, 33, 41, 41, 61]. These methods have found numerous applications in unsupervised [2, 43] and weakly-supervised machine learning algorithms [27, 44, 56].
In this work, we show how motion segmentation can be bootstrapped to group instances even when they are static.
We build our approach on top of the framework for unsu-pervised object discovery proposed by Locatello et al. [38], and show how to scale it from toy images to realistic videos.
We extend the architecture to videos of arbitrary length by introducing a spatio-temporal memory module [4], and sim-plify the grouping mechanism to scale the model to realistic scenes with large resolution and dozens of objects. We then demonstrate the importance of inductive biases based on in-dependent object motion on the emergent representation and the extent to which it captures objects. In particular, we show how motion segments (Figure 1, top left) can guide the atten-tion operation to discover static objects. Crucially, we show that motion segmentation of varying quality – even when sparse and noisy – can be sufficient to bias the model towards segmenting both moving and static instances (Figure 1, top right). Our approach only requires videos for training, and can segment objects in static images at inference time.
To go beyond the toy data used in [38], while still being able to thoroughly analyze the various aspects of the method, we utilize a new, photo-realistic, synthetic dataset collected using the ParallelDomain platform [1] (TRI-PD). It consists of hundreds of videos, with crowded, street driving scenes, and comes with a full set of ground truth annotations, includ-ing object segmentations, 3D coordinates, and optical flow, allowing us to ablate the importance of the quality of the motion segmentation to the method’s performance. Finally, we demonstrate that the resulting method generalizes to real videos on the challenging KITTI dataset [19], and com-pare it to existing heuristic- and learning-based approaches.
Our code, models, and synthetic data are made available at https://github.com/zpbao/Discovery_Obj_
Move/. 2.