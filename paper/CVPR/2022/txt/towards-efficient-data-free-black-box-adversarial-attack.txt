Abstract
Classic black-box adversarial attacks can take advan-tage of transferable adversarial examples generated by a similar substitute model to successfully fool the target model. However, these substitute models need to be trained by target models’ training data, which is hard to acquire due to privacy or transmission reasons. Recognizing the limited availability of real data for adversarial queries, re-cent works proposed to train substitute models in a data-free black-box scenario. However, their generative adver-sarial networks (GANs) based framework suffers from the convergence failure and the model collapse, resulting in low efficiency. In this paper, by rethinking the collabora-tive relationship between the generator and the substitute model, we design a novel black-box attack framework. The proposed method can efficiently imitate the target model through a small number of queries and achieve high at-tack success rate. The comprehensive experiments over six datasets demonstrate the effectiveness of our method against the state-of-the-art attacks. Especially, we con-duct both label-only and probability-only attacks on the
Microsoft Azure online model, and achieve a 100% attack success rate with only 0.46% query budget of the SOTA method [49]. 1.

Introduction
Recently, deep neural networks (DNNs) have been em-ployed as a fundamental technique in the advancement of artificial intelligence in both established and emerging fields [24–28, 31–33, 42, 45, 46, 48]. Despite the success of
DNNs, recent studies have identified that DNNs are vulner-able to adversarial examples [3, 6, 13, 16, 30, 41]. A virtu-ally imperceptible perturbation to an image can lead a well
*Both author contributed equally to this work. Work is completed dur-ing Jie Zhang’s internship at Tencent Youtu Lab.
‡Corresponding author.
Figure 1. Efficiency comparison with the state-of-the-art meth-ods DaST [49] and DFME [43]. The left subplot shows substitute models accuracy and the right subplot shows untargetd attack suc-cessful rate. Attacks are conducted on MNIST in probability-only scenarios with query budget Q = 40k (1k = 1000). trained DNN to misclassify. Consequently, the security con-cerns about DNNs have attracted many researchers’ interest in studying the adversarial vulnerability and robustness of networks [29].
Classical works [2, 13, 34] perform attacks in the white-box setting: with full access to the model’s parameters and architectures, they can directly use gradient-based opti-mization to find successful adversarial examples. However, this attack scenario is usually unavailable in real-world de-ployment due to privacy and security. As a more practical scenario in real-world systems, black-box attacks assume that attackers can only query the target network and obtain its outputs (probability or label) for a given input. By query-ing the target network with real images, malicious attack-ers can train substitute models to imitate the target models.
Then the substitute models can be used to generate adver-sarial examples [8, 17, 39] to attack the target model based on the transferability [10, 11, 41] of these adversarial ex-amples. However, substitute models need to be trained by target models’ training data, which is hard to acquire due to privacy or transmission reasons.
Recently, some researchers [43, 44, 49] have recognized the limited availability of real data for adversarial queries
and proposed to train substitute models in a data-free black-box scenario. By adopting the principle of generative adver-sarial networks (GANs), they [43, 49] tried to address this problem with a competition game: A generator is respon-sible for synthesising some input images, and the substitute model is trained to imitate the target model on these images.
In this game, the two adversaries — a substitute model and a generator model, respectively try to minimize and max-imize the matching rate between the substitute model and the target model. However, it is very difficult to accurately quantify substitute-target disagreement in a black-box sce-nario, let alone directly using this object to train the genera-tor. Consequently, this unstable training process makes the models hard to converge. Even after an unlimited number of queries, their approach inevitably leads to model collapse, and can barely reach their ideal Nash equilibrium point in practice (We empirically verify these phenomenons in Sec-tion 4). Though the prior art has shed the light on data-free substitute models training, these methods require a large number of queries, which is not practical in real-world set-tings (e.g., 2M (million) queries to attack the online model on Microsoft Azure [49]). Actually, commercial models are often deployed as pay-per-query prediction APIs for the sake of the protection of data privacy. It remains an open and very challenging problem: how to effectively learn a substitute model with a limited query budget?
In this paper, we consider a more stringent yet more prac-tical adversarial scenario, a black-box model with no access to the real data and limited budgets for querying the target model. Rethinking the collaborating relationship between the generator and the substitute model, we design a pow-erful black-box attack framework. As shown in Figure 1, the proposed method can efficiently imitate the target model through a small number of queries and achieve high attack success rate in both probability and label based black-box settings. Our contributions are as follows: (1) We revisit the convergence problem of previous data-free attack methods caused by their unstable training pro-cess.
Instead of training the generator with the inaccu-rate substitute-target disagreement, we change the game be-tween the generator and the substitute model. The two col-laborating players are no longer forced to directly compete
Instead, we give them in one minimize-maximize game. different objectives. Especially for the generator, we reset its objective as synthesising surrogate dataset whose distri-bution is close to the target training data. While, the substi-tute model aim to efficiently imitate the target model with the generated training examples. In our new game, the gen-erator and the substitute model have relatively independent optimization processes, which allows the substitute model to converge more stably to the target model. (2) Besides the problem of convergence, the previous methods suffer from the model collapse, resulting in low substitute model accuracy and low attack success rate. We attempt to alleviate the mode collapse problem in data-free substitute model training, through the lens of balancing data distribution and promoting data diversity. On one hand, we maximize the information entropy of the synthetic data in each batch. When it maximizes, the categories are evenly distributed. On the other hand, we randomly smooth the pseudo ground-truth labels and steer the generator to syn-thesis diverse data in each category. (3) To further improve the training efficiency of the sub-stitute model, we propose to go deeper into the utilization of synthetic data. To achieve higher attack success rate, the substitute model are encouraged to have decision bound-aries that are highly consistent with the target model. Ac-cordingly, we argue that there are two types of data that need to be given extra attention. And we design two losses to boost the training of the substitute model. (4) Our empirical evaluations on six datasets under both untargeted and targeted attacks show that the proposed method can efficiently imitate a target model using a small number of queries and successfully generate adversarial ex-amples using the substitute model. Specifically, we achieve 98.0% untargeted attack success rate in the label-only scenario on CIFAR10 with only 3.75% query budget of the previous SOTA method DFME [43]. Moreover, we con-duct both label-only and probability-only attacks on the Mi-crosoft Azure online model, and achieve a 100% attack success rate with only 0.46% query budget of the previ-ous SOTA method DaST [49]. 2.