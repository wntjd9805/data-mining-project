Abstract
The nonuniform quantization strategy for compressing neural networks usually achieves better performance than its counterpart, i.e., uniform strategy, due to its supe-rior representational capacity. However, many nonuni-form quantization methods overlook the complicated pro-jection process in implementing the nonuniformly quantized weights/activations, which incurs non-negligible time and space overhead in hardware deployment. In this study, we propose Nonuniform-to-Uniform Quantization (N2UQ), a method that can maintain the strong representation abil-ity of nonuniform methods while being hardware-friendly and efficient as the uniform quantization for model infer-ence. We achieve this through learning the flexible in-equidistant input thresholds to better fit the underlying distribution while quantizing these real-valued inputs into equidistant output levels. To train the quantized network with learnable input thresholds, we introduce a gener-alized straight-through estimator (G-STE) for intractable backward derivative calculation w.r.t. threshold param-eters. Additionally, we consider entropy preserving reg-ularization to further reduce information loss in weight quantization. Even under this adverse constraint of im-posing uniformly quantized weights and activations, our
N2UQ outperforms state-of-the-art nonuniform quantiza-tion methods by 0.5 ∼ 1.7% on ImageNet, demonstrating the contribution of N2UQ design. Code and models are available at: https://github.com/liuzechun/
Nonuniform-to-Uniform-Quantization. 1.

Introduction
Deep Neural Networks (DNNs) have demonstrated great success in various real-world applications [16, 41]. De-spite their remarkable results, the large model size and high computational cost hinder pervasive deployment of DNNs, especially on resource-constrained devices. A number of
Figure 1. (a) Previous nonuniform quantization function outputs weights and activations in in-equidistant levels, which requires the post-processing of mapping floating-point levels to binary digits in order to obtain the speed-up effect of quantization [1, 13, 50]. (b)
The proposed N2UQ learns input thresholds to allow more flexibil-ity, while outputs uniformly quantized values, enabling hardware-friendly linear mappings and efficient bitwise operations. The in-tractable gradient computation w.r.t. input thresholds is tackled with the proposed generalized straight-through estimator (G-STE). approaches have been proposed to compress and acceler-ate DNNs, including channel pruning [27, 29], quantiza-tion [31, 47, 50], neural architecture search [6, 43], etc.
Among these methods, quantization-based methods have shown promising results in compressing the model size by representing weights with fewer bits, and faster inference by replacing computationally-heavy convolution operations with efficient bitwise operations [36, 47]. Despite these ad-vantages, quantized DNNs still have a non-negligible per-formance gap from their full-precision counterparts, espe-cially with extremely low-bit quantization. For example,
the 2-bit classic uniformly quantized ResNet-50 achieves 67.1% top-1 accuracy [50] on ImageNet dataset, a drop of 9.9% compared to a real-valued ResNet-50. This perfor-mance gap mainly results from the quantization error in rep-resenting real-valued weights and activations with a limited number of quantized levels and the inflexibility for uniform quantizers to adapt to different distributions of input values.
To better fit underlying distributions and mitigate quan-tization errors, several previous studies proposed nonuni-form quantization by adjusting the quantization resolution according to the density of real-valued distribution [46, 47].
However, the accuracy improvement of nonuniform quanti-zation usually comes at the expense of hardware implemen-tation efficiency [13, 50]. Since the output of the nonuni-form quantization are floating-point weights and activa-tions, their multiplication can no longer be directly accel-erated by the bitwise operation between binaries [22]. A common solution to addressing this issue has been build-ing look-up tables (LUTs) to map floating-point values to binary digits [15, 46], as shown in Fig. 1(a). This post-processing incurred by nonuniform quantization costs more hardware area and consumes additional energy compared to uniform quantization [1, 13].
The goal of this study is to develop a new quantization method maintaining the hardware projection simplicity as uniform quantization and meanwhile offering the flexibil-ity to achieve the merit of nonuniform quantization. De-spite that it is desirable to enforce a quantizer’s outputs (i.e., quantized weights/activations) to have uniform quantization levels in order not to incur additional post-processing tasks, each of the output levels does not necessarily need to rep-resent an equal range of the real-valued input. As shown in Fig. 1 (b), we enforce the output quantization levels to be equidistant while learning the thresholds on the input values to incorporate more flexibility in fitting the underlying real-valued distributions for quantization. We name this quan-tizer design as Nonuniform-to-Uniform Quantizer (N2UQ). it is challenging to optimize such a non-uniform-to-uniform quantizer, which can automatically learn to adapt the input thresholds through network train-ing for higher precision. Because the gradient computation w.r.t. the threshold parameters is intractable, and cannot be resolved by the existing gradient estimation method for quantization, i.e., the straight-through estimator (STE) [2].
STE simply estimates the incoming gradient to a threshold operation to be equal to the outgoing gradient, which by definition is unable to incorporate the threshold difference in training neither to update the threshold with gradients.
However,
To circumvent this challenge, we revisit the earliest derivation of STE from the stochastic binarization [2] and derive a novel and more flexible backward approxima-tion method for quantization. We name it as Generalized
Straight-Through Estimator (G-STE). It degenerates to STE when all the input intervals are equal-sized, while for the scenarios that require nonuniform input thresholds, it auto-matically adapts the thresholds with gradient learning and provides a finer-grained approximation to the quantization function. Specifically, G-STE encodes the expectation of stochastic quantization into the backward approximation to the forward deterministic quantization functions, which nat-urally converts the intractable gradient computation w.r.t. the input threshold parameters to that w.r.t. the slopes, and encodes the influence from input threshold difference to the remaining network in the backward gradient computation.
Moreover, we propose the weight regularization that considers the overall entropy in a weight filter to further re-duce the information loss arising from weight quantization.
We extensively evaluate the effectiveness of the proposed
N2UQ with the collective contributions of the threshold-learning quantizer via G-STE and the weight regularization on ImageNet with different architectures and different bit-width constraints. Under all deployment scenarios, N2UQ consistently improves the accuracy by a significant margin compared to the state-of-the-art methods, including both uniform and nonuniform quantization.
The contribution of this paper includes four aspects:
• We propose Nonuniform-to-Uniform Quantizer (N2UQ) for improving the quantization precision via learning input thresholds, while maintaining hardware-friendliness in im-plementation similar to uniform quantization.
• We propose Generalized Straight-Through Estimator (G-STE) to tackle intractable gradient computation w.r.t. in-put threshold parameters in N2UQ. G-STE calculates the expectation of the stochastic quantization as the backward approximation to the forward deterministic quantization.
• Based on entropy analysis, we propose a novel weight reg-ularization considering the overall weight distribution for further preserving information in weight quantization.
• We demonstrate that even under the strict constraint of fix-ing the quantized weights and activations to be uniform and only learning input thresholds, N2UQ exceeds state-of-the-art nonuniform quantization method with 0.5∼1.7% higher accuracy on ImageNet. Specifically, the 2-bit ResNet-50 model achieves 76.4% top-1 accuracy on ImageNet, re-ducing the gap to its real-valued counterpart to only 0.6%, demonstrating the effectiveness of N2UQ design. 2.