Abstract
Model-agnostic meta-learning (MAML) and its variants have become popular approaches for few-shot learning.
However, due to the non-convexity of deep neural nets (DNNs) and the bi-level formulation of MAML, the theo-retical properties of MAML with DNNs remain largely un-known. In this paper, we ﬁrst prove that MAML with over-parameterized DNNs is guaranteed to converge to global optima at a linear rate. Our convergence analysis indicates that MAML with over-parameterized DNNs is equivalent to kernel regression with a novel class of kernels, which we name as Meta Neural Tangent Kernels (MetaNTK).
Then, we propose MetaNTK-NAS, a new training-free neu-ral architecture search (NAS) method for few-shot learn-ing that uses MetaNTK to rank and select architectures.
Empirically, we compare our MetaNTK-NAS with previ-ous NAS methods on two popular few-shot learning bench-marks, miniImageNet, and tieredImageNet. We show that the performance of MetaNTK-NAS is comparable or bet-ter than the state-of-the-art NAS method designed for few-shot learning while enjoying more than 100x speedup. We believe the efﬁciency of MetaNTK-NAS makes itself more practical for many real-world tasks. Our code is released at github.com/YiteWang/MetaNTK-NAS. 1.

Introduction
Meta-learning, or learning-to-learn (LTL) [59], has re-ceived much attention due to its applicability in few-shot image classiﬁcation [23, 69], meta reinforcement learning
[18, 23, 62], and other domains such as natural language processing [8, 78] and computational biology [43]. The pri-mary motivation for meta-learning is to fast learn a new task from a small amount of data, with prior experience on sim-ilar but different tasks. There are a few meta learning ap-proaches designed for few shot image classiﬁcation, such as metric-based [57, 63], model-based [45, 56], optimizer-based [39,52]. Model-agnostic meta-learning (MAML) is a
*equal contribution popular gradient-based meta-learning approach, due to its simplicity and good performance in many meta-learning tasks [18, 19]. MAML formulates a bi-level optimiza-tion problem, where the inner-level objective represents the adaption to a given task, and the outer-level objective is the meta-training loss. There are many variants of MAML,
[20, 21, 31, 46, 51], and they are almost always applied to-gether with deep neural networks (DNNs) in practice.
Even though MAML with DNNs is empirically success-ful, this approach still lacks a thorough theoretical under-standing. For example, the most common practice is to use gradient descent approach (e.g., SGD or Adam [34]) to train
MAML with DNNs, and the optimization can usually ob-tain almost zero training loss and 100% training accuracy (i.e., global convergence) with suitable hyper-parameters
[3, 19]. However, prior theoretical works could not account for the global convergence of MAML trained with gradient descent on non-linear neural nets of more than two layers.
Hence, a crucial question that remains unknown for MAML optimization is:
Can MAML with DNNs converge to global minima?
This question motivates us to analyze the optimization properties of MAML with DNNs, and we provide a posi-tive answer with rigorous theoretical analysis. Brieﬂy, for over-parameterized DNNs, we analyze the optimization tra-jectory of MAML with square loss and prove that the train-ing loss is guaranteed to converge to zero at a linear rate.
Additionally, in this convergence analysis, we ﬁnd the DNN trained by MAML can be described by a kernel regression, with a novel class of kernels that we name as Meta Neural
Tangent Kernels (MetaNTK).
One may wonder whether our theory has any practical implications. Intuitively, our theory reveals that MetaNTK is closely related to the performance of MAML. To demon-strate the practical value of our theory, we provide a con-crete use case of MetaNTK: MetaNTK can help us efﬁ-ciently ﬁnd neural net architecture for few-shot learning.
Most meta-learning algorithms adopt standard network structures such as ConvNets [36], ResNets [22] and Wide
ResNets [79] for few-shot image classiﬁcation, the most
popular task to benchmark meta-learning. However, these network structures were developed on supervised learning benchmarks such as CIFAR [35], and ImageNet [12], and recently, it has been shown that these popular structures actually overﬁt to the supervised learning task on these datasets [54]. This indicates that the popular network struc-tures may not be optimal for tasks other than supervised learning, such as few-shot learning. Thus, one may natu-rally consider neural architecture search (NAS) [15, 42, 81] to automatically search for neural net architectures that are suitable for few-shot learning. To this end, prior works
[16, 32, 40] designed NAS methods speciﬁc for few-shot learning, but they require substantial computational cost (e.g., the search cost of [32] and [16] on mini-ImageNet is 100 and 7 GPU days, respectively; the training of [40] takes 150 GPU days on miniImageNet), which makes them im-practical for many real-world tasks and not environmental-friendly [13, 74]. Hence, a natural question is:
Can we accelerate NAS for few-shot learning to have much lower or even negligible search cost (compared to training cost)?
We provide an efﬁcient solution to this quest, MetaNTK-NAS, which is inspired by the MetaNTK we derive in our global convergence analysis of MAML. Brieﬂy, we use the condition number of MetaNAK as an indicator for the train-ability of networks under MAML. Since MetaNTK is di-rectly computed at initialization, no training is needed in the search stage, leading to a surprisingly small search cost (e.g., less than 0.07 GPU day on mini-ImageNet).
Our main contributions are summarized below:
• Global Convergence and Induced Kernels of MAML1:
We prove that with over-parameterized DNNs (i.e., DNNs with a large number of neurons in each layer), MAML is guaranteed to converge to global optima with zero train-ing loss at a linear rate. The key to our proof is to de-velop bounds on the gradient of the MAML objective, and then analyze the optimization trajectory of DNN pa-rameters trained under MAML. Furthermore, we show that in the over-parameterization regime, the output of
MAML-trained networks becomes equivalent to the out-put of a special kernel regression with a new class of ker-nels, Meta Neural Tangent Kernels (MetaNTK).
• Theory-Inspired Efﬁcient NAS for Few-Shot Learn-ing: We propose MetaNTK-NAS, a new NAS method for few-shot learning that takes advantage of MetaNTK.
Brieﬂy, it uses the condition number of the MetaNTK of each network as an indicator for its trainability under meta-learning. Empirically, our MetaNTK-NAS is com-parable or better than MetaNAS [16], the state-of-the-art NAS method for few-shot learning, on both miniIm-ageNet and tieredImageNet, while consuming 100x less cost in the search process. 1This part was also presented in a prior tech report of ours [65]. 2.