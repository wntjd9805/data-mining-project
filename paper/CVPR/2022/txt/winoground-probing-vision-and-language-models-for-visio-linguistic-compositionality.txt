Abstract
We present a novel task and dataset for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning, which we call
Winoground. Given two images and two captions, the goal is to match them correctly—but crucially, both captions contain a completely identical set of words, only in a dif-ferent order. The dataset was carefully hand-curated by ex-pert annotators and is labeled with a rich set of fine-grained tags to assist in analyzing model performance. We probe a diverse range of state-of-the-art vision and language mod-els and find that, surprisingly, none of them do much better than chance. Evidently, these models are not as skilled at visio-linguistic compositional reasoning as we might have hoped. We perform an extensive analysis to obtain insights into how future work might try to mitigate these models’ shortcomings. We aim for Winoground to serve as a useful evaluation set for advancing the state of the art and driv-ing further progress in the field. The dataset is available at https://huggingface.co/datasets/facebook/winoground. 1.

Introduction
Despite the impressive performance of pretrained vision and language transformers on a wide variety of multimodal tasks [43, 47, 50], they remain poorly understood [6, 15, 42, 61]. One important question is to what extent such mod-els are able to conduct unimodal and multimodal compo-sitional reasoning. For humans, the visual differences be-tween images depicting “the tree is in the shopping cart” and “the shopping cart is in the tree” will be blatantly obvi-ous, even when the words in the captions are identical—but is the same true for machines?
While matching simple images and captions may seem almost too trivial a task, recent work in NLP has shown
*Equal contribution. TT, AS, and DK conducted most of the work for this paper when they were at Facebook AI Research. (a) some plants surrounding a lightbulb (b) a lightbulb surrounding some plants
Figure 1. An example from Winoground. The two sentences con-tain the same words but in a different order. The task of under-standing which image and caption match is trivial for humans but proves much more difficult for vision and language models. Ev-ery model that we tested (UNITER, ViLLA, VinVL, VisualBERT,
ViLT, LXMERT, ViLBERT, UniT, CLIP, VSE++, and VSRN) fails to correctly pair the images and captions, except the large check-point of ViLLA by a very thin margin (0.00013 confidence). that transformers are often remarkably insensitive to word order [63]. Understanding the relationship between text in captions and corresponding visual content is a fundamental goal of computer vision, and the fact that different word or-ders correspond to wildly different visual depictions should be reflected in the capabilities of our models.
Motivated by this, we propose a novel task, called
Winoground, for measuring visio-linguistic compositional reasoning, whereby two images and two captions have to be matched correctly; both captions contain exactly the same set of words, ordered in such a way that each describes pri-marily one of the images. To perform well on Winoground, models must not only encode text and images well (i.e., be sensitive to the compositional structure present in each modality), but they also must be able to synthesize informa-tion across the two modalities.
We draw inspiration from the Winograd Schema Chal-lenge [40], which tests the commonsense capabilities of models. In the challenge, a model is given two sentences
that minimally differ and is tasked with performing coref-erence resolution. The Winograd twin sentence format has been used for a variety of language-related tasks [53,54,82].
In this work, we study the image-grounding of twin sen-tences with identical but differently ordered words.
Winoground was hand-crafted by expert annotators and is labeled with a rich set of fine-grained tags to assist in an-alyzing model performance. In efforts to shed better light on what exactly models learn, the NLP community has de-signed a wide variety of “probing tasks”: specialized, tar-geted tasks meant specifically for evaluation. The primary purpose of Winoground is to serve as a probing task for vi-sion and language models. See Fig. 1 for an example.
We evaluate a variety of state-of-the-art vision and lan-guage (V&L) transformers [9, 19, 31, 36, 43, 47, 50, 68, 81] and RNN-based models [17, 41]. Surprisingly, all of the models rarely—and if so only barely—outperform chance.
Our findings indicate that the visio-linguistic compositional reasoning capabilities of these models fall dramatically short of what we might have hoped.
In what follows, we introduce the Winoground task and dataset. We then describe the models we tested and discuss our findings. Next, we conduct an analysis of the perfor-mance of different models. We hope that insights from this work will lead to more robust vision and language models. 2.