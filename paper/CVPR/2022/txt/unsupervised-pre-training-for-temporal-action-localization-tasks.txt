Abstract
Unsupervised video representation learning has made remarkable achievements in recent years. However, most existing methods are designed and optimized for video clas-sification. These pre-trained models can be sub-optimal for temporal localization tasks due to the inherent discrep-ancy between video-level classification and clip-level local-ization. To bridge this gap, we make the first attempt to propose a self-supervised pretext task, coined as Pseudo
Action Localization (PAL) to Unsupervisedly Pre-train fea-ture encoders for Temporal Action Localization tasks (UP-TAL). Specifically, we first randomly select temporal re-gions, each of which contains multiple clips, from one video as pseudo actions and then paste them onto different tem-poral positions of the other two videos. The pretext task is to align the features of pasted pseudo action regions from two synthetic videos and maximize the agreement between them. Compared to the existing unsupervised video rep-resentation learning approaches, our PAL adapts better to downstream TAL tasks by introducing a temporal equivari-ant contrastive learning paradigm in a temporally dense and scale-aware manner. Extensive experiments show that
PAL can utilize large-scale unlabeled video data to sig-nificantly boost the performance of existing TAL methods.
Our codes and models will be made publicly available at https://github.com/zhang-can/UP-TAL. 1.

Introduction
Model pre-training is an effective technique for training deep networks in many computer vision tasks. The core idea is to learn general representations on large-scale la-beled or unlabeled data, and utilize the learned representa-tions to improve the performance of downstream tasks with limited data. This is especially beneficial for tasks that re-quire enormous human effort to annotate data, such as tem-poral action localization (TAL).
Despite the prevailing use of ready-made feature extrac-*Work done during an internship at Tencent AI Lab.
)
% (
G
V
A
@
P
A m 34 32 30 28
TAC
MoCo-v2
Ours
)
% ( y c a r u c c
A 90 80 70 (a) Action Localization (TAL) (b) Action Classification (TAC)
Figure 1. Comparison of Kinetics-400 pre-trained models by fine-tuning on downstream TAL (ActivityNet v1.3) and TAC (UCF101) datasets. ‘TAC’ means supervised TAC pre-training, and we treat MoCo-v2 [16] with video input as our baseline.
Instance-level discrimination is not well-aligned with TAL, thus unsupervised pre-training tailored for TAL is on demand. tors [10, 48, 55] pre-trained on temporal action classifica-tion (TAC) in TAL, this pre-training strategy is sub-optimal as the inherent discrepancy between TAC and TAL exists.
Without a doubt, this discrepancy impedes further perfor-mance improvement of TAL. Though some recent works
[1, 61, 62] attempt to tackle this issue, they still rely on large-scale annotated video data. Recently, unsupervised pre-training has attracted great attention due to its potentials in exploiting large amounts of unlabeled data. Contrastive learning [15, 16, 25, 29, 43] is one of the most popular di-rections that focus on instance discrimination, which pulls instance-level positive pairs closer while repelling negative ones apart in the embedding space. To fill the gap between the upstream pre-training and the downstream tasks, recent contrastive learning methods focus on specifically design-ing pretext tasks for various downstream image tasks, e.g., object detection [56,59,64], semantic segmentation [51,56], etc. In contrast, the progress of unsupervised pre-training in video domain is relatively lagging behind and most existing methods [2,28,32,44,45,53] are still designed and evaluated for classification tasks.
In this paper, we make the first attempt on unsuper-vised pre-training for TAL tasks. One possible way [45] to achieve this is to directly extend the image contrastive learning idea to the video domain, where a video is treated
as an instance and the clips are regarded as views of in-stances. Those clip embeddings from the same video are pulled closer while those from different videos are pushed apart. Clearly, this way only focuses on instance (video-level) discrimination, i.e., learning time-invariant features for specific video instances, which is required by TAC task in essence. In contrast, TAL expects the representations to be equivariant to temporal translation and scale. For ex-ample, if we change the start time and duration of an ac-tion instance in the input video, the output classification responses of TAC should be unchanged, while the output localization predictions of TAL need to be altered accord-ingly. The inherent discrepancy between these two tasks attracts our attention to question the suitability of the exist-ing instance discrimination paradigm for TAL. Indeed, as shown in Fig. 1, such video-level discrimination is bene-ficial for TAC tasks, but not well-aligned with TAL tasks.
So, it is desirable and challenging to design a new learning scheme that can be transferred well on TAL tasks.
Motivated by the inherent discrepancy between TAC and
TAL, we introduce temporal equivariant contrastive learn-ing paradigm by designing a new unsupervised pretext task called Pseudo Action Localization (PAL). Specifically, to mimic the TAL-tailored data with temporal boundaries, we first construct our training set by transforming the existing large-scale TAC datasets in a cheap manner. We randomly crop two temporal regions with random temporal lengths and scales from one video as pseudo actions. Each of these regions includes multiple consecutive clips. Then we paste them onto different temporal positions of other randomly selected background videos. With the preset temporal trans-formation (paste location, clip length, sampling scale), the model is able to align the pseudo action features of two syn-thesized videos. Such transformation and alignment process are named as input-level transformation and feature-level equi-transformation in our paper. Moreover, to better align the upstream pre-training pipeline to the downstream TAL architecture, we follow the way of estimating temporal lo-cations in TAL tasks [36, 38] by applying several layers of temporal convolutions to process the sequential clip-level features. Thereby, the information of surrounding back-ground clips is highly involved in the final output features of pseudo action regions. With the random paste operation, the diversity of background-involvement is increased. Fur-ther, we propose to maximize the agreement between two aligned pseudo action region features such that the learned features are forced to focus on the most discriminative and background-irrelevant parts, thus enhancing their robust-ness and achieving the equivariance requirement in TAL.
We summarize our main contributions as follows: (1)
To our best knowledge, this is the FIRST work focusing on unsupervised pre-training for temporal action localiza-tion tasks (UP-TAL). (2) We design an intuitive and effec-tive self-supervised pretext task customized for TAL, called
PAL. A time-equivariant contrastive learning paradigm is also introduced to perform transformed foreground discrim-ination, customized for TAL representation learning. (3)
Extensive experiments on ActivityNet v1.3 [7], Charades-STA [22] and THUMOS’14 [31] datasets show that PAL transfers well on various downstream TAL-related tasks:
Temporal Action Detection (TAD), Action Proposal Gener-ation (APG) and Video Grounding (VG). Notably, our PAL even surpasses the supervised pre-training when using the same amount of video data. 2.