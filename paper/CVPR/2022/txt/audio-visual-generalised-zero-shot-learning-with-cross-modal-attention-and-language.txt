Abstract i.e.
Learning to classify video data from classes not in-cluded in the training data, video-based zero-shot learning, is challenging. We conjecture that the natural alignment between the audio and visual modalities in video data provides a rich training signal for learning discrim-inative multi-modal representations. Focusing on the rel-atively underexplored task of audio-visual zero-shot learn-ing, we propose to learn multi-modal representations from audio-visual data using cross-modal attention and exploit textual label embeddings for transferring knowledge from seen classes to unseen classes. Taking this one step fur-ther, in our generalised audio-visual zero-shot learning set-ting, we include all the training classes in the test-time search space which act as distractors and increase the dif-ficulty while making the setting more realistic. Due to the lack of a unified benchmark in this domain, we introduce a (generalised) zero-shot learning benchmark on three audio-visual datasets of varying sizes and difficulty, VGGSound,
UCF, and ActivityNet, ensuring that the unseen test classes do not appear in the dataset used for supervised training of the backbone deep models. Comparing multiple rele-vant and recent methods, we demonstrate that our proposed
AVCA model achieves state-of-the-art performance on all three datasets. Code and data are available at https:
//github.com/ExplainableML/AVCA-GZSL. 1.

Introduction
Most zero-shot learning (ZSL) methods developed for image classification [5, 6, 60, 61, 72, 82] and action recog-nition [12, 13, 31, 84] only use unimodal input, e.g. images.
However, humans leverage multi-modal sensory inputs in their everyday activities. Imagine the situation in which the sound of a dog barking is audible but the dog is visually oc-Figure 1. Our audio-visual (generalised) ZSL framework aligns an audio-visual embedding with the corresponding textual label embedding via cross-modal attention. It can classify videos from previously unseen classes (e.g. elephant trumpeting) by predict-ing the class (red) whose textual label embedding (purple cross) is closest to the audio-visual embedding (blue star). cluded. In this case, we cannot understand the scene when relying on visual information alone. Using multiple modal-ities, such as vision and sound, allows to gather context and capture complementary information. Similarly, using both visual and audio information allows for a richer train-ing signal for learning frameworks. This paper investigates the challenging task of (generalised) ZSL with multi-modal audio-visual data by leveraging the natural alignment of au-dio and visual information in videos.
Recently, [42, 54] have explored the task of zero-shot video recognition using multi-modal visual and audio infor-mation as inputs. However, the AudioSetZSL dataset [54] used for this, contains an overlap between the classes used for validation and testing. This results in learning stronger representations for classes overlapping with the training and validation sets (which covers all the classes in this dataset) and hinders the modelâ€™s capability to learn sufficiently gen-eralisable representations that allow information transfer. In real-world applications, such models perform well on seen classes, but poorly on previously truly unseen classes. In this work, we propose three benchmarks of varying size and difficulty curated from the VGGSound [18], UCF101 [62], and ActivityNet [25] datasets that could act as a unified and challenging playground for Generalised ZSL (GZSL) and
ZSL research in the audio-visual domain. We suggest us-ing audio and visual features extracted using SeLaVi [9] pretrained using self-supervision. Throughout this work, we use features that were obtained from training in a self-supervised fashion to reduce the information leakage from supervised pre-training to the zero-shot task which has been identified as a problem in other ZSL benchmarks [13].
We tackle the audio-visual generalised zero-shot learn-ing task with our Audio-Visual Cross-Attention (AVCA) framework which is trained to align a rich learnt audio-visual representation with textual label embeddings. Our multi-stream architecture contains an audio and a visual branch which exchange information using cross-attention between the two modalities. AVCA is computationally lightweight and efficient since it uses audio and visual features extracted from pretrained networks as inputs in-stead of raw audio and image data. Our proposed frame-work is trained using multiple novel loss functions that are based on triplet losses and a regularisation loss that ensures that salient unimodal information is preserved in the learnt multi-modal representations. Our experiments show that
AVCA achieves state-of-the-art performance on the three introduced benchmark datasets. We show that using multi-modal input data leads to stronger (G)ZSL performance than using unimodal data.
To summarise, our contributions are as follows: (1) We introduce three novel benchmarks for audio-visual (gen-eralised) zero-shot learning curated from the VGGSound,
UCF101, and ActivityNet datasets; (2) We propose AVCA, a cross-modal model for audio-visual (G)ZSL which lever-ages cross-modal attention between audio and visual in-formation; (3) We show that AVCA yields state-of-the-art performance on all proposed audio-visual (G)ZSL bench-marks, outperforming the state-of-the-art unimodal and multi-modal zero-shot learning methods. Furthermore, we provide a qualitative analysis of the learnt multi-modal em-bedding space, demonstrating well-separated clustering for both seen and unseen classes. 2.