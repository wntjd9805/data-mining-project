Abstract
Annotating images with pixel-wise labels is a time-consuming and costly process. Recently, DatasetGAN [78] showcased a promising alternative – to synthesize a large labeled dataset via a generative adversarial network (GAN) by exploiting a small set of manually labeled, GAN-generated images. Here, we scale DatasetGAN to Ima-geNet scale of class diversity. We take image samples from the class-conditional generative model BigGAN [5] trained on ImageNet, and manually annotate only 5 images per class, for all 1k classes. By training an effective feature segmentation architecture on top of BigGAN, we turn Big-GAN into a labeled dataset generator. We further show that
VQGAN [18] can similarly serve as a dataset generator, leveraging the already annotated data. We create a new
ImageNet benchmark by labeling an additional set of real images and evaluate segmentation performance in a variety of settings. Through an extensive ablation study, we show big gains in leveraging a large generated dataset to train different supervised and self-supervised backbone models on pixel-wise tasks. Furthermore, we demonstrate that us-ing our synthesized datasets for pre-training leads to im-provements over standard ImageNet pre-training on several downstream datasets, such as PASCAL-VOC, MS-COCO,
Cityscapes and chest X-ray, as well as tasks (detection, seg-mentation). Our benchmark will be made public and main-tain a leaderboard for this challenging task. Project Page: https://nv-tlabs.github.io/big-datasetgan/ 1.

Introduction
The ImageNet dataset [65] has served as a cornerstone of modern computer vision and deep learning. It is used as a testbed for innovating in the domain of large-scale classi-fication, and has enabled incredible advancements over the
Importantly, it has also been com-years [15, 29, 42, 67]. monly used for pre-training backbone models, with either supervised or recently self-supervised pre-training, leading to almost guaranteed performance gains on a plethora of downstream datasets and tasks [24, 77]. ImageNet contains a million images with 1000-way classification labels. This huge class diversity is what makes pre-trained networks generalize well to a variety of downstream applications.
In this paper, we aim to enhance ImageNet with pixel-wise labels, to enable large-scale multi-class segmenta-tion challenges and offer opportunities for new pre-training strategies for dense downstream prediction tasks. However, instead of manually labeling masks for 1M images, which is time-consuming and costly, we instead synthesize high quality labeled data at a fraction of the cost.
We build on top of DatasetGAN [78], which introduced a simple idea: to manually annotate a very small set of GAN-generated images with pixel-wise labels, and add a shallow segmentation branch on top of the GAN’s feature maps, which is trained on this small dataset. It was shown that the generator’s feature maps are incredibly powerful and se-mantically meaningful, and allow the segmentation branch to produce very accurate labels for new random samples from the GAN. This means that the GAN is successfully re-purposed as a dataset generator, producing samples in the form of images and their pixel-wise labels. The authors showed that synthesizing a large dataset and using it to train downstream segmentation networks leads to extremely high performance at only a fraction of the labeling cost. How-ever, DatasetGAN utilized StyleGAN [38] as the generative model, which is limited to single class modeling.
We make several contributions: (i) we propose a novel dataset synthesis method to scale DatasetGAN to the Ima-geNet scale with minimal human labeling effort. Specifi-cally, we adopt the class-conditional generative model Big-Figure 2. BigDatasetGAN overview: (1) We sample a few images per class from BigGAN and manually annotate them with masks. (2) We train a feature interpreter branch on top of BigGAN’s and VQGAN’s features on this data, turning these GANs into generators of labeled data. (3) We sample large synthetic datasets from BigGAN & VQGAN. (4) We use these datasets for training segmentation models.
GAN [5], which produces high quality image samples for the 1k ImageNet classes. By manually labeling a handful of sampled images per class using a single expert annotator to ensure consistency and accuracy, we are able to synthesize a high quality labeled synthetic dataset. We further show that
VQGAN [18] can also serve as a dataset generator without the need for additional annotation. We call a dataset gener-ator for ImageNet obtained like that BigDatasetGAN. (ii) Next, we experimentally show the benefits of lever-aging synthesized datasets for various downstream dense prediction tasks and datasets. We demonstrate signif-icant performance gains on PASCAL-VOC, MS-COCO,
Cityscapes and chest X-ray for tasks such as object detec-tion and instance segmentation, leveraging several different backbone models. We compare several supervised and self-supervised methods and show that when these utilize our synthetic datasets performance is significantly improved. (iii) Finally, we annotate a held-out subset of real Ima-geNet with pixel-wise labels and introduce a new semantic segmentation benchmark. We perform extensive analyses of several existing methods on this benchmark. The anno-tated data and our benchmark will be hosted online, main-taining a leaderboard for a suite of segmentation challenges. 2.