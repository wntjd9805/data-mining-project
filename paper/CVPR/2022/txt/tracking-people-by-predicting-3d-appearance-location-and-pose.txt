Abstract
We present an approach for tracking people in monoc-ular videos by predicting their future 3D representations.
To achieve this, we first lift people to 3D from a single frame in a robust manner. This lifting includes informa-tion about the 3D pose of the person, their location in the 3D space, and the 3D appearance. As we track a person, we collect 3D observations over time in a track-let representation. Given the 3D nature of our observa-tions, we build temporal models for each one of the previ-ous attributes. We use these models to predict the future state of the tracklet, including 3D appearance, 3D loca-tion, and 3D pose. For a future frame, we compute the similarity between the predicted state of a tracklet and the single frame observations in a probabilistic manner. As-sociation is solved with simple Hungarian matching, and the matches are used to update the respective tracklets. We evaluate our approach on various benchmarks and report state-of-the-art results. Code and models are available at: https://brjathu.github.io/PHALP. 1.

Introduction
When we watch a video, we can segment out individ-ual people, cars, or other objects and track them over time.
The corresponding task in computer vision has been studied for several decades now, with a fundamental choice being whether to do the tracking in 2D in the image plane, or of 3D objects in the world. The former seems simpler because it obviates the need for inferring 3D, but if we do take the step of back-projecting from the image to the world, other aspects such as dealing with occlusion become easier. In the 3D world the tracked object doesn’t disappear, and even young infants are aware of its persistence behind the oc-cluder. In our recent work [35], we presented experimental evidence that performance is better with 3D representations.
In this paper, we will take this as granted, and proceed to develop a system in the 3D setting of the problem. While our approach broadly applies to any object category where parameterized 3D models are available and can be inferred from images, we will limit ourselves in this paper to study-Figure 1. Tracking people by predicting and matching in 3D.
The top row shows our tracking results at three different frames.
The results are visualized by a colored head-mask for unique iden-tities. The second and third rows show renderings of the 3D states of the two people in their associated tracklets. The bottom row shows the bottom-up detections in each image frame which, after being lifted to 3D, will be matched with the 3D predictions of each tracklet in the corresponding frame. Note how in the middle frame of second row, the 3D representation of the person persists even though they are occluded in the image. Readers are encouraged to watch the videos at the project website. ing people, the most important case in practice.
Once we have accepted the philosophy that we are track-ing 3D objects in a 3D world, but from 2D images as raw data, it is natural to adopt the vocabulary from control the-ory and estimation theory going back to the 1960s. We are interested in the “state” of objects in 3D, but all we have access to are “observations” which are RGB pixels in 2D.
In an online setting, we observe a person across multiple time frames, and keep recursively updating our estimate of the person’s state — their appearance, location in the world, and pose (configuration of joint angles). Since we have a dynamic model (a “tracklet”), we can also predict states at future times. When the next image frame comes in, we detect the people in it, lift them to 3D, and in that set-ting solve the association problem between these bottom-up detections and the top-down predictions of the different tracklets for this frame. Once the observations have been associated with the tracklets, the state of each person is re-Figure 2. PHALP: Predicting Human Appearance, Location and Pose for Tracking: We perform tracking of humans in 3D from monocular video. For every input bounding box, we estimate a 3D representation based on the 3D appearance, 3D pose and 3D location of the person. During tracking, these are integrated to form corresponding tracklet-based representations. We perform tracking by predicting the future representation of each person and using it to solve for association given the detected bounding boxes of a future frame. estimated and the process continues. Fig. 1 shows this pro-cess on a real video. Note that during a period of occlusion of a tracklet, while no new observations are coming in, the state of the person keeps evolving following their dynamics.
It is not the case that “Out of sight, out of mind”!
In an abstract form, the procedure sketched in the pre-vious paragraph is basically the same as that followed in multiple computer vision papers from the 1980s and 1990s.
The difference is that in 2022 we can actually make it work thanks to the advances brought about by deep learning and big data, that enable consistent and reliable lifting of peo-ple to 3D. For this initial lifting, we rely on the HMAR model [35]. This is applied on every detected bounding box of the input video and provides us with their initial, single frame, observations for 3D pose, appearance as well as lo-cation of the person in the 3D space.
As we link individual detections into tracklets, these rep-resentations are aggregated across each tracklet, allowing us to form temporal models, i.e., functions for the aggregation and prediction of each representation separately (see right side of Fig. 2). More specifically, for appearance, we use the canonical UV map of the SMPL model to aggregate ap-pearance, and employ its most recent prediction as person’s appearance. For pose, we aggregate information using a modification of the HMMR model [17], where through its
“movie strip” representation, we can produce 3D pose pre-dictions. Finally, for 3D location, we use linear regression to predict the future location of the person.
This modeling enables us to develop our tracking sys-tem, PHALP (Predicting Human Appearance, Location and Pose for tracking), which aggregates information over time, uses it to predict future states, and then associates the predictions with the detections. First, we predict the 3D location, 3D pose and 3D appearance for each tracklet for a short period of time. For a future frame, these predic-tions need to be associated with the detected people of the frame. To measure similarity, we adopt a probabilistic in-terpretation and compute the posterior probabilities of ev-ery detection belonging to each one of the tracklets, based on the three basic attributes. With the appropriate similarity metric, association is then easily resolved by means of the
Hungarian algorithm. The newly linked detections can now update the temporal model of the corresponding tracklets for 3D appearance, 3D location and 3D pose in an online manner and we continue the procedure by rolling-out fur-ther prediction steps. The final output is an identity label for each detected bounding box in the video. Notably, this approach can also be applied on videos with shot changes, e.g., movies [12], with minor modifications. Effectively, we modify our similarity to include only appearance and 3D pose information for these transitions, since they (unlike lo-cation) are not affected by the shot boundary. 2.