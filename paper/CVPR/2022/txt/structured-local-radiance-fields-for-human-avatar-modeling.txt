Abstract
It is extremely challenging to create an animatable clothed human avatar from RGB videos, especially for loose clothes due to the difficulties in motion modeling. To ad-dress this problem, we introduce a novel representation on the basis of recent neural scene rendering techniques. The core of our representation is a set of structured local ra-diance fields, which are anchored to the pre-defined nodes sampled on a statistical human body template. These local radiance fields not only leverage the flexibility of implicit representation in shape and appearance modeling, but also factorize cloth deformations into skeleton motions, node residual translations and the dynamic detail variations in-side each individual radiance field. To learn our represen-tation from RGB data and facilitate pose generalization, we propose to learn the node translations and the detail varia-tions in a conditional generative latent space. Overall, our method enables automatic construction of animatable hu-man avatars for various types of clothes without the need for scanning subject-specific templates, and can generate realistic images with dynamic details for novel poses. Ex-periment show that our method outperforms state-of-the-art methods both qualitatively and quantitatively. 1.

Introduction
Animatable human avatar modeling is of great impor-tance in many applications such as content creation and en-tertainment, and virtual characters have become ubiquitous in our lives with the rise of computer graphics in movies and games. Traditional methods for high-quality human avatar reconstruction are often costly and tedious, due to the diffi-culties in modeling the complex dynamics of clothes. Be-sides, they typically presume the availability of a subject-specific template [22] and its accurate registration to the in-put frames [6, 78], which are difficult to acquire in practice.
With the rapid development in computer vision in the past ten years, researchers have started to explore the pos-sibility of automatic human avatar reconstruction without pre-scanning efforts. Pioneer studies deformed a statisti-Figure 1. Example results produced by our method. Our method can learn animatable human avatars with various cloth topologies and realistic dynamic details. Top row: driving video, from which the animation poses are extracted. Bottom two rows: animation results rendered from the front and the back view. cal human body template (e.g., SMPL [40]) to model the clothed human geometry and appearance [2–4]. Neural tex-ture maps and image-to-image networks are later adopted to achieve photo-realistic rendering [36, 37, 58, 65]. Recently, neural radiance representations, which implicitly encode shape and appearance using neural networks, are also ap-plied in pursuit of higher-fidelity results [35, 49, 54]. These methods typically define the radiance field in a canonical pose, and warp it to live poses using linear blending skin-ning (LBS) under the guidance of the SMPL surface.
Despite the differences in the representations inside the aforementioned approaches, we find that there is one thing in common: they all heavily rely on the skeleton or the surface of SMPL model for cloth motion modeling. This is apparent in methods based on the SMPL topology, ei-ther using traditional texture maps [2–4] or neural textures
[36, 37, 58, 65]. Even in state-of-the-art methods based on
implicit fields [35, 49, 54], researchers still assumed that skin motions can be propagated to approximate the cloth deformations, which, unfortunately, only holds for tight-fitting clothes. When applying these methods to loose clothes, articulation motions based on solely body joints cannot express the complete information about the wrin-kles and non-rigid deformations. Some methods learned to directly regress cloth deformations from body pose config-urations [35]; however, the complexity gap between body poses and cloth details results in a one-to-many mapping problem, leading to under-fitting issues where the network learns averaged, blurry appearance. Suffering from this fa-tal limitation, no methods have demonstrated animatable human characters wearing skirts or dresses so far.
To overcome this limitation and fill the void, we propose a new representation for clothed human characters. Our representation is built upon neural radiance fields [46], or
NeRF in short, for its excellent performance in learning the appearance of static scenes. To extend NeRF for dynamic character modeling, we break a global NeRF into a set of structured local radiance fields, which are attached to the pre-defined nodes on the SMPL model. Each local radiance field is responsible for representing the shape and appear-ance in the local space around its corresponding node. The local radiance fields can be driven by the body skeleton, while having their own residual movements to represent the non-rigid deformation of garments. Furthermore, each ra-diance field is conditioned on a dynamic detail embedding, which encodes the high-frequency dynamic details that can-not be modeled via node translation. In this way, our repre-sentation decomposes the cloth deformations in a coarse-to-fine manner: the coarsest level is the skeleton motion, the middle level is the residual movements of the local radiance fields, and the finest level is the time-varying details inside each radiance field.
However, employing such a representation for avatar modeling is not straight-forward as the node-related vari-ables (i.e., the node residual translations and the dynamic detail embeddings) are difficult to acquire in practice. Al-though we can obtain these variables for training frames through naive optimization with image evidence, it remains unclear how to compute them for unseen poses. Alter-natively, one can train a network that directly regresses these variables from body poses, but this will result into the aforementioned under-fitting issues due to information deficiency [6]. In order to achieve a balance between data fitting and generalization, we draw inspiration from [6] and learn the node-related variables in a conditional generative latent space. Specifically, we introduce a tiny conditional variational auto-encoder (cVAE) [68] for each local radi-ance field. Conditioned on the pose parameters, the cVAE decoders convert the latent bottlenecks into node-related variables. For the input of the cVAE encoder, we find that the time stamp [16, 57, 77] is an effective option, because it is simple, distinguishable, and naturally guarantees the temporal smoothness of the node-related variables thanks to the low-frequency bias in MLPs [71].
Intuitively, the time stamp is provided as an auxiliary input to help our network distinguish similar poses at different frames, while the VAE property can push the latent space to be uninfor-mative, thereby encouraging the network to mainly rely on pose conditions when inferring node-related variables. With all of these building blocks, our network can be trained in an end-to-end manner, eventually producing a realistic dy-namic human avatar.
Overall, our proposed method offers the new ability to automatically create an animatable human character with general, dynamic garments. This is achieved by using only
RGB videos, without any pre-scanning efforts. Compared to methods that heavily depend on the topology of a naked human body template, our approach is powerful yet general in terms of both appearance learning and motion modeling, and able to generate realistic dynamic details. To the best of our knowledge, our method is the first one that demon-strates automatic human avatar creation for dresses. Exper-iments prove that our method outperforms state-of-the-art approaches qualitatively and quantitatively. 2.