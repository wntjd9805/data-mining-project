Abstract
Given an image with multiple people, our goal is to di-rectly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without know-ing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from in-fants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional imaginary Bird’s-Eye-View representation to explicitly rea-son about depth. BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Sec-ond, height varies with age, making it impossible to resolve depth without also estimating the age of people in the im-age. To do so, we exploit a 3D body model space that lets
BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a “Relative
Human” (RH) dataset that includes age labels and rela-tive depth relationships between the people in the images.
Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV out-performs existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code1 and dataset2 are released for research purposes.
*This work was done when Yu Sun was an intern at Explore Academy of JD.com.
†Corresponding author. 1https://github.com/Arthur151/ROMP 2https://github.com/Arthur151/Relative_Human
1.

Introduction
In this article, we focus on simultaneously estimating the 3D pose and shape of all people in an RGB image along with their relative depth. There has been rapid progress [22] on regressing the 3D pose and shape of individual (cropped) people [4, 15, 16, 18, 19, 26, 29, 35, 44, 45, 47, 49] as well as the direct regression of groups [11, 34]. Neither class of methods explicitly reasons about the depth of people in the scene. Such depth reasoning is critical to enable a deeper understanding of the scene and the multi-person in-teractions within it. To address this, we propose a unified method that jointly regresses multiple people and their rel-ative depth relations in one shot from an RGB image.
While previous multi-person methods perform well in constrained experimental settings, they struggle with severe occlusion, diverse body size and appearance, the ambiguity of monocular depth, and in-the-wild cases [11, 25, 38, 48].
These challenges lead to unsatisfactory performance in crowded scenes, including detection misses, similar predic-tions for overlapping people, and all predictions having a similar height. We observe two inter-related limitations that result in these failures. First, the architecture of the regres-sion networks is closely tied to the 2D image, while the peo-ple actually inhabit 3D space. We address this with a new architecture that reasons in 3D. Second, depth estimation is fundamentally ambiguous due to the unknown height of the people in the image and it is difficult to obtain training data of images with ground-truth height and depth. To address this, we present a new dataset and novel losses that allow training without having metric depth.
We observe that crowded scenes contain rich information about the relative relationships between people, which can be exploited for both training and validation of depth rea-soning. However, we still lack a powerful representations to learn from these cases. A few learning-based methods have been proposed for reasoning about the depth of predicted body meshes [11] or 3D poses [25, 38, 48]. Unfortunately, they all reason about depth via 2D representations, such as
RoI-aligned features [11, 25] or a 2D depth map [38, 48].
These regression-based 2D representations have inherent drawbacks for representing the 3D world. The lack of an explicit 3D representation in the networks makes it chal-lenging for these methods to deal with crowded scenes in which people overlap at different depths. Therefore, we ar-gue that an explicit 3D representation is needed.
To achieve this, we develop BEV (for Bird’s Eye View), a unified one-stage method for monocular reconstruction and depth reasoning of multiple 3D people. We take inspira-tion from ROMP [34], a one-stage, multi-person, regression method that directly estimates multiple 2D front-view maps for 2D human detection, positioning, and mesh parameter regression without depth reasoning. With ROMP, the net-work can only reason about the 2D location of people in the image plane. To go beyond this, we need to enable the network to efficiently reason about depth as well. To that end, we introduce a new imaginary 2D “bird’s-eye-view” map that represents the likely centers of bodies in depth. To be clear, BEV takes only a single 2D image; the overhead view is inferred, not observed. BEV uses a powerful and efficient localization pipeline, performing bird’s-eye-view-based coarse detection and fine localization in parallel. We employ the 2D heatmaps for coarse detection from both the front (image) and bird’s eye views. BEV combines these heatmaps to obtain a 3D heatmap, as illustrated in Fig. 2.
By learning the front and the bird’s-eye view together, BEV explicitly models how people appear in images and in depth.
This enables BEV to learn from available 2D and 3D anno-tations. BEV also uses a novel 3D Offset map to refine the initial coarse detections. From these coarse and fine maps, we obtain the 3D translation of all people in the scene. BEV transforms these predictions from the latent 3D Center-map space to an explicit camera-centric 3D space. Given these 3D translation predictions, BEV samples the features of all the people from a predicted mesh feature map and regresses the final SMPL [23] parameters. Distinguishing people at different depths enables BEV to estimate multiple people even with severe occlusion as illustrated in Fig. 1.
Even with a powerful 3D representation, we need an appropriate training scheme to ensure generalization. The main reason is that without knowing subject height, we lack effective constraints to alleviate the depth/height ambigu-ity under perspective projection. In particular, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. The ambiguity causes incorrect depth estimates for children and infants, limiting the generalization of existing methods. Unfortu-nately, existing 3D datasets with multiple people have lim-ited diversity in height and age, so they cannot be used to improve or evaluate generalization.
Since collecting ground-truth 3D data in the wild is diffi-cult, we instead train BEV using cost-effective weak labels of in-the-wild images. Specifically, we collect a dataset, named “Relative Human” (RH), that contains weak anno-tations of depth layers and human ages categorized into the groups adult, teenager, child, and infant. Moreover, we propose a weakly supervised training scheme (WST) to effectively learn from these weak supervision signals.
For instance, we use a piece-wise loss function that ex-ploits the depth layers to penalize incorrect relative depth orders. Exploiting age information to constrain height is tricky. While age and height are correlated, heights can vary significantly within the same age group. Consequently, we develop an ambiguity-compatible mixed loss function that encourages body shapes with heights that lie within an ap-propriate range for each age group.
We evaluate BEV on three multi-person datasets: in-the-wild using the 2D RH dataset and in 3D using the real CMU
Panoptic [13] and the synthetic AGORA [28] datasets.
On RH, compared with previous methods [11, 25, 38, 48],
BEV is more accurate in relative depth reasoning and pose estimation. On CMU Panoptic, BEV outperforms pre-vious methods [6, 11, 34, 42, 43] in 3D pose estimation.
On AGORA, BEV significantly improves detection and achieves state-of-the-art results on “AGORA kids” in terms of the mesh reconstruction error. Also, fine-tunning on RH in a weakly supervised manner significantly improves the results for all age groups, especially for young people.
In summary, the main contributions are: (1) We construct a 3D representation to alleviate the monocular depth am-biguity via combining a front-view representation with an imaginary bird’s eye view. (2) We collect the Relative Hu-man dataset with weak annotations of in-the-wild images, which facilitates the training and evaluation on monocular (3) We develop depth reasoning in multi-person scenes. a weakly supervised training scheme to learn from weak depth annotations and to exploit age information. 2.