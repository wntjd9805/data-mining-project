Abstract
Diffusion models learn to restore noisy data, which is corrupted with different levels of noise, by optimizing the weighted sum of the corresponding loss terms, i.e., denois-ing score matching loss. In this paper, we show that restor-ing data corrupted with certain noise levels offers a proper pretext task for the model to learn rich visual concepts. We propose to prioritize such noise levels over other levels dur-ing training, by redesigning the weighting scheme of the ob-jective function. We show that our simple redesign of the weighting scheme significantly improves the performance of diffusion models regardless of the datasets, architectures, and sampling strategies. 1.

Introduction
Diffusion models [14, 36], a recent family of generative models, have achieved remarkable image generation perfor-mance. Diffusion models have been rapidly studied, as they offer several desirable properties for image synthesis, in-cluding stable training, easy model scaling, and good dis-tribution coverage [27]. Starting from Ho et al. [14], recent works [8, 27, 40] have shown that the diffusion models can render high-fidelity images comparable to those generated by generative adversarial networks (GANs) [12], especially in class-conditional settings, by relying on additional efforts such as classifier guidance [8] and cascaded models [32].
However, the unconditional generation of single models still has considerable room for improvement, and performance has not been explored for various high-resolution datasets (e.g., FFHQ [20], MetFaces [18]) where other families of generative models [3, 11, 20, 22, 41] mainly compete.
Starting from tractable noise distribution, a diffusion model generates images by progressively removing noise.
To achieve this, a model learns the reverse of the prede-fined diffusion process, which sequentially corrupts the con-∗Correspondence to: Sungroh Yoon (sryoon@snu.ac.kr) tents of an image with various levels of noise. A model is trained by optimizing the sum of denoising score match-ing losses [43] for various noise levels [39], which aims to learn the recovery of clean images from corrupted im-ages. Instead of using a simple sum of losses, Ho et al. [14] observed that their empirically obtained weighted sum of losses was more beneficial to sample quality. Their weighted objective is the current de facto standard objec-tive for training diffusion models [8, 25, 27, 32, 40]. How-ever, surprisingly, it remains unknown why this performs well or whether it is optimal for sample quality. To the best of our knowledge, the design of a better weighting scheme to achieve better sample quality has not yet been explored.
Given the success of diffusion models with the standard weighted objective, we aim to amplify this benefit by ex-ploring a more appropriate weighting scheme for the ob-jective function. However, designing a weighting scheme is difficult owing to two factors. First, there are thousands of noise levels; therefore, an exhaustive grid search is impos-sible. Second, it is not clear what information the model learns at each noise level during training, therefore hard to determine the priority of each level.
In this paper, we first investigate what a diffusion model learns at each noise level. Our key intuition is that the diffu-sion model learns rich visual concepts by solving pretext tasks for each level, which is to recover the image from corrupted images. At the noise level where the images are slightly corrupted, images are already available for percep-tually rich content and thus, recovering images does not require prior knowledge of image contexts. For example, the model can recover noisy pixels from neighboring clean pixels. Therefore, the model learns imperceptible details, rather than high-level contexts. In contrast, when images are highly corrupted so that the contents are unrecogniz-able, the model learns perceptually recognizable contents to solve the given pretext task. Our observations motivate us to propose P2 (perception prioritized) weighting, which aims to prioritize solving the pretext task of more important noise levels. We assign higher weights to the loss at levels where
the model learns perceptually rich contents while minimal weights to which the model learns imperceptible details.
To validate the effectiveness of the proposed P2 weight-ing, we first compare diffusion models trained with previous standard weighting scheme and P2 weighting on various datasets. Models trained with our objective are consistently superior to the previous standard objective by large margins.
Moreover, we show that diffusion models trained with our objective achieve state-of-the-art performance on CelebA-HQ [17] and Oxford-flowers [28] datasets, and comparable performance on FFHQ [20] among various types of gen-erative models, including generative adversarial networks (GANs) [12]. We further analyze whether P2 weighting is effective to various model configurations and sampling steps. Our main contributions are as follows:
• We introduce a simple and effective weighting scheme of training objectives to encourage the model to learn rich visual concepts.
• We investigate how the diffusion models learn visual concepts from each noise level.
• We show consistent improvement of diffusion mod-els across various datasets, model configurations, and sampling steps. 2.