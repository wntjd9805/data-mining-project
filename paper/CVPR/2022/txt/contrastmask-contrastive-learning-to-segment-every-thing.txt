Abstract
Mask RCNN [17] OPMask [2]
ContrastMask
Partially-supervised instance segmentation is a task which requests segmenting objects from novel categories via learning on limited base categories with annotated masks thus eliminating demands of heavy annotation burden. The key to addressing this task is to build an effective class-agnostic mask segmentation model. Unlike previous meth-ods that learn such models only on base categories, in this paper, we propose a new method, named ContrastMask, which learns a mask segmentation model on both base and novel categories under a unified pixel-level contrastive learning framework. In this framework, annotated masks of base categories and pseudo masks of novel categories serve as a prior for contrastive learning, where features from the mask regions (foreground) are pulled together, and are con-trasted against those from the background, and vice versa.
Through this framework, feature discrimination between foreground and background is largely improved, facilitating learning of the class-agnostic mask segmentation model.
Exhaustive experiments on the COCO dataset demonstrate the superiority of our method, which outperforms previous state-of-the-arts. 1.

Introduction
Instance segmentation is one of the most fundamental tasks in computer vision, which requests pixel-level predic-tion on holistic images and identifies each individual ob-ject. Many works [8, 13, 17, 19, 26, 31, 39, 42] have boosted instance segmentation performance by relying on a large amount of available pixel-level annotated data. However, performing pixel-level annotation (mask annotation) is sig-nificantly burdensome, which hinders the further develop-ment of instance segmentation on massive novel categories.
Since box-level annotations are much cheaper and eas-ier to obtain than mask annotations [12], a common way
†Work done during an internship at Youtu Lab, Tencent. (cid:66)Corresponding Author.
Figure 1. Visualization results of Mask R-CNN [17], OPMask [2] and the proposed ContrastMask on novel categories. to address the aforementioned issue is to perform partially-supervised instance segmentation [15, 18, 22, 45]. This in-stance segmentation task was first proposed in the paper
“Learning to Segment Every Thing” [18], where object cat-egories are divided into two splits: base and novel. Both of them have box-level annotations, while only base cat-egories have additional mask annotations. Then the goal of this task is by taking advantage of the data of base cat-egories with mask annotations to generalize instance seg-mentation models to novel categories. The main obstacle to achieve favorable instance segmentation performance under the partially-supervised setting is how to distinguish fore-ground and background within each box for an arbitrary category via learning on the data with limited annotations.
Previous methods [2, 15, 18, 22, 29, 30, 45] addressed this task via learning a class-agnostic mask segmentation model to separate foreground and background, by capturing class-agnostic cues, such as shape bases [22] and appear-ance commonalities [15]. However, these methods learn the class-agnostic mask segmentation model only on base categories, ignoring a large amount of training data from novel categories, and consequently lack a bridge to trans-fer the segmentation capability of the mask segmentation model on base categories to novel categories.
To build this bridge, in this paper, we propose Con-trastMask, a new partially-supervised instance segmenta-tion method, which learns a class-agnostic mask segmen-tation model on both base and novel categories under a unified pixel-level contrastive learning framework. In this framework, we design a new query-sharing pixel-level con-trastive loss to fully exploit data from all categories. To this end, annotated masks of base categories or pseudo masks of novel categories computed by Class Activation
Map (CAM) [2, 44] serve as a region prior, which indicates not only the foreground and background separation, but also shared queries, positive keys and negative keys. Con-cretely, given a training image batch containing both base categories and novel categories, we establish two shared queries: a foreground query and a background query, which are obtained by averaging features within and outside the mask regions, including both the annotated and the pseudo masks, respectively. Then, we perform a special sampling strategy to select proper keys. By introducing the proposed loss, we expect to pull keys within/outside the mask regions towards the foreground/background shared query and con-trast it against keys outside/within the mask regions. Fi-nally, features learned by our pixel-level contrastive learn-ing framework are fused into a class-agnostic mask head to perform mask segmentation.
Compared with previous methods, ContrastMask enjoys several benefits: 1) It fully exploits training data, making those from novel categories also contribute to the optimiza-tion process of the segmentation model; 2) More impor-tantly, it builds a bridge to transfer the segmentation capa-bility on base categories to novel categories by the unified pixel-level contrastive learning framework, especially the shared queries for both base and novel categories, which consistently improves feature discrimination between fore-ground and background for both base and novel categories.
A visualization result of comparison with other methods is shown in Fig. 1.
Without bells and whistles, ContrastMask surpasses all previous state-of-the-art partially-supervised instances seg-mentation methods on the COCO dataset [25], by large mar-gins. Notably, with the ResNeXt-101-FPN [24, 40] as the backbone, our method achieves 39.8 mAP for mask seg-mentation on novel categories. 2.