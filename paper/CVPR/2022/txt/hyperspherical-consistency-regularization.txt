Abstract
Recent advances in contrastive learning have enlight-ened diverse applications across various semi-supervised fields. Jointly training supervised learning and unsuper-vised learning with a shared feature encoder becomes a common scheme. Though it benefits from taking advantage of both feature-dependent information from self-supervised learning and label-dependent information from supervised learning, this scheme remains suffering from bias of the classifier.
In this work, we systematically explore the re-lationship between self-supervised learning and supervised learning, and study how self-supervised learning helps ro-bust data-efficient deep learning. We propose hyperspher-ical consistency regularization (HCR), a simple yet effec-tive plug-and-play method, to regularize the classifier us-ing feature-dependent information and thus avoid bias from labels. Specifically, HCR first project logits from the clas-sifier and feature projections from the projection head on the respective hypersphere, then it enforces data points on hyperspheres to have similar structures by minimizing bi-nary cross entropy of pairwise distances’ similarity met-rics. Extensive experiments on semi-supervised and weakly-supervised learning demonstrate the effectiveness of our method, by showing superior performance with HCR. 1.

Introduction
The last decade has witnessed revolutionary advances in deep learning across various computer vision fields such as image classification [26,29,38,72], object detection [48,64– 66], and semantic segmentation [25, 47, 67] in the presence of large-scale labeled datasets. However, massive collec-tion and accurate annotation of datasets are time-consuming and expensive.
In many practical situations, only small-scale high-quality labeled datasets are available. For this reason, semi-supervised learning (SSL) that learning from few labeled data and a large number of unlabeled data has received broad attention [4, 5, 42, 62, 63, 69, 71, 73, 84, 85].
*Equal contribution
Figure 1. Illustration of contrastive learning on the hypersphere.
Red arrows denote positive pairs tend to attract each other, and the gray arrow denotes negative pairs tend to repel each other.
With the development of contrastive learning [7, 9–11, 20, 24, 27, 28, 45, 74, 80, 90, 94], recent SSL algorithms [23, 34, 43, 70, 79, 81, 92] tend to extend self-supervised learn-ing into supervised learning by adding a branch network as a projection head that jointly learns from feature-dependent and label-dependent information. Though the feature en-coder is supposed to learn better by making agreements from different views on latent spaces, the classifier which determines the ultimate predictions still suffers from the bias of semi-supervision or weak-supervision. Typically,
[33, 93] found that data imbalance is not the key issue in learning high-quality representations from long-tail data, while simply adjusting the classifier with balanced sam-pling can effectively alleviate the imblanced bias. This phe-nomenon suggests that decent representation may help but not be enough for robust learning, while regularizing the classifier is necessary to improve learning performance.
A vast number of current empirical contrastive learning methods [9–11,20,24,28,45] project feature embeddings on a hypersphere through ℓ2 normalization while maximizing distances between negative pairs and minimizing distances between positive pairs, as shown in Figure 1. Restricting the output space to a unit hypersphere can improve training stability in machine learning where dot products are ubiq-uitous [77, 80, 86]. Besides, well-clustered features on the hypersphere are linearly separable from the rest of the fea-ture space. The above desirable traits are considered to be useful while regularizing the classifier.
Figure 2. Linear classifier learns to separate the hypersphere through the hyperplane.
In this work, we analyze the relationship between the projection head and the classifier, and propose hyperspheri-cal consistency regularization (HCR) to constrain the latent hyperspherical space. As shown in Figure 2, a decent clas-sifier is able to find an optimal hyperplane in a hypersphere manifold, and data points on the classifier’s hyperplane can be reprojected on a hypersphere. HCR assumes data points on the projection head’s hypersphere and the classifier’s hy-persphere have similar geometric structures, and preserves such structures by making distributions of pairwise dis-tances consistent. Experiments on semi-supervised learning and weakly-supervised learning indicate HCR can consid-erably improve the generalization ability. 2.