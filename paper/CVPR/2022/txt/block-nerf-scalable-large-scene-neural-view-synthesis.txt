Abstract
We present Block-NeRF, a variant of Neural Radiance
Fields that can represent large-scale environments. Specif-ically, we demonstrate that when scaling NeRF to render city-scale scenes spanning multiple blocks, it is vital to de-compose the scene into individually trained NeRFs. This decomposition decouples rendering time from scene size, en-ables rendering to scale to arbitrarily large environments, and allows per-block updates of the environment. We adopt several architectural changes to make NeRF robust to data captured over months under different environmental condi-tions. We add appearance embeddings, learned pose reﬁne-ment, and controllable exposure to each individual NeRF, and introduce a procedure for aligning appearance between adjacent NeRFs so that they can be seamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to cre-ate the largest neural scene representation to date, capable of rendering an entire neighborhood of San Francisco. 1.

Introduction
Recent advancements in neural rendering such as Neural
Radiance Fields [40] have enabled photo-realistic reconstruc-*Work done as an intern at Waymo. tion and novel view synthesis given a set of posed camera im-ages [3, 38, 44]. Earlier works tended to focus on small-scale and object-centric reconstruction. Though some methods now address scenes the size of a single room or building, these are generally still limited and do not na¨ıvely scale up to city-scale environments. Applying these methods to large environments typically leads to signiﬁcant artifacts and low visual ﬁdelity due to limited model capacity.
Reconstructing large-scale environments enables several important use-cases in domains such as autonomous driv-ing [30, 43, 69] and aerial surveying [14, 33]. For example, a high-ﬁdelity map of the operating domain can serve as a prior for robot navigation. Large-scale scene reconstruc-tions can be used for closed-loop robotic simulations [13].
Autonomous driving systems are commonly evaluated by re-simulating previously encountered scenarios. Any deviation from the recorded encounter, however, may change the vehi-cle’s trajectory, requiring high-ﬁdelity novel view renderings along the altered path. Scene conditioned NeRFs can further augment simulation scenarios by changing environmental lighting conditions, such as camera exposure, weather, or time of day.
Reconstructing such large-scale environments introduces additional challenges, including the presence of transient objects (cars and pedestrians), limitations in model capacity, 1
along with memory and compute constraints. Furthermore, training data for such large environments is highly unlikely to be collected in a single capture under consistent condi-tions. Rather, data for different parts of the environment may need to be sourced from different data collection efforts, in-troducing variance in both scene geometry (e.g., construction work and parked cars), as well as appearance (e.g., weather conditions and time of day).
We extend NeRF with appearance embeddings and learned pose reﬁnement to address the environmental changes and pose errors in the collected data. We addi-tionally add exposure conditioning to provide the ability to modify the exposure during inference. We refer to this modiﬁed model as a Block-NeRF. Scaling up the network capacity of Block-NeRF enables the ability to represent in-creasingly large scenes. However this approach comes with a number of limitations; rendering time scales with the size of the network, networks can no longer ﬁt on a single compute device, and updating or expanding the environment requires retraining the entire network.
To address these challenges, we propose dividing up large environments into individually trained Block-NeRFs, which are then rendered and combined dynamically at inference time. Modeling these Block-NeRFs independently allows for maximum ﬂexibility, scales up to arbitrarily large en-vironments and provides the ability to update or introduce new regions in a piecewise manner without retraining the entire environment as demonstrated in Figure 1. To com-pute a target view, only a subset of the Block-NeRFs are rendered and then composited based on their geographic lo-cation compared to the camera. To allow for more seamless compositing, we propose an appearance matching technique which brings different Block-NeRFs into visual alignment by optimizing their appearance embeddings. 2.