Abstract
The resurgence of unsupervised learning can be at-tributed to the remarkable progress of self-supervised learn-ing, which includes generative (G) and discriminative (D) models. In computer vision, the mainstream self-supervised learning algorithms are D models. However, designing a D model could be over-complicated; also, some studies hinted that a D model might not be as general and interpretable as a G model. In this paper, we switch from D models to
G models using the classical auto-encoder (AE). Note that a vanilla G model was far less efficient than a D model in self-supervised computer vision tasks, as it wastes model capability on overfitting semantic-agnostic high-frequency details.
Inspired by perceptual learning that could use cross-view learning to perceive concepts and semantics1, we propose a novel AE that could learn semantic-aware representation via cross-view image reconstruction. We use one view of an image as the input and another view of the same image as the reconstruction target. This kind of AE has rarely been studied before, and the optimization is very difficult. To enhance learning ability and find a feasible so-lution, we propose a semantic aligner that uses geometric transformation knowledge to align the hidden code of AE to help optimization. These techniques significantly improve the representation learning ability of AE and make self-supervised learning with G models possible. Extensive ex-periments on many large-scale benchmarks (e.g., ImageNet,
COCO 2017, and SYSU-30k) demonstrate the effectiveness of our methods. Code is available at https://github. com/wanggrun/Semantic-Aware-AE. 1.

Introduction
Learning representations without human annotations is a long-standing vision full of expectations [5]. Although experiencing a downturn, it has gained a renaissance. Re-cently, the resurgence of unsupervised learning is attributed 1Following [26], we refer to semantics as visual concepts, e.g., a semantic-ware model indicates the model can perceive visual concepts, and the learned features are efficient in object recognition, detection, etc.
Figure 1. A comparison among a discriminative model (D model), an existing semantic-agnostic generative model (G model), and our semantic-aware generative model. to the remarkable process of self-supervised learning (SSL), which can be divided into two groups, i.e., generative mod-els (G models) and discriminative models (D models).
In computer vision, the mainstream SSL algorithms be-long to D models that learn representations via agent tasks, e.g., patch ordering [18], solving jigsaw puzzles [43], and rotation prediction [23]. Of all the agent tasks, contrastive learning [12–14, 25, 27] and metric learning [61] are cur-rently the most successful, which randomly augments each image into different views and compares the (dis)similarity between different views (see Figure 1 (a)). But as pointed out by [4,27,61], without careful design, a contrastive learn-ing algorithm would collapse. Special regularizations (e.g., losses [4], normalizations [27], centering [10]), unusual optimizations (e.g., gradient stopping [14], mean teacher
[51]), and non-trival architectures (e.g., additional predic-tors [25]) that are difficult to explain are often needed. Be-sides, some studies also suggested that a D model might hold some disadvantages compared to a G model in gen-eralization and interpretability [3, 6, 26]. Specifically, G models might be more effective in pretraining foundation models [6] for fine-tuning tasks or downstream tasks, and the development of G models helps unify the pretraining paradigms in the CV and NLP domains [3, 17]. Moreover, with G models, one can further conduct a counterfactual in-tervention for explainability [1].
In this paper, we switch from D models to G models
Figure 2. Examples of the cross-view image generation by our semantic-aware AE on the validation set. In each triplet, the left is the input, the middle is our generated result, and the right is the reconstruction target. The generated image is similar to the reconstruction target.
Although some may be slightly different from the reconstruction target, the generated images are reasonable (semantically plausible). using classical auto-encoders (AEs)2. Note that previous works seldom used a G model because it was not as effi-cient as a D model. For example, a typical G model Big-BiGAN [19] with ResNet-50 [30] achieved a 55.4% top-1 accuracy on the task of linear evaluation on ImageNet [48], which is 20.5 points lower than the triplet loss model [61], one of the best-performing D models. Similarly, the lat-est G model BEiT [3] with basic DeiT [52] only obtained 56.7% top-1 accuracy, still holding a 19.2% disadvantage compared to the D model. Actually, BEiT, iGPT [11], and MAE [26] can only perform well in pretraining tasks but fail to do well in direct discriminative representation learning tasks, e.g., a linear evaluation on ImageNet. A G model’s inefficiency is caused by the waste of capability on overfitting semantic-agnostic local high-frequency de-tails [3, 11, 19] and the ignorance to high-level semantics.
For instance, a traditional AE uses an image as an input and the same image as the regression target, making the model overly focused on semantic-agnostic information compres-sion rather than visual concepts (see Figure 1 (b)).
To make G models feasible in SSL, we need to tackle the above semantic agnosticism problem. Fortunately, Becker
& Hinton (1992) found that cross-view learning could en-able models to perceive concepts and semantics, and they proposed perceptual learning [5]. Inspired by this prior art, we propose a novel AE that could learn semantic-aware rep-resentation via cross-view image reconstruction. We take a view of the image as input and force the AE to reconstruct another view of the image (see Figure 1 (c)). However, this 2Sometimes, there is a minor controversy about whether a vanilla AE counts as a G model. However, the community reached a consensus that
AE new varieties like denoising AE [53], masked AE [3, 26], and varia-tional AE [35] are G models, because they can generate things that are not included in an input, e.g., our semantic-aware AE can generate a new im-age with a different angle from the input. These AE generators are similar conditional generators in GAN [24], e.g., Conditional GAN [33], Cycle-GAN [66], and StyleGAN [34], with inputs being conditions. rarely-explored AE model is hard to optimize in practice.
To solve this problem, we further propose a novel semantic alignment technology. Using the geometric transformation knowledge, we can adjust the hidden code of AE to ensure that the code semantic is aligned with the reconstruction tar-get, thereby improving the learning and optimization capa-bilities. These techniques significantly improve the repre-sentation learning ability of AE and make SSL with G mod-els possible in computer vision, leading to a state-of-the-art performance in feature learning, generalizability, and ex-plainability. Figure 2 shows some results of our cross-view image generation, which are promising.
In summary, our contributions are three-fold.
• We seek the possibility of replacing D models with
G models in SSL in computer vision. We rethink the inefficiency of G models from the perspective of over-fitting semantic-agnostic local high-frequency details and propose a novel semantic-aware AE inspired by perceptual learning. Our AE uses one image view as input and another view of the same image as the recon-struction target, which is rarely explored before.
• To help semantic-aware AE optimization, we propose a novel semantic alignment technique that uses the geometric transformation knowledge to semantically align the hidden AE codes to the reconstruction target.
These technologies significantly improve the represen-tation learning ability of AE and make the SSL with G model possible in computer vision.
• Extensive experiments show state-of-the-art perfor-mance of our method on several large-scale bench-marks (e.g., ImageNet [48], SYSU-30k [59], and
COCO 2017 [40]) and varieties of tasks, demonstrat-ing the effectiveness (e.g., feature learning, generaliz-ability, and interpretability) of our method.
2.