Abstract
This work digs into a root question in human percep-tion: can face geometry be gleaned from one’s voices? Pre-vious works that study this question only adopt develop-ments in image synthesis and convert voices into face im-ages to show correlations, but working on the image do-main unavoidably involves predicting attributes that voices cannot hint, including facial textures, hairstyles, and back-grounds. We instead investigate the ability to reconstruct 3D faces to concentrate on only geometry, which is much more physiologically grounded. We propose our analy-sis framework, Cross-Modal Perceptionist, under both su-pervised and unsupervised learning. First, we construct a dataset, Voxceleb-3D, which extends Voxceleb and includes paired voices and face meshes, making supervised learning possible. Second, we use a knowledge distillation mecha-nism to study whether face geometry can still be gleaned from voices without paired voices and 3D face data under limited availability of 3D face scans. We break down the core question into four parts and perform visual and nu-merical analyses as responses to the core question. Our
ﬁndings echo those in physiology and neuroscience about the correlation between voices and facial structures. The work provides future human-centric cross-modal learning with explainable foundations. See our project page. 1.

Introduction
This work studies to what extent voice can hint face ge-ometry motivated by recent studies on voice-face matching and cross-modal learning [29, 53, 60]. Many physiologi-cal attributes are embedded in voices. For example, speech is produced by articulatory structures, such as vocal folds, facial muscles, and facial skeletons, which are all densely connected. Such a fact intuitively indicates potential corre-lations between voices and face shapes [19]. Experiments in cognitive science point out that audio cues are associated with visual cues in human perception– especially in recog-nizing a person’s identity [4]. Recent neuroscience research further shows that two parallel processing of low-level audi-Figure 1. Cross-Modal Perceptionist. We study the correlations between voices and face geometry under both supervised and un-supervised learning settings. This work targets at more explainable human-centric cross-modal learning for biometric applications. tory and visual cues are integrated in the cortex, where voice processing affects facial structural analysis for the percep-tion purpose [58].
Traditional research in the voice domain focuses on uti-lizing voice inputs for predicting more conspicuous at-tributes which include speaker identity [6, 28, 42], age
[15, 41, 47], gender [27], and emotion [52, 59]. A novel di-rection in recent development goes beyond predicting these attributes and tries to reconstruct 2D face images from voice
[8,35,54]. Their research is built on an observation that one can approximately envision how an unknown speaker looks when listening to the speaker’s voice. Attempts towards val-idating this assumptive observation include the work [35] for image reconstruction and works [8, 54] using generative adversarial networks (GANs). They aim to output face im-ages from only a speaker’s voice.
However, face images from voices are inherently ill-the task involves predicting extraneous attributes posed: that voices cannot hint, including image backgrounds, hairstyles, headgears, or beards. These attributes are appar-ently that one can choose without changing voices. Simi-lar concerns arise regarding the correlations between voices and facial textures or ethnicity. [35] demonstrates a t-SNE plot in which ethnicity is scattered across all samples, in-dicating its low correlations to voices. As a result, quanti-fying the differences between an output face image and a
reference is hard and less grounded.
Instead of producing face images, our analysis moves to the 3D domain with mesh representations and predicts one’s face geometry or skull structures from voices, which is free from the above issues. Working on 3D meshes is less ambiguous than images because the former includes less noisy variations unrelated to a speaker’s voice, such as stylistic variations, hairstyles, background, and facial tex-tures. Moreover, meshes enable more straightforward quan-tiﬁcation of differences between prediction and groundtruth in the Euclidean space– unlike the case in using face im-ages, where sources of differences involve backgrounds and hairstyles.
From the perspective of 3D faces, much research atten-tion has been paid to 3D reconstruction from monocular images [16, 46, 56, 62] or video sequences [12, 25] for 3D face animation or talking face synthesis.
In contrast, we are the ﬁrst to investigate the correlations between one’s 3D face geometry and voices, and we focus on the analysis of the face geometry gleaned from one’s voices. Our goal is to validate the correlations between voices and face geom-etry towards more explainable human-centric cross-modal learning with neuroscience support.
The analysis inevitably involves acquiring large-scale 3D face scans with paired voices, which is expensive and subject to privacy. To deal with this issue, we propose a novel Voxceleb-3D dataset that includes paired voices and 3D face models. Voxceleb-3D is inherited from two widely used datasets: Voxceleb [30]) and VGGFace [37], which in-clude voice and face images of celebrities, respectively. The approach [63] we adopt to create Voxceleb-3D is inspired by 300W-LP-3D [62], the most-used 3D face dataset, and we will describe details in Sec.3.2.
Our analysis framework Cross-Modal Perceptionist (CMP), investigates the feasibility to predict face meshes using 3D Morphable Models (3DMM, Sec.3.1) from voices on the following two scenarios (Fig. 1). We ﬁrst train neural networks directly from Voxceleb-3D in a supervised learn-ing manner using the paired voices and 3DMM parameters (Sec.3.2). We further investigate an unsupervised learning setting to inspect whether face geometry can still be gleaned without paired voices and 3D faces, which is a more re-In this case, we use knowledge distilla-alistic scenario. tion (KD) [20] to transfer knacks from the state-of-the-art method for 3D faces from images, SynergyNet [56], into our student network and jointly train speech-to-image and image-to-3D blocks (Sec.3.3).
We design a set of metrics to measure the geometric ﬁt-ness based on points, lines, and regions for both the super-vised and the unsupervised scenarios. The evaluation at-tempts to show correlations between 3D faces and voices with straightforward neural network-based approaches. The analysis with CMP enables us to comprehend the corre-lations between face geometry and voices. Our research lays explainable foundations for human-centric cross-modal learning and biometric applications using voice-face corre-lations, such as security and surveillance when only voice is given.
Our goal is not to recover high-quality 3D face meshes from voices comparable to synthesis from visual modalities such as image or video inputs, but we try to answer the core question under our CMP framework: can face geometry be gleaned from voice? We break down the question into four parts and will answer them through experiments.
Q1.
Is it feasible to predict visually reasonable face meshes from voice?
Q2. How stable is the mesh prediction from different utterances of the same person?
Q3. Compared with face meshes produced by cascad-ing separately trained speech-to-image and image-to-3D-face methods, can the performance of a joint training ﬂow, where mesh prediction is trained with voice information, improve? How much?
Q4. What is the major improvement that voice informa-tion can bring in the joint training ﬂow?
Our contributions are summarized. 1. Towards explainable cross-modal learning, we are the ﬁrst to study the correlations between face geometry and voices. human-centric 2. We devise an analysis framework, Cross-Modal Per-ceptionist, which studies both supervised and unsuper-vised approaches to learn face meshes from voices. 3. We show extensive analysis and discussion and answer to four breakdown questions to validate the correla-tions between voices and face shapes 2.