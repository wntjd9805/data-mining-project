Abstract
A complex action consists of a sequence of atomic actions that interact with each other over a relatively long period of time. This paper introduces a probabilis-tic model named Uncertainty-Guided Probabilistic Trans-former (UGPT) for complex action recognition. The self-attention mechanism of a Transformer is used to capture the complex and long-term dynamics of the complex actions. By explicitly modeling the distribution of the attention scores, we extend the deterministic Transformer to a probabilistic
Transformer in order to quantify the uncertainty of the pre-diction. The model prediction uncertainty is used to im-prove both training and inference. Specifically, we propose a novel training strategy by introducing a majority model and a minority model based on the epistemic uncertainty.
During the inference, the prediction is jointly made by both models through a dynamic fusion strategy. Our method is validated on the benchmark datasets, including Breakfast
Actions, MultiTHUMOS, and Charades. The experiment re-sults show that our model achieves the state-of-the-art per-formance under both sufficient and insufficient data. 1.

Introduction
In general, a complex action refers to a high-level ac-tivity like “making a sandwich”, which consists of a se-quence of atomic actions such as “cutting bun” and “smear-ing butter” as illustrated in Fig. 1. In this paper, we deal with complex action recognition. It has many applications such as visual surveillance [23], human-robot interactions
[24], and sports analysis [33]. Complex action recogni-tion is challenging because of the following reasons: (1) complex actions have relatively long temporal durations, which makes it difficult for conventional dynamic models to capture the long-range dependencies; (2) people per-form the same complex actions differently, which causes a large intra-class variation; and (3) background and irrel-evant frames contained in the video may cause difficulties for the recognition.
Most existing approaches directly recognize the com-Figure 1. Two samples of “making a sandwich” in Breakfast Ac-tions [18]. A complex action consists of a sequence of atomic actions, which may vary in durations, orders, etc. plex actions without explicitly considering the underlying dynamics [1, 6, 29, 42]. In fact, the information of atomic actions and their interactions are often ignored. However, the underlying interactions among atomic actions over time are crucial for understanding complex actions [21, 44, 47].
Furthermore, complex actions may last for a long period, traditional sequence modeling architectures such as recur-rent neural network [30], long short-term memory [12] and gated recurrent unit network [5] may not effectively and ef-ficiently handle the long-range dependencies.
In this paper, we propose to use a Transformer [36] as the backbone of our model to explicitly capture the long-term dependencies among atomic actions. Transformers were first proposed for the language translation task to capture the dependencies among words. The self-attention mechanism, which is the core of a Transformer, is now widely used for computer vision tasks such as image generation [45], object detection [20], and group activity recognition [8]. Consid-ering its capabilities of capturing complex and long-range dynamics, Transformer is suitable for modeling complex actions.
While powerful, the conventional Transformer cannot ef-fectively quantify its prediction uncertainty, which is essen-tial to improving the model performance under noisy and imbalanced data distribution. To address this issue, we in-troduce a probabilistic Transformer. Specifically, we treat the attention scores of a Transformer as random variables to capture the stochastic dependencies and uncertainty in the inputs. We further propose to employ the negative log-likelihood loss function to train a multilayer perceptron to produce the distribution parameters for the attention scores.
The probabilistic attention scores allow us to accurately quantify the epistemic uncertainties of the model predic-tion. Guided by the prediction uncertainty, we introduce a novel training and inference strategy, whereby we train two models that respectively focus on low-uncertainty samples and high-uncertainty samples, which we refer to as majority model and minority model. During the inference, the two models are combined dynamically based on the uncertainty of the input to perform the final prediction. Experiments show the proposed probabilistic Transformer achieves state of the art performance and is robust under noisy and insuf-ficient data.
The main contributions of this paper are summarized as:
• We propose to exploit the self-attention mechanism of a Transformer to capture the long-term and complex dynamics for complex actions.
• To model the stochasticity in the data and in the com-plex action, we introduce the probabilistic Transformer that allows accurately quantifying the epistemic uncer-tainty of the prediction.
• Based on the estimated epistemic uncertainty, we pro-pose a novel strategy for both model training and in-ference by introducing a majority model and minority model, which improves both the model prediction ac-curacy and robustness.
• Our method achieved SOTA performance on bench-mark datasets, including Breakfast Actions, Multi-THUMOS, and Charades under both sufficient or in-sufficient training data. 2.