Abstract
Moir´e patterns, appearing as color distortions, severely degrade image and video qualities when ﬁlming a screen with digital cameras. Considering the increasing demands for capturing videos, we study how to remove such undesir-able moir´e patterns in videos, namely video demoir´eing. To this end, we introduce the ﬁrst hand-held video demoir´eing dataset with a dedicated data collection pipeline to ensure spatial and temporal alignments of captured data. Further, a baseline video demoir´eing model with implicit feature space alignment and selective feature aggregation is devel-oped to leverage complementary information from nearby frames to improve frame-level video demoir´eing. More im-portantly, we propose a relation-based temporal consis-tency loss to encourage the model to learn temporal con-sistency priors directly from ground-truth reference videos, which facilitates producing temporally consistent predic-tions and effectively maintains frame-level qualities. Ex-tensive experiments manifest the superiority of our model.
Code is available at https://daipengwa.github. io/VDmoire_ProjectPage/. 1.

Introduction
Video is an important source of entertainment, infor-mation recording and dissemination through social media.
When photographing a video on a screen, frequency alias-ing leads to moir´e patterns (Fig. 1) which appear as colored stripes, severely degrading the visual quality and ﬁdelity of captured contents. Although many research efforts have been made to remove such moir´e patterns in a single im-age [14,15,25,31,40,54] and attained notable progress with deep learning [14, 15, 25, 40, 54], video demoir´eing is still an unexplored research problem as far as we know, which is yet of great signiﬁcance due to the ubiquity and importance of video data in our daily life.
This paper investigates the problem of video demoir´eing.
Compared to image demoir´eing, this task offers more op-portunities for high-quality frame-level restoration through
*Corresponding Author s e m a r f e r i o
M s t l u s e r r u
O t-20 t
Figure 1. The ﬁrst row shows moir´e frames at different times, and the second row shows our demoir´ed results. Please see our videos, which are clean and temporally consistent. t+20 leveraging auxiliary information from nearby video frames but is yet more challenging as it requires not only frame-level visual quality but also temporal consistency.
The state-of-the-art image demoir´eing method [54] fails to recover temporally consistent videos due to its inabil-ity to access temporal information/supervision. Using ex-isting post-processing methods such as [18, 22]; in doing so, however, the chance is lost to utilize video informa-tion for enhancing frame-level quality. Besides, these post-processing methods are susceptible to artifacts in demoir´ed results, and complicate the system design, leading to in-creased computational costs. Another widely adopted strat-egy is to incorporate a ﬂow-based consistency regulariza-tion [21, 37, 51, 52] on the predicted videos during train-ing, which encourages aligned pixels from nearby frames to have the same pixel intensity values. While simple, such regularization ignores natural intensity changes of pixels in videos (Fig. 3 (a)), is prone to errors in estimated optical
ﬂows (Fig. 3 (b) and (c)), and has the potential to propa-gate artifacts of one frame to nearby frames. Consequently, the improved temporal consistency tends to sacriﬁce frame-level quality and ﬁdelity, leading to blurry and low-contrast results (Fig. 7 (a): blurry textures).
In this work, we present a simple video demoir´eing model to leverage multiple video frames and a new relation-based consistency loss to improve video-level temporal con-sistency without sacriﬁcing frame-level qualities. Besides, we construct the ﬁrst hand-held video demoir´eing dataset to facilitate further studies on learning-based approaches.
We analyze the characteristics of moir´e patterns in  
videos and develop a video demoir´eing baseline model fol-lowing [40, 49, 50] with a selective aggregation scheme to adaptively combine aligned features and a pyramid archi-tecture to enlarge the receptive ﬁeld. The baseline model can effectively leverage nearby frames for a better frame-level demoir´eing. Deep supervision at different scales is adopted during training to facilitate model optimization.
Moreover, inspired by the observation that human beings can perceive video ﬂickering [11] directly from consecu-tive frames without using explicitly aligned videos, we pro-pose a simple relation-based temporal consistency loss that encourages the direct relations (e.g., pixel intensity differ-ences) of predicted video frames to follow those of ground-truth frames.
In particular, we exploit such relations at multiple levels, including pixel level using pixel intensity differences and patch level using intensity statistics (e.g., mean) changes considering different patch sizes.
Instead of constraining intensities of aligned pixels to be identical, our relation-based regularization directly matches the natu-ral relations and changes of nearby video frames with those of ground-truth videos. This simple design bypasses the aforementioned drawbacks of ﬂow-based consistency regu-larization and avoids sacriﬁcing frame-level qualities while still being able to enforce the model to learn temporal con-sistency priors from ground-truth videos.
Further, as there are no available datasets for developing and evaluating video demoir´eing methods, we collect a new video demoir´eing dataset with a dedicated pipeline to en-sure spatial and temporal alignments between moir´e videos and corresponding ground-truth ones.
Finally, extensive experiments on our video demoir´eing dataset demonstrate the superior performance of our method. In particular, our method obtains 22% improve-ments in terms of LIPIS in comparison with MBCNN [54] and more than 75% of users preferred our results when com-pared with results without using the multi-scale relation-based consistency loss. 2.