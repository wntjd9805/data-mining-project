Abstract
×
Unstructured pruning is well suited to reduce the mem-ory footprint of convolutional neural networks (CNNs), both at training and inference time. CNNs contain parameters arranged in K
K filters. Standard unstructured prun-ing (SP) reduces the memory footprint of CNNs by setting filter elements to zero, thereby specifying a fixed subspace that constrains the filter. Especially if pruning is applied before or during training, this induces a strong bias. To overcome this, we introduce interspace pruning (IP), a gen-eral tool to improve existing pruning methods. It uses filters represented in a dynamic interspace by linear combinations of an underlying adaptive filter basis (FB). For IP, FB co-efficients are set to zero while un-pruned coefficients and
In this work, we provide mathe-FBs are trained jointly. matical evidence for IP’s superior performance and demon-strate that IP outperforms SP on all tested state-of-the-art unstructured pruning methods. Especially in challenging situations, like pruning for ImageNet or pruning to high sparsity, IP greatly exceeds SP with equal runtime and pa-rameter costs. Finally, we show that advances of IP are due to improved trainability and superior generalization ability. 1.

Introduction
Deep neural networks (DNNs) have shown state-of-the-art (SOTA) performance in many artificial intelligence ap-plications [52, 54, 72, 77, 80]. In order to solve these tasks, large models with up to billions of parameters are required.
However, training, transferring, storing and evaluating such large models is costly [61,65]. Pruning [19,20,22,30,34,50] sets parts of the network’s weights to zero. This reduces the model’s complexity and memory requirements, speeds up inference [4] and may lead to an improved generalization ability [3, 24, 34]. In recent years, training sparse models
‡Corresponding author. §Equal contribution. became of interest, providing the benefits of reduced mem-ory requirements and runtime not only for inference but also for training [13, 14, 35, 47, 49, 55, 66, 71, 75].
In this work, we mainly focus on methods that prune individual parameters before training, while the number of zeroed coefficients is kept fixed during training. With this unstructured pruning, a network’s memory footprint can be reduced. To lower the runtime in addition, specialized soft-and hardware is needed [11, 18, 21, 51]. For training sparse networks, we distinguish between (i) pruning at initializa-tion (PaI) [9, 35, 66, 71, 75] which prunes the network at ini-tialization and fixes zeroed parameters during training, (ii) finding the sparse architecture to be finally trained by iter-ative train-prune-reset cycles, a so called lottery ticket (LT)
[14,15], and (iii) dynamic sparse training (DST) [13,39,49] which prunes the network at initialization, but allows the pruning mask to be changed during training.
∈
× (cid:80)K2
RK×K with kernel size K
Convolutional neural networks (CNNs) are composed of layers, each having a certain number of input- and output channels. Every combination of input- and output channel is linked by a filter h
K.
A weight of h is a spatial coefficient hi,j for a spatial co-ordinate (i, j). Filters h can also be modeled in an inter-g(n) : λn space, a linear space n=1 λn spanned
{
∈ g(1), . . . , g(K2)
RK×K by a filter basis (FB)
:=
{
[12, 69]. One possibility for a FB is the standard basis which yields the spatial rep-B
} resentations. General interspace representations are more flexible since bases are not fixed. We represent h in an interspace in order to learn the FB spanning this space along with the FB coefficients λ, and thereby obtain a better representation for h. Thus, setting coefficients of flexible, adaptive FBs to zero will improve results compared to prune spatial coefficients. e(n) : n = 1, . . . , K 2
{
}
} ⊂
:=
R
F
F
·
For deep networks, where the layers’ purposes are usu-ally unknown to the experts but learnt during training, we believe that filters should train their bases along with their is dynamic, can be shared for any coefficients. A FB
F
(a) (b) (c)
Figure 1. (a) Overview of SP and IP. Contrarily to SP (b), IP (c) produces spatially dense filters after training sparse networks. As for SP, sparsity in the interspace can be used to reduce memory requirements and, by the linearity of convolutions, also computational costs.
·
× number of K
K filters, and is optimized jointly with its FB coefficients λ. By fitting an interspace to sparse filters dur-ing training, we overcome the lack of prior knowledge for a basis that is well suited to describe filters with few non-zero coefficients. If a filter is pruned to a single FB coefficient, g(n), it is not restricted since g(n) can change. h = λn
Thus, pruning interspace coefficients of dynamic FBs keeps the CNN flexible and is called interspace pruning (IP). A 1-e(n) directly predefines h to stay on sparse filter h = hin,jn · e(n) the fixed subspace span
. Pruning spatial coefficients
{ is called standard pruning (SP). w.r.t. the standard basis
B
During training sparse CNNs, the problem of vanishing gradients due to spatial sparsity often occurs [66, 71, 74].
In contrast, IP pruned networks are able to learn spatially dense FBs during training, even when using sparse inter-space coefficients, see Fig. 1. Therefore, IP leads to an im-proved information flow and better trainable models.
}
Although IP yields dense spatial representations, the lin-earity of convolutions can be used to reduce the number of computations for CNNs with sparse interspace coefficients.
Compared to SP, IP only increases the number of required computations by a small, constant count. However, as IP provides superior sparse models, IP generates CNNs with faster inference speed than SP while matching the dense performance. Further, the dynamic achieved by interspace representations is cheap in terms of memory. A FB has
F
K 4 parameters as it contains K 2 filters of size K
K. A single FB can be shared for all K
K filters in a CNN.
Also, more than one FB can be used with just a small in-crease in memory requirements. For cost reasons, we do not use more FBs than the number of layers in a CNN in our experiments, resulting in all FBs creating an overhead of at most 0.01% of the dense network’s parameters. De-spite adding only few additional costs compared to using spatial weights, interspace representations significantly im-prove results for sparse and dense training.
Our core contributions are:
×
×
•
Representing and training convolutional filters in the interspace, a linear space spanned by a trainable FB.
•
•
•
The FB is optimized jointly with the FB coefficients.
Formulating the concept of pruning for filters with in-terspace representation as general method to improve performance of CNNs with sparse coefficients.
Theoretical proof of IP’s improvements in Thm. 1.
Experiments showing that IP exceeds SP for equal run-time and memory costs on SOTA sparse training meth-ods and pruning methods which are applied during training or on pre-trained models. We demonstrate that
IP’s superiority is achieved by improved trainability, and at lower sparsity also due to better generalization. 2. Broader Impact
Pruning can lower costs for training, storing and eval-uating DNNs. We are not aware of any negative outcome directly induced by this work. Nevertheless, as tool to im-prove pruning, and therefore to reduce costs for CNNs, IP could be used for any CNN based application with negative ethical or societal impact. As authors, we distance ourselves from such applications and the use of our method therein.
As we show in the paper, IP improves unstructured prun-ing in general and is not restricted to a special scenario.
We see IP as a tool which is applied in combination with
SOTA SP techniques to lower costs further. Consequently, our work is to the advantage of everyone using pruning and, by the improved generalization ability obtained by training with interspace representations, deep learning in general. 3.