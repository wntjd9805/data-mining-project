Abstract
Semantic segmentation of 3D medical images is a chal-lenging task due to the high variability of the shape and pattern of objects (such as organs or tumors). Given the recent success of deep learning in medical image segmen-tation, Neural Architecture Search (NAS) has been intro-duced to find high-performance 3D segmentation network architectures. However, because of the massive computa-tional requirements of 3D data and the discrete optimiza-tion nature of architecture search, previous NAS methods require a long search time or necessary continuous relax-ation, and commonly lead to sub-optimal network architec-tures. While one-shot NAS can potentially address these disadvantages, its application in the segmentation domain has not been well studied in the expansive multi-scale multi-path search space. To enable one-shot NAS for medical im-age segmentation, our method, named HyperSegNAS, intro-duces a HyperNet to assist super-net training by incorpo-rating architecture topology information. Such a Hyper-Net can be removed once the super-net is trained and in-troduces no overhead during architecture search. We show that HyperSegNAS yields better performing and more intu-itive architectures compared to the previous state-of-the-art (SOTA) segmentation networks; furthermore, it can quickly and accurately find good architecture candidates under dif-ferent computing constraints. Our method is evaluated on public datasets from the Medical Segmentation Decathlon (MSD) challenge, and achieves SOTA performances. 1.

Introduction
Automated medical image segmentation is an active research area with many clinical applications including anatomical analysis [29] and disease diagnosis [14]. Med-ical image segmentation remains challenging despite ad-vances in deep learning methods. To address the large vari-ability of target objects, the limited number of data samples, and the higher dimensional and high resolution nature of (a) DiNTS [16] Searched Architecture, 5,067 MB (b) HyperSegNAS Searched Architecture, 5,358 MB (c) Five-fold cross-validation for 1a and 1b
Figure 1. Architectures found from DiNTS [16] (1a) and our proposed HyperSegNAS (1b) based on the Pancreas dataset in
MSD [1]. Under similar compute costs, HyperSegNAS’ archi-tecture uses more skip connections and ensure features of multiple scales are propagated for predictions. Our architecture performs significantly better than DiNTS, as shown in 1c. 3D images, researchers have spent a great effort in design-ing efficient neural architectures under various computing settings [9, 10, 27, 28, 32].
Neural Architecture Search (NAS) has surfaced in re-sponse to handcrafted approaches and promises to find well performing architecture through automated algorithms.
While initially focusing on classification [5, 26, 39, 43, 48],
NAS has been also introduced to the segmentation domain in recent years [16, 21, 25, 37, 38, 40, 44, 47]. Segmentation
networks usually preserve features from multiple scales and aggregate them together to accurately segment objects with different sizes. Accordingly, the search space for multi-scale, multi-path architectures can be very complex, as the feasible network configurations increase exponentially with scale. C2FNAS [44] needs close to one GPU year to search for a single 3D segmentation architecture based on the Evo-lutionary Algorithm (EA), despite building on a U-shape network architecture. On the other hand, DiNTS [16] re-laxes the architecture search problem from a discrete formu-lation to a continuous and differentiable one, thus it greatly speeds up the searching process. Such a relaxation, how-ever, may lead to (1) an optimization gap when the contin-uous architecture/edge weights are discretized for deploy-ment [8,35] and (2) possibly infeasible architectures, which require ad-hoc logic to be handled.
Beyond finding the best performing architecture, balanc-ing other computing constraints to the search process is an-other important aspect of NAS and has led to popularity in methods like one-shot NAS [2, 3, 5–7, 12, 18, 22, 30, 33, 41, 43], where all sub-nets are trained with shared parameters under a large super-net. This aspect is especially important for medical imaging segmentation task for which memory usage is a major challenge when searching for the optimal architecture across various devices and resource constraints.
Once training is completed, the shared parameters can be used to evaluate the possible sub-nets and select the best performers given the specified constraints. So far, one-shot approaches have not been applied to the 3D segmentation’s search space yet. As there are no clear training strategies, such as the unilateral augmented (UA) principle [34] used in progressive shrinking [5] and the Sandwich Rule [42,43], in a multi-scale, multi-path search space [25], we find that the one-shot training scheme with randomly sampled architec-ture topology frequently leads to sub-optimal performance.
We propose HyperSegNAS, which follows the one-shot
NAS approach and seeks to address its issues in the segmen-tation space. Within HyperSegNAS, we propose a novel
Meta-Assistant Network (MAN) to improve super-net train-ing. Specifically, both the sampled architecture and the in-put image are fed to MAN. MAN then dynamically mod-ifies the shared weights during the training process based on deploying architectures. We show that such an architec-ture topology-aware training method significantly improves sub-net performances. When training is completed, an an-nealing process is used to gradually remove MAN from the main network. Even after removing MAN, the sub-net per-formances remain high and can be efficiently evaluated.
Benefiting from the one-shot paradigm, HyperSegNAS has many advantages. Firstly, HyperSegNAS is much more efficient at evaluating over many architectures under differ-ent computing constraints, as the shared parameters only
In comparison, approaches in need to be trained once.
[16,44] need to search from scratch to obtain each single ar-chitecture. Secondly, HyperSegNAS evaluates on discrete architectures, which eliminates the discretization gap and infeasible configurations that arise in differentiable NAS.
Finally, HyperSegNAS can accurately fit to the given con-straints due to the decoupling between training and search-ing. As demonstrated in Figures 1a and 1b, our search al-gorithm results in a very different architecture as compared to DiNTS’s under similar computing constraints. Not only is our architecture significantly better performing, as shown in Fig. 1c, it also follows conventional intuitions in propa-gating multi-scale features through the use of skip connec-tions. Finally, HyperSegNAS achieves new SOTA perfor-mance on multiple tasks in the Medical Segmentation De-cathlon (MSD) challenge [1] using similar computing bud-get as DiNTS.
In summary, our main contributions are listed below:
• We propose HyperSegNAS, a one-shot NAS approach at finding optimal 3D segmentation architectures. Hy-perSegNAS can search for efficient architectures that accurately adapt to different computing constraints.
• We propose a HyperNet structure called Meta-Assistant Network (MAN) to address the large search space of segmentation networks by incorporating rele-vant meta information to NAS; MAN can be removed after training, and does not add computing overhead for searching and re-training.
• We achieve better results on both low- and high-compute architectures compared to DiNTS and new
SOTA performances in multiple tasks in MSD. 2.