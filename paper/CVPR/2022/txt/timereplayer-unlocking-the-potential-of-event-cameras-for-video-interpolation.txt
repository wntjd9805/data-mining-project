Abstract
Recording fast motion in a high FPS (frame-per-second) requires expensive high-speed cameras. As an alternative, interpolating low-FPS videos from commodity cameras has attracted signiﬁcant attention. If only low-FPS videos are available, motion assumptions (linear or quadratic) are necessary to infer intermediate frames, which fail to model complex motions. Event camera, a new camera with pix-els producing events of brightness change at the temporal resolution of µs (10−6 second ), is a game-changing de-vice to enable video interpolation at the presence of arbi-trarily complex motion. Since event camera is a novel sen-sor, its potential has not been fulﬁlled due to the lack of processing algorithms. The pioneering work Time Lens in-troduced event cameras to video interpolation by designing optical devices to collect a large amount of paired training data of high-speed frames and events, which is too costly to scale. To fully unlock the potential of event cameras, this paper proposes a novel TimeReplayer algorithm to interpo-late videos captured by commodity cameras with events. It is trained in an unsupervised cycle-consistent style, cancel-ing the necessity of high-speed training data and bringing the additional ability of video extrapolation.
Its state-of-the-art results and demo videos in supplementary reveal the promising future of event-based vision. 1.

Introduction
Figure 1. Visual results of interpolating complex motion. Both models with linear motion assumption (SuperSloMo [9]) and quadratic motion assumption (QVI [38]) fail to model the motion of the soccer ball correctly. Beneﬁting from the high temporal res-olution of event data, the proposed method can correctly model nonlinear motion and interpolate intermediate frames. This ﬁgure contains animations. Best viewed in Acrobat Reader.
Traditionally, recording fast motion in high temporal res-olution has been the exclusive functionality of high-speed cameras, which are too expensive to be embedded in per-sonal devices such as smartphones. Video interpolation, the research ﬁeld of inferring intermediate frames between two
Corresponding authors: Xu Jia, Ziyang Zhang, and Wenhui Wang.
Webpage: https://sites.google.com/view/timereplayer. given frames, has attracted considerable research interest.
It can go beyond the temporal resolution limit of commod-ity cameras, and can be applicable in diverse downstream tasks such as slow motion generation [2,9,38], video editing
[32, 46], and virtual reality [1]. Without any further infor-mation or proper assumption, the video interpolation prob-lem is under-determined. In the past years, the computer
vision community has explored a lot in the direction of ex-ploiting proper assumptions. [9] proposes SuperSloMo for video interpolation with linear motion assumption, and [38] improves the result based on a quadratic motion assump-tion. As the assumption becomes more and more compli-cated, two obstacles get in the way: (1) linear motion [9] can interpolate video given two frames, but quadratic mo-tion [38] requires four consecutive frames to calculate the acceleration for quadratic interpolation. As the assumption becomes more complex, more frames and more computa-tional costs are needed to interpolate every frame. (2) the underlying motion in the video can be arbitrary, making it difﬁcult to verify pre-set assumptions on the motion type.
When the assumption and the motion type mismatch, the interpolated video may look unrealistic.
To deal with the inherent lack of intermediate informa-tion of conventional frame-based cameras, we bring in a novel neuromorphic sensor, event camera [6, 14, 33, 40], as a promising solution for video interpolation in case of complex motion. The event camera senses the dynamic change of pixel intensity, where an event is triggered once the change exceeds a certain threshold. The asynchronously event-driven processing fashion leads to a frame-less event stream with microsecond temporal resolution, as well as low power consumption and bandwidth [8, 28].
Since event camera almost records continuously streaming inten-sity change, it is able to store rich inter-frame information, which is critical to recovering intermediate video frames.
The introduction of event camera for video interpolation helps ease the difﬁculty in modeling complex motion.
Another crucial issue with the task of video interpolation is that it is difﬁcult to collect ground truth for video interpo-lation in the real world scenarios. Most existing video inter-polation models [2, 9, 23, 38] are trained on high-frame-rate dataset, which is constructed by taking an average of con-secutive frames of high-frame-rate videos recorded by high-speed cameras. However, these models would suffer from the domain gap between synthetic data and real world data.
Therefore, it is important to equip the models with the learn-ing ability to generalize to data without ground truth. There have been a few works [18, 31] aiming to address this issue.
They base their methods on cycle consistency, where mul-tiple intermediate frames are predicted and are then used to reconstruct a middle input frame. However, to achieve cy-cle consistency, these methods have to assume uniform mo-tion between consecutive frames at large timesteps, which makes them also suffer from the same problems as those uniform motion assumption-based methods.
In this work we introduce data from event camera into the video interpolation model design and propose an unsu-pervised learning framework to train the video interpolation model. Speciﬁcally, we introduce event stream to help di-rectly estimate optical ﬂows between an intermediate frame and input frames instead of computing them as proportions of the computed optical ﬂow between input frames. In this way the uniform linear motion assumption is broken and the proposed optical ﬂow estimation module can compute any complex nonlinear motion. Then an intermediate frame can be predicted by warping input frames according to the esti-mated optical ﬂows and taking a weighted average of them.
With the help of event streams and approximated inverse ones, a video interpolation model is able to predict an inter-mediate frame given two input frames and can also recon-struct an input frame when the estimated intermediate frame and another input frame are given. Therefore, loss functions can be computed between the reconstruction of input frames and the original ones and are used to train the video inter-polation model. Extensive experiments and ablation study on synthetic benchmark datasets demonstrate the effective-ness of the proposed framework, especially on videos with complex motion. In addition, we also further conduct an experiment on real data to validate its generalization ability.
Contributions of our work can be summarized as below.
• A novel video frame interpolation method with both video frame and event stream as input is proposed to address complex nonlinear motion.
• We design an unsupervised learning framework for video interpolation with event stream by applying cy-cle consistency.
• The proposed method performs favorably against state-of-the-art approaches on both synthetic bench-marks datasets and real data. When trained with ad-ditional unsupervised data, it achieves the best result thanks to the unsupervised nature, showing the promis-ing future of event-based video interpolation. 2.