Abstract
Video Panoptic Segmentation (VPS) aims at assigning a class label to each pixel, uniquely segmenting and identify-ing all object instances consistently across all frames. Clas-sic solutions usually decompose the VPS task into several sub-tasks and utilize multiple surrogates (e.g. boxes and masks, centers and offsets) to represent objects. However, this divide-and-conquer strategy requires complex post-processing in both spatial and temporal domains and is vul-nerable to failures from surrogate tasks. In this paper, in-spired by object-centric learning which learns compact and robust object representations, we present Slot-VPS, the first end-to-end framework for this task. We encode all panoptic entities in a video, including both foreground instances and background semantics, with a unified representation called panoptic slots. The coherent spatio-temporal object’s in-formation is retrieved and encoded into the panoptic slots by the proposed Video Panoptic Retriever, enabling to local-ize, segment, differentiate, and associate objects in a unified manner. Finally, the output panoptic slots can be directly converted into the class, mask, and object ID of panoptic objects in the video. We conduct extensive ablation stud-ies and demonstrate the effectiveness of our approach on two benchmark datasets, Cityscapes-VPS (val and test sets) and VIPER (val set), achieving new state-of-the-art perfor-mance of 63.7, 63.3 and 56.2 VPQ, respectively. 1.

Introduction
Video panoptic segmentation (VPS) [17, 35, 47] aims at classifying all foreground instances (things), e.g. cars, peo-ple, etc., and countless background semantics (stuff ), e.g.
*Corresponding author.
Figure 1. Comparison between previous works [17, 42] and the proposed Slot-VPS. VPSNet represents objects with multiple rep-resentations, relies on several sub-networks, and requires complex post-processing (e.g. NMS, things-stuff fusion, similarity score fu-sion for tracking), while we introduce panoptic slots to uniformly represent panoptic objects (i.e. things and stuff) in a video, en-abling a unified end-to-end framework. Code will be made pub-licly available at https://github.com/SAITPublic/SlotVPS. sky, road, etc., segmenting and tracking all object instances consistently across all frames. It is beneficial to many high-level video understanding tasks, such as Video Question
Answering [33] and Video Captioning [50], and various real-world applications, such as autonomous driving and video editing.
Existing methods [17, 35, 47] model things and stuff in a video separately with several sub-networks (as shown in
Figure 1) tailored to different sub-tasks including semantic segmentation [6, 30], instance segmentation [2, 14, 21], and tracking [38, 46]. Complicated post-processing in both spa-tial (e.g. things-stuff fusion) and temporal (e.g. similarity scores fusion for instance association) domains is needed to
fuse predictions of different subtasks into final VPS results.
However, such a decomposed pipeline suffers from sev-eral issues. First, the complicated post-processing is time-consuming and requires manual parameter tuning, which is likely to produce sub-optimal results. Second, erroneous predictions from different branches will adversely affect each other and harm the overall performance. For exam-ple, the inaccurate boxes will also lead to incomplete seg-mentation masks, and missing centers will deteriorate the temporal tracking results, which can barely be corrected by post-processing. Third, end-to-end training is blocked, po-tentially hindering the model from learning features directly optimized for the VPS task.
To solve the aforementioned problems, motivated by object-centric representation learning which learns the com-pact and robust representations of objects, we introduce a unified end-to-end framework, Slot-VPS, as illustrated in
Figure 1. All panoptic objects (including both stuff and things) in the video are represented as a unified represen-tation named panoptic slots. Panoptic slots are a set of learnable parameters and can be updated through interact-ing with features extracted from videos. Each panoptic slot is responsible for a stuff class or an object instance in the video, enabling the direct predictions of the class, mask, and object ID of each panoptic object in an end-to-end fashion.
To encode the spatio-temporal information of video-level panoptic objects into the panoptic slots, we introduce the Video Panoptic Retriever (VPR). VPR incorporates a
Panoptic Retriever to retrieve the location and appearance information from the spatial features for panoptic object lo-calization and segmentation, and a Video Retriever to cor-relate slots across different time steps for temporally associ-ating object instances. Furthermore, during the above pro-cess, softmax-based operation, which normalizes the con-tributing weights of each slot, is performed to encourage the panoptic slots to compete and be distinct from each other such that the redundancy among slots will be suppressed.
Finally, the spatio-temporal coherent panoptic slots, carry-ing both object’s spatial information and temporal identifi-cation information, can be utilized to directly predict final results, i.e. class, mask, and object ID of panoptic objects in the video.
To our best knowledge, this is the first fully unified end-to-end framework for the VPS task. It does not rely on any surrogates in both spatial and temporal domains and hence bypasses the drawbacks of dependence on complex post-processing and influence from sub tasks’ failures.
Experimental results on the Cityscapes-VPS [17] and
VIPER [17] datasets demonstrate the effectiveness of our method. Thanks to the unified end-to-end framework and the object-centric learning, our method outperforms the state-of-the-art [17, 35] on the val and test sets (63.7, 63.3
VPQ) of Cityscapes-VPS, and the val set (56.2 VPQ) of
Figure 2. Speed-Accuracy trade-off curve on the Cityscapes-VPS val set. The latency is measured on V100 GPU.
VIPER with better efficiency.
Our main contributions can be summarized as follows:
• We propose to uniformly represent all panoptic objects in the video with a unified representation (i.e. panoptic slots), and introduce Slot-VPS, the first unified end-to-end pipeline for the VPS task.
• To spatially localize, segment, differentiate and tem-porally associate objects, Video Panoptic Retriever (VPR) is developed to retrieve and encode spatio-temporal coherent objects’ information into panoptic slots.
• Our method outperforms the state-of-the-arts [17, 35] on both Cityscapes-VPS and VIPER datasets. What’s more, as shown in Figure 2, our model has better effi-ciency. 2.