Abstract
Zero-shot sketch-based image retrieval typically asks for a trained model to be applied as is to unseen categories. In this paper, we question to argue that this setup by defini-tion is not compatible with the inherent abstract and sub-jective nature of sketches – the model might transfer well to new categories, but will not understand sketches existing in different test-time distribution as a result. We thus extend
ZS-SBIR asking it to transfer to both categories and sketch distributions. Our key contribution is a test-time training paradigm that can adapt using just one sketch. Since there is no paired photo, we make use of a sketch raster-vector reconstruction module as a self-supervised auxiliary task.
To maintain the fidelity of the trained cross-modal joint em-bedding during test-time update, we design a novel meta-learning based training paradigm to learn a separation be-tween model updates incurred by this auxiliary task from those off the primary objective of discriminative learning.
Extensive experiments show our model to outperform state-of-the-arts, thanks to the proposed test-time adaption that not only transfers to new categories but also accommodates to new sketching styles. 1.

Introduction
Sketch-based image retrieval (SBIR) is by now a well-established topic in the vision community [14, 16]. Re-search efforts have mainly focused on addressing the sketch-photo domain gap, incurred by abstraction [33], drawing style [47] and stroke saliency [19]. Despite great strides made, the field remains plagued by the data scarcity problem – sketches are notoriously difficult to collect [4, 6].
Zero-shot SBIR (ZS-SBIR) in particular represents the main body of work behind this push for addressing data scarcity.
It specifically examines the scarcity issue from a category transfer perspective, and strives for utilising sketch-photo pairs from seen categories to train a model that
*Interned with SketchX
Figure 1. Normal ZS-SBIR methods obtain lower accuracies as they retrieve from unseen data using model weights trained on seen data. During inference, our model (Sketch3T) adapts to the test distribution via an auxiliary task, before retrieval, scoring better. could be directly applied on those unseen (see Fig. 1(a)).
In this paper, we question this otherwise commonly ac-cepted setup at definition level. We importantly argue that the very assumption of being able to apply a trained model as is to unseen categories, is by definition incompatible with the inherent subjective nature of sketch data. This largely results in a model that might well understand the seman-tic category shift, but not acute to changes in sketching style and abstraction level (both being prevalent problems in sketch [47]). Alleviating this problem is particularly crucial for the practical adaption of SBIR, as otherwise retrieval performance will incur a significant drop – a system that understands “my” sketches, might not understand “yours”.
This paper thus extends the conventional definition of
ZS-SBIR to embrace this new problem, i.e., a new ZS-SBIR framework that (i) not only transfers knowledge on to un-known categories, (ii) but also adapts to the unique style of new sketches. We implement this by adopting a test-time-training framework that adapts to new categories and new styles at inference time. That is, instead of anticipating the distribution shifts via normal training, we intend to learn
them at test time. The beauty of our solution lies in that we achieve a higher accuracy without any additional train-ing sketch-photo pairs, but with just a single query sketch, no more than what is required in a typical ZS-SBIR setup (Fig. 1). It follows that this single sketch will first adjust the model to unseen style and category, and then use again as query to retrieve using the updated model, all test time.
Implementing this test-time-training framework despite intuitive is not trivial. There are two major challenges:
Firstly, we have access to only the query sketches during inference, without any label or paired photo for supervision.
Secondly, this test-time update should not degrade the joint embedding (that conducts retrieval) which has been learned using sketch-pairs. Solution to the first issue requires a task where labels can be obtained freely/synthetically during in-ference itself. Here we make clever use of the vectorised na-ture of sketches, and utilise a self-supervised task of sketch-raster to sketch-vector translation [5] to update the feature-extractor at inference.
It follows that via this translation operation, the model adapts itself to the new style/category of the test sketch.
The second issue gets tackled at model design. In par-ticular, we consolidate the said sketch self-reconstruction module as an auxiliary task within a meta-learning frame-work [26].
It follows that the model is meta-learned in a way, such that updates on the auxiliary task only hap-pens in the inner loop, which then prevents it from dis-torting the joint embedding space whose updates occurs elsewhere in the outer loop via a triplet loss. This train-ing strategy essentially ensures that the trained model now knows how to accommodate the auxiliary task loss with-out affecting the latent space too adversely, and accordingly defends itself against test-time updates from the sketch self-reconstruction auxiliary task.
More specifically, our framework shares a feature extrac-tor amongst three diverging branches (Fig. 2 (left)): (i) a primary branch learns the cross-modal embedding over a triplet loss [66] using paired sketch-photo information, (ii) an auxiliary sketch branch that focuses on self-modal reconstruction to update and condition the shared feature-extractor towards better sketch-encoding, and (iii) an aux-iliary photo branch, where we use photo-to-edgemap trans-lation to condition the photo features. Having this photo branch also presents the option of updating the model on the unseen test-set photo-gallery to yield better photo fea-tures for retrieval, however is not compulsory. Note that only the auxiliary sketch branch get updated (and hence the shared feature extractor) at test-time upon a query sketch.
Our contributions are: (a) We offer a fresh extension on the ZS-SBIR paradigm, by proposing a novel test-time training framework that dynamically adapts a trained en-coder to new sketches (b) To retrain transferable cross-modal embedding knowledge during inference, we propose a meta-learning framework that integrates primary discrim-inative learning with auxiliary tasks, such that updates from the latter are constrained towards benefiting the primary ob-jective. (c) Extensive experiments and ablation confirm our method to be superior to existing state-of-the-arts. 2.