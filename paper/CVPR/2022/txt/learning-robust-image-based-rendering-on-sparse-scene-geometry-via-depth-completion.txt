Abstract
Recent image-based rendering (IBR) methods usually adopt plenty of views to reconstruct dense scene geometry.
However, the number of available views is limited in prac-tice. When only few views are provided, the performance of these methods drops off significantly, as the scene geom-etry becomes sparse as well. Therefore, in this paper, we propose Sparse-IBRNet (SIBRNet) to perform robust IBR on sparse scene geometry by depth completion. The SIBR-Net has two stages, geometry recovery (GR) stage and light blending (LB) stage. Specifically, GR stage takes sparse depth map and RGB as input to predict dense depth map by exploiting the correlation between two modals. As in-accuracy of the complete depth map may cause projection biases in the warping process, LB stage first uses a bias-corrected module (BCM) to rectify deviations, and then ag-gregates modified features from different views to render a novel view. Extensive experimental results demonstrate that our method performs best on sparse scene geometry than re-cent IBR methods, and it can generate better or comparable results as well when the geometric information is dense.1 1.

Introduction
Image-based rendering (IBR), as one of the classic ap-proaches of novel view synthesis, aims to synthesize novel view from real views. It has been widely used to enhance the visualization in various applications, such as virtual nav-igation [15], video stabilization [24], AR\VR [37, 38]. A novel view is generated in IBR by warping pixels from source view into target view with scene geometry and ag-gregating them with blending methods. The output quality depends on the accuracy and completeness of geometry in-formation and the effectiveness of blending strategy. 1This work is supported by NSFC (Grant No.: U2001209, 61902076) and Natural Science Foundation of Shanghai (21ZR1406600).
* Corresponding author: Bo Yan.
Figure 1. Strategy Comparison. (a) simulates dense view sam-pling on a tank model, while the red cameras represent sparse view sampling. (b) shows the strategy of recent IBR methods.
They adopt dense views to generate a dense depth map for a better warped result (below the depth map) and then synthesize a novel view. (c) shows the paradigm of our SIBRNet. Given sparse views, a sparse depth map is generated by MVS before sent into a geom-etry recovery stage for completion. The complete depth map has a comparable warped result to the dense depth map. And then a light blending stage is applied to render the final result. Red boxes indicate the effect of two stages.
Recent IBR methods [1, 31, 32] ask for dense geome-try to guarantee adequate correct projections. As shown in
Figure 1.(b), they use plentiful views to reconstruct dense proxy geometry by multi-view stereo algorithm (MVS)
[16,34,51]. This process is time-consuming and can only be conducted offline. Moreover, dense views are not available easily in practice. When input views are sparse, the scene geometry produced by MVS becomes sparse as well, lead-ing to a rapid decline on performance for these methods.
In order to reduce the dependence on dense views, we propose SIBRNet to perform robust IBR on sparse scene geometry. It contains two stages, geometry recovery (GR) stage and light blending (LB) stage. Inspired by recent ge-ometry recovery works like point cloud upsampling [21,54] and depth completion [25, 46, 56], the GR stage introduces a learning-based depth completion network into IBR to pre-dict a complete depth map from the sparse one. As shown
Figure 2. Visual comparisons with recent IBR methods on Tanks and Temples dataset. FVS [31] and SVS [32] will produce a blur in the area where the input sparse depth map misses, while our method can still synthesize a photorealistic result. in Figure 1.(c), given a sparse depth map, the GR stage can ensure a comparable warped result to the dense depth map.
The LB stage is used to aggregate different source views and synthesize a novel view. In this way, our method can synthesize much better results than recent IBR methods on sparse scene geometry. In addition, although our method focuses on sparse geometry input, it can also achieve better or comparable results when the input geometry is dense.
Specifically, the GR stage takes a sparse depth map and the associated RGB image as input to predict a complete depth map.
In order to take advantage of the correlation between depth and color, we design two subnetworks to ex-tract different information. The final complete depth map is generated by fusing the results from two subnetworks. The
LB stage utilizes the complete depth maps to warp source view feature maps into target view through a 3D warping.
As the inaccurate depth value will produce projection bi-ases, we design a bias-corrected module (BCM) to rectify these warped features. After that, a Bi-ConvLSTM is ap-plied for aggregating information from different view points and synthesizing a candidate image and a confidence map for each view. Finally, a novel view is generated by blend-ing all candidate images with a softmax.
We also propose a new dataset called Surround to evalu-ate IBR methods in a surround setting. In addition, in order to train and evaluate our method on sparse scene geome-try, we preprocess two public datasets, Tanks and Temples,
Free View Synthesis, and our Surround dataset to gener-ate depth maps at different sparsity levels. Quantitative and qualitative experiments on the three datasets show that our method is robust on different depth sparsity. It can generate more realistic results, especially on sparse scene geometry, as shown in Figure 2. Our method has obvious advantages in the depth missing area. Our code and dataset will be re-leased here.
In summary, this paper has the following contributions:
• In order to reduce the dependence of existing methods on dense input views, we propose a two-stage model named SIBRNet to perform robust IBR on sparse scene geometry by introducing a learning-based depth completion network for the first time. It is robust for scene geometry on different sparsity levels.
• The inaccurate complete depth value will result in pro-jection biases during the warping process, which may cause image distortions in the final result. Therefore, we design a bias-corrected module (BCM) using de-formable convolution to rectify these deviations.
• A new dataset called Surround is proposed for evaluat-ing. It contains a 360-degree panorama for each scene and is useful to evaluate IBR methods in a surround setting. 2.