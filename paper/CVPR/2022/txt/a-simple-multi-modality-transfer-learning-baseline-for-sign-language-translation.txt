Abstract
This paper proposes a simple transfer learning base-line for sign language translation. Existing sign language datasets (e.g. PHOENIX-2014T, CSL-Daily) contain only about 10K-20K pairs of sign videos, gloss annotations and texts, which are an order of magnitude smaller than typi-cal parallel data for training spoken language translation models. Data is thus a bottleneck for training effective sign language translation models. To mitigate this problem, we propose to progressively pretrain the model from general-domain datasets that include a large amount of external supervision to within-domain datasets. Concretely, we pre-train the sign-to-gloss visual network on the general domain of human actions and the within-domain of a sign-to-gloss dataset, and pretrain the gloss-to-text translation network on the general domain of a multilingual corpus and the within-domain of a gloss-to-text corpus. The joint model is fine-tuned with an additional module named the visual-language mapper that connects the two networks. This sim-ple baseline surpasses the previous state-of-the-art results on two sign language translation benchmarks, demonstrat-ing the effectiveness of transfer learning. With its simplicity and strong performance, this approach can serve as a solid baseline for future research. 1.

Introduction
Sign languages are visual signals for communication among the deaf and hard of hearing. These languages are primarily expressed through manual articulations, but are also greatly aided by the movement of body, head, mouth, eyes and eyebrows. While technology for automatic ma-chine translation of spoken languages have successfully been deployed in production [8,25,33,44], research on sign language translation (SLT) lags behind and is still in early-stage development. An effective system for automatic sign language translation may help to build a bridge between
*Accomplished during Yutong Chen’s internship at MSRA.
Figure 1. We decouple sign language translation into a visual task (left part) and a language task (right part), and propose a visual-language mapper (V-L Mapper) to bridge the connection between them. The decoupling allows both the visual and language net-works to be effectively and independently pretrained before joint training. Both spatio-temporal information from sign videos and semantic knowledge from text transcriptions are encoded through
VL-Mapper. hearing-impaired and unimpaired people.
Existing sign language translation methods follow the framework of neural machine translation (NMT) originally developed for spoken languages [4–6, 46, 49, 50], with the distinction that the source language is represented as spatio-temporal pixels instead of discrete tokens. To be con-crete, sign videos are first fed into a video backbone net-work to extract an intermediate representation, which is then mapped to the target language text via NMT. The inter-mediate representation is usually supervised by glosses1 [6, 49,50], where each gloss corresponds to the semantic mean-ing of a single sign (e.g. happy, sad) in the continuous video input.
Despite adopting the formulation of advanced neural machine translation, the current results are far from sat-1Glosses are the word-for-word transcription of sign language where each gloss is a unique label for a sign. Typically, we identify each gloss by a capitalized word which is loosely associated with the sign’s meaning.
The best reported sign language translation isfactory. performance [49] on the PHOENIX-Weather-2014T test dataset [4] is 24.32 in terms of BLEU-4, while a baseline transformer achieves a 30.9 BLEU-4 score for English to
German translation [33]. We hypothesize that the key fac-tor that hinders the progress of sign language translation is the scale of the training data. To effectively train a typical
NMT model, it usually requires a corpus of 1M paralleled samples [42]. However, existing sign language datasets are an order of magnitude smaller, containing only fewer than 20K paralleled samples [4, 49].
In this paper, we study a multi-modal pretraining ap-proach to cope with the data scarcity issue for sign language translation. While pretraining and transfer learning has greatly improved performance in tasks of vision [11,17,34], language [10, 25, 27, 33, 40] and cross-modality [26, 31, 35, 39, 47], they are still under explored in SLT. Our work aims to exploit their strength in SLT.
SLT can be broken down into two disjoint tasks: a visual action recognition task that converts sign videos into seman-tic glosses (Sign2Gloss), and a language translation task that maps glosses into spoken language texts (Gloss2Text).
Our transfer learning approach progressively pretrains each task separately and then finetunes the joint model. For
Sign2Gloss, we first pretrain the visual model on a gen-eral domain to learn generic human actions [21, 28], and then we transfer it to within the domain to learn fine-grained glosses. Similarly for Gloss2Text, we adopt mBART [33], a denoising auto-encoder pretrained on a large-scale general-domain multilingual corpus, and transfer it to the within-domain task of gloss-to-text translation. By leveraging ex-isting datasets and supervisions that can effectively transfer to sign language translation, the necessity of gathering large parallel data is lessened.
With well-trained Sign2Gloss and Gloss2Text mod-ules, we can build a two-staged pipeline known as
Sign2Gloss2Text to generate a gloss sequence from the video and then translate the predicted gloss sequence into text. This two-staged pipeline is also implemented in [4, 6, 46, 49] and shows promising results. However, glosses are discrete representations of the language modality, with-out encoding any spatio-temporal visual information from sign videos such as facial expressions2, which may lead to degraded translation performance. For example, hearing-impaired individuals use exaggerated facial expressions to convey the adverb ‘Extremely’, but this kind of information is ignored in gloss annotation. In contrast, labelers and lin-guists have to take into account these adverbs to produce translated sentences that are complete and semantically ac-curate. Incorporation of both visual and language modali-ties is thus needed. 2In sign languages, facial expressions are used to express both linguistic information and emotions.
To this end, we introduce a visual-language mapper which connects the visual features before gloss classifica-tion in the visual model to the gloss embedding in the trans-lation model. With this mapper, the full model is jointly optimized and the discrete gloss representation is circum-vented in joint training. The mapper is simply implemented as a fully connected MLP with two hidden layers. Figure 1 shows our design.
In contrast to previous works which attempt to improve translation performance by integrating multiple cues from mouthing or pose in a handcrafted manner [5, 50] or by adopting advanced machine translation techniques such as back-translation [49], our overall framework is extremely simple, resulting in a transfer learning approach on top of a standard NMT model. Some previous works conduct transfer learning for SLT by pretraining visual backbone on human action recognition [29] or loading pretrained word embeddings [29, 46], while we are the first to adopt both general-domain and within-domain pretraining in a progres-sive manner and incorporate pretrained spoken language model into SLT. Our experimental results demonstrate that this progressive pretraining of visual and translation models greatly boosts performance. Our simple approach surpasses all existing methods by large margins, including those that employ semi-supervised learning, on PHOENIX-2014T [4] and CSL-Daily [49]. 2.