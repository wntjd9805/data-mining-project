Abstract
High annotation costs are a substantial bottleneck in ap-plying modern deep learning architectures to clinically rel-evant medical use cases, substantiating the need for novel algorithms to learn from unlabeled data. In this work, we propose ContIG, a self-supervised method that can learn from large datasets of unlabeled medical images and ge-netic data. Our approach aligns images and several ge-netic modalities in the feature space using a contrastive loss. We design our method to integrate multiple modal-ities of each individual person in the same model end-to-end, even when the available modalities vary across indi-viduals. Our procedure outperforms state-of-the-art self-supervised methods on all evaluated downstream bench-mark tasks. We also adapt gradient-based explainability algorithms to better understand the learned cross-modal as-sociations between the images and genetic modalities. Fi-nally, we perform genome-wide association studies on the features learned by our models, uncovering interesting re-lationships between images and genetic data.1 1.

Introduction
Medical imaging plays a vital role in patient healthcare.
It aids in disease prevention, early detection, diagnosis, and treatment. However, efforts to employ machine learning al-gorithms to support in clinical settings are often hampered by the high costs of required expert annotations [41]. At the same time, large-scale biobank studies have recently started to aggregate unprecedented scales of multimodal data on human health. For example, the UK Biobank (UKB) [96] contains data on 500, 000 individuals, including a wide range of imaging modalities such as retinal fundus images and cardiac, abdominal, and brain MRI. Similar studies are
*Equal contribution 1Source code at: https://github.com/HealthML/ContIG
Figure 1. Overview of our contrastive learning method from imag-ing and genomic data. It learns representations by bringing the modalities of each individual closer in the embedding space, and apart from different individuals’. In this example, the modalities are retinal fundus images (in brown), SNP data (in green), and polygenic risk scores (PGS) (in purple). Our method handles miss-ing modalities (e.g. missing PGS for the person in the upper right). currently underway in other countries, such as the Nationale
Kohorte (NaKo) [15], BioMe [13], FinnGen [34], Estonia
Biobank [14], and others. While some of these studies also include phenotypic descriptions, e.g. a person’s medical his-tory, such data tend to be both highly incomplete and biased due to clinical practices and assessment methods [76], mak-ing learning from them challenging and error-prone. On the other hand, genetic data is increasingly abundant. While chip-based genotyping technology has enabled the study of common genetic variation at scale [108], the exponentially decreasing costs of genomic sequencing is driving progress for rare genetic variation [80]. Due to these advances, the
UKB and other biobanks often contain a rich array of ge-netic and genomic measurements. Genetic data is gener-ally less susceptible to bias factors, and most diseases have at least a partially genetic cause, with some genetic disor-ders being exclusively attributed to genetic mutations [110].
Similarly, most other traits – not directly related to diseases –, e.g. height and human personality, are also strongly in-fluenced by genetics [64, 130]. Complementary imaging-genetics datasets are increasingly also available in other ap-plication settings, e.g. plant breeding [116].
Unlabelled medical images carry valuable information about organ structures, and an organism’s genome is the blueprint for biological functions in the individual’s body.
Clearly, integrating these distinct yet complementary data modalities can help create a more holistic picture of phys-ical and disease traits. Integrating these data types, how-ever, is non-trivial and challenging. The human genome consists of three billion base pairs, yet most genetic dif-ferences between individuals have little effect. This leads to challenges both in terms of computational aspects, and in terms of statistical efficiency. Unfortunately, it is not clear a priori which parts of the genome are relevant and which are not. Typically, genome-wide association stud-ies (GWAS) [50, 69] use statistical inference techniques to discover relationships between genetic variations and par-ticular physical or disease traits. To date, thousands of scientific works have found more than 300, 000 genetic-phenotype associations [51]. However, even now a large portion of known or presumed heritability of traits is not yet accounted for by the individual genome-trait associations, a phenomenon known as “missing heritability” [70]. Better methods to find – and explain – the relationships between genetic and imaging modalities may help close this gap.
Therefore, the growing number of biobanks of unlabeled multimodal (i.e. imaging-genetics) data, calls for solutions that can: (i) learn semantic data representations without requiring expensive expert annotations, (ii) integrate these data modalities end-to-end in an efficient manner, and (iii) explain discovered cross-modal correspondences (associa-tions). Self-supervised (unsupervised) representation learn-ing offers a viable solution when unlabeled data is abundant and labels are scarce. These methods witnessed a surge of interest after proving successful in several application domains [54]. The representations learned by these meth-ods facilitate data-efficient fine-tuning on supervised down-stream tasks, reducing significantly the burden of manual annotation. Furthermore, such methods allow for integrat-ing multiple data modalities as distinct views, which can lead to considerable performance gains. Despite the recent advancements in self-supervised methods, e.g. contrastive learning, only little work has been done to adopt these meth-In fact, we are not aware of ods in the medical domain. any prior work that leverages self-supervised representation learning on combined imaging and genetic modalities. We believe self-supervised learning has the potential to address the above challenges inherent to the medical domain.
Contributions. (i) We propose a self-supervised method, called ContIG, that can learn from multimodal datasets of unlabeled medical images and genetic data. ContIG aligns these modalities in the representation space using a con-trastive loss, which enables learning semantic representa-tions in the same model end-to-end. Our approach handles the case of multiple genetic modalities, in conjunction with images, even when the available modalities vary across in-(ii) We adapt gradient-based explainability al-dividuals. gorithms to better understand the learned cross-modal cor-respondences (associations) between the images and ge-netic modalities. Our method discovers interesting associ-ations, and we confirm their relevance by cross-referencing biomedical literature.
Our work presents a framework on how to exploit in-expensive self-supervised solutions on large corpora (e.g.
Biobanks) of (medical) images and genetic data. 2.