Abstract
Recently self-supervised representation learning has drawn considerable attention from the scene text recogni-tion community. Different from previous studies using con-trastive learning, we tackle the issue from an alternative perspective, i.e., by formulating the representation learning scheme in a generative manner. Typically, the neighbor-ing image patches among one text line tend to have simi-lar styles, including the strokes, textures, colors, etc. Moti-vated by this common sense, we augment one image patch and use its neighboring patch as guidance to recover itself.
Specifically, we propose a Similarity-Aware Normalization (SimAN) module to identify the different patterns and align the corresponding styles from the guiding patch.
In this way, the network gains representation capability for distin-guishing complex patterns such as messy strokes and clut-tered backgrounds. Experiments show that the proposed
SimAN significantly improves the representation quality and achieves promising performance. Moreover, we surpris-ingly find that our self-supervised generative network has impressive potential for data synthesis, text image editing, and font interpolation, which suggests that the proposed
SimAN has a wide range of practical applications. 1.

Introduction
The computer vision community has witnessed the great success of supervised learning over the last decade. How-ever, the supervised learning methods heavily rely on labor-intensive and expensive annotations. Otherwise, they might suffer from generalization problems. Recently self-supervised representation learning has become a promising alternative and is thus attracting growing interest [24,34]. It has been shown that the self-supervised representations can benefit subsequent supervised tasks [6–10, 18].
Despite the fast-paced improvements of representation learning on single object recognition/classification tasks,
*Corresponding author.
Figure 1. Scene text representation learning in (a) the contrastive and (b) the generative manner (ours). We estimate the similarity of the content representations between the augmented patch and its neighboring patch, and align the corresponding styles to recon-struct the augmented patch. Only high-quality representations are distinguishable so that a precise reconstruction can be achieved. the field of scene text recognition is meeting extra chal-lenges. For instance, multiple characters in one image can-not be regarded as one entity [38, 61]. Directly adopt-ing current non-sequential contrastive learning schemes for sequence-like characters [44] usually leads to performance deterioration [1]. This suggests the gap between the non-sequential and sequential schemes. Therefore, it is desir-able to design a specific representation learning scheme for scene text recognition.
As a scene text image containing dense characters is significantly different from a natural image, SeqCLR [1]
divided one text line into several instances using certain strategies and performed contrastive learning on these in-stances. The learning scheme is shown in Figure 1 (a). The
SeqCLR designed for sequence-to-sequence visual recogni-tion outperformed the representative non-sequential method
SimCLR [7]. Although it brought a huge leap forward, the representation learning of scene text remains a challenging open research problem, where the nature of scene text has not been fully explored.
Thus, we review several properties of scene text that differ from those of general objects (e.g., face, car, and dog). For instance, one feature that highlights scene text is its constant stroke width [13]. Simultaneously, it is ob-served that color similarity typically occurs across one text line. These specialties provided cues for hand-crafted fea-tures, such as connected components [41], stroke width transform [13, 58], and maximally stable extremal region trees [21], which were popular before the dramatic success of deep neural networks.
In this paper, we explore the representation learning from a new perspective by considering the above unique proper-ties of scene text. The learning scheme is shown in Figure 1 (b). Specifically, we randomly crop two neighboring im-age patches from one text line. One patch is augmented and the other one guides the recovery of the augmented one.
As one text line usually exhibits consistent styles, includ-ing the strokes, textures, colors, etc., the original styles of the augmented patch can be found on the neighboring patch according to similar content patterns. Thus, we propose a
Similarity-Aware Normalization (SimAN) module to align corresponding styles from the neighboring patch by esti-mating the similarity of the content representations between these two patches. This means that the representations are required to be sufficiently distinguishable so that different patterns can be identified and the corresponding styles can be correctly aligned. Only in this way, the network can produce a precise recovered image patch. Therefore, the proposed SimAN enables high-quality self-supervised rep-resentation learning in a generative way. Moreover, we find that our self-supervised network has competitive per-formance with state-of-the-art scene text synthesis meth-ods [17, 23, 35, 59]. It is also promising to apply SimAN to other visual effect tasks, such as text image editing and font interpolation.
To summarize, our contributions are as follows:
• We propose a generative (opposite of contrastive [34]) representation learning scheme by utilizing the unique properties of scene text, which might inspire rethink-ing the learning of better representations for sequential data like text images. To the best of our knowledge, this is the first attempt for scene text recognition.
• We propose a SimAN module, which estimates the similarity of the representations between the aug-mented image patch and its neighboring patch to align corresponding styles. Only if the representations are sufficiently distinguishable, different patterns can be identified and be aligned with correct styles. Other-wise, the network might result in a wrong recovered image, e.g., in different colors.
• The proposed SimAN achieves promising representa-tion performance. Moreover, the self-supervised net-work shows impressive capabilities to synthesize data, edit text images and interpolate fonts, suggesting the broad practical applications of the proposed approach. 2.