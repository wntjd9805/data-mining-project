Abstract
Modern camera localization methods that use image re-trieval, feature matching, and 3D structure-based pose esti-mation require long-term storage of numerous scene images or a vast amount of image features. This can make them un-suitable for resource constrained VR/AR devices and also raises serious privacy concerns. We present a new learned camera localization technique that eliminates the need to store features or a detailed 3D point cloud. Our key idea is to implicitly encode the appearance of a sparse yet salient set of 3D scene points into a convolutional neural network (CNN) that can detect these scene points in query images whenever they are visible. We refer to these points as scene landmarks. We also show that a CNN can be trained to regress bearing vectors for such landmarks even when they are not within the camera’s field-of-view. We demonstrate that the predicted landmarks yield accurate pose estimates and that our method outperforms DSAC*, the state-of-the-art in learned localization. Furthermore, extending HLoc (an accurate method) by combining its correspondences with our predictions boosts its accuracy even further. 1.

Introduction
Camera localization is the task of estimating the 3D po-sition and 3D orientation of a camera from a query image with respect to a pre-built scene map. This task is a fun-damental building block to enable VR/AR systems that al-low users to persistently interact with the surrounding 3D scene. These scenes are often private spaces; e.g., homes, where existing localization methods that use retrieval and feature matching [1,4,16,38,57,62] are not suitable because stored images or features can be inverted to reveal sensitive scene content (raising serious privacy concerns [54,72,73]).
Furthermore, existing localization methods usually require long term storage of many images or a vast amount of fea-tures and 3D points. In lifelong localization settings, new images and features will be continuously added, causing the database to grow over time. The ensuing memory foot-print may also exceed the limits of on-device localization for VR/AR systems. Map pruning can help [12, 39, 71], but it’s efficacy for lifelong localization is unproven.
Learned localization approaches such as absolute pose regression [29,30,84] and scene coordinate regression [7,8, 10, 69] address both aforementioned issues. These methods implicitly encode scene information in the learned param-eters of a convolutional neural network (CNN), rather than explicitly storing images or features. Thus, they preserve privacy by design. However, their performance is not yet on par with the top performing methods that use retrieval, fea-ture matching, and structure-based pose estimation [65,67].
In this paper, we present a new learned method for cam-era localization that (1) preserves privacy, (2) requires low storage, and (3) outperforms the state-of-the-art storage-free pose regression methods. Our idea is inspired by the
recent success of landmark detection in human pose estima-tion [46,86] and keypoint recognition for objects [34,51,77] and faces [18]. Instead of human body joints or object key-points, we recognize salient, scene-specific 3D points called scene landmarks from a query image as shown in Figure 1.
This landmark recognition approach is privacy preserving and requires low data storage as no visual features need to be retained. The landmark recognition establishes 2D-3D correspondences that can be used to robustly estimate the camera pose. We implement the proposed idea by train-ing a scene-specific CNN architecture that detects the land-marks, i.e., regresses the 2D coordinates of the landmarks in the input image. We show that running structure from motion (SfM) on the mapping images is sufficient to find a set of salient landmarks and automatically produce the data needed to train the architecture.
Unlike human pose estimation where most landmarks are typically visible (up to occlusion), most of the scene landmarks are not expected to be simultaneously visible, due to limited camera field-of-view and because landmarks in different parts of the scene cannot be observed simultane-ously. We address this challenge by proposing a new Neu-ral Bearing Estimator (NBE) that can directly regress the 3D bearing vectors for the scene landmarks in the camera coordinate frame. NBE learns a global scene representa-tion, similar to PoseNet [30], while learning to predict the direction vectors of scene landmarks even when they are invisible. We show that NBE is highly effective and outper-forms PoseNet by a significant margin. Our full approach combines scene landmark detection and the NBE method.
Although our method learns to predict 2D-3D corre-spondences similar to existing scene coordinate regression (SCR) approaches [10, 69], there are several crucial differ-ences. First, SCR methods predict dense 3D world coordi-nates for scene points observed at every pixel. In contrast, we assume the 3D coordinates for a few salient scene land-marks are given, and we only infer their 2D positions in the image. Thus, our matches are extremely sparse (between 10–40) compared to SCR approaches that use thousands.
Our method has comparable performance to HLoc [59, 60] and DSAC* [10] on 7-scenes [69] but outperforms
DSAC* on our new INDOOR-6 dataset that contains chang-ing scenes, day/night images, and strong illumination varia-tions. Although we have motivated our method based on the need to avoid storage, we show that it is also useful when storage is not a concern. The 2D–3D correspondences re-covered from our method appear to complement those re-covered by other methods; e.g., by combining our method with HLoc, we boost its accuracy even further.
Contributions. This paper presents the following techni-cal contributions: (1) a new formulation for heatmap-based landmark localization and bearing angle estimation that is privacy preserving and can be used to localize a camera in a pre-built map; (2) a new dataset that can be used to ef-fectively evaluate camera localization performance in chal-lenging scenarios; and (3) superior results with low storage compared to existing storage-free localization methods. 2.