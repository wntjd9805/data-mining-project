Abstract
As a common problem in the visual world, contex-tual bias means the recognition may depend on the co-occurrence context rather than the objects themselves, which is even more severe in multi-label tasks due to mul-tiple targets and the absence of location. Although some studies have focused on tackling the problem, removing the negative effect of context is still challenging because it is difﬁcult to obtain the representation of contextual bias. In this paper, we propose a simple but effective framework em-ploying causal inference to mitigate contextual bias. We
ﬁrst present a Structural Causal Model (SCM) clarifying the causal relation among object representations, context, and predictions. Then, we develop a novel Causal Context
Debiasing (CCD) Module to pursue the direct effect of an instance. Speciﬁcally, we adopt causal intervention to elim-inate the effect of confounder and counterfactual reasoning to obtain a Total Direct Effect (TDE) free from the contex-tual bias. Note that our CCD framework is orthogonal to existing statistical models and thus can be migrated to any other backbones. Extensive experiments on several multi-label classiﬁcation datasets demonstrate the superiority of our model over other state-of-the-art baselines. 1.

Introduction
Context is a very common element in the visual world.
For a single instance in an image, its context consists of other co-occurrence instances together with the back-ground.
In multi-target tasks like multi-label classiﬁca-tion and detection, context (or, instance relation) modeling seems to have considerable potential to improve the perfor-mance. For example, the cutlery in the soup is probably
In fact, from recur-a spoon rather than a fork or knife. rent neural networks [40, 46], to graph convolutional net-works [5,10,47], until the popular transformer-based frame-works [19, 53], recent years have witnessed numerous at-tempts to model the label relations in multi-instance images.
Despite the remarkable progress these models have made, they may overlook a basic question: is modeling con-text always beneﬁcial in visual recognition? As is illustrated in Fig. 1, we uncovered an ever-overlooked phenomenon in multi-label classiﬁcation: context may mislead the clas-siﬁer, either giving the nonexistent object a high score in the scene where it usually arises, or ignoring the object ap-pearing in a rare background. The occurrence can be partly blamed on the biased data or weak backbones; but too much attention on the label relationships will ultimately aggravate the bias. Although some works [30,47] have questioned the necessity of label modeling, they do not address the prob-lem of contextual bias and lack the fundamental theory.
Contextual bias is not a fresh topic in the academic world. In fact, it is widely reported in many ﬁelds [11, 32].
Recent works [13] also give insight into the reason for contextual bias in computer vision: neural networks are statistics-based and ”lazy”. When the networks ﬁnd the context is enough to recognize most of the objects, they of-ten do not focus on the representations of instances. When the training data is limited (e.g. few-shot learning) or deﬁ-cient (e.g. long-tail distribution), the problem is more se-vere and obvious.
In fact, there have been some works
[36, 49, 52] attending to the bias in these situation, never-theless, they do not work on balanced datasets like MS-COCO [23] or other common tasks.
Paradoxically, context is not always bad. On one hand, bias towards context would mislead the prediction in some cases. On the other hand, it is somewhat reasonable. The appearance of contextual bias means the network indeed captured the inter-dependencies of classes: fork or knife is indeed more rational to appear on the dining table as op-posed to on the street or grasslands, and rough intervention will damage the learning of the feature [18]. Actually, in the tasks like scene graph generation [45] or human object in-teractions [45] where the datasets are seriously biased, con-text priori has proved to be beneﬁcial to the results. How-ever, it does not mean contextual bias can be neglected. A more robust and sensible prediction should come from the object itself rather than the context, and classiﬁcation by context is more likely the expediency. For tasks like multi-label classiﬁcation, whose datasets are balanced and large, contextual bias will damage the ﬁnal prediction.
Mitigating the contextual bias in common multi-label
Figure 1. Examples that contextual information can do evil. We show two labels in MS-COCO: apple and knife. (a)Label dependencies
P (X|Y ) of the two categories, which are computed through dividing the number of co-occurrence by the number of apple/knife. (b)
Examples from the ResNet101 baseline. The left column depicts the objects in their common context, which is in large number and correctly predicted. And the right depicts the opposite: the objects in the unusual context or the absence of objects in their common context, which is in few number and incorrectly predicted. tasks is challenging.
In a relatively unbiased dataset, the contextual bias is not very obvious, hence it is difﬁcult to obtain the representation of the context. Thanks to the the-ory of causality, we can revisit the context in a causal view: the image-speciﬁc contextual message is indeed a mediator preventing results from being generated directly by the rep-resentations of instances. Consequently, the ﬁnal prediction is a mixed effect of the object and the context. Besides, the prior context knowledge (e.g., biased datasets or pretrain model) acts as a confounder giving rise to spurious corre-lation among labels. As is illustrated in the last column of
Fig.1(b), even though the scene of dining table is not the direct cause of apple or knife, the biased prior knowledge still fools the classiﬁer to learn a correction between them.
In this paper, we build a Structural Causal Model [29] in Section 3 clarifying the causalities among elements men-tioned above. With the assistance of causal inference, we propose a novel debiasing paradigm: Causal Context Debi-asing (CCD) Module, to conquer the effect of contextual bias. We ﬁrst implement the backdoor adjustment [28], which is essentially a causal intervention turning off the confounding switch and treating every contextual content equally in the prediction. Then, by counterfactual infer-ence, we elegantly eliminate the effect of contextual bias and obtain the direct causal effect from the objects them-selves, without hurting the feature representation learning.
It is worth noting that our method is model-agnostic, hence, our approach can be used on a variety of backbones and achieve performance improvements. Moreover, different from many recent classiﬁcation models which introduce complicated architecture (e.g. GCN and transformer) or ad-ditional external information (e.g., word embeddings), the parameter increasement from our method is very limited.
The main contributions of our paper are summarized as:
• We establish a Structural Causal Model (SCM) to un-cover the causal relevance among contextual priori, object feature, contextual bias, and ﬁnal prediction in multi-target visual tasks. We ﬁnd the image-speciﬁc context is indeed a mediator and contextual priori is a confounder, which sheds some light on how prediction is inﬂuenced by the context.
• We propose a simple, effective and model-agnostic framework for contextual debiasing based on causal inference. By the combination of backdoor interven-tion and counterfactual reasoning, we remove the ob-stacle of the confounder as well as contextual bias, ob-taining the direct effect caused by target instances.
• We conduct extensive experiments in multi-label clas-siﬁcation to verify the effectiveness of our methods.
Results on three widely-used datasets MS-COCO [23],
PASCAL-VOC [12] and NUS-WIDE [6] show that our approach can signiﬁcantly improve both CNN and transformer backbones, outperforming the state-of-the-art on these datasets. 2.