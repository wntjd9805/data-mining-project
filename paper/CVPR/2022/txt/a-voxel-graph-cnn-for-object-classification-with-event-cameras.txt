Abstract
Event cameras attract researchers’ attention due to their low power consumption, high dynamic range, and extremely high temporal resolution. Learning models on event-based object classification have recently achieved massive success by accumulating sparse events into dense frames to apply traditional 2D learning methods. Yet, these approaches ne-cessitate heavy-weight models and are with high computa-tional complexity due to the redundant information intro-duced by the sparse-to-dense conversion, limiting the po-tential of event cameras on real-life applications. This study aims to address the core problem of balancing accuracy and model complexity for event-based classification mod-els. To this end, we introduce a novel graph representa-tion for event data to exploit their sparsity better and cus-tomize a lightweight voxel graph convolutional neural net-work (EV-VGCNN) for event-based classification. Specif-ically, (1) using voxel-wise vertices rather than previous point-wise inputs to explicitly exploit regional 2D seman-tics of event streams while keeping the sparsity; (2) propos-ing a multi-scale feature relational layer (MFRL) to extract spatial and motion cues from each vertex discriminatively concerning its distances to neighbors. Comprehensive ex-periments show that our model can advance state-of-the-art classification accuracy with extremely low model complex-ity (merely 0.84M parameters). 1.

Introduction
Each pixel of event cameras is independent and only re-port lightness changes at the correspondent location (Fig. 1). This novel working principle enables their output to be sparse and non-redundant [28, 37]. Consequently, event
*: Corresponding author
This work was supported by the National Natural Science Founda-tion of China (61873220, 92167102, 62102083, 62173286, 61875068,
Jiangsu Province 62177018), (BK20210222), the Research Grants Council of Hong Kong (CityU 11213420), and the Science and Technology Development Fund, Macau
SAR (0022/2019/AKP). the Natural Science Foundation of
Figure 1. Left: A sketch of the working principle of event cam-eras (the detailed working principle is introduced in supplemen-tary material). Events are produced asynchronously according to the lightness (ln L) changes. Red and blue arrows represent pos-itive and negative events, respectively. Right: The RGB image captured from the traditional RGB camera (top) and event signals in the original format produced from an event camera (bottom). cameras hold advantages of low power consumption, high response speed, and high dynamic range compared to tra-ditional cameras [17]. How to tailor models for event data with the particular format to perform core vision tasks, such as object classification, has been a trending topic. Moti-vated by the huge success of learning-based methods on vision tasks, developing data-driven approaches for event data becomes a leading choice. For instance, many stud-ies [5, 12, 19, 50, 51] resort to 2D convolutional neural net-works (CNNs) by converting sparse events to dense frames.
These works achieve advanced performance utilizing well-pretrained 2D CNNs. Yet, the constructed dense representa-tion and large size models sacrifice the sparsity of event data (Fig. 2 (a)) while limiting the potential of event cameras on mobile or wearable applications.
To exploit the sparsity of event data and build low-complexity models, recent researchers migrate learning models initially designed for point clouds to event data, such as the works [43, 47] migrate models from pointnet-like architecture [38] and the approaches [3, 31] utilize graph neural networks. Although these approaches advance in exploiting the sparsity advantage of event data, a funda-mental question has never been investigated: Is point-wise
input (taking event points as processing units) proper for event-based vision tasks?
Intuitively, each point in 3D point clouds can be used as a key point to building the external structure of an ob-ject [8]. Therefore, these points do well in describing the geometry, which is the key for classifying 3D objects. In-stead, event data are more like 2D videos recorded asyn-chronously. Event-based classification models require the ability to accurately extract 2D semantics from the event data rather than their “geometry” (Fig. 1), which usually contains motion cues or motion trajectories. Thus, we be-lieve that using raw events as input is not suitable, as it is difficult for sparse event points to provide decisive features (e.g., local 2D semantics) for event-based models.
To overcome the lacking of characterizing local 2D se-mantics in the popular point-based solutions, this study pro-poses a novel voxel-wise representation for event data. In-spired by the traditional image domain: it is challenging to extract decisive features from images using discrete or dis-continuous pixels (like sparse point-wise input). Thus, we propose a representation to encode locally coherent 2D se-mantics by describing the regional events contained in each voxel.
In specific, we build a graph by voxelizing event points, selecting representative voxels as vertices (Fig. 2 (c)), and connecting them according to their spatio-temporal relationships for further processing. Each voxel in the graph can be analogous to frame patches of still images that con-tain essential cues such as local textures and contours [14], which can help the network recognize 2D scenes effectively.
Besides the proposed representation, a lightweight graph-based learning architecture (EV-VGCNN) is intro-duced. The critical problem for designing an event-based graph model is how to learn the embeddings for the edges and vertices’ features. First, we learn a scoring matrix for each vertex according to spatio-temporal geometry and uti-lize the learned matrix to achieve feature aggregation across its neighbors attentively. Moreover, for a vertex in the event-based graph, its adjacent neighbors usually convey lo-cal spatial messages, while distant neighbors are more likely to carry motion cues or global changes.
Inspired by this variation, we design a multi-scale feature relational layer (MFRL) to extract semantic and motion cues from each ver-tex discriminatively. Specifically, two learnable blocks in
MFRL are applied to adjacent and distant neighbors respec-tively, and the obtained features are aggregated as the joint representation of a vertex. Finally, we cascade multiple
MFRL modules with graph pooling operations and a clas-sifier as the EV-VGCNN to perform end-to-end object clas-sification. The proposed model achieves SOTA accuracy while holding surprisingly low model complexity.
The main contributions of this paper are summarized as follows: (1) We introduce a novel method to construct event-based graph representation with correspondence to
Figure 2. Visual comparison of three types of event-based repre-sentation: (a) the frame-based representation by integrating events into dense frames; (b) the point-based representation generated by sampling a subset of event signals; (c) representative event voxels selected as vertices in the proposed graph. the properties of event data, which can effectively utilize informative features from voxelized event streams while maintaining the sparse and non-redundant advantage of event data. (2) We introduce the MFRL module composed of several SFRLs to discriminatively learn spatial semantics and motion cues from the event-based graph according to spatio-temporal relations between vertices and their neigh-bors. (3) Extensive experiments show that our model enjoys noticeable accuracy gains with extremely low model com-plexity (merely 0.84M parameters). 2.