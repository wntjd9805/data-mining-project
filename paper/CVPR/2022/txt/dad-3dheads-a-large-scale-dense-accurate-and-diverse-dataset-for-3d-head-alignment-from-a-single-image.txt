Abstract 1.

Introduction
We present DAD-3DHeads, a dense and diverse large-scale dataset, and a robust model for 3D Dense Head Align-ment in-the-wild. It contains annotations of over 3.5K land-marks that accurately represent 3D head shape compared to the ground-truth scans. The data-driven model, DAD-3DNet, trained on our dataset, learns shape, expression, and pose parameters, and performs 3D reconstruction of a FLAME mesh. The model also incorporates a landmark prediction branch to take advantage of rich supervision and co-training of multiple related tasks. Experimentally, DAD-3DNet outperforms or is comparable to the state-of-the-art models in (i) 3D Head Pose Estimation on AFLW2000-3D and BIWI, (ii) 3D Face Shape Reconstruction on NoW and
Feng, and (iii) 3D Dense Head Alignment and 3D Land-marks Estimation on DAD-3DHeads dataset. Finally, diver-sity of DAD-3DHeads in camera angles, facial expressions, and occlusions enables a benchmark to study in-the-wild generalization and robustness to distribution shifts. The dataset webpage is https://p.farm/research/dad-3dheads.
Tremendous progress in 3D face analysis has been made since the first 3D morphable model (3DMM) [4] from an image had been proposed [22]. The use cases for precise 3D face models are abundant: accurate face recognition and face detection [16], realistic 3D avatars and animation for
VR and games [37], face re-enactment and synthesis for dubbing [59], virtual mirrors and try-on, statistical shape models for medical tasks such as segmentation and analysis of variations in anatomical structures [73].
These applications require not only accurate 3D face ge-ometry but also (1) handling the diversity, e.g., ethnic, age, gender subgroups, and (2) generalizing to in-the-wild de-ployment conditions, i.e., beyond controlled capture and be-yond the data they are trained on. The largest face models up-to-date [8, 45] have focused on the (1) aspect by collect-ing diverse 3D face and head scans, and building 3DMMs models for different age, gender and ethnicity. In-the-wild generalization has been identified as a pressing challenge of the next generation 3D face models [22]. This (2) aspect of in-the-wild generalization is the focus of our study.
*These authors contributed equally to this work.
The progress that we have witnessed with deep learn-ing has impacted closely related facial analysis tasks such as Landmark Localisation [14, 20, 46, 51, 57], Facial Align-ment in 2D and 3D [2, 10–12, 17, 31, 32, 48, 63, 70], and
Face Detection [2, 17, 20, 25, 46, 70]. This has been driven by the community effort towards collecting and annotating large image datasets captured in unconstrained conditions, building enhanced models that can take advantage of such large datasets, and most importantly openness, i.e., making the models and datasets publicly available for research use.
Nevertheless, 3D face or head alignment from a single image in the wild remains an open challenge. The diffi-culty comes from (1) lack of 2D-3D ground-truth data and, as a result, (2) ambiguity of the task and reliance on 3D shape priors. Many methods have been developed to fill the gap of missing 2D-3D annotations (1), primarily using 2D landmarks datasets for fitting, or exploring extra knowl-edge such as identity invariance [53], or co-training with related face detection [20], [16] tasks to drive the recovery of 3D face geometry. Up until now, evaluation of the effi-ciency of these approaches has been problematic due to the lack of ground-truth data. Regarding (2), the state-of-the-art 3D face reconstruction methodologies such as non-linear 3DMMs and deep learning models [5, 6, 8, 38, 45] are based on learning a statistical 3D facial model and fitting it to the image as a shape (or shape and texture) prior. This direc-tion has a long history tracing back to the seminal work of
Blanz and Vetter [4]. It relies on a large and diverse dataset of 3D/4D scans to build the statistical 3D face model that can be decomposed into facial shape (identity and expres-sion), and the camera parameters. This comes at the cost of laborious data collection with expensive 3D acquisition devices, and the fact that 3D acquisition devices cannot op-erate in arbitrary conditions. Hence, the current 3D facial databases have limited data sample size and have been cap-tured not-quite-in-the-wild [53].
In this work, we show that without expensive devices, like scanners, that are difficult to deploy in the wild, we can collect accurate annotations of 3D landmarks directly from images, which is labor-efficient and effective to push the state-of-the-art results for 3D head recovery from im-ages.
Our contributions are as follows:
• A new Dense, Accurate and Diverse dataset for 3D
Dense Head Alignment in-the-wild, DAD-3DHeads.
It has over 3.5K verified accurate landmarks, the dens-est annotations for 3D dense head alignment in-the-wild currently available. DAD-3DHeads contains a va-riety of extreme poses, facial expressions, challenging illuminations, and severe occlusions cases. Accuracy and consistency of the annotations are compared to the ground truth 4D scans and head pose labels.
• A novel way to address the problems of shape re-construction and pose estimation simultaneously dur-ing training via optimizing two loss components: (i)
Shape+Expression Loss and (ii) Reprojection Loss. (i) is based on the normalized 3D vertices that en-ables disentangling the shape and expression informa-tion from the pose; (ii) is based on the full head dense 2D landmarks and assesses the pose accuracy. That makes the rich annotations fully utilized, which could not have been done previously due to the lack of GT annotations. Extensive ablation studies show the im-portance of both loss components.
• DAD-3DNet model that maps an input image to 3D mesh representation consistent with the FLAME topol-ogy. The model is trained end-to-end by regressing the 3DMM parameters and recovering the 3D head geom-etry with differential FLAME decoder. The proposed approach learns the head shape, pose, and expres-sion simultaneously. DAD-3DNet outperforms state-of-the-art on a range of tasks, suggesting that dense supervision as provided in our dataset, enables a holis-tic framework for 3D Head Analysis from images.
• A novel benchmark with the evaluation protocol for quantitative assessment of 3D dense head fitting, i.e. 3D Head Estimation from dense annotations. Our eval-uation protocol introduces two novel metrics: Repro-jection NME computing the NME of the reprojected 3D vertices onto the image plane, and Zn Accuracy evaluating the ordinal distance of the Z-coordinate and accuracy of the 3D fitting. 2.