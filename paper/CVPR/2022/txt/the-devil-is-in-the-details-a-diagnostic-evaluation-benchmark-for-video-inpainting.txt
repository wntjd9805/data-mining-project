Abstract
Quantitative evaluation has increased dramatically among recent video inpainting work, but the video and mask content used to gauge performance has received relatively little attention. Although attributes such as camera and background scene motion inherently change the difﬁculty of the task and affect methods differently, existing evalu-ation schemes fail to control for them, thereby providing minimal insight into inpainting failure modes. To address this gap, we propose the Diagnostic Evaluation of Video
Inpainting on Landscapes (DEVIL) benchmark, which con-sists of two contributions: (i) a novel dataset of videos and masks labeled according to several key inpainting failure modes, and (ii) an evaluation scheme that samples slices of the dataset characterized by a ﬁxed content attribute, and scores performance on each slice according to recon-struction, realism, and temporal consistency quality. By re-vealing systematic changes in performance induced by par-ticular characteristics of the input content, our challeng-ing benchmark enables more insightful analysis into video inpainting methods and serves as an invaluable diagnos-tic tool for the ﬁeld. Our code and data are available at github.com/MichiganCOG/devil. 1.

Introduction
Video inpainting, i.e., the task of ﬁlling in missing pix-els in a video with plausible values, pushes the boundaries of modern video editing techniques and enables remarkable applications for ﬁlm and social media such as watermark and foreground object removal [1, 29]. Compared to image inpainting, video inpainting is more challenging due to the additional temporal dimension, which not only increases the complexity of the solution space, but also places additional constraints on what constitutes a high-quality prediction— in particular, predictions must be coherent in terms of both spatial structure and motion. Despite the difﬁculty of the task, modern results have become quite compelling thanks to the increasing amount of attention that the problem has received as of late [4, 9, 16, 20, 25, 28, 30, 31].
Quantitative evaluation has increased dramatically among recent video inpainting work; however, existing evaluation schemes underemphasize the importance of the contents of the videos and masks used to gauge perfor-mance. Typically, video inpainting is evaluated as a recon-struction problem: performance is quantiﬁed by masking out arbitrary regions from the video (i.e., “corrupting” it) and scoring the model’s ability to recover the masked-out values [4, 18, 28]. However, the difﬁculty of reconstruction depends on the mask’s shape and motion, as well as the con-tent present in the “uncorrupted” video. For example, given a static mask, it is harder to inpaint a video captured by a
ﬁxed camera than one captured by a moving camera. In the former case, the region beneath the mask is never visible, so the model effectively needs to “hallucinate” its appearance; in the latter case, the model could transfer appearance infor-mation from other frames, a strategy that lies at the heart of many video inpainting approaches [14, 18, 20, 26].
The difﬁculty of video inpainting is inherently tied to the content of the videos and masks being inpainted; with this principle in mind, we push for more emphasis on content-informed diagnostic evaluation, which can help identify the strengths and weaknesses of modern inpainting methods and improve ablative analysis. To date, the videos used for evaluation have been underappreciated in this regard, hav-ing been sourced from datasets for other tasks (e.g., facial analysis [22, 23] and object localization [21, 27]) rather than selected to represent important inpainting scenarios. In par-ticular, they contain biases that are essential for the original task, but hinder ﬁne-grained analysis for video inpainting.
For example, object localization videos consistently include prominent, moving foreground objects; as a result, stan-dard inpainting evaluation schemes inevitably underrepre-sent performance on foreground-free videos. Furthermore, other types of motion, e.g., camera and background scene motion, noticeably impact video inpainting performance, but are not controlled for in standard datasets.
In this work, we propose the Diagnostic Evaluation of
It
Video Inpainting on Landscapes (DEVIL) benchmark.
Figure 1: A visual overview of our DEVIL dataset. (a) The content attributes that characterize our dataset and are used to create dataset slices for evaluation (i.e., sets of video-mask pairs with a ﬁxed attribute). We label low/high background scene motion or camera motion for videos exhibiting these attribute settings beyond a certain threshold (Section 4.2). For occlusion masks, we construct sampling parameters that capture the desired attribute settings and use them to render masks (Section 4.3). (b) Videos, masks, and annotations from our dataset. A given video or mask may have multiple attribute labels or none; labels for the same attribute are mutually exclusive (e.g., a mask cannot have both low and high FG displacement). is composed of two parts—the DEVIL dataset and the
DEVIL evaluation scheme—which combine to enable a
ﬁner-grained analysis than has been possible in prior work.
Such granularity is achieved through content attributes, i.e., properties of source videos or masks that characterize key failure modes by affecting how easily video inpainting mod-els can borrow appearance information from nearby frames.
Speciﬁcally, the DEVIL dataset contains source videos la-beled with low/high camera and background scene motion attributes, and occlusion masks labeled with low/high fore-ground displacement, pose motion, and size attributes (Fig-ure 1). Meanwhile, the DEVIL evaluation scheme con-structs several slices of the DEVIL dataset—sets of video-mask pairs in which exactly one content attribute is kept
ﬁxed—and summarizes inpainting quality through metrics that capture reconstruction performance, realism, and tem-poral consistency (Section 5.2). By controlling for con-tent attributes and summarizing inpainting quality per at-tribute across several metrics, our DEVIL benchmark pro-vides valuable insight into the failure modes of a given in-painting model and how mistakes manifest in the output.
We use our novel benchmark to analyze the strengths and weaknesses of seven state-of-the-art video inpainting methods. By quantifying their inpainting quality on ten
DEVIL dataset slices under ﬁve evaluation metrics, we pro-vide the most comprehensive and ﬁne-grained evaluation of modern video inpainting methods to our knowledge. Our head-to-head, multi-faceted comparisons allow us to draw several important conclusions. For example, we show that video inpainting methods in which time and optical ﬂow are carefully modeled consistently achieve the best perfor-mance across several types of input data. We also show that the relative rankings between methods are highly sensitive to metrics as well as source video and mask content, high-lighting the need for comprehensive evaluation. Finally, we show that controlling for source video and mask attributes reveals insightful failure modes which can be traced back to the design of the inpainting method in question.
Our comprehensive diagnostic benchmark enables in-sightful analysis and serves as an invaluable tool for video inpainting research. To summarize, we provide the follow-ing contributions:
• We present the ﬁrst diagnostic dataset speciﬁcally de-signed for video inpainting to our knowledge, which in-cludes annotations for content-based attributes that rep-resent numerous inpainting failure modes;
• We introduce a novel and comprehensive evaluation scheme that spans ten dataset slices and ﬁve evaluation metrics of video inpainting quality;
• We analyze seven state-of-the-art algorithms on our benchmark, providing the most comprehensive quantita-tive evaluation of video inpainting methods to date; and
• We identify systematic errors among video inpainting methods and highlight directions for future work.
Our benchmark is available at https://github.com/
MichiganCOG/devil. 2.