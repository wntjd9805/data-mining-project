Abstract
Generative transformers have experienced rapid popu-larity growth in the computer vision community in synthe-sizing high-fidelity and high-resolution images. The best generative transformer models so far, however, still treat an image naively as a sequence of tokens, and decode an image sequentially following the raster scan ordering (i.e. line-by-line). We find this strategy neither optimal nor efficient.
This paper proposes a novel image synthesis paradigm us-ing a bidirectional transformer decoder, which we term
MaskGIT. During training, MaskGIT learns to predict ran-domly masked tokens by attending to tokens in all direc-tions. At inference time, the model begins with generating all tokens of an image simultaneously, and then refines the image iteratively conditioned on the previous generation.
Our experiments demonstrate that MaskGIT significantly outperforms the state-of-the-art transformer model on the
ImageNet dataset, and accelerates autoregressive decoding by up to 48x. Besides, we illustrate that MaskGIT can be easily extended to various image editing tasks, such as in-painting, extrapolation, and image manipulation. Project page: masked-generative-image-transformer.github.io. 1.

Introduction
Deep image synthesis as a field has seen a lot of progress in recent years. Currently holding state-of-the-art results are
˚ Currently affiliated with Microsoft Azure AI.
Generative Adversarial Networks (GANs), which are capa-ble of synthesizing high-fidelity images at blazing speeds.
They suffer from, however, well known issues include train-ing instability and mode collapse, which lead to a lack of sample diversity. Addressing these issues still remains open research problems.
Inspired by the success of Transformer [46] and GPT [5] in NLP, generative transformer models have received grow-ing interests in image synthesis [7, 15, 37]. Generally, these approaches aim at modeling an image like a sequence and leveraging the existing autoregressive models to generate image. Images are generated in two stages; the first stage is to quantize an image to a sequence of discrete tokens (or visual words). In the second stage, an autoregressive model (e.g., transformer) is learned to generate image tokens se-quentially based on the previously generated result (i.e. au-toregressive decoding). Unlike the subtle min-max opti-mization used in GANs, these models are learned by max-imum likelihood estimation. Because of the design differ-ences, existing works have demonstrated their advantages over GANs in offering stabilized training and improved dis-tribution coverage or diversity.
Existing works on generative transformers mostly focus on the first stage, i.e. how to quantize images such that in-formation loss is minimized, and share the same second stage borrowed from NLP. Consequently, even the state-of-the-art generative transformers [15, 35] still treat an im-age naively as a sequence, where an image is flattened into
Figure 2. Comparison between sequential decoding and MaskGIT’s scheduled parallel decoding. Rows 1 and 3 are the input latent masks at each iteration, and rows 2 and 4 are samples generated by each model at that iteration. Our decoding starts with all unknown codes (marked in lighter gray), and gradually fills up the latent representation with more and more scattered predictions in parallel (marked in darker gray), where the number of predicted tokens increases sharply over iterations. MaskGIT finishes its decoding in 8 iterations compared to the 256 rounds the sequential method takes. a 1D sequence of tokens following a raster scan order-ing, i.e. from left to right line-by-line (cf . Figure 2). We find this representation neither optimal nor efficient for im-ages. Unlike text, images are not sequential. Imagine how an artwork is created. A painter starts with a sketch and then progressively refines it by filling or tweaking the de-tails, which is in clear contrast to the line-by-line printing used in previous work [7, 15]. Additionally, treating image as a flat sequence means that the autoregressive sequence length grows quadratically, easily forming an extremely long sequence–longer than any natural language sentence.
This poses challenges for not only modeling long-term cor-relation but also renders the decoding intractable. For exam-ple, it takes a considerable 30 seconds to generate a single image on a GPU autoregressively with 32x32 tokens.
This paper introduces a new bidirectional transformer for image synthesis called Masked Generative Image Trans-former (MaskGIT). During training, MaskGIT is trained on a similar proxy task to the mask prediction in BERT [11]. At inference time, MaskGIT adopts a novel non-autoregressive decoding method to synthesize an image in constant number of steps. Specifically, at each iteration, the model predicts all tokens simultaneously in parallel but only keeps the most confident ones. The remaining tokens are masked out and will be re-predicted in the next iteration. The mask ratio is decreased until all tokens are generated with a few itera-tions of refinement. As illustrated in Figure 2, MaskGIT’s decoding is an order-of-magnitude faster than the autoregre-sive decoding as it only takes 8 steps, instead of 256 steps, to generate an image and the predictions within each step are parallelizable. Moreover, instead of conditioning only on previous tokens in the order of raster scan, bidirectional self-attention allows the model to generate new tokens from generated tokens in all directions. We find that the mask scheduling (i.e. fraction of the image masked each iteration) significantly affects generation quality. We propose to use the cosine schedule and substantiate its efficacy in the abla-tion study.
On the ImageNet benchmark, we empirically demon-strate that MaskGIT is both significantly faster (by up to 48x) and capable of generating higher quality samples than the state-of-the-art autoregressive transformer, i.e. VQ-GAN, on class-conditional generation with 256ˆ256 and 512ˆ512 resolution. Even compared with the leading GAN model, i.e. BigGAN, and diffusion model, i.e. ADM [12],
MaskGIT offers comparable sample quality while yield-ing more favourable diversity. Notably, our model estab-lishes new state-of-the-arts on classification accuracy scores (CAS) [36] for synthesizing 256ˆ256 and 512ˆ512 im-ages. To our knowledge, this paper provides the first ev-idence demonstrating the efficacy of the masked modeling for image generation on the common ImageNet benchmark.
Furthermore, MaskGIT’s multidirectional nature makes it readily extendable to image manipulation tasks that are otherwise difficult for autoregressive models. Fig. 1 shows a new application of class-conditional image editing in which MaskGIT re-generates content inside the bounding box based on the given class while keeping the context (out-side of the box) unchanged. This task, which is either infea-sible for autoregressive model or difficult for GAN models, is trivial for our model. Quantitatively, we demonstrate this flexibility by applying MaskGIT to image inpainting, and
image extrapolation in arbitrary directions. Even though our model is not designed for such tasks, it obtains comparable performance to the dedicated models on each task. 2.