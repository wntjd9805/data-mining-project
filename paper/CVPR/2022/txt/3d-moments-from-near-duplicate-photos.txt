Abstract
We introduce 3D Moments, a new computational photog-raphy effect. As input we take a pair of near-duplicate photos, i.e., photos of moving subjects from similar viewpoints, com-mon in people’s photo collections. As output, we produce a video that smoothly interpolates the scene motion from the ﬁrst photo to the second, while also producing camera motion with parallax that gives a heightened sense of 3D.
To achieve this effect, we represent the scene as a pair of feature-based layered depth images augmented with scene
ﬂow. This representation enables motion interpolation along with independent control of the camera viewpoint. Our sys-tem produces photorealistic space-time videos with motion parallax and scene dynamics, while plausibly recovering regions occluded in the original views. We conduct exten-sive experiments demonstrating superior performance over baselines on public datasets and in-the-wild photos. Project page: https://3d-moments.github.io/. 1.

Introduction
Digital photography enables us to take scores of photos in order to capture just the right moment. In fact, we often end up with many near-duplicate photos in our image collections as we try to capture the best facial expression of a family member, or the most memorable part of an action. These near-duplicate photos end up just lying around in digital storage, unviewed.
In this paper, we aim to utilize such near-duplicate photos to create a compelling new kind of 3D photo enlivened with animation. We call this new effect 3D Moments: given a pair of near-duplicate photos depicting a dynamic scene from nearby (perhaps indistinguishable) viewpoints, such as the images in Fig. 1 (left), our goal is to simultaneously enable cinematic camera motion with 3D parallax (including novel, extrapolated viewpoints) while faithfully interpolating scene motion to synthesize short space-time videos like the one shown in Fig. 1 (right). 3D Moments combine both camera and scene motion in a compelling way, but involve very challenging vision problems: we must jointly infer 3D geometry, scene dynamics, and content that becomes newly disoccluded during the animation.
Despite great progress towards each of these individ-ual problems, tackling all of them jointly is non-trivial, especially with image pairs with unknown camera poses as input. NeRF-based view synthesis methods for dynamic scenes [15, 27, 28, 49] require many images with known cam-era poses. Single-photo view synthesis methods (sometimes called 3D Photos or 3D Ken Burns [11, 25, 38]) can create
animated camera paths from a single photo, but cannot rep-resent moving people or objects. Frame interpolation can create smooth animations from image pairs, but only in 2D.
Furthermore, naively applying view synthesis and frame interpolation methods sequentially results in temporally in-consistent, unrealistic animations.
To address these challenges, we propose a novel ap-proach for creating 3D Moments by explicitly modeling time-varying geometry and appearance from two uncali-brated, near-duplicate photos. The key to our approach is to represent the scene as a pair of feature-based layered depth images (LDIs) augmented with scene ﬂows. We build this representation by ﬁrst transforming the input photos into a pair of color LDIs, with inpainted color and depth for oc-cluded regions. We then extract features for each layer with a neural network to create the feature LDIs. In addition, we compute optical ﬂow between the input images and combine it with the depth layers to estimate scene ﬂow between the
LDIs. To render a novel view at a novel time, we lift these feature LDIs into a pair of 3D point clouds, and employ a depth-aware, bidirectional splatting and rendering module that combines the splatted features from both directions.
We extensively test our method on both public multi-view dynamic scene datasets and in-the-wild photos in terms of rendering quality, and demonstrate superior performance compared to state-of-the-art baselines.
In summary, our main contributions include: (1) the new task of creating 3D Moments from near-duplicate photos of dynamic scenes, and (2) a new representation based on feature LDIs augmented with scene ﬂows, and a model that can be trained for creating 3D Moments. 2.