Abstract
Many recent self-supervised frameworks for visual rep-resentation learning are based on certain forms of Siamese networks. Such networks are conceptually symmetric with two parallel encoders, but often practically asymmetric as numerous mechanisms are devised to break the symmetry.
In this work, we conduct a formal study on the importance of asymmetry by explicitly distinguishing the two encoders within the network – one produces source encodings and the other targets. Our key insight is keeping a relatively lower variance in target than source generally beneﬁts learning.
This is empirically justiﬁed by our results from ﬁve case studies covering different variance-oriented designs, and is aligned with our preliminary theoretical analysis on the baseline. Moreover, we ﬁnd the improvements from asym-metric designs generalize well to longer training schedules, multiple other frameworks and newer backbones. Finally, the combined effect of several asymmetric designs achieves a state-of-the-art accuracy on ImageNet linear probing and competitive results on downstream transfer. We hope our exploration will inspire more research in exploiting asym-metry for Siamese representation learning. 1.

Introduction
Despite different motivations and formulations, many re-cent un-/self-supervised methods for visual representation learning [1, 6–8, 18, 19, 44] are based on certain forms of
Siamese networks [4]. Siamese networks are inherently symmetric, as the two encoders within such networks share many aspects in design. For example, their model architec-tures (e.g., ResNet [20]) are usually the same; their network weights are often copied over; their input distributions – typically compositions of multiple data augmentations [8] – are by default identical; and their outputs are encouraged to be similar for the same image. Such a symmetric structure not only enables straightforward adaptation from off-the-shelf, supervised learning architectures to self-supervised learning, but also introduces a minimal inductive bias to
Figure 1. Asymmetry for Siamese representation learning. For the two encoders in a Siamese network, we treat one as a source encoder, and the other as a target encoder. We ﬁnd it generally beneﬁcial to have relatively lower variance in target than source. learn representations invariant w.r.t. various transformations in computer vision [10].
However, symmetry is not the only theme in these frame-works.
In fact, numerous mechanisms were proposed to break the conceptual symmetry. For example, BYOL [18] and SimSiam [10] place a special predictor head on one of the encoders, so architecture-wise they are no longer symmetric; MoCo [19] introduces momentum encoder, in which the weights are computed with moving-averages in-stead of directly copied; SwAV [6] and DINO [7] addition-ally adopt a multi-crop [27] strategy to enhance the augmen-tation on one side, shifting the data distribution asymmetric between encoders; even the InfoNCE loss [28] treats out-puts from two encoders differently – one is positive-only and the other also involves negatives. Among them, some speciﬁc asymmetric designs are crucial and well-studied (e.g., stop-gradient to prevent collapse [10]), but the gen-eral role of asymmetry for Siamese representation learning is yet to be better understood.
In this paper, we conduct a more formal study on the importance of asymmetry for Siamese learning. Deviat-ing from the original meaning of ‘Siamese’, we explic-itly mark the two encoders within the network function-ally different: a source encoder and a target encoder.1 The 1Depending on the context, source has also been referred as query/on-∗: work done during internship at FAIR. †: equal contribution. line/student; and target as key/teacher in the literature [18, 19, 32].
source encoder generates source encodings, and updates its weights via normal gradient-based optimization like in su-pervised learning. The target encoder updates its weights only with their source counterparts, and outputs target en-codings which in turn judge the quality of sources. This asymmetric encoder formulation also covers symmetric en-coders (e.g., in SimCLR [8]), where the target weights can be simply viewed as source duplicates.
With this distinction, our key insight is that keeping a rel-atively lower variance in target encodings than source can help representation learning (illustrated in Fig. 1). We sys-tematically study this phenomenon with our MoCo v2 [9] variant beyond existing – but scattered – evidence in the literature [5, 6, 19, 24, 37]. Speciﬁcally, given a variance-oriented design, we ﬁrst quantify its encoding variance with our baseline model, and then apply it to source or target (or both) encoders and examine the inﬂuence on learned repre-sentations. In total, we have conducted ﬁve case studies to explore various design spaces, ranging from encoder inputs, to intermediate layers and all the way to network outputs.
The results are well-aligned with our insight: designs that increase encoding variance generally help when applied to source encoders, whereas ones that decrease variance favor target. We additionally provide a preliminary theoretical analysis taking MoCo pre-training objective as an example, aimed at revealing the underlying cause.
Our observation generalizes well. First, we show the improvements from asymmetry – lower variance in target than source – can hold with longer pre-training schedules, suggesting they are not simply an outcome of faster con-vergence. Second, directly applying proper asymmetric designs from MoCo v2 to a variety of other frameworks (e.g., BYOL [18], Barlow Twins [44]) also works well, despite notable changes in objective function (contrastive or non-contrastive), model optimization (large-batch train-ing [43] or not), etc. Third, using MoCo v3 [11], we also experimented a more recent backbone – Vision Trans-former (ViT) [14] – and ﬁnd the generalization still holds well. Finally, several asymmetric designs are fairly com-positional: their combined effect enables single-node pre-trained MoCo v2 to reach a top-1 linear probing accuracy of 75.6% on ImageNet, a state-of-the-art with ResNet-50 backbone. This model also demonstrates good transferring ability to other downstream classiﬁcation tasks [8, 15, 18].
In summary, our study reveals an intriguing correlation between the relative source-target variance and the learned representation quality. We have to note that such correla-tion has limitations, especially as self-supervised learning follows a staged evaluation paradigm and the ﬁnal result is inevitably inﬂuenced by many other factors. Nonetheless, we hope our exploration will raise the awareness of the im-portant role played by asymmetry for Siamese representa-tion learning, and inspire more research in this direction. 2.