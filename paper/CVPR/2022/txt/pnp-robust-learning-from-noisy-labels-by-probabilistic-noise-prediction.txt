Abstract
Label noise has been a practical challenge in deep learn-ing due to the strong capability of deep neural networks in
ﬁtting all training data. Prior literature primarily resorts to sample selection methods for combating noisy labels. How-ever, these approaches focus on dividing samples by order sorting or threshold selection, inevitably introducing hyper-parameters (e.g., selection ratio / threshold) that are hard-to-tune and dataset-dependent. To this end, we propose a simple yet effective approach named PNP (Probabilistic
Noise Prediction) to explicitly model label noise. Speciﬁ-cally, we simultaneously train two networks, in which one predicts the category label and the other predicts the noise type. By predicting label noise probabilistically, we iden-tify noisy samples and adopt dedicated optimization objec-tives accordingly. Finally, we establish a joint loss for net-work update by unifying the classiﬁcation loss, the auxil-iary constraint loss, and the in-distribution consistency loss.
Comprehensive experimental results on synthetic and real-world datasets demonstrate the superiority of our proposed method. The source code and models have been made avail-able at https://github.com/NUST- Machine-Intelligence-Laboratory/PNP. 1.

Introduction
Although deep neural networks (DNNs) have attained impressive achievements, surpassing traditional methods in various vision tasks [3, 15, 28, 31, 38, 46], their requirement for large-scale high-quality human-labeled training samples (e.g., ImageNet [4] and COCO [21]) can often pose a bot-tleneck when applied to real-world scenarios. Precise anno-tation is always labor-expensive and time-consuming, espe-cially when domain-speciﬁc expert knowledge is necessary (e.g., ﬁne-grained visual categorization [13, 24, 41]). To al-leviate this issue, one promising alternative is to resort to web images for training deep networks [19,22,34,35,39,43,
*Corresponding author.
Clean : 50%
ID : 30%
OOD : 20% (a) (b) (c)
N o i s y s a m p l e
C l e a n s a m p l e
I
D n o i s y s a m p l e
O
O
D n o i s y s a m p l e
Figure 1. Early sample selection methods (a) tend to divide sam-ples into two subsets (i.e., clean and noisy), neglecting differences between ID and OOD noisy samples. Some recent methods (b) en-deavor to identify clean, ID noisy, and OOD noisy samples. How-ever, these methods usually suffer from tuning dataset-dependent threshold hyper-parameters (denoted as decision boundaries in (a) and (b)). In contrast, our proposed approach (c) seeks to model noisy labels in a probabilistic manner. PNP employs a dedicated predictor to estimate the probability distribution of noise type. 45, 47–49, 53, 54]. However, noisy labels are inevitable in web images [34]. It has been demonstrated that noisy labels can impair the performance of deep networks since the over-parameterization equips DNNs with not only large learn-ing capacities but also strong memorization power [16, 52].
Consequently, it is of great signiﬁcance to develop robust models for learning from noisy labels.
Noisy labels in real-world datasets can be categorized into two types: open-set and closed-set [48]. In the closed-set scenario, the true label of a noisy sample comes from a known label space
Yknown present in the training data.
Conversely, in the open-set scenario, true labels of sam-Yknown. In other words, closed-set noisy ples are outside samples are in-distribution (ID) ones, while open-set noisy samples are out-of-distribution (OOD) ones. A large body of prior literature primarily focuses on closed-set scenarios,
assuming that only in-distribution noise exists. However, the ID-noise-only assumption may not hold true in real-world applications. Recently, an increasing number of re-searchers have been attracted to the open-set noisy problem, which is also the primary focus of this work.
There are mainly two common strategies to tackle noisy labels: loss correction [5, 9, 26, 29, 33, 39, 50, 56] and sam-ple selection [1, 6, 11, 25, 42, 51]. Classic loss correction methods either attempt to estimate the noise transition ma-trix [2, 5, 9, 26, 33] or seek to regularize losses based on network predictions [29, 56]. Unfortunately, the noise tran-sition matrix is challenging to estimate, while prediction-based loss correction suffers from error accumulation.
Sample selection methods essentially follow an intuitive but straightforward idea: eliminating noisy data and train-ing with the cleaner subset. Researchers have recently wit-nessed that deep networks tend to ﬁt clean and simple pat-terns before memorizing noisy labels [16,52]. Accordingly, many approaches have been proposed to exploit this ob-servation and regard low-loss samples as clean ones. For example, Co-teaching [6] maintains two networks simul-taneously and enables them to select low-loss samples for their peer networks. Early sample selection methods usu-ally split samples into two subsets: clean and noisy, neglect-ing the difference between in-distribution noisy and out-of-distribution noisy labels. More recently, CRSSC [34] and Jo-SRC [48] are proposed to divide samples into three groups: clean ones, in-distribution noisy ones, and out-of-distribution noisy ones, and treat them differently. The for-mer employs a two-step sample selection process to cat-egorize samples into three groups, while the latter pro-poses global sample selection criteria to distinguish differ-ent types of noise. Despite that promising results have been observed, existing methods inevitably involve hard-to-tune and dataset-dependent threshold hyper-parameters for se-lecting samples, posing a limit to the reliability and scalabil-ity of these methods in various larger real-world scenarios.
To address aforementioned issues, we propose a sim-ple yet effective approach, named PNP (Probabilistic Noise
Prediction), to probabilistically model label noise in an end-to-end manner. Speciﬁcally, we simultaneously train two networks, in which one (i.e., label predictor network) pre-dicts the category of the input data while the other (i.e., noise predictor network) predicts the noise type (i.e., clean
/ ID noisy / OOD noisy). The clean, ID noisy, OOD noisy samples can be naturally identiﬁed according to the predic-tion from the noise predictor network. To enable effective learning of the noise predictor network, we propose to op-timize it in a regression manner, using JS divergence be-tween prediction-label pairs and prediction-prediction pairs.
Finally, we impose a consistency regularization on in-distribution data to further advance the learning of our label predictor network and noise predictor network. A compar-ison between our PNP and existing sample selection meth-ods is visualized in Fig. 1. Our major contributions are: (1) We propose a simple yet effective approach, named
PNP, to combat noisy labels. PNP simultaneously predicts the category label and noise type for all training samples.
By adopting distinct loss functions for different samples,
PNP can robustly learn from noisy training data. (2) PNP employs an auxiliary regression loss for em-powering the model to learn to predict the noise type of each sample. JS divergence between prediction-label pairs and prediction-prediction pairs is adopted to approximate the ground-truth noise type. Furthermore, consistency be-tween different views of in-distribution data is encouraged to reinforce the recognition ability. (3) We evaluate two paradigms of sample selection in our method: PNP-hard (hard selection) and PNP-soft (soft se-lection). We validate the effectiveness and superiority of our method by providing extensive experimental results on both synthetic and real-world noisy datasets. Moreover, compre-hensive ablation studies are established to verify each com-ponent of our approach. 2.