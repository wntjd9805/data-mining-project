Abstract
Recent transformer-based solutions have been intro-duced to estimate 3D human pose from 2D keypoint se-quence by considering body joints among all frames glob-ally to learn spatio-temporal correlation. We observe that the motions of different joints differ significantly. How-ever, the previous methods cannot efficiently model the solid inter-frame correspondence of each joint, leading to insuf-ficient learning of spatial-temporal correlation. We pro-pose MixSTE (Mixed Spatio-Temporal Encoder), which has a temporal transformer block to separately model the tem-poral motion of each joint and a spatial transformer block to learn inter-joint spatial correlation. These two blocks are utilized alternately to obtain better spatio-temporal fea-ture encoding. In addition, the network output is extended from the central frame to entire frames of the input video, thereby improving the coherence between the input and out-put sequences. Extensive experiments are conducted on three benchmarks (i.e. Human3.6M, MPI-INF-3DHP, and
HumanEva). The results show that our model outperforms the state-of-the-art approach by 10.9% P-MPJPE and 7.6%
MPJPE. The code is available at https://github. com/JinluZhang1126/MixSTE. 1.

Introduction 3D human pose estimation from monocular observations is a fundamental vision task that reconstructs 3D body joint locations from the input images or video. Since this task can obtain meaningful expressions of body geometry and motion, it has a wide range of applications, such as action recognition [54, 55], virtual human [5–7, 52], and human-robot interaction [11, 43, 50]. Most recent works are based on the 2D-to-3D lifting pipeline [1, 4, 28, 31, 37, 46, 57], which detects 2D keypoints firstly and then lift them to 3D.
Due to the depth ambiguity of monocular data, multiple po-tential 3D poses may be mapped from the same 2D pose, so
*Corresponding author: tuzhigang@whu.edu.cn
†Work done at Wuhan University
Figure 1. Top: Overview of spatio-temporal correlation modeling.
Each 2D keypoint is separated in the temporal domain to learn dif-ferent motion trajectories of body joints, and the spatial and tem-poral correlation are alternately stacked to improve the sequence coherence modeling ability. Bottom: Accuracy (MPJPE) and effi-ciency (FPS) comparison with different methods on Human3.6M dataset, the blue and orange colors indicate that the input sequence length T is equal to 81 and 243, respectively. it is difficult to recover an accurate 3D pose merely based on the information of a single frame 2D keypoints.
Notable progress has been made by exploiting tempo-ral information contained in the input video to address the above issues in a single frame [1, 4, 16, 28, 37, 46]. Re-cently, driven by the success of transformer [45] for its abil-ity to model sequence data, Zheng et al. [57] introduces a transformer-based 3D human pose estimation network. It takes advantage of spatio-temporal information for estimat-ing the more accurate central-frame pose in video. By mod-eling spatial correlations between all joints and temporal
correlations among consecutive frames, PoseFormer [57] achieves performance improvement. However, it ignores the motion differences among body joints, which causes the insufficient learning of spatio-temporal correlation. More-over, it increases the dimension of the temporal transformer module, which limits the usage of longer input sequence.
Poseformer [57] takes a video as input and only esti-mates the human pose of the central frame, which we sum-marize this pipeline as the seq2frame approach. Many re-cent methods [1,4,28,37,57] follow it and they utilize adja-cent frames to improve the accuracy of estimating the pose of a certain moment, but the sequence coherence is ignored due to the single frame output. Additionally, during the in-ference, these seq2frame solutions need to input a 2D key-point sequence repeatedly with large overlap to obtain 3D poses of all frames, which brings redundant calculation. In contrast to the seq2frame approach, there is also the seq2seq approach, which regresses the 3D pose sequence from the input 2D keypoints. These methods [16, 46] mainly depend on long short-term memory (LSTM) [15] cell or graph con-volution network (GCN) [21], and perform well in learning temporal information among continuous estimation results.
However, current seq2seq networks lack the global model-ing ability between input and output sequences, which tend to be excessively smooth [37] in the output poses of a long sequence. The low efficiency of LSTM [15] is also a severe issue for estimating human pose from video.
While previous work has focused on associating all joints in the spatial and temporal domains, we observe that the motion trajectories of the different body joints vary from frame to frame and should be learned separately. Addition-ally, the input 2D keypoint sequence and the output 3D pose sequence have solid global coherence, and they should be tightly coupled to promote accurate and smooth 3D poses.
Motivated by the above observations, in this work, we propose MixSTE to learn the separate temporal motion of each body joint and imbue sequential coherent human pose sequence in a seq2seq approach.
In contrast to the prior method [57] which reconstructs the central frame and ig-nores the single joint motion, the MixSTE lifts 2D key-point sequence to 3D pose sequence via a novel seq2seq architecture and a set of motion-aware constraints. Specif-ically, as shown at the top of Figure 1, we propose the joint separation to consider temporal motion information of each joint. It takes each 2D joint as an individual fea-ture (which is referred to as a token in transformer) to suf-ficiently learn spatio-temporal correlation and helps to re-duce the dimension of the joint features in temporal do-main. Moreover, we propose an alternating design with seq2seq to flexibly obtain better sequence coherence within a long sequence, which decreases redundant calculation and excessive smoothness.
In this way, temporal motion tra-jectories of different body joints could be adequately con-sidered to predict accurate 3D pose sequence. To the best of our knowledge, the proposed method is the first to uti-lize the transformer encoder in the seq2seq pipeline, which enhances learning spatio-temperal correlation for accurate pose estimation and significantly improves the inference speed from seq2frame methods (see the bottom of Fig.1)
Besides, our approach can easily adapt to any length of the input sequence.
Our contributions to 3D human pose estimation can be summarized in three folds:
• The MixSTE is proposed to effectively capture the temporal motion of different body joints over the long sequence, which helps to model sufficient spatio-temporal correlation.
• We propose a novel alternating design with transformer-based seq2seq model to learn the global coherence between sequences to improve the accuracy of reconstruction poses.
• Our approach achieves state-of-the-art performance on three benchmarks and has outstanding generalization. 2.