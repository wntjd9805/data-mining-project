Abstract
Aligning signals from different modalities is an important step in vision-language representation learning as it affects the performance of later stages such as cross-modality fusion.
Since image and text typically reside in different regions of the feature space, directly aligning them at instance level is challenging especially when features are still evolving dur-ing training. In this paper, we propose to align at a higher and more stable level using cluster representation. Specif-ically, we treat image and text as two “views” of the same entity, and encode them into a joint vision-language coding space spanned by a dictionary of cluster centers (codebook).
We contrast positive and negative samples via their cluster assignments while simultaneously optimizing the cluster cen-ters. To further smooth out the learning process, we adopt a teacher-student distillation paradigm, where the momen-tum teacher of one view guides the student learning of the other. We evaluated our approach on common vision lan-guage benchmarks and obtain new SoTA on zero-shot cross modality retrieval while being competitive on various other transfer tasks. 1.

Introduction
Vision language (V&L) representation learning is the problem of learning a unified feature embedding using both image and text signals. Pretrained V&L models have a great diversity of applications in various downstream tasks across different settings, e.g. via transfer learning [8, 28, 49]. The main tasks in V&L pretraining include aligning the feature spaces of different modalities (multi-modal alignment [8, 25, 28, 31]) and capturing the interaction across modalities (cross-modal fusion, [12, 44]). Late fusion approaches such as CLIP [37] and ALIGN [21] focused on the first task, while early fusion approaches such as OSCAR [28], VinVL [49] and VilLT [22] focused on the second one. In this work, we adopt a hybrid approach similar to ALBEF [25], where features from image and text modalities were first aligned
*The first two authors contributed equally.
Figure 1. We propose to use a learnable codebook to better align the image and text modalities. The codebook serves as a “bridge” between the image and text features. Each codeword can be inter-preted as a prototype, which enables contrasting image and text at the cluster level. We then solve an optimal transport [1] problem to optimize the distance between each modality to the prototypes, which in turn optimizes the alignment between the two modalities.
Prototype vectors are learned along with the feature encoders in our V&L framework. and then fused using a transformer encoder. The main focus of our work is on the feature alignment stage, which is challenging due to the fact that image and text inputs have very different characteristics. Existing approaches such as
CLIP [37] and ALIGN [21] have to rely on large training resources and on massive amount of data to obtain good alignments (400M and 1.8B image-text pairs respectively).
In this work, we propose a more efficient alignment strat-egy by using a codebook that quantizes the common text-image feature space into codewords. These codewords or cluster centers provide a more stable means for contrastive reasoning compared to individual text or visual features. We took the inspiration from SwAV [4], which was developed for self-supervised visual representation learning. In [4], two augmented versions (views) of the same input image were passed through a deep network for feature extraction. Visual embedding was learned by optimizing an objective function that enforces the consistency between the feature from one
view and the assigned cluster from the other view. SwAV achieved impressive performance in various transfer tasks (see [4]). Here, we carried out contrastive reasoning across modalities (image-text) instead of cross image views. De-tails are in Section 3.1, but in a nutshell, we use a learnable codebook for both image and text modalities and train our model to predict the codeword assignment using either text or visual information. Effectively, visual and text features are lined up via aligning with the common codewords during training. See Figure 1 for an illustration.
The codebook can be considered as a quantized sample of the underlying output feature distribution. It is end-to-end learnable together with the model parameters. To avoid abrupt changes during training, we further employ momen-tum distillation, which has been widely used in previous self-supervised learning works such as BYOL [16], DINO [5],
MoCo [18]. In brief, similar to ALBEF [25], for each of the image, text and fusion encoders, there is a corresponding encoder that is updated through moving average without gra-dient back propagation. These momentum encoders serve as teachers to guide the self-supervised learning process.
Different from ALBEF [25], we use the teachers to guide codebook learning as well as for the cross-modal and intra-modal alignment.
The above two components are wired up to support the stable update of the codebook which, in turn, provides an ef-ficient regularization mean for cross modality alignment. Ex-periment results (Section 4) show that our approach is com-petitive with state of the art across various benchmarks even when comparing with approach that use massive amount of data such as CLIP [37] and ALIGN [21]. In summary, our main contributions are as follows,
• We propose a codebook-based approach for efficient
It is an exten-vision-language alignment learning. sion from self-supervised vision representation learning (SSL) to the multimodal setting.
• We introduce a new distillation algorithm that helps unimodal and crossmodal contrastive optimization as well as helps stablize codebook learning.
The rest of the paper is organized as follows. We intro-duce related work to ours in Section 2. In Section 3, we describe our framework, called Codebook Learning with
Distillation (CODIS), and its two components, multimodal codebook learning and teacher-student distillation. Exper-imental results are presented in Section 4. Section 5 con-cludes the paper. wise, previous approaches can be broadly classified into two categories early fusion and late fusion. In early-fusion ap-proaches [8, 22, 28, 41], image and text are transformed into sequences (tokenization) and passed to a single encoder (typ-ically Transformer-based) for embedding generation. Thus multimodal signals are fused in the early stage. Whereas in late-fusion works [21, 37], separate encoders are used for image and text. Extracted features are typically fused during the later fine tuning stage. Our work is a hybrid be-tween these two approaches, similar to [25, 48]. The main difference is the codebook and various related contrastive losses.
In vision language learning, codebook has been used in a number of recent works, mostly for image tokenization.
BEiT [2] constructed a dictionary of visual words, then used it to form mask image modeling task in the same fashion as mask language modeling. SOHO [20] integrated visual dictionary to the main model and jointly trained both of them.
Both works quantized the visual input space. In contrast, our codebook is used to quantize the joint output space , where multimodal views are aligned via optimal transport [1].
Other concurrent works to ours include [25, 27]. They both align cross-modal instances using InfoNCE [33]. In contrast, we enforce both unimodal and cross-modal alignment, both at the instance level and at the cluster level.
Self-supervised Contrastive Learning The goal of con-trastive learning [17] is to attract positive sample pairs and repulse the negative sample pairs. Recently, it has been widely used in computer vision for unsupervised, semi-supervised [13] and self-supervised representation learn-ing [5,7,18]. Contrastive reasoning is typically formed based on two augmented views of the same input image. One of the main challenge is feature collapsing, and in practice, a large number of negative samples are required, through either large batch size [7] or memory banks [18, 45], to al-leviate this problem. Several recent works have shown that one can learn unsupervised features without discriminating instances. Deep clustering [3] and SwAV [4] incorporate online clustering into Siamese networks. In BYOL [16], features are trained by matching them to representations ob-tained by a momentum encoder. DINO [5] instantiates the momentum encoder with a vision-transformer and adopts a teacher-student distillation paradigm [13, 19, 47]. Our alignment techniques and momentum update were inspired by these works and can be considered as extensions to the multimodal setting. 3. Method 2.