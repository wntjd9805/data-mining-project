Abstract
We present a method that achieves state-of-the-art re-sults on challenging (few-shot) layout-to-image genera-tion tasks by accurately modeling textures, structures and relationships contained in a complex scene. After compressing RGB images into patch tokens, we propose the Transformer with Focal Attention (TwFA) for explor-ing dependencies of object-to-object, object-to-patch and patch-to-patch. Compared to existing CNN-based and
Transformer-based generation models that entangled mod-eling on pixel-level&patch-level and object-level&patch-level respectively, the proposed focal attention predicts the current patch token by only focusing on its highly-related tokens that speciﬁed by the spatial layout, thereby achieving disambiguation during training. Furthermore, the proposed
TwFA largely increases the data efﬁciency during training, therefore we propose the ﬁrst few-shot complex scene gener-ation strategy based on the well-trained TwFA. Comprehen-sive experiments show the superiority of our method, which signiﬁcantly increases both quantitative metrics and qual-itative visual realism with respect to state-of-the-art CNN-based and transformer-based methods. Code is available at https://github.com/JohnDreamer/TwFA. 1.

Introduction
∗Equal Contribution. †Corresponding author.
This research is partly supported by NSFC, China (No: 61876107,
U1803261), and Dr. Chaoyue Wang is supported by ARC FL-170100117.
Generating photo-realistic images is the ever-lasting goal in computer vision. Despite achieving remarkable progress on image generation for both simple scenario, e.g., faces,
cars, and cats [14, 15, 30], and single object, e.g., Ima-geNet [1, 42], the image generation for complex scenes composed of multiple objects of various categories is still a challenging problem.
In this paper, we focus on one representative com-plex scene image generation task, layout to image gener-ation [45] (L2I), which aims to generate complex scenes conditioned on speciﬁed layouts. The layout, as illustrated in Figure 1, consists of a set of object bounding boxes and corresponding categories, thus providing a sketch of the ex-pected complex scene image. Compared with other condi-tions for complex scene generation, including textual de-scriptions [27], scene graphs [13, 23], and segmentation masks [20], layouts are much more user-friendly, control-lable and ﬂexible [45]. Ambitiously, we further propose a new few-shot layout to image generation task (few-shot
L2I), which aims to generate complex scenes with a novel object category after providing only a few images contain-ing the novel objects.
As to the complex scene generation, including (few-shot)
L2I tasks, the core challenge is how to synthesize a photo-realistic image with reasonable object-level relationships, clear patch-level instance structures, and reﬁned pixel-level textures. Existing attempts to the L2I task can be divided into two categories, i.e., CNN-based [18, 20, 24, 33, 34, 46] and Transformer-based [12], according to their generator.
The CNN-based methods deploy an encoder-decoder gen-erator [13, 24] where the encoder transfers the layout into an image feature map, and the decoder upsamples the fea-ture map into the target image. Those methods capture the object relationships in the encoder by a self-attention [35] or a convLSTM [46], and model the instance structures and textures simultaneously in the decoder by upsampling con-volutions. In contrast, the Transformer-based methods tok-enize the layout into object tokens and employ a pre-trained compression model to quantize the image into a sequence of discrete patch tokens, thus simplifying the image generation task as an image patch composition task implemented by a
Transformer. Those methods produce the detailed textures with the compression model, and model both relationships and structures by the Transformer.
However, the entangled modeling on patch-level and pixel-level (CNN-based methods) or object-level and patch-level (Transformer-based methods) prevents the model from capturing inherent instance structures, leading to blurry or crumpled objects, and increases the burden on the few-shot learning because the model must learn the two levels information simultaneously with only a few images. To this end, upon the Transformer-based methods, we propose a Transformer with Focal Attention (TwFA) to separately model image compositions on object-level and patch-level by distinguishing between object and patch tokens. Dif-ferent from vanilla self-attention, which neglects the com-position prior of spatial layouts, our focal attention further constrains each token can only attend on its related tokens according to the spatial layouts. Speciﬁcally, to model ob-ject relationships, an object token attends on all object to-kens to capture the global information. To model instance structures, a patch token attends on the object it belongs to and the patches inner the object bounding box. By the pro-posed Focal Attention, the TwFA focuses on generating the current patch without any disturbance from other objects or patches thus increasing the data efﬁciency during training.
Therefore, the focal attention makes the TwFA can fast learn the novel object category with only a few images.
We validate the effectiveness of the proposed TwFA on COCO-stuff [2, 22] and Visual Genome [17] datasets.
TwFA improves the state-of-the-arts [12,20] FID score from 29.56 to 22.15 (-25.1%) on COCO-stuff, and from 19.14 to 17.74 (-7.3%) on Visual Genome. Morevoer, TwFA demon-strates the superiority on the few-shot L2I task with strong performance and impressive visualizations. 2.