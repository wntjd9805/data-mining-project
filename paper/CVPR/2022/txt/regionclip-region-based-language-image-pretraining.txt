Abstract
Contrastive language-image pretraining (CLIP) using image-text pairs has achieved impressive results on image classification in both zero-shot and transfer learning set-tings. However, we show that directly applying such mod-els to recognize image regions for object detection leads to unsatisfactory performance due to a major domain shift:
CLIP was trained to match an image as a whole to a text de-scription, without capturing the fine-grained alignment be-tween image regions and text spans. To mitigate this issue, we propose a new method called RegionCLIP that signifi-cantly extends CLIP to learn region-level visual representa-tions, thus enabling fine-grained alignment between image regions and textual concepts. Our method leverages a CLIP model to match image regions with template captions, and then pretrains our model to align these region-text pairs in the feature space. When transferring our pretrained model to the open-vocabulary object detection task, our method outperforms the state of the art by 3.8 AP50 and 2.2 AP for novel categories on COCO and LVIS datasets, respec-tively. Further, the learned region representations support zero-shot inference for object detection, showing promis-ing results on both COCO and LVIS datasets. Our code is available at https://github.com/microsoft/RegionCLIP. 1.

Introduction
The recent advances in vision-language representation learning has created remarkable models like CLIP [37],
ALIGN [26] and Florence [59]. Such models are trained using hundreds of millions of image-text pairs by match-ing images to their captions, achieving impressive results of recognizing a large set of concepts without manual la-bels, and capable of transferring to many visual recognition tasks. Following their success on image classification, a nat-ural question is whether these models can be used to reason
*Work done as an intern at Microsoft Research.
Figure 1. (a). A pretrained CLIP model [37] failed to capture lo-calization quality. (b). A major drop on accuracy when using the same pretrained CLIP to classify image regions. (c). Our key idea is learning to match image regions and their text descriptions. about image regions, e.g., for tasks like object detection.
To answer this question, we construct a simple R-CNN style [16] object detector using a pretrained CLIP model, similar to adapting a convolutional network pretrained on
ImageNet. This detector crops candidate object regions from an input image, and applies the CLIP model for de-tection by matching visual features of cropped regions to text embeddings of object categories. Fig. 1(a-b) shows the results on LVIS dataset [19]. When using object propos-als [42] as the input regions, scores from CLIP often fail to capture the localization quality (Fig. 1a). Even with ground-truth object boxes, classification accuracy using CLIP drops significantly from 60% on ImageNet to 19% on LVIS, with a similar number of classes (Fig. 1b). There is thus a major performance degradation when applying a pretrained CLIP model for object detection. How can we empower a vision-language pretrained model to reason about image regions?
We believe the main gap lies in the training of these vision-language models. Many existing vision-language
models, including CLIP, are trained to match an image with its image-level text description. The training is unaware of the alignment between local image regions and text tokens.
Thus, the models are unable to precisely ground a textual concept to an image region. Further, cropping local image regions and matching them to text tokens largely ignore the surrounding visual context that is critical for object recogni-tion, not to mention the high computational cost, e.g. a few seconds per image on a modern GPU.
In this paper, we explore learning region representations for object detection via vision-language pretraining. Our key idea is to explicitly align image regions and text to-kens during pretraining. However, two key challenges arise.
First, the fine-grained alignment between image regions and text tokens is not available in image-text pairs and expensive to annotate. Second, the text description of an image is of-ten incomplete, i.e. many image regions are not described by the text. To address these challenges, we propose to bootstrap from a pretrained vision-language model to align image regions and text tokens, and to fill in the missing re-gion descriptions, as illustrated in Fig. 1c.
Specifically, our method starts with a pool of object con-cepts parsed from text corpus, and synthesizes region de-scriptions by filling these concepts into pre-defined tem-plates. Given an input image and its candidate regions from either object proposals or dense sliding windows, a pre-trained CLIP model is used to align the region descriptions and the image regions, creating “pseudo” labels for region-text alignment. Further, we combine “pseudo” region-text pairs and ground-truth image-text pairs to pretrain our vision-language model via contrastive learning and knowl-edge distillation. Although the “pseudo” region-text pairs are noisy, they still provide useful information for learning region representations, and thus help to bridge the gap in object detection, as validated by our experiments.
We pretrain our RegionCLIP model on image captioning datasets (e.g., Conceptual Caption [45]) and mainly evalu-ate our method on the benchmarks of open-vocabulary ob-ject detection (COCO [32] and LVIS [19] datasets). When transferred to open-vocabulary object detection, our pre-trained model establishes new state of the art (SoTA) on
COCO and LVIS. For instance, our method outperforms previous methods [18, 60] by at least 3.8 AP50 and 2.2 AP for novel categories on COCO and LVIS. Moreover, our model supports zero-shot inference and outperforms a set of strong baselines by a clear margin.
Our contributions are summarized as follows: (1) We propose a novel method that aligns image regions and their text descriptions without manual annotation, thereby en-abling vision-language pretraining for learning visual re-(2) A key technical innovation that gion representations. facilitates our pretraining is a scalable approach using text prompts to align the object descriptions with image regions, without relying on human annotations nor limited to the text paired with an image. (3) Our pretrained model presents strong results when transferred to open-vocabulary object detection, and demonstrates promising capability on zero-shot inference for object detection. 2.