Abstract
In this paper, we study Multiscale Vision Transformers (MViTv2) as a unified architecture for image and video classification, as well as object detection. We present an improved version of MViT that incorporates decomposed relative positional embeddings and residual pooling con-nections. We instantiate this architecture in five sizes and evaluate it for ImageNet classification, COCO detection and Kinetics video recognition where it outperforms prior work. We further compare MViTv2sâ€™ pooling attention to window attention mechanisms where it outperforms the latter in accuracy/compute. Without bells-and-whistles, MViTv2 has state-of-the-art performance in 3 domains: 88.8% ac-curacy on ImageNet classification, 58.7 APbox on COCO object detection as well as 86.1% on Kinetics-400 video classification. Code and models are available at https:
//github.com/facebookresearch/mvit. 1.

Introduction
Designing architectures for different visual recognition tasks has been historically difficult and the most widely adopted ones have been the ones that combine simplicity and efficacy, e.g. VGGNet [67] and ResNet [37]. More recently Vision Transformers (ViT) [17] have shown promis-ing performance and are rivaling convolutional neural net-works (CNN) and a wide range of modifications have re-cently been proposed to apply them to different vision tasks [1, 2, 21, 55, 68, 73, 78, 90].
While ViT [17] is popular in image classification, its usage for high-resolution object detection and space-time video understanding tasks remains challenging. The den-sity of visual signals poses severe challenges in compute and memory requirements as these scale quadratically in complexity within the self-attention blocks of Transformer-based [76] models. The community has approached this burden with different strategies: Two popular ones are (1) local attention computation within a window [55] for object detection and (2) pooling attention that locally aggregates features before computing self-attention in video tasks [21]. (a) Image classification (b) Object detection (c) Video recognition
Figure 1. Our MViTv2 is a multiscale transformer with state-of-the-art performance across three visual recognition tasks.
The latter fuels Multiscale Vision Transformers (MViT) [21], an architecture that extends ViT in a simple way: instead of having a fixed resolution throughout the net-work, it has a feature hierarchy with multiple stages starting from high-resolution to low-resolution. MViT is designed for video tasks where it has state-of-the-art performance.
In this paper, we develop two simple technical improve-ments to further increase its performance and study MViT as a single model family for visual recognition across 3 tasks: image classification, object detection and video classifica-tion, in order to understand if it can serve as a general vision backbone for spatial as well as spatiotemporal recognition tasks (see Fig. 1). Our empirical study leads to an improved architecture (MViTv2) and encompasses the following: (i) We create strong baselines that improve pooling at-tention along two axes: (a) shift-invariant positional embed-dings using decomposed location distances to inject position information in Transformer blocks; (b) a residual pooling connection to compensate the effect of pooling strides in attention computation. Our simple-yet-effective upgrades lead to significantly better results. (ii) Using the improved structure of MViT, we employ a standard dense prediction framework: Mask R-CNN [36] with Feature Pyramid Networks (FPN) [53] and apply it to object detection and instance segmentation.
We study if MViT can process high-resolution visual input by using pooling attention to overcome the compu-tation and memory cost involved. Our experiments sug-gest that pooling attention is more effective than local win-dow attention mechanisms (e.g. Swin [55]). We further develop a simple-yet-effective Hybrid window attention scheme that can complement pooling attention for better accuracy/compute tradeoff. (iii) We instantiate our architecture in five sizes of in-creasing complexity (width, depth, resolution) and report a practical training recipe for large multiscale transformers.
The MViT variants are applied to image classification, object detection and video classification, with minimal modifica-tion, to study its purpose as a generic vision architecture.
Experiments reveal that our MViTv2 achieves 88.8% accuracy for ImageNet-1K classification, with pretrain-ing on ImageNet-21K (and 86.3% without), as well as 58.7 APbox on COCO object detection using only Cas-cade Mask R-CNN [6]. For video classification tasks,
MViT achieves unprecedented accuracies of 86.1% on
Kinetics-400, 87.9% on Kinetics-600, 79.4% on Kinetics-700, and 73.3% on Something-Something-v2. Our video code will be open-sourced in PyTorchVideo 1,2 [19, 20]. 2.