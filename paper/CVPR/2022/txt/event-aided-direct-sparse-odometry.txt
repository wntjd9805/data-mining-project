Abstract 1.

Introduction
We introduce EDS, a direct monocular visual odome-try using events and frames. Our algorithm leverages the event generation model to track the camera motion in the blind time between frames. The method formulates a di-rect probabilistic approach of observed brightness incre-ments. Per-pixel brightness increments are predicted using a sparse number of selected 3D points and are compared to the events via the brightness increment error to esti-mate camera motion. The method recovers a semi-dense 3D map using photometric bundle adjustment. EDS is the first method to perform 6-DOF VO using events and frames with a direct approach. By design it overcomes the problem of changing appearance in indirect methods. Our results out-perform all previous event-based odometry solutions. We also show that, for a target error performance, EDS can work at lower frame rates than state-of-the-art frame-based
VO solutions. This opens the door to low-power motion-tracking applications where frames are sparingly triggered
“on demand” and our method tracks the motion in between.
We release code and datasets to the public.
Multimedia Material
Visual Odometry (VO) is a paramount tool in computer vision, robotics and, any application that requires spatial reasoning [1–3]. In recent years, considerable progress has been made on this topic [1, 4–6]. However, VO systems are limited by the capabilities of their physical devices (sensors, processors, and power). Some of these limitations (e.g., motion blur, dynamic range) can been tackled with novel and/or more robust sensors, such as event cameras.
Event cameras [7–9] are bio-inspired sensors that work radically different from traditional cameras. Instead of cap-turing brightness images at a fixed rate, they measure asyn-chronous, per-pixel brightness changes, called “events”.1
This principle of operation endows event cameras with out-standing properties, such as low latency, high temporal reso-lution (in the order of µs) and low power (milliwatts instead of watts). The large potential of event cameras to tackle VO and related problems in challenging scenarios has been in-vestigated in [10–21]. We refer to a recent comprehensive survey paper for further details [22].
Event-based VO is a challenging problem that has been addressed step-by-step in scenarios with increasing com-plexity. Two fundamental challenges arise when working with event cameras: noise (caused by timestamp jitter, pixel
Code and dataset can be found at: https://rpg.ifi.uzh.ch/eds 1See an illustrative animation: https://youtu.be/LauQ6LWTkxM?t=30
manufacturing mismatch or non-linear circuity effects) and data association [22, 23], i.e., establishing correspondences between events to identify which events are triggered by the same scene point. This is so because each event carries lit-tle information and the temporal edge patterns conveyed by the events depend on motion. These two issues make event-based keypoints used by indirect methods difficult to detect and track with sufficient stability; for this reason, grayscale frames [14] or motion compensation and inertial sensor fu-sion [18] have been used to mitigate their effect.
Event-based methods might be categorized accord-ing to whether they exploit the Event Generation Model (EGM) [13, 24] or not [14, 17, 25, 26]. The EGM states how events are created when a predefined contrast threshold is reached [7, 22].
It is a photometric relationship between brightness “changes” (i.e., events) and “absolute” bright-ness. Experiments on event-based feature tracking [23] have shown that methods exploiting the EGM achieve higher accuracy than those that do not. However such a comparison is yet to be performed for 6-DOF camera track-ing (i.e., ego-motion estimation). Current solutions [13, 17] are prone to lose tracking, either because the convergence of the estimated 3D map is slow [13] or because the edge-patterns change quickly from one packet of events to the next one [17]. That is, there is no long-term appearance, like the grayscale frames in [23], to latch onto and improve tracking robustness, which we intend to do.
We propose to tackle the event-based VO problem by un-derstanding and overcoming the shortcomings of previous methods. To the best of our knowledge, this work is the first monocular method to perform 6-DOF VO using events and frames with a direct approach. Our contribution lies in the front-end, fusing the information from events and frames tightly using the EGM (as opposed to previous works that loosely coupled them [18]). Estimated poses, points, and selected frames are fed into a sliding-window photomet-ric bundle adjustment (PBA) back-end, which minimizes brightness errors. This is the first time that PBA is used in the context of event-camera VO. Internally, we adopt a key-frame-based approach, recovering inverse depth at pix-els with high gradient as in DSO [5]. However, our method allows tracking the camera motion in the blind time between frames using only events. This opens the door for frames to be triggered sparingly, i.e., “on demand”, thus potentially saving energy in the system, which is a desirable feature in
AR/VR applications and in platforms with a limited power budget. Our contributions are summarized as follows:
• The first formulation of a monocular 6-DOF visual odometry combining events and grayscale frames in a direct approach with photometric bundle adjustment.
• Camera motion tracking using a sparse set of pix-els, minimizing the normalized brightness change of
Events Frames D/I EGM Remarks
Kim et al. [13]
Rebecq et al. [17]
Kueng et al. [14]
Rosinol et al. [18]
This work
✓
✓
✓
✓
✓
✗
✗
✓
✓
✓
D
D
I
I
D
✓
✗
✗
✗
✓
Three parallel EKFs
Parallel tracking and mapping
Tracking event features for VO
Loosely coupled front-end
Tightly coupled front-end
Table 1. Comparison of event-based monocular 6-DOF VO/SLAM methods. The columns indicate the type of input (events and/or grayscale frames), the type of method (Direct or Indirect), and whether the method exploits the event generation model (EGM). projected (inverse depth) points.
• A compelling evaluation on publicly available datasets outperforming previous solutions, and a sensitivity study to gain insights about our method.
• A new dataset with high quality events, color frames, and IMU data to foster research in monocular VO. 2.