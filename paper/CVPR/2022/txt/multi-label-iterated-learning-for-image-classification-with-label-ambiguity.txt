Abstract
Kt
Transfer learning from large-scale pre-trained models has become essential for many computer vision tasks. Recent studies have shown that datasets like ImageNet are weakly labeled since images with multiple object classes present are assigned a single label. This ambiguity biases models towards a single prediction, which could result in the sup-pression of classes that tend to co-occur in the data. Inspired by language emergence literature, we propose multi-label it-erated learning (MILe) to incorporate the inductive biases of multi-label learning from single labels using the framework of iterated learning. MILe is a simple yet effective procedure that builds a multi-label description of the image by prop-agating binary predictions through successive generations of teacher and student networks with a learning bottleneck.
Experiments show that our approach exhibits systematic ben-eﬁts on ImageNet accuracy as well as ReaL F1 score, which indicates that MILe deals better with label ambiguity than the standard training procedure, even when ﬁne-tuning from self-supervised weights. We also show that MILe is effective reducing label noise, achieving state-of-the-art performance on real-world large-scale noisy data such as WebVision. Fur-thermore, MILe improves performance in class incremental settings such as IIRC and it is robust to distribution shifts.
Code: https://github.com/rajeswar18/MILe 1.

Introduction
Large-scale datasets with human-annotated labels have been central to the development of modern state-of-the-art neural network-based artiﬁcial perception systems [23, 24, 32]. Improved performance on ImageNet [17] has led to remarkable progress in tasks and domains that leverage Im-ageNet pretraining [11, 42, 70]. However, these weakly-annotated datasets and models tend to project a rich, multi-label reality into a paradigm that envisions one and only one label per image. This form of simpliﬁcation often hinders
*Equal contribution.
Teacher
BCE loss
// stop gradient
Copy weights multi-label prediction bottleneck
Human
Tree
Car
House single-label  ground-truth
Student
BCE loss
Ks
Figure 1. Multi-label Iterated Learning (MILe) builds a multi-label representation of the images from singly-labeled ground-truth.
In this example, a model produces multi-label binary predictions for the next generation, obtaining Car and House for an image weakly labeled with House. model performance by asking models to predict a single la-bel, when trained on real-world images that contain multiple objects.
Given the importance of the problem, there is growing recognition of single-labeled datasets as a form of weak supervision and an increasing interest in evaluating the lim-its of these singly-labeled benchmarks. A series of recent studies [8, 54, 56, 59, 65] highlight the problem of label ambiguity in ImageNet. In order to obtain a better estimate of model performance, Beyer et al. [8] and Shankar et al.
[54] introduced multi-label evaluation sets. They identiﬁed softmax cross-entropy training as one of the main reasons for low multi-label performance since it promotes label ex-clusiveness. They also showed that replacing the softmax with sigmoid activations and casting the output as a set of binary classiﬁers results in better multi-label validation per-formance. Several other studies have explored ways to over-come the shortcomings in existing validation procedures by improving the pipelines for gathering labels [6, 48, 58].
In order to obtain a more complete description of images from weakly-supervised or semi-supervised data, a number
of methods leverage a noisy signal such as pseudo-labels [65] or textual descriptions crawled from the web [47]. In this work, we observe that the process of building a rich repre-sentation of data from a noisy source shares some properties with the process of language emergence studied in the cog-nitive science literature. In particular, Kirby [29] proposed that structured language emerged from an inter-generational iterated learning process [29, 30, 31]. According to the theory, a compositional syntax emerges when agents learn by imitation from previous generations in the presence of a learning bottleneck. This bottleneck forces noisy fragments of the language to be forgotten when transmitted to new generations. Conversely, those fragments that can be reused and composed to enrich the language tend to be passed to subsequent generations. We show that the same procedure can be applied to settings that leverage a weak or noisy su-pervisory signal such as [47, 65] to build a richer description of images while reducing the noise.
In this work, we propose multi-label iterated learning (MILe) to learn to predict rich multi-label representations from weakly supervised (single-labeled) training data. We do so by introducing two different learning bottlenecks. First, we replace the standard convolutional neural network out-put softmax with a hard multi-label binary prediction. Sec-ond, we transmit these binary predictions through successive model generations, with a limited training iterations between each generation.
In our experiments, we demonstrate that MILe allevi-ates the label ambiguity problem by improving the F1 score of supervised and self-supervised models on the ImageNet
ReaL [8] multi-label validation set. In addition, experiments on WebVision [37] show that iterated learning increases ro-bustness to label noise and spurious correlations. Finally, we show that our approach can help in continual learning scenarios such as IIRC [1] where newly introduced labels co-occur with known labels. Our contributions are:
• We propose MILe, a multi-label iterated learning algo-rithm for image classiﬁcation that builds a rich multi-label representation of data from weak single labels.
• We show that models trained with MILe are more robust to noise and perform better on ImageNet, ImageNet-ReaL, WebVision, and multiple setups such as super-vised learning (Section 4.1), self-supervised ﬁne-tuning and semi-supervised learning (Section 4.2), continual learning (Supplementary 2), and domain generalization (Supplementary 5).
• We provide insights on the predictions made by models trained with iterated learning (Section 4.3). 2.