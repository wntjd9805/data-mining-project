Abstract
Automatic lip-reading (ALR) aims to recognize words us-ing visual information from the speaker’s lip movements.
In this work, we introduce a novel type of sensing de-vice, event cameras, for the task of ALR. Event cameras have both technical and application advantages over con-ventional cameras for the ALR task because they have higher temporal resolution, less redundant visual informa-tion, and lower power consumption. To recognize words from the event data, we propose a novel Multi-grained
Spatio-Temporal Features Perceived Network (MSTP) to perceive fine-grained spatio-temporal features from mi-crosecond time-resolved event data. Specifically, a multi-branch network architecture is designed, in which differ-ent grained spatio-temporal features are learned by oper-ating at different frame rates. The branch operating on the low frame rate can perceive spatial complete but tem-poral coarse features. While the branch operating on the high frame rate can perceive spatial coarse but temporal refinement features. And a message flow module is devised to integrate the features from different branches, leading to perceiving more discriminative spatio-temporal features.
In addition, we present the first event-based lip-reading dataset (DVS-Lip) captured by the event camera. Experi-mental results demonstrated the superiority of the proposed model compared to the state-of-the-art event-based action recognition models and video-based lip-reading models. 1.

Introduction
Automatic lip-reading (ALR), also known as visual lan-guage recognition, aims to decode the text content through the visual information of the speaker’s lip movements. ALR has great applications in biometric identification [2], im-proved hearing aids [40], speech recognition in noisy en-vironments [29], so that it has attracted much attention in
*Co-first author. †Corresponding author.
Figure 1. Visualization of event frames of different temporal reso-lutions (25FPS and 200FPS, respectively) and their corresponding feature maps and feature points that have been dimensionally re-duced by t-SNE [41]. The low-rate event frames contain complete spatial features but coarse temporal features, while the high-rate event frames contain fine temporal features but incomplete spatial features. the field of computer vision and pattern recognition over a long period.
In this paper, we introduce a novel type of optical sen-sor, event cameras [18], to tackle automatic lip-reading problem. Event cameras are biologically inspired optical sensors. Unlike conventional cameras that capture images at a fixed rate, event cameras capture per-pixel brightness changes asynchronously in the microsecond level. For the
ALR task that requires the perception of fine-grained spatio-temporal features, event cameras have significant advan-tages over conventional cameras in terms of technology and applications: 1) the high temporal resolution of event cam-eras allow them to record finer-grained movements; 2) their output does not contain much redundant visual information since only brightness changes of the scene are recorded; 3) they are low-power and can work on challenging lighting conditions which are essential in real-world applications.
To correctly recognize words from the event stream, it is necessary to perceive fine-grained spatio-temporal fea-tures from the event stream. There are already a number of event-based action recognition methods [3, 6, 38, 42–44].
Point-cloud-based [42] and graph-based methods [6, 44] treat events data as point clouds and graph nodes, respec-tively. However, during the conversion from the original event data to the point clouds or graph nodes, the fine-grained temporal and spatial information contained in the event data is discarded. SNN-based methods [3, 38] pro-cess the input events stream asynchronously with spiking neural networks, but they are difficult to train since no ef-ficient back-propagation algorithm exists. Existing CNN-based methods [19,43] convert the asynchronous event data into fixed-rate frame-like representations and feed them into standard deep neural networks. They also lose varying de-grees of spatial or temporal information depending on the temporal resolution of the event frames. In summary, ex-isting event-based action recognition methods are not suit-able for the ALR task, which requires the perception of fine-grained spatio-temporal features from the event data.
In this work, we choose to convert the event data into multi-grained event frames. As illustrated in Figure 1, the low-rate event frames contain complete spatial features but coarse temporal features due to high temporal compres-sion, while the high-rate event frames contain fine tempo-ral features but incomplete spatial features since each event frame is composed of a small number of events. To take full advantage of the abundant spatio-temporal informa-tion in the event data, we propose a Multi-grained Spatio-Temporal Features Perceived Network (MSTP) that takes multi-grained event frames as input to recognize words. The proposed model contains two branches, of which the first branch takes the low-rate event frames as input, allowing the model to perceive complete spatial structure informa-tion. In contrast, the second branch takes the high-rate event frames as input, enabling the model to perceive fine tempo-ral features. Furthermore, we devise a message flow mod-ule (MFM) to merge multi-grained spatio-temporal features learned by different branches, leading to perceiving more discriminative spatio-temporal features.
Due to the lack of available datasets for event-based lip-reading, we collected the first event-based lip-reading dataset (called DVS-Lip) using event camera DAVIS346.
The DVS-Lip contains a total of 19,871 samples, an ex-ample of which is shown in Figure 3. To explore the ad-vantages of event cameras in capturing fine-grained move-ment evolution information, we divided the vocabulary of the DVS-Lip dataset into two parts. The first part con-sists of the 25 pairs of visually similar words selected from the LRW dataset [11], and the second part consists of an-other 50 randomly selected words from the vocabulary of the LRW dataset. More details can be found in Sect. 4.1.
We validate the effectiveness of the proposed MSTP by conducting extensive experiments on the DVS-Lip dataset.
Quantitative results show that: 1) MSTP outperforms ex-isting event-based action recognition models and the state-of-the-art video-based lip-reading models on the proposed dataset for both common and visually similar words; 2) The proposed message flow module has a more significant im-provement on visually similar words recognition, thus it is beneficial to perceive fine-grained spatio-temporal features.
The major contributions of our work can be summarized in the following four aspects:
• To the best of our knowledge, it is the first work to study event-based automatic lip-reading. And we pro-pose a novel event-based automatic lip-reading frame-work MSTP to perceive multi-grained spatio-temporal features for words recognition.
• We devise a message flow module to merge multi-grained spatio-temporal features for more discrimina-tive features perceiving.
• Considering the lack of a relevant benchmark, we col-lected the first event-based lip-reading dataset (DVS-Lip), which will be made available to the community.
• Extensive experiments conducted on the DVS-Lip dataset show that the proposed method outperforms the state-of-the-art event-based and video-based methods. 2.