Abstract
We propose TubeR: a simple solution for spatio-temporal video action detection. Different from existing methods that depend on either an off-line actor detector or hand-designed actor-positional hypotheses like proposals or anchors, we propose to directly detect an action tubelet in a video by si-multaneously performing action localization and recognition from a single representation. TubeR learns a set of tubelet-queries and utilizes a tubelet-attention module to model the dynamic spatio-temporal nature of a video clip, which ef-fectively reinforces the model capacity compared to using actor-positional hypotheses in the spatio-temporal space.
For videos containing transitional states or scene changes, we propose a context aware classification head to utilize short-term and long-term context to strengthen action classi-fication, and an action switch regression head for detecting the precise temporal action extent. TubeR directly produces action tubelets with variable lengths and even maintains good results for long video clips. TubeR outperforms the previous state-of-the-art on commonly used action detection datasets AVA, UCF101-24 and JHMDB51-21. Code will be available on GluonCV(https://cv.gluon.ai/). 1.

Introduction
This paper tackles the problem of spatio-temporal human action detection in videos [3, 17, 39], which plays a central role in advanced video search engines, robotics, and self-driving cars. Action detection is a compound task, requir-ing the localization of per-frame person instances, the link-ing of these detected person instances into action tubes and the prediction of their action class labels. Two approaches for spatio-temporal action detection are prevalent in the lit-erature: frame-level detection and tubelet-level detection.
Frame-level detection approaches detect and classify the ac-tion independently on each frame [14, 29, 32], and then link per-frame detections together into coherent action tubes. To compensate for the lack of temporal information, several
*Equally contributed and work done while at AWS AI Labs
Figure 1. TubeR takes as input a video clip and directly outputs tubelets: sequences of bounding boxes and their action labels. Tu-beR runs end-to-end without person detectors, anchors or proposals. methods simply repeat 2D proposals [12, 15, 35] or offline person detections [9, 28, 37, 43] over time to obtain spatio-temporal features (Figure 1 top left).
Alternatively, tubelet-level detection approaches [16, 19, 26, 33, 45, 49], directly generate spatio-temporal volumes from a video clip to capture the coherence and dynamic na-ture of actions. They typically predict action localization and classification jointly over spatio-temporal hypotheses, like 3D cuboid proposals [16, 19] (Figure 1 top right). Unfortu-nately, these 3D cuboids can only capture a short period of time, also when the spatial location of a person changes as soon as they move, or due to camera motion. Ideally, this family of models would use flexible spatio-temporal tubelets that can track the person over a longer time, but the large configuration space of such a parameterization has restricted previous methods to short cuboids. In this work we present a tubelet-level detection approach that is able to simulta-neously localize and recognize action tubelets in a flexible manner, which allows tubelets to change in size and location over time (Figure 1 bottom). This allows our system to lever-age longer tubelets, which aggregate visual information of a person and their actions over longer periods of time.
We draw inspiration from sequence-to-sequence mod-elling in natural language processing (NLP), particularly machine translation [21, 24, 36, 40], and its application to object detection, DETR [4]. Being a detection framework,
DETR can be applied as a frame-level action detection ap-proach trivially, but the power of the transformer framework, on which DETR is built, is its ability to generate complex structured outputs over sequences. In NLP, this typically takes the form of sentences but in this work we use the no-tion of decoder queries to represent people and their actions over video sequences, without having to restrict tubelets to fixed cuboids.
We propose a tubelet-transformer, we call TubeR, for localizing and recognizing actions from a single representa-tion. Building on the DETR framework [4], TubeR learns a set of tubelet queries to pull action-specific tubelet-level features from a spatio-temporal video representation. Our
TubeR design includes a specialized spatial and temporal tubelet attention to allow our tubelets to be unrestricted in their spatial location and scale over time, thus overcoming previous limitations of methods restricted to cuboids. Tu-beR regresses bounding boxes within a tubelet jointly across time, considering temporal correlations between tubelets, and aggregates visual features over the tubelet to classify actions. This core design already performs well, outperform-ing many previous model designs, but still does not improve upon frame-level approaches using offline person detectors.
We hypothesize that this is partially due to the lack of more global context in our query based feature as it is hard to clas-sify actions referring to relationships such as ‘listening-to’ and ‘talking-to’ by only looking at a single person. There-fore, we introduce a context aware classification head that, along with the tubelet feature, takes the full clip feature from which our classification head can draw contextual informa-tion. This design allows the network to effectively relate a person tubelet to the full scene context where the tubelet appears and is shown to be effective on its own in our results section. One limitation of this design is the context feature is only drawn from the same clip our tubelet occupies. It has been shown [43] to be important to also include long term contextual features for the final action classification. Thus, we introduce a memory system inspired by [44] to compress and store contextual features from video content around the tubelet. We feed this long term contextual memory to our classification head using the same feature injection strategy and again show this gives an important improvement over the short term context alone. We test our full system on three popular action detection datasets (AVA [15], UCF101-24 [34] and JHMDB51-21 [18]) and show our method can outperform other state-of-the-art results.
In summary, our contributions are as follows: 1. We propose TubeR: a tubelet-level transformer frame-work for human action detection. 2. Our tubelet query and attention based formulation is able to generate tubelets of arbitrary location and scale. 3. Our context aware classification head is able to aggre-gate short-term and long-term contextual information. 4. We present state-of-the-art results on three challenging action detection datasets. 2.