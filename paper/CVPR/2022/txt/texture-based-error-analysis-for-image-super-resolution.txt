Abstract
Evaluation practices for image super-resolution (SR) use a single-value metric, the PSNR or SSIM, to determine model performance. This provides little insight into the source of errors and model behavior. Therefore, it is benefi-cial to move beyond the conventional approach and recon-ceptualize evaluation with interpretability as our main pri-ority. We focus on a thorough error analysis from a variety of perspectives. Our key contribution is to leverage a texture classifier, which enables us to assign patches with seman-tic labels, to identify the source of SR errors both globally and locally. We then use this to determine (a) the semantic alignment of SR datasets, (b) how SR models perform on each label, (c) to what extent high-resolution (HR) and SR patches semantically correspond, and more. Through these different angles, we are able to highlight potential pitfalls and blindspots. Our overall investigation highlights numer-ous unexpected insights. We hope this work serves as an initial step for debugging blackbox SR networks. 1.

Introduction
The standard practice of training and evaluating image
SR models has not changed drastically in recent years. Neu-ral networks are optimized on the Diverse 2K (DIV2K) [40]
HR images. The models are then evaluated on five bench-mark datasets: Set5 [3], Set14 [45], B100 [28], Ur-ban100 [15], and Manga109 [29] with two metrics, the peak-signal-to-noise ratio (PSNR) and the structural simi-larity index (SSIM) [43]. This protocol ultimately shapes the performance of the network and provides a means by which models are compared against each other.
We note several limitations to this standard approach.
First, a single value is used to represent the model’s per-formance on an entire dataset. There is immense variabil-ity within a single image, let alone an entire dataset. Us-ing only the PSNR does little to showcase the true perfor-mance or indicate to the user where and how the model fails.
Second, because we do not have a concrete description of the datasets, these performance results might be mislead-ing. For example, models might perform well on patches
Figure 1. Visual examples of errors from state-of-the-art SR mod-els. Some reconstructions completely change the semantic mean-ing of a patch.
In the bottom left row, the digits “094” on the airplane wing have been obscured or changed to a different digit.
Especially at inference time, it is critical to detect such mistakes. that are not reflective of the dataset’s overall content. Addi-tionally, the datasets have not been probed, and it is unclear whether these are appropriate choices for the task, whether that be training or evaluation.
Moreover, during inference time, we do not have access to the original HR image. It has been demonstrated that neu-ral networks make overly confident predictions [14]. Even more troubling, SR networks frequently change the seman-tic meaning of images. Figure 1 shows the original HR im-age alongside several reconstructions by popular SR mod-els.
In the top left figure, the airplane wing contains the digits “094”. However, in the reconstructions by all three
SR models, the numbers have been obscured or transformed to a different number. Similarly, in the following image of the bird, the orientation, width, and number of lines have changed. These types of errors are significant as they can alter the semantic meaning of the image. The importance of catching and understanding such errors is compounded in safety-critical domains such as biomedicine. For example,
Weigert et al. [44] introduced CARE networks to restore fluorescence microscopy data. They showed that important biological structures can appear and disappear randomly, significantly influencing downstream analysis.
To begin to address these concerns, we take a closer look at the error sources for each step of the SR framework. The
Figure 2. Overview of our texture-based error analysis framework for image SR. After training a texture classification model, we can analyze the source of SR errors from multiple perspectives, including (a) different texture distributions of datasets, (b) SR performance vs. texture classes, and (c) texture alignment between HR and SR pairs. This approach provides insights that the PSNR/SSIM alone cannot. guiding principle to our analysis is a human-centric one.
That is, we want to use language which is intuitive, easily recognizable, and sufficiently expressive. Our investigation is designed through the lens of textures and patterns. The analysis, as described in Figure 2, is conducted by partition-ing the natural image manifold into semantically meaning-ful groups (e.g., grid, striped, polka-dotted, etc.).
Our experiments address the following questions:
• What type of semantic information is encoded in each
SR dataset, and to what extent do training and evalua-tion sets align?
• Where do model failures occur relative to these seman-tic groups? What is the nature of those images?
• Do image SR models preserve the semantic meaning of patches?
This work results in the following main insights and con-tributions: (1) We provide a comprehensive semantic profile of each dataset, which is valuable to assess the diversity of the datasets and capture the degree of semantic alignment between them. We find that the datasets in the SR frame-work are biased towards certain semantic groups. (2) We verify that the semantic class label can serve as a proxy for the difficulty of a patch. This equips us with a mean-ingful way to think and communicate about the data. (3)
The labels the SR models perform the best on are not the labels that reflect the content of the entire dataset. This motivates us to consider alternative methods of evaluation beyond PSNR. (4) For some semantic labels, there is a sur-prisingly small benefit from deep learning, as simple bicu-bic upsampling can even outperform deep models. (5) We propose a simple metric to curate a training set strategically.
We demonstrate that training on only those patches of inter-est can accelerate training, even when these patches com-prise only 20 percent of the original training data. (6) SR models surprisingly alter the semantic meaning of patches. 2.