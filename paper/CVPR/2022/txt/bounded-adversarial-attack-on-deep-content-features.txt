Abstract
We propose a novel adversarial attack targeting content features in some deep layer, that is, individual neurons in the layer. A naive method that enforces a ﬁxed value/percentage bound for neuron activation values can hardly work and generates very noisy samples. The reason is that the level of perceptual variation entailed by a ﬁxed value bound is non-uniform across neurons and even for the same neuron.
We hence propose a novel distribution quantile bound for activation values and a polynomial barrier loss function.
Given a benign input, a ﬁxed quantile bound is translated to many value bounds, one for each neuron, based on the distri-butions of the neuron’s activations and the current activation value on the given input. These individualized bounds enable
ﬁne-grained regulation, allowing content feature mutations with bounded perceptional variations. Our evaluation on Im-ageNet and ﬁve different model architectures demonstrates that our attack is effective. Compared to seven other latest adversarial attacks in both the pixel space and the feature space, our attack can achieve the state-of-the-art trade-off between attack success rate and imperceptibility. 1 1.

Introduction
Adversarial attack is a prominent security threat for Deep
Learning (DL) applications. With a benign input, perturba-tion is applied to the input to derive an adversarial example, which causes the DL model to misclassify. An underlying as-sumption is that adversarial samples should be perceptually close to real inputs [10]. Without this assumption, adversar-ial samples could merely be too different from real inputs and become unseen samples, in which case misclassiﬁcation is well expected. Traditionally, imperceptibility is ensured by having bounded perturbation in the pixel space. The bound is usually small, e.g., [−4, 4] in the RGB range of [0, 255], such that perturbations are imperceptible by humans.
Researchers have recently shown adversarial examples with large pixel distances (from the original inputs) can 1Code and Samples are available on Github [37]. be generated. Such distances are usually way beyond the bounds that many existing defense and validation techniques aim to protect, providing a new attack vector. These tech-niques focus on mutating meta-features of original inputs, such as colors and styles, due to the difﬁculty of harness-ing perturbations on content features, such as shapes and local patterns, denoted by individual neurons. While us-ing adversarial samples generated by these techniques can harden the model in meta-feature space, making the model robust to color and style changes, they offer limited protec-tion when the attacker is able to mutate individual content-features/neurons in an imperceptible way. In addition, the perturbations generated by these methods are pervasive and hence more visible in human eyes, making them less desir-able when being used in real attacks.
In this paper, we propose a new attack vector in the fea-ture space that can perform imperceptible content feature perturbation. Such perturbations cannot be expressed by pixel bounds or meta-feature bounds (e.g., bounds on mean and standard deviation of activation values) and hence pose a new challenge to existing defense techniques. The essence of our technique is to bound the perturbations of individ-ual neurons. Given a uniform bound at the perception level (e.g., allowing 10% perceptual perturbation for each content-feature/neuron), it is projected to various value bounds for individual neurons which have different activation value ranges and distributions. Gradient back-propagation is used to mutate input pixels, just like in traditional adversarial attack, while the mutations are constrained by the internal bounds. A naive method is to ﬁrst proﬁle the activation value ranges of individual neurons and then limit the variation of each neuron to a ﬁxed portion of its value range. However, this method does not work because the perception of a ﬁxed activation value perturbation varies substantially (even for a single feature) depending on the activation value itself.
For example, a change of 1.0 when the activation value is 0.0 admits a substantially different level of perceptual varia-tions compared to the same change when the value is 15.0.
To achieve a uniform perceptual bound, we propose to use a distribution quantile bound. More speciﬁcally, given a model, we identify internal values that approximate normal
(a) Original (ℓ! ℓ" conf.  succ.) (b) BIM (5/255, 64, -10.4,     ) (c) BIM (16/255, 460, 8.6,      ) (d) FS (255/255, 3.8k, 15.4,      ) (e) SM (243/255, 15k, 31.5,     ) (f) D2B (58/255, 586, 34.8,      )
Figure 1. Adversarial examples of different attacks/attack-settings. The ﬁrst column represents the original images. The second and third columns show examples from Basic Iterative Method (BIM) with a small and a large pixel distances, respectively. The following columns are samples from Feature Space Attack(FS), Semantic Attack (SM), and our Deep Distribution Bounded Attack (D2B). For each set of images, the ﬁrst row presents the adversarial examples. The second row shows the perturbations applied to the original image. We enlarge the perturbations of BIM by 10 times and the others’ by 5 times for better illustration. On the bottom of each column, there is a quadruple representing the ℓ∞, ℓ2 distances, the attack conﬁdence of each example and the attack success. A positive value implies a successful attack and a large value indicates the model is very conﬁdent about the (misclassiﬁcation) result. distributions. We call them a throttle plane (see Section 3). After identifying the throttle plane, we collect the acti-vation distribution over the training set for each neuron on the plane. Given a benign sample, its activation on each neuron (on the plane) is acquired. The bound for its value change is dynamically computed based on a ﬁxed quantile of the distribution (e.g., 10%) and the activation itself. This is called the quantile bound. Our technique is hence called
D2B (Deep Distribution Bounded Attack). We then enforce the value bounds using a polynomial internal barrier loss (Section 3.2). Note that such value bounds are completely dynamic while they denote the same perceptual bound. Our results show that the method can mutate content features in a way that is less human perceptible. According to our human study in Section 4, our technique can achieve 95% attack success rate, and yet humans cannot easily distinguish the adversarial examples from the benign ones within a short time. In comparison to traditional pixel space attacks that generate noise-like perturbations, our attack generates pertur-bations piggy-backing on existing semantically meaningful features, making them difﬁcult to detect.
Example. The second and third columns of Figure 1 (in the blue dashed box) show some samples with a small pixel bound (i.e., ℓ∞ = 5/255, meaning the maximum pixel value change is 5 out of 255) and a larger bound (i.e.,
ℓ∞ = 16/255) for the BIM attack2. Observe that with the larger bound, the adversarial perturbation is detectable by 2We use BIM instead of other pixel space attacks such as PGD because we found that (compared to BIM) the random initialization of PGD degrades imperceptibility at a non-trivial scale, in exchange for just a slightly higher success rate. Hence, we consider BIM a more compelling baseline when considering the balance between attack success rate and imperceptibility. human eyes, suggesting using a large bound in the pixel space is undesirable. The third and fourth columns (in the orange dashed box) show some examples of attacks in the feature space, namely feature space attack [38] and semantic attack [2]. Details of these attacks can be found in Section 2.
Observe that they have much larger ℓ∞ and ℓ2 distances (from the original inputs in the ﬁrst column) than the exam-ples generated by the BIM attacks. While their perturbations are less perceptible than the examples generated by pixel space attacks given a similar pixel distance, the perturbations are quite noticeable in human eyes due to their pervasive nature. This is conﬁrmed by our human study (Section 4), in which we show that with an 80% attack success rate, humans can easily recognize the adversarial examples.
The last column of Figure 1 shows a typical example generated by our technique and its pixel-level contrast with the original image. Observe that the differences largely pig-gyback on the content features of the original example (by having similar shapes and local patterns as the original input), making them more human imperceptible. More importantly, these perturbations are quite different from those by exist-ing pixel space and feature space attacks, suggesting a new threat.
We conduct experiments on ImageNet and ﬁve models, in-cluding both naturally and adversarially trained models. Our results show that existing adversarial training has little effect on our attack. Although comparing different attacks may be comparing apple with orange, we study the correlations between attack success rate and human imperceptibility for seven related attacks perturbing either the pixel or internal space, to understand the high level positioning of our attack.
Our results show that our attack produces adversarial exam-Figure 2. Workﬂow of our attack. It consists of three steps: 1⃝ throttle plane (TP) selection, 2⃝ internal distribution boundary constraint, and 3⃝ adversarial sample generation with combined losses. ples that are less human perceptible when achieving the same level of attack conﬁdence/success-rate. Further evaluation against three different detection techniques demonstrates that our attack has better/comparable persistence while having better imperceptibility due to its new attack vector. 2.