Abstract
Federated Learning (FL) is an emerging distributed learning paradigm under privacy constraint. Data hetero-geneity is one of the main challenges in FL, which results in slow convergence and degraded performance. Most ex-isting approaches only tackle the heterogeneity challenge by restricting the local model update in client, ignoring the performance drop caused by direct global model ag-gregation. Instead, we propose a data-free knowledge dis-tillation method to fine-tune the global model in the server (FedFTG), which relieves the issue of direct model aggre-gation. Concretely, FedFTG explores the input space of local models through a generator, and uses it to transfer the knowledge from local models to the global model. Be-sides, we propose a hard sample mining scheme to achieve effective knowledge distillation throughout the training. In addition, we develop customized label sampling and class-level ensemble to derive maximum utilization of knowl-edge, which implicitly mitigates the distribution discrep-ancy across clients.
Extensive experiments show that our FedFTG significantly outperforms the state-of-the-art (SOTA) FL algorithms and can serve as a strong plugin for enhancing FedAvg, FedProx, FedDyn, and SCAFFOLD. 1.

Introduction
With the explosive growth of data and the strict privacy-protection policy, reckless data transmission and aggrega-tion gradually become unacceptable due to the high band-width cost and risk of privacy leakage. Recently, Federated
Learning (FL) [30, 31] has been proposed to replace the tra-ditional heavily centralized learning paradigm and protect data privacy. It has been successfully applied in real-world
*Part of this work was done during Ling Zhang’s internship at JD Ex-plore Academy.
†Corresponding author.
Figure 1. Comparison between FedAvg [30] and FedFTG. By fine-tuning the global model using generated hard samples, FedFTG alleviates the performance decrease after model aggregation. tasks, such as smart city [16, 34, 42], health care [8, 26, 27], and recommender system [9, 10], etc.
One of the main challenges in FL is the data heterogene-ity, i.e., the data in clients are non-identically and indepen-dently distributed (Non-IID). It has been verified that the vanilla FL algorithm, FedAvg [30], leads to drifted local models and forgets the global knowledge catastrophically in this scenario, which further induces degraded performance and slow convergence [14, 19, 21]. This is because the lo-cal model is updated merely with local data, i.e., minimiz-ing the local empirical loss. However, minimizing the local empirical loss is fundamentally inconsistent with minimiz-ing the global empirical loss [1, 24, 29] in Non-IID FL.
To tackle the data heterogeneity challenge, most existing methods, e.g., FedProx [23], SCAFFOLD [18], FedDyn [1],
MOON [22] constrain the direction of local model update to align the local and global optimization objectives. Re-cently, FedGen [43] learns a lightweight generator to gen-erate pseudo feature and broadcasts it to clients to regulate local training. However, all these methods merely conduct simple model aggregation to get the global model in server, which ignores local knowledge incompatibility and induces knowledge forgetting in the global model. In addition, [36]
shows that directly aggregating models will largely degrade the performance while fine-tuning can greatly boost the ac-curacy. These motivate us to fine-tune the aggregated global model in the server with the knowledge in local models. On the other hand, merely aggregating local models in server ignores the server’s rich computing resources that could be potentially utilized to improve the performance of FL, such as the computing source in cross-silo FL [17].
Motivated by these observations, we propose a novel ap-proach that boosts the performance of standard FL by on-the-fly fine-tuning the global model via data-free knowl-edge distillation (FedFTG), which simultaneously refines the model aggregation procedure and exploits the rich com-puting power of the sever. Concretely, FedFTG models the input space of local models through an auxiliary genera-tor in the server, then generates pseudo data to transfer the knowledge in local models to the global model to improve the performance. To facilitate effective knowledge distilla-tion throughout the training, FedFTG iteratively explores the hard samples in data distribution, which will induce prediction disagreement between local models and global model. Figure 1 compares FedFTG with FedAvg. FedFTG fine-tunes the global model with the hard samples to correct the model shift after model aggregation. The generator and global model are adversarially trained in a data-free manner, thus the whole procedure will not violate the privacy policy in FL. Considering the label distribution shift in data hetero-geneity scenario, we further propose customized label sam-pling and class-level ensemble techniques, which explore the distribution correlation of clients and exploit maximum utilization of knowledge.
FedFTG is orthogonal to several existing local optimiz-ers, such as FedAvg, FedProx, FedDyn, SCAFFOLD and
MOON, as it only modifies the procedure of global model aggregation in the server. Consequently, FedFTG can be seamlessly embedded into these local FL optimizers, tak-ing their advantages to further improve the performance of
FedFTG. Extensive experiments on various settings verify that FedFTG achieves superior performance compared with state-of-the-art (SOTA) methods.
The main contributions of this work are four-fold:
• We propose FedFTG to fine-tune the global model in server via data-free distillation, which simultaneously enhances the model aggregation step and utilizes the computing power of the server.
• We develop hard sample mining to effectively trans-fer knowledge to global model. Besides, we propose customized label sampling and class-level ensemble to facilitate maximum utilization of knowledge.
• We demonstrate that FedFTG is orthogonal to exiting local optimizers and can serve as a strong and versatile plugin to enhance the performance of FedAvg, Fed-Prox, FedDyn, SCAFFOLD and MOON.
• We verify the superiority of FedFTG against several
SOTA methods for FL, including FedAvg, FedProx,
FedDyn, SCAFFOLD, MOON, FedGen and FedDF, with extensive experiments on five benchmarks. 2.