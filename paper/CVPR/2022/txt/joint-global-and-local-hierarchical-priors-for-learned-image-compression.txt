Abstract
Recently, learned image compression methods have out-performed traditional hand-crafted ones including BPG.
One of the keys to this success is learned entropy models that estimate the probability distribution of the quantized latent representation. Like other vision tasks, most recent learned entropy models are based on convolutional neu-ral networks (CNNs). However, CNNs have a limitation in modeling long-range dependencies due to their nature of local connectivity, which can be a significant bottleneck in image compression where reducing spatial redundancy is a key point. To overcome this issue, we propose a novel entropy model called Information Transformer (Informer) that exploits both global and local information in a content-dependent manner using an attention mechanism. Our ex-periments show that Informer improves rate–distortion per-formance over the state-of-the-art methods on the Ko-dak and Tecnick datasets without the quadratic computa-tional complexity problem. Our source code is available at https://github.com/naver-ai/informer. 1.

Introduction
More than one trillion photos are taken every year and the number is increasing [12], leading to an ever-increasing demand for improved compression efficiency. Recently, ad-vances in deep learning have led to significant progress in learned image compression [7, 8, 16, 26, 32, 36, 40, 43–45].
Generally, learned image compression follows a transform coding framework [22] consisting of transformation and en-tropy coding (see the top of Fig. 1). In this framework, an image is first transformed into a quantized latent repre-sentation that enables more effective compression than the original image. Then, the quantized latent representation is encoded to a bitstream by a standard entropy coding algo-rithm (e.g., arithmetic coding [42]). An entropy model, i.e., a prior probability model on the quantized latent represen-tation, is required for the entropy coding algorithm. Deep
*Work done while doing an internship at NAVER AI Lab.
Figure 1. Overview of the proposed method. Our Informer is a learned entropy model capturing global dependencies in a content-dependent manner using the attention mechanism [46]. neural networks are employed for the transformation and entropy model in this framework [7, 8, 16, 32, 36, 40, 43], where both are learned in an end-to-end manner to fully uti-lize the strong capability of deep neural networks [25].
Since the length of the bitstream relies on the entropy model, designing an accurate entropy model is important for compression efficiency, which is our main focus in this pa-per. The goal of entropy models is to estimate a joint prob-ability distribution over the elements of the quantized latent representation. A simple way to do this is to assume com-plete independence among the elements [7]. However, this approach yields limited compression efficiency since the as-sumption does not hold in most practical cases [8]. Thus, how to model the remaining dependencies has been an im-portant issue in learned image compression [8,32,36,40]. It is popular to extract additional features, called “hyperprior” or “hierarchical prior”, capturing the dependencies from the latent representation using convolutional neural networks (CNNs). This approach has contributed to learning accurate entropy models, making learned image compression meth-ods outperform hand-crafted image codes such as BPG [10].
However, despite the significant progress, CNN-based entropy models still have limitations in capturing the depen-dencies due to the nature of CNNs. First, existing entropy models do not make full use of global information due to
the local receptive field of CNNs. This issue can be criti-cal in modeling long-range dependencies. For example, in the case of Fig. 1, the CNN-based approach cannot fully capture the dependencies among the red windows that re-peatedly appear across the whole image due to the localized receptive field. Second, the receptive fields of previous en-tropy models cannot exclude nearby elements with different contents due to the content-independent property of convo-lution operations [38]. In other words, no matter how dif-ferent the contents of two elements are, they are processed within the same receptive field if they are located nearby. In
Fig. 1, although the red window and the bricks have quite different contents, both are used simultaneously in the pro-cess of capturing dependencies.
To overcome these limitations, we propose a novel en-tropy model, called Information Transformer (Informer), that captures both global and local dependencies in a content-dependent manner using the attention mechanism of Transformer [46] (Fig. 1). In contrast to convolution operations, the attention mechanism has known to be ef-fective in modeling long-range dependencies in a content-dependent manner [38]. Based on the joint autoregressive and hierarchical priors [36], which is the basis of the lat-est entropy models [16, 40], we introduce two novel hyper-priors, i.e., a global hyperprior and a local hyperprior. To model global dependencies of the quantized latent represen-tation, our Informer first extracts a global hyperprior con-sisting of different vectors that attend to different areas of an image by using the cross-attention mechanism [4,14,34].
Furthermore, our Informer extracts a local hyperprior spe-cialized for local information by using 1×1 convolutional layers. Our local hyperprior prevents our global hyperprior from utilizing only local information and thus allows our In-former to consider global and local information effectively.
Compared to the baseline entropy model [36], Informer improves the rate–distortion performance of learned im-age compression methods on the popular Kodak [31] and
Tecnick [2] datasets. In addition, Informer achieves better performance than the recently proposed global reference model [40] aiming at capturing global dependencies; In-former not only yields higher rate–distortion performance but also avoids the quadratic computational complexity problem of the global reference model. Our main contri-butions can be summarized as follows:
• We propose joint global and local hyperpriors that ef-fectively model two different types of dependencies between the elements of the quantized latent represen-tation using the attention mechanism.
• We demonstrate that our Informer with the joint global and local hyperpriors improves rate–distortion perfor-mance of learned image compression while addressing the quadratic computational complexity problem. 2.