Abstract
Recent works show that convolutional neural network (CNN) architectures have a spectral bias towards lower frequencies, which has been leveraged for various image restoration tasks in the Deep Image Prior (DIP) framework.
The beneﬁt of the inductive bias the network imposes in the
DIP framework depends on the architecture. Therefore, re-searchers have studied how to automate the search to de-termine the best-performing model. However, common neu-ral architecture search (NAS) techniques are resource and time-intensive. Moreover, best-performing models are de-termined for a whole dataset of images instead of for each image independently, which would be prohibitively expen-sive. In this work, we ﬁrst show that optimal neural archi-tectures in the DIP framework are image-dependent. Lever-aging this insight, we then propose an image-speciﬁc NAS strategy for the DIP framework that requires substantially less training than typical NAS approaches, effectively en-abling image-speciﬁc NAS. We justify the proposed strat-egy’s effectiveness by (1) demonstrating its performance on a NAS Dataset for DIP that includes 522 models from a particular search space (2) conducting extensive experi-ments on image denoising, inpainting, and super-resolution tasks. Our experiments show that image-speciﬁc metrics can reduce the search space to a small cohort of models, of which the best model outperforms current NAS approaches for image restoration. Codes and datasets are available at https://github.com/ozgurkara99/ISNAS-DIP. 1.

Introduction
Convolutional neural networks (CNNs) have been ubiq-uitously utilized in almost every ﬁeld of computer vision.
Particularly, researchers harness the power of CNNs in im-age restoration tasks [1, 2, 3, 4], which refers to the task of recovering the original image from a corrupted version. The
*equal contribution success of CNNs comes as a result of their ability to learn a mapping from a corrupted image to its uncorrupted counter-part. However, the ground truth labels are not always avail-able to learn such a mapping for a given domain, limiting the use of approaches under supervised settings. To tackle this problem, researchers orient their attention towards un-supervised approaches. Recent discoveries have shown that the architecture of CNNs contains an intrinsic prior that can be used in image restoration tasks [5, 6]. This insight led to the Deep Image Prior (DIP) framework [5], which works solely with the degraded image and can produce competi-tive results for image restoration tasks without a supervised training phase. It offers an alternative solution to restora-tion problems by suggesting a new regularizer: the network architecture itself. In addition to this empirical discovery,
Rahaman et al. [7] investigated the spectral bias of neural networks towards low frequencies theoretically, which can explain the impressive performance of the DIP framework.
Chakrabarty [8] further explored the underlying reason be-hind the success of DIP in denoising natural images. The work demonstrates that the network tends to behave simi-larly to a low pass ﬁlter at the early stages of iterations. Fi-nally, DeepRED [9] merged the concept of “Regularization by Denoising” (RED) by adding explicit priors to enhance
DIP.
One problem faced in the DIP framework is that the architectural design of the network has a substantial im-pact on the performance. Recent works attempted to auto-mate the search process of network architecture for various tasks, which is referred to as the Neural Architecture Search (NAS). In the context of DIP, Chen et al. [10] applied NAS to the DIP framework. However, current NAS approaches come with substantial computational costs, as they require optimizing a large number of architectures to determine the optimum. Moreover, this cost prohibits determining the op-timum architecture for every image; instead, existing NAS approaches search for the best architecture for a dataset of images.
Our work.
In this paper, we propose novel image-dependent metrics to determine optimal network architec-tures in the DIP framework with minimal training. Unlike previous works, we apply our metrics to DIP for ﬁnding image-speciﬁc architectures since performance is strongly dependent on the content of the image that is to be restored.
We ﬁrst motivate image-speciﬁc NAS, by showing that in a given search space, there is only a small overlap of the best architectures for different images. This is illustrated in
Figure 1, where the matrices show the number of overlaps between the top 10 models (of a total of 522 models) for each image for denoising and inpainting.
To identify architectures that are ﬁtting for a speciﬁc im-age, we propose image-dependent metrics that measure the property of how far the power spectral density (PSD) of the generated initial output of a network is from that of the cor-rupted image and use it as our metric. The intuition relies on the fact that the more these two are similar, the better the model will reconstruct the image since it is closer to the solution space. (a) Denoising (b) Inpainting
Figure 1: The overlap of the best-performing architectures between different images is shown here. The numbers indi-cate how many of the top-10 models of the search space for image x are also in the top-10 for image y. This is shown for the task of denoising and inpainting in (a) and (b), respec-tively. E.g. the value at the intersection of chest and lena in the denoising heatmap, which is 2, indicates that there are 2 models in each of the images’ best-performing 10 models that are the same.
We motivate the choice of metrics by looking at the cor-relation between the metrics’ values and image restoration performance. There is an imperfect correlation; hence we select a small cohort of architectures to optimize based on the metrics’ values. A ﬁnal selection is then made by se-lecting the model whose output is closest to the average of outputs of all models.
We conduct experiments on conventional datasets for im-age denoising, image inpainting, and single image super-resolution tasks using the proposed strategy. For each im-age in the datasets, we run our ISNAS algorithm to identify the optimal image-speciﬁc models. The results demonstrate that our method is superior to the state-of-the-art work [10] in terms of its quality improvement.
The main contributions can be summarized as follows:
• We empirically show the necessity of identifying image-speciﬁc models to augment the quality of DIP.
• We present novel metrics to be used in NAS requir-ing only the randomly initialized CNN network. These metrics allow ranking architectures within any search space without lengthy optimization as a surrogate to their success on image restoration tasks.
• We introduce two selection procedures among a sub-set of models for ﬁnding optimal architectures in an unsupervised fashion for DIP.
• We generate a NAS Dataset for DIP having 522 mod-els optimized for ten images from different domains, including image denoising and image inpainting tasks,.
• Extensive experiments on commonly used datasets and
NAS Dataset for DIP validate our approach. 2.