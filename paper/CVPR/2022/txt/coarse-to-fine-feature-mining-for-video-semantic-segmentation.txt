Abstract
The contextual information plays a core role in seman-tic segmentation. As for video semantic segmentation, the contexts include static contexts and motional contexts, corresponding to static content and moving content in a video clip, respectively. The static contexts are well ex-ploited in image semantic segmentation by learning multi-scale and global/long-range features. The motional con-texts are studied in previous video semantic segmentation.
However, there is no research about how to simultane-ously learn static and motional contexts which are highly correlated and complementary to each other. To address this problem, we propose a Coarse-to-Fine Feature Min-ing (CFFM) technique to learn a uniﬁed presentation of static contexts and motional contexts. This technique con-sists of two parts: coarse-to-ﬁne feature assembling and cross-frame feature mining. The former operation prepares data for further processing, enabling the subsequent joint learning of static and motional contexts. The latter opera-tion mines useful information/contexts from the sequential frames to enhance the video contexts of the features of the target frame. The enhanced features can be directly ap-plied for the ﬁnal prediction. Experimental results on popu-lar benchmarks demonstrate that the proposed CFFM per-forms favorably against state-of-the-art methods for video semantic segmentation. Our implementation is available at https://github.com/GuoleiSun/VSS-CFFM. 1.

Introduction
Semantic segmentation aims at assigning a semantic la-bel to each pixel in a natural image, which is a fundamen-tal and hot topic in the computer vision community. It has wide range of applications in both academic and industrial
ﬁelds. Thanks to the powerful representation capability of deep neural networks [28, 39, 71, 73] and large-scale im-age datasets [3, 14, 20, 59, 102], tremendous achievements have been seen for image semantic segmentation. How-*The corresponding author: yun.liu@vision.ee.ethz.ch
Figure 1. Illustration of static contexts (in blue) and motional con-texts (in red) across neighbouring video frames. The human and horse are moving objects, while the grassland and sky are static background. Note that the static stuff is helpful for the recognition of moving objects, i.e., a human is riding a horse on the grassland. ever, video semantic segmentation has not been witnessed such tremendous progress [23, 35, 53, 60] due to the lack of large-scale datasets. For example, Cityscapes [14] and
NYUDv2 [70] datasets only annotate one or several nonad-jacent frames in a video clip. CamVid [2] only has a small scale and a low frame rate. The real world is actually dy-namic rather than static, so the research on video semantic segmentation (VSS) is necessary. Fortunately, the recent es-tablishment of the large-scale video segmentation dataset,
VSPW [58], solves the problem of video data scarcity. This inspires us to denoting our efforts to VSS.
As widely accepted, the contextual information plays a central role in image semantic segmentation [5–7, 9, 16, 27, 33, 36, 37, 45, 50, 88, 90, 93, 96, 98, 99, 103, 107]. When considering videos, the contextual information is twofold: static contexts and motional contexts, as shown in Fig. 1.
The former refers to the contexts within the same video frame or the contexts of unchanged content across differ-ent frames.
Image semantic segmentation has exploited such contexts (for images) a lot, mainly accounting for multi-scale [6, 7, 9, 88] and global/long-range information
[33, 96, 98, 107]. Such information is essential not only for understanding the static scene but also for perceiving the holistic environment of videos. The latter, also known as temporal information, is responsible for better parsing moving object/stuff and capturing more effective scene rep-resentations with the help of motions. The motional context
learning has been widely studied in video semantic segmen-tation [4, 23, 32, 34, 35, 42, 47, 53, 54, 60, 69, 85, 105], which usually relies on optical ﬂows [19] to model motional con-texts, ignoring the static contexts. Although each single as-pect, i.e., static or motional contexts, has been well studied, how to learn static and motional contexts simultaneously deserves more attention, which is important for VSS.
Furthermore, static contexts and motional contexts are highly correlated, not isolated, because both contexts are complementary to each other to represent a video clip.
Therefore, the ideal solution for VSS is to jointly learn static and motional contexts, i.e., generating a uniﬁed repre-sentation of static and motional contexts. A na¨ıve solution is to apply recent popular self-attention [18, 76, 80] by tak-ing feature vectors at all pixels in neighboring frames as to-kens. This can directly model global relationships of all to-kens, of course including both static and motional contexts.
However, this na¨ıve solution has some obvious drawbacks.
For example, it is super inefﬁcient due to the large num-ber of tokens/pixels in a video clip, making this na¨ıve solu-tion unrealistic. It also contains too much redundant com-putation because most content in a video clip usually does not change much and it is unnecessary to compute atten-tion for the repeated content. Moreover, the too long length of tokens would affect the performance of self-attention, as shown in [12,21,29,56,84] where the reduction of the token length through downsampling leads to better performance.
More discussion about why traditional self-attention is in-appropriate for video context learning can be found in §3.1.
In this paper, we propose a new Coarse-to-Fine Feature
Mining (CFFM) technique, which consists of two parts: coarse-to-ﬁne feature assembling and cross-frame feature mining. Speciﬁcally, we ﬁrst apply a lightweight deep net-work [83] to extract features from each frame. Then, we as-semble the extracted features from neighbouring frames in a coarse-to-ﬁne manner. Here, we use a larger receptive ﬁeld and a more coarse pooling if the frame is more distant from the target. This feature assembling operation has two mean-ings. On one hand, it organizes the features in a multi-scale way, and the farthest frame would have the largest receptive
ﬁeld and the most coarse pooling. Since the content in a few sequential frames usually does not change suddenly and most content may only have a little temporal inconsistency, this operation is expected to prepare data for learning static contexts. On the other hand, this feature assembling oper-ation enables a large perception region for remote frames because the moving objects may appear in a large region for remote frames. This makes it suitable for learning mo-tional contexts. At last, with the assembled features, we use the cross-frame feature mining technique to iteratively mine useful information from neighbouring frames for the target frame. This mining technique is a specially-designed non-self attention mechanism that has two different inputs, unlike commonly-used self-attention that only has one in-put [18, 76]. The output features enhanced by the CFFM can be directly used for the ﬁnal prediction. We describe the technical motivations for CFFM in detail in §3.1.
The advantages of this new video context learning mech-anism are four-fold. (1) The proposed CFFM technique can learn a uniﬁed representation of static contexts and motional contexts, both of which are of vital importance for VSS. (2)
The CFFM technique can be added on top of frame fea-ture extraction backbones to generate powerful video con-textual features, with low complexity and limited compu-(3) Without bells and whistles, we achieve tational cost. state-of-the-art results for VSS on standard benchmarks by using the CFFM module. (4) The CFFM technique has the potential to be extended to improve other video recognition tasks that need powerful video contexts. 2.