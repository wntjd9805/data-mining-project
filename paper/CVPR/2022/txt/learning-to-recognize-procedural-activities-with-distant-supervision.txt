Abstract
In this paper we consider the problem of classifying fine-grained, multi-step activities (e.g., cooking different recipes, making disparate home improvements, creating various forms of arts and crafts) from long videos spanning up to several minutes. Accurately categorizing these activ-ities requires not only recognizing the individual steps that compose the task but also capturing their temporal depen-dencies. This problem is dramatically different from tradi-tional action classification, where models are typically op-timized on videos that span only a few seconds and that are manually trimmed to contain simple atomic actions.
While step annotations could enable the training of mod-els to recognize the individual steps of procedural activi-ties, existing large-scale datasets in this area do not include such segment labels due to the prohibitive cost of manu-ally annotating temporal boundaries in long videos. To ad-dress this issue, we propose to automatically identify steps in instructional videos by leveraging the distant supervi-sion of a textual knowledge base (wikiHow) that includes detailed descriptions of the steps needed for the execution of a wide variety of complex activities. Our method uses a language model to match noisy, automatically-transcribed speech from the video to step descriptions in the knowl-edge base. We demonstrate that video models trained to recognize these automatically-labeled steps (without man-ual supervision) yield a representation that achieves supe-rior generalization performance on four downstream tasks: recognition of procedural activities, step classification, step forecasting and egocentric video classification. 1.

Introduction
Imagine being in your kitchen, engaged in the prepara-tion of a sophisticated dish that involves a sequence of com-plex steps. Fortunately, your J.A.R.V.I.S.1 comes to your rescue. It actively recognizes the task that you are trying to accomplish and guides you step-by-step in the successful
*Research done while XL was an intern at Facebook AI Research. 1A fictional AI assistant in the Marvel Cinematic Universe. execution of the recipe. The dramatic progress witnessed in activity recognition [9,11,51,53] over the last few years has certainly made these fictional scenarios a bit closer to real-ity. Yet, it is clear that in order to attain these goals we must extend existing systems beyond atomic-action classification in trimmed clips to tackle the more challenging problem of understanding procedural activities in long videos spanning several minutes. Furthermore, in order to classify the proce-dural activity, the system must not only recognize the indi-vidual semantic steps in the long video but also model their temporal relations, since many complex activities share sev-eral steps but may differ in the order in which these steps appear or are interleaved. For example, “beating eggs” is a common step in many recipes which, however, are likely to differ in the preceding and subsequent steps.
In recent years, the research community has engaged in the creation of several manually-annotated video datasets for the recognition of procedural, multi-step activities.
However, in order to make detailed manual annotations pos-sible at the level of both segments (step labels) and videos (task labels), these datasets have been constrained to have a narrow scope or a relatively small scale. Examples include video benchmarks that focus on specific domains, such as recipe preparation or kitchen activities [11, 27, 40, 62], as well as collections of instructional videos manually-labeled for step and task recognition [50, 63]. Due to the large cost of manually annotating temporal boundaries, these datasets have been limited to a small size both in terms of number of tasks (about a few hundreds activities at most) as well as amount of video examples (about 10K samples, for roughly 400 hours of video). While these benchmarks have driven early progress in this field, their limited size and narrow scope prevent the training of modern large-capacity video models for recognition of general procedural activities. the
On the other end of the scale/scope spectrum,
HowTo100M dataset [34] stands out as an exceptional re-source. It is over 3 orders of magnitude bigger than prior benchmarks in this area along several dimensions: it in-cludes over 100M clips showing humans performing and narrating more than 23,000 complex tasks for a total dura-tion of 134K hours of video. The downside of this massive
amount of data is that its scale effectively prevents manual annotation. In fact, all videos in HowTo100M are unverified by human annotators. While this benchmark clearly ful-fills the size and scope requirements needed to train large-capacity video models, its lack of segment annotations and the unvalidated nature of the videos impedes the training of accurate step or task classifiers.
In this paper we present a novel approach for train-ing models to recognize procedural steps in instructional video without any form of manual annotation, thus en-abling optimization on large-scale unlabeled datasets, such as HowTo100M. We propose a distant supervision frame-work that leverages a textual knowledge base as a guid-ance to automatically identify segments corresponding to different procedural steps in video. Distant supervision has been used in Natural Language Processing [35, 39, 42] to mine relational examples from noisy text corpora using a knowledge base. In our setting, we are also aiming at re-lation extraction, albeit in the specific setting of identifying video segments relating to semantic steps. The knowledge base that we use is wikiHow [2]—a crowdsourced multi-media repository containing over 230,000 “how-to” articles describing and illustrating steps, tips, warnings and require-ments to accomplish a wide variety of tasks. Our system uses language models to compare segments of narration au-tomatically transcribed from the videos to the textual de-scriptions of steps in wikiHow. The matched step descrip-tions serve as distant supervision to train a video under-standing model to learn step-level representations. Thus, our system uses the knowledge base to mine step examples from the noisy, large-scale unlabeled video dataset. To the best of our knowledge, this is the first attempt at learning a step video representation with distant supervision.
We demonstrate that video models trained to recognize these pseudo-labeled steps in a massive corpus of instruc-tional videos, provide a general video representation trans-ferring effectively to four different downstream tasks on new datasets. Specifically, we show that we can apply our model to represent a long video as a sequence of step em-beddings extracted from the individual segments. Then, a shallow sequence model (a single Transformer layer [52]) is trained on top of this sequence of embeddings to perform temporal reasoning over the step embeddings. Our exper-iments show that such an approach yields state-of-the-art results for classification of procedural tasks on the labeled
COIN dataset, outperforming the best reported numbers in the literature by more than 16%. Furthermore, we use this testbed to make additional insightful observations: 1. Step labels assigned with our distant supervision framework yield better downstream results than those obtained by using the unverified task labels of
HowTo100M. 2. Our distantly-supervised video representation outper-forms fully-supervised video features trained with ac-tion labels on the large-scale Kinetics-400 dataset [9]. 3. Our step assignment procedure produces better down-stream results than a representation learned by directly matching video to the ASR narration [33], thus show-ing the value of the distant supervision framework.
We also evaluate the performance of our system for classification of procedural activities on the Breakfast transfer learn-Furthermore, we present dataset [27]. ing results on three additional downstream tasks on datasets different from that used to learn our representa-tion (HowTo100M): step classification and step forecast-ing on COIN, as well as categorization of egocentric videos on EPIC-KITCHENS-100 [10]. On all of these tasks, our distantly-supervised representation achieves higher accu-racy than previous works, as well as additional baselines that we implement based on training with full supervision.
These results provide further evidence of the generality and effectiveness of our unsupervised representation for under-standing complex procedural activities in videos. We will release the code and the automatic annotations provided by our distant supervision2. 2.