Abstract
Defocus deblurring is a challenging task due to the spa-tially varying nature of defocus blur. While deep learning approach shows great promise in solving image restoration problems, defocus deblurring demands accurate training data that consists of all-in-focus and defocus image pairs, which is difficult to collect. Naive two-shot capturing can-not achieve pixel-wise correspondence between the defo-cused and all-in-focus image pairs. Synthetic aperture of light fields is suggested to be a more reliable way to gener-ate accurate image pairs. However, the defocus blur gen-erated from light field data is different from that of the im-ages captured with a traditional digital camera. In this pa-per, we propose a novel deep defocus deblurring network that leverages the strength and overcomes the shortcoming of light fields. We first train the network on a light field-generated dataset for its highly accurate image correspon-dence. Then, we fine-tune the network using feature loss on another dataset collected by the two-shot method to allevi-ate the differences between the defocus blur exists in the two domains. This strategy is proved to be highly effective and able to achieve the state-of-the-art performance both quan-titatively and qualitatively on multiple test sets. Extensive ablation studies have been conducted to analyze the effect of each network module to the final performance. 1.

Introduction
The use of large camera aperture can increase the luminous flux so that the image can be captured with a shorter ex-posure time. However, this also reduces the depth of field (DOF) - only points near the focal plane can be captured sharply, while a point far from the focal plane will cast to the camera sensor, instead of a single image point, a spot called the Circle of Confusion (COC) [25] and result in defocus blur. Shallow DOF is sometimes an aesthetic effect pur-sued sedulously by the photographer [7, 30], but it may also
∗ denotes equal contribution and † denotes corresponding author.
Figure 1. Cross correlation between defocused and all-in-focus image pair in DPDD [2] (top) and LFDOF dataset [29] (bottom). degrade important visual information. Thus, restoring an all-in-focus image from its defocused version is highly de-manded to reveal the latent information and benefit to arti-ficial intelligence applications such as object detection [26] and text recognition [19]. Despite its great potential, de-focus deblurring remains a challenging problem due to its spatially varying nature - every point has its own diame-ter of COC depending on the depth of the corresponding scene point. Besides, the shape of COC varies with respect to the relative position from the optical axis. To address defocus blur, the most intuitive way is to first estimate the blur kernel for each pixel, then apply non-blind deconvolu-tion [5, 12, 15, 17, 24, 32]. However, both steps have limita-tions. First, blur kernel estimation is not accurate and is of-ten based on simple Gaussian [12, 15, 24] or disk kernel [5] assumption. Second, deconvolution tends to introduce ring-ing artifact on edges due to the Gibbs phenomenon [41] even if an accurate blur kernel is given.
Recently, researchers have adopted end-to-end deep neu-ral networks to directly restore sharp images from defocus blur [2, 16, 36], which largely outperform the conventional two-step approaches in terms of performance and efficiency.
These networks are all trained on a dataset called Dual-Pixel
Defocus Deblurring (DPDD) [2] which is captured sequen-tially with different aperture sizes to attain defocused and all-in-focus image pairs. However, it is hardly possible to capture defocused and all-in-focus pairs with accurate cor-respondence in two shots, especially for outdoor scenes due to moving objects (e.g., plants, cars) and illuminance varia-tion. To this end, another dataset LFDOF [29] is built utiliz-ing the benefit of light field refocusing and synthetic aper-ture to generate a large number of defocused images with a variety of DOFs and focal distances from a single light field sample. To examine the consistency among the image pairs, we select similar scenes from the two datasets and calculate the cross correlation between defocused and all-in-focus pairs. As shown in Fig 1, LFDOF has strong cross correlation in the sharp regions, whereas DPDD does not hold consistence even at the sharp regions (the tree trunk is in focus in both defocused and all-in-focus images). How-ever, despite good pixel-wise consistency of LFDOF, the defocus blur generated from light field data is not the same as that captured with a conventional digital camera (see Sec. 3). In this paper, we intend to make full use of the advan-tages of LFDOF and DPDD datasets to train a deep network for defocus deblurring.
In summary, the contributions of this paper are as follows:
• We analyze the characteristics of two defocus blur datasets LFDOF and DPDD and develop a novel train-ing strategy for single image defocus deblurring. We also estimate and compare the Point Spread Function (PSF) of light field generated defocus blur against that captured with a conventional digital camera.
• We propose an end-to-end network architecture equipped with a novel dynamic residual block to re-construct the sharp image in a coarse-to-fine manner.
• We conduct extensive experiments to evaluate the ef-fect of each network module and demonstrate the state-of-the-art performance quantitatively and qualitatively on multiple test sets. 2.