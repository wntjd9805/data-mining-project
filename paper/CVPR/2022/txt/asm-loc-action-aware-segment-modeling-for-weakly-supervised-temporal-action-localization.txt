Abstract
Weakly-supervised temporal action localization aims to recognize and localize action segments in untrimmed videos given only video-level action labels for training. Without the boundary information of action segments, existing meth-ods mostly rely on multiple instance learning (MIL), where the predictions of unlabeled instances (i.e., video snippets) are supervised by classifying labeled bags (i.e., untrimmed videos). However, this formulation typically treats snippets in a video as independent instances, ignoring the underly-ing temporal structures within and across action segments.
To address this problem, we propose ASM-Loc, a novel
WTAL framework that enables explicit, action-aware seg-ment modeling beyond standard MIL-based methods. Our framework entails three segment-centric components: (i) dynamic segment sampling for compensating the contribu-tion of short actions; (ii) intra- and inter-segment attention for modeling action dynamics and capturing temporal de-pendencies; (iii) pseudo instance-level supervision for im-proving action boundary prediction. Furthermore, a multi-step refinement strategy is proposed to progressively im-prove action proposals along the model training process.
Extensive experiments on THUMOS-14 and ActivityNet-v1.3 demonstrate the effectiveness of our approach, estab-lishing new state of the art on both datasets. The code and models are publicly available at https://github. com/boheumd/ASM-Loc. 1.

Introduction action
Weakly-supervised localization temporal (WTAL) has attracted increasing attention in recent years. Unlike its fully-supervised counterpart, WTAL only requires action category annotation at the video level, which is much easier to collect and more scalable for building large-scale datasets. To tackle this problem, recent works [1–12] mostly rely on the multiple instance learning (MIL) framework [13], where the entire untrimmed video is treated as a labeled bag containing multiple unlabeled
Figure 1. Action-aware segment modeling for WTAL. Our
ASM-Loc leverages the action proposals as well as the proposed segment-centric modules to address the common failures in exist-ing MIL-based methods. instances (i.e., video frames or snippets). The action clas-sification scores of individual snippets are first generated to form the temporal class activation sequences (CAS) and then aggregated by a top-k mean mechanism to obtain the final video-level prediction [3, 6, 8, 14].
While significant improvement has been made in prior work, there is still a huge performance gap between the weakly-supervised and fully-supervised settings. One ma-jor challenge is localization completeness, where the mod-els tend to generate incomplete or over-complete action seg-ments due to the inaccurate predictions of action bound-aries. Another challenge is the missed detection of short action segments, where the models are biased towards seg-ments with longer duration and produce low-confidence predictions on short actions. Figure 1 demonstrates an ex-ample of these two common errors. Although these chal-lenges are inherently difficult due to the lack of segment-level annotation, we argue that the absence of segment-based modeling in existing MIL-based methods is a key rea-son for the inferior results. In particular, these MIL-based
methods treat snippets in a video as independent instances, where their underlying temporal structures are neglected in either the feature modeling or prediction stage.
In this paper, we propose a novel framework that en-ables explicit, action-aware segment modeling for weakly-supervised temporal action localization, which we term
ASM-Loc. To bootstrap segment modeling, we first gen-erate action proposals using the standard MIL-based meth-ods. These proposals provide an initial estimation of the action locations in the untrimmed video as well as their du-ration. Based on the action proposals, we introduce three segment-centric modules that correspond to the three stages of a WTAL pipeline, i.e., the feature extraction stage, the feature modeling stage and the prediction stage.
First, a dynamic segment sampling module is proposed to balance the contribution of short-range and long-range action segments. As shown in Figure 1, action proposals with short duration are up-sampled along the temporal di-mension, with the scale-up ratios dynamically computed ac-cording to the length of the proposals. Second, intra- and inter-segment attention modules are presented to capture the temporal structures within and across action segments at the feature modeling stage. Specifically, the intra-segment attention module utilizes self-attention within action pro-posals to model action dynamics and better discriminate foreground and background snippets. On the other hand, the inter-segment attention module utilizes self-attention across different actions proposals to capture the relation-ships, facilitating the localization of action segments that involve temporal dependencies (e.g., “CricketBowling” is followed by “CricketShotting” in Figure 1). Note that both attention modules are segment-centric, which is critical to suppress the negative impact of noisy background snippets in untrimmed videos. Third, a pseudo instance-level loss is introduced to refine the localization result by providing fine-grained supervision. The pseudo instance-level labels are derived from the action proposals, coupled with uncer-tainty estimation scores that mitigate the label noise effects.
Finally, a multi-step proposal refinement is adopted to pro-gressively improve the quality of action proposals, which in turn boosts the localization performance of our final model.
We summarize our main contributions as follows:
• We show that segment-based modeling can be utilized to narrow the performance gap between the weakly-supervised and supervised settings, which has been ne-glected in prior MIL-based WTAL methods.
• We introduce three novel segment-centric modules that enable action-aware segment modeling in different stages of a WTAL pipeline.
• We provide extensive experiments to demonstrate the effectiveness of each component of our design. Our
ASM-Loc establishes new state of the art on both
THUMOS-14 and ActivityNet-v1.3 datasets. 2.