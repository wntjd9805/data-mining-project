Abstract 1.

Introduction
This paper presents new hierarchically cascaded trans-formers that can improve data efficiency through attribute surrogates learning and spectral tokens pooling. Vision transformers have recently been thought of as a promis-ing alternative to convolutional neural networks for vi-sual recognition. But when there is no sufficient data, it gets stuck in overfitting and shows inferior performance.
To improve data efficiency, we propose hierarchically cas-caded transformers that exploit intrinsic image structures through spectral tokens pooling and optimize the learnable parameters through latent attribute surrogates. The intrin-sic image structure is utilized to reduce the ambiguity be-tween foreground content and background noise by spec-tral tokens pooling. And the attribute surrogate learning scheme is designed to benefit from the rich visual infor-mation in image-label pairs instead of simple visual con-cepts assigned by their labels. Our Hierarchically Cas-caded Transformers, called HCTransformers, is built upon a self-supervised learning framework DINO and is tested on several popular few-shot learning benchmarks.
In the inductive setting, HCTransformers surpass the
DINO baseline by a large margin of 9.7% 5-way 1-shot accuracy and 9.17% 5-way 5-shot accuracy on miniImageNet, which demonstrates HCTransformers are efficient to extract discriminative features. Also, HCTrans-formers show clear advantages over SOTA few-shot clas-sification methods in both 5-way 1-shot and 5-way 5-shot including settings on four popular benchmark datasets, miniImageNet, tieredImageNet, FC100, and CIFAR-FS.
The trained weights and codes are available at https:
//github.com/StomachCold/HCTransformers.
*Corresponding author:wfge@fudan.edu.cn
Few-shot learning [16, 33, 53] refers to the problem of learning from a very small amount of labeled data, which is expected to reduce the labeling cost, achieve a low-cost and quick model deployment, and shrink the gap between human intelligence and machine models. The key prob-lem of few-shot learning is how to efficiently learn from the rich information hidden in annotated data. Inspired by the part-whole hierarchical concepts used in GraphFPN [71] and GLOM [21], part layout information of objects/scenes contains various visual information. If it can be embedded in vision transformers to guide the feature learning, we will get discriminative feature representations. Meanwhile, to avoid the concentration of visual information on single con-cepts, we need to expand the hidden information of image labels into a much more general semantic representation.
Then how to mine such latent information and generate a complete description of visual concepts becomes important.
In this paper, we aim to improve the data efficiency in
ViT [13] for few-shot image classification. To be specific, we design a meta feature extractor composed of three con-secutively cascaded transformers, each of which models the dependency of image regions at different semantic levels.
The output tokens of a previous transformer are passed to a spectral tokens pooling layer to produce the input tokens for the subsequent. The spectral tokens pooling is partly based on spectral clustering [38, 64], where features of to-kens within the same clusters are averaged to generate new token descriptors for the subsequent transformer. The moti-vation behind the spectral tokens pooling is to bring the im-age segmentation hierarchy into transformers. That means when the transformer performs self-attention, it needs to consider the image layout not simply through the positional embedding, but from the semantic relationship of different image regions. In our implementation, each token can be thought to represent some specific region within an image.
We treat every token as a vertex in a graph and the token similarity matrix describes the edge connectivity. Thus, the spectral tokens pooling becomes an image segmentation problem and can be solved efficiently as that in normalized cut [49].
In practice, we insert two spectral tokens pooling lay-ers between three transformers. Since they capture the se-mantic dependencies of tokens in different hierarchies, we call them Hierarchically Cascaded Transformers (written as
HCTransformers). Besides, we don’t utilize the supervision information directly as that in other state-of-the-art few-shot learning methods [11, 12, 18, 58, 73]. Instead, we introduce a latent attribute surrogates learning scheme to learn robust representations of visual concepts. We hallucinate some la-tent semantic surrogates for each class to guide the learning of deep models. The latent semantic surrogates also have learnable parameters that can be jointly learned with the parameters of transformers end-to-end. In fact, it’s a kind of weakly supervised learning by generalizing image-level annotation into attribute-level supervision. Based on such a latent attribute surrogate learning scheme, we avoid di-rectly mapping an image into a single visual concept from a predefined set of object categories.
The contributions of this paper are as follows:
• We employ ViT as the meta feature extractor for few-shot learning and propose Hierarchically Cascaded
Transformers (HCTransformers), which greatly im-prove the data efficiency through attribute surrogates learning and spectral tokens pooling.
• We propose a latent supervision propagation scheme for transformers in a weakly supervised manner.
It converts the image label prediction task into a latent attribute surrogates learning problem.
In this way, both the class and patch tokens can be supervised effi-ciently.
• We introduce a novel spectral tokens pooling scheme to transformers. It models the dependency relationship of image regions in both the spatial layout and the se-mantic relationship. Due to such a mechanism, ViT can learn much more discriminative features at differ-ent semantic hierarchies.
• Experiments demonstrate that our HCTransformers surpass its DINO baseline on miniImageNet [53], and outperform other state-of-the-art algorithms signifi-cantly on multiple few-shot learning benchmarks, in-cluding miniImageNet [53], tieredImageNet [46], and
CIFAR-FS [4] and FC-100 [39]. 2.