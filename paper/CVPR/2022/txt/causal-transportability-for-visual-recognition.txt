Abstract
Visual representations underlie object recognition tasks, but they often contain both robust and non-robust features.
Our main observation is that image classifiers may perform poorly on out-of-distribution samples because spurious cor-relations between non-robust features and labels can be changed in a new environment. By analyzing procedures for out-of-distribution generalization with a causal graph, we show that standard classifiers fail because the association between images and labels is not transportable across set-tings. However, we then show that the causal effect, which severs all sources of confounding, remains invariant across domains. This motivates us to develop an algorithm to es-timate the causal effect for image classification, which is transportable (i.e., invariant) across source and target envi-ronments. Without observing additional variables, we show that we can derive an estimand for the causal effect under empirical assumptions using representations in deep mod-els as proxies. Theoretical analysis, empirical results, and visualizations show that our approach captures causal in-variances and improves overall generalization. 1.

Introduction
Visual representations underlie most object recognition systems today [17,18,31,46]. By learning from large image datasets, convolutional networks have been able to create excellent visual representations that improve many down-stream image classification tasks [17,18,33]. However, cen-tral to this framework is the need to generalize to new visual distributions at inference time [2, 3, 6, 12, 22, 26, 44, 47, 61].
The most popular technique to use representations is to fine-tune the backbone model or fit a linear model on the tar-get classification task [31]. Although this approach is effec-tive on in-distribution benchmarks, the resulting classifier also inherits the biases from the target dataset. Given the nature of how data is collected, essentially every realistic
*Equal Contribution. image dataset will have spurious features, which will impact the generalization of computer vision systems. Specifically, the learned representation will encode features that corre-spond to spurious correlations found in the training data.
In this paper, we investigate visual representations for object recognition through the lenses of causality [7,42,43].
Specifically, we will revisit the out-of-distribution image classification task through causal-transportability language
[9, 11, 19], which will allow us to formally model both con-founding and structural invariances shared across disparate environments. In our context, we will show how different environments select a distinct set of robust and non-robust features in constructing the input dataset. The training en-vironment may tend to select specific nuisances with the given category, creating spurious correlations between the nuisances and the predicted class.
In fact, standard clas-sifiers will tend to use those spurious correlations, which analytically explains why they result in poor generalization performance to novel target distributions [25, 49, 55].
First, we will show that the association between image and label is not in generalizable (in causal language, trans-portable) across domains. We then note that the causal ef-fect from the input to the output, which severs any spuri-ous correlations, is invariant when the environment changes with respect to the features’ distributions. This motivates us to pursue to an image classification strategy that will leverage causal effects, instead of merely the association, and will act as an anchor, providing stability across chang-ing conditions and allowing extrapolation to more likely succeed. Getting the causal effect for natural images is challenging because there are innumerable unobserved con-founding factors within realistic data. Under some rela-tively mild assumptions, we will be able to extract the robust features from observational data through both causal and deep representations [8, 14, 20, 34–36, 48], and then use the representations as proxies for identifying the causal effect without requiring observations of the confounding factors.
For both supervised and self-supervised representations, our experimental results show that incorporating the causal structure improves performance when generalizing to new
domains. Our method is compatible with many existing rep-resentations without requiring re-training, making the ap-proach effective to deploy in practice. Compared to the standard techniques to use representations, our causally motivated approach can obtain significant gain on CM-NIST (up to 40% gain), WaterBird (up to 25% gain),
ImageNet-Sketch (up to 8% gain), and ImageNet-Rendition (up to 7%) datasets. Our work illustrates the importance of causal quantities in out-of-distribution image classifica-tion and proposes an effective empirical method that allows the learning of a classifier robust to domain change. Our code is available at https://github.com/cvlab-columbia/CT4Recognition. 2.