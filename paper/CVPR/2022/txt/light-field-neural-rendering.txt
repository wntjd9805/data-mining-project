Abstract
Classical light ﬁeld rendering for novel view synthesis can accurately reproduce view-dependent effects such as re-ﬂection, refraction, and translucency, but requires a dense view sampling of the scene. Methods based on geometric re-construction need only sparse views, but cannot accurately model non-Lambertian effects. We introduce a model that combines the strengths and mitigates the limitations of these two directions. By operating on a four-dimensional repre-sentation of the light ﬁeld, our model learns to represent view-dependent effects accurately. By enforcing geometric constraints during training and inference, the scene geom-etry is implicitly learned from a sparse set of views. Con-cretely, we introduce a two-stage transformer-based model that ﬁrst aggregates features along epipolar lines, then aggregates features along reference views to produce the color of a target ray. Our model outperforms the state-of-the-art on multiple forward-facing and 360◦ datasets, with larger margins on scenes with severe view-dependent vari-ations. Code and results can be found at light-ﬁeld-neural-rendering.github.io. 1.

Introduction
Synthesizing a novel view given a sparse set of images is a long-standing challenge in computer vision and graphics
[10, 41, 42]. Recent advances in 3D neural rendering for view synthesis, in particular NeRF [31] and its successors
[15, 16, 27, 33, 36, 58], have brought us tantalizingly close to the capability of creating photo-realistic images in com-plex environments. One reason for NeRF’s success is its im-plicit 5D scene representation which maps a 3D scene point and 2D viewing direction to opacity and color. In princi-ple, such a representation could be perfectly suited to mod-eling view-dependent effects such as the non-Lambertian reﬂectance of specular and translucent surfaces. However, without regularization, this formulation permits degenerate solutions due to the inherent ambiguity between 3D sur-* Work done while interning at Google.
Figure 1. Novel view synthesis. On top is the target image to be rendered, from the Lab scene in the Shiny dataset [55]. Bottom row shows crops of novel views generated by our proposed model,
NeX [55], and NeRF [31]. Unlike NeX and NeRF that fail to syn-thesize refractions on the test tube, our model almost perfectly re-constructs these complex view-dependent effects. We indicate the
PSNR of the rendered images within parenthesis (higher is better).
Images can be zoomed for detail. face and radiance, where an incorrect shape (opacity) can be coupled with a high-frequency radiance function to min-In practice, NeRF imize the optimization objective [61]. avoids such degenerate solutions through its neural archi-tecture design, where the viewing direction is introduced only in the last layers of the MLP, thereby limiting the ex-pressivity of the radiance function, which effectively trans-lates to a smooth BRDF prior [61]. Thus, NeRF manages to avoid degenerate solutions at the expense of ﬁdelity in non-Lambertian effects (Fig. 1 highlights this particular limita-tion of the NeRF model). Photo-realistic synthesis of non-Lambertian effects is one of the few remaining hurdles for neural rendering techniques.
In this paper, we formulate view synthesis as rendering a
sparsely observed light ﬁeld. The 4D light ﬁeld [26], which measures the radiance along rays in empty space, is often used for view synthesis [6, 24, 26]. Rendering a novel view from a densely sampled light ﬁeld can be achieved with sig-nal processing techniques (e.g., interpolation) and without any model of the 3D geometry, but no such straightfor-ward method exists with sparse light ﬁelds. From sparse images, rendering often utilizes additional 3D geometric constraints, such as predicted depth maps [24, 46], but per-formance is sensitive to accurate depth estimates which are difﬁcult to obtain for non-Lambertian surfaces.
Motivated by these limitations, we introduce a novel method for rendering a sparse light ﬁeld. Our neural render-ing function operates in the style of image based rendering, where a target ray is synthesized using only observed rays from nearby views. In lieu of explicit 3D information, our transformer based rendering function is trained to fuse rays from nearby views exploiting an additional inductive bias in the form of a multi-view geometric constraint, namely the epipolar geometry. As shown in Fig. 1, our model is able to faithfully reconstruct the sharp details and lighting effect in the most challenging scene in the Shiny dataset [55].
Contributions. Our main contribution is the novel light
ﬁeld based neural view synthesis model, capable of pho-torealistic modeling of non-Lambertian effects (e.g., spec-ularities and translucency). To address the core challenge of sparsity of initial views, we leverage an inductive bias in the form of a multi-view geometric constraint, namely the epipolar geometry, and a transformer-based ray fusion.
The resulting model produces higher ﬁdelity renderings for forward-facing as well as 360° captures, compared to state-of-the-art, achieving up to 5 dB improvement in the most challenging scenes. Further, as a byproduct of our design, we can easily obtain dense correspondences and depth with-out further modiﬁcations, as well as transparent visualiza-tion of the rendering process itself. Through ablations we illustrate the importance of our individual design choices. 2.