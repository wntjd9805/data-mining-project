Abstract
In light of the success of contrastive learning in the im-age domain, current self-supervised video representation learning methods usually employ contrastive loss to facil-itate video representation learning. When naively pulling two augmented views of a video closer, the model how-ever tends to learn the common static background as a shortcut but fails to capture the motion information, a phe-nomenon dubbed as background bias. Such bias makes the model suffer from weak generalization ability, lead-ing to worse performance on downstream tasks such as action recognition. To alleviate such bias, we propose
Foreground-background Merging (FAME) to deliberately compose the moving foreground region of the selected video onto the static background of others. Specifically, with-out any off-the-shelf detector, we extract the moving fore-ground out of background regions via the frame differ-ence and color statistics, and shuffle the background re-gions among the videos. By leveraging the semantic con-sistency between the original clips and the fused ones, the model focuses more on the motion patterns and is debi-ased from the background shortcut. Extensive experiments demonstrate that FAME can effectively resist background cheating and thus achieve the state-of-the-art performance on downstream tasks across UCF101, HMDB51, and Div-ing48 datasets. The code and configurations are released at https://github.com/Mark12Ding/FAME. 1.

Introduction
The recent development of deep learning has promoted a series of applications in videos, such as video recogni-tion [14, 50, 56], video retrieval [16, 66], and video object
*Work done during an internship at Tencent AI Lab.
†Corresponding author. Email: xionghongkai@sjtu.edu.cn. (a) Results of vanilla contrastive learning. (b) Results of our approach FAME.
Figure 1. Class-agnostic activation map [3] visualization of impor-tant areas. The heatmap indicates how much the pretrained model attends to the region. Compared to the conventional approach, our method mitigates the background bias significantly. segmentation [11, 28, 69]. While various large-scale bench-marks [1, 5, 18] are the key to those successes, the costly manual annotation involved in fully-supervised methods excludes the potential utilization of millions of uncurated videos on the Internet. To further advance the video-related research, learning video representation in an unsupervised manner is of great significance and emerges as a general trend in the computer vision community.
Recently, unsupervised learning in images [9, 40, 49, 58] has achieved competitive performances compared to their supervised counterparts, especially with the contrastive self-supervised learning formulation [7, 24]. The common idea of contrastive learning is to pull ‘positive’ pairs to-gether in the embedding space and push apart the anchor from ‘negative’ samples. Due to inaccessibility to the la-bel, a positive pair is usually formed by data augmentations of the anchor sample while the negative samples come from other samples. Inspired by these successes, various attempts have also been made in self-supervised video representa-tion learning [15, 44]. However, we find applying vanilla
discovery method extracts dynamic areas on which we ex-pect the model to put emphasis. Then, we fuse the extracted foreground regions of each video with random backgrounds from other videos to form new action samples. In this way, when we force the model to learn the consistent representa-tion between original clips and distracting clips, the model has to learn representations that are sensitive to motion pat-terns and overcome the background cheating. We evalu-ate the proposed FAME on three action recognition bench-marks. The superior experimental performance verifies that
FAME enables self-supervised contrastive video represen-tation learning to generalize better and distill the motion-aware representations. In short, we summarize our contri-butions as follows:
• We demonstrate the background bias caused by vanilla contrastive learning and propose a simple yet effective augmentation method FAME to help the model break background shortcuts and learn motion-aware repre-sentations.
• Our method enhances the conventional contrastive learning without whistle and bell and achieves the start-of-the-art performance among UCF101,
HMDB51, and Diving48 datasets. 2.