Abstract
We present a novel approach for image-animation of a source image by a driving video, both depicting the same type of object. We do not assume the existence of pose models and our method is able to animate arbitrary ob-jects without the knowledge of the object’s structure. Fur-thermore, both, the driving video and the source image are only seen during test-time. Our method is based on a shared mask generator, which separates the foreground ob-ject from its background, and captures the object’s general pose and shape. To control the source of the identity of the output frame, we employ perturbations to interrupt the un-wanted identity information on the driver’s mask. A mask-refinement module then replaces the identity of the driver with the identity of the source. Conditioned on the source image, the transformed mask is then decoded by a multi-scale generator that renders a realistic image, in which the content of the source frame is animated by the pose in the driving video. Due to the lack of fully supervised data, we train on the task of reconstructing frames from the same video the source image is taken from. Our method is shown to greatly outperform the state-of-the-art methods on mul-tiple benchmarks. Our code and samples are available at https://github.com/itsyoavshalev/Image-Animation-with-Perturbed-Masks. 1.

Introduction
The ability to reanimate a still image based on a driving video has been extensively studied in recent years [12, 15, 20]. The developed methods achieve an increased degree of accuracy in both, maintaining the source identity, as ex-tracted from the source frame, and in replicating the motion pattern of the driver’s frame. In addition, the recent meth-ods also show good generalization to unseen identities and are relatively robust, and have fewer artifacts than the older methods. The relative ease with how these methods can be applied out-of-the-box has led to their adoption in various visual effects.
Interestingly, some of the most striking results have been obtained with model-free methods, i.e., that do not rely, for example, on post-extraction models [10, 11, 16, 17, 22, 25].
This indicates that such methods can convincingly disentan-gle shape and identity from motion [7, 13].
There are, however, a few aspects in which such meth-ods still need to improve. First, the generated videos are with noticeable artifacts. Second, some of the identity of the source image is lost and replaced by identity elements from the driving video. Third, the animation of the gener-ated video does not always match the motion in the driver video.
Here, we propose a method that is preferable to the ex-isting work in terms of motion accuracy, identity and back-ground preservation, and quality of the generated video.
Our method relies on a mask-based representation of the driving pose and explicit conditioning on the source fore-ground mask. Source and driver masks are extracted by the same network. The driver mask goes through an additional stage that replaces the identity information in the mask.
The reliance on masks has many advantages. First, it eliminates many of the identity cues from the driving video.
Second, it explicitly models the region that needs to be re-placed in the source image. Third, it is common to both source and driver, thus allowing, with proper augmentation, to train only on source videos. Fourth, it captures a detailed description of the object’s pose and shape.
Interestingly, unlike many of the previous methods, we do not rely on GANs [9] to generate proper outputs from combinations of different inputs.
Instead, we employ an encoder-decoder, in which the identity is manipulated in or-der to direct the networks toward employing specific parts of the information from each input. To summarize, our con-tributions are: (i) An image animation method that gen-eralizes to unseen identities of the same type, and is able to animate arbitrary objects better than previous work; (ii)
Innovative use of perturbations over masks, in order to in-terrupt the driver’s identity, which is then replaced with the source’s identity by the mask refinement module; (iii) A comprehensive evaluation of several different applications, which show a sizable improvement over the current image animation state-of-the-art.
2.