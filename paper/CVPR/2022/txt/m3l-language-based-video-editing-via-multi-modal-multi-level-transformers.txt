Abstract
Video editing tools are widely used nowadays for digi-tal design. Although the demand for these tools is high, the prior knowledge required makes it difficult for novices to get started. Systems that could follow natural language in-structions to perform automatic editing would significantly improve accessibility. This paper introduces the language-based video editing (LBVE) task, which allows the model to edit, guided by text instruction, a source video into a target video. LBVE contains two features: 1) the scenario of the source video is preserved instead of generating a completely different video; 2) the semantic is presented differently in the target video, and all changes are controlled by the given instruction. We propose a Multi-Modal Multi-Level Trans-former (M3L) to carry out LBVE. M3L dynamically learns the correspondence between video perception and language semantic at different levels, which benefits both the video understanding and video frame synthesis. We build three new datasets for evaluation, including two diagnostic and one from natural videos with human-labeled text. Extensive experimental results show that M3L is effective for video editing and that LBVE can lead to a new field toward vision-and-language research. 1.

Introduction
Video is one of the most direct ways to convey informa-tion, as people are used to interacting with this world via dy-namic visual perception. Nowadays, video editing tools like
Premiere and Final Cut are widely applied for digital de-sign usages, such as film editing or video effects. However, those applications require prior knowledge and complex op-erations to utilize successfully, which makes it difficult for novices to get started. For humans, natural language is the most natural way of communication. If a system can follow the given language instructions and automatically perform related editing actions, it will significantly improve accessi-Figure 1. The introduced language-based video editing (LBVE) task. LBVE requires to edit a source video S into the target video
T guided by the instruction X. bility and meet the considerable demand.
In this paper, we introduce language-based video edit-ing (LBVE), a general V2V task, where the target video is controllable directly by language instruction. LBVE treats a video and an instruction as the input, and the tar-get video is edited from the textual description. As illus-trated in Fig. 1, the same person performs different hand gestures guided by the instruction. Different from text-to-video (T2V) [3,34,38,43], video editing enjoys two follow-ing feature: 1) the scenario (e.g., scene or humans) of the source video is preserved instead of generating all content from scratch; 2) the semantic (e.g., property of the object or moving action) is presented differently in the target video.
The main challenge of LBVE is to link the video percep-tion with language understanding and reflect what seman-tics should be manipulated during the video generation but under a similar scenario. People usually take further editing steps onto a base video rather than create all content from the beginning. We believe that our LBVE is more practical and corresponding to human daily usage.
To tackle the LBVE task, we propose a multi-modal multi-level transformer (M3L) to perform video editing conditioning on the guided text. As shown in Fig. 2, M3L contains a multi-modal multi-level Transformer where the encoder models the moving motion to understand the entire video, and the decoder serves as a global planner to generate
each frame of the target video. For better video perception to link with the given instruction, the incorporated multi-level fusion fuses between these two modalities. During encoding, the local-level fusion is applied with the text to-kens for fine-grained visual understanding, and the global-level fusion extracts the key feature of the moving motion.
Reversely, during decoding, we first adopt global-level fu-sion from whole instruction to give a high-level plan for the target video, and then the local-level fusion can further gen-erate each frame in detail with the specific property. With multi-level fusion, M3L learns explicit vision-and-language perception between the video and given instruction, yield-ing better video synthesis.
For evaluation, we collect three datasets under the brand-new LBVE task. There are E-MNIST and E-CLEVR, where we build from hand-written number recognition MNIST
[32] and compositional VQA CLEVR [27], respectively.
Both E-MNIST and E-CLEVR are prepared for evaluating the content replacing (different numbers or shapes and col-ors) and semantic manipulation (different moving directions or related positions). As a new task, diagnostic datasets help analyze the progress and discover the shortcomings. To investigate the capability of LBVE for natural video with open text, E-JESTER is built upon the same person per-forming different hand gestures with human instruction.
Our experimental results show that the multi-modal multi-level transformer (M3L) can carry out the LBVE task, and the multi-level fusion further helps between video per-ception and language understanding in both aspects of con-tent replacing and semantic manipulation. In summary, our contributions are four-fold:
• We introduce the LBVE task to manipulate video content controlled by text instructions.
• We present M3L to perform LBVE, where the multi-level fusion further helps between video perception and lan-guage understanding.
• For evaluation under LBVE, we prepare three new datasets containing two diagnostic and one natural video with human-labeled text.
• Extensive ablation studies show that our M3L is adequate for video editing, and LBVE can lead to a new field to-ward vision-and-language research. 2.