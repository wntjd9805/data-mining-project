Abstract
Saliency detection with light field images is becoming at-tractive given the abundant cues available, however, this comes at the expense of large-scale pixel level annotated
In this paper, we data which is expensive to generate. propose to learn light field saliency from pixel-level noisy labels obtained from unsupervised hand crafted featured-based saliency methods. Given this goal, a natural question is: can we efficiently incorporate the relationships among light field cues while identifying clean labels in a unified framework? We address this question by formulating the learning as a joint optimization of intra light field features fusion stream and inter scenes correlation stream to gen-erate the predictions. Specially, we first introduce a pixel forgetting guided fusion module to mutually enhance the light field features and exploit pixel consistency across it-erations to identify noisy pixels. Next, we introduce a cross scene noise penalty loss for better reflecting latent struc-tures of training data and enabling the learning to be in-variant to noise. Extensive experiments on multiple bench-mark datasets demonstrate the superiority of our framework showing that it learns saliency prediction comparable to state-of-the-art fully supervised light field saliency meth-ods. Our code is available at https://github.com/
OLobbCode/NoiseLF. 1.

Introduction
Saliency detection imitates the human attention mech-anism and allows us to focus on the most visually dis-tinctive regions out of an overwhelming amount of infor-mation. This problem has attracted much attention given the broad applications in computer vision, such as image and video segmentation, visual tracking and robot naviga-tion [7, 13, 51]. Existing saliency detection methods can be roughly divided into three categories based on the 2D (RGB), 3D (RGB-D) and 4D (light field) input images. Un-like the former two, light field provides multi-view images of the scene through an array of lenslets and produces a stack of focal slices, containing abundant spatial parallax
*Equal contribution
†Corresponding author
Figure 1. Saliency detection in challenging light field scenes. (a) all-focus images; (b) pixel-level noisy labels; (c)-(e) results of our method, fully supervised light field method Mo-LF [57] and RGB method F3Net [44]; (f) ground truth saliency maps, depicted only for illustration purposes and not used in our training. information as well as depth information [11,12,24]. More-over, light field data consists of an all-focus central view and a focal stack, where the stack of focal slices (similar to human visual perception) are observed in sequence with a combination of eye movements and shifts in visual atten-tion [33]. Such a comprehensive 4D data provides abundant cues for saliency detection in challenging scenes e.g. sim-ilar foreground and background, small salient objects and complex background, as shown in Figure 1 (a).
Early light field saliency detection works have been dom-inated by fully supervised methods which require large amounts of accurate pixel-level annotations aligned with the all-focus central view for training [31, 40, 56–58]. This expensive and time-consuming labelling process hinders the applicability of fully supervised methods to large scale problems. If the tedious pixel-level annotation process can somehow be avoided, we can exploit the unlimited supply of light field images from hand-held cameras (such as Lytro
Illum [1] and Raytrix [2]) for large scale applications. In this paper, we are interested in learning light field saliency prediction from single per-pixel noisy labels, where the per-pixel noisy labels are produced by existing low cost off-the-shelf conventional unsupervised saliency detection meth-ods. These labels are noisy compared to the ground truth human annotations and can have method-specific bias in predicting the saliency map. In our configuration, for each
light field image in the training data, only a single noisy saliency map is available.
Directly training the light field saliency detection net-work on the pixel-level noisy labels may guide the network to overfit to the corrupted labels [49]. Additionally, previ-ous light field saliency detection methods lack a global per-spective to explore patterns in the relationships between the whole dataset. To effectively leverage these noisy but in-formative saliency maps, we propose a new perspective to the light field saliency detection problem: how to efficiently incorporate the relations among light field cues while iden-tifying clean labels in a unified framework? To this end, we make two major contributions, intra light field features fu-sion and across scenes correlation. Firstly, we introduce a pixel forgetting guided fusion module to explore the interac-tions among all-focus central view image and focal slices, and exploit pixel consistency across training iterations to identify noisy pixels. Specially, we perform the interaction process in a reciprocating fashion, where mutual guidance first emphasizes the useful features and suppresses the un-necessary ones from focal slices using the all-focus central view. Next, the weighted focal stack features are used to gradually refine the spatial information of all-focus central view for accurately identifying salient objects. For the ini-tial noisy estimations of the updated focal stack features and all-focus central view features, we introduce pixel forget-ting event to evolve across the training iterations and define a forgetting matrix to identify noisy pixels. The final pre-diction comprises pixels with high certainty from the initial noisy estimations. Thus we can simultaneously explore the abundant light field cues and identify inliers for our model.
Secondly, we propose a cross-scene noise penalty loss to capture the global correlation of the noise space for better reflecting intrinsic structures of the whole training data to enable more robust saliency predictions. The first term of our cross-scene noise penalty loss evaluates the network’s prediction on training light field images using noisy labels, and the second term is defined on several independent ran-domly selected light field images to penalize the networks from overly agreeing with the pixel-level noisy labels. Both terms encode the knowledge of noise rates implicitly and allow our light field saliency prediction model to become invariant to pixel-level noise.
To the best of our knowledge, this is the first work that proposes the idea of considering light field saliency de-tection as learning from pixel-level noisy labels which is a completely different direction from existing fully super-vised methods. Our main contributions are: (1) We formu-late the saliency prediction as a joint optimization of intra light field features fusion stream and inter scenes correla-tion stream. (2) We introduce a pixel forgetting guided fu-sion module to mutually enhance the light field features and exploit pixel consistency across iterations to identify noisy (3) We propose a cross-scene noise penalty pixel labels. loss for better reflecting latent structures of training data and enabling the learning to be invariant to label noises.
We perform thorough experimental evaluations of the pro-posed model, which achieves comparable performance with state-of-the-art fully supervised light field saliency predic-tion methods. 2.