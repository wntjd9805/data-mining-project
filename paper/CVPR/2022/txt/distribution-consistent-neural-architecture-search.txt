Abstract
Recent progress on neural architecture search (NAS) has demonstrated exciting results on automating deep network architecture designs. In order to overcome the unafford-able complexity of training each candidate architecture from scratch, the state-of-the-art one-shot NAS approaches adopt a weight-sharing strategy to improve training efficiency. Al-though the computational cost is greatly reduced, such one-shot process introduces a severe weight coupling problem that largely degrades the evaluation accuracy of each can-didate. The existing approaches often address the problem by shrinking the search space, model distillation, or few-shot training. Instead, in this paper, we propose a novel distribution consistent one-shot neural architecture search algorithm. We first theoretically investigate how the weight coupling problem affects the network searching performance from a parameter distribution perspective, and then pro-pose a novel supernet training strategy with a Distribution
Consistent Constraint that can provide a good measurement for the extent to which two architectures can share weights.
Our strategy optimizes the supernet through iteratively in-ferring network weights and corresponding local sharing states. Such joint optimization of supernet’s weights and topologies can diminish the discrepancy between the weights inherited from the supernet and the ones that are trained with a stand-alone model. As a result, it enables a more accu-rate model evaluation phase and leads to a better searching performance. We conduct extensive experiments on bench-mark datasets with multiple searching spaces. The resulting architecture achieves superior performance over the cur-rent state-of-the-art NAS algorithms with comparable search costs, which demonstrates the efficacy of our approach. 1.

Introduction
Neural architecture search (NAS) has drawn massive re-search attention due to its efficacy in automating architecture
*Corresponding author.
Figure 1. Comparison with state-of-the-art methods on ImageNet. engineering. A line of NAS algorithms have been success-fully applied in the image classification [14, 18, 21, 41, 42] and other related fields (e.g., object detection [4, 35], seg-mentation [20, 39]). Among manys, early NAS approaches struggle to solve the weight optimization and automated ar-chitecture engineering problems in a nested manner. A large number of candidate architectures are sampled and trained from scratch, and the computation cost is thus unaffordable on large datasets.
Recent research hotspot lies in the one-shot NAS algo-rithms with an additional weight-sharing supernet for archi-tecture performance evaluation. A supernet that contains all candidate architectures is trained only once. Each archi-tecture inherits its weights from the supernet. After that, a search strategy (e.g., reinforcement learning, evolutionary algorithms, etc.) is applied to select the best-performing sub-architecture. The computation cost is thus greatly re-duced. Though promising results have been achieved, this global weight-sharing mechanism introduced by the one-shot algorithms leads to severe weight coupling problem
[11, 14, 34], and results in at least two limitations. First of all, sub-architectures with shared weights influence each other during the supernet training process. The same op-erator in different architectures may have different or even
opposite gradient directions. The gradient descent direction for one architecture may be the gradient ascent direction for another one. The global weight sharing will lead to a zigzag optimization process. Second, operators with high
FLOPs are less frequently sampled than others under a pre-defined latency constraint. Architectures with such operators are usually insufficiently trained. Both limitations would cause non-trivial parameter distribution variance between the model trained from scratch and that inherited from the supernet, leading to inaccurate architecture evaluation.
Several recent works are proposed to address such a prob-lem from various perspectives, including search space nar-rowing [21, 27], knowledge distillation [18, 26, 38], few-shot supernet training [40], to name a few. Though promising, these methods are designed to improve the evaluation per-formance of the candidate sub-architectures in an empirical way, but the underlying reason, i.e., the distribution gap be-tween the weights inherited from supernet and the weights trained with stand-alone networks, has not been touched and properly addressed.
In this paper, we make the first attempt to theoretically analyze how weight sharing affects such a distribution gap.
We manage to prove that the distribution gap is actually determined by the accumulated likelihood probability of each candidate architecture and the joint likelihood of pair-wise architectures (i.e., any two architectures with shared weights). However, previous NAS algorithms only take into account the first likelihood probability for network training.
Therefore, we propose a new supernet training metric with a distribution consistency constraint that considers both of the two likelihoods. It leads to two iterative sub-processes when training supernet, i.e., optimizing the supernet weights and inferring the local weight sharing states. This enables us to simultaneously supervise the supernet training and shrink the network parameter distribution gap.
However, the above optimizing process is intractable since the computational cost for the joint likelihood of pair-wise networks is unaffordable and the solution space of local weight sharing states is too huge. Therefore, we innovatively propose a layer-wise optimization strategy with a clustering mechanism to avoid the computation for joint likelihood of all candidate architecture pairs and restrain the space of weight sharing states. The clustering algorithm is performed on network’s architecture in a self-supervised manner and we use cluster center’s weight sharing state to represent all weight sharing states of architectures in the same clus-ter. This greatly reduces the computational complexity and makes the whole optimization process feasible.
To summary, our main contributions are three-fold:
• We are the first to solve the weight sharing problem directly from the perspective of diminishing the dis-tribution gap between the weights inherited from the supernet and the weights trained with stand-alone net-work. Such gap is believed to be the principal reason that impedes the one-shot NAS progress.
• We propose a novel joint training formula to iteratively update the supernet weights and topology, which facili-tates a feasible optimization process.
• Our searched architectures deliver the new state-of-the-art performance on different benchmark datasets and search spaces. 2.