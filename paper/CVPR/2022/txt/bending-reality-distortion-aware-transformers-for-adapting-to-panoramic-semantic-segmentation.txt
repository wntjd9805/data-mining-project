Abstract
Panoramic images with their 360◦ directional view en-compass exhaustive information about the surrounding space, providing a rich foundation for scene understand-ing. To unfold this potential in the form of robust panoramic segmentation models, large quantities of expensive, pixel-wise annotations are crucial for success. Such annotations are available, but predominantly for narrow-angle, pinhole-camera images which, off the shelf, serve as sub-optimal re-sources for training panoramic models. Distortions and the distinct image-feature distribution in 360◦ panoramas im-pede the transfer from the annotation-rich pinhole domain and therefore come with a big dent in performance. To get around this domain difference and bring together semantic annotations from pinhole- and 360◦ surround-visuals, we propose to learn object deformations and panoramic im-age distortions in the Deformable Patch Embedding (DPE) and Deformable MLP (DMLP) components which blend into our Transformer for PAnoramic Semantic Segmenta-tion (Trans4PASS) model. Finally, we tie together shared semantics in pinhole- and panoramic feature embeddings by generating multi-scale prototype features and aligning them in our Mutual Prototypical Adaptation (MPA) for unsupervised domain adaptation. On the indoor Stan-ford2D3D dataset, our Trans4PASS with MPA maintains comparable performance to fully-supervised state-of-the-arts, cutting the need for over 1, 400 labeled panoramas.
On the outdoor DensePASS dataset, we break state-of-the-art by 14.39% mIoU and set the new bar at 56.38%.1 1.

Introduction
Panoramic 360◦ cameras have received an increasing amount of attention in fields, such as omnidirectional sens-ing in automated vehicles [13, 75] and bringing immer-sive viewing experiences to augmented- and virtual reality
*Corresponding author (e-mail: kailun.yang@kit.edu). 1Code will be made publicly available at https://github.com/ jamycheung/Trans4PASS.
Figure 1: Semantic segmentation of (a) narrow-angle pinhole im-age and (b) 360◦ panoramic image. Compared to (c) standard
Patch Embeddings, our (d) Deformable Patch Embedding parti-tions 360◦ images while considering distortions, e.g. in sidewalks. displays [69, 71]. Opposed to images captured with pin-hole cameras, that occupy narrow Fields of View (FoV), panoramic images offer omni-range perception, benefiting the detection of road scene objects and indoor scene ele-ments [13, 20]. In particular, dense semantic segmentation on panoramic images, facilitates a high-level holistic pixel-wise understanding of surrounding environments [45, 73].
Panoramic semantic segmentation is usually performed on 2D panoramas that were transformed using equirectan-gular projection [58, 75], which is accompanied by image distortions and object deformations (see Fig. 1). Further, in the 360◦ image domain, labeled data is scarce which ne-cessitates model training to be carried out on semantically matching narrow-FoV pinhole datasets. These two circum-stances culminate in a significantly degraded performance on panoramic segmentation as compared to the pinhole counterpart [72] and as such they have to be adequately ad-dressed. Considering the intricacies of panoramas, convo-lution variants [10, 55, 59] and attention-augmented mod-els [75] were proposed to mitigate image distortions and enlarge receptive fields of Convolutional Neural Networks (CNNs). However, they remain sub-optimal in handling the severe deformations from pinhole- to panoramic data, and fail in establishing long-range contextual dependencies in the ultra-wide 360◦ images, which prove essential for accu-rate semantic segmentation [17, 94].
In light of these challenges, we propose a Transformer for PAnoramic Semantic Segmentation (Trans4PASS) ar-chitecture, and overcome image distortions and object deformations with two novel design choices: Our De-formable Patch Embedding (DPE) is located at the early image sequentialization- and intermediate feature interpre-tation stages empowering the model to learn characteristic panoramic image distortions and preserve semantics. Sec-ondly, with the Deformable MLP (DMLP) module in the feature parsing stage, we mix patches with learned spatial offsets to enhance global context modeling.
The challenging mismatch between the label-rich pinhole- and the label-scarce panoramic domain can also be addressed by unsupervised domain adaptation (UDA), considering labeled 2D Pinhole images as source- and 360◦ Panoramas as target domain. Following previous works [45, 75], we refer to this scenario as PIN2PAN. Tak-ing this view on the learning problem, shows to be a vital ingredient for circumventing the expensive panoramic im-age annotation process while satisfying the need for large-scale annotated data [94] to train robust segmentation trans-formers. Unlike common adversarial-learning [44] and pseudo-label self-learning [97] methods for UDA, we put forward Mutual Prototypical Adaptation (MPA), which gen-erates mutual prototypes for pinhole- and panoramic multi-scale feature embeddings, distilling prototypical knowledge of both domains, which proves advantageous to domain-separate distillation [84]. On top, we show MPA works with pseudo-labels in a joint manner and provides a complemen-tary alignment incentive in the feature space.
To verify the capability for generalization to diverse scenarios of our solution, we evaluate Trans4PASS on i.e., both indoor- and outdoor panoramic-view datasets,
Stanford2D3D [1] and DensePASS [45] benchmarks. On
DensePASS, it outperforms the previous best result [88] by
>10.0% in mIoU. Our solution achieves top performance among unsupervised methods on Stanford2D3D and even ranks higher than many competing supervised methods.
In summary, we deliver the following contributions: (1) We consider panoramic deformations in our distortion-aware Transformer for Panoramic Semantic Segmenta-tion (Trans4PASS) with deformable patch embedding-and deformable MLP modules. (2) We present Mutual Prototypical Adaptation to trans-fer models via distilling dual-domain prototypical knowledge, boosting performance by coupling it with pseudo-labels in feature- and output space. (3) Our framework for transferring models from PIN2PAN yields excellent results on two competitive bench-marks: On Stanford2D3D we circumvent using 1, 400 expensive panorama labels while achieving compara-ble results and on DensePASS we boost state-of-the-art performance by an absolute 14.39% in mIoU. 2.