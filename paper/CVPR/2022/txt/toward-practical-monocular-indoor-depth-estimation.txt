Abstract 1.

Introduction
The majority of prior monocular depth estimation meth-ods without groundtruth depth guidance focus on driving scenarios. We show that such methods generalize poorly to unseen complex indoor scenes, where objects are cluttered and arbitrarily arranged in the near ﬁeld. To obtain more robustness, we propose a structure distillation approach to learn knacks from an off-the-shelf relative depth estima-tor that produces structured but metric-agnostic depth. By combining structure distillation with a branch that learns metrics from left-right consistency, we attain structured and metric depth for generic indoor scenes and make inferences in real-time. To facilitate learning and evaluation, we col-lect SimSIN, a dataset from simulation with thousands of environments, and UniSIN, a dataset that contains about 500 real scan sequences of generic indoor environments.
We experiment in both sim-to-real and real-to-real settings, and show improvements, as well as in downstream applica-tions using our depth maps. This work provides a full study, covering methods, data, and applications aspects.
This work proposes a practical indoor depth estimation framework that has the following features: learning from off-the-shelf estimators and left-right image pairs without their depth annotations, efﬁcient training data collection, high generalizability to cross-dataset inference, and accu-rate and real-time depth sensing. Our work applies to consumer-level AR/VR, such as 3D indoor scene recon-struction and virtual object insertion and interaction with environment [32]
Although self-supervised depth estimation, especially using left-right consistency, has attracted much research in-terest recently, popular works, such as MonoDepth [23],
MonoDepth2 [24], DepthHints [74], and ManyDepth [75], mainly focus on driving scenes and are trained on large-scale driving datasets like KITTI [22] and Cityscapes [13], and it is unclear how these methods apply on indoor envi-ronments. Learning indoor depth via self-supervision is ar-guably more challenging for a number of reasons: (1) struc-ture priors: depth estimation for driving scenes imposes a strong scene structure prior to the learning paradigm. The upper parts of images, commonly occupied by the sky or buildings, are typically farther away; on the other hand, the 1
lower parts are usually roads extending to the distance [15].
By contrast, the structure priors are much weaker for indoor environments since objects can be cluttered and arranged (2) distribution: scene depth arbitrarily in the near ﬁeld. for driving scenarios tends to distribute more evenly across near to far ranges on roads, whereas indoor depth can be concentrated in either near or far ranges, such as zoom-in views of desks or ceilings. The uneven depth distribution makes it challenging to predict accurate metric depth for in-door scenes. (3) camera pose: depth-sensing devices can move in 6DoF for indoor captures, but they are typically anchored on cars for collecting driving data where transla-tions are usually without elevation and rotations are domi-nated by yaw angle. Therefore, a desirable network needs to be more robust to arbitrary camera poses and complex scene structures for indoor cases. (4) untextured surfaces: large untextured regions, such as walls, make the commonly used photometric loss ambiguous.
In this work we propose DistDepth, a structure distilla-tion approach to enhance depth accuracy trained by self-supervised learning. DistDepth uses an off-the-shelf rela-tive depth estimator, DPT [59, 60] that produces structured but only relative depth (output values reﬂect depth-ordering relations but are metric-agnostic). Our structure distillation strategy encourages depth structural similarity both statis-tically and spatially. In this way, depth-ordering relations from DPT can be effectively blended into metric depth esti-mation branch trained by left-right consistency. Our learn-ing paradigm only needs an off-the-shelf relative depth esti-mator and stereo image inputs without their curated depth annotations. Given a monocular image at test time, our depth estimator can predict structured and metric-accurate depth with high generalizability to unseen indoor scenes (Sec. 3.2). Distillation also helps downsize DPT’s large vi-sion transformer to a smaller architecture, which enables real-time inference on portable devices.
We next describe our dataset-level contributions. Current publicly available stereo datasets are either targeting driving scenarios [9, 13, 20, 22, 76], small-scale and lacking scene variability [65, 66], rendered from unrealistic-scale 3D an-imations [6, 52], or collected in-the-wild [35, 70]. Popular indoor datasets are either small-scale (Middlebury [65]) or lacking stereo pairs (NYUv2 [54]). There is currently no large-scale indoor stereo dataset to facilitate left-right con-sistency for self-supervised studies. We utilize the popu-lar Habitat simulator [63, 69] to collect stereo pairs in 3D indoor environments. Commonly-used environments are chosen, including Replica [68], Matterport3D (MP3D) [8], and Habitat-Matterport 3D (HM3D) [57], to create Sim-SIN, a novel dataset consisting of about 500K simulated stereo indoor images across about 1K indoor environments (Sec. 4). With SimSIN, we are able to investigate per-formances of prior self-supervised frameworks on indoor scenes [24, 74, 75]. We show that we can ﬁt on SimSIN by directly training those models, but such models generalize poorly to heterogeneous domain of unseen environments.
Using our structure distillation strategy, however, can pro-duce highly structured and metric-accurate depth on unseen data (Sec. 5).
Several commercial-quality simulations and real data are utilized for evaluation, including a challenging virtual apart-ment (VA) sequence [1, 2], pre-rendered scenes in Hyper-sim [61], and real monocular images in NYUv2 [67]. To further investigate the gap between training on simulation training on real data, we further collect UniSIN, a v.s. dataset including 500 real stereo indoor sequences, amount-ing to 200K images, in a university across buildings and spaces using off-the-shelf high-performing stereo cameras.
We show that our DistDepth trained on simulation data only has on-par performance with those trained on real data.
Our DistDepth is especially capable of 1. attaining zero-shot cross-dataset inference, and 2. closing the gap between sim-to-real and real-to-real learning, as shown in Fig. 1.
Throughout the work we visualize depth maps in actual met-ric ranges unless marked as relative depth. We summarize our contributions as follows. 1. We propose DistDepth, a framework that distills depth-domain structure knowledge into a self-supervised depth estimator to obtain highly structured and metric-accurate depth maps. 2. We present SimSIN, a large-scale indoor simulation dataset that fuels the study of indoor depth estimation via left-right consistency, and a real dataset, UniSIN, that tar-gets at studying the gap between training on simulation and real data. 3. We attain a practical indoor depth estimator: learning without curated depth groundtruth, efﬁcient and effective data collection by simulation, high generalizability, and ac-curate and real-time inference for depth sensing. 2.