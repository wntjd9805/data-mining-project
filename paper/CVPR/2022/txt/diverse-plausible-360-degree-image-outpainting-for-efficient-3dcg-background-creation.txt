Abstract 1.

Introduction
We address the problem of generating a 360-degree im-age from a single image with a narrow field of view by es-timating its surroundings. Previous methods suffered from overfitting to the training resolution and deterministic gen-eration. This paper proposes a completion method us-ing a transformer for scene modeling and novel methods to improve the properties of a 360-degree image on the output image. Specifically, we use CompletionNets with a transformer to perform diverse completions and Adjust-mentNet to match color, stitching, and resolution with an input image, enabling inference at any resolution. To im-prove the properties of a 360-degree image on an output image, we also propose WS-perceptual loss and circular in-ference. Thorough experiments show that our method out-performs state-of-the-art (SOTA) methods both qualitatively and quantitatively. For example, compared to SOTA meth-ods, our method completes images 16 times larger in reso-lution and achieves 1.7 times lower Fr´echet inception dis-tance (FID). Furthermore, we propose a pipeline that uses the completion results for lighting and background of 3DCG scenes. Our plausible background completion enables per-ceptually natural results in the application of inserting vir-tual objects with specular surfaces.
In recent three-dimensional computer graphics (3DCG) production, 360-degree images are helpful for efficiently creating lighting and backgrounds. For example, a designer might spend much time creating 3D objects in the near field and creating the background quickly by using 2D images with a narrow field of view (NFoV) images or 360-degree images. However, the production method of creating the background by placing 2D images behind a 3D object can-not fully represent the scenery reflected on the surface of the 3D object. Of course, this problem does not occur if the image surrounds the object in 360 degrees. How-ever, 360-degree images, especially high dynamic range im-ages (HDRI), are generally more expensive to prepare than
NFoV images.
This paper addresses the problem of converting an NFoV image into a 360-degree image by complementing its sur-roundings to obtain a 360-degree environment consistent with the image given as a partial background (Fig. 1). By solving this problem, users can use only a NFoV image to reflect the surrounding environment to objects [1, 24], or in the case of HDRI, to achieve natural shadows and global illumination through Image-Based Lighting [6, 24].
For use by designers, it is desirable to infer NFoV im-ages of any size and to have choices by generating diverse 360-degree images. However, existing methods are deter-ference as a new auto-regressive order for a transformer. It improves the connectivity at both ends of an image at the pixel and semantic levels by performing inference while cir-culating the image. Second, to further improve the percep-tual quality, we propose a WS-perceptual loss function for training of CompletionNets. This loss function reflects that 360-degree images have different information content along the latitudinal direction and improves the performance of 360-degree image modeling by focusing on computing the loss in the information-rich regions.
Our thorough experiments show not only that the pro-posed method can perform diverse completions at arbi-trary resolutions but also that the proposed method out-performs several state-of-the-art methods both qualitatively and quantitatively. For example, in terms of FID score, our method shows 1.7% lower improvement than 360IC and achieves plausible completion for images with 16 times as many pixels (1024×512) as EnvMapNet [24] (256×128).
Moreover, we propose a pipeline to create an HDR envi-ronment map from a single NFoV image and use it as light-ing and background in 3DCG. Through demonstrations, we show that our method reaches the quality of 360-degree im-age completion, which can be used for 3DCG and is helpful for efficient background creation.
The proposed method produces a plausible 360-degree image and provides various completion results, allowing de-signers to choose their preferred result among them. Con-sidering these characteristics, we conclude with a discus-sion of potential applications.
Our contributions can be summarized as follows:
• We propose AdjustmentNet to introduce a transformer into 360-degree image outpainting, which enables di-verse and arbitrary-resolution outputs.
• We propose two novel techniques for acquiring the properties of 360-degree images: WS-perceptual loss for the training of CompletionNets and circular infer-ence for the transformer. These allow us to outperform previous methods both quantitatively and qualitatively.
• We demonstrate that our high-resolution and plausible completion renders natural-looking scenes even when specular virtual objects are close to a camera or the camera views all around on 3DCG scenes. 2.