Abstract 3D object detection is a crucial research topic in com-puter vision, which usually uses 3D point clouds as input in conventional setups. Recently, there is a trend of leverag-ing multiple sources of input data, such as complementing the 3D point cloud with 2D images that often have richer color and fewer noises. However, due to the heterogeneous geometrics of the 2D and 3D representations, it prevents us from applying off-the-shelf neural networks to achieve multimodal fusion. To that end, we propose Bridged Trans-former (BrT), an end-to-end architecture for 3D object de-tection. BrT is simple and effective, which learns to identify 3D and 2D object bounding boxes from both points and im-age patches. A key element of BrT lies in the utilization of object queries for bridging 3D and 2D spaces, which unifies different sources of data representations in Trans-former. We adopt a form of feature aggregation realized by point-to-patch projections which further strengthen the in-teraction between images and points. Moreover, BrT works seamlessly for fusing the point cloud with multi-view im-ages. We experimentally show that BrT surpasses state-of-the-art methods on SUN RGB-D and ScanNetV2 datasets. 1.

Introduction 3D object detection, which aims at identifying or locat-ing objects in 3D scenes, is drawing increasing attention and is acting as a fundamental task towards scene under-standing. Many successful attempts [3,15,21,25] have been made using point cloud data as input. These attempts in-clude converting the points to regular format (e.g., 3D voxel grids [32], polygon meshes [11], multi-views [29]), or using (cid:66) Corresponding author: Fuchun Sun. 3D specific operators (e.g., symmetric functions [23], vot-ing [21]) to design grouping strategies for points. In addi-tion, since Transformers could be naturally permutation in-variant and capable of capturing large-scale data interaction, they are lately applied to 3D object detection and demon-strate superior performance [15,19]. Besides handling point cloud learning tasks, Transformers have swept across vari-ous 2D tasks, e.g., image classification [6,14], object detec-tion [2, 8, 39], and semantic segmentation [33, 37].
Deep multimodal learning by leveraging the advantage of multiple modalities has shown its superiority on various applications [1, 31]. Despite the success of Transformers in 2D or 3D single-modal object detection tasks, the attempt of combining advantages from both point clouds and im-ages remains scarce. For 3D learning tasks, the point cloud provides essential geometrical cues, while the information in rich color images can complement the point cloud by ful-filling the missing color information and correcting noise errors. As a result, the performance of 3D object detection could be potentially improved by the involvement of 2D im-ages. One intuitive method is to lift 3-dimensional RGB vectors from images to extend the point features. A CNN-based 3D detection model, imVoteNet [20], points out the difficulty in migrating 2D/3D discrepancies by this intuitive method, and instead, imVoteNet substitutes the RGB vec-tors with image features extracted by a pre-trained 2D de-tector. However, simultaneously relying on both the image voting and point cloud voting assumptions in [20] could ac-cumulate the intrinsic grouping errors as mentioned by [15].
To avoid the learning process of point clouds being im-pacted by middle-level 2D/3D feature interaction, [20] com-bines multimodal features over the first layer, which poten-tially prevents the network from fully exploiting their se-mantic interaction or migrating multimodal discrepancies.
In this work, we propose Bridged Transformer (BrT) – a
simple and effective Transformer framework for 3D object detection. BrT bridges the learning processes of images and point clouds inside Transformer. This approach takes the sampled points and image patches as input. To protect the self-learning process of each modality, attentions between point tokens and image patch tokens are blocked but bridged by object queries throughout the Transformer layers. To strengthen the interaction between images and points, BrT is also equipped with powerful bridging designs from two perspectives. Firstly, we leverage conditional object queries for images and points that are aware of the learned proposal points. Such design together with aligned positional em-beddings tells Transformer that object quires of images and points are aligned. Secondly, despite the perspective from object queries, we perform point-to-patch projections to ex-plicitly leverage the spatial relationships of both modalities.
BrT avoids the grouping errors due to its natural ability of capturing long-range dependencies and global contextual information, and instead of lifting image features to point clouds at the beginning layer in [20], BrT allows the full propagation of feature interaction in the whole network. As an additional advantage, BrT can be extended to combine point clouds with multi-view images.
We evaluate BrT on both SUN RGB-D and ScanNetV2 datasets, where respectively, BrT achieves remarkably 2.4% and 2.2% improvements over state-of-the-art methods.
To summarize, the contributions of our work are:
• We propose BrT, a novel framework for 3D object de-tection that bridges the learning processes of images and point clouds inside Transformer.
• We propose to strengthen the interaction between im-ages and points from two perspectives including condi-tional object queries and the point-to-patch projection.
• BrT achieves the state-of-the-art on two benchmarks, which demonstrates the superiority of our design and also the potential in multi-view scenarios. 2.