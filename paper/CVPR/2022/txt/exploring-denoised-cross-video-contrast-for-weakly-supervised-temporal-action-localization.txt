Abstract
Weakly-supervised temporal action localization aims to localize actions in untrimmed videos with only video-level labels. Most existing methods address this problem with a
“localization-by-classiﬁcation” pipeline that localizes ac-tion regions based on snippet-wise classiﬁcation sequences.
Snippet-wise classiﬁcations are unfortunately error prone due to the sparsity of video-level labels. Inspired by recent success in unsupervised contrastive representation learn-ing, we propose a novel denoised cross-video contrastive al-gorithm, aiming to enhance the feature discrimination abil-ity of video snippets for accurate temporal action localiza-tion in the weakly-supervised setting. This is enabled by three key designs: 1) an effective pseudo-label denoising module to alleviate the side effects caused by noisy con-trastive features, 2) an efﬁcient region-level feature con-trast strategy with a region-level memory bank to capture
“global” contrast across the entire dataset, and 3) a diverse contrastive learning strategy to enable action-background separation as well as intra-class compactness & inter-class separability. Extensive experiments on THUMOS14 and
ActivityNet v1.3 demonstrate the superior performance of our approach. 1.

Introduction
As a fundamental yet challenging computer vision task, temporal action localization aims to localize the occur-rences of prescribed action categories in untrimmed videos.
It has received extensive research attention due to its wide applications in surveillance [49], video summarization [32], and highlight detection [55], etc. Many existing meth-ods [4, 7, 28, 43, 56, 66, 68] are based on fully-supervised training, which rely heavily on densely annotated frame la-bels that are typically laborious and time-consuming to ac-quire. On the other hand, it is much easier for users to provide video-level tags describing scene context and con-tent. This naturally gives rise to the weakly-supervised tem-Jingjing Li does this work during an internship at Tencent AI Lab.
† Equal contribution. ∗ Corresponding author. poral action localization, or WS-TAL, where cheap video-level tags are utilized as an alternative supervision sig-nal [38,41,50]. Most existing WS-TAL methods [18,25,38, 39, 41, 50, 60, 64] follow a “localization-by-classiﬁcation” pipeline: a snippet-wise classiﬁcation is carried out over time to generate the Temporal Class Activation Sequence, also called T-CAS or T-CAM [38, 41]; this is followed by selecting snippets with high responses to localize the plausi-ble action regions. Given the sparsity nature of video-level labels, however, snippet-wise classiﬁcations are often error-prone, which may severely damage the ﬁnal localization performance.
To learn a good T-CAS for action localization, it be-comes crucial to enhance the feature discrimination abil-ity of various video snippets in snippet-wise classiﬁcation.
Generally, the snippet feature embedding space is expected to satisfy two properties: 1) action snippets should be sep-arable from the background snippets that do not belong to any action classes, i.e., action-background separation; 2) action snippets from a same class should be closer than those from different classes, i.e., intra-class compactness
& inter-class separability. This has led to several prior studies [36, 41, 64] exploring deep metric learning [15, 26] or contrastive learning [5] to foster learning discriminative features. As illustrated in Fig. 1 (a) & (b), their focus is mostly on action-background separation, by pushing action features of a speciﬁc class to be close and pulling action features away from the background ones, either within in-dividual videos [64], or within a carefully-designed mini-batch [36, 41]. They unfortunately fail to capture the inter-class separability, and ignore the useful “global” contrast across training videos in the entire dataset. Given the lack of frame-level annotations, snippet-wise pseudo-labels [64] or attention-based mechanisms [36, 41] are often used in-ternally as a substitute. As illustrated in Fig. 1 (a), action-background separation is performed based on pseudo-labels over the snippets of each video.
In Fig. 1 (b), attention-pooled video-level features from a mini-batch are engaged in the feature contrastive training process. Due to the noisy pseudo-labels or false activations in the learned attention se-Figure 1. Different contrastive learning schemes. (a) Exploiting snippet-wise contrastive learning within single video to separate snippet-wise actions from backgrounds with pseudo-labels (e.g., [64]). (b) Exploiting deep metric learning within mini-batch to separate video-level actions from backgrounds with attention-weighted pooling (e.g., [36, 41]). (c) Our denoised cross-video contrastive algorithm with 1) pseudo-label denoising module, 2) region-level feature contrastive learning across entire dataset, and 3) action-background separation, as well as intra-class compactness & inter-class separability. quence, these strategies would inevitably give rise to noisy contrastive features. Incorporating these noisy contrastive features may unnecessarily complicate the snippet feature training, and result in suboptimal performance of action lo-calization.
The above observations motivate us to propose a novel
Denoised Cross-video Contrastive (DCC) algorithm tai-lored for weakly-supervised temporal action localization.
As illustrated in Fig. 1 (c), it contains three key ideas.
First, to account for the pseudo-label noises that are ubiq-uitous in weakly-supervised TAL, a pseudo-label denois-ing (PLD) module is devised to reduce the negative im-pacts of noisy contrastive features. By down-weighting the conﬁdence scores of incorrect pseudo-labels, more ac-curate contrastive features can be generated. Second, to capture “global” contrast across the entire dataset, we pro-pose a region-level feature contrast strategy which, together with a region-level memory bank, allow our learned model to preserve “global” informative features across the entire dataset. Third, a diverse contrastive training strategy is proposed to enforce contrasts between actions and back-grounds, and between different action classes. It is capa-ble of promoting action-background separation, inter-class separation and intra-class compactness. Note that our DCC algorithm is performed only during training, so it does not incur additional computational cost in testing.
Here we summarize our main contributions. (1) A novel denoised cross-video contrastive algorithm is proposed for weakly-supervised TAL. It reduces the inﬂuence of noisy contrastive features; it also captures “global” contrast across the entire dataset, and simultaneously promotes action-background separation, inter-class separability as well as intra-class compactness. As a result, the discrimination ability of snippet features is signiﬁcantly enhanced. (2) Ex-tensive experiments on THUMOS14 and ActivityNet v1.3 datasets demonstrate the superior performance of our ap-proach over the state-of-the-art methods. Speciﬁcally, we observe a 16.7% improvement over the baseline in terms of average mAP of IoU thresholds from 0.1 to 0.7 on THU-MOS14, a signiﬁcant amount without incurring extra com-putation cost in inference. 2.