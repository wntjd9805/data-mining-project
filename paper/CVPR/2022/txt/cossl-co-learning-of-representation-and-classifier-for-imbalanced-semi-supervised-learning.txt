Abstract
Standard semi-supervised learning (SSL) using class-balanced datasets has shown great progress to leverage un-labeled data effectively. However, the more realistic set-ting of class-imbalanced data – called imbalanced SSL – is largely underexplored and standard SSL tends to under-perform.
In this paper, we propose a novel co-learning framework (CoSSL), which decouples representation and classifier learning while coupling them closely. To handle the data imbalance, we devise Tail-class Feature Enhance-ment (TFE) for classifier learning. Furthermore, the cur-rent evaluation protocol for imbalanced SSL focuses only on balanced test sets, which has limited practicality in real-world scenarios. Therefore, we further conduct a com-prehensive evaluation under various shifted test distribu-tions. In experiments, we show that our approach outper-forms other methods over a large range of shifted distribu-tions, achieving state-of-the-art performance on benchmark datasets ranging from CIFAR-10, CIFAR-100, ImageNet, to
Food-101. Our code will be made publicly available. 1.

Introduction
Imbalanced data distributions are ubiquitous, and pose great challenges for standard deep learning methods. Many approaches have been proposed for long-tailed recognition, where the number of (labeled) examples exhibits a long-tailed distribution with heavy class imbalance [17, 18, 33, 37, 39, 52]. While semi-supervised learning (SSL) in the class-balanced setting has shown great promise, in this pa-per we are interested in the challenging and realistic setting of imbalanced SSL where both the labeled and the unlabeled data are class-imbalanced, as shown in Fig. 1.
Despite a few pioneer works [30, 54], existing solutions from long-tailed recognition and SSL do not generalize well to this setting. On the one hand, long-tailed recog-nition [6, 9, 19, 20, 24] is not designed to utilize unlabeled
Figure 1. Conventional recognition tasks focus on constrained settings: long-tailed recognition does not involve unlabeled data; semi-supervised learning (SSL) assumes class-balanced distribu-tions for both labeled and unlabeled data. In this work, we aim at imbalanced SSL, where the training data is partially annotated, and both labeled and unlabeled data are not manually balanced. This setting is more general and poses great challenges to existing al-gorithms. A robust learning algorithm should still be able to learn a good classifier under this setting. data despite being good at handling data imbalance. Semi-supervised learning (SSL) [1, 3, 4, 34, 42, 46, 48–50], on the other hand, can effectively leverage unlabeled data but can not address data imbalance. In certain cases, standard
SSL methods trained with imbalanced unlabeled datasets can lead to even worse results than a simple re-balancing method without using any unlabeled data [30], which coun-ters the promise of SSL.
In this paper, we address the imbalanced SSL problem by leveraging strong SSL algorithms [3, 4, 50, 55] and recent success of decoupling representation and classifier learn-ing from long-tailed recognition [29]. To this end, we pro-pose CoSSL, a novel co-learning framework for imbalanced
SSL, which closely couples representation and classifier while the training of them is decoupled. As shown in Fig. 2, CoSSL consists of three modules: semi-supervised rep-resentation learning, classifier learning, and pseudo-label generation. In our co-learning framework, the representa-tion learning module and the classifier learning module are trained separately without the gradient exchange. Nonethe-less, the two modules in CoSSL are still connected via a shared encoder and pseudo-label generation.
It can then bootstrap itself by exchanging information between the two modules: 1) a shared encoder from the representation learn-ing is passed to classifier training for feature extraction; and 2) the enhanced classifier is used to generate better pseudo-labels for the representation learning. We show the superi-ority of our co-learning framework empirically, outperform-ing previous state-of-the-art methods by a large margin, es-pecially in the case of severe imbalance. Moreover, we pro-pose Tail-class Feature Enhancement (TFE) for improved classifier learning for imbalanced SSL, which utilizes unla-beled data as a source of augmentation to enhance the data diversity of tail classes, leading to a more robust classifier.
Furthermore, the standard evaluation protocol of long-tailed recognition and SSL normally assumes that the test data are from a uniform class distribution [3, 4, 7, 29, 36, 41, 50, 51, 53]. However, this is insufficient to reflect the diversity of real-world applications, where users may have different needs. It is strongly desired that the trained model can perform well over a large range of varying distributions, including those that are radically different from the training distribution. Therefore, in this paper, we adopt the shifted evaluation from [23], where the test data are from variously shifted class distributions. We further distinguish between unknown shifted evaluation and known shifted evaluation, depending on whether test distribution is known a priori during training. This evaluation protocol can be used for long-tailed recognition as well.
Our contributions are: (1) We propose a novel co-learning framework CoSSL for imbalanced SSL, which de-couples representation and classifier learning while cou-pling them closely via a shared encoder and pseudo-label generation. (2) We devise a novel Tail-class Feature En-hancement (TFE) method to increase the data diversity of tail classes by utilizing unlabeled data, leading to more ro-bust classifiers. (3) We propose new evaluation criteria for imbalanced SSL and conduct a comprehensive evaluation.
CoSSL achieves new state-of-the-art results on multiple im-balanced SSL benchmarks across a wide range of evaluation settings. 2.