Abstract
We present a coarse-to-fine discretisation method that en-ables the use of discrete reinforcement learning approaches in place of unstable and data-inefficient actor-critic meth-ods in continuous robotics domains. This approach builds on the recently released ARM algorithm, which replaces the continuous next-best pose agent with a discrete one, with coarse-to-fine Q-attention. Given a voxelised scene, coarse-to-fine Q-attention learns what part of the scene to
‘zoom’ into. When this ‘zooming’ behaviour is applied it-eratively, it results in a near-lossless discretisation of the translation space, and allows the use of a discrete action, deep Q-learning method. We show that our new coarse-to-fine algorithm achieves state-of-the-art performance on several difficult sparsely rewarded RLBench vision-based robotics tasks, and can train real-world policies, tabula rasa, in a matter of minutes, with as little as 3 demonstrations. 1.

Introduction
In this paper, we are interested in a general real-world ma-nipulation algorithm that can use a small number of demon-strations, along with a small amount of sparsely-rewarded exploration data, to accomplish a diverse set of tasks, both in simulation and the real world. To develop such an approach, two paradigms come to mind: imitation learning (IL) and re-inforcement learning (RL). Imitation learning methods, such as behaviour cloning, suffer from compounding error due to covariate shift, while reinforcement learning suffers from long training times that often require millions of environment interactions. Recently however, Q-attention and the ARM system [14] has been shown to bypass many flaws that come with reinforcement learning, most notably the large training burden and exploration difficulty with sparsely-rewarded and long-horizon tasks.
Unfortunately, like many modern continuous control RL algorithms, ARM’s next-best pose agent follows an actor-critic paradigm, which can be particularly unstable when
Figure 1. C2F-ARM learns sparsely-rewarded tasks with only 3 demonstrations. Real-world tasks include: turning on a light, pulling cloth from shelf, pulling a toy car, taking a lid off a saucepan, and folding a towel. learning from sparsely-rewarded and image-based tasks [14]: two properties that are particularly important for robot ma-nipulation tasks. In this paper, we re-examine how best to represent the continuous control actions needed for robot ma-nipulation, abandoning the standard actor-critic approach, in favour of a more stable discrete action Q-learning approach.
The challenge therefore becomes how to effectively discre-tise 6D poses. Discretisation of rotation and gripper action is trivial given its bounded nature, but translation remains challenging given that is usually a much larger space. We solve this problem via a coarse-to-fine Q-attention, where we start with a coarse voxelisation of the translation space, use 3D Q-attention to identify the next most interesting point, and gradually make the resolution higher at each point.
With this new coarse-to-fine Q-attention, we present our
Coarse-to-Fine Attention-driven Robotic Manipulation (C2F-ARM) system. We benchmark the system in simulation against other robot learning algorithms from both the re-Figure 2. Summary of coarse-to-fine Q-attention. Observation data (RGB and point cloud) from M cameras are given to each depth of the
Q-attention. Each depth of the Q-attention gives the locations of the most interesting point in space (at the current resolution), which is then used as the voxel centroid for the Q-attention at the next depth. Intuitively, this can be thought of as ‘zooming’ into a specific part of the scene to gain more accurate 3D information. The highlighted red voxel corresponds to the highest value. inforcement learning and imitation learning literature, and show that C2F-ARM is more sample-efficient and stable to train than other methods. We also show that C2F-ARM is capable of learning 5 diverse sets of sparsely-rewarded real-world tasks from only 3 demonstrations.
To summarise, the paper presents the following three contributions: (1) A novel way to discretise the transla-tion space via coarse-to-fine Q-attention, allowing us to discard the often unstable actor-critic framework for a sim-pler deep Q-learning approach. (2) Our manipulation sys-tem, C2F-ARM, which uses the coarse-to-fine Q-attention along with a control agent to achieve sample-efficient learn-ing of sparsely-rewarded tasks in both simulation and real-(3) The first use of a voxel representation for world. vision-based reinforcement learning for 6D robot manipula-tion. Code in supplementary material, and videos found at: sites.google.com/view/c2f-q-attention. 2.