Abstract
Humans are in constant contact with the world as they move through it and interact with it. This contact is a vi-tal source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we ac-cumulate HSIs across multiple input images, and use these in optimizing the 3D scene to reconstruct a consistent, phys-ically plausible, 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) hu-mans who move in a scene are occluded by, or occlude, objects, thus constraining the depth ordering of the objects, (2) humans move through free space and do not interpene-trate objects, (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Us-ing these constraints in an optimization formulation across all observations, we signiﬁcantly improve 3D scene layout reconstruction. Furthermore, we show that our scene re-construction can be used to reﬁne the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimates qualitatively and quantitatively using the PROX and PiGraphs datasets.
The code and data are available for research purposes at https://mover.is.tue.mpg.de. 1.

Introduction
Human behavior and the interaction of humans with their environment are fundamentally about the 3D world. Hence, 3D reconstruction of both the human and scene can facilitate behavior analysis. Where and how humans interact with a scene can be used to predict future motions and interactions for human-centered AI and robots, or to synthesize these for
AR/VR and other computer-graphics applications.
Tremendous progress has been made in reconstructing 3D human bodies [12, 37, 39, 44–46, 58, 66, 67, 79, 92, 93] and 3D scenes [6, 17, 31, 64, 95] from monocular images or videos, typically in isolation from each other. In real life, though, humans always interact with scenes. Consequently, humans (partially) occlude the scene, and the scene (par-tially) occludes humans. Strong human-scene occlusion can cause problems for both scene and human reconstruction.
In contrast, recent work on human-scene interaction (HSI), estimates humans and scenes together [10, 26, 87].
PROX [26] demonstrates how HSI can be used to constrain 3D human pose estimation, but it requires a 3D scan of the full scene to be known a priori. This is often unrealistic and cumbersome, as it requires one to conduct ofﬂine 3D reconstruction by walking around the scene with a depth sensor [103] to observe it from many view points.
What we need, instead, is a method that estimates the scene and humans from images of a single color camera.
This is challenging, as the lack of depth information causes the scale and placement of objects to be inconsistent w.r.t. the humans interacting with them. This leads to physically implausible results, like humans penetrating objects, or lack-ing physical contact when walking, sitting, or lying down, causing bodies to “hover” in the air (see Fig. 2). Methods that reconstruct 3D humans from single views leverage sta-tistical body models [38, 56, 66, 90] as priors on the body shape and pose. However, the same tools do not exist for the collective space of 3D scene layouts. This is due to the enormous space of possible object arrangements in indoor 3D scenes, the large number of different object classes, and the huge inter-class (e.g., chairs and desks) and intra-class (e.g., desk chair and club chair) shape variability.
To address the above issues, we present MOVER, which stands for “human Motion driven Object placement for Vi-sual Environment Reconstruction”. MOVER leverages infor-mation across several HSI frames to estimate both a plausible 3D scene and a moving human that interacts with the scene.
Figure 1 provides a high-level overview. MOVER takes as input: (1) a set of color frames from a static monocular cam-era, (2) a 3D human mesh inferred for each frame [45, 66], and (3) a 3D shape inferred for each object detected in the scene [42, 64]. As output, MOVER produces a reﬁned 3D scene, comprised of repositioned input objects, so that it is consistent with the estimated 3D human; i.e., it satisﬁes the expected contacts on the body [27], while preventing inter-penetration. MOVER uses a novel optimization scheme, that jointly optimizes over camera pose, ground-plane pose, and the size and position of 3D objects, while being constrained by various HSI constraints.
MOVER takes three types of HSI constraints into ac-count: (1) humans who move in a scene are occluded or occlude objects, thus, deﬁning the depth ordering of the ob-jects (c.f. [75]), (2) humans move in free space that is not occupied by objects and do not interpenetrate objects, (3) contact between humans and objects means that the contact-ing parts of their surfaces occupy the same place in space.
Thus, we leverage both explicit (i.e., contact) and implicit (i.e., free space, no penetrations) HSI cues. MOVER is able to use these because it employs detailed meshes for both the scene and the moving human. In contrast, the few attempts that have been made in this direction use oversimpliﬁed shapes [10], i.e., 3D bounding boxes for objects and skele-tons for humans, work only for static humans that contact (a) 3D scene reconstruction [64] and HPS [66] in isolation. (b) HolisticMesh [87]. L: single-image results. R: multiple-images result.
Figure 2. Where existing methods struggle: (a) humans in esti-mated scenes penetrate objects or lack contact with objects and
“hover” in the air when estimated in isolation [64, 66] (b) humans interpenetrate objects, even, when the 3D scenes and humans are jointly optimized with single (left) or sequential images (right) [87].
In contrast, we leverage human scene interaction constraints in a global optimization across all input frames, to compute a scene that is coherent with the human motions (see Fig. 1). a single object [96], or do not integrate information across several interaction frames [10, 87, 96].
Comparisons against the state of the art on the PROX [26] and PiGraphs [74] datasets show, that MOVER estimates more accurate and realistic 3D scene layouts that satisfy the expected contacts, while minimizing penetrations, w.r.t. the moving humans. Interestingly, we ﬁnd that MOVER’s estimated 3D scene can be used to reﬁne the human poses, with a PROX-like method [26]. While estimating 3D scenes and humans from a single camera is challenging, our results suggest that they are synergistic tasks that beneﬁt each other. 2.