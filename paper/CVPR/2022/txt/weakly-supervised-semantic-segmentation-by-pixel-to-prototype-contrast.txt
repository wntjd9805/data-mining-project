Abstract
Though image-level weakly supervised semantic seg-mentation (WSSS) has achieved great progress with Class
Activation Maps (CAMs) as the cornerstone, the large su-pervision gap between classification and segmentation still hampers the model to generate more complete and pre-cise pseudo masks for segmentation.
In this study, we propose weakly-supervised pixel-to-prototype contrast that can provide pixel-level supervisory signals to narrow the gap. Guided by two intuitive priors, our method is exe-cuted across different views and within per single view of an image, aiming to impose cross-view feature semantic con-sistency regularization and facilitate intra(inter)-class com-pactness(dispersion) of the feature space. Our method can be seamlessly incorporated into existing WSSS models with-out any changes to the base networks and does not incur any extra inference burden. Extensive experiments manifest that our method consistently improves two strong baselines by large margins, demonstrating the effectiveness. Specif-ically, built on top of SEAM, we improve the initial seed mIoU on PASCAL VOC 2012 from 55.4% to 61.5%. More-over, armed with our method, we increase the segmentation mIoU of EPS from 70.8% to 73.6%, achieving new state-of-the-art. 1.

Introduction
Benefiting from large-scale pixel-level annotations, se-mantic segmentation [38] has achieved remarkable progress in recent years. However, obtaining such precise pixel-wise annotations is laborious and time-consuming. To re-lieve this burden, many works resort to weakly supervised semantic segmentation (WSSS) that aims at learning seg-mentation models from weak labels such as image tags
[2, 21, 22, 27, 32, 53, 62], bounding boxes [39], points [4] and scribbles [47]. Among them, image-level WSSS that requires only image tags has been extensively studied in the
*Corresponding author.
Figure 1. Initial seed quality vs. segmentation performance. Our method enables consistent performance improvements over state-of-the-arts, i.e., SEAM [53] and EPS [32], without bringing any changes to the base networks during inference. computer vision community.
Image-level WSSS is a challenging task since the image tags indicate only the existence of object categories and do not inform accurate object locations that are essential for se-mantic segmentation. To tackle this issue, Class Activation
Maps (CAMs) [67] that identify which parts of the image contribute the most to the classification have been widely adopted to roughly estimate the regions of the target ob-jects. The regions, also known as seeds, are used to gener-ate pseudo ground truths for training segmentation models.
However, CAMs only cover parts of objects, resulting in in-accurate and incomplete supervision. This issue is rooted in the supervision gap between the classification and the seg-mentation tasks. Specifically, classification networks super-vised by image tags tend to focus on the most discrimina-tive regions of objects for achieving a better performance of correctly tagging the image, while the segmentation task requires pixel-level supervision to assign a category to each pixel within the whole image. Narrowing down the super-vision gap is crucial for WSSS, motivating us to explore pixel-wise supervisory signals that are complementary to image tags.
Inspired by the compelling contrastive self-supervised algorithms [20], we develop a novel weakly-supervised pixel-to-prototype contrastive learning method for WSSS, which can provide pixel-level supervision to improve the quality of CAMs and the performance of segmentation. Our method is based on two implicit but valuable priors: (i) features should retain semantic consistency across differ-ent views of an image; and (ii) pixels sharing the same la-bel should have similar representations in the feature space, and vice versa. With these priors as guidelines, the pixel-to-prototype contrast is executed across different views and within per single view of each image, respectively, leading us to the cross-view contrast and intra-view contrast.
Our method is instantiated with a unified pixel-to-prototype contrastive learning formulation, which shapes the pixel embedding space through a prototype-based met-ric learning methodology. The core idea is pulling pixels together to their positive prototypes and pushing them away from their negative prototypes to learn discriminative dense visual representations. In our method, a prototype is defined as a representative embedding of a category. It is estimated from pixel-wise feature embeddings with the top activations in the CAMs. During learning, the polarity of each proto-type is determined by the pseudo label of each pixel coupled to it in the current mini-batch. However, when generating pseudo masks from CAMs, there emerges a tricky problem: the over-activated and under-activated regions may corrupt the contrastive learning, especially the intra-view contrast.
To alleviate this issue, we adopt two strategies: semi-hard prototype mining and hard pixel sampling to reduce the in-accurate contrasts as well as better utilize hard samples.
Recently, Wang et al. [53] propose SEAM to mitigate the supervision gap issue with a CAM equivariance constraint that enforces the CAMs to have the same spatial transforma-tion as the input images. Our method has two major differ-ences. Firstly, our method imposes regularizations to pixel-level features, enforcing the pixel embedding to be simi-lar to the positive prototype and dissimilar to negative pro-totypes, while SEAM calculates consistency loss between
CAMs of different views of the same image. Moreover, both cross-view and intra-view regularizations are consid-ered in our work, while SEAM only integrates equivariant regularization across views.
Our method can be seamlessly incorporated into exist-ing WSSS models without any changes to the base net-works. It requires only additional common projectors dur-ing training and does not incur an extra inference burden.
Experiments demonstrate that our method improves state-of-the-art models by large margins. As shown in Fig. 1, our method consistently improves two strong baseline models w.r.t. both initial seed qualities and segmentation perfor-mance. We also validate our method through an extensive ablation study, where we find that each component con-tributes substantial performance improvements.
To summary, our main contributions are as follows:
• We propose weakly-supervised pixel-to-prototype contrast for WSSS. It enables the pixels to receive su-pervision from the reliable prototype of each class un-der the WSSS setting, which substantially narrows the gap between classification and segmentation.
• We propose to perform pixel-to-prototype contrastive learning both within a view and across different views of an image, which significantly improves the qualities of CAMs and the subsequent segmentation masks.
• Our method shows impressive results, surpassing base-line models by large margins and achieving the top per-formance on the standard benchmark. 2.