Abstract
Accurate object rearrangement from vision is a crucial problem for a wide variety of real-world robotics applica-tions in unstructured environments. We propose IFOR, It-erative Flow Minimization for Robotic Object Rearrange-ment, an end-to-end method for the challenging problem of object rearrangement for unknown objects given an RGBD image of the original and final scenes. First, we learn an optical flow model based on RAFT to estimate the rel-ative transformation of the objects purely from synthetic data. This flow is then used in an iterative minimization algorithm to achieve accurate positioning of previously un-seen objects. Crucially, we show that our method applies to cluttered scenes, and in the real world, while training only on synthetic data. Videos are available at https:
//imankgoyal.github.io/ifor.html. 1.

Introduction
Object rearrangement is the capability of an embodied agent to physically re-configure the objects in a scene into a
*Work done while authors were interns at NVIDIA desired goal configuration [2]. It is an essential skill in day-to-day activities like setting a dining table, putting away groceries, and organizing a desk. Endowing robots with this capability is crucial for deploying them to assist people with everyday tasks [2].
With varying task setups, the desired goal state can be provided in different forms, for instance, a compact state representation [32, 67] or natural language descrip-tions [36, 55]. In this work, we address the rearrangement task where the goal state is specified by an RGB-D im-age [34, 51], as shown in Fig. 1. This setup lends itself well to many scenarios where the goal state can be snapped once, either in the first place or from a one-time demonstra-tion. For instance, a user can set the dining table once to their preference and take a photo, and a robot assistant can restore the table back to the desired state from any configu-ration.
Traditionally, object rearrangement problems have been studied in the robotics community, often in the context of
Task and Motion Planning (TAMP) [15]. Despite much re-cent progress [10, 16, 26], most TAMP approaches still rely on a strict set of assumptions on the perception front. First,
the objects and scenes are often assumed to be known a pri-ori, provided with high fidelity 3D models. This makes the approaches difficult to be deployed in unseen environments or environments without models. Second, given the mod-els, the planning front often assumes accurate pose infor-mation at the input. This makes explicit object pose esti-mation [33, 35, 50, 61, 66] a necessary part of the pipeline, and the full system susceptible to pose estimation error from real vision systems.
Recent efforts in robotics have attempted to relax these constraints by leveraging the power of deep learning. A re-cent approach called NeRP, proposed by Qureshi et al. [51], has allowed for rearranging objects unseen at the training time, by representing the observed objects with learned embeddings.
It also removes the need of explicit object pose estimation for planning by leveraging recent progress on learning-based grasp planners [62] and collision detec-tors [7]. However, NeRP only allows moving objects with 2D in-plane translations on the table surface and allows no change in their orientation. This prevents its applications in realistic scenarios that require moving objects with more complex transformations such as those shown in Fig. 1.
We propose a new approach to image-guided robotic ob-ject rearrangement with RGB-D input. It achieves, for the first time to the best our of knowledge, the ability to handle unknown objects with translation as well as planar rotations.
The key to our method is re-formulating object rear-rangement as an iterative minimization of optical flow be-tween the current observed image and the goal image. By using optical flow as an intermediate representation, we can capitalize on the cutting edge development in flow estima-tion models [63]. Although these flow models were orig-inally developed for consecutive video frames with small pixel displacements, we show that with proper training, these models can excel at estimating flow with large dis-placements from arbitrary transformation of objects. Us-ing this estimated flow, together with the depth input and generic object segmentation models [69], we can obtain dense 3D correspondences for each object. This provides a general representation that allows us to solve for the desired transformation of objects with simple optimization. Fur-thermore, with such a general representation, our method trained entirely on synthetic data transfers well to the real world in a zero shot manner.
To summarize, we introduce IFOR, Iterative Flow Min-imization for Unseen Object Rearrangement. IFOR is to our knowledge the first system capable of rearranging un-seen objects, given an RGB-D image goal, that handles both translation and rotation. Our approach is trained solely on simulation data and transfers to the real world in a zero-shot manner. Finally, we perform a set of experiments showing our method allows rearrangement of novel objects in clut-tered scenes with a real robot. 2.