Abstract 3D face reconstruction from a single image is a task that has garnered increased interest in the Computer Vision community, especially due to its broad use in a number of applications such as realistic 3D avatar creation, pose in-variant face recognition and face hallucination. Since the introduction of the 3D Morphable Model in the late 90’s, we witnessed an explosion of research aiming at particu-larly tackling this task. Nevertheless, despite the increasing level of detail in the 3D face reconstructions from single images mainly attributed to deep learning advances, finer and highly deformable components of the face such as the tongue are still absent from all 3D face models in the liter-ature, although being very important for the realness of the 3D avatar representations. In this work we present the first, to the best of our knowledge, end-to-end trainable pipeline that accurately reconstructs the 3D face together with the tongue. Moreover, we make this pipeline robust in “in-the-wild” images by introducing a novel GAN method tailored for 3D tongue surface generation. Finally, we make pub-licly available to the community the first diverse tongue dataset, consisting of 1, 800 raw scans of 700 individuals varying in gender, age, and ethnicity backgrounds*. As we demonstrate in an extensive series of quantitative as well as qualitative experiments, our model proves to be robust and realistically captures the 3D tongue structure, even in adverse “in-the-wild” conditions. 1.

Introduction
Recently, 3D face reconstruction from single “in-the-wild” images has been a very active topic in Computer Vi-sion with applications ranging from realistic 3D avatar cre-ation to image imputation and face recognition [12, 14, 22, 37, 39, 44]. Nevertheless, despite the improvement in the quality of the 3D reconstructions, all of these methods do not accommodate any statistical variations in the oral cav-ity let alone a tongue template mesh. As a result, the oral region is completely disregarded from the final result.
Being able to reconstruct the tongue expression has mul-tiple advantages in various applications. First of all, the generated avatars would be more realistic and would be able
*Authors contributed equally.
*Project url: www.github.com/steliosploumpis/tongue
to mimic many more facial expressions. Moreover, speech animation tasks would be improved as the inclusion of the oral cavity plays a significant role. Finally, face recognition applications could be enhanced as more extreme poses and expressions would be modeled.
However, as we already pointed out, all of the current state-of-the-art (SOTA) methods [14, 39, 44] do not con-tain the tongue component in their implementations. This is because of two reasons: a) there is no publicly available tongue dataset, and b) it is very challenging to carry out 3D reconstruction of the face together with the tongue in
“in-the-wild” conditions, because of the highly deformable nature of the human tongue.
To tackle the absence of tongue data, we collected a large and diverse dataset of textured 3D tongue point-clouds (more info about the data in Section 3). Having captured the data, we created a pipeline which is comprised of the following parts: a) a tongue point-cloud autoencoder (AE) which is used to derive useful 3D features of our raw col-lected 3D data, b) a tongue image encoder optimized based on the aforementioned 3D features, c) a shape decoder which translates the encoder outputs to the parameter space of the Universal Head Model (UHM) [33]. We should note that the UHM in our case is further rigged/modified so that it can model various tongue shapes/expressions, as explained in Section 3. We begin by training the AE in step a) and then we train steps b-c) in an end-to-end fashion so that the out-put tongue expression of the UHM model is as close as pos-sible to the corresponding ground-truth 3D tongue point-cloud of the 2D tongue image.
Since there is a lack of ground-truth 3D tongue data cor-responding to “in-the-wild” 2D tongue images, the pipeline we described so far is only trained using our collected data which were captured under controlled conditions. This re-sults in sub-optimal performance in “in-the-wild” condi-tions. To remedy this, we developed a novel conditional
GAN framework that is able to generate accurate 3D tongue point-clouds based on the image encoder outputs (step b) of the pipeline). Having created new image/point-cloud pairs of “in-the-wild” tongue data, we re-train the pipeline using also these new data. As we show in Section 4, this addition substantially improves the quality of the tongue reconstruc-tions. To summarize, the contributions of our work are the following:
• We release a dataset of 1, 800 raw tongue scans of vari-ous shapes and positions, corresponding to around 700 subjects. Being the first such diverse tongue dataset, it can be proven very useful to the community.
• We present a complete pipeline trained in an end-to-end fashion that is able to reconstruct the 3D face to-gether with the tongue from a single image.
• To make this pipeline robust to “in-the-wild” images, we introduce a novel GAN framework which is able to accurately reconstruct 3D tongues from “in-the-wild” images with an increasing level of detail. 2.