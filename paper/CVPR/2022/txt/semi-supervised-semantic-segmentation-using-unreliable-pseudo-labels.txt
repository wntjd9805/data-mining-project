Abstract
The crux of semi-supervised semantic segmentation is to assign adequate pseudo-labels to the pixels of unlabeled images. A common practice is to select the highly confident predictions as the pseudo ground-truth, but it leads to a problem that most pixels may be left unused due to their unreliability. We argue that every pixel matters to the model training, even its prediction is ambiguous.
Intuitively, an unreliable prediction may get confused among the top classes (i.e., those with the highest probabilities), how-ever, it should be confident about the pixel not belonging to the remaining classes. Hence, such a pixel can be convincingly treated as a negative sample to those most unlikely categories. Based on this insight, we develop an effective pipeline to make sufficient use of unlabeled data.
Concretely, we separate reliable and unreliable pixels via the entropy of predictions, push each unreliable pixel to a category-wise queue that consists of negative samples, and manage to train the model with all candidate pixels.
Considering the training evolution, where the prediction becomes more and more accurate, we adaptively adjust the threshold for the reliable-unreliable partition. Experi-mental results on various benchmarks and training settings demonstrate the superiority of our approach over the state-of-the-art alternatives.1 1.

Introduction
Semantic segmentation is a fundamental task in the com-puter vision field, and has been significantly advanced along 1Project: https://haochen-wang409.github.io/U2PL.
∗Corresponding author. This work is sponsored by National Natural
Science Foundation of China (62176152).
†Equal contribution, and this work is done during the internship at
SenseTime Research.
‡Rui Zhao is also with Qing Yuan Research Institute, Shanghai Jiao
Tong University.
Figure 1. Category-wise performance and statistics on number of pixels with reliable and unreliable predictions. Model is trained using 732 labeled images on PASCAL VOC 2012 [16] and evaluated on the remaining 9, 850 images. with the rise of deep neural networks [7,30,36,47]. Existing supervised approaches rely on large-scale annotated data, which can be too costly to acquire in practice. To alleviate this problem, many attempts [1,4,11,17,23,34,44,49] have been made towards semi-supervised semantic segmenta-tion, which learns a model with only a few labeled samples and numerous unlabeled ones. Under such a setting, how to adequately leverage the unlabeled data becomes critical.
A typical solution is to assign pseudo-labels to the pixels without annotations. Concretely, given an unlabeled image, prior arts [28,42] borrow predictions from the model trained on labeled data, and use the pixel-wise prediction as the
“ground-truth” to in turn boost the supervised model. To mitigate the problem of confirmation bias [2], where the model may suffer from incorrect pseudo-labels, existing approaches propose to filter the predictions with their confidence scores [43, 44, 51, 52]. In other words, only the highly confident predictions are used as the pseudo-labels, while the ambiguous ones are discarded.
However, one potential problem caused by only using reliable predictions is that some pixels may never be learned
in the entire training process. For example, if the model cannot satisfyingly predict some certain class (e.g., chair in Fig. 1), it becomes difficult to assign accurate pseudo-labels to the pixels regarding such a class, which may lead to insufficient and categorically imbalanced training. From this perspective, we argue that, to make full use of the unlabeled data, every pixel should be properly utilized.
As discussed above, directly using the unreliable pre-dictions as the pseudo-labels will cause the performance
In this paper, we propose an alternative degradation [2]. way of Using Unreliable Pseudo-Labels. We call our framework as U2PL. First, we observe that, an unreliable prediction usually gets confused among only a few classes instead of all classes. Taking Fig. 2 as an instance, the pixel with white cross receives similar probabilities on class motorbike and person, but the model is pretty sure about this pixel not belonging to class car and train.
Based on this observation, we reconsider the confusing pixels as the negative samples to those unlikely categories.
Specifically, after getting the prediction from an unlabeled image, we employ the per-pixel entropy as the metric (see
Fig. 2) to separate all pixels into two groups, i.e., a reliable one and an unreliable one. All reliable predictions are used to derive positive pseudo-labels, while the pixels with unreliable predictions are pushed into a memory bank, which is full of negative samples. To avoid all negative pseudo-labels only coming from a subset of categories, we employ a queue for each category. Such a design ensures that the number of negative samples for each class is balanced. Meanwhile, considering that the quality of pseudo-labels becomes higher along with the model gets more and more accurate, we come up with a strategy to adaptively adjust the threshold for the partition of reliable and unreliable pixels.
We evaluate the proposed U2PL on PASCAL VOC 2012 [16] and Cityscapes [12] under a wide range of training settings, where our approach surpasses the state-of-the-art competitors. Furthermore, through visualizing the segmentation results, we find that our method achieves much better performance on those ambiguous regions (e.g., the border between different objects), thanks to our ade-quate use of the unreliable pseudo-labels. 2.