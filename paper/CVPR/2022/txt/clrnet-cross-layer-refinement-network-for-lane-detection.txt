Abstract
Lane is critical in the vision navigation system of the in-telligent vehicle. Naturally, lane is a trafﬁc sign with high-level semantics, whereas it owns the speciﬁc local pattern which needs detailed low-level features to localize accu-rately. Using different feature levels is of great importance for accurate lane detection, but it is still under-explored.
In this work, we present Cross Layer Reﬁnement Net-work (CLRNet) aiming at fully utilizing both high-level and low-level features in lane detection. In particular, it ﬁrst de-tects lanes with high-level semantic features then performs reﬁnement based on low-level features. In this way, we can exploit more contextual information to detect lanes while leveraging local detailed lane features to improve localiza-tion accuracy. We present ROIGather to gather global con-text, which further enhances the feature representation of lanes. In addition to our novel network design, we introduce
Line IoU loss which regresses the lane line as a whole unit to improve the localization accuracy. Experiments demon-strate that the proposed method greatly outperforms the state-of-the-art lane detection approaches. Code is avail-able at:https://github.com/Turoad/CLRNet. 1.

Introduction
Lane detection is an important yet challenging task in computer vision, which requires the network to predict lanes in an image. Detecting lanes can beneﬁt many ap-plications, such as autonomous driving and the Advanced
Driver Assistance System (ADAS), which helps intelligent vehicles localize themselves better and drive safer.
Beneﬁting from the effective feature representation of
CNN, many approaches [17, 19, 33] have obtained promis-ing performance. However, there are still some challenges for detecting accurate lanes. Lane has high-level seman-tics, whereas it owns the speciﬁc local pattern which needs detailed low-level features to localize accurately. How to utilize different feature levels effectively in CNN remains
∗Equal contribution. (a) (c) (b) (d)
Figure 1. Illustrations of hard cases for lane detection. (a) The detection result of low-level features. It mistakes landmark as lane due to losing global context. (b) The detection result of high-level features. It predicts inaccurate localization of the lane. (c) The case that lane is almost occupied by the car. (d) The case that lane is blurred by the extreme lighting condition. a problem. As we can see in Fig. 1(a), the landmark and lane line have different semantics, but they share the similar feature (e.g., the long white line). It is hard to distinguish them without high-level semantics and global context. On the other hand, the locality is also essential since lane is long and thin with the simple local pattern. We show the detec-tion result of high-level features in Fig 1(b), though the lane is detected, its location is not precise. Thus, the low-level and high-level information are complementary for accurate lane detection. Previous works either model local geometry of lanes and integrate them into global results [20] or con-struct a fully-connected layer with global features to predict lanes [19]. These detectors have demonstrated the impor-tance of local or global features for lane detection, but they don’t take advantage of both features, yielding inaccurate detection performance.
Another common problem in lane detection is no visual evidence for the presence of lanes. As shown in Fig. 1(c), the lane is occupied by the car while in Fig. 1(d), the lane
is hard to recognize due to the extreme lighting condition.
In the literature, SCNN [17] and RESA [33] propose a message-passing mechanism to gather global context, but these methods perform pixel-wise prediction and don’t take lane as a whole unit. Thus their performances lag behind many state-of-the-art detectors.
In this paper, we propose a new framework, Cross Layer
Reﬁnement Network (CLRNet), which fully utilizes low-level and high-level features for lane detection. Speciﬁ-cally, we ﬁrst perform detection in high semantic features to coarsely localize lanes. Then, we perform reﬁnement based on ﬁne-detail features to get more precise locations.
Progressively reﬁning the location of lane and feature ex-traction leads to high accuracy detection results. To solve the problem of non-visual evidence of lane, we introduce
ROIGather to capture more global contextual information by building the relation between the ROI lane feature and the whole feature map. Moreover, we deﬁne the IoU of lane lines and propose the Line IoU (LIoU) loss to regress the lane as a whole unit and considerably improve the perfor-mance compared with standard loss, i.e., smooth-l1 loss.
We demonstrate the effectiveness of our method on three lane detection benchmarks, i.e., CULane [17], Tusimple
[26], and LLAMAS [2]. The experiment results show our method achieves state-of-the-art accuracy on all datasets.
The main contributions can be summarized as follows:
• We demonstrate low-level and high-level features are complementary for lane detection, and we propose a novel network architecture (CLRNet) to fully utilize low-level and high-level features for lane detection.
• We propose ROIGather to further enhance the repre-sentation of lane features by gathering global context, which can also be plugged into other networks.
• We propose Line IoU (LIoU) loss tailored for lane de-tection, regressing the lane as the whole unit and con-siderably improving the performance.
• To better compare the localization accuracy of differ-ent detectors, we also adopt the new mF1 metrics. We demonstrate the proposed method greatly outperforms other state-of-the-art approaches on three lane detec-tion benchmarks. 2.