Abstract or e.g.,
The performance of current Scene Graph Generation models is severely hampered by some hard-to-distinguish
“woman-on/standing on/walking on-predicates,
“woman-near/looking at/in front of-child”. beach”
While general SGG models are prone to predict head predicates and existing re-balancing strategies prefer tail categories, none of them can appropriately handle these hard-to-distinguish predicates. To tackle this issue, inspired by fine-grained image classification, which focuses on differentiating among hard-to-distinguish object classes, we propose a method named Fine-Grained Predicates
Learning (FGPL) which aims at differentiating among hard-to-distinguish predicates for Scene Graph Gener-ation task.
Specifically, we first introduce a Predicate
Lattice that helps SGG models to figure out fine-grained predicate pairs.
Then, utilizing the Predicate Lattice, we propose a Category Discriminating Loss and an
Entity Discriminating Loss, which both contribute to distinguishing fine-grained predicates while maintaining learned discriminatory power over recognizable ones. The proposed model-agnostic strategy significantly boosts the performances of three benchmark models (Transformer,
VCTree, and Motif) by 22.8%, 24.1% and 21.7% of Mean
Recall (mR@100) on the Predicate Classification sub-task, respectively. Our model also outperforms state-of-the-art methods by a large margin (i.e., 6.1%, 4.6%, and 3.2% of
Mean Recall (mR@100)) on the Visual Genome dataset.
Codes are publicly available1. 1.

Introduction
Scene graph generation plays a vital role in visual un-derstanding, which intends to detect instances together with
*Corresponding author. 1https://github.com/XinyuLyu/FGPL
Figure 1. The illustration of handling hard-to-distinguish pred-icates for SSG models. (b) Transformer (FGPL) outperforms both Transformer and Transformer (Re-weight) on Group Mean (c) Transformer [20, 23] is prone to predict head pred-Recall. (e) (d) Transformer (Re-weight) prefers tail categories. icates.
Transformer (FGPL) can appropriately handle hard-to-distinguish predicates, e.g., “woman-on/standing on/walking on-beach” or
“woman-near/looking at/in front of-child”. their relationships. By ultimately representing image con-tents in a graph structure, scene graph generation serves as a powerful means to bridge the gap between visual scenes and human languages, benefiting several visual-understanding tasks, such as image retrieval [16, 33], image captioning [6, 30], and visual question answering [9, 10, 12, 17, 18, 22, 26].
Prior works [8, 13, 14, 19, 22, 27, 32] have devoted great efforts to exploring representation learning for scene graph generation, but the biased prediction is still challenging be-cause of the long-tailed distribution of predicates in SGG datasets. Trained with severely skewed class distributions, general SGG models are prone to predict head predicates, as results of Transformer [20, 23] shown in Fig. 1(c). Re-cent works [2, 7, 11, 31] have exploited re-balancing meth-ods to solve the biased prediction problem for scene graph generation, making predicates distribution balanced or the learning process smooth. As demonstrated in Fig. 1(b),
Transformer (Re-weight) achieves a more balanced perfor-mance than Transformer. However, relying on the class dis-tribution, existing re-balancing strategies prefer predicates from tail categories while being hampered by some hard-to-distinguish predicates. For instance, as shown in Fig. 1(d),
Transformer (Re-weight) misclassifies “woman-in front of-child” as “woman-looking at-child” in terms of visual cor-relations between “in front of” and “looking at”.
The origin of the issue lies in the fact that differentiating hard-to-distinguish predicates requires exploring their cor-relations. Underestimating correlations among predicates, existing methods [28,31] cannot choose hard-to-distinguish ones for sufficient punishment. To acquire complete predi-cate correlations, we consider contextual information since correlations between a pair of predicates may dramatically vary with contexts as stated in [15]. Particularly, con-texts are regarded as visual or semantic information of predicates’ objects and subjects in scene graph generation.
Take predicate correlations analysis between “watching” and “playing” as an example. “Watching/playing” is weakly correlated or distinguishable in Fig. 2(b), while they are strongly correlated or hard-to-distinguish in Fig. 2(a).
Inspired by the above observations, we propose a Fine-Grained Predicates Learning (FGPL) framework by thor-oughly exploiting predicate correlations. We first introduce a Predicate Lattice to help understand ubiquitous predicate correlations concerning all scenarios in the SGG dataset.
With the Predicate Lattice, we devise a Category Discrim-inating Loss (CDL) and an Entity Discriminating Loss (EDL), which both discriminate hard-to-distinguish predi-cates while maintaining learned discriminatory power over recognizable ones. In particular, Category Discriminating
Loss (CDL) attempts to figure out and differentiate hard-to-distinguish predicates. Furthermore, as predicates’ correla-tion varies with contexts of entities, Entity Discriminating
Loss (EDL) adaptively adjusts the discriminating process according to predictions of entities. Using CDL and EDL, our method can determine whether predicate pairs are hard-to-distinguish or not during training, which guarantees a more balanced learning process among different categories than previous methods [2, 7, 11, 28, 31].
Contribution: Our main contributions are summarized as follows: 1). We propose a novel plug-and-play Fine-Grained Predicates Learning (FGPL) framework to differ-entiate hard-to-distinguish predicates for scene graph gen-eration. 2). We devise a Predicate Lattice to obtain com-plete predicate correlations between each predicate pair concerning context information. Category Discriminating
Loss (CDL) aims at figuring out and differentiating hard-to-distinguish predicates. Moreover, Entity Discriminating
Figure 2. The illustration of predicate correlations concern-ing contexts. The predicate correlation between “watching” and
“playing” varies with contexts. Especially, “watching/playing” is weakly correlated or distinguishable in (b), while they are strongly correlated or hard-to-distinguish in (a).
Loss (EDL) adaptively adjusts the discriminating process according to predictions of entities. 3). Our FGPL greatly boosts performances of three benchmark models (Trans-former, VCTree, and Motif) by 22.8%, 24.1%, and 21.7% of Mean Recall (mR@100) on Predicate Classification sub-task and achieves superior performances over state-of-the-art methods by a large margin (i.e., 6.1%, 4.6% and 3.2% of
Mean Recall (mR@100)) on Visual Genome dataset. 2.