Abstract
Modern GANs excel at generating high quality and di-verse images. However, when transferring the pretrained
GANs on small target data (e.g., 10-shot), the generator tends to replicate the training samples. Several methods have been proposed to address this few-shot image gener-ation task, but there is a lack of effort to analyze them under a unified framework. As our first contribution, we propose a framework to analyze existing methods during the adapta-tion. Our analysis discovers that while some methods have disproportionate focus on diversity preserving which impede quality improvement, all methods achieve similar quality af-ter convergence. Therefore, the better methods are those that can slow down diversity degradation. Furthermore, our analysis reveals that there is still plenty of room to further slow down diversity degradation.
Informed by our analysis and to slow down the diver-*Corresponding Author sity degradation of the target generator during adaptation, our second contribution proposes to apply mutual informa-tion (MI) maximization to retain the source domain’s rich multi-level diversity information in the target domain gen-erator. We propose to perform MI maximization by con-trastive loss (CL), leverage the generator and discrimina-tor as two feature encoders to extract different multi-level features for computing CL. We refer to our method as Dual
Contrastive Learning (DCL). Extensive experiments on sev-eral public datasets show that, while leading to a slower diversity-degrading generator during adaptation, our pro-posed DCL brings visually pleasant quality and state-of-the-art quantitative performance. 1.

Introduction
Powerful Generative Adversarial Networks (GANs) [4, 14, 24] have been built in recent years that can generate im-ages with high fidelity and diversity [6, 44]. Unfortunately,
these GANs often require large-scale datasets and compu-tational expensive resources to achieve good performance.
For example, StyleGAN [23] is trained on Flickr-Faces-HQ (FFHQ) [23], which contains 70,000 images, with almost 56
GPU days. When the dataset size is decreased, however, the generator often tends to replicate the training data [11].
Would it be possible to generate sufficiently diverse im-ages, given only limited training data? For example, with 10-shot sketch style human faces [45], could we generate diverse face sketch paintings? This few-shot image gener-ation task is important in many real-world applications with limited data, e.g., artistic domains. It can also benefit some downstream tasks, e.g., few-shot image classification [3]. 1.1. A Closer Look at Few-shot Image Generation
To address this few-shot image generation task, instead of training from scratch [42, 51], recent literature focus on transfer learning [2, 35, 50] based ideas, i.e., leveraging the prior knowledge of a GAN pretrained on a large-scale, di-verse dataset of the source domain and adapting it to a small target domain, without access to the source data. The early method is based on fine-tuning [47]. In particular, starting from the pretrained generator Gs, the original GAN loss [14] is used to adapt the generator to the new domain: min
Gt max
Dt
Ladv = Ex∼pdata(x)[log Dt(x)] (1)
+ Ez∼pz(z)[log (1 − Dt(Gt(z)))], where z is sampled from a Gaussian noise distribution pz(z), pdata(x) is the probability distribution of the real target do-main data x, Gt and Dt are generator and discriminator of the target domain, and Gt is initialized by the weights of Gs.
This GAN loss in Eqn. 1 forces Gt to capture the statistics of the target domain data, thereby to achieve both good qual-ity (realisticness w.r.t. target domain data) and diversity, the criteria for a good generator.
However, for few-shot setup (e.g. only 10 target domain images), such approach is inadequate to achieve diverse tar-get image generation as very limited samples are provided to define pdata(x). Recognizing that, recent methods [28, 33] have focused disproportionately on improving diversity by preserving diversity of the source generator during the gen-In [28], Elastic Weight Consolidation erator adaptation. (EWC) [25] is proposed to limit changes in some impor-tant weights to preserve diversity.
In [33], an additional
Cross-domain Correspondence (CDC) loss is introduced to preserve the sample-wise distance information of source to maintain diversity, and the whole model is trained via a multi-task loss with the diversity loss Ldist as an auxiliary task to regularize the main GAN task with loss Ladv: min
Gt max
Dt
Ladv + Ldist. (2)
In [33], a patch discriminator [20, 59] is also used to further improve the performance in Ladv. Details of Ldist in [33].
Diversity preserving methods [28, 33] have demonstrated impressive results based on Fr´echet Inception Distance (FID)
[18] which measures the quality and diversity of the gen-erated samples simultaneously. However, second thoughts about these methods reveal some questions:
• With disproportionate focus on diversity preserving in recent works [28,33], will quality of the generated sam-ples be compromised? For example, in Eqn. 2, Ladv is responsible for quality improvement during adapta-tion, but Ldist may compete with Ladv as it has been observed in multi-task learning [12, 39]. We note that this has not been analyzed thoroughly.
• With recent works’ strong focus on diversity preserv-ing [28, 33], will there still be room to further improve via diversity preserving? How could we know when the gain of diversity preserving approaches become satu-rated (without excessive trial and error)? 1.2. Our Contributions
In this paper, we take the first step to address these re-search gaps for few-shot image generation. Specifically, as our first contribution, we propose to independently ana-lyze the quality and diversity during the adaptation. Using this analysis framework, we obtain insightful information on quality/diversity progression. In particular, on one hand, it is true that strong diversity preserving methods such as [33] indeed impede the progress of quality improvement. On the other hand, interestingly, we observe that these methods can still reach high quality rather quickly, and after quality con-verges they have no worse quality compared to other meth-ods such as [47] which uses simple GAN loss (Eqn. 1).
Therefore, methods with disproportionate focus on preserv-ing diversity [33] stand out from the rests as they can pro-duce slow diversity-degrading generators, maintaining good diversity of generated images when their quality reaches the convergence. Furthermore, our analysis reveals that there is still plenty of room to further slow down the diversity degra-dation across several source → target domain setups.
Informed by our analysis, our second contribution is to propose a novel strong regularization to take a further step in slowing down the diversity degradation, with the understand-ing that it is unlikely to compromise quality as observed in our analysis. Our proposed regularization is based on the observation that rich diversity exists in the source images at different semantic levels: diversity in middle levels such as hair style, face shape, and that in high levels such as facial ex-pression (smile, grin, concentration). However, such source diversity can be easily ignored in the images produced by tar-get domain generators. Therefore, to preserve source diver-sity information, we propose to maximize the mutual infor-mation (MI) between the source/target image features orig-inated from the same latent code, via contrastive loss [34] (CL). To compute CL, we leverage the generator and dis-criminator as two feature encoders to extract image features
at multiple feature scales, such that we can preserve diver-sity at various levels. By combining the two feature encoders (generator and discriminator), we gain additional feature di-versity. We show that our proposed Dual Contrastive Learn-ing (DCL) outperforms the previous work in slowing down the diversity degradation without compromising the image quality on the target domain, and hence achieving the state-of-the-art performance. 2.