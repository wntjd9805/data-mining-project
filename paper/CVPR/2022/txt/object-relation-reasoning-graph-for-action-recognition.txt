Abstract
Action recognition is a challenging task since the at-tributes of objects as well as their relationships change con-stantly in the video. Existing methods mainly use object-level graphs or scene graphs to represent the dynamics of objects and relationships, but ignore modeling the ﬁne-grained relationship transitions directly. In this paper, we propose an Object-Relation Reasoning Graph (OR2G) for reasoning about action in videos. By combining an object-level graph (OG) and a relation-level graph (RG), the pro-posed OR2G catches the attribute transitions of objects and reasons about the relationship transitions between objects simultaneously. In addition, a graph aggregating module (GAM) is investigated by applying the multi-head edge-to-node message passing operation. GAM feeds back the in-formation from the relation node to the object node and en-hances the coupling between the object-level graph and the relation-level graph. Experiments in video action recog-nition demonstrate the effectiveness of our approach when compared with the state-of-the-art methods. 1.

Introduction
Action recognition is one of the fundamental tasks in the
ﬁeld of video understanding [8], and it remains an active topic in the vision research community. The goal of action recognition is to identify activities in the video according to object states. As it is difﬁcult to capture object transitions by a global representation of the video [2, 25], object-based action recognition has attracted increasing attention.
Object-based methods [30] mainly represent the objects as the nodes of a graph and obtain the action category through graph reasoning at the object-level. However, these methods often fail to explicitly model the interaction be-tween objects. To consider object interactions in action recognition, current efforts [14] decompose the objects and relationships in a video according to the event segmenta-tion theory [18], where events can be divided into consistent
∗Corresponding author: Zhenzhong Chen.
Figure 1. Illustration of ﬁne-grained relationship transition in the videos. Two examples of different action labels with same subject-object pairs but different relationship transitions are presented. groups or represented as hierarchical structures. However, these methods represent the dynamics of objects and rela-tionships by aggregating them from the scene graph, and ignore modeling and reasoning about ﬁne-grained relation-ship transitions.
Fine-grained relationship transition plays an important role in distinguishing action categories, especially actions with similar characteristics. Figure 1 shows two examples of similar video content.
In both cases, the subject is a person and the objects are a chair and a pillow. However, there are some slight differences in the relationship transi-tions between the subject and objects, resulting in different actions. For the visual relationship between the <person-chair> pair, when the transition is ‘sitting on’ → ‘not con-tacting’, it represents the action of ‘Someone is standing up from somewhere’. However, when the transition is ‘not contacting’ → ‘sitting on’, it represents another action of
‘Someone is going from standing to sitting’. For the visual relationship between the <person-pillow> pair, when the transition is ‘holding’ → ‘leaning on’, it represents the ac-tion of ‘Taking a pillow from somewhere’ and ‘Putting a pillow somewhere’. However, in the transition of ‘holding’
→ ‘touching’ → ‘carring’, the actions change to ‘Holding a pillow’ and ‘Snuggling with a pillow’. Therefore, model-ing the relationship transition of independent subject-object pairs across key frames is crucial for the action recognition task. It helps the network to reason the video actions in a human-like way, thus to enhance the explainability of the network and get more precise action categories.
In this paper, we propose an Object-Relation Reasoning
Graph (OR2G) to model the ﬁne-grained transition of the objects and relationships in a video as shown in Figure 2.
Based on the above analysis, we split multiple objects and relationships into independent items, and further decom-pose the actions in detail. Firstly, for ﬁne-grained modeling of object attribute transition, we propose an actor-centric object-level graph (OG). By modeling the dependencies be-tween the subject and the objects in an actor-centric way, the object-level graph captures more critical information about object interactions. Secondly, considering the impact of re-lationship transition between the subject and the objects, a relation-level graph (RG) is proposed to model the depen-dencies among ﬁne-grained relationships along the tempo-ral dimension. Finally, to enhance the coupling between the two graphs, we propose a graph aggregating module (GAM). With a multi-head attention edge-to-node message passing operation, the information of relation-level graph feeds back to the object-level graph in spatial dimension.
Our contributions can be summarized as follows:
• OR2G is proposed to model ﬁne-grained attribute and relationship transition for the challenging problem of distinguishing actions with similar characteristics. By reasoning on both object-level graph and relation-level graph, OR2G explains more clearly how actions occur through the slight transition of attributes and relation-ships, and create interpretable representation for a va-riety of complex actions.
• A graph aggregating module which adopts multi-head attention edge-to-node message passing operation is proposed to make the two graphs more coupled. The information of relation-level graph feeds back to the object-level graph in the spatial dimension, permitting a more reasonable utilization of relationship transition information.
The remainder of this article is structured as follows.
Section 2 gives an overview of related work. Section 3 presents our proposed method. Section 4 provides the im-plementation details and experimental results on the public action recognition dataset, followed by the conclusions in
Section 5. 2.