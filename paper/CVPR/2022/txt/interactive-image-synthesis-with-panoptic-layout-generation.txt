Abstract
Interactive image synthesis from user-guided input is a challenging task when users wish to control the scene struc-ture of a generated image with ease. Although remarkable progress has been made on layout-based image synthesis approaches, existing methods require high-precision inputs such as accurately placed bounding boxes, which might be constantly violated in an interactive setting. When place-ment of bounding boxes is subject to perturbation, layout-based models suffer from “missing regions” in the con-structed semantic layouts and hence undesirable artifacts in the generated images. In this work, we propose Panop-tic Layout Generative Adversarial Network (PLGAN) to ad-dress this challenge. The PLGAN employs panoptic the-ory which distinguishes object categories between “stuff” with amorphous boundaries and “things” with well-deﬁned shapes, such that stuff and instance layouts are constructed through separate branches and later fused into panoptic layouts. In particular, the stuff layouts can take amorphous shapes and ﬁll up the missing regions left out by the in-stance layouts. We experimentally compare our PLGAN with state-of-the-art layout-based models on the COCO-Stuff, Visual Genome, and Landscape datasets. The ad-vantages of PLGAN are not only visually demonstrated but quantitatively veriﬁed in terms of inception score, Fr´echet inception distance, classiﬁcation accuracy score, and cov-erage. The code is available at https://github.com/wb-ﬁnalking/PLGAN. 1.

Introduction
Tremendous progress has been made on conditional im-age synthesis for creative design. Among different for-mats of conditional inputs are categories [2, 14, 25–27, 41], source images [15,22,44], text description [31,38,42], scene graphs [1, 12, 40] and semantic layouts [28, 36, 45]. To date, interactive image synthesis via conditional generative mod-els remains a contemporary challenge. Text-to-image mod-*Corresponding author. m
I 2 d i r
G
N
A
G
L
P
Object Input
Instance Layout Generated Image
Instance/Stuff Input Panoptic Layout Generated Image
Figure 1. Scene-to-image synthesis by Grid2Im [1] vs. PLGAN.
Unlike the instance layout utilized by Grid2Im that treats all ob-jects as instances (things), the panoptic layout by PLGAN distin-guishes objects between instances and stuff, thus eliminating ar-tifacts in the missing regions (marked black in the layout). els usually suffers from reasoning object locations and rela-tions [42]. Image synthesis from semantic layouts provides an alternative way for computer-human interaction [9, 21] and yields aesthetically pleasing results [28, 36, 45]. How-ever, high-quality semantic layouts demand professional skills in free-hand drawing from scratch by users, which prevent them from being used as a drag-and-drop GUI for novice users. In this respect, scene graph has attracted much attention in recent years [1, 12], as it requires only multiple object placements on the artboard and allows user-friendly manipulation with individual objects.
A milestone in scene-graph-to-image synthesis is made by Grid2Im [1], which roughly consists of two stages: lay-out construction and image generation. First, the input con-ditions are passed to construct an instance layout with per-object masks and bounding boxes. Secondly, conditioned on the instance layout a photo-realistic image is sythe-sized as the outcome of the image generation stage. Whilst
Grid2Im [1] requires ground truth segmentation maps as su-pervision signals, LostGAN [34, 35] can learn the interme-diate instance layout in a weakly-supervised way.
Besides [1, 34, 35], instance layout-based generative models such as [9, 12, 21, 43] have also driven progress on many cross-domain image synthesis tasks. A common caveat among aforementioned methods consists in that they are sensitive to spatial perturbation of scene objects and suf-fer from the region missing problem, especially in interac-tive scenarios. They may predict intermediate layouts with empty areas where pixels may not have correct category in-formation. During the training phase, ground-truth bound-ing boxes and masks usually covers the whole image lat-tice. However, in interactive scenarios users can place ob-jects with bounding boxes arbitrarily. In addition, the pre-dicted per-object masks will not ﬁll up the corresponding bounding boxes. Therefore, the intermediate layout may not be covered completely by object masks, yielding the region missing problem. An illustration of region missing by Grid2Im [1] is displayed in Fig. 1. Unsurprisingly, an imperfect semantic layout containing empty areas induces undesirable artifacts in the generated image.
In this work, we propose Panoptic Layout Generative
Adversarial Network (PLGAN) for interactive image syn-thesis. Different from prior works that treat all objects in-variably as instances (or things), we employ the panoptic segmentation theory [17] that splits objects into uncount-able stuff (which refers to amorphous background such as grass, sky or sea) and countable things (which are fore-ground objects with well-deﬁned shapes such as people, an-imals or vehicles). Furthermore, we develop the panoptic layout generation (PLG) module, which utilizes in paral-lel a stuff branch for stuff layout construction and an in-stance branch for instance layout construction. The instance branch predicts per-instance bounding boxes and maskes as in [1, 34]. The stuff branch generates pixel-wise masks for all stuff objects that cover the whole image lattice. Then the instance layout and the stuff layout are combined into a panoptic layout through an Instance- and Stuff-Aware Nor-Images synthesis from malization (ISA-Norm) module. such a panoptic layout successfully eliminates missing re-gions and behaves more robust to perturbation of object lo-cations; see Fig. 1 for a visualized example.
Our contributions are summarized as follows:
• We leverage panoptic layouts in interactive image syn-thesis to resolve the region missing problem inheritted by current instance layout-based approaches.
• Regarding model architecture, we propose to separate treatment of stuff and thing objects during layout con-structions and later fuse the constructed instance and stuff layouts into a panoptic layout via Instance- and
Stuff-Aware Normalization (ISA-Norm).
• Our experiments show qualitative and quantitative comparisons on the COCO-Stuff dataset, Visual
Genome, and Landscape datasets, and demonstrate of the merits of our PLGAN over state-of-the-art layout-based approaches. 2.