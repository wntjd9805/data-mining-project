Abstract
The task of Human-Object Interaction (HOI) detection could be divided into two core problems, i.e., human-object association and interaction understanding.
In this paper, we reveal and address the disadvantages of the conventional query-driven HOI detectors from the two aspects. For the association, previous two-branch methods suffer from com-plex and costly post-matching, while single-branch methods ignore the features distinction in different tasks. We pro-pose Guided-Embedding Network (GEN) to attain a two-branch pipeline without post-matching.
In GEN, we de-sign an instance decoder to detect humans and objects with two independent query sets and a position Guided Embed-ding (p-GE) to mark the human and object in the same position as a pair. Besides, we design an interaction de-coder to classify interactions, where the interaction queries are made of instance Guided Embeddings (i-GE) gener-ated from the outputs of each instance decoder layer. For the interaction understanding, previous methods suffer from long-tailed distribution and zero-shot discovery. This pa-per proposes Visual-Linguistic Knowledge Transfer (VLKT) training strategy to enhance interaction understanding by transferring knowledge from a visual-linguistic pre-trained model CLIP. In specific, we extract text embeddings for all labels with CLIP to initialize the classifier and adopt a mimic loss to minimize the visual feature distance between
GEN and CLIP. As a result, GEN-VLKT outperforms the state of the art by large margins on multiple datasets, e.g.,
+5.05 mAP on HICO-Det. The source codes are available at https://github.com/YueLiao/gen-vlkt. 1.

Introduction
Human-Object Interaction (HOI) detection is a signifi-cant task to make a machine understand human activities in a static image at a fine-grained level. In this task, human ac-tivities are represented as a series of HOI triplets <Human,
*Equal contribution
â€ Corresponding author (liusi@buaa.edu.cn)
Figure 1. Our GEN-VLKT pipeline. We propose GEN, a query-based HOI detector with two-branch decoders, where we design a guided embedding association mechanism to replace the tra-ditional post-matching process for simplifying the association.
Moreover, we devise a training strategy VLKT, where we transfer knowledge from the large-scale visual-linguistic pre-trained model
CLIP to enhance interaction understanding.
Object, Verb>, so an HOI detector is required to localize human and object pairs and recognize their interactions.
The core problems of HOI detection are to explore how to associate the interactive human and object pairs and under-stand their interactions. Thus, we consider improving the
HOI detector from the two aspects and design a unified and superior HOI detection framework. We first revisit the ef-forts conducted by traditional methods in such two aspects.
For the association problem, it can be mainly divided into two paradigms, i.e., bottom-up and top-down. Bottom-up methods [6, 7, 21] detect humans and objects first and then associate humans and objects through a classi-fier or a graph model. Top-down methods usually de-sign an anchor to denote the interaction, e.g., interaction point [23] and queries [4, 31, 46], and then find the cor-responding human and object through pre-defined associa-tive rules. Benefiting from the development of visual trans-former, query-based methods are leading the performance of HOI detection, which are mainly two streams, i.e., two-branch prediction-then-matching manner [4] and single-branch directly-detection manner [31, 46]. The two-branch manner predicts interaction then matches with human and object, struggling with designing effective matching rules and complicated post-processing. The single-branch man-ner proposes to detect the human, object and the corre-sponding interaction based on a single query with multiple heads in an end-to-end manner. However, we argue that the three tasks, i.e., human detection, object detection and inter-action understanding, exist significant differences in feature representation, where human and object detection mainly focus on the features in their corresponding regions, while interaction understanding attends human posture or context.
To improve this, as shown in Figure 1a, we propose to keep the two-branch architecture while removing the com-plicated post-matching. To this end, we propose Guided
Embedding Network (GEN), where we adopt an architec-ture of a visual encoder followed by two-branch decoders, i.e., instance decoder and interaction decoder, and design a guided embedding mechanism to guide the association be-forehand. The two branches are both with a query-based transformer decoder architecture. For the instance decoder, we design two independent query sets for human and object detection. Further, we develop a position Guided Embed-ding (p-GE) to distinguish different human-object pairs by assigning the human query and object query at the same position as a pair. For the interaction decoder, we devise an instance Guided Embedding (i-GE), where we generate each interaction query guided by specific human and object queries to predict its HOIs. Hence, GEN can allow different features for different tasks and guide the association during network forward while without post-matching.
For the interaction understanding problem, most con-ventional methods directly apply a multi-label classifier fit-ted from the dataset to recognize the HOIs. However, such paradigms suffer from the long-tailed distribution and zero-shot discovery due to the complicated human activities with various interactive objects in realistic scenes. Though re-cent methods propose to alleviate such problems with data-augmentation [14] or carefully designed loss [44], the per-formance gain and extension ability are restricted to the limited training scale due to the expensive HOI annotation.
We might as well set our sights on image-text data, which can be easily obtained from the internet, while HOI triplets can be naturally converted into text descriptions. Thanks to the development of visual-linguistic pre-trained mod-els [26, 29, 40], especially, CLIP [29] establishes a strong visual-linguistic model trained on about 400 million image-text pairs and shows its powerful generalization ability on about 30 tasks. Thus, CLIP can cover most HOI scenes in real life and bring a new idea to understand HOIs.
To improve this, as shown in Figure 1b, we design a Visual-Linguistic Knowledge Transfer (VLKT) training strategy to transfer the knowledge from CLIP to the HOI detector to enhance interaction understanding without addi-tional computation cost. We consider two main problems in our VLKT. On the one hand, we design a text-driven classi-fier for prior knowledge integration and zero-shot HOI dis-covery. In detail, we first covert each HOI triplet label into a phrase description, then extract their text embeddings based on the text encoder of CLIP. Finally, we apply the text em-beddings of all HOI labels to initialize the weight of the classifier. In this manner, we can easily extend a novel HOI category only by adding its text embedding into the matrix.
Meanwhile, we also adopt the CLIP-initialized object clas-sifier for novel object extension. On the other hand, for text-driven classifier and visual feature alignment, we present a knowledge distillation method to guide the visual features of HOI detection to mimic the CLIP features. Therefore, based on VLKT, the model can well capture information from CLIP and easily extend to novel HOI categories with-out extra cost during inference.
Finally, we propose a novel unified HOI detection frame-work GEN-VLKT based on the above two designs. We have verified the effectiveness of our GEN-VLKT on two repre-sentative HOI detection benchmarks, i.e., HICO-Det [28] and V-COCO [9]. Our GEN-VLKT has significantly im-proved the existing methods on both two benchmarks and the zero-shot settings of the HICO-Det dataset. Specifi-cally, our GEN-VLKT has achieved a 5.05 mAP gain on
HICO-Det and a 5.28 AP promotion on V-COCO compared with the previous state-of-the-art method QPIC [31]. It also promotes performance impressively by a 108.12% relative mAP gain for unseen object zero-shot setting compared to the previous state-of-the-art method ATL [14]. 2.