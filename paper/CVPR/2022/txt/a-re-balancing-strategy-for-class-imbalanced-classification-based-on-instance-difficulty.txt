Abstract
Real-world data often exhibits class-imbalanced distri-butions, where a few classes (a.k.a. majority classes) oc-cupy most instances and lots of classes (a.k.a. minority classes) have few instances. Neural classification models usually perform poorly on minority classes when training on such imbalanced datasets. To improve the performance on minority classes, existing methods typically re-balance the data distribution at the class level, i.e., assigning higher weights to minority classes and lower weights to majority classes during the training process. However, we observe that even the majority classes contain difficult instances to learn. By reducing the weights of the majority classes, such instances would become more difficult to learn and hurt the overall performance consequently. To tackle this prob-lem, we propose a novel instance-level re-balancing strat-egy, which dynamically adjusts the sampling probabilities of instances according to the instance difficulty. Here the instance difficulty is measured based on the learning speed of instance, which is inspired by the human-leaning pro-cess (i.e., easier instances will be learned faster). We theo-retically prove the correctness and convergence of our re-sampling algorithm. Empirical experiments demonstrate that our method significantly outperforms state-of-the-art re-balancing methods on the class-imbalanced datasets. 1.

Introduction
Over the years, the performance of the classification has witnessed incredible progress on high-quality synthetic datasets, e.g., CIFAR [40], ImageNet [36], MS-COCO [26].
However, the datasets in real-world applications often ex-hibit imbalanced data distributions [27, 30]. This imbalance is intuitively reflected by the sizes of different classes. On one hand, there are some classes that have a large number of
∗ Corresponding author: Jiafeng Guo (a) The Digit Distribution. (b) Accuracy of Evens.
Figure 1. Binary Classification on the Long-Taied MNIST. Fig. 1a shows the digit and class distribution of the training data. Fig. 1b shows the validation accuracy of digits in the even class. ”Others” denotes the average accuracy of digit 0, 2, 4, 6. ”Normal” shows the results of base model. ”Balance” shows the results of the class-balance loss which reduces the weight of the majority class. instances. We call them majority classes in this paper. On the other hand, there are also some classes that have rarely few instances. We call them minority classes in this pa-per. Such class-imbalanced distributions pose critical chal-lenges for neural classification models. Neural models per-form with biases toward the majority classes when training on such datasets [1, 37]. Therefore, models usually perform poorly on the minority classes [7].
The class-imbalanced classification problem has at-tracted a lot of attention in the machine learning community
[13, 27]. Researchers have introduced a variety of strate-gies to re-balance the data distribution when training the model. The mainstream solutions are re-sampling and re-weighting. Re-sampling methods directly adjust the train-ing data by repeating the instances of minority classes and removing some instances of majority classes [6, 16, 41, 44].
The re-weighting methods focus on the cost(e.g., loss) of different classes, specifically paying more attention to the minority classes’ cost and less on the majority classes [5,7].
In summary, existing methods typically consider and solve the imbalance problem at the class level by adjusting the observed class distribution.
However, these class-level re-balancing strategies are too coarse to distinguish the difference of instances. Even within a majority class, there are some difficult instances for classification models to learn. By reducing the weights of the majority classes, such instances would be further ig-nored and become more difficult to learn, which hurts the models’ performance. Fig. 1 gives a support example by a simulation experiment. A neural model needs to learn the binary classification of odd and even numbers on the digi-tal pictures of 0-9(digits 0,2,4,6,8 are labeled as even, digits 1,3,5,7,9 are labeled as odd), whose distribution is shown as Fig. 1a. When testing the model on the validation set, we find that pictures of digit 8 only have a 65% probability to be right inferred as odd numbers. Compared with other even digits, digit 8 is badly learned by the model.
If we adopt the class-balance loss [7] (i.e., an effective class-level re-balancing method) in the training process, the weight of the majority class(i.e., even) will be reduced. Compared with the 1% drop of other even digits, the accuracy of digit 8 has dropped significantly. It indicates that digit 8 should not be treated as same as other digits in the even class.
In the above case, adjusting the sub-class (i.e., digit) dis-tribution seems to be an effective solution. However, in most other cases, we do not know the labels of the sub-classes, and we even can not determine whether sub-classes exist. Moreover, even in a sub-class, the difficulty of differ-ent instances is also different. Therefore, we need to con-sider the weight adjustment at the instance level. At the instance level, we can not assign weights like existing class-level methods, because each instance usually appears only once. Even worse, the model’s performance on the training set also can not reflect the difficulty of instances, because most instances in the training set can be correctly inferred after training.
However, different instances are learned at different speeds.
Inspired by the process of human learning, the speed and difficulty of learning are usually strongly cor-related. By the study of instance learning speed in the model’s training process, we find that the instance learning process is directly affected by the data distribution. Specifi-cally, instances have intermittent unlearning events during the learning process, which are performed as loss incre-ments. Instances from the minority classes or minority sub-classes usually have more unlearning events in training. So these instances are learned more slowly. Therefore, identi-fying instances with slow learning speed as more difficult instances and increasing their weights in learning can effec-tively balance the data distribution.
Based on the above analyses, we design an instance dif-ficulty model according to the learning speed and propose a novel instance-level re-balancing strategy for training clas-sification models. Specifically, we record the predictions of each instance after each training epoch, and measure the in-stance difficulty based on the prediction variations. Then our method re-samples the data according to the instance difficulty model by assigning higher weights to difficult in-stances. In addition, we prove the correctness and conver-gence of our re-sampling strategy theoretically.
In this paper, we conduct some empirical experiments to show the multifaceted capabilities of our method. Specifi-cally, the long-tailed classification experiments indicate that our strategy outperforms some strong baselines on the class-imbalanced datasets. Especially, we achieve new state-of-the-art results on the long-tailed CIFAR-10/-100 for image classification. The simulation experiments further verify the data re-balancing ability of our method, which reduces the imbalance ratio of labeled classes and unlabeled sub-classes. And the generality experiments show the generality of our methods on several different datasets.
The key contributions of this paper can be summarized as follows: (1) We demonstrate the pitfalls of popular class-level methods, and point out the importance of the instance-level distribution adjustment. (2) We theoretically propose a new difficulty definition for instances inspired by the learn-ing speed, and we analyze the relationship of our difficulty and data distribution. (3) We propose an instance-level re-balancing strategy. It empirically performs well with theo-retical proof. 2.