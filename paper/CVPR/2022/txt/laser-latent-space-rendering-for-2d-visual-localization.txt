Abstract
We present LASER, an image-based Monte Carlo Local-ization (MCL) framework for 2D ﬂoor maps. LASER in-troduces the concept of latent space rendering, where 2D pose hypotheses on the ﬂoor map are directly rendered into a geometrically-structured latent space by aggregat-ing viewing ray features. Through a tightly coupled ren-dering codebook scheme, the viewing ray features are dy-namically determined at rendering-time based on their ge-ometries (i.e. length, incident-angle), endowing our repre-sentation with view-dependent ﬁne-grain variability. Our codebook scheme effectively disentangles feature encoding from rendering, allowing the latent space rendering to run at speeds above 10KHz. Moreover, through metric learn-ing, our geometrically-structured latent space is common to both pose hypotheses and query images with arbitrary ﬁeld of views. As a result, LASER achieves state-of-the-art per-formance on large-scale indoor localization datasets (i.e.
ZInD [5] and Structured3D [38]) for both panorama and perspective image queries, while signiﬁcantly outperform-ing existing learning-based methods in speed. 1.

Introduction
Camera localization aims to estimate the spatial rela-tionship between a given input image w.r.t. an environ-mental representation. Diverse application-driven variants have been addressed in the computer vision, robotics, and
AR/VR literature. Particular problem instances are deﬁned in terms of the scope of the pose geometric model (e.g.
SE(2) vs SE(3)), the type of input query imagery (e.g. RGB, depth), as well as the type of the environmental geometric reference (e.g. geometric maps, registered image collec-tions). Instances where the queries and the geometric refer-ence share the same domain deﬁne camera localization as a direct geometric registration problem (e.g. ICP [20], SfM-based geometric veriﬁcation [25]). Conversely, whenever query observations and the geometric reference are from different domains, it requires the design of integrative cross-modality data representations able to distinguish and asso-ciate input observations and reference data.
Figure 1. LASER diagram. Learning-based MCL frameworks encode camera pose hypotheses and query images into a common metric space to measure their similarities. Compared to existing works, LASER directly renders latent features and has a reduced sampling dimension.
This work focuses on solving for the camera pose of a query panorama/perspective image w.r.t. a 2D ﬂoor map, under the Monte Carlo Localization (MCL) framework [7].
MCL adopts a generative framework, where the solution (i.e. camera pose) space is systematically sampled to ren-der observation hypotheses and states the problem in terms of a maximum likelihood search and/or optimization w.r.t. a query observation. Note that in this work, we solely fo-cus on improving the measurement model, hence we are only interested in localizing individual queries without ini-tialization. Yet, it is straight-forward to integrate our work into a full MCL framework with customizable temporal up-dates. Conventional MCL methods [7, 33] require depth sensors and have limited robustness to environmental vari-ability (e.g. furniture/object changes) due to their explicit geometric modeling. Extensions leveraging image-based room layout estimation [2, 31] address content variability at the expense of imposing environmental or capture assump-tions such as Manhattan world, known ceiling or camera height. Recent supervised learning approaches [12,36] have learned a common latent space for both synthesized and
explicit expensive circumvent query observations. However, their explicit high-ﬁdelity rendering and CNN-based encoding corresponding to in-dividual pose hypothesis is computationally burdensome.
Given the time-sensitivity and the accuracy dependency on the number of samples for MCL applications, their compu-tational burden compromises estimation accuracy for online operation. Moreover, the attained latent space representa-tions are not geometrically interpretable while lacking ex-pressiveness and level of detail due to the coarse-level ho-mogenization common to convolutional architectures. We address these challenges within a geometrically-structured metric learning framework, which performs latent-space rendering while prioritizing applicability to unseen environ-ments, computational efﬁciency, estimation accuracy and robustness.
We rendering-and-encoding of sampling observations by directly rendering features in a learnable common metric space from a rasterized 2D ﬂoor map. Such latent space rendering is enabled by a rendering codebook, which allows map points to have view-dependent dynamic features for representing rendering-time dynamics (i.e. viewing ray geometries such as length and incident-angle).
Importantly, we structure said latent space to be geometrically meaningful by en-coding visibility-based omni-directional observations into discretized circular (i.e. angular cyclical) representations.
This representation, namely circular feature, along with the view-dependent feature encoding from the rendering codebook, provides ﬁne-grain structured descriptors for geometry and semantics at a high sampling FPS of 10KHz.
Extensive experiments on Structured3D [38] and ZInD datasets [5] show that our proposed framework signif-icantly outperforms state-of-the-art frameworks both in accuracy and speed. The main technical contributions and innovations driving these performance gains are: (1) Map-aware 2D visual localization framework: While existing MCL frameworks render hypotheses over a local scope (i.e. contents visible to the camera), LASER uses a 2D variant of the PointNet [21] to attain latent encoding from 2D point cloud maps. The PointNet learns global con-text from the map and provides the latent feature with map-level scope which improves LASER’s recall. (2) Latent space rendering based on codebook scheme:
LASER obviates the redundant rendering-and-encoding of an intermediate representation for individual samples by di-rectly render features in the latent space. Powered by our rendering codebook scheme, the features are dynamically determined in rendering-time to encode ﬁne-grain ray ge-ometries. Such design achieves signiﬁcantly higher sam-pling speed and accuracy at the same time. (3) Geometrically-structured metric learning: LASER structures the metric learning to be geometrically meaning-ful using a rotationally-covariant 2D omni-directional cir-cular feature. Its ﬁne-grain structured variability implicitly expresses environmental layouts for high-accuracy localiza-tion, and seamlessly supports query images with arbitrary
ﬁeld of views. In addition, this choice implicitly encodes many orientations that reduces the rotation dimension from the MCL sampling space. 2.