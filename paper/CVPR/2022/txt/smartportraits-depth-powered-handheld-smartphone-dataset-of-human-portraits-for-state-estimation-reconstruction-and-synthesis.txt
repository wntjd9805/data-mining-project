Abstract
We present a dataset of 1000 video sequences of human portraits recorded in real and uncontrolled conditions by using a handheld smartphone accompanied by an external high-quality depth camera. The collected dataset contains 200 people captured in different poses and locations and its main purpose is to bridge the gap between raw measure-ments obtained from a smartphone and downstream appli-cations, such as state estimation, 3D reconstruction, view synthesis, etc. The sensors employed in data collection are the smartphoneâ€™s camera and Inertial Measurement Unit (IMU), and an external Azure Kinect DK depth camera software synchronized with sub-millisecond precision to the smartphone system. During the recording, the smartphone flash is used to provide a periodic secondary source of light-ning. Accurate mask of the foremost person is provided as well as its impact on the camera alignment accuracy.
For evaluation purposes, we compare multiple state-of-the-art camera alignment methods by using a Motion Cap-ture system. We provide a smartphone visual-inertial bench-mark for portrait capturing, where we report results for multiple methods and motivate further use of the provided trajectories, available in the dataset, in view synthesis and 3D reconstruction tasks. 1.

Introduction
Realistic rendering of people, and in general of objects, has recently achieved an unprecedented level of detail and realism [4, 27, 46, 49, 76, 87, 93, 94] with potential ground-breaking applications in telepresence, VR and AR. Most of these methods have focused on static scenes and synthetic data, leaving apart the computational time required, which is still prohibitive. In contrast, many potential usages of re-construction and rendering are ideal candidate applications for smartphones, or other consumer-level devices, whose sensors are improving every year but still of limited quality.
Our objective is to create a dataset that recreates in the wild conditions emulating smartphone users.
The SmartPortrait dataset1 is an effort to bridge the gap between realistic raw data obtained from people, collected 1https://MobileRoboticsSkoltech.github.io/SmartPortraits/
from a handheld smartphone and the down-stream recon-struction applications, for instance 3D portrait reconstruc-tion, view synthesis, etc. The key component that links both views is camera pose state estimation. A usual prac-tice is to obtain these poses by using a reliable but com-putationally demanding Structure from Motion (SfM) algo-rithm such as COLMAP [71] or multi-modal SLAM meth-ods [13, 41, 68, 80]. The trajectories provided in our dataset emulate handheld movements, as if the user, or close-by users, were recording the sequences (see Fig. 1).
Many view synthesis methods [28, 49, 57, 74] generate their own datasets just by using state estimation methods.
These single camera free-viewpoint images can only be considered if the scene is static. We asked volunteers to stay as still as they could while recording them in a semicir-cular trajectory from close and mid distances. We observed that most of the volunteers slightly changed their postures so we should expect some degree of displacement, which transforms the problem into non-static.
The SmartPortrait dataset is obtained in a variety of emplacements, under different lightning conditions, plus a flashing light from the smartphone at regular intervals. The smartphone camera is complemented with a high-quality depth sensor, adding robustness and multi-modality.
We provide a recorded dataset consisting of smartphone video images, IMU data, perfectly time aligned, and an ex-ternal depth camera from Azure Kinect DK. The evaluation includes two steps: first we compare the most promising methods with a reference trajectory obtained from a motion capture (MoCap) system. Second, for some environments, it is not possible to deploy the MoCap system. Therefore, we provide a reference trajectory, obtained from the previ-ous best performing method, and provide an upper bound of the error by using a non-reference metric [40]. In further evaluations, we benchmark multiple state of the art methods for visual SLAM, SfM and Visual-Inertial based methods.
Next, we want to connect the problem of camera pose estimation with two downstream tasks: 3D reconstruction with COLMAP [72], ACMP [89] and SOTA view synthesis algorithms (NeRF [49], FVS [66], SVS [67]). These appli-cations will help us to understand the importance of pose estimation and its correlation with other tasks.
Ethical considerations. We asked all participants in the dataset for a signed consent to record their portraits and publicly release them for purely academic purposes. We ex-plicitly indicated in the agreement their right, at any time, of removing all their data. 2.