Abstract
Videos show continuous events, yet most — if not all — video synthesis frameworks treat them discretely in time.
In this work, we think of videos of what they should be — time-continuous signals, and extend the paradigm of neural representations to build a continuous-time video generator.
For this, we ﬁrst design continuous motion representations through the lens of positional embeddings. Then, we explore the question of training on very sparse videos and demon-strate that a good generator can be learned by using as few as 2 frames per clip. After that, we rethink the traditional image + video discriminators pair and design a holistic dis-criminator that aggregates temporal information by simply concatenating frames’ features. This decreases the training cost and provides richer learning signal to the generator, making it possible to train directly on 10242 videos for the
ﬁrst time. We build our model on top of StyleGAN2 and it is just 5% more expensive to train at the same resolution while achieving almost the same image quality. Moreover, our latent space features similar properties, enabling spa-tial manipulations that our method can propagate in time.
We can generate arbitrarily long videos at arbitrary high frame rate, while prior work struggles to generate even 64 frames at a ﬁxed rate. Our model is tested on four mod-ern 2562 and one 10242-resolution video synthesis bench-marks. In terms of sheer metrics, it performs on average 30% better than the closest runner-up. Project website:
⇡
⇡ https://universome.github.io/stylegan-v. 1.

Introduction
Recent advances in deep learning pushed image genera-tion to the unprecedented photo-realistic quality [8, 28] and spawned a lot of its industry applications. Video generation, however, does not enjoy a similar success and struggles to
ﬁt complex real-world datasets. The difﬁculties are caused not only by the more complex nature of the underlying data distribution, but also due to the computationally inten--D
H
N
A
G o
C o
M
N
A
G
D
I s r u
O
-D
H
N
A
G o
C o
M
N
A
G
D
I s r u
O 0 1/4 sec 1/2 sec 1 sec 5 sec 1 min 1 hour 0 1/4 sec 1/2 sec 1 sec 5 sec 1 min 1 hour
Figure 1. Examples of 1-hour long videos, generated with different methods. MoCoGAN-HD [65] fails to generate long videos due to the instability of the underlying LSTM model when unrolled to large lengths. DIGAN [80] struggles to generate long videos due to the entanglement of spatial and temporal positional embed-dings. StyleGAN-V (our method) generates plausible videos of arbitrary length and frame-rate. Also, unlike DIGAN, it learns temporal patterns not only in terms of motion, but also appearance transformations, like time of day and weather changes. sive video representations employed by modern generators.
They treat videos as discrete sequences of images, which is very demanding for representing long high-resolution videos and induces the use of expensive conv3d-based ar-chitectures to model them [13, 54, 55, 67]. 1
In this work, we argue that this design choice is not optimal and propose to treat videos in their natural form: as continuous signals x(t), that map any time coordinate 1E.g., DVD-GAN [13] requires
⇡
$30k to train on 2562 resolution [65]
G sample
CLIP edit animate
"Blue sky with clouds" project
CLIP edit animate
"Person with mustaches"
Figure 2. Our model enjoys all the perks of StyleGAN2 [30], including the ability of semantic manipulation. In this example, we edited a generated frame (top row) or projected off-the-shelf image (bottom row) with CLIP and animated it with our model. To the best of our knowledge, our work is the ﬁrst one which demonstrates such capabilities for video generators. exposition (§3.3) and practical experiments (see Table 2).
Finally, since our model sees only 2-4 randomly sampled frames per video, it is highly redundant to use expensive conv3d-blocks in the discriminator, which are designed to operate on long sequences of equidistant frames. That’s why we replace it with a conv2d-based model, which ag-gregates information temporarily via simple concatenation and is conditioned on the time distances between its input frames. Such redesign improves training efﬁciency (see Ta-ble 1), provides more informative gradient signal to the gen-erator (see Fig 4) and simpliﬁes the overall pipeline (see
§3.2), since we no longer need two different discriminators to operate on image and video levels separately, as modern video synthesis models do (e.g., [13, 55, 67]).
We build our model, named StyleGAN-V, on top of the image-based StyleGAN2 [30]. It is able to produce arbi-trarily long videos at arbitrarily high frame-rate in a non-autoregressive manner and enjoys great training efﬁciency 5% costlier than the classical image-based
— it is only 10% worse
StyleGAN2 model [30], while having only plain image quality in terms of FID [23] (see Fig 3). This allows us to easily scale it to HQ datasets and we demon-strate that it is directly trainable on 10242 resolution.
⇡
⇡
For empirical evaluation, we use 5 benchmarks: Face-Forensics 2562 [53], SkyTimelapse 2562 [78], UCF101 2562 [62], RainbowJelly 2562 (introduced in our work) and
MEAD 10242 [72]. Apart from our model, we train from scratch 5 different methods and measure their performance using the same evaluation protocol. Frechet Video Distance (FVD) [68] serves as the main metric for video synthesis, but there is no complete ofﬁcial implementation for it (see
§4 and Appx C). This leads to discrepancies in the eval-uation procedures used by different works because FVD, similarly to FID [23], is very sensitive to data format and sampling strategy [46]. That’s why we implement, docu-ment and release our complete FVD evaluation protocol.
In terms of sheer metrics, our method performs on average 30% better than the closest runner-up.
⇡ 2.