Abstract
We tackle the problem of learning object detectors with-out supervision. Differently from weakly-supervised object detection, we do not assume image-level class labels. In-stead, we extract a supervisory signal from audio-visual data, using the audio component to “teach” the object de-tector. While this problem is related to sound source local-isation, it is considerably harder because the detector must classify the objects by type, enumerate each instance of the object, and do so even when the object is silent. We tackle this problem by first designing a self-supervised framework with a contrastive objective that jointly learns to classify and localise objects. Then, without using any supervision, we simply use these self-supervised labels and boxes to train an image-based object detector. With this, we outperform previous unsupervised and weakly-supervised detectors for the task of object detection and sound source localization.
We also show that we can align this detector to ground-truth classes with as little as one label per pseudo-class, and show how our method can learn to detect generic objects that go beyond instruments, such as airplanes and cats. 1.

Introduction
While recent progress in learning image and video rep-resentations has been substantial [22, 33, 38, 93], this has not yet translated into an ability to learn interpretable and actionable concepts automatically. By that, we mean that some manual labels are still required in order to map unsu-pervised representations to useful concepts such as image classes or object detections.
In this paper, we thus con-sider the problem of learning interpretable concepts without any manual supervision. In particular, we focus on a prob-lem that has not been explored extensively in the literature: learning to simultaneously detect and classify objects with no manual labels at all.
*Joint first authors.
†Work done during an internship at FAIR.
Figure 1. We train an object detector simply by watching videos. Without using any manual annotations, we learn to detect different objects in images, by first self-labelling boxes and object categories and then using those as targets to teach a detector. The detection results shown are outputs from our trained model; for visualisation purposes we show Hungarian-matched labels.
This problem is related to weakly supervised object detection (WSOD [16, 61]), with the difference that, in
WSOD, the learning algorithm is given image-level labels telling it whether the image contains an occurrence of a given object type or not. Inspired by recent work in self-supervised learning, we seek to replace this source of ex-ternal supervision with an internal supervisory signal ex-tracted from the observation of video data. Videos are far richer than images, for example because they contain mo-tion. Here, we focus on the multi-modal aspect of videos and use sound as a weak and noisy cue to learn about ob-jects in the visual component of the data.
The power of multi-modal self-supervision has been demonstrated before in self-supervised representation learning, and, closely related, in video clustering [9]. How-ever, while video clustering can provide an interpretation of the data in terms of discrete classes, it does not pro-vide any information about the location of the relevant ob-jects in images. On the other hand, sound source locali-sation [7, 11, 49, 65] has considered precisely the problem of localizing the source of sounds in images.
It is there-fore tempting to trivially combine image classification and sound source localisation in the hope of learning the type and location of objects automatically.
Unfortunately, such an approach does not lead to a satis-factory object detector. To understand why, it is important to note that the goal of sound source localisation is to lo-calize the sound while it is being heard. This is insufficient for a detector because many objects emit sounds only oc-casionally and they become invisible to source localisation when they are silent. Instead, a detector that works in the visual domain should be responsive even when the object cannot be heard. Furthermore, source localisation meth-ods generally only extract a heatmap giving the distribution of possible object locations; in contrast, a detector solves the much harder problem of enumerating all individual ob-ject instance that occur in an image by outputting instance-specific bounding boxes.
In order to solve these issues, we should treat the sound component as a useful cue to learn an object detector, but not as a cue which is necessary for detection. Instead, we consider the problem of taking as input a collection of raw videos and producing a list of object classes and locations, in order to train an image-based detector.
On a high level, our method is based on the following observation: we can use a sound source localisation net-work to learn about possible locations of sounding objects in videos. From this, we can extract a collection of bound-ing box pseudo-annotations for the objects and use those to learn a standard object detector. Because the latter only uses the visual modality, it immediately transfers to the detection of objects even when no relevant sound is present.
However, one challenge is that sound source localisa-tion does not provide the necessary class information to train class-specific detectors, effectively resulting in only learning a region proposal network for generic objects, with high rates of false positives. To this end, we note that most sound source localizers are based on noise-contrastive for-mulations that, together with clustering-based approaches, are currently state-of-the-art in self-supervised representa-tion learning. From this, we derive a joint formulation that can simultaneously benefit from and learn to localize sound sources and classify them without any supervision. The re-sulting output can then be used to train any off-the-shelf ob-ject detector such as a Faster-RCNN [72] to learn an object detector without any supervision, as shown in Fig. 1. 2.