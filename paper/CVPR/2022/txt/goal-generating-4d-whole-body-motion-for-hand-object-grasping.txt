Abstract
Generating digital humans that move realistically has many applications and is widely studied, but existing meth-ods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied, but the fo-cus has been on generating realistic static grasps of objects.
To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state space of poses is sig-niﬁcantly larger, the scales of hand and body motions dif-fer, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Addi-tionally, the head is involved because the avatar must look at the object to interact with it. For the ﬁrst time, we ad-dress the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As in-put, our method, called GOAL, takes a 3D object, its pose, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks.
First, GNet generates a goal whole-body grasp with a re-alistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion be-tween the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object con-tact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex off-sets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that
GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL’s gener-ated motions approach the realism of GRAB’s ground truth.
GOAL takes a step towards generating realistic full-body object grasping motion. Our models and code are available at https://goal.is.tue.mpg.de.
1.

Introduction
Virtual humans are important for movies, games, AR/VR and the metaverse. Not only do they need to look realistic, but also must move and interact realistically. Most work on human motion generation has focused only on bodies, with-out the head and hands. Often, these bodies are considered in “isolation”, with no scene or object context. Other work focuses on bodies interacting with scenes, but ignores the hands. Similarly, work on generating hand grasps often ig-nores the body. We argue that these are all just parts of the problem. What we really need, instead, is to generate the motion of whole-body avatars grasping objects, by jointly considering the body, head, feet, hands, as well as objects.
We address this here for the ﬁrst time.
The problem is challenging and multifaceted. Think of how we grasp objects in real life (see Fig. 2); we walk to-wards the object with our feet contacting the ground, we ori-ent our head to look at the object, lean our torso and extend our arms to reach it, and dexterously pose our hands to es-tablish ﬁne contact and grasp it. Humans are able to grace-fully execute these steps, yet, these are challenging and in-volve motion planning, motor control, and spatial aware-ness. Some of these steps have been studied separately, but we cannot simply combine the partial solutions since the entire action must be coordinated. This is challenging be-cause: (1) full bodies have a much higher-dimensional state space than bodies or hands alone; (2) the body and hands have very different sizes, motion scales and level of dex-terity; (3) the body, head and hands must move in a coor-dinated fashion. Currently, there are no automatic tools to generate such coordinated full-body grasping motions.
We address this with GOAL, which stands for Generat-ing Object-interActing whoLe-body motions. GOAL gen-erates whole-body avatar motion for grasping an unknown object, by jointly considering the body, head, feet, hands and the object. GOAL takes three inputs: (1) a 3D object, (2) its position and orientation, and (3) a “starting” 3D body pose and shape, positioned near the object and roughly ori-ented towards it. As output, GOAL generates a sequence of 3D body poses from the starting pose through to an ob-ject grasp. To do so, GOAL uses two novel networks (for an overview see Fig. 3): (1) First, GNet generates a “goal” whole-body grasp, with a realistic body pose, head pose, arm pose, and hand pose, as well as realistic ﬁnger-object and foot-ground contact. GNet is formulated as a condi-tional variational auto-encoder (cVAE), thus, it learns a dis-tribution over grasping poses, and can generate a variety of
“goal” grasps. (2) Then, MNet inpaints the motion between the “starting” and “goal” poses, by generating a sequence of whole-body poses in an auto-regressive fashion. This is challenging because the avatar needs to (see Fig. 1) walk by taking a number of steps proportional to the distance to the object, while having natural foot-ground contact with-Figure 2. Grasping an object involves several motions. We walk towards the object with our feet contacting the ﬂoor, we orient our head to look at the object, we lean our torso, extend our arms, and pose our hand to contact and grasp the object. The depicted examples use motions captured in the GRAB dataset [57]. out “skating”, and continuously orient the head to look at the object. Then, when it is near the object, it needs to slow down, stop walking, lean the torso, extend the arms to reach the object. It must also pose the hand to contact the object and grasp it. All body parts need to move gracefully and in full coordination, so that the motion looks natural.
Achieving this level of realism requires technical nov-elties. GOAL goes beyond recent work [38, 65, 67] to jointly infer both SMPL-X [46] parameters and 3D off-sets. GNet infers 3D hand-to-object vertex offsets to give spatial awareness and guide object grasping. MNet infers 3D SMPL-X vertex offsets to guide SMPL-X deformation from the previous to the current frame. These offsets lie in 3D Euclidean space, thus, they can be more accurately in-ferred than SMPL-X parameters, and are used in an ofﬂine optimization scheme to reﬁne SMPL-X poses. We train
GNet and MNet on the GRAB [57] dataset, which contains whole-body SMPL-X humans grasping objects.
We evaluate GOAL, both quantitatively and qualita-tively, on withheld parts of the GRAB dataset. Speciﬁcally, we withhold 5 objects for testing. Results show that GOAL generalizes well and produces natural motions for full-body walking and object grasping; see Fig. 1. Quantitative evalu-ation shows that GOAL outperforms baselines, and ablation studies show a positive contribution of all major compo-nents. A perceptual study veriﬁes the above, while showing that GOAL’s generated motions achieve a level of realism comparable to GRAB’s ground-truth motions.
GOAL takes a step towards generating whole-body grasp motion for realistic avatars. Models and code are available at https://goal.is.tue.mpg.de.
2.