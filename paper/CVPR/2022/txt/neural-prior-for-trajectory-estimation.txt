Abstract
Neural priors are a promising direction to capture low-level vision statistics without relying on handcrafted regu-larizers. Recent works have successfully shown the use of neural architecture biases to implicitly regularize image de-noising, super-resolution, inpainting, synthesis, scene ï¬‚ow, among others. They do not rely on large-scale datasets to capture prior statistics and thus generalize well to out-of-Inspired by such advances, we in-the-distribution data. vestigate neural priors for trajectory representation. Tra-ditionally, trajectories have been represented by a set of handcrafted bases that have limited expressibility. Here, we propose a neural trajectory prior to capture continu-ous spatio-temporal information without the need for ofï¬‚ine data. We demonstrate how our proposed objective is opti-mized during runtime to estimate trajectories for two impor-tant tasks: Non-Rigid Structure from Motion (NRSfM) and lidar scene ï¬‚ow integration for self-driving scenes. Our re-sults are competitive to many state-of-the-art methods for both tasks. 1.

Introduction
Representing space-time with 3D trajectories provides longer-term information about the dynamics of a 3D scene compared to pairwise representations such as scene ï¬‚ow. It also enables generic priors to solve underconstrained low-level vision tasks, especially for problems that need to be agnostic to different objects and scenes. Here we aim to model a general and dataless prior for estimating 3D trajec-tories through a neural runtime optimization approach.
Most works studying trajectory priors are from the Non-Rigid Structure from Motion (NRSfM) ï¬eld, where they are handcrafted to solve the ill-posed inverse problem of lift-ing dynamic 2D points to 3D. The most straightforward trajectory prior assumes that points move smoothly over time [56, 57]. However, it does not provide enough con-straints to disambiguate the camera and point motion. On the other hand, a collection of trajectories observed from a scene contain statistics for stronger priors. The seminal work of Akhter et al. [3] proposed to represent trajectories
Neural trajectory prior (i) spatial smoothness (iii) compressibility
Applications (a) NRSfM
ğ© (ğ‘¢, ğ‘£) or (ğ‘¥, ğ‘¦, ğ‘§, ğ‘¡)
ğ‘“!
MLP
ğ‘“"
ğ›— weight decoder (ii) temporal smoothness
â€¦
ğœ (ğ‘¢, ğ‘£) (b) scene flow integration 
ğ­â€²
ğ‘“#
ğœ! ğœ"
â€¦
ğœ#
âˆ—
ğœ¶ continuous traj. basis functions 
âˆ— weights
ğœ (ğ‘¥, ğ‘¦, ğ‘§, ğ‘¡)
Figure 1. Our neural trajectory prior (NTP) regularizes the mo-tion of a point to be smooth in space by the continuous mapping of a coordinate MLP fÏ•. Trajectories are represented as a linear combination of overcomplete continuous basis functions Ï„ 1, Ï„ 2,
..., Ï„ K . The linear weights Î± is regularized to be compressible to lower dimensional Ï•, and the basis functions are generated by a temporal coordinate MLP fÏ„ , which implicitly enforces temporal smoothness. We show the results of applying NTP on NRSfM and lidar scene ï¬‚ow integration. by linearly combining a small set of Discrete Cosine Trans-form (DCT) bases. However, it is well known that such low-rank linear models are not sufï¬cient to represent com-plex motions and dense data where the number of trajecto-ries is far greater than the length of the sequence. Recent state-of-the-art NRSfM methods [33, 34, 53] combine both shape and trajectory priors or are tailored towards speciï¬c scenes such as deforming surfaces [31, 42, 44]. In this pa-per, we revisit the idea of a general trajectory prior. Our approach differs from current methods in that we use the architectural regularization properties of neural networks.
We are inspired by the recent innovations of using co-ordinate MLPs [36, 38, 40, 45] and propose a new general neural trajectory prior to model continuous spatio-temporal motions of dynamic scenes (see Fig. 1). We utilize the smoothness bias of coordinate MLPs to enforce trajectories to be temporally smooth and encourage nearby positions to share similar motions. We also introduce a bottleneck layer to the model, thus effectively constraining the output tra-jectories to be compressible to low dimensions. MLPs with bottleneck has previously been used to regularize shapes for shape-based NRSfM approaches [10, 53, 59, 60], showing 1
greater expressibility and accuracy compared to low-rank models on large datasets. To our knowledge, we are the ï¬rst to propose such a strategy for modeling trajectories.
Our neural trajectory prior can be easily integrated to problems outside the NRSfM domain, such as estimating long-term scene ï¬‚ow from lidar point clouds. Despite the remarkable progress on scene ï¬‚ow estimation given a pair of point clouds [21, 30, 37, 47, 62], achieving similar per-formance over longer sequences has still been challeng-ing. We are interested in integrating sparse 3D lidar points across tens of frames back to a single reference frame, where the scene is dynamic with multiple objects. In ad-dition, working with lidar point clouds has its unique chal-lenges due to the sparsity and unstructured nature of the signals. This makes image-based multi-frame ï¬‚ow meth-ods [16,26,27,50] non-applicable. We demonstrate that our proposed neural trajectory prior is sufï¬cient to regularize scene ï¬‚ow integration both spatially and temporally while outperforming naive Euler-based integration methods pro-posed by state-of-the-arts scene ï¬‚ow estimators [30, 36].
Contributions. We propose a general neural trajectory prior. It uses a bottleneck architecture to regularize trajec-tories and a coordinate MLPs to regularize spatio-temporal dependencies when dealing with dense problems. We val-idate the effectiveness of the bottleneck trajectory prior by achieving competitive results with NRSfM methods that use both shape and trajectory priors. Next, we show that it can be paired with existing neural shape priors to achieve state-of-the-art results across well-known sparse NRSfM bench-marks. Also, we show that our method outperforms a re-cently proposed neural-based method [53] on dense NRSfM benchmarks while being signiï¬cantly faster. For lidar scene
ï¬‚ow integration, we show that our approach produces better trajectories by modeling spatio-temporal information than if naively integrating pairwise ï¬‚ows from scene ï¬‚ow esti-mators. We demonstrate the applicability of our scene ï¬‚ow integration through point cloud densiï¬cation. 2.