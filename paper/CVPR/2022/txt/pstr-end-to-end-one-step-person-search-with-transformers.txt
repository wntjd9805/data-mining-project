Abstract
We propose a novel one-step transformer-based person search framework, PSTR, that jointly performs person de-tection and re-identification (re-id) in a single architec-ture. PSTR comprises a person search-specialized (PSS) module that contains a detection encoder-decoder for per-son detection along with a discriminative re-id decoder for person re-id. The discriminative re-id decoder utilizes a multi-level supervision scheme with a shared decoder for discriminative re-id feature learning and also comprises a part attention block to encode relationship between dif-ferent parts of a person. We further introduce a sim-ple multi-scale scheme to support re-id across person in-stances at different scales. PSTR jointly achieves the di-verse objectives of object-level recognition (detection) and instance-level matching (re-id). To the best of our knowl-edge, we are the first to propose an end-to-end one-step transformer-based person search framework. Experiments are performed on two popular benchmarks: CUHK-SYSU and PRW. Our extensive ablations reveal the merits of the proposed contributions. Further, the proposed PSTR sets a new state-of-the-art on both benchmarks. On the challeng-ing PRW benchmark, PSTR achieves a mean average preci-sion (mAP) score of 56.5%. The source code is available at https://github.com/JialeCao001/PSTR. 1.

Introduction
Person search aims to detect and identify a target person from a gallery of real-world uncropped images, which can be seen as a joint task of person detection [1, 19, 21, 32] and re-identification (re-id) [6, 17, 34]. Person search involves addressing the challenges of these two diverse sub-tasks as well as jointly optimizing them in a unified framework.
Person search approaches can be roughly divided into
*Corresponding author: Yanwei Pang
Figure 1. Comparison of our PSTR architecture (c) with the exist-ing two-step (a) and one-step paradigms (b). (a) Within the two-step paradigm, person detection and re-id sub-tasks are performed with two separate independent networks. Here, bounding-boxes are first predicted by a detection network and then cropped and resized (C&R) before being fed to a re-id network. (b) Within the one-step paradigm, detection and re-id branches share the same backbone network. (c) Distinct from these two paradigms, our
PSTR is an end-to-end one-step transformer-based architecture with a person-search specialized module to jointly perform detec-tion and re-id without requiring an NMS post-processing step. two-step [4, 10, 35] and one-step methods [5, 29, 31]. Two-step approaches typically disentangle the two sub-tasks, where person detection and re-id are performed separately (Fig. 1(a)). First, an off-the-shelf detection network (e.g.,
Faster R-CNN [22]) is employed to detect pedestrians. Sec-ond, the detected pedestrians are cropped and resized into a fixed resolution, followed by utilizing a re-id network to identify cropped pedestrians. While achieving promising performance, most two-step approaches are computation-ally expensive. In contrast, one-step approaches simultane-ously detect and identify the persons using a single network (Fig. 1(b)). First, the features are extracted by a shared net-work. Then, person detection and re-id are performed by two branches within the same network.
Despite recent progress in person search, both two-step and one-step approaches employ hand-designed mecha-nisms, such as non-maximum suppression (NMS) proce-dure to filter out duplicate predictions for each person. Re-cently, transformers [9, 24] have shown promising results in several vision tasks, including object detection [2, 37]. The encoder-decoder design of transformer-based object detec-tors alleviates the need to employ different hand-designed components, leading to a simpler end-to-end trainable ar-chitecture. Further, the transformer architecture can be eas-ily extended to a multi-task learning framework [13, 27].
Despite their recent success, transformers are yet to be in-vestigated for person search. In this work, we investigate the problem of designing a simple but accurate end-to-end one-step transformer-based person search framework.
When designing a one-step transformer-based person search framework, a straight-forward way is to adopt an ob-ject detector, such as DETR [2] to detect persons, while the re-ID sub-task can be performed in different ways. (i) The transformer decoder within object detector can be modified by introducing an auxiliary task of re-id. (ii) Two separate standard encoder-decoder networks can be utilized to per-form detection and re-id sub-tasks. However, we observe these strategies struggle to achieve satisfactory results. 1.1. Motivation
We consider two desirable properties when designing a transformer-based person search framework.
Improved re-id feature discriminability: The sub-tasks of detection and re-id within person search have differ-ent objectives. Person detection strives to perform object-level recognition and localization by differentiating the per-son category from background. Here, all person instances within and across images are grouped into a single person category. On the other hand, person re-id sub-task aims to identify a person at instance-level. Here, a person instance is desired to be matched with a database of images, thereby requiring to discriminate among instances of different per-sons within the same person category. Therefore, trans-former re-id decoders need to be distinct from their detector counterparts and are desired to generate discriminative fea-tures specialized to perform instance-level matching.
Encoding multi-scale information for re-id: Scale varia-tion is a challenging problem in person search. The same person captured by different cameras may have a large variation in scale, which increases the difficulty for per-son matching. Most existing approaches either follow the strategy where pedestrians are first detected and then re-sized into a fixed resolution or adopt a feature RoI pool-ing scheme [22] to obtain scale-invariant representation. In-stead of image resizing or feature pooling, we look into an 50 40 30
P
A m 90 85 80 75 70 y c a r u c c
A 1
-p o
T
Method
NAE [5]
NAE+ [5]
AlignPS [30]
DMRN [11]
SeqNet [16]
PSTR (ours) mAP Time 43.3 44.0 45.9 46.9 46.7 49.5 83 98 61 66 86 56
Method
NAE [5]
NAE+ [5]
AlignPS [30]
DMRN [11]
SeqNet [16]
PSTR (ours)
Top-1 Time 80.9 81.1 81.9 83.3 83.4 87.8 83 98 61 66 86 56 50 60 70 80 90 100
Inference time (ms) 65 50 60 70 80 90 100
Inference time (ms)
Figure 2. Accuracy (AP) vs. speed (ms) comparison with existing one-step methods on PRW test set. All methods use a ResNet50 backbone and the speed is reported on a V100 GPU. Our end-to-end one-step transformer-based PSTR outperforms existing meth-ods in terms of both speed and accuracy. approach to encode multi-scale information within a trans-former architecture for re-id in person search. 1.2. Contributions
We propose a novel end-to-end one-step transformer-based person search framework, named PSTR. Our PSTR treats person search as a sequence prediction problem, where all persons in an image are detected along with their respective re-id features (Fig. 1(c)). To this end, we in-troduce a person search-specialized (PSS) module within
PSTR that performs both detection and re-id. The PSS mod-ule aims to improve feature discriminability of re-id features by introducing a discriminative re-id decoder that utilizes a multi-level supervision scheme with a shared decoder de-sign. Further, we introduce a part attention block within the discriminative re-id decoder to capture the relationship of different parts. Moreover, we propose a simple multi-scale scheme of our discriminative re-id decoder to address the issue of person matching at different scales. To the best of our knowledge, PSTR is the first end-to-end one-step per-son search framework based on transformers.
We validate PSTR on CUHK-SYSU [29] and PRW [35].
Our comprehensive ablations reveal the merits of the con-tributions. Further, PSTR sets a new state-of-the-art on both benchmarks. When using ResNet50 [12], PSTR achieves a mAP score of 49.5% on PRW benchmark, while running at a speed of 56 milliseconds (ms) on a single V100 GPU (see
Fig. 2). With a transformer-based backbone [26], PSTR ob-tains the best reported results with a mAP score of 56.5%. 2.