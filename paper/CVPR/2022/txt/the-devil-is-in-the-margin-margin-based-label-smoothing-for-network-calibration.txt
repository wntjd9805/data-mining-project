Abstract
In spite of the dominant performances of deep neural networks, recent works have shown that they are poorly calibrated, resulting in over-confident predictions. Miscal-ibration can be exacerbated by overfitting due to the mini-mization of the cross-entropy during training, as it promotes the predicted softmax probabilities to match the one-hot la-bel assignments. This yields a pre-softmax activation of the correct class that is significantly larger than the remaining activations. Recent evidence from the literature suggests that loss functions that embed implicit or explicit maximiza-tion of the entropy of predictions yield state-of-the-art cal-ibration performances. We provide a unifying constrained-optimization perspective of current state-of-the-art calibra-tion losses. Specifically, these losses could be viewed as approximations of a linear penalty (or a Lagrangian term) imposing equality constraints on logit distances. This points to an important limitation of such underlying equality con-straints, whose ensuing gradients constantly push towards a non-informative solution, which might prevent from reach-ing the best compromise between the discriminative perfor-mance and calibration of the model during gradient-based optimization. Following our observations, we propose a simple and flexible generalization based on inequality con-straints, which imposes a controllable margin on logit dis-tances. Comprehensive experiments on a variety of image classification, semantic segmentation and NLP benchmarks demonstrate that our method sets novel state-of-the-art re-sults on these tasks in terms of network calibration, without affecting the discriminative performance. The code is avail-able at https://github.com/by-liu/MbLS . 1.

Introduction
With the advent of deep neural networks (DNNs), we have witnessed a dramatic performance improvement in a variety of computer vision and NLP tasks across different
*Corresponding author: bingyuan.liu@etsmtl.ca applications, such as image classification [12] or seman-tic segmentation [3]. Nevertheless, recent studies [8, 21] have shown that these high-capacity models are poorly cal-ibrated, often resulting in over-confident predictions. As a result, the predicted probability values associated with each class overestimate the actual likelihood of correctness.
Quantifying the predictive uncertainty for modern DNNs has received an increased attention recently, with a variety of alternatives to better calibrate network outputs. A simple strategy consists in including a post-processing step during the test phase to transform the output of a trained network
[5, 8, 28, 32], with the parameters of this additional opera-tion determined on a validation set. Despite their simplic-ity and low computational cost, these methods were shown to be effective when training and testing data are drawn from the same distribution. However, one of their observed limitations is that the choice of the transformation param-eters, such as temperature scaling, is highly dependent on the dataset and network. A more principled alternative is to explicitly maximize the Shannon entropy of the predictions during training by integrating a term into the learning ob-jective, which penalizes confident output distributions [25].
Furthermore, recent efforts to quantify the quality of predic-tive uncertainties have focused on investigating the effect of the entropy on the training labels [21,22,31]. Findings from these works evidence that, popular losses, which modify the hard-label assignments, such as label smoothing [27] and focal loss [17], implicitly integrate an entropy maximization objective and have a favourable effect on model calibration.
As shown comprehensively in the recent study in [21], these losses, with implicit or explicit maximization of the entropy, represent the state-of-the-art in model calibration.
Contributions are summarized as follows:
• We provide a unifying constrained-optimization per-spective of current state-of-the-art calibration losses.
Specifically, these losses could be viewed as approxi-mations of a linear penalty (or a Lagrangian term) im-posing equality constraints on logit distances. This points to an important limitation of such underly-ing hard equality constraints, whose ensuing gradients constantly push towards a non-informative solution, which might prevent from reaching the best compro-mise between the discriminative performance and cal-ibration of the model during gradient-based optimiza-tion.
• Following our observations, we propose a simple and flexible generalization based on inequality constraints, which imposes a controllable margin on logit dis-tances.
• We provide comprehensive experiments and ablation studies over two standard image classification bench-marks (CIFAR-10 and Tiny-ImageNet), one fine-grained image classification dataset (CUB-200-2011), one semantic segmentation dataset (PASCAL VOC 2012) and one NLP dataset (20 Newsgroups), with various network architectures. Our empirical results demonstrate the superiority of our method compared to state-of-the-art calibration losses. Our findings sug-gest that, for complex datasets, such as fine-grained image classification, our margin-based method yields substantial improvements in term of calibration. 2.