Abstract
Fine-grained image classiﬁcation is a challenging com-puter vision task where various species share similar vi-sual appearances, resulting in misclassiﬁcation if merely based on visual clues. Therefore, it is helpful to leverage additional information, e.g., the locations and dates for data shooting, which can be easily accessible but rarely exploited.
In this paper, we ﬁrst demonstrate that exist-ing multimodal methods fuse multiple features only on a single dimension, which essentially has insufﬁcient help in feature discrimination. To fully explore the potential of multimodal information, we propose a dynamic MLP on top of the image representation, which interacts with mul-timodal features at a higher and broader dimension. The dynamic MLP is an efﬁcient structure parameterized by the learned embeddings of variable locations and dates. It can be regarded as an adaptive nonlinear projection for gener-ating more discriminative image representations in visual tasks. To our best knowledge, it is the ﬁrst attempt to ex-plore the idea of dynamic networks to exploit multimodal information in ﬁne-grained image classiﬁcation tasks. Ex-tensive experiments demonstrate the effectiveness of our method. The t-SNE algorithm visually indicates that our technique improves the recognizability of image representa-tions that are visually similar but with different categories.
Furthermore, among published works across multiple ﬁne-grained datasets, dynamic MLP consistently achieves SOTA results1 and takes third place in the iNaturalist challenge at
FGVC82. Code is available at https://github.com/megvii-research/DynamicMLPForFinegrained.
*Corresponding author. †Works is done as interns in Megvii Research.
Lingfeng Yang, Xiang Li, Juntian Tao, and Jian Yang are from PCA Lab,
Key Lab of Intelligent Perception and Systems for High-Dimensional In-formation of Ministry of Education, and Jiangsu Key Lab of Image and
Video Understanding for Social Security, School of Computer Science and
Engineering, Nanjing University of Science and Technology. 1https://paperswithcode.com/dataset/inaturalist 2https://www.kaggle.com/c/inaturalist-2021/leaderboard (a) (b) (c) (d) (e)
Figure 1. Visualization of t-SNE [44] representations under well trained models. Various similar species from the genus Turdus in the iNaturalist 2021 dataset are depicted in different colors. (a):
The visualization of an image-only model. (b): Concatenating the image, location, and date features before the classiﬁcation head is a typical strategy of utilizing additional information to help clas-siﬁcation. The concatenated representation is more discrimina-tive than the original. The concatenation strategy can be regarded as a baseline for all methods that involve additional information. (c): Intuitively, our proposed dynamic MLP expands the diversity among different ﬁne-grained species compared to the image-only or concatenation framework. (d): The 3-d visualization of image representation from the concatenation strategy. (e): The 3-d visu-alization of our dynamic MLP. 1.

Introduction
Fine-grained image classiﬁcation [1, 3, 5, 49, 53–55] is a challenging computer vision task that distinguishes ﬁne categories of objects or species. In contrast to traditional image classiﬁcation [6, 18, 19, 26, 50], ﬁne-grained image
Turdus pilaris
Turdus rufiventris
Turdus falcklandii (a) (b) (c)
Figure 2. The pilaris, ruﬁventris, and falcklandii are species be-longing to the genus Turdus. (a): They are visually similar and hard to recognize based on their appearance. (b): Their habitats vary widely. (c): Their activity frequency varies throughout the year, so the data amount at different times is different. classiﬁcation has difﬁculty identifying different species but with almost the same appearances. Those visually similar samples are practically impossible to differentiate if merely based on images. Apart from several popular ﬁne-grained methods that focus on the discriminative regions of im-ages [3, 49, 53–55], multi-branch learning [1, 5, 13, 41, 57], or particular data augmentations [20, 25], another available direction is to introduce additional information, e.g., ge-ographical and temporal information, to help ﬁne-grained image classiﬁcation [8, 28, 37, 48].
Images usually contain additional information, e.g., ge-ographic location and time, denoting where and when to shoot, which can help with ﬁne-grained classiﬁcation. For example, we choose three species from the genus Turdus, such as pilaris, ruﬁventris, and falcklandii, that are visually similar and thus difﬁcult to distinguish (Fig. 2 (a)). How-ever, their living locations (Fig. 2 (b)) and temporal dis-It indicates that tribution (Fig. 2 (c)) are quite different. geographical and temporal information can be helpful to fa-cilitate their accurate classiﬁcation. Further, additional in-formation such as locations or dates has already been pro-vided in some well-known datasets like BirdSnap [4], Plant-CLEF [15], YFCC100M [39], and iNaturalist [45, 46], and is widely available on the internet [11, 14, 21, 22].
Several works have proposed the use of additional infor-mation in ﬁne-grained image classiﬁcation. [29, 31, 35, 37, 38] directly concatenate the image feature with the multi-modal feature before the ﬁnal classiﬁcation head. Further-more, the addition [8] and multiplication [28, 38] opera-tions are adopted to fuse the features or predictions from the last network layer. The core strategies are summarized in Fig. 3 (b)-(d), respectively. However, they only refer to a single dimension between image and multimodal features.
As shown in Fig. 1 (a)-(b), the concatenation strategy only pulls the cluster apart vertically—in one dimension. More analysis can be found in Sec.3.3.
To fully exploit the potential effect of additional infor-mation, we propose to involve the higher-dimensional inter-action between the multimodal representations. Evidently, since species with similar appearances have related image features extracted by the same network, a ﬁxed projection fails to distinguish accurately if their categories are differ-ent. Especially when their locations are numerically close, existing multimodal methods technically lack the potential to make a distinction. Thus, a dynamic, instance-wise pro-jection, which maps similar image features to different posi-tions in the feature space, can manage to classify accurately.
Different from existing works, we propose dynamic MLP to exploit the additional information in the form of adap-tive perceptron weight to enhance the representation ability of image features (Fig. 3 (e)). Speciﬁcally, the weights of dynamic MLP are generated from the multimodal features extracted from the additional information. Then the image feature is updated by the dynamic MLP, where it is condi-tionally transformed by the adaptive weight. The projecting process in the dynamic MLP involves high-dimensional in-teraction between the image feature and multimodal feature and is veriﬁed to be more efﬁcient in separating the deci-sion boundaries of similar species. In Fig. 1, the clusters denoting similar categories are pulled apart in all directions evenly, which is more effective than the former work.
To verify the effectiveness of our proposed dynamic
MLP, we conduct extensive experiments on four well-known ﬁne-grained image classiﬁcation datasets (iNatural-ist 2017, 2018, 2021 [45,46] and YFCC100M-GEO100 [37, 39]). Notably, our dynamic MLP consistently outperforms previous works by 0.2% ∼ 5.8% top-1 accuracy across a variety of popular benchmarks.
Overall, our contributions can be summarized as follows:
• We propose the dynamic MLP, an end-to-end trainable framework that jointly exploits images and additional infor-mation with high efﬁciency. To the best of our knowledge, we are the ﬁrst to use dynamic MLP in multimodal ﬁne-grained image classiﬁcation tasks.
• Compared to existing published works, our method consistently achieves SOTA results on multiple datasets, speciﬁcally 76.81%, 83.67%, and 91.39% top-1 accuracy on iNaturalist 2017, 2018, and 2021, respectively.
• An ensemble of dynamic MLP reaches 94.75% top-1 accuracy on the iNaturalist 2021 dataset, which achieves 3rd place in the FGVC8 [10] at CVPR 2021. 2.