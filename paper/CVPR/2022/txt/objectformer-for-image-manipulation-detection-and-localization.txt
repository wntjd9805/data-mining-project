Abstract
Recent advances in image editing techniques have posed serious challenges to the trustworthiness of multimedia data, which drives the research of image tampering detec-tion. In this paper, we propose ObjectFormer to detect and localize image manipulations. To capture subtle manipula-tion traces that are no longer visible in the RGB domain, we extract high-frequency features of the images and combine them with RGB features as multimodal patch embeddings.
Additionally, we use a set of learnable object prototypes as mid-level representations to model the object-level con-sistencies among different regions, which are further used to refine patch embeddings to capture the patch-level con-sistencies. We conduct extensive experiments on various datasets and the results verify the effectiveness of the pro-posed method, outperforming state-of-the-art tampering de-tection and localization methods. 1.

Introduction
With the rapid development of deep generative models like GANs [13, 25, 47] and VAEs [18, 33], a multitude of image editing applications have become widely accessible to the public [10,20,29,38]. These editing tools make it easy and effective to produce photo-realistic images and videos that could be used for entertainment, interactive design, etc., which otherwise requires professional skills. However, there are growing concerns on the abuse of editing tech-niques to manipulate image and video content for malicious purposes. Therefore, it is crucial to develop effective image manipulation detection methods to examine whether images have been modified or not and identify regions in images that have been modified.
Image manipulation techniques can be generally classi-fied into three types: (1) splicing methods which copy re-gions from one image and paste to other images, (2) copy-†Corresponding author.
Figure 1. Tampering images usually contain manipulated objects.
Thus, exploiting object-level consistency is crucial for manipula-tion detection. move which shifts the spatial locations of objects within im-ages, and (3) removal methods which erase regions from images and inpaint missing regions with visually plausible contents. As shown in Figure 1, to produce semantically meaningful and perceptually convincing images, these ap-proaches oftentimes manipulate images at the object-level, i.e., adding/removing objects in images. While there are some recent studies focusing on image manipulation detec-tion [16, 43, 46], they typically use CNNs to directly map input images to binary labels (i.e., authentic/manipulated) without explicitly modeling object-level representations.
In contrast, we posit that image manipulation detection should not only examine whether certain pixels are out of distribution, but also consider whether objects are consistent with each other.
In addition, visual artifacts brought by image editing that are no longer perceptible in the RGB domain are oftentimes noticeable in the frequency domain [6, 31, 39]. This demands a multimodal approach that jointly models the RGB domain and the frequency do-main to discover subtle manipulation traces.
In this paper, we introduce ObjectFormer, a multimodal transformer framework for image manipulation detection
and localization. ObjectFormer builds upon transformers due to their impressive performance on a variety of vision tasks like image classification [12, 15, 24], object detec-tion [5,49], video classification [4,23,40,41], etc. More im-portantly, transformers are natural choices to model whether patches/pixels are consistent in images, given that they ex-plore the correlations between different spatial locations us-ing self-attention.
Inspired by object queries that are au-tomatically learned [1, 49], we use a set of learnable pa-rameters as object prototypes (serving as mid-level object representations) to discover the object-level consistencies, which are further leveraged to refine the patch embeddings for patch-level consistencies modeling.
With this in mind, ObjectFormer first converts an im-age from the RGB domain to the frequency domain using
Discrete Cosine Transform and then extracts multimodal patch embeddings with a few convolutional layers. The
RGB patch embeddings and the frequency patch embed-dings are further concatenated to complement each other.
Further, we use a set of learnable embeddings as object queries/prototypes, interacting with the derived patch em-beddings to learn consistencies among different objects. We refine patch embeddings with these object prototypes with cross-attention. By iteratively doing so, ObjectFormer de-rives global feature representations that explicitly encode mid-level object features, which can be readily used to de-tect manipulation artifacts. Finally, the global features are used to predict whether images have been modified and the corresponding manipulation mask in a multi-task fashion.
The framework can be trained in an end-to-end manner.
We conduct experiments on commonly used image tamper-ing datasets, including CASIA [11], Columbia [35], Cov-erage [42], NIST16 [27], and IMD20 [28]. The results demonstrate that ObjectFormer outperforms state-of-the-art tampering detection and localization methods. In summary, our work makes the following key contributions:
• We introduce ObjectFormer, an end-to-end multi-modal framework for image manipulation detection and localization, combining RGB features and fre-quency features to identify the tampering artifacts.
• We explicitly leverage learnable object prototypes as mid-level representations to model object-level consis-tencies and refined patch embeddings to capture patch-level consistencies.
• We conduct extensive experiments on multiple bench-marks and demonstrate that our method achieves state-of-the-art detection and localization performance. 2.