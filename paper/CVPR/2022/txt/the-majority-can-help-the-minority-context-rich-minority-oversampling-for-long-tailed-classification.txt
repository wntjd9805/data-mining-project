Abstract
The problem of class imbalanced data is that the gener-alization performance of the classifier deteriorates due to the lack of data from minority classes. In this paper, we pro-pose a novel minority over-sampling method to augment di-versified minority samples by leveraging the rich context of the majority classes as background images. To diversify the minority samples, our key idea is to paste an image from a minority class onto rich-context images from a majority class, using them as background images. Our method is sim-ple and can be easily combined with the existing long-tailed recognition methods. We empirically prove the effectiveness of the proposed oversampling method through extensive ex-periments and ablation studies. Without any architectural changes or complex algorithms, our method achieves state-of-the-art performance on various long-tailed classifica-tion benchmarks. Our code is made available at https:
//github.com/naver-ai/cmo. 1.

Introduction
Real-world data are likely to be inherently imbal-anced [13, 19, 29, 30], where the number of samples per class differs greatly. If models are trained on an imbalanced dataset, they can be easily biased toward majority classes and tend to have a poor generalization ability on recogniz-ing minority classes (i.e., overfitting).
A simple and straightforward method to overcome the class imbalance problem is to repeatedly oversample the minority classes [6,44]. However, these naive oversampling can intensify the overfitting problem, since the repeatedly selected samples have less diversity but almost similar im-age contexts [39]. For example, consider a minority class of
‘snow goose,’ in which the geese always stand upon grass in the training images. If samples are drawn from these limited training samples [44] or even if new samples are produced
*The work for this paper was performed as part of an internship at
NAVER AI Lab.
Figure 1. Concept of context-rich minority oversampling. In the real-world long-tailed dataset iNaturalist 2018 [19], the number of samples from the head class and the tail class is extremely differ-ent (Upper). Simple random oversampling method repeatedly pro-duces context-limited images from minority classes. We propose a novel context-rich oversampling method to generate diversified minority images. To this end, we oversample the tail-class images with various sizes. Then, these patches are pasted onto the head-class images to have various backgrounds. Our key idea is to bring rich contexts from majority samples into minority samples. by interpolating within the class [6], only context-limited images will be created as in Figure 1. Our goal is to solve the aforementioned problem by introducing a simple context-rich oversampling method.
We pay attention to the characteristics of long-tailed 1
distributions; that is, majority class samples are data-rich and information-rich. Unlike the existing re-sampling meth-ods that ignore (i.e., undersample) majority samples, our method uses the affluent information of the majority sam-ples to generate new minority samples. Specifically, our idea is to leverage the rich major-class images as the back-ground for the newly created minor-class images. Figure 1 illustrates the concept of our proposed context-rich over-sampling strategy. Given an original image from a minority class, the object is cropped in various sizes and pasted onto the various images from majority classes. Then, we can cre-ate images with more diverse contexts (e.g., ‘snow goose’ images with the sky, road, roof, crows, etc). Since this is an interpolation of the majority and minority class samples, it generates diversified data around the decision boundary, and as a result, it improves the generalization performance for minority classes.
To this end, we adopt an image-mixing data augmenta-tion method, CutMix [49]. As our key idea is to transfer rich contexts from majority to minority samples, we apply a simple and effective data sampling method to generate new minority-centric images with majority’s contexts. However, naive use of CutMix may exacerbate the overfitting prob-lem in favor of the majority classes because it may gener-ate more majority-centric samples than minority samples.
We solve this problem by sampling the background images and the foreground patches from different distributions to achieve the desired minority oversampling.
Our key contributions can be summarized as follows: (1) We propose a novel context-rich minority oversam-pling method that generates various samples by leverag-ing the rich context of the majority classes as background images. (2) Our method requires little additional computa-tional cost and can be easily integrated into many end-to-end deep learning algorithms for long-tailed recognition. (3)
We demonstrate that significant performance improvements and state-of-the-art performance can be achieved by apply-ing the proposed oversampling to existing commonly used loss functions without any architectural changes or complex algorithms. (4) We empirically prove the effectiveness of the proposed oversampling method through extensive ex-periments and ablation studies. We believe that our study offers a useful and universal minority oversampling method for research into long-tailed classification. 2.