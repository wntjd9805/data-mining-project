Abstract
Representing scenes with multiple semitransparent col-ored layers has been a popular and successful choice for real-time novel view synthesis. Existing approaches infer colors and transparency values over regularly spaced lay-ers of planar or spherical shape. In this work, we introduce a new view synthesis approach based on multiple semitrans-parent layers with scene-adapted geometry. Our approach infers such representations from stereo pairs in two stages.
The ﬁrst stage produces the geometry of a small number of data-adaptive layers from a given pair of views. The second stage infers the color and transparency values for these lay-ers, producing the ﬁnal representation for novel view syn-thesis.
Importantly, both stages are connected through a differentiable renderer and are trained end-to-end. In the experiments, we demonstrate the advantage of the proposed approach over the use of regularly spaced layers without adaptation to scene geometry. Despite being orders of mag-nitude faster during rendering, our approach also outper-forms the recently proposed IBRNet system based on im-plicit geometry representation. 1.

Introduction
Recent years have seen rapid progress in image-based rendering and novel view synthesis, with a multitude of var-ious methods based on neural rendering approaches [32].
Among this diversity, the approaches that are based on semitransparent multi-layer representations [21, 29, 30, 34, 39] stand out due to their combination of fast rendering time, compatibility with traditional graphics engines, and good quality of re-rendering in the vicinity of the input frames.
Existing approaches [4, 17, 21, 29, 30, 34, 39] build multi-layer representations over grids of regularly spaced surfaces
*Most of the work was done while Victor Lempitsky was at Samsung
AI Center such as planes or spheres with uniformly changing inverse depth. As the number of layers is necessarily limited by resource constraints and the risk of overﬁtting, this num-ber is usually taken to be relatively small (e.g. 32). The re-sulting semi-transparent representation may therefore only coarsely approximate the true geometry of the scene, which limits the generalization to novel views and introduces arte-facts. The most recent works [4, 17] use excessive num-ber of spheres (up to 128) and then merge the resulting ge-ometry using a non-learned post-processing merging step.
While the merge step creates scene-adapted and compact geometric representation, it is not incorporated into the learning process of the main matching network, and de-grades the quality of novel view synthesis [4].
The coarseness of layered geometry used by multi-layer approaches is in contrast to more traditional image-based rendering methods that start by estimating the non-discretized scene geometry in the form of mesh [25, 33], view-dependent meshes [11], a single-layer depth map [23, 27, 37]. Geometry estimates may come from multiview dense stereo matching or from monocular depth. All these approaches obtain a ﬁner approximation to scene geometry, although most of them have to use a relatively slow neural rendering step to compensate for the errors in the geometry estimation.
Our approach called StereoLayers (Fig. 1) combines scene geometry adaptation with multi-layer representation.
This model is designed for a case known as stereo magni-ﬁcation problem: it reconstructs the scene from as few as two input images. The proposed method starts by building a geometric proxy that is customized to a particular scene.
The proxy is formed by a small number of mesh layers with continuous depth coordinate values.
In the second stage, similarly to other multi-layer approaches, we estimate the transparency and color textures for each layer, resulting in the ﬁnal representation of the scene. When processing a new scene, both stages take the same pair of images of that scene as input. Two deep neural networks trained on a dataset of similar scenes are used to implement these two
Figure 1. The proposed StereoLayers pipeline estimates scene-adjusted multi-layer geometry from the plane sweep volume using a pre-trained geometry network, and after that estimates the color and transparency values using a pretrained coloring network. The layered geometry represents the scene as an ordered set of mesh layers. The geometry and the coloring networks are trained together end-to-end. stages. Crucially, we train both neural networks together in an end-to-end fashion using the differentiable rendering framework [16].
We compare our approach to the previously proposed methods that use regularly spaced layers on the popular
RealEstate10k [39] and LLFF [21] datasets.
In addition, we propose a more challenging new dataset for novel view synthesis benchmarking.
In both cases, we observe that scene-adaptive geometry in our approach results in better novel view synthesis quality than the use of non-adaptive geometry. To put our work in a broader context, we also compare our system’s performance with the IBRNet sys-tem [36], and observe the advantage of our approach, in ad-dition to the considerably faster rendering time. In general, our approach produces very compact scene representations that are amenable for real-time rendering even on low-end devices.
To sum up, our contributions are as follows. First, we propose a new method for the geometric reconstruction of a scene from pairs of stereo. The method represents scenes using a small number of semitransparent layers with scene-adapted geometry. Unlike other related methods, ours uses two jointly (end-to-end) trained deep networks, the ﬁrst of which estimates the geometry of the layers, while the sec-ond estimates the transparency and color textures of the lay-ers. Finally, we evaluate our approach on previously pro-posed datasets and introduce a new challenging dataset for training and evaluation of novel view synthesis methods. 2.