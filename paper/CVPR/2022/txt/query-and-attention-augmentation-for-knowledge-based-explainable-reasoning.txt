Abstract
Explainable visual question answering (VQA) models have been developed with neural modules and query-based knowledge incorporation to answer knowledge-requiring questions. Yet, most reasoning methods cannot effectively generate queries or incorporate external knowledge during the reasoning process, which may lead to suboptimal re-sults. To bridge this research gap, we present Query and
Attention Augmentation, a general approach that augments neural module networks to jointly reason about visual and external knowledge. To take both knowledge sources into account during reasoning, it parses the input question into a functional program with queries augmented through a novel reinforcement learning method, and jointly directs aug-mented attention to visual and external knowledge based on intermediate reasoning results. With extensive experi-ments on multiple VQA datasets, our method demonstrates significant performance, explainability, and generalizabil-ity over state-of-the-art models in answering questions re-quiring different extents of knowledge. Our source code is available at https://github.com/SuperJohnZhang/QAA. 1.

Introduction
Reasoning about knowledge is essential for general in-telligent behavior [32]. Humans have the innate ability to acquire and incorporate concepts from multiple knowledge sources, yet to simulate this mechanism with machine in-telligence is nontrivial. Visual question answering (VQA) is a typical task that requires both knowledge acquisition and knowledge reasoning abilities. A desirable VQA sys-tem should understand both inputs (i.e., image and question) and perform cross-modal reasoning by seeking supporting logic and evidence that lead to a reasonable answer.
Most VQA methods learn to answer questions based on the statistical correlations between the multi-modal inputs and the answer [5, 10, 11]. Studies have shown that such implicit data-driven methods tend to exploit language pri-Figure 1. Based on neural module networks and explicit knowl-edge representation, we develop knowledge-augmented queries and memory-augmented attention to jointly reason about both the visual [V] and external [E] knowledge. These query and atten-tion augmentation methods generalize explainable visual reason-ing models to better answer knowledge-requiring questions. ors to achieve high performance, instead of reasoning based on logic and evidence [33]. To perform multimodal reason-ing, recent studies have leveraged neural modules networks (NMNs) [1] that explicitly model the multi-step reasoning process [15, 17, 43]. They parse the input question into a functional program and dynamically assemble a network of explainable neural modules to execute the program. They not only achieve remarkable performances in VQA but also provide step-by-step explanations to help understand the reasoning process behind the predicted answer [17, 33].
NMNs are commonly developed on datasets of syn-thetic and structured questions, such as CLEVR [19] and
GQA [16], which are limited in generalization. To answer more general VQA questions while maintaining explain-ability, several studies have incorporated external knowl-edge based on explicit scene graph modeling [7, 45] or im-plicit feature enrichment [22, 26, 42]. They query external concepts from knowledge bases, integrate the acquired ex-ternal knowledge with the observed visual knowledge, and finally conduct the reasoning on the integrated knowledge space [7, 41, 45]. Such approaches result in a loose integra-tion between knowledge and reasoning, which may be sub-optimal when dealing with complex reasoning problems.
In this work, we propose Query and Attention Augmenta-tion, an NMN-based explainable visual reasoning method that answers knowledge-requiring questions by jointly rea-soning about both visual knowledge (i.e., the visual fea-tures) and external knowledge (i.e., the semantic embed-dings of external concepts). Different from previous meth-ods that incorporate knowledge prior to the reasoning pro-cess, it tightly couples knowledge incorporation with rea-soning, which addresses two major research gaps:
First, previous methods generate functional program based only on the input question, without considering the visual or external information. As shown in Fig. 1, to an-swer the question “What can this place in the image be used for?”, they may generate a program of two functions: 1) recognizing the place and 2) finding its usage. Two input tokens (e.g., place and used) can be extracted from the ques-tion and used as queries to guide the model’s attention and reasoning. Since they are extracted from the question only, the queries may be less relevant to the context and result in a wrong answer (e.g., Parking). In this work, we pro-pose to augment these question-based queries with visual and external knowledge, so that they can be more specific and relevant. For example, as shown in Fig. 1, after the augmentation, two sets of queries are generated to guide the reasoning of visual knowledge (e.g., bus and drive) and external knowledge (e.g., road and transport), respectively.
Compared with the original queries, they guide the attention of NMNs more directly to find the answer.
Second, previous methods typically acquire and incor-porate external knowledge as supporting features prior to reasoning [7, 41, 45]. However, during multi-step visual reasoning, the reasoning context is dynamically updated throughout the process, where additional knowledge may need to be acquired and understood along the way. To enable this ability, we propose to jointly reason about the visual and external knowledge and use a novel memory-augmented attention method to integrate their intermediate results for reasoning, so the knowledge is integrated during the reasoning process instead of only at the beginning of it. As shown in Fig. 1, jointly directing attention to impor-tant visual knowledge (e.g., Bus) and external knowledge (e.g., Road, Transport) throughout the reasoning process can help NMNs make better use of both knowledge sources to find the correct answer (e.g., Transport).
In sum, by addressing these challenges, our proposed method allows NMNs to accurately direct attention to im-portant features in both visual and external knowledge and answer knowledge-requiring questions. The contributions of this work are summarized as follows: 1. To the best of our knowledge, this work is the first at-tempt to jointly reason about visual knowledge and external knowledge based on neural module networks. 2. With reinforcement learning, we generate knowledge-augmented queries to incorporate visual and external knowledge into the functional program. 3. By sharing intermediate results between the two knowledge sources with memory-augmented attention, we enable external knowledge incorporation throughout the reasoning process. 4. Our extensive experiments on multiple VQA datasets demonstrate the effectiveness, generalizability, and explain-ability of the proposed method. 2.