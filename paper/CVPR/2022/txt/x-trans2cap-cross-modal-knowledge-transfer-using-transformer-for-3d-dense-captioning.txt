Abstract 3D dense captioning aims to describe individual objects in 3D scenes by natural language, where 3D scenes are usu-ally represented as RGB-D scans or point clouds. However, only exploiting single modal information, e.g., point cloud, previous approaches fail to produce faithful descriptions.
Though aggregating 2D features into point clouds may be beneficial, it introduces an extra computational burden, es-pecially in the inference phase. In this study, we investigate a cross-modal knowledge transfer using Transformer for 3D dense captioning, namely X-Trans2Cap. Our proposed
X-Trans2Cap effectively boost the performance of single-modal 3D captioning through the knowledge distillation en-abled by a teacher-student framework. In practice, during the training phase, the teacher network exploits auxiliary 2D modality and guides the student network that only takes point clouds as input through the feature consistency con-straints. Owing to the well-designed cross-modal feature fu-sion module and the feature alignment in the training phase,
X-Trans2Cap acquires rich appearance information embed-ded in 2D images with ease. Thus, a more faithful caption can be generated only using point clouds during the infer-ence. Qualitative and quantitative results confirm that X-Trans2Cap outperforms previous state-of-the-art by a large margin, i.e., about +21 and +16 CIDEr points on ScanRefer and Nr3D datasets, respectively. 1.

Introduction
Hitherto, the computer vision community has witnessed significant progress in image captioning [3, 25, 33, 46, 50] and dense captioning [24–26, 30] under the success of deep learning techniques. Unlike image captioning describing a 2D image with a single sentence, dense captioning (DC) better interprets “A picture is worth a thousand words”.
* Corresponding author: Zhen Li. † Equal first authorship.
Figure 1. The motivation of cross-modal knowledge transfer. (a) Previous methods use the extra 2D modality as the input in both training and inference phases. (b) In contrast, we exploit a teacher-student framework with multi-modal data during training.
For inference, the student network only takes 3D modality input.
That is to say, for DC task, each object in an image is first perceived, then is provided more customized and detailed descriptions according to its nature and context.
Most recently, 3D cross-modal learning in vision and language has gained an increasing amount of interest as well. Several datasets [1, 6, 15] and downstream applica-tions [20, 56] are proposed and investigated. Unlike 2D images with regular grids and dense pixels, 3D data rep-resented by a set of points are unordered and scattered in the 3D space, impeding the direct extension of 2D-based methods to 3D scenarios. To perform dense captioning on 3D point clouds, [9] proposes the first method, namely
Scan2Cap, by directly combining 3D object detection with natural language generation. Specifically, Scan2Cap first employs a detection backbone to obtain object proposals,
and then applies a relational graph and a context-aware attention captioning module to learn object relations and generate tokens. Besides, multi-view features extracted by the pre-trained E-Net [36] are further projected onto the input point cloud to enhance final captioning. However,
Scan2Cap still has several issues: 1) The object representa-tions in Scan2Cap are defective since they are solely learned from sparse 3D point clouds, thus failing to provide strong texture and color information compared with the ones gen-erated from 2D images. 2) It requires the extra 2D input in both training and inference phases, as shown in Figure 1 (a).
However, the extra 2D information is usually computation-intensive and unavailable during inference. For instance, a model training with both 2D and 3D inputs cannot apply to
LiDAR scenarios that only contains 3D point cloud.
To address the above issues, we explore how to ease the barrier of cross-modal learning on 2D and 3D data, and investigate how to effectively combine the merits of both modalities for 3D dense captioning in this paper. To this end, we first time present a flexible and novel cross-modal framework, namely X-Trans2Cap1, which transfers color and texture-aware information from 2D image into 3D ob-ject representation using Transformer [44]. Concretely, all the instances in a given scene can be firstly extracted by 3D object detection. Subsequently, the 3D features of each instance and its 2D counterpart are processed by a teacher-student framework. Within this framework, the teacher net-work takes the multi-modal inputs, while the student one only leverages the 3D inputs. Considering different modali-ties for teacher and student streams, we innovatively design a Transformer-based knowledge transfer framework with more flexible input control and better representation. More-over, to further enhance the knowledge transfer, a modified knowledge distillation operation with cross-modal fusion (CMF) module and cross-modal feature alignment objec-tive is proposed for knowledge generalization. Owing to the end-to-end training scheme, the priors in the 2D modal-ity can inherently improve the teacher network and the stu-dent as well, i.e., our model takes advantage of the color and texture aware 2D representation and reduces the ex-tra computational cost. Therefore, in the inference phase,
X-Trans2Cap can perform superior captioning performance with only 3D inputs, as shown in Figure 1 (b).
Sufficient experiments evaluated on the ScanRefer [6] and Nr3D [1] datasets have demonstrated the effectiveness of our proposed X-Trans2Cap. In specific, with the extra 2D priors and the novel framework design, X-Trans2Cap can effectively learn a better 3D object representation and boost the performance over the model without 2D priors, i.e., improving the CIDEr points on ScanRefer from 75.75 to 87.09. This result also exceeds the previous state-of-the-art Scan2Cap by about 21 CIDEr. 1https://github.com/CurryYuan/X-Trans2Cap
In summary, our main contributions are threefold:
• We first time propose X-Trans2Cap, a simple but ef-fective cross-modal knowledge transfer framework for 3D dense captioning, in which an enhanced 3D repre-sentation with 2D priors is achieved.
• X-Trans2Cap leverages a modified knowledge distilla-tion method through a novel cross-modal fusion mod-ule and feature alignment techniques merged in Trans-former, eliminating extra computation burdens during inference while achieving superior knowledge transfer.
• Our X-Trans2Cap gains significant performance boost on the ScanRefer [6] (+21.0 CIDEr) and Nr3D [1] (+16.7 CIDEr) datasets. 2.