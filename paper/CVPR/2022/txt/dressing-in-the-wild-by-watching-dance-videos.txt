Abstract
While significant progress has been made in garment transfer, one of the most applicable directions of human-centric image generation, existing works overlook the in-the-wild imagery, presenting severe garment-person mis-alignment as well as noticeable degradation in fine tex-ture details. This paper, therefore, attends to virtual try-on in real-world scenes and brings essential improvements in authenticity and naturalness especially for loose garment (e.g., skirts, formal dresses), challenging poses (e.g., cross arms, bent legs), and cluttered backgrounds. Specifically, we find that the pixel flow excels at handling loose gar-ments whereas the vertex flow is preferred for hard poses, and by combining their advantages we propose a novel gen-erative network called wFlow that can effectively push up garment transfer to in-the-wild context. Moreover, former approaches require paired images for training. Instead, we cut down the laboriousness by working on a newly con-structed large-scale video dataset named Dance50k with self-supervised cross-frame training and an online cycle op-timization. The proposed Dance50k can boost real-world virtual dressing by covering a wide variety of garments un-der dancing poses. Extensive experiments demonstrate the superiority of our wFlow in generating realistic garment transfer results for in-the-wild images without resorting to expensive paired datasets. 1 1Xiaodan Liang is the corresponding author. The project page of wFlow is https://awesome-wflow.github.io.
1.

Introduction
Garment transfer, the process of transferring garments onto a query person image without changing the person’s identity, is a central problem in human-centric image gen-eration that promises great commercial potential. However, when getting down to in-the-wild scenarios, general solu-tions are required that can leverage easily accessible train-ing data, handle arbitrary garments, and cope with complex poses presented in real world environments.
Unluckily, most existing works [10, 11, 13, 32, 42, 46, 47, 49, 51] serve to fit an in-shop garment to a target person by utilizing either pixel flow [12] or TPS transformation [2].
Despite their promise, these methods become less effective at exchanging garments directly between two persons, due to the deficiency of the 2D transformations when faced with large pose variations. Also, previous methods require paired data for training, i.e., a person and its associated garment image, which further leads to laborus collection process.
These limitations largely hinder their practical use and raise the need for scalable solutions that can be trained on eas-ily accessible data. [27] takes a step forward by replacing the 2D pixel flow with 3D SMPL [29] vertex flow, allowing person-to-person garment transfer and can address complex poses or severe self-occlusion. However, it is error prone to loose garments that can not be modeled as part of the SMPL surface. Albeit subjects to simple poses, the 2D pixel flow can then again predict more faithful pixel mapping for these challenging loose garments.
Therefore, in this paper, we propose wFlow that effi-ciently integrates respective advantages of the 2D pixel flow and the 3D vertex flow. Based on the wFlow, a robust gar-ment transfer network is developed to tackle the essential challenges on in-the-wild imagery.
In particular, we de-sign a self-supervised training scheme that works on easily obtainable dance videos by exploiting cross-frame consis-tency, getting rid of the hard-to-get paired dataset.
Our insight is that a well-designed flow-based model trained on multi-pose images of the same person, which is easily accessible in dance videos, can generalize well at testing to transfer garments between different persons by adding protected body parts that guides the network to fo-cus on the garment regions. Thus, we collect thousands of single-person dance videos with diverse garment types as the training dataset, and sample from it a plethora of multi-pose person frames to train our model in a similar fashion of pose transfer [24, 34], where the designed wFlow that associates different frames allows us to self-supervise the training procedure without ground truth flow supervision.
To fully exploit the wFlow, we first pass the source and query person representation through a conditional segmen-tation network, producing a person segmentation that com-plies with both the source garment and the query pose.
Given the predicted segmentation, a pixel flow network is employed to estimate the pixel-wise correspondences be-tween the source and the query images. Thereafter, we compute 3D SMPL vertex flow directly from the inputs and project it to image plane where the pixel flow is also injected to form the proposed wFlow. The warped garment can now be obtained by applying the wFlow on the source garment.
Finally, a skip-connected inpainting network leverages the warped garment along with the protected person regions and fuses them with the inpainted query background. Addition-ally, for garments presented scarcely in training data, we further formulate a cyclic online optimization to enhance the quality of their transfer results. Thanks to the contribu-tory video data and the potent texture mapping ability of the wFlow, our model can handle arbitrary garments and seam-lessly transfer them onto challenging posed query persons.
Overall, we present three main contributions:
• We are the first to explore the in-the-wild garment transfer problem. By exploiting a self-supervised training scheme that works on easily accessible dance videos, our model generates surprising results with sharp textures and intact garment shape.
• To facilitate arbitrary garment transfer under complex poses in real-world scenario, we introduce a novel wFlow (flow in-the-wild) that integrates both 2D and 3D information along with a cyclic online optimization that further enhances the synthesis quality.
• We construct a new large-scale video dataset called
Dance50k, containing 50k sequences of dancing peo-ple wearing a wide variety of garments, which is use-ful for the development of human-centric image/video processing not limited to virtual try-on. 2.