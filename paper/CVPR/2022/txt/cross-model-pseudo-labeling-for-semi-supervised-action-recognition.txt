Abstract
Typically in recent work,
Semi-supervised action recognition is a challenging but important task due to the high cost of data annotation. A common approach to this problem is to assign unlabeled data with pseudo-labels, which are then used as additional supervision in training. the pseudo-labels are obtained by training a model on the labeled data, and then using conﬁdent predictions from the model to teach itself. In this work, we propose a more effec-tive pseudo-labeling scheme, called Cross-Model Pseudo-Labeling (CMPL). Concretely, we introduce a lightweight auxiliary network in addition to the primary backbone, and ask them to predict pseudo-labels for each other. We observe that, due to their different structural biases, these two models tend to learn complementary representations from the same video clips. Each model can thus beneﬁt from its counterpart by utilizing cross-model predictions as supervision. Experiments on different data partition protocols demonstrate the signiﬁcant improvement of our framework over existing alternatives. For example, CMPL achieves 17.6% and 25.1% Top-1 accuracy on Kinetics-400 and UCF-101 using only the RGB modality and 1% labeled data, outperforming our baseline model, FixMatch [17], by 9.0% and 10.3%, respectively. 1 1.

Introduction
The rapid development of deep learning has led to great success in action recognition.
In the standard supervised learning protocol, a considerable number of annotated videos is needed but difﬁcult to acquire in practice. On the other hand, about 500 hours of video is uploaded to
YouTube every minute worldwide, providing a tremendous amount of unlabeled data. Leveraging such unlabeled videos for semi-supervised learning could thus be of great beneﬁt for action recognition. 1Project page is at https : / / justimyhxu . github . io / projects/cmpl/.
Figure 1. Category-wise performance gap between small and large networks under the supervised training setting given 1% labeled videos in Kinetics-400 [2]. Accs and Accl denote the accuracy of the small (3D-ResNet50×1/4) and the large (3D-ResNet50) network, respectively. For categories on which the large network performs poorly (i.e., the left half of the ﬁgure), the small network behaves better even with much lower model capacity. Concretely, the small network tends to perform well on classes with stronger temporal dynamics, i.e., “Swinging Legs”, while the large network better recognizes actions mainly charac-terized by spatial information, i.e., “Testifying”. See Sec. 4.4 for further discussion.
To gain supervision from unlabeled data, a common practice is to assign pseudo-labels to these data and treat them as “ground-truth” for training [11, 17, 19, 26]. Speciﬁ-cally, existing approaches train a model on labeled data and then use it to predict the unlabeled videos. If the conﬁdence in a prediction is high enough, the prediction will be taken as the pseudo-label of the video, to be used in network train-ing henceforth. The quantity and quality of pseudo-labels therefore has a signiﬁcant impact in the current learning scheme. However, the limited discriminative power derived from a small amount of labeled data leads to inadequate pseudo-labels and limits the gain from unlabeled data.
17.6% and 25.1% Top-1 accuracy on Kinetics-400 [2] and
UCF-101 [18], surpassing FixMatch [17], by 9.0% and 10.3%, respectively. We also conduct a comprehensive empirical analysis to study how the cross-model supervision helps improve performance. This analysis shows that the primary backbone has large improvement on classes for which the auxiliary network works very well, supporting our motivation that the auxiliary network can complement the backbone. 2.