Abstract
In a point cloud sequence, 3D object tracking aims to predict the location and orientation of an object in the cur-rent search point cloud given a template point cloud. Mo-tivated by the success of transformers, we propose Point
Tracking TRansformer (PTTR), which efficiently predicts high-quality 3D tracking results in a coarse-to-fine manner with the help of transformer operations. PTTR consists of three novel designs. 1) Instead of random sampling, we de-sign Relation-Aware Sampling to preserve relevant points to given templates during subsampling. 2) Furthermore, we propose a Point Relation Transformer (PRT) consist-ing of a self-attention and a cross-attention module. The global self-attention operation captures long-range depen-dencies to enhance encoded point features for the search area and the template, respectively. Subsequently, we gen-erate the coarse tracking results by matching the two sets of point features via cross-attention. 3) Based on the coarse tracking results, we employ a novel Prediction Refinement
Module to obtain the final refined prediction. In addition, we create a large-scale point cloud single object tracking benchmark based on the Waymo Open Dataset. Extensive experiments show that PTTR achieves superior point cloud tracking in both accuracy and efficiency. Our code is avail-able at https://github.com/Jasonkks/PTTR. 1.

Introduction
With the rapid development of 3D sensors in the past decade, solving various vision problems [16, 20, 21, 23, 24, 26, 29, 42, 43, 45] with point clouds has attracted increasing attention due to the huge potential in applications such as autonomous driving, motion planning, and robotics. As a long-standing research problem in computer vision, object tracking with point clouds has also drawn wide research in-terests. 3D object tracking aims to detect not only object poses and positions in each frame but also object motion
*Work done during an internship at Sensetime
†Equal contribution
‡Corresponding author
Figure 1. (a) 3D point cloud object tracking aims to track the tar-get object based on a given template point cloud. (b) PTTR out-performs existing approaches by large margins on KITTI tracking dataset [9]. (c) We visualize our tracking results over consecutive frames, including “100 Frames”, “200 Frames” and “300 Frames”, which demonstrate the robustness of PTTR for long-term tracking. trajectories across consecutive frames. However, 3D track-ing still faces a number of open and challenging problems such as LiDAR point cloud sparsity, random shape incom-pleteness, texture feature absence, etc.
Existing 3D object tracking approaches can be largely categorized into two groups: multi-object tracking (MOT) and single-object tracking (SOT). MOT methods [36,39,40, 46] generally adopt a detect-to-track strategy by first detect-ing objects in each frame and then matching the detections across consecutive frames based on the estimated location or speed. In contrast, most SOT methods are only required to process a subset of point clouds, which usually come with much lower computational consumption and higher throughput. We study SOT in this work, and our objective is to estimate the location and orientation of a single object in the search frame given an object template.
The pioneer 3D SOT method SC3D [10] first generates a series of candidates given the last location of a specific object, and the prediction is made by selecting the best-matched candidate in the latent space. However, it is not end-to-end trainable and suffers from low inference speed 1
due to requiring a large number of candidates. Without us-ing many candidates, P2B [25] first use cosine similarity to fuse features of the search region with the template and then adopts the prediction head of VoteNet [22] to generate the final prediction. Following P2B, SA-P2B [52] adds an extra auxiliary network to predict the object structure. In a sim-ilar framework, 3D-SiamRPN [8] uses a cross-correlation module for feature matching and an RPN head for final pre-diction. These methods [10, 25, 52] essentially perform a linear matching process between features in the search do-main and the template, which cannot adapt to different 3D observations caused by random noise, sparsity, and occlu-sions. Moreover, the inclusion of complex prediction heads as in detection models highly limits their tracking speed, which is a crucial factor for online applications.
In this work, we design Point Tracking TRansformer (PTTR), a novel tracking paradigm that achieves high-tracking in a coarse-to-fine fashion. quality 3D object
Specifically, PTTR first extracts point features from the template and search area individually using the PointNet++
[24] backbone. To alleviate the point sparsity issue, we pro-pose a sampling strategy termed Relation-Aware Sampling, which can preserve more points that are relevant to the given template by leveraging the relation-aware feature similar-ities between the search and the template. We then pro-pose a novel Point Relation Transformer (PRT) equipped with Relation Attention Module to match search and tem-plate features and generate a coarse prediction based on the matched feature. PRT first utilizes a self-attention opera-tion to adaptively aggregate point features for the template and the search area individually, and then performs feature matching with a cross-attention operation. Moreover, we propose a lightweight Prediction Refinement Module to re-fine the coarse prediction with local feature pooling. We highlight that PTTR is more efficient than existing methods despite its prediction refinement process.
KITTI tracking dataset [9] has been widely adopted in 3D tracking evaluations. However, it has clear constraints including limited sample size and highly imbalanced class distributions. We create a new point cloud tracking bench-mark named Waymo SOT Dataset based on Waymo Open
Dataset [30], which has a large sample size as well as bal-anced class distributions. The new benchmark is thus com-plementary to the KITTI tracking dataset by offering more holistic and comprehensive evaluations to the 3D track-ing research community. Extensive experiments on both datasets demonstrate the superior performance of PTTR in both accuracy and efficiency.
Our key contributions are summarized as: 1) We propose
PTTR, a transformer-based 3D point cloud object track-ing method, which employs a novel coarse-to-fine tracking paradigm to first generate coarse global prediction and re-fine it with Local Pooling. 2) We design two novel modules in PTTR including Point Relation Transformer for effec-tive feature aggregation and matching, and Relation-Aware
Sampling for preserving more template-relevant points. 3)
PTTR surpasses previous SoTA methods by large margins in performance with lower computational cost. 4) We gen-erate a new large-scale point cloud tracking dataset based on the Waymo Open Dataset [30] to facilitate more com-prehensive evaluations of 3D object tracking approaches. 2.