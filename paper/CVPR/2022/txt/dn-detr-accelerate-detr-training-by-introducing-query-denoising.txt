Abstract
We present in this paper a novel denoising training method to speedup DETR (DEtection TRansformer) train-ing and offer a deepened understanding of the slow conver-gence issue of DETR-like methods. We show that the slow convergence results from the instability of bipartite graph matching which causes inconsistent optimization goals in early training stages. To address this issue, except for the
Hungarian loss, our method additionally feeds ground-truth bounding boxes with noises into Transformer decoder and trains the model to reconstruct the original boxes, which effectively reduces the bipartite graph matching difficulty and leads to a faster convergence. Our method is universal and can be easily plugged into any DETR-like methods by adding dozens of lines of code to achieve a remarkable im-provement. As a result, our DN-DETR results in a remark-able improvement (+1.9AP) under the same setting and achieves the best result (AP 43.4 and 48.6 with 12 and 50 epochs of training respectively) among DETR-like methods with ResNet-50 backbone. Compared with the baseline un-der the same setting, DN-DETR achieves comparable per-formance with 50% training epochs. Code is available at https://github.com/FengLi-ust/DN-DETR. 1.

Introduction
Object detection is a fundamental task in computer vi-sion which aims to predict the bounding boxes and classes of objects in an image. While having made remarkable
*Indicates equal contribution.
†This work was done when Feng Li, Hao Zhang, and Shilong Liu were interns at IDEA.
‡Corresponding author.
Figure 1. Convergence curve between our model DN-Deformable-DETR built upon Deformable DETR with denoising training and previous models under ResNet-50 backbone.
[14, 15] were mainly based progress, classical detectors on convolutional neural networks, until Carion et al. [1] re-cently introduced Transformers [17] into object detection and proposed DETR (DEtection TRansformer).
In contrast to previous detectors, DETR uses learnable queries to probe image features from the output of Trans-former encoders and bipartite graph matching to perform set-based box prediction. Such a design effectively elimi-nates hand-designed anchors and non-maximum supperes-sion (NMS) and makes object detection end-to-end opti-mizable. However, DETR suffers from prohibitively slow training convergence compared with previous detectors. To obtain a good performance, it usually takes 500 epochs of training on the COCO detection dataset, in contrast to 12 epochs used in the original Faster-RCNN training.
Much work [3, 11, 12, 16, 18, 20] has tried to iden-tify the root cause and mitigate the slow convergence is-sue. Some of them address the problem through improv-ing the model architecture. For example, Sun et al. [16] attribute the slow convergence issue to the low efficiency
of the cross-attention and proposed an encoder-only DETR.
Dai et al. [3] designed a ROI-based dynamic decoder to help the decoder focus on regions of interest. More recent works propose to associate each DETR query with a specific spa-tial position rather than multiple positions for more efficient feature probing [11, 12, 18, 20]. For instance, Conditional
DETR [12] decouples each query into a content part and a positional part, enforcing a query to have a clear correspon-dence with a specific spatial position. Deformable DETR
[20] and Anchor DETR [18] directly treat 2D reference points as queries to perform cross-attention. DAB-DETR
[11] interprets queries as 4-D anchor boxes and learns to progressively improve them layer by layer.
Despite all the progress, few work pays attention to the bipartite graph matching part for more efficient training. In this study, we find that the slow convergence issue also re-sults from the discrete bipartite graph matching component, which is unstable especially in the early stages of training due to the nature of stochastic optimization. As a conse-quence, for the same image, a query is often matched with different objects in different epochs, which makes optimiza-tion ambiguous and inconstant.
To address this problem, we propose a novel training method by introducing a query denoising task to help stabi-lize bipartite graph matching in the training process. Since previous works have shown effective to interpret queries as reference points [18, 20] or anchor boxes [11] which con-tain positional information, we follow their viewpoint and use 4D anchor boxes as queries. Our solution is to feed noised ground truth bounding boxes as noised queries to-gether with learnable anchor queries into Transformer de-coders. Both kinds of queries have the same input format of (x, y, w, h) and can be fed into Transformer decoders si-multaneously. For noised queries, we perform a denoising task to reconstruct their corresponding ground truth boxes.
For other learnable anchor queries, we use the same training loss including bipartite matching as in the vanilla DETR.
As the noised bounding boxes do not need to go through the bipartite graph matching component, the denoising task can be regarded as an easier auxiliary task, helping DETR alleviate the unstable discrete bipartite matching and learn bounding box prediction more quickly. Meanwhile, the de-noising task also helps lower the optimization difficulty be-cause the added random noise is usually small. To maxi-mize the potential of this auxiliary task, we also regard each decoder query as a bounding box + a class label embedding so that we are able to conduct both box denoising and label denoising.
In summary, our method is a denoising training ap-proach. Our loss function consists of two components. One is a reconstruction loss and the other is a Hungarian loss which is the same as in other DETR-like methods. Our method can be easily plugged into any existing DETR-like method. For convenience, we utilize DAB-DETR [11] to evaluate our method since their decoder queries are explic-itly formulated as 4D anchor boxes (x, y, w, h). For DETR variants that only support 2D anchor points such as anchor
DETR [18], we can do denoising on anchor points. For those that do not support anchors like the vanilla DETR [1], we can do linear transformation to map 4D anchor boxes to the same latent space as for other learnable queries.
To the best of our knowledge, this is the first work to introduce the denoising principle into detection models. We summarize our contribution as follows: 1. We design a novel training method to speedup DETR training. Experimental results show that our method not only accelerates training convergence, but also leads to a remarkably better training result — achieve the best result among all detection algorithms in the 12-epoch setting. Moreover, our method shows a re-markable improvement (+1.9 AP) over our baseline
DAB-DETR and can be easily integrated into other
DETR-like methods. 2. We analyze the slow convergence of DETR from a novel viewpoint and give a deeper understanding of
DETR training. We design a metric to evaluate the instability of bipartite matching and verify that our method can effectively lower the instability. 3. We conduct a series of ablation studies to analyze the effectiveness of different components of our model such as noise, label embedding, and attention mask. 2.