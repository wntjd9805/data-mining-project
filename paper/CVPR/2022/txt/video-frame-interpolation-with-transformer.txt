Abstract
Video frame interpolation (VFI), which aims to synthe-size intermediate frames of a video, has made remarkable progress with development of deep convolutional networks over past years. Existing methods built upon convolu-tional networks generally face challenges of handling large motion due to the locality of convolution operations. To overcome this limitation, we introduce a novel framework, which takes advantage of Transformer to model long-range pixel correlation among video frames. Further, our net-work is equipped with a novel cross-scale window-based attention mechanism, where cross-scale windows interact with each other. This design effectively enlarges the recep-tive ﬁeld and aggregates multi-scale information. Extensive quantitative and qualitative experiments demonstrate that our method achieves new state-of-the-art results on various benchmarks. 1.

Introduction
Video frame interpolation (VFI) is a fundamental video processing task in which intermediate frames are synthe-sized between given consecutive ones to increase the frame rate.
It is effective in alleviating motion blur and judder, and has become a compelling strategy for numerous appli-cations, such as novel view synthesis [15, 20], video com-pression [48], video restoration [16, 21, 49], and slow mo-tion generation [2, 19, 27, 32, 35, 38]. Many popular algo-rithms adopt optical ﬂow warping [2,3,19, 25,27, 32, 33,36, 37, 42, 50] to tackle this challenging task. Though achiev-ing remarkable performance, these methods built upon con-volutional networks generally face challenges of capturing long-range spatial interactions due to the intrinsic locality of convolution operations, thus limited in handling large mo-tion, which is one of the main challenges of VFI.
Recently, natural language processing (NLP) [4, 12, 46] and computer vision [6, 14, 26] tasks achieve notable progress using Transformers, which is a highly adaptive ar-chitecture with strong modeling capability.
In this work, we are inspired to explore the application of Transformers
Figure 1. Visual comparison on a challenging sample from Mid-dlebury dataset [1]. Our method produces the result more ap-pealing than the three leading VFI methods, i.e., AdaCoF [22],
RIFE [17] and ABME [37]. in the context of video frame interpolation and introduce a novel network, VFIformer. With the attention mechanism as the core operation, VFIformer is able to model pixel cor-respondence between different frames. Besides, its strong capability of capturing long-range dependency is helpful for handling large motion (see Fig. 1).
Since the vanilla Transformer needs high memory and computational cost, the proposed VFIformer is designed in a UNet [40] architecture where features are processed in different scales to reduce the computational complex-ity and enlarge the receptive ﬁeld. Besides, to overcome the quadratic complexity, inspired by recent work [11, 23,
26, 47], our VFIformer is built upon window-based atten-tion where feature maps are divided into non-overlapping sub-windows. Self-attention is only performed within each sub-window. Despite computationally efﬁcient, such an ap-proach prohibits information interaction between different windows and leads to limited receptive ﬁeld. We address this problem by proposing cross-scale window-based atten-tion, where the attention is computed among feature win-dows of different scales.
Our design enjoys two merits. (1) Compared with win-dows at the original scale, the corresponding windows at coarser scales cover more content. As a result, the interac-tion among these windows effectively enlarges the recep-tive ﬁeld. (2) Features at coarser scales naturally contain smaller displacement and thus provide informative motion priors for the original scale to facilitate synthesis.
Our contributions are summarized as follows:
• We propose a novel framework integrated with the
Transformer for the VFI task, which takes advantage of the Transformer to model long-range pixel correla-tions among the video frames.
• A cross-scale window-based attention mechanism is introduced to enlarge the receptive ﬁeld of current window-based attention to adapt to the challenges of large motions in the VFI task.
• Our model achieves state-of-the-art performance for the VFI task on multiple public benchmarks. 2.