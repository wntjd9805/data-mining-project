Abstract
Self- and cross-attention in Transformers provide for high model capacity, making them viable models for ob-ject detection. However, Transformers still lag in per-formance behind CNN-based detectors. This is, we be-lieve, because: (a) Cross-attention is used for both clas-sification and bounding-box regression tasks; (b) Trans-former’s decoder poorly initializes content queries; and (c)
Self-attention poorly accounts for certain prior knowledge which could help improve inductive bias. These limitations are addressed with the corresponding three contributions.
First, we propose a new Detection Split Transformer (DE-STR) that separates estimation of cross-attention into two independent branches – one tailored for classification and the other for box regression. Second, we use a mini-detector to initialize the content queries in the decoder with classi-fication and regression embeddings of the respective heads in the mini-detector. Third, we augment self-attention in the decoder to additionally account for pairs of adjacent object queries. Our experiments on the MS-COCO dataset show that DESTR outperforms DETR and its successors. 1.

Introduction
This paper addresses a basic vision problem, that of object detection in images. Recently proposed DEtec-tion TRansformer (DETR) [2] and its successors, such as, e.g., Conditional DETR (C-DETR) [19] and Anchor
DETR [25], have been demonstrated as competitive per-formers on the benchmark MS-COCO dataset [15], de-spite using a simpler backbone network with single-scale features than more complex state-of-the-art (SOTA) CNNs
[20, 22, 24, 27]. This success has been ascribed to Trans-formers’ high model capacity, since they estimate self-attention and cross-attention, thereby explicitly capturing relationships between parts and larger spatial contexts in the image. However, recent findings [3, 5] suggest that Trans-formers lack certain inductive biases possessed by CNNs to help them constrain the hypothesis space. Therefore,
Transformers require longer training (e.g., DETR needs 500 training epochs) and larger amount of training data to com-pensate.
In this work, we identify and address three key limitations of the mentioned family of detector Transform-ers, and thus improve their inductive bias.
The first limitation concerns cross-attention. DETR’s de-coder computes cross-attention between the encoder’s out-put embedding and a set of learnable object queries, for es-timating relationships between these queries and the entire image context. This cross-attention is then used for both classification and bounding-box regression of the queries.
The same holds for other Transformer detectors.
Motivated by the success of FCOS detector [24] that splits the classification and box regression heads, as our first contribution, we propose to split estimation of cross-attention into two independent branches – one for classi-fication, and the other for regression. Hence the name of our new Detection Split Transformer (DESTR). Since the branches will not share weights, each will likely focus on a different set of optimal features, as desired, rather than jointly use suboptimal features for both classification and regression. This is illustrated in Fig. 1, where we show cross-attention maps computed by DETR, C-DETR, and our DESTR. As can be seen, DETR’s cross-attention ap-pears to focus on most discriminative object parts which may not necessarily be informative for box regression. On the other hand, C-DETR’s cross-attention seems to primar-ily focus on shape cues. DESTR’s classification and re-gression cross-attention maps differ, as intended, where the former highlights more class-characteristic regions to help classification, and the latter has higher values on horizontal and vertical edges in the image to guide the box prediction, and looks more dilated to the same regions.
The second limitation concerns the poor initialization and need for long training of content queries in the decoder.
Features of the learnable object queries in the decoder con-sist of the content embedding (a.k.a. content query) and po-sitional embedding (a.k.a. spatial query). DETR learns the positional embedding so it captures a spatial distribution of objects in training images [2]. However, the content queries
Figure 1. Cross-attention maps estimated by: (a) DETR trained with 500 training epochs, and (b) C-DETR trained with 50 epochs. (c) DESTR’s classification cross-attention fo-cuses on discriminative object parts. (d) DESTR’s regression cross-attention is more di-lated to the same region comparing to (c). DESTR is trained with 50 epochs. R50 is used as backbone for all these three models. For better visualization, the figure shows square-root values of cross-attention. Warmer colors indicate higher cross-attention values.
Pair self-attention: Since
Figure 2. the remotes occur next to the cats and couch, we increase their attention by considering attention of pairs (cid:0)(cat1, re-mote1),(remote2, cat2)(cid:1), and (cid:0)(couch, re-mote1),(couch, remote2)(cid:1). are inferred for every image from scratch. This makes train-ing difficult due to the higher dependence of cross-attention on the content than spatial query [2, 19], especially in the initial stages of training when the content queries are not
“strong” enough to match well the positional embedding of the keys. To address these issues C-DETR [19]: (a) sepa-rates the content and positional dot-products when comput-ing cross-attention, thereby relaxing their inter-dependence; and (b) conditions the positional embedding of each query with the corresponding decoder output embedding of the previous stage, and thus constrains the content to focus more on discriminative regions within the previously pre-dicted bounding box of the query. We adopt these modifi-cations of C-DETR, since they enable a significant decrease of training epochs, and further extend the framework with the remaining two contributions.
As our second contribution, we propose to learn not only the positional embedding, but also the content embedding, and to do so by following our first contribution – i.e., to learn the content as the separate classification and regres-sion embeddings. To this end, we insert a mini-detector after the encoder to predict a set of initial object propos-als. Features of these object candidates, used by the mini-detector’s classification and regression heads, can be passed to the decoder and thus initialize the classification and re-gression queries, instead of inferring them from scratch.
Our grounding of object queries on initial object propos-als is expected to facilitate training. Importantly, this will also enable DESTR to consider a flexible number of object queries in both training and testing, rather than use a pre-defined fixed number as in DETR and C-DETR.
Finally, as out third contribution, we seek to incorporate certain prior knowledge in self-attention of the decoder, and in this way better constrain the hypothesis space. We ex-pect that object instances occur within similar surrounding spatial contexts, which could improve estimation of self-attention in the decoder. Therefore, instead of computing the common self-attention for every query in isolation, we compute pair self-attention for every two pairs of queries, where each pair has been predicted to be spatially adjacent by the previous decoder stage. That is, we condition our pair self-attention on the corresponding decoder output of the previous stage. The example in Fig. 2 illustrates advan-tages of the proposed strategy. One of the remote controllers is partially occluded, and, therefore, provides a low atten-tion to another fully visible remote. But since both remotes occur next to the cats and couch, we could increase their attention by considering attention of pairs (cid:0)(left cat, left re-mote), (right remote, right cat)(cid:1) and (cid:0)(couch, left remote), (couch, right remote)(cid:1). We do so without increasing com-plexity to quadratic in the number of queries.
In summary, our three main contributions include: 1. The classification and regression branches in the decoder compute separately their respective cross-attentions, instead of sharing the same cross-attention; 2. A mini-detector is inserted after the encoder for learn-ing classification, regression and positional embed-dings. Embeddings of the object queries in the decoder are initialized with the corresponding classification, re-gression and positional embeddings of object propos-als predicted by the mini-detector; 3. Pair self-attention of the queries and their adjacent spa-tial contexts is estimated in the decoder, instead of the common self-attention for every individual query.
Our experiments demonstrate that DESTR outperforms
C-DETR and other recent Transformer detectors with a sig-nificant margin on MS-COCO-val [15]. DESTR is also a competitive performer relative to recent CNN-based detec-tors; although, a direct comparison with CNNs is unfair, since they typically use multi-scale features and more com-plex backbone networks.
In the following, Sec. 2 reviews related work, Sec. 3 specifies DESTR, Sec. 4 presents our experimental evalu-ation, and Sec 5 concludes the paper. 2.