Abstract
In this work we propose Identity Consistency Trans-former, a novel face forgery detection method that focuses on high-level semantics, speciﬁcally identity information, and detecting a suspect face by ﬁnding identity inconsis-tency in inner and outer face regions. The Identity Consis-tency Transformer incorporates a consistency loss for iden-tity consistency determination. We show that Identity Con-sistency Transformer exhibits superior generalization abil-ity not only across different datasets but also across various types of image degradation forms found in real-world appli-cations including deepfake videos. The Identity Consistency
Transformer can be easily enhanced with additional identity information when such information is available, and for this reason it is especially well-suited for detecting face forg-eries involving celebrities. 1 1.

Introduction
Deepfake techniques [1–5,9,32,46,47] have been largely advanced to be able to create incredibly realistic fake im-ages of which the face is replaced with someone else in another image. The malicious usage and spread of deep-fake have raised serious societal concerns and posed an in-creasing threat to our trust in online media. Therefore, face forgery detection is in urgent need and has gained a consid-erable amount of attention recently.
Notably, the overwhelming majority among all cases of face forgeries involve politicians, celebrities and corporate leaders, as their photos and videos are easier to ﬁnd on the web and hence easily manipulated to generate impressively photo-realistic deepfake. Yet previous detection algorithms make predictions about the forgery based only on the sus-pect images, and neglect to exploit those freely available
*Work done during an internship at Microsoft Research Asia.
†Dongdong Chen is the corresponding author. 1Code will be released at https://github.com/LightDXY/
ICT_DeepFake
ICT:1.10
ICT:1.06
ICT:1.15
ICT:1.24
ICT:1.04
ICT:0.66
ICT:0.68
ICT:0.78
Figure 1. Five fake (1st row) and real (2nd row) faces and their identity consistency scores. Higher ICT score indicates the inner and outer face more likely from two people, i.e. DeepFake face.
ICT:0.70
ICT:0.68 data. In this paper, we argue that the images / videos avail-able online can not only be used in generating face forgeries but also be utilized to detect them, and try to protect people whose face is accessible online and thus vulnerable for face manipulation, i.e., celebrities in a broad sense.
Recently, numerous efforts have been devoted to detect-ing face forgery and achieve promising detection perfor-mance. Most existing methods aim to discriminate fake im-ages by exploiting low-level textures and searching for the underlying generation artifacts [7,10,33,35,38,42,44,45,49, 52, 67, 68]. While deploying these techniques in real-world (1) deep-products, we observe two common problems: fake detection is usually performed on suspected videos and the video frames have image degradations, such as image rescale, noise and video codec conversion; and (2) when the generated deepfake is convincingly photo-realistic, the low-level traces of forgeries become very hard to detect. These problems make the deepfake detection unstable with video input. We wish to make deepfake detection signiﬁcantly more robust by making heavy use of semantically meaning-ful identity information.
In this paper we propose a new face forgery detection technique called Identity Consistency Transformer (ICT) based on high-level semantics. The key idea is to de-tect identity consistency in the suspect image, i.e., whether the inner face and the outer face belongs to the same per-son. This turns out to be a non-trivial task. A na¨ıve so-(b) (a)
Figure 2. Forgery regions of current face forgery methods. (a) DeepFake in FF++. (b) DeepFake in Google Deepfake Detection. (c)
DeepFake in CelebDF. (d) DeepFake in DeepFaceLab. (e)Face2face. (f) FSGAN (g) DF-VAE. They replace the inner face region with different shapes while keep the outer face unchanged. (d) (e) (c) (g) (f) lution would be to compare identity vectors extracted us-ing the off-the-shelf face recognition model from the inner-and outer- faces, similar to the popular face veriﬁcation technique. Unfortunately existing face veriﬁcation meth-ods [22, 59] tend to characterize the most discriminative region, i.e., the inner face for veriﬁcation and fail to cap-ture the identity information in the outer face. With Identity
Consistency Transformer, we train a model to learn a pair of identity vectors, one for the inner face and the other for the outer face, by designing a Transformer such that the inner and the outer identities can be learned simultaneously in a seamlessly uniﬁed model. Our Identity Consistency Trans-former incorporates a novel consistency loss to pull the two identities together when their labels are the same and con-sequently push them away when they are associated with different labels.
Our approach only assumes the identity discrepancy and thus can be trained with a large number of swapped faces obtained from swapping different identities, without any fake images generated by face manipulation methods. Em-pirically we show that Identity Consistency Transformer exhibits signiﬁcantly superior performance in the situa-tions where low-level texture based methods fail. Figure 1 presents ﬁve examples illustrating that state-of-the-art de-tection methods [7, 35, 52] fail to detect them while our method uncovers quite different identity consistency scores and thus can differentiate real and fake images.
Another advantage of Identity Consistency Transformer is that it can be easily enhanced with additional identity in-formation when such information happens to be available, as is the case with celebrities. For celebrities, their refer-ence images are available and with the Identity Consistency
Transformer we construct an authentic reference set consist-ing of the extracted identity vector pairs of the celebrities and thus create a new consistency score to enhance iden-tity consistency detection. The resulting reference-assisted
ICT (ICT-Ref) achieves the state-of-the-art performance on conventional benchmark datasets, demonstrating its strong ability for additional protection for celebrities. Finally, we show in our experiments that the proposed ICT and ICT-Ref signiﬁcantly improve the generalization ability in two direc-tions: 1) across different datasets and 2) more importantly, across different image degradation forms in real-world ap-plications including video applications. 2.