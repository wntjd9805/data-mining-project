Abstract
The goal of video highlight detection is to select the most attractive segments from a long video to depict the most interesting parts of the video. Existing methods typically focus on modeling relationship between different video seg-ments in order to learning a model that can assign highlight scores to these segments; however, these approaches do not explicitly consider the contextual dependency within indi-vidual segments. To this end, we propose to learn pixel-level distinctions to improve the video highlight detection. This pixel-level distinction indicates whether or not each pixel in one video belongs to an interesting section. The advan-tages of modeling such fine-level distinctions are two-fold.
First, it allows us to exploit the temporal and spatial re-lations of the content in one video, since the distinction of a pixel in one frame is highly dependent on both the con-tent before this frame and the content around this pixel in this frame. Second, learning the pixel-level distinction also gives a good explanation to the video highlight task regard-ing what contents in a highlight segment will be attractive to people. We design an encoder-decoder network to esti-mate the pixel-level distinction, in which we leverage the 3D convolutional neural networks to exploit the temporal con-text information, and further take advantage of the visual saliency to model the spatial distinction. State-of-the-art performance on three public benchmarks clearly validates the effectiveness of our framework for video highlight de-tection. 1.

Introduction
Along with the explosive development of mobile de-vices, a tremendous number of videos are now produced and uploaded to the Internet every day. As a result, pick-ing the most attractive video clips from a lengthy video to create a selection of shining moments is becoming increas-ingly important, especially for social video platforms such
*Work done during an internship at Alibaba Group
†The corresponding author
Figure 1. Video highlight detection is highly context-dependent.
While previous methods are usually trained to predict the highlight score for a video segment directly, our method takes the tempo-ral and spatial information into account and predicts the fine-level pixel-level distinction as the surrogate task. as YouTube and Instagram. As a result, video highlight de-tection, which aims to select the most attractive segments from an unedited video, has drawn increasing interest from in the research community.
Most existing works [9, 12, 36] interpret the video high-light detection task as a segment-level ranking problem.
These approaches treat each segment as an individual sam-ple and extract the features for video segments. They then compare pairwise segments in order to learn a model that as-signs highlight scores to these segments, such that the high-light segments receive higher scores than the non-highlight segments from the raw video. Recently, SL [34] devel-oped a set-based mechanism that is capable of identifying whether or not a video segment is highlight by transformer.
However, these existing methods do utilize both tempo-ral and localized information, but not explicitly considering the contextual dependency within the segment, which is in fact crucial for video highlight detection.
Intuitively, when people watch videos, a specific part is
considered to be interesting, usually depends on the previ-ous parts they have watched. For example, considering a video in which a gymnast performs a somersault, the jump-ing up before the somersault and jumping down after the somersault are visually quite similar; however, people tend to rate the jumping up as more appealing than the jumping down; because the former contributes to the the climax of the somersault, while the latter decays the highlight level af-ter climax. This indicates that predicting the highlight score of one frame highly depends on the context before the cur-rent frame.
Similarly, the spatial context is also important for video highlight detection. A dog might not be interesting if it ap-pears together with a group of dogs, while it will definitely be the focus in a dog show scenario. In this case, the con-text information within one frame would be very helpful for estimating the highlight score.
Accordingly, to exploit the temporal and spatial context for video highlight detection, in this paper, we cast the video highlight detection into a new task: pixel-level distinction estimation. More specifically, rather than assigning high-light scores to video segments (as in the existing works), we aim to predict the attractiveness of each pixel in the video.
Such fine-level task offers two benefits. First, as the distinc-tion of a pixel in one frame often depends on the temporal and spatial context, predicting pixel-level distinction allows to exploit such context information in our model, leading to more robust highlight detection results. Second, learn-ing the pixel-level distinction also offers a good explanation for the video highlight task also to what content in a high-light segment might be more appealing to people, making the video highlight detection model more explainable. Af-ter estimating the pixel-level distinction, the highlight score of a video segment can be readily obtained by averaging the distinctions of all pixels in the segment.
We develop an encoder-decoder network to estimate the pixel-level distinction. This network is designed to output a distinction map for each frame in the input video. To exploit the temporal context, we employ a 3D convolutional neural networks to incorporate the frames before the current frame in order to predict the distinction map. To model the spa-tial distinction, we take advantage of the visual saliency to generate pixel-level pseudo-distinction labels for frames in the highlight segments. We demonstrate that the strategies discussed above can be simply integrated into the encoder-decoder network.
Experiments benchmarks-YouTube [24], TvSum [23] and CoSum [5]-show that our proposed approach outperforms existing methods by clear margins. We further validate the effectiveness of our proposed modules with ablation studies, and provide qualitative results to show the explainable ability of our proposed model. challenging three on
In summary, the main contributions of this paper are as follows:
• We propose a new pixel-level distinction estimation task for video highlight detection, which is able to ex-plore the fine-level context in order to predict the at-tractiveness of specific segments.
• We design an encoder-decoder network for estimating the pixel-level distinction, which takes advantage of the 3D convolutional neural networks and the visual saliency map to exploit the temporal and spatial con-text, respectively.
• We achieve new state-of-the-art performance on three public benchmarks. Moreover, our model also exhibits good explainable ability, and is able to directly out-put the most appealing regions in the highlighted video segments. 2.