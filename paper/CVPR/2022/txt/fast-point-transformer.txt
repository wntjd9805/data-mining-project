Abstract
The recent success of neural networks enables a better in-terpretation of 3D point clouds, but processing a large-scale 3D scene remains a challenging problem. Most current approaches divide a large-scale scene into small regions and combine the local predictions together. However, this scheme inevitably involves additional stages for pre- and post-processing and may also degrade the final output due to predictions in a local perspective. This paper introduces
Fast Point Transformer that consists of a new lightweight self-attention layer. Our approach encodes continuous 3D coordinates, and the voxel hashing-based architecture boosts computational efficiency. The proposed method is demon-strated with 3D semantic segmentation and 3D detection.
The accuracy of our approach is competitive to the best voxel-based method, and our network achieves 129 times faster inference time than the state-of-the-art, Point Trans-former, with a reasonable accuracy trade-off in 3D semantic segmentation on S3DIS dataset. 1.

Introduction 3D scene understanding is a fundamental task due to its importance to various fields, such as robotics, intelligent agents, and AR/VR. Recent approaches [6, 10, 22, 26, 27, 34, 37] utilize the deep learning frameworks, but processing a large-scale 3D scene as a whole remains a challenging prob-lem because it involves extensive computation and memory budgets. As an alternative, some methods crop 3D scenes and stitch predictions [18, 26, 27, 34, 35, 41], or others ap-proximate point coordinates for efficiency [6, 10, 23, 50].
Such techniques, however, typically lead to a substantial in-crease of inference time and/or degrade the final output due to the local or approximate predictions. Achieving both fast inference time and high accuracy is thus one of the primary challenges in the 3D scene understanding tasks.
The pioneering 3D understanding approaches, Point-[26] and PointNet++ [27] process point clouds
Net with multi-layer perceptrons (MLPs), which preserve permutation-invariance of the point clouds. Such point-based methods introduce impressive results [22, 37] recently, and Point Transformer [49] shows superior accuracy based on the local self-attention mechanism. However, it involves manual grouping of point clouds using k nearest neighbor search. Furthermore, scene-level inference with the point-based methods typically requires dividing a large-scale scene into smaller regions and stitching the predictions on them.
While Voxel-based methods [1, 6, 10, 13, 19, 23, 24, 36, 50] are alternatives for a large-scale 3D scene understanding due
to their effectiveness of the network design, they may lose fine geometric patterns due to quantization artifacts. Hybrid methods [21, 33, 34] reduce the quantization artifacts by utilizing both point-level and voxel-level features. However, approaches in this category require additional memory space to cache both features.
We propose Fast Point Transformer, which effectively en-codes continuous positional information of large-scale point clouds. Our approach leverages local self-attention [29, 38] of point clouds with voxel hashing architecture. To achieve higher accuracy, we present centroid-aware voxelization and devoxelization techniques that preserve the embedding of continuous coordinates. The proposed approach reduces quantization artifacts and allows the coherency of dense predictions regardless of rigid transformations. We also in-troduce a reformulation of the standard local self-attention equation to reduce space complexity further. The proposed local self-attention module can replace the convolutional lay-ers for 3D scene understanding. Based on this, we introduce a local self-attention based U-shaped network, which natu-rally builds a feature hierarchy without manual grouping of point clouds. As the result, Fast Point Transformer collects rich geometric representations and exhibits a fast inference time even for large-scale scenes.
We conduct experiments using two datasets of large-scale scenes: S3DIS [2] and ScanNet [7]. Our method shows competitive accuracy in the semantic segmentation task on various voxel hashing configurations. We also apply the Fast
Point Transformer network as a backbone of VoteNet [25] to show the applicability in the 3D object detection task. We use ScanNet [7] dataset for the 3D detection, and our model shows better accuracy (mAP) than other baselines that use point- or voxel-based network backbones. Besides, we intro-duce a novel consistency score metric, named CScore, and demonstrate that our model outputs more coherent predic-tions under rigid transformations.
In summary, our contributions are as follows: 1. We propose a novel local self-attention-based network, called Fast Point Transformer that can handle large-scale 3D scenes quickly. 2. We introduce a lightweight local self-attention module that effectively learns continuous positional information of 3D point clouds while reducing space complexity. 3. We show that our model produces significantly more coherent predictions than the previous voxel-based ap-proaches using the proposed evaluation metric. 4. We demonstrate fast inference of our voxel-hashing-based architecture; our network performs a 129 times faster inference than Point Transformer does, obtaining a reasonable accuracy trade-off in 3D semantic segmen-tation on S3DIS dataset [2]. 2.