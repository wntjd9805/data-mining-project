Abstract
The backbone of most deep-learning-based continuous sign language recognition (CSLR) models consists of a vi-sual module, a sequential module, and an alignment mod-ule. However, such CSLR backbones are hard to be trained sufficiently with a single connectionist temporal classifica-tion loss. In this work, we propose two auxiliary constraints to enhance the CSLR backbones from the perspective of consistency. The first constraint aims to enhance the vi-sual module, which easily suffers from the insufficient train-ing problem. Specifically, since sign languages convey in-formation mainly with signers’ faces and hands, we insert a keypoint-guided spatial attention module into the visual module to enforce it to focus on informative regions, i.e., spatial attention consistency. Nevertheless, only enhanc-ing the visual module may not fully exploit the power of the backbone. Motivated by that both the output features of the visual and sequential modules represent the same sentence, we further impose a sentence embedding consistency con-straint between them to enhance the representation power of both the features. Experimental results over three rep-resentative backbones validate the effectiveness of the two constraints. More remarkably, with a transformer-based backbone, our model achieves state-of-the-art or compet-itive performance on three benchmarks, PHOENIX-2014,
PHOENIX-2014-T, and CSL. 1.

Introduction
Hearing-impaired people usually use sign languages as their communication method. Video-based continuous sign language recognition (CSLR) aims to transcribe a sign lan-guage video into a sequence of glosses (basic lexical units in a sign language).
In recent years, deep learning tech-niques dominate CSLR modeling because of their superior-ity over traditional methods [31, 32, 55]. According to [32], the backbone of most deep-learning-based CSLR models consists of three components: a visual module, a sequen-tial (contextual) module, and an alignment module. Within
Figure 1. An overview of the CSLR backbone and the proposed consistency constraints. First, our SAC constraint leverages pose keypoints heatmaps to enforce the visual module to focus on in-formative regions. Second, our SEC constraint aligns the visual and sequential features at the sentence level, which can enhance the representation power of both the features simultaneously. this framework, the visual module first extracts visual fea-tures from input videos. Then the sequential module ex-tracts sequential and contextual information from the visual features. Finally, the alignment module aligns the sequen-tial features with the gloss label sequence and computes its probability.
As a common practice, the connectionist temporal clas-sification (CTC) [14] loss is adopted as the main objec-tive function to train such CSLR backbones. However, only using the CTC loss may lead to the insufficient train-ing problem that the extracted features are not represen-tative enough to be used to yield accurate recognition re-sults [10, 11, 17, 31, 37, 39, 55]. Two kinds of methods can relieve this issue. First, [11, 17, 37–39, 55] use a stage opti-mization strategy to iteratively refine the extracted features, which is time-consuming since the model needs to adapt to a different objective in a new stage [10]. As an alterna-tive solution, auxiliary learning can keep the whole model end-to-end trainable by just adding one or more auxiliary tasks [10, 31]. In this work, we propose two novel auxiliary constraints from the perspective of information consistency.
Our first constraint aims to enhance the visual module, which plays a key role in feature extraction but easily suffers from the insufficient training problem [11,31,55]. Since the information of sign languages is mainly included in signers’
faces and hands [23, 55], to enrich the visual features, some
CSLR models [36,55,56] leverage an off-the-shelf pose de-tector [7, 43] to locate the face and hands and then crop the feature maps to form a multi-stream architecture. However, the multi-stream architecture will introduce many more pa-rameters and the cropping operation cannot fully exploit the rich information contained in the pose keypoints heatmaps.
As shown in Figure 1, we find that the heatmaps can re-flect the importance of different spatial positions, which is quite similar to the attention mechanism. Thus, as shown in Figure 2, we insert a lightweight spatial attention module guided by keypoints heatmaps into the visual module to en-force it to focus on informative regions, which leads to our spatial attention consistency (SAC) constraint.
Only enhancing the visual module may not fully exploit the power of the backbone. Some works [17, 31] show that explicitly enforcing the consistency between the visual and sequential modules can strengthen their cooperation, and give better performance. VAC [31] treats the visual and se-quential modules as the student and teacher, respectively, and achieves knowledge distillation between them. Simi-larly, SMKD [17] achieves knowledge transfer by sharing classifiers. Knowledge distillation can be seen as a kind of consistency since the KL-divergence loss used in [31] is a measurement of the distance between two probability dis-tributions. However, the above two methods share the same deficiency that the consistency is measured at the frame level, i.e., the probability distribution is computed for each frame independently. We think that there should be differ-ences between the distributions of the visual and sequential modules at the frame level since the sequential module gath-ers contextual information for each frame; otherwise, the sequential module may be removed. Motivated by that both the visual and sequential features represent the same sen-tence, we impose a sentence embedding consistency (SEC) constraint between them. As shown in Figure 2, we build a sentence embedding extractor that can be co-trained with the CSLR backbone, and then minimize the distance be-tween the sentence embeddings of visual and sequential fea-tures but maximize the distance between the sentence em-beddings of visual and negative sequential features.
In summary, our main contributions are:
• We propose a spatial attention consistency constraint, which can enhance the visual module by leveraging pose keypoints heatmaps to guide an inner spatial at-tention module.
• We propose a sentence embedding consistency con-straint, which can align the visual and sequential fea-tures at the sentence level and enhance the representa-tion power of both the features simultaneously. three representative CSLR back-performance of bones with negligible extra cost. More remarkably, with a transformer-based backbone, our consistency-enhanced CSLR (C2SLR) model can achieve state-of-the-art (SOTA) or competitive performance on three benchmarks, while the whole model is trained in an end-to-end manner. 2.