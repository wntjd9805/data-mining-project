Abstract
Estimating the articulated 3D hand-object pose from a single RGB image is a highly ambiguous and challeng-ing problem, requiring large-scale datasets that contain diverse hand poses, object types, and camera viewpoints.
Most real-world datasets lack these diversities.
In con-trast, data synthesis can easily ensure those diversities separately. However, constructing both valid and diverse hand-object interactions and efficiently learning from the vast synthetic data is still challenging. To address the above issues, we propose ArtiBoost, a lightweight online data enhancement method. ArtiBoost can cover diverse hand-object poses and camera viewpoints through sam-pling in a Composited hand-object Configuration and View-point space (CCV-space) and can adaptively enrich the current hard-discernable items by loss-feedback and sam-ple re-weighting. ArtiBoost alternatively performs data exploration and synthesis within a learning pipeline, and those synthetic data are blended into real-world source data for training. We apply ArtiBoost on a simple learn-ing baseline network and witness the performance boost on several hand-object benchmarks. Our models and code are available at https://github.com/lixiny/
ArtiBoost. 1.

Introduction
Articulated bodies, such as the human hand, body, and linkage mechanism, can be observed every day in our life.
Their joints, links, and movable parts depict the functional-ity of the articulation body. Extracting their transient con-figuration from image or video sequence, which is often re-ferred to as Pose Estimation [8, 36, 37], can benefit many downstream tasks in robotics and augment reality. Pose es-timation for multi-body articulations is especially challeng-ing as it suffers from severe self- or mutual occlusion. In
⋆The first two authors contribute equally.
†Cewu Lu is the corresponding author. He is the member of Qing Yuan
Research Institute and MoE Key Lab of Artificial Intelligence, AI Institute,
Shanghai Jiao Tong University, and Shanghai Qi Zhi Institute, China.
Figure 1. An intuitive illustration of the CCV-space. this work, we paid attention to a certain type of multi-body articulations – composited hand and object poses during their interaction [5, 11, 18, 20, 23–25, 29, 31, 42, 68]. Hands are the primary means by which humans manipulate ob-jects in the real-world, and the hand-object pose estimation (HOPE) task holds great potential for understanding human behavior [13, 17, 35, 45, 60].
As the degrees of freedom (DoF) grows, the proper amount of data to cover the pose distribution has grown ex-ponentially. More than the most common articulation bod-ies, the human hand has 16 joints and approximately 21
DoF. Preparing such diverse training data for the HOPE task can be very challenging. The real-world recording and an-notation methods [3, 6, 13, 20] tend to hinder the pose diver-sity. For example, the multi-view-based approaches [3, 20] require the subject to maintain a static grasping pose in a video sequence. As a result, their recording process is in-efficient, and their pose diversity is insufficient.
In con-trast, data synthesis is efficient and annotation-free, and has been widely adopted in single-body N-D pose estima-tion [9, 14, 41, 62, 64, 70]. However, these methods are in-eligible for multi-body articulation, in which the poses are restricted by mutual contact and obstruction. Data synthesis for HOPE tasks requires us to simulate virtual hand-object interaction (grasp) that mimics the underlying pose distribu-tion of their real-world counterparts. Conventional method either manually articulated [10, 50, 51] the hand model for grasp, or relegated [2, 25, 33] grasp synthesis to an off-the-shelf grasping simulator: GraspIt [47]. However, the man-ual methods are difficult to scale their data in large amounts, and the simulation method also sacrificed the diversity of hand poses. GraspIt optimizes for hand-crafted grasp met-rics [12] that do not reflect the pose distribution of a 21-DoF dexterous hand. Besides, even with a vast amount of syn-thetic grasps, not every hand-object configuration is help-ful for training. For example, similar configurations may have already been observed multiple times, and those eas-ily discernable samples may have a frequent appearance.
Hence, offline data synthesis, without repeatedly commu-nicating with the model during training, is still considered inefficient for a learning task.
To address the above issues, we propose an online data enhancement method ArtiBoost, to effectively boost the articulated hand-object pose estimation via two alternative steps, namely exploration and synthesis. First, to describe the observation of hand-object interaction, we design a three-dimensional discrete space: Composited hand-object
Configuration and Viewpoint space (CCV-space) where ob-ject types, hand pose, and viewpoint are its components.
Second, to construct valid and diverse hand-object poses in CCV-space, we design a fitting-based grasp synthesis method that exploits the contact constraints [68] between hand and object vertices to simulate MANO hand [52] grasping a given object. After the CCV-space is established, we next describe how ArtiBoost enhances the HOPE tasks.
At the exploration step, ArtiBoost explores the CCV-space and samples different hand-object-viewpoint triplets from it. Then at the synthesis step, the hand and object in the triplet will be rendered on the image from the view-point in the triplet. These synthetic images are mixed with the real-world source images in batches to train the HOPE model. Later, the training losses are fed back to the ex-ploration step and guide it to re-weight the current hard-discernable triplets for the next round of sampling. With such communication in the training loop, ArtiBoost can adaptively adjust its sampling weights to select more hard-discernable data for the current HOPE model. As the HOPE model becomes powerful, it can also continuously pro-mote the current ArtiBoost to evolve. ArtiBoost is model-agnostic, which means it can be plugged into any modern
CNN architecture. In this paper, we plug ArtiBoost into a simple classification-based (e.g. [56]) and regression-based (e.g. [1]) pose estimation model to show its efficacy. For evaluation, we report those models’ performance on two challenging HOPE benchmarks: HO3D (v1-v3) [19–21] and DexYCB [6]. Without whistles and bells, those sim-ple baseline models can outperform the results of previous state-of-the-arts.
In this paper, we propose to boost the performance of
HOPE task by enhancing the diversity of underlying poses distribution in the training data. We summarize our contri-butions as follows. (1) To describe the composited hand-object-viewpoint poses distribution, we design the CCV-space. (2) To overcome the scarcity of such poses in the previous dataset, we design a contact-guided grasp synthe-sis method and simulate both valid and diverse hand-object poses to fill the CCV-space. (3) To help HOPE model effi-ciently fit the underlying poses distribution, we parallelize the data synthesis with the learning pipeline, leverage the training feedback, and adopt a sample re-weighting strat-egy. Finally, We conduct extensive experiments to validate our technical contributions (Sec. 4.2, 4.3) and reveal the po-tential applications (Sec. 4.4). 2.