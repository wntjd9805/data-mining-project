Abstract
While local-window self-attention performs notably in vision tasks, it suffers from limited receptive field and weak modeling capability issues. This is mainly because it per-forms self-attention within non-overlapped windows and shares weights on the channel dimension. We propose Mix-Former to find a solution. First, we combine local-window self-attention with depth-wise convolution in a parallel de-sign, modeling cross-window connections to enlarge the re-ceptive fields. Second, we propose bi-directional interac-tions across branches to provide complementary clues in the channel and spatial dimensions. These two designs are integrated to achieve efficient feature mixing among win-dows and dimensions. Our MixFormer provides compet-itive results on image classification with EfficientNet and shows better results than RegNet and Swin Transformer.
Performance in downstream tasks outperforms its alterna-tives by significant margins with less computational costs in 5 dense prediction tasks on MS COCO, ADE20k, and
LVIS. Code is available at https://github.com/
PaddlePaddle/PaddleClas. 1.

Introduction
The success of Vision Transformer (ViT) [10, 36] in im-age classification [8] validates the potential to apply Trans-former [38] to vision tasks. Challenges remain for down-stream tasks, especially the inefficiency in high-resolution vision tasks and the ineffectiveness in capturing local rela-tions. One possible solution is to use local-window self-attention. It performs self-attention within non-overlapped windows and shares weights on the channel dimension. Al-though this process improves efficiency, it poses the issues of limited receptive field and weak modeling capability.
*Equal Contribution.
†Corresponding author.
Figure 1. The Mixing Block. We combine local-window self-attention with depth-wise convolution in a parallel design. The captured relations within and across windows in parallel branches are concatenated and sent to the Feed-Forward Network (FFN) for output features.
In the figure, the blue arrows marked with
Channel Interaction and Spatial Interaction are the proposed bi-directional interactions, which provide complementary clues for better representation learning in both branches. Other details in the block, such as module design, normalization layers, and short-cuts, are omitted for a neat presentation.
A common approach to expand receptive field is to cre-ate cross-window connections. Windows are connected by shifting [30], expanding [37, 49], or shuffling [22] opera-tions. Convolution layers are also employed as they capture natural local relations. Researches [22, 53] combine local-window self-attention with depth-wise convolution base on this and provide promising results. Still, the operations cap-ture intra-window and cross-window relations in successive steps, leaving these two types of relations less interweaved.
Besides, neglect of modeling weakness in these attempts hinders further advances in feature representation learning.
We propose Mixing Block to address both these is-sues (Figure 1). First, we combine local-window self-attention with depth-wise convolution, but in a parallel way. The parallel design enlarges the receptive fields by modeling intra-window and cross-window relations simul-taneously. Second, we introduce bi-directional interactions across branches(illustrated as blue arrows in Figure 1). The interactions offset the limits caused by the weight sharing mechanism1, and enhance the modeling ability in channel and spatial dimensions by providing complementary clues for local-window self-attention and depth-wise convolution respectively. The above designs are integrated to achieve complementary feature mixing across windows and dimen-sions.
We present MixFormer to verify the block’s efficiency and effectiveness. A series of MixFormers with com-putational complexity ranging from 0.7G (B1) to 3.6G (B4) are built to perform distinguished in multiple vi-sion tasks, including image classification, object detection, instance segmentation, semantic segmentation, etc. On
ImageNet-1K [8], we achieve competitive results with Ef-ficientNet [35], surpassing RegNet [32] and Swin Trans-former [30] by a large margin. MixFormer markedly out-performs its alternatives in 5 dense prediction tasks with lower computational costs. With Mask R-CNN [16](1×) on MS COCO [29], MixFormer-B4 shows a boost of 2.9 box mAP and 2.1 mask mAP on Swin-T [30] while requir-ing less computational cost. Substituting the backbone in
UperNet [46], MixFormer-B4 delivers a 2.2 mIoU gain over
Swin-T [30] on ADE20k [55]. Plus, MixFormer is effective in keypoint detection [29] and long-tail instance segmenta-tion [13]. In brief, our MixFormer achieves state-of-the-art performance on multiple vision tasks as an efficient general-purpose vision transformer. 2.