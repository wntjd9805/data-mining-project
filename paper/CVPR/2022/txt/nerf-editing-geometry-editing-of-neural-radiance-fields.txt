Abstract
Implicit neural rendering, especially Neural Radiance
Field (NeRF), has shown great potential in novel view syn-thesis of a scene. However, current NeRF-based methods cannot enable users to perform user-controlled shape de-formation in the scene. While existing works have pro-posed some approaches to modify the radiance ﬁeld ac-cording to the user’s constraints, the modiﬁcation is limited to color editing or object translation and rotation. In this paper, we propose a method that allows users to perform controllable shape deformation on the implicit representa-tion of the scene, and synthesizes the novel view images of the edited scene without re-training the network. Speciﬁ-cally, we establish a correspondence between the extracted explicit mesh representation and the implicit neural repre-sentation of the target scene. Users can ﬁrst utilize well-developed mesh-based deformation methods to deform the mesh representation of the scene. Our method then utilizes user edits from the mesh representation to bend the camera rays by introducing a tetrahedra mesh as a proxy, obtaining the rendering results of the edited scene. Extensive exper-iments demonstrate that our framework can achieve ideal editing results not only on synthetic data, but also on real scenes captured by users. 1.

Introduction
Novel view synthesis has been extensively studied in computer vision and computer graphics. In particular, the recently proposed neural radiance ﬁeld (NeRF) [43] has in-spired a large number of follow-up works aiming to achieve better visual effects [36], faster rendering speed [18, 77],
†: Authors contributed equally
*Corresponding Author is Lin Gao (gaolin@ict.ac.cn) generalization to different scenes [78], relighting [4,60], ap-plying to dynamic scenes [48], and reducing the number of inputs [29]. However, as an implicit modeling method, the neural radiance ﬁeld is difﬁcult for users to edit or modify the scene objects, which is relatively easy with the explicit representation. The mesh representation, as a kind of ex-plicit representation, is commonly used in shape modeling and rendering. There is a lot of research work on mesh de-formation or editing [80]. However, it is difﬁcult to obtain an accurate explicit representation of a real-world scene.
From a sparse set of images, one can use some Multi-View
Stereo (MVS) method [52] to reconstruct the point cloud or mesh representation of the scene, but the quality is gener-ally poor. Rendering the reconstructed representation un-der novel views will lead to unrealistic results. Therefore, based on the promising novel view synthesis ability of im-plicit representations, such as NeRFs, further studying how to edit the implicit representation has become a new explo-ration direction.
Some works have already studied how to edit NeRF. For example, EditingNeRF [38] was the ﬁrst to propose editing on the implicit radiance ﬁeld. They train on a set of syn-thetic models from the same category, such as chairs and tables from ShapeNet [5], and introduce shape code and color code to represent the geometry and appearance of dif-ferent models, respectively. The user selects a desired color and draw a few coarse scribbles on an image of a speciﬁed view to indicate what should be changed. Then local edits are propagated to 3D regions through updating the network based on the loss between the original image and the edited image. This work is limited to color modiﬁcation or the re-moval of certain parts of the shape, and it is impossible to make substantial modiﬁcations to the shape, such as shape deformation. A recent work, ObjectNeRF [74], proposed to learn a decompositional neural radiance ﬁeld, which sepa-rates the objects and the background. As such, it can dupli-Figure 1. We propose a method to edit a static neural radiance ﬁeld (NeRF). Users only need to capture multi-view images to build a
NeRF representation, and then they can explicitly and intuitively edit the implicit representation of the scene. Our method can perform user-controlled shape deformation on the geometry of the scene, which contains multiple objects. cate, move or rotate the objects for editable scene render-ing. However, it does not support shape deformation either.
Meanwhile, some works [48, 67] consider using NeRF to model dynamic scenes and using Multi-Layer Perceptron (MLP) to predict scene changes. However, they either limit the edits to human bodies [46, 82], or can only learn motion information from the recorded videos, and cannot perform active editing [48].
In this paper, we propose a method for editing neural radiance ﬁeld that combines the advantage of explicit rep-resentations for easy local editing and the advantage of im-plicit representations for realistic rendering effects. Differ-ent from the previous work [38, 74], we focus on the geo-metric content of the scene, as shown in Fig. 1, supporting users to edit the scene geometry, and can perform photo-realistic rendering from novel views. As far as we know, we are the ﬁrst to perform user-controlled shape deforma-tion on the NeRF of general scenes. To this end, we ﬁrst extract an explicit triangular mesh representation from the trained NeRF. The explicit mesh representation is then in-tuitively deformed by the user. Next, a tetrahedral mesh is built from the triangular mesh representation, which wraps around the triangular mesh. We use the deformation of the triangular mesh to drive the deformation of the tetrahedral mesh, which propagates the deformation of the scene geo-metric surface to the spatial discrete deformation ﬁeld. Fi-nally, we use tetrahedral vertex interpolation to complete the propagation from the discrete deformation ﬁeld to the continuous deformation ﬁeld. The rays passing through the tetrahedral mesh will be bent accordingly following the con-tinuous deformation ﬁeld, so that the ﬁnal rendering result conforms to the user’s edits. Our method is general, not limited to speciﬁc shapes such as human bodies, and appli-cable to arbitrary shapes such as animal models and general man-made objects. 2.