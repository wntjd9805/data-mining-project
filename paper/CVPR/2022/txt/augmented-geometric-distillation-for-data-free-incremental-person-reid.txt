Abstract
Incremental learning (IL) remains an open issue for Per-son Re-identification (ReID), where a ReID system is ex-pected to preserve preceding knowledge while learning in-crementally. However, due to the strict privacy licenses and the open-set retrieval setting, it is intractable to adapt ex-isting class IL methods to ReID. In this work, we propose an Augmented Geometric Distillation (AGD) framework to tackle these issues. First, a general data-free incremental framework with dreaming memory is constructed to avoid privacy disclosure. On this basis, we reveal a “noisy distil-lation” problem stemming from the noise in dreaming mem-ory, and further propose to augment distillation in a pair-wise and cross-wise pattern over different views of mem-ory to mitigate it. Second, for the open-set retrieval prop-erty, we propose to maintain feature space structure during evolving via a novel geometric way and preserve relation-ships between exemplars when representations drift. Exten-sive experiments demonstrate the superiority of our AGD to baseline with a margin of 6.0% mAP / 7.9% R@1 and it could be generalized to class IL. Code is available here†. 1.

Introduction
Person re-identification (ReID) aims at identifying all images of the same person as the query from a gallery set of large scale. Training on a certain dataset empirically em-powers a ReID system to expert in the corresponding do-main. However, it inhibits the ReID system from adapting to the ever-changing environment, especially when dealing with the streamed data or a sequence of ReID tasks from incremental domains. We expect the system can widen its generalization in incremental domain and retain its capabil-ity in base domain simultaneously, which is, briefly, to ac-cumulate new knowledge while avoiding Catastrophic For-getting [12, 29]. To overcome such similar limitation, Class
Incremental Learning (CIL) [5,10,18,24,33,44] is proposed
∗Corresponding author
†https://github.com/eddielyc/Augmented-Geometric-Distillation
Figure 1. Illustration of data-free IL-ReID framework. The model keeps evolving when training on a sequence of ReID tasks. Eval-uation is adopted in all seen domains. Replaying is in a data-free setting [37, 47] due to privacy issues in ReID, where no preceding real data is stored, instead, dreaming memory drives relaying. in classification task and efforts have been devoted to figur-ing out how to learn incrementally.
Despite the great success in CIL, it still faces challenges when directly adopted to a ReID system due to the strict pri-vacy issues and the open-set retrieval setting. First, in CIL, reminding the networks of previous knowledge via replay-ing pre-stored exemplars is well-recognized [5, 19, 33] to alleviate catastrophic forgetting. However, replaying mem-ory of real data faces risks of violating privacy licenses in
ReID. Second, on one side, ReID is substantially an open-set retrieval task, which puts more attention on construct-ing a robust feature space when compared with the close-set classification, since not only representations but also their neighborhoods play key roles in retrieval ranking. On the other side, feeding new knowledge sequentially will in-evitably cause semantic drift [49] and distort the preceding feature space, resulting in forgetting. Hence, there exists a critical yet ignored contradiction between stabilizing the feature space for preceding domains and adapting feature space for the incremental domain.
Considering the limitations aforementioned, we conduct further research on Incremental ReID (IL-ReID) [32] and propose a novel Augmented Geometric Distillation (AGD) framework which consists of Augmented Distillation (AD) and Geometric Distillation (GD). First, to tackle the pri-vacy issue, we first construct a general data-free incre-mental framework for IL-ReID (overview in Fig. 1), in which dreaming memory, generated by DeepInversion [47], drives replaying procedure without access to preceding real data. Unfortunately, due to the poor quality, directly replay-ing these dreaming exemplars will induce a phenomenon termed “noisy distillation”, during which, noisy knowledge will be transferred into the evolving model and aggravate forgetting. To alleviate this problem, we further propose to augment distillation itself. Enlightened by contrastive learning, we produce different views of memory and dis-till in a pair-wise and cross-wise pattern to strengthen the robustness and reduce the perturbation.
Second, to handle the contradiction caused by open-set retrieval property, we propose the geometric distillation (GD) tailored-made for retrieval task that our intuition is to maintain the structure of the preceding feature space while drifting instead of to stabilize the whole space and to pe-nalize drift. The structure of preceding space is formulated with exemplars in dreaming memory. To prevent exemplars from drifting in their own manners arbitrarily and ”roiling” the space structure, we encourages exemplars to drift in a consistent manner, so that the structure could be maintained via similarity criterion in a novel geometric way. This al-lows to adapt the feature space for new knowledge while preserving rich preceding information for retrieval, offering a compromise between learning and memorizing.
To conclude, our contributions could be summarized as: i) We construct a data-free incremental framework for
It serves without pri-ReID with dreaming memory. vacy issues; ii) We propose Augmented Distillation (AD), where dis-tillation is conducted in a pair-wise and cross-wise pat-tern to address the “noisy distillation” phenomenon in dreaming memory; iii) We propose Geometric Distillation (GD) to adapt new and preceding knowledge for retrieval tasks via main-taining space structure geometrically when drifting; iv) We adapt mainstream solutions in CIL to ReID. Exten-sive experiments indicate that our AGD is superior to baseline with a margin of 6.0% mAP / 7.9% R@1 and it is promising to be generalized to CIL. 2.