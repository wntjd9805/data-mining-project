Abstract
Temporal action detection (TAD) is an important yet challenging task in video understanding. It aims to simulta-neously predict the semantic label and the temporal inter-val of every action instance in an untrimmed video. Rather than end-to-end learning, most existing methods adopt a head-only learning paradigm, where the video encoder is pre-trained for action classification, and only the detec-tion head upon the encoder is optimized for TAD. The ef-fect of end-to-end learning is not systematically evaluated.
Besides, there lacks an in-depth study on the efficiency-accuracy trade-off in end-to-end TAD. In this paper, we present an empirical study of end-to-end temporal action detection. We validate the advantage of end-to-end learn-ing over head-only learning and observe up to 11% perfor-mance improvement. Besides, we study the effects of mul-tiple design choices that affect the TAD performance and speed, including detection head, video encoder, and reso-lution of input videos. Based on the findings, we build a mid-resolution baseline detector, which achieves the state-of-the-art performance of end-to-end methods while run-ning more than 4× faster. We hope that this paper can serve as a guide for end-to-end learning and inspire future research in this field. Code and models are available at https://github.com/xlliu7/E2E-TAD. 1.

Introduction
With the development of information technology, the numbers of videos generated and accessed are rapidly in-creasing, underscoring the need for automatic video under-standing, such as human action recognition and temporal action detection (TAD)1. Action recognition aims to predict the action label ( e.g., basketball dunk) of a short, trimmed video. Differently, TAD aims to determine the label, as well as the temporal interval of every action instance in a long untrimmed video. It is more challenging and also practical
*Corresponding author 1Also known as temporal action localization (TAL).
Figure 1. Head-only learning (a) vs. end-to-end learning (b) for temporal action detection. Solid arrows and dashed arrows repre-sent forward pass and the gradient flow of back propagation.
Method
E2E
Flow FLOPs
Latency mAP
THUMOS14
MUSES [25]
AFSD [18]
Ours
AFSD [18]
Ours
✓
✓
✓
✓
✓
✓ 17.5T 2780G 475G 72s*+2.1s 2472ms 587ms 53.4 52.0 54.2
ActivityNet
✓ 499G 62G 291ms 63ms 34.39 35.10
Table 1. Comparison between the baseline detector built in this work (ours) with state-of-the-art methods. The latency and FLOPs are measured at the video level. The time of optical flow extrac-tion is not included in latency. *The time cost of I3D [6] feature extraction. E2E: end-to-end. in real-world actions, such as security surveillance, sports analysis, and smart video editing.
Owing to the strong discriminative power of neural net-works, deep learning methods have dominated the field of temporal action detection [20, 48, 49, 53]. As depicted in Fig. 1, a temporal action detector typically consists of a video encoder and a detection head, similar to the
backbone-head structure in object detection [12, 33, 39].
Different from modern object detectors that are trained end-to-end2, most TAD methods adopt a head-only learning paradigm. They first pre-train the video encoder on a large action recognition dataset (e.g., Kinetics [6]) then freeze it for offline feature extraction. After that, only the detection head upon the features is trained for the TAD task on the target datasets. This leaves the video features sub-optimal and restricts the performance.
Although a few works [18, 28, 45] have adopted end-to-end learning, there lacks an in-depth analysis of it. The ac-tual benefit of end-to-end learning is still unclear. Besides, the effects of many factors in end-to-end TAD, such as the video encoder, the detection head, the image and temporal resolution of input videos, are not systematically studied.
In a way, lack of such a study blocks the research of end-to-end TAD. Moreover, existing works more or less neglect the efficiency, which is an important factor in real-world applications. For example, in large-scale systems, such as online video platforms, running time determines computa-tional expenses. Unfortunately, most methods do not dis-cuss the computation cost. A few works discuss the running time of certain parts of the full model, e.g., the detection head [20,26,30,54] or report inference speed (FPS) [18,45].
But they do not explore the efficiency-accuracy trade-off.
This paper aims to address the above issues.
We conduct an empirical study of end-to-end temporal action detection. Four video encoders and three detection heads with different high-level designs are evaluated on two standard TAD datasets, i.e., THUMOS14 and Activi-tyNet. Firstly, we uncover the benefit of end-to-end learn-ing. It is shown that end-to-end trained video encoders with a medium image resolution (962) can match or surpass pre-trained ones with standard image resolution (2242) in terms of TAD performance. Secondly, we evaluate the effect of a series of design choices on performance and efficiency, in-cluding detection head, video encoder, image resolution and temporal resolution. It may serve as a guide for seeking the efficiency-accuracy trade-off. Lastly, we build a baseline detector based on our study. It achieves state-of-the-art per-formance of end-to-end TAD while running more than 4× faster (see Tab. 1). Specifically, it can process a 4-minute video in only 0.6 seconds. We hope that our work can facil-itate future research in temporal action detection. 2.