Abstract
We introduce the first end-to-end learning-based solu-tion to near-field Photometric Stereo (PS), where the light sources are close to the object of interest. This setup is es-pecially useful for reconstructing large immobile objects.
Our method is fast, producing a mesh from 52 512×384 resolution images in about 1 second on a commodity GPU, thus potentially unlocking several AR/VR applications. Ex-isting approaches rely on optimization coupled with a far-field PS network operating on pixels or small patches. Us-ing optimization makes these approaches slow and memory intensive (requiring 17GB GPU and 27GB of CPU memory) while using only pixels or patches makes them highly sus-ceptible to noise and calibration errors. To address these issues, we develop a recursive multi-resolution scheme to estimate surface normal and depth maps of the whole im-age at each step. The predicted depth map at each scale is then used to estimate ‘per-pixel lighting’ for the next scale.
This design makes our approach almost 45× faster and 2◦ more accurate (11.3◦ vs. 13.3◦ Mean Angular Error) than the state-of-the-art near-field PS reconstruction technique, which uses iterative optimization. 1.

Introduction
In this work, we introduce a fast light-weight Photomet-ric Stereo (PS) technique for near-field illumination. Pho-tometric Stereo aims to reconstruct object geometry from a sequence of images captured with a static camera and varying light sources. Existing near-field PS approaches are slow and extremely memory intensive. Being fast and light-weight enables users to capture images and process them on their laptop within a few seconds, allowing multiple retakes if needed. This light-weight reconstruction technique can be extremely useful for several AR/VR applications. While our method is primarily developed for calibrated lighting, in line with existing far-field approaches, we also show how our method can be extended to uncalibrated real-world cap-tures by introducing a calibration network.
Near-field PS is often preferred over far-field or distant lighting-based PS for both practical and theoretical rea-sons. It is extremely useful for capturing large objects, e.g. furniture or humans, especially in a confined space like a room [3, 16, 25]. This is because far-field PS approaches assume the lighting to be distant, e.g. 10× the object di-mensions is suggested by [33, 34], causing it to be unsuit-able for 3D imaging in many indoor spaces. Additionally, low-intensity LED lights on handheld devices (e.g. flash-light on a phone) may not be bright enough to illuminate an object from a large distance [28]. Theoretically, in the case of uncalibrated lighting, near-field PS has no linear ambigu-ity in contrast to far-field PS where there is the well-known
Generalized Bas-Relief ambiguity [4], as shown in [25].
We make our method fast and accurate by forgoing tra-ditional optimization in favor of a recursive multi-scale al-gorithm. Our proposed method consists of two recursive networks one for predicting surface normal and another for depth maps. At each step of the recursion, we increase the input image resolution by a factor of 2. We first analytically estimate the relative lighting direction and attenuation fac-tor for each pixel in the image (termed ‘per-pixel lighting’ for clarity) by upsampling the predicted depth map from the previous step. We then infer the surface normal for this scale given the input image, ‘per-pixel lighting,’ and esti-mated normal map from the previous scale. Finally, the depth map is predicted conditioned on the estimated nor-mal map and the depth map from the previous scale. The number of steps for this recursion is dictated by the input image resolution making the inference extremely fast, re-quiring only a few forward passes. We also improve infer-ence speed by using a recursive deep network for estimating depth map from normals instead of solving normal integra-tion by e.g. solving the Poisson equation [13, 30], making it more robust to outliers during training. The recursion al-lows the use of one network for all scales, thus heavily re-ducing the memory footprint. This approach is also more robust to noise and lighting calibration errors than existing per-pixel based methods [18, 31] as the recursion leads to a larger receptive field for the network.
Our method is built on the shoulders of existing near-field and far-field PS techniques by adapting the ideas that can best improve performance, inference speed, and mem-ory requirements. Our recursive approach is inspired by
[17], which uses a single network for predicting normal at each scale conditioned on the image and the estimated nor-mal from the previous scale. It is non-trivial to adapt the recursion idea proposed in [17] from far-field distant light-ing to near-field because per-pixel lighting directions are not known a priori. Our ablation study shows that a trivial ex-tension of [17] to near-field PS that does not refine lighting directions based on depth performs significantly worse (by 3.5◦) than our proposed approach. The idea of using depth map to predict ‘per-pixel lighting’ is inspired by [18,25,31].
However, these approaches operate on pixels or patches us-ing iterative optimization, causing extensive memory usage, slow inference speed and making them highly susceptible to noise and lighting calibration errors.
We first evaluate our method quantitatively on the
LUCES dataset [21] with calibrated lighting and show that our method is 2◦ more accurate in surface normal predic-tion (11.3◦ vs. 13.3◦ Mean Angular Error) than state-of-the-art near-field PS approach L20 [18], and another prior ap-proach S20 [31]. In terms of computational efficiency, our method requires 4GB CPU memory and 12GB GPU mem-ory compared to 27GB CPU and 17GB GPU of L20 [18] for 1024×786 resolution, while S20 [31] fails to scale up to this resolution. Our inference speed is 1.3 secs compared to 59.5 secs of L20 [18] and 2435 secs of S20 [31] for 52 512×384 resolution images; tested on the same hardware.
For many practical applications, such as quickly recon-structing 3D models at the home, calibrated lighting is im-practical. In the absence of calibrated lighting, we also in-troduce an additional lighting calibration network. We first show that on the LUCES dataset with uncalibrated lighting our method is more robust than existing approaches, pro-ducing 14.11◦ Mean Angular Error (MAE) vs 18.85◦ of
L20 and 16.03◦ of S20. Finally, we capture a few real-world objects with near-field lighting with a commodity flashlight and show that our reconstructed mesh is qualitatively more accurate than existing approaches S20 [31] and L20 [18], after using the same calibration network, see Fig. 1 and 4.
In summary our contributions are as follows:
• A state-of-the-art, fast, light-weight, near-field PS method with 45× faster inference speed and significantly lower memory requirements than existing methods.
• We build on [17], developed for far-field PS, by incor-porating ‘per-pixel lighting’, adding recursive depth pre-diction from normal, and allowing the flexibility to use unstructured lighting.
• We also introduce a calibration network to facilitate un-calibrated capture in-the-wild with an iPhone camera and a handheld flashlight. 2. Prior Work
Research on Photometric Stereo (PS), introduced in [33], can be divided along a number of dimensions: diffuse vs. specular materials, calibrated vs. uncalibrated lighting, dis-tant vs. nearby lights. In this work, we focus on near-field
PS with both known and unknown lighting conditions.
Far-Field Photometric Stereo. We briefly mention some recent far-field PS works that are particularly relevant to this work. For a more comprehensive survey see [2, 9].
Our work is inspired by [17] which introduces a recursive neural net to predict surface normal at each scale given the input image at that scale and the predicted normal map from the previous scale. The authors showed that using a re-cursive architecture significantly improves performance by capturing global context that is often absent in per-pixel techniques [14] and patch-based techniques [6].
Near-Field Photometric Stereo. Solutions to near-field
PS can be roughly divided into two broad approaches.
The first approach relies on a three step iterative refine-ment [3, 5, 8, 18, 24, 25, 28], starting with an initial shape, e.g. a plane, until convergence: (1) based on the current shape calculate the light directions and intensity at each point; (2) using these light estimates, predict surface nor-mals; (3) integrate normals to update the shape. Logothetis et al. [18] uses a per-pixel far-field deep neural network in step (2) while the rest of these methods are purely optimiza-tion driven. In contrast, we use two deep recursive neural nets for steps (2) and (3), trained on the whole image for near-field lighting.
Direct optimization approaches rely on inverting the im-age formation process, often by solving a system of PDEs
[22, 23, 27–29]. For a detailed discussion of these methods see [29]. In [34] the authors use a local-global mesh defor-mation scheme to optimize a mesh that reconstructs the im-ages. Santo et al. [31] also optimizes a reconstruction loss.
However, as part of their forward pass they decompose ob-servations into reflectance and normal using a far-field deep neural network.
R r0, ..., rR−1
I j i
Ni
Di
Aj i , Lj i pj, dj, µj
U p(I) ones(r × r) number of resolutions sequence of resolutions r0 = 64, ri+1 = 2ri, rR−1 input image reso-lution jth image at resolution ri normal at resolution ri depth at resolution ri per-pixel light attenuation and direc-tion at resolution ri for image j light parameters of jth image upsample I by a factor of 2 r × r array of ones
Table 1. Summary of major notations used throughout the text.
Light Calibration. Research on uncalibrated PS either separately estimates lighting or alternately solves for light and shape simultaneously using a variational approach [10].
For the former, the lighting estimation can be physically performed by inserting additional objects [11, 16] in the scene or by using a deep network for prediction [6, 7, 15].
While the these methods have been introduced for far-field
PS, we propose a calibration network for near-field PS.
Normal Integration. Normal integration techniques es-timate a depth map that is consistent with a normal map. For a detailed discussion see [30]. Ho et al. [12] uses the sim-ilarity between normal integration and shape from shading (SfS) to develop a normal integration technique. Similarly, we also introduce a deep network for faster and stable nor-mal integration during training based on SfS. 3.