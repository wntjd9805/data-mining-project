Abstract
The visual relationship recognition (VRR) task aims at understanding the pairwise visual relationships between in-teracting objects in an image. These relationships typically have a long-tail distribution due to their compositional na-ture. This problem gets more severe when the vocabulary becomes large, rendering this task very challenging. This paper shows that modeling an effective message-passing flow through an attention mechanism can be critical to tack-ling the compositionality and long-tail challenges in VRR.
The method, called RelTransformer, represents each im-age as a fully-connected scene graph and restructures the whole scene into the relation-triplet and global-scene con-texts. It directly passes the message from each element in the relation-triplet and global-scene contexts to the target relation via self-attention. We also design a learnable mem-ory to augment the long-tail relation representation learn-ing. Through extensive experiments, we find that our model generalizes well on many VRR benchmarks. Our model outperforms the best-performing models on two large-scale long-tail VRR benchmarks, VG8K-LT (+2.0% overall acc) and GQA-LT (+26.0% overall acc), both having a highly skewed distribution towards the tail. It also achieves strong results on the VG200 relation detection task. Our code is available at https://github.com/Vision-CAIR/
RelTransformer. 1.

Introduction
The Visual Relationship Recognition (VRR) task goes beyond recognizing individual objects by comprehensively understanding relationships between interacting objects in a visual scene. Owing to the enriched scene understanding provided by VRR, it benefits various other vision tasks such as image captioning (e.g., [10,46,47]), VQA (e.g., [15,40]), image generation (e.g., [16]), and 3D scene synthesis (e.g.,
[31]). However, due to the imbalanced class distribution in many VRR datasets [15, 20], predictions of the most exist-ing models are dominated by the head/frequent relations, lacking generalization on tail/low-shot relationships.
Many previous approaches characterize the VRR prob-lem under a graph scenario. Popular graph-based methods iteratively pass massages from other direct or indirect nodes to the relation along with the structure of the graph using the long short-term memory [39, 44, 50], or graph attention net-works [24, 45]. However, the graph structure may implic-itly constraint the relation to focus on its nearby neighbors.
This phenomenon has been observed in recent works [2,43], showing that graph neural networks incline to pay most at-tention to the local surrounding nodes but could not bene-fit much from distant nodes [2], and node representations will become indistinguishable if there are many layers [43].
Such problems can also be seen in long short-term memory networks due to their iterative message-passing learning na-ture. However, the target relation can also benefit a lot from the distant nodes. e.g., when we predict the “holding” re-lationship between “player” and “bat” in Fig. 1, the distant objects such as “catcher” and “umpire” can provide a con-text that the “player” is in a baseball game, and those can help the model better predict “holding” relationship.
To alleviate the aforementioned problems, we propose to adapt self-attention mechanisms originally introduced in Transformer [41] to tackle the VRR challenges. Self-attention can be viewed as a non-local mean operation [3], which computes the weighted average of all the inputs.
When it applies to the VRR problem, it assumes that the relation has a full connection with all the other nodes in the graph and directly passes the messages among them via attention. In contrast to GNN/LSTM approaches, this strategy can allow the relation to have a larger attention scope and pass the message regardless of the graph struc-ture or spatial constraints.
It also avoids the valuable in-formation from distant nodes being suppressed by nearby neighbors. Hence, each relation can selectively attend to its relevant features without spatial constraints and learn a richer-contextualized representation, which can benefit the long-tail visual relationship understanding.
In our approach, dubbed as RelTransformer, we recon-struct the scene graph into the relation-triplet and global-scene context as we demonstrated in the Fig. 1. The relation triplet here refers to the target relation and its referred sub-ject and object, such as ⟨player, holding, bat⟩ in the figure.
The global context represents all the relation triplets that are gathered for each appearing relation. We directly con-nect the target relation “holding” with every element from the relation triplet and global context, and pass their infor-mation to the target relation via self-attention. Furthermore, since the long-tail relations tend to be amenable to forget-ting, we also propose a novel memory attention module to augment the relation representation with external persistent memory vectors, as we will detail later.
We showcase the effectiveness of our model on VG200
[20] and two recently proposed large-scale long-tail VRR benchmarks, GQA-LT [1] and VG8K-LT [1]. GQA-LT and
VG8K-LT scale the number of relation types up to 300 and 2,000 compared to only 50 relation types in VG200.
These two benchmarks are highly skewed (e.g., the VG8K-LT benchmark ranges from 14 to 618,687 examples per re-lation type) and offer us a suitable platform for studying long-tail VRR problems. Our approach achieves the state-of-the-art on those three datasets in our experimental re-sults, demonstrating its effectiveness. We also conducted several ablative experiments and showed the usefulness of each component design in RelTransformer. 2.