Abstract
Contrastive learning (CL) is widely known to require many negative samples, 65536 in MoCo for instance, for which the performance of a dictionary-free framework is often inferior because the negative sample size (NSS) is limited by its mini-batch size (MBS). To decouple the NSS from the MBS, a dynamic dictionary has been adopted in a large volume of CL frameworks, among which arguably the most popular one is MoCo family. In essence, MoCo adopts a momentum-based queue dictionary, for which we perform a fine-grained analysis of its size and consistency.
We point out that InfoNCE loss used in MoCo implicitly at-tract anchors to their corresponding positive sample with various strength of penalties and identify such inter-anchor hardness-awareness property as a major reason for the ne-cessity of a large dictionary. Our findings motivate us to simplify MoCo v2 via the removal of its dictionary as well as momentum. Based on an InfoNCE with the pro-posed dual temperature, our simplified frameworks, Sim-MoCo and SimCo, outperform MoCo v2 by a visible mar-gin. Moreover, our work bridges the gap between CL and non-CL frameworks, contributing to a more unified under-standing of these two mainstream frameworks in SSL. Code is available at: https://bit.ly/3LkQbaT. 1.

Introduction
Self-supervised learning (SSL) has become increasingly popular in various domains, ranging from NLP [12, 30, 34, 38, 42] to visual representation [6, 22, 36], in the past few years. Especially, contrastive learning (CL) frame-works [2,6,22,23,25,36,44,47,50,53,65] have attracted sig-nificant attention due to its intuitive motivation. In essence,
CL is designed to attract the anchor sample [46] close to the positive sample, i.e. another augmented view of the same image, and simultaneously repulse it from negative
*equal contribution.
Figure 1.
Intra-anchor and Inter-anchor hardness-aware proper-ties. The former is indicated by different repulsing weights (see
Eq 5) for different negative samples based on their hardness, i.e. 1 ̸= p1 p1
K (K denotes NSS), and the latter is indicated by different weights being put on different anchor samples to attract the corresponding positive sample, i.e. (cid:80)K j ̸= (cid:80)K j with three anchor images as a motivation example. j ̸= (cid:80)K 2... ̸= p1 j=1 p1 j=1 p2 j=1 p3 samples, i.e. views from different images. With the popu-lar InfoNCE loss [36], CL is widely reported to require a
[6] large amount of negative samples [22]. For example, shows that increasing the mini-batch size (MBS) to a large value, 4096 for instance, is essential for achieving compet-itive performance, for which there are multiple challenges, such as GPU memory concern or difficulty to train with a large MBS [6, 57]. Thus, a major line of CL frameworks, such as MoCo [22], have emerged to decouple the required large negative sample size (NSS) from the MBS with a dy-namic dictionary. Despite much effort in the dictionary de-sign [22,53], why contrastive InfoNCE requires a large dic-tionary (or many negative samples) is not well understood.
Our investigation of the above problem centers around an interesting hardness-aware property [46] of InfoNCE. A large volume of works [3, 26, 28, 35, 45, 60] have studied strategies of mining hard negative samples, i.e. those sam-ples that are similar to the anchor sample. We point out that
the anchor sample also has this hardness property. Concep-tually, an anchor sample is considered hard when it is still far from the positive sample and/or close to negative sam-ples. InfoNCE loss has been identified to have the hardness-aware property [46], which contributes to dimensional de-correlation [61], is critical for performance.
Prior works
[46, 61] mainly study the hardness-awareness within an anchor, which is therefore termed intra-anchor hardness-aware property here. As in Fig-ure 1, it indicates that the gradient puts different weights (see Eq 5) on various negative samples for repulsing the anchor from them with different strength of penalties, i.e. pi 1 ̸= pi
̸= pi
K (the fixed superscript i, 1 for in-In contrast, the inter-stance, denotes the same anchor). anchor hardness-aware property indicates different weights being on anchors for attracting them to their correspond-ing positive sample with different penalties, i.e. (cid:80)K j ̸= (cid:80)K j=1 p1 j=1 p2 j with three anchors as an example.
Overall, our contributions are summarized as follows. j ̸= (cid:80)K j=1 p3 2...
• We point out that anchors have hardness property, for which contrastive InfoNCE loss by default attracts them to their corresponding positive samples with var-ious strength of penalties. Recognizing this, we disen-tangle InfoNCE into vector and scalar components that reflect intra-anchor and inter-anchor hardness-aware properties, respectively. Such a decomposed loss facil-itates a fine-grained analysis on the MoCo dictionary and we reveal: (i) a small dictionary is sufficient for the vector component which, however, requires high con-sistency between encoders for representing the nega-tive and positive keys; (ii) the scalar component re-quires a very large dictionary but is less sensitive to such consistency.
• We identify that the increase of dictionary size and temperature both help alleviate the inter-anchor hardness-aware sensitivity for improving performance.
Our findings help simplify MoCo family via remov-ing their dictionary and momentum.
Specifically, we propose dual temperature for realizing indepen-dent control of intra-anchor and inter-anchor proper-ties. Without a dictionary, our proposed SimMoCo achieves comparable or superior performance over the baseline MoCo v2. Notably, our dictionary-free and momentum-free SimCo is simple yet effective.
• Our investigation helps bridge the gap between CL and non-CL frameworks, contributing to a unified perspec-tive on these two major SSL frameworks. 2.