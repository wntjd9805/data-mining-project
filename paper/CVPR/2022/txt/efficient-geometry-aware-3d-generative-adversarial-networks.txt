Abstract
Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing chal-lenge. Existing 3D GANs are either compute-intensive or make approximations that are not 3D-consistent; the for-mer limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efﬁciency and image quality of 3D GANs without overly relying on these approximations. We introduce an expres-sive hybrid explicit-implicit network architecture that, to-gether with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling fea-ture generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efﬁciency and expressive-ness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments. 1.

Introduction
Generative adversarial networks (GANs) have seen im-mense progress, with recent models capable of generat-ing high-resolution, photorealistic images indistinguishable from real photographs [27–29]. Current state-of-the-art
GANs, however, operate in 2D only and do not explicitly model the underlying 3D scenes.
Recent work on 3D-aware GANs has begun to tackle the problem of multi-view-consistent image synthesis and, to a lesser extent, extraction of 3D shapes without being super-vised on geometry or multi-view image collections. How-*Equal contribution.
†Part of the work was done during an internship at NVIDIA.
Project page: https://github.com/NVlabs/eg3d
Figure 1. Our 3D GAN enables synthesis of scenes, producing high-quality, multi-view-consistent renderings and detailed geom-etry. Our approach trains from a collection of 2D images without target-speciﬁc shape priors, ground truth 3D scans, or multi-view supervision. Please see the accompanying video for more results. ever, the image quality and resolution of existing 3D GANs have lagged far behind those of 2D GANs. Furthermore, their 3D reconstruction quality, so far, leaves much to be desired. One of the primary reasons for this gap is the com-putational inefﬁciency of previously employed 3D genera-tors and neural rendering architectures.
In contrast to 2D GANs, 3D GANs rely on a combination of a 3D-structure-aware inductive bias in the generator net-work architecture and a neural rendering engine that aims at providing view-consistent results. The inductive bias can be modeled using explicit voxel grids [14, 21, 47, 48, 68, 74] or neural implicit representations [4, 47, 49, 58]. While successful in single-scene “overﬁtting” scenarios, neither of these representations is suitable for training a high-resolution 3D GAN because they are simply too memory inefﬁcient or slow. Training a 3D GAN requires rendering tens of millions of images, but state-of-the-art neural vol-ume rendering [45] at high-resolutions with these represen-tations is computationally infeasible. CNN-based image up-sampling networks have been proposed to remedy this [49], but such an approach sacriﬁces view consistency and im-pairs the quality of the learned 3D geometry.
We introduce a novel generator architecture for unsuper-vised 3D representation learning from a collection of single-view 2D photographs that seeks to improve the computa-tional efﬁciency of rendering while remaining true to 3D-grounded neural rendering. We achieve this goal with a two-pronged approach. First, we improve the computational ef-ﬁciency of 3D-grounded rendering with a hybrid explicit– implicit 3D representation that offers signiﬁcant speed and memory beneﬁts over fully implicit or explicit approaches without compromising on expressiveness. These advan-tages enable our method to skirt the computational con-straints that have limited the rendering resolutions and qual-ity of previous approaches [4, 58] and forced over-reliance on image-space convolutional upsampling [49]. Second, although we use some image-space approximations that stray from the 3D-grounded rendering, we introduce a dual-discrimination strategy that maintains consistency between the neural rendering and our ﬁnal output to regularize their undesirable view-inconsistent tendencies. Moreover, we in-troduce pose-based conditioning to our generator, which de-couples pose-correlated attributes (e.g., facial expressions) for a multi-view consistent output during inference while faithfully modeling the joint distributions of pose-correlated attributes inherent in the training data.
As an additional beneﬁt, our framework decouples fea-ture generation from neural rendering, enabling it to di-rectly leverage state-of-the-art 2D CNN-based feature gen-erators, such as StyleGAN2, to generalize over spaces of 3D scenes while also beneﬁting from 3D multi-view-consistent neural volume rendering. Our approach not only achieves state-of-the-art qualitative and quantitative results for view-consistent 3D-aware image synthesis, but also generates high-quality 3D shapes of the synthesized scenes due to its strong 3D-structure-aware inductive bias (see Fig. 1).
Our contributions are the following:
• We introduce a tri-plane-based 3D GAN framework, which is both efﬁcient and expressive, to enable high-resolution geometry-aware image synthesis.
• We develop a 3D GAN training strategy that pro-motes multi-view consistency via dual discrimination and generator pose conditioning while faithfully mod-eling pose-correlated attribute distributions (e.g., ex-pressions) present in real-world datasets.
• We demonstrate state-of-the-art results for uncondi-tional 3D-aware image synthesis on the FFHQ and
AFHQ Cats datasets along with high-quality 3D ge-ometry learned entirely from 2D in-the-wild images.
Figure 2. Neural implicit representations use fully connected lay-ers (FC) with positional encoding (PE) to represent a scene, which can be slow to query (a). Explicit voxel grids or hybrid variants using small implicit decoders are fast to query, but scale poorly with resolution (b). Our hybrid explicit–implicit tri-plane repre-sentation (c) is fast and scales efﬁciently with resolution, enabling greater detail for equal capacity. 2.