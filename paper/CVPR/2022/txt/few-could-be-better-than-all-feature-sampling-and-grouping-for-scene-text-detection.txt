Abstract
Recently, transformer-based methods have achieved promising progresses in object detection, as they can elim-inate the post-processes like NMS and enrich the deep rep-resentations. However, these methods cannot well cope with scene text due to its extreme variance of scales and aspect ratios.
In this paper, we present a simple yet ef-fective transformer-based architecture for scene text detec-tion. Different from previous approaches that learn robust deep representations of scene text in a holistic manner, our method performs scene text detection based on a few rep-resentative features, which avoids the disturbance by back-ground and reduces the computational cost. Specifically, we first select a few representative features at all scales that are highly relevant to foreground text. Then, we adopt a transformer for modeling the relationship of the sam-pled features, which effectively divides them into reason-able groups. As each feature group corresponds to a text in-stance, its bounding box can be easily obtained without any post-processing operation. Using the basic feature pyra-mid network for feature extraction, our method consistently achieves state-of-the-art results on several popular datasets for scene text detection. 1.

Introduction
Figure 1. The illustration of feature sampling and grouping. (a)
The confidence score map for text regions indicates the pixel im-portance for text detection. (b) The text features at red points con-taining geometric and context information of foreground text are selected by scores. (c) The sampled features from the same text in-stance are implicitly grouped at the feature level by a transformer. (d) The bounding boxes can be easily obtained from the grouped features.
Scene text detection has been an active research field for a long time, because of its wide range of practical ap-plications, such as scene understanding, automatic driving, and photo translation. As a key prior component of scene text reading, scene text detection aims to precisely locate text in scene images. Despite the noticeable improvement achieved by existing methods [10, 40, 41, 49], it is still a challenging task due to the variety of scene text, e.g. differ-ent scales, complicated illumination, perspective distortion,
*Corresponding Author multi-orientations, and complex shapes. Moreover, most scene text detection methods depend on complicated pro-cessing to generate or refine the predicted results, such as anchor generation, non-maximum suppression (NMS) [31], binarization [17], or contour extraction [36].
Inspired by the advantages of the transformer [39] in nat-ural language processing (NLP), lots of works [2, 5, 23, 28, 35, 53] introduce it into vision tasks to extract global-range features and model long-distance dependencies in images, while showing promising performance. Especially in object
detection, DETR-based methods [2,28,53] successfully use transformers to remove the complicated hand-designed pro-cesses (e.g. NMS and anchor generation) from the former object detection frameworks [8, 20, 33].
Although transformers bring advantages in global-range feature modeling to DETR-based frameworks [2], they may suffer from handling the small objects and the high com-putational complexity. For instance, a recent DETR-based scene text detector [32] cannot achieve the satisfactory detection accuracy on the ICDAR2015 dataset [12] and
ICDAR2017-MLT dataset [30], since the text instances in these two datasets have much larger variance of scales and aspect ratios. It is often insufficient for transformers to cap-ture small text on the feature map at small scales, while the time cost of a DETR-based method with multi-scale feature maps is unpredictable. Essentially, unexpected background noise in higher-resolution feature maps would significantly increase the computational cost and disturb the transformer modeling. Though, some recent works [28, 53] improve the efficiency of transformer-based object detectors by op-timizing the attention operations, they fail to achieve the competitive results in scene text detection (refer to the re-sults reported in Tab. 5).
In this paper, we propose a simple yet effective transformer-based architecture for scene text detection. We argue that feature learning with the relationship of all pixels is not necessary, as foreground text instances only occupy a few small and narrow regions in scene images. Intuitively, we firstly sample and collect the features that are highly relevant to scene text as illustrated in Fig. 1(a)(b). Then, we adopt a transformer for modeling the relationship of the sampled features so that they can be properly grouped. As shown in Fig. 1(c)(d), benefiting from the powerful atten-tion mechanism of the transformer, each feature group will correspond to a text instance, which is quite convenient for predicting its bounding box.
Different from the previous scene text detection meth-ods [1, 15, 17, 40, 49, 51] that usually learn the deep repre-sentations of scene text images in a holistic manner with
CNNs, our detection method based on only a few repre-sentative features has three prominent advantages: 1) it can significantly eliminate the redundant background informa-tion, which is beneficial for improving the effectiveness and efficiency of the detection process; 2) Using a trans-former to group the sampled features, we can obtain more accurate grouping results and bounding boxes without any post-processing operation; 3) As the feature sampling and grouping are implemented in an end-to-end fashion, the two stages can jointly improve the final detection perfor-mance. To verify the effectiveness of the proposed feature sampling-and-grouping scheme, we conduct extensive ex-periments on several popular datasets [3, 11, 12, 30, 47, 48] for scene text detection, consistently achieving the state-of-of-art results. In addition, the comparison with the recent transformer-based detectors [2, 28, 32, 53] also proves the effectiveness of our method. 2.