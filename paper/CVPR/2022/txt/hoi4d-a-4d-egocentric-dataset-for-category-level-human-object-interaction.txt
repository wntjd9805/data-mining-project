Abstract
We present HOI4D, a large-scale 4D egocentric dataset with rich annotations, to catalyze the research of category-level human-object interaction. HOI4D consists of 2.4M
RGB-D egocentric video frames over 4000 sequences col-lected by 9 participants interacting with 800 different ob-ject instances from 16 categories over 610 different indoor rooms. Frame-wise annotations for panoptic segmentation, motion segmentation, 3D hand pose, category-level object pose and hand action have also been provided, together with reconstructed object meshes and scene point clouds.
With HOI4D, we establish three benchmarking tasks to pro-mote category-level HOI from 4D visual signals includ-ing semantic segmentation of 4D dynamic point cloud se-quences, category-level object pose tracking, and egocen-tric action segmentation with diverse interaction targets.
In-depth analysis shows HOI4D poses great challenges to existing methods and produces huge research opportunities. 1.

Introduction
Tremendous progress [9, 14, 44, 47] has been made for naming objects and activities in images, videos or 3D point clouds over the last decade facilitated by significant dataset and benchmark efforts. However, these perception out-comes fail to satisfy the needs of more and more critical applications such as human-assistant robots and augmented reality where the perception of interactions from 4D ego-centric sensory inputs (e.g., temporal streams of colored point clouds) is required. It becomes highly desirable for a computer vision system to build up a detailed understand-ing of human-object interaction from an egocentric point of view. Such understanding should unify semantic un-derstanding of 4D dynamic scenes, the 3D pose of human hands under object occlusion, the 3D pose and functionality
*Equal contribution.
†Corresponding author.
Figure 1. Overview of HOI4D: We construct a large-scale 4D egocentric dataset with rich annotation for category-level human-object interaction. Frame-wise annotations for action segmenta-tion(a), motion segmentation(b), panoptic segmentation(d), 3D hand pose and category-level object pose(c) are provided, together with reconstructed object meshes(e) and scene point cloud. of novel objects of interaction interest, as well as the action and intention of humans, which pose new challenges for to-day’s computer vision systems.
To help tackle these challenges, large-scale and annotation-rich 4D egocentric HOI datasets as well as the corresponding benchmark suites are strongly needed. Re-cently some efforts [18, 19, 22] have been made to fulfill such needs. However, most of these works focus on what we called instance-level human-object interaction where the objects being interacted with all come from a very small pool of instances whose exact CAD models and sizes are known beforehand. This impedes their application to per-ceiving human interaction with the vast diversity of ob-jects in our daily life. Moreover, these works tend to ig-nore articulated objects while only focusing on rigid ob-jects with which the interaction patterns are relatively sim-pler. These limitations partially come from the challenging and cumbersome nature of jointly capturing hands, objects and real scenes in an egocentric manner. Curating synthetic datasets [20] might be an alternative. Nonetheless simu-lating natural human motion and functional grasping for generic objects are still open research problems, making it
hard for existing synthetic datasets to reach a realism suffi-cient for sim-to-real transfer.
To cope with the above limitations, we present, for the first time, a large-scale 4D egocentric dataset for category-level human-object interaction as depicted in Figure 1. We draw inspirations from recent category-level object pose es-timation and pose tracking works [24,42,44] and aim to pro-pel 4D HOI perception to a new era to handle category-level object variations in cluttered scenes. We collect a richly an-notated 4D egocentric dataset, HOI4D, to depict humans interacting with various objects while executing different tasks in indoor environments. HOI4D consists of 2.4M
RGB-D egocentric video frames over 4000 sequences of 9 participants interacting with 800 object instances. These object instances are evenly split into 16 categories includ-ing both rigid and articulated objects. Also instead of using a lab setting like most previous works, the camera wearers execute tasks revealing the functionality of each category without wearing any markers in 610 different indoor scenes.
HOI4D is associated with reconstructed scene point clouds and object meshes for all sequences. HOI4D provides anno-tations for frame-wise panoptic segmentation, motion seg-mentation, 3D hand pose, rigid and articulated object pose, and action segmentation, delivering unprecedented levels of detail for human-object interaction at the category level.
The rich annotations in HOI4D also enable benchmark-ing on a series of category-level HOI tasks. In this paper, we focus on three tasks in particular: semantic segmenta-tion of 4D dynamic point cloud sequences, category-level object pose tracking for hand-object interaction and egocen-tric hand action recognition with diverse interaction targets.
We provide an in-depth analysis of existing approaches to these tasks. Our experiments suggest that HOI4D has posed great challenges to today’s computer vision algorithms. For category-level object and part pose tracking task, most of the previous datasets use synthetic data under simple scenes without hand occlusion. With the help of the proposed
HOI4D dataset, researchers can now work on this more challenging task with real-world data. Due to a lack of an-notated indoor datasets, semantic segmentation of 4D point clouds has been studied mainly for autonomous driving applications. HOI4D introduces more challenges such as heavy occlusion, fast ego-motion, and very different sensor noise patterns. Fine-grained action segmentation of video can help AI better comprehend interaction, but we found that existing coarse-grained methods cannot directly pro-cess fine-grained data well.
In summary, our contributions can be listed below:
• We present the first dataset, HOI4D, for 4D egocen-tric category-level human-object interaction. HOI4D covers richly annotated 4D visual sequences captured while humans interact with a large number of object instances. The huge category-level object variations allow perceiving human interactions with potentially unseen objects.
• We present a data collection and annotation pipeline combining human annotations with automatic algo-rithms, effectively scaling up our dataset.
• We benchmark on three category-level HOI tasks cov-ering 4D dynamics scene understanding, category-level object pose tracking and hand action segmenta-tion. We provide a thorough analysis of existing meth-ods and point out new challenges HOI4D has posed. 2.