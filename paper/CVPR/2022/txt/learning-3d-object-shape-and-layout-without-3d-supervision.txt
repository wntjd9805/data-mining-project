Abstract
A 3D scene consists of a set of objects, each with a shape and a layout giving their position in space. Understanding 3D scenes from 2D images is an important goal, with ap-plications in robotics and graphics. While there have been recent advances in predicting 3D shape and layout from a single image, most approaches rely on 3D ground truth for training which is expensive to collect at scale. We overcome these limitations and propose a method that learns to predict 3D shape and layout for objects without any ground truth shape or layout information: instead we rely on multi-view images with 2D supervision which can more easily be col-lected at scale. Through extensive experiments on ShapeNet,
Hypersim, and ScanNet we demonstrate that our approach scales to large datasets of realistic images, and compares favorably to methods relying on 3D ground truth. On Hy-persim and ScanNet where reliable 3D ground truth is not available, our approach outperforms supervised approaches trained on smaller and less diverse datasets. 1 1.

Introduction
A 3D scene consists of a set of objects, speciﬁed by a 3D shape for each object and the 3D layout of objects in space. Understanding this 3D scene structure is critical for navigating or interacting with the world. Unfortunately, di-rectly measuring or perceiving 3D structure is often imprac-tical. For this reason, inferring the shape and layout of 3D scenes from 2D images has long been a fundamental prob-lem in computer vision, with wide applications in robotics, autonomous vehicles, graphics, AR/VR, and beyond.
The rise of deep learning has dramatically improved 3D understanding from a single image. Methods have advanced from estimating 3D shapes of isolated objects [6, 11, 51] to predicting multiple shapes in complex scenes [14] and even jointly predicting shape and layout [38, 50]. While impres-sive, these methods share a ﬂaw: they use ground truth 3D shape and layout for training. Creating large, varied training sets with this data is impractical, limiting the scalability and utility of methods relying on strong 3D supervision. 1Project page https://gkioxari.github.io/usl/
Figure 1. We propose an end-to-end model which takes an input image, detects all objects in 2D and predicts their 3D shapes and layouts. We learn from multiple 2D scene views, e.g. frames of videos, and without any 3D supervision.
Some recent approaches take an extreme position, and train on collections of images without any 3D supervision whatsoever [15,22,27,28,53]. While admirable, overcoming the fundamental ambiguities of 3D from a single image requires strong category-speciﬁc shape priors, making it difﬁcult to scale to the complexities of the real world.
Another natural way to predict 3D structure is to use mul-tiple views. Multi-view images give weak 3D supervision and can be captured at scale using videos or multi-camera rigs. Classical techniques such as Structure from Motion and Multi-View Stereo [18] reconstruct full 3D scenes with-out 3D supervision, but require many views, do not predict semantics, and are not typically learned from data.
More recently, differentiable rendering has enabled a new wave of methods that predict 3D shapes without strong 3D supervision [5, 24, 34, 41]. During training a model inputs a single image and outputs a 3D shape, which is rendered from one or more auxiliary views; comparing the rendered prediction with 2D silhouettes in auxiliary views provides a training signal. This pipeline is promising since it requires no ground truth 3D shapes, instead learning solely from multi-view images and 2D image supervision which can
both be collected at scale. However, to date this technique has only been applied to simple images with a single object.
In this paper, we aim to predict 3D object shapes and layout in complex scenes from a single image, as in Fig. 1.
Crucially we do not use ground truth shapes or layouts during training; instead we learn from object silhouettes in multi-view images. We build on Mesh R-CNN [14], which predicts 3D shapes, but not layouts, for objects in complex scenes and relies on 3D shape supervision during training. We augment
Mesh R-CNN with a layout network that estimates each object’s 3D location, and replace expensive mesh supervision with scalable multi-view supervision. Like prior work [5, 24, 34, 41] we learn via differentiable rendering and 2D losses; however these methods only predict 3D shape – to also predict layout we use a distance transform loss. We call our
Unsupervised approach for Shape and Layout estimation
USL. At test time, USL inputs a single RGB image and jointly detects objects and predicts their 3D shape and layout.
We show results on three datasets to demonstrate the utility of our scalable multi-view supervision. First, we show results on Scene-ShapeNet, a synthetic dataset with scenes composed of multiple ShapeNet [3] objects where our method shows strong performance compared with Mesh
R-CNN trained using strong 3D shape supervision. We then experiment on Hypersim [44], demonstrating that our ap-proach scales to complex, realistic scenes with many objects.
Finally, we show results on ScanNet [7] where camera poses are estimated from BundleFusion [8] and 2D silhouettes are estimated using PointRend [26], showing that we can learn from noisy real-world video without expensive ground truth. 2.