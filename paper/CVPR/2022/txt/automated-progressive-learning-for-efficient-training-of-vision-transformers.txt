Abstract
Recent advances in vision Transformers (ViTs) have come with a voracious appetite for computing power, high-lighting the urgent need to develop efficient training meth-ods for ViTs. Progressive learning, a training scheme where the model capacity grows progressively during training, has started showing its ability in efficient training. In this paper, we take a practical step towards efficient training of ViTs by customizing and automating progressive learning. First, we develop a strong manual baseline for progressive learn-ing of ViTs, by introducing momentum growth (MoGrow) to bridge the gap brought by model growth. Then, we pro-pose automated progressive learning (AutoProg), an effi-cient training scheme that aims to achieve lossless accel-eration by automatically increasing the training overload on-the-fly; this is achieved by adaptively deciding whether, where and how much should the model grow during pro-gressive learning. Specifically, we first relax the optimiza-tion of the growth schedule to sub-network architecture op-timization problem, then propose one-shot estimation of the sub-network performance via an elastic supernet. The searching overhead is reduced to minimal by recycling the parameters of the supernet. Extensive experiments of ef-ficient training on ImageNet with two representative ViT models, DeiT and VOLO, demonstrate that AutoProg can accelerate ViTs training by up to 85.1% with no perfor-mance drop.1 1.

Introduction
With powerful high model capacity and large amounts of data, Transformers have dramatically improved the perfor-mance on many tasks in computer vision (CV) [54,69]. The pioneering ViT model [21], scales the model size to 1,021 billion FLOPs, 250× larger than ResNet-50 [31]. Through pre-training on the large-scale JFT-3B dataset [86], the re-Figure 1. Accuracy of ViTs (DeiT [69] and VOLO [85]) during training. Smaller ViTs converge faster in terms of runtime2. Mod-els in the legend are sorted in increasing order of model size.
Model
CO2e (lbs)3 ImageNet Acc. (%)
ResNet-50 [21, 31]
BERTbase [18]
Avg person per year [62]
ViT-H/14 [21]
CoAtNet [15] 267 1,438 11,023 22,793 183,256 77.54
--88.55 90.88
Table 1. The growth in training scale of vision models results in considerable growth of environmental costs. The CO2e of human life and a language model, BERT [18] are also included for com-parison. The results of ResNet-50, ViT-H/14 are from [21], and trained on JFT-300M [63]. CoAtNet is trained on JFT-3B [86]. cently proposed ViT model, CoAtNet [15], reached state-of-the-art performance, with about 8× training cost of the original ViT. The rapid growth in training scale inevitably leads to higher computation cost and carbon emissions. As shown in Tab. 1, recent breakthroughs of vision Transform-ers have come with a voracious appetite for computing power, resulting in considerable growth of environmental costs. Thus, it becomes extremely important to make ViTs training tenable in computation and energy consumption.
In mainstream deep learning training schemes, all the network parameters participate in every training iteration.
However, we empirically found that training only a small 2We refer runtime to the total GPU hours used in forward and backward pass of the model during training.
†Corresponding author. 1Code: https://github.com/changlin31/AutoProg. 3CO2 equivalent emissions (CO2e) are calculated following [56], using
U.S. average energy mix, i.e., 0.429 kg of CO2e/KWh.
part of the parameters yields comparable performance in early training stages of ViTs. As shown in Fig. 1, smaller
ViTs converge much faster in terms of runtime (though they would be eventually surpassed given enough training time).
The above observation motivates us to rethink the efficiency bottlenecks of training ViTs: does every parameter, every input element need to participate in all the training steps?
Here, we make the Growing Ticket Hypothesis of ViTs: the performance of a large ViT model, can be reached by first training its sub-network, then the full network af-ter properly growing, with the same total training itera-tions. This hypothesis generalizes the lottery ticket hypoth-esis [24] by adding a finetuning procedure at the full model size, changing its scenario from efficient inference to effi-cient training. By iteratively applying this hypothesis to the sub-network, we have the progressive learning scheme.
Recently, progressive learning has started showing its
In the field of capability in accelerating model training.
NLP, progressive learning can reduce half of BERT pre-training time [25]. Progressive learning also shows the abil-ity to reduce the training cost for convolutional neural net-works (CNNs) [66]. However, these algorithms differ sub-stantially from each other, and their generalization ability among architectures is not well studied. For instance, we empirically observed that progressive stacking [25] could result in significant performance drop (about 1%) on ViTs.
To this end, we take a practical step towards sustainable deep learning by generalizing and automating progressive learning on ViTs. To the best of our knowledge, we are among the pioneering works to tackle the efficiency bot-tlenecks of training ViT models. We formulate progres-sive learning with its two components, growth operator and growth schedule, and study each component separately by controlling the other.
First, we present a strong manual baseline for progres-sive learning of ViTs by developing the growth operator.
To ablate the optimization of the growth operator, we in-troduce a uniform linear growth schedule in two dimen-sions of ViTs, i.e., number of patches and network depth.
To bridge the gap brought by model growth, we propose momentum growth (MoGrow) operator with an effective momentum update scheme. Moreover, we present a novel automated progressive learning (AutoProg) algorithm that achieves lossless training acceleration by automatically ad-justing the training overload. Specifically, we first relax the optimization of the growth schedule to sub-network archi-tecture optimization problem. Then, we propose one-shot estimation of sub-network performance via training an elas-tic supernet. The searching overhead is reduced to minimal by recycling the parameters of the supernet.
The proposed AutoProg achieves remarkable training ac-celeration for ViTs on ImageNet. Without manually tuning, it consistently speeds up different ViTs training by more than 40%, on disparate variants of DeiT and VOLO, includ-ing DeiT-tiny and VOLO-D2 with 72.2% and 85.2% Im-ageNet accuracy, respectively. Remarkably, it accelerates
VOLO-D1 [85] training by up to 85.1% with no perfor-mance drop. While significantly saving training time, Au-toProg achieves competitive results when testing on larger input sizes and transferring to other datasets compared to the regular training scheme.
Overall, our contributions are as follows:
• We develop a strong manual baseline for progressive learning of ViTs, by introducing MoGrow, a momen-tum growth strategy to bridge the gap brought by model growing.
• We propose automated progressive learning (Auto-Prog), an efficient training scheme that aims to achieve lossless acceleration by automatically adjusting the growth schedule on-the-fly.
• Our AutoProg achieves remarkable training accelera-tion (up to 85.1%) for ViTs on ImageNet, performing favourably against the original training scheme. 2.