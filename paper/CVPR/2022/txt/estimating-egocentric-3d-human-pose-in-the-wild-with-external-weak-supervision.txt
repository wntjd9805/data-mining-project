Abstract
Egocentric 3D human pose estimation with a single fish-eye camera has drawn a significant amount of attention re-cently. However, existing methods struggle with pose esti-mation from in-the-wild images, because they can only be trained on synthetic data due to the unavailability of large-scale in-the-wild egocentric datasets. Furthermore, these methods easily fail when the body parts are occluded by or interacting with the surrounding scene. To address the shortage of in-the-wild data, we collect a large-scale in-the-wild egocentric dataset called Egocentric Poses in the Wild (EgoPW). This dataset is captured by a head-mounted fish-eye camera and an auxiliary external camera, which pro-vides an additional observation of the human body from a third-person perspective during training. We present a new egocentric pose estimation method, which can be trained on the new dataset with weak external supervision. Specifi-cally, we first generate pseudo labels for the EgoPW dataset with a spatio-temporal optimization method by incorporat-ing the external-view supervision. The pseudo labels are then used to train an egocentric pose estimation network.
To facilitate the network training, we propose a novel learn-ing strategy to supervise the egocentric features with the high-quality features extracted by a pretrained external-view pose estimation model. The experiments show that our method predicts accurate 3D poses from a single in-the-wild egocentric image and outperforms the state-of-the-art methods both quantitatively and qualitatively. 1.

Introduction
Egocentric motion capture using head- or body-mounted cameras has recently become popular because traditional motion capture systems with outside-in cameras have limi-tations when the person is moving around in a large space and thus restrict the scope of applications. Different from traditional systems, the egocentric motion capture system
Figure 1. Compared with Mo2Cap2, our method gets a more ac-curate egocentric pose from a single in-the-wild image, especially when the body parts are occluded. Note that the external images are only used for visualization, not the inputs to our method. is mobile, flexible, and has no requirements on recording space, which enables capturing a wide range of human ac-tivities for many applications, such as wearable medical monitoring, sports analysis, and xR.
In this work, we focus on estimating the full 3D body pose from a single head-mounted fisheye camera. The most related works are Mo2Cap2 [44] and xR-egopose [35].
While these methods have produced compelling results, they are only trained on synthetic images as limited real data exists and, therefore, suffer from significant performance drop on real-world scenarios. Furthermore, these methods often struggle with the cases when parts of the human body are occluded by or interacting with the surrounding scene (see the Mo2Cap2 results in Fig. 1). This is due to the do-main gap between synthetic and real data, but also due to their limited capability of handling occlusions.
To address the issue of the limited real egocentric data,
we capture a large-scale in-the-wild egocentric dataset called Egocentric Poses in the Wild (EgoPW). This is cur-rently the largest egocentric in-the-wild dataset, containing more than 312k frames and covering 20 different daily ac-tivities in 8 everyday scenes. To obtain the supervision for the network training, one possibility is using a multi-view camera setup to capture training data with ground truth 3D body poses or apply multi-view weak supervision. How-ever, this setup is impractical for recording in an environ-ment with limited space (e.g. in the small kitchen shown in
Fig. 3), which is a common recording scenario. Therefore, considering a trade-off between flexibility and 3D accuracy, we use a new device setup consisting of an egocentric cam-era and a single auxiliary external camera. We demonstrate that the external view can provide additional supervision during training, especially for the highly occluded regions in the egocentric view (e.g. the lower body part).
To handle occlusions and estimate accurate poses, we propose a new egocentric pose estimation method for train-ing on the EgoPW dataset in a weakly supervised way.
Specifically, we propose a spatio-temporal optimization method to generate accurate 3D poses for each frame in the EgoPW dataset. The generated poses are further used as pseudo labels for training an egocentric pose estimation network [44]. To improve the network performance, we fa-cilitate the training of the egocentric pose estimation net-work with the extracted features from the external pose es-timation network which has been trained on a large in-the-wild body pose dataset. Specifically, we enforce the feature extracted from these two views to be similar by fooling a discriminator not being able to detect which view the fea-tures are from. To further improve the performance of the pose estimation network, besides the EgoPW dataset, we also use a synthetic dataset [44] to train the network and adopt a domain adaptation strategy to minimize the domain gap between synthetic and real data.
We evaluate our method on the test data provided by
Wang et al. [42] and Xu et al. [44]. Our method signifi-cantly outperforms the state-of-the-art methods both quan-titatively and qualitatively. We also show qualitative re-sults on various in-the-wild images, demonstrating that our method can predict accurate 3D poses on very challenging scenes, especially when the body joints are seriously oc-cluded (see our results in Fig. 1). To summarize, our contri-butions are presented as follows:
• A new method to estimate egocentric human pose with weak supervision from an external view, which sig-nificantly outperforms existing methods on in-the-wild data, especially when severe occlusions exist;
• A large in-the-wild egocentric dataset (EgoPW) cap-tured with a head-mounted fisheye camera and an ex-It is publicly available in https: ternal camera.
//people.mpi- inf.mpg.de/˜jianwang/ projects/egopw;
• A new optimization method to generating pseudo la-bels for the in-the-wild egocentric dataset by incorpo-rating the supervision from an external view;
• An adversarial method for training the network by learning the feature representation of egocentric im-ages with external feature representation. 2.