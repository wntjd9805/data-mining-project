Abstract
Client-wise data heterogeneity is one of the major is-sues that hinder effective training in federated learning (FL).
Since the data distribution on each client may vary dramat-ically, the client selection strategy can significantly influ-ence the convergence rate of the FL process. Active client selection strategies are popularly proposed in recent stud-ies. However, they neglect the loss correlations between the clients and achieve only marginal improvement com-pared to the uniform selection strategy. In this work, we propose FedCor—an FL framework built on a correlation-based client selection strategy, to boost the convergence rate of FL. Specifically, we first model the loss correlations be-tween the clients with a Gaussian Process (GP). Based on the GP model, we derive a client selection strategy with a significant reduction of expected global loss in each round.
Besides, we develop an efficient GP training method with a low communication overhead in the FL scenario by uti-lizing the covariance stationarity. Our experimental results show that compared to the state-of-the-art method, FedCorr can improve the convergence rates by 34% ∼ 99% and 26% ∼ 51% on FMNIST and CIFAR-10, respectively. 1.

Introduction
As a newly emerging distributed learning paradigm, fed-erated learning (FL) [9, 12, 13, 17, 23] has recently attracted attention because of the offered data privacy. FL aims at deal-ing with scenarios where training data is distributed across a number of clients. Considering limited communication band-width and the privacy requirement, in each communication round, FL usually selects only a fraction of clients, and the selected clients will perform multiple iterations of local up-dating without exposing their own datasets [23]. This special
*Corresponding Author scenario also introduces other challenges that distinguish FL from the conventional distributed learning [2, 35].
One major challenge in FL is the high degree of client-wise data heterogeneity [17], which is the inherent charac-teristic of a large number of clients. There have been many studies [10, 15, 16, 18, 20, 25, 27, 32] trying to tackle non-IID (independent and identically distributed) and unbalanced data of the clients in FL. Most of these studies [10,18,20,32] focus on amending the local model updates or the central aggregation based on FedAvg [23].
Recently, active client selection arises as a complement of the aforementioned studies, aiming at accelerating the convergence of FL with non-IID data. Some recent studies propose to assign higher probability of being selected to the clients with larger training loss value [4, 6]. However, they neglect the correlations between the clients and consider their losses independently, which leads to only marginal performance improvement.
In this paper, we propose a correlation-based active client selection strategy that can effectively alleviate the accuracy degradation caused by data heterogeneity and significantly boost the convergence of FL.
Our key idea is mainly based on the following intuitions: 1. Clients do not contribute equivalently. For example, training with a large and balanced dataset on a “good” client can reduce the losses of most clients, while train-ing with a small and extremely biased dataset on a “bad” client may increase the losses of other clients. 2. Clients do not contribute independently. The influence of selecting one client depends on the other selected clients because their local updates will be aggregated.
A toy experiment shown in Fig. 1 also illustrates the ne-cessity of considering the correlations for client selection. In this experiment, each client has only one data sample, and thus each data point in the figure represents a client. The task is to select two clients (different markers represent the client selections of different strategies) for training a binary
selection, which tries to strategically select clients for train-ing in each round in stead of uniformly selecting. Goetz et al. [6] first propose to assign a high selection probabil-ity to the clients with large local loss. Cho et al. [4] select
C clients with the largest loss among a randomly sampled subset A ⊆ U with size d > C to reduce the selection bias.
However, neither of them consider the correlations between clients while making the client selection. 3. Preliminary
FL seeks for a global model w that achieves the best performance (e.g., the highest classification accuracy) on all
N clients. The global loss function in FL is defined as:
L(w) =
N (cid:88) k=1
|Dk| j |Dj| (cid:80) l(w; Dk) =
N (cid:88) k=1 pklk(w), (1) lk(w) = l(w; Dk) = 1
|Dk| (cid:88)
ξ∈Dk l(w; ξ), (2) where l(w; ξ) is the objective loss of data sample ξ evaluated on model w. We refer to lk(w) as the local loss of client k, which is evaluated with the local dataset Dk (of size |Dk|) on client k. The weight pk = |Dk|/ (cid:80) j |Dj| of the client k is proportional to the size of its local dataset.
In consideration of the privacy and communication con-straints, FL algorithms usually assume partial client par-ticipation and perform local model updates.
In particu-lar, in communication round t, only a subset Kt with size
|Kt| = C ≤ N of the overall client set U is selected to receive the global model wt and conduct training with their local dataset for several iterations independently. After the local training, the server collects the trained models from these selected clients and aggregates them (usually by av-eraging [23]) to produce a new global model wt+1. We formulate this procedure as follows: wt+1 k = wt − ηt ˜∇lk(wt), (cid:88) wt+1 k wt+1(Kt) = 1
C
= wt − k∈Kt
ηt
C
˜∇lk(wt), (cid:88) k∈Kt (3) (4) (5) where ηt is the learning rate and ˜∇lk(wt) is the equivalent cumulative gradient [32] in the t-th communication round.
More specifically, for an arbitrary optimizer on the client k, it produces ∆wt,τ k as the local model update at the τ -th iteration in this round, and the cumulative gradient is calculated as ˜∇lk(wt) = (cid:80) k = −ηdt,τ
τ dt,τ k . 4. Methodology
Figure 1. A toy experiment of different client selection strategies. classifier (shown as the lines). The selection strategy that in-dependently selects two clients with the highest local losses (“Ind Result”) fails to reduce the global loss. In contrast, our method considers the correlations between the clients (“Cor
Result”) and derives a client selection that can achieve an almost lowest global loss (“Opt Result”).
Based on the above intuitions, this work proposes Fed-Cor, an FL framework built on a correlation-based client selection strategy, to boost the convergence of FL. Our main contributions are summarized as follows: 1. We model the client loss changes with a Gaussian Pro-cess (GP) and propose an interpretable client selection strategy with a significant reduction of the expected global loss in each communication round. 2. We propose a GP training method that utilizes the co-variance stationarity to reduce the communication cost.
Experiments show that the GP trained with our method can capture the client correlations well. 3. Experimental results demonstrate that FedCor stabilizes the training convergence and significantly improves the convergence rates by 34% ∼ 99% and 26% ∼ 51% on
FMNIST and CIFAR-10, respectively. 2.