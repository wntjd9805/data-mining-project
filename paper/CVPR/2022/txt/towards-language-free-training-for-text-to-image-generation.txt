Abstract
One of the major challenges in training text-to-image generation models is the need of a large number of high-quality image-text pairs. While image samples are often easily accessible, the associated text descriptions typically require careful human captioning, which is particularly
In this paper, we propose the time- and cost-consuming. first work to train text-to-image generation models without any text data. Our method leverages the well-aligned multi-modal semantic space of the powerful pre-trained CLIP model: the requirement of text-conditioning is seamlessly alleviated via generating text features from image features.
Extensive experiments are conducted to illustrate the effec-tiveness of the proposed method. We obtain state-of-the-art results in the standard text-to-image generation tasks.
Importantly, the proposed language-free model outperforms most existing models trained with full image-text pairs. Fur-thermore, our method can be applied in fine-tuning pre-trained models, which saves both training time and cost in training text-to-image generation models. Our pre-trained model obtains competitive results in zero-shot text-to-image generation on the MS-COCO dataset, yet with around only 1% of the model size and training data size relative to the recently proposed large DALL-E model. 1.

Introduction
Automatic synthesis of realistic images from arbitrary text description is one of the core aspirations in artifi-cial intelligence. Most existing works achieve the goal by consuming a large number of high quality image-text pairs [7,38,53,56,59], which, however, often requires heavy workload of precise human captioning and filtering. For in-stance, MS-COCO [27], the most commonly used dataset in text-to-image generation tasks, requires over 70,000 worker
*The research of the first and eighth authors was supported in part by
NSF through grants IIS-1910492. hours in gathering and annotating the captions. Even for less curated datasets such as Google Conceptual Captions
[41], it consists of 3.3 million image-text pairs that are heav-ily filtered from 5 billion images from around 1 billion En-glish webpages. In practice, for a customized domain, it is infeasible to collect such a large number of image-text pairs for model training, due to the high cost of human caption-ing and filtering. This challenge renders the unprecedented importance of the zero-shot text-to-image generation tasks, where no domain-specific image-text pairs are used to train a model to generate images in a given domain.
Recently, several attempts have been made to tackle zero-shot text-to-image generation problem, by pre-training giant generative models on web-scale image-text pairs, such as DALL-E [38] and CogView [7]. Both are auto-regressive
Transformer models built for zero-shot text-to-image gener-ation, as they can generate corresponding images given ar-bitrary text description without training on domain-specific datasets. However, to ensure good performance, these mod-els require a gigantic scale of data collections, model size and model training. Specifically, DALL-E contains over 12 billion parameters and is trained on a dataset consisting of 250 million image-text pairs; CogView is a model with 4 billion parameters trained on 30 million image-text pairs.
For this reason, hundreds of GPUs are required in training these models, which significantly increases carbon footprint and decrease the inclusivity: making it extremely difficult for more researchers to participate the study of this topic.
It is therefore desired to provide affordable solutions to build text-to-image generation models for the settings of limited image-text pair data, by reducing the requirements on model size, data collections and model training. In terms of data collections, in the ideal scenarios, the language-free setting is probably the minimal and cheapest require-ment, where only image data is provided. This is impor-tant because collecting only image data is much easier than constructing high-quality image-text pairs, given the ample domain-specific image datasets available online.
Figure 1. Model size vs performance of zero-shot image-to-text generation on the COCO dataset. LAFITE has much smaller model size, especially when considering trainable parameters (Left figure), but shows higher Inception score (Middle figure) and lower FID (Right figure). Please refer to Section 4 for details.
To this end, we propose LAFITE1
, a generative adver-sarial approach to significantly lowering the cost barrier and to building efficient text-to-image generation models, based on the pre-trained CLIP model [37]. Specifically, (i) we take advantages of CLIP’s property on image-text feature alignment in the joint semantic space, to construct pseudo image-text feature pairs; (ii) we propose a text-to-image
GAN (Generative Adversarial Network) model [11] that can effectively leverage pseudo image-text feature pairs. Our major contributions can be summarized as followings:
• We propose LAFITE, a versatile system that works ef-fectively in a large range of text-to-image generation settings, including language-free, zero-shot and fully-supervised learning.
• To the best of our knowledge, LAFITE is the first work that enables the language-free training for the text-to-image generation task. We propose two novel schemes to construct pseudo image-text feature pairs, and con-duct comprehensive study for the new setting. The effectiveness is validated with quantitative results on several datasets with different training schemes (train-ing from scratch and fine-tuning from pre-trained gen-erative models).
• In zero-shot text-to-image generation settings, LAFITE outperforms the prior art DALL-E and CogView on the COCO benchmark, with less than 1% of the train-able model parameter size (with frozen CLIP model weights). Please see Figure 1 for comparisons.
• In the standard fully supervised settings, LAFITE out-performs several state-of-the-art (SoTA) methods by a large margin. Surprisingly, even our language-free model shows superior performance than most existing models that are trained with full image-text pairs. 2.