Abstract
Continual learning (CL) is under-explored in the video domain. The few existing works contain splits with imbal-anced class distributions over the tasks, or study the prob-lem in unsuitable datasets. We introduce vCLIMB, a novel video continual learning benchmark. vCLIMB is a stan-dardized test-bed to analyze catastrophic forgetting of deep models in video continual learning.
In contrast to previ-ous work, we focus on class incremental continual learn-ing with models trained on a sequence of disjoint tasks, and distribute the number of classes uniformly across the tasks. We perform in-depth evaluations of existing CL meth-ods in vCLIMB, and observe two unique challenges in video data. The selection of instances to store in episodic mem-ory is performed at the frame level. Second, untrimmed training data influences the effectiveness of frame sampling strategies. We address these two challenges by proposing a temporal consistency regularization that can be applied on top of memory-based continual learning methods. Our approach significantly improves the baseline, by up to 24% on the untrimmed continual learning task. The code of our benchmark can be found at: https://vclimb.netlify.app/. 1.

Introduction
Deep neural networks rely on large-scale datasets to achieve state-of-the-art performance on modern computer vision tasks [6, 10, 18, 20]. A significant result of such pre-training is enabling feature reuse [12, 41], by means of fine-tuning the learned weights for smaller downstream tasks
[3,22,28]. Due to legal or technical constraints, and the fact that labeling data is expensive and time-consuming [34], real-world deep-learning pipelines would rarely involve a single fine-tuning stage. Instead, these pipelines could re-quire the sequential fine-tuning of large models in a set
∗Work done during an internship at KAUST. of independent tasks that are learned sequentially. Under these conditions, deep neural networks suffer from what is known as catastrophic forgetting [13], where the fine-tuning on novel tasks significantly reduces the performance of the model in a previously learned task, and drift [29], where un-seen training data does not fit the previously estimated class distribution. Continual learning [11] directly models such a scenario, by adapting a neural network model into a sequen-tial series of tasks. We focus on a special case of CL: class incremental learning (CIL), where the labels and data are mutually exclusive between tasks, training data is available only for the current task, and there are no tasks ids.
With 500 hours of video from diverse categories up-loaded to YouTube every minute and a billion people ac-tively using TikTok every month [4, 36], video content of varying quality is available at an unprecedented scale. With such large volumes of data, it is important to develop mod-els that can effectively learn from continuous streams of untrimmed video data. Remarkably, few research efforts have addressed continual learning with video [24, 26, 44].
Despite these current works, video continual learning meth-ods still show a large variability in their experimental pro-tocols, making direct comparisons hard to establish. These protocols present the following limitations. (1) They are not publicly available. (2) They do not explore a realistic setup with untrimmed videos. (3) Most of them leverage a large pretraining step, which warms up the model by learning a sample of classes from the same distribution and is not al-ways available in a real continual learning scenario.
We directly address these limitations and propose vCLIMB (video CLass IncreMental Learning Benchmark), a novel benchmark devised for the evaluation of continual learning in video. Our test-bed defines a fixed task split on the original training and validation sets of three well known video datasets: UCF101 [33], ActivityNet [5] and
Kinetics [6]. vCLIMB follows the standard class incremen-tal continual learning scenario, but includes some modifi-cations to better fit the nature of video data in human ac-tion recognition tasks. First, to achieve fair comparisons between video CIL methods that use memory, we re-define memory size [27, 38] to be the total number of frames, in-stead of the total number of video instances, that the mem-ory can store. We report this as Memory Frame Capacity in our tables to avoid confusion with the memory size de-fined in image CIL. This means we are not only concerned about selecting the best videos to store in memory, but we also want to identify the best set of frames to keep in mem-ory. Second, since fine-grained temporal video annotations are expensive (especially for long videos), we analyze the effect of using trimmed and untrimmed video data in con-tinual learning. To the best of our knowledge, this is the first work to explore continual learning with untrimmed videos.
Using vCLIMB’s data splits, we establish an initial set of baselines by adapting well-known continual learning meth-ods [1, 17, 27, 38] from the image recognition domain into the activity recognition domain. After benchmarking these baseline methods, we propose a novel strategy for video continual learning that leverages the inherent temporal con-sistency in video data to better approach the continual action recognition problem. We believe that vCLIMB’s standard-ized approach to prototyping and evaluating video continual learning will enable future works in this area.
Contributions. This paper proposes vCLIMB, a novel benchmark for continual learning in video, which focuses on the activity classification task. Our work brings the fol-(1) A standardized benchmark for lowing contributions: continual learning in video action recognition, which de-fines the training protocols and associated metrics for three video datasets; this novel continual learning setup includes a more realistic combination of trimmed and untrimmed videos. (2) We re-purpose and evaluate four baseline meth-ods from the image domain into the video domain. (3) We present a novel strategy based on consistency regularization that can be built on top of memory-based methods to reduce memory consumption while improving performance. 2.