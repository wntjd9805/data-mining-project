Abstract
Dense correspondence between humans carries powerful semantic information that can be utilized to solve funda-mental problems for full-body understanding such as in-the-wild surface matching, tracking and reconstruction. In this paper we present BodyMap, a new framework for obtain-ing high-definition full-body and continuous dense corre-spondence between in-the-wild images of clothed humans and the surface of a 3D template model. The correspon-dences cover fine details such as hands and hair, while capturing regions far from the body surface, such as loose clothing. Prior methods for estimating such dense surface correspondence i) cut a 3D body into parts which are un-wrapped to a 2D UV space, producing discontinuities along part seams, or ii) use a single surface for representing the whole body, but none handled body details. Here, we intro-duce a novel network architecture with Vision Transformers that learn fine-level features on a continuous body surface.
BodyMap outperforms prior work on various metrics and datasets, including DensePose-COCO by a large margin.
Furthermore, we show various applications ranging from multi-layer dense cloth correspondence, neural rendering with novel-view synthesis and appearance swapping. 1.

Introduction
Several fundamental problems related to human un-derstanding in images can be addressed by labeling every pixel covering the human body with semantic information.
This enables numerous applications including video anal-ysis, image editing, texture generation and style transfer.
From a single RGB image of a human, the literature has proposed methods to extract sparse information such as 2D body keypoints (e.g., face, hands, body joints), or 2D seg-mentation masks (e.g., for full body, clothes, hair or skin), and also 3D body pose and shape parameters defined by a template body model [6, 8, 13, 33, 35, 44], while work on dense surface correspondence has further enabled pixel-level understanding by establishing unique correspondences
*This work was conducted during an internship at Meta RLR.
Figure 1. We introduce BodyMap — a method that establishes accurate dense correspondences between a 2D image and the sur-face of a 3D clothed human with high precision. Our approach handles loose clothes, different hairstyles and various accessories, like hats and bags, providing crisp silhouettes, and works well in multi-person cases with occlusions. between 2D pixels covering the visible regions of the human body and 3D points on the surface of a body template.
In the seminal work DensePose [26], correspondences are estimated between image pixels belonging to the hu-man body and points in disjoint parts of a human body tem-plate located using UV coordinates, similar to local texture mapping. The method is trained on the large in-the-wild dataset DensePose-COCO and is robust to human pose vari-ability, image resolution, diversity in clothing, and occlu-sions. However, it has some inherent limitations that im-pact methods that rely on it (e.g., for clothed-human ap-plications) [1, 21, 46]. First, the discretization generated by
dividing the body into disjoint parts produces clearly visible seams and discontinuities between them that are not optimal for learning models. Second, the DensePose estimates suf-fer from inaccuracy as reported in prior work [26, 28, 39], mainly due to the difficulty in acquiring ground-truth anno-tations for the task [2, 23]. Follow-up methods have tackled some of its shortcomings and a few recent works addressed the discontinuity of UV maps [4, 27, 48]. HumanGPS [41] proposes to predict per-pixel embeddings using geodesic distances between corresponding points on the surface of a 3D human scan and does not produce an explicit map-ping. None of the proposed approaches has established high-definition correspondences for areas with finer details such as hair and hands (with fingers), with generalization to clothed humans, especially with loose clothing.
In this work we introduce a novel technique to establish high-definition full-body and continuous dense correspon-dence between images of clothed humans and the human body surface. Our method, which we term as BodyMap, takes as input an RGB image of a human and outputs ac-curate per-pixel continuous correspondence estimates for each foreground pixel (i.e. including the full body, with clothes and hair). We designed a transformer-based ar-chitecture that learns appearance-based and Continuous-Surface-Embeddings-based representations to infer accu-rate dense surface correspondence for the depicted human.
Our variant of Vision Transformer [11] as a computational block of the encoder brings its advantageous properties for dense prediction tasks. The vector dimension is kept con-stant throughout all processing stages as well as global re-ceptive field for every stage. With these properties, our net-work is well designed for dense correspondence prediction.
Furthermore, we capitalize on the power of synthetic data. Since no real-world dataset provides ground-truth an-notation at the quality we aim for (fingers, clothes, hair), we created a synthetic dataset of animated 3D clothed hu-man scans.
In that way, we obtained ground-truth dense correspondence for a large variety of humans with diverse clothing, in different poses and from different viewpoints.
A differentiating factor of our framework is that it is not tied to a human body with topology constraints, and can handle layered representations such as humans with separate cloth geometries. To summarize, our key contributions are:
• BodyMap is the first method to establish dense continu-ous correspondence for every foreground pixel of clothed humans, whether that is fingers, hair, or clothes that are displaced from the human body with high-precision — something that all prior works fail to achieve.
• A novel transformer-based architecture designed specifi-cally for this task that when trained in a multi-task learn-ing manner with per-pixel classification losses for each channel significantly outperforms prior works across sev-eral datasets and tasks.
• We achieve state-of-the-art results on DensePose COCO by a large margin. We show our approach can be applied to real-world applications such as novel view synthesis.
Our method can be extended to learn layered representa-tions with clothed humans and predict per-geometry sur-face correspondences. 2.