Abstract an algorithm that generates visual patterns:
Synthesis of ergodic, stationary visual patterns is widely applicable in texturing, shape modeling, and digital con-tent creation. The wide applicability of this technique thus requires the pattern synthesis approaches to be scalable, di-verse, and authentic. In this paper, we propose an exemplar-based visual pattern synthesis framework that aims to model the inner statistics of visual patterns and generate new, ver-satile patterns that meet the aforementioned requirements.
To this end, we propose an implicit network based on gen-erative adversarial network (GAN) and periodic encoding, thus calling our network the Implicit Periodic Field Net-work (IPFN). The design of IPFN ensures scalability: the implicit formulation directly maps the input coordinates to features, which enables synthesis of arbitrary size and is computationally efficient for 3D shape synthesis. Learn-ing with a periodic encoding scheme encourages diversity: the network is constrained to model the inner statistics of the exemplar based on spatial latent codes in a periodic field. Coupled with continuously designed GAN training procedures, IPFN is shown to synthesize tileable patterns with smooth transitions and local variations. Last but not least, thanks to both the adversarial training technique and the encoded Fourier features, IPFN learns high-frequency functions that produce authentic, high-quality results. To validate our approach, we present novel experimental re-sults on various applications in 2D texture synthesis and 3D shape synthesis. 1.

Introduction
The synthesis of visual patterns, may that be a wooden texture for painting, or a simulation of natural cave systems, is a technique that is applied ubiquitously in computer-aided design and digital content creation. Visual patterns can be understood as arts, shapes, or natural textures following cer-tain geometric structures. In an application context, let us start by defining several characteristics that are desirable for
• Authenticity. Probably the most prioritized quality of synthesized visual patterns is its visual quality. When patterns are synthesized from an exemplar, the quality is determined by whether they faithfully recreate the source pattern.
• Diversity. It would be undesirable for a synthesizer to only copy patterns from the source. Diversity is thus an equally important measurement that evaluates whether the synthesized patterns vary from the source and each other. We strive to achieve two different levels of di-versity: the patterns should be diversified both within a generated sample and across samples.
• Scalability. As patterns are usually demanded at dif-ferent and potentially large scales for many practical applications, we want a scalable synthesizer to be able to efficiently generate patterns of arbitrary size. Scala-bility is particularly valuable when it comes to the syn-thesis of 3D models, as the extra dimension translates to a much larger amount of computations.
A scalable design choice leads us to formulate the syn-thesis problem as generating patterns from a continuous, real coordinate space. This is generally known as the im-plicit formulation, where a nonlinear function maps points defined in R2 or R3 to features that represent the synthe-sized subjects. In particular, the implicit function has been shown to be an efficient representation for synthesizing 3D volumes [12, 22, 24].
Patterns that scale well to an infinitely large space, in general, possess a stationary property - a shift-invariant structure that can be expanded by tiling or stacking blocks of elements. We therefore develop our method by pivot-ing on the fact that many types of natural and artistic pat-terns can be analyzed and recreated in a stationary frame-work. The goal of synthesizing an authentic and diverse
stationary pattern from an exemplar, however, requires care-ful modeling that is compatible with the underlying struc-ture of the pattern.
Generative adversarial networks (GAN) [9] is one of the most promising techniques so far to model data distribution in an unsupervised manner and has been frequently adapted to convolutional models that synthesize visually authentic images [2, 14, 31, 32, 37, 42]. How would a GAN gener-ator be leveraged to modeling stationary pattern? As all stationary patterns contain a repeating structure with local variations that “vivifies” its appearance, in an ideal situa-tion, a stationary pattern can be modeled by a discrete ran-dom field, where each random variable is associated with a patch of the basic element. Thus a natural GAN for-mulation models image patches with a spatially defined la-tent field [2, 14]. In a convolutional framework, however, problems arise when fake samples generated from a dis-crete noise image are discriminated from randomly sampled patches from a real image. The first problem is that the sam-pled patch does not necessarily agree with the scale of the repeating structure. The second problem is that the sam-pled patch can be arbitrarily shifted from the center of a sta-tionary element. A typical deconvolutional network [6, 39] that upsamples from an evenly-spaced noise image may not sufficiently address the previously mentioned problems. To study their effects we designed a convolutional network fol-lowing the DCGAN architecture [29] to synthesize a hon-eycomb pattern from a 2 × 2 noise map, which is trained against random patches sampled from a source image. The comparison between its result and that synthesized by our generator network that is trained with an identical discrim-inator is shown in Figure. 1. We found that the noise map does not capture well the honeycomb structure as seams and intercepting elements are visible from the synthesized im-age.
Though various techniques have been proposed in the past to address the aforementioned issues (e.g. [2, 14]), in this paper, we consider a more natural way to model sta-tionary patterns with an implicit periodic generator. The core of the formulation is to match the repeatable structure of a stationary pattern to the period of a learnable continu-ous periodic function. Instead of modeling the pattern with a discrete noise tensor, we define latent variables in a contin-uous space, where the extent of each latent factor is learned to match with the extent of the repeating elements. The ben-efits of this design align well with the desirable character-istics for visual pattern synthesis: 1) learned periodicity of the implicit field encourages latent factors to model the sta-tionary variations observed from the exemplar pattern; 2) a continuous representation provides flexibility during train-ing to learn a distribution from randomly shifted patches cropped from the exemplar; 3) a Fourier encoding scheme learns high-frequency details from the exemplar. This al-Figure 1. Comparison between synthesized honeycomb from a DCGAN convolution generator and the periodic MLP generator. Seams and inter-cepting patterns are visible in the former result due to difficulty for the convolution generator to capture the repeating structure. lows our model to synthesize visually authentic results, and 4) multilayer perceptron (MLP) that implicitly takes coor-dinates as input scales well to the generation of 3D shapes when compared to 3D convolution. Based on these design choices, we term our network the Implicit Periodic Field
Network (IPFN).
We validate our proposed design by showing various ap-plications in texture image synthesis and 3D volume syn-thesis in Section. 4. Specifically, besides synthesizing sta-tionary patterns, we design a conditional formulation of our model to tackle the synthesis of directional patterns and to provide application in controllable shape generation. An ablation study is also conducted to verify the effectiveness of our design choices. 2.