Abstract
In this paper, we propose a simple yet effective video super-resolution method that aims at generating high-fidelity high-resolution (HR) videos from low-resolution (LR) ones. Previous methods predominantly leverage tem-poral neighbor frames to assist the super-resolution of the current frame. Those methods achieve limited performance as they suffer from the challenges in spatial frame align-ment and the lack of useful information from similar LR neighbor frames.
In contrast, we devise a cross-frame non-local attention mechanism that allows video super-resolution without frame alignment, leading to being more robust to large motions in the video. In addition, to acquire general video prior information beyond neighbor frames, and to compensate for the information loss caused by large motions, we design a novel memory-augmented attention module to memorize general video details during the super-resolution training. We have thoroughly evaluated our work on various challenging datasets. Compared to other re-cent video super-resolution approaches, our method not only achieves significant performance gains on large mo-tion videos but also shows better generalization. Our source code and the new Parkour benchmark dataset is available at https://github.com/jiy173/MANA. 1.

Introduction
Video super-resolution (VSR) task aims to generate high-resolution (HR) videos from low-resolution (LR) in-put videos and recover high frequency details in the frames.
It is attracting more attention due to its potential application in online video streaming services and the movie industry.
There are two major challenges in the VSR tasks. The first challenge comes from the dynamic nature of videos.
To ensure temporal consistency and improve visual fidelity, previous methods generally seek to fuse information from multiple neighbor frames. Due to the motion across frames, neighbor frames need to be aligned before fusion. Recent works have proposed various ways for aligning neighbor frames to the current frame, either by explicit warping us-ing optical flow [2,17,21,28] or learning implicit alignment using deformable convolution [29, 32]. However, the qual-ity of these works highly depends on the accuracy of spatial
Figure 1. Our memory-augmented cross-frame non-local attention approach is robust to large motion videos (first row). Our method reconstructs visually pleasing details on repetitive patterns (left ex-ample) and thin structures (right example) while other video super-resolution methods fail in these cases. Best viewed in PDF. alignment of neighbor frames, which is difficult to achieve in videos with large motions. As an example shown in Fig. 1 column (a), the method EDVR [32] and TOFlow [37] fail in the scenario of large motions due to fusing misaligned frames. This hinders the application of existing VSR meth-ods in real-world videos such as sports videos (see our Park-our dataset in Sec. 4.1), and entertainment videos from ani-mation, movies and vlogs.
The second challenge comes from the irreversible loss of high-frequency detail and the lack of useful information in the low-resolution video. Recent learning based single im-age super-resolution (SISR) works [5, 12, 13, 16, 18, 26, 30, 34,38,49] have intensively studied the visual reconstruction from LR images by learning general image prior to help re-cover high-frequency details or transferring texture from an
HR reference image. One straightforward solution for video super-resolution (VSR) is to directly apply SISR methods to each frame, but it does not guarantee temporal consistency in the visual appearance. Instead, most VSR methods try to fuse information from neighbor frames for HR frame recon-struction, and thus generate results superior to SISR meth-ods. However, we argue that the information acquired from neighbor frames is still limited, especially for videos with large motions. In such a scenario, the correlations among neighbor frames become smaller due to less similar neigh-bor frames, which makes it difficult to mine useful informa-tion from neighbor frames. As a result, the VSR essentially degrades to the single image super-resolution.
To address the aforementioned challenges, we propose a
Memory-Augmented Non-local Attention (MANA) frame-work for video super-resolution (VSR). Our MANA takes a set of consecutive low-resolution video frames as inputs, and produces the high-resolution version of the temporal center frame by referring to the information from its neigh-bor frames. Since consecutive frames share a large portion of visual contents, this scheme implicitly ensures tempo-ral consistency in the result. But most importantly, MANA consists of two novel modules, which are specifically de-signed for solving the VSR challenges.
To solve the frame-alignment challenge, we design the
Cross-Frame Non-local Attention module which allows us to fuse neighbor frames without aligning them towards the current frame. Conventional non-local attention [33] com-putes the pair-wise correlation between each pixel in the query and key. In the video super-resolution (VSR) case, however, it is improper to treat pixels in all spatial loca-tions equally like conventional non-local attention. We ob-serve that the pixels near the query are more likely to be good correspondences thanks to the nature of continuity.
Therefore, unlike conventional non-local attention, we em-ploy a trainable Gaussian map centered at the query pixel to weight the correlations. This is helpful for keeping a good balance between all information sources, and effectively re-duces mistaken correspondences that will negatively affect the accuracy of super-resolution. The Gaussian weighted cross-frame non-local attention enables our work to cir-cumvent the frame-alignment operation, which usually per-forms poorly in videos with large motions. As an example,
Fig. 1(a) illustrates that our method can reconstruct sharp details like the stripes on the roof and the waving arm in fast-moving frames.
To solve the challenge of the lack of information from neighbor frames, we seek to fuse useful video prior in-formation beyond the current video. This means that the network should memorize previous experiences in super-resolving other videos in the training set. Based on this prin-ciple, we introduce a Memory-Augmented Attention module to our network. In this module, we maintain a 2D memory bank which is completely learned during the training. The purpose is to summarize the representative local details in the entire training set and use them as an external reference for super-resolving the current video frame. To our knowl-edge, our work is the first VSR method that leverages the memory bank mechanism to incorporate information be-yond the current video. Thanks to the general video prior captured by this module, our method can recover details that are missing in the LR video like the balcony railings shown in Fig. 1(b).
To verify the superiority of our MANA method on videos with large motions, we collect the Parkour bench-mark dataset. Both qualitative and quantitative results on this dataset have demonstrated that our MANA signifi-cantly outperforms all previous approaches.
In addition, we also evaluate MANA on other public datasets includ-ing Vimeo90K [37], SPMC [28], and Vid4 [21]. Our ap-proach still achieves better or very competitive results. It is worth noting that MANA shows better generalization, because it is superior to other approaches on SPMC and
Parkour datasets, which are very different from the training dataset Vimeo90K.
To summarize, our contributions include the follows:
Cross-frame non-local attention. We introduce a Gaus-sian weighted cross-frame non-local attention that liberates the video super-resolution from the error-prone frame align-ment process, and effectively balances the local and non-local information sources. This design makes our method robust to videos with large motions (See Sec. 3.2).
Video super-resolution beyond current video. To the best of our knowledge, we are the first to leverage the memory-augmentation attention to incorporate general video prior to assist current video super-resolution. (See Sec. 3.3).
New benchmark for large motion video super-resolution. We introduce the Parkour dataset containing large motion videos. To our knowledge, this is the first benchmark for evaluating VSR methods in large motion cases (See Sec. 4.1). 2.