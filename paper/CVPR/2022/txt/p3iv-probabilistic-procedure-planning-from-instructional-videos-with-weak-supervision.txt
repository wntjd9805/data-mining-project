Abstract
In this paper, we study the problem of procedure plan-ning in instructional videos. Here, an agent must pro-duce a plausible sequence of actions that can transform the environment to a desired goal from a given start state. When learning procedure planning from instructional videos, most recent work leverages intermediate visual ob-servations as supervision, which requires expensive anno-tation efforts to localize precisely all the instructional steps in training videos. In contrast, we remove the need for ex-pensive temporal video annotations and propose a weakly supervised approach by learning from natural language in-structions. Our model is based on a transformer equipped with a memory module, which maps the start and goal ob-servations to a sequence of plausible actions. Furthermore, we augment our model with a probabilistic generative mod-ule to capture the uncertainty inherent to procedure plan-ning, an aspect largely overlooked by previous work. We evaluate our model on three datasets and show our weakly-supervised approach outperforms previous fully supervised state-of-the-art models on multiple metrics. 1.

Introduction
Procedure planning is a natural task for humans – one must plan out a sequence of actions that takes one from the current state to the desired goal. While effortless for hu-mans, procedure planning is notoriously hard for artiﬁcial agents. Nevertheless, solving procedure planning is of great importance for building next-level artiﬁcial intelligence sys-tems capable of analyzing and mimicking human behaviour, and eventually assisting humans in goal-directed problem solving, e.g., cooking, assembling furniture or tasks that can be represented as a clear set of instructions. Tradition-ally, procedure planning has been addressed in structured environments, such as object manipulation on a table sur-face [13, 43]. While restricting the environment helps im-prove planning, it also limits the range of possible applica-Figure 1. Illustration of weak language supervision for procedure planning. Fully supervised approaches (bottom row) learn models from step labels, ai, and intermediate visual representations, vi, over T ﬁnite steps. This strategy requires knowing the starting, si, and ending, ei, timestamps, for each intermediate step. In contrast, our approach (top row) exploits natural language representations, li, of the intermediate labels, ai, as a surrogate supervision, which only requires labeling the order of events. Note that the action label, ai, is a discrete variable, whereas the action language repre-sentation, li, is a pre-trained continuous embedding. tions. Here, we follow more recent work [8] and tackle pro-cedure planning in the realm of instructional videos [46,54].
Given visual observations of the start and goal states, the task is to predict a sequence of high-level actions needed to achieve a goal; see Fig 1. This task is particularly challeng-ing as it requires parsing unstructured environments, rec-ognizing human activities and understanding human-object interactions. Yet, the range of applications for such planners is broad, which motivates research efforts on this problem.
Current approaches for procedure planning from instruc-tional videos share a serious limitation – reliance on strong supervision with expensive annotations [6, 8, 45]. Speciﬁ-cally, all such methods require access to (i) a list of action labels used to transition from start to the goal state and (ii)
the visual representation of the intermediate states. Using such intermediate visual representations entails very expen-sive annotation of the start and end times of each interme-diate instructional step; see Fig 1 (bottom).
In contrast, our work removes the need for intermediate visual states during training and instead uses their linguistic representa-tion for supervision. Relying on language representations allows us to better leverage instructional videos and sig-niﬁcantly reduce the labeling effort; see Fig. 1 (top). For example, language annotations for the intermediate instruc-tional steps could be extracted from general procedure de-scriptions available in recipes or websites, e.g., WikiHow
[21]. In contrast, to obtain the timestamps for intermediate instructional steps one must watch the entire instructional video. In addition, a language representation can be a more stable supervisory signal [40], as the description of a given step (e.g., add seasoning) remains the same, while its visual observation varies across different videos.
Previous work on procedure planning from video relies on a two-branch autoregressive approach while adopting different architectures to model these branches [6, 8, 45]. In such models, one branch is dedicated to predicting actions based on the previous observation, while the other approxi-mates the observation given the previous action in a step-by-step manner. Such models are cumbersome and compound errors, especially for longer sequences. In contrast, we rely on a single branch non-autoregressive model, implemented as a transformer [47] that generates all intermediate steps in parallel conditioned on the start and goal observations.
Another important factor in procedure planning is to model the uncertainty inherent to the prediction task. For example, given a set of ingredients and the goal of making a pancake, the intermediate steps could be either (i) [add wet ingredients −→ add dry ingredients −→ whisk mixture] or (ii) [add dry ingredients −→ add wet ingredients −→ whisk mixture]. This example shows that in realistic scenarios some plans can vary even under a shared common goal.
This observation is usually handled in physical path plan-ning tasks (e.g., robotic arms are allowed to follow multiple feasible trajectories [43]); yet, effort is lacking on proba-bilistic modeling of procedure planning from instructional videos. While previous work included a probabilistic com-ponent at training time [6], we are the ﬁrst to use and bene-ﬁt from multiple plausible plans at inference. We explicitly handle uncertainty in procedure planning with a dedicated generative module that can produce multiple feasible plans.
In summary, the main technical con-tributions of our work are threefold. (i) We introduce a weakly supervised approach for procedure planning, which leverages powerful language representations extracted from pre-trained text-video embeddings. (ii) We tackle the task with a simpler single branch model, which can generate all intermediate steps in parallel, rather than relying on
Contributions. the two-branch auto-regressive approach used in previous work. (iii) We propose a generative adversarial framework, trained with an extra adversarial objective, to capture the stochastic property of planned procedures. We evaluate our approach on three widely used instructional videos datasets and show state-of-the-art performance across different pre-diction time horizons, even while relying on weaker su-pervision. We also show the advantage of modeling un-certainty. Our code is available at: https://github. com/SamsungLabs/procedure-planning. 2.