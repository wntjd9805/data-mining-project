Abstract
In this paper, we introduce a novel deep learning method for photo-realistic manipulation of the emotional state of actors in “in-the-wild” videos. The proposed method is based on a parametric 3D face representation of the actor in the input scene that offers a reliable disentanglement of the facial identity from the head pose and facial expressions. It then uses a novel deep domain translation framework that alters the facial expressions in a consistent and plausible manner, taking into account their dynamics. Finally, the al-tered facial expressions are used to photo-realistically ma-nipulate the facial region in the input scene based on an especially-designed neural face renderer. To the best of our knowledge, our method is the first to be capable of con-trolling the actor’s facial expressions by even using as a sole input the semantic labels of the manipulated emotions, while at the same time preserving the speech-related lip movements. We conduct extensive qualitative and quanti-tative evaluations and comparisons, which demonstrate the effectiveness of our approach and the especially promising results that we obtain. Our method opens a plethora of new possibilities for useful applications of neural render-ing technologies, ranging from movie post-production and video games to photo-realistic affective avatars. 1.

Introduction
Photo-realistically manipulating faces in images or videos has received substantial attention lately, with impres-sive results that expand the range of creative video editing, content creation and the VFX industry. Yet, when it comes to altering the facial emotion in videos, existing techniques exhibit severe limitations. The importance of this type of manipulation is clearly illustrated when shooting movies, as capturing the desired actor’s emotion typically requires multiple efforts, despite the uttered words being predefined.
A robust solution to emotion editing would conveniently place the manipulation of facial performance in the post-production stage.
In the past, the problem has been tackled by assuming that different recordings of the same script being acted in
multiple emotions are available; hence, enabling to switch or blend between takes, after perfect synchronization has been achieved [33]. However, in a more realistic sce-nario, one would like to make, e.g., a neutral actor look happy, without using pre-existing footage. Combining per-formances from unpaired data is much more challenging.
Recently, image-to-image translation has been success-fully applied to emotion editing by casting the problem in the image space [7, 9]. These methods deal with static im-ages, where altering the mouth shape (e.g. opening a closed mouth to show surprise) is acceptable, if not desired. How-ever, without placing any specific constraint on the mouth region, lip synchronization may be lost when they are di-rectly applied to video sequences.
More related to this task, is the face reenactment prob-lem, where the facial performance of a source actor is trans-ferred to a target one, making the latter mimic the expres-sions of the former. State-of-the-art techniques [13, 23, 51] achieve compelling photo-realism by training a neural ren-derer conditioned on a facial representation (e.g. 3DMMs).
Nonetheless, this is substantially different from semanti-cally controlling the target actor, since the expressions are merely copied from another subject. Instead, we would like to edit the actor’s own expressions based on the desired emotion, while preserving the mouth motion. Recent meth-ods address only one aspect of this problem. For instance,
DSM [39] generates novel expressions based on emotional labels without retaining the original speech, while [22] pre-serves mouth movements, but the proposed manipulation is limited to matching a single target speaking style.
In this work, we propose a hybrid method, in which a parametric 3D face representation is translated to different domains, and then used to drive synthesis of the target face by means of a video-based neural renderer. Our method, which we call Neural Emotion Director (NED), achieves photo-realistic manipulation of the emotional state of actors in “in-the-wild” videos, see e.g. Fig. 1. It can translate a facial performance to any of the 6 basic emotions (angry, happy, surprise, fear, disgust, sadness) plus neutral, using only as input its semantic label, while retaining the original mouth motion. It also allows to attach a specific style to the target actor, without requiring person-specific training. This means that the reference style can be extracted at test time from any given video: our system can, for example, make
Robert De Niro yell in the way of Al Pacino, without ever seeing footage of the latter during training. Our contribu-tions can be summarized as follows:
• To the best of our knowledge, we propose the first video-based method for “directing” actors in “in-the-wild” condi-tions, by translating their facial expressions to multiple un-seen emotions or styles, without altering the uttered speech.
• We introduce an emotion-translation network, which we call 3D-based Emotion Manipulator, that receives a se-quence of expression parameters and translates them to a given target domain or a reference style and is trained on non-parallel data. We train this network on 2 large video databases annotated with emotion labels.
• We design a video-based face renderer, to decode the para-metric representation back to photo-realistic frames. Build-ing upon robust, state-of-the-art face editing techniques (face segmentation, alignment, blending) we modify only the face area, while the background remains unchanged, making it possible to manipulate challenging scenes.
• We conduct extensive qualitative and quantitative exper-iments, user studies and ablation studies to evaluate our method and compare it with recent state-of-the-art methods.
The experiments demonstrate the effectiveness and advan-tages of our method, which achieves promising results in very challenging scenarios as the ones encountered in movie scenes with moving background objects.
• We release our code and trained models [1]. 2.