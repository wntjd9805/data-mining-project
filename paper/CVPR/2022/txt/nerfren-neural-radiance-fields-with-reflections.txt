Abstract
Neural Radiance Fields (NeRF) has achieved unprece-dented view synthesis quality using coordinate-based neu-ral scene representations. However, NeRF’s view depen-dency can only handle simple reflections like highlights but cannot deal with complex reflections such as those from glass and mirrors.
In these scenarios, NeRF models the virtual image as real geometries which leads to inaccurate depth estimation, and produces blurry renderings when the multi-view consistency is violated as the reflected objects may only be seen under some of the viewpoints. To over-come these issues, we introduce NeRFReN, which is built upon NeRF to model scenes with reflections. Specifically, we propose to split a scene into transmitted and reflected components, and model the two components with separate neural radiance fields. Considering that this decomposition is highly under-constrained, we exploit geometric priors and apply carefully-designed training strategies to achieve reasonable decomposition results. Experiments on various self-captured scenes show that our method achieves high-quality novel view synthesis and physically sound depth es-timation results while enabling scene editing applications. 1.

Introduction
Photorealistic novel view synthesis (NVS) from unstruc-tured image collections is crucial to creating immersive virtual experiences. Despite having achieved significant progress in controlled settings, challenges still exist in han-dling light transport at the surface of different materials. For example, reflections caused by glass or mirrors commonly exist in real-world scenes, posing great difficulties for novel view synthesis due to the severe view-dependent effects.
Neural Radiance Fields (NeRF) [12], as an emerging technique in this field, has achieved impressive view syn-thesis quality by adopting volumetric representations with coordinate-based neural networks. By conditioning radi-*Research done when Yuan-Chen Guo was an intern at Tencent AI Lab.
†Corresponding author: hooyeeevan2511@gmail.com.
Figure 1. NeRF models the stable reflection image as real ge-ometries rather than view-dependent effect of the points on the re-flective surface. We illustrate two problems NeRF may encounter when modeling scenes with complex reflections: (1) inaccurate depth estimation in reflective regions, as in (a) and (b); (2) inac-curate rendering when the multi-view consistency is violated, as in the magnified area in (b). NeRFReN tackles these problems by modeling the transmitted and reflected component of the scene with separate NeRFs, and synthesizing views in the image domain (bottom). See Sec. 3.1 for more detailed analysis. ance on both coordinates and viewing directions, NeRF is able to faithfully handle view-dependent effects like high-lights. However, in the presence of severe reflections that contain a stable virtual image other than highlights, NeRF tends to model the scene geometry behind the reflective sur-face as translucent (like fogs) and regard the reflected ge-ometries as appearing at some virtual depth. The reason is that NeRF does not explain the view-dependency of a reflecting surface point by changing the color of this very point when viewed from different directions, but explain it by utilizing all the spatial points behind it along the cam-era ray to get the correct color through volume rendering as in Eq. (2), resulting in foggy geometries and physically wrong depth. Although this works well for view synthesis in certain scenarios, there are two inherent limitations: (1)
it is hard to estimate the correct scene geometry, prevent-ing it from further explanations or editing; (2) inaccurate renderings are generated when the multi-view consistency does not hold since the reflected objects may only be seen in some of the images, which limits the views NeRF can correctly synthesize.
To this end, we propose NeRFReN, which enhances
NeRF to handle scenes with ReflectioNs. Instead of repre-senting the whole scene with a single neural radiance field, we propose to model the transmitted and reflected parts of the scene with separate neural radiance fields. To synthesize novel views, the transmitted image It and reflected image Ir rendered by the corresponding radiance fields are composed in an additive way, where the reflected image Ir is weighted by a learned reflection fraction map β:
To summarize, our contributions are:
• We analyze NeRF’s behavior and limitations in modeling reflective regions, and propose to use separate transmit-ted and reflected neural radiance fields for scenes with complex reflections.
• We devise specific network architectures, exploit geomet-ric priors, and apply carefully-designed warm-up training strategy, to achieve physically sound decomposition re-sults with competitive novel view synthesis quality com-pared to vanilla NeRF on several real world scenes, in-cluding challenging cases like scenes with mirrors.
• We also investigate depth estimation and scene editing as further applications based on the decomposed results.
I = It + βIr (1) 2.