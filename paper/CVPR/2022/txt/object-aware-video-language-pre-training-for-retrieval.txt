Abstract
Recently, by introducing large-scale dataset and strong transformer network, video-language pre-training has shown great success especially for retrieval. Yet, existing video-language transformer models do not explicitly fine-grained semantic align. In this work, we present Object-aware Transformers, an object-centric approach that ex-tends video-language transformer to incorporate object representations. The key idea is to leverage the bounding boxes and object tags to guide the training process. We evaluate our model on three standard sub-tasks of video-text matching on four widely used benchmarks. We also provide deep analysis and detailed ablation about the pro-posed method. We show clear improvement in performance across all tasks and datasets considered, demonstrating the value of a model that incorporates object representations into a video-language architecture. The code has been re-leased in https://github.com/FingerRec/OA-Transformer. 1.

Introduction
Learning scalable video-text representations for retrieval requires the understanding of both visual and textual clues, as well as the semantic alignment between these two modal-ities.
Large-scale contrastive-based pre-training meth-ods [4, 19] dominate the recent literature, where a “dual-encoder” framework (a video encoder and a text encoder) is trained in an end-to-end manner. Although these methods have led to great performance advances, we figure out that the lack of regularization on fine-grained semantic associa-tions hinders their further improvements.
Thanks to the great progress of image-text pre-training
[9, 21, 22, 24, 35, 37, 44], a series of methods attempt to leverage an off-the-shelf object detection model to gener-ate richer information for cross-modality understanding, in-*Corresponding Author.
Figure 1. (a). Masking object-irrelevant region keep the se-mantic unchanged. From this example, we observe: 1. The ob-ject region is highly overlapped with visual salient region. 2. The predicted Object Tags has semantic relation with caption. e.g.,
Music and Ratio. Laptop and Computer. (b). Our method vs.
SOTA on three downstream tasks. Motivated by (a), by incor-porating the object into the learning of video-language pretraining with simple Object-guided Masking, we show promising results over multiple downstream video-language tasks. cluding the visual objects and their tag concepts. The ob-ject information, together with the raw image and sentence, are then fed into a joint encoder for cross-modality inter-action, leading to better correlations between regions and phrases. Given the success of object information in image-it is intuitive to exploit the objects to im-text pre-training, prove video-text retrieval. However, there exist some main challenges that prevent us from na¨ıvely employing existing object-based techniques on video-text pre-training.
Fig. 1(a) shows that object boxes and tags always focus on the salient regions and semantics, which are considered
Figure 2. Visualization of the cross-modality attention on a video-text sample. This video is retrieved by the baseline dual encoder network [4] wrongly but correctly by our Object-aware
Transformer (OA-Trans). as the most important in each video. Existing object-based image-text pre-training methods either adopt an image-text joint encoder [21, 22] or cross-modality co-attention mod-ules [24] for interaction between cross-modality local fea-tures. Despite the results being positive, it is impractical to adapt this paradigm from image domain to video domain.
This is because all these methods require pre-extracted of-fline object feature for whole dataset. It would lead to un-affordable computational overhead to extract all objects, due to the billion-level frames. Moreover, their downstream performance heavily depends on the quality of the objects since they also need the objects as input for inference.
To this end, we introduce a simple yet effective paradigm for video-text pre-training, namely Object-aware Trans-former (OA-Trans), which explicitly enhances the fine-grained video-text the dominant “dual-encoder” framework at the same time maintaining its re-trieval efficiency during inference. This is achieved by two novel designs in our method as follows. interaction of (1) Single anchor frame that encodes object informa-tion.
Instead of replacing all sampled video frames with their extracted object regions, we balance the matching re-call and efficiency via combining whole frames together with a novel anchor frame that encodes object information.
Specifically, we propose to only extract object regions on this anchor frame and softly mask out the non-object re-gions on this anchor frame. (2) A novel 4-stream object-aware contrastive (OAC) loss. The input to our OA-Trans for pretraining include four stream: raw video, anchor frame, object tags (pre-dicted object categories), and raw text. To explore how to combine these four streams, we do extensive experimen-tal explorations and find out it works the best to contrast the raw video stream with the object tags stream and the raw text stream with the anchor frame stream. Note that the objects are only used for pre-training in our method, so the quality of detection has less effect on the downstream tasks and we do not need any extra computational over-head for downstream retrieval. As shown in Figure 2, a dual-network spreads its attention over the whole frame ran-domly while OA-Trans with OAC loss can successfully fo-cus on the “People” region.
Our contributions are as follows:
• We are the first to successfully develop an object-aware dual encoder model, namely OA-Trans, for end-to-end video-language pre-training.
• To alleviate the heavy cost of extracting object boxes, we propose to unify sampled whole frames with a sin-gle anchor frame whose non-object regions have been masked.
• We design a novel object-aware contrastive loss based on our unique input streams of video frames, textual query, the masked image, and predicted object tags on the anchor object frame.
• Our OA-Trans achieves significant improvements of
Recall@1 on 4 benchmarks with three downstream e.g. MSVD (from 46.2% to tasks (Figure 1 (b)). 51.4%). 2.