Abstract 1.

Introduction
We propose a novel approach for 3D video synthesis that is able to represent multi-view video recordings of a dy-namic real-world scene in a compact, yet expressive repre-sentation that enables high-quality view synthesis and mo-tion interpolation. Our approach takes the high quality and compactness of static neural radiance fields in a new direc-tion: to a model-free, dynamic setting. At the core of our approach is a novel time-conditioned neural radiance field that represents scene dynamics using a set of compact la-tent codes. We are able to significantly boost the training speed and perceptual quality of the generated imagery by a novel hierarchical training scheme in combination with ray importance sampling. Our learned representation is highly compact and able to represent a 10 second 30 FPS multi-view video recording by 18 cameras with a model size of only 28MB. We demonstrate that our method can render high-fidelity wide-angle novel views at over 1K resolution, even for complex and dynamic scenes. We perform an exten-sive qualitative and quantitative evaluation that shows that our approach outperforms the state of the art. Project web-site: https://neural-3d-video.github.io/.
∗ Equal contribution. TL’s work was done during an internship at
Reality Labs Research.
Photorealistic representation and rendering of dynamic real-world scenes are highly challenging research topics, yet with many important applications that range from movie production to virtual and augmented reality. Dynamic real-world scenes are notoriously hard to model using classical mesh-based representations, since they often contain thin structures, semi-transparent objects, specular surfaces, and topology that constantly evolves over time due to the often complex scene motion of multiple objects and people.
In theory, the 6D plenoptic function P (x, d, t) is a suit-able representation for this rendering problem, as it com-pletely explains our visual reality and enables rendering every possible view at every moment in time [1]. Here, x ∈ R3 is the camera position in 3D space, d = (θ, ϕ) is the viewing direction, and t is time. Thus, fully measuring the plenoptic function requires placing an omnidirectional camera at every position in space at every possible time.
Neural radiance fields (NeRF) [38] offer a way to cir-cumvent this problem: instead of directly encoding the plenoptic function, they encode the radiance field of the scene in an implicit, coordinate-based function, which can be sampled through ray casting to approximate the plenop-tic function. However, the ray casting, which is required to train and to render a neural radiance field, involves hun-dreds of MLP evaluations for each ray. While this might be acceptable for a static snapshot of a scene, directly re-constructing a dynamic scene as a sequence of per-frame neural radiance fields would be prohibitive as both stor-age and training time increase linearly with time. For example, to represent a 10 second, 30 FPS multi-view video recording by 18 cameras, which we later demonstrate with our method, a per-frame NeRF would require about 15 000 GPU hours in training and about 1 GB in storage.
More importantly, such obtained representations would only reproduce the world as a discrete set of snapshots, lack-ing any means to reproduce the world in-between. On the other hand, Neural Volumes [32] is able to handle dynamic objects and even renders at interactive frame rates. Its limi-tation is the dense uniform voxel grid that limits resolution and/or size of the reconstructed scene due to the inherent
O(n3) memory complexity.
In this paper, we propose a novel approach for 3D video synthesis of complex, dynamic real-world scenes that en-ables high-quality view synthesis and motion interpolation while being compact. Videos typically consist of a time-invariant component under stable lighting and a contin-uously changing time-variant component. This dynamic component typically exhibits locally correlated geometric deformations and appearance changes between frames. By exploiting this fact, we propose to reconstruct a dynamic neural radiance field based on two novel contributions.
First, we extend neural radiance fields to the space-time domain.
Instead of directly using time as input, we pa-rameterize scene motion and appearance changes by a set of compact latent codes. Compared to the more obvious choice of an additional ‘time coordinate’, the learned latent codes show more expressive power, allowing for recording the vivid details of moving geometry and texture. They also allow for smooth interpolation in time, which enables vi-sual effects such as slow motion or ‘bullet time’. Second, we propose novel importance sampling strategies for dy-namic radiance fields. Ray-based training of neural scene representations treats each pixel as an independent training sample and requires thousands of iterations to go through all pixels observed from all views. However, captured dy-namic video often exhibits a small amount of pixel change between frames. This opens up an opportunity to signif-icantly boost the training progress by selecting the pixels that are most important for training. Specifically, in the time dimension, we schedule training with coarse-to-fine hierar-chical sampling in the frames. In the ray/pixel dimension, our design tends to sample those pixels that are more time-variant than others. These strategies allow us to shorten the training time of long sequences significantly, while retain-ing high quality reconstruction results. We demonstrate our approach using a multi-view rig based on 18 GoPro cam-eras. We show results on multiple challenging dynamic en-vironments with highly complex view-dependent and time-dependent effects. Compared to the na¨ıve per-frame NeRF baseline, we show that with our combined temporal and spatial importance sampling we achieve one order of mag-nitude acceleration in training speed, with a model that is 40 times smaller in size for 10 seconds of a 30 FPS 3D video.
In summary we make the following contributions:
• We propose a novel dynamic neural radiance field based on temporal latent codes that achieves high quality 3D video synthesis of complex, dynamic real-world scenes.
• We present novel training strategies based on hierarchical training and importance sampling in the spatiotemporal domain, which boost training speed significantly and lead to higher quality results for longer sequences.
• We provide our datasets of time-synchronized and cal-ibrated multi-view videos that covers challenging 4D scenes for research purposes*. 2.