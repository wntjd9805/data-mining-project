Abstract
Machine translation between many languages at once is highly challenging, since training with ground truth re-quires supervision between all language pairs, which is dif-ficult to obtain. Our key insight is that, while languages may vary drastically, the underlying visual appearance of the world remains consistent. We introduce a method that uses visual observations to bridge the gap between lan-guages, rather than relying on parallel corpora or topo-logical properties of the representations. We train a model that aligns segments of text from different languages if and only if the images associated with them are similar and each image in turn is well-aligned with its textual descrip-tion. We train our model from scratch on a new dataset of text in over fifty languages with accompanying images.
Experiments show that our method outperforms previous work on unsupervised word and sentence translation us-ing retrieval. Code, models and data are available on globetrotter.cs.columbia.edu 1.

Introduction
Researchers have been building machine translation models for over 60 years [20], converting input sentences in one language to equivalent ones in another. In recent years, sequence-to-sequence deep learning models have overtaken statistical methods as the state-of-the-art in this field, with widespread practical applications. However, these models require large supervised corpora of parallel text for all lan-guage pairs, which are expensive to collect and often im-practical for uncommon pairs.
Rather than attempting to manually gather this ground truth, we use a source of supervision natural to the world: its consistent visual appearance. While language can take on many shapes and forms, visual observations are universal, as depicted in Fig. 1. This property can be freely leveraged to learn correspondences between the different languages of the world without any cross-lingual supervision.
Since we can learn how similar two images are to each other [12], and how compatible an image is with a textual
Figure 1. While each language represents a bicycle with a differ-ent word, the underlying visual representation remains consistent.
A bicycle has similar appearance in the UK, France, Japan and
India. We leverage this natural property to learn aligned multilin-gual representations for machine translation without paired train-ing corpora. description [36], we can introduce a transitive relation to if estimate how similar two sentences are to each other: (and only if) each sentence matches its image, and the two images match, then the two sentences should also match.
We propose a multimodal contrastive approach to solve this problem, using vision to bridge between otherwise unre-lated languages.
In our experiments and visualizations, we show that the transitive relations through vision provide excellent self-supervision for learning machine translation. Although we train our approach without paired language data, our ap-proach is able to translate between 52 different languages better than several baselines. While vision is necessary for our approach during learning, there is no dependence on vi-sion during inference. After learning language representa-tions, our approach can translate both individual words and full sentences using retrieval.
Our contribution is threefold. First, we propose a method that leverages cross-modal alignment between language and vision to train a multilingual translation system without any parallel corpora. Second, we show that our method outper-forms previous work by a significant margin on both sen-tence and word translation, where we use retrieval to test translation. Finally, to evaluate and analyze our approach, we release a federated multimodal dataset spanning 52 dif-ferent languages. Overall, our work shows that ground-ing language in vision yields models that are significantly more robust across languages, even in cases where ground truth parallel corpora are not available. Code, data, and pre-trained models will be released. 2.