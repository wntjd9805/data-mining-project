Abstract
Adversarial learning has achieved remarkable perfor-mances for unsupervised domain adaptation (UDA). Exist-ing adversarial UDA methods typically adopt an additional discriminator to play the min-max game with a feature ex-tractor. However, most of these methods failed to effectively leverage the predicted discriminative information, and thus cause mode collapse for generator. In this work, we ad-dress this problem from a different perspective and design a simple yet effective adversarial paradigm in the form of a discriminator-free adversarial learning network (DALN), wherein the category classifier is reused as a discrimina-tor, which achieves explicit domain alignment and category distinguishment through a unified objective, enabling the
DALN to leverage the predicted discriminative information for sufficient feature alignment. Basically, we introduce a
Nuclear-norm Wasserstein discrepancy (NWD) that has defi-nite guidance meaning for performing discrimination. Such
NWD can be coupled with the classifier to serve as a discrim-inator satisfying the K-Lipschitz constraint without the re-quirements of additional weight clipping or gradient penalty strategy. Without bells and whistles, DALN compares favor-ably against the existing state-of-the-art (SOTA) methods on a variety of public datasets. Moreover, as a plug-and-play technique, NWD can be directly used as a generic regular-izer to benefit existing UDA algorithms. Code is available at https://github.com/xiaoachen98/DALN . 1.

Introduction
Deep neural networks (DNNs) have achieved a significant progress in many computer vision tasks [4, 5, 16, 37]. How-ever, the success of these methods highly depends on large amounts of annotated data [13, 47, 51], which is extremely time-consuming and expensive to obtain. Moreover, due to
*indicates equal contribution.
†Corresponding author. (a) Bi-classifier (b) Extra Discriminator (c) Ours
Figure 1. Illustration of different adversarial paradigms, in which
G, C, and D denote the feature extractor, task-specific classifier, and discriminator, respectively. Different from typical paradigms that adopt an (a) additional classifier C ′ (called bi-classifier) or (b) additional discriminator D, we present a different perspective for
UDA and introduce a simple but effective adversarial paradigm illustrated in (c), in which the original task-specific classifier C is reused as a implicit discriminator, achieving explicit domain alignment and category distinguishment via a unified objective. the discrepancy [31,32] between training data and real-world testing data, the DNN model trained on annotated data may suffer from a dramatic performance decline in testing set despite extensive annotation efforts. To address this problem, unsupervised domain adaptation (UDA) [6, 9, 30, 48], which aims to transfer knowledge from a labeled source domain to an unlabeled target domain in the presence of a domain shift, has been deeply explored.
Inspired by the theoretical analysis of Ben-David et al. [2], the existing UDA methods usually explore the idea of learn-ing domain-invariant feature representations. Generally, these methods can be categorized into two branches, i.e., moment matching methods [20, 24, 25, 43, 49] and adver-sarial learning methods [11, 12, 25, 39]. Moment matching methods explicitly reduce the domain shift by matching a well-defined distribution discrepancy of the source and target domain features. Adversarial learning methods im-plicitly mitigate the domain shift by playing an adversarial min-max two-player game, which drives the generator to extract indistinguishable features to fool the discriminator.
Encouraged by the remarkable performance achieved by ad-versarial learning, increasingly more researchers have been devoted to developing a UDA method based on an adversar-ial paradigm [9, 10, 21, 23, 27, 42].
Basically, adversarial learning-based UDA methods usu-ally follow two lines of adversarial paradigms. One line
[10,19,21,27,39] leverages the disparity of two task-specific classifiers C and C ′ (as shown in Fig. 1(a)), which can be deemed as a discriminator, to implicitly achieve adversarial learning and improve feature transferability. This paradigm enables UDA methods to reduce the class-level domain dis-crepancy. However, the methods following this paradigm are prone to be affected by ambiguous predictions and thus hin-der the adaption optimization. The other line [9, 11, 12, 25] directly constructs an additional domain discriminator D as shown in Fig. 1(b), which improves the feature transferabil-ity by sufficiently confusing the cross-domain feature repre-sentations. However, the methods following this paradigm usually focus on the domain-level feature confusion, which may hurt the category-level information and thus cause mode collapse problem [18, 42].
To address these problems, we present a different perspec-tive for UDA and introduce a simple but effective adversarial paradigm illustrated in Fig. 1(c). In this paradigm, the orig-inal task-specific classifier is coupled with a novel discrep-ancy to serve as a discriminator/critic, which simultaneously achieves domain alignment and category distinguishment through a unified objective, enabling the model to lever-age the predicted discriminative information to capture the multi-modal structures [12, 25] of the feature distributions.
Particularly, when classifier C is used for classification, it helps achieve category-level distinguishment; furthermore, when C serves as a discriminator, it achieves feature-level alignment. The novel discrepancy, called Nuclear-norm
Wasserstein discrepancy (NWD), leverages the advantages of the Nuclear norm and 1-Wasserstein distance to encour-age the prediction determinacy and diversity. Different from the discrepancy metrics used in existing adversarial meth-ods [11, 42, 50], the NWD not only has a promising theo-retical generalization bound but also has definite guidance meaning for performing discrimination, i.e., naturally giving high scores to the source domain samples and low scores to the target domain samples due to the supervised training on the source domain. Such guidance encourages the intra-class and inter-class correlations of the target domain to approach those of the source domain. Moreover, in contrast to the existing Wasserstein discrepancy used in recent work [40], the NWD enables the adversarial UDA paradigm to satisfy the K-Lipschitz constraint without the need to set up an additional weight clipping [1] or gradient penalty [15].
Based on the introduced paradigm, we propose a discriminator-free adversarial learning network (DALN), which achieves adversarial UDA classification without ex-plicit domain discriminator. Benefiting from the definite guidance of the NWD, the proposed DALN converges rapidly and achieves competitive prediction determinacy and diversity. Note that, the DALN is considerably different from recent approaches [42, 50] that integrate the discrimi-nator into the classifier. DALN directly reuses the original task-specific classifier without requiring any additional com-ponents, making it quite simple and efficient. Extensive experiments on a variety of datasets demonstrate that the proposed DALN outperforms the existing state-of-the-art (SOTA) methods. Moreover, we show that the proposed
NWD is general and plug-and-play, which can be used as a regularizer to benefit the existing methods, which helps them achieve more competitive performance. The main con-tributions of this work are summarized as follows:
• We present a different perspective for UDA by introducing a simple yet effective adversarial paradigm, in which the original task-specific classifier is reused as a discriminator.
Based on this, we propose a new UDA method, namely
DALN, which can leverage the predicted discriminative information for sufficient feature alignment.
• We introduce a new discrepancy, termed NWD, which has a theoretical generalization bound and definite guidance meaning. Such discrepancy enables the implicitly con-structed discriminator to satisfy the K-Lipschitz constraint without the requirements of additional weight clipping and gradient penalty strategies.
• Without bells and whistles but only a few lines of code, the proposed method achieves highly competitive performance on various public datasets. By taking the proposed NWD as a regularizer for existing methods, these methods can achieve more competitive performance. 2.