Abstract
We present a novel class incremental learning approach based on deep neural networks, which continually learns new tasks with limited memory for storing examples in the previous tasks. Our algorithm is based on knowledge dis-tillation and provides a principled way to maintain the rep-resentations of old models while adjusting to new tasks ef-fectively. The proposed method estimates the relationship between the representation changes and the resulting loss increases incurred by model updates. It minimizes the up-per bound of the loss increases using the representations, which exploits the estimated importance of each feature map within a backbone model. Based on the importance, the model restricts updates of important features for robust-ness while allowing changes in less critical features for flex-ibility. This optimization strategy effectively alleviates the notorious catastrophic forgetting problem despite the lim-ited accessibility of data in the previous tasks. The exper-imental results show significant accuracy improvement of the proposed algorithm over the existing methods on the standard datasets. Code is available.1 1.

Introduction
Deep neural networks have achieved outstanding results in various applications including computer vision [13, 29, 31, 32, 38], natural language processing [45, 47], speech recognition [5, 10], robotics [25], bioinformatics [6], and many others. Despite their impressive performance on of-fline learning problems, it is still challenging to train models for a sequence of tasks in an online manner, where only a limited number of examples in the previous tasks are avail-able due to memory constraints or privacy issues.
Although fine-tuning is a good strategy to learn a new task given an old model, it is not effective for streaming tasks due to the catastrophic forgetting problem [30]; the 1https://github.com/kminsoo/AFC
Figure 1. Illustration of the proposed method. The current model optimizes the task loss over a mini-batch sampled from the data of the current task and exemplar sets for the previous tasks while it aims to differently minimize the discrepancy in each feature map over the previous model based on the corresponding importance. model performs well on the current task while it often fails to generalize on the previous ones. Therefore, incremental learning, the framework capacitating online learning with-out forgetting, has been studied actively. To mitigate the catastrophic forgetting problem, several branches of algo-rithms have been discussed. Architectural approaches em-ploy network expansion schemes [27, 41, 49] instead of us-ing static models. Rehearsal methods present the strate-gies to summarize the tasks in the past by storing exem-plar sets [4, 37] or generate samples by estimating the data distribution in the previous tasks [43]. On the other hand, parameter regularization methods [3, 22, 51] prevent impor-tant weights from deviating the previously learned models.
Knowledge distillation approaches [21, 26] focus on mini-mizing the divergences from the representations of the pre-vious models while effectively adapting to new tasks.
We present a knowledge distillation approach for class incremental learning, where the model incrementally learns new classes via knowledge distillation, with limited acces-sibility to the data from the classes in the old tasks. Existing approaches based on knowledge distillation simply mini-mize the distances between the representations of the old and new models without considering which feature maps are important to maintain the previously acquired knowl-edge. Although [35] addresses this issue, it is limited to the heuristic assignments of the importance weights. On the contrary, we estimate how the representation change given by a model update affects the loss, and show that the min-imization of the loss increases can be achieved by a proper weighting of feature maps for knowledge distillation.
In a nutshell, the proposed approach maintains important fea-tures for robustness, while making less critical features flex-ible for adaptivity. Note that our optimization strategy aims to minimize the upper bound of the expected loss increases over the previous tasks, which eventually reduces the catas-trophic forgetting problem. Figure 1 illustrates the main idea of the proposed approach. The main contributions and characteristics of our algorithm are summarized below:
• We propose a simple but effective knowledge distilla-tion algorithm for class incremental learning based on knowledge distillation with feature map weighting.
• We theoretically show that the proposed approach min-imizes the upper bound of the loss increases over the previous tasks, which is derived by recognizing the re-lationship between the distribution shift in a feature map and the change in the loss.
• Experimental results demonstrate that the proposed technique outperforms the existing methods by large margins in various scenarios.
The rest of the paper is organized as follows. Section 2 discusses related works to class incremental learning. The details of our method is described in Section 3, and the ex-perimental results are presented in Section 4. Finally, we conclude this paper in Section 5. 2.