Abstract
Part segmentations provide a rich and detailed part-level description of objects. However, their annotation requires an enormous amount of work, which makes it difficult to apply standard deep learning methods. In this paper, we propose the idea of learning part segmentation through un-supervised domain adaptation (UDA) from synthetic data.
We first introduce UDA-Part, a comprehensive part segmen-tation dataset for vehicles that can serve as an adequate benchmark for UDA1. In UDA-Part, we label parts on 3D
CAD models which enables us to generate a large set of annotated synthetic images. We also annotate parts on a number of real images to provide a real test set. Secondly, to advance the adaptation of part models trained from the synthetic data to the real images, we introduce a new UDA algorithm that leverages the object’s spatial structure to guide the adaptation process. Our experimental results on two real test datasets confirm the superiority of our ap-proach over existing works, and demonstrate the promise of learning part segmentation for general objects from syn-thetic data. We believe our dataset provides a rich testbed to study UDA for part segmentation and will help to signifi-cantly push forward research in this area. 1.

Introduction
Part-based object representations are of key importance for many computer vision tasks such as object recogni-tion [1, 9, 72, 98], pose estimation [13, 32, 93, 96], action detection [85], and scene understanding [65, 70, 75]. Cur-rently, part-based approaches often represent objects as a set of sparse keypoints, because these are easy to annotate in large-scale datasets for training deep neural networks. By contrast, part segmentations provide a richer and more de-tailed part-level object description. Instead of recognizing specific parts sparsely on the object (e.g., keypoint or part
*Corresponding author: qingliu.research@gmail.com
†This work is done during internship at Johns Hopkins University. 1https://qliu24.github.io/udapart/
Figure 1. An illustration of learning part segmentation through unsupervised domain adaptation (UDA) from synthetic vehicles.
Based on part annotation on 3D CAD models, we propose to use
UDA to learn from large-scale labeled synthetic samples and un-labeled real images, and the goal is to make accurate part segmen-tation predictions on real test images. detection), part segmentation gives a complete description of an object by assigning every pixel belonging to the object one and only one part label. This is a lot more challenging task and requires a much greater annotation effort.
Given their recent success, deep learning methods have dominated the studies of computer vision, including ob-ject segmentation [7, 10, 47]. However, these deep models usually require a large amount of annotated training data to achieve satisfying performance. Existing part segmen-tation datasets mostly contain only a small number of im-ages [9, 74], or define only a small number of parts per ob-ject category [74, 97], or focus on a single object category, such as humans [18, 19, 43, 94] and faces [32, 38, 39]. These limitations inhibit effective training of standard deep seg-mentation networks and have largely impeded the develop-ment of computer vision models that leverage part informa-tion. By contrast, 3D CAD models are available for many different objects, and, once annotated, can be used to gen-erate large-scale part segmentation datasets automatically.
In this work, we propose to solve part segmentation on general objects by learning from synthetic data (Figure 1)
In the first step, and address the problem in two steps. we introduce UDA-Part, a comprehensive part segmenta-tion dataset that can serve as an adequate benchmark for
UDA. UDA-Part is composed of 21 3D CAD models from 5 vehicle categories. For each category, we define a fine-grained set of parts which are consistently annotated across all CAD models of the corresponding category. Based on these CAD models and their part annotations, we are able to render a large-scale synthetic image dataset with auto-matically generated part segmentation ground-truth. These synthetic data are sufficient to train deep neural networks and may also be used for model evaluation or diagnosis. To evaluate how the models trained from synthetic data per-form on the real images, we also label parts on 200 real images collected from PASCAL3D+ [93] and include them as the target test set in UDA-Part.
Secondly, we introduce a new unsupervised domain adaptation (UDA) algorithm for part segmentation. UDA has been explored for image classification [30,59], keypoint detection [53, 103], and semantic segmentation [31, 105], where it achieves satisfactory results on real images with little annotation cost. To further advance UDA performance on part segmentation, we introduce Geometric-Matching
Guided domain adaptation (GMG). GMG conducts cross-domain geometric matching based on a global transfor-mation function between real and synthetic images. The function puts a smoothness constraint on the matching and thus adaptively preserves the spatial relations between the parts. Once an optimal match is found, GMG transfers the synthetic labels to the real images and retains the high-confidence results as pseudo-labels for a joint training pro-cess. In short, GMG makes part-relation-aware adaptation by explicitly using the object structure depicted in the syn-thetic samples. In our experiments, GMG outperforms other
UDA baselines for part segmentation on both the UDA-Part real test images and the PascalPart [9] test set.
In summary, our main contributions are: 1. We propose to learn part segmentation for general ob-jects through unsupervised domain adaptation (UDA) from synthetic data. 2. We introduce a new part segmentation dataset for ve-hicles called UDA-Part which can serve as a comprehensive benchmark for part segmentation through UDA. 3. We introduce a new UDA algorithm for part segmen-tation called Geometric-Matching Guided domain adapta-tion (GMG), which leverages the object’s spatial structure to guide the adaptation and achieves superior results. 2.