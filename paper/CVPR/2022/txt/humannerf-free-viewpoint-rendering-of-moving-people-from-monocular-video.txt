Abstract 1.

Introduction
We introduce a free-viewpoint rendering method – Hu-manNeRF – that works on a given monocular video of a hu-man performing complex body motions, e.g. a video from
YouTube. Our method enables pausing the video at any frame and rendering the subject from arbitrary new cam-era viewpoints or even a full 360-degree camera path for that particular frame and body pose. This task is partic-ularly challenging, as it requires synthesizing photorealis-tic details of the body, as seen from various camera angles that may not exist in the input video, as well as synthesiz-ing fine details such as cloth folds and facial appearance.
Our method optimizes for a volumetric representation of the person in a canonical T-pose, in concert with a motion field that maps the estimated canonical representation to every frame of the video via backward warps. The motion field is decomposed into skeletal rigid and non-rigid motions, pro-duced by deep networks. We show significant performance improvements over prior work, and compelling examples of free-viewpoint renderings from monocular video of moving humans in challenging uncontrolled capture scenarios. 1e.g., https://youtu.be/0ORaAnJYROg 2https://grail.cs.washington.edu/projects/humannerf/
Given a single video of a human performing an activ-ity, e.g., a YouTube or TikTok video of a dancer, we would like the ability to pause at any frame and rotate 360 degrees around the performer to view them from any angle at that moment in time (Figure 1). This problem – free-viwepoint rendering of a moving subject – is a longstanding research challenge, as it involves synthesizing previously unseen camera views while accounting for cloth folds, hair move-ment, and complex body poses [4, 5, 14, 17, 26, 37, 57, 63].
The problem is particularly hard for the case of “in-the-wild” videos taken with a single camera (monocular video), the case we address in this paper.
Previous neural rendering methods [2, 32, 35, 36, 48, 64, 73] typically assume multi-view input, careful lab capture, or do not perform well on humans due to non-rigid body motion. Human-specific methods typically assume a SMPL template [33] as a prior, which helps constrain the motion space but also introduces artifacts in clothing and complex motions that are not captured by the SMPL model [47, 48].
Recently deformable NeRF methods [45,46,49,62] perform well for small deformations, but not for large, full body mo-tions like dancing.
We introduce a method, called HumanNeRF, that takes as input a single video of a moving person and, after per-frame, off-the-shelf segmentation (with some manual clean-up) and automatic 3D pose estimation, optimizes for a canonical, volumetric T-pose of the human together with motion field that maps the estimated canonical volume to each video frame via a backward warping. The motion field combines skeletal rigid motion with non-rigid motion, each represented volumetrically. Our solution is data-driven, with the canonical volume and motion fields derived from the video itself and optimized for large body deformations, trained end-to-end, including 3D pose refinement, without template models. At test time, we can pause at any frame in the video and, conditioned on the pose in that frame, render the resulting volumetric representation from any viewpoint.
We show results on a variety of examples: existing lab datasets, videos we captured outside the lab, and downloads from YouTube (with creator permission). Our method out-performs the state-of-the-art numerically and produces sig-nificantly higher visual quality. Please refer to the project page to see the results in motion. 2.