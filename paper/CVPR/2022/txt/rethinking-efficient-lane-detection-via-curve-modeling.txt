Abstract
This paper presents a novel parametric curve-based method for lane detection in RGB images. Unlike state-of-the-art segmentation-based and point detection-based methods that typically require heuristics to either decode predictions or formulate a large sum of anchors, the curve-based methods can learn holistic lane representations natu-rally. To handle the optimization difficulties of existing poly-nomial curve methods, we propose to exploit the parametric
B´ezier curve due to its ease of computation, stability, and high freedom degrees of transformations. In addition, we propose the deformable convolution-based feature flip fu-sion, for exploiting the symmetry properties of lanes in driv-ing scenes. The proposed method achieves a new state-of-the-art performance on the popular LLAMAS benchmark.
It also achieves favorable accuracy on the TuSimple and
CULane datasets, while retaining both low latency (>150
FPS) and small model size (<10M). Our method can serve as a new baseline, to shed the light on the parametric curves modeling for lane detection. Codes of our model and PytorchAutoDrive: a unified framework for self-driving perception, are available at: https://github.com/ voldemortX/pytorch-auto-drive . 1.

Introduction
Lane detection is a fundamental task in autonomous driv-ing systems, which supports the decision-making of lane-keeping, centering and changing, etc. Previous lane de-tection methods [2, 10] typically rely on expensive sensors such as LIDAR. Advanced by the rapid development of deep learning techniques, many works [14, 16, 17, 24, 31] are proposed to detect lane lines from RGB inputs captured by commercial front-mounted cameras.
*Equal Contribution.
†Corresponding Authors.
‡Lizhuang Ma led this project, and he is a member of Qing Yuan Re-search Institute, Shanghai Jiao Tong University.
Figure 1. Lane detection strategies. Segmentation-based and point detection-based representations are local and indirect. The abstract coefficients (a, b, c, d) used in polynomial curve are hard to opti-mize. The cubic B´ezier curve is defined by 4 actually existing control points, which roughly fit line shape and wrap the lane line in its convex hull (dashed red lines). Best viewed in color.
Deep lane detection methods can be classified into three categories, i.e., segmentation-based, point detection-based, and curve-based methods (Figure 1). Among them, by relying on classic segmentation [5] and object detection
[20] networks, the segmentation-based and point detection-based methods typically achieve state-of-the-art lane detec-tion performance. The segmentation-based methods [16,17, 31] exploit the foreground texture cues to segment the lane pixels and decode these pixels into line instances via heuris-tics. The point detection-based methods [12, 24, 29] typi-cally adopt the R-CNN framework [8, 20], and detect lane lines by detecting a dense series of points (e.g., every 10 pixels in the vertical axis). Both kinds of approaches repre-sent lane lines via indirect proxies (i.e., segmentation maps and points). To handle the learning of holistic lane lines, under cases of occlusions or adverse weather/illumination conditions, they have to rely on low-efficiency designs, such as recurrent feature aggregation (too heavy for this real-time task) [17, 31], or a large number of heuristic anchors (> 1000, which may be biased to dataset statistics) [24].
On the other hand, there are only a few methods [14, 23] proposed to model the lane lines as holistic curves (typi-cally the polynomial curves, e.g., x = ay3 + by2 + cy + d).
While we expect the holistic curve to be a concise and el-egant way to model the geometric properties of lane line, the abstract polynomial coefficients are difficult to learn.
Previous studies show that their performance lag behind the well-designed segmentation-based and point detection-based methods by a large margin (up to 8% gap to state-of-the-art methods on the CULane [17] dataset). In this paper, we aim to answer the question of whether it is possible to build a state-of-the-art curve-based lane detector.
We observe that the classic cubic B´ezier curves, with sufficient freedom degrees of parameterizing the deforma-tions of lane lines in driving scenes, remain low computa-tion complexity and high stability. This inspires us to pro-pose to model the thin and long geometric shape proper-ties of lane lines via B´ezier curves. The ease of optimiza-tion from on-image B´ezier control points enables the net-work to be end-to-end learnable with the bipartite matching loss [28], using a sparse set of lane proposals from sim-ple column-wise Pooling (e.g., 50 proposals on the CU-Lane dataset [17]), without any post-processing steps such as the Non-Maximum Suppression (NMS), or hand-crafted heuristics such as anchors, hence leads to high speed and small model size. In addition, we observe that lane lines appear symmetrically from a front-mounted camera (e.g., between ego lane lines, or immediate left and right lanes).
To model this global structure of driving scenes, we further propose the feature flip fusion, to aggregate the feature map with its horizontally flipped version, to strengthen such co-existences. We base our design of feature flip fusion on the deformable convolution [32], for aligning the imper-fect symmetries caused by, e.g., rotated camera, changing lane, non-paired lines. We conduct extensive experiments to analyze the properties of our method and show that it performs favorably against state-of-the-art lane detectors on three popular benchmark datasets. Our main contributions are summarized as follows:
• We propose a novel B´ezier curve-based deep lane de-tector, which can model the geometric shapes of lane lines effectively, and be naturally robust to adverse driving conditions.
• We propose a novel deformable convolution-based fea-ture flip fusion module, to exploit the symmetry prop-erty of lanes observed from front-view cameras.
• We show that our method is fast, light-weight, and ac-curate through extensive experiments on three popular lane detection datasets. Specifically, our method out-performs all existing methods on the LLAMAS bench-mark [3], with the light-weight ResNet-34 backbone. 2.