Abstract
Test-time adaptation is a special setting of unsupervised domain adaptation where a trained model on the source domain has to adapt to the target domain without access-ing source data. We propose a novel way to leverage self-supervised contrastive learning to facilitate target feature learning, along with an online pseudo labeling scheme with refinement that significantly denoises pseudo labels. The contrastive learning task is applied jointly with pseudo la-beling, contrasting positive and negative pairs constructed similarly as MoCo but with source-initialized encoder, and excluding same-class negative pairs indicated by pseudo la-bels. Meanwhile, we produce pseudo labels online and re-fine them via soft voting among their nearest neighbors in the target feature space, enabled by maintaining a mem-ory queue. Our method, AdaContrast, achieves state-of-the-art performance on major benchmarks while having several desirable properties compared to existing works, including memory efficiency, insensitivity to hyper-parameters, and better model calibration. Code is released at https:
//github.com/DianCh/AdaContrast. 1.

Introduction
Deep networks are remarkably successful in learning vi-sual tasks when training and test data follow the same distri-bution. However, their ability to generalize to unseen data suffers in the presence of domain shift [42, 43]. Building models that can adapt to distribution shifts is the focus of domain adaptation where the goal is to transfer knowledge from a labeled source domain to a new but related target domain [2, 13, 19, 31, 44, 53]. In this work we focus on the problem of test-time [50, 56] or source-free [23, 28, 58] do-main adaptation where the source data is no longer available during adapting to unlabeled test data. Since test-time adap-tation (TTA) only requires access to the source model, it is appealing to real-world applications where data privacy and transmission bandwidth become critical issues.
*Work done when author previously worked at UC Berkeley.
†Author is now at Google.
Figure 1. Illustration of how our method, AdaContrast, leverages target domain data vs. prior works. (a) Without adaptation, source-only model is only evaluated on target data. (b) With pseudo label-ing, the source classifier predictions are used as pseudo labels for self-training. (c) Existing pseudo labeling approach, SHOT [28], uses offline global refinement to reduce noisy pseudo labels. (d)
In AdaContrast, we consider two kinds of relations among tar-get samples: we use contrastive learning to exploit information from sample pairs to learn better target representation, while refin-ing pseudo labels by aggregating knowledge in the neighborhood.
Colors indicate pseudo-labeled classes.
However, the challenging setting of TTA raises two ma-jor questions: 1) how to learn the target-domain represen-tation without the help of ground truth annotation 2) how to build the target-domain classifier with only the source-domain classifier available as a proxy for the source do-main. To address these difficulties, existing works have leveraged image/feature generation [23, 27], class proto-types [28, 57], entropy minimization [28, 56], self-training or pseudo labeling [28], and self-supervised auxiliary task training [50]. Generative models have the drawback of re-quiring a large computation capacity for generating target-style images/features [27]. Entropy minimization-based methods have been competitive but the direct optimization of entropy disrupts the model calibration on target. Pseudo-labeling methods have shown promising results but their performance can suffer from noisy pseudo labels [28]. Test-time training [50] introduced a self-supervised auxiliary ro-tation prediction task to be optimized jointly during both source and target training. This approach is limited because it requires altering the source training protocols, which may not be feasible for all models of interest. Moreover, the contrastive learning paradigm has been shown to learn more
transferable representations compared to rotation prediction as a pre-text task. Recently, [55] used self-supervised learn-ing in the pre-training stage, however, we argue this method does not fully leverage the strength of self-supervised rep-resentation learning during the adaptation stage.
In this work, we introduce AdaContrast, a novel test-time adaptation strategy that uses self-supervised con-trastive learning on the target domain to exploit the pair-wise information among target samples, which is optimized jointly with pseudo labeling. Compared to the pretrain-and-finetune paradigm [4, 7, 17], the joint optimization on tar-get domain allows the model to reuse source knowledge to quickly adapt, while benefiting mutually with pseudo label-ing. The intuition is that a better target representation facil-itates the learning of the decision boundaries [3], while the useful priors contained in pseudo labels further enhances the effectiveness of contrastive learning in representation learning. We also show that our auxiliary contrastive learn-ing brings robustness to the pseudo labeling, preventing di-vergence and allowing the online pseudo labels to consis-tently provide high-accuracy supervision.
As for the pseudo labeling, we introduce a new online pseudo label refinement scheme that results in generating significantly more correct pseudo labels by using soft k-nearest neighbors voting [33] in the target domain’s feature space for each target sample. As shown in Fig. 1, un-like prior works which typically require an offline global memory bank to store pseudo labels/features generated ev-ery single or a few epochs [28, 55], we produce and refine pseudo labels at a per-batch basis by aggregating probabili-ties from nearest neighbors based on feature distances. Re-lying on a relatively small memory queue instead makes our approach both computationally affordable and suitable for online streaming where target data cannot be revisited such as robotics applications.
The two key factors in AdaContrast, self-supervised con-trastive learning trained jointly with pseudo labeling, of-fer several empirical merits including hyper parameter in-sensitivity and better model calibration. Hyper-parameter selection in TTA setting is a key design factor that is often neglected in TTA literature where tuning hyper-parameters is not an option due to lack of access to tar-get labels. We empirically show our proposed AdaContrast approach consistently performs well under a wide range of hyper-parameters. We also found AdaContrast to have a better model calibration [16, 41] compared to entropy minimization-based methods [28]. We have evaluated our method on major domain adaptation benchmarks where it achieves state-of-the-art test-time adaptation performance.
Its 86.8% average accuracy and 84.5% overall accuracy on
VisDA-C surpass the previous state-of-the-art by +3.8% and
+6.2%, respectively. We are also the first TTA method to evaluate on the large-scale DomainNet dataset, where Ada-Contrast achieves state-of-the-art 67.8% accuracy averaged over 7 domain shifts. 2.