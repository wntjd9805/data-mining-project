Abstract
Weakly supervised temporal action localization aims to localize temporal boundaries of actions and simultaneously identify their categories with only video-level category la-bels. Many existing methods seek to generate pseudo la-bels for bridging the discrepancy between classification and localization, but usually only make use of limited contex-tual information for pseudo label generation. To alleviate this problem, we propose a representative snippet summa-rization and propagation framework. Our method seeks to mine the representative snippets in each video for propagat-ing information between video snippets to generate better pseudo labels. For each video, its own representative snip-pets and the representative snippets from a memory bank are propagated to update the input features in an intra-and inter-video manner. The pseudo labels are generated from the temporal class activation maps of the updated fea-tures to rectify the predictions of the main branch. Our method obtains superior performance in comparison to the existing methods on two benchmarks, THUMOS14 and Ac-tivityNet1.3, achieving gains as high as 1.2% in terms of average mAP on THUMOS14. Our code is available at https://github.com/LeonHLJ/RSKP. 1.

Introduction
Temporal action localization in videos has a wide range of applications in different scenarios. This task aims to lo-calize action instances in untrimmed videos along the tem-poral dimension. Most existing methods [4, 18, 19, 46, 54] are trained in a fully supervised manner where both video-level labels and frame-wise annotations are provided.
In contrast to these strong supervision based methods, weakly-supervised temporal action localization method attempt to localize action instances in videos, leveraging only video-level supervisions. This setting enables the method to by-*Corresponding author.
Figure 1. Two examples of the action of “Clean and Jerk”. The barcode is the ground-truth (GT). The line charts are snippet-wise classification scores. We show the detection results of two meth-ods, TSCN [50] and STPN [33]. The instance in the orange box is hard for both the two methods, because most of the snippets show only a part of the athlete. In contrast, the instances (black box and green box) with frontal view and whole body is much easier to be detected. A direct way to address this issue would be to propagate (red lines) knowledge of representative snippets to other snippets. pass the manual annotations of temporal boundaries, which are laborious, expensive and prone to large variations [39].
Due to the absence of fine-grained annotations, exist-ing works mainly embrace a localization-by-classification pipeline [42, 53], where a classifier is trained with video-level annotations of action categories [31] and is used to obtain a sequence of class logits or predictions, i.e., tempo-ral class activation maps (TCAMs). Usually, detection re-sults are obtained from TCAMs via a post-processing step
[33,42] (e.g., thresholding) or a localization branch [23,41].
Therefore, the quality of TCAMs determines the upper bound of the model. However, there is generally be a dis-crepancy between classification and localization [20,22,24].
Since each video generally contains multiple snippets1, with only video-level annotations, the model would easily fo-cus on the contextual background or discriminative snippets that contribute most to video-level classification, which hin-1In this paper, we view snippets as the smallest granularity since the high-level features of consecutive frames vary smoothly over time [11,45].
ders the generation of high quality TCAMs.
To address this issue, pseudo label-based methods [27, 35, 49, 50] were proposed to generate snippet-wise pseudo labels for bridging the gap between classification and lo-calization. However, existing methods only leverage lim-ited information, i.e., the information within each snippet, to generate pseudo labels, which is insufficient to generate high quality pseudo labels. In Figure 1, we show the detec-tion results of two methods. The first method TSCN [50] is a pseudo label-based method, while the second method
STPN [33] is a simple baseline model without using pseudo labels. As we can see, even if TSCN achieves much gain over STPN, neither of the two methods successfully detects the difficult action instance in the orange box, which only shows partial body of the athlete. Obviously, the pseudo labels generated from the inaccurate TCAMs are also inac-curate. In contrast, for the easy instance, e.g., the one in the blue box, both of the two methods accurately detect it.
The above observation motivate us to introduce contex-tual information for pseudo label generation. Specifically, we propose to propagate the knowledge of those represen-tative snippets (e.g., the black and blue boxes in Figure 1) in an intra- and inter-video manner to facilitate the pseudo label generation, especially for those difficult snippets (e.g., the orange box in Figure 1). To achieve this goal, a major issue is how to determine the representative snippets and how to propagate their useful knowledge to other snippets.
Besides, it should be effective to summarize and propagate snippet-level knowledge across videos, so as to take advan-tage of the large variation of videos in a large scale dataset.
We present a representative snippet knowledge propaga-tion framework. To facilitate knowledge propagation, we propose to mine the representative snippets, which can mit-igate the influence of outlier snippets to serve as a bridge to propagate knowledge between snippets. Specifically, we utilize the expectation-maximization (EM) attention [17] to handle the variations caused by different camera views [51], sub-action differences [8, 20], confusing background con-text [22, 24] and to capture the important semantic of each video, which are severed as the representative snippets in our method. After that, we employ a memory bank to store representative snippets of high confidences for each class.
This design enables our method to leverage representative snippets in an inter-video fashion, and also avoids much
GPU memory cost during training. Furthermore, to prop-agate the knowledge of representative snippets, we propose a bipartite random walk (BiRW) module, which integrates the random walk operation to update the features of the in-put video with intra- and inter-video representative snippets.
The TCAMs of the updated features serve as online refined pseudo labels to rectify the predictions of the main branch.
The contribution of this paper is three-fold. (a) We pro-pose a novel representative snippet knowledge propagation framework for weakly supervised temporal action localiza-tion, which generates better pseudo labels via representative snippet knowledge propagation to effectively alleviates the discrepancy between classification and detection. (b) The proposed framework can be applied to most existing meth-ods to consistently improve their localization performance. (c) Compared with state-of-the-art methods, the proposed framework yields improvements of 1.2% and 0.6% aver-age mAP on THUMOS14 and ActivityNet1.3. 2.