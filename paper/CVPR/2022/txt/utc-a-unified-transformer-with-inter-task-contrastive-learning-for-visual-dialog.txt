Abstract
Visual Dialog aims to answer multi-round, interactive questions based on the dialog history and image content.
Existing methods either consider answer ranking and gen-erating individually or only weakly capture the relation across the two tasks implicitly by two separate models. The research on a universal framework that jointly learns to rank and generate answers in a single model is seldom ex-plored. In this paper, we propose a contrastive learning-based framework UTC to unify and facilitate both discrim-inative and generative tasks in visual dialog with a single model. Specifically, considering the inherent limitation of the previous learning paradigm, we devise two inter-task contrastive losses i.e., context contrastive loss and answer contrastive loss to make the discriminative and genera-tive tasks mutually reinforce each other. These two com-plementary contrastive losses exploit dialog context and target answer as anchor points to provide representation learning signals from different perspectives. We evaluate our proposed UTC on the VisDial v1.0 dataset, where our method outperforms the state-of-the-art on both discrimina-tive and generative tasks and surpasses previous state-of-the-art generative methods by more than 2 absolute points on Recall@1. 1.

Introduction
Recently, an increasing amount of attention has been paid to vision and language understanding. Many related tasks in this intersecting field have been designed and intro-duced for different scenarios, such as Moment Localization with Natural Language [21, 25], Image Captioning [6], Vi-sual Question Answering [4], and Visual Dialog [14, 17].
Among them, Visual Dialog is designed to interact with
*This work was done under the guidance of Yudong Zhu, and the cor-responding author is Xiaodong Gu (xdgu@fudan.edu.cn) humans about an unseen image through continuous com-munication. In general, there are two types of settings in visual dialog: a discriminative decoder that ranks the pre-defined answer candidates in the discriminative setting, and a generative decoder that synthesizes the target answer in the generative setting.
Compared with visual question answering, visual dialog not only demands that the agent is able to engage in a ques-tion about an image but also requires the agent to fully ex-ploit the clues in previous questions and answers. Thus, the interactions among an answer candidate, a question, a dia-log history and an image are the key to produce a correct answer.
As shown in Figure 1(a), most of the current visual di-alog models [19, 22] focus on designing various attention mechanisms to capture such interaction in the discrimina-tive setting while training answer ranking and generating tasks individually. Recently, several related works [8, 16] weakly capture the relation across the generative and dis-criminative tasks by training the entire network using the two decoders simultaneously. Though these models have impressive results, the designing of a unified model to fa-cilitate the training of both answer ranking and generating tasks remains two challenges.
On the one hand, the limitation originates from the in-herently different peculiarities of the two tasks. As shown in Figure 1(a and b), the discriminative setting can capital-ize on the unrestricted message passing across answer can-didates and multi-modal context. While in the generation task, the models need to autoregressively decode the answer word by word, which makes the message passing from an-swers to multi-modal context restricted. This raises the first challenge for our unified model: how to fully transfer the rich semantic clues of answer candidates in the discrimina-tive task to answer generation.
On the other hand, the discriminative setting focuses on the alignment of dialog context and answer, and most of
to distinguish the paired dialog context representations from other negative similar dialogs by contrastive learning. Fur-thermore, the contrastive learning scheme also enables the generative task to utilize enhanced dialog context represen-tation information from the discriminative task. Our contri-butions are as follows:
• We introduce a unified model for Visual Dialog, which processes all interactions between different entities for both discriminative and generative tasks in a single model.
• The target answers and dialog context are employed as anchor points to help facilitate the training of dis-criminative and generative tasks. Compared with pre-vious methods, two inter-task contrastive losses enable the bidirectional information flow between all answer and dialog context pairs of the two tasks, which signif-icantly eases the training of both tasks.
• We conduct extensive experiments on VisDial bench-marks to analyze how our model performs on both tasks with various training aspects. The qualitative re-sults indicate that our model obtains reliable improve-ment on both tasks with inter-task contrastive learning. 2.