Abstract scenarios.
We propose an approach to estimate arm and hand dy-namics from monocular video by utilizing the relationship between arm and hand. Although monocular full human motion capture technologies have made great progress in recent years, recovering accurate and plausible arm twists and hand gestures from in-the-wild videos still remains a challenge. To solve this problem, our solution is proposed based on the fact that arm poses and hand gestures are highly correlated in most real situations. To fully exploit arm-hand correlation as well as inter-frame information, we carefully design a Spatial-Temporal Parallel Arm-Hand
Motion Transformer (PAHMT) to predict the arm and hand dynamics simultaneously. We also introduce new losses to encourage the estimations to be smooth and accurate. Be-sides, we collect a motion capture dataset including 200K frames of hand gestures and use this data to train our model.
By integrating a 2D hand pose estimation model and a 3D human pose estimation model, the proposed method can produce plausible arm and hand dynamics from monocular video. Extensive evaluations demonstrate that the proposed method has advantages over previous state-of-the-art ap-proaches and shows robustness under various challenging
*These authors contributed equally to this work. 1.

Introduction
Human arm-hand dynamics is an important part of full human motion capture, and can also be used for the con-trol of human-machine interface. However, although some methods are proposed to capture full human motion includ-ing hand gestures [30, 33, 40, 45, 47], most of them fail to take into account the correlation between arm and hand, treating body motion capture and gesture estimation as two separate tasks, resulting in inaccurate predictions in chal-lenging scenarios. Zhou et al. [47] propose to learn human motion by considering body-hand correlation, but such cor-relation is only utilized in predicting 2D hand key-points and 3D body key-points, the final motion of body and hands are learned separately by different models. Ng et al. [29] in-troduce to learn the body-hand correlation to estimate con-versational hand gestures. However, the method is restricted to the domain of conversational gesture prediction and can-not produce arm motions.
In this paper, we focus on the task of capturing accurate and plausible arm-hand dynamics from monocular video.
Specifically, we introduce a spatial-temporal Parallel Arm-Hand Motion Transformer (PAHMT) to take full advantage of arm-hand correlations and inter-frame information. The
estimation of 2D hand key-points and 3D arm key-points are first achieved by a light weight hand key-point detector and a 3D human key-point estimator respectively, which are used as input in our model. The PAHMT mainly consists of a spatial transformer and a temporal transformer. The spatial transformer is responsible for extracting the spatial feature, namely global correlations between arm and hand as well as local correlations between different joints, while the temporal transformer is designed to utilize inter-frame information. Besides, we introduce two losses to encourage the predictions to be smooth and accurate. To train the pro-posed model, we collect a dataset of 200K frames of human motion (including hand gestures). Most of the collected se-quences are dancing or sport motions that cover a lot of arm motions and hand gestures. We demonstrate that the pro-posed model can produce plausible estimation of arm-hand dynamics even in difficult scenarios such as occlusion or motion blur.
Our contributions can be summarized as follows:
• We propose to capture arm and hand dynamics simul-taneously by leveraging the arm-hand correlations. By exploiting such correlations, the proposed model can make reasonable estimations of arm twists and hand gestures;
• We design a spatial-temporal parallel transformer model to make full use of arm-hand correlation as well as inter-frame information, which enhances the robust-ness of the prediction; In addition, we introduce two losses to encourage smooth and accurate predictions;
• Extensive evaluations demonstrate that the pro-posed method outperforms existing state-of-the-art ap-proaches and shows robustness under various chal-lenging scenarios. 2.