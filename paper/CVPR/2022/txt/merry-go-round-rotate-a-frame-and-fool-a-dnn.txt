Abstract
A large proportion of videos captured today are first per-son videos shot from wearable cameras. Similar to other computer vision tasks, Deep Neural Networks (DNNs) are the workhorse for most state-of-the-art (SOTA) egocentric vision techniques. On the other hand DNNs are known to be susceptible to Adversarial Attacks (AAs) which add im-perceptible noise to the input. Both black-box, as well as white-box attacks on image as well as video analysis tasks have been shown. We observe that most AA techniques ba-sically add intensity perturbation to an image. Even for videos, the same process is essentially repeated for each frame independently. We note that definition of impercep-tibility used for images may not be applicable for videos, where a small intensity change happening randomly in two consecutive frames may still be perceptible. In this paper we make a key novel suggestion to use perturbation in opti-cal flow to carry out AAs on a video analysis system. Such perturbation is especially useful for egocentric videos, be-cause there is lot of shake in the egocentric videos anyways, and adding a little more, keeps it highly imperceptible. In general our idea can be seen as adding structured, para-metric noise as the adversarial perturbation. Our imple-mentation of the idea by adding 3D rotations to the frames, reveal that using our technique, one can mount a black-box
AA on an egocentric activity detection system in one-third of the queries compared to the SOTA AA technique. 1.

Introduction
Despite achieving superior performance on a variety of computer vision tasks [3,11,12,33], Deep Neural Networks (DNNs) remain remarkably susceptible to imperceptible ad-versarial perturbations [37]. The goal of an adversarial at-tack (AA) is, given a clean image, I, create an adversar-ial perturbation (P ), which when added to the clean image, generates an adversarial sample Iadv = I + P , which tricks a DNN model into producing an incorrect prediction. Since the purpose is to attack a system, the perturbation should be imperceptible to the humans.
Figure 1. A brief pipeline for the proposed system. SR denote
Success Rate, and ANQ denotes Average Number of Queries. Suc-cessful Attack demands high SR and low ANQ. For a given in-put video of size T × H × W × C, where T is the number of frames, and H, W , and C are height, width, and channel respec-tively of each frame, an intensity based attack needs to predict
T × H × W × C parameters. Whereas, our proposed parametric perturbation attack, using rotation based transformation, predicts only T × 3 parameters. This reduces the query budget to predict the parameters. Geometric transformations are natural perturba-tions and do not disturb semantic integrity of an image or a video.
The simplest setting to mount such an AA is when the adversary gets full access to the model (M ), including input(X)/output(Y ), and the exact gradients (G). One can, then, simply backpropagate the loss corresponding to the desired (incorrect) output, and use it guide the perturbation in the input [16, 25, 37]. The setting is called white box at-tacks, but is usually impractical in real life, due to unavail-ability of the full access to the model. The alternate setting is the black box setting when an adversary has access to X, and Y , but not G. In this formulation the primary challenge becomes estimating the gradient at the input without hav-ing access to G [6, 16, 17]. The quality of an AA technique is usually determined by how imperceptible the P is, and additionally in case of black box attacks, how many (X,Y ) pairs a technique needs to find a P corresponding to a par-ticular I.
Researchers have shown both white box and black box attacks for a variety of DNN models across range of tasks
[37]. Further, relevant to our context, the attacks have been shown when the input to the model is an image [16,17], or a video [20,49]. Our focus in this paper is on mounting black box adversarial attacks on video analysis (VA) systems .
We note that most of the techniques for AA on a VA sys-tem trivially extends the black box pipeline from images to videos. The videos are broken down into frames, and adversarial examples are created by adding random pertur-bations in the pixel intensities [20, 49]. For a successful attack, these methods require a large number of queries on the target model. For example [20] requires 23K queries on an average for generating a single adversarial sample.
We would like to emphasize that a frame-wise attack, using intensity-based noise, do not coordinate the adversarial per-turbations between consecutive frames. While a change in intensity level of a few individual pixels may be impercep-tible in an individual frame, when played as a video, such random flashes are easily detected by a human being.
One of the key ideas of this paper is to parameterize the perturbation. The parameterization has two advantages, (1) it is easier to regularize within, and across the frames, and (2) one can perturb a large number of pixels, by estimating only a few parameters, thus reducing the query budget, an important consideration in a black box attack. While, the idea of parametric perturbation is generic and can be used in variety of settings, given our focus to videos, we consider it for attack on VA systems, and even more specifically, on egocentric VA systems.
We observe that one of the simplest ways to perform co-ordinated change in intensity levels of large number of pix-els, across frames of a video, is by geometrically transform-ing each frame. The transformation will cause change in the optical flow, which is an important cue for many VA tasks.
At the same time, performing frame-wise geometric trans-formation maintains semantic integrity of frame contents, keeping it imperceptible to human beings.
Contributions: The key contributions of this work are: 1. We propose to add novel parametric perturbations to mount an AA attack against a computer vision system. 2. For a VA system, we suggest use of geometric transfor-mations to implement such parametric perturbations. 3. We propose a novel DNN architecture for predicting a mix of intensity, and geometric perturbations which can successfully fool a VA system to carry out black box AA attack. 4. Our exhaustive experiments on multitude of benchmark datasets, and VA tasks for egocentric, and third person videos show that our proposed architecture outperforms
SOTA techniques, managing to fool a DNN in one-third of the queries as needed by the SOTA. 2.