Abstract
Novel view synthesis from a single image has recently attracted a lot of attention, and it has been primarily ad-vanced by 3D deep learning and rendering techniques.
However, most work is still limited by synthesizing new views within relatively small camera motions. In this pa-per, we propose a novel approach to synthesize a consistent long-term video given a single scene image and a trajec-tory of large camera motions. Our approach utilizes an autoregressive Transformer to perform sequential model-ing of multiple frames, which reasons the relations between multiple frames and the corresponding cameras to predict the next frame. To facilitate learning and ensure consis-tency among generated frames, we introduce a locality con-straint based on the input cameras to guide self-attention among a large number of patches across space and time.
Our method outperforms state-of-the-art view synthesis ap-proaches by a large margin, especially when synthesizing long-term future in indoor 3D scenes. Project page at https://xrenaa.github.io/look-outside-room/. 1.

Introduction
Single-image view synthesis has attracted a lot of atten-tion in computer vision and computer graphics. It brings a photo to life by extrapolating beyond the input pixels and generating new pixels following the geometric structure of the scene. At the same time, the generated pixels need to be semantically coherent with the existing pixels. Current view 1
synthesis methods which learn 3D geometric representation have shown encouraging results in generating high-quality novel views [39, 56, 69]. However, these approaches can only generate views within a limited range of camera mo-tion. For example, it will be very challenging for current ap-proaches to synthesize what is outside the door of the room shown in the first row of Figure 1.
When synthesizing images with large camera view changes, we would also expect the generated images to be consistent. That is, when we are synthesizing with a path walking towards the door in a room, we hope that the sur-roundings of the path should not change all the time and reveal a single underlying world. To this end, we propose to solve the problem extended based on view synthesis: Given a single image of the 3D scene and a long-term camera tra-jectory as inputs, synthesize a consistent video as the out-put. For example, given a single input image of a room (first row of Figure 1), we synthesize the video on walking towards the door, going through the door, and navigating into a hallway with a painting on the wall. Solving such a task not only has wide applications in content generation and editing but also helps build a differentiable simulator for model-based planning and control in robotics.
To solve this problem, we seek help from autoregressive models [8, 41, 42, 60], which have shown tremendous suc-cess in extrapolating the contents beyond the input image.
For example, Rombach et al. [46] proposes to use an au-toregressive Transformer to implicitly perform large geo-metric transformation for view synthesis. To handle the un-certainty with a large transformation, the model is trained under a probabilistic framework which allows for sampling different novel views with the same camera. While gener-ating realistic novel views even given a large transforma-tion, it also leads to inconsistent and diverse outputs along a given trajectory due to the probabilistic sampling.
In this paper, to synthesize consistent long-term videos, we propose to leverage the autoregressive Transformer for sequential modeling in time with locality constraints. In-stead of learning the autoregressive model between only two views of the scene [46], our work leverages the con-tinuity in videos and perform sequential modeling with multiple video frames. Given a sequence of input im-ages {x1, x2, ..., xt−1} and the previous camera trajectory
{C2, C3, ..., Ct−1} and the camera for the future frame Ct, we provide a probabilistic framework to predict the future frame via sampling from p(xt|x1, C2, x2, C3, ..., xt−1, Ct).
By conditioning multiple frames during sampling, it en-sures the consistency between generated views and histori-cal views. When inference with our Transformer model, we can start with a single input image and gradually increase the inputs using the predicted frames and previous frames.
However, it is very challenging to learn such a sequen-tial model with the autoregressive Transformer, which uses self-attention to model a large number of relations between every two patches across space and time in the input video.
To facilitate training, our key insight is that not every re-lational pair is equally important, and we can incorporate a locality constraint to guide the model to concentrate on the critical dependencies. Such locality constraints are in-troduced by the cameras. Intuitively, given a relative cam-era between two frames, we can roughly locate where the overlapping pixels are and where are the new pixels to syn-thesize. To incorporate this knowledge, we compute a bias using an MLP, which takes the relative camera as inputs, namely Camera-Aware Bias. We add this bias to the affin-ity matrix while performing the self-attention operation. In this way, each patch will have a stronger bias on depend-ing on or attending to relevant patches connected by the camera. Empirically, we find the Camera-Aware Bias not only makes the optimization much easier but also plays a vital role in enforcing the consistency between frames dur-ing generation.
We perform our experiments on multiple datasets, in-cluding the RealEstate10K [73] and Matterport3D [6], which mainly focus on 3D indoor scenes. Our model is able to synthesize new views with large camera motion, and gen-erate a long-term video given a single image input as visual-ized in Figure 1. Our method not only outperforms state-of-the-art approaches on standard view synthesis metrics, but also achieves a significantly better gain when evaluating in terms of long-range future frames. We highlight our main contributions as follows:
• A novel Transformer model on synthesizing a consis-tent long-term video given a single image and a trajec-tory as inputs.
• A novel locality constraint using Camera-Aware Bias, which facilitates optimization during learning and en-forces the consistency between generated frames.
• State-of-the-art performance in view synthesis. Our method outperforms baselines by a large margin on the long-term frames. 2.