Abstract (cid:3) (cid:5)(cid:6)(cid:4)(cid:2)(cid:8)(cid:1)(cid:7)
Despite the potential of multi-modal pre-training to learn highly discriminative feature representations from complementary data modalities, current progress is being slowed by the lack of large-scale modality-diverse datasets.
By leveraging the natural suitability of E-commerce, where different modalities capture complementary semantic in-formation, we contribute a large-scale multi-modal pre-The dataset comprises 5 training dataset M5Product. modalities (image, text, table, video, and audio), covers over 6,000 categories and 5,000 attributes, and is 500
⇥ larger than the largest publicly available dataset with a sim-ilar number of modalities. Furthermore, M5Product con-tains incomplete modality pairs and noise while also hav-ing a long-tailed distribution, resembling most real-world problems. We further propose Self-harmonized ContrAstive
LEarning (SCALE), a novel pretraining framework that integrates the different modalities into a uniﬁed model through an adaptive feature fusion mechanism, where the importance of each modality is learned directly from the modality embeddings and impacts the inter-modality con-trastive learning and masked tasks within a multi-modal transformer model. We evaluate the current multi-modal pre-training state-of-the-art approaches and benchmark their ability to learn from unlabeled data when faced with the large number of modalities in the M5Product dataset.
We conduct extensive experiments on four downstream tasks and demonstrate the superiority of our SCALE model, pro-viding insights into the importance of dataset scale and di-versity. Dataset and codes are available at 1. 1.

Introduction
Self-supervised learning has been driving the rapid de-velopment of ﬁelds such as computer vision and natural 1 https://xiaodongsuper.github.io/M5Product_dataset/
Equal contribution. ? Corresponding Author.
†
Category
Image
Caption
Table
Video
Audio
Image
Text
Bubble Matt Blind Box
Storage Ladder
Transparent Display Dust-proof Doll Hand-made
Jasmine Doll Acrylic Box Holder
Table
Item: Blind Box Ladder Storage Box
Brand: Tang Craftsman
Material: Wood
Color:  White, Light Gray, Dark Gray
Applicable People:  Adults
Applicable Scene: Study
Video
Audio
Figure 1. Our M5Product dataset contains a large variety of modalities (image, text, table, video and audio) that depict the categories, descriptions, materials, properties and purposes of E-commerce products, and diverse real-world data samples. language processing, as well as research on multi-modal representation learning.
In particular, it has been shown both from a theoretical [18] and a practical [16, 58] per-spective that large scale datasets with diverse modalities can effectively enhance the discrimination of generated fea-tures and thus improve the performance in vision-language tasks. However, current advances are severely limited by the lack of such large-scale diverse-modality datasets, with the largest public multi-modal datasets only containing text and image modalities and no category information [41].
Given the prevalence of online shopping in daily life, with its natural occurrence of multi-modal information
and diverse categories, multi-modal pre-training on E-commercial products has received increasing attention and led the developments of next-generation technology for sev-eral downstream tasks (e.g., multi-modal retrieval, multi-modal classiﬁcation, and clustering). However, even among the present product datasets (e.g., RPC checkout [48], Dress
Retrieval [9] and Product1M [55]), the number of categories is insufﬁcient to robustly verify the performance of down-stream tasks.
More importantly, research community the current mostly focuses on two modalities (text and image) in both general multi-modal and E-commerce datasets, while ignor-ing the importance of additional complementary informa-tion from structural data as well as video and audio modal-ities. Tabular data for instance can provide detailed infor-mation about properties and characteristics, such as brand, materials, attributes, and scenarios, while audio and video can convey different perspectives, scales, affordances, sell-ing points, characteristics, and use scenarios that are not obvious from images or text alone. The focus on these two modalities is partly due to the lack of datasets with diverse modalities as well as an under-exploration of approaches to balance the modality importance in these settings. In partic-ular, two key challenges are: 1) Modality Interaction: How to learn common representations from unimodal, bimodal, trimodal, and even multi-modal relationships between dif-ferent modalities using an elegant approach that scales to a large number of modalities; 2) Modality Noise: How to reduce the inﬂuence of modality noise (missing and incom-pleted modalties) during the training process.
To address the problem of insufﬁcient modality diver-sity and limited scale, while at the same time providing a challenging real-world scenario, we present a very large-scale E-commerce multi-modal product dataset M5Product, which is one of the largest and most diverse multi-modal product dataset to date. Our M5Product dataset contains more than 6 million multi-modal samples from 6,232 cat-egories and has more complex and diverse modalities than existing datasets. This allows M5Product to be used for more comprehensive evaluation of the practical application and generalization abilities of multi-modal pretraining mod-els and can improve the modality fusion performance, fa-cilitating new directions in multimodal research. Figure 1 shows the ﬁve modalities (image, caption, video, audio, and speciﬁcation (table)) of our dataset.
To further address the modality fusion limitations of ex-isting methods as well as handle modality noise, we propose a generic framework that takes ﬁve-modality data as inputs, as shown in Figure 2. The framework consists of a sim-ple and efﬁcient multi-modal ﬁve stream pre-training model named Self-harmonized ContrAstive LEarning (SCALE) and is evaluated on several downstream tasks and compared with several recent state-of-the-art vision-language mod-els [7, 27, 30, 38, 42, 45, 56]. SCALE increases modality alignment effectiveness by implementing a self-harmonized strategy that adapts the alignment weights between different modalities in the contrastive learning modules and masked tasks to adaptively integrate complementary modality infor-mation. In summary, our contributions are as follows:
• We provide the largest ﬁve-modality E-commerce dataset M5Product. Through its large scale, diver-sity, complex real scenarios and number of modali-ties, M5Product provides a comprehensive environ-ment for evaluating the generalization performance of multi-modal pre-training models.
• Our Self-harmonized Contrastive Learning (SCALE) framework learns adaptive modality interactions, re-sulting in more effective modality fusion. We com-pare SCALE to a comprehensive set of baseline meth-ods and demonstrate its superior performance on the
M5Product dataset.
• Interesting Observations: 1) In large-scale and com-plex scenarios, the complementary gain of different modalities increases. Learning modality alignment weights allows our SCALE framework to effectively coordinate complementary information to achieve bet-ter performance. 2) For multi-modal pre-training mod-els in the E-commerce domain, dataset scale and di-versity are relatively important for the downstream tasks. Given the large-scale and diverse products, our
SCALE framework generalizes better than other base-lines to downstream tasks. 2.