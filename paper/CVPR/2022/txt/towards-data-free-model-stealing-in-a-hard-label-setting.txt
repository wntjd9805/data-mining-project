Abstract
Machine learning models deployed as a service (MLaaS) are susceptible to model stealing attacks, where an adver-sary attempts to steal the model within a restricted access framework. While existing attacks demonstrate near-perfect clone-model performance using softmax predictions of the classification network, most of the APIs allow access to only the top-1 labels.
In this work, we show that it is indeed possible to steal Machine Learning models by accessing only top-1 predictions (Hard Label setting) as well, without access to model gradients (Black-Box setting) or even the training dataset (Data-Free setting) within a low query bud-get. We propose a novel GAN-based framework1 that trains the student and generator in tandem to steal the model ef-fectively while overcoming the challenge of the hard label setting by utilizing gradients of the clone network as a proxy to the victim’s gradients. We propose to overcome the large query costs associated with a typical Data-Free setting by utilizing publicly available (potentially unrelated) datasets as a weak image prior. We additionally show that even in the absence of such data, it is possible to achieve state-of-the-art results within a low query budget using synthetically crafted samples. We are the first to demonstrate the scala-bility of Model Stealing in a restricted access setting on a 100 class dataset as well. 1.

Introduction
Deep learning based systems have progressed leaps and bounds over the past few years, enabling their deployment in critical applications such as self-driving cars, surveil-lance systems and biomedical applications. Furthermore, organizations often provide pretrained machine learning models as a service (MLaaS) where the end user is allowed to query the model and get access to its predictions via APIs for use in various applications.
However, exposing the predictions of the models through queries makes the model susceptible to model stealing at-1Project Page: https://sites.google.com/view/dfms-hl
Figure 1. Model Stealing Attack and its vulnerabilities: An adversary queries the victim Model V with proxy data to obtain its labels. The labelled training data is used to train a clone Model C which can be further used by the adversary to stage membership inference [31], model inversion [10] or adversarial attacks [43]. tacks, which attempt to clone the victim model even in a black-box setting that restricts access to its gradients. Pro-tecting the privacy of an ML model is of paramount im-portance as organizations invest significant resources on cutting edge research and also on gathering and labelling large amounts of training data [14] for achieving competent performance on various tasks.
In addition, recent works
[29, 32, 35, 42] have shown that an adversary could train a substitute model via model stealing and use it for craft-ing adversarial examples [12] in a black-box setting, which poses a serious threat when the model is deployed in secu-rity critical applications. A stolen model could also com-promise the privacy of users by leaking confidential data through a membership inference attack [31] or model inver-sion [40, 41]. Fig.-1 showcases some of the possible ma-licious outcomes of Model Stealing.
In order to prevent model stealing attacks, some defenses attempt to perturb the softmax predictions of the model, while preserving its top-1 prediction [22]. In this work we consider the problem of model stealing in a more practical and challenging hard label setting, where only the top-1 prediction of the model
is accessible, and is thus effective even in the presence of such defenses.
In a model stealing attack, an adversary first queries a black-box victim model V with input data and obtains a prediction for it as shown in Fig.1. This data along with victim model predictions is used to train a clone model C.
In a practical scenario, the attacker would not have access to the training data, and hence we consider the problem of
Data-Free Model Stealing (DFMS) in this work. In such a data-free scenario, the attacker could use publicly available related datasets [26, 29], or synthetically generated samples
[34] to query the model. While the use of publically avail-able datasets assumes access to related data, the data-free generative approach could suffer from a large query budget, as the synthetic data can be far from the true training data distribution. In this work, we overcome both challenges by utilizing the available data that may be unrelated to the orig-inal training dataset, as a weak image prior. This enables the generation of representative samples under a low query budget, which is a crucial requirement in model stealing at-tacks, since MLaaS APIs work on a pay-per-query basis.
While existing algorithms for Data-Free Knowledge Dis-tillation [1, 9, 23, 25, 39] and Model Extraction [18, 34] achieve near perfect clone-model accuracy, there are addi-tional challenges in a Model Stealing framework due to the lack of access to gradients and a hard-label setting. There-fore, we consider a practical setup of data-free hard-label model stealing and overcome the challenges by utilizing the clone model’s gradients as a proxy to the gradients of the victim model. As the clone model starts training, it acts as a useful proxy for the victim model, and helps the generator learn to generate rich informative samples, which boosts the clone accuracy further. We explicitly enforce the generation of a class-balanced dataset from the generator that is also more aligned with the distribution of the training dataset.
Additionally, we also utilize an adversarial loss in a GAN framework [11], by using publicly available potentially un-related data, which we refer to as proxy data [1]. While this could be completely unrelated to the original training dataset, it still helps in enforcing a weak image prior in the generated data. This in turn reduces the number of victim model queries needed to perform Model Stealing. In fact, we show that it is possible to even use synthetic samples, such as multiple overlapping shapes with a planar back-ground, to steal a model in a completely data-free setting.
Our method achieves a significant improvement over
ZSDB3KD [37], a zero-shot data-free method in a simi-lar hard label setting using only synthetic samples. In the upcoming sections, we describe our approach in detail and show results on various datasets.
Our key contributions are listed below:
• We propose DFMS-HL, a Data-Free Model Stealing (DFMS) attack in a Hard-Label (HL) setting, to train a clone model with the help of unrelated proxy data or manually crafted synthetic data. We show that DFMS-HL outperforms the existing baseline ZSDB3KD [37] and results in a significant reduction of around 500× in the number of queries to the victim model.
• We demonstrate state-of-the-art results on the CIFAR-10 dataset using unrelated proxy samples, such as a given subset (containing 40 or 10 non-overlapping classes) from CIFAR-100, or synthetic data.
• We are the first to show noteworthy results of data-free model stealing on a dataset with a larger number of classes such as CIFAR-100. This demonstrates that our approach is both effective and scalable.
• We compare our method with the state-of-the-art model stealing attacks MAZE [17] and DFME [34], which additionally utilize softmax predictions of the victim model. Although we consider a more restrictive setting, we achieve a comparable accuracy using the
DFMS-HL approach, and a significant boost of around 3% using a Soft-Label (SL) variant of the proposed method (DFMS-SL). 2.