Abstract
We attempt to connect the data from complementary views, i.e., top view from drone-mounted cameras in the air, and side view from wearable cameras on the ground. Col-laborative analysis of such complementary-view data can facilitate to build the air-ground cooperative visual system for various kinds of applications. This is a very challeng-ing problem due to the large view difference between top and side views. In this paper, we develop a new approach that can simultaneously handle three tasks: i) localizing the side-view camera in the top view; ii) estimating the view direction of the side-view camera; iii) detecting and asso-ciating the same subjects on the ground across the com-plementary views. Our main idea is to explore the spatial position layout of the subjects in two views. In particular, we propose a spatial-aware position representation method to embed the spatial-position distribution of the subjects in different views. We further design a cross-view video col-laboration framework composed of a camera identification module and a subject association module to simultaneously perform the above three tasks. We collect a new synthetic dataset consisting of top-view and side-view video sequence pairs for performance evaluation and the experimental re-sults show the effectiveness of the proposed method. 1.

Introduction
With the advancement of mobile-camera technologies, human group events such as surprise parties, group games and sports events, are increasingly recorded by various mo-bile cameras. Wearable cameras, such as GoPro or mobile phone camera, worn by one of the persons (referred to as subjects in this paper) on the ground can provide side views of the human group [7, 24, 33]. Unmanned aerial vehicles (UAVs), such as drones in the air, can provide top views of the same human group [10]. The video analysis tasks
†Co-corresponding authors. in such two views are both well studied [17, 39, 40]. How-ever, the collaborative analysis of these two views is rarely studied. We can see from Figure 1 that the data collected from these two views well complement each other – the top-view video contains no mutual occlusions and well exhibits a global picture and the spatial distribution of the subjects, while the side-view video can capture the detailed appear-ance, behavior, and activity of subjects of interest in a much closer distance. We believe that their collaborative analysis can help build the air-ground cooperative visual system for comprehensive scene understanding, activity analysis, etc.
To achieve this goal, the first challenging problem is to effectively connect these two complementary views. For this we propose to study the following three tasks as shown in Figure 1. Task I: Camera location identification – to lo-calize the side-view camera in the top-view video; Task II:
View direction estimation – to infer the view direction of the side-view camera (in the top view); Task III: Cross-view multiple human detection and association – to detect every subject present in each view and identify the same person across the two views.
This is a very challenging problem and different from existing works. The biggest challenge lies in that the large (approximately orthogonal) view difference in our setting, which makes the classical features, e.g., appearance and motion, no longer useful for connecting the two views.
Specifically, Tasks I & II are different from prior works on identifying the first-person camera in a third-person cam-era [6,35], where the third-person cameras usually adopt the egocentric or surveillance cameras, and their altitudes and angles are similar with the first-person cameras. In this pa-per, the third-person camera is mounted on a drone, leading to very limited field-of-view (FOV) overlap with the first-person view. This makes prior approaches [6, 35] on mod-eling the cross-view correspondence fail in our tasks. Task
III looks like a specific person re-identification (re-id) prob-lem – for each subject in one view, re-identifying him/her in the other view. However, this is a very challenging person re-id problem because the same subject may show totally
Figure 1. An illustration of the top-view (a) and side-view (b) images. The former is taken by a camera mounted to a drone in the air and the latter is taken by a GoPro worn by a wearer who walks on the ground. To connect such two views, we attempt to answer the following three questions: Q1: Who takes the picture (b) in (a)? Q2: Where does he/she look at in (a)? Q3: Who are the same person across (a) and (b)? True answers of these three questions are shown in (c): the blue box indicates the side-view camera (Q1), the two blue arrows indicate view direction of the side-view camera (Q2) and identical-color boxes across (b) and (c) indicate the same persons (Q3). different appearance in top and side views, not to mention that the top view of subjects contains very limited features by only showing the top of heads and shoulders, as shown in Figure 1.
In this paper, we develop a new approach to explore and leverage the mutual dependence among the above three tasks to solve them simultaneously. Our main idea is to explore the spatial position layout of the subjects in two views. Specifically, we apply a human detection module to detect all the humans in the top and side views, respectively.
Based on the detection results, we use a spatial-aware po-sition representation to embed the spatial-position distribu-tion of the subjects in different views. To bridge the view gap across the top and side views, we apply the polar trans-form to the top-view representation for rendering the 360-degree subject distribution appearing in the FOV of the side-view camera. Based on such spatial-aware position repre-sentation, we design a camera identification module and a subject association module to simultaneously infer the side-view camera location and its view direction in the top view, and also match the subjects across the two views. In the experiments, we collect a new large-scale synthetic dataset consisting of rich annotations for model training and per-formance evaluation. Experimental results verify that the proposed method can effectively handle the proposed three tasks.
The main contributions of this paper are: ❶ This is the first deep model to jointly handle the above three funda-mental tasks for complementary-view crowded-scene anal-ysis, including the side-view camera localization (Task I), view direction estimation (Task II), and cross-view multi-human detection and association (Task III); ❷ We de-velop a new spatial-aware deep framework including the spatial-aware position representation and complementary-view collaboration network to model and associate the sub-jects’ spatial layout across the complementary views; ❸ We collect a new large-scale rich-annotation dataset of top-view and side-view videos for training and evaluating the proposed method. The dataset is released to the public at https://github.com/RuizeHan/DMHA. 2.