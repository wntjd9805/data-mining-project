Abstract
Learning discriminative deep feature embeddings by us-ing million-scale in-the-wild datasets and margin-based softmax loss is the current state-of-the-art approach for face recognition. However, the memory and computing cost of the Fully Connected (FC) layer linearly scales up to the number of identities in the training set. Besides, the large-scale training data inevitably suffers from inter-class con-flict and long-tailed distribution.
In this paper, we pro-pose a sparsely updating variant of the FC layer, named
Partial FC (PFC). In each iteration, positive class centers and a random subset of negative class centers are selected to compute the margin-based softmax loss. All class cen-ters are still maintained throughout the whole training pro-cess, but only a subset is selected and updated in each it-eration. Therefore, the computing requirement, the prob-ability of inter-class conflict, and the frequency of passive update on tail class centers, are dramatically reduced. Ex-tensive experiments across different training data and back-bones (e.g. CNN and ViT) confirm the effectiveness, robust-ness and efficiency of the proposed PFC. The source code is available at https://github.com/deepinsight/ insightface/tree/master/recognition. 1.

Introduction
Face recognition is playing an increasingly important role in modern life and has been widely used in many real-world applications, such as face authentication on mo-bile devices. Recently, face recognition has witnessed great advance along with the collection of large-scale train-ing datasets [3, 50], the evolution of network architectures
* corresponding author. InsightFace is a nonprofit Github project for 2D and 3D face analysis.
Figure 1. PFC picks the positive center by using the label and ran-domly selects a significantly reduced number of negative centers to calculate partial image-to-class similarities. PFC kills two birds (efficiency and robustness) with one stone (partial sampling).
[13, 30], and the design of margin-based and mining-based loss functions [8, 17, 25, 30, 35, 36, 38, 41].
Even though the softmax loss [3] and its margin-based
[8, 25, 35, 36] or mining-based [17, 38, 41] variants achieved state-of-the-art performance on deep face recognition, the training difficulty accumulates along with the growth of identities in the training data, as the memory and comput-ing consumption of the Fully Connected (FC) layer linearly scales up to the number of identities in the training set.
When there are large-scale identities in the training dataset, the cost of storage and calculation of the final linear matrix can easily exceed the capabilities of current GPUs, resulting in tremendous training time or even a training failure.
To break the computing resource constraint, the most straightforward solution is to reduce the number of classes used during training. Zhang et al. [42] propose to use a hashing forest to partition the space of class weights into small cells but the complexity of walking through the forest to find the closest cell is O(logN). Li et al. [22] randomly split training identities into groups and identities from each group share one anchor, which is used to construct the vir-tual fully-connected layer. Even though Virtual FC reduces
the FC parameters by more than 100 times, there is an ob-vious performance drop compared to the conventional FC solution. SST [11] and DCQ [21] directly abandon the
FC layer and employ a momentum-updated network to pro-duce class weights. However, the negative class number is constrained to past several hundred steps and two networks need to be maintained in the GPU.
Besides the training difficulty on large-scale datasets, celebrity images gathered from the internet and cleaned by automatic methods [45, 50] exhibit long-tailed distribu-tion [24, 48] as well as label noise [34]. Some well-known celebrities have abundant images (head classes) from the search engine while most celebrities have only a few images (tail classes) on the web. To keep hard training samples, the thresholds used in intra-class and inter-class cleaning steps in [50] are relatively relaxed, leaving label flip noises in the
WebFace42M dataset. Wang et al. [34] point out that label flips deteriorate the model’s performance heavier than out-liers as the margin-based softmax loss can not easily handle inter-class conflict during training.
To alleviate the above-motioned problems, we propose a sparsely updated fully connected layer, named Partial FC (PFC), for training large-scale face recognition. In the pro-posed PFC, the conventional FC layer is still maintained throughout the whole training process but the updating fre-quency is significantly decreased as we only sample parts of negative class centers in each iteration. As illustrated in
Fig. 1, positive class centers are selected and a subset of negative class centers are randomly selected to compute the margin-based softmax loss. As only a subset of inter classes is selected for each iteration, the computing requirement, the frequency of passive update on tail class centers, and the probability of inter-class conflict are dramatically reduced.
Extensive experiments across different training datasets and backbones (e.g. CNN [13] and ViT [10]) confirm the effec-tiveness, robustness and efficiency of the proposed PFC un-der a large range of sampling ratios. The advantages of the proposed PFC can be summarized as follows:
• Efficient. Under the high-performance mode, PFC-0.1 (sampling ratio) applied to ResNet100 can efficiently train 10M identities on a single server with around 2.5K samples per second, which is five times faster than the model parallel solution. Under the ultra-fast mode, the sampling ratio of PFC can be decreased to an extremely low status (around 0.01) where no ex-tra negative class is selected. For PFC-0.008 with
ResNet100 trained on WebFace42M, the computation cost on the FC layer can be almost neglected while the verification accuracy on IJB-C reaches 97.51%.
• Robust. PFC is amazingly robust under inter-class conflict, label-flip noise, and real-world long-tailed distribution. Assisted by a simple online abnormal inter-class filtering, PFC can further improve robust-ness under heavy inter-class conflict.
• Accurate. The proposed PFC has obtained state-of-the-art performance on different benchmarks, achiev-ing 98.00% on IJB-C and 97.85% on MFR-all. 2.