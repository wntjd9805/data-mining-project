Abstract
Most existing deep learning-based approaches for monoc-ular 3D object detection directly regress the dimensions of objects and overlook their importance in solving the ill-posed problem. In this paper, we propose a general method to learn appropriate embeddings for dimension estimation in monocular 3D object detection. Specifically, we consider two intuitive clues in learning the dimension-aware embed-dings with deep neural networks. First, we constrain the pair-wise distance on the embedding space to reflect the similarity of corresponding dimensions so that the model can take advantage of inter-object information to learn more discriminative embeddings for dimension estimation. Sec-ond, we propose to learn representative shape templates on the dimension-aware embedding space. Through the atten-tion mechanism, each object can interact with the learnable templates and obtain the attentive dimensions as the initial estimation, which is further refined by the combined features from both the object and the attentive templates. Experimen-tal results on the well-established KITTI dataset demonstrate the proposed method of dimension embeddings can bring consistent improvements with negligible computation cost overhead. We achieve new state-of-the-art performance on the KITTI 3D object detection benchmark. 1.

Introduction 3D object detection aims to estimate the 3D locations, poses, and sizes of surrounding objects and serves as one fundamental task in sensing the 3D environment. It has been widely used in fields including autonomous driving and robot navigation. Most existing methods rely on point clouds from
LiDAR devices [18, 40, 41, 51] or binocular images from stereo cameras [7, 20, 37, 45] for accurate 3D object detec-*Corresponding author.
Figure 1. Overview of the proposed dimension embeddings for monocular 3D object detection. We first project the features of objects onto the dimension-aware embedding space, where objects with similar dimensions are pulled together and objects with di-vergent dimensions are pushed away. Compared with the direct method in the dotted line, our method based on dimension embed-dings can utilize inter-object similarities as useful information to learn more discriminative features for dimension estimation. tion. Though these methods can benefit from direct distance information and achieve impressive performance, the Li-DAR sensors are still expensive for large-scale applications and stereo cameras can suffer from inaccurate calibrations.
In comparison, monocular methods, which only utilize one color camera and reason about appearance information, have been a promising alternative and receiving increasingly more attention [1, 6, 8, 16, 23, 27, 32, 36, 50, 52, 55].
Considering the ill-posed nature of monocular 3D object detection, the 3D dimension and the distance of objects are fundamentally entangled. Existing methods [3, 21, 26â€“28, 31,
52, 55] usually regard the estimation of object distance as the key challenge and design various approaches to solve the problem. Some methods [26, 36, 52, 55, 59] directly regress the object depth with keypoints-based detectors. Recently, more methods [19,21,27,28,32,55] compute the distance by combining the camera matrix, 2D geometric constraints, and 3D priors like the estimated dimensions and regressed 3D keypoints. The utilized 2D geometric constraints can include the sizes of 2D bounding boxes [19, 28] and predefined keypoints [21, 27, 55]. These more explainable methods can generate more accurate 3D locations and also generalize to different camera calibrations. To this end, we claim that the estimation of 3D dimensions is of vital importance for monocular 3D object detection. On the one hand, the 3D dimension directly serves as an element to form the desired 3D bounding box. On the other hand, solving the object distance with geometry heavily relies on the accuracy of 3D dimensions, because convolutional networks [11, 22, 54, 59] are skilled in predicting 2D bounding boxes and keypoints as accurate geometric constraints.
Though the importance of dimension estimation is clari-fied, most existing methods for monocular 3D object detec-tion simply regress the absolute 3D dimension [27, 31, 52] or the relative offset with respect to the mean object dimension of each category [28, 55, 59]. To further improve the dimen-sion estimation, we propose to leverage two important clues: (1) Since the consideration of similarity is fundamental for humans to reason about the world, we argue that modeling the similarities among dimensions of different objects is an effective way to provide additional information for dimen-sion reasoning. (2) The mapping relationship from visual appearances to object dimensions is strongly related to the underlying subcategories. For example, the dimension of one car can be easily predicted if its specific type can be determined and has been memorized by the network.
To explicitly embed these clues into the neural network, we first propose to learn a dimension-aware embedding space where two objects are projected to be close to each other if they have similar dimensions and far apart otherwise, as illustrated in Figure 1. This is achieved by encouraging the pair-wise embedding distance to be proportional to the distance between their corresponding dimensions. In this way, the network can fully exploit the rich information of similarities among different objects and learn more discrimi-native features for dimension estimation. To further consider the information of latent subcategories, we define learnable shape templates on the embedding space and enable the inter-action between extracted objects and the templates through the attention mechanism [46]. The attention scores and the template dimensions can be combined to produce the initial dimension estimations, which are further refined with the attentive features for final prediction.
We conduct extensive experiments on KITTI 3D ob-ject detection benchmark [10] with multiple state-of-the-art methods. The experimental results demonstrate our pro-posed dimension embedding is generalized, effective, and lightweight. The main contributions of the paper can be sum-marized in the following three aspects: (1) We propose to utilize the inter-object dimension similarities as useful infor-mation for dimension learning and formulate the solution as learning a dimension-aware embedding space. (2) We design the learnable shape templates to implicitly utilize the infor-mation of underlying subcategories. Through the attention mechanism, dimension embeddings of objects can aggregate attentive features and dimensions from these templates and formulate the dimension estimation as a coarse-to-fine refine-ment. (3) Our proposed dimension embedding module can be generally applied to existing keypoints-based methods for monocular 3D object detection and bring consistent improve-ment. We also achieve new state-of-the-art performance on the test set of KITTI 3D object detection benchmark, with real-time latency. 2.