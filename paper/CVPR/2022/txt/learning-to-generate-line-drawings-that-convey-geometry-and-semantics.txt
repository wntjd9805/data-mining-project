Abstract
This paper presents an unpaired method for creating line drawings from photographs. Current methods often rely on high quality paired datasets to generate line drawings.
However, these datasets often have limitations due to the subjects of the drawings belonging to a specific domain, or in the amount of data collected. Although recent work in unsupervised image-to-image translation has shown much progress, the latest methods still struggle to generate com-pelling line drawings. We observe that line drawings are en-codings of scene information and seek to convey 3D shape and semantic meaning. We build these observations into a set of objectives and train an image translation to map photographs into line drawings. We introduce a geometry loss which predicts depth information from the image fea-tures of a line drawing, and a semantic loss which matches the CLIP features of a line drawing with its corresponding photograph. Our approach outperforms state-of-the-art un-paired image translation and line drawing generation meth-ods on creating line drawings from arbitrary photographs. 1.

Introduction
Through introspection and experimentation, human artists have learned to create line drawings that provide compelling depictions of shape and meaning. A longstand-ing goal of non-photorealistic rendering is to reproduce this feat and, given an input image, to automatically gen-erate line drawings that are effective at conveying geom-etry and identity. Manually instilling these qualities into computer-generated line drawings is difficult however be-cause the goals are defined in elusive terms of human per-ception and cognition. Generating line drawings from pho-tographs presents additional challenges: most photographs lack ground-truth geometry data, and often portray complex scenes with multiple subjects and interactions. Naturally, it would make sense to learn from drawings created by hu-mans or to use humans to evaluate automatic line draw-ing methods. Unfortunately, the creation of such datasets is challenging and scalability is low.
In this paper, we seek to automatically generate effective line drawings from photographs without requiring paired training data and without requiring human judgment of the implied shape. Our key idea is to view the problem as an encoding through a line drawing and to maximize the quality of this encoding through explicit geometry, seman-tic, and appearance decoding objectives. Our method ap-proaches line drawing generation as an unsupervised image translation problem which uses various losses to assess the information communicated in a line drawing. This evalua-tion is performed by deep learning methods which decode depth, semantics, and appearance from line drawings. The aim is for the extracted depth and semantic information to match the scene geometry and semantics of the input pho-tographs. Appearance preservation follows from cycle con-sistency [45, 81, 86]. With these objectives, our method is able to create convincing line drawings given unpaired data.
Our main contributions are as follows. We present an unsupervised method for automatic line generation which explicitly instills geometry and semantic information into drawings. We apply our method on many styles of line drawings and present results in Section 4. We also provide analysis of the geometry and semantic information con-veyed by our drawings, visual comparisons against several baselines, and an ablation study. 2.