Abstract
While real world challenges typically define visual cat-egories with language words or phrases, most visual clas-sification methods define categories with numerical indices.
However, the language specification of the classes provides an especially useful prior for biased and noisy datasets, where it can help disambiguate what features are task-relevant. Recently, large-scale multimodal models have been shown to recognize a wide variety of high-level con-cepts from a language specification even without additional image training data, but they are often unable to distin-guish classes for more fine-grained tasks. CNNs, in contrast, can extract subtle image features that are required for fine-grained discrimination, but will overfit to any bias or noise in datasets. Our insight is to use high-level language specifi-cation as advice for constraining the classification evidence to task-relevant features, instead of distractors. To do this, we ground task-relevant words or phrases with attention maps from a pretrained large-scale model. We then use this grounding to supervise a classifier’s spatial attention away from distracting context. We show that supervising spatial attention in this way improves performance on classifica-tion tasks with biased and noisy data, including ∼3−15% worst-group accuracy improvements and ∼41−45% relative improvements on fairness metrics. 1.

Introduction
When trained with limited or biased data, visual models often learn unwanted correlations. For example, consider building a classifier to distinguish two fine-grained cate-gories of birds: “landbird” and “waterbird”. The background features from their corresponding habitats such as forests or beaches might be highly or perfectly correlated with the numerical class labels. A baseline model may mistakenly learn the unintended “location” task instead of the actual task, and fail on examples of birds out of their usual habitat (Fig. 1). However, knowledge that the task is about birds can disambiguate what the model is meant to learn.
*Equal contribution.
Figure 1. Guiding attention with language. Sample attention from the Waterbirds biased dataset. During training, Landbirds mostly appear on land backgrounds and Waterbirds mostly appear on water backgrounds. At testing, each class appears equally on land or on water. A CNN trained on this task learns to look at the background, but if we use a multimodal model to translate the language specification “a photo of a bird” into spatial supervision, we can ensure that our CNN learns task-relevant features.
Previous work has considered incorporating knowledge of the task as language specifications in the form of class names or class descriptions which can directly serve as a prior over visual model parameters [33, 47]. Several zero-shot meth-ods condition models on attribute labels [17, 26, 49] (e.g., beak shape or wing color) or class descriptions [7, 19, 32, 54] (e.g., from Wikipedia) to enable transfer to unseen classes.
However, this relies on the language specification being class-discriminative – an assumption which does not hold for some real-world tasks where only high-level task specifi-cation is given (e.g., in Fig. 1, we may only know that this is a “bird” dataset, without the class names being provided or even existing yet). Additionally, simply conditioning on lan-guage embeddings may not prevent a model from attending to spurious correlations in biased datasets. are language
Even when specifications class-discriminative, such models will perform poorly when there is insufficient image and text data to learn a multimodal model for rare or fine-grained classes (e.g., a large-scale model such as CLIP [33] may not have seen enough exam-ples of the relatively rare “landbird” or “waterbird” classes during pretraining to have good zero-shot performance).
To address these limitations, we propose a new framework called Guiding visual Attention with Language Specifi-cation, or GALS, in which we translate available language specification provided by the task metadata into spatial at-tention that is used to supervise a CNN’s attention during training. Fig. 1 displays how GALS is able to pull the model’s attention away from the distractor features while retaining enough flexibility to pick up on fine-grained features which were not captured by the multimodal model.
Specifically, we first leverage an off-the-shelf pretrained vision-language model to ground textual information into each given image and obtain a respective saliency map. This is efficient and involves no additional overhead (i.e., no need for training or per-instance manual annotation). Next, we aim to leverage the obtained saliency map to inform the vi-sual classifier. To do this, we guide the classifier’s attention towards the area highlighted by the saliency from the lan-guage specification. Finally, the visual classifier still needs to solve the more fine-grained task, after obtaining the high-level attention guidance. It thus retains some flexibility, e.g. it may even attend to some useful (non-harmful) context. In practice, we use the recent powerful CLIP [33] model to ground textual information into images. We leverage the
“Right for the Right Reasons” method [37] to enforce that the classifier indeed attends according to the given guidance.
With this approach, we can incorporate language specifi-cation via an auxiliary loss during training, and thus the vision-language model is not needed during inference.
We show how GALS can assist in training on data with explicit and implicit bias. On the synthetic Waterbirds dataset [39] which contains a known, explicit bias (the im-age backgrounds), our method is able to achieve ∼2−7% per-group accuracy improvements over baselines, includ-ing a model which uses an unsupervised attention mecha-nism instead of guidance from language. GALS also shows a 15% improvement on the worst-group accuracy in the challenging scenario where class labels are perfectly cor-related with the distracting backgrounds (Sec. 4.2). For implicit bias, where training and test distributions differ in unknown ways, we see that GALS achieves ∼41−45% rela-tive improvements on fairness metrics for apparent gender recognition (Sec. 4.4). We also show a 2% accuracy im-provement on a red-meat classification task from a subset of Food-101 [2], where an implicit bias emerges from noisy training labels (Sec. 4.3). Lastly, we demonstrate that the quality of classifiers’ explanations improves with the given advice (12.8% improvement in Pointing Game [50] accuracy, described in Sec. 4.5). Code and datasets can be found at https://github.com/spetryk/GALS. 2.