Abstract
To overcome the overfitting issue of dehazing models trained on synthetic hazy-clean image pairs, many recent methods attempted to improve models’ generalization abil-ity by training on unpaired data. Most of them simply for-mulate dehazing and rehazing cycles, yet ignore the physi-cal properties of the real-world hazy environment, i.e. the haze varies with density and depth. In this paper, we pro-pose a self-augmented image dehazing framework, termed
D4 (Dehazing via Decomposing transmission map into
Density and Depth) for haze generation and removal. In-stead of merely estimating transmission maps or clean con-tent, the proposed framework focuses on exploring scatter-ing coefficient and depth information contained in hazy and clean images. With estimated scene depth, our method is capable of re-rendering hazy images with different thick-nesses which further benefits the training of the dehazing network.
It is worth noting that the whole training pro-cess needs only unpaired hazy and clean images, yet suc-ceeded in recovering the scattering coefficient, depth map and clean content from a single hazy image. Comprehensive experiments demonstrate our method outperforms state-of-the-art unpaired dehazing methods with much fewer pa-rameters and FLOPs. Our code is available at https:
//github.com/YaN9-Y/D4. 1.

Introduction
Haze is a kind of natural phenomenon caused by the scat-tering effect of aerosol particles in the atmosphere. It can cause severe disclarity to visual content, which brings trou-ble to both human observers and computer vision systems.
⋆Corresponding author.
This work was supported by the National Natural Science Foundation of China under Grant no. 62072327, and TSTC under Grant no. 20JC-QNJC01510. Dr Chaoyue Wang is supported by ARC FL-170100117.
Dehazing methods aim to remove the haze and improve the visual quality of real-world hazy images, which can bene-fit computer vision tasks like image segmentation [4, 38], object detection [15, 34] on the hazy weather.
The degradation of haze effect can be formulated by
Koschmieder’s law [29, 30]:
I(z) = J(z)t(z) + A(1 − t(z)), (1) where I(z) indicates the z-th pixel of observed hazy image,
J(z) and A are the scene radiance and global atmosphere light, respectively. Transmission map t(z) = e−βd(z) is defined by the scene depth d(z) and the scattering coeffi-cient β that reflects the haze density.
With the great learning capability of deep neural net-works [12, 13], plenty of methods were proposed to solve the image restoration tasks [3, 25, 35, 46, 48, 49], as well as image dehazing [7, 27, 31, 36], in a supervised manner.
Through training on a large amount of synthetic hazy-clean image pairs, supervised deep dehazing methods achieved impressive results on specific test sets.
However, there exists a relatively large domain gap be-tween synthetic and real-world hazy images. Dehazing models that are solely trained on paired synthetic images are easy to over-fitting, and generalize poorly to real-world hazy conditions.
Since the desired real-world hazy&clean image pairs are nearly unreachable, in recent years, many unpaired deep learning methods were proposed to explore the dehazing cues from unpaired training data. Among them, construct-ing the dehazing cycle and rehazing cycle is widely adopted
[8, 9, 17, 26, 45, 50] since it provides a simple and effective scheme for keeping content consistency while performing
If the hazy and clean image do-domain transformation. mains can be accurately modeled, the cycle framework is expected to gain promising performance on unpaired dehaz-Figure 1.
Illustration of (i) previous CycleGAN-based dehazing methods, (ii) our proposed method and, (iii) the results comparison.
Compared to RefineDNet [51], our method can better remove the haze. The CycleGAN-based methods can only generate haze with fixed density for certain clean image, and the generated haze is not consistent with the depth. ing. However, we argue that simply inheriting the Cycle-GAN [52] framework from unpaired image-to-image trans-lation methods would fail to handle unpaired image dehaz-ing tasks. Existing cycle-based dehazing methods ignore the physical properties of real-world hazy environments, i.e. the real-world haze varies with density and depth. As shown in Fig. 1 (i) and (iii)-(e), CycleGAN-based methods easily collapse to synthesizing haze with fixed density, and may incorrectly model the haze effect, e.g. the haze should be thicker accompanied with the increasing of scene depth.
In this paper, we propose a novel dehazing framework termed D4, i.e. Dehazing via Decomposing transmission map into Density and Depth, for unpaired haze synthesis and removal. Following the hazy image formation process, we explicitly model the scattering coefficient β and depth map d(z) of the target scene. As shown in Fig. 1 (ii), on the Dehzing-Rehazing branch, our model is trained to di-rectly estimate both transmission map and scattering coeffi-cient from a hazy image. According to the physics process expressed in Eq. (1), scene depth and clean content then can be derived directly. On the Hazing-Dehazing branch, our model aims to estimate the depth information of the in-put clean image, then synthesize hazy images with different densities, i.e. scattering coefficients. Considering the fact that ‘spatial-variant haze thickness provides an additional cue for perceiving scene depth’, depth maps estimated from hazy images act as pseudo ground truth of the depth of clean images. Similarly, in the Hazing-Dehazing branch, the ran-domly sampled scattering coefficients β in the Hazing step act as pseudo ground-truth of the density predicted in the
Dehazing step.
Finally, with our novel unpaired dehazing framework, we can (i) estimate depth maps from clean images (Fig. 1 (g)); (ii) synthesize realistic hazy images with various den-sities (data augmentation) (Fig. 1 (h),(i)); and (iii) achieve better dehazing performance than state-of-the-arts unpaired dehazing methods with less parameters and FLOPs.
Overall, our contributions can be summarized as follows:
• We propose a novel unpaired dehazing framework, which explicitly models the scattering coefficient (i.e. density) and the depth map of hazy scenes. The pro-posed physics-based framework largely alleviates the ill-posed problem that existed in existing unpaired de-hazing methods.
• Inspired by the intuition: ‘spatial-variant haze thick-ness reflect scene depth’, our model learns to predict depth information from hazy images. Then, with only unpaired hazy and clean images, our model are trained to predict depth information from clean images.
• With estimated scene depth, our model is able to gen-erate hazy images with different thickness by altering the scattering coefficient. Such characteristic acts as a self-argumentation strategy for better training the de-hazing network.
• Extensive experiments on both synthetic and real im-ages are conducted to reveal the effectiveness of our design. The proposed D4 framework shows clear ad-vantages on generalization ability over state-of-the-arts methods. 2.