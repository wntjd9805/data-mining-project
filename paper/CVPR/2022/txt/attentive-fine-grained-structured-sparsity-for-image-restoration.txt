Abstract
Image restoration tasks have witnessed great perfor-mance improvement in recent years by developing large deep models. Despite the outstanding performance, the heavy computation demanded by the deep models has re-stricted the application of image restoration. To lift the re-striction, it is required to reduce the size of the networks while maintaining accuracy. Recently, N :M structured pruning has appeared as one of the effective and practical pruning approaches for making the model efﬁcient with the accuracy constraint. However, it fails to account for differ-ent computational complexities and performance require-ments for different layers of an image restoration network.
To further optimize the trade-off between the efﬁciency and the restoration accuracy, we propose a novel pruning method that determines the pruning ratio for N :M struc-tured sparsity at each layer. Extensive experimental results on super-resolution and deblurring tasks demonstrate the efﬁcacy of our method which outperforms previous prun-ing methods signiﬁcantly. PyTorch implementation for the proposed methods will be publicly available at https:
//github.com/JungHunOh/SLS_CVPR2022 1.

Introduction
Advances in deep learning has brought success in image restoration tasks such as super-resolution [29, 48] and de-blurring [39, 60, 61]. Due to the heavy computational bur-den required by such methods, however, computing high-resolution images in practical applications has been chal-lenging. Network pruning is one of the most popular tools to alleviate the computational burden of neural networks by eliminating weights that are less critical to the accuracy. It has shown remarkable efﬁcacy in ﬁnding submodels for a better trade-off between accuracy and efﬁciency for image classiﬁcation [9, 15, 17, 30, 34] and segmentation [14, 57].
Unstructured pruning [11, 13, 26] aims to ﬁnd and re-move individual weights that have relatively less impact on model accuracy. Still, accelerating the resulting models is difﬁcult due to irregular sparsity patterns of weight tensors,
Trade-off between image restoration performance
Figure 1. (PSNR) vs computational costs (MACs) on super-resolution (top) and deblurring tasks (bottom). We compare our method to the magnitude-based ﬁlter pruning [28] and the existing methods on
N :M sparsity (One-shot pruning [37] and SR-STE [64]). considering the complex nature of parallelization on GPUs.
On the other hand, structured pruning removes predeter-mined structures (e.g., a ﬁlter in convolution layers [28] or a channel of feature maps [17]) to enable the acceleration of pruned networks on GPUs. However, we empirically ﬁnd image restoration models to be often susceptible to substan-tial performance degradation from structured pruning.
Recently, N :M ﬁne-grained structured sparsity [19, 37, 64] has emerged as a better alternative, combining the strengths of both pruning methods: namely, ﬁne-grained sparsity from unstructured pruning and hardware acceleration-ability from structured pruning. N :M struc-tured sparsity enforces N number of weights in each group of M number of consecutive weights to have non-zero val-ues. Such sparsity constraint has the potential of hardware acceleration, where 2:4 sparsity pattern has recently been supported in NVIDIA Ampere generation GPUs [37]. How-ever, training a network with N :M sparsity has been proven to be difﬁcult, which the existing works on N :M sparsity have focused on improving [37, 64]. Such difﬁculty has prevented the direct application of the already developed pruning techniques, such as pruning with layer-wise vary-ing pruning ratios [27, 34], which is known to be crucial to the performance of the pruned networks. Particularly, we observed that several layers (e.g. the last upsampling layer) in image restoration networks are very sensitive to pruning with respect to the performance.
Here, we propose a layer-wise N :M sparsity search framework for efﬁcient image restoration networks, named
Searching for Layer-wise N :M structured Sparsity (SLS).
In the prior arts [9, 17, 30, 31, 33], a ﬁlter or a channel is used as a unit of pruning but it is challenging to deﬁne the pruning unit in the case of N :M sparsity. To this end, we propose to consider the original weight tensor as the sum of sparse tensors whose conﬁgurations are determined by the magnitude of weights. We use each of sparse tensor as the unit of pruning in our N :M sparsity search problem.
To learn how many units to preserve, we propose a train-able score for each pruning unit, which is designed to en-sure units with the lowest magnitude-based importance are removed ﬁrst for better performance.
Furthermore, since image restoration tasks often have different computational constraints, we present an adaptive inference method that uses several models trained by SLS with different efﬁciency. The proposed adaptive inference technique determines which pruned model should be used at inference time depending on the restoration difﬁculty of an input image patch. The adaptive inference method fur-ther improves the efﬁciency-accuracy trade-off and enables a ﬂexible adoption of the computational budgets.
We summarize our contributions as follows:
• Observing the pruning sensitivity of each layer to be dif-ferent, we propose a novel method, SLS, to determine the layer-wise N :M sparsity levels.
• From the mixture of the pruned models with different computational costs and accuracy, we propose to ﬁnd a better trade-off with additional controllability at infer-ence time.
• By extensive experiments with super-resolution and de-blurring, we empirically validate our pruned models gen-erally achieve state-of-the-art performance. 2.