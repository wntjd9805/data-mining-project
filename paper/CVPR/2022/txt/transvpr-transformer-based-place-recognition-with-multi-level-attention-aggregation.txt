Abstract
Visual place recognition is a challenging task for appli-cations such as autonomous driving navigation and mobile robot localization. Distracting elements presenting in com-plex scenes often lead to deviations in the perception of vi-sual place. To address this problem, it is crucial to integrate information from only task-relevant regions into image rep-In this paper, we introduce a novel holistic resentations. place recognition model, TransVPR, based on vision Trans-formers. It benefits from the desirable property of the self-attention operation in Transformers which can naturally ag-gregate task-relevant features. Attentions from multiple lev-els of the Transformer, which focus on different regions of interest, are further combined to generate a global image representation. In addition, the output tokens from Trans-former layers filtered by the fused attention mask are con-sidered as key-patch descriptors, which are used to perform spatial matching to re-rank the candidates retrieved by the global image features. The whole model allows end-to-end training with a single objective and image-level supervi-sion. TransVPR achieves state-of-the-art performance on several real-world benchmarks while maintaining low com-putational time and storage requirements. 1.

Introduction
Visual Place Recognition (VPR) is an essential and chal-lenging problem in autonomous driving and robot localiza-tion systems, which is usually defined as an image retrieval problem [27]. Given a query image, the algorithm has to determine whether it is taken from a place already seen and identify the corresponding images from a database. There are two types of image representations commonly used in
VPR tasks. Global image features [2, 8, 18, 21, 37, 38, 54] abstract the whole image into a compact feature vector without geometrical information. Patch-level descriptors
*Equal contribution.
â€ Corresponding author.
Visualization of multi-level attentions from
Figure 1.
TransVPR. Low-level attention maps mainly focus on small ob-jects and textural areas on the surface of buildings. Mid-level at-tentions focus on objects in the air, such as street lamps and tree canopies, while high-level attentions tend to outline the contours of the ground and the lane lines. All these attention masks are combined to generate global image representations as well as key-patch descriptors.
[12, 14, 23, 26, 32, 60] describe particular patches or key-points in an image and can be used to perform spatial match-ing between image pairs using cross-matching algorithms (e.g. RANSAC [16]). To achieve a good trade-off between accuracy and efficiency, a commonly used two-stage strat-egy is to retrieve candidates with global features and then re-rank them using patch-level descriptor matching [40,51].
Several recent researches [6, 19, 45, 52] have attempted to design a holistic system to extract both types of features.
Most recently, Patch-NetVLAD [19] has used an integral feature space to derive patch descriptors from the global im-age feature and has achieved state-of-the-art performance in several benchmarks. However, an important factor that may reduce the robustness of Patch-NetVLAD is that its ex-tracted features unselectively encode the information from all regions of an image.
It needs to be emphasized that the ability to identify task-relevant regions in an image is critical to VPR systems.
This is because distracting elements and dynamic objects in a scene (e.g. the sky, the ground, untextured walls, cars, pedestrians, etc.) are not helpful to recognize a place and se-riously harm the VPR performance [27]. In order to detect keypoints or regions of interest, several CNN-based meth-ods have been proposed [9, 12, 23, 24, 57, 59, 60].
Recently, the Transformer [56] architecture has obtained competitive results in multiple computer vision tasks [7, 13]. Unlike CNNs, the self-attention operation in vision
Transformers can dynamically aggregates global contex-tual information and implicitly select task-relevant infor-mation. To benefit from this property of vision Transform-ers and improve the robustness of place recognition, this work brings the following contributions: Firstly, we pro-pose a Transformer-based novel place recognition model,
TransVPR, which can adaptively extract robust image rep-resentations from distinctive regions in an image. Sec-ondly, inspired by previous studies on CNNs which com-bine multi-level feature maps to enrich image representa-tions [9, 59, 61], we fuse multi-level attentions, which focus on different semantically meaningful regions (see Fig. 1), to generate global image representations. The effectiveness of this procedure is demonstrated by qualitative and quan-titative experiments. Finally, the output tokens of Trans-former layers filtered by the fused attention mask are further employed as patch-level descriptors to perform geometrical verification. All components in TransVPR are tightly cou-pled so that the whole model allows end-to-end optimiza-tion with a single training objective and only image-level supervision. Experimental results show that the proposed
TransVPR achieves superior performance on VPR bench-mark datasets with low computational time and memory requirements. It outperforms the state-of-the-art VPR ap-proaches [2,6,17,19,41] by significant margins (5.8% abso-lute increase on Recall@1 compared with the best baseline method, DELG [6]). 2.