Abstract
Transformers have achieved great success in pluralis-tic image inpainting recently. However, we find existing transformer based solutions regard each pixel as a token, thus suffer from information loss issue from two aspects: 1) They downsample the input image into much lower res-olutions for efficiency consideration, incurring information loss and extra misalignment for the boundaries of masked regions. 2) They quantize 2563 RGB pixels to a small num-ber (such as 512) of quantized pixels. The indices of quan-tized pixels are used as tokens for the inputs and predic-tion targets of transformer. Although an extra CNN net-work is used to upsample and refine the low-resolution re-sults, it is difficult to retrieve the lost information back. To keep input information as much as possible, we propose a new transformer based framework “PUT”. Specifically, to avoid input downsampling while maintaining the compu-tation efficiency, we design a patch-based auto-encoder P-VQVAE, where the encoder converts the masked image into non-overlapped patch tokens and the decoder recovers the masked regions from the inpainted tokens while keeping the unmasked regions unchanged. To eliminate the information loss caused by quantization, an Un-Quantized Transformer (UQ-Transformer) is applied, which directly takes the fea-tures from P-VQVAE encoder as input without quantization and regards the quantized tokens only as prediction tar-gets. Extensive experiments show that PUT greatly outper-forms state-of-the-art methods on image fidelity, especially for large masked regions and complex large-scale datasets. 1.

Introduction
Image inpainting, which focuses on filling meaningful and plausible contents in missing regions for the damaged
*Work done during an internship at Microsoft
†Corresponding author
Figure 1. Top: Existing transformer based methods [49]. The output is produced by ICT [49]. Bottom: Our transformer based method. “Tokenize” here means getting the indices of quantized pixels or features, and “De-Tokenize” is the inverse operation. images, has always been a hot topic in computer vision areas and widely used in various applications [1, 35, 39, 42, 47, 48, 56]. Traditional methods [1, 3, 9] based on tex-ture matching can handle simple cases very well but strug-gle for complex natural images. In the last several years, benefiting from development of CNNs, tremendous success
[28, 30, 34, 54] has been achieved by learning on large-scale datasets. However, due to the inherent properties of CNNs, i.e., local inductive bias and spatial-invariant kernels, such methods still do not perform well in understanding global structure and inpainting large masked/missing regions.
Recently, transformers have demonstrated their power in various vision tasks [4–8, 13, 16, 37, 50], thanks to their ca-pability of modeling long-term relationship. Some recent works [49] also attempt to apply transformers for pluralis-tic image inpainting, and have achieved remarkable success in better diversity and large region inpainting quality. As shown in the top row of Figure 1, they follow the similar design: 1) Downsample the input image into lower resolu-tions and quantize the pixels; 2) Use the transformer to re-patches as the prediction targets for masked patches, but takes the un-quantized feature vectors from the encoder as input. Compared to taking the quantized tokens as input, this design can avoid the information loss and help UQ-Transformer make more accurate predictions.
To demonstrate the superiority, we conduct extensive ex-periments on FFHQ [25], Places2 [61] and ImageNet [11].
The results show that our method outperforms CNN based pluralistic inpainting methods by a large margin on different evaluation metrics. Benefiting from less information loss, our method also achieves much higher fidelity than exist-ing transformer based solutions, especially for large region inpainting and complex large-scale datasets. 2.