Abstract
Classification on long-tailed distributed data is a challenging problem, which suffers from serious class-imbalance and accordingly unpromising performance es-pecially on tail classes. Recently, the ensembling based methods achieve the state-of-the-art performance and show great potential. However, there are two limitations for cur-rent methods. First, their predictions are not trustworthy for failure-sensitive applications. This is especially harmful for the tail classes where the wrong predictions is basically fre-quent. Second, they assign unified numbers of experts to all samples, which is redundant for easy samples with excessive computational cost. To address these issues, we propose a Trustworthy Long-tailed Classification (TLC) method to jointly conduct classification and uncertainty estimation to identify hard samples in a multi-expert framework. Our
TLC obtains the evidence-based uncertainty (EvU) and ev-idence for each expert, and then combines these uncer-tainties and evidences under the Dempster-Shafer Evidence
Theory (DST). Moreover, we propose a dynamic expert en-gagement to reduce the number of engaged experts for easy samples and achieve efficiency while maintaining promis-ing performances. Finally, we conduct comprehensive ex-periments on the tasks of classification, tail detection, OOD detection and failure prediction. The experimental results show that the proposed TLC outperforms existing methods and is trustworthy with reliable uncertainty. 1.

Introduction
Data in real-world applications are usually long-tailed distributed over a series of categories [28, 34, 37, 44, 50, 51].
The frequencies of different categories vary a lot, with the head classes abundant in training samples, and the tail
*Corresponding author. classes having only few training samples. Besides, there may also be new categories which models have not seen be-fore [37], exceeding the tail of long-tailed distribution and being termed as out-of-distribution (OOD) data [32]. The long-tailed classification is very challenging since models need to handle the few-shot learning problem (and even with OOD data sometimes) for the tail classes, and the over-all class-imbalance (models are trained on much more head samples than tail samples) would also deviate the models to focus extremely on the head classes [7]. These problems cause the models to perform unpromisingly especially on the tail classes [5, 19].
Existing algorithms address long-tailed classification mainly by rebalancing the training of different classes to as-sign larger importance to tail samples [7, 10, 33, 52], trans-ferring knowledge between the head and tail classes [37, 57], ensembling statically sampled data groups [53, 55] (complementary ensembling), or ensembling individual classifiers in a multi-expert framework [51] (redundant en-sembling). The redundant ensembling achieves the state-of-the-art performance mainly by reducing the model variance to obtain robust predictions [51]. However, there are two major limitations for redundant ensembling methods. First, they are usually vulnerable to yielding unreliable prediction (i.e., over-confident prediction). This also prevents the en-sembling methods from perceiving the wrong predictions and OOD samples, and is especially harmful for the tail classes where the predictions have averagely more errors than the head classes [5, 19]. Consequently, their deploy-ment in some failure-sensitive applications (e.g., disease di-agnosis [2], automatic driving [54] and robotics [12]) is lim-ited. Second, redundant ensembling usually assumes that all classifiers should be trained on all samples [51], which is static and often induces excessive computational cost by uniformly assigning experts to all classes. The expert re-dundancy is severe especially on head classes, where com-petitive classification performance can be achieved with
much fewer experts.
For these issues, we propose a novel Trustworthy Long-tailed Classification (TLC) method to jointly conduct clas-sification and uncertainty estimation in a unified frame-work. First, we introduce the evidence and its associ-ated uncertainty under the Dempster-Shafer Evidence The-ory (DST) [13]. With the help of evidence-based uncer-tainty (EvU), our model can perceive hard samples in long-tailed classification, promoting the trustworthiness by de-tecting the tail and OOD samples, and identifying poten-tially wrong predictions. Second, we propose to combine the evidences from different experts with a uncertainty-based multi-expert fusion strategy under the Dempster’s rule. We leverage the advantages of multiple experts to ob-tain accurate uncertainty and robust prediction. Moreover, we propose to reduce the number of engaged experts dy-namically for the easy samples to jointly promote the effi-ciency while maintaining promising performances. For ex-ample, the actually needed number of experts for the head classes is less than that for tail classes (the head classes con-tain more easy samples). Therefore, we need to dynam-ically assign fewer experts in the training of head classes for efficiency. We achieve the dynamic expert engagement by incrementally adding experts when the previously added experts are all uncertain about their predictions. The main contributions are summarized as follows:
• We introduce the evidence-based uncertainty (EvU) to promote the trustworthiness of long-tailed classifica-tion. To the best of our knowledge, the proposed TLC is the first work asserting trustworthiness in long-tailed classification.
• We propose a multi-expert fusion strategy based on the uncertainty of each expert under the Dempster-Shafer
Evidence Theory (DST), which promotes the classi-fication performance and trustworthiness by reliably perceiving hard samples.
• We achieve efficiency in training multiple experts by dynamically reducing the engaged experts with uncer-tainty, and obtain promising performances meanwhile.
• We conduct experiments on classification, tail & OOD sample detection and failure prediction, and evaluate the results with diverse metrics, which validates that the proposed TLC outperforms existing methods in the above tasks and is trustworthy with reliable uncer-tainty. The code1 is publicly available. 2.