Abstract
Visual grounding focuses on establishing fine-grained alignment between vision and natural language, which has essential applications in multimodal reasoning systems. Ex-isting methods use pre-trained query-agnostic visual back-bones to extract visual feature maps independently without considering the query information. We argue that the visual features extracted from the visual backbones and the fea-tures really needed for multimodal reasoning are inconsis-tent. One reason is that there are differences between pre-training tasks and visual grounding. Moreover, since the backbones are query-agnostic, it is difficult to completely avoid the inconsistency issue by training the visual back-bone end-to-end in the visual grounding framework. In this paper, we propose a Query-modulated Refinement Network (QRNet) to address the inconsistent issue by adjusting inter-mediate features in the visual backbone with a novel Query-aware Dynamic Attention (QD-ATT) mechanism and query-aware multiscale fusion. The QD-ATT can dynamically compute query-dependent visual attention at the spatial and channel levels of the feature maps produced by the visual backbone. We apply the QRNet to an end-to-end visual grounding framework. Extensive experiments show that the proposed method outperforms state-of-the-art methods on five widely used datasets. Our code is available at https:
//github.com/LukeForeverYoung/QRNet. 1.

Introduction
Visual grounding [25, 32, 36, 59], i.e., localizing the referent object in an image according to the given natu-ral language query, is a fundamental component of multi-The work was done when Jiabo Ye was working as an intern at Alibaba
DAMO Academy.
Figure 1. (a) A typical end-to-end visual grounding framework that uses two individual encoders to extract visual and textual fea-tures for cross-modal interaction. (b) Our visual grounding frame-work based on a Query-modulated Refinement Network (QRNet). modal reasoning system. Compared with conventional ob-ject detection methods [38, 39] which can only recognize the restricted categories contained in the training data, vi-sual grounding has the advantage of detecting novel combi-nations of categories and attributes expressed in free-form text. In recent years, it has attracted much attention in the field of computer vision and machine learning due to its potential applications in many downstream tasks, such as visual question answering [15, 50, 63], visually-grounded language navigation [2, 47] and image captioning [1, 8, 57].
The early methods of visual grounding focus on extend-ing the popularly used one-stage and two-stage object de-tection architectures. One-stage methods [11, 23, 54, 56] use a pre-trained fully convolutional network (e.g. Dark-net53 [38], ResNet [20]) to directly extract pixel-level fea-ture maps and leverage manually-defined dense anchors to return the most likely candidate for the query text. These methods are easy and efficient for learning or inference, but they cannot perform well on complex queries that have var-ious objects and relations. Two-stage methods [52, 53, 58] use an off-the-shelf detector (e.g. Faster R-CNN [39]) to extract the region proposals and return the one that best matches the query text using the modality-shared represen-tations. These methods always have better performance than the one-stage ones by introducing more complicated multimodal fusion and reasoning mechanisms [33, 52, 53].
However, the complicated fusion modules cannot be jointly learned with the detectors, which may limit their ability in multimodal reasoning. More recently, Transformer [46] has been applied in visual grounding [11, 24] to conduct the multimodal reasoning more succinctly based on pixel-level feature maps without region proposals or dense anchors.
Although existing visual grounding methods, especially the Transformer-based methods [11, 24], have achieved promising results, we argue that they do not pay enough at-tention to the visual backbone which plays a crucial role in effective multimodal reasoning. Since the visual backbone determines whether all integral visual content in the image is successfully extracted for matching the query text. Cur-rently, the most widely used backbones are the CNN model (e.g., ResNet [20]) pre-trained for image classification on
ImageNet and the detector (e.g., Faster R-CNN [39] and
Mask R-CNN [19]) pre-trained for general object detection of close-set categories. Therefore, the difference between the visual grounding task and the pre-training task of the backbones may lead to an inconsistency between the vi-sual features produced by the backbones and the ones re-ally needed for the multimodal reasoning. As shown in
Figure 1(a), the pre-trained visual backbone extracts gen-eral purposed visual features sensitive to regions that may contain the objects of pre-defined categories. Whereas, the visual grounding requires the backbone to localize a differ-ent object referred to by the query. A straightforward way to alleviate the inconsistency is learning the visual ground-ing model in an end-to-end form as in [11]. However, it still cannot completely avoid the inconsistency because the backbone is query-agnostic. In other words, given the same image, the query-agnostic backbone will always output the same feature map no matter what the query sentences are. maps of the visual backbone with the guidance of the query text, which benefits the cross-modal alignment between the query and the relevant region to make a correct prediction.
The QRNet is designed based on Swin-Transformer [31] and a novel Query-aware Dynamic Attention (QD-ATT), which can help the QRNet extract query-refined visual fea-ture maps from the visual backbone and fuse the multi-scale features with the query guidance. The QD-ATT dy-namically computes textual-dependent visual attentions at the spatial and channel level of the feature maps produced by the visual backbone. The spatial and channel atten-tions are further multiplied with the original feature maps to obtain query-refined hierarchical visual feature maps. To comprehensively consider the fine-grained visual features of the candidate regions at different scales, we aggregate the query-refined visual feature maps obtained at different stages of the QRNet by a query-aware multiscale fusion scheme.
We instantiate the proposed QRNet by building a flexi-ble visual grounding framework based on the recently pro-posed TransVG [11]. We adopt the same multi-layer visual-linguistic transformer as in [11] to perform intra- and inter-modal reasoning based on the output token sequence of the
QRNet. The complete pipeline significantly outperforms existing methods, e.g., TransVG [11] (3.75% on Refer-ItGame and 2.85% on Flickr30K Entities). Note that the proposed QD-ATT can be easily applied to other pre-trained visual backbones, e.g., ResNet [20].
The main contributions of this paper are three-fold:
• We propose a query-modulated refinement network to address the inconsistency issue caused by the pre-trained visual backbone through adjusting the visual feature maps with the guidance of query text.
• We propose a novel query-aware dynamic attention mechanism, which can dynamically compute query-dependent spatial and channel attentions for refining visual features.
• We build a flexible visual grounding framework based on the query-modulated refinement network and demonstrate that it achieves significantly better perfor-mance than existing methods on five widely used pub-lic datasets. 2.