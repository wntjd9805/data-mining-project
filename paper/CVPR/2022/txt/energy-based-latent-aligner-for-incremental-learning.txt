Abstract
Deep learning models tend to forget their earlier knowl-edge while incrementally learning new tasks. This behavior emerges because the parameter updates optimized for the new tasks may not align well with the updates suitable for older tasks. The resulting latent representation mismatch causes forgetting. In this work, we propose ELI: Energy-based Latent Aligner for Incremental Learning, which first learns an energy manifold for the latent representations such that previous task latents will have low energy and the current task latents have high energy values. This learned manifold is used to counter the representational shift that happens during incremental learning. The implicit regu-larization that is offered by our proposed methodology can be used as a plug-and-play module in existing incremental learning methodologies. We validate this through extensive evaluation on CIFAR-100, ImageNet subset, ImageNet 1k and Pascal VOC datasets. We observe consistent improve-ment when ELI is added to three prominent methodologies in class-incremental learning, across multiple incremental settings. Further, when added to the state-of-the-art incre-mental object detector, ELI provides over 5% improvement in detection accuracy, corroborating its effectiveness and complementary advantage to the existing art. Code is avail-able at: https:// github.com/ JosephKJ/ ELI . 1.

Introduction
Learning experiences are dynamic in the real-world, re-quiring models to incrementally learn new capabilities over
Incremental Learning (also called continual learn-time. ing) is a paradigm that learns a model MTt at time step t, such that it is competent in solving a continuum of tasks Tt = {τ1, τ2, · · · , τt} introduced to it during its life-time. Each task τi contains instances from a disjoint set of classes. Importantly, the training data for the previous tasks
{τ1, · · · , τt−1} cannot be accessed while learning τt, due to privacy, memory and/or computational constraints.
We can represent an incremental model MTt, as a com-θ and a trailing net-ϕ that solves the task using the extracted features: position of a latent feature extractor F Tt work F Tt
Figure 1. We illustrate an Incremental Learning model trained on a continuum of tasks in the top part of the figure. While learning the current task τt (zoomed-in), the latent representation of Task τt−1 data gets disturbed, as shown by red arrows. ELI learns an energy manifold, and uses it to counteract this inherent representational shift, as illustrated by green arrows, thereby alleviating forgetting.
ϕ ◦ F Tt
MTt(x) = (F Tt
θ )(x); where x ∈ Tt. A naive ap-proach for learning incrementally would be to use data sam-ples from the current task τt to finetune the model trained until the previous task MTt−1. Doing so will bias the inter-nal representations of the network to perform well on τt, in-turn significantly degrading the performance on old tasks.
This phenomenon is called catastrophic forgetting [13, 35].
The incremental learning problem requires accumulat-ing knowledge over a long range of learning tasks with-out catastrophic forgetting. The main challenge is how to consolidate conflicting implicit representations across dif-ferent training episodes to learn a generalized model ap-plicable to all the learning experiences. To this end, ex-isting approaches investigate regularization-based methods
[2, 23, 24, 29, 40, 55] that constrain θ and ϕ such that the model performs well on all the tasks. Exemplar replay-based methods [7, 8, 21, 33, 41] retain a subset of data-points from each task, and rehearse them to learn a con-tinual model. Dynamically expanding models [34, 44, 45], enlarge θ and ϕ while learning incrementally.
Complementary to the existing methodologies, we in-troduce a novel approach which minimizes the represen-tational shift in the latent space of an incremental model, using a learned energy manifold. The energy modeling of-fers a natural mechanism to deal with catastrophic forget-ting which we build upon. Fig. 1 illustrates how our pro-posed methodology, ELI: Energy-based Latent Aligner for
Incremental Learning, helps to alleviate forgetting. After learning the current task τt, the features from the feature extractor (referred to as latents henceforth), of the previous task data zTt = F Tt
θ (x), x ∈ τt−1 drift as shown by the red arrows. The first step in our approach is to learn an energy manifold where the latent representations from the model trained until the current task MTt have higher en-ergy, while the latents from the model trained till the pre-vious task MTt−1 have lower energy. Next, the learned energy-based model (EBM) is used to transform the previ-ous task latents zTt (obtained via passing the previous task data through current model) which had drifted away, to al-ternate locations in the latent space such that the representa-tional shift is undone (as shown by the green arrows). This helps alleviate forgetting in incremental learning. We ex-plain how this transformation can be achieved in Sec. 3. We also present a proof-of-concept with MNIST (Fig. 3) which mimics the above setting. The latent space visualization and accuracy regain after learning the new task correlates with the illustration in Fig. 1, which reinforces our intuition.
A unique characteristic of our energy-based latent aligner is its ability to extend and enhance existing con-tinual learning methodologies, without any change to their methodology. We verify this by adding ELI to three promi-nent class-incremental methods: iCaRL [41], LUCIR [20] and AANet [31] and the state-of-the-art incremental Object
Detector: iOD [22]. We conduct thorough experimental evaluation on incremental versions of large-scale classifi-cation datasets like CIFAR-100 [25], ImageNet subset [41] and ImageNet 1k [9]; and Pascal VOC [12] object detec-tion dataset. For incremental classification experiments, we consider two prominent setups: adding classes to a model trained with half of all the classes as first task, and the general incremental learning setting which considers equal number of classes for all tasks. ELI consistently improves performance across all datasets and on all methods in incre-mental classification settings, and obtains impressive per-formance gains on incremental Object Detection, compared to current state-of-the-art [22], by 5.4%, 7% and 3% while incrementally learning 10, 5 and a single class respectively.
To summarize, the key highlights of our work are:
• We introduce a novel methodology ELI, which helps to counter the representational shift that happens in the la-tent space of incremental learning models.
• Our energy-based latent aligner can act as an add-on mod-ule to existing incremental classifiers and object detec-tors, without any changes to their methodology.
• ELI shows consistent improvement on over 45 experi-ments across three large scale incremental classification datasets, and improves the current state-of-the-art incre-mental object detector by over 5% mAP on average. 2.