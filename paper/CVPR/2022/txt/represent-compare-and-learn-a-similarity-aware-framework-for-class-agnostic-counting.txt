Abstract
Class-agnostic counting (CAC) aims to count all in-stances in a query image given few exemplars. A stan-dard pipeline is to extract visual features from exemplars and match them with query images to infer object counts.
Two essential components in this pipeline are feature rep-resentation and similarity metric. Existing methods either adopt a pretrained network to represent features or learn a new one, while applying a naive similarity metric with fixed inner product. We find this paradigm leads to noisy simi-larity matching and hence harms counting performance. In this work, we propose a similarity-aware CAC framework that jointly learns representation and similarity metric. We first instantiate our framework with a naive baseline called
Bilinear Matching Network (BMNet), whose key component is a learnable bilinear similarity metric. To further embody the core of our framework, we extend BMNet to BMNet+ that models similarity from three aspects: 1) representing the instances via their self-similarity to enhance feature ro-bustness against intra-class variations; 2) comparing the similarity dynamically to focus on the key patterns of each exemplar; 3) learning from a supervision signal to impose explicit constraints on matching results. Extensive exper-iments on a recent CAC dataset FSC147 show that our models significantly outperform state-of-the-art CAC ap-proaches.
In addition, we also validate the cross-dataset generality of BMNet and BMNet+ on a car counting dataset
CARPK. Code is at tiny.one/BMNet 1.

Introduction
Object counting aims to infer the number of objects from an image. Most existing methods focus on a specific cate-gory, e.g., crowd [42], animal [2], or car [26], while requir-ing numerous training data to learn a good model. In con-trast, given only one exemplar of a novel category, e.g., a
*Corresponding author
Figure 1. Visualizations of intermediate similarity maps in class-agnostic counting. Compared with the state-of-the-art Fam-Net [29], our model (BMNet+) generates high-fidelity results. car, even a child can easily capture its visual properties and count cars in new scenes. Recently, CAC (Class Agnostic
Counting) [21,29,40], which counts objects of arbitrary cat-egories given only few exemplars, is proposed to reduce the reliance on training data. CAC points out a promising direc-tion for object counting, i.e., from learning to count objects to learning the way to count.
Generally, existing CAC methods [21, 29, 40] work in an extract-and-match pipeline. They first extract visual fea-tures from exemplars and match these features with those of query images. Similarity matching results are then used as intermediate representations to infer object counts. In-tuitively, two factors play critical roles: feature represen-tation and similarity metric. Existing methods either use a learnable [21, 40] or a fixed feature extractor [29], but ap-ply a similarity metric with some pre-defined rules, e.g., in-ner product [29, 40]. We find this can yield unsatisfactory matching results. From Fig 1, by examining a recent model
FamNet [29], we observe obvious noise on background and weak responses on target positions. The resulting density map may be erroneous given such ambiguity.
In this work, we present a generic similarity-aware framework for CAC, which jointly learns representation and similarity metric in an end-to-end manner. Our goal is to seek better similarity modeling that can generalize well to novel categories. First, we instantiate a bilinear matching network (BMNet), which extends the fixed inner product to a learnable bilinear similarity metric and also allows learn-able representation through back-propagation. Unlike fixed inner product, the bilinear similarity metric captures flexible interactions among feature channels to measure similarity.
Then, we extend BMNet to BMNet+ to embody the core motivation of our framework from three aspects: represent-ing instances via self-similarity, comparing the similarity dynamically, and learning with explicit, similarity-aware supervision. In particular, we apply self-attention [43] to represent self-similarity among features to mitigate intra-class variations. It augments the feature of each instance with information from other intra-class instances such that complementary clues like scales or viewpoints can be of-fered. The dynamic similarity metric applies a feature selec-tion module to the exemplars to find key patterns and hence embraces both dynamism and selectivity. Then, inspired by metric learning [25], the similarity loss imposes an explicit supervision on the intermediate similarity map to pull the exemplar and the target close but to push the exemplar and background away.
Experiments on the public benchmark FSC147 [29] show that our method outperforms the previous best ap-proaches by large margins, with a relative improvement of
+33.72% and +33.79% on the validation and test sets in terms of mean absolute error. According to Fig. 1, our method outputs better intermediate similarity results and presents generality over different categories. The ablation study validates the three main components within BMNet+.
And we further show the cross-dataset generality of our models on a car counting dataset CARPK [13].
Our contributions are two-fold:
• A generic CAC framework that includes the existing pipeline and also generalizes it with joint representation learning and similarity learning;
• BMNet and BMNet+: two CAC models instantiated from our framework, which models packed similarity. 2.