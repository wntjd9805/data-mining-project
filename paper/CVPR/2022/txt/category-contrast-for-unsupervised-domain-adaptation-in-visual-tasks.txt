Abstract
Instance contrast for unsupervised representation learn-In this ing has achieved great success in recent years. work, we explore the idea of instance contrastive learn-ing in unsupervised domain adaptation (UDA) and pro-pose a novel Category Contrast technique (CaCo) that in-troduces semantic priors on top of instance discrimination for visual UDA tasks. By considering instance contrastive learning as a dictionary look-up operation, we construct a semantics-aware dictionary with samples from both source and target domains where each target sample is assigned a (pseudo) category label based on the category priors of source samples. This allows category contrastive learn-ing (between target queries and the category-level dictio-nary) for category-discriminative yet domain-invariant fea-ture representations: samples of the same category (from ei-ther source or target domain) are pulled closer while those of different categories are pushed apart simultaneously. Ex-tensive UDA experiments in multiple visual tasks (e.g., seg-mentation, classiﬁcation and detection) show that CaCo achieves superior performance as compared with state-of-the-art methods. The experiments also demonstrate that
CaCo is complementary to existing UDA methods and gen-eralizable to other learning setups such as unsupervised model adaptation, open-/partial-set adaptation etc. 1.

Introduction
Though deep neural networks (DNNs) [20, 57] have rev-olutionized various computer vision tasks [4, 20, 47, 57], they generally perform not well on new domains due to the cross-domain mismatch. Unsupervised domain adaptation (UDA) aims to mitigate the cross-domain mismatch via ex-ploiting unlabelled target-domain samples. To achieve this purpose, researchers have designed different unsupervised training objectives on target-domain samples to train a well-performed model in target domain [7, 30, 40, 59, 62, 63, 69].
The existing unsupervised losses can be broadly classi-*Corresponding author.
ﬁed into three categories: 1) adversarial loss that enforces source-like target representations [38, 40, 53, 59, 60, 62, 63]; 2) image translation loss that translates source images to have target-like styles and appearance [8,27,36,72,74]; and 3) self-training loss that re-trains networks iteratively with conﬁdently pseudo-labelled target samples [15, 36, 80, 81].
Unsupervised representation learning [5,19,41,44,58,68, 73, 77, 78] addresses a related problem, i.e., unsupervised network pre-training which aims to learn discriminative em-beddings from unlabelled data.
In recent years, instance contrastive learning [5, 19, 42, 58, 68, 73] has led to ma-jor advances in unsupervised representation learning. De-spite different motivations, instance contrast methods can be thought of as a dictionary look-up task [19] that trains a visual encoder by matching an encoded query q with a dictionary of encoded keys k: the encoded query should be similar to the encoded positive keys and dissimilar to en-coded negative keys. With no labels available for unlabelled data, the positive keys are often randomly augmented ver-sions of query samples, and all other samples are considered as negative keys.
In this work, we explore the idea of instance contrast in UDA. Considering contrastive learning as a dictionary look-up task, we hypothesize that a UDA dictionary should be category-aware and domain-mixed with keys from both source and target domains.
Intuitively, a category-aware dictionary with category-balanced keys will encourage to learn category-discriminative yet category-unbiased repre-sentations, while the keys from both source and target do-mains will allow to learn invariant representations within and across the two domains, both being aligned with the objective of UDA.
With above motivation, this paper presents Category
Contrast (CaCo) as a way of building category-aware and domain-mixed dictionaries with corresponding contrastive losses for UDA. As shown in Fig. 1, this dictionary in-cludes keys that are evenly sampled in both categories and domains, where each target key comes with a pre-dicted pseudo category. Take the illustrative dictionary
K = {kc m}1≤c≤C,1≤m≤M as an example. Each category c will have M keys while each domain has (C × M )/2 keys.
(cid:1850)(cid:3047)(cid:512)(cid:1850)(cid:3046) Target/Source data
Rand sampling
Category labeling (cid:3548)(cid:1877)(cid:512)(cid:1834) Predicted category label/uncertainty (cid:1850)(cid:3047) (cid:1850)(cid:3047) (cid:1850)(cid:3046) (cid:1876)(cid:3044) encoder (cid:1876)(cid:3038) momentum  encoder (cid:1869) (cid:1863) (cid:4666)(cid:1869)(cid:481) (cid:3548)(cid:1877)(cid:3044)(cid:4667)
Similarity (cid:2326)(cid:3004)(cid:3028)(cid:3047)(cid:3015)(cid:3004)(cid:3006)
Current data  enqueue (cid:4666)(cid:1863)(cid:481) (cid:3548)(cid:1877)(cid:3038)(cid:481) (cid:1834)(cid:3038)(cid:4667) (cid:2869) (cid:2869) (cid:1710) (cid:1863)(cid:3015) (cid:2869) (cid:2869) (cid:1863)(cid:2871) (cid:1863)(cid:2869) (cid:1863)(cid:2870) (cid:2870) (cid:1710) (cid:1863)(cid:3015) (cid:2870) (cid:1863)(cid:2871) (cid:2870) (cid:1863)(cid:2870) (cid:2870) (cid:1863)(cid:2869) (cid:1709) (cid:1709) (cid:1709) (cid:1709) (cid:1712) (cid:3004) (cid:3004) (cid:1710) (cid:1863)(cid:3015) (cid:3004) (cid:1863)(cid:2870) (cid:3004) (cid:1863)(cid:2870) (cid:1863)(cid:2869)
Oldest data  dequeue
Categorical domain-mixed dictionary
Figure 1. The proposed Category Contrast trains an unsupervised domain adaptive encoder by matching a query q (from an unlabelled target sample xq ∈ Xt) to a dictionary of keys via a category contrastive loss LCatNCE. The dictionary keys are domain-mixed from both source domain Xs (in red with labels) and target domain Xt (in blue with pseudo labels), which allows to learn invariant representations within and across the two domains. They are also category-ware and category-balanced allowing to learn category-discriminative yet category-unbiased representations. Note the category-balanced means that each query q is compared with all the dictionary keys (in loss computation) that are evenly distributed over all data categories which mitigates data imbalance issue.
The network learning will thus strive to minimize a cate-gory contrastive loss LCatNCE between target queries and dictionary keys: samples of the same category are pulled close while those of different categories are pushed away.
This naturally leads to category-discriminative yet domain-invariant representations that perfectly match the objective of UDA.
With the category-aware and domain-mixed dictionary together with the category contrastive loss, the proposed
Category Contrast tackles the UDA challenges with three desirable features: 1) It concurrently minimizes the intra-category variation and maximizes the inter-category dis-tance with the category-aware dictionary design; 2) It achieves inter-domain and intra-domain alignment simul-taneously thanks to the domain-mixed dictionary design by including both source and target samples; 3) It greatly mit-igates the data balance issue due to the category-balanced dictionary design which allows to compute contrast losses evenly across all categories during learning.
We summary the contributions of this paper as follows: (1) we explore instance contrast for UDA, aiming to learn discriminative representation for unlabelled target-domain samples. (2) we propose Category Contrast that builds a category-aware and domain-mixed dictionary with a cat-egory contrastive loss.
It encourages to learn category-discriminative yet domain-invariant representation that per-fectly matches the objective of UDA. (3) extensive experi-ments demonstrate that our CaCo achieves superior UDA performance consistently as compared with state-of-the-art. Additionally, CaCo complements previous UDA ap-proaches and generalizes to other learning setups that in-volves unlabeled data. 2.