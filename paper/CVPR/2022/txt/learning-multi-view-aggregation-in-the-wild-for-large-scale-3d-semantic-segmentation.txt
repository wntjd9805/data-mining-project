Abstract
Recent works on 3D semantic segmentation propose to exploit the synergy between images and point clouds by pro-cessing each modality with a dedicated network and project-ing learned 2D features onto 3D points. Merging large-scale point clouds and images raises several challenges, such as constructing a mapping between points and pixels, and ag-gregating features between multiple views. Current methods require mesh reconstruction or specialized sensors to recover occlusions, and use heuristics to select and aggregate avail-able images. In contrast, we propose an end-to-end trainable multi-view aggregation model leveraging the viewing condi-tions of 3D points to merge features from images taken at ar-bitrary positions. Our method can combine standard 2D and 3D networks and outperforms both 3D models operating on colorized point clouds and hybrid 2D/3D networks without requiring colorization, meshing, or true depth maps. We set a new state-of-the-art for large-scale indoor/outdoor semantic segmentation on S3DIS (74.7 mIoU 6-Fold) and on KITTI-360 (58.3 mIoU). Our full pipeline is accessible at https:
//github.com/drprojects/DeepViewAgg, and only requires raw 3D scans and a set of images and poses. 1.

Introduction
The fast-paced development of dedicated neural archi-tectures for 3D data has led to significant improvements in the automated analysis of large 3D scenes [24]. All top-performing methods operate on colorized point clouds, which requires either specialized sensors [62], or running a colorization step which is often closed-source [1–3] and sensor-dependent [33]. However, while colorized point clouds carry some radiometric information, images com-bined with dedicated 2D architectures are better suited for learning textural and contextual cues. A promising line of work sets out to leverage the complementarity between 3D point clouds and images by projecting onto 3D points
Figure 1. Combining 2D and 3D Information. We propose to merge the complementary information between point clouds and a set of co-registered images. Using a simple visibility model, we can project 2D features onto the 3D points and use viewing conditions to select features from the most relevant images. We represent images at their position with the symbol and color the 3D points according to the image they are seen in. the 2D features learned from real [19, 29, 32] or virtual im-ages [15, 36]
Combining point clouds and images with arbitrary poses (i.e. in the wild) as represented in Figure 1, involves recover-ing occlusions and computing a point-pixel mapping, which is typically done using accurate depth maps from specialized sensors [12, 56] or a potentially costly meshing step [8]. Fur-thermore, when a point is seen in different images simultane-ously, the 2D features must be merged in a meaningful way.
In the mesh texturation literature, multi-view aggregation is typically addressed by selecting images for each triangle based on their viewing conditions, e.g. distance, viewing angle, or occlusion [4, 39, 59]. Hybrid 2D/3D methods for large-scale point cloud analysis usually rely on heuristics to select a fixed number of images per point and pool their features uniformly without considering viewing conditions.
Multi-view aggregation has also been extensively studied for shape recognition [21, 54, 61], albeit in a controlled and synthetic setting not entirely applicable to the analysis of large scenes.
In this paper, we propose to learn to merge features from multiple images with a dedicated attention-based scheme.
For each 3D point, the information from relevant images is aggregated based on the point’s viewing condition. Thanks to our GPU-based implementation, we can efficiently com-pute a point-pixel mapping without mesh or true depth maps, and without sacrificing precision. Our model can handle large-scale scenes with an arbitrary number of images per point taken at any position (with camera pose information), which corresponds to a standard industrial operational set-ting [26, 48, 58]. Using only standard 2D and 3D backbone networks, we set a new state-of-the-art for the S3DIS and
KITTI-360 datasets. Our method improves on both standard and hybrid 2D/3D approaches without requiring point cloud colorization, mesh reconstruction, or depth sensors. In this paper, we present a novel and modular multi-view aggrega-tion method for semantizing hybrid 2D/3D data based on the viewing conditions of 3D points in images. Our approach combines the following advantages:
• We set a new state-of-the-art for S3DIS 6-fold (74.7 mIoU), and KITTI-360 Test (58.3 mIoU) without using points’ colorization.
• Our point-pixel mapping operates directly on 3D point clouds and images without requiring depth maps, mesh-ing, colorization, or virtual view generation.
• Our efficient GPU-based implementation handles arbi-trary numbers of 2D views and large 3D point clouds. 2.