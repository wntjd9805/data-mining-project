Abstract 1.

Introduction
Prior work for articulated 3D shape reconstruction of-ten relies on specialized multi-view and depth sensors or pre-built deformable 3D models. Such methods do not scale to diverse sets of objects in the wild. We present a method that requires neither of them. It aims to create high-fidelity, articulated 3D models from many casual RGB videos in a differentiable rendering framework. Our key in-sight is to merge three schools of thought: (1) classic de-formable shape models that make use of articulated bones and blend skinning, (2) canonical embeddings that estab-lish correspondences between pixels and a canonical 3D model, and (3) volumetric neural radiance fields (NeRFs) that are amenable to gradient-based optimization. We in-troduce neural blend skinning models that allow for dif-ferentiable and invertible articulated deformations. When combined with canonical embeddings, such models allow us to establish dense correspondences across videos that can be self-supervised with cycle consistency. On real and syn-thetic datasets, our method shows higher-fidelity 3D recon-structions than prior works for humans and animals, with the ability to render realistic images from novel viewpoints.
Project page: https://banmo-www.github.io/.
*Work done when interning at Meta AI
We are interested in developing tools that can reconstruct accurate and animatable models of 3D objects from casu-ally collected videos. A representative application is con-tent creation for virtual and augmented reality, where the goal is to 3D-ify images and videos captured by users for consumption in a 3D space or creating animatable assets such as avatars. For rigid scenes, traditional Structure from
Motion (SfM) approaches can be used to leverage large col-lection of uncontrolled images, such as images downloaded form the web, to build accurate 3D models of landmarks and entire cities [1, 42, 43]. However, these approaches do not generalize to deformable objects such as family members, friends or pets, which are often the focus of user content.
We are thus interested in reconstructing 3D deformable objects from casually collected videos. However, individ-ual videos may not contain sufficient information to obtain good reconstruction of a given subject. Fortunately, we can expect that users may collect several videos of the same sub-jects, such as filming a family member or a pet over the span of several months or years. In this case, we wish our system to gather information from all available videos into a single 3D model, bridging any time discontinuity.
In this paper, we present BANMo, a Builder of
Animatable 3D Neural Models from multiple casual RGB videos. By consolidating the 2D cues from thousands of images into a fixed canonical space, BANMo learns a high-fidelity neural implicit model for appearance, 3D shape, and articulations of the target non-rigid object. The articulation of the output model of BANMo is expressed by a neural blend skinning model, similar to [5, 39, 59, 60], making the output animatable by manipulating bone transformations.
As shown in NRSfM [4], reconstructing a freely mov-ing non-rigid object from monocular video is challenging, where epipolar constraints are not directly applicable. We address three core challenges in BANMo: (1) how to repre-sent 3D geometry and appearance of the target in a canon-ical space; (2) how to deform 3D points between canonical space and individual time instances; (3) how to find pixel or part correspondences over videos given different viewpoint, lighting, background, and object deformations.
Concretely, we utilize neural implicit functions [29] to represent color and 3D surface in the canonical space. This representation enables higher-fidelity 3D geometry recon-struction compared to approaches based on 3D meshes [59, 60]. The use of neural blending skinning in BANMo pro-vides a way to constrain the deformation space of the target object, allowing better handling of pose variations and de-formations with unknown camera parameters, compared to dynamic NeRF approaches [5,22,33,38]. To find correspon-dences, we present a module that performs dense match-ing between pixels and an implicit feature volume. Finally, for robust and efficient optimization over a large number of video frames, we pre-train a pose network for human and quadruped animals to provide initial camera orientations. In a nutshell, BANMo presents a way to merge the recent non-rigid object reconstruction approaches [59,60] in a dynamic
NeRF framework [5, 22, 33, 38], to achieve higher-fidelity non-rigid object reconstruction. We show experimentally that BANMo produces higher-fidelity 3D shape details than previous state-of-the art approaches [60], by taking better advantage of the large number of frames in multiple videos. 2.