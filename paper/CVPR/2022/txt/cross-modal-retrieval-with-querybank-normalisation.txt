Abstract
Profiting from large-scale training datasets, advances in neural architecture design and efficient inference, joint embeddings have become the dominant approach for tack-ling cross-modal retrieval.
In this work we first show that, despite their effectiveness, state-of-the-art joint em-beddings suffer significantly from the longstanding “hub-ness problem” in which a small number of gallery embed-dings form the nearest neighbours of many queries. Draw-ing inspiration from the NLP literature, we formulate a simple but effective framework called Querybank Normal-isation (QB-NORM) that re-normalises query similarities to account for hubs in the embedding space. QB-NORM improves retrieval performance without requiring retrain-ing. Differently from prior work, we show that QB-NORM works effectively without concurrent access to any test set queries. Within the QB-NORM framework, we also pro-pose a novel similarity normalisation method, the Dynamic
Inverted Softmax, that is significantly more robust than ex-isting approaches. We showcase QB-NORM across a range of cross modal retrieval models and benchmarks where it consistently enhances strong baselines beyond the state of the art. Code is available at https://vladbogo. github.io/QB-Norm/. 1.

Introduction
As the improving price-performance of hardware under-pinning sensors, storage and networking continues to en-able the expansion of humanity’s digital archives, the ca-pacity to efficiently search data takes on greater commer-cial and scientific importance. An appealing way to search such data is via natural language queries, in which the user describes the target of their search exactly as they would to another human, rather than employing specialised database languages such as Structured Query Language (SQL).
Towards this goal, a rich body of research literature has
*Equal contribution. †Corresponding authors.
Figure 1. Left: The hubness problem. We consider the prob-lem of cross modal retrieval in which queries q1 and q2 are com-pared against a gallery of samples, x1 and x2. As we show in
Sec. 3.2, the high-dimensional joint embeddings employed by modern methods for cross-modal retrieval suffer from the “hub-ness problem” [80]. A hub (e.g. x2) is the nearest neighbour to multiple queries (q1 and q2), producing poor quality retrieval re-sults (bottom left). Right: Querybank Normalisation employs a querybank to normalise similarities, reducing the similarity of hub x2 to query q1, improving the retrieval results (bottom right). studied the problem of cross modal retrieval, the task of searching a gallery of samples in one modality given a query in another. In particular, there has been significant progress in recent years for systems that can efficiently search im-ages [90], audio [74] and videos [103] with natural language queries by employing cross modal embeddings.
The dominant cross modal embedding paradigm em-ploys deep neural networks that project modality-specific samples into a high-dimensional, real-valued vector space in which they can be directly compared via an appropriate distance metric. A key challenge for such methods, intrin-sic to such high-dimensional spaces, is the emergence of
“hubs” [80]—embedding vectors that appear amongst the nearest neighbour sets of disproportionately many other em-bedding vectors (Fig. 1, left). To illustrate this challenge,
we show empirically in Sec. 3.2 and Fig. 2 that hubness is prevalent among a range of leading retrieval methods. Hubs have consequences: if left unaddressed, they lead to a sig-nificant degradation in the search ranking yielded by a re-trieval system [8]. The hubness problem has received con-siderable attention [8, 64, 80] and a number of approaches have been proposed to address it [31], with notable con-tributions in the NLP literature focusing on bilingual word translation [21, 24, 88]. One contribution of our work is to show how each of these methods can be interpreted within a single unifying conceptual framework termed Querybank
Normalisation (QB-NORM, Fig. 1, right), that employs a querybank of samples during inference to reduce the influ-ence of hubs in the gallery. We observe that existing meth-ods have two challenges: (1) To date, these approaches have only been shown to work with concurrent access to multi-ple test queries—an assumption that is impractical for real-world retrieval systems; (2) They are sensitive to querybank selection, and indeed actively harm performance for cer-tain querybanks (Tab. 2). To address the first challenge, we demonstrate through careful experiments (Tab. 1) that QB-NORM does not require concurrent access to test queries to be effective. To address the second challenge, we pro-pose a new normalisation method, Dynamic Inverted Soft-max (DIS), that operates as a module within the QB-NORM framework. We show that DIS provides effective normali-sation, yet is more robust than prior approaches [21, 24, 88].
We make the following contributions: (1) We motivate our study by demonstrating that the longstanding problem of hubness remains a significant concern in modern cross modal embeddings for retrieval; (2) We propose Query-bank Normalisation (QB-NORM), a simple non-parametric framework that brings significant gains in retrieval perfor-mance without requiring model fine-tuning; (3) We provide the first (to the best of our knowledge) demonstration that
Querybank Normalisation methods retain their effective-ness for cross modal retrieval with no access to test queries beyond the current query; (4) We propose the Dynamic In-verted Softmax, a novel normalisation method for Query-bank Normalisation that is more robust than prior literature; (5) We show that QB-NORM is highly effective across a broad range of tasks, models and benchmarks. 2.