Abstract
Facial action unit (AU) recognition is formulated as a supervised learning problem by recent works. However, the complex labeling process makes it challenging to pro-vide AU annotations for large amounts of facial images.
To remedy this, we utilize AU labeling rules defined by the Facial Action Coding System (FACS) to design a novel knowledge-driven self-supervised representation learning framework for AU recognition. The representation encoder is trained using large amounts of facial images without AU annotations. AU labeling rules are summarized from FACS to design facial partition manners and determine correla-tions between facial regions. The method utilizes a back-bone network to extract local facial area representations and a project head to map the representations into a low-dimensional latent space. In the latent space, a contrastive learning component leverages the inter-area difference to learn AU-related local representations while maintaining intra-area instance discrimination. Correlations between facial regions summarized from AU labeling rules are also explored to further learn representations using a predict-ing learning component. Evaluation on two benchmark databases demonstrates that the learned representation is powerful and data-efficient for AU recognition. 1.

Introduction
Facial AUs defined by the Facial Action Coding Sys-tem [4] describe the activities of sets of specific facial mus-cles. Nearly all facial behaviors can be represented through one or more AUs. Automatic facial AU recognition has at-tracted attention due to its potential in a wide variety of ap-plications.
The majority of current works on facial AU recogni-tion are supervised, requiring fully AU-labeled images for
In general, there are two different approaches training. to supervise AU recognition. The first treats AU recog-*This is the corresponding author.
Figure 1. (b)Left: (a)The judgment areas of AU12 are shown.
Facial areas are divided into eight parts according to AU-related appearance changes. Right: The relationships between facial ar-eas. Corresponding AUs are labeled on the edges. nition as a multi-label classification problem to be solved by directly constructing an end-to-end deep network [2, 5].
However, AUs are typically correlated to partial facial ar-eas. These works only utilize global facial information for AU recognition, limiting their performance. Recent works [14, 15, 17, 30] have opted for the second approach, which tries to learn more AU-specific patterns to enhance
AU recognition. For example, these works locate AUs based on facial landmarks and muscles. The nearby areas of specific AUs are used to predict their labels. Leverag-ing more AU-specific patterns can effectively improve AU recognition. However, AU labels must be annotated by ex-perienced experts, which is time and labor intensive. Exist-ing AU-labeled databases are too limited to take advantage of these supervised methods.
Recently, several works have tackled the issue of AU annotations. Some works [25, 27] try to perform semi-supervised AU recognition. These works summarize la-bel distribution from ground-truth AU labels, and use the learned distribution to improve AU recognition. However, the summarized patterns may not be consistent with the true distribution due to limited ground-truth AU labels. Li et al.
[16] have created self-supervised learning methods in which large amounts of unlabeled images are used to learn repre-sentations. They utilize the transformation between two ad-jacent frames as the supervisory signal to learn AU-related
global facial representations, ignoring the local property of
AUs. There are also several self-supervised methods [1, 6] that learn powerful visual representations for image classifi-cation via contrastive learning. However, both Li et al. [16] and the contrastive learning works design self-supervised tasks using random augmentation or temporal information.
They do not fully leverage task-related domain knowledge.
To address these obstacles, we propose a novel knowledge-driven self-supervised representation learning framework for AU recognition to alleviate the demand for
AU labels. Specifically, we first summarize AU label-ing rules taken from FACS as domain knowledge. FACS determines AUs according to different facial appearance changes. For example, as shown in Figure 1a, some key facial appearance changes of AU12 consist of lip corners raising, infraorbital triangle raising, and so on. AU-related appearance changes are summarized, and facial areas are divided into eight sections according to the locations of the appearance changes, as shown in the left image of Fig-ure 1b. There are also correlations between local areas, summarized in the right image of Figure 1b. The sum-marized knowledge is leveraged to design a self-supervised representation learning framework. A backbone network extracts local representations for each facial part, and a project head maps the local features into a low-dimensional latent space. In the latent space, a contrastive learning com-ponent and a predicting learning component train the fea-ture encoder. The challenge with contrastive learning is designing reasonable data pairs. Positive pairs pull closer and negative pairs push apart. In our contrastive learning component, the embeddings from the same and symmetri-cal areas are treated as positive pairs according to AU la-beling rules. All others are regarded as negative pairs. In addition, for each area, the embeddings from the same in-put image are treated as positive pairs to maintain intra-area instance discrimination. We propose a predicting learning component to leverage the summarized inter-area relation-ships to enhance representation learning. A group of pre-dictors are used to learn correlations between the embed-dings from different areas in the latent space. The inter-area correlations are utilized as supervisory signals. Finally, the proposed representation learning framework is trained on a large available unlabeled database. AU classifiers are fur-ther trained on two benchmark databases to evaluate the ef-ficacy of the learned representations for AU recognition.
The contributions of the paper can be summarized as follows. We propose a novel knowledge-driven self-supervised representation learning framework for AU recognition, which can learn AU-related local representa-tions from large amounts of available unlabeled images.
Unlike previous self-supervised learning methods ignoring task-related domain knowledge, we leverage both the differ-ence and correlation between local facial areas as supervi-sory signals under the guidance of AU labeling rules. Eval-uation on two benchmark databases shows that the learned local features are powerful and data-efficient for AU recog-nition compared to state-of-the-art self-supervised, semi-supervised, and supervised methods. 2.