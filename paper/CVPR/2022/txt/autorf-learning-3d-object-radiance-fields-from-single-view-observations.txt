Abstract
We introduce AutoRF – a new approach for learning neural 3D object representations where each object in the training set is observed by only a single view. This set-ting is in stark contrast to the majority of existing works that leverage multiple views of the same object, employ ex-plicit priors during training, or require pixel-perfect anno-tations. To address this challenging setting, we propose to learn a normalized, object-centric representation whose embedding describes and disentangles shape, appearance, and pose. Each encoding provides well-generalizable, com-pact information about the object of interest, which is de-coded in a single-shot into a new target view, thus en-abling novel view synthesis. We further improve the re-construction quality by optimizing shape and appearance codes at test time by ﬁtting the representation tightly to the input image.
In a series of experiments, we show that our method generalizes well to unseen objects, even across different datasets of challenging real-world street scenes such as nuScenes, KITTI, and Mapillary Metropo-lis. Additional results can be found on our project page https://sirwyver.github.io/AutoRF/. 1.

Introduction
In this work, we address the challenging problem of in-ferring 3D object information from individual images taken
Work was done during Norman’s and Andrea’s internships at Meta
Reality Labs Zurich. in the wild. Providing an objects’ 3D shape with 6DOF pose and corresponding appearance from a single image is key to enabling immersive experiences in AR/VR, or in robotics to decompose a scene into relevant objects for subsequent interaction. The underlying research problem is related to novel view synthesis or inverse graphics, and has recently gained a lot of attraction in our community
[10,12,17,21,34,39,42,43], leading to remarkable improve-ments in terms of monocular 3D reconstruction ﬁdelity.
Many existing works [10, 17, 21, 34, 42, 43] are limited in their applicability – in particular due to their imposed data and supervision requirements: The majority of works require multiple views and non-occluded visibility of the same physical object, near-perfect camera pose informa-tion, and the object of interest being central, at high reso-lution, and thus the most prominent content of the image.
Due to a lack of real-world datasets providing such features (with the notable, recently released exception of [33]), an overwhelming number of methods have only shown experi-mental results on synthetic datasets, and thus under perfect data conditions, or require large datasets of CAD models to construct a shape prior. When applied to real data the exist-ing domain gap becomes evident, typically leading to major performance degradation.
Our work investigates the limits of novel view synthe-sis from monocular, single-image real-world data. We fo-cus on street-level imagery where objects like cars have high variability in scale and can be very small compared to the full image resolution. Also, such objects are often
occluded or may suffer from motion blur as a consequence of the data acquisition setup. We only consider a single-view object scenario during both training and inference, i.e., we do not impose constraints based on multiple views of the same object. For supervision, we limit our method to learning only from machine-generated predictions, leverag-ing state-of-the-art and off-the-shelf, image-based 3D ob-ject detection [20, 35] and instance/panoptic segmentation algorithms [11, 18, 32], respectively. This data setting also enables us to benchmark our results on existing autonomous driving research datasets [2, 7, 28]. However, the absence of human quality control requires our method to cope with la-bel noise introduced by machine-predictions from monocu-lar 3D object detectors (itself addressing an ill-posed prob-lem), and imperfect instance segmentation masks.
Our proposed method follows an encoder/decoder ar-chitecture trained on images with machine-predicted 3D bounding boxes and corresponding 2D panoptic segmen-tation masks per image. The encoder learns to transform a training sample from its actual (arbitrary) pose and scale representation into two canonical, object-centric encodings representing shape and appearance, respectively. The de-coder translates the object’s shape and appearance codes into an object-centric, implicit radiance ﬁeld representa-tion, which provides occupancy and color information for given 3D points and viewing directions in object space. Our training procedure beneﬁts from the segmentation mask to gather information about the object’s foreground pixels and to cope with potential occlusions, while it leverages the pose information provided by the 3D bounding box to enforce the object-centric representation. At test time, we further opti-mize predicted latent codes to ﬁt the representation tightly to the given input image by using a photometric loss for-mulation. Ultimately, our architecture can learn strong im-plicit priors that also generalize across different datasets.
We provide insightful experimental evaluations and ablation studies on challenging real-world and controllable synthetic datasets, deﬁning a ﬁrst state of the art for the challenging training setting that we consider. In summary, our key con-tributions and differences with respect to existing works are:
• We introduce novel view synthesis based on 3D object priors, learnt from only single-view, in-the-wild observa-tions where objects are potentially occluded, have large variability in scale, and may suffer from degraded image quality. We neither leverage multiple views of the same object, nor utilise large CAD model libraries, or build upon speciﬁc, pre-deﬁned shape priors.
• We successfully exploit machine-generated, 3D bound-ing boxes and panoptic segmentation masks and thus im-perfect annotations for learning an implicit object repre-sentation that can be applied to novel view synthesis on real-world data. Most previous works have shown experi-ments on synthetic data or require the object of interest to be non-occluded and the main content of the image (ex-cept for [13] leveraging masks from [11]).
• Our method efﬁciently encodes shape- and appearance properties for the objects of interest, which we are able to decode to a novel view in a single shot, and option-ally ﬁne-tune further at test time. This enables corrections from potential domain shifts and to generalise across dif-ferent datasets, which has not been demonstrated so far. 2.