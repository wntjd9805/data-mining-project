Abstract
Exemplar-based image translation establishes dense correspondences between a conditional input and an exem-plar (from two different domains) for leveraging detailed exemplar styles to achieve realistic image translation. Ex-isting work builds the cross-domain correspondences im-plicitly by minimizing feature-wise distances across the two domains. Without explicit exploitation of domain-invariant features, this approach may not reduce the domain gap effectively which often leads to sub-optimal correspon-dences and image translation. We design a Marginal Con-trastive Learning Network (MCL-Net) that explores con-trastive learning to learn domain-invariant features for re-alistic exemplar-based image translation. Specifically, we design an innovative marginal contrastive loss that guides to establish dense correspondences explicitly. Nevertheless, building correspondence with domain-invariant semantics alone may impair the texture patterns and lead to degraded texture generation. We thus design a Self-Correlation Map (SCM) that incorporates scene structures as auxiliary infor-mation which improves the built correspondences substan-tially. Quantitative and qualitative experiments on multifar-ious image translation tasks show that the proposed method outperforms the state-of-the-art consistently. 1.

Introduction
Image-to-image translation refers to the image genera-tion conditioned on the certain inputs from other domains
[25, 29, 32], and has delivered impressive advances with the emergence of Generative Adversarial Networks (GANs) [5] in recent years. As a typical ill-posed problem, diverse so-lutions are naturally allowed in image translation tasks as one conditional input could corresponds to multiple image instances. Faithful control of the generation style not only enables diverse generation given certain conditions but also flexible user control of the desired generation. However, yielding high-fidelity images with controllable styles still remains a grand challenge.
*Corresponding author
Figure 1. Learning domain-invariant features for building corre-spondence across domains: We exploit contrastive learning be-tween the Conditional Input and the Ground Truth by pulling fea-tures at the same position closer while pushing those at different positions apart. With the learned Condition Encoder and Image
Encoder, explicit feature correspondences can be built up between the Conditional Input and the Exemplar Image.
To tackle the style control challenge, one predominant approach employs variational autoencoders (VAEs) to reg-ularize the image style into a Gaussian distribution as in
SPADE [25], yet this approach often suffer from compro-mised generation quality due to the posterior collapse phe-nomenon [18] of VAEs. Another direction is to leverage the styles extracted from the exemplars for guiding the gen-eration. For instance, Zhu et al. [47] propose to achieve style control for each semantic region with a dedicated style code. However, the latent codes are typically extracted via a style encoder, which often reflects the global styles only and struggles to capture the detailed structures. Very re-cently, dense correspondence has been actively explored in exemplar-based image translation, and emerges as a promis-ing approach to achieve faithful style control. Specifically,
Zhang et al. [40] propose to establish dense semantic corre-spondence between conditioned input and a given exemplar so as to offer dense style guidance in translation. Zheng et al. [43] and CoCosNet v2 [45] enables the establishment of
dense correspondence in high-resolution features via Patch-Match to further enhance the preservation of detailed styles.
Zhan et al. [34] introduce a general image translation frame-work that incorporates optimal transport for feature align-ment between conditional inputs and style exemplars. As a matching problem across very different domains, the key of building their correspondence lies in how to effectively learn the domain-invariant features to facilitate the proper matching. However, the aforementioned methods implicitly align the domain gaps for correspondence building by mini-mizing a pseudo pair loss and a L1 loss between features of condition and real image, which does not explicitly model the domain invariant features and may lead to sub-optimal feature correspondence.
In this paper, we propose Marginal Contrastive Learning
Network (MCL-Net) for exemplar-based image transla-tion, which introduce contrastive learning [2,6,31] to effec-tively extract domain-invariant features for building cross-domain correspondence as shown in Fig. 1.
In particu-lar, contrastive learning is applied to the features of the conditional input and its ground truth that are extracted by separate encoders. Each input feature vector is treated as an anchor, while the feature vector in the same spa-tial location is treated as positive sample and the remain-ing feature vectors as negative samples. By maximizing the mutual information between condition features and im-age features, contrastive learning could explicitly yield the domain-invariant features. Furthermore, a marginal con-trastive loss (MCL) is proposed to enhance the discrim-inability of domain-invariant features, which could effec-tively curb over-smoothed or inaccurate correspondence. A deviation angle in MCL serves as a penalty to the positive anchor for margin preserving.
On the other hand, most previous methods build the correspondence relying on the corresponding local fea-tures without knowing the scene structures.
It means the scene structure could be impaired while building correspon-dence, which leads to distorted texture patterns and de-graded generation performance. Therefore, the scene struc-tures such as the object shapes should be leveraged to fa-cilitate the correspondence building, especially the preser-vation of fine texture patterns. Inspired by the observation that the self-attention maps could encode intact image struc-tures even with appearance variations [42], we design a self-correlation map (SCM) mechanism to explicitly represent the scene structures associated with feature and effectively facilitate the correspondence building.
The contributions of this work can be summarized in three aspects. First, we introduce contrastive learning into the exemplar-based image translation framework to explic-itly learn domain-invariant features for building correspon-dence. Second, we propose a novel marginal contrastive loss to boost the feature discriminability in the representa-tion space, which greatly benefits the establishment of ex-plicit and accurate correspondence. Third, we design a self-correlation map to properly represent the scene structures and effectively facilitate the preservation of texture patterns while building correspondence. 2.