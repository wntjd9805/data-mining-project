Abstract
Noisy training set usually leads to the degradation of generalization and robustness of neural networks. In this paper, we propose using a theoretically guaranteed noisy label detection framework to detect and remove noisy data for Learning with Noisy Labels (LNL). Specifically, the linear we design a penalized regression to model relation between network features and one-hot labels, where the noisy data are identified by the non-zero mean shift parameters solved in the regression model. To make the framework scalable to datasets that contain a large number of categories and training data, we propose a split algorithm to divide the whole training set into small pieces that can be solved by the penalized regression in parallel, leading to the Scalable Penalized Regression (SPR) framework. We provide the non-asymptotic probabilistic condition for SPR to correctly identify the noisy data. While
SPR can be regarded as a sample selection module for standard supervised training pipeline, we further combine it with semi-supervised algorithm to further exploit the support of noisy data as unlabeled data. Experimental results on several benchmark datasets and real-world noisy datasets show the effectiveness of our framework. Our code and pretrained models are released at https:// github.com/Yikai-Wang/SPR-LNL. 1.

Introduction
Deep learning has achieved remarkable success on many topics of supervised learning with millions of labeled training data. The performance heavily relies on the quality of label annotation since neural networks are susceptible to noisy labels and even can easily memorize randomly leading to the degradation labeled annotations [63], of generalization and robustness.
In many real-world scenarios, it is expensive and difficult to obtain precise labels, exposing a realistic challenge for supervised deep models to learn with noisy data.
*Corresponding author.
There is a large literature for this challenge from various perspectives, including modifying the network architectures loss functions [11, 27, 53, 65], or
[6, 12, 13, 59] or dynamically selecting clean data during training [5, 14, 17, 27,34,40,44,61]. Particularly, the dynamic sample selection methods adopt the spirit of providing only clean data for the training. Such a spirit can form a ‘virtuous’ cycle between the noisy data elimination and network training: the elimination of noisy data can help the network training; and on the other hand, the improved network is empowered with a better ability in picking up clean data. As this virtuous cycle evolves, the performance can be improved. to identify outliers
Typical principles include large loss [14], inconsistent prediction [67], and irregular feature representation [57]. The former two principles focus on the label space, while the last one focuses on the feature space of the same class.
In this paper, we unify the label and feature space and assume linear relationship between the feature-label pair (denoted as (xi, yi)) of data i by yi = x⊤ i β + ε, (1) where xi ∈ Rp is the feature vector, and yi ∈ Rc is the one-hot label vector; β ∈ Rp×c is the fixed (unknown) coefficient matrix and ε ∈ Rc is random noise. This linear relation is approximately established as the networks are trained to minimize the divergence between a (soft-max) linear projection of the feature and one-hot label vector. For a well-trained network, the output prediction of clean data is expected to be as similar to a one-hot vector as possible, while for noisy data the output is dense. Intuitively, when the linear relation is well-approximated without soft-max operation, the corresponding data is likely to be clean data.
The simplest way to identify the suspected outliers in the linear model is checking the predict error, or residual,
ˆβ, where ˆβ is the estimate of β. The ri = yi − x⊤ i larger ∥r∥ indicates more possibility for the instance i to be outlier/noisy data. The classical statistical method to test whether the instance ri is non-zero is using the leave-one-out approach [38] to test the externally studentized residual
ti = (cid:16)
ˆσ−i
ˆβ−i yi − x⊤ i (cid:0)X ⊤ 1 + x⊤ i
−iX−i
, (cid:17)1/2 (cid:1)−1 xi (2) where ˆσ is the scale estimate and the subscript −i indicates estimates based on the n − 1 observations, leaving out the i-th data where we are testing. Equivalently, the linear regression model can be re-formulated into explicitly representing the residual by the mean-shift parameter γ as in [39],
Y = Xβ + γ + ε,
εi,j ∼ N (0, σ2), (3) where we have the feature X ∈ Rn×p, and label Y ∈ Rn×c paired and stacked by rows; and each row of γ ∈ Rn×c,
γi, represents the predict residual of the corresponding data.
This formulation has been widely studied in different research topics, including economics [4, 18, 32, 33], robust regression [8, 39], statistical ranking [9], face recognition [56], semi-supervised few-shot learning [54, 55], and Bayesian preference learning [43], to name a few. The focused formulation is different depending on the specific research tasks. For example, for the robust regression problem, the target is to get a robust estimate ˆβ against the influence of γ. Here for solving the problem of learning with noisy labels, we instead aim to amplify the impact of γ such that non-zero values can represent the noisy label that existed in the training set.
To this end, from the statistical perspective, this paper starts from Eq. (3) to build up a sample selection framework, dubbed Scalable Penalized Regression (SPR), which has theoretical guarantees of consistently identifying noisy data, and thus can efficiently learn with noisy labels.
Naturally, we expect γ in Eq. (3) to be sparse and only a small number of γi are non-zeros, indicating that those data are noisy or outlying. Thus a sparse penalty is utilized on
γi to encourage that the non-zero solution is restricted in a small portion. We thus optimize the induced penalized regression problem to solve γ and identify the instances with non-zero γi as noisy data. Theoretically, in terms of the model selection consistency theory [51, 66], there is some nice statistical property and theoretical insight in our SPR framework, as we can guarantee that, by meeting certain conditions, our SPR should at least in principle, successfully identify all the noisy data.
To incorporate Eq. (3) into the end-to-end training pipeline of deep architecture, the simplest way is to solve Eq. (3) for each training mini-batch to detect and remove noisy data. However, when we train large model with small batch size, the information of current mini-batch may not be identifiable enough to distinguish true pattern from noise.
On the other hand, use SPR on the whole training data after training an epoch leads to an unacceptable computation cost due to the quadratically increased complexity of solving Eq. (3) with the training data. To design a proper optimization environment for solving Eq. (3) that is data-efficient and identifiable, we utilize the whole training set and propose a split algorithm to divide it into small pieces that are class balance with proper data size such that the noisy pattern is identifiable and can be solved efficiently in parallel, making SPR scalable to large datasets.
Inspired by [69], to further encourage the linear relation between features and labels, we propose using a sparse penalty on the fully-connected output before it is soft-maxed. Moreover, we utilize SPR to train the network in a semi-supervised manner using CutMix [62], regarding the detected noisy data as unlabeled data to fully utilize the feature information. We conduct extensive experiments to validate the effectiveness of our framework on several benchmark datasets and real-world noisy datasets.
Contributions. Our contributions are as follows:
• We present a statistical approach, SPR, to identify noisy data under a general scenario with theoretical guarantees.
• A split algorithm is proposed to make SPR scalable to large datasets.
• A sparse penalty is proposed to encourage the linear relation, and a full training framework that combines SPR with semi-supervised methods is designed.
• Experiments on benchmark datasets and real-world noisy datasets validate the effectiveness of SPR. 2.