Abstract
Spatial reasoning on multi-view line drawings by state-of-the-art supervised deep networks is recently shown with puzzling low performances on the SPARE3D dataset [14].
Based on the fact that self-supervised learning is helpful when a large number of data are available, we propose two self-supervised learning approaches to improve the base-line performance for view consistency reasoning and cam-era pose reasoning tasks on the SPARE3D dataset. For the first task, we use a self-supervised binary classifica-tion network to contrast the line drawing differences be-tween various views of any two similar 3D objects, enabling the trained networks to effectively learn detail-sensitive yet view-invariant line drawing representations of 3D objects.
For the second type of task, we propose a self-supervised multi-class classification framework to train a model to select the correct corresponding view from which a line drawing is rendered. Our method is even helpful for the downstream tasks with unseen camera poses. Experiments show that our method could significantly increase the base-line performance in SPARE3D, while some popular self-supervised learning methods cannot. 1.

Introduction
Human visual reasoning, especially spatial reasoning, has been widely studied from psychological and educa-tional perspectives [17, 19]. Researches show that trained humans can achieve good performance on spatial reasoning tasks [32] because they can solve these tasks using spatial memory, logic, and imagination. However, the spatial rea-soning of deep networks is yet to be explored and improved.
In other visual learning tasks such as image classification, object detection, and segmentation, state-of-the-art deep networks have shown their superior performance to humans by memorizing indicative visual patterns from enormous image instances for prediction. However, it seems diffi-cult for deep networks to reason using the same mechanism about the spatial information such as the view consistency
Figure 1. We significantly improve SPARE3D baselines using self-supervised learning approaches. Note that in this figure, all the networks are trained using the same amount of data (data gener-ated from 5,000 CAD models) as used on the SPARE3D dataset. and camera poses from 2D images [14].
To our best knowledge, we are the first to investigate spatial reasoning tasks in the SPARE3D1 dataset [14], that provides several challenging spatial reasoning tasks in line drawings (Figure 1). The dataset is unique in that: (1) it uses line drawings as inputs, (2) it is a non-categorical dataset, meaning there is no class label information for each ob-ject, (3) it defines spatial reasoning instead of recognition tasks. Specifically, we focus on the view-consistency rea-soning task (the Three-view to Isometric or T2I) and the two camera-pose reasoning tasks (the Isometric to Pose or
I2P, and the Pose to Isometric or P2I). The task examples are illustrated in Figure 1 and for more details of the task
*Equal contribution.
â€ The corresponding author is Chen Feng cfeng@nyu.edu. 1Note that we use the latest dataset and benchmark results updated by the SPARE3D authors after CVPR 2020.
settings, we refer the readers to the SPARE3D paper [14]. 2.