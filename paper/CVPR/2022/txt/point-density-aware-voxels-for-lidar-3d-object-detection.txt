Abstract
LiDAR has become one of the primary 3D object de-tection sensors in autonomous driving. However, LiDAR’s diverging point pattern with increasing distance results in a non-uniform sampled point cloud ill-suited to discretized volumetric feature extraction. Current methods either rely on voxelized point clouds or use inefficient farthest point sampling to mitigate detrimental effects caused by density variation but largely ignore point density as a feature and its predictable relationship with distance from the LiDAR sensor. Our proposed solution, Point Density-Aware Voxel network (PDV), is an end-to-end two stage LiDAR 3D object detection architecture that is designed to account for these point density variations. PDV efficiently localizes voxel features from the 3D sparse convolution backbone through voxel point centroids. The spatially localized voxel features are then aggregated through a density-aware RoI grid pool-ing module using kernel density estimation (KDE) and self-attention with point density positional encoding. Finally, we exploit LiDAR’s point density to distance relationship to re-fine our final bounding box confidences. PDV outperforms all state-of-the-art methods on the Waymo Open Dataset and achieves competitive results on the KITTI dataset. 1.

Introduction 3D object detection is one of the key perception prob-lems in the autonomous vehicle space as object pose estima-tion directly impacts the effectiveness of downstream tasks in the perception pipeline. Within the autonomous driving sensor stack, LiDAR has become one of the most popular sensors used for 3D object detection [23,25,37], because of the accurate 3D point cloud it produces through laser light.
However, the reliance on LiDAR data comes at the cost of point density variations across distance. Other factors such as occlusion play a role, but the primary reason is the natural divergence of points from the LiDAR with in-creasing distance due to the angular offsets between the Li-DAR lasers. Thus, objects located at farther distances return fewer points than objects located closer to the LiDAR. (a) (b) (c)
Figure 1. Voxel feature localization using (a) voxel centers, (b) far-thest point sampling, and (c) voxel point centroids on a vehicle in the Waymo Open Dataset [29]. By using the raw point cloud to localize voxel features, voxel point centroids provide dense geo-metric information for second-stage proposal refinement.
Voxel-based methods [4,37,40,43] typically ignore point density, solely relying on the quantized representation of the point cloud. When a high voxel resolution is afforded, as is the case on the KITTI dataset [6], voxel-based meth-ods [41] have outperformed point-based and point-voxel-based methods. However, on datasets with larger input spaces such as the Waymo Open Dataset [29], the voxel resolution is limited due to memory constraints. Fine ob-ject details are therefore lost due to spatial misalignment between the voxel features and the point cloud as shown in
Figure 1 (a), resulting in a degradation in performance.
Other methods [23, 25] attempt to remedy point density variations through farthest point sampling (FPS) as seen in Figure 1 (b). Although effective at sampling locations on non-uniformly distributed point clouds, the computation scales poorly as a function of the number of points in the point cloud, increasing runtime and limiting the number of sampled points for second-stage proposal refinement.
Point density also affects detection of smaller objects such as pedestrians and cyclists. These objects have less surface area to intersect the LiDAR’s laser beams, resulting in poorer object localization. Perhaps informatively, cur-rent state-of-the-art methods have largely ignored detection performance for pedestrians and cyclists, focusing solely on the car or vehicle class [4, 14, 15, 41]. As we move towards
datasets with higher environment coverage, it is necessary for architectures to be scalable to larger input spaces and to serve as a multi-class solution for 3D object detection.
We therefore propose Point Density-Aware Voxel net-work (PDV) to resolve these identified issues by leveraging voxel point centroid localization and feature encodings that directly account for point density in multi-class 3D object detection. We summarize our approach with the following three contributions. (1) Voxel Point Centroid Localization. PDV partitions the
LiDAR points in each non-empty voxel to calculate a point centroid for each voxel feature, as shown in Figure 1 (c).
By localizing the voxel features with point centroids for second-stage proposal refinement, PDV uses the point den-sity distributions to retain fine-grained position informa-tion in the feature encodings without requiring an expensive point cloud sampling method such as FPS. (2) Density-Aware RoI Grid Pooling. We augment region of interest (RoI) grid pooling [23] to encode local point density as an additional feature. First, we use kernel den-sity estimation (KDE) [17, 21] to encode local voxel fea-ture density at each grid point ball query, followed by self-attention [33] between grid points with a novel point density positional encoding. Density-aware RoI grid pooling cap-tures localized point density information in the context of the whole region proposal for second-stage refinement. (3) Density Confidence Prediction. We further refine our bounding box confidence predictions by using the fi-nal bounding box centroid location and the number of raw
LiDAR points within the final bounding box as additional features. Thus, we use the inherent relationship between distance and point density established by LiDAR for more informed confidence predictions.
PDV outperforms all current state-of-the-art methods on the Waymo Open Dataset [29] with an increase of
+0.65%/+1.25%, +0.53%/+0.46%, and +0.49%/+0.71% on the vehicle, pedestrian, and cyclist LEVEL 1/LEVEL 2 mAPH classes, respectively, and achieves competitive per-formance on the KITTI dataset [6]. 2.