Abstract
Recently, vision-language pre-training shows great po-tential in open-vocabulary object detection, where detec-tors trained on base classes are devised for detecting new classes. The class text embedding is firstly generated by feeding prompts to the text encoder of a pre-trained vision-language model.
It is then used as the region classi-fier to supervise the training of a detector. The key ele-ment that leads to the success of this model is the proper prompt, which requires careful words tuning and ingenious design. To avoid laborious prompt engineering, there are some prompt representation learning methods being pro-posed for the image classification task, which however can only be sub-optimal solutions when applied to the detec-tion task. In this paper, we introduce a novel method, de-tection prompt (DetPro), to learn continuous prompt rep-resentations for open-vocabulary object detection based on the pre-trained vision-language model. Different from the previous classification-oriented methods, DetPro has two highlights: 1) a background interpretation scheme to in-clude the proposals in image background into the prompt training; 2) a context grading scheme to separate propos-als in image foreground for tailored prompt training. We assemble DetPro with ViLD, a recent state-of-the-art open-world object detector, and conduct experiments on the LVIS as well as transfer learning on the Pascal VOC, COCO,
Objects365 datasets. Experimental results show that our
DetPro outperforms the baseline ViLD [7] in all settings, e.g., +3.4 APbox and +3.0 APmask improvements on the novel classes of LVIS. Code and models are available at https://github.com/dyabel/detpro. 1.

Introduction
Object detection aims at locating bounding boxes of ob-jects in an image as well as assigning labels to them. In last
†Corresponding author. few years, object detection [19, 20] achieves great success in solving the closed-set problem, i.e., detectors can detect classes present in the training set. To increase the detec-tion vocabulary, the common practice is by collecting more data with desired classes. Besides the expensive labeling cost in this process, it often leads to a long-tailed distri-bution [8, 13] of object classes: detectors need to be care-fully designed to avoid overfitting on frequently-occurred categories in the dataset. In contrast, an alternative way for increasing the detection vocabulary is open-vocabulary ob-ject detection (OVOD), where detectors are trained on base classes and equipped with ability to detect new classes.
Recently, ViLD [7] introduces a framework for open-vocabulary object detection, which distills the knowledge from a pre-trained vision-language model into a detector. It is inspired by the recent progress of vision-language pre-training, e.g, CLIP [18] and ALIGN [10], where two sepa-rate encoders, namely image encoder and text encoder, are used to maximize the alignment between images and cor-responding texts. In ViLD’s implementation, they feed text descriptions of base classes, known as prompt, into the text encoder of CLIP to generate the class text embedding. The embedding is then utilized to classify object proposals and supervise the detector training. To perform open-set ob-ject detection, the base class text embedding is replaced with the embedding of both base and novel classes. The prompt design, also known as prompt engineering, is cru-cial in this process as we observe a slight word change in it would end up with clear positive or negative impact on the detection performance. Designing proper prompts requires domain expertise and carefully word tuning from human, as of [7]. To avoid such high-end and rather laborious de-mand from human, the alternative way is to automatically learn prompt’s context using continuous representations, we name it as prompt representation learning in our work.
In this paper, we propose a novel method named detec-tion prompt (DetPro) to learn prompt representations, in the setting of open-vocabulary object detection with pre-trained
vision-language model (OVOD-VLM). There are some re-cent works focusing on prompt representation learning such as CoOp [38], who targets for improving image classifi-cation accuracy based on the pre-trained vision-language models. Directly applying CoOP into the OVOD-VLM is not realistic: image classification only needs to recognize the correct labels of input images while object detection requires detectors to distinguish foregrounds from back-grounds, and classify region proposals in foregrounds into different object classes. We thus introduce a new Detec-tion Prompt (DetPro) to automatically learn prompt repre-sentations in OVOD-LVM based on positive and negative proposals w.r.t. ground truth in images.
Prompt learning in object detection faces two critical is-sues: 1) Negative proposals, despite being very important to object detection, do not correspond to specific object classes, therefore can not be easily included into the prompt learning process. 2) Unlike objects in image classification being centered and big in images, objects in positive pro-posals are often associated with different levels of contexts, learning one prompt context for these proposals can not be sufficient. To tackle them, we introduce,
• a background interpretation scheme for negative pro-posal inclusion, which optimizes the embedding of negative proposals to be away from all other class em-bedding;
• a context grading scheme with tailored positive pro-posals, which tailors the prompt representation learn-ing with different positive proposal sets corresponding to different context levels.
We assemble DetPro with ViLD [7], and conduct a se-ries of experiments on LVIS and transfer the LVIS-trained model to other datasets including Pascal VOC, COCO and
Objects365.
In all settings, our DetPro outperforms the
ViLD, e.g., +3.4 APbox and +3.0 APmask improvements on the novel classes of LVIS. 2.