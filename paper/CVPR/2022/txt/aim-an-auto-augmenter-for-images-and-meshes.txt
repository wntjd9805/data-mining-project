Abstract
Data augmentations are commonly used to increase the robustness of deep neural networks. In most contempo-rary research, the networks do not decide the augmenta-tions; they are task-agnostic, and grid search determines their magnitudes. Furthermore, augmentations applicable to lower-dimensional data do not easily extend to higher-dimensional data and vice versa. This paper presents an auto-augmenter for images and meshes (AIM) that easily incorporates into neural networks at training and inference times. It jointly optimizes with the network to produce con-strained, non-rigid deformations in the data. AIM predicts sample-aware deformations suited for a task, and our ex-periments confirm its effectiveness with various networks. 1.

Introduction
Deep neural networks are prevailing in various computer vision tasks. They are commonplace for the analysis of dig-ital images [15, 16, 38] and 3D graphics [8, 14]. These net-works try to emulate human cognition in a computerized environment. However, despite the success of deep learning in recent years, it is still not as robust as human vision.
Learning methods for vision-based tasks need to dis-associate between what an object looks like and where it lies in space. To this end, it is common to pre-process in-put data to neural networks with augmentation approaches.
Some augmentations make neural networks more tolerant of geometric changes in data. For instance, augmentation techniques such as affine transformations, random horizon-tal flipping, and random cropping are standard for image processing. For mesh analysis, jittering of mesh elements is performed along with affine transformations. These aug-mentations also form the basis of other advanced data augmentation strategies [5, 11, 21, 32, 42, 55] and frame-works [2, 54]. The augmentations mentioned above do not directly participate in the learning process and are not de-pendent on a task’s objective. Thus, they are non-learnable and task-agnostic.
Figure 1. AIM performs non-rigid deformations on the input data to a task network during training and testing. AIM learns to detect critical regions in the samples to solve a task and increases their spatial coverage. As shown, it applies to both images and meshes.
Conversely, task-aware augmentation approaches [5, 12, 20, 26, 37] jointly optimize with neural networks. A com-mon theme to this set of approaches is to sequentially learn which transformations suit a task, where to employ them, and to what extent they should be applied. However, at present, many learnable augmentation methods are con-strained to the dimensions of their underlying representa-tions. Methods applicable to 2D images are either unsuit-able or have no clear way to extend to 3D data and vice versa. In this work, we build upon these insights and short-comings to propose an Auto-Augmenter for Images and
Meshes (AIM).
Eye movements in human beings are primarily catego-rized into four categories: fixation [39], saccades [10], sta-bilization [4], and smooth pursuit [24]. By fixating eyes at specific locations, the human visual system can enhance their resolution to process fine spatial details. AIM closely imitates human fixation. It is implemented as a data pre-processor to jointly optimize with existing neural networks for image and mesh analysis. First, AIM infers regions in the images and meshes that contain critical information to solve a task. Then, it increases the spatial resolution of these regions and simultaneously reduces spatial coverage of non-crucial areas. A visual illustration of this process is in Fig. 1.
The main contributions of this paper are:
• AIM’s novel and differentiable spatial warper.
• An attention module for graph data.
• A novel directional consistency loss to constrain defor-mations produced by AIM.
• Experiments on multiple data sets for image classifica-tion, mesh classification, and mesh segmentation. 2.