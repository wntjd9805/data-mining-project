Abstract
Sparsely annotated semantic segmentation (SASS) aims to train a segmentation network with coarse-grained (i.e., point-, scribble-, and block-wise) supervisions, where only a small proportion of pixels are labeled in each image. In this paper, we propose a novel tree energy loss for SASS by providing semantic guidance for unlabeled pixels. The tree energy loss represents images as minimum spanning trees to model both low-level and high-level pair-wise affini-ties. By sequentially applying these affinities to the net-work prediction, soft pseudo labels for unlabeled pixels are generated in a coarse-to-fine manner, achieving dynamic online self-training. The tree energy loss is effective and easy to be incorporated into existing frameworks by com-bining it with a traditional segmentation loss. Compared with previous SASS methods, our method requires no multi-stage training strategies, alternating optimization proce-dures, additional supervised data, or time-consuming post-processing while outperforming them in all SASS settings.
Code is available at https://github.com/megvii-research/TreeEnergyLoss. 1.

Introduction
Semantic segmentation, aiming to assign each pixel a se-mantic label for given images, is one of the fundamental tasks in computer vision. Previous methods [4,18,25,26,36] tend to leverage large amounts of fully annotated labels like
Fig. 2(b) to achieve satisfying performance. However, man-ually annotating such high-quality labels is labor-intensive.
To reduce the annotation cost and preserve the segmentation performance, some recent works research on semantic seg-mentation with sparse annotations, such as point-wise [2] and scribble-wise ones [17]. As shown in Fig. 2(c-d), the point-wise annotation assigns each semantic object with a
*This work was performed during an internship at MEGVII Technol-ogy. This work was supported by The National Key Research and De-velopment Program of China (2020AAA0105200) and Beijing Academy of Artificial Intelligence (BAAI). Corresponding author: Jianbing Shen.
Email: shenjianbingcg@gmail.com
Figure 1. Illustration of current SASS approaches. S and E denote the segmentation and auxiliary models, respectively. Our method leverages the minimum spanning trees (MSTs) to capture both low-level and high-level affinities to generate soft pseudo labels, performing online self-training. single-pixel label while the scribble-wise annotation draws at least a scribble label for the object.
As illustrated in Fig. 1(a-d), existing approaches are mainly based on auxiliary tasks, pseudo proposals, regular-ized losses, and consistency learning to solve SASS. How-ever, there are some shortcomings in these approaches. The predictive error from the auxiliary task [15,34,35] may hin-der the performance of semantic segmentation. The pro-posal generation [17, 39, 42] is time-consuming and usually calls for a multi-stage training strategy. The regularized
Figure 2. Different types of training annotations for semantic segmentation. The background class is annotated in black. losses [20, 21, 29, 30, 32] ignore the domain gap between the visual information and the high-level semantics, and the consistency learning [3, 13, 22, 24, 44, 45] fails to directly supervise the unlabeled pixels at the category level. In this paper, we aim to alleviate these shortcomings and introduce a simple yet effective solution.
In SASS, each image can be divided into labeled and unlabeled regions. The labeled region can be directly su-pervised by the ground truth, while how to learn from the unlabeled region is an open question. For the region of the same object, the labeled and unlabeled pixels share simi-lar patterns on low-level color (RGB value of image) and high-level responses (features produced by CNN). Utiliz-ing such similarity prior in SASS is intuitive. Inspired by the tree filter [1, 41], which can model the pair-wise sim-ilarity with its structure-preserving property, we leverage this property to generate soft pseudo labels for unlabeled regions and achieve online self-training.
Specifically, we introduce a novel tree energy loss (TEL) based on the low-level and the high-level similarities of im-age (see Fig. 1(e)). In TEL, two minimum spanning trees (MSTs) are built on the low-level color and the high-level semantic features, respectively. Each MST is obtained by sequentially eliminating connections between adjacent pix-els with large dissimilarity, so less related pixels are sepa-rated and the essential relation among pixels is preserved.
Then, two structure-aware affinity matrices obtained by ac-cumulating the edge weights along the MSTs are multiplied with the network predictions in a cascading manner, pro-ducing soft pseudo labels. Finally, the generated pseudo labels are assigned to the unlabeled regions. Combining the
TEL with a standard segmentation loss (e.g., cross-entropy loss), any segmentation network can learn extra knowledge from unlabeled regions via dynamic online self-training.
To comprehensively validate the effectiveness of TEL, we further enrich the SASS scenarios by introducing a block-wise annotation setting (see Fig. 2(e)), where the amount of annotations is located between the full and scrib-ble settings. In this way, we can grade the SASS into three levels, i.e., point, scribble, and block. Experimental re-sults show that TEL can significantly boost segmentation performance without introducing extra computational costs during inference. Equipped with recent segmentation net-works, our method can achieve state-of-the-art performance under various annotated settings.
The main contributions are summarized as follow. We propose a novel tree energy loss (TEL) for SASS. TEL leverages minimum spanning trees to model the low-level and high-level structural relation among pixels. A cas-caded filtering operation is further introduced to dynami-cally generate soft pseudo labels from network predictions in a coarse-to-fine way. TEL is clean and easy to be plugged into most existing segmentation networks. For comprehen-sive validation, we further introduce a block-annotated set-ting for SASS. Our method outperforms the state-of-the-arts under the point-, scribble- and block-annotated settings. 2.