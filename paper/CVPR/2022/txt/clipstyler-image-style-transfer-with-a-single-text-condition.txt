Abstract
Existing neural style transfer methods require reference style images to transfer texture information of style images to content images. However, in many practical situations, users may not have reference style images but still be inter-ested in transferring styles by just imagining them. In order to deal with such applications, we propose a new frame-work that enables a style transfer ‘without’ a style image, but only with a text description of the desired style. Using the pre-trained text-image embedding model of CLIP, we demonstrate the modulation of the style of content images only with a single text condition. Specifically, we propose a patch-wise text-image matching loss with multiview aug-mentations for realistic texture transfer. Extensive experi-mental results confirmed the successful image style transfer with realistic textures that reflect semantic query texts. 1.

Introduction
Style transfer aims to transform a content image by trans-ferring the semantic texture of a style image. The seminar work of neural style transfer proposed by Gatys et al. [7] uses a pre-trained VGG network to transfer the style texture
by calculating the style loss that matches the Gram matrices of the content and style features. Their style loss has be-come a standard in later works including stylization through pixel optimization for a single content image [3], arbitrary style transfer which operates in real-time for various style images [9, 16, 18, 27], and optimizing feedforward network for stylizing each image [10, 24].
Although these approaches for style transfer can success-fully create visually pleasing new artworks by transferring styles of famous artworks to common images, they require a reference style image to change the texture of the content image. However, in many practical applications, reference style images are not available to users, but the users are still interested in ‘imitiating’ the texture of the style images. For example, users can imagine being able to convert their own photos into Monet or Van Gogh styles without ever owning paintings by the famous painters. Or you can convert your daylight images into night images by mere imagination. In fact, to overcome this limitation of the existing style trans-fer and create a truly creative artwork, we should be able to transfer a completely novel style that we imagine.
Toward this goal, several methods have attempted to ma-nipulate images with a text condition which conveys the de-sired style. Using pre-trained text-image embedding mod-els, these method usually deliver semantic information of text condition to the visual domain. However, these meth-ods often have disadvantages in that semantics are not prop-erly reflected due to the performance limitations of the em-bedding model [28, 29], and the manipulation is restricted to a specific content domain (such as human face) as the method heavily rely on pre-trained generative models [20].
To address this, here we propose a novel image style transfer method to deliver the semantic textures of text conditions using recently proposed text-image embedding model of CLIP [21]. Specifically, rather than resorting to pixel-optimization or manipulating the instance normaliza-tion layer as in AdaIN [9], we propose to train a lightweight
CNN network that can express the texture information with respect to text conditions and produce realistic and color-ful results. More specifically, the content image is trans-formed by the lightweight CNN to follow the text condition by matching the similarity between the CLIP model output of transferred image and the text condition. Furthermore, when the network is trained for multiple content images, our method enables text-driven style transfer regardless of content images.
Our method comes from several technical innovations in the implementation. First, instead of optimizing the loss by using the image directly, we propose to use a patch-wise CLIP loss to guide the network to function as a brush-stroke. Specifically, to calculate the proposed loss, we first sample patches of the output image and apply augmenta-tion with different perspective views. Afterwards, we ob-tain the CLIP loss by calculating the similarity between the query text condition and the processed patches. By apply-ing this patch-wise CLIP loss, we found that we can transfer styles to each local area of the content image. Furthermore, the augmentation induces the patch style to be more vivid and diverse. Additionally, to overcome the patch-dependent over-stylization problem, we propose a novel threshold reg-ularization so that the patches with abnormally high scores do not affect the network training.
Extensive experimental results show that our model can transfer a variety of unique styles based on text conditions, which enable a wider range of style transfer than the exist-ing methods using style images. 2.