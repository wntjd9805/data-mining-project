Abstract
Face recognition has achieved significant progress in deep learning era due to the ultra-large-scale and well-labeled datasets. However, training on the outsize datasets is time-consuming and takes up a lot of hardware resource.
Therefore, designing an efficient training approach is in-dispensable. The heavy computational and memory costs mainly result from the million-level dimensionality of the fully connected (FC) layer. To this end, we propose a novel training approach, termed Faster Face Classifica-tion (F2C), to alleviate time and cost without sacrificing the performance. This method adopts Dynamic Class Pool (DCP) for storing and updating the identities’ features dy-namically, which could be regarded as a substitute for the
FC layer. DCP is efficiently time-saving and cost-saving, as its smaller size with the independence from the whole face identities together. We further validate the proposed
F2C method across several face benchmarks and private datasets, and display comparable results, meanwhile the speed is faster than state-of-the-art FC-based methods in terms of recognition accuracy and hardware costs. More-over, our method is further improved by a well-designed dual data loader including indentity-based and instance-based loaders, which makes it more efficient for updating
DCP parameters. 1.

Introduction
Deep Neural Networks (DNNs) has achieved many re-markable results in computer vision tasks [4, 6, 7, 26, 27, 37, 38, 39, 40]. Face recognition can be regarded as one of the most popular research topics in computer vision. Many large scale and well-labelled datasets have been released over the past decade [11, 15, 47, 49, 53]. The training
*Equal contribution. (kai.wang@comp.nus.edu.sg, wang-shuo514@sina.com)
²Corresponding author (youy@comp.nus.edu.sg). process of face recognition aims to learn identity-related embedding space, where the intra-class distances are re-duced and inter-class distances are enlarged in the mean-while. Previous works [11, 42, 43] have proved that train-ing on a large dataset can obtain a substantial improvement over a small dataset. To this end, academia and industry collected ultra-large-scale datasets including 10 even 100 million face identities. Google collected 200 million face images consisting of 8 million identities [28]. Tsinghua in-troduced WebFace260M [53] including 260 million faces, which is the largest public face dataset and achieves state-of-the-art performance.
In general, these ultra-large-scale datasets boost the face recognition performance by a large margin. However, with the growth of face identities and limitations of hardware, there are mainly two problems in training phase. The first problem results from the training time and hardware re-source occupancy. As shown in Fig 1, the time cost and
GPU memory occupancy of the FC layer are much greater than those of the backbone when the face identities reach 10 million. To address these issues, many previous meth-ods [1, 20] focus on reducing the time and resource cost of the FC layer. Previous methods can be summarized into two categories. One [1] tries to distribute the whole FC to dif-ferent GPUs, introducing heavy communication costs. The other [50] attempts to reduce the computing cost by select-ing a certain ratio of neurons from the FC layer randomly, but it still needs to store the whole FC parameters. When the identities reach 10 or 100 million, storing the whole FC parameters is extremely expensive. How to effectively re-duce the computational and memory costs caused by the high-dimensional FC layer? An intuitive idea is to decrease the size of FC or design an alternative paradigm, which is hardly explored before. The second problem is related to the update efficiency and speed of FC parameters. As pointed by [9], the optimal solution for the class center is actually the mean of all samples of this class. Identities that have rare samples with very low frequency of sampling will have very little opportunity to updat class centers through their
(a) Comparison of backbone and FC time cost (ms). (b) The memory occupancy of the FC layer at training phase (G).
Figure 1: Visualization of training time and GPU memory occupancy. Figure 1a shows the forward time comparison of backbone (ResNet50) and the FC layer. Given an image, the time cost of FC increases sharply with the growing number of face identities but the time of backbone stays unchanged. Figure 1b illustrates the GPU memory occupancy with the size of face identities. Even the V100 32G GPU can only store the FC parameters with the output size of about 6 millions (The dimension of face recognition is usually 512). Therefore, it is very necessary to design a method that reduces the training time and hardware cost of the FC layer. samples, which may hamper feature representation.
To tackle aforementioned issues, we propose an efficient training approach for ultra-large-scale face datasets, termed as Faster Face Classification (F2C). In F2C, we first intro-duce twin backbones named Gallery Net (G-Net) and Probe
Net (P-Net) to generate identity centers and extract face fea-tures, respectively. G-Net has the same structure with P-Net and inherits the parameters from P-Net in a moving average manner. Considering that the most time-consuming part of the ultra-large-scale training lies at the FC layer, we pro-pose Dynamic Class Pool (DCP) to store the features from
G-Net and calculate the logits with positive samples (whose identities appear in DCP) in each mini-batch. DCP can be regarded as a substitute for the FC layer and its size is much smaller than FC, which is the reason why F2C can largely reduce the time and resource cost compared to the FC layer.
For negative samples (whose identities do not appear in the
DCP), we minimize the cosine similarities between nega-tive samples and DCP. To improve the update efficiency and speed of DCP parameters, we design a dual data loader in-cluding identity-based and instance-based loaders. The dual data loader loads images from given dataset by instances and identities to generate batches for training. Finally, we conduct sufficient experiments on several face benchmarks to prove F2C can achieve comparable results and a higher training speed than normal FC-based method. F2C also ob-tains superior performance than previous methods in term of recognition accuracy and hardware cost. Our contribu-tions can be summarized as follows. 1) We propose an efficient training approach F2C for ultra-large-scale face recognition training, which aims to reduce the training time and hardware costs while keeping comparable performance to state-of-the-art FC-based meth-ods. 2) We design DCP to store and update the identities’ fea-tures dynamically, which is an alternative to the FC layer.
The size of DCP is much smaller than FC and independent of the whole face identities, so the training time and hard-ware costs can be decreased substantially. 3) We design a dual data loader including identity-based and instance-based loaders to improve the update efficiency of DCP parameters. 2.