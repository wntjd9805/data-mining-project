Abstract
Due to the constraints of the imaging device and high cost in operation time, computer tomography (CT) scans are usually acquired with low within-slice resolution. Im-proving the inter-slice resolution is beneﬁcial to the disease diagnosis for both human experts and computer-aided sys-tems. To this end, this paper builds a novel medical slice synthesis to increase the inter-slice resolution. Consider-ing that the ground-truth intermediate medical slices are al-ways absent in clinical practice, we introduce the incremen-tal cross-view mutual distillation strategy to accomplish this task in the self-supervised learning manner. Speciﬁcally, we model this problem from three different views: slice-wise interpolation from axial view and pixel-wise interpolation from coronal and sagittal views. Under this circumstance, the models learned from different views can distill valuable knowledge to guide the learning processes of each other.
We can repeat this process to make the models synthesize intermediate slice data with increasing between-slice reso-lution. To demonstrate the effectiveness of the proposed ap-proach, we conduct comprehensive experiments on a large-scale CT dataset. Quantitative and qualitative comparison results show that our method outperforms state-of-the-art algorithms by clear margins. 1.

Introduction
High-resolution CT volume data can provide high-quality detail for organs and tissues, thus are valuable for computer-aided diagnosis. However, due to the constraints of the imaging device, the between-slice resolution of the acquired CT volume is not sufﬁciently high in practical clin-ical scenarios, which makes these volume data hard to pro-vide the desired imaging detail for the disease diagnosis.
To solve this problem, a novel task, called medical slice synthesis, has been arising recently. The goal is to syn-thesize intermediate imagery content between original ad-*Equal contribution.
†Corresponding author.
Figure 1. Pixel-wise interpolation in coronal and sagittal views, and slice-wise interpolation in axial view can increase the inter-slice resolution of the input volume individually. We propose a cross-view knowledge distillation framework to settle the self-supervised CT slice synthesis task. jacent slices. Peng et al., [24] made the earliest attempt by implementing pixel-wise interpolation processes on the coronal-view and sagittal-view images and then fusing the results interpolated from two views. However, this method requires large-scaled ground-truth training data, which we cannot conveniently acquire in practice.
This paper explores a self-supervised learning frame-work to train the slice synthesizer without the ground-truth data. Speciﬁcally, we ﬁnd that another under-explored way is to formulate it as a slice-wise interpolation problem for the axial-view images (See Fig. 1). Namely, intermediate slices can be inferred from the context information of two adjacent slices in the axial view. Since pixel-wise and slice-wise interpolation modeling tries to synthesize the missing detail by exploring different kinds of spatial context, the two modeling processes tend to capture helpful yet distinct pat-terns towards the same ultimate goal. Thus, we can jointly use the two modeling processes to address the medical slice synthesis problem and collaborate them to provide com-plementary knowledge for each other. Each interpolation model can be learned under the guidance of the other ones, thus avoiding the requirement of ground-truth training data.
Figure 2. Slice-wise interpolation in axial view (SInt-A), and pixel-wise interpoloatin in coronal (PInt-C) and sagittal (PInt-S) views, have their own superiority in synthesizing inter-slice images. Our proposed cross-view mutual distillation can combine the learned knowledge from three types of interpolation algorithms.
We propose an incremental cross-view mutual distilla-tion pipeline for training medical CT slice synthesis mod-els to take advantage of slice synthesis algorithms from multiple views. Considering that structural information appears to have different characteristics across views and models learned from different views have their superiority (see Fig. 2), we involve three modeling components in the learning process: 1) slice-wise interpolation in axial view; 2) pixel-wise interpolation in coronal view; 3) pixel-wise interpolation in sagittal view. We set up a U-shape network with memorization capacity to implement the slice-wise in-terpolation and adopt an existing image super-resolution network [20] to achieve pixel-wise interpolation.
To lean such deep models, we propose a two-stage learn-ing framework. In the ﬁrst learning stage, we downsample the resolution of original volumes and then use the down-sampled and original volume data to learn single-view slice synthesis models. To enable the model to upscale the reso-lution of the original volume data without any external su-pervision, we further design a cross-view mutual distilla-tion process in the second learning stage. We constrain the pairs of predictions on the original volume data produced by axial-view slice-wise interpolation and coronal/sagittal-view pixel-wise interpolation models. An illustration of our proposed method is presented in Fig. 1. The knowledge dis-tillation mechanism enables the slice-wise and pixel-wise interpolation models to learn from each other and fuse the advantages of different image recovery models learned from different perspectives. Finally, we incrementally increase the between-slice resolution from the three perspectives and apply the cross-view mutual distillation on predictions with very high resolution, enhancing the knowledge exchange across views in self-supervised slice synthesis.
The main contributions of this paper are as follows.
• A pioneering effort is made to implement the self-supervised CT slide synthesis, modeling slice-wise in-terpolation for the axial view and pixel-wise interpola-tion for the coronal and sagittal views.
• A novel self-supervised learning framework is estab-lished, based on single-view internal learning and in-cremental cross-view mutual distillation.
• Extensive experiments on a CT collection (composed of three existing CT datasets) demonstrate that our pro-posed method achieves state-of-the-art performance. 2.