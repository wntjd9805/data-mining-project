Abstract
One-shot object detection aims at detecting novel ob-jects according to merely one given instance. With extreme data scarcity, current approaches explore various feature fusions to obtain directly transferable meta-knowledge. Yet, their performances are often unsatisfactory. In this paper, we attribute this to inappropriate correlation methods that misalign query-support semantics by overlooking spatial structures and scale variances. Upon analysis, we lever-age the attention mechanism and propose a simple but ef-fective architecture named Semantic-aligned Fusion Trans-former (SaFT) to resolve these issues. Specifically, we equip SaFT with a vertical fusion module (VFM) for cross-scale semantic enhancement and a horizontal fusion mod-ule (HFM) for cross-sample feature fusion. Together, they broaden the vision for each feature point from the support to a whole augmented feature pyramid from the query, fa-cilitating semantic-aligned associations. Extensive exper-iments on multiple benchmarks demonstrate the superior-ity of our framework. Without fine-tuning on novel classes, it brings significant performance gains to one-stage base-lines, lifting state-of-the-art results to a higher level. 1.

Introduction
Recent years have witnessed the flourish of large-scale perception systems like [3, 23]. Yet it has a long way to go towards real human-like intelligence. Being one of the underlying problems, few-shot learning received more and more interest from language [1, 17, 46, 58] to vision
[15, 24, 37, 47, 49, 52, 54] related tasks. This scenario aims at learning a well-generalized model with scarcely labeled data, which challenges conventional learning paradigms.
To bridge the aforementioned gap in few-shot object detection (FSD), existing literature suggests drawing sup-port from transfer-learning [7, 16, 48, 54, 55, 65] or meta-learning [15, 24, 27, 28, 56, 57, 61]. Although the former is simple to conduct through pretraining on massive base classes and fine-tuning on scant novel ones, it suffers from the two-stage redundant procedures. The network should
*The work was done when the author was with MSRA as an intern.
Figure 1. Comparison of semantic-aligned fusion and conven-tional fusion. Heatmaps and detection results of these two are based on our SaFT and a baseline with original cross-sample atten-tion accordingly. Comparing the two schemes, semantic-aligned fusion activates more concentrated heatmaps on various feature levels and produces better OSD results. always utilize new-coming few-shot data to optimize pa-rameters before it can well recognize these novel classes, thereby limiting its application. In contrast, the latter trend considers meta-knowledge extraction from sampled meta-tasks. This line of frameworks is expected to adapt directly to similarly organized tasks even without online fine-tuning, though it usually helps in performance. At present, this of-fline meta-learning paradigm is preferred by one-shot object detection (OSD) specific pipelines, with an out-of-the-box availability.
In such a setting, the model should be well constructed to learn the relatedness between a given scene, i.e., the query, and an example patch, i.e., the support. To facilitate this, a series of works [15, 21, 24, 28, 38, 40, 57, 61] investigate cross-sample feature fusion, which augments query features with support representations via sample or ROI level corre-lation. However, neglecting semantic mismatches in space and scale limits their performances in one-shot scenarios.
Concretely, the traditional paradigm suggests generating a prototype [15, 24] or a kernel [61] from the support to as-sociate with query features. With most spatial information compressed, the long-range structure-dependent relation in between remains hardly mined. Although pooled proto-types in Fig. 2(b) and learned kernels in Fig. 2(c) are effec-Figure 2. Visualization of different fusion approaches. We present previous fusion schemes in (b), (c), (d), and our proposed semantic-aligned attention as a sort of semantic-aligned fusion in (e). Images are split into patches for illustration, with each patch representing the receptive field of a feature point. The only green patch indicates the query where a response is expected, blue patches are values that contribute to one feature point of the fusion result, and yellow ones are keys that interact with these values. Green lines in (e) suggest two pairs of ideal matches, where different granularities of query features are used. Support samples are scaled in reweighting and correlation schemes to visualize their compression in spatial information. tive in distinguishing one category from another, they con-tain fewer positioning priors and thus hinder their localiza-tion ability. Furthermore, these schemes match global sup-port representations with local query contexts, regardless of semantic misalignment. An emerging trend [6, 22] seeks help from the attention mechanism for adaptive feature fu-sion. While easing problems discussed to some extent, they commonly focus on feature pairs on a single scale as shown in Fig. 2(d), leaving the multi-scale detection task for later anchor-based detector heads. Therefore, it makes no sense when targets are scattered on different scales. For instance, in Fig. 2(e), ideal matches for the bus wheel and rear win-dows lie in two distinct levels of query features, making any single-scale attempt sub-optimal. A simple multi-scale implementation cannot resolve it either, since it fuses the query and the support one scale at a time. Without cross-scale long-range interactions, this rigid manner is likely to fail in cases with semantic missing, such as occlusions or query-support inconsistencies in shape and size.
To encourage more appropriate and sufficient feature in-teractions in OSD, we propose to adaptively fuse each fea-ture point from the support with each from the query feature pyramid. Thus the original attention mechanism is extended to semantic-aligned attention as illustrated in Fig. 2(e). Fea-tures from each side are first deconstructed into semantic units, i.e., feature points. Then these units interacts one an-other in a global manner, not only between query-support sample pairs (horizontally) but also among different scales (vertically). Since objects and parts of objects might ex-ist in different scales and locations, the association pro-cess weighted collocates multiple semantic units to make a proper match. In this way, semantic-aligned attention en-riches the semantic space that each feature point can utilize, thereby promoting better alignments between the query and the support.
Our Semantic-aligned Fusion Transformer (SaFT) im-plements this fusion scheme, with Fig. 3 demonstrating its overall structure.
It follows a one-stage proposal-free design and can be easily extended to two-stage pipelines through cascading proposal-based heads. Compared with allied frameworks that employ reweighting or correlation,
SaFT alternatively contains a vertical fusion module (VFM) and a horizontal fusion module (HFM). The former is placed after the feature extractor to together form a Siamese backbone followed by the latter. VFM prepares scale-attended features via vertical attention (VA) in Fig. 5, and
HFM utilizes them from query and support with horizon-tal attention (HA) in Fig. 4. Note that a single level of support feature interacts with multiple from the other side for a comprehensive view. Thanks to the cross-scale and cross-sample relatedness modeled by the attention mecha-nism, SaFT achieves remarkable performance gains in both
PASCAL-VOC and MS-COCO datasets.
We conclude our contributions as three-fold. 1. To the best of our knowledge, our Semantic-aligned Fu-sion Transformer is the first to carry out the offline one-shot object detection task with proposal-free one-stage detectors, producing better performance than state-of-the-art two-stage models. 2. We discuss the problems of query-support feature fusion and propose a unified attention mechanism to tackle se-mantic misalignment in space and scale. Our implemen-tation of this can be used as a general fusion neck. 3. Through qualitative and quantitative experiments, we prove that our novel semantic-aligned fusion is superior to conventional association methods by involving cross-scale long-range relations and collecting more compre-hensive meta-knowledge.
2.