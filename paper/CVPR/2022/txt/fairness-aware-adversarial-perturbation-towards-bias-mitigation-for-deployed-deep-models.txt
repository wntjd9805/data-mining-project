Abstract
Prioritizing fairness is of central importance in artifi-cial intelligence (AI) systems, especially for those societal applications, e.g., hiring systems should recommend appli-cants equally from different demographic groups, and risk assessment systems must eliminate racism in criminal jus-tice. Existing efforts towards the ethical development of AI systems have leveraged data science to mitigate biases in the training set or introduced fairness principles into the training process. For a deployed AI system, however, it may not allow for retraining or tuning in practice. By contrast, we propose a more flexible approach, i.e., fairness-aware adversarial perturbation (FAAP), which learns to perturb input data to blind deployed models on fairness-related fea-tures, e.g., gender and ethnicity. The key advantage is that
FAAP does not modify deployed models in terms of param-eters and structures. To achieve this, we design a discrimi-nator to distinguish fairness-related attributes based on la-tent representations from deployed models. Meanwhile, a perturbation generator is trained against the discrimina-tor, such that no fairness-related features could be extracted from perturbed inputs. Exhaustive experimental evaluation demonstrates the effectiveness and superior performance of the proposed FAAP. In addition, FAAP is validated on real-world commercial deployments (inaccessible to model pa-rameters), which shows the transferability of FAAP, foresee-ing the potential of black-box adaptation. 1.

Introduction
AI systems have been widely deployed in many high-stakes applications, e.g., face recognition [3,21], hiring pro-∗Xiaowei Dong is the corresponding author. cess [14, 15], health care [13], etc. However, some existing
AI systems are found to treat individuals unequally based on protected attributes, e.g., ethnicity, gender, and nation-ality. Such biases are referred to as unfairness. For in-stance, Amazon realized that their automatic recruitment system presents skewness between male and female candi-dates [12], i.e., male candidates are with higher probability to be hired as compared to female candidates. The COM-PAS, which is an assessment system of recidivating risk, is found to have racial prejudice [6]. Such unfairness has been a subtle and ubiquitous nature of AI systems, thus it is non-trivial to mitigate the unfairness, ideally without touching the deployed models.
Many works have been proposed to mitigate unfair-ness/biases, which can be divided into three categories i.e., pre-according to the stage de-biasing is applied, processing, in-processing, and post-processing. From the perspective of pre-processing, [8, 16, 17, 27, 31] mitigated biases in the training dataset, thus mitigating the bias dur-ing training the model. For the in-processing methods,
[1, 19, 30] introduced fairness-related penalties into the learning process to train a fairer model. These methods need to retrain or fine-tune the target models, while these are unsuitable if the models are deployed without access to their training set. [7] proposed a boosting method to post-process a deployed deep learning model to produce a new classifier that has equal accuracy in different people groups.
However, [7] needs to replace the final classifier and cannot ensure statistical and predictive parity, e.g., individuals in different groups are equally treated in prediction.
To the best of our knowledge, existing works are not suitable to improve fairness at the inference phase with-out changing the deep model. Therefore, it is imperative to propose a practical approach to mitigate the unfairness of deployed models without changing their parameters and structures. Since deep models tend to learn spurious corre-fairness-related attributes based on latent representa-tions from deployed models. Meanwhile, a generator is trained adversarially to perturb input data to prevent the deployed models from extracting fairness-related features. This design effectively decorrelates fairness-related/protected attributes from predictions.
• Extensive experiments demonstrate the superior per-formance of the proposed FAAP. In addition, evalu-ation on real-world commercial APIs shows the trans-ferability of FAAP, which indicates the potential of fur-ther exploring our method in the black-box scenario. 2.