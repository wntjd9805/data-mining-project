Abstract
The key challenge of zero-shot learning (ZSL) is how to infer the latent semantic knowledge between visual and at-tribute features on seen classes, and thus achieving a de-sirable knowledge transfer to unseen classes. Prior works either simply align the global features of an image with its associated class semantic vector or utilize unidirectional at-tention to learn the limited latent semantic representations, which could not effectively discover the intrinsic semantic knowledge (e.g., attribute semantics) between visual and at-tribute features. To solve the above dilemma, we propose a Mutually Semantic Distillation Network (MSDN), which progressively distills the intrinsic semantic representations between visual and attribute features for ZSL. MSDN incor-porates an attribute→visual attention sub-net that learns attribute-based visual features, and a visual→attribute at-tention sub-net that learns visual-based attribute features.
By further introducing a semantic distillation loss, the two mutual attention sub-nets are capable of learning collab-oratively and teaching each other throughout the training process. The proposed MSDN yields signiﬁcant improve-ments over the strong baselines, leading to new state-of-the-art performances on three popular challenging bench-marks. Our codes have been available at: https:// github.com/shiming-chen/MSDN . 1.

Introduction
Recently, deep learning performs achievements on ob-ject recognition [12, 39, 40]. Based on the prior knowledge of seen classes, humans possess a remarkable ability to rec-ognize new concepts (classes) using shared and distinct at-tributes of both seen and unseen classes [17]. Inspired by this cognitive competence, zero-shot learning (ZSL) is pro-posed under a challenging image classiﬁcation setting to
∗Corresponding author
Figure 1. Motivation illustration. An unseen sample shares differ-ent partial information with a set of seen samples, and this partial information is represented as the abundant knowledge of seman-tic attributes (e.g., “bill color yellow”, “leg color red”). The key challenge of ZSL is how to infer the latent semantic knowledge be-tween visual and attribute features on seen classes, allowing effec-tive knowledge transfer to unseen classes. As such, properly dis-tilling the intrinsic semantic knowledge/representations (e.g., at-tribute semantics) between visual and attribute features from seen to unseen classes is very necessary for ZSL. mimic the human cognitive process [19, 28]. ZSL aims to tackle the unseen class recognition problem by transferring semantic knowledge from seen classes to unseen ones. It is usually based on the assumption that both seen and unseen classes can be described through the shared semantic de-scriptions (e.g., attributes) [18]. Based on the classes that a model sees in the testing phase, ZSL methods can be catego-rized into conventional ZSL (CZSL) and generalized ZSL (GZSL) [44], where CZSL aims to predict unseen classes, while GZSL can predict both seen and unseen ones.
ZSL has achieved signiﬁcant progress, with many ef-forts focus on embedding-based methods, generative meth-ods, and common space learning-based methods. As shown in Fig. 2 (a), embedding-based methods aim to learn a visual→semantic mapping to map the visual fea-Figure 2. Four investigated ZSL paradigms. (a) Embedding-based method. (b) Generative method. (c) Common space learning-based method. (d) Ours proposed mutually semantic distillation network (MSDN). The semantic space S is represented by the class semantic vector annotated by humans based on the attribute descriptions. The visual space V is learned by a CNN backbone (e.g., ResNet101). The common space O is a shared latent space between visual mapping and semantic mapping. The attribute space A is learned by a language model (e.g., Glove [31]). Filled triangles, circles, squares and diamonds denote the sample features in S, V, O and A, respectively. tures into the semantic space for visual-semantic inter-action [2, 4, 5, 32, 46, 48]. The embedding-based meth-ods usually have a large bias towards seen classes under the GZSL setting, since the embedding function is solely learned by seen class samples. To solve this issue, the generative ZSL methods (see Fig. 2(b)) are proposed to learn semantic→visual mapping to generate visual features of unseen classes [3, 6, 8, 34, 35, 38, 43, 50], and thus con-verting ZSL into a conventional classiﬁcation problem. As shown in Fig. 2(c), common space learning learns a com-mon representation space where both visual features and semantic representations are projected for knowledge trans-fer [7, 10, 23, 34, 37, 41]. However, they simply utilize the global features representations and have neglected the ﬁne-grained details in the training images.
As shown in Fig. 1, an unseen sample shares different partial information with a set of seen samples, and this par-tial information is represented as the abundant knowledge of semantic attributes (e.g., “bill color yellow”, “leg color red”). Thus, the key challenge of ZSL is to infer the la-tent semantic knowledge between visual and attribute fea-tures on seen classes, and thus allowing desirable knowl-edge transfer to unseen classes. Recently, some attention-based ZSL methods [5, 25, 46–48, 54] leverage attribute de-scriptions as guidance to discover discriminative part/ﬁne-grained features, enabling to match the semantic represen-tations more accurately. Unfortunately, they simply uti-lize unidirectional attention, which only focuses on limited semantic alignments between visual and attribute features without any further sequential learning. As such, properly discovering the intrinsic and more sufﬁcient semantic rep-resentations (e.g., attribute semantics) between visual and attribute features for knowledge transfer of ZSL is of great importance.
In light of the above observation, we propose a Mutually
Semantic Distillation Network (MSDN) for ZSL, as shown in Fig. 2(d), to explore the intrinsic semantic knowledge between visual and attribute features. MSDN consists of an attribute→visual attention sub-net, which learns attribute-based visual features, and a visual→attribute attention sub-net, which learns visual-based attribute features. These two mutual attention sub-nets act as a teacher-student network for guiding each other to learn collaboratively and teach-ing each other throughout the training process. As such,
MSDN can explore the most matched attribute-based vi-sual features and visual-based attribute features, enabling to effectively distill the intrinsic semantic representations for desirable knowledge transfer from seen to unseen classes (Fig. 1). Speciﬁcally, each attention sub-net is trained with an attribute-based cross-entropy loss with self-calibration
[5, 14, 25, 48, 54]. To encourage mutual learning between the attribute→visual attention sub-net and visual→attribute attention sub-net, we further introduce a semantic distilla-tion loss that aligns each other’s class posterior probabili-ties. The quantitative and qualitative results well demon-strate the superiority and great potential of MSDN.
Our contributions are summarized as: i) We propose a
Mutually Semantic Distillation Network (MSDN), orthog-onal to existing ZSL methods, which distills the intrinsic semantic representations for effective knowledge transfer from seen to unseen classes for ZSL. ii) We introduce a se-mantic distillation loss to enable mutual learning between the attribute→visual attention sub-net and visual→attribute attention sub-net in MSDN, encouraging them to learn attribute-based visual features and visual-based attribute features by distilling the intrinsic semantic knowledge for iii) We conduct ex-semantic embedding representations. tensive experiments to show that our MSDN achieves sig-niﬁcant performance gains over the counterparts on three benchmarks, i.e., CUB [42], SUN [30] and AWA2 [44]. 2.