Abstract
LiDAR and camera are two important sensors for 3D ob-ject detection in autonomous driving. Despite the increas-ing popularity of sensor fusion in this field, the robustness against inferior image conditions, e.g., bad illumination and sensor misalignment, is under-explored. Existing fu-sion methods are easily affected by such conditions, mainly due to a hard association of LiDAR points and image pixels, established by calibration matrices.
We propose TransFusion, a robust solution to LiDAR-camera fusion with a soft-association mechanism to han-dle inferior image conditions. Specifically, our TransFu-sion consists of convolutional backbones and a detection head based on a transformer decoder. The first layer of the decoder predicts initial bounding boxes from a LiDAR point cloud using a sparse set of object queries, and its second decoder layer adaptively fuses the object queries with use-ful image features, leveraging both spatial and contextual relationships. The attention mechanism of the transformer enables our model to adaptively determine where and what information should be taken from the image, leading to a robust and effective fusion strategy. We additionally design an image-guided query initialization strategy to deal with objects that are difficult to detect in point clouds. TransFu-sion achieves state-of-the-art performance on large-scale datasets. We provide extensive experiments to demonstrate its robustness against degenerated image quality and cali-bration errors. We also extend the proposed method to the 3D tracking task and achieve the 1st place in the leader-board of nuScenes tracking, showing its effectiveness and generalization capability. [code release] 1.

Introduction
As one of the fundamental tasks in self-driving, 3D ob-ject detection aims to localize a set of objects in 3D space and recognize their categories. Thanks to the accurate depth information provided by LiDAR, early works such as VoxelNet [67] and PointPillar [14] achieve reasonably good results using only point clouds as input. However, these LiDAR-only methods are generally surpassed by the methods using both LiDAR and camera data on large-scale datasets with sparser point clouds, such as nuScenes [1] and
Waymo [42]. LiDAR-only methods are surely insufficient for robust 3D detection due to the sparsity of point clouds.
For example, small or distant objects are difficult to detect in LiDAR modality. In contrast, such objects are still clearly visible and distinguishable in high-resolution images. The complementary roles of point clouds and images motivate researchers to design detectors utilizing the best of the two worlds, i.e., multi-modal detectors.
Existing LiDAR-camera fusion methods roughly fall into three categories: result-level, proposal-level, and point-level. The result-level methods, including FPointNet [29] and RoarNet [39], use off-the-shelf 2D detectors to seed 3D proposals, followed by a PointNet [30] for object lo-calization. The proposal-level fusion methods, including
MV3D [5] and AVOD [12], perform fusion at the region proposal level by applying RoIPool [31] in each modality for shared proposals. These coarse-grained fusion meth-ods show unsatisfactory results since rectangular regions of interest (RoI) usually contain lots of background noise.
Recently, a majority of approaches have tried to do point-level fusion and achieved promising results. They first find a hard association between LiDAR points and image pix-els based on calibration matrices, and then augment LiDAR features with the segmentation scores [46, 51] or CNN fea-tures [10, 22, 40, 47, 62] of the associated pixels through point-wise concatenation. Similarly, [16, 17, 50, 59] first project a point cloud onto the birdâ€™s eye view (BEV) plane and then fuse the image features with the BEV pixels.
Despite the impressive improvements, these point-level fusion methods suffer from two major problems, as shown in Fig. 1. First, they simply fuse the LiDAR features and image features through element-wise addition or concate-nation, and thus their performance degrades seriously with low-quality image features, e.g., images in bad illumina-tion conditions. Second, finding the hard association be-tween sparse LiDAR points and dense image pixels not only wastes many image features with rich semantic information, but also heavily relies on high-quality calibration between two sensors, which is usually hard to acquire due to the in-herent spatial-temporal misalignment [63].
To address the shortcomings of the previous fusion ap-proaches, we introduce an effective and robust multi-modal
Figure 1. Left: An example of bad illumination conditions. Right:
Due to the sparsity of point clouds, the hard-association based fu-sion methods waste many image features and are sensitive to sen-sor calibration, since the projected points may fall outside objects due to a small calibration error. detection framework in this paper. Our key idea is to repo-sition the focus of the fusion process, from hard-association to soft-association, leading to the robustness against degen-erated image quality and sensor misalignment.
Specifically, we design a sequential fusion method that uses two transformer decoder layers as the detection head.
To our best knowledge, we are the first to use transformer for LiDAR-camera 3D detection. Our first decoder layer leverages a sparse set of object queries to produce ini-tial bounding boxes from LiDAR features. Unlike input-independent object queries in 2D [2, 44], we make the ob-ject queries input-dependent and category-aware so that the queries are enriched with better position and category infor-mation. Next, the second transformer decoder layer adap-tively fuses object queries with useful image features as-sociated by spatial and contextual relationships. We lever-age a locality inductive bias by spatially constraining the cross attention around the initial bounding boxes to help the network better visit the related positions. Our fusion mod-ule not only provides rich semantic information to object queries, but also is more robust to inferior image conditions since the association between LiDAR points and image pix-els are established in a soft and adaptive way. Finally, to handle objects that are difficult to detect in point clouds, we introduce an image-guided query initialization module to involve image guidance on the query initialization stage.
Overall, the corporation of these components significantly improves the effectiveness and robustness of our LiDAR-camera 3D detector. To summarize, our contributions are fourfold: 1. Our studies investigate the inherent difficulties of
LiDAR-camera fusion and reveal a crucial aspect to robust fusion, namely, the soft-association mechanism. 2. We propose a novel transformer-based LiDAR-camera fusion model for 3D detection, which performs fine-grained fusion in an attentive manner and shows supe-rior robustness against degenerated image quality and sensor misalignment. 3. We introduce several simple yet effective adjustments for object queries to boost the quality of initial bound-ing box predictions for image fusion. An image-guided query initialization module is also designed to handle objects that are hard to detect in point clouds. 4. We achieve the state-of-the-art 3D detection per-formance on nuScenes and competitive results on
Waymo. We also extend our model to the 3D track-ing task and achieve the 1st place in the leaderboard of the nuScenes tracking challenge. 2.