Abstract
Cluster discrimination is an effective pretext task for un-supervised representation learning, which often consists of two phases: clustering and discrimination. Clustering is to assign each instance a pseudo label that will be used to learn representations in discrimination. The main chal-lenge resides in clustering since prevalent clustering meth-ods (e.g., k-means) have to run in a batch mode. Besides, there can be a trivial solution consisting of a dominating cluster. To address these challenges, we first investigate the objective of clustering-based representation learning.
Based on this, we propose a novel clustering-based pre-text task with online Constrained K-means (CoKe). Com-pared with the balanced clustering that each cluster has ex-actly the same size, we only constrain the minimal size of each cluster to flexibly capture the inherent data structure.
More importantly, our online assignment method has a the-oretical guarantee to approach the global optimum. By de-coupling clustering and discrimination, CoKe can achieve competitive performance when optimizing with only a sin-gle view from each instance. Extensive experiments on Ima-geNet and other benchmark data sets verify both the efficacy and efficiency of our proposal. 1.

Introduction
Recently, many research efforts have been devoted to un-supervised representation learning that aims to leverage the massive unlabeled data to obtain applicable models. Differ-ent from supervised learning, where labels can provide an explicit discrimination task for learning, designing an ap-propriate pretext task is essential for unsupervised repre-sentation learning. Many pretext tasks have been proposed, e.g., instance discrimination [12], cluster discrimination [3], invariant mapping [9,14], solving jigsaw puzzles [20], patch inpainting [21], etc. Among them, instance discrimination that identifies each instance as an individual class [12] is popular due to its straightforward objective. However, this
Figure 1. Illustration of CoKe. When a mini-batch arrives, each instance will be assigned to a cluster with our online assignment method. Then, in epoch t, representations from the encoder net-work are optimized by discrimination using pseudo labels and cluster centers obtained from epoch t − 1. The pseudo labels from epoch t − 1 were stored to be retrieved in epoch t using the unique id for each image. pretext task can be intractable on large-scale data sets. Con-sequently, contrastive learning is developed to mitigate the large-scale challenge [6, 15, 28] with a memory bank [15] or training with a large mini-batch of instances [6], which requires additional computation resources.
Besides instance discrimination, cluster discrimination is also an effective pretext task for unsupervised repre-sentation learning [1, 3–5, 18, 30, 31]. Compared with in-stance discrimination that assigns a unique label to each instance, cluster discrimination partitions data into a pre-defined number of groups that is significantly less than the total number of instances. Therefore, the classification task after clustering becomes much more feasible for large-scale data. Furthermore, learning representations with clusters will push similar instances together, which may help ex-plore potential semantic structures in data. Unfortunately, the clustering phase often needs to run multiple iterations over the entire data set, which has to be conducted in a batch mode to access representations of all instances [3].
Therefore, online clustering is adopted to improve the ef-ficiency, while the collapsing problem (i.e., a dominating
cluster that contains most of instances) becomes challeng-ing for optimization. To mitigate the problem, ODC [30] has to memorize representations of all instances and de-compose the dominating large cluster with a conventional batch mode clustering method. Instead, SwAV [4] incorpo-rates a balanced clustering method [1] and obtains assign-ment with a batch mode solver for instances from only the last few mini-batches, which outperforms the vanilla online clustering in ODC [30] significantly. However, using only a small subset of data to generate pseudo labels can fail to capture the global distribution. Besides, balanced clustering constrains that each cluster has exactly the same number of instances, which can result in a suboptimal partition of data.
To take the benefits of cluster discrimination but miti-gate the challenge, we, for the first time, investigate the objective of clustering-based representation learning from the perspective of distance metric learning [22]. Our anal-ysis shows that it indeed learns representations and rela-tionships between instances simultaneously, while the cou-pled variables make the optimization challenging. By de-coupling those variables appropriately, the problem can be solved in an alternating manner between two phases, that is, clustering and discrimination. When fixing represen-tations, clustering is to discover relationship between in-stances. After that, the representations can be further re-fined by discrimination using labels from clustering. This finding explains the success of existing cluster discrimina-tion methods. However, most existing methods have to con-duct expensive clustering in a batch mode, while our analy-sis shows that an online method is feasible to optimize the objective.
Based on the observation, we propose a novel pretext task with online Constrained K-means (CoKe) for unsuper-vised representation learning. Concretely, in the clustering phase, we propose a novel online algorithm for constrained k-means that lower-bounds the size of each cluster. Differ-ent from balanced clustering, our strategy is more flexible to model inherent data structure. In addition, our theoret-ical analysis shows that the proposed online method can achieve a near-optimal assignment.
In the discrimination phase, we adopt a standard normalized Softmax loss with labels and centers recorded from the last epoch to learn rep-resentations. By decoupling the clustering and discrimina-tion phases, CoKe can learn representations with a single view from each instance effectively and can be optimized with a small batch size. In addition, two variance reduction strategies are proposed to make the clustering robust for augmentations. Fig. 1 illustrates the framework of CoKe, which demonstrates a simple framework without additional components (e.g., momentum encoder [14, 15], batch mode solver [4, 30], etc.). Besides, only one label for each in-stance is kept in memory, which is an integer and the storage cost is negligible.
Extensive experiments are conducted on both down-stream tasks and clustering to demonstrate the proposal.
With only a single view from each instance for training,
CoKe already achieves a better performance than MoCo-v2 [8] that requires two views. By including additional views in optimization, CoKe demonstrates state-of-the-art performance on ImageNet and clustering. 2.