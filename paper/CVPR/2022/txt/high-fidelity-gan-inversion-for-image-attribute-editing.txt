Abstract
We present a novel high-fidelity generative adversar-ial network (GAN) inversion framework that enables at-tribute editing with image-specific details well-preserved (e.g., background, appearance, and illumination). We first analyze the challenges of high-fidelity GAN inversion from the perspective of lossy data compression. With a low bit-rate latent code, previous works have difficulties in preserv-ing high-fidelity details in reconstructed and edited images.
Increasing the size of a latent code can improve the accu-racy of GAN inversion but at the cost of inferior editability.
To improve image fidelity without compromising editabil-ity, we propose a distortion consultation approach that em-ploys a distortion map as a reference for high-fidelity recon-struction.
In the distortion consultation inversion (DCI), the distortion map is first projected to a high-rate latent map, which then complements the basic low-rate latent code with more details via consultation fusion. To achieve high-fidelity editing, we propose an adaptive distortion align-ment (ADA) module with a self-supervised training scheme, which bridges the gap between the edited and inversion images. Extensive experiments in the face and car do-mains show a clear improvement in both inversion and edit-ing quality. The project page is https://tengfei-wang.github.io/HFGI/. 1.

Introduction t u p n
I n o i s r e v n
I
) e 4 e (
) e l y t s e
R n o i s r e v n
I (
) s r u
O ( n o i s r e v n
I 1 t i d
E
) s r u
O ( 2 t i d
E
) s r u
O (
Figure 1. High-fidelity image inversion and editing (age, smile, eyes, color, grass). Our method performs well on details preser-vation in both inverted and edited results such as background, makeup, beard/hair style, reflection and shadow.
Image attribute editing is the task of modifying de-sired attributes of a given image while preserving other de-tails. With the rapid advancement of generative adversar-ial networks (GANs) [9], a promising direction is to ma-nipulate images with the strong control capacity of Style-GAN [19,20]. To enable real-world image editing, GAN in-version techniques [40] have been recently explored, which aim at projecting images to the latent space of a pre-trained
GAN generator.
Existing GAN inversion approaches either perform per-image optimization [1, 17, 45] or learn a data-driven en-coder [26, 34]. Optimization approaches achieve higher reconstruction accuracy by over-fitting on a single image, but the latent code may get out of GAN manifold, lead-ing to inferior editing quality.
In contrast, encoder-based
GAN inversion methods are faster and show better edit-ing performance due to knowledge learned from numerous training images. Nevertheless, their reconstruction results are usually inaccurate and of low fidelity: these methods can reconstruct a coarse layout (low-frequency patterns), but the image-specific details (high-frequency patterns) are often ignored. For example, the reconstructed face im-ages typically possess averaged patterns that agree with the majority of training images (e.g., normal pose/expression,
occlusion/shadow-free), and the details that present minor-ity patterns (e.g., background, illumination, accessory) in training data are subject to distortion.
It is highly desir-able to preserve these image-specific details in reconstruc-tion and editing with high fidelity.
Though some works tried to improve the reconstruction accuracy of encoder-based methods, their editing perfor-mance usually decreases [34]. To analyze the limitation of existing approaches, we consider the GAN inversion prob-lem as a lossy data compression system with a frozen de-coder. According to Rate-Distortion theory [29], revers-ing a real-world image to a low-dimensional latent code would inevitably lead to information loss. As conjectured by information bottleneck theory [33], the lost information is primarily image-specific details as the deep compression model tends to retain common information of a domain.
Based on these analyses and experimental observations, we present the Rate-Distortion-Edit trade-off for GAN inver-sion, which further inspires our framework.
According to this trade-off, the low-rate latent codes are insufficient for high-fidelity GAN inversion. However, it is non-trivial to improve the reconstruction accuracy by di-rectly increasing the rate. A higher-rate latent codes can easily achieve a low distortion by overfiting on the recon-struction process, but would suffer a dramatic editing per-formance drop. To achieve both accuracy and editability (high-fidelity editing), we propose a novel framework that equips low-rate encoder models with distortion consulta-tion. The consultation branch serves as a ‘cheat sheet’ for generation that only conveys the ignored image-specific information. Specifically, we leverage the distortion map between source and low-fidelity reconstructed image as a reference and project it to higher-rate latent maps. Com-pared with high-rate latent codes inferred from a full image, the distortion map only conveys image-specific details and can thus alleviate the aforementioned overfitting issue. The high-rate latent map and low-rate latent code are further em-bedded and fused in the generator via consultation fusion.
Our scheme shows a clear improvement in reconstruction quality, and no test-time optimization is involved.
For attribute editing, following previous works, we per-form vector arithmetic [25] on the low-rate latent code, while the consultation is desired to bring back lost details.
While the distortion consultation substantially contributes to the inversion quality, it cannot directly apply the distor-tion map observed on the inversion image for editing due to the misalignment between inverted and edited images.
To this end, we additionally design an adaptive distortion alignment (ADA) network to adjust the distortion map with the edited images. To disentangle the alignment from the consultation encoder and stabilize the training, we impose intermediate supervision on ADA by proposing an align-ment regularization with a self-supervised training scheme.
Extensive experiments show that our method signifi-cantly outperforms current approaches in terms of details preservation in both reconstructed and edited results. On account of the high-fidelity inversion capacity, our approach is robust to viewpoint and illumination fluctuation and can thus perform temporally consistent editing on videos. Our primary contributions can be summarized as follows.
• We propose a distortion consultation inversion scheme that combines both high reconstruction quality and compelling editability with consultation fusion.
• For high-fidelity editing, we propose the adaptive dis-tortion alignment module with a self-supervised learn-ing scheme. By alignment, the distortion information can be propagated well to the edited images.
• Our method outperforms state-of-the-art approaches qualitatively and quantitatively on diverse image do-mains and videos. The framework is simple, fast and can be easily applied to GAN models. 2.