Abstract
The semantic segmentation of nighttime scenes is a chal-lenging problem that is key to impactful applications like self-driving cars. Yet, it has received little attention com-pared to its daytime counterpart.
In this paper, we pro-pose NightLab, a novel nighttime segmentation frame-work that leverages multiple deep learning models imbued with night-aware features to yield State-of-The-Art (SoTA) performance on multiple night segmentation benchmarks.
Notably, NightLab contains models at two levels of gran-ularity, i.e. image and regional, and each level is com-posed of light adaptation and segmentation modules. Given a nighttime image, the image level model provides an ini-tial segmentation estimate while, in parallel, a hardness detection module identiﬁes regions and their surrounding context that need further analysis. A regional level model focuses on these difﬁcult regions to provide a signiﬁcantly improved segmentation. All the models in NightLab are trained end-to-end using a set of proposed night-aware losses without handcrafted heuristics. Extensive experi-ments on the NightCity [44] and BDD100K [59] datasets show NightLab achieves SoTA performance compared to concurrent methods. Code and dataset are available at https://github.com/xdeng7/NightLab. 1.

Introduction
Semantic segmentation is a fundamental task in com-puter vision on which there has been much progress recently with the introduction of deep semantic parsing methods, e.g., DeepLab [4, 6] and Transformers [13, 26]. However, the focus has been almost entirely limited to daytime bench-marks such as CityScapes [9] and ADE20k [62]. Much less progress has been made on the nighttime problem such as establishing strong benchmarks and designing effective ar-chitectures. Yet, success on the nighttime scene segmenta-tion is crucial for a number of impactful applications such
Figure 1. Visual comparisons of segmentation results from UPer-Swin [26] and our proposed NightLab. NightLab shows im-provements on the parts of motorcycle and rider, where UPer-Swin predicts rider as person, and motorcycle as car, and poles are miss-ing. NightLab is able to provide details for small objects. as autonomous driving [29], robotic vision [11], etc.
There are far fewer open-source labelled nighttime im-ages than daytime ones. Most nighttime image collections contain only unlabeled images and so there has been a lot of work [54] on unsupervised domain adaptation between daytime and nighttime for segmentation. Our experience, based on experiments, is that these adaptation frameworks perform poorly in practice due to the large domain gap be-tween daytime and nighttime scenes.
Recently, Tan et al. [44] proposed NightCity which makes progress on two key challenges in nighttime seg-mentation: the lack of a large realistic dataset and the large illumination variation that results from over or under ex-posure in night scenes. The NightCity effort resulted a large dataset of densely labelled images and a segmenta-tion model that contains an exposure-guided layer designed for light changes. The model is shown to outperform unsu-pervised methods.
*This work was done when Xueqing was interned at ByteDance.
Our work in this paper takes additional steps in this di-Figure 2. NightLab overview and inference. With the input images, there is a duel-level architecture to produce the ﬁnal output. Note the hard contexts (boxes) here are automatically discovered without ground truth. 1) The image-level networks ΦI seg is used to create predictions P I for the whole images. Most of the easy regions Reasy can be accurately predicted by ΦI seg. 2) Then, hard regions
Rhard will be detected by HDM ΦR det with the input images. Once the regions are discovered, they will be zoom-in and processed by seg to obtain local prediction P R of Rhard. At last, P R will be merged back to P I to generate the ﬁnal segmentation output.
ΦR light and ΦR light, ΦI rection and proposes NightLab, a nighttime segmentation framework focused on architecture optimization using real labelled night images which results in a signiﬁcant, i.e.,
∼10%, absolute improvement over the original NightCity baseline [44]. Speciﬁcally, we employ effective model ar-chitecture design to achieve two goals related to the large il-lumination variation in nighttime images. First, is to reduce the amount of light variation. Rather than performing sim-ple exposure enhancement, we propose a regularized light adaptation module (ReLAM) based on a large amount of day and night images. Different from image translation ap-proaches [1,21,64] that can signiﬁcantly alter image appear-ance, ReLAM preserves night image texture which helps avoid large domain shifts during adaptation, yielding better generalization for night images. Second, due to the low-light levels and blurry texture, small objects are often not distinguishable based on their appearance alone. Therefore, as illustrated in early work on object understanding [30], context is crucial for helping resolve potential ambiguity in certain nighttime image regions. While most deep networks contain multi-scale structures by enumerating scales such as HRNet [49], night images have objects with substantial scale variation, e.g., road light and bicycle as illustrated in
Fig. 1. Such variation is often beyond the scope of the enu-merated scales in modern networks. To tackle this issue, we propose a hardness detection module (HDM), which adopts the idea of regional proposal network (RPN) from objec-tion detection. Our HDM identiﬁes regions, along with their context, that need additional attention and analysis. Finally, we adopt the SoTA architecture of Swin-Transformer [26] as our segmentation encoder and embed DeformConv [10] as the decoder. This provides improved architecture capac-ity and context modeling ability.
In summary, as illustrated in Fig. 2, inference in
NightLab works as follows. Given a night image, the image level model ﬁrst adapts the image light through Re-LAM (ΦI light) and sends it to an image-level segmentation model (ΦI seg), producing an overall segmentation. In tan-dem, HDM is used to detect hard regions that need further analysis. These regions are cropped, batched, and sent to a regional level model which adapts and segments these re-gional patches similar to the image level. Here, our regional model is not trained over the full set of classes but is limited to a subset of automatically identiﬁed difﬁcult classes such as bicycle and road light to better mine the context informa-tion needed to distinguish their semantics. The segmented results from the region level are then merged with the image level parsing results, yielding the ﬁnal segmentation.
Finally, since we found many mislabelled pixels in the
NightCity validation images as shown in Sec. 4, we man-ually relabel the dataset so that the evaluation of our and other methods is more meaningful. Extensive experimen-tation shows NightLab outperforms concurrent methods and ablative studies demonstrate the contribution of each of the proposed modules
In summary, our contribution includes: 1. We propose NightLab, a dual-level architecture with novel modules including ReLAM and HDM specif-ically designed for night scene segmentation. The framework achieves SoTA performance on multiple nighttime benchmarks. 2. We propose an effective training pipeline for the archi-tecture whose modularity provides good interpretabil-ity of our improvements. 3. We derive a more accurate benchmark dataset from
NightCity and conduct extensive experiments that in-vestigate a variety of night scene segmentation strate-gies. Our benchmark dataset and strong baseline serve as a good starting point for future researchers. 2.