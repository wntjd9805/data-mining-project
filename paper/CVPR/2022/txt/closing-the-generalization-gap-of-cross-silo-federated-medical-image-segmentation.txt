Abstract
Cross-silo federated learning (FL) has attracted much attention in medical imaging analysis with deep learning in recent years as it can resolve the critical issues of insuffi-cient data, data privacy, and training efficiency. However, there can be a generalization gap between the model trained from FL and the one from centralized training. This impor-tant issue comes from the non-iid data distribution of the local data in the participating clients and is well-known as client drift. In this work, we propose a novel training frame-work FedSM to avoid the client drift issue and successfully close the generalization gap compared with the centralized training for medical image segmentation tasks for the first time. We also propose a novel personalized FL objective formulation and a new method SoftPull to solve it in our proposed framework FedSM. We conduct rigorous theoreti-cal analysis to guarantee its convergence for optimizing the non-convex smooth objective function. Real-world medical image segmentation experiments using deep FL validate the motivations and effectiveness of our proposed method. 1.

Introduction
Deep learning models have shown success in computer vision tasks in recent years [19, 41, 42]. However, training deep models that generalize well on unseen test data may require massive training data. Unfortunately, we are usu-ally faced with insufficient data in a single medical institu-tion for the medical image segmentation task due to the ex-pensive procedure of collecting enough patients’ data with experts’ labeling.
A straightforward solution to address the insufficient data issue is gathering data from all the available medical
*Work done during an internship at NVIDIA. A.X. and H.H. were partially supported by NSF IIS 1845666, 1852606, 1838627, 1837956, 1956002, IIA 2040588.
Implementation of this work is available at https://github.com/NVIDIA/NVFlare/examples/FedSM institutions, while the amount of data owned by any sin-gle institution may be insufficient to train a well-performing deep model. However, this approach will raise the concern for data privacy. On one hand, collecting medical data is expensive as mentioned above, and those data have become a valuable asset at a medical institution. Institutions with more data may be more reluctant to contribute their data.
In addition, medical institutions bear the obligation to keep the data collected from patients secure. Gathering data may expose patients to the risk of data leakage.
Of course, we can leverage the existing vanilla dis-tributed training method [31,49,50] to keep the institution’s data local and share only the gradient with a central server.
But the training of deep model requires many iterations to converge, leading to unacceptable communication com-plexity for vanilla distributed training. It is not secure nei-ther as recent works [14, 52, 54, 55] have shown that pixel-level images can be recovered from the leaked gradient.
Recently, federated learning (FL) [13, 16, 29, 48] have been proposed to tackle all the above issues (insufficient data, data privacy, training efficiency).
In medical appli-cations, we are most interested in the cross-silo federated learning where we have a limited number of participating clients compared with cross-device federated learning (e.g., mobile devices) [17, 26, 36]. Specifically, in each training round of FedAvg [38], the de facto algorithm for FL, each client will perform local training with the global model re-ceived from a central server for multiple iterations. Then the server gathers all the local models from each client and averages them as the new global model. Nevertheless, for FedAvg and its variants, a non-negligible issue called
“client drift” arises due to non-iid data distribution on dif-ferent clients. The local models on different clients will gradually diverge from each other during the local train-ing. Client drift can drastically jeopardize the training per-formance of the global model when the data similarity de-creases (more non-iid) [20, 21]. Theoretically, it leads to a convergence rate more sensitive to the number of local
training steps [53].
Throughout this paper, we refer to centralized training as gathering data from clients and then training the model.
Note that centralized training is impractical as it violates data privacy, but offers a performance upper bound for FL algorithms. Despite numerous efforts and previous works, there is still a generalization gap between FL and the cen-tralized training. In this paper, unlike any previous works, we propose a novel training framework called Federated
Super Model (FedSM) to avoid confronting the difficult client drift issue at all for FL medical image segmentation tasks. In FedSM, instead of finding one global model that fits all clients’ data distribution, we propose to produce per-sonalized models to fit different data distributions well and a novel model selector to decide the closest model/data dis-tribution for any test data.
We summarize our contributions as follows.
• We propose a novel training framework FedSM to avoid the client drift issue and close the generalization gap be-tween FL and centralized training for medical segmenta-tion tasks for the first time to the best of our knowledge.
• We propose a novel formulation for personalized FL op-timization, and a novel personalized method called Soft-Pull to solve it in our framework FedSM. A rigorous convergence analysis with common assumptions in FL is given for the proposed method.
• Experiments in real-world FL medical image segmenta-tion tasks validate our motivation and the superiority of our methods over existing FL baselines. 2.