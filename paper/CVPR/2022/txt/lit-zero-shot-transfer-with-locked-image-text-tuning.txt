Abstract
This paper presents contrastive-tuning, a simple method employing contrastive training to align image and text mod-els while still taking advantage of their pre-training. In our empirical study we ﬁnd that locked pre-trained image mod-els with unlocked text models work best. We call this in-stance of contrastive-tuning “Locked-image Tuning” (LiT), which just teaches a text model to read out good repre-sentations from a pre-trained image model for new tasks.
A LiT model gains the capability of zero-shot transfer to new vision tasks, such as image classiﬁcation or retrieval.
The proposed LiT is widely applicable; it works reliably with multiple pre-training methods (supervised and unsu-pervised) and across diverse architectures (ResNet, Vision
Transformers and MLP-Mixer) using three different image-text datasets. With the transformer-based pre-trained ViT-g/14 model, the LiT model achieves 84.5% zero-shot trans-fer accuracy on the ImageNet test set, and 81.1% on the challenging out-of-distribution ObjectNet test set. 1.

Introduction
Transfer learning [44] has been a successful paradigm in computer vision [32, 33, 42]. Zero-shot learning [35, 36, 65] is an alternative approach aiming to develop models that can handle a new task without task-speciﬁc data or adaptation protocols. Recently it was demonstrated that web-sourced paired image-text data can be used to pre-train strong mod-els for zero-shot transfer [30, 45]. Zero-shot transfer dif-fers from classical zero-shot learning in that the transfer setup may see relevant supervised information during pre-training; it is zero-shot insofar as no supervised examples are used during the transfer protocol. GPT-3 [3] explored a similar zero-shot transfer setup using model prompting via natural language.
In [30, 45] authors propose a contrastive learning frame-work where an image model (or image tower) is trained si-multaneously with a text model (or text tower). Both towers are trained to minimize a contrastive loss, which encourages (cid:63)equal technical contribution, †equal advising
Figure 1. Comparison to the previous SOTA methods. Left: re-sults on public YFCC100m subset, with from-scratch, ﬁne-tuned from a pre-trained image model, and LiT with a pre-trained image model. The proposed LiT improves over 30% ImageNet zero-shot transfer accuracy on YFCC100m subset. Right: results on pri-vately gathered data, LiT halves the gap between previous from-scratch methods CLIP [45], ALIGN [30] and supervised ﬁne-tuning [12, 68]. representations of paired images and texts to be similar, and representations of non-paired images and texts to be dis-similar. At test time, the resulting model can be used for zero-shot image classiﬁcation by comparing the image em-bedding with embeddings of textual class descriptions.
In this paper, we adopt a contrastive learning frame-work and propose a more data- and compute-efﬁcient strat-egy named contrastive-tuning. The key idea is to tune the text tower using image-text data, while using a pre-trained, strong image model as the image tower. During training, both towers’ weights can be locked or unlocked, leading to different design choices that are illustrated in Figure 2.
Speciﬁcally, we ﬁnd that locking the image tower works best, as shown in Figure 1. We call this speciﬁc instance of contrastive-tuning “Locked-image Tuning” (LiT), which just teaches a text model to read out suitable representations from a pre-trained image model. LiT achieves better results compared with the from-scratch CLIP [45] or ALIGN [30] models. With the pre-trained model ViT-g/14 [68], LiT achieves 84.5% zero-shot transfer accuracy on ImageNet, halving the gap between previous best zero-shot transfer re-sults [30,45] and supervised ﬁne-tuning results [12,68]. The best LiT model also sets new state-of-the-art on several out-of-distribution (OOD) ImageNet test variants, compared to previous supervised and unsupervised methods. For exam-ple, it achieves 81.1% accuracy on the challenging Object-Net test set [1], outperforming the previous state-of-the-art method [45] by 7.8%.
We believe the reason that LiT works well lies in its de-coupling of data sources and techniques for learning image descriptors and vision-language alignment. Image-text data can be great for learning correspondences between natural language and the visual world, but, at the same time, it may not be precise and clean enough to result in state-of-the-art image descriptors. In this paper we carefully investigate this hypothesis and support it with empirical evidence.
The proposed LiT works with both supervised and self-supervised pre-trained models. We verify LiT across three image-text datasets, with Vision Transformer [20],
ResNet [32], and MLP-Mixer [60] architectures. We also show that with a self-supervised pre-trained model, i.e.
DINO [4] or MoCo-v3 [10], LiT achieves better perfor-mance compared to from-scratch contrastive-learning.
Another contribution of this paper is the proposed recipe for high-performance zero-shot models that can be trained using only modest computational resources and public datasets. By re-using already pre-trained models (e.g. pub-licly released in the literature), the computational resources used to train the image models can be amortized. Fur-thermore, we explore publicly available datasets such as
YFCC100m [59] and CC12M [5]. Combined with the computational efﬁciency, we hope to facilitate contributions from a wider audience to research in zero-shot transfer. 2.