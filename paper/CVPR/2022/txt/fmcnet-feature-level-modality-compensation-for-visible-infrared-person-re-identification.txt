Abstract
For Visible-Infrared person Re-IDentification (VI-ReID), existing modality-specific information compensation based models try to generate the images of missing modal-ity from existing ones for reducing cross-modality dis-crepancy. However, because of the large modality dis-crepancy between visible and infrared images, the gen-erated images usually have low qualities and introduce much more interfering information (e.g., color inconsis-tency).
This greatly degrades the subsequent VI-ReID performance. Alternatively, we present a novel Feature-level Modality Compensation Network (FMCNet) for VI-ReID in this paper, which aims to compensate the miss-ing modality-specific information in the feature level rather than in the image level, i.e., directly generating those miss-ing modality-specific features of one modality from exist-ing modality-shared features of the other modality. This will enable our model to mainly generate some discrim-inative person related modality-specific features and dis-card those non-discriminative ones for benefiting VI-ReID.
For that, a single-modality feature decomposition module is first designed to decompose single-modality features into modality-specific ones and modality-shared ones. Then, a feature-level modality compensation module is present to generate those missing modality-specific features from ex-isting modality-shared ones. Finally, a shared-specific fea-ture fusion module is proposed to combine the existing and generated features for VI-ReID. The effectiveness of our proposed model is verified on two benchmark datasets. 1.

Introduction
Person Re-IDentification (ReID) aims at matching the given pedestrians from an image gallery taken by differ-ent cameras. Most existing ReID models focus on the visible-visible image matching (i.e., VV-ReID). However,
*Equally corresponding authors.
Figure 1. Illustration of the differences between our model and (a) Existing modality-shared feature existing VI-ReID models. learning based models. (b) Existing image-level compensation based models. (c) Our proposed feature-level compensation based model. these models may have poor performance when visible cameras cannot well capture information, such as at night.
Compared with visible cameras, infrared cameras can still capture clear images under those poor illumination condi-tions. Moreover, most cameras in modern surveillance sys-tems support autoswitch between the visible and infrared modes under different illumination conditions. Accord-ingly, Visible-Infrared ReID (i.e., VI-ReID) has raised more and more attention recently.
The main challenge of VI-ReID lies in the modality dis-crepancy between the visible and infrared images. Mean-while, it also surfers from large person variations, such as viewpoints and postures. As shown in Fig. 1(a), most exist-ing models [1–7] try to extract the discriminative modality-shared features for VI-ReID. Although great improvements have been achieved, these models inevitably discard lots of discriminative person-related modality-specific informa-tion, which may also benefit VI-ReID. Considering that,
some works [8, 9] propose the idea of modality-specific in-formation compensation, which attempts to first generate those missing modality-specific information from existing modality and then jointly uses the generated and original information for VI-ReID.
However, existing modality-specific information com-pensation based models usually achieve inferior results compared with those modality-shared feature learning based models. This may impute to the image-level com-pensation of existing models. That is, as shown in Fig. 1(b), existing models first generate the images of missing modality from the images of existing modality and then ex-tract discriminative person features from the paired images for VI-ReID. However, it is very difficult to generate high-quality images of one modality from another modality, due to the large modality discrepancy between the visible and infrared images. Especially, when generating visible im-ages from infrared images, much more noisy information (e.g., color inconsistency), instead of discriminative per-son features, will be introduced for VI-ReID. Besides, these existing modality-specific information compensation based models usually follow a two-stage structure and are not end-to-end trainable, where the image generation sub-networks and VI-ReID subnetworks are independent trained.
Actually, compared with the modality discrepancy be-tween visible and infrared images, their features’ discrep-ancy has been reduced to some extents, since some com-mon semantics information usually coexists in the unimodal visible and infrared features. Therefore, the translation be-tween visible data and infrared data in the feature level may be easier than that in the image level. Meanwhile, as discussed in some existing works [10–12], the single-modality features (e.g., unimodal visible features or in-frared features) can be decomposed into their own modality-specific features and modality-shared features. The dif-ficulties for cross-modality translation can be further re-duced by generating those missing modality-specific fea-tures from existing modality-shared features rather than from the whole single-modality features. More impor-tantly, compared with image-level translation, the feature-level translation allows us to flexibly control the generation of those missing modality-specific features as our require-ments by designing some dedicated loss functions. For ex-ample, we can only generate some discriminative person-related modality-specific features and discard those non-discriminative ones for benefiting VI-ReID.
Considering that, we will present a novel end-to-end feature-level modality-specific information compensation based model, i.e., the Feature-level Modality Compensation
Network (FMCNet), for VI-ReID in this paper. As shown in Fig. 1(c), our proposed FMCNet aims to compensate those missing modality-specific information in the feature level rather than in the image level, i.e., directly generat-ing those missing modality-specific features of one modal-ity from existing modality-shared features of other modal-ity. To this end, a Single-modality Feature Decomposi-tion (SFD) module is first utilized to decompose the input single-modality features into their own modality-specific and modality-shared features, respectively. Meanwhile, a modality decomposition loss is designed to facilitate the decomposition of those single-modality features. Then, a
Feature-level Modality Compensation (FMC) module is de-signed to generate the missing modality-specific features of one modality from the existing modality-shared ones of the other modality for each sample image. Finally, a Shared-specific Feature Fusion (SFF) module is designed to jointly use the existing modality-shared and modality-specific fea-tures as well as the generated modality-specific features for
VI-ReID.
Similarly, cm-SSFT [13] also tries to simultaneously ex-ploit those modality-shared and modality-specific features for VI-ReID. It achieves shared-specific feature transfer by modeling the affinities among different samples. Specially, those missing modality-specific features in the cm-SSFT are transfered from all the samples of the other modality in the gallery. This may also introduce more modality-specific information of other identities, thus easily leading to sub-optimal results. Different from cm-SSFT, our proposed model does not rely on other samples and is able to directly and flexibly generate those missing modality-specific fea-tures from its own modality-shared features.
In summary, the main contributions of this work are as follows: (1) A novel FMCNet is presented, which proposes feature-level rather than image-level modality-specific in-formation compensation for VI-ReID. This enables our model to focus on generating some required missing modality-specific features (e.g., discriminative person-related ones) for VI-ReID. (2) Our proposed FMCNet provides an unified end-to-end framework, achieving unimodal feature decompo-sion, modality-specific feature compensation and modality shared-specific feature fusion for VI-ReID via the proposed
SFD, FMC and SFF modules, respectively. (3) Our model significantly outperforms those image-level compensation based models and obtains competitive and even better results than some state-of-the-art modality-shared feature learning based ones. 2.