Abstract
Recently, Sharpness-Aware Minimization (SAM), which connects the geometry of the loss landscape and general-ization, has demonstrated a signiﬁcant performance boost on training large-scale models such as vision transform-ers. However, the update rule of SAM requires two se-quential (non-parallelizable) gradient computations at each step, which can double the computational overhead. In this paper, we propose a novel algorithm LookSAM - that only periodically calculates the inner gradient ascent, to signiﬁ-cantly reduce the additional training cost of SAM. The em-pirical results illustrate that LookSAM achieves similar ac-curacy gains to SAM while being tremendously faster - it en-joys comparable computational complexity with ﬁrst-order optimizers such as SGD or Adam. To further evaluate the performance and scalability of LookSAM, we incorporate a layer-wise modiﬁcation and perform experiments in the large-batch training scenario, which is more prone to con-verge to sharp local minima. Equipped with the proposed algorithms, we are the ﬁrst to successfully scale up the batch size when training Vision Transformers (ViTs). With a 64k batch size, we are able to train ViTs from scratch in min-utes while maintaining competitive performance. The code is available here: https://github.com/yong-6/LookSAM 1.

Introduction
It has been observed that sharp local minima usually leads to signiﬁcantly dropped generalization performance of deep networks, and many methods have been proposed for mitigating this issue [3, 12, 19, 23, 26, 38]. In particu-lar, Foret et al. [13] recently proposed an algorithm named
Sharpness Aware Minimization (SAM), which explicitly penalizes the sharp minima and biases the convergence to a ﬂat region. SAM has been used to achieve state-of-the-art performance in many applications. For instance, Chen et al.
[4] showed that SAM optimizer can improve the valida-tion accuracy of Vision Transformer models (ViTs) [10] on
ImageNet-1k by a signiﬁcant amount (+5.3% when training from scratch). However, the update rule of SAM involves two sequential (non-parallelizable) gradient computations at each step, which will double the training time.
In this paper, we aim to improve the efﬁciency of SAM and apply it to large-scale training problems. Each step of
SAM consists of two gradient computations – one for ad-versarial perturbation to the weights and the other for com-puting the ﬁnal update. A naive idea to speedup SAM is to compute the ﬁrst gradient (adversarial perturbation on weights) only periodically and use standard SGD/Adam up-dates in between. Unfortunately, this leads to signiﬁcantly degraded performance, as shown in our experiments. To resolve this issue, we decompose the SAM’s update direc-tion into two components — the one that lies parallel to the original SGD direction and the other orthogonal com-ponent. Since the second direction captures the differences between SAM’s update and SGD’s update, we hypothesize that this component can bias learning towards a ﬂat region.
Interestingly, we show this second direction tends to remain similar across nearby iterations, both empirically and theo-retically. Based on this ﬁnding, we develop a novel Look-SAM optimizer to reuse this direction across nearby itera-tions. The resulting LookSAM only needs to periodically calculate the inner gradient ascent and signiﬁcantly reduce the computational complexity of SAM while maintaining similar generalization performance.
As SAM has become a crucial component for training large-scale Vision Transformer models (ViTs) [4], to fur-ther evaluate the performance and scalability of the pro-posed algorithm, we consider a challenging task — apply-ing LookSAM to conduct large-batch training for ViTs. As pointed out in [45, 48], large-batch training often introduces the non-uniform instability problem across different layers.
Hence, we also adopt a layer-wise scaling rule for weight perturbation, namely Look-LayerSAM optimizer. The pro-posed optimizer can successfully train ViTs with 64K batch size within an hour while maintaining competitive perfor-mance.
Our contributions can be summarized in three folds.
• We develop a novel algorithm, called LookSAM, to
speed up the training of SAM. Instead of computing the inner gradient ascent at every step, our method only computes it periodically while being able to ap-proximate the original SAM’s direction for every up-date. The empirical results illustrate that LookSAM achieves similar accuracy gains to SAM while enjoy-ing comparable computational complexity with ﬁrst-order optimizers such as SGD or Adam.
• Inspired by the successes of layer-wise scaling pro-posed in large-batch training [46, 48], we develop an algorithm to scale up the batch size of Look-SAM by adopting layer-wise scaling rule for weight perturbation (Look-LayerSAM). The proposed Look-LayerSAM can scale up the batch size to 64k, which is a new record for ViT training and is 16 compared with previous training settings.
⇥
• Our proposed Look-LayerSAM can achieve
⇥ speedup over the training settings in [10] with a 4k batch size, and we can ﬁnish the ViT-B-16 training in 0.7 hour. To the best of our knowledge, this is a new speed record for ViT training.
⇠ 8 2.