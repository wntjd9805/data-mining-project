Abstract
Making generative models 3D-aware bridges the 2D image space and the 3D physical world yet remains chal-lenging. Recent attempts equip a Generative Adversarial
Network (GAN) with a Neural Radiance Field (NeRF), which maps 3D coordinates to pixel values, as a 3D prior.
However, the implicit function in NeRF has a very local receptive field, making the generator hard to become aware of the global structure. Meanwhile, NeRF is built on volume rendering which can be too costly to produce high-resolution results, increasing the optimization difficulty. To alleviate these two problems, we propose a novel frame-work, termed as VolumeGAN, for high-fidelity 3D-aware image synthesis, through explicitly learning a structural representation and a textural representation. We first learn a feature volume to represent the underlying structure, which is then converted to a feature field using a NeRF-like model. The feature field is further accumulated into a 2D feature map as the textural representation, followed by a neural renderer for appearance synthesis. Such a design enables independent control of the shape and the appearance. Project page is at https://genforce. github.io/volumegan. 1.

Introduction
Learning 3D-aware image synthesis draws wide atten-tion recently [3,30,35]. An emerging solution is to integrate a Neural Radiance Field (NeRF) [28] into a Generative
Adversarial Network (GAN) [7]. the 2D
Convolutional Neural Network (CNN) based generator is replaced with a generative implicit function, which maps the raw 3D coordinates to point-wise densities and colors conditioned on the given latent code. Such an implicit function encodes the structure and the texture of the output image in the 3D space.
Specifically,
However, there are two problems of directly employing
NeRF [28] in the generator. On one hand, the implicit function in NeRF produces the color and density for each 3D point using a Multi-Layer Perceptron (MLP) network.
Figure 1. Images of faces and cars synthesized by VolumeGAN, which enables the control of viewpoint, structure, and texture.
With a very local receptive field, it is hard for the MLP to represent the underlying structure globally when synthe-sizing images. Thus only using the 3D coordinates as the inputs [3, 30, 35] is not expressive enough to guide the gen-erator with the global structure. On the other hand, volume rendering generates the pixel values of the output image separately, which requires sampling numerous points along the camera ray regarding each pixel. The computational cost hence significantly increases when the image size becomes larger. It may cause the insufficient optimization of the model training, and further lead to unsatisfying performance for high-resolution image generation.
Prior work has found that 2D GANs benefits from valid representations learned by the generator [36, 44, 45]. Such generative representations describe a synthesis with high-level features. For example, Xu et al. [44] confirm that a face synthesis model is aware of the landmark positions of the output face, and Yang et al. [45] identify the multi-level variation factors emerging from generating bedroom images. These representative features encode rich texture and structure information, thereby enhancing the synthesis quality [16] and the controllability [36] of image GANs. In contrast, as mentioned above, existing 3D-aware generative models directly render the pixel values from coordinates [3, 35], without learning explicit representations.
In this work, we propose a new generative model, termed as VolumeGAN, which achieves 3D-aware image synthesis through explicitly learning a structural and a textural repre-sentation. Instead of using the 3D coordinates as the inputs, we generate a feature volume using a 3D convolutional network, which encodes the relationship between various spatial regions and hence compensates for the insufficient receptive field caused by the MLP in NeRF. With the feature volume modeling the underlying structure, we query a co-ordinate descriptor from the feature volume to describe the structural information for each 3D point. We then employ a NeRF-like model to create a feature field, by taking the coordinate descriptor attached with the raw coordinate as the input. The feature field is further accumulated into a 2D feature map as the textural representation, followed by a CNN with 1 × 1 kernel size to finally render the output image. In this way, we separately model the structure and the texture with the 3D feature volume and the 2D feature map, enabling the disentangled control of the shape and the appearance.
We evaluate our approach on various datasets and demonstrate its superior performance over existing alter-natives.
In terms of the image quality, VolumeGAN achieves substantially better Fr´echet Inception Distance (FID) score [11]. Taking the FFHQ dataset [16] under 256 × 256 resolution as an instance, we improve the FID from 36.7 to 9.1. We also enable 3D-aware image synthesis on the challenging indoor scene dataset, i.e., LSUN bed-room [47]. Our model also suggests stable control of the object pose and shows better consistency across different viewpoints, benefiting from the learned structural represen-tation (i.e., the feature volume). Furthermore, we conduct a detailed empirical study on the learned structural and textural representations, and analyze the trade-off between the image quality and the 3D property. 2.