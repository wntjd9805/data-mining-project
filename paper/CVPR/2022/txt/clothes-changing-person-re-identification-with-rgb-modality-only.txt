Abstract
The key to address clothes-changing person re-identiﬁcation (re-id) is to extract clothes-irrelevant features, e.g., face, hairstyle, body shape, and gait. Most current works mainly focus on modeling body shape from multi-modality information (e.g., silhouettes and sketches), but do not make full use of the clothes-irrelevant information
In this paper, we propose in the original RGB images. a Clothes-based Adversarial Loss (CAL) to mine clothes-irrelevant features from the original RGB images by pe-nalizing the predictive power of re-id model w.r.t. clothes.
Extensive experiments demonstrate that using RGB im-ages only, CAL outperforms all state-of-the-art methods on widely-used clothes-changing person re-id benchmarks.
Besides, compared with images, videos contain richer ap-pearance and additional temporal information, which can be used to model proper spatiotemporal patterns to assist clothes-changing re-id. Since there is no publicly avail-able clothes-changing video re-id dataset, we contribute a new dataset named CCVID and show that there exists much room for improvement in modeling spatiotemporal informa-tion. The code and new dataset are available at: https:
//github.com/guxinqian/Simple-CCReID. 1.

Introduction
Person re-identiﬁcation (re-id) [12,23,54] aims to search the target person from surveillance videos across different locations and times. Most existing works [11, 19, 40] as-sume that pedestrians do not change their clothes in a short period of time. However, if we want to re-identify a pedes-trian over a long period of time, the clothes-changing prob-lem cannot be avoided. Besides, clothes-changing problem also exists in some short-time real-world scenarios, e.g., criminal suspects usually change their clothes to avoid be-ing identiﬁed and tracked. Due to the crucial role in in-telligent surveillance system, clothes-changing person re-id [7, 49] has attracted increasing attention in recent years.
Humans can distinguish their acquaintances, even if
Figure 1. The visualization of (a) two original images, (b) the learned feature maps only with identiﬁcation loss, and (c) the learned feature maps with identiﬁcation loss and the proposed
CAL. Note that all training settings of (b) and (c) are consistent except loss functions. (b) only highlights face as the clothes-irrelevant features, while (c) highlights more clothes-irrelevant features, e.g., face, hairstyle, and body shape. (Since different samples of the same person in the training set mostly wear the same shoes, shoes are also highlighted.) these acquaintances wear clothes that they have never seen before. The reason is that the human brain can decouple and utilize clothes-irrelevant features, e.g., face, hairstyle, body shape, and gait. To avoid the interference of clothes, some clothes-changing re-id methods [6, 18] and gait recognition methods [4, 52] model body shape and gait from multi-modality inputs (e.g., skeletons [35], silhouettes [4], radio signals [7], contour sketches [49], and 3D shape [6]) or by disentangled representation learning [52]. However, multi-modality-based methods need additional models or equip-ment to capture multi-modality information, and learning disentangled representations is usually time-consuming.
Actually, the original RGB modality contains rich clothes-irrelevant information which is largely underuti-lized by the current methods. As for some clothes-changing re-id methods [6, 35], although they use a strong backbone (i.e. ResNet [15]) to extract features from the original im-ages, without a properly designed loss function, the learned feature map only focuses on some simple clothes-irrelevant information, e.g., face (see Fig. 1 (b)), while other crucial clothes-irrelevant information is omitted. As for most gait recognition methods [4, 13], they usually discard the orig-inal input videos and resort to other modality inputs, e.g., silhouettes.
To better mine the clothes-irrelevant information in RGB
modality, in this paper, we propose Clothes-based Adver-sarial Loss (CAL). Speciﬁcally, we add a clothes classiﬁer after the backbone of the re-id model and deﬁne CAL as a multi-positive-class classiﬁcation loss, where all clothes classes belonging to the same identity are mutually posi-tive classes. To the best of our knowledge, this is the ﬁrst work that uses multi-positive-class classiﬁcation to formu-late multi-class adversarial learning. During training, min-imizing CAL can force the backbone of the re-id model to learn clothes-irrelevant features by penalizing the predictive power of the re-id model w.r.t. different clothes of the same identity. With backpropagation, the learned feature map can highlight more clothes-irrelevant features, e.g., hairstyle and body shape, compared with the feature map trained only with identiﬁcation loss (see Fig. 1 (c)). Extensive exper-iments on widely used clothes-changing re-id benchmarks demonstrate that using RGB images only, CAL outperforms all state-of-the-art methods.
Most current clothes-changing person re-id works [35, 49, 50] mainly focus on image-based setting, where both query and gallery samples are images. However, in many real-world re-id scenarios, both query and gallery sets usu-ally consist of lots of videos. Compared with images, videos contain richer appearance information and additional tem-poral information.
It is more promising to learn proper spatiotemporal patterns from videos, e.g., gait, which may be helpful for clothes-changing re-id. Since there is no publicly available dataset, we reconstruct a new Clothes-Changing Video person re-ID (CCVID) dataset from the raw data of a gait recognition dataset (i.e. FVG [52]) and provide ﬁne-grained clothes labels. Extensive evaluations of state-of-the-art methods show that the utilization of richer appearance information and additional temporal informa-tion can boost the performance of clothes-changing per-son re-id signiﬁcantly. We hope CCVID can inspire more clothes-changing video person re-id studies in the future. 2.