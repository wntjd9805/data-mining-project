Abstract
This paper studies semi-supervised learning of seman-tic segmentation, which assumes that only a small portion of training images are labeled and the others remain unla-beled. The unlabeled images are usually assigned pseudo labels to be used in training, which however often causes the risk of performance degradation due to the confirma-tion bias towards errors on the pseudo labels. We present a novel method that resolves this chronic issue of pseudo labeling. At the heart of our method lies error localiza-tion network (ELN), an auxiliary module that takes an im-age and its segmentation prediction as input and identifies pixels whose pseudo labels are likely to be wrong. ELN enables semi-supervised learning to be robust against in-accurate pseudo labels by disregarding label noises during training and can be naturally integrated with self-training and contrastive learning. Moreover, we introduce a new learning strategy for ELN that simulates plausible and di-verse segmentation errors during training of ELN to en-hance its generalization. Our method is evaluated on PAS-CAL VOC 2012 and Cityscapes, where it outperforms all existing methods in every evaluation setting. 1.

Introduction
Recent advances in semantic segmentation have been at-tributed to supervised learning of deep neural networks [6, 7, 33, 37, 50] on large-scale datasets [10, 12, 15, 32]. How-ever, collecting training data for semantic segmentation is labour-intensive and time-consuming due to the prohibitive cost of pixel-wise class labeling, which often leads to a dataset limited in terms of the number of annotated data and class diversity. To address this issue, label efficient learning, such as semi-supervised learning [3, 5, 19, 24, 26, 27, 29, 30, 35, 36], unsupervised learning [9, 46], weakly super-vised learning [1, 2, 4, 11, 23, 28, 42, 48], and synthetic-to-real domain adaptation [20, 25, 31, 44, 45, 47, 53], has been proposed for semantic segmentation.
This paper studies semi-supervised learning of semantic segmentation, which assumes that only a subset of train-ing images are assigned segmentation labels while the oth-ers remain unlabeled. Undoubtedly, the key to the suc-cess of this task is to utilize the unlabeled images effec-tively. Self-training [5, 27, 35, 49] and contrastive learn-ing [3, 29, 51, 52] are techniques commonly used for the purpose in literature. Self-training generates pseudo labels of unlabeled images using a model trained on labeled ones, and uses them for supervised learning. Meanwhile, con-trastive learning forces feature vectors corresponding to the same pseudo label to be close to each other. Although these techniques have improved the performance of semi-supervised semantic segmentation substantially, they share a common drawback: Since predictions for unlabeled im-ages are usually corrupted by errors, learning using such predictions as supervision causes confirmation bias towards the errors and returns corrupted models consequently. Most existing methods alleviate this issue simply by not using un-certain predictions as supervision [3, 19, 29, 36], but their performance depends heavily on hand-tuned thresholds.
A recent approach deals with errors on pseudo labels by learning and exploiting an auxiliary network that corrects the errors [27, 35]; this model, called error correction net-work (ECN), learns from the difference between predictions of the main segmentation network and their ground truth la-bels on the labeled subset of training images. Ideally, ECN can significantly improve the quality of pseudo labels, but in practice, its advantage is often limited due to the chal-lenges in its training. Since the segmentation network is quickly overfitted to a small number of labeled images, its outputs used as input to ECN do not cover a wide variety of prediction errors that ECN faces in testing, which results in limited generalization capability of ECN.
We present a novel method that is also dedicated to han-dling errors on pseudo labels yet better generalizes to those of arbitrary unlabeled images. The core of our method is the error localization network (ELN), which identifies pix-els with erroneous pseudo labels in the form of binary seg-mentation. As will be demonstrated empirically, simply dis-regarding invalid pseudo labels, instead of correcting them, is sufficient to alleviate the confirmation bias and to learn accurate segmentation models. More importantly, since er-Figure 1. Our semi-supervised learning framework incorporating ELN. It employs two segmentation networks, the student (s), which will be our final model, and the teacher (t) used for generating pseudo labels. The student is trained using the pseudo labels of the teacher in two different ways, self-training and contrastive learning. To be specific, the decoder has two heads, one for segmentation (Seg) and the other for feature embedding (Proj); self-training and contrastive learning are applied to outputs of the Seg and Proj heads, respectively. Then the teacher is updated by an exponential moving average (EMA) of the student. ELN allows both self-training and contrastive learning to be robust against noises on pseudo labels by identifying and disregarding pixels whose pseudo labels are likely to be noisy. ror localization is a class-agnostic subproblem of error cor-rection and accordingly easier to solve, it is more straight-forward to train an accurate and well-generalizable network for the target task.
Moreover, we design a novel training strategy for ELN to further improve its generalization. Specifically, we at-tach multiple auxiliary decoders to the main segmentation network and train them to achieve different accuracy levels so that they simulate the segmentation network at different training stages. ELN is then trained to localize errors on the predictions given by the auxiliary decoders as well as the main segmentation network. This strategy improves gen-eralization of ELN since such predictions used as input to
ELN potentially exhibit error patterns that the segmentation network causes during self-training with unlabeled images.
The trained ELN is then used for semi-supervised learn-ing of semantic segmentation; the overall pipeline incor-porating ELN is illustrated in Fig. 1. Our framework ex-ploits unlabeled images in two ways: self-training and con-trastive learning, both relying on pseudo labels. To this end, we adopt two segmentation networks: A student network, which will be our final model, and a teacher network gener-ating pseudo labels and updated by an exponential moving average of the student. Self-training is done by learning the student using pseudo labels produced by the teacher. Mean-while, contrastive learning encourages embedding vectors of the student and teacher to be similar if their pseudo la-bels are identical. ELN helps improve the effect of both self-training and contrastive learning by filtering out poten-tially erroneous pseudo labels.
Following the convention, the proposed method is eval-uated on the PASCAL VOC 2012 [12] and Cityscapes [10] datasets while varying the number of labeled training im-ages, and it demonstrates superior performance to previous work on both of the datasets. In brief, our main contribution is three-fold.
• We propose error localization, a new approach to deal-ing with errors on pseudo labels. It is simple yet effec-tive and can be naturally incorporated with self-training and contrastive learning. Moreover, we empirically demonstrate the superiority of error localization to error correction.
• We develop a new strategy for generating diverse and plausible prediction errors intentionally during the train-ing of ELN. This improves the generalization of ELN even using a small number of labeled data for training.
• Segmentation networks trained by our method achieves the state of the art on two benchmark datasets, PASCAL
VOC 2012 and Cityscapes, in every setting. 2.