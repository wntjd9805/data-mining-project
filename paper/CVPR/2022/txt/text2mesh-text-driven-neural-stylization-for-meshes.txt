Abstract
In this work, we develop intuitive controls for edit-ing the style of 3D objects. Our framework, Text2Mesh, stylizes a 3D mesh by predicting color and local geo-metric details which conform to a target text prompt.
We consider a disentangled representation of a 3D ob-ject using a ﬁxed mesh input (content) coupled with a learned neural network, which we term a neural style
ﬁeld network (NSF). In order to modify style, we ob-tain a similarity score between a text prompt (describ-ing style) and a stylized mesh by harnessing the rep-resentational power of CLIP. Text2Mesh requires nei-ther a pre-trained generative model nor a specialized 3D mesh dataset.
It can handle low-quality meshes (non-manifold, boundaries, etc.) with arbitrary genus, and does not require UV parameterization. We demon-strate the ability of our technique to synthesize a myr-iad of styles over a wide variety of 3D meshes. Our code and results are available in our project webpage: https://threedle.github.io/text2mesh/. 1.

Introduction
Editing visual data to conform to a desired style, while preserving the underlying content, is a longstanding objec-tive in computer graphics and vision [14,21,23,24,30]. Key challenges include proper formulation of content, style, and the constituents for representing and modifying them.
To edit the style of a 3D object, we adapt a formulation of geometric content and stylistic appearance commonly used in computer graphics pipelines [2]. We consider content as the global structure prescribed by a 3D mesh, which deﬁnes the overall shape surface and topology. We consider style as the object’s particular appearance or affect, as determined by its color and ﬁne-grained (local) geometric details. We propose expressing the desired style through natural lan-guage (a text prompt), similar to how a commissioned artist is provided a verbal or textual description of the desired work. This is facilitated by recent developments in joint embeddings of text and images with CLIP [44]. A natural cue for modifying the appearance of 3D shapes is through 2D projections, as they correspond with how humans and machines perceive 3D geometry. We use a neural network to synthesize color and local geometric details over the 3D input shape, which we refer to as a neural style ﬁeld (NSF).
The weights of the NSF network are optimized such that the resulting 3D stylized mesh adheres to the style described by text. In particular, our neural optimization is guided by multiple 2D (CLIP-embedded) views of the stylized mesh matching our target text. Results of our technique, called
Text2Mesh, are shown in Fig. 1. Our method produces dif-ferent colors and local deformations for the same 3D mesh content to match the speciﬁed text. Moreover, Text2Mesh produces structured textures that are aligned with salient features (e.g. bricks in Fig. 2), without needing to esti-mate sharp 3D curves or a mesh parameterization [33, 52].
Our method also demonstrates global understanding; e.g. in Fig. 3 human body parts are stylized in accordance with their semantic role. We use the weights of the NSF net-work to encode a stylization (e.g. color and displacements) over the explicit mesh surface. Meshes faithfully portray 3D shapes and can accurately represent sharp, extrinsic features
Stained Glass
Fur
Burgundy Crochet
Cactus
Bark
Brick
Wood
Chainmail
Wicker
Wood
Wicker
Bark
Colorful Crochet
Colorful Doily
Cactus
Embroidered
Beaded
Chainmail
Cactus
Spotted Fur
Wicker
Robot
Poncho
Metal Scales
Wood
Beaded
Embroidered
Figure 2. Given a source mesh (gray), our method produces stylized meshes (containing color and local geometric displacements) which
Insets show a close up of the stylization (with color), and the underlying geometry produced by the conform to various target texts. deformation component (without color). Insets of the source mesh are also shown on the left most column. using a high level of detail. Our neural style ﬁeld is comple-mentary to the mesh content, and appends colors and small displacements to the input mesh. Speciﬁcally, our neural style ﬁeld network maps points on the mesh surface to style attributes (i.e., RGB colors and displacements).
We guide the NSF network by rendering the stylized 3D mesh from multiple 2D views and measuring the similar-ity of those views against the target text, using CLIP’s em-bedding space. However, a straightforward optimization of the 3D stylized mesh which maximizes the CLIP similar-ity score converges to a degenerate (i.e. noisy) solution (see
Fig. 5). Speciﬁcally, we observe that the joint text-image embedding space contains an abundance of false positives, where a valid target text and a degenerate image (i.e. noise, artifacts) result in a high similarity score. Therefore, em-ploying CLIP for stylization requires careful regularization.
We leverage multiple priors to effectively guide our NSF network. The 3D mesh input acts as a geometric prior that imposes global shape structure, as well as local details that indicate the appropriate position for stylization. The weights of the NSF network act as a neural prior (i.e. reg-ularization technique), which tends to favor smooth solu-tions [19, 46, 58]. In order to produce accurate styles which contain high-frequency content with high ﬁdelity, we use a frequency-based positional encoding [56]. We garner a strong signal about the quality of the neural style ﬁeld by rendering the stylized mesh from multiple 2D views and then applying 2D augmentations. This results in a system which can effectively avoid degenerate solutions, while still maintaining high-ﬁdelity results.
The focus of our work is text-driven stylization, since text is easily modiﬁable and can effectively express com-plex concepts related to style. Text prescribes an abstract notion of style, allowing the network to produce different
Input
Steve Jobs
Yeti
Astronaut Buzz Lightyear Ninja
Lawyer
Messi
Batman
Hulk
Figure 3. Given the same input bare mesh, our neural style ﬁeld network produces deformations for outerwear of various types (capturing
ﬁne details such as creases in clothing and complementary accessories), and distinct features such as muscle and hair. The synthesized colors consider both local geometric details and global part-aware semantics. Insets of the source mesh are shown in the top row and insets of the stylized output are shown in the middle (uncolored) and bottom (colored) rows. valid stylizations which still adhere to the text. Beyond text, our framework extends to additional target modalities, such as images, 3D meshes, or even cross-modal combinations.
In summary, we present a technique for the semantic ma-nipulation of style for 3D meshes, harnessing the represen-tational power of CLIP. Our system combines the advan-tages of explicit mesh surfaces and the generality of neural
ﬁelds to facilitate intuitive control for stylizing 3D shapes.
A notable advantage of our framework is its ability to han-dle low-quality meshes (e.g., non-manifold) with arbitrary genus. We show that Text2Mesh can stylize a variety of 3D shapes with many different target styles. 2.