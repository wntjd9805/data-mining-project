Abstract
Real-world data typically follow a long-tailed distribu-tion, where a few majority categories occupy most of the data while most minority categories contain a limited num-ber of samples. Classification models minimizing cross-entropy struggle to represent and classify the tail classes.
Although the problem of learning unbiased classifiers has been well studied, methods for representing imbalanced data are under-explored.
In this paper, we focus on rep-resentation learning for imbalanced data. Recently, su-pervised contrastive learning has shown promising perfor-mance on balanced data recently. However, through our theoretical analysis, we find that for long-tailed data, it fails to form a regular simplex which is an ideal geomet-ric configuration for representation learning. To correct the optimization behavior of SCL and further improve the per-formance of long-tailed visual recognition, we propose a novel loss for balanced contrastive learning (BCL). Com-pared with SCL, we have two improvements in BCL: class-averaging, which balances the gradient contribution of neg-ative classes; class-complement, which allows all classes to appear in every mini-batch. The proposed balanced contrastive learning (BCL) method satisfies the condition of forming a regular simplex and assists the optimization of cross-entropy. Equipped with BCL, the proposed two-branch framework can obtain a stronger feature represen-tation and achieve competitive performance on long-tailed benchmark datasets such as CIFAR-10-LT, CIFAR-100-LT,
ImageNet-LT, and iNaturalist2018. 1.

Introduction
Deep neural networks have achieved remarkable success in a series of computer vision tasks, such as image recogni-tion [4,12,25], video analysis [37,49], object detection [35],
*Indicates equal contribution.
†Jingjing-Chen is the corresponding author.
Figure 1.
Illustration of Balanced Contrastive Learning. Head classes dominant the training procedure of SCL and compress the representation space of tail classes on the hypersphere (denoted by
⋆). BCL learns an embedding space that treats all classes equally and forms a regular simplex (denoted by •). etc. These achievements are owing largely to the availabil-ity of large-scale dataset such as ImageNet [11], where each class has sufficient and equal amount of training samples.
However, real-world datasets are often imbalanced, where many classes have only a few samples and few classes have a great number of samples. Deep models trained with such unbalanced data usually generalize badly on balanced test-ing data, especially for rare classes. Improving recognition performance with unbalanced data poses a huge challenge to modern deep learning methods.
To tackle the problem of learning with imbalanced data, early methods mainly focus on re-sampling the training data
[1, 2, 14, 33] or re-weighting the loss functions [9, 21, 45] to pay more attention to rare classes. Recently, diverse methods have emerged. For example, Logit compensa-tion methods [3, 31, 38] calibrate distribution between the training data and the test data. Decoupling [23] adopts a two-stage training scheme where the classifier is re-balanced in the second stage. The work in [44] has multiple distribution-aware experts for responding to samples of dif-ferent class frequencies. Nevertheless, contrastive learning approaches are less explored before, not until contrastive learning [8, 22, 42] are introduced. We attach great im-portance to representation learning because it’s the most re-markable capability of deep models.
In this paper, we focus on using supervised contrastive learning (SCL) [24] to assist representation learning. Su-pervised contrastive loss has achieved better performance than supervised cross-entropy loss on large-scale classifi-cation problems. The work in [16] then has explained in detail the reason for the excellent performance of SCL on the balanced datasets. Despite the great success, some re-cent work [8, 22] indicate that high-frequency classes dom-inate SCL for representing imbalanced data, which results in unsatisfactory performance across all classes. To analyze the optimizing behavior of SCL in learning the representa-tions for long-tailed data, we depict the geometric arrange-ment of representations of training instances when the lower bound of loss is achieved. Specifically, we decouple the lower bound of loss by deriving two competing dynamics: an attraction term and a repulsion term as in [16]. We reveal that the long-tailed distribution mainly affects the repulsion term. At the minimal supervised contrastive loss, the rep-resentations of classes of long-tailed data no longer attain a regular simplex configuration. In other words, when all in-stances with the same label collapse to points, these points are not equidistant from each other. A regular simplex con-figuration empirically confers important benefits, such as better generalization performance [32]. Besides, it has been proved to be the target geometric configuration of SCL on balanced data [16], hence forming a regular simplex config-uration will benefit recognition on long-tailed data [15].
Inspired by our analysis, we urge the model learning on imbalanced data to form a regular simplex, and propose a balanced contrastive learning (BCL) method (illustrated in
Fig. 1). We have two modifications in BCL that distinguish it from SCL. First, class-complement introduces the class-center embeddings, i.e., prototypes, as instances for com-parison in every mini-batch. Second, class-averaging has the gradient contributions of all negatives of each class av-eraged for every mini-batch. Through these two improve-ments, BCL ensures that the overall lower bound of the loss is a class-independent constant, and alleviates the imbal-ance problem of SCL when representing long-tailed data.
Furthermore, We adopt a cross-entropy loss with logit com-pensation to obtain a balanced classifier. Logit compensa-tion can effectively alleviate overlooking tail classes in the classifier learning [3,20,31,34]. Overall, we propose a two-branch framework to implement the mentioned techniques, i.e., a contrastive learning branch with BCL and a classifi-cation branch with logit compensated cross-entropy.
Our main contributions are as follows:
• We present a theoretical analysis showing that super-vised contrastive learning forms an undesired asym-metric geometry configuration for long-tailed data due to the overwhelming numerical dominance of the head classes.
• Motivated by our analysis, we extend supervised contrastive learning to balanced contrastive learning, which overcomes the imbalance problem and remains a regular simplex configuration of long-tailed data.
• The proposed two-branch framework combines the classification module and the balanced contrastive learning module, achieving competitive results on sev-eral popular long-tailed datasets. 2.