Abstract
Concurrent perception datasets for autonomous driving are mainly limited to frontal view with sensors mounted on the vehicle. None of them is designed for the over-looked roadside perception tasks. On the other hand, the data captured from roadside cameras have strengths over frontal-view data, which is believed to facilitate a safer and more intelligent autonomous driving system. To ac-celerate the progress of roadside perception, we present the first high-diversity challenging Roadside Perception 3D dataset- Rope3D from a novel view. The dataset consists of 50k images and over 1.5M 3D objects in various scenes, which are captured under different settings including var-ious cameras with ambiguous mounting positions, cam-era specifications, viewpoints, and different environmental conditions. We conduct strict 2D-3D joint annotation and comprehensive data analysis, as well as set up a new 3D roadside perception benchmark with metrics and evalua-tion devkit. Furthermore, we tailor the existing frontal-view monocular 3D object detection approaches and propose to leverage the geometry constraint to solve the inherent ambi-guities caused by various sensors, viewpoints. Our dataset is available on https://thudair.baai.ac.cn/rope. 1.

Introduction
Autonomous driving plays a crucial role in helping re-duce traffic accidents and improve transportation efficiency.
Current perceptual systems mainly equip the moving vehi-cle with LiDAR or camera sensors. Owing to the move-ment, the vehicle perceptual system can not observe sur-roundings for a long period. In addition, since the mounted sensor is relatively low (usually on the top of a vehicle), the perceptual range is comparatively limited and is vulnerable to occlusion. On the contrary, the data captured from road-*indicates equal contribution
†Xiao Tan (tanxchong@gmail.com is the corresponding author.)
Figure 1. The comparison of (a) frontal view and (b) roadside camera view with a pitch angle. The car view focuses more on the frontal area whereas the roadside camera observes the scene in a long-term and large-range manner. Vehicles can be easily occluded by closer objects in frontal view but the roadside view alleviates the risk. For example, for car-view (a), the white van is occluded by the black jeep whereas in roadside view (b) they are both visible, corresponding to the white and pink 3D boxes in (c).
The triangle mark denotes the same LiDAR-mounted vehicle. side cameras has its inherent strengths in terms of robust-ness to occlusion and long-time event prediction, since they are collected from cameras mounted on poles a few meters above the ground. The comparisons between two different views of data are depicted in Fig. 1.
The importance of roadside perception is listed as fol-lows: (1) Cooperative to Autonomous driving (AD). AD still faces safety challenges and uncontrolled threats due to blind spots. Instead, the roadside view can cover the blind spots for two extra advantages over car views: a long-range global perspective to extend vehicles’ perception field spa-tially and temporally and global trajectory prediction for
safety. For example, a pedestrian walking behind a parked vehicle might suddenly crash into a moving vehicle since vehicle sensors fail to detect abrupt changes in the environ-ment owing to the limited perceptual range or heavy occlu-sion. On the contrary, the roadside view is capable of behav-ior prediction timely. (2) Global perception. Further objects are occluded (even with 360◦ sensors) by closer objects in existing car-view datasets, causing blind spots. Thanks to roadside cameras mounted overhead, the invisible region is now visible. Besides, Autonomous vehicles (AV) can be in-formed to choose a faster lane when having a dead car in the queue since the roadside view perceives globally. (3) Cost-efficient. In terms of cost, it is worthy for ensuring safety by cooperative perception and cost-efficient since informa-tion from roadside cameras can broadcast to all surrounding
AVs. (4) Intelligent traffic control. The roadside perception also facilitates smart traffic control and flow management.
The critical contribution of roadside perceptual systems in facilitating a safer and more intelligent autonomous driving system has been acknowledged in many works [8, 34, 41].
However, existing researches on the roadside percep-tual ability focus only on 2D tasks such as 2D detection and tracking, the ability of 3D localization is still under-explored [29, 30, 39]. In this work, we focus on monocular 3D detection that localizes objects in 3D space from a single image. Although abundant perception datasets have been published to fuel the development in autonomous driving from vehicle view, such as KITTI [12], nuScenes [5], A*3D
[32] and Waymo [38], none of them is designed particu-larly for the overlooked roadside 3D perception task. We hence release the first large-scale high-diversity Roadside
Perception Dataset (Rope3D), with the hope of bridging this gap. Compared with the existing vehicle view datasets, the roadside perceptual data can be different in three ways.
First, the ambiguity lies everywhere due to various cameras specifications such as distinct pitch angles of the viewpoint, mounting heights as well as various roadside environments, which increases the difficulty of monocular 3D detection tasks to a great extent. Second, since the roadside cameras are mounted on the poles instead of on top of the vehicle in frontal view, thus the assumption of the camera’s optical axis being parallel to the ground is no more valid, leading to the incompatibility of directly applying the existing monoc-ular 3D detection approaches using this prior. Third, due to a much larger sensible range of the roadside perceptual system, a larger number of objects are expected to observe in roadside view, increasing the density and difficulty of a perceptual system. All these differences prevent directly ap-plying most existing 3D detection methods. We hence tai-lor existing monocular 3D object detection methods to the roadside application.
To summarize, our contributions are as follows:
• We present the first challenging high-diversity road-side dataset termed “Rope3D”, consisting of 50k im-ages and over 1.5M 3D objects collected across a va-riety of lighting conditions (daytime / night / dusk), different weather conditions (rainy / sunny / cloudy), and distinct road scenes with different camera specifi-cations like focal length and viewpoints.
• We specially tailor current frontal-view monocular 3D detection methods to deal with the roadside view data and conduct a comprehensive study with the new 3D detection metrics particularly designed for roadside 3D detection tasks, hoping to facilitate the development of monocular 3D perception tasks in roadside scenarios. 2.