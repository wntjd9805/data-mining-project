Abstract
Modern image generative models show remarkable sam-ple quality when trained on a single domain or class of objects. In this work, we introduce a generative adversar-ial network that can simultaneously generate aligned im-age samples from multiple related domains. We leverage the fact that a variety of object classes share common at-tributes, with certain geometric differences. We propose
Polymorphic-GAN which learns shared features across all domains and a per-domain morph layer to morph shared features according to each domain.
In contrast to previ-ous works, our framework allows simultaneous modelling of images with highly varying geometries, such as images of human faces, painted and artistic faces, as well as mul-tiple different animal faces. We demonstrate that our model produces aligned samples for all domains and show how it can be used for applications such as segmentation trans-fer and cross-domain image editing, as well as training in low-data regimes. Additionally, we apply our Polymorphic-GAN on image-to-image translation tasks and show that we can greatly surpass previous approaches in cases where the geometric differences between domains are large. 1.

Introduction
Generative adversarial networks (GANs) have achieved remarkable image synthesis quality [5, 11, 27, 28]. More-over, GANs like StyleGAN [27, 28] have been shown to form a semantic understanding of the modeled images in their features [3, 4, 14, 21, 24, 34, 54, 59, 61, 66, 70], which has been leveraged in diverse applications, including image editing [9,19,30,31,36,58,73], inverse rendering [68], style transfer [1,29], image-to-image translation [7,8,22,49], and semi-supervised learning [34, 66, 70].
GANs are usually trained on images from individual do-mains, such as human faces [25, 27]. However, there are many related domains which share similar semantics and characteristics, such as animal faces or face paintings. In our work, we aim to train a generative model with a shared backbone to produce aligned samples from multiple related domains. By aligned, we mean images that share common attributes and conditions across domains, such as pose and lighting. This has an obvious computational advantage by sharing weights across domains, but more importantly, it affords a variety of applications such as transferring seg-mentation labels from one domain to another, in which such information may not be available. Furthermore, by editing one domain, we get edits in other domains for free.
The main obstacle to building a GAN that simultane-ously synthesizes outputs from different domains is that even though the semantics are often shared, the geometry can vary significantly (consider, for example, the face of a human, a dog, and a cat). This prevents a natural shar-ing of generator features among such semantically aligned, but geometrically varying domains. Common approaches such as fine-tuning a pre-trained GAN [26, 43, 64] unfortu-nately lose the ability to sample from the parent domain, or, more generally, multiple domains at the same time. Learn-ing shared representations between multiple domains has been studied in the transfer and multi-task learning liter-ature [10, 39, 40, 72], but there has been little progress in generative models [2, 8, 20, 37, 38].
To overcome these challenges, we propose Polymorphic-GAN (PMGAN). It leverages a shared generator network together with novel morph maps that geometrically deform and adapt the synthesis network’s feature maps. In particu-lar, PMGAN builds on the StyleGAN2 [28] architecture and augments the model with a MorphNet that predicts domain-specific morph maps which warp the main generator’s fea-tures according to the different domains’ geometries. An additional shallow convolutional neural network is then suf-ficient to render these morphed features into correctly styl-ized and geometrically aligned outputs that are also seman-tically consistent across multiple domains.
By sharing as many generator layers as possible, the im-pressive semantic properties of StyleGAN’s latent space are shared across all modeled domains, while the geometric dif-ferences are still correctly reflected due to the additional morph operations. Because of that, our PMGAN enables many relevant applications in a unique and novel way. We extensively analyze PMGAN and validate the method on (i) We perform previously impossi-the following tasks: ble expressive image editing across different domains. (ii)
We use PMGAN for image-to-image translation across do-mains, and outperform previous methods in cases where the geometric gap between domains is large. (iii) We leverage
PMGAN’s learnt morph maps for zero-shot semantic seg-mentation transfer across domains. (iv) Finally, sharing the generator’s features across domains is advantageous when involving domains with little training data. In these cases the main generator can be learnt primarily from a domain with much data, which benefits all other domains. In sum-mary, our PMGAN is the first generative model that natu-rally and easily allows users to synthesize aligned samples from multiple semantically-related domains at the same time, enabling novel and promising applications. 2.