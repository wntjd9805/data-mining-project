Abstract
Recent techniques have been successful in reconstructing surfaces as level sets of learned functions (such as signed dis-tance ﬁelds) parameterized by deep neural networks. Many of these methods, however, learn only closed surfaces and are unable to reconstruct shapes with boundary curves. We propose a hybrid shape representation that combines explicit boundary curves with implicit learned interiors. Using ma-chinery from geometric measure theory, we parameterize currents using deep networks and use stochastic gradient descent to solve a minimal surface problem. By modifying the metric according to target geometry coming, e.g., from a mesh or point cloud, we can use this approach to repre-sent arbitrary surfaces, learning implicitly deﬁned shapes with explicitly deﬁned boundary curves. We further demon-strate learning families of shapes jointly parameterized by boundary curves and latent codes. 1.

Introduction
Shape representation is a crucial component of geometry processing and learning algorithms. Depending on the target application, different representations have varying tradeoffs.
Broadly, shape representations fall naturally into two classes:
Lagrangian or explicit; Eulerian or implicit. In this work, we show how to use the theory of currents from geometric measure theory to design a ﬂexible neural representation that combines favorable aspects from each category, represent-ing the interiors of surfaces implicitly while maintaining an explicit representation of their boundaries.
Lagrangian representations encode a shape by giving co-ordinates of points or parameterizing regions of the shape.
To represent a curve in a Lagrangian way, one might give co-ordinates of successive points along the curve. Analogously, to represent a surface in 3D, one might use a mesh, which assembles the surface out of simple patches. Lagrangian rep-resentations afford great precision but require predetermined combinatorial structures, making it difﬁcult to represent fam-ilies of shapes with varying topology.
*Authors contributed equally to this work.
In contrast, Eulerian representations encode a shape via a function on some background domain. For example, a surface might be encoded as the level set of a scalar function sampled on a regular grid. Level sets of signed distance
ﬁelds (SDFs) form one popular implicit representation. Im-plicit functions naturally capture topological variation, but traditional implicit shape representations, in which the back-ground geometry must be discretized with a ﬁxed grid or mesh, waste resolution on regions far away from the level set of interest. Recent neural implicit representations alleviate this problem [7, 38, 46]. The universal approximation and differentiability properties of neural networks make them an appealing alternative to regular grid discretizations.
Neural implicit representations come with their own lim-itations. Like other implicit representations based on level sets, most neural implicit representations can only encode closed surfaces, which lack boundary curves. Boundaries are desirable as they can provide manipulation handles for con-trollable deformation, and common boundaries can be used to stitch together surfaces into a larger articulated surface.
In this paper, we describe a new way to encode neural implicit surfaces with boundaries, which can then be com-bined into more complex hybrid surfaces. The key to our representation is the theory of currents from geometric mea-In this theory, k-dimensional submanifolds sure theory. are deﬁned by their integration against differential k-forms, generalizing how distributions (0-currents) are deﬁned by integration against smooth functions. Current spaces are complete normed linear spaces that make optimization over surfaces convenient, and the boundary operator also becomes linear on these spaces. Classically, currents were the key to solving Plateau’s minimal surface problem by transforming it into mass norm minimization. We adopt the mass norm as the primary loss function encouraging our neural currents to converge to smooth surfaces.
We demonstrate our representation with three applica-tions. We ﬁrst demonstrate how it enables computing mini-mal surfaces efﬁciently through stochastic gradient descent.
Then, by modifying the background metric used to deﬁne the mass norm, we reconstruct arbitrary surfaces from data. Fi-nally, we demonstrate the ﬂexibility of our representation by encoding families of surfaces with explicit boundary control.
Contributions.
In summary, we
• propose a new neural implicit surface representation with explicit boundary curves;
• show how to use SGD on the mass norm to compute mini-mal surfaces;
• introduce a custom background metric and additional loss terms to represent surfaces from data; and
• describe a framework for learning families of surfaces parameterized by their boundaries along with a latent code. 2.