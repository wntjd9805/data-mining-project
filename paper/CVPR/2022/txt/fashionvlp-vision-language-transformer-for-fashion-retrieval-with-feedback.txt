Abstract
Fashion image retrieval based on a query pair of refer-ence image and natural language feedback is a challenging task that requires models to assess fashion related informa-tion from visual and textual modalities simultaneously. We propose a new vision-language transformer based model,
FashionVLP, that brings the prior knowledge contained in large image-text corpora to the domain of fashion image re-trieval, and combines visual information from multiple lev-els of context to effectively capture fashion-related informa-tion.While queries are encoded through the transformer lay-ers, our asymmetric design adopts a novel attention-based approach for fusing target image features without involving text or transformer layers in the process. Extensive results show that FashionVLP achieves the state-of-the-art perfor-mance on benchmark datasets, with a large 23% relative improvement on the challenging FashionIQ dataset, which contains complex natural language feedback. 1.

Introduction
The task of feedback-based fashion image retrieval in-volves fetching images of clothing items that match a cus-tomer’s needs and preferences. A customer starts with an initial request to search for a fashion item and participates in multiple turns of interaction with the conversational as-sistant until they get the result that they are satisfied with. A key challenge in this use-case is to retrieve a new candidate image based on both the previously retrieved image and the new feedback provided by the customer. Figure 1 shows examples of feedback-based fashion image retrieval.
Substantial progress [6, 19, 25, 49] has been made on this topic by designing strong image-text composers us-ing image and text features from separate neural networks.
Recently, Vision-Language Pre-trained (VLP) transformers
[8, 26, 28, 33, 44, 45, 54, 57, 59] have been shown to be capa-ble of learning joint representations for images and text di-*Equal contribution. Completed during Zhaoheng’s internship at Amazon. (a) Example of simple modification (b) Example of compound modification
Figure 1. Fashion image retrieval with textual feedback. The input query to the system includes a reference image and a comment specifying changes to be made to the image. The system retrieves fashion items with the desired changes accordingly. rectly by training on large-scale image-text corpora. In this work, we propose a VLP transformer-based model, Fash-ionVLP, for fashion image retrieval with textual feedback, which leverages prior knowledge from large corpora and image features from multiple fashion-related context levels.
Our model is composed of two parallel blocks – one for processing the reference image and the feedback, and an-other for processing target images. The reference block starts with extracting image features at multiple-levels of context: (1) whole image, (2) cropped image of clothing, (3) regions around fashion landmarks [32], and (4) regions of interest determined by a pretrained object detector. These features along with object tags from the detector and word tokens from the textual feedback are then fed into a multi-layer transformer model to compute a final joint represen-tation for reference. On the target side, features at contexts (1)–(3) are computed using only image feature extractors for efficient low-cost inference, and fused using a contex-tual attention mechanism instead of transformer layers to generate a target encoding for each candidate image. The model is trained using cosine similarity and a batch-based classification loss where the target for each reference im-age is used as a negative sample for other reference images.
Finally, retrieval is performed by ranking candidates using cosine similarities between reference and target encodings.
We evaluate FashionVLP on three common fashion im-age retrieval datasets: FashionIQ [51], Shoes [3] and Fash-ion200K [17]. Unlike other datasets, FashionIQ contains real human comments on specific reference-target image pairs and is hence much more challenging for the fashion image retrieval task. Results show that FashionVLP im-proves the performance on FashionIQ by a significant rel-ative performance gain of 23%. This validates the capa-bility of our framework in dealing with complicated real-life image-feedback pairs when conducting fashion image retrieval. Our model also surpasses the state-of-the-art on
Shoes and Fashion200K datasets.
Our work makes the following contributions:
• We propose a new transformer-based model that lever-ages prior knowledge from large image-text corpora for fashion image retrieval with textual feedback.
• We provide a way for effectively incorporating multiple levels of fashion-related visual context for both reference and candidate images within our asymmetric design.
• Our model outperforms previous works on benchmark datasets, with 23% relative gain on FashionIQ. 2.