Abstract
We apply style transfer on mesh reconstructions of in-door scenes. This enables VR applications like experienc-ing 3D environments painted in the style of a favorite artist.
Style transfer typically operates on 2D images, making styl-ization of a mesh challenging. When optimized over a vari-ety of poses, stylization patterns become stretched out and inconsistent in size. On the other hand, model-based 3D style transfer methods exist that allow stylization from a sparse set of images, but they require a network at infer-ence time. To this end, we optimize an explicit texture for the reconstructed mesh of a scene and stylize it jointly from all available input images. Our depth- and angle-aware op-timization leverages surface normal and depth data of the underlying mesh to create a uniform and consistent styliza-tion for the whole scene. Our experiments show that our method creates sharp and detailed results for the complete scene without view-dependent artifacts. Through extensive ablation studies, we show that the proposed 3D awareness enables style transfer to be applied to the 3D domain of a mesh. Our method 1 can be used to render a stylized mesh in real-time with traditional rendering pipelines. 1.

Introduction
Creating 3D content from RGB-D scans is a popular topic in computer vision [1, 12, 29, 43, 44]. We tackle a 1https://lukashoel.github.io/stylemesh/ novel use case in this area: stylization of a reconstructed mesh with an explicit RGB texture. Neural Style Trans-fer (NST) shows great results for stylization of images or videos, but stylization of 3D content like meshes has been underexplored. We synthesize a texture for the mesh which is a combination of observed RGB colors and a painting’s artistic style. After stylization, one could explore the space in VR and see it painted in the style of Van Gogh.
Our use case is similar to prior texture mapping meth-ods [2,16,26,27,29,53,57] which construct a texture from a set of posed RGB images, but we produce a stylized texture rather than directly matching input images. This is difficult since style transfer losses are typically defined on 2D im-age features [20], so NST does not immediately generalize to 3D meshes. Recently, style transfer has been combined with novel view synthesis to stylize arbitrary scenes with a neural renderer from a sparse set of input images [7,25,35].
These model-based methods require a forward pass during inference and cannot directly be applied to meshes. Kato et al. [32] and Mordvintsev et al. [42] use differentiable ren-dering to bridge the gap between image style transfer and texture mapping: backpropagating image losses to a texture representation enables consistent mesh stylization.
However, applying these methods to room-scale geom-etry is challenging as the resulting stylization patterns are noisy and can contain view-dependent stretch and size arti-facts. For example, optimizing a surface from a small graz-ing angle creates patterns in the image plane for that pose.
Viewing the same surface from an orthogonal angle then
shows stretched-out patterns due to the perspective distor-tion. Similarly, seeing an object from close and far-away viewpoints mixes small and large patterns on the same sur-face. Perceiving the depth thus becomes harder, due to in-consistent stylization sizes. These issues arise because 2D style transfer losses do not incorporate 3D data like surface normals and depth. Instead, textures are separately stylized in each pose’s image plane.
To this end, we formulate an energy minimization prob-lem over the texture that combines texture mapping with style transfer (similar to [42]) and minimizes style transfer losses for each pose in a 3D-aware manner that avoids view-dependent artifacts. First, we utilize depth to render im-age patches at increasingly larger screen-space resolutions.
By splitting the style loss calculation over these patches, we create larger stylization patterns in the foreground than the background. As a result, patterns have the same size in world-space and are optimized in a view-independent way. Second, we use the angle between the surface normal and view direction to determine the degree of stylization for each pixel. By calculating Gram matrices from differ-ent style image resolutions (similar to [39]) areas seen from small grazing angles are stylized with coarse details, which are later refined if they are seen from better angles. Third, we avoid discretization artifacts by scaling gradients with per-pixel angle and depth weights during backpropagation.
Compared to state-of-the-art 3D style transfer methods, our experiments show an improvement in terms of 3D con-sistent stylization both qualitatively and quantitatively. Ad-ditionally, our explicit texture representation allows for di-rect usage with traditional rendering pipelines.
To summarize, our contributions are:
• Style transfer for room-scale indoor scene meshes with a new texture optimization, which results in 3D consis-tent textures and mitigates view-dependent artifacts.
• A depth-aware optimization at different screen-space resolutions, that creates equally-sized stylization pat-terns in the world-space of the mesh.
• An angle-aware optimization at different stylization details, that creates unstretched stylization patterns in the world-space of the mesh. inaccuracies in pose, geometry, color and distortions to find the best texture for the scene. In contrast, we aim to create a texture that is also styled to a specific image and avoid view-dependent stylization artifacts by introducing depth-and angle-awareness into the optimization.
Image Style Transfer. NST, first introduced in Gatys et al. [20], can be optimization-based [8, 20, 21] or model-based [15, 28, 30, 52]. It is inherently defined in the image domain by matching CNN features either globally or in a local, patch-based manner [20, 31, 34, 36, 41]. Thus, it can-not directly utilize 3D data like depth or surface normals of a mesh. This can lead to view-dependent stylization arti-facts when optimizing a texture through multiple poses. We induce 3D-awareness into optimization-based NST by split-ting the loss calculation across different image segments.
Video Style Transfer. Video style transfer (VST) methods consistently stylize RGB video frames with a given style.
These methods are optimization-based [46, 47] or model-based [5, 6, 18, 19, 22, 54, 55] and employ temporal consis-tency or optical flow constraints. Other methods combine features in a temporally consistent way, without using opti-cal flow or depth constraints directly [15,37]. VST methods can be combined with texture mapping to achieve consistent stylization of indoor scenes. However, since VST optimiza-tions are unaware of the underlying 3D structures, the re-sulting textures are often blurry or low-detail. 3D Style Transfer. Lifting style transfer into 3D has been explored for texturing individual objects [32, 42, 56] or faces [23]. However, they focus on isolated objects (not room-scale scenes) and do not utilize 3D data. In contrast, our method stylizes complete indoor scenes in a 3D-aware way. Another line of work applies exemplar-based NST to 3D models [24, 49], guiding the stylization process explic-itly from (hand-crafted) examples. In contrast, we follow original NST by stylizing 3D scene models from artistic paintings and camera images. Cao et al. [3] stylize indoor scenes using a point cloud that cannot be directly used to texture a mesh. Other methods combine novel view synthe-sis and NST for consistent stylization from only a few input images [7, 25, 35]. In contrast, we do not require a network during inference to produce stylization results; our results can be rendered by a standard graphics pipeline. 2.