Abstract
Video-and-language pre-training has shown promising improvements on various downstream tasks. Most previous methods capture cross-modal interactions with a standard transformer-based multimodal encoder, not fully address-ing the misalignment between unimodal video and text fea-tures. Besides, learning fine-grained visual-language align-ment usually requires off-the-shelf object detectors to pro-vide object information, which is bottlenecked by the detec-tor’s limited vocabulary and expensive computation cost.
In this paper, we propose Align and Prompt: a new video-and-language pre-training framework (ALPRO), which operates on sparsely-sampled video frames and achieves more effective cross-modal alignment without ex-plicit object detectors. First, we introduce a video-text con-trastive (VTC) loss to align unimodal video-text features at the instance level, which eases the modeling of cross-modal interactions. Then, we propose a novel visually-grounded pre-training task, prompting entity modeling (PEM), which learns fine-grained alignment between visual region and text entity via an entity prompter module in a self-supervised way. Finally, we pretrain the video-and-language trans-former models on large webly-source video-text pairs using the proposed VTC and PEM losses as well as two standard losses of masked language modeling (MLM) and video-text matching (VTM). The resulting pre-trained model achieves state-of-the-art performance on both text-video retrieval and videoQA, outperforming prior work by a substantial margin. Implementation and pre-trained models are avail-able at https://github.com/salesforce/ALPRO. 1.

Introduction representations that
Video-and-language pre-training aims to jointly learn multimodal transfer effectively to downstream tasks, such as text-video retrieval and videoQA-video question answering. Compared with images, videos usually contain more redundancy in consecutive frames.
This challenges models on both capacity and computation efficiency. Most prior approaches [29, 34, 36, 38, 47, 56] circumvent the expensive computation overhead by using offline-extracted video features. Since the video feature ex-Figure 1. Generating supervision for region-entity alignment.
Above: previous methods (e.g. ActBERT [56]) rely on object de-tectors with expensive computation cost and limited object cat-egories, leaving text data unexploited. Below: ALPRO generates soft entity labels with a prompter module, which computes similar-ities between video crops and textual entity prompts. ALPRO re-quires no detector while taking advantage of video-text alignment to generate entity labels with a large vocabulary, thus strengthen-ing the cross-modal learning. tractors are fixed without finetuning, these approaches are suboptimal when transferring to distinct target domains. In contrast, recent emerging approaches [3, 25] sample frames sparsely from videos, which enable end-to-end pre-training and finetuning of video backbones.
In this work, we adopt the sparse video-text pre-training paradigm consid-ering their effectiveness on downstream tasks.
Despite their promising performance, current video-text pre-training models have several limitations. (1) The inter-action between video and text features is commonly mod-eled trivially using either dot-product [3,36,38,51] or cross-modal transformer encoders [25, 29, 47, 56]. However, fea-tures from individual modalities typically reside in different embedding spaces. Such misalignment makes it less effec-(2) Many tive to directly model cross-modal interaction. visually-grounded pre-training tasks [29, 47] do not explic-itly model fine-grained regional visual information (e.g. ob-jects), which proves important for downstream tasks em-phasizing on visual reasoning (e.g. videoQA). Although there are attempts which employ object detectors [7, 56] to generate pseudo-labels as supervision, they suffer from im-precise detections and a restricted number of object cate-gories. For example, detectors trained on MSCOCO [30] (3) recognize less than a hundred different categories.
The previous sparse pre-training model [25] is trained with image-text pairs using an image encoder, which makes it less effective in modeling temporal information.
In this paper, we tackle these challenges with a new video-and-language pre-training framework: Align and
Prompt (ALPRO). Architecture-wise, ALPRO first encodes frames and text independently using a transformer-based video encoder and a text encoder, and then employs a mul-timodal encoder to capture cross-modal interaction. AL-PRO learns both instance-level video-text alignment and fine-grained region-entity alignment. The instance-level alignment is learned by applying a video-text contrastive loss (VTC) on the unimodal features, which encourages paired video-text instances to have similar representations.
In order to better capture fine-grained visual information and strengthen region-entity alignment, ALPRO introduces a new visually-grounded pre-training task, called prompt-ing entity modeling, where we ask the video-text model to predict entities appearing in randomly-selected video crops using jointly video and text inputs (see Figure 1). To ad-dress the unavailability of entity annotations, we design a standalone entity prompter module that generates reliable pseudo-labels. Specifically, the entity prompter consists of two unimodal encoders to extract video and text features, respectively. We first train the entity prompter using only
VTC loss and freeze its parameters thereafter. Then during pre-training, we feed video crops and text prompts (e.g. “A video of {Entity}.”) to the prompter, where each
Entity is from the frequent nouns appearing in the pre-training corpus. We then compute the normalized similar-ity between the entity prompts and the video crop as the pseudo-label to supervise the pre-training.
Our key contributions are: (1) We introduce ALPRO, the first generic video-language pre-training method that learns effective cross-modal representations from sparse video frames and texts. (2) We introduce a video-text con-trastive loss to better align instance-level unimodal rep-resentations, thus easing the modeling of cross-modal in-teraction. (3) We propose a novel visually-grounded pre-training task, prompting entity modeling, that enables the model to capture fine-grained region-entity alignment. (4)
We demonstrate the effectiveness of ALPRO on both video-text retrieval and videoQA. ALPRO significantly improves over previous state-of-the-art methods, for example, achiev-ing 3.0% and 5.4% absolute lift in recall scores on the fine-tuning and zero-shot text-video retrieval task on MSRVTT. 2.