Abstract 1.

Introduction
Temporal alignment of fine-grained human actions in videos is important for numerous applications in computer vision, robotics, and mixed reality. State-of-the-art methods directly learn image-based embedding space by leveraging powerful deep convolutional neural networks. While being straightforward, their results are far from satisfactory, the aligned videos exhibit severe temporal discontinuity without additional post-processing steps. The recent advancements in human body and hand pose estimation in the wild promise new ways of addressing the task of human action alignment in videos.
In this work, based on off-the-shelf human pose estimators, we propose a novel context-aware self-supervised learning architecture to align sequences of actions. We name it CASA. Specifically, CASA employs self-attention and cross-attention mechanisms to incorporate the spatial and temporal context of human actions, which can solve the temporal dis-continuity problem. Moreover, we introduce a self-supervised learning scheme that is empowered by novel 4D augmentation techniques for 3D skeleton representations. We systematically evaluate the key components of our method. Our experiments on three public datasets demonstrate CASA significantly im-proves phase progress and Kendall’s Tau scores over the pre-vious state-of-the-art methods.
Temporal alignment of human activities in videos aims to identify sequential per-frame correspondence between two video instances of the same action as shown in Fig. 1. This is challenging due to large variation in speed of actions, se-vere self-occlusion, and diverse backgrounds across differ-ent videos. Furthermore, an accurate temporal alignment of human activities requires semantic understanding of human motion and causal reasoning of the action stages. When it comes to hand-centric fine-grained activities under first-person views, the challenges are amplified by the varying viewpoints and embodied movement of camera wearers. State-of-the-art methods leverage large-scale datasets and powerful deep con-volution neural networks to learn image-based representation to perform temporal video alignment [16, 22]. Despite rapid progress in terms of accuracy and advanced learning schemes, the results are still far from applicable to real-world applica-tions.
Recent advancements and growing availability of head-mounted devices (e.g. Microsoft HoloLens [56]) enable new ways of communication and collaboration. For instance, the built-in hand tracking system of HoloLens provides real-time accurate hand pose estimation of the camera wearer. Such sys-tems promise a revolution in how hand motion and actions can be captured, modeled, and analyzed. Consequently, they point towards a new way to align fine-grained hand-centric actions in videos based on 3D skeleton motion extracted from off-the-the best performance in most phase classification tasks of three datasets. Furthermore, in terms of phase progress and
Kendall’s tau, our method significantly outperforms the previ-ous state-of-the-art methods [16, 22]. The results demonstrate the importance of knowing the context of action and the appli-cability of utilizing 3D poses for fine-grained video alignment tasks.
In summary, our contributions are:
Contributions. (1) we propose a novel attention-based and context-aware dense alignment framework for fine-grained human action analysis; (2) we introduce novel 4D augmentation strategies for 3D skeletons in self-supervised learning that consider both tem-poral and spatial augmentation; (3) to the best of our knowl-edge, it is the first work to perform 3D skeleton-based fine-grained video alignment using self-supervised learning. We prove the utility of our 3D skeleton-based temporal alignment methods by largely outperforming the state-of-the-art in three public datasets. 2.