Abstract
Existing point cloud segmentation methods require a large amount of annotated data, especially for the outdoor point cloud scene. Due to the complexity of the outdoor 3D scenes, manual annotations on the outdoor point cloud scene are time-consuming and expensive.
In this paper, we study how to achieve scene understanding with limited annotated data. Treating 100 consecutive frames as a se-quence, we divide the whole dataset into a series of se-quences and annotate only 0.1% points in the first frame of each sequence to reduce the annotation requirements. This leads to a total annotation budget of 0.001%. We propose a novel temporal-spatial framework for effective weakly su-pervised learning to generate high-quality pseudo labels from these limited annotated data. Specifically, the frame-work contains two modules: an matching module in tem-poral dimension to propagate pseudo labels across differ-ent frames, and a graph propagation module in spatial di-mension to propagate the information of pseudo labels to the entire point clouds in each frame. With only 0.001% annotations for training, experimental results on both Se-manticKITTI and SemanticPOSS shows our weakly super-vised two-stage framework is comparable to some existing fully supervised methods. We also evaluate our framework with 0.005% initial annotations on SemanticKITTI, and achieve a result close to fully supervised backbone model. 1.

Introduction
Recently, outdoor 3D semantic segmentation is attract-ing more research attention since the introduce of sev-eral large datasets, e.g., SemanticKITTI [1] and Semantic-POSS [19]. The outdoor 3D point cloud dataset organises the data as several sequences of point clouds, i.e. 4D point cloud. Then, multiple scans in point cloud sequences are
*Corresponding author: G. Lin. (e-mail: gslin@ntu.edu.sg) (a) Full annotation. (b) Unsupervised super-voxel segmentation. (c) Partially annotated super-voxel level labels.
Figure 1. An example of super-voxel segmentation and our weak annotation. Super-voxel segmentation segments the whole point cloud scan into several small units, each containing points within the same class. Therefore, we assign the point level initial annotation to all the points in the same super-voxel. superimposed together and divided as small tiles to reduce manual annotation costs. However, the annotation cost on the small tiles is still high. In SemanticKITTI [1], the anno-tation on one 100m × 100m tile of highway scene requires an average of 1.5 hours, and the annotation on one tile of more complex scenes requires an average of 4.5 hours.
The whole annotation task on SmeanticKITTI requires over 1700 hours. Therefore, the research on accelerating the an-notation process is valuable and desirable. We here resort to weakly supervised learning to tackle this annotation issue.
For indoor 3D point cloud scenes, there are several weakly supervised methods [18,27,29,33] proposed for ac-celerating the annotation process. MPRM [29] generates pseudo labels of an indoor 3D scene based on the 2D in-formation. Other approaches [18, 33] annotate a subset of the whole point cloud scene and update weak pseudo la-bels with the annotated points. For outdoor 3D point cloud scenes, there is no existing weakly supervised segmenta-tion methods available. Directly applying techniques devel-oped for indoor scenes to outdoor scenes can not perform well due to following reasons. Firstly, there is no colour in-formation in outdoor LiDAR point clouds, while methods designed for indoor scenes rely on the colour information to generate and smooth the pseudo labels. Secondly, a typi-cal outdoor point cloud scene contains about 100,000 points for a 150m × 150m area, which is much more sparse than an indoor point cloud scene. Thirdly, as a single outdoor 4D point cloud contains several corresponding point cloud scans, methods proposed for single point cloud scans in the indoor case require extra burdens to generate pseudo labels for each point cloud scan separately.
In this work, we propose a novel weakly supervised framework to reduce the annotation cost in the outdoor point cloud scenario. We exploit the temporal information among point cloud sequences and only annotate 0.1% points in one frame per 100-frame sequence in 4D point clouds.
However, training on a weakly labelled dataset with only 0.001% annotated points is unable to learn good features for achieving satisfiable performance. This problem can be concluded as the cold-start problem. To generate more su-pervision at minimum annotation cost, we apply an efficient super-voxel segmentation [17] on the dataset and assign the labels of annotated points to their belonged super-voxels.
Inspired by ScanNet [7] and OTOC [18], super-voxel seg-mentation segment a point cloud into several small groups, and the points in each group share a same semantic label.
We show an example of our annotations in Figure 1.
We then design two modules, i.e., temporal matching (TM) and spatial graph propagation (SGP), to propa-gate the annotations to the whole dataset. TM is designed to generate seeding points in different frames by tempo-ral propagation. For TM , we design two approaches with greedy matching and optimal transport matching. SGP fur-ther propagates the searched results to the whole point cloud scene in the spatial dimension.
Furthermore, we propose a two-stage training strategy, which consists of a seed point propagation stage and a dense scene propagation stage. Firstly, the seed point propagation stage propagates initial annotations only along the tempo-ral dimension with TM to generate high-quality pseudo la-bels under the cold-start scenario. We improve the feature quality by training a new segmentation model on the small amount of high-quality pseudo labels.
In the second stage, we use the new segmentation model from the previous stage to generate features, and based on the new features, we use a dense scene propagation strategy to combine TM and SGP to propagate the label informa-tion to the whole dataset. We continue training the model from the previous stage with more pseudo labels to improve the performance further. We evaluate our method on two outdoor segmentation datasets, i.e., SemanticKITTI [1] and
SemanticPOSS [19]. Experimental results show that our method achieves comparable performance with some fully supervised methods. We summarize the main contributions as follows:
• We propose a novel two-stage weakly supervised seg-mentation framework to exploit spatial and tempo-ral information across frames. The first stage (seed point propagation) generates seeding points in differ-ent frames based on weak annotations (0.001% anno-tated points). The second stage (dense scene propaga-tion) propagates high-confident points in both tempo-ral and spatial dimensions.
• We propose a temporal propagation module using tem-poral matching to propagate pseudo labels to differ-ent frames. There are two matching strategies, greedy matching and optimal transport matching to search the points from the annotated objects in different frames.
• We develop a spatial graph propagation module to propagate pseudo labels along spatial dimension in the dense scene propagation stage. Spatial graph propaga-tion generates dense pseudo labels to further improve the model.
• Experimental results on both SemanticKITTI and Se-manticPOSS show that our weakly supervised two-stage framework achieves on par performance with some existing fully supervised methods, while we only use 0.001% annotations for training. Furthermore, we evaluate our weakly supervised method with 0.005% initial annotations on SemanticKITTI, and performs close to our fully supervised backbone network. 2.