Abstract
Local density of point clouds is crucial for represent-ing local details, but has been overlooked by existing point cloud compression methods. To address this, we propose a novel deep point cloud compression method that preserves local density information. Our method works in an auto-encoder fashion: the encoder downsamples the points and learns point-wise features, while the decoder upsamples the points using these features. Specifically, we propose to encode local geometry and density with three embed-dings: density embedding, local position embedding and ancestor embedding. During the decoding, we explicitly predict the upsampling factor for each point, and the di-rections and scales of the upsampled points. To mitigate the clustered points issue in existing methods, we design a novel sub-point convolution layer, and an upsampling block with adaptive scale. Furthermore, our method can also compress point-wise attributes, such as normal. Extensive qualitative and quantitative results on SemanticKITTI and
ShapeNet demonstrate that our method achieves the state-of-the-art rate-distortion trade-off. 1.

Introduction
Point cloud is one of the most important and widely used 3D representation in many applications, such as au-tonomous driving, robotics and physics simulation [13].
With the rapid development of 3D scanning technology, complex geometry can now be effectively captured as large point clouds with fine details. As a consequence, point cloud compression becomes crucial for storage and trans-mission. Particularly, to achieve favorable compression ra-tio, the community has been focusing on lossy methods and pondering the key question: what properties of point clouds should be preserved, given limited bitrate budget?
Besides the global geometry, we argue that local density
âˆ—indicates equal contribution.
Yun He, Xinlin Ren and Xiangyang Xue are with the School of Com-puter Science, Fudan University.
Yanwei Fu is with the School of Data Science, Fudan University.
Figure 1. We argue that local density is an important character-istic of the point cloud and should be preserved during compres-sion. Existing methods that ignore the local density exhibit ar-tifacts such as uniform distribution (G-PCC [12]) and clustered points (Depoco [40]), resulting in worse reconstruction, especially when the bitrate is low. is an important characteristic and should be preserved as much as possible. Firstly, preserving density usually leads to less outliers, and thus smaller reconstruction error. Sec-ondly, point clouds captured in practice, e.g. from LiDAR, are rarely with uniformly distributed points. Losing local density means losing important traits such as scanning res-olution and occlusion. Thirdly, point clouds are often pro-cessed or simplified to be denser on regions of interest or with complex geometry, such as human face, hand, etc. Pre-serving density during compression means more budget is spent on these regions. Last but not the least, if the decom-pressed point cloud has significantly different density from the raw one, downstream applications such as semantic seg-mentation may be affected.
Mathematically, a point cloud can be considered as a set, often with different cardinality and permutation settings [8], which makes it difficult for image/video compression or conventional learning-based solutions that assume fixed di-mensional and ordered input. A typical strategy of exist-ing lossy methods is to voxelize the points before compres-sion [12, 27, 28, 37, 38]. While this allows leveraging con-ventional methods [7, 22], it obviously loses the local den-sity, and has a precision capped by the voxel size. Recent methods [17, 41] utilize PointNet [24] or PointNet++ [25] to ignore the cardinality and permutation with max pool-ing, and preserve density to some extent. However, the de-compressed point clouds always lose local details and suf-fer from clustered points issue, since most of the local ge-ometry has been discarded by max pooling. Depoco [40] adopts KPConv [35] to capture more local spatial informa-tion than pooling, but clustered points artifact still exists due to feature replication, see Fig 1. Alternatively, Zhao et al. [46] introduces attention mechanism to handle different cardinalities and permutations, though it is not designed for compression purpose.
In this paper, we propose a novel density-preserving deep point cloud compression method which yields superior rate-distortion trade-off to prior arts, and more importantly preserves the local density. Our method has an auto-encoder architecture, trained with an entropy encoder end-to-end.
The contributions of our paper are summarized as follows.
On the encoder side: three types of feature embeddings are designed to capture local geometry distribution and density.
On the decoder side: to mitigate the clustered points issue, we propose 1) the sub-point convolution to promote feature diversity during upsampling; 2) learnable number of upsam-pling points, and scale for their offsets in different regions.
We conduct extensive experiments and ablation studies to justify these contributions. Additionally, we demonstrate that our method can be easily extended to jointly compress attributes such as normal. 2.