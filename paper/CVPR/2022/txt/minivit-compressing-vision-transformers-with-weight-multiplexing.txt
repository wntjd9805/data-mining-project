Abstract
Vision Transformer (ViT) models have recently drawn much attention in computer vision due to their high model capability. However, ViT models suffer from huge num-ber of parameters, restricting their applicability on devices with limited memory. To alleviate this problem, we propose
MiniViT, a new compression framework, which achieves pa-rameter reduction in vision transformers while retaining the same performance. The central idea of MiniViT is to multi-plex the weights of consecutive transformer blocks. More specifically, we make the weights shared across layers, while imposing a transformation on the weights to increase diversity. Weight distillation over self-attention is also ap-plied to transfer knowledge from large-scale ViT models to weight-multiplexed compact models. Comprehensive exper-iments demonstrate the efficacy of MiniViT, showing that it can reduce the size of the pre-trained Swin-B transformer by 48%, while achieving an increase of 1.0% in Top-1 ac-curacy on ImageNet. Moreover, using a single-layer of pa-rameters, MiniViT is able to compress DeiT-B by 9.7 times from 86M to 9M parameters, without seriously compromis-ing the performance. Finally, we verify the transferabil-ity of MiniViT by reporting its performance on downstream benchmarks. Code and models are available at here. 1.

Introduction
“Only Mini Can Do It.”
— BMW Mini Cooper
Large-scale pre-trained vision transformers, such as
ViT [18], CvT [58], and Swin [36], have recently drawn a great deal of attention due to their high model capabilities and superior performance on downstream tasks. However, they generally involve giant model sizes and large amounts of pre-training data. For example, ViT uses 300 million images to train a huge model with 632 million parame-ters, achieving state-of-the-art performance on image clas-sification [18]. Meanwhile, the Swin transformer uses 200-300 million parameters, and is pre-trained on ImageNet-∗Equal contributions. Work done when Jinnian and Kan were interns of Microsoft. †Corresponding author.
Figure 1. Comparisons between MiniViTs and popular vision transformers, such as DeiT [52] and Swin Transformers [36] 22K [16], to attain promising results on downstream detec-tion and segmentation tasks [36].
Hundreds of millions of parameters consume consider-able storage and memory, making these models unsuitable for applications involving limited computational resources, such as edge and IoT devices, or in which real-time predic-tions are needed. Recent studies reveal that the large-scale pre-trained models are over-parametrized [30] . Therefore, it is necessary and feasible to eliminate redundant param-eters and the computational overhead of these pre-trained models without compromising their performance.
Weight sharing is a simple, but effective, technique to reduce model sizes. The original idea of weight sharing in neural networks was proposed in the 1990s by LeCun and Hinton [33,40], and recently reinvented for transformer model compression in natural language processing (NLP)
[32]. The most representative work, ALBERT [32], in-troduces a cross-layer parameter sharing method to pre-vent the number of parameters from growing with network depth. Such technique can significantly reduce the model size without seriously hurting performance, thus improving parameter efficiency. However, the efficacy of weight shar-ing in vision transformer compression is not well explored.
To examine this, we perform the cross-layer weight shar-ing [32] on DeiT-S [52] and Swin-B [52] transformers. Un-expectedly, this straightforward usage of weight sharing brings two severe issues: (1) Training instability. We ob-served that weight sharing across transformer layers makes the training become unstable, and even causes training col-lapse as the number of shared layers increases, as visualized in Fig. 4. (2) Performance degradation. The performance of weight-shared vision transformers drops significantly com-pared to the original models. For example, it leads to a 5.6% degradation in accuracy for Swin-S, although weight shar-ing can reduce the number of model parameters by fourfold.
To investigate the underlying reasons for these observa-tions, we analyze the ℓ2-norm of gradients during training and the similarities between intermediate feature represen-tations from the model before and after weight sharing (cf.
Sec. 4.2). We found that strictly identical weights across
In par-different layers is the main cause of the issues. ticular, the layer normalization [5] in different transformer blocks should not be identical during parameter sharing, be-cause the features of different layers have various scales and statistics. Meanwhile, the ℓ2-norm of the gradient becomes large and fluctuates across different layers after weight shar-ing, leading to training instability. Finally, the Central Ker-nel Alignment (CKA) [29] values, a popular similarity met-indicating ric, drop significantly in the last few layers, that feature maps generated by the model before and after weight sharing become less correlated, which can be the reason of performance degradation.
In this paper, we propose a new technique, called weight multiplexing, to address the above issues. It consists of two components, weight transformation and weight distillation, to jointly compress pre-trained vision transformers. The key idea of weight transformation is to impose transforma-tions on the shared weights, such that different layers have slightly different weights, as shown in Fig. 2. This operation can not only promote parameter diversity, but also improve training stability. More concretely, we impose simple lin-ear transformations on the multi-head self-attention (MSA) module and the multilayer perceptron (MLP) module for each weight-shared transformer layer. Each layer includes separate transformation matrices, so the corresponding at-tention weights and outputs of MLP are different across layers. The layer normalization for different layers is also separated, in contrast to sharing the same parameters. As such, the optimization of weight sharing transformer net-works becomes more stable, as demonstrated in Fig. 4.
To mitigate performance degradation, we further equip weight multiplexing with weight distillation, such that the information embedded in the pre-trained models can be transferred into the weight-shared small ones, which are much more compact and lightweight.
In contrast to pre-vious works that only rely on prediction-level distillation
[27, 52], our method additionally considers both attention-level and hidden-state distillation, allowing the smaller model to closely mimic the behavior of the original pre-trained large teacher model.
The experiments demonstrate that our weight multiplex-ing method achieves clear improvements in accuracy over the baselines and compresses pre-trained vision transform-ers by 2 times while transferring well to downstream tasks.
For instance, with the proposed weight multiplexing, the
Mini-Swin-B model with 12-layer parameters obtains 0.8% higher accuracy than the 24-layer Swin-B. Moreover, Mini-DeiT-B with 9M parameters achieves 79.8% top-1 accuracy on ImageNet, being 9.7 times smaller than DeiT-B (with 86 parameters and 81.8% accuracy). The 12M tiny model compressed by our approach transfers well to downstream object detection, achieving an AP of 48.6 on the COCO val-idation set, which is on par with the original Swin-T using 28M parameters.
We summarize our contributions as follows:
• We systematically investigate the efficacy of weight sharing in vision transformers, and analyze the under-lying reasons of issues brought by weight sharing.
• We propose a novel compression framework termed
MiniViT for general vision transformers. Experimen-tal results demonstrate that MiniViT can achieve a large compression ratio without losing accuracy. Further-more, the performance of MiniViT transfers well to downstream benchmarks. 2.