Abstract
Quantization is an efficient network compression ap-proach to reduce the inference time. However, existing ap-proaches ignored the distribution difference between train-ing and testing data, thereby inducing a large quantization error in inference. To address this issue, we propose a new quantization scheme, Alignment Quantization with ADMM-based Correlation Preservation (AlignQ), which exploits the cumulative distribution function (CDF) to align the data to be i.i.d. (independently and identically distributed) for quantization error minimization. Afterward, our theoreti-cal analysis indicates that the significant changes in data correlations after the quantization induce a large quanti-zation error. Accordingly, we aim to preserve the relation-ship of data from the original space to the aligned quanti-zation space for retaining the prediction information. We design an optimization process by leveraging the Alter-nating Direction Method of Multipliers (ADMM) optimiza-tion to minimize the differences in data correlations before and after the alignment and quantization. In experiments, we visualize non-i.i.d. in training and testing data in the benchmark. We further adopt domain shift data to compare
AlignQ with the state-of-the-art. Experimental results show that AlignQ achieves significant performance improvements especially in low-bit models. Code is available at https:
//github.com/tinganchen/AlignQ.git. 1.

Introduction
Convolutional neural networks (CNNs) have been demonstrated as effective models in computer vision tasks, such as image segmentation [2, 35] and object detection
[16, 30, 34]. However, CNNs are suffered from large com-putation costs and memory storage when deployed on the resource-limited mobile devices [7]. Therefore, various model acceleration methods are proposed, including prun-ing [20, 22, 27, 29], quantization [6, 42, 43] and structure simplification [10, 45]. Quantization has recently received
Figure 1. Motivation of AlignQ. Figure (a) presents an example of quantization on non-i.i.d. training and testing data. A quantization range learned from training data may induce a large quantization error when applied in the testing data with different distributions.
Figure (b) and Figure (c) illustrate our motivation to address the issue in Figure (a). In Figure (b), we propose to align the data to the same space for quantization to minimize the quantization error. In addition, as shown in Fig (c), we observe that the sig-nificant changes in data correlations induce a large quantization error. Accordingly, we aim to preserve the data correlations after the alignment and quantization to retain the prediction information in the original space for further reducing the quantization error. increasing attentions due to the effectiveness of acceleration on inference by reducing the bit widths of model weights and activations.
In existing quantization research, quantization-aware training (QAT) learned quantization parameters, including clipping ranges and scale parameters, from the training data and applied them to the testing data [3, 8, 11, 28, 47, 48]. In contrast, zero-shot quantization (ZSQ) adopted the concepts of knowledge distillation and employed the batch normal-ization means and variances from the full-precision model, to learn a quantized model that can generate similar features to reduce the quantization error [5,9,19,31,44]. The learned batch statistics are also utilized in testing. However, the pre-vious approaches ignored the difference between the train-ing and testing data. As shown in Fig. 1 (a), the real-world image data are usually collected under inconsistent quali-ties, such as different colors, brightness, and rotations, lead-ing to non-i.i.d. (independently and identically distributed) data [21]. Accordingly, it may induce a large quantization error when using the trained parameters in testing.
To address this issue, we propose AlignQ to align the data into the same domain for quantization to minimize the quantization error (illustrated in Fig. 1 (b)). In this paper, our idea is to exploit the cumulative distribution function (CDF) as the alignment function since the CDF of an arbi-trary continuous distribution follows the uniform distribu-tion [24] (demonstrated in Sec. 3.1). The uniform space is appropriate for uniform quantization that is hardware-friendly with a few simple operations [3, 8, 28]. In addition,
CDF retains the data order, i.e., larger values still exceed small values after the transformation.
Furthermore, our theoretical analysis indicates that no-table changes in data correlations after quantization induce a larger quantization error. Therefore, as shown in Fig. 1 (c), we aim to preserve the data correlations after the alignment-quantization process. We leverage the Alternating Direc-tion Method of Multipliers (ADMM) optimization to min-imize the differences to reduce the quantization error. To achieve the two goals in this paper, to minimize 1) the prediction loss of the quantized models and 2) the differ-ences of data correlations before and after the alignment-quantization process, ADMM addresses this multi-goal op-timization problem by dividing it into sub-problems and solving them [4].
To verify that the proposed AlignQ can reduce the quan-tization error derived from the non-i.i.d. in training and test-ing data, we compare with the state-of-the-art not only on the benchmark datasets, including CIFAR-10 [25], SVHN
[33], ImageNet [37], but also on domain shift benchmarks, including digits [12, 13, 26, 33] and Office-31 [38].
The contributions are summarized as follows: 1. We make the first attempt to design a new quantization scheme, AlignQ, that aligns the non-i.i.d. data to be i.i.d. to minimize the quantization error. 2. We prove that the changes in data correlation af-ter quantization induce a large quantization error and thereby leverage the ADMM optimization procedure to minimize the differences of the data correlations be-fore and after quantization to reduce the error. 3. We compare AlignQ with the state-of-the-art on benchmarks and the domain shift datasets. Experimen-tal results show that AlignQ achieves significant per-formance improvements, especially at low bit widths. 2.