Abstract (cid:3)(cid:18)(cid:19)(cid:24)(cid:18)(cid:1)(cid:2)(cid:12)(cid:23)(cid:15)(cid:19)(cid:18)(cid:22) (cid:8)(cid:18)(cid:16)(cid:18)(cid:19)(cid:24)(cid:18)(cid:1)(cid:2)(cid:12)(cid:23)(cid:15)(cid:19)(cid:18)(cid:22) (cid:4)(cid:16)(cid:15)(cid:10)(cid:3)(cid:19)(cid:14)(cid:17) (cid:2)(cid:12)(cid:10)(cid:11)(cid:3)(cid:19)(cid:14)(cid:17) (cid:2)(cid:6)(cid:14)(cid:14)(cid:9)(cid:18)(cid:5)(cid:11)(cid:18)(cid:16)(cid:20)
Temporal Action Localization (TAL) has experienced re-markable success under the supervised learning paradigm.
However, existing TAL methods are rooted in the closed set assumption, which cannot handle the inevitable unknown actions in open-world scenarios. In this paper, we, for the
ﬁrst time, step toward the Open Set TAL (OSTAL) problem and propose a general framework OpenTAL based on Ev-idential Deep Learning (EDL). Speciﬁcally, the OpenTAL consists of uncertainty-aware action classiﬁcation, action-ness prediction, and temporal location regression. With the proposed importance-balanced EDL method, classiﬁ-cation uncertainty is learned by collecting categorical ev-idence majorly from important samples. To distinguish the unknown actions from background video frames, the ac-tionness is learned by the positive-unlabeled learning. The classiﬁcation uncertainty is further calibrated by leverag-ing the guidance from the temporal localization quality. The
OpenTAL is general to enable existing TAL models for open set scenarios, and experimental results on THUMOS14 and
ActivityNet1.3 benchmarks show the effectiveness of our method. The code and pre-trained models are released at https://www.rit.edu/actionlab/opental. 1.

Introduction
Temporal Action Localization (TAL) aims to tempo-rally localize and recognize human actions in an untrimmed video. With the success of deep learning in video under-standing [4,10,15,20,34] and object detection [3,9,49], TAL has experienced remarkable advance in recent years [12,36, 62, 69]. However, these works are rooted in the closed set assumption that testing videos are assumed to contain only the pre-deﬁned action categories, which is impractical in an open world where unknown human actions are inevitable to appear. In this paper, we for the ﬁrst time step forward the
Open Set Temporal Action Localization (OSTAL) problem.
OSTAL aims to not only temporally localize and rec-ognize the known actions but also reject the localized un-known actions. As shown in Fig. 1, given an untrimmed (cid:5)(cid:20)(cid:14)(cid:18)(cid:1)(cid:10)(cid:19)(cid:21)(cid:17)(cid:13)(cid:1)(cid:9)(cid:15)(cid:13)(cid:14)(cid:19) (cid:7)(cid:2)(cid:4)(cid:1)(cid:7)(cid:11)(cid:22)(cid:16) (cid:5)(cid:6)(cid:7)(cid:2)(cid:4)(cid:1)(cid:7)(cid:11)(cid:22)(cid:16) (cid:4)(cid:16)(cid:15)(cid:10)(cid:3)(cid:19)(cid:14)(cid:17) (cid:2)(cid:12)(cid:10)(cid:11)(cid:3)(cid:19)(cid:14)(cid:17) (cid:17) (cid:4)(cid:16)(cid:15)(cid:10)(cid:3)(cid:19)(cid:14)(cid:17) (cid:1)(cid:6)(cid:7)(cid:13)(cid:10)(cid:18)(cid:16)(cid:19)(cid:15)(cid:8) (cid:4)(cid:16)(cid:15)(cid:10)(cid:3)(cid:19)(cid:14)(cid:17) (cid:2)(cid:12)(cid:10)(cid:11)(cid:3)(cid:19)(cid:14)(cid:17) (cid:3) (cid:1)(cid:3)(cid:2)(cid:3)(cid:4)(cid:5)(cid:3) (cid:1)(cid:6)(cid:7)(cid:13)(cid:10)(cid:18)(cid:16)(cid:19)(cid:15)(cid:8)
Figure 1. OSTAL and TAL Tasks. The OSTAL task is different from the TAL in that, there exist unknown actions in untrimmed open-world videos and the OSTAL models need to reject the pos-itively localized action (e.g., HammerThrow) as the Unknown, rather than falsely assign it a known label such as the LongJump. video (the top row) from open world, traditional TAL (the middle row) could falsely accept the unknown action clip
HammerThrow as one of the known actions such as the
LongJump, while the proposed OSTAL (the bottom row) could correctly reject the clip as the Unknown. Besides, both tasks need to differentiate between foreground actions and the