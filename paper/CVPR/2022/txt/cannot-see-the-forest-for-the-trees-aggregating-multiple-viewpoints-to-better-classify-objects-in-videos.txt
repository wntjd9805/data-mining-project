Abstract
Recently, both long-tailed recognition and object track-ing have made great advances individually. TAO bench-mark presented a mixture of the two, long-tailed object tracking, in order to further reﬂect the aspect of the real-world. To date, existing solutions have adopted detectors showing robustness in long-tailed distributions, which de-rive per-frame results. Then, they used tracking algorithms that combine the temporally independent detections to ﬁnal-ize tracklets. However, as the approaches did not take tem-poral changes in scenes into account, inconsistent classiﬁ-cation results in videos led to low overall performance. In this paper, we present a set classiﬁer that improves accuracy of classifying tracklets by aggregating information from multiple viewpoints contained in a tracklet. To cope with sparse annotations in videos, we further propose augmen-tation of tracklets that can maximize data efﬁciency. The set classiﬁer is plug-and-playable to existing object track-ers, and highly improves the performance of long-tailed ob-ject tracking. By simply attaching our method to QDTrack on top of ResNet-101, we achieve the new state-of-the-art, 19.9% and 15.7% TrackAP50 on TAO validation and test sets, respectively. Our code is available at this link1. 1.

Introduction
Object tracking is a long standing problem in computer vision as it plays a key role in surveillance and self-driving applications. There are numerous datasets and benchmarks for tracking [1, 14, 23, 31, 47] and also a long list of track-ing algorithms [3, 4, 41, 51, 58]. As with many other com-puter vision tasks, the performance of tracking algorithms has also taken a leap with deep learning.
Even with the great progress in object tracking, the per-formance of state-of-the-art trackers starts to degrade in the real-world scenarios with a large vocabulary of objects [9].
This is because most tracking benchmarks include only a small set of objects such as pedestrian, vehicles, and an-1https://github.com/sukjunhwang/set classifier
…
…
Tracklet
Per-frame 
Classifier
Set
Classifier
Person
Person
Person
Person
Wheelchair
Chair
Chair
Chair
Person
Wheelchair (a) (b)
Figure 1. (a) Per-frame classiﬁer receiving an instantaneous scene struggles on tail categories (e.g., wheelchair). On the other hand, (b) our proposed set classiﬁer shows robustness on tail categories by aggregating multiple viewpoints of a tracklet, taking the whole spatio-temporal feature into account. imals, for targeting speciﬁc applications like autonomous driving. To deploy the trackers in the real-world in a gen-eral environment, it is essential for the trackers to be able to deal with a much larger set of objects as in the image de-tection problem [15]. For this purpose, a new benchmark called TAO [9] for tracking any object has been recently in-troduced. This dataset contains over 800 categories, an or-der of magnitude more than previous tracking benchmarks.
In [9], it was shown that most up-to-date trackers do not adapt well with increased number of object vocabulary.
While tracking algorithms have focused on accurately ﬁnd-ing object boxes and tracking them, less attention has been paid on the classiﬁcation of objects, primarily due to a small set of vocabulary. As the object category grows to a realistic size, classiﬁcation becomes crucial for the overall tracking performance. After a thorough analysis, it was suggested in [9] that “large-vocabulary tracking requires jointly im-proving tracking and classiﬁcation accuracy”.
In this paper, we show that aggregating multiple view-points of a tracklet is the key to classifying the large vocabu-lary in videos. A tracklet refers to a set of boxes in different frames that share the same identity. Although appearance of objects in tracklets may go through great changes, existing methods [3, 9, 35] determine the category of a tracklet from the collection of per-frame classiﬁcation results as shown in Fig. 1 (a). Since changes in scenes from temporal varia-tions are not considered, they are vulnerable to appearance changes including motion blur or occlusion. More impor-tantly such cases bring critical deterioration of performance in tail classes. Speciﬁcally, detectors trained on imbalanced data are more conﬁdent in frequent classes, and such cases bring critical deterioration of performance in tail classes.
To this end, we propose a set classiﬁer that takes the spatio-temporal features of a whole tracklet into account (Fig. 1 (b)). With this design, the set classiﬁer is supplied with sufﬁcient information to determine a category from the large vocabulary. Therefore, the set classiﬁer gains robust-ness against temporal shifts and the ability to avoid a col-lapse of ﬁnal predictions from transient failures, leading to noticeable improvements of accuracy in the tail.
To fully supervise the set classiﬁer to obtain the abil-ity of exploiting spatio-temporal information, the module gets trained with video data. In contrast to existing meth-ods that can only classify the large vocabulary using frame-wise detections [3, 9, 35], the essence of the set classiﬁer is the ability to evaluate a whole tracklet by aggregating infor-mation from multiple sources. The structural design of the set classiﬁer is simple yet powerful; it is a stack of a few transformer layers [46]. Receiving multiple regional fea-tures [16] corresponding to the predicted boxes that com-pose a tracklet, the set classiﬁer attends to relevant infor-mation that are necessary for the classiﬁcation of the large vocabulary.
However, due to immense efforts required to annotate a video [9], the annotation budget is insufﬁcient to give the supervision of classifying tracklets under the complicated long-tailed scenarios. As a solution to this dilemma, we present augmentation methods of generating tracklets that have video characteristics: a variety of viewpoints of an ob-ject. Speciﬁcally, tracklets are dynamically generated utiliz-ing regional proposals [38] from multiple source videos and images. To further make the most out of the limited number of annotations, the augmented tracklets can be composed of multiple identities, and we introduce a training procedure for the set classiﬁer using such tracklets. With our meth-ods, an enormous number of tracklet samples composed of rare classes can be obtained, and the set classiﬁer gains the ability to successfully distinguish the large vocabulary in videos.
Adoption of our set classiﬁer results in high performance improvement in the long-tailed tracking. With the plug-and-playable design, we show experimental results on top of re-cently proposed QDTrack [35], and achieve new state-of-the-art on the challenging TAO [9] benchmark: 19.9% and 15.7% TrackAP50 on validation and test sets, respectively.
Furthermore, taking the same approach, we also achieve a competitive result of 37.7% AP on the video instance seg-mentation dataset, YouTube-VIS 2019 [54].
Our work can be summarized as follows:
• We propose the set classiﬁer which classiﬁes a tracklet as a whole by aggregating information from multiple view-points.
• We introduce augmentation methods that can generate augmented tracklets of near inﬁnite diversities – unlim-ited number of tracklets of tail classes can be obtained.
• We propose a new training procedure that facilitates the supervision of the set classiﬁer using the augmented tracklets. Moreover, we suggest auxiliary losses that bring further improvements in accuracy.
• We achieve new state-of-the-art on TAO, and also show the effectiveness of our method on YouTube-VIS 2019. 2.