Abstract
Input images
Feature extraction (frozen)
FCL (retrainable)
EM prototypes
Continually learning new classes from fresh data with-out forgetting previous knowledge of old classes is a very challenging research problem. Moreover, it is imperative that such learning must respect certain memory and com-putational constraints such as (i) training samples are lim-ited to only a few per class, (ii) the computational cost of learning a novel class remains constant, and (iii) the memory footprint of the model grows at most linearly with the number of classes observed. To meet the above con-straints, we propose C-FSCIL, which is architecturally com-posed of a frozen meta-learned feature extractor, a trainable fixed-size fully connected layer, and a rewritable dynami-cally growing memory that stores as many vectors as the number of encountered classes. C-FSCIL provides three update modes that offer a trade-off between accuracy and compute-memory cost of learning novel classes. C-FSCIL exploits hyperdimensional embedding that allows to contin-ually express many more classes than the fixed dimensions in the vector space, with minimal interference. The qual-ity of class vector representations is further improved by aligning them quasi-orthogonally to each other by means of novel loss functions. Experiments on the CIFAR100, mini-ImageNet, and Omniglot datasets show that C-FSCIL out-performs the baselines with remarkable accuracy and com-pression. It also scales up to the largest problem size ever tried in this few-shot setting by learning 423 novel classes on top of 1200 base classes with less than 1.6% accuracy drop. Our code is available at https://github.com/
IBM/constrained-FSCIL. 1.

Introduction
Deep convolutional neural networks (CNNs) have achieved remarkable success in various computer vision tasks, such as image classification [15,24,26,47], stemming from the availability of large curated datasets as well as huge computational and memory resources. This, however, poses significant challenges for their applicability to smart i
T m e
C o n v
...
C o n v
C o n v
...
C o n v
Retraining
∼90◦
Retraining
∼90◦
∼90◦
∼90◦
Figure 1. Overview of C-FSCIL which maps input images to quasi-orthogonal prototypes such that the prototypes of different classes encounter small interference. agents deployed in new and dynamic environments, where there is a need to continually learn about novel classes from very few training samples, and under resource constraints.
We consider the challenging scenario of learning from an online stream of data, including never-seen-before-classes, where we impose constraints on the sample size, computa-tional cost, and memory size.
Let us first focus on the sampling constraint of training data.
Inspired by human-like sequential learning, classi-cal connectionist networks can be naively trained sequen-tially, e.g., first on a set of old classes, and then on a set of novel classes, whereby the training dataset of old classes is no longer available. As a result, the new learning may interfere catastrophically with the old learning by overwrit-ing weights involved in representing the old learning (and thereby forgetting) [33]. This effect is known as catas-trophic interference, or catastrophic forgetting, and causes the classification accuracy to deteriorate [12,33]. To address the catastrophic forgetting problem, research efforts have
been directed to, e.g., freezing parts of the network weights, while simultaneously growing other parts of the network to extend the learning ability. Among them, class-incremental learning (CIL) [4,18,34,39,52] aims to learn a unified clas-sifier in which the encountered novel classes—that were not seen before in the continual data stream—are added into the recognition tasks without forgetting the previously observed classes. One step further, very recently, few-shot CIL (FS-CIL) [2,5–7,40,44,48,56] algorithms have been proposed to continually extend learning to novel classes with only a few data samples. Requiring FSCIL to be trained with very few novel training samples makes it more challenging compared to CIL, which usually learns new classes from large-scale training samples.
To facilitate few-shot learning, memory-augmented neu-ral networks (MANNs) separate information processing from memory storage [13, 14, 21, 23, 42, 46, 51]. MANNs incorporate an explicit memory (EM) into an embedding network such that the network can write the embedding of few data samples to the memory as class prototypes, and read from these individual entries during inference. This separation enables the network to offload new prototypes to the EM, where they do not endanger the previously learned prototypes to be overwritten, leading to remarkable accu-racy with few classes (typically 20 classes). However, this neat feature of MANNs has not been exploited in CIL or
FSCIL. This is mainly due to the fact that the interference between different class prototypes increases with a grow-ing number of classes, as experienced in FSCIL. To allow continual learning in MANNs, the representational power of the embedding network should be naturally extensible to a large number of classes with minimal interference. This critical requirement can be met by exploiting hyperdimen-sional computing [10, 19, 35], where classes can be repre-sented by random high-dimensional vectors with a dimen-sionality in the order of thousands. This new combination of MANNs and hyperdimensional computing has been re-cently developed in [21] for few-shot learning.
Hyperdimensional computing is characterized by the fol-lowing properties: (i) A randomly chosen vector is quasi-orthogonal to other random vectors with very high proba-bility (the “curse” of dimensionality), therefore the repre-sentation of a novel class is not only incremental to the old learning but also causes minimal interference. This phe-nomenon is known as concentration of measure [28], with the peculiar property that pseudo-orthogonality converges to exact orthogonality with increasing dimensionality. (ii)
The number of such quasi-orthogonal vectors grows expo-nentially with the dimensionality, which provides a suffi-ciently large capacity to accommodate novel classes over time. (iii) Counterintuitively, quasi-orthogonal vectors can still encode semantic information. We can describe a con-cept in a scene (e.g., a black dog) with a vector by binding xdog), which is quasi-orthogonal atomic vectors (xblack quasi-orthogonal to all other involved vectors (atomic and composite). The bound vector can be decomposed to xblack (cid:16) and xdog, revealing the semantic relation between a black dog and a black cat (both include xblack) [9, 16]. In fact, fixed quasi-orthogonal vectors have been successfully used as class vectors achieving improved performance in the su-pervised classification tasks [16, 17].
In this paper, we enhance MANNs by exploiting the rep-resentational power of hyperdimensional computing to per-form FSCIL. During FSCIL operations, it is constrained to either no gradient updates or a small constant number of it-erations for learning novel classes, and a linear growth in the memory size with respect to the encountered classes.
The contributions of this work are summarized as follows:
First, we propose C-FSCIL, which is architecturally composed of a frozen feature extractor, a trainable fixed-size fully connected layer, and a rewritable dynamically growing EM that stores as many vectors as the number of classes encountered so far (See Fig. 1). The frozen part is separated from the growing part by inserting the fully connected layer, which outputs class vectors in a hyperdi-mensional embedding space whose dimensionality remains fixed, and is therefore independent of the number of classes in the past and future. The feature extractor is a CNN that is meta-learned by proper sharpened attention, which strives to represent dissimilar images with quasi-orthogonal vec-tors. The C-FSCIL architecture is presented in Section 4.1.
Second, C-FSCIL with three update modes offers a trade-off between accuracy and the compute-memory cost of learning a novel class. The simple yet powerful Mode 1 creates and updates an averaged prototype vector as the mean-of-exemplars in the EM, without any gradient com-putation (see Section 4.2.1). Mode 2 bipolarizes the pro-totype vectors, and retrains the fully connected layer with-out exceeding a small constant number of iterations; for re-training it requires storing an averaged activation pattern for every class in a globally averaged activation (GAA) mem-ory (see Section 4.2.2). Mode 3 nudges the averaged proto-type vectors to align them quasi-orthogonally to each other, while remaining in close proximity to the original averaged prototypes, using a combination of novel loss functions. It then retrains the fully connected layer without exceeding the constant number of iterations (see Section 4.2.3).
Third, C-FSCIL leads to higher accuracy, compute-memory efficiency, and scalability compared to FSCIL baselines, as shown in Section 5. Experiments on the CI-FAR100, miniImageNet, and Omniglot datasets demon-strate that C-FSCIL outperforms the baselines even in
Mode 1 where prototypes are simply obtained in one pass
In Om-without any gradient-based parameter updates. niglot, C-FSCIL scales up to 1623 classes, whereby 423 novel classes are incrementally added to 1200 base classes, with less than 2.6%, 1.4%, and 1.6% accuracy drops when using Modes 1, 2, and 3, respectively. Thanks to the quasi-orthogonality of the class prototypes in the EM, they can be compressed by 2×, causing 1.7%–3.5% accuracy drop during the course of FSCIL.
2.