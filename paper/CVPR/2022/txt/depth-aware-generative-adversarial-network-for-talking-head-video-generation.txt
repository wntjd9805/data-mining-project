Abstract
Talking head video generation aims to produce a syn-thetic human face video that contains the identity and pose information respectively from a given source image and a driving video. Existing works for this task heavily rely on 2D representations (e.g. appearance and motion) learned from the input images. However, dense 3D facial geometry (e.g. pixel-wise depth) is extremely important for this task as it is particularly beneficial for us to essentially generate accurate 3D face structures and distinguish noisy informa-tion from the possibly cluttered background. Nevertheless, dense 3D geometry annotations are prohibitively costly for videos and are typically not available for this video gen-eration task. In this paper, we introduce a self-supervised face-depth learning method to automatically recover dense 3D facial geometry (i.e. depth) from the face videos with-out the requirement of any expensive 3D annotation data.
Based on the learned dense depth maps, we further pro-pose to leverage them to estimate sparse facial keypoints that capture the critical movement of the human head. In a more dense way, the depth is also utilized to learn 3D-aware cross-modal (i.e. appearance and depth) attention to guide the generation of motion fields for warping source image representations. All these contributions compose a novel depth-aware generative adversarial network (DaGAN) for talking head generation. Extensive experiments conducted demonstrate that our proposed method can generate highly realistic faces, and achieve significant results on the unseen human faces. 1 1.

Introduction
In this paper, we target the task of generating a realis-tic talking head video of a person using a source image of that person and a driving video, possibly derived from an-other person [27, 28, 31]. In the real world, a wide range of practical applications can be benefited from this task such
*Corresponding author 1https://github.com/harlanhong/CVPR2022-DaGAN
Figure 1. Qualitative results of the learned depth maps (Fig. 1b) of the face images (Fig. 1a) using a self-supervised manner, and dense depth-aware attention maps (Fig. 1c), which can attend to important semantic parts of the face such as eyes. as role-playing video games and virtual anchors.
Rapid progress has been achieved on talking head video generation in terms of both quality and robustness in recent years, using generative adversarial networks (GANs) [5]. A successful direction for the task in the literature focuses on decoupling identity and pose information from the face im-ages [22, 24, 31]. For instance, pioneering works [22, 24] propose to model relative poses between two face images based on estimated sparse facial keypoints, and the poses are further used to generate dense motion fields, which warps the feature maps of the source image to drive the im-age generation. Similarly, Eurkov et al. [1] aimed to specifi-cally learn two latent codes for the pose and the identity, and then input them into a designed generator network for face video synthesis. More than that, data augmentation strate-gies [1,39] are also explored to more effectively perform the disentanglement of the pose and identity information. Al-though these methods show highly promising performance on the task, they still pay large attention to learning more representative 2D appearance and motion features from the input images. However, for face video generation, 3D dense geometry is critically important for the task while rarely in-vestigated in the existing methods.
The dense 3D geometry (e.g. pixel-level depth) can bring
several significant benefits for the talking-head video gen-eration. First, as the video captures the moving heads in a realistic 3D physical world, the 3D geometry can greatly fa-cilitate an accurate recovery of 3D face structures, and the model capability of maintaining a realistic 3D face structure is a key factor for generating high-fidelity face videos. Sec-ond, the dense geometry can also help the model to robustly distinguish the noisy background information for genera-tion especially under cluttered background conditions. Fi-nally, the dense geometry is also particularly useful for the model to identify expression-related micro-movements on the faces. However, a severe issue of utilizing the 3D dense geometry to significantly boost the generation is that the 3D geometry annotations are highly expensive and typically not available for this task.
To address this problem, in this paper, we first propose to learn the pixel-wise depth map (see Fig. 1b) via geometric warping and photometric consistency in a self-supervised manner, to automatically recover dense 3D facial geometry from the training face videos, without requiring any expen-sive 3D geometry annotations. Based on the learned dense facial depth maps, we further propose two mechanisms to effectively leverage the depth information for better talking-head video generation. The first mechanism is depth-guided facial keypoint detection. The facial keypoints estimated by the network should well reflect the structure of the face, as they are further used to produce the motion field for feature warping, while the depth map explicitly indicates the 3D structure of the face. Thus, we combine geometry represen-tations learned from the input depth maps with the appear-ance representations learned from the input images, to pre-dict more accurate facial keypoints. The second mechanism is a cross-modal attention mechanism to guide the learning of the motion field. The motion field may contain noisy information from the cluttered background, and cannot ef-fectively capture the expression-related micro-movements as they are generated from sparse facial keypoints. There-fore, we propose to learn depth-aware attention to have pixel-wise 3D geometry constraint on the motion field (see
Fig. 1c), to drive the generation with more fine-grained de-tails of facial structure and movements.
All the above-illustrated contributions compose a Depth-aware Generative Adversarial Network (DaGAN) to ad-vance talking head video generation. Extensive experi-ments are conducted to qualitatively and quantitatively eval-uate the proposed DaGAN model on two different datasets, i.e. VoxCeleb1 [18] and CelebV [27]. The experimental re-sults show that our proposed self-supervised depth learn-ing strategy can produce accurate depth maps on both the source and the target human face images. Our DaGAN model can also generate higher-quality face images com-pared with state-of-the-art methods. More specifically, our model is able to better preserve facial details, yielding a syn-thesized face with a more accurate expression and pose.
In summary, the main contribution is three-fold:
• In this work, we propose to introduce a self-supervised face-depth learning method to recover explicit dense 3D facial geometry (i.e. depth maps) from face videos for talking head video generation, and utilize the learned depth to boost the performance.
• We propose a novel depth-aware generative adversarial network for talking head generation, which effectively incorporates the depth information into the genera-tion network via two carefully designed mechanisms, i.e. depth-guided facial keypoint estimation, and cross-modal (i.e. depth and image) attention learning.
• Extensive experimental results show accurate depth re-covery of face images and also achieve superior gener-ation performance compared with state-of-the-arts. 2.