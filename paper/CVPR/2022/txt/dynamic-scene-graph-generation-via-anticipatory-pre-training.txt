Abstract
Humans can not only see the collection of objects in visual scenes, but also identify the relationship be-tween objects. The visual relationship in the scene can be abstracted into the semantic representation of a triple (cid:104)subject, predicate, object(cid:105) and thus results in a scene graph, which can convey a lot of information for visual un-derstanding. Due to the motion of objects, the visual re-lationship between two objects in videos may vary, which makes the task of dynamically generating scene graphs from videos more complicated and challenging than the conven-tional image-based static scene graph generation. Inspired by the ability of humans to infer the visual relationship, we propose a novel anticipatory pre-training paradigm based on Transformer to explicitly model the temporal correla-tion of visual relationships in different frames to improve dynamic scene graph generation.
In pre-training stage, the model predicts the visual relationships of current frame based on the previous frames by extracting intra-frame spa-tial information with a spatial encoder and inter-frame tem-poral correlations with a progressive temporal encoder. In the ﬁne-tuning stage, we reuse the spatial encoder and the progressive temporal encoder while the information of the current frame is combined for predicting the visual relation-ship. Extensive experiments demonstrate that our method achieves state-of-the-art performance on Action Genome dataset. 1.

Introduction
Scene graph abstracts the visual relationships as a graph structure, where the objects are represented as nodes and their relationships are represented as edges. It is a promis-ing way to represent semantics of visual content, which can bridge the large gap between vision and natural lan-In recent years, scene graph generation has at-guage.
∗indicates corresponding author: Changsheng Xu.
Figure 1. Given previous frames, humans can easily infer the vi-sual relationship contained in the current frame. Because the tem-poral correlation of different relationships is a kind of common sense to humans. But this kind of temporal reasoning is difﬁcult for computers. tracted more and more attention and has been successfully applied in multiple tasks, e.g., image retrieval [19], im-age captioning [14, 21, 48, 49] and visual question answer-ing [11, 12, 53].
Existing scene graph generation methods can be roughly grouped into two categories, the static scene graph gen-eration, i.e, generating scene graph from a single image, and dynamic scene graph generation, i.e., generating scene graph from a video. For static scene graph generation, exist-ing methods [4, 22, 51, 52] generally use popular object de-tectors, such as Faster R-CNN [34] and Mask R-CNN [15], to extract objects, and then predict the relationship between objects based on the visual and semantic features. Although static scene graph generation methods have achieved signif-icant progress, the dynamic scene graph generation is less studied, which is more challenging because the objects in video are moving, and thus cause the change of the relation-ships. The static scene graph generation methods cannot be directly applied to solve dynamic scene graph generation since they ignore the temporal information in videos. To im-prove the prediction accuracy, existing dynamic scene graph generation methods focus on capturing temporal informa-tion by 3D convolution model [39] and transformer [7, 32].
Existing dynamic scene graph generation methods ex-plore the temporal structure information in feature-level and model the dynamic scene graph generation as a classiﬁca-tion task, which results in that they cannot explicitly cap-ture the temporal correlation of visual relationships. In con-trast, humans can easily infer the subsequent relationships based on the past relationships according to their tempo-ral correlations. As shown in Figure 1, after observing (cid:104)person, looking at, cup(cid:105) and (cid:104)person, holding, cup(cid:105), humans can infer that the subsequent relationships are likely to be consistent with the previous relationships or change to (cid:104)person, drinking f rom, cup(cid:105). This kind of reasoning ability comes from humans’ experience and commonsense in the real world. To make the dynamic scene graph gen-eration model explicitly capture the temporal correlation of visual relationships like humans, there are at least two chal-lenges to be resolved. (1) Since the temporal and spatial in-formation in the videos are heavily entangled, it is difﬁcult to explicitly capture the temporal correlations. (2) Existing datasets, e.g., Action Genome [18], only have scene graph annotations in key-frame level due to the high cost, which hinders the consecutive modeling of the temporal correla-tions.
In this paper, we propose an anticipatory pre-training paradigm to predict dynamic scene graph in videos to han-dle the above challenges. The anticipatory scene graph gen-eration task is deﬁned as using previous frames to predict the relationships in the current frame. Using the antici-patory pre-training paradigm has the two advantages. (1)
Since the goal of the pre-training task is predicting visual relationships in unseen frames, it can induce the model to explicitly extract the temporal correlations in task-level. (2)
Based on the pretext task, we can use a large amount of unlabeled data to train the anticipatory model with the su-pervision of key frame labels, thus can alleviate the problem of insufﬁcient annotations.
The proposed anticipatory pre-training paradigm is in-In stantiated as an anticipatory Transformer architecture. pre-training stage, the model consists of a spatial encoder to extract intra-frame spatial information and a progressive temporal encoder to capture inter-frame temporal correla-tions based on both visual and semantic features. To en-hance the perception of visual content in long-sequence frames, we design an efﬁcient comprehensive short-term and long-term attention mechanism in the progressive tem-poral encoder to capture the long-term visual context from labeled and unlabeled frames for each relationship without adding too many parameters. Finally, we predict the vi-sual relationship in current frame based on the output of the progressive temporal encoder. In the ﬁne-tuning stage, we reuse the spatial encoder in the pre-training model to obtain the spatial information of the current frame, and sequen-tially combine it with the output of the progressive tempo-ral encoder to predict the visual relationship in the current frame.
The main contributions of this work are summarized as follows: 1. We propose a novel anticipatory pre-training paradigm for dynamic scene graph generation, which explicitly models the temporal correlation of visual relationships in the task-level. 2. We instantiate the anticipatory pre-training paradigm with a Transformer architecture. which can not only capture the spatial and temporal information from the labeled training videos based on the visual and seman-tic features, but also efﬁciently capture the short-term and long-term visual context from unlabeled data for each relationship. 3. We evaluate the proposed pre-training paradigm on public Action Genome dataset. The extensive ex-periment results demonstrate that our model achieves state-of-the-art results. 2.