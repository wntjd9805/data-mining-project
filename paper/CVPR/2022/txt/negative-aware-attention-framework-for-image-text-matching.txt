Abstract
Image-text matching, as a fundamental task, bridges the gap between vision and language. The key of this task is to accurately measure similarity between these two modal-ities. Prior work measuring this similarity mainly based on matched fragments (i.e., word/region with high rele-vance), while underestimating or even ignoring the effect of mismatched fragments (i.e., word/region with low rel-evance), e.g., via a typical LeaklyReLU or ReLU opera-tion that forces negative scores close or exact to zero in attention. This work argues that mismatched textual frag-ments, which contain rich mismatching clues, are also cru-cial for image-text matching. We thereby propose a novel
Negative-Aware Attention Framework (NAAF), which ex-plicitly exploits both the positive effect of matched frag-ments and the negative effect of mismatched fragments to jointly infer image-text similarity. NAAF (1) delicately de-signs an iterative optimization method to maximally mine the mismatched fragments, facilitating more discrimina-tive and robust negative effects, and (2) devises the two-branch matching mechanism to precisely calculate similar-ity/dissimilarity degrees for matched/mismatched fragments with different masks. Extensive experiments on two bench-mark datasets, i.e., Flickr30K and MSCOCO, demonstrate the superior effectiveness of our NAAF, achieving state-of-the-art performance. Code will be released at: https:
//github.com/CrossmodalGroup/NAAF. 1.

Introduction
Image-text matching, which devotes to bridging the se-mantic gap between these two heterogeneous modalities, is a fundamental task in computer vision (CV) and natural lan-guage processing (NLP). This matching task aims to search images for a given textual description or find texts w.r.t. an image query. The critical challenge of image-text match-ing lies in accurately learning semantic correspondence be-*Zhendong Mao is the corresponding author.
Figure 1. Motivation of the negative-aware attention. (a) Exist-ing methods mainly find matched fragments, e.g., “boys”, “trees”, to compute image-text (I-T) similarity, while the effect of mis-matched fragments, e.g., “football”, is weakened or ignored, by the typical LeaklyReLU or ReLU. (b) shows the false-positive prob-lem of existing methods, where the I-T pair can still obtain a high similarity, contributed by most matched fragments, and may rank quite the top as correct. (c) In our method, both mismatched and matched fragments are mined to produce negative and positive ef-fects, respectively, thereby downgrading the false-positive pairs. tween images and texts to measure their similarity.
Generally, there are two paradigms among existing image-text matching approaches [2, 7, 27]. The first one tends to perform global-level matching, i.e., finding the se-mantic correspondence between the full text and the whole image [3, 9, 24, 43]. They typically project the holistic im-ages and texts into a common latent space and then match the two modalities. The second paradigm focuses on exam-ining local-level matching, i.e., matching between salient regions in images and words in texts [19, 23]. Local-level matching takes into account fine-grained semantic corre-spondence between images and texts.
Recently, attention-based local-level matching has been proposed and quickly becomes the mainstream in image-text matching. SCAN [23], as well as its various variants
[2, 5, 7, 13, 16, 26, 27, 39], is a representative method of this
kind. The key idea is to discover all word-region alignments by attending to relevant fragments w.r.t. each query frag-ment from another modality.
In summary, matched frag-ments (i.e., word-region pairs with high relevance scores) will contribute a lot to the final image-text similarity, while the effect of mismatched fragments (i.e., word-region pairs with low relevance scores) will be weakened or even erased, e.g., via a typical LeakyReLU or ReLU that forces negative scores close or exact to zero during the attention process
[2, 5, 7, 13, 16, 23, 26, 27, 39]. Although achieving promising performance, these methods completely ignore the crucial role of mismatched textual fragments in proving image-text mismatching, since they describe contents not in the image. (In fact, images usually contain more background object regions, thus we principally focus on mismatched textual fragments, i.e., words.)
Consequently, existing methods, which mainly find the matched fragments while underestimating or neglecting the effect of mismatched ones, will be inevitably prone to pro-duce false-positive matching. Namely, image-text pairs containing many matched fragments but a few mismatched textual fragments (directly indicating image-text mismatch-ing), can still obtain the high similarity and may rank quite the top as correct, which is certainly not a satisfying re-sult (Fig. 1(b)). Therefore, we argue that a reasonable matching framework should simultaneously consider two aspects, i.e., the overall matching score of an image-text pair is determined not only by the positive effect of matched fragments, but also by the negative effect of mismatched ones (e.g., words not mentioned in the image will probably downgrade the overall matching score). For example, as shown in Fig. 1(c), by further emphasizing and mining the negative effect of mismatched fragments w.r.t. “f ootball”, it will be easy to eliminate this false-positive pair.
To this end, we propose a novel negative-aware atten-tion framework which, for the first time to our knowledge, explicitly considers both positively matched and negatively mismatched fragments to jointly measure image-text sim-ilarity (Fig. 1(a)). Different from conventional matching mechanisms that focus on matched fragments unilaterally, our attention framework can effectively mine mismatched textual fragments and use them to accurately reflect how dissimilar the two modalities are. In this sense, we call it negative-aware attention framework (NAAF). As illustrated in Fig. 2, NAAF consists of two modules. (1) We devise a two-branch matching to solve the lack utilization of mis-matched fragments, which contains the negative and posi-tive attention with different masks, one to precisely calcu-late the dissimilarity degrees of mismatched fragments, and the other the similarity degrees of matched ones. (2) We propose a new iterative optimization method to explicitly model and mine the mismatched fragments. Concretely, based on the similarity distributions of mismatched and matched fragments, we first adaptively learn the optimal boundary between them by minimizing the penalty prob-ability of their error overlaps, which can theoretically guar-antee the mining accuracy. Then, the learned boundary is integrated into the attention matching process to optimize more discriminative similarity distributions. Such iterative optimizing forcedly separates these two types of distribu-tions as far as possible, enabling maximally mining the mis-matched textual fragments. In this way, NAAF not only fo-cuses on matched fragments but also discriminates subtle mismatched ones across modalities towards more accurate image-text matching.
The major contributions of this work are summarized as follows. 1) We propose a novel two-branch matching mod-ule, which jointly utilizes both mismatched and matched textual fragments to make accurate image-text matching.
To the best of our knowledge, this is the first framework that explicitly exploits both negative effects of mismatched clues and positive effects of matched clues in image-text match-ing. 2) We propose a novel iterative optimization method with negative mining strategies, which can explicitly drive more negative effects of mismatched fragments, and theo-retically guarantee the mining accuracy, yielding more com-prehensive and interpretable image-text similarity measure-ment. 3) Extensive experiments on two benchmarks, i.e.,
Flickr30K and MS-COCO, show that NAAF outperforms compared methods. Analyses also well demonstrate the su-periority and reasonableness of our method. 2.