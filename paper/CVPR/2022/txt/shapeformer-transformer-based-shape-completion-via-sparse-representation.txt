Abstract
We present ShapeFormer, a transformer-based network that produces a distribution of object completions, condi-tioned on incomplete, and possibly noisy, point clouds. The resultant distribution can then be sampled to generate likely completions, each exhibiting plausible shape details while being faithful to the input.
To facilitate the use of transformers for 3D, we intro-duce a compact 3D representation, vector quantized deep implicit function (VQDIF), that utilizes spatial sparsity to represent a close approximation of a 3D shape by a short sequence of discrete variables. Experiments demonstrate that ShapeFormer outperforms prior art for shape comple-tion from ambiguous partial inputs in terms of both comple-tion quality and diversity. We also show that our approach effectively handles a variety of shape types, incomplete pat-terns, and real-world scans. 1.

Introduction
Shapes are typically acquired with cameras that probe and sample surfaces. The process relies on line of sight and, at best, can obtain partial information from the visible parts of objects. Hence, sampling complex real-world geometry is inevitably imperfect, resulting in varying sampling densi-ties and missing parts. This problem of surface completion has been extensively investigated over multiple decades [5].
The central challenge is to compensate for incomplete data by inspecting non-local hints in the observed data to infer missing parts using various forms of priors.
Recently, deep implicit function (DIF) has emerged as an effective representation for learning high-quality surface completion. To learn shape priors, earlier DIFs [13, 41, 48] encode each shape using a single global latent vector.
Combining a global code with region-speciﬁc local latent codes [14, 15, 22, 27, 35, 50] can faithfully preserve geomet-ric details of the input in the completion. However, when presented with ambiguous partial input, for which multiple
* Corresponding author: Hui Huang (hhzhiyan@gmail.com)
Project page: https://shapeformer.github.io
Figure 1. ShapeFormer predicts multiple completions for a real-world scan of a sports car (left column), a chair with missing parts (middle column), and a partial point cloud of human lower legs (right column). The input point clouds are superimposed with the generated shapes to emphasize the faithfulness of the completion to the input point cloud. plausible completions are possible (see Fig. 1), the deter-ministic nature of local DIF usually fails to produce mean-ingful completions for unseen regions. A viable alternative is to combine generative models to handle the input uncer-tainty. However, for representations that contain enormous statistical redundancy, as in the case of current local meth-ods, such combination [57] excessively allocates model ca-pacity towards perceptually irrelevant details [21, 25].
We present ShapeFormer, a transformer-based autore-gressive model that learns a distribution over possible shape completions. We use local codes to form a sequence of dis-crete, vector quantized features, greatly reducing the repre-sentation size while keeping the underlying structure. Ap-plying transformer-based generative models toward such se-quences of discrete variables have been shown to be effec-tive for generative pretraining [3, 11], generation [23, 53] and completion [64] in image domain.
However, directly deploying transformers to 3D feature grids leads to a sequence length cubic in the feature reso-lution. Since transformers have an innate quadratic com-plexity on sequence length, only using overly coarse fea-ture resolution, while feasible, can barely represent mean-ingful shapes. To mitigate the complexity, we ﬁrst intro-duce Vector Quantized Deep Implicit Functions (VQDIF), a novel 3D representation that is both compact and struc-tured, that can represent complex 3D shapes with accept-able accuracy, while being rather small in size. The core idea is to sparsely encode shapes as sequences of discrete 2-tuples, each representing both the position and content of a non-empty local feature. These sequences can be decoded to deep implicit functions from which high-quality surfaces can subsequently be extracted. Due to the sparse nature of 3D shapes, such encoding reduces the sequence length from cubic to quadratic in the feature resolution, thus enabling effective combination with generative models.
ShapeFormer completes shapes by generating complete sequences, conditioned on the sequence for partial observa-tion. It is trained by sequentially predicting the conditional distribution of both location and content over the next el-ement. Unlike image completion [64], where the model is trained with the BERT [3, 20] objective to only predict for unseen regions, in the 3D shape completion setting, the input features may also come from both noisy and in-complete observations, and keeping them intact necessarily yields noisy results. Hence, in order to generate whole com-plete sequences from scratch while being faithful to the par-tial observations, we adapt the auto-regressive objective and prepend the partial sequence to the complete one to achieve conditioning. This strategy has been proved effective for conditional synthesis for both text [39] and images [23].
We demonstrate the ability of ShapeFormer to produce diverse high-quality completions for ambiguous partial ob-servations of various shape types, including CAD mod-els and human bodies, and of various incomplete sources such as real-world scans with missing parts. In summary, our contributions include: (i) a novel DIF representation based on sequences of discrete variables that compactly represents satisfactory approximations of 3D shapes; (ii) a transformer-based autoregressive model that uses our new representation to predict multiple high-quality completed shapes conditioned on the partial input; and (iii) state-of-the-art results for multi-modal shape completion in terms of completion quality and diversity. The FPD score on PartNet is improved by at most 1.7 compared with prior multi-modal method cGAN [67]. 2.