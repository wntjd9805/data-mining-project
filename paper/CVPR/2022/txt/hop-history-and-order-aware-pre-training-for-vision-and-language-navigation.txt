Abstract
Pre-training has been adopted in a few of recent works for Vision-and-Language Navigation (VLN). However, pre-vious pre-training methods for VLN either lack the ability to predict future actions or ignore the trajectory contexts, which are essential for a greedy navigation process. In this work, to promote the learning of spatio-temporal visual-textual correspondence as well as the agent’s capability of decision making, we propose a novel history-and-order aware pre-training paradigm (HOP) with VLN-specific ob-jectives that exploit the past observations and support future action prediction. Specifically, in addition to the commonly used Masked Language Modeling (MLM) and Trajectory-Instruction Matching (TIM), we design two proxy tasks to model temporal order information: Trajectory Order Mod-eling (TOM) and Group Order Modeling (GOM). Moreover, our navigation action prediction is also enhanced by intro-ducing the task of Action Prediction with History (APH), which takes into account the history visual perceptions. Ex-tensive experimental results on four downstream VLN tasks (R2R, REVERIE, NDH, RxR) demonstrate the effectiveness of our proposed method compared against several state-of-the-art agents. 1.

Introduction
Vision-and-Language Navigation (VLN) has received large attention in communities of computer vision, natu-ral language processing and robotics due to its great im-portance towards real-world applications such as domestic assistants [3, 5, 7, 17, 28, 29, 38]. VLN requires an agent to navigate to a target location in a 3D simulated environ-ment, according to a given natural language instruction. In the past few years, a great variety of VLN tasks have been proposed, including navigation with low-level instructions
*Corresponding author
Figure 1. Illustration of the proposed pre-training and fine-tuning paradigm for VLN. The model is pre-trained with five proxy tasks, and fine-tuned on four downstream VLN tasks: R2R, RxR,
REVERIE and NDH (detailed in Section 3). such as R2R [3] and RxR [18], communicative and cooper-ative instructions such as NDH [36], and high-level instruc-tions for remote object grounding such as REVERIE [32] and SOON [40].
Despite their differences, the agent’s navigation is mostly formulated as a sequential text-to-image grounding problem. That is, positioned at a particular node on a pre-defined connectivity graph, the agent traverses the environ-ment by selecting the adjacent node that has the maximum correspondence between the image representation and the instruction. As a result, the visual-textual matching is con-sidered to be the keystone of addressing VLN tasks.
Inspired by the great success of Vision-Language
BERT pre-training on several visual-textual matching tasks, such as image-text retrieval [19] and referring expression grounding [39], several pre-training methods have been pro-posed for VLN [8, 9, 13, 27]. These approaches are able to achieve better performance, but they still suffer from some limitations. VLN-BERT [27] pre-trains its model by pre-dicting the compatibility of a pair of instruction and visual trajectory. In the downstream tasks, it formulates the navi-gation as a trajectory selection problem. AirBERT [8] fur-ther adopts a binary classification task to predict whether the given instruction and visual trajectory are paired. Both
VLN-BERT and AirBERT discard navigating action pre-diction during pre-training, weakening the relation between the learned representation and the final goal: navigation ac-tion prediction. By contrast, PREVALENT [9] introduces a single-step action prediction task, aiming to learn action-oriented generic visiolinguistic representation, which can be applied to the greedy search VLN. However, PREVA-LENT largely overlooked the important historical context in pre-training. It only takes the static panoramic image of a single step as visual input, while failing to take into ac-count the history trajectory information. Indeed, VLN is a
Partially Observable Markov Decision Process (POMDP), where the agents rely heavily on the past experiences for making future action decisions. Furthermore, VLN is a spatio-temporal task which is sensitive to the sequence or-der of the trajectory. Thus the ability of temporal order reasoning is also beneficial to the action decision making.
Nevertheless, all the above three methods do not explicitly mine temporal order information from either instructions or visual observations.
To address the above mentioned issues, in this work, we propose a novel history-and-order aware pre-training paradigm to enhance the learning of visual-textual corre-spondence for VLN task. First, we provide history visual observations to the action prediction task, called Action Pre-diction with History (APH), which helps the model locate the sub-instruction to be executed and thus improve the ac-tion prediction accuracy. Second, we design two order-aware proxy tasks, Trajectory Order Modeling (TOM) and
Group Order Modeling (GOM). Given an instruction, TOM requires the model to recover the order of shuffled visual trajectory from a fine-grained level, and GOM requires the model to predict the order of two groups of sub-trajectories from a coarse level. These two tasks explicitly equip the model with the ability to understand the temporal order within instructions, in addition to the visual-textual match-ing capability. The overall of the proposed pre-training and fine-tuning tasks are illustrated in Figure 1.
To comprehensively evaluate our proposed pre-training methods, we conduct experiments on four downstream tasks: R2R [3], RxR [18], NDH [36], REVERIE [32].
Each task poses a very different challenge to evaluate the agent. R2R serves as an in-domain task, which can verify the agent’s generalization ability to unseen environments.
The other three tasks are out-of-domain, which are used to study the generalization ability to new tasks. RxR is known for longer instructions. NDH features dialog instructions.
REVERIE is characterized by high-level, short instructions.
With our proposed pre-training tasks, the fine-tuned down-stream model performs favorably on all these tasks: 59%
SPL on R2R, 0.33 sDTW on RxR, 3.31 GP on NDH, and 24.34% SPL (14.34% RGSPL) on REVERIE. 2.