Abstract
For autoregressive (AR) modeling of high-resolution im-ages, vector quantization (VQ) represents an image as a se-quence of discrete codes. A short sequence length is im-portant for an AR model to reduce its computational costs to consider long-range interactions of codes. However, we postulate that previous VQ cannot shorten the code se-quence and generate high-ﬁdelity images together in terms of the rate-distortion trade-off. In this study, we propose the two-stage framework, which consists of Residual-Quantized
VAE (RQ-VAE) and RQ-Transformer, to effectively generate high-resolution images. Given a ﬁxed codebook size, RQ-VAE can precisely approximate a feature map of an image and represent the image as a stacked map of discrete codes.
Then, RQ-Transformer learns to predict the quantized fea-ture vector at the next position by predicting the next stack of codes. Thanks to the precise approximation of RQ-VAE, we can represent a 256×256 image as 8×8 resolution of the feature map, and RQ-Transformer can efﬁciently reduce the computational costs. Consequently, our framework out-performs the existing AR models on various benchmarks of unconditional and conditional image generation. Our ap-proach also has a signiﬁcantly faster sampling speed than previous AR models to generate high-quality images.
Figure 1. Examples of our conditional generation for 256×256 images. The images in the ﬁrst row are generated from the classes of ImageNet. The images in the second row are generated from text conditions (“A cheeseburger in front of a mountain range cov-ered with snow.” and “a cherry blossom tree on the blue ocean”).
The text conditions are unseen during the training. 1.

Introduction
Vector quantization (VQ) becomes a fundamental tech-nique for autoregerssive (AR) models to generate high-resolution images [5, 11, 12, 33, 40]. Speciﬁcally, an image is represented as a sequence of discrete codes, after the fea-ture map of the image is quantized by VQ and rearranged by an ordering such as raster-scan [30]. After the quantization,
AR model is trained to sequentially predict the codes in the
*Equal contribution
†Corresponding author sequence. That is, AR models can generate high-resolution images without predicting whole pixels in an image.
We postulate that reducing the sequence length of codes is important for AR modeling of images. A short sequence of codes can signiﬁcantly reduce the computational costs of an AR model, since an AR uses the codes in previous positions to predict the next code. However, previous stud-ies have a limitation to reducing the sequence length of im-ages in terms of the rate-distortion trade-off [38]. Namely,
VQ-VAE [40] requires an exponentially increasing size of
codebook to reduce the resolution of the quantized feature map, while conserving the quality of reconstructed images.
However, a huge codebook leads to the increase of model parameters and the codebook collapse problem [8], which makes the training of VQ-VAE unstable.
In this study, we propose a Residual-Quantized VAE (RQ-VAE), which uses a residual quantization (RQ) to pre-cisely approximate the feature map and reduce its spatial resolution.
Instead of increasing the codebook size, RQ uses a ﬁxed size of codebook to recursively quantize the feature map in a coarse-to-ﬁne manner. After D iterations of RQ, the feature map is represented as a stacked map of
D discrete codes. Since RQ can compose as many vectors as the codebook size to the power of D, RQ-VAE can pre-cisely approximate a feature map, while conserving the in-formation of the encoded image without a huge codebook.
Thus, RQ-VAE can further reduce the spatial resolution of the quantized feature map than previous studies [12,33,40].
For example, our RQ-VAE can use 8×8 resolution of fea-ture maps for AR modeling of 256×256 images.
In addition, We propose RQ-Transformer to predict the codes extracted by RQ-VAE. For the input of RQ-Transformer, the quantized feature map in RQ-VAE is con-verted into a sequence of feature vectors. Then, RQ-Transformer predicts the next D codes to estimate the fea-ture vector at the next position. Thanks to the reduced res-olution of feature maps by RQ-VAE, RQ-Transformer can signiﬁcantly reduce the computational costs and easily learn the long-range interactions of inputs. We also propose two training techniques for RQ-Transformer, soft labeling and stochastic sampling for the codes of RQ-VAE. They fur-ther improve the performance by resolving the exposure bias [34] in the training of RQ-Transformer. Consequently, our model can generate high-quality images in Figure 1.
Our main contributions are summarized as follows. 1) We propose RQ-VAE, which represents an image as a stacked map of discrete codes, while producing high-ﬁdelity reconstructed images. 2) We propose RQ-Transformer to effectively predict the codes of RQ-VAE and its training techniques to resolve the exposure bias. 3) We show that our approach outperforms previous AR models and signiﬁcantly improves the quality of generated images, computational costs, and sampling speed. 2.