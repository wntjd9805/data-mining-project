Abstract
Estimating the pose and shape of hands and objects un-der interaction finds numerous applications including aug-mented and virtual reality. Existing approaches for hand and object reconstruction require explicitly defined physical constraints and known objects, which limits its application domains. Our algorithm is agnostic to object models, and it learns the physical rules governing hand-object interaction.
This requires automatically inferring the shapes and physi-cal interaction of hands and (potentially unknown) objects.
We seek to approach this challenging problem by propos-ing a collaborative learning strategy where two-branches of deep networks are learning from each other. Specifically, we transfer hand mesh information to the object branch and vice versa for the hand branch. The resulting optimi-sation (training) problem can be unstable, and we address this via two strategies: (i) attention-guided graph convo-lution which helps identify and focus on mutual occlusion and (ii) unsupervised associative loss which facilitates the transfer of information between the branches. Experiments using four widely-used benchmarks show that our frame-work achieves beyond state-of-the-art accuracy in 3D pose estimation, as well as recovers dense 3D hand and object shapes. Each technical component above contributes mean-ingfully in the ablation study. 1.

Introduction
Understanding human hand and object interaction is fun-damental for meaningful interpretation of human action and behaviour [65, 72]. With the advent of deep learning and
RGB-D sensors, pose estimation of isolated hands has made significant progress, e.g., depth-based [12,69,74,81,82] and
RGB-based [51, 60, 63, 77, 85] methods. However, despite a strong link to real applications such as augmented and vir-tual reality [32, 52, 71], joint reconstruction of hand and ob-ject [33, 35] has received relatively less attention. In this paper, we focus on the problem of hand and object recon-struction from a single RGB image (see Fig. 1).
Figure 1. We propose a collaborative learning framework which al-lows sharing of mesh information across hand and object branches iteratively. Our model jointly reconstructs hand and object meshes from a monocular RGB image.
Joint hand and object pose estimation is a challenging problem. First, while self-occlusion in hand is a well-known problem [56, 80], when interacting with objects, hands (and objects) exhibit even greater occlusion from almost any point of view mutually [53]. Secondly, first-person-view (e.g., FHB [24] dataset) often exhibits large degree of er-ratic camera motion. Recent works [23, 42, 65] have been able to tackle some major challenges in joint hand-object pose estimations in colour input. However, in the absence of physical constraints, and with sparse keypoint detection, they often lead to erroneous pose estimation or mesh recon-structions (e.g. hands penetrating objects).
To fundamentally understand hand-object interactions, it is essential to fully recover 3D information, and ac-cordingly, there has been significant improvements towards hand mesh estimations from single RGB image [3, 4, 10, 19, 25, 41, 50, 83, 84, 86]. Hasson et al. [35] further pro-posed attraction and repulsion loss terms to generate physi-cally plausible reconstructions. Recent optimisation-based approaches [14, 34] that rely on these contact terms are limited to scenarios where hand and object are already in contact. However, the ability to reason pre-grasp stages are equally important as it allows robots to infer human in-tents [48] and learn manipulation skills from humans [45].
Therefore, we propose a strategy that is not restricted by these contact terms and is able to learn the context of actual as well as near physical contact.
Our novel collaborative learning framework allows hand and object branches to boost each other in a progressive and iterative fashion. There are two motivations for this strat-egy: 1) estimating the pose of interacting hands and objects is a highly-correlated task and 2) mutual occlusions can be tackled by simultaneously sharing mesh information. This is supported by the fact that the image encoder struggles to extract useful features under mutual occlusion, and there-fore capturing object mesh information would compensate this limitation for hand reconstruction (the same in object branch). Previous attempts in this context share informa-tion across branches via simple branch stacking [79] where communication bottleneck exists: We empirically observed that performance gain across network inference iterations are limited in this approach. We explicitly address this by a new unsupervised associative loss facilitating the informa-tion transfer. Further, to address frequently occurring occlu-sions in hand-object interaction scenarios, we propose an attention-guided graph convolution that can be trained in an unsupervised manner. Our graph convolution demonstrates the ability to improve mesh quality as well as correct hand and object poses.
Our contributions are the following: 1. We propose an end-to-end trainable collaborative learning strategy for hand-object reconstruction from a single RGB image. 2. We design an attention-guided graph convolution to capture mesh information dynamically. 3. We introduce an unsupervised training strategy for ef-fective feature transfer between hand-object branches. 4. We demonstrate that our model achieves highly physi-cally plausible results without contact terms.
We evaluate our method on four hand-object datasets i.e.
FHB [24], ObMan [35], HO-3D [31] and DexYCB [17] and demonstrate that our method significantly outperform state-of-the-art approaches. 2.