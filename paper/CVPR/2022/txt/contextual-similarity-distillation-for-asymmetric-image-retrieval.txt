Abstract
Asymmetric image retrieval, which typically uses small model for query side and large model for database server, is an effective solution for resource-constrained scenarios.
However, existing approaches either fail to achieve feature coherence or make strong assumptions, e.g., requiring la-beled datasets or classiﬁers from large model, etc., which limits their practical application. To this end, we propose a ﬂexible contextual similarity distillation framework to en-hance the small query model and keep its output feature compatible with that of the large gallery model, which is crucial with asymmetric retrieval.
In our approach, we learn the small model with a new contextual similarity con-sistency constraint without any data label. During the small model learning, it preserves the contextual similarity among each training image and its neighbors with the features ex-tracted by the large model. Note that this simple constraint is consistent with simultaneous ﬁrst-order feature vector preserving and second-order ranking list preserving. Ex-tensive experiments show that the proposed method outper-forms the state-of-the-art methods on the Revisited Oxford and Paris datasets. 1.

Introduction
Most existing image retrieval methods [4, 37, 41, 46, 47, 49] use the same model to map both query and gallery images to feature vectors, which is denoted as symmet-ric retrieval [6, 12]. To achieve high retrieval accuracy, they usually simply select a large model for feature extrac-tion, which suffers inefﬁciency issue.
In some practical scenarios with limited computing and memory resources, it is hardly affordable to use a such as mobile search, large model for feature extraction on the user side, and a
*Corresponding Author: Min Wang and Wengang Zhou. g : negative sample; f i
Figure 1. Illustration of existing methods for asymmetric retrieval and our contextual similarity distillation framework. fq (orange) and fg (blue) denote the embedding vectors from the lightweight query model and the large gallery model, respectively. f + g : posi-tive sample; f − g: The i-th nearest neighbor of fg. Previous methods (a) and (b) require the labels of the dataset for asymmetric distillation, e.g., AML [6] requires triplet labels and HVS [12] requires semantic category labels and the classifer from the large model. Our method (c) is free of supervision from training datasets. During knowledge transfer, it preserves the con-textual similarity between training samples and their neighbors. lightweight model is more preferable. A naive solution is to directly use lightweight models to extract features for both gallery and queries, which, however, usually degrades the retrieval accuracy due to inferior representation capability of lightweight models. In practice, gallery images can be processed ofﬂine with sufﬁcient computing resources while queries undergo feature extraction on the end-user side with limited computing power. In such an asymmetric retrieval setting [6,12], it is feasible to adopt a large model for index-ing gallery images and a lightweight one for queries, which
makes a trade-off between retrieval accuracy and efﬁciency.
Lightweight model adaptation is the core problem in asymmetric retrieval. Speciﬁcally, an optimal lightweight model is supposed to map queries into the same embed-ding space as the gallery embeddings extracted by a large one. The recent advances [6, 12, 28, 40] generally intro-duce feature compatibility restrictions into the framework of knowledge distillation and make great progress. In those approaches, they reuse the classiﬁer in the learned large model [12, 28, 40] or use large model to extract features of positive and negative samples for contrastive learning [6], which are shown in Fig. 1 (a) and (b). However, these methods assume the existence of datasets with speciﬁc la-bels when adapting lightweight models or the availability of same training set as the large model, which may be unavail-able in real retrieval scenarios. Besides, they only consider the ﬁrst-order feature preserving restrictions, but ignore the second-order neighbor relationships between images, which have been proven effective for feature learning in [29, 44].
To address above issues, we propose a ﬂexible Con-textual Similarity Distillation (CSD) framework to transfer knowledge from large gallery models to lightweight query models while keeping the feature compatibility, as shown in Fig. 1 (c). In our framework, we adopt a new contex-tual similarity consistency constraint to guide the learning of a lightweight model with a large pretrained ﬁxed model.
Specially, for each training image, we ﬁrst extract its feature using the large ﬁxed model and retrieve its neighbors as an-chors in the gallery. The cosine similarities between the training image and its neighboring anchors are used as the contextual similarity to describe the neighbor relationship.
Further, we extract the visual feature of the same training image with the lightweight model and compute its contex-tual similarity vector over the features of its neighboring anchor images, which are extracted by the large model. Fi-nally, we optimize the consistency of the contextual similar-ity between the large and lightweight models. Remarkably, the whole framework requires no supervision from training datasets during knowledge transfer.
Compared with previous approaches, our framework has two advantages. First, it takes into account contextual consistency constraint for training the lightweight model, which simultaneously optimizes the ﬁrst-order feature pre-serving and the second-order neighbor relationship preserv-ing. Second, our framework does not require any super-vision from training datasets during knowledge transfer.
Therefore, it is possible to train lightweight models using a large amount of unlabeled data, which facilitates the appli-cation of our approach in a variety of real-world scenarios.
To evaluate our approach, we conduct comprehensive ex-periments on the Revisited Oxford and Paris datasets, which are further mixed with one million distractors. Ablation studies demonstrate the effectiveness and generalizability of our framework. Our approach surpasses all state-of-the-art methods by a considerable margin. 2.