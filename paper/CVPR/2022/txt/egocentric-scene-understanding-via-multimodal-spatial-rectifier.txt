Abstract
In this paper, we study a problem of egocentric scene understanding, i.e., predicting depths and surface normals from an egocentric image. Egocentric scene understand-ing poses unprecedented challenges: (1) due to large head movements, the images are taken from non-canonical view-points (i.e., tilted images) where existing models of geom-etry prediction do not apply; (2) dynamic foreground ob-jects including hands constitute a large proportion of vi-sual scenes. These challenges limit the performance of the existing models learned from large indoor datasets, such as ScanNet [6] and NYUv2 [36], which comprise predom-inantly upright images of static scenes. We present a mul-timodal spatial rectifier that stabilizes the egocentric im-ages to a set of reference directions, which allows learning a coherent visual representation. Unlike unimodal spatial rectifier that often produces excessive perspective warp for egocentric images, the multimodal spatial rectifier learns from multiple directions that can minimize the impact of the perspective warp. To learn visual representations of the dy-namic foreground objects, we present a new dataset called
EDINA (Egocentric Depth on everyday INdoor Activities) that comprises more than 500K synchronized RGBD frames and gravity directions. Equipped with the multimodal spa-tial rectifier and the EDINA dataset, our proposed method on single-view depth and surface normal estimation sig-nificantly outperforms the baselines not only on our ED-INA dataset, but also on other popular egocentric datasets, such as First Person Hand Action (FPHA) [18] and EPIC-KITCHENS [7]. 1.

Introduction
We interact with surrounding objects in structured yet rather complex, unorganized, and dynamic environments, enabled by our robust egocentric perception that facilitates understanding 3D scene geometry around us. Such innate perceptual ability shows in stark contrast with that of ex-isting computer vision systems, trained to operate on im-ages depicting static and well-organized scenes recorded by carefully controlled cameras [6, 19, 36]. These trained models [14, 23] are, despite their remarkable performance, shown to be highly brittle when predicting the scene ge-ometry of egocentric images that observe unscripted every-day activities, including diverse hand-object interactions, captured by in situ embodied sensors such as head/body-mounted cameras [8]. This requires additional sensors such as IMU and depth sensors in augmented/mixed reality de-vices (e.g., Hololens and Magic Leap One) to deliver inter-active and immersive experiences in our daily spaces.
In this paper, we study a problem of egocentric 3D
scene understanding—predicting depths and surface nor-mals from a single view egocentric image.
In addition to challenges of classic scene understanding problems [6], egocentric scene understanding poses two more challenges: (1) Images are no longer upright. Head movements induce significant roll and pitch motions where the scene is often depicted in a tilted way. In particular, by the nature of hand-eye coordination, egocentric images inherently are affected by severe pitch motion when manipulating objects, which is substantially different from the existing data distribution, e.g., ScanNet [6], NYUv2 [36], and KITTI [19]. (2) Im-ages include not only background objects, e.g., furniture, room layout, and walls, but also dynamic foreground ob-jects, e.g., humans and arms/hands (see Figure 1). Classic scene understanding mainly focuses on reconstructing the overall geometric layout made of such background objects while the foreground ones are considered as outliers.
In contrast, these foregrounds are more salient in egocentric scenes as they are highly indicative of evolving activities.
We conjecture that the challenges of egocentric scene understanding can be addressed by an image stabilization method that incorporates the fundamentals of equivariance, called spatial rectifier [8]—an image warping that trans-forms a titled image to a canonical orientation (i.e., gravity-aligned) such that a prediction model can learn from the upright images. This is analogous to our robust perception through mental stabilization of visual stimuli [54]. How-ever, the spatial rectifier shows inferior performance on pre-dicting 3D geometry of egocentric images that involve sub-stantial head movement (e.g., nearly 90 degree pitch), lead-ing to excessive perspective warps. We present a multi-modal spatial rectifier by generalizing the canonical direc-tion, i.e., instead of unimodal gravity-aligned direction, we learn multiple reference directions from the orientations of the egocentric images, which allows minimizing the impact of excessive perspective warping. Our multimodal spatial rectifier makes use the clusters of egocentric images based on the distribution of surface normals into multiple pitch modes, where we learn a geometric predictor (surface nor-mals or depths) that is specialized for each mode to rectify associated roll angles.
To facilitate learning the visual representation of dy-namic egocentric scenes, we present a new dataset called
EDINA (Egocentric Depth on everyday INdoor Activities).
Our dataset comprises 16 hours RGBD recording of indoor activities including cleaning, cooking, eating, and shopping.
Our dataset provides a synchronized RGB, depth, surface normal, and the 3D gravity direction to train our multimodal spatial rectifier and geometry prediction models. Our depth and surface normal predictors learned from the EDINA out-perform the baseline predictors not only on EDINA dataset but also other datasets, such as EPIC-KITCHENS [7] and
First Person Hand Action (FPHA) [18].
Our contributions include: (1) a multimodal spatial recti-fier; (2) a large dataset of egocentric RGBD with the gravity that is designed to study egocentric scene understanding, by capturing diverse daily activities in the presence of dynamic foreground objects; (3) comprehensive experiments to high-light the effectiveness of our multimodal spatial rectifier and our EDINA dataset towards depth and surface normal pre-diction on egocentric scenes. 2.