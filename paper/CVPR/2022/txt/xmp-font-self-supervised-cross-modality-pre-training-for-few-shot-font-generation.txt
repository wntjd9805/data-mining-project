Abstract
Generating a new font library is a very labor-intensive and time-consuming job for glyph-rich scripts. Few-shot font generation is thus required, as it requires only a few glyph references without ﬁne-tuning during test. Existing methods follow the style-content disentanglement paradigm and expect novel fonts to be produced by combining the style codes of the reference glyphs and the content representa-tions of the source. However, these few-shot font genera-tion methods either fail to capture content-independent style representations, or employ localized component-wise style representations, which is insufﬁcient to model many Chi-nese font styles that involve hyper-component features such as inter-component spacing and “connected-stroke”. To re-solve these drawbacks and make the style representations more reliable, we propose a self-supervised cross-modality pre-training strategy and a cross-modality transformer-based encoder that is conditioned jointly on the glyph image
*These authors contributed equally and should be considered co-ﬁrst authors.
†Corresponding author. and the corresponding stroke labels. The cross-modality encoder is pre-trained in a self-supervised manner to al-low effective capture of cross- and intra-modality corre-lations, which facilitates the content-style disentanglement and modeling style representations of all scales (stroke-level, component-level and character-level).
The pre-trained encoder is then applied to the downstream font gen-eration task without ﬁne-tuning. Experimental comparisons of our method with state-of-the-art methods demonstrate our method successfully transfers styles of all scales.
In addition, it only requires one reference glyph and achieves the lowest rate of bad cases in the few-shot font generation task (28% lower than the second best). 1.

Introduction
The few-shot font generation task (FFG) aims to produce a new font library using only a few glyphs as reference, without additional ﬁne-tuning of the model at the testing stage. FFG is especially a desirable task when designing a new font library for glyph-rich scripts such as Chinese
(the total number of characters exceeds 80,000), as the tra-ditional manual font design process is very laborious. FFG is also desired when the target style glyphs are too rare to collect (e.g., historical handwriting).
Since font styles are highly complex and ﬁne-grained, a simple analysis of the low-level textures of a few reference examples is impossible to perform successful style trans-fer as in [11, 16, 24, 25, 28]. A common paradigm used for
FFG is to disentangle font-speciﬁc style and content infor-mation from the given glyphs, and synthesize a new glyph by combining the style embeddings extracted from the ref-erence set and the content representations of the source glyph [2,3,9,22,29,30,33,35,41,45]. Early attempts of this stream [9,45] employ the universal style representations, us-ing a simple convolutional encoder to extract style embed-dings directly from the reference glyph images. However, the universal style representations show limited capabilities in capturing reliable and content-independent style repre-sentations due to limited awareness of the character struc-tures and correlations between different regions of the input glyph. More advanced architectures such as DM-Font [3],
LF-Font [29], MX-Font [30] propose to use structure-aware style representations and learn the localized component-wise style representations.
To make the localized style representation possible, these methods either condition the style encoders jointly upon the glyph image and the corresponding component labels or in-troduce component-label-guided losses to train the style en-coder. The structure-aware localized style representations remarkably improve the reliability of the style representa-tions. However, as mentioned in [29], learning component-wise styles solely is insufﬁcient for component-rich glyphs like Chinese characters that have over 200 different types of components. It is hard to cover all component types with a few reference glyphs during test. To relieve this prob-lem, LF-Font [29] simpliﬁes the component-wise styles by a product of component factor and style factor, inspired by low-rank matrix factorization. MX-Font [30] extracts mul-tiple style features not explicitly conditioned on component labels, but automatically by multiple experts to represent different local concepts, thus enabling the model to be gen-eralized to a character with unseen components.
Such solutions relieve the “unseen components” issue to some extent. However, they are prone to generating bad cases when failing to generalize the unseen compo-nent styles from seen components. On the other hand, component-wise style representations are incapable of cap-turing character-level style features (e.g., inter-component spacing), which is an important perspective in many Chi-nese font libraries: see Figure 1 (a) for the Chinese font styles of all three scales.
To address these issues, we make two signiﬁcant changes. First, we employ the stroke labels rather than the component labels as the atomic representation of charac-ter structure, as the stroke set used in Chinese is signif-icantly fewer (about 28) than that of the component set (more than 200), which can be easier to cover with a few reference glyphs or generalized from seen strokes. On the other hand, to enhance the awareness of the stroke-level styles while not losing component-level or character-level style features, we propose to use the uniﬁed all-scale style representations instead of the localized component-or stroke-wise styles. This can be achieved by introduc-ing a cross-modality transformer-based encoder that is con-ditioned jointly on the glyph image and the corresponding stroke labels. On one hand, the self-attention layers used in the encoder is good at capturing both local and global style features. On the other hand, the self-supervised pre-training of the cross-modality encoder inspires the learning the glyph-stroke alignments, which further facilitates the content-style disentanglement and modeling of style rep-resentations at multiple scales in the downstream training phase.
In addition to the cross-modality pre-training mecha-nism, we propose a LSTM-based stroke loss and a style-content decoupling network which considers spatial infor-mation conservation, to enhance the reliability of the model further. Comprehensive analyses of the experimental results demonstrate our method achieves signiﬁcantly lower rate of bad cases than prior FFG methods and it can successfully generate novel glyphs based on only one reference exam-ple.
To sum up, the major contributions of the paper include:
• For the ﬁrst time, we introduce the cross-modality transformer-based encoder and the mechanism of cross-modality pre-training to the FFG task. The self- and cross-attention layers in the transformer-based encoder pre-trained with self-supervised signals help capture local and global style features (stroke-level, component-level and character-level features) and learn the glyph-stroke alignments, thus enhancing the structure-awareness of style representations and facilitating the style-content disentanglement in the downstream FFG task.
• We elaborate a style-content decoupling network com-posed of Efﬁcient Channel Attention (ECA) modules
[31], and employ an 8×8 feature map instead of a sim-ple average-pooled vector to represent styles or con-tents with the expect to conserve spatial information, which prove to be effective in increasing the reliability of the model.
• We also propose a novel stroke loss based on a pre-trained LSTM-based stroke order predictor, to enforce the correct stroke order of the generated glyph in-stead of the existence of stroke labels only, which
proves to beneﬁt the structure preservation and faithful generation of stroke-order-related style features (e.g.,
“connected-stroke”).
• Experimental results see powerful generalizability of our model to unseen font domains. Our model can per-form successful font style transfer with only one refer-ence glyph. consists of three encoders: a glyph processing module, a stroke processing encoder, and a cross-modality module.
Next, to endow our model with the capability of connect-ing a glyph image and its related stroke labels, we pre-train the model with large amounts of glyph-stroke pairs, via self-supervised signals (reconstruction of the input data).
This task helps in learning both intra-modality and cross-modality relationships. 2.