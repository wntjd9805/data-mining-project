Abstract
Volumetric neural rendering methods, such as neural ra-diance fields (NeRFs), have enabled photo-realistic novel view synthesis. However, in their standard form, NeRFs do not support the editing of objects, such as a human head, within a scene. In this work, we propose RigNeRF, a system that goes beyond just novel view synthesis and enables full control of head pose and facial expressions learned from a single portrait video. We model changes in head pose and facial expressions using a deformation field that is guided by a 3D morphable face model (3DMM). The 3DMM ef-fectively acts as a prior for RigNeRF that learns to predict only residuals to the 3DMM deformations and allows us to render novel (rigid) poses and (non-rigid) expressions that were not present in the input sequence. Using only a smartphone-captured short video of a subject for training, we demonstrate the effectiveness of our method on free view synthesis of a portrait scene with explicit head pose and expression controls. 1.

Introduction
Photo-realistic editing of human portraits is a long-standing topic in the computer graphics and computer vision community. It is desirable to be able to control certain at-tributes of a portrait, such as 3D viewpoint, lighting, head pose, and even facial expression, after capturing. It also has great potential in AR/VR applications where a 3D immersive experience is valuable. However, it is a challenging task: modeling and rendering a realistic human portrait with com-plete control over 3D viewpoint, facial expressions, and head pose in natural scenes remains elusive, despite the longtime interest and recently increased research. 3D Morphable Face Models (3DMMs) [4] were among the earliest attempts towards a fully controllable 3D human
*Work done while interning at Adobe Research.
head model. 3DMMs use a PCA-based linear subspace to control face shape, facial expressions, and appearance in-dependently. A face model of desired properties can be rendered in any view using standard graphics-based render-ing techniques such as rasterization or ray-tracing. However, directly rendering 3DMMs [4], which only models face re-gion, is not ideal for photo-realistic applications as its lacks essential elements of the human head such as hair, skin de-tails, and accessories such as glasses. Therefore, it is better employed as an intermediate 3D representation [21, 47, 48] due to its natural disentanglement of face attributes such as shape, texture, and expression, which makes 3DMMs an ap-pealing representation for gaining control of face synthesis.
On the other hand, recent advances on neural rendering and novel view synthesis [3, 6, 13, 14, 28, 29, 32, 33, 35, 39, 50, 52,53] have demonstrated impressive image-based rendering of complex scenes and objects. Despite that, existing works are unable to simultaneously generate high quality novel views of a given natural scene and control the objects within it, including that of the human face and its various attributes.
In this work, we would like to introduce a system to model a fully controllable portrait scene: with camera view control, head pose control, as well as facial expression control.
Control of head-pose and facial expressions can be en-abled in NeRFs via a deformation module as done in
[35, 36, 39]. However, since those deformations are learnt in a latent space, they cannot be explicitly controlled. A natural way add control to head-pose and facial expressions, via de-formations, is by parameterizing the deformation field using the 3DMM head-pose and facial expression space. However, as shown in Fig 7, such naive implementation of a deforma-tion field leads to artefacts during the reanimation due to the loss of rigidity and incorrect modelling of facial expressions.
To address these issue, we introduce RigNeRF, a method that leverages a 3DMM to generate a coarse deformation field which is then refined by corrective residual predicted by an
MLP to account for the non-rigid dynamics, hair and acces-sories. Beyond giving us a controllable deformation field, the 3DMM acts as an inductive bias allowing our network to generalize to novel head poses and expressions that were not observed in the input video.
Our model is designed to be trained on a short video captured using a mobile device. Once trained, RigNeRF allows for explicit control of head pose, facial expression and camera viewpoint. Our results capture rich details of the scene along with details of the human head such as the hair, beard, teeth and accessories. Videos reanimated using our method maintain high fidelity to both the driving morphable model in terms of facial expression and head-pose and the original captured scene and human head.
In summary, our contributions in this paper are as follows: 1) We propose a neural radiance field capable of full control of the human head along with simultaneously modelling the full 3D scene it is in. 2) We experimentally demonstrate the loss of rigidity when dynamic neural radiance fields are reanimated. 3) We introduce a deformation prior that ensures rigidity of the human head during reanimation thus significantly improves its quality. 2.