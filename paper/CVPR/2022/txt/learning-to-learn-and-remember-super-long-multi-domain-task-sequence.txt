Abstract
Catastrophic forgetting (CF) frequently occurs when learning with non-stationary data distribution. The CF issue remains nearly unexplored and is more challenging when meta-learning on a sequence of domains (datasets), called sequential domain meta-learning (SDML). In this work, we propose a simple yet effective learning to learn approach, i.e., meta optimizer, to mitigate the CF problem in SDML. We first apply the proposed meta optimizer to the simplified setting of
SDML, domain-aware meta-learning, where the domain la-bels and boundaries are known during the learning process.
We propose dynamically freezing the network and incorpo-rating it with the proposed meta optimizer by considering the domain nature during meta training. In addition, we extend the meta optimizer to the more general setting of SDML, domain-agnostic meta-learning, where domain labels and boundaries are unknown during the learning process. We propose a domain shift detection technique to capture latent domain change and equip the meta optimizer with it to work in this setting. The proposed meta optimizer is versatile and can be easily integrated with several existing meta-learning algorithms. Finally, we construct a challenging and large-scale benchmark consisting of 10 heterogeneous domains with a super long task sequence consisting of 100K tasks. We perform extensive experiments on the proposed benchmark for both settings and demonstrate the effectiveness of our proposed method, outperforming current strong baselines by a large margin. 1.

Introduction
Catastrophic forgetting (CF) [47] frequently occurs when learning with data distribution shift. The CF issue is largely overlooked in the more challenging problem setting, i.e., meta-learning on a sequence of domains, where domain shift occurs sequentially when the model meta-learns on a large number of tasks and aims to generalize to the unseen tasks from previous domains. This has significant implications for real-world applications, for example:
• Robot learns on many visual recognition tasks, where each task may consist of only a small number of labeled image data. It may sequentially go through numerous environ-ments as illustrated in Fig. 1. When adapting to a new environment, the skills learned in previous environments may be easily forgotten.
• For a personalized dialogue/recommendation system [44, 50], where learning the personal model for each user is viewed as an individual task, the user base may shift over time, e.g., the system is first deployed for Canadian users, then the company extends its market to Europe. While learning about European users, the system may quickly forget previous Canadian users’ habits.
We generalize and formulate the above problem setting as sequential domain meta-learning (SDML), where a model is required to make proper decisions based on only a few train-ing examples with the underlying environments/domains constantly changing. Recent work reveals that catastrophic forgetting often occurs when transferring a meta-learning model to a new context [55, 79]. We expect that adjustments to a new environment/domain should not erase the learned knowledge from old ones. On the other hand, most existing works of continual learning [58, 61] can only mitigate the forgetting on a short sequence of (typically less than 50) tasks. These continual learning methods are infeasible to be directly applied in SDML with a super long task sequence consisting of (at least) 100K tasks, which is our main focus.
We propose to learn a meta optimizer to mitigate the catastrophic forgetting issue during the learning process. In-tuitively, more important parameters for previous domains should be updated more slowly to avoid forgetting and less important parameters could be updated faster for efficient learning of the current domain. To achieve this goal, we store a small number of tasks in a memory buffer and cal-culate the gradient of the meta loss for the memory tasks with respect to the learnable learning rates at each iteration.
Figure 1. Demonstration of SDML learning scenario
The gradient corresponds to the degree of catastrophic inter-ference between current tasks and previous memory tasks.
The meta optimizer dynamically adjusts the learning rates according to this gradient. Next, we apply the proposed optimizer to the simplified setting of SDML, domain-aware meta-learning, where the domain labels and boundaries are known during the learning process. To incorporate the fact of heterogeneous domain nature (where different domains do not share categories) in SDML, we propose to dynami-cally freeze the network and integrate it with the proposed meta optimizer during meta training. In addition, we extend the meta optimizer to the more general setting of SDML, domain-agnostic meta-learning, where domain labels and boundaries are unknown during the learning process. We propose a domain shift detection technique to capture latent domain change and equip the meta optimizer with it.
Most existing meta-learning benchmarks are designed for the stationary setting and are not suitable for evaluating the
CF issue in SDML. To evaluate the proposed methods, we construct a large-scale and challenging dataset consisting of a sequence of 10 heterogeneous domains for the SDML setting. We integrate the proposed methods with both rep-resentative metric-based and gradient-based meta-learning approaches. Results on both domain-aware and domain-agnostic meta-learning demonstrate that our method signifi-cantly outperforms related strong baselines by a large margin.
Our contributions can be summarized as the following:
• To our best knowledge, we are the first to tackle the CF issue when learning on a super long task sequence of at least 100K tasks with sequential domain shift.
• We propose a meta optimizer to address the catastrophic forgetting issue of SDML, a more challenging problem than existing continual learning methods trying to address.
• We apply the proposed meta optimizer to the domain-aware and domain-agnostic meta-learning setting of
SDML. The proposed method is versatile and can be eas-ily integrated into both metric-based and gradient-based meta-learning approaches.
• To verify the effectiveness of the proposed method, we con-struct a challenging and large-scale dataset consisting of 10 heterogeneous domains. Comprehensive experiments demonstrate that our method outperforms related strong baselines by a large margin. 2.