Abstract
Image outpainting seeks for a semantically consistent extension of the input image beyond its available content.
Compared to inpainting — ﬁlling in missing pixels in a way coherent with the neighboring pixels — outpainting can be achieved in more diverse ways since the problem is less constrained by the surrounding pixels. Existing im-age outpainting methods pose the problem as a conditional image-to-image translation task, often generating repetitive structures and textures by replicating the content available in the input image.
In this work, we formulate the prob-lem from the perspective of inverting generative adversar-ial networks. Our generator renders micro-patches condi-tioned on their joint latent code as well as their individual positions in the image. To outpaint an image, we seek for multiple latent codes not only recovering available patches but also synthesizing diverse outpainting by patch-based generation. This leads to richer structure and content in the outpainted regions. Furthermore, our formulation al-lows for outpainting conditioned on the categorical input, thereby enabling ﬂexible user controls. Extensive experi-mental results demonstrate the proposed method performs favorably against existing in- and outpainting methods, fea-turing higher visual quality and diversity.
1.

Introduction
Given an input image, we can easily picture how adjacent images might look. For example, given an image of moun-tains, we can picture the surroundings covered by forests or snow, imagine a lake beneath the hillside, and visualize cliffs near the ocean. This mental skill depends on our prior experience and exposure to diverse scenery. In other words, this is an image outpainting task. It enableS various content creation applications such as image editing using extrap-olated regions, panorama image generation, and extended immersive experience in virtual reality, to name a few.
Recent advances in image inpainting [20, 23, 33, 34] do not directly address the outpainting problem as the former has more context to deal with — the missing pixels have a larger amount of available surrounding pixels, serving as the boundary conditions and providing crucial guidance for in-painting. In contrast, the outpainting problem can rely only on the context of the available image, with only a scarce number of pixels near the boundary available as the bound-ary condition. A similar analogy is between video inter-polation and video prediction, where the former deals with existing events while the latter tries to model multiple fu-tures.
In the literature, image outpainting is addressed from the image-to-image translation (I2I) perspective [27,31]. These methods aim to learn a deterministic mapping from the do-main of partial images to the domain of complete outpainted images. This formulation is limited in several respects.
First, the available pixels serve as a strong source of context, thereby facilitating leakage of textures and structures of the input to the output and leading to the repetitive nature of the outpainting (as shown in panorama results in [27]). Second, existing I2I-based methods are deterministic, while in real-ity there exist numerous ways each image can be outpainted.
Applying the available multimodal I2I methods [13, 17] to the outpainting problem is non-trivial.
In this work, we tackle the outpainting problem by in-verting generative adversarial networks (GANs) [1,4,7,37].
We ﬁrst extend a StyleGAN2-based [15] generator to per-form generation in a coordinate conditional manner and independently generate spatially consistent micro-patches.
Each micro-patch shares the global latent code with the rest of micro-patches in the image, while having a unique co-ordinate label. Outpainting can then be formulated as ﬁnd-ing the optimal latent codes for the available input micro-patches, followed by generating the desired regions by pro-viding the proper coordinate conditioning. To search for the latent code, we propose a GAN inversion process that
ﬁnds multiple latent codes producing diverse outpainted re-gions, unlocking diversity in the output.
In addition, we propose a categorical generation schema to enable ﬂexible user control. Figure 1 shows examples of multi-modal and categorical outpainting.
We qualitatively and quantitatively evaluate the proposed method on the Place365 [36] dataset, and the Flickr-Scenery dataset which we collected. We use Fr´echet Inception Dis-tance (FID) [12] and conduct a user study to evaluate the realism of outpainted images. Since the proposed method can achieve multi-modal generation, we measure the diver-sity using the Learned Perceptual Image Patch Similarity (LPIPS) metric [35]. Finally, we demonstrate the scenario of categorical generation in the outpainting area and the panorama generation. 2.