Abstract
We introduce DiffPhy, a differentiable physics-based model for articulated 3d human motion reconstruction from video. Applications of physics-based reasoning in human motion analysis have so far been limited, both by the com-plexity of constructing adequate physical models of artic-ulated human motion, and by the formidable challenges of performing stable and efficient inference with physics in the loop. We jointly address such modeling and infer-ence challenges by proposing an approach that combines a physically plausible body representation with anatomi-cal joint limits, a differentiable physics simulator, and opti-mization techniques that ensure good performance and ro-bustness to suboptimal local optima.
In contrast to sev-eral recent methods [39, 42, 55], our approach readily sup-ports full-body contact including interactions with objects in the scene. Most importantly, our model connects end-to-end with images, thus supporting direct gradient-based physics optimization by means of image-based loss func-tions. We validate the model by demonstrating that it can accurately reconstruct physically plausible 3d human mo-tion from monocular video, both on public benchmarks with available 3d ground-truth, and on videos from the internet. 1.

Introduction
We seek to contribute to the development of physics-based methodology as one of the building blocks in con-structing accurate and robust 3d visual human sensing sys-tems. Incorporating the laws of physics into the visual rea-soning process is appealing as it promotes the plausibility of estimated motion and facilitates more efficient use of train-ing examples [9]. We focus on articulated human motion as an epitome of a real-world prediction task that is both well studied and challenging. Existing state-of-the-art ap-proaches demonstrate relatively high accuracy in terms of joint position estimation metrics [23, 24, 54, 62]. However, predictions can sometimes be physically implausible, even for simple motions such as walking and running. For in-stance, estimates can include unreasonably abrupt transi-tions in world space, or artifacts such as foot skating or non-equilibrium states [39,42]. Many methods are typically trained on large motion capture datasets and encounter diffi-culties when tested on motions not well represented in those training sets. Arguably, imposing some form of physics-based generally valid prior on the articulated motion esti-mates should greatly improve the plausibility of results.
However, physics-based reasoning comes at the cost of substantial modeling and inference complexity. Typically, physics-based articulated estimation methods rely on rigid body dynamics (RBD) [10, 44], a formulation that intro-duces many auxiliary variables corresponding to forces act-ing at the body joints at each time step. Moreover, physical contact results in non-smooth effects where small changes to model parameters might result in substantially different motions. Therefore inferring physics variables given the inherent uncertainty in monocular video, and under con-tact discontinuities, becomes significantly difficult, algo-rithmically and computationally. Despite such challenges, a number of recent methods successfully apply physics-based constraints for articulated human motion estimation
[2, 39, 42, 59]. One possibility to cope with modeling com-plexity, explored in recent work, is to simplify the physics and model contacts only between the body and the feet
[39, 42, 55]. Others use auxiliary external forces applied at the body to compensate for modeling error [42, 59].
In this paper, we aim to broaden the methodology for physics-based articulated human motion estimation.
Specifically, we demonstrate that we can successfully lever-age recent progress in differentiable simulation [17, 19, 52] in order to incorporate physics-based constraints into the ar-ticulated 3d human motion reconstruction. Our approach,
DiffPhy, relies on gradient-based optimization, connects end-to-end with images, and does not require simplifying assumptions on contacts or the introduction of external non-physical residual forces.
Figure 1. Overview of DiffPhy. Given kinematic estimates (described in §3.1) of a subject’s body shape β, the body’s initial pose and velocity s0, and time-varying 3d poses ¯q0:T with detected 2d keypoints, our model reconstructs the motion in physical simulation, by minimizing a differentiable loss L (see §3.5). DiffPhy optimizes the control trajectory ˆq0:T containing joint angle targets to PD controllers (cf . (4)). In turn, the PD controllers compute a torque vector τ , which actuates motors in the joints of the simulated body. DiffPhy integrates a full-featured differentiable simulator, TDS [17] (described in §3.2), that supports complex contacts. Each subject is represented by means of a personalised physical model (see §3.3). In addition, we optimize the initial state (see §3.6), which makes DiffPhy robust to low quality initial estimates. The outputs are 3d pose estimates that align with visual evidence and respect physical constraints. 2.