Abstract 1.

Introduction
A common practice in transfer learning is to initialize the downstream model weights by pre-training on a data-In object detection specifically, abundant upstream task. the feature backbone is typically initialized with ImageNet classifier weights and fine-tuned on the object detection task. Recent works show this is not strictly necessary un-der longer training regimes and provide recipes for train-ing the backbone from scratch. We investigate the oppo-site direction of this end-to-end training trend: we show that an extreme form of knowledge preservation—freezing the classifier-initialized backbone— consistently improves many different detection models, and leads to considerable resource savings. We hypothesize and corroborate experi-mentally that the remaining detector components capacity and structure is a crucial factor in leveraging the frozen backbone. Immediate applications of our findings include performance improvements on hard cases like detection of long-tail object classes and computational and memory re-source savings that contribute to making the field more ac-cessible to researchers with access to fewer computational resources.
Transfer learning [35] is a widely adopted practice in deep learning, especially when the target task has a smaller dataset; the model is first pre-trained on an upstream task in which a larger amount data is available and then fine-tuned on the target task. Transfer learning from ImageNet or even larger [5, 16, 47] or weakly labeled [33] datasets was repeatedly shown to yield performance improvements across various vision tasks, architectures, and training pro-cedures [23, 33, 47].
For object detection [20] it is common practice to ini-tialize the model’s backbone (see Figure 2 for a model di-agram) with weight values obtained by pretraining on an image classification task, such as ImageNet [44]. Tradition-ally, the backbone is fine-tuned while training the other de-tector components from scratch. Two lines of work have re-cently made seemingly contradictory observations on trans-fer learning for object detection. On one hand, Sun et al. [47] show that object detectors benefit from the amount of classification data used in pre-training. On the other hand, more recent papers have reported that the perfor-mance gap between transferring from a pre-trained back-bone initialization and training the backbone from scratch with smaller, in-domain datasets vanishes with longer train-ing [8, 14, 26, 45].
annotations available. Our results show that our extreme formulation of model reuse has a clear positive impact on classes with a low number of annotations, such as those found in long-tail object recognition. 2.