Abstract 1.

Introduction
Gradient based meta-learning methods are prone to overfit on the meta-training set, and this behaviour is more prominent with large and complex networks. Moreover, large networks restrict the application of meta-learning models on low-power edge devices. While choosing smaller networks avoid these issues to a certain extent, it affects the overall generalization leading to reduced performance.
Clearly, there is an approximately optimal choice of net-work architecture that is best suited for every meta-learning problem, however, identifying it beforehand is not straight-forward. In this paper, we present METADOCK, a task-specific dynamic kernel selection strategy for designing compressed CNN models that generalize well on unseen tasks in meta-learning. Our method is based on the hypoth-esis that for a given set of similar tasks, not all kernels of the network are needed by each individual task. Rather, each task uses only a fraction of the kernels, and the se-lection of the kernels per task can be learnt dynamically as a part of the inner update steps. METADOCK com-presses the meta-model as well as the task-specific inner models, thus providing significant reduction in model size for each task, and through constraining the number of ac-tive kernels for every task, it implicitly mitigates the issue of meta-overfitting. We show that for the same inference budget, pruned versions of large CNN models obtained us-ing our approach consistently outperform the conventional choices of CNN models. METADOCK couples well with popular meta-learning approaches such as iMAML [22].
The efficacy of our method is validated on CIFAR-fs [1] and mini-ImageNet [28] datasets, and we have observed that our approach can provide improvements in model accuracy of up to 2% on standard meta-learning benchmark, while reducing the model size by more than 75%. Our code is available at https://github.com/transmuteAI/
MetaDOCK.
*indicates equal contribution. Arnav Chavan is the corresponding au-thor.
An important characteristic desired by any artificial in-telligence (AI) system is the ability to solve tasks under several different conditions, and that it can adapt quickly in unseen environments with the help of limited data. While the field of deep learning has progressed significantly with the development of several efficient algorithms, it is well known that the standard learning methods tend to perform well when training is done with the millions of data points and break down when the statistics of the inference data differs from that of the training set. Meta-learning tries to solve this problem by learning-to-learn the model weights over different episodes from a family of tasks with limited data. This learning process helps the models to generalize well and adapt quickly over unseen tasks with the help of few examples. Such setting has many practical advantages in vision and reinforcement learning problems like few-shot image classification [3, 11, 22, 28], navigation [30], domain adaptation [5] to name a few.
The key idea behind these kind of meta-learning ap-proaches is to learn generalized weights which can be mod-ified easily for a newer task. Gradient based approaches tend to face the problem of overfitting to a greater ex-tent than other methods. The two kind of overfitting ob-served in them are - 1) Inner-task overfitting, which refers to task-specific overfitting of the meta-model, an extensive research has been done on this problem [13, 33] as it com-monly seen in all the deep learning approaches and several methods have been proposed to counter this, for example, inner-regularization, dropout, weight decay, learning rate scheduling, etc. 2) Inter-task overfitting or meta-overfitting, that is overfitting of meta-model on seen tasks and failure to generalize well on unseen tasks, the study on this problem is limited, few of the common approaches are adding im-plicit regularizations [22], choosing larger CNN networks to increase learning capacity [13], using initialization tech-niques to improve generalisation [25].
In this paper, we propose to improve the generaliza-tion of gradient based meta-learning models as well as
the associated memory efficiency through a pruning-in-learning strategy. We present Meta-Learning with Dynamic
Optimization of CNN Kernels (METADOCK), a gradient-based dynamic learning scheme to identify the optimal set of kernels for each meta-learning task. METADOCK is based on the hypothesis that each task needs only a small subset of kernels from the complete set of kernels that exist in the traditional meta-learning models, and using exces-sive kernels could potentially cause meta-overfitting. This reduction of the number of kernels per task breaks down further into two parts: reducing the kernels in the final meta-model and learning to further optimize the choice at the task-level during the inner update steps. METADOCK uses the gradient information accumulated during the inner update steps to activate/deactivate the kernels in the meta-model.
The major contributions of METADOCK are are fol-lows:
• We demonstrate that meta-learning models can be made to generalize better on unseen tasks through ef-ficient pruning of the excessive and undesired parts of the network at the meta-level as well as for each task.
• Our METADOCK strategy dynamically identifies the right set of kernels that are to be retained for each task, and discards the rest. This helps to avoid overfit-ting and improves the reliability of the meta-learning methods.
• Through pruning the meta-model as well as the task-specific models, METADOCK reduces the size of the model significantly. The resultant smaller meta-models are better suited for deployment on low-power devices and improve the inference efficiency of the model.
• We demonstrate through successful integration of
METADOCK with popular meta-learning approach: iMAML wherein METADOCK improves the perfor-mance on unseen tasks on benchmark datasets. 2.