Abstract
Action detection is a significant and challenging task, es-pecially in densely-labelled datasets of untrimmed videos.
Such data consist of complex temporal relations including composite or co-occurring actions. To detect actions in these complex settings, it is critical to capture both short-term and long-term temporal information efficiently. To this end, we propose a novel ‘ConvTransformer’ network for action detection: MS-TCT1. This network comprises of three main components: (1) a Temporal Encoder mod-ule which explores global and local temporal relations at multiple temporal resolutions, (2) a Temporal Scale Mixer module which effectively fuses multi-scale features, creat-ing a unified feature representation, and (3) a Classification module which learns a center-relative position of each ac-tion instance in time, and predicts frame-level classification scores. Our experimental results on multiple challenging datasets such as Charades, TSU and MultiTHUMOS, val-idate the effectiveness of the proposed method, which out-performs the state-of-the-art methods on all three datasets. 1.

Introduction
Action detection is a well-known problem in computer vision, which is aimed towards finding precise temporal boundaries among actions occurring in untrimmed videos.
It aligns well with real-world settings, because every minute of a video is potentially filled with multiple actions to be de-tected and labelled. There are public datasets [10, 41, 51] which provide dense annotations to tackle this problem, having an action distribution similar to the real-world.
However, such data can be challenging, with multiple ac-tions occurring concurrently over different time spans, and having limited background information. Therefore, under-standing both short-term and long-term temporal dependen-cies among actions is critical for making good predictions.
For instance, the action of ‘taking food’ (see Fig. 1) can get context information from ‘opening fridge’ and ‘making sandwich’, which correspond to the short-term and long-1Code/ Models: https://github.com/dairui01/MS-TCT
Figure 1. Complex temporal relations in untrimmed videos:
Here, we show a typical distribution of actions in a densely-labelled video, which consists of both long-term and short-term dependencies among actions. term action dependencies, respectively. Also, the occur-rence of ‘putting something on the table’ and ‘making sand-wich’ provide contextual information to detect the compos-ite action ‘cooking’. This example shows the need for an effective temporal modeling technique for detecting actions in a densely-labelled videos.
Towards modeling temporal relations in untrimmed videos, multiple previous methods [8, 9, 11, 12, 30, 38] use 1D temporal convolutions [30]. However, limited by their kernel size, convolution-based methods can directly access local information only, not learning direct relations between temporally-distant segments in a video (here, we consider a set of consecutive frames as a segment). Thus, such meth-ods fail to model long-range interactions between segments which may be important for action detection. With the suc-cess of Transformers [16, 34, 44, 56] in natural language processing and more recently in computer vision, recent methods [42, 43] have leveraged multi-head self-attention (MHSA) to model long-term relations in videos for ac-tion detection. Such attention mechanisms can build di-rect one-to-one global relationships between each temporal segment (i.e., temporal token) of a video to detect highly-correlated and composite actions. However, existing meth-ods rely on modeling such long-term relationships on input frames themselves. Here, a temporal token covers only a few frames, which is often too short w.r.t. to the duration
of action instances. Also, in this setting, transformers need to explicitly learn strong relationships between adjacent to-kens which arise due to temporal consistency, whereas it comes naturally for temporal convolutions (i.e., local induc-tive bias). Therefore, a pure transformer architecture may not be sufficient to model complex temporal dependencies for action detection.
To this end, we propose Multi-Scale Temporal ConvTrans-former (MS-TCT), a model which benefits from both convolutions and self-attention. We use convolutions in a token-based architecture to promote multiple temporal scales of tokens, and to blend neighboring tokens impos-ing a temporal consistency with ease. In fact, MS-TCT is built on top of temporal segments encoded using a 3D con-volutional backbone [4]. Each temporal segment is consid-ered as a single input token to MS-TCT, to be processed in multiple stages with different temporal scales. These scales are determined by the size of the temporal segment, which is considered as a single token at the input of each stage. Having different scales allows MS-TCT to learn both fine-grained relations between atomic actions (e.g. ‘open fridge’) in the early stages, and coarse relations between composite actions (e.g. ‘cooking’) in the latter stages. To be more specific, each stage consists of a temporal convo-lution layer for merging tokens, followed by a set of multi-head self-attention layers and temporal convolution layers, which model global temporal relations and infuse local in-formation among tokens, respectively. As convolution in-troduces an inductive bias [15], the use of temporal con-volution layers in MS-TCT can infuse positional informa-tion related to tokens [21, 23], even without having any po-sitional embeddings, unlike pure transformers [16]. Fol-lowed by the modeling of temporal relations at different scales, a mixer module is used to fuse the features from each stages to get a unified feature representation. Finally, to pre-dict densely-distributed actions, we introduce a heat-map branch in MS-TCT in addition to the usual multi-label clas-sification branch. This heat-map encourages the network to predict the relative temporal position of instances of each action class. Fig. 2 shows the relative temporal position, which is computed based on a Gaussian filter parameter-ized by the instance center and its duration. It represents the relative temporal position w.r.t. to the action instance center at any given time. With this new branch, MS-TCT can embed a class-wise relative temporal position in token representations, encouraging discriminative token classifi-cation in complex videos.
To summarize, the main contributions of this work are to (1) propose an effective and efficient ConvTransformer for modeling complex temporal relations in untrimmed videos, (2) introduce a new branch to learn the position rel-ative to instance-center, which promotes action detection in densely-labelled videos, and (3) improve the state-of-the-art on three challenging densely-labelled action datasets.
Figure 2. Relative temporal position heat-map (G∗): We present a video clip which contains two overlapping action in-stances. The Gaussians indicate the intensities of temporal heat-maps, which are centered at the mid point of each action, in time. 2.