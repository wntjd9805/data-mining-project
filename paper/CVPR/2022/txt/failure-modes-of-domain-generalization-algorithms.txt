Abstract
Domain generalization algorithms use training data from multiple domains to learn models that generalize well to un-seen domains. While recently proposed benchmarks demon-strate that most of the existing algorithms do not outperform simple baselines, the established evaluation methods fail to expose the impact of various factors that contribute to the poor performance. In this paper we propose an evaluation framework for domain generalization algorithms that allows decomposition of the error into components capturing dis-tinct aspects of generalization. Inspired by the prevalence of algorithms based on the idea of domain-invariant representa-tion learning, we extend the evaluation framework to capture various types of failures in achieving invariance. We show that the largest contributor to the generalization error varies across methods, datasets, regularization strengths and even training lengths. We observe two problems associated with the strategy of learning domain-invariant representations.
On Colored MNIST, most domain generalization algorithms fail because they reach domain-invariance only on the train-ing domains. On Camelyon-17, domain-invariance degrades the quality of representations on unseen domains. We hy-pothesize that focusing instead on tuning the classifier on top of a rich representation can be a promising direction. 1.

Introduction
Over the past decade machine learning research was pre-dominantly focused on settings where the learner observes training data from an unknown distribution and is evaluated on testing data, sampled from the same distribution. While modern deep learning approaches excel in such settings, they do significantly worse when the test data comes from a dif-ferent distribution [32, 41]. These methods might rely on dataset biases to perform well, and fail when those biases are eliminated [4, 10].
The goal of generalizing beyond training distribution is formulated in the domain generalization (DG) task, where the learner observes training data from multiple domains and is evaluated on unseen domains. Naturally, it is as-sumed that training and testing domains have some invariant properties or mechanisms, which allow generalization from one to another. At a high level, all domain generalization approaches seek to capture those invariances, but do that differently. A few of the possible directions of achieving do-main generalization are: learning domain-invariant represen-tations [9, 24], learning class-conditioned domain-invariant representations [8, 21, 45], using robust loss functions [36], learning invariant causal predictors [3], and using meta-learning [20]. Most of the methods listed above outperform the straightforward empirical risk minimization (ERM) ap-proach on toy domain generalization instances (e.g., colored
MNIST). However, Gulrajani and Lopez-Paz [14] demon-strate that when evaluated on realistic DG instances, these methods are unable to outperform ERM significantly. To im-prove domain generalization methods or propose new ones, we need to understand why and how do domain generaliza-tion methods fail. This is the main goal of this paper.
Our contributions are threefold. First, we characterize the general failure modes of domain generalization methods: training set underfitting, test set inseparability, training-test misalignment and classifier non-invariance. We develop tools that measure the contribution of each of these failures in the total error of a given model. Inspired by the popularity of the methods based on invariant representation learning, we also characterize failure modes related to achieving domain invariance. Second, we identify two common patterns of generalization failures. In the first pattern, many algorithms achieve domain-invariant representations across the training domains, but not on unseen domains, while the generaliza-tion error is negatively correlated with the representation invariance on unseen domains. The second pattern is when domain invariance is increased across all domains, but the increase coincides with a degradation of the representations of unseen domains, thus limiting the accuracy of the mod-els. Third, we show that by fixing the representations it is possible to isolate the classifier non-invariance failure, and significantly improve the generalization even with the most basic algorithms. These findings additionally confirm that domain-invariant representations are neither necessary nor sufficient for successful domain generalization.
2.