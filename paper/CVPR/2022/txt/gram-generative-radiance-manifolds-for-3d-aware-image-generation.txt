Abstract 1.

Introduction 3D-aware image generative modeling aims to generate 3D-consistent images with explicitly controllable camera poses. Recent works have shown promising results by train-ing neural radiance field (NeRF) generators on unstruc-tured 2D images, but still cannot generate highly-realistic images with fine details. A critical reason is that the high memory and computation cost of volumetric representation learning greatly restricts the number of point samples for radiance integration during training. Deficient sampling not only limits the expressive power of the generator to han-dle fine details but also impedes effective GAN training due to the noise caused by unstable Monte Carlo sampling. We propose a novel approach that regulates point sampling and radiance field learning on 2D manifolds, embodied as a set of learned implicit surfaces in the 3D volume. For each viewing ray, we calculate ray-surface intersections and ac-cumulate their radiance generated by the network. By train-ing and rendering such radiance manifolds, our generator can produce high quality images with realistic fine details and strong visual 3D consistency. 1
*Work done when YD and JX were interns at MSRA. 1Project page: https://yudeng.github.io/GRAM/
Learning 3D-aware image generation with Generative
Adversarial Networks (GAN) [17] has attracted a surge of attention in recent years [10–12,21,31,41,42,44,55]. Given an unstructured 2D image collection, GANs are trained to synthesize geometrically-consistent multiview imagery of novel instances.
In particular, methods [10, 21, 55] that use the volumetric rendering paradigm [15, 24] to compos-ite an output image have demonstrated impressive results with more “strict” 3D consistency by virtue of an explicit, physics-based rendering process.
Notwithstanding the promising results shown by these methods, the image quality still lags far behind traditional 2D image synthesis, for which state-of-the-art GAN mod-els [25, 26] can generate high-resolution and photorealis-tic images. One prominent hurdle is the high computation and memory requirements for training a volumetric repre-sentation. Methods [10, 55] that use neural radiance field (NeRF) [39] generators can greatly reduce the complexity of voxel-based approaches [21], but the volume integrations approximated by sampling points along viewing rays are still costly for both training and inference.
This problem becomes even more pronounced in GAN
training where a full image (rather than sparse pixels) needs to be rendered to train the discriminator. One workaround is to render patches during training [55], but using a patch discriminator may lead to inferior image generation qual-the state-of-the art ity. With an image discriminator, method [10] can only afford training on smaller image res-olution and with significantly reduced number of sampling points per ray (typically a few dozens) compared to standard
NeRF [39]. However, we observed that radiance integration using Monte Carlo sampling becomes unstable with insuffi-cient samples. The integrated colors among adjacent pixels suffer from intractable noise patterns that are detrimental to
GAN training (e.g., see Fig. 11). An even worse issue is that optimizing a full radiance volume requires the sampling to cover both low-frequency regions and high-frequency de-tails, leading to even less sample budget for the latter. Con-sequently, it is extremely difficult to generate fine details as they simply can be missed by the sampling.
This paper presents a novel method named Generative
Radiance Manifolds (GRAM). Different from the previous methods, we constrain our point sampling and radiance field learning on 2D manifolds, embodied as a set of implicit sur-faces. These implicit surfaces are shared for the trained ob-ject category, jointly learned with GAN training, and fixed at inference time. To generate an image, we accumulate the radiance along each ray using ray-surface intersections as point samples.
There are several advantages of our GRAM method.
First, by confining sampling and radiance learning in a re-duced space rather than anywhere in the volume, it greatly facilitates fine detail learning. The network can easily learn to generate thin structures and texture details on the surface manifolds which are guaranteed to have projections on the image and receive supervision during GAN training. Be-sides, our generated images are free from the noise pattern caused by inadequate Monte Carlo sampling, as the ray-surface intersections are deterministically calculated and smoothly varying across rays. Even with very few point samples (i.e., learning very few surfaces), our method can still learn to generate high-quality results. As a byproduct, at inference time we can render a generated instance in real time by pre-extracting the surfaces with their radiance.
Our implicit surfaces are defined as a set of isosurfaces in a scalar field predicted by a light-weight MLP network. An-other MLP for radiance generation is employed, for which we use a structure similar to [10]. We extract ray-surface in-tersections in a differentiable manner, and the whole frame-work is trained end-to-end using adversarial learning. Or-thogonal to our novel radiance manifold design, we also explore network architecture and training method enhance-In particular, we modify the network structure of ments.
[10] inspired by [26] and remove the progressive growing strategy used therein. Progressive growing not only in-troduces additional hyperparameters to tune but may also lead to degraded image quality shown in traditional 2D
GAN [26]. We also empirically find that our method gener-ates better results by removing it.
Our method is evaluated on multiple datasets including
FFHQ [25], Cats [67], and CARLA [13, 55]. We show that our 3D-aware generation method significantly outperforms the prior art. It can synthesize highly realistic images with geometrically-consistent fine details, which are unseen in previous results. We believe our method makes a significant step towards diminishing the quality gap between 3D-aware generation and traditional 2D image generation. 2.