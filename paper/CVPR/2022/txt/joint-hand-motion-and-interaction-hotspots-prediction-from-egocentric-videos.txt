Abstract
We propose to forecast future hand-object interactions given an egocentric video. Instead of predicting action labels or pixels, we directly predict the hand motion trajectory and the future contact points on the next active object (i.e., interaction hotspots).
This relatively low-dimensional representation provides a con-crete description of future interactions. To tackle this task, we first provide an automatic way to collect trajectory and hotspots labels on large-scale data. We then use this data to train an
Object-Centric Transformer (OCT) model for prediction. Our model performs hand and object interaction reasoning via the self-attention mechanism in Transformers. OCT also provides a probabilistic framework to sample the future trajectory and hotspots to handle uncertainty in prediction. We perform experi-ments on the Epic-Kitchens-55, Epic-Kitchens-100 and EGTEA
Gaze+ datasets, and show that OCT significantly outperforms state-of-the-art approaches by a large margin. Project page is available at https://stevenlsw.github.io/hoi-forecast. 1.

Introduction
Achieving the ability to predict a person’s intent, preference and future activities is one of the fundamental goals for AI sys-tems. This is particularly useful when it comes to egocentric video data for applications such as augmented reality (AR) and robotics.
Imagining with an egocentric view inside the kitchen (e.g., Fig-ure 1), if an AI system can forecast what the human would do next, an AR headset could provide useful and timely guidance, and a robot can react and collaborate with the human more smoothly.
What space should the model predict on? Recent ap-proaches [25,26,28,72] have been proposed to predict the discrete future action category given a sequence of frames as inputs, namely action anticipation. However, predicting a semantic label does not reveal how the human moves and what the human will interact with in the future. On the other hand, predicting pixels for future frames [9, 44, 53, 84] is very challenging due to its high dimension outputs with large uncertainties. Instead of adopting these two representations, our work is inspired by recent work on human motion trajectory prediction [11] which takes images as inputs and outputs the coordinates of future pose joints. Trajectory not only provides a concrete description of motion, but also is a much smaller space to predict compared
*Work partially done during an internship at Intel Labs.
Figure 1. Going beyond predicting a single action label in the future, we propose to jointly predict the future hand motion trajectories (blue and red lines) and interaction hotspots (heatmaps) on the next-active object in egocentric videos. to pixel prediction. However, unlike previous works, prediction in egocentric videos also involves dense interactions with objects, which cannot be modeled by trajectory alone.
In this paper, we propose to jointly predict the future hand motion trajectory and the interaction hotspots (affordance) of the next-active object, given a sequence of input frames from an egocentric video. Starting from the final frame of the input video, we will predict the trajectories for both hands by sampling from a probabilistic distribution inferred by the model. Instead of learning a deterministic model, we tackle the uncertainty of future in a probabilistic manner. At the same time, we will predict the contact points on the next-active object interacted by the future hands. These contact points are also represented via probabilistic distributions in the form of interaction hotspots [61] and conditioned on the predicted hand trajectory. To perform joint predictions, we introduce a Transformer-based model and an automatic way to generate a large-scale dataset for training.
Instead of collecting annotations for hand trajectories and interaction hotspots with high-cost human labor, we propose an automatic manner to collect the data in a large-scale. Given a video, we call the input frames to our model the observation frames and the predicted ones are called future frames. We first utilize off-the-shelf hand detectors [73] to locate hands in all the future frames. Since the camera is usually moving in egocentric videos, we leverage homography in nearby frames and project the detected future hands’ locations back to the last observation frame. In this way, all the detections are aligned in the same coordinate system. Similarly, we also detect the locations where
the hand interacts with the object in future frames, and project them back to the last observation frame. This process prepares the data for training our prediction model, and we generate labels for Epic-Kitchens-55, Epic-Kitchens-100 and EGTEA Gaze+ datasets without any human labor.
With the collected data, we propose to learn a novel Object-Centric Transformer (OCT) model which captures the hand-object relations from videos for hand trajectory and interaction hotspots prediction. Given the observation frames as inputs, we first extract their visual representations with a ConvNet. We perform hand and object detection and adopt RoI Align [34] to extract their features.
We take both hand and object features as object-centric tokens, and the average-pooled frame feature as image context tokens.
We forward all tokens from all input frames to a Transformer encoder, which performs hand, object and environment context interaction reasoning using self-attention. Instead of decoding in a deterministic manner, we adopt the Conditional Variational Au-toencoders (C-VAE) as network head in the Transformer decoder to model the uncertainty in prediction. Specifically, we compute cross-attention between the output tokens from the Transformer encoder and predicted future hand locations in the Transformer decoder. The obtained tokens are taken as conditional variables for the C-VAE. The decoder will predict both the hand trajectories and interaction hotspots jointly, and the training is supervised by a reconstruction loss corresponding to the ground-truths.
We perform evaluation on Epic-Kitchens-55 [15], Epic-Kitchens-100 [16] and EGTEA Gaze+ [46] datasets. We manually annotate the validation sets with trajectory and hotspots labels using the Amazon Mechanical Turk platform. Our OCT model significantly outperforms the baselines on both hand trajectory and interaction hotspots prediction tasks. Interestingly, we find that trajectory estimation helps interaction hotspots prediction and with more automatic annotated training data we can get better results.
Finally, we experiment with fine-tuning the trained model on the action anticipation task, and find that predicting hand trajectory and interaction hotspots can benefit classifying future actions.
Our contributions are the following:
• We propose to jointly predict hand trajectory and interaction hotspots from egocentric videos, and collect new training and test annotations.
• A novel Object-Centric Transformer which models the hand and object interactions for predicting future trajectory and affordance.
• We not only achieve state-of-the-art performance on both prediction tasks on Epic-Kitchens and EGTEA Gaze+ datasets, but also show our model can help the action anticipation task. 2.