Abstract
Original
Augmented
Original
Augmented
Post-hoc explanation methods, e.g., Grad-CAM, enable humans to inspect the spatial regions responsible for a par-ticular network decision. However, it is shown that such explanations are not always consistent with human priors, such as consistency across image transformations. Given an interpretation algorithm, e.g., Grad-CAM, we introduce a novel training method to train the model to produce more consistent explanations. Since obtaining the ground truth for a desired model interpretation is not a well-defined task, we adopt ideas from contrastive self-supervised learning, and apply them to the interpretations of the model rather than its embeddings. We show that our method, Contrastive
Grad-CAM Consistency (CGC), results in Grad-CAM in-terpretation heatmaps that are more consistent with hu-man annotations while still achieving comparable classi-fication accuracy. Moreover, our method acts as a reg-ularizer and improves the accuracy on limited-data, fine-grained classification settings.
In addition, because our method does not rely on annotations, it allows for the in-corporation of unlabeled data into training, which enables better generalization of the model. The code is available here: https://github.com/UCDvision/CGC 1.

Introduction
Deep neural networks have become ubiquitous in many applications owing to their performance on several com-puter vision tasks. Although they have been instrumental in achieving state-of-the-art accuracy, deep neural networks are widely considered to be black box systems, which is not desirable. For example, if an AI system is deployed to identify a malignant tumor from CT scans, it is important for medical experts to understand the reasoning behind the decision-making process [38]. This not only enables build-ing trust, but also helps identify any spurious correlations e n i l e s a
B s r u
O
R.V
Puma
Figure 1. Our method significantly improves the consistency of
Grad-CAM explanation heatmaps under data augmentation. For both the RV and the Puma, our method highlights the same por-tions of the image in both the original and augmented versions. that the network may have inadvertently learned to use to make its decision [33]. In recent years, there have been at-tempts to open this black box by designing frameworks to explain the network’s decision-making process. Post-hoc explanation methods such as CAM [45], Grad-CAM [31], and Full-Grad [35] generate a heatmap in the size of the image with higher values corresponding to the regions that contributed most to the network’s decision.
Unlike image labels, there can be multiple valid explana-tions for a given image category. Furthermore, a valid ex-planation might not involve the entire object-segmentation area. For example, in order to correctly classify an image of a dog, the network might rely on just the facial features of the dog, or the texture of the fur and the tail, or a com-bination of both. Each of these are valid explanations, and hence, generating ground truth annotations for explanations
Figure 2. The block diagram of our method. Our method consists of both cross-entropy loss (LCE) and contrastive Grad-CAM consis-tency loss (LCGC ). We load a batch of random images, and consider one to be the query image. We feed the query image to the network and calculate LCE. We calculate Grad-CAM for this image on the top predicted category and then augment the heatmap. We then augment all the images in the batch, feed them to our model, and calculate the Grad-CAM heatmap for the top predicted category. The top category is chosen using the original image and not the augmented one. (Note that all random images are also augmented with independently sampled parameters. We do not show this to reduce the clutter.) The heatmap from the augmented query image is considered to be the positive example. The heatmaps from the other random images in the batch are the negative examples. The augmented query heatmap, the positive heatmap, and the negative heatmaps are all fed into our contrastive comparison function to produce our LCGC term. is a not well defined task. This makes it difficult to directly supervise the network’s explanation during training.
It is shown that most interpretation methods are not con-sistent with spatial transformation of the images. For in-stance, shifting an image does not shift the interpretation
In addition, in fine-heatmap in the same way [10, 17]. grained visual categorization, it is important to learn the subtle yet discriminative features across classes (e.g., wing color, beak and eyes for a bird) [41] and hence the network interpretation should focus on the most salient features that discriminate the correct class from other classes. Assuming
Grad-CAM is a truthful interpretation method, we are inter-ested in improving the training process of the deep network so that its Grad-CAM interpretation is more consistent with respect to spatial transformations, thus making the model more interpretable. We use Grad-CAM as the key interpre-tation method for the rest of the paper since it passes the sanity check introduced in [1] and is end-to-end differen-tiable.
Inspired by self-supervised learning, we argue that a spa-tial affine transformation on an image should correspond to such transformation in the interpretation. For instance, given image of a “dog”, the heatmap for “dog” category should highlight the dog and it should shift or zoom if we shift or zoom into the image. Figure 1 shows an example where the change in Grad-CAM heatmap for the baseline network computed for the “RV” and “Puma” categories is not consistent with the spatial affine transformation applied to the images. We adopt ideas from recently developed con-trastive self-supervised learning literature [11, 13, 23] and design a loss function that encourages the Grad-CAM of an image to be close to the Grad-CAM of an augmented ver-sion of the same image while being far from the Grad-CAM of other random images.
We evaluate our CGC method for both classification ac-curacy and quality of explanations measured by the “Con-tent Heatmap” (CH) metric introduced in [27]. A similar metric was also introduced in [36] to measure the energy of explanation heatmap inside an adversarial patch. CH measures the cumulative heatmap contained within an an-notated object mask and is a proxy to measure the consis-tency of an explanation heatmap with respect to human an-notations. Moreover, since our method guides the model to focus mainly on the most discriminative features of the im-ages, it improves the accuracy in fine-grained classification settings while improving the consistency of interpretations.
The effect is even more prominent in learning with fewer labels on fine-grained tasks. We believe this is due to the regularization effect that our method adds to the training process. In addition, since our loss function does not need
labeled data, it can benefit from unlabeled data during train-ing. Our experiments show that our method improves the consistency of interpretation with respect to human annota-tions as well as the classification accuracy in limited labels and fine-grained settings. 2.