Abstract
Despite the popularity of Model Compression and Mul-titask Learning, how to effectively compress a multitask model has been less thoroughly analyzed due to the chal-lenging entanglement of tasks in the parameter space.
In this paper, we propose DiSparse, a simple, effective, and first-of-its-kind multitask pruning and sparse training scheme. We consider each task independently by disen-tangling the importance measurement and take the unani-mous decisions among all tasks when performing parame-ter pruning and selection. Our experimental results demon-strate superior performance on various configurations and settings compared to popular sparse training and pruning methods. Besides the effectiveness in compression, DiS-parse also provides a powerful tool to the multitask learn-ing community. Surprisingly, we even observed better per-formance than some dedicated multitask learning methods in several cases despite the high model sparsity enforced by DiSparse. We analyzed the pruning masks generated with DiSparse and observed strikingly similar sparse net-work architecture identified by each task even before the training starts. We also observe the existence of a "water-shed" layer where the task relatedness sharply drops, imply-ing no benefits in continued parameters sharing. Our code and models will be available at: https://github.com/SHI-Labs/DiSparse-Multitask-Model-Compression. 1.

Introduction
Convolutional Neural Networks (CNNs) [29] are consid-ered the go-to architecture for computer vision, ever since the inception of AlexNet [28], especially in fundamental vision tasks such as image classification [8], object detec-tion [22, 35] and segmentation [3, 38]. As more complex and difficult vision tasks are explored, substantial efforts are devoted to scaling deep convolutional networks to enor-mous sizes. Many models exist with parameters as many as billions, which significantly challenges those targeting edge device applications. Therefore, effectively compress-ing deep convolutional networks for efficient storage and computation has been a very active research area, and var-Figure 1. Converged training and validation multitask loss on
Cityscapes. Our method DiSparse obtains the best training and validation behavior in both static and dynamic sparse training paradigms comparing to other methods. DiSparse even beats the unsparsified baseline at several sparsity levels. ious approaches have been proposed and developed [6, 9] over the years.
Generally, neural network compression techniques can be categorized [9] into pruning [20, 30, 32], quantization [4, 37, 56], low-rank factorization [10, 33, 58], and knowledge distillation [25, 26, 36]. Network pruning, as a popular sub-field of model compression, aims to discard certain param-eters in the model, while retaining performance as much as possible. Pruning methods usually try to assign the best saliency score (also referred to as importance score) to each parameter and perform selection and pruning based on these importance measurements. Despite the diversity in the pruning schemes proposed in recent years, chasing sparsity in the network, either in a structured or unstruc-tured manner, has been one of the central themes since the very beginning [21,30]. Parameter efficiency of sparse neu-ral networks has been demonstrated [19, 52], and multiple works [15,48] have shown inference time speedups are pos-sible using sparsity for convolutional neural networks.
Notwithstanding the increasing attention on model prun-ing and sparse training, effectively sparsifying a multi-task network remains unexplored in spite of its importance.
Multitask Learning (MTL) focuses on simultaneously solv-ing multiple related tasks using a single model, which can significantly reduce the training and inference time and im-prove the generalization performance through learning a shared representation across tasks [2].
It has a wide ap-plication in many problems like autonomous driving and indoor navigation robot where multiple tasks like seman-tic segmentation and depth estimation need to be performed simultaneously. A compact and efficient multitask net-work makes real-time performance possible on edge de-vices where computational resources are limited. Multitask network naturally brings storage and speed advantages over its single-task counterparts by a commonly shared back-bone adopted in many popular MTL works [12, 27, 49].
However, how to further compress and sparsify such net-works hasn’t been carefully analyzed. Compression on mul-titask networks with comparable performance as on single-task ones is very challenging because the shared space con-tains heavily entangled and intertwined features, causing the traditional pruning and sparse training algorithm to fail.
A fraction of parameters in the shared space, though not im-portant for one task, could be crucial for the performance of another. A few MTL works have explored the problem of entangled features and showed disentangling representation into shared and task-private spaces will improve the model performance [34, 55].
We propose the first-of-its-kind pruning scheme that en-forces sparsity in multitask networks by taking the entan-gled nature of their features into consideration. We argue that the key to properly compressing a multitask model is correctly identifying saliency scores for each task in the shared space, therefore Sparsifying in a Disentangled man-ner (DiSparse). We take unanimous selection decisions among all tasks, which means that a parameter is removed only if it’s shown to be not critical for any task. This pre-vents extreme degradation in performance for certain tasks due to sparsification, leading to a more balanced network.
We conduct extensive experiments to validate the effi-ciency of our proposed scheme. We demonstrate compres-sion performance on models of different structures with datasets of various sizes [7,46,57]. We show results on pre-trained network, static sparse network at initialization, and dynamically growing sparse network at initialization. For each paradigm, we offered a slightly altered variant with the same core idea. We compared with popular pruning and sparse training methods and found that our proposed method is superior in both the training and validation phase, attaining lower training loss in a shorter period of time and better evaluation metrics on the validation set across differ-ent sparsity levels. Even more surprisingly, we observed better performance than some dedicated multitask learning approaches [1, 41, 50] in several cases in spite of the high sparsity enforced in our model, showing the effectiveness of DiSparse in multitask training. Besides the demonstra-tion of superior model performance, we provide interesting observations in our experiments with DiSparse. We com-pute the Intersection over Union (IoU) of the binary pruning masks generated with DiSparse for each task to indicate task relatedness or similarity. Surprisingly, we observed strik-ingly similar sparse network architecture identified by each task even before the training starts. This offers a glimpse of the transferable subnetwork architecture across domains.
Moreover, we observe the existence of a "watershed" layer where the task relatedness sharply drops, implying no ben-efits in continued parameters sharing. Exploitation of such property with DiSparse could save tremendous labor and computation cost by obtaining a better multitask architec-ture pre-training. These observations show that DiSparse does not only provide the compression community with the first-of-its-kind multitask sparsification scheme but also a powerful tool to the multitask learning community.
Our contributions can be summarized as follows:
• Proposing a simple, effective, and first-of-its-kind pruning and sparse training scheme for multitask net-work by disentangling the importance measurements among tasks, leading to a more balanced network.
• Performing an extensive empirical study on multiple vision tasks and datasets, which demonstrates the su-periority of DiSparse compared to popular pruning and sparse training algorithms and even several dedicated multitask learning methods.
• Studying and discussing task relatedness and multitask model architecture design with DiSparse, which pro-vides a valuable tool to the multitask learning commu-nity from a compression perspective. 2.