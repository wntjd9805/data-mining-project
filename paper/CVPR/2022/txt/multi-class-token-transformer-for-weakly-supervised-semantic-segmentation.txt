Abstract
This paper proposes a new transformer-based frame-work to learn class-specific object localization maps as pseudo labels for weakly supervised semantic segmenta-tion (WSSS). Inspired by the fact that the attended regions of the one-class token in the standard vision transformer can be leveraged to form a class-agnostic localization map, we investigate if the transformer model can also effectively capture class-specific attention for more discriminative ob-ject localization by learning multiple class tokens within the transformer. To this end, we propose a Multi-class To-ken Transformer, termed as MCTformer, which uses multi-ple class tokens to learn interactions between the class to-kens and the patch tokens. The proposed MCTformer can successfully produce class-discriminative object localiza-tion maps from the class-to-patch attentions corresponding to different class tokens. We also propose to use a patch-level pairwise affinity, which is extracted from the patch-to-patch transformer attention, to further refine the local-ization maps. Moreover, the proposed framework is shown to fully complement the Class Activation Mapping (CAM) method, leading to remarkably superior WSSS results on the PASCAL VOC and MS COCO datasets. These results underline the importance of the class token for WSSS. 1 1.

Introduction
Weakly supervised semantic segmentation (WSSS) aims to alleviate the reliance on pixel-level ground-truth labels by using weak supervision. A critical step for this task is to generate high-quality pseudo segmentation ground-truth labels by using weak labels.
Image-level labels can pro-vide simple weak labels which only indicate the presence or absence of certain classes without any ground-truth lo-calization information. Previous WSSS methods generally rely on Class Activation Mapping (CAM) [51] to extract ob-1https://github.com/xulianuwa/MCTformer
Figure 1. (a) In previous vision transformers [10], only one class token (red square) is used to aggregate information from patch to-kens (blue square). The learned patch attentions corresponding to the class token generate a class-agnostic localization map. (b)
In contrast, the proposed MCTformer uses multiple class tokens to learn interactions between class tokens and patch tokens. The learned class-to-patch attentions of different class tokens can pro-duce class-specific object localization maps. ject localization maps from Convolutional Neural Networks (CNNs). Despite using complex CAM expansion strategies or multiple training steps, existing methods still exhibit lim-ited performance in terms of both completeness of the local-ized objects and accuracy.
Vision Transformer (ViT) [10], as the first transformer model specifically designed for computer vision, has re-cently achieved performance breakthroughs on multiple vi-sion tasks [18]. Particularly, ViT has achieved state-of-the-art performance for large-scale image recognition, thanks to its strong capability to model long-range contexts. ViT splits the input image into non-overlapping patches and transforms them into a sequence of vectors. ViT also uses one extra class token to aggregate information from the en-tire sequence of the patch tokens. Although the class token has been removed in a number of recent transformer meth-ods [7, 8, 29], this work will underline its importance for weakly supervised semantic segmentation.
A recent work, DINO [3], revealed that there was explicit information about the semantic segmentation of an image in self-supervised ViT features. More specifically, it was ob-served that a semantic scene layout can be discovered from the attention maps of the class token. These attention maps lead to promising results in the unsupervised segmentation task. Although it was demonstrated that different heads in the transformer attention can attend to different semantic regions of an image, it remains unclear how to associate a head to a correct semantic class. That is, these attention maps are still class-agnostic (see Figure 1).
It is challenging to exploit class-specific attention from transformers. We argue that existing transformer-based works have a common issue, i.e., using only one class to-ken, which makes the accurate localization of different ob-jects on a single image challenging. There are two main reasons for this. First, a one-class-token design essentially inevitably captures context information from other object categories and the background.
In other words, it natu-rally learns both class-specific and generic representations for different object classes as only one class token is con-sidered, thus resulting in a rather non-discriminative and noisy object localization. Second, the model uses the only one-class token to learn interactions with patch tokens for a number of distinct object classes in a dataset. The model capacity is consequently not adequate enough to achieve the targeted discriminative localization performance.
To tackle these issues, a straightforward idea is to lever-age multiple class tokens, which will be responsible for learning representations for different object classes. To this end, we propose a Multi-class Token Transformer (MCT-former), in which multiple class-specific tokens are em-ployed to exploit class-specific transformer attention. Our goal of having class-specific tokens cannot be achieved by simply increasing the number of class tokens in ViT, be-cause these class tokens still do not have specific mean-ings. To ensure that each class token can effectively learn high-level discriminative representations of a specific object class, we propose a class-aware training strategy for multi-ple class tokens. More specifically, we apply average pool-ing on the output class tokens from the transformer encoder along the embedding dimension, to generate class scores, which are directly supervised by the ground-truth class la-bels. This thus builds a one-to-one strong connection be-tween each class token and the corresponding class label.
Through this design, one significant advantage is that the learned class-to-patch attention of different classes can be directly used as class-specific localization maps.
It is worth noting that the learned patch-to-patch atten-tion, as a byproduct of training without additional compu-tation, can serve as a patch-level pairwise affinity. This can be used to further refine the class-specific transformer at-tention maps, dramatically improving the localization per-formance. Moreover, we also show that the proposed trans-former framework fully complements the CAM method when applied on patch tokens (by simultaneously learning to classify with class-token and patch-token based represen-tations). This leads to high consistency between class to-kens and patch tokens, thus considerably enhancing the dis-criminative ability of their derived object localization maps.
In summary, the main contribution is three-fold:
• We propose to exploit class-specific transformer atten-tions for weakly supervised semantic segmentation.
• We propose an effective transformer framework, which includes a novel multi-class token transformer (MCTformer) coupled with a class-aware training strategy, to learn class-specific localization maps from the class-to-patch attention of different class tokens.
• We propose to use the patch-to-patch transformer at-tentions as a patch-level pairwise affinity, which can significantly refine the class-specific transformer at-tentions. Furthermore, the proposed MCTformer can fully complement the CAM mechanism, leading to high-quality object localization maps.
The proposed method can generate high-quality class-specific multi-label localization maps for WSSS, establish-ing new state-of-the-art results on PASCAL VOC (mIoU of 71.6% on the test set) and MS COCO (mIoU of 42.0%). 2.