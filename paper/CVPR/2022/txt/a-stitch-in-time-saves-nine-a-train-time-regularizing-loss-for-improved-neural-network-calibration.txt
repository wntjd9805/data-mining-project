Abstract
Deep Neural Networks (DNNs) are known to make over-conﬁdent mistakes, which makes their use problematic in safety-critical applications. State-of-the-art (SOTA) calibra-tion techniques improve on the conﬁdence of predicted labels alone, and leave the conﬁdence of non-max classes (e.g. top-2, top-5) uncalibrated. Such calibration is not suitable for
§Equal contribution label reﬁnement using post-processing. Further, most SOTA techniques learn a few hyper-parameters post-hoc, leaving out the scope for image, or pixel speciﬁc calibration. This makes them unsuitable for calibration under domain shift, or for dense prediction tasks like semantic segmentation. In this paper, we argue for intervening at the train time itself, so as to directly produce calibrated DNN models. We pro-pose a novel auxiliary loss function: Multi-class Difference in Conﬁdence and Accuracy (MDCA), to achieve the same.      
MDCA can be used in conjunction with other application/task speciﬁc loss functions. We show that training with MDCA leads to better calibrated models in terms of Expected Cal-ibration Error (ECE), and Static Calibration Error (SCE) on image classiﬁcation, and segmentation tasks. We report
ECE (SCE) score of 0.72 (1.60) on the CIFAR 100 dataset, in comparison to 1.90 (1.71) by the SOTA. Under domain shift, a ResNet-18 model trained on PACS dataset using MDCA gives a average ECE (SCE) score of 19.7 (9.7) across all domains, compared to 24.2 (11.8) by the SOTA. For segmen-tation task, we report a 2× reduction in calibration error on PASCAL-VOC dataset in comparison to Focal Loss [32].
Finally, MDCA training improves calibration even on imbal-anced data, and for natural language classiﬁcation tasks. 1.

Introduction
Deep Neural Networks (DNNs) have shown promising results for various pattern recognition tasks in recent years.
In a classiﬁcation setting, with input x ∈ X , and label y ∈ Y = {1, . . . , K}, a DNN typically outputs a conﬁdence score vector s ∈ RK. The vector, s, is also a valid probability vector, and each element of s is assumed to be the predicted conﬁdence for the corresponding label. It has been shown in recent years that the conﬁdence vector, s, output by a DNN is often poorly calibrated [14, 36]. That is:
P (cid:16) y = y∗ b s[ y](cid:17) ̸= s[ b y], b (cid:12) (cid:12) (1) y, and y∗ are the predicted, and true label respectively where for a sample. E.g. if a DNN predicts a class “truck” for b an image with score 0.7, then a network is calibrated, if the probability that the image actually contains a truck is 0.7. If the probability is lower, a network is said to be over-conﬁdent, and under-conﬁdent if probability is higher. For a pixel-wise prediction task like semantic segmentation, we would like to calibrate prediction for each pixel. Similarly, we would like calibration to hold not only for the predicted s[y], but for the whole vector s (all label, i.e. y = arg max y∈Y b labels), i.e., ∀y ∈ Y.
One of the main reasons for the miscalibration is the speciﬁc training regimen used. Most modern DNNs, when trained for classiﬁcation in a supervised learning setting, are trained using one-hot encoding that have all the prob-ability mass centered in one class; the training labels are thus zero-entropy signals that admit no uncertainty about the input [48]. The DNN is thus trained to become over-conﬁdent. Besides creating a general distrust in the model predictions, the miscalibration is especially problematic in safety critical applications, such as self-driving cars [13], legal research [51] and healthcare [10, 46], where giving the correct conﬁdence for a predicted label is as important as the correct label prediction itself.
Researchers have tried to address miscalibration by learn-ing a post-hoc transformation of the output vector so that the conﬁdence of the predicted label matches with the likelihood of the label for the sample [15, 17]. Since such techniques focus on the predicted label only, they could end up calibrat-ing only the label which has maximum conﬁdence for each sample. Hence, in a multi-class setting, the labels with non-maximal conﬁdence scores remain uncalibrated. This makes any post-processing for label reﬁnement, such as posterior inference using MRF-MAP [4], ineffective.
In this paper we argue for the calibration at the train-time.
Unlike post-hoc calibration techniques that use limited pa-rameters1, a train time strategy allows exploiting millions of learnable parameters of DNN itself, thus providing a ﬂexible learning more suited to image and pixel speciﬁc transforma-tion for model calibration. Our experiments under domain shift, and for a dense predict task (semantic segmentation) shows the strength of the approach.
Armed with the above insight, we propose a novel aux-iliary loss function: Multi-class Difference in Conﬁdence and Accuracy (MDCA). The proposed loss function is de-signed to be used during the training stage in conjunction with other application speciﬁc loss functions, and overcomes the non-differentiablity of the loss functions proposed in earlier methods. Though we do not advocate it, the proposed technique is complimentary to the post-hoc techniques which may still be used after the training, if there is a separate hold-out dataset available for exploitation. Since ours is a train time calibration approach, it implies good regularization for the predictions. We show that models trained using our loss function remain calibrated even under domain shift.
Contributions: We make the following key contributions: (1) A trainable DNN calibration method with inclusion of a novel auxiliary loss function, termed MDCA, that takes into account the entire conﬁdence vector in a multi-class setting. Our loss function is differentiable and can be used in conjunction with any existing loss term. We show experi-ments with Cross-Entropy, Label Smoothing [38], and Focal
Loss [32]. (2) Our approach is on par with post-hoc meth-ods [14, 23] without the need for hold-out set making the deployment more practical (See Tab. 6). (3) Our loss func-tion is a powerful regularizer, maintaining calibration even under domain/dataset drift and dataset imbalance which We demonstrate on PACS [30], Rotated MNIST [29] and imbal-anced CIFAR 10 datasets. (4) Although the focus is primarily on image classiﬁcation, our experiments on multi-class se-mantic segmentation show that our technique outperforms
TS based calibration, and Focal Loss [32]. We also show the effectiveness of our approach on natural language classiﬁca-tion task on 20Newsgroup dataset [27]. 1For example, Temperature scaling (TS) calibrates uses a single global scalar, T ; and Dirichlet Calibration (DC) uses O(K2) hyper-parameters for
K classes to calibrate the model output
2.