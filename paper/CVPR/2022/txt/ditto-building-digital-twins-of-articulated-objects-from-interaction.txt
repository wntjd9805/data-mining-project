Abstract
Digitizing physical objects into the virtual world has the potential to unlock new research and applications in em-bodied AI and mixed reality. This work focuses on recre-ating interactive digital twins of real-world articulated ob-jects, which can be directly imported into virtual environ-ments. We introduce Ditto to learn articulation model es-timation and 3D geometry reconstruction of an articulated object through interactive perception. Given a pair of vi-sual observations of an articulated object before and af-ter interaction, Ditto reconstructs part-level geometry and estimates the articulation model of the object. We em-ploy implicit neural representations for joint geometry and articulation modeling. Our experiments show that Ditto effectively builds digital twins of articulated objects in a category-agnostic way. We also apply Ditto to real-world objects and deploy the recreated digital twins in physical simulation. Code and additional results are available at https://ut-austin-rpl.github.io/Ditto/ 1.

Introduction
Synthetic data has played a steadily more vital role in fueling emerging AI applications, from training and proto-typing computer vision models [20, 42] to teaching robots to perform physical tasks [2, 28, 57]. As modern AI models become larger and increasingly data-hungry, virtual plat-forms and synthetic datasets supply a massive amount of cheap training data. For vision models to benefit from syn-thetic data, realism is key â€” the distribution mismatch be-tween the real and virtual worlds hinders the generalization of models trained in simulation. A promising path towards closing the reality gap is digitizing physical objects and recreating them in virtual environments. Research on 3D vision and SLAM [3,12,34,35,53] has made significant ad-vances in capturing realistic objects and scenes with static 3D models. Nonetheless, the burgeoning body of embodied
AI and mixed reality research calls for interactive digital twins of physical objects that can be spawned in simulated environments and interact with virtual agents. Building dig-ital twins of articulated objects is particularly challenging
Figure 1. We build digital twins of articulated objects through in-teractive perception. Given visual observations before and after interaction, our method jointly reconstructs the part-level geom-etry and articulation model of the object. Our recreated digital twins can be spawned in physics engines and are fully interactive in robot simulation and AR/VR applications. as it requires not only a good understanding of its overall geometry but the part compositions as well as the kinematic relations between the parts.
Recent efforts in embodied AI platforms [22,23,50] have incorporated interactive articulated objects, such as cabinets and drawers, in simulated household environments and em-ployed them for training virtual agents. Even so, they heav-ily rely on graphics designers and engineers to author and curate the object models, limiting the scalability of the as-set acquisition process. Developing vision-based methods to automate the estimation [1, 17] and reconstruction [33] of articulated objects has been an active line of research, ac-celerated by new tools developed from the 3D vision com-munity, including geometric deep learning [19, 37, 39] and implicit neural representations [10, 36]. The majority of prior work focuses on solving individual components of the problem rather than constructing a full-fledged model. Sev-eral recent works [24, 52] have studied the joint learning of part segmentation and joint estimation. However, they in-fer part-level geometry on the point cloud which cannot be used for physical simulation, because physical simulation requires compact geometry of the object such as the mesh
for collision computation. 2.