Abstract
Audio-Guided video object segmentation is a challeng-ing problem in visual analysis and editing, which automat-ically separates foreground objects from the background in a video sequence according to the referring audio expres-sions. However, existing referring video object segmen-tation works mainly focus on the guidance of text-based referring expressions, due to the lack of modeling the se-mantic representations of audio-video interaction contents.
In this paper, we consider the problem of audio-guided video semantic segmentation from the viewpoint of end-to-end denoising encoder-decoder network learning. We pro-pose the wavelet-based encoder network to learn the cross-modal representations of the video contents with audio-form queries. Specifically, we adopt the multi-head cross-modal attention layers to explore the potential relations of video and query contents. A 2-dimension discrete wavelet trans-form is merged into the transformer encoder to decompose the audio-video features. Next, we maximize mutual infor-mation between the encoded features and multi-modal fea-tures after cross-modal attention layers to enhance the au-dio guidance. Then, a self attention-free decoder network is developed to generate the target masks with frequency-domain transforms. In addition, we construct the first large-scale audio-guided video semantic segmentation dataset.
The extensive experiments show the effectiveness of our method1.
†Equal contribution.
*Corresponding Author. 1Code is available at: https://github.com/asudahkzj/Wnet.git
Figure 1. The audio-guided video object segmentation task. 1.

Introduction
Referring video object segmentation aims to segment video objects referred by given language expressions, which has attracted wide attention due to its applicability to many practical problems including video analysis and video edit-ing [33,35,49,50,61]. Currently, most referring video object segmentation approaches mainly focus on the guidance of text-guided referring expressions [18, 19, 31, 33, 35, 49, 61, 63], which can learn the multi-modal representation from the interaction network layer, and then generate the object masks to the given text references. The existing works have achieved promising performance in text-based video object segmentation, but they may still be ineffectively applied to the audio-guided video object segmentation due to the lack of modeling the semantic representation of audio-video in-teraction contents.
The audio-guided video analysis is a simulation of hu-man cognition, comparing with the text-guided analysis
[44]. Humankind use speech exclusively long before the invention of writing. People also learn and use language in the real world, as to collaborate, describe and relate their vi-sual environment, talk about each other, and so on. Further-more, in the natural scene, audio interaction is more con-venient and common than text interaction. Although audio
inputs can be converted to text inputs through ASR mod-els [3, 4, 46], the process will produce unavoidable losses.
Since Harwath and Glass’s collection of spoken captions for
Flickr8k [14], more works address cognitive and linguistic questions [8, 10–12]. Other work addresses applied tasks, including multi-modal retrieval [22], cross-modality align-ment [13,27], retrieving speech in different languages using images as a pivot modality [1,26,39], and speech-to-speech retrieval [1, 39]. Our work focuses on the audio-guided video object segmentation tasks, shown as Fig. 1. The au-dio guidance often contains rich semantic information, such as the accent, emotion and speed. These extra factors can facilitate the object segmentation. The same object can cor-respond to different pronunciations, while the same pronun-ciation can point to different objects. Thus, the simple ex-tension of the existing segmentation works based on text-based guidance is difficult for modeling the semantic rep-resentation of audio-video interaction contents. Inspired by
MulT [51], we use multi-head cross-modal attention layers to fuse the video embeddings and audio embeddings. Dif-ferent from the MulT model [51], we extend dimensions of inputs and apply it to large-scale natural language datasets.
The cross-modal transformers referred to text embeddings are all removed.
One other bottleneck is the noise problem, derived from acquisition noise and fusing noise [7]. For the acquisition noises, we use a pre-trained MFCC model [5] to extract acoustic features, which is widely used in automatic speech and speaker recognition. In this paper, we focus on the pro-cessing of fusing noise. There is a large gap between video and audio representations. The joint representations reflect important information considering multi-modal alignment.
Audio and video features have different redundant parts irrelevant phonemes and pixels), likewise termed as (i.e. noise. These noises are difficult to handle only by con-volution operations and attention mechanisms in the time domain. As mentioned in [29], noises are likely to con-centrate at high frequencies. Recently, Fnet [30] has been proposed to learn the frequency-domain-level representa-tion with Fourier transforms for recognition tasks, while it only aims to speed up the encoder architectures but fails to obtain improvement in performances. Low-pass filter-ing on Fourier analysis cannot effectively distinguish the high-frequency parts of the required signal from the high-frequency interference caused by noise. If the low-pass fil-tering is too narrow, parts of the required signal are treated as noise and its morphological information is erased, which leads to the distortion of the original signal [45]. representation yet to our knowledge. We are the first to de-vise the DWT-transformer for the audio-visual joint repre-sentation to filter the noise and outliers, as a priori. The layers of the whole transformer encoder are reduced, which obtains a sizable performance boost in terms of speed and model consumption. Inspired by the AMDIM [2], we maxi-mize mutual information between the encoded features and multi-modal features after the cross-modal attention to en-hance the audio guidance.
The main contributions of this paper are as follows: (i) Unlike the previous studies, we study the problem of audio-guided video object segmentation from the viewpoint of end-to-end denoising encoder-decoder network learning. (ii) We propose the wavelet-based encoder network to learn the cross-modal representations of the video contents with audio-form queries. (iii) We construct a large-scale dataset for audio-guided video object segmentation and validate the effectiveness of our proposed method through extensive ex-periments. 2.