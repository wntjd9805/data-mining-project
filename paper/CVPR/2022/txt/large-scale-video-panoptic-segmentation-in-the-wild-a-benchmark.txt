Abstract
In this paper, we present a new large-scale dataset for the video panoptic segmentation task, which aims to as-sign semantic classes and track identities to all pixels in a video. As the ground truth for this task is difficult to an-notate, previous datasets for video panoptic segmentation are limited by either small scales or the number of scenes.
In contrast, our large-scale VIdeo Panoptic Segmentation in the Wild (VIPSeg) dataset provides 3,536 videos and 84,750 frames with pixel-level panoptic annotations, cov-ering a wide range of real-world scenarios and categories.
To the best of our knowledge, our VIPSeg is the first at-tempt to tackle the challenging video panoptic segmentation task in the wild by considering diverse scenarios. Based on VIPSeg, we evaluate existing video panoptic segmen-tation approaches and propose an efficient and effective clip-based baseline method to analyze our VIPSeg dataset.
Our dataset is available at https://github.com/
VIPSeg-Dataset/VIPSeg-Dataset/. 1.

Introduction
Panoptic segmentation unifies semantic and instance segmentation tasks by assigning a semantic label and an in-stance ID to every pixel in an image, which is a fundamental research topic in computer vision and has many practical applications such as detailed action understanding, video
†Corresponding author.
‡Part of this work was done when Jiaxu Miao was an intern at Baidu
Research. editing, autonomous driving, and augmented reality. Re-cently, plenty of approaches [15,16,26,27,29,31,34,51,63, 68, 74] have been proposed for panoptic segmentation and achieved remarkable progress.
Although the image panoptic segmentation task has been well explored, video panoptic segmentation [26] (VPS) is still a challenging problem. VPS model should not only provide unique and consistent semantic predictions within a video, but also associate instance IDs for the same object across frames. Recently some approaches and datasets [26, 48, 60, 61] have been proposed for video panoptic seg-mentation. However, there are many limitations in cur-rent VPS benchmarks. First, existing VPS datasets [26, 60] are small-scaled due to the exhausting labeling cost. For example, Cityscapes-VPS [26] only contains 500 videos with six annotated frames per video. KITTI-STEP [60] and MOTChallenge-STEP [60] only contain 50 and 4 video sequences, respectively. The video panoptic segmentation task is constrained by existing datasets due to their insuffi-cient videos [60] and short video length [26]. Second, the diversity of existing VPS datasets is restricted, i.e., only the street view scene is considered in previous datasets. Thus, the categories of things with pixel-level annotation are lim-ited and biased. Some previous datasets [60] only focus on people and vehicles. The diversity issue prevents these datasets from being general in real-world applications (e.g. video editing, augmented reality), which contains many scenes and hundreds of things in our daily life.
To advance the research on video panoptic segmenta-tion, we present a new dataset in this work, targeting large-scale VIdeo Panoptic Segmentation in the wild (VIPSeg).
The dataset contains a wide range of real-world scenarios (e.g., 232 scenes) and categories (e.g. 124 classes). To-tally we annotated 3,536 videos and 84,750 frames with pixel-level panoptic annotations, including both semantic categories for background stuff (e.g., sky,ground) and track identities for foreground things (e.g., person, cats, cars). To the best of our knowledge, our VIPSeg is the first attempt to tackle the challenging video panoptic segmentation task in the wild by considering diverse scenarios. Since semantic
IDs and instance IDs of all pixels are annotated, our VIPSeg can also applied to other video tasks including Video Ob-ject Segmentation, Video Semantic Segmentation,Video In-stance Segmentation, etc..
Annotating such a large-scale video panoptic segmenta-tion is difficult and expensive, since semantic classes and tracking ids of every pixel are required. To overcome ex-hausting human efforts, we propose a Sparse-to-Dense In-teractive Annotation strategy to efficiently annotate panop-tic masks by the collaboration of humans and computers.
Concretely, we first propose to annotate instances for each frame at a sparse frame rate (1 fps) and associate instances using a tracking model [71] and manual correction. After that, we adopt a video object segmentation model AOT [71] to extend the frame rate from 1 fps to 5 fps and manually refine instance masks to improve segmentation quality.
We conduct extensive experiments on VIPSeg to evalu-ate existing video panoptic segmentation models. Most of existing works [26, 48] on VPS inference predictions iter-atively, where they generate the next frame prediction by taking the previous results as the reference. However, real-world videos would last long, and the iterative inference would be less efficient in applications. Thus, we propose an clip-based model extended from PanopticFCN [34] to divide a video into non-overlapping clips and individually generate predictions for each clip. The clip-based method could process video panoptic segmentation in parallel and be more efficient in real applications. We adopt the clip-based model to evaluate and analyze our VIPSeg dataset. 2.