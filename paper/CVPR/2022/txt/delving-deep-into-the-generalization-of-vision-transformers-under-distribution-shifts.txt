Abstract 1.

Introduction
Vision Transformers (ViTs) have achieved impressive performance on various vision tasks, yet their generaliza-tion under distribution shifts (DS) is rarely understood. In this work, we comprehensively study the out-of-distribution (OOD) generalization of ViTs. For systematic investiga-tion, we first present a taxonomy of DS. We then perform extensive evaluations of ViT variants under different DS and compare their generalization with Convolutional Neu-Important observations are ral Network (CNN) models. obtained: 1) ViTs learn weaker biases on backgrounds and textures, while they are equipped with stronger inductive biases towards shapes and structures, which is more con-sistent with human cognitive traits. Therefore, ViTs gen-eralize better than CNNs under DS. With the same or less amount of parameters, ViTs are ahead of corresponding
CNNs by more than 5% in top-1 accuracy under most types of DS. 2) As the model scale increases, ViTs strengthen these biases and thus gradually narrow the in-distribution and OOD performance gap. To further improve the gener-alization of ViTs, we design the Generalization-Enhanced
ViTs (GE-ViTs) from the perspectives of adversarial learn-ing, information theory, and self-supervised learning. By comprehensively investigating these GE-ViTs and comparing with their corresponding CNN models, we observe: 1) For the enhanced model, larger ViTs still benefit more for the
OOD generalization. 2) GE-ViTs are more sensitive to the hyper-parameters than their corresponding CNN models. We design a smoother learning strategy to achieve a stable train-ing process and obtain performance improvements on OOD data by 4% from vanilla ViTs. We hope our comprehensive study could shed light on the design of more generalizable learning architectures. Codes and datasets are released in https://github.com/Phoenix1153/ViT OOD generalization.
*These authors contributed equally to this work. (cid:12)Corresponding author.
Recently, transformer has made remarkable achievements in vision tasks, such as e.g. image classification [7, 8, 27], ob-ject detection [4, 36], and image processing [6]. Despite the encouraging performance achieved on standard benchmarks and several properties revealed in recent works [1, 5, 20, 21], the generalization ability of Vision Transformers (ViTs) is still less understood. While the traditional train-test scenario assumes the test data for model evaluation are independent identically distributed (IID) with sampled training data, this assumption does not always hold in real-world scenarios.
Thus, out-of-distribution (OOD) generalization is a highly desirable capability of machine learning models. Recent works indicate current CNN architectures generalize poorly on various distribution shifts (DS) [11, 13, 14], whereas the investigation on ViTs remains scarce. Therefore, in this paper, we mainly focus on delving deep into the OOD gener-alization of ViTs under DS.
To comprehensively study the OOD generalization ability of ViTs, we first define a categorization of commonly appear-ing DS based on the modified semantic concepts in images.
Generally, an image for classification contains a foreground object and background information. The foreground object consists of hierarchical semantic concepts including pixel-level elements, object textures, and shapes, object parts, and object itself [35]. A distribution shift usually causes variance on one or more semantics and we thus present a taxonomy of DS into four conceptual groups: background shifts, cor-ruption shifts, texture shifts, and style shifts.
With the taxonomy of DS, we investigate the OOD gen-eralization of ViTs by comparison with CNNs in each case.
While models are desired to generalize to arbitrary OOD scenarios, the no-free-lunch theorem for machine learn-ing [3, 12, 32] demonstrates that there is no entirely general-purpose learning algorithm, and that any learning algorithm implicitly or explicitly will generalize better on some dis-tributions and worse on others. Thus some set of induc-tive biases are demanded to acquire generalization. Hence,
to achieve human-level generalization capability, machine learning models are supposed to have inductive biases that are most relevant to the human prior in the world. There have been many attempts to inject inductive biases into deep learning models that humans may exploit for the cognition operating at the level of conscious processing, e.g. the con-volution [16] and self-attention mechanism [29]. Therefore, we examine whether transformers are equipped with induc-tive biases that are more related to human cognitive traits to better investigate the generalization properties of ViTs under DS. Extensive evaluations reveal the following ob-servations on the OOD generalizations of ViTs: 1) ViTs learn weaker biases on backgrounds and textures, while they are equipped with stronger inductive biases towards shapes and structures, which is more consistent with human cogni-tive traits. Therefore, ViTs generalize better than CNNs in most cases. Specifically, ViT not only achieves better per-formance on OOD data but also has smaller generalization gaps between IID and OOD datasets. 2) As the model scale increases, ViTs strengthen these biases and thus gradually narrow the IID and OOD generalization gaps, especially in the case of corruption shifts and background shifts. In other words, larger ViTs are better at diminishing the effect of local changes. 3) ViTs trained with larger patch size deal with texture shifts better, yet are inferior in other cases.
After validating the superiority of ViTs in dealing with
OOD data, we focus on further improving their general-ization capacity. Specifically, we design Generalization-Enhanced ViTs (GE-ViTs) from the perspectives of adversar-ial training [10], information theory [24] and self-supervised learning [34]. Equipped with GE-ViTs, we achieve signif-icant performance boosts towards OOD data by 4% from vanilla ViTs. By performing an in-depth investigation on different models, we draw the following conclusions: 1) For the enhanced transformer models, larger ViTs still benefit more for the OOD generalization. 2) GE-ViTs are more sensitive to the hyper-parameters than their corresponding
CNN models. 2.