Abstract
Contrastive learning (or its variants) has recently be-come a promising direction in the self-supervised learn-ing domain, achieving similar performance as supervised learning with minimum ﬁne-tuning. Despite the labeling ef-ﬁciency, wide and large networks are required to achieve high accuracy, which incurs a high amount of computation and hinders the pragmatic merit of self-supervised learn-ing. To effectively reduce the computation of insigniﬁcant features or channels, recent dynamic pruning algorithms for supervised learning employed auxiliary salience pre-dictors. However, we found that such salience predictors cannot be easily trained when they are na¨ıvely applied to contrastive learning from scratch. To address this issue, we propose contrastive dual gating (CDG), a novel dy-namic pruning algorithm that skips the uninformative fea-tures during contrastive learning without hurting the train-ability of the networks. We demonstrate the superiority of CDG with ResNet models for CIFAR-10, CIFAR-100, and ImageNet-100 datasets. Compared to our implemen-tations of state-of-the-art dynamic pruning algorithms for self-supervised learning, CDG achieves up to 15% accu-racy improvement for CIFAR-10 dataset with higher com-putation reduction. 1.

Introduction
The success of the conventional supervised learning relies on the large-scale labeled dataset to minimize the loss and achieve high accuracy. However, manually an-notating millions of data samples is labor-intensive and time-consuming. This promotes the self-supervised learn-ing (SSL) to be an attractive solution, since artiﬁcial labels are used instead of human-annotated ones for training.
The state-of-the-art self-supervised learning frame-works, such as SimCLR [3] and MoCo [11], utilize the con-cept of contrastive learning (CL) [9] with wide and deep models to achieve comparable performance as the super-vised training counterpart. Figure 1 shows the CIFAR-10 inference accuracy vs. the number of ﬂoating-point opera-(cid:20) (cid:19) (cid:18) (cid:9) (cid:17) (cid:14) (cid:16) (cid:13) (cid:15) (cid:14) (cid:14) (cid:4) (cid:12) (cid:14) (cid:10) (cid:12) (cid:13) (cid:12) (cid:11) (cid:10) (cid:9) (cid:2) (cid:9) (cid:8) (cid:7) (cid:6) (cid:5) (cid:4) (cid:3) (cid:0) (cid:2) (cid:11)(cid:9) (cid:11)(cid:8) (cid:11)(cid:7) (cid:12)(cid:13)(cid:14)(cid:15)(cid:13)(cid:16)(cid:17)(cid:10)(cid:9)(cid:18)(cid:19)(cid:10)(cid:20)(cid:21)
%(cid:26)&’(cid:14) (cid:27)(cid:18)(cid:10)(cid:8)(cid:18)(cid:20)(cid:18) (cid:0) (cid:18)(cid:12)(cid:13)(cid:14)(cid:15)(cid:13)(cid:16)(cid:17)(cid:10)(cid:9)(cid:18)(cid:19)(cid:5)(cid:20)(cid:21) (cid:12)(cid:13)(cid:14)(cid:15)(cid:13)(cid:16)(cid:17)(cid:10)(cid:9)(cid:18)(cid:19)(cid:7)(cid:20)(cid:21) (cid:22)(cid:23)(cid:24)(cid:25)(cid:26)(cid:12)(cid:27)(cid:18)(cid:12)(cid:13)(cid:14)(cid:15)(cid:13)(cid:16)(cid:17)(cid:10)(cid:9)(cid:18)(cid:19)(cid:7)(cid:20)(cid:21) (cid:12)(cid:13)(cid:14)(cid:15)(cid:13)(cid:16)(cid:17)(cid:5)(cid:0)(cid:18)(cid:19)(cid:5)(cid:20)(cid:21) (cid:22)(cid:23)(cid:24)(cid:25)(cid:26)(cid:12)(cid:27)(cid:18)(cid:12)(cid:13)(cid:14)(cid:15)(cid:13)(cid:16)(cid:17)(cid:10)(cid:9)(cid:18)(cid:19)(cid:5)(cid:20)(cid:21) (cid:11)(cid:5) (cid:22)(cid:23)(cid:24)(cid:25)(cid:26)(cid:12)(cid:27)(cid:18)(cid:12)(cid:13)(cid:14)(cid:15)(cid:13)(cid:16)(cid:17)(cid:10)(cid:9)(cid:18)(cid:19)(cid:10)(cid:20)(cid:21) (cid:12)(cid:13)(cid:14)(cid:15)(cid:13)(cid:16)(cid:17)(cid:5)(cid:0)(cid:18)(cid:19)(cid:10)(cid:20)(cid:21) (cid:11)(cid:0) (cid:9)(cid:9) (cid:9)(cid:8) (cid:9)(cid:7) (cid:22)(cid:23)(cid:24)(cid:25)(cid:26)(cid:12)(cid:27)(cid:18)(cid:12)(cid:13)(cid:14)(cid:15)(cid:13)(cid:16)(cid:17)(cid:5)(cid:0)(cid:18)(cid:19)(cid:5)(cid:20)(cid:21) (cid:22)(cid:23)(cid:24)(cid:25)(cid:26)(cid:12)(cid:27)(cid:18)(cid:12)(cid:13)(cid:14)(cid:15)(cid:13)(cid:16)(cid:17)(cid:5)(cid:0)(cid:18)(cid:19)(cid:10)(cid:20)(cid:21) (cid:18)(cid:25)$"(cid:16)(cid:30)!(cid:14)(cid:16)(cid:23)(cid:31)(cid:13)(cid:18)(cid:26)(cid:13)!(cid:30)"(cid:23)"#(cid:27)(cid:18)(cid:22)(cid:23)(cid:24)(cid:25)(cid:26)(cid:12) (cid:18)(cid:22)(cid:28)(cid:29)(cid:13)(cid:30)(cid:31)(cid:23)(cid:14)(cid:13) (cid:18)(cid:26)(cid:13)!(cid:30)"(cid:23)"# (cid:0)(cid:2)(cid:0)(cid:3)(cid:4)(cid:0) (cid:5)(cid:2)(cid:0)(cid:3)(cid:4)(cid:6) (cid:7)(cid:2)(cid:0)(cid:3)(cid:4)(cid:6) (cid:8)(cid:2)(cid:0)(cid:3)(cid:4)(cid:6) (cid:9)(cid:2)(cid:0)(cid:3)(cid:4)(cid:6) (cid:10)(cid:2)(cid:0)(cid:3)(cid:4)(cid:7)
%(cid:26)&’(cid:14)(cid:18)(cid:19)((cid:23)))(cid:23)$"(cid:14)(cid:21)(cid:18)
Figure 1. Inference accuracy of various ResNet models with su-pervised and self-supervised training [3] from scratch. After con-trastive pre-training, models are ﬁne-tuned on 50% of training set. tions (FLOPs). By training from scratch, SimCLR [3] re-quires a model that is 4 times wider (ResNet-18 (4×)) to achieve similar accuracy as the baseline model trained with supervised learning (ResNet-18 (1×)). On the other hand, it is also difﬁcult to achieve good accuracy with the com-pact model architecture (e.g., ResNet-20). The extraordi-nary computation cost necessitates efﬁcient computation re-duction techniques for self-supervised learning.
Under the context of supervised learning, network spar-siﬁcation has been widely studied. Both static weight prun-ing [10, 21] and dynamic computation skipping [1, 8, 14, 16, 20] have achieved high accuracy with pruned architecture or sparse features. A recent work [2] reported the transferabil-ity of applying the lottery ticket hypothesis [7] to SSL for the downstream tasks. However, the requirements of self-supervised pretraining and iterative searching greatly limit the practicality of the algorithm. Sparsifying the SSL mod-els that are trained from scratch is still largely unexplored, despite its importance.
To address this research gap, we investigate efﬁcient dy-namic sparse feature learning by training the model from scratch in a self-supervised fashion. Most of the prior works on dynamic computation reduction [1, 8, 16, 20] ex-ploit the spatial sparsity by using an auxiliary mini neu-Contrastive branch 2.