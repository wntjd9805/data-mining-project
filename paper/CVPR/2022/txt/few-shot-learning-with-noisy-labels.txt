Abstract
Few-shot learning (FSL) methods typically assume clean support sets with accurately labeled samples when training on novel classes. This assumption can often be unrealistic: support sets, no matter how small, can still include misla-beled samples. Robustness to label noise is therefore es-sential for FSL methods to be practical, but this problem surprisingly remains largely unexplored. To address misla-beled samples in FSL settings, we make several technical contributions. (1) We offer simple, yet effective, feature ag-gregation methods, improving the prototypes used by Pro-toNet, a popular FSL technique. (2) We describe a novel
Transformer model for Noisy Few-Shot Learning (TraNFS).
TraNFS leverages a transformer’s attention mechanism to weigh mislabeled versus correct samples. (3) Finally, we extensively test these methods on noisy versions of MiniIm-ageNet and TieredImageNet. Our results show that TraNFS is on-par with leading FSL methods on clean support sets, yet outperforms them, by far, in the presence of label noise. 1.

Introduction
Modern few-shot learning (FSL) methods aim to learn classifiers for novel classes from only a handful of exam-ples. These methods, however, generally assume that the few support set samples used for training were carefully se-lected to represent their class. Unfortunately, real-world set-tings rarely offer such guarantees. In fact, even carefully an-notated and curated datasets often contain mislabeled sam-ples [9, 33, 45, 53], due to automated weakly supervised an-notation, ambiguity, or even human error.
Whereas there are plenty of methods designed for learn-ing with noise in many-shot supervised settings [1, 19, 20, 25, 31, 51], noise in few-shot settings remains largely unex-plored. This dearth is surprising considering the utility of
FSL methods in settings where human supervision cannot easily be provided: e.g. in fully automated systems which learn many novel classes [12,43,56,57], making human cu-ration of the labels of every support set, unrealistic.
Fig. 1 shows the challenge of learning from few, possibly
Figure 1. Few-shot learning with mislabeled samples. A 5-shot, 5-way support set of MiniImageNet [48] images. Rows show sup-port set samples of each novel class. Two samples in each row were mislabeled by symmetric label flips (Sec. 6.1). Can you spot which ones? See Supp. for answers and more examples. mislabeled, examples. It presents a sample 5-shot, 5-way support set from MiniImageNet [48]. Each row includes the support set training images of one of the five classes. Two of the samples in each row are mislabeled with symmet-ric label noise (Sec. 6.1). With so few examples, spotting mislabeled images can be difficult, even for humans with considerable prior knowledge, which FSL methods lack.
As we later demonstrate empirically, FSL methods are especially vulnerable to such label noise. When training from few samples, each sample represents a significant con-tribution to the final decision boundary. Thus, even a sin-gle noisy example can be destructive to the model’s accu-racy. We illustrate this observation in Fig. 2, which reports the performance of ProtoNet [40], a popular FSL method, on MiniImageNet with noisy labels. ProtoNet averages the convolutional features of each class’s support set into class prototypes. Queries are then classified by the class of their nearest neighbor prototype. Fig. 2 shows the effect of in-creasing the number of mislabeled samples, compared with a model trained after mislabeled samples were removed
(i.e., smaller, but cleaner, support sets). The widening gap between the two curves reflects the degradation of accuracy when mislabeled samples are not accounted for.
We address the vulnerability of FSL methods to label noise by making a number of technical innovations. We begin by exploring simple, yet effective alternatives to the design of ProtoNet [40]. Specifically, we replace the mean operator, used by ProtoNet for aggregating support set fea-tures, with more robust methods. We evaluate an un-weighted option, the median, and options which weigh sup-port set samples based on feature similarities. We show that these changes already improve robustness to label noise.
We then introduce our Transformer model for Noisy
Few-Shot Learning (TraNFS). Unlike previous methods,
TraNFS learns to aggregate support samples into class rep-resentations. The transformer architecture offers a natural means for processing variable numbers of shots and ways with permutation invariance. Robustness to label noise is achieved by leveraging a modified version of the trans-former’s self-attention mechanism [47]. This modified self-attention used by TraNFS compares support set samples and downweights samples considered likely to be mislabeled.
We test our proposed methods extensively on versions of MiniImageNet [48] and TieredImageNet [38] with three methods of adding label noise. Our results show that the proposed TraNFS (and even the simpler modifications of
ProtoNet) surpass popular FSL methods by wide margins in the presence of label noise, while offering comparable performance in the absence of label noise.
To summarize, we make the following contributions.
• We propose median and similarity weighting as simple yet effective substitutes to ProtoNet’s mean prototypes. transformer model
• We present TraNFS, a novel adapted to FSL with noisy labels.
• We extensively benchmark many popular FSL meth-ods on three types of support set noise pollution: sym-metric, paired, and outlier. 2.