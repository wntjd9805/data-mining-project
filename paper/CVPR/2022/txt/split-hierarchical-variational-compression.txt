Abstract
Variational autoencoders (VAEs) have witnessed great suc-cess in performing the compression of image datasets. This success, made possible by the bits-back coding framework, has produced competitive compression performance across many benchmarks. However, despite this, VAE architectures are cur-rently limited by a combination of coding practicalities and compression ratios. That is, not only do state-of-the-art methods, such as normalizing flows, often demonstrate out-performance, but the initial bits required in coding makes single and parallel image compression challenging. To remedy this, we introduce
Split Hierarchical Variational Compression (SHVC). SHVC introduces two novelties. Firstly, we propose an efficient au-toregressive prior, the autoregressive sub-pixel convolution, that allows a generalisation between per-pixel autoregressions and fully factorised probability models. Secondly, we define our coding framework, the autoregressive initial bits, that flexibly supports parallel coding and avoids – for the first time – many of the practicalities commonly associated with bits-back coding.
In our experiments, we demonstrate SHVC is able to achieve state-of-the-art compression performance across full-resolution lossless image compression tasks, with up to 100x fewer model parameters than competing VAE approaches. 1.

Introduction
The volume of data, measured in terms of IP traffic, is currently witnessing an exponential year-on-year growth [9].
Fuelled by the demand for high-resolution media content, it is estimated that 80% of this data is in the form of images and video [9]. Data service providers, such as cloud and streaming platforms, have consequently seen costs associated with transmission and storage become prohibitively expensive.
For example, an increased demand for streaming services forced major providers to throttle the maximum resolution of video content to 720p during the coronavirus pandemic. As such, these challenges have renewed the need for the development of high-performance data compression codecs.
*co-first author. The work of Tom Ryder is conducted during his employment at Huawei Technologies R&D UK.
One solution to this problem has been the development of approaches using likelihood-based generative models capable of discrete density estimation [3,6,13,14,18,22,24,35,36,42,43].
Such methods operate by learning a deep probabilistic model of the data distribution, which, in combination with entropy coders, can be used to compress data. Here, according to Shannon’s source coding theorem [21], the minimal required average codelength is bounded by the expected negative log-likelihood of the data distribution.
From this family of generative models, there have emerged three dominant modes for data compression: normalizing flows [3, 14, 42, 43], variational autoencoders [18, 24, 36] and autoregressive models [15, 31, 37] 1. In fact, each of these approaches can be thought of as a traversal on the Pareto frontier of inference speed and compression performance. With broad generality, autoregressive models can often be the most powerful but the slowest; variational autoencoders are often the weakest but the fastest; and normalizing flows – depending on the variant – sit somewhere in between.
In this paper, we consider data compression with VAEs, and focus on extending the efficient frontier; obtaining solutions faster than popular VAEs that achieve state-of-the-art com-pression ratios. Use of VAEs, however, poses two outstanding challenges. Firstly, we should achieve competitive coding ratios without greatly sacrificing time complexity. For example, best iterates currently require one of two ingredients to improve per-formance: building either a deep hierarchy of latent variables [8] or use of autoregressive priors [11,29]. The latter idea, especially popular in the codecs of the lossy compression community [25], posits a model that flexibly learns both local (via autoregression) and global (via hierarchical latent representation) data modalities (e.g. low-frequency information). Whilst these approaches, such as MS-PixelCNN [29] and PixelVAE [11], have had some suc-cess in achieving more efficient trade-offs, generation of even moderately sized images is still to the order of minutes [22].
Secondly, there should exist a practical means by which to efficiently perform single-image compression. Single-image compression then permits parallel coding, which is highly 1Most recently, score-based generative models have been adapted for data compression [16], but current approaches require an impractical number of operations at inference time.
desirable. However, translating a VAE into a lossless codec is currently achieved using the bits-back coding framework (predominantly, bits-back ANS), which requires a large number of initial bits [12,36,39] (see Section 3.1). Whilst this is a trivial number of bits on large image datasets (where we can amortize this cost), it renders bits-back an impractical approach for single-image compression. Furthermore, even large datasets are often coded such that images are interlinked. Access to a single image in the middle of a sequence would therefore require all prior images in the bitstream to be additionally decompressed.
To that end, we propose two novelties for use in VAE-based compression designed to address these challenges. The first, our autoregressive sub-pixel convolution, introduces a simple autoregressive factorisation – not dissimilar from the transforma-tions used in normalizing flows [3, 42, 43] – designed to present an efficient interpolation between fully-factorised probability distributions and the impractical per-pixel autoregressions.
Built from a modified space-to-depth convolution operator, we losslessly downsample data variables before performing a computationally efficient autoregression along the channel dimension. Our autoregressive operator is then advantaged by a number of network evaluations invariant to data dimensions, with each autoregression crucially performed on a downsam-pled version on the input tensor. More broadly, we view this framework as a generalisation of many popular autoregressive
“context” models used in data compression [11, 26, 31, 41].
Our second contribution, autoregressive initial bits, presents a general framework for avoiding the impracticalities of bits-back ANS, allowing for eminently parallelizable coding.
This technique, highly compatible with our autoregressive model, partitions the data variable into two splits such that the second partition is conditionally independent of the latent variable(s), given the first. In this way, we illustrate how we can use the entropy coding of the conditionally independent partition to both supply and remove the initial bits necessitated by bits-back ANS. We demonstrate that this approach reduces the bit overhead on a per-image basis by close to 20x.
Finally, we combine the above contributions to present our codec, Split Hierarchical Variational Compression (SHVC).
SHVC posits a hierarchical VAE of general-form autoregressive priors that permits parallel coding. Using our framework, we outperform all other VAE-based compression approaches with fewer latent variables and a comparable number of neural network evaluations. We further illustrate the effectiveness of our architecture by training a small model which is able to outperform similar VAE approach Bit-Swap [18] – but with 100x fewer model parameters. 2.