Abstract
Recently deep learning methods have shown signiﬁcant progress in data clustering tasks. Deep clustering method-s (including distance-based methods and subspace-based methods) integrate clustering and feature learning into a uniﬁed framework, where there is a mutual promotion be-tween clustering and representation. However, deep sub-space clustering methods are usually in the framework of self-expressive model and hence have quadratic time and space complexities, which prevents their applications in large-scale clustering and real-time clustering. In this pa-per, we propose a new mechanism for deep clustering. We aim to learn the subspace bases from deep representation in an iterative reﬁning manner while the reﬁned subspace bases help learning the representation of the deep neural networks in return. The proposed method is out of the self-expressive framework, scales to the sample size linearly, and is applicable to arbitrarily large datasets and online clustering scenarios. More importantly, the clustering ac-curacy of the proposed method is much higher than its com-petitors. Extensive comparison studies with state-of-the-art clustering approaches on benchmark datasets demonstrate the superiority of the proposed method. 1.

Introduction
Clustering is a fundamental issue in machine learning, which aims to separate samples into classes in the absence of label information, under the requirement of high intra-class similarity and low inter-class similarity. Many classi-cal clustering algorithms such as k-means [29] and spectral clustering (SC) [30] have showed great success in real ap-plications. However, they are not effective in handling da-ta with complicated structures or/and high-dimensionality, which can be improved by using reﬁned features of the da-∗Jicong Fan is the corresponding author. ta.
Indeed, some previous works [14, 37, 38, 47] utilized the feature learning techniques such as non-negative ma-trix factorization [2], auto-encoder (AE) [1] and its vari-ants [24, 31, 36] to learn low-dimensional embeddings for clustering, which increased the clustering accuracy. Nev-ertheless, since these methods are two-stage clustering and the feature learning is not speciﬁc to clustering, it is not guaranteed that the learned representations are appropriate for clustering.
Recently, a few researchers [3, 9, 26, 43, 46] have pro-posed end-to-end clustering methods, such as deep em-bedded clustering (DEC) [40], joint unsupervised learning (JULE) [41], deep adaptive clustering (DAC) [6], and deep comprehensive correlation mining (DCCM) [39]. In these methods, the clustering objectives are integrated with the network optimization process, which provides an approach to learning clustering-oriented embedded representations.
However, most deep clustering methods use the Euclidean distance-based measure in identifying clusters, whereas Eu-clidean distance is not always valid or reasonable for differ-ent types of data structures.
Subspace clustering assumes that data lie in different subspaces [11]. A category of classical subspace cluster-ing methods such as sparse subspace clustering (SSC) [11] and low-rank representation (LRR) [27] are mainly based on spectral clustering [30] and outperformed k-means and classical spectral clustering in many tasks such as face im-age clustering. Recently, a few researchers [8, 21, 25, 44] showed that joint subspace clustering and deep learning have promising performance on benchmark datasets. How-ever, these approaches can hardly be extended to large-scale datasets because they need to learn a self-expressive matrix leading to quadratic time and space complexities. Conse-quently, some latest works [12, 48, 49] dedicate to improv-ing the efﬁciency of subspace clustering.
In this paper, we aim to provide an approach to efﬁcient and accurate deep subspace clustering. We propose to learn a set of subspace bases from the latent features extracted
by t-SNE [35]. It provided a clustering model that achieves simultaneous optimization of cluster centers and embedded features. Chang et al. [5] proposed deep self-evolution clus-tering (DSEC), which is a self-evolving-based algorithm to train the network alternatively with chosen pairs of patterns.
In [39], Wu et al. presented a method called DCCM that us-es pseudo-labels for self-supervision and uses mutual infor-mation to capture more discriminative representations for clustering. The partition conﬁdence maximisation (PICA) proposed by Huang et al. [20] minimizes a partition uncer-tainty index and learns the most conﬁdent clustering alloca-tion. Note that these deep clustering approaches assign clus-ters using Euclidean distance, which may not useful when the clusters do not concentrate on the mean values. 2.2. Subspace Clustering
Classical subspace clustering such as SSC [11], LR-R [27], Kernel-SSC [32] aim to learn a self-expressive afﬁn-ity matrix for spectral clustering. Ji et al. [21] proposed deep subspace clustering network (DSC-Net) that incorpo-rated a self-expression module with auto-encoder network.
DSC-Net showed signiﬁcant improvement on several image datasets, compared to SSC and LRR. Zhou et al. [52] pro-vided a method called deep adversarial subspace clustering (DASC) that utilized generative adversarial network [16] to provide an adversarial learning, which improved the perfor-mance of deep subspace clustering. Zhou et al. [51] pro-posed distribution preserving subspace clustering (DPSC) to retain the latent distribution in the subspace to improve the feature learning ability of the subspace clustering mod-el. On the other hand, a few researchers tried to reduce the complexity of subspace clustering [7,12,13,33,49]. For ex-ample, Zhang et al. [49] proposed the k-subspace clustering network (k-SCN) to integrate the update of subspace into the learning of embedded space for addressing the draw-back of learning the afﬁnity matrix. Fan [12] proposed a method called k-factorization subspace clustering (k-FSC), which has linear time and space complexity and is able to handle missing data and streaming data.
Figure 1. Illustration of the proposed method. The auto-encoder network is used to learn embedded representation Z for input data, then Z combines with subspace D to construct the subspace afﬁn-ity vector which in turn yields the normalized subspace afﬁnity S.
Subsequently, the reﬁned subspace afﬁnity (cid:101)S is computed from S to provide self-supervised information. Note that d is the dimen-sion of subspace, LRecon and LSub represent the reconstruction loss and the discrepancy between (cid:101)S and S, and the network is trained by jointly optimizing them. by deep auto-encoder, where the bases and network param-eters are iteratively reﬁned. The network structure of the proposed method is illustrated in Fig. 1. Our contributions are as follows.
• We present a novel deep subspace clustering method that is out of the conventional self-expressive frame-work.
• Our method has linear time and space complexity and hence is applicable to large-scale subspace clustering.
• We generalize the method to online clustering such that we can handle arbitrarily large datasets and streaming datasets effectively.
• We analyze the feasibility of using deep neural net-work to convert distance-based clustering and sub-space clustering. results on many benchmark datasets (e.g.
Numerical
Fashion-MNIST, STL-10, and REUTERS-10K) showed that our method is more effective than its competitors. 2.