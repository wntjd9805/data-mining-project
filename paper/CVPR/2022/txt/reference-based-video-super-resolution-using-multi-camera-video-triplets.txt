Abstract
We propose the first reference-based video super-resolution (RefVSR) approach that utilizes reference videos for high-fidelity results. We focus on RefVSR in a triple-camera setting, where we aim at super-resolving a low-resolution ultra-wide video utilizing wide-angle and tele-photo videos. We introduce the first RefVSR network that re-currently aligns and propagates temporal reference features fused with features extracted from low-resolution frames. To facilitate the fusion and propagation of temporal reference features, we propose a propagative temporal fusion mod-ule. For learning and evaluation of our network, we present the first RefVSR dataset consisting of triplets of ultra-wide, wide-angle, and telephoto videos concurrently taken from triple cameras of a smartphone. We also propose a two-stage training strategy fully utilizing video triplets in the proposed dataset for real-world 4× video super-resolution.
We extensively evaluate our method, and the result shows the state-of-the-art performance in 4× super-resolution. 1.

Introduction
Recent mobile devices such as Apple iPhone or Sam-sung Galaxy series are manufactured with at least two or three asymmetric multi-cameras typically having different but fixed focal lengths. In a triple camera setting, each ultra-wide, wide-angle, and telephoto camera has a different field of view (FoV) and optical zoom factor. One advantage of such configuration is that, compared to an ultra-wide cam-era, a wide-angle camera captures a subject with more de-tails and higher resolution, and the advantage escalates even further with a telephoto camera. A question naturally fol-lows is why not leverage higher-resolution frames of a cam-era with a longer focal length to improve the resolution of frames of a camera with a short focal length.
Utilizing a reference (Ref) image to reconstruct a high-resolution (HR) image from a low-resolution (LR) image has been widely studied in previous reference-based image
Code and dataset: https://github.com/codeslake/RefVSR
Figure 1. Comparison on 8K 4×SR video results from a real
HD video between state-of-the-art (SOTA) RefSR approach [26] and the proposed RefVSR approach. Our method learns to super-resolve an LR video by utilizing relevant high-quality patches of reference frames and robustly recovers sharp textures of both in-side and outside the overlapped FoV between the input ultra-wide and reference wide-angle frames (white dashed box). super-resolution (RefSR) approaches [2,23,26,28,29,32,34, 35]. However, it has not been explored yet to utilize a Ref video for video super-resolution (VSR). In this paper, we expand the RefSR to the VSR task and introduce reference-based video super-resolution (RefVSR) that can be applied for videos captured in an asymmetric multi-camera setting.
RefVSR inherits objectives of both RefSR and VSR tasks and utilizes a Ref video for reconstructing an HR video from an LR video. Applying RefVSR for a video cap-tured in an asymmetric multi-camera setting requires con-sideration of the unique relationship between LR and Ref frames in multi-camera videos. In the setting, a pair of LR and Ref frames at each time step shares almost the same content in their overlapped FoV (top and middle rows of the leftmost column in Fig. 1). Moreover, as a video exhibits a motion, neighboring Ref frames might contain high-quality contents useful for recovering the outside the overlapped
FoV (the bottom row of the leftmost column in Fig. 1).
For successful RefVSR in an asymmetric multi-camera setting, we take advantage of temporal Ref frames in re-constructing regions both inside and outside the overlapped
FoV. In previous RefSR approaches [26, 28, 29, 32], global matching has been a common choice for establishing non-local correspondence between a pair of LR and Ref images.
However, given a pair of LR and Ref video sequences, it is not straightforward to directly apply global matching be-tween an LR frame and multiple Ref frames. To utilize as many frames as possible in the global matching for large real-world videos (e.g., HD videos), we need a framework capable of managing Ref frames in a memory-efficient way.
We propose the first end-to-end learning-based RefVSR network that can generally be applied for super-resolving an LR video using a Ref video. Our network adopts a bidi-rectional recurrent pipeline [4, 7, 8] to recurrently align and propagate Ref features that are fused with the features of
LR frames. Our network is efficient in terms of computa-tion and memory consumption because the global matching needed for aligning Ref features is performed only between a pair of LR and corresponding Ref frames at each time step. Still, our network is capable of utilizing temporal Ref frames, as the aligned Ref features are continuously fused and propagated in the pipeline.
As a key component for managing Ref features in the pipeline, we propose a propagative temporal fusion module that fuses and propagates only well-matched Ref features.
The module leverages the matching confidence computed during the global matching between LR and Ref features as the guidance to determine well-matched Ref features to be fused and propagated. The module also accumulates the matching confidence throughout the pipeline and uses the accumulated value as the guidance when fusing the propa-gated temporal Ref features.
To train and validate our model, we present the first Re-fVSR dataset consisting of 161 video triplets of ultra-wide, wide-angle, and telephoto videos simultaneously captured with triple cameras of a smartphone. Wide-angle and tele-photo videos have the same size as ultra-wide videos but their resolutions are 2× and 4× the resolution of ultra-wide videos, respectively. With the RefVSR dataset, we train our network to super-resolve an ultra-wide video 4× to produce an 8K video with the same resolution as a telephoto video.
To this end, we propose a two-stage training strategy that fully utilizes video triplets in the proposed dataset. We show that, with our training strategy, our network can success-fully learn super-resolution of a real-world HD video and produce a high-fidelity 8K video.
To summarize, our contributions include:
• the first RefVSR framework with the focus on videos recorded in an asymmetric multi-camera setting,
• the propagative temporal fusion module that effec-tively fuses and propagates temporal Ref features,
• the RealMCVSR dataset, which is the first dataset for the RefVSR task, and
• the two-stage training strategy fully utilizing video triplets for real-world 4×VSR. 2.