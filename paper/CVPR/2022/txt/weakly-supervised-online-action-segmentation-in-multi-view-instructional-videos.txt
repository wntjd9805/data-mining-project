Abstract
This paper addresses a new problem of weakly-supervised online action segmentation in instructional videos. We present a framework to segment streaming videos online at test time using Dynamic Programming and show its advantages over greedy sliding window ap-proach. We improve our framework by introducing the
Online-Ofﬂine Discrepancy Loss (OODL) to encourage the segmentation results to have a higher temporal consis-tency. Furthermore, only during training, we exploit frame-wise correspondence between multiple views as supervision for training weakly-labeled instructional videos.
In par-ticular, we investigate three different multi-view inference techniques to generate more accurate frame-wise pseudo ground-truth with no additional annotation cost. We present results and ablation studies on two benchmark multi-view datasets, Breakfast and IKEA ASM. Experimental results show efﬁcacy of the proposed methods both qualitatively and quantitatively in two domains of cooking and assem-bly. 1.

Introduction
Action understanding in untrimmed instructional videos is important in many applications, where agents learn by ob-servation of other agents performing complex tasks. Such videos are characterized by composition of a sequence of low-level atomic actions, e.g., crack eggs and whisk eggs, that form a high-level task, e.g., making eggs. This contex-tual dependency between actions as well as other attributes in instructional videos have inspired new research [5,19,33, 39, 48] that has advanced the ﬁeld.
A fully-supervised training of these videos would re-quire not only the labels for each action, but also their tem-poral assignment (start and end time) with ordering con-straints. However, creating fully annotated clips with action
*Work done during Reza’s internship at Honda Research Institute, USA
Figure 1. Top: online segmentation, where the frame of interest at time t is identiﬁed either greedily by the f function or through
DP-based online inference based on current and past predictions.
Bottom: Ofﬂine segmentation after observing the whole sequence. assignments and labels on the temporal boundaries of indi-vidual actions is manually intensive and is therefore both time consuming and expensive. This limits the scale and practicality at which fully-supervised video datasets can be created. Furthermore, the subjective nature of labeling the start and end time of each action results in ambiguities and inconsistencies. In weakly-supervised action segmentation these limitations are addressed by using only the ordered se-quence of action labels per video during training, and forgo subjective labeling of start and end time of each action.
Another important consideration in action understand-ing relates to requirements for processing the videos online versus ofﬂine, which is not addressed in existing weakly-supervised segmentation methods [6, 27, 47]. Online pro-cessing with low latency is an increasingly important part of interactive applications where real-time, or near real-time feedback is critical. For example, interactive applications such as human-robot interaction, error correction in man-ufacturing assembly, and virtual rehabilitation require im-mediate feedback from the intelligent system as the video streams arrive.
The work presented in this paper considers the two aforementioned aspects in action segmentation: weak-supervision and online processing aimed at temporally par-titioning videos into action segments. To our knowledge, our work is the ﬁrst to address the problem of weakly super-vised online action segmentation. Speciﬁcally, we present a framework to segment streaming instructional videos online at test time using Dynamic Programming (DP). We show the advantages of using DP as opposed to the greedy slid-ing window approach that are frequently used in previous online action understanding work [10, 16, 55] (Fig. 1).
We also introduce the Online-Ofﬂine Discrepancy Loss (OODL). Ofﬂine segmentation refers to inference after ob-serving the video in its entirety. Ofﬂine segmentation is a non-causal procedure that is generally expected to be more accurate than its online counterpart that makes inference from partial observations. Indeed, there is a trade-off be-tween accuracy of the recognized actions and low-latency ( Sec.6.2.1). The OODL loss uses the ofﬂine segmentation result as a reference and penalizes its difference with on-line segmentation results generated at each time step in the video. Effectively, this encourages the segmentation results inferred at different observation end points in the video to have higher temporal consistency with respect to each other.
Furthermore, due to lack of frame-level annotation in weakly-labeled videos, frame-wise correspondence be-tween multiple synchronized views of the same recording can provide helpful cues about the temporal location of each action during training. Our work is the ﬁrst to use the su-pervision of frame-level correspondence between different views for action segmentation. We compare three ways to exploit this multi-view correspondence to generate more ac-curate frame-level pseudo ground-truth for weakly-labeled videos. This is in contrast to previous segmentation meth-ods [4, 6, 27], where different views are treated indepen-dently, discarding important multi-view information. Note that we only use the multi-view correspondence at training time and our method segments each video independently at test time with no access to other views. Also, our frame-work utilizes no additional annotation cost, as it is trained independent of the label and number of view points.
In summary, our main contributions are as follows: 1) We are the ﬁrst to address the problem of weakly-supervised online action segmentation in instructional videos, and offer a DP-based framework. 2) We introduce the Online-Ofﬂine Discrepancy Loss (OODL). The OODL loss utilizes the ofﬂine segmentation result as a reference to train the online model by minimizing the difference between online and ofﬂine inference results. 3) We use frame-wise multi-view correspondence, dur-ing training only, to generate more accurate action pseudo-ground-truth in weakly-labeled videos with no additional annotation cost. Our work is the ﬁrst to incorporate multi-view video understanding in action segmentation. 4) We present results and a detailed ablation study on two benchmark multi-view datasets in domains of cooking and assembly: Breakfast [24] and IKEA [2]. We show quantita-tively and qualitatively how our contributions consistently improve various suggested baselines on both datasets. 2.