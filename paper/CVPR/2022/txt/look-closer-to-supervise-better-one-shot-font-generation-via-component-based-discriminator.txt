Abstract
Automatic font generation remains a challenging re-search issue due to the large amounts of characters with complicated structures. Typically, only a few samples can serve as the style/content reference (termed few-shot learn-ing), which further increases the difﬁculty to preserve local style patterns or detailed glyph structures. We investigate the drawbacks of previous studies and ﬁnd that a coarse-grained discriminator is insufﬁcient for supervising a font generator. To this end, we propose a novel Component-Aware Module (CAM), which supervises the generator to decouple content and style at a more ﬁne-grained level, i.e., the component level. Different from previous studies strug-gling to increase the complexity of generators, we aim to perform more effective supervision for a relatively simple generator to achieve its full potential, which is a brand new perspective for font generation. The whole frame-work achieves remarkable results by coupling component-level supervision with adversarial learning, hence we call it
Component-Guided GAN, shortly CG-GAN. Extensive ex-periments show that our approach outperforms state-of-the-art one-shot font generation methods. Furthermore, it can be applied to handwritten word synthesis and scene text im-age editing, suggesting the generalization of our approach. 1.

Introduction
To better tackle the few-shot font generation issue, we rethink the following two questions: 1) What determines people’s judgment on font styles? 2) How do people learn to write a new character/glyph in the correct structure? To answer the ﬁrst question intuitively, we present a text string in three different font styles in Figure 1. Since their overall architectures are similar, we naturally pay more attention to local details, including endpoint shapes, corner sharpness,
∗Corresponding author.
Figure 1. A same text string presented in three different font styles. stroke thickness, joined-up writing pattern, etc., which ap-pear at a more local level, i.e., the components of a char-acter. Although the components cannot present some font style properties, such as the inclination and aspect ratio, we argue that the components determine the font style to a greater extent than the whole character shape. As for the second question, one strong assumption is that when people learn a complicated glyph, they ﬁrst learn the components that form the character. Intuitively, if all the components in a glyph are properly written, we can obtain the glyph cor-rectly. Drawing inspiration from the above observations, an intuitive method for few-shot font generation is to utilize the component information that is largely correlated with the font style properties and glyph structures.
Few-shot font generation (FFG) has received consider-able research interest in recent years due to its critical ap-plications [3, 25, 26, 31, 36, 38]. An ideal FFG system can greatly reduce the burden of the time-consuming and labor-intensive font design process, particularly for those lan-guage systems with a massive number of glyphs, e.g., Chi-nese with more than 25,000 glyphs. Another application is to create a cross-lingual font library, e.g., Chinese to Ko-rean, given the fact that Adobe and Google take years to create the Source Han Sans Font, a universal font style that supports Chinese, Korean, and Japanese simultaneously.
Recently, several attempts have been made to few-shot font generation; however, they all have certain limitations and further improvements are therefore required. For in-stance, [26] learns to map the source font style to a ﬁxed
target font style, and thus has to be retrained for another new style. “zi2zi” [38] learns multiple font styles by adding a pre-deﬁned style category embedding, but still cannot generalize to unseen font styles. One notable approach is
EMD [36], which is generalizable to unseen styles by disen-tangling the style and content representation, but the result is not promising due to its ﬂaw in loss function design.
Lately, several methods utilize the idea of composition-ality. Still, they have signiﬁcant drawbacks. For instance,
CalliGAN [31] generates glyph images conditioned on the learned embeddings of the component labels and style la-bels, hence cannot generalize to either unseen styles or un-seen components. DM-Font [3] employs a dual-memory ar-chitecture for font generation. However, it requires a refer-ence set containing all the components to extract the stored information, which is unacceptable for the FFG scenario.
LF-Font [25] can be extended to unseen styles conditioned on the component-wise style features. However, its visual quality decreases signiﬁcantly in the one-shot generation scenario. Although these component-based algorithms ad-vance by successfully encoding the diverse local styles, they explicitly depend on the component category inputs to ex-tract style features, and thus the capability of cross-lingual font generation is entirely beyond their reach. Meanwhile, the above methods [3, 25, 26, 31, 36, 38] have a common limitation, that is, they require large amounts of paired data for pixel-level strong supervision. Although DG-Font [32] achieves unsupervised font generation, the generated glyphs often contain characteristic artifacts. Overall, the perfor-mance of the state-of-the-arts is still unsatisfactory.
In this paper, we propose a novel component-guided gen-erative network, namely CG-GAN, which might provide a new perspective for few-shot font generation. The proposed method is inspired by two human behavior: 1) people nat-urally pay more attention to component parts when distin-guishing font styles, and 2) people learn a new glyph by ﬁrst learning its components. Such a human learning scheme is perfectly adopted in our proposed Component-Aware Mod-ule (CAM), supervising the generator at the component level for both styles and contents. Speciﬁcally, CAM ﬁrst employs an attention mechanism for component extraction, acting as a loss function to supervise whether each compo-nent is transferred properly during the generation. Then the learned attention maps, which represent the corresponding component information, are used to conduct per-component style classiﬁcation and realism discrimination. Finally, with the multiple component-level discriminative outputs, CAM can feedback more ﬁne-grained information to the gener-ator by backpropagation, encouraging the generator to si-multaneously focus on three critical aspects at the compo-nent level: style consistency, structural correctness, and im-age authenticity. Therefore, the quality of the generated glyph images is signiﬁcantly boosted. Since CAM is only performed as the component-level supervision during train-ing, it will not bring additional compute time in inference.
In addition, paired training data are not required with our method. Once the model is trained, our generator is capa-ble to generalize to unseen style, unseen content, even other unseen language glyphs, i.e., cross-lingual font generation.
Essentially, our goal is to seek an algorithm that can ef-fectively enhance the representational ability of the genera-tor. The proposed CG-GAN allows to employ style-content disentanglement at a more ﬁned grained level, i.e., the com-ponent level, thus enabling to extract high-quality represen-tations from even a single reference image. The method of utilizing component-level supervision rather than pixel-level strong supervision is a human-like method that shows effectiveness in capturing localized style patterns and pre-serving detailed glyph structures. Compared with exist-ing component-based methods, CG-GAN has two outstand-ing properties:1) the performance improvement is gained by providing more effective supervision for the generator, not by struggling to increase the complexity of the generator; and 2) the generator is able to capture local style patterns without explicit dependency on the predeﬁned component categories, showing remarkable one-shot Chinese font gen-eration and cross-lingual font generation abilities. font generation.
Extensive experiments demonstrate that our proposed
CG-GAN signiﬁcantly outperforms state-of-the-arts in one-shot
Furthermore, by coupling the component-level guidance with a novel framework design,
CG-GAN can ﬂexibly extend to two other different tasks: handwriting generation and scene text editing, producing stunning results that far exceed our expectations, indicating the signiﬁcant potential of our proposed method. 2.