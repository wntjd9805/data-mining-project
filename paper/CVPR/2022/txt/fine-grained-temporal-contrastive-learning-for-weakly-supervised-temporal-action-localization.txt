Abstract
We target at the task of weakly-supervised action lo-calization (WSAL), where only video-level action labels are available during model training. Despite the recent progress, existing methods mainly embrace a localization-by-classiﬁcation paradigm and overlook the fruitful ﬁne-grained temporal distinctions between video sequences, thus suffering from severe ambiguity in classiﬁcation learn-ing and classiﬁcation-to-localization adaption. This paper argues that learning by contextually comparing sequence-to-sequence distinctions offers an essential inductive bias in
WSAL and helps identify coherent action instances. Specif-ically, under a differentiable dynamic programming formu-lation, two complementary contrastive objectives are de-signed, including Fine-grained Sequence Distance (FSD) contrasting and Longest Common Subsequence (LCS) con-trasting, where the ﬁrst one considers the relations of var-ious action/background proposals by using match, insert, and delete operators and the second one mines the longest common subsequences between two videos. Both contrast-ing modules can enhance each other and jointly enjoy the merits of discriminative action-background separation and alleviated task gap between classiﬁcation and localiza-tion. Extensive experiments show that our method achieves state-of-the-art performance on two popular benchmarks.
Our code is available at https : / / github . com /
MengyuanChen21/CVPR2022-FTCL. 1.

Introduction
Action localization is one of the most fundamental tasks in computer vision, which aims to localize the start and end timestamps of different actions in an untrimmed video [41, 63, 67, 75]. In the past few years, the performance has gone through a phenomenal surge under the fully-supervised set-ting. However, collecting and annotating precise frame-…
…
Attention
Mechanism
MIL
Formulation class activation sequence CAS
Temporal Aggregation
Feature extractors  Snippet features
Video-level Classification Loss
Figure 1. Pipeline of the localization-by-classiﬁcation paradigm.
It ﬁrst extracts snippet-level features and adopts attention/MIL mechanisms for learning CAS under video-level supervisions. wise information is a bottleneck and consequently limits the scalability of a fully supervised framework for real-world scenarios. Therefore, weakly-supervised action localiza-tion (WSAL) has been explored [26, 27, 56, 69], where only video-level category labels are available.
To date in the literature, current approaches mainly embrace a localization-by-classiﬁcation paradigm [54, 57, 65, 68], which divides each input video into a series of
ﬁxed-size non-overlapping snippets and aims for generating the temporal Class Activation Sequences (CAS) [56, 71].
Speciﬁcally, as shown in Figure 1, by optimizing a video-level classiﬁcation loss, most existing WSAL approaches adopt the multiple instance learning (MIL) formulation [45] and attention mechanism [56] to train models to assign snip-pets with different class activations. The ﬁnal action lo-calization results are inferred by thresholding and merging these activations. To improve the accuracy of learned CAS, various strategies have been proposed, such as uncertainty modeling [69], collaborative learning [26, 27], action unit memory [42], and causal analysis [37], which have obtained promising performance.
Despite achieving signiﬁcant progress, the above learn-ing pipelines still suffer from severe localization ambi-Action instance 1
Video1
Action-action  distinctions
Action-background  distinctions
Video2
Action instance 2