Abstract
We propose a new 3D spatial understanding task for 3D question answering (3D-QA). In the 3D-QA task, models receive visual information from the entire 3D scene of a rich RGB-D indoor scan and answer given textual ques-tions about the 3D scene. Unlike the 2D-question answer-ing of visual question answering, the conventional 2D-QA models suffer from problems with spatial understanding of object alignment and directions and fail in object local-ization from the textual questions in 3D-QA. We propose a baseline model for 3D-QA, called the ScanQA1, which learns a fused descriptor from 3D object proposals and en-coded sentence embeddings. This learned descriptor cor-relates language expressions with the underlying geomet-ric features of the 3D scan and facilitates the regression of 3D bounding boxes to determine the described objects in textual questions. We collected human-edited question– answer pairs with free-form answers grounded in 3D ob-jects in each 3D scene. Our new ScanQA dataset contains over 41k question–answer pairs from 800 indoor scenes obtained from the ScanNet dataset. To the best of our knowledge, ScanQA is the ﬁrst large-scale effort to perform object-grounded question answering in 3D environments. 1.

Introduction
In recent years, signiﬁcant advances have been achieved in vision-and-language tasks and datasets, and several new datasets have been created to develop models that under-stand textual expressions, such as captions or questions, which are grounded in two-dimensional (2D) images, such as image captioning [12, 42], understanding referring ex-pressions [25, 50], image region and phrase correspon-dence [37], and visual question answering (VQA) [6, 20, 23]. VQA is successful in grasping object features visual-ized in 2D frames. However, when we develop models that understand the spatial information of 3D scenes, such as
“What is between the table and TV set?” or “Where is the
⇤ denotes equally contributed. 1https://github.com/ATR-DBI/ScanQA
Figure 1. We introduce the new task of question answering for 3D modeling. Given inputs of an entire 3D modeling and a linguistic question, models predict an answer phrase and the corresponding 3D-bounding boxes. suitcase located?”, the existing models based on 2D images have several challenges in accurately understanding the 3D world. For example, 2D images lack an accurate sense of the relative directions and distances in the 3D scenes, i.e., the stereoscopic attribute-perception problem. Some ob-jects are hidden by other objects when they overlap, i.e., the occlusion problem. When multiple images are used in 2D-image-based question answering models, such models often encounter difﬁculties in tracking and recognizing whether some objects are the same object between images, i.e., the object localization and identiﬁcation problem.
Currently, 3D spatial-understanding models can be de-veloped for the 3D object localization task of ScanRe-fer [10], the dialog-based localization of ReferIt3D [1], and the 3D object captioning task of Scan2Cap [13]. Embod-ied question answering [17, 46, 49] is an important task for navigating agents in a 3D scene. We consider that 3D spatial understanding datasets contribute to developing models that comprehend the embodied 3D scene and ask and answer questions about the 3D environment as humans do. However, unlike their 2D image counterparts, ques-tion answering datasets on 3D environmental annotations
are still limited in terms of dataset size and question vari-ety because existing datasets often rely on template-based question–answer collections.
In this paper, we propose a 3D question answering (3D-QA) task that uses 3D spatial information instead of 2D images to comprehend real-world information through the question answering form. In the 3D-QA task, models an-swer a question for a 3D scene as well as the object local-ization described in the question. We present the overview of the task in Fig. 1. This 3D-QA task setting is reason-able when external sensors or mobile robots collect sufﬁ-cient visual information to construct a 3D scene before the
QA task. We assume that this is plausible when the model can use the preliminarily captured visual information from the 3D scene because of prior navigation in the scene, such as vision-and-language navigation [5]. This task is also applicable to real-world services that use preliminarily ex-tracted 3D scenes, such as interactive virtual room-viewing services or searching in indoor scenes.
For the 3D-QA task, we developed a novel ScanQA dataset based on RGB-D scans of an indoor scene and an-notations derived from the ScanNet dataset [15]. We auto-matically generated questions from the object captions of
ScanRefer [10] using question generation models. How-ever, these auto-generated questions included many invalid questions; therefore, we ﬁltered the invalid questions and reﬁned them if necessary. We collected free-form answers and object annotations from humans using a newly devel-oped interactive 3D scene viewer. In total, we gathered 41k question–answer pairs with 32k unique questions. We pro-pose a 3D-QA model with textual and 3D scene encoding and several baseline models, including 2D image models (2D-QA), a combination of 3D object localization models
[10, 38], and a question answering model [51]. We con-ﬁrmed that the ScanQA model outperformed the baseline models in most evaluations, including exact matching and image captioning metrics in the proposed ScanQA dataset. 2.