Abstract
We propose to investigate detecting and characterizing the 3D planar articulation of objects from ordinary RGB videos. While seemingly easy for humans, this problem poses many challenges for computers. Our approach is based on a top-down detection system that finds planes that can be articulated. This approach is followed by optimizing for a 3D plane that explains a sequence of detected articu-lations. We show that this system can be trained on a com-bination of videos and 3D scan datasets. When tested on a dataset of challenging Internet videos and the Charades dataset, our approach obtains strong performance. 1.

Introduction
How would you make sense of Figure 1? Behind the set of RGB pixels that make up the video is a real 3D trans-formation consisting of a 3D planar door rotating about an axis. The goal of this paper is to give the same ability to computers. We focus on planar articulation taking the form of a rotation or translation along an axis. This special case of articulation is ubiquitous in human scenes and under-standing it lets a system understand objects ranging from refrigerators and drawers to closets and cabinets. While we often learn about these shapes and articulations with phys-ical embodiment [50], we have no difficulty understanding them from video cues alone, for instance while watching a movie or seeing another person perform an action. We for-malize this ability for computers as recognizing and charac-terizing a class-agnostic planar articulation via a 3D planar segment, articulation type (rotation or translation), 3D ar-ticulation axis, and articulation angle.
This problem is beyond the current state of the art in scene understanding since it requires reconciling single image 3D understanding with dynamic 3D understanding.
While there has been substantial work on 3D reconstruction from a single image [4, 9, 12, 61], including work dedicated to planes [33], these works focus on reconstructing static scenes. On the other hand, while there has been work under-standing articulation, these works often require the place-ment of tags for tracking [36, 41], a complete 3D model or depth sensor [20, 30, 39], or successful 3D human recon-struction [65]. Moreover, making progress is challenging because of data. Unsupervised approaches based on motion analysis [42, 51] require something to track, which breaks in realistic data since many human-made articulated ob-jects are untextured (e.g., refrigerators) or transparent (e.g., ovens). While supervised approaches [30, 38, 39] can per-haps bypass tracking features, they seemingly require ac-cess to large amounts of RGBD data of interactions. For now, this data does not exist, and training on synthetic data can fall short when tested on real data (as our experiments empirically demonstrate).
We overcome these challenges with a learning-based ap-proach that combines both detection and 3D optimization
and is trained with supervision from multiple sources (Sec-tion 4). The foundation of our approach is a top-down de-tection approach that recognizes articulation axes and types and 3D planes; this approachâ€™s outputs are processed with an optimization method that seeks to explain the per-frame results in terms of a single coherent 3D articulation.
Via this model, we show that one can build an under-standing of 3D object dynamics via a mix of 2D supervision on Internet videos of objects undergoing articulation as well as 3D supervision on existing 3D datasets that do not de-pict articulations. To provide 2D supervision, we introduce (Section 3) a new set of 9447 Creative Commons Internet videos. These videos depict articulation with a variety of objects as well as negative samples and come with sparse frame annotations of articulation boxes, axes, and surface normals that can be used for training and evaluating planar articulation models.
Our experiments (Section 5) evaluate how well our ap-proach can recognize and characterize articulation. We evaluate on our new dataset of videos as well as the Cha-rades [48] dataset. We compare with a variety of alter-nate approaches, including bottom-up signals like optical flow [53] and changes in surface normals [3], training on synthetic data [64], as well as systems that analyze human-object interaction [65]. Our approach outperforms these approaches on our data, often even when the baselines are given access to ground-truth location of articulation.
Our primary contributions include: (1) The new task of detecting 3D object articulation on unconstrained ordi-nary RGB videos without requiring RGBD video at train-ing time; (2) A dataset of Internet videos, with sparse frame annotations of articulation boxes, axes, and surface normals that can be used for training and evaluating planar articu-lation models; (3) A top-down detection network and op-timization to tackle this problem, which has strong perfor-mance on the Internet video dataset and Charades. 2.