Abstract
Aggressive data augmentation is a key component of the strong generalization capabilities of Vision Transformer (ViT). One such data augmentation technique is adversar-ial training (AT); however, many prior works [28, 45] have shown that this often results in poor clean accuracy.
In this work, we present pyramid adversarial training (Pyra-midAT), a simple and effective technique to improve ViT’s overall performance. We pair it with a “matched” Dropout and stochastic depth regularization, which adopts the same
Dropout and stochastic depth configuration for the clean and adversarial samples. Similar to the improvements on
CNNs by AdvProp [61] (not directly applicable to ViT), our pyramid adversarial training breaks the trade-off between in-distribution accuracy and out-of-distribution robustness for ViT and related architectures. It leads to 1.82% abso-lute improvement on ImageNet clean accuracy for the ViT-B model when trained only on ImageNet-1K data, while simultaneously boosting performance on 7 ImageNet ro-bustness metrics, by absolute numbers ranging from 1.76% to 15.68%. We set a new state-of-the-art for ImageNet-C (41.42 mCE), ImageNet-R (53.92%), and ImageNet-Sketch (41.04%) without extra data, using only the ViT-B/16 back-bone and our pyramid adversarial training. Our code is publicly available at pyramidat.github.io. 1.

Introduction
One fascinating aspect of human intelligence is the abil-ity to generalize from limited experiences to new environ-ments [30]. While deep learning has made remarkable progress in emulating or “surpassing” humans on classifica-tion tasks, deep models have difficulty generalizing to out-of-distribution data [31]. Convolutional neural networks (CNNs) may fail to classify images with challenging con-texts [22], unusual colors and textures [16, 19, 58] and com-mon or adversarial corruptions [17, 20]. To reliably deploy neural networks on diverse tasks in the real world, we must
*Equal contribution, ordered alphabetically.
†Currently affiliated with Microsoft Azure AI.
Figure 1. Top: Visualization of our learned multi-scale pyramid perturbations. We show the original image, multiple scales of a perturbation pyramid, and the perturbed image. Bottom: We show thumbnails of in-distribution and out-of-distribution datasets, and the gains from applying our technique on each dataset. (Note that lower is better for ImageNet-C.) improve their robustness to out-of-distribution data.
One major line of research focuses on network design.
Recently the Vision Transformer (ViT) [14] and its vari-ants [2, 32, 46, 55] have advanced the state of the art on
In particular, ViT a variety of computer vision tasks. models are more robust than comparable CNN architec-tures [36, 38, 49, 49]. With a weak inductive bias and pow-erful model capacity, ViT relies heavily on strong data aug-mentation and regularization to achieve better generaliza-tion [51, 55]. To further push this envelope, we explore us-ing adversarial training [29, 65] as a powerful regularizer to improve the performance of ViT models.
Prior work [56] suggests that there exists a performance trade-off between in-distribution generalization and robust-ness to adversarial examples. Similar trade-offs have been
observed between in-distribution and out-of-distribution generalization [45, 65]. These trade-offs have primarily been observed in the context of CNNs [7, 45]. However, recent work has demonstrated the trade-off can be broken.
AdvProp [61] achieves this via adversarial training (abbre-viated AT) with a “split” variant of Batch Normalization
[24] for EfficientNet [53]. In our work, we demonstrate that the trade-off can be broken for the newly introduced vision transformer architecture [14].
We introduce pyramid adversarial training (abbreviated as PyramidAT) that trains the model with input images al-tered at multiple spatial scales, as illustrated in Fig. 1; the pyramid attack is designed to make large edits to the image in a structured, controlled manner (similar to augmenting brightness) and small edits to the image in a flexible manner (similar to pixel adversaries). Using these structured, multi-scale adversarial perturbations leads to significant perfor-mance gains compared to both baseline and standard pixel-wise adversarial perturbations. Interestingly, we see these gains for both clean (in-distribution) and robust (out-of-distribution) accuracy. We further enhance the pyramid at-tack with additional regularization techniques: “matched”
Dropout and stochastic depth. Matched Dropout uses the same Dropout configuration for both the regular and adver-sarial samples in a mini-batch (hence the word matched).
Stochastic depth [23, 51] randomly drops layers in the net-work and provides a further boost when matched and paired with matched Dropout and multi-scale perturbations.
Our ablation studies confirm the importance of matched
Dropout when used in conjunction with the pyramid adver-sarial training. They also reveal a complicated interplay be-tween adversarial training, the attack being used, and net-work capacity. We additionally show that our approach is applicable to datasets of various scales (ImageNet-1K and
ImageNet-21K) and for a variety of network architectures such as ViT [14], Discrete ViT [37], ResNet [18], and MLP-Mixer [54]. Our contributions are summarized below:
• To our knowledge, we appear to be the first to demonstrate that adversarial training improves ViT model performance on both ImageNet [12] and out-of-distribution ImageNet robustness datasets [16, 19, 20, 22, 58].
• We demonstrate the importance of matched Dropout and stochastic depth for the adversarial training of ViT.
• We design pyramid adversarial training to gener-ate multi-scale, structured adversarial perturbations, which achieve significant performance gains over non-adversarial baseline and adversarial training with pixel perturbations. using only our pyramid adversarial training and the standard ViT-B/16 backbone. We further improve our results by incorporating extra ImageNet-21K data.
• We perform numerous ablations which highlight sev-eral elements critical to the performance gains. 2.