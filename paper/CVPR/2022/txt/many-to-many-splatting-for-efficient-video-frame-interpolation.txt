Abstract
Motion-based video frame interpolation commonly relies on optical flow to warp pixels from the inputs to the desired interpolation instant. Yet due to the inherent challenges of motion estimation (e.g. occlusions and discontinuities), most state-of-the-art interpolation approaches require sub-sequent refinement of the warped result to generate satis-fying outputs, which drastically decreases the efficiency for multi-frame interpolation. In this work, we propose a fully differentiable Many-to-Many (M2M) splatting framework to interpolate frames efficiently. Specifically, given a frame pair, we estimate multiple bidirectional flows to directly for-ward warp the pixels to the desired time step, and then fuse any overlapping pixels. In doing so, each source pixel ren-ders multiple target pixels and each target pixel can be syn-thesized from a larger area of visual context. This estab-lishes a many-to-many splatting scheme with robustness to artifacts like holes. Moreover, for each input frame pair,
M2M only performs motion estimation once and has a mi-nuscule computational overhead when interpolating an ar-bitrary number of in-between frames, hence achieving fast multi-frame interpolation. We conducted extensive experi-ments to analyze M2M, and found that it significantly im-proves the efficiency while maintaining high effectiveness. 1.

Introduction
Video frame interpolation (VFI) aims to increase frame rates of videos by synthesizing intermediate frames in be-tween the original ones [1, 40]. As a classic problem in video processing, VFI contributes to many practical appli-cations, including slow-motion animation [12], video edit-ing [22], video compression [45], etc.
In recent years, a plethora of techniques for video frame interpolation have been proposed [18,23,24,36,44,47,51]. However, frame in-terpolation remains an unsolved problem due to challenges like occlusions, large motion, and lighting changes.
The referenced research can roughly be categorized into motion-free and motion-based, depending on whether or not
*Work primarily done while Ping was interning at Adobe.
Figure 1. Performance for ×8 interpolation on a “2K” version of X-TEST [39]. Runtimes for all methods were measured using a Titan X GPU. The size of each circle indicates the number of model parameters. Results for related methods include RIFE [11],
SoftSplat [28], AdaCof [16], SepConv [30], XVFI [39], DAIN [2],
ABME [33], and CAIN [32]. We evaluate our proposed M2M splatting using two different off-the-shelf flow estimators, “PWC” denoting PWC-Net [42] and “DIS” denoting DISFLow [15]. cues like optical flow are incorporated [15,42]. Motion-free models typically rely on kernel prediction [6, 9, 31, 34] or spatio-temporal decoding [7, 8, 13], which are effective but limited to interpolating frames at fixed time steps and their runtime increases linearly in the number of desired output frames. On the other end of the spectrum, motion-based ap-proaches establish dense correspondences between frames and apply warping to render the intermediate pixels.
A common motion-based technique estimates bilateral flow for the desired time step and then synthesizes the inter-mediate frame via backward warping [2, 11, 12, 32, 33]. The estimation of bilateral motion is challenging though and in-correct flows can easily degrade the interpolation quality.
As a result, for each time step, these methods typically ap-ply a synthesis network to refine the bilateral flows. An-other motion-based solution is to forward warp pixels to the desired time step via optical flow [1]. However, forward warping is subject to holes and ambiguities where multiple pixels map to the same location. Therefore, image refine-ment networks are commonly adopted to correct remaining artifacts [27, 28, 46]. However, both of these approaches re-Figure 2. (a) Many-to-one splatting versus (b) many-to-many splatting for zooming motion in a scene containing blue and or-ange pixels. M2O splatting may results in holes, while M2M splat-ting allows for a more flexible image formation model. quire significant amounts of compute, and the refinement networks need to be executed for each of the desired inter-polation instants. This decreases their efficiency in multi-frame interpolation tasks since their runtime increases lin-early in the number of desired output frames.
We address these challenges and strive for efficiency with a Many-to-Many (M2M) splatting framework. Specif-ically, our proposed M2M splatting estimates multiple bidi-rectional flow fields and then efficiently forward warps the input images to the desired time step before fusing any over-lapping pixels. Since we directly operate on pixel colors, the quality and resolution of the underlying optical flow play a critical role. For this reason, we first apply an off-the-shelf optical flow estimator [15, 42] to extract the inter-frame motion between the two input frames at a coarse level. Based on this low-resolution optical flow estimate, a
Motion Refinement Network (MRN) predicts multiple flow vectors for each pixel at the full-resolution which we then use for our image synthesis through many-splatting.
Conventional motion-based frame interpolation meth-ods only estimate one inter-frame motion vector for each pixel [2, 11, 27, 28, 32, 33, 46]. However and as shown in
Fig. 2 (a), forward warping with such a motion field mani-fests as many-to-one splatting, leaving unnecessary holes in the warped result. To overcome this limitation, we model a many-to-many relationship among pixels by predicting multiple motion vectors for each of the input pixels, and then forward warping the pixels to multiple locations at the desired time step. As shown in Fig. 2 (b), many-to-many splatting allows for more complex interactions among pix-els, i.e. each source pixel is allowed to render multiple target pixels and each target pixel can be synthesized with a larger area of visual context. Unsurprisingly, many-to-many splat-ting leads to many more overlapping pixels. To merge these, we further introduce a learning-based fusion strategy which adaptively combines pixels that map to the same location.
Since the optical flow estimation step in our pipeline predicts time-invariant correspondence estimates, it only needs to be performed once for a given input frame pair.
Once the many-to-many inter-frame motion has been es-tablished, generating new in-between frames only requires warping and fusing the input images. This is in stark con-trast to previous approaches that leverage refinement net-works [27, 28], allowing us to perform multi-frame interpo-lation an order of magnitude faster as shown in Fig. 1.
In summary, we propose 1) a Motion-Refinement Net-work that estimates a many-to-many relationship between the two input images, 2) a learning-based pixel fusion strat-egy which resolves ambiguities between overlapping pix-els, and 3) a well-motivated Many-to-Many (M2M) splat-ting synthesis model for efficient and effective frame inter-polation. Our experiments demonstrate that M2M achieves high effectiveness with fast speed, e.g. ∼40 ms/f using a
Titan X to perform ×8 interpolation of 2K videos. 2.