Abstract masked input
HOG prediction original image
We present Masked Feature Prediction (MaskFeat) for self-supervised pre-training of video models. Our approach first randomly masks out a portion of the input sequence and then predicts the feature of the masked regions. We study five different types of features and find Histograms of Ori-ented Gradients (HOG), a hand-crafted feature descriptor, works particularly well in terms of both performance and efficiency. We observe that the local contrast normaliza-tion in HOG is essential for good results, which is in line with earlier work using HOG for visual recognition. Our approach can learn abundant visual knowledge and drive large-scale Transformer-based models. Without using ex-tra model weights or supervision, MaskFeat pre-trained on unlabeled videos achieves unprecedented results of 86.7% with MViTv2-L on Kinetics-400, 88.3% on Kinetics-600, 80.4% on Kinetics-700, 38.8 mAP on AVA, and 75.0% on
SSv2. MaskFeat further generalizes to image input, which can be interpreted as a video with a single frame and ob-tains competitive results on ImageNet. 1.

Introduction
Self-supervised pre-training has been phenomenally suc-cessful in natural language processing powering large-scale
Transformers [66] with billion-scale data [6, 21]. The un-derlying idea is an astonishingly simple mask-and-predict task, that is, first masking out some tokens within a text and then predicting the invisible content given the visible text.
Humans have a remarkable ability to predict how the world appears and moves when observing it as a contin-uous stream of spatiotemporal information. Consider the examples in the 1st column of Fig. 1. Even without seeing the masked content, we are able to understand the object structure and draw a rough outline or silhouette of imagined information (up to some details), by using visual knowl-edge about the visible structures.
In this work, we show that predicting certain masked features (e.g. gradient his-tograms in the 2nd column) can be a powerful objective for self-supervised visual pre-training, especially in the video domain which contains rich visual information.
Figure 1. Example HOG predictions on unseen validation input.
Our model is learned by predicting features (middle) given masked inputs (left). Original images (right) are not used for prediction.
More qualitative examples are in the Appendix.
One essential difference between vision and language is that vision has no pre-existing vocabulary to shape the pre-diction task into a well-defined classification problem. In contrast, the raw spatiotemporal visual signal is continuous and dense posing a major challenge to masked visual pre-diction. One immediate solution is to imitate the language vocabulary by building a visual vocabulary that discretizes frame patches into tokens, as explored in BEiT [2, 58].
However, this requires an external tokenizer which can be limited in compute-intensive video understanding scenario.
We present Masked Feature Prediction (MaskFeat), a pre-training objective that directly regresses features of the masked content. Specifically, our approach ingests the masked space-time input with a vision Transformer back-bone [23, 46] and predicts a certain feature representation of the masked content. In this way, the pre-trained model acquires an adequate understanding of the complex space-time structures within dense visual signals.
We study a broad spectrum of feature types, from pixel colors and hand-crafted feature descriptors, to discrete vi-sual tokens, activations of deep networks, and pseudo-labels from network predictions. Our study reveals: (i) Simple histogram of oriented gradients (center col-umn in Fig. 1), as in the popular HOG [18] and SIFT [49] descriptors which dominated visual recognition for over a decade, is a particularly effective target for MaskFeat in terms of both performance and efficiency. (ii) The discretization (tokenization) of visual signals is not necessary for masked visual prediction, and continuous feature regression (i.e. MaskFeat) can work well. (iii) Semantic knowledge from human annotations is not always helpful for MaskFeat, but characterizing local pat-terns seems important. For example, predicting supervised features from CNNs or ViTs trained on labeled data leads to degraded performance.
Our approach is conceptually and practically simple.
Compared to contrastive methods that require a siamese structure and two or more views of each training sample (e.g., [16, 29, 37]), MaskFeat uses a single network with a single view of each sample; and unlike contrastive methods that strongly rely on carefully designed data augmentation,
MaskFeat works fairly well with minimal augmentation.
Compared to previous masked visual prediction meth-ods [2, 62], MaskFeat with HOG does not involve any ex-ternal model, such as a dVAE tokenizer [58] that introduces not only an extra pre-training stage on 250M images, but also non-negligible training overhead in masked modeling.
We show that MaskFeat can pre-train large-scale video models that generalize well. Transformer-based video mod-els, though powerful, are previously known to be prone to over-fitting and heavily rely on supervised pre-training [1, 46] on large-scale image datasets, e.g., ImageNet-21K (IN-21K) [20]. While MaskFeat opens the door for directly pre-training on unlabeled videos which shows enormous bene-fits for video understanding.
Our results on standard video benchmarks are ground-breaking: MaskFeat pre-trained MViTv2-L [46] gets 86.7% top-1 accuracy on Kinetics-400 [43] without using any ex-ternal data, greatly surpassing the best prior number of this kind by +5.2%, and also methods using large-scale image datasets, e.g., IN-21K and JFT-300M [61]. When trans-ferring to downstream tasks, MaskFeat gets unprecedented results of 38.8 mAP on action detection (AVA [35]) and 75.0% top-1 accuracy on human-object interaction classifi-cation (SSv2 [33]). When generalized to the image domain,
MaskFeat also obtains competitive 84.0% top-1 with ViT-B and 85.7% with ViT-L using only ImageNet-1K [20].
Our code will be available in PyTorchVideo1,2 [25, 26]. 1https://github.com/facebookresearch/pytorchvideo 2https://github.com/facebookresearch/mvit
Figure 2. MaskFeat pre-training. We randomly replace the in-put space-time cubes of a video with a [MASK] token and di-rectly regress features (e.g. HOG) of the masked regions. After pre-training, the Transformer is fine-tuned on end tasks. 2. Method
We start by describing MaskFeat and its instantiations for video and image understanding in §2.1. We then intro-duce and discuss five candidates for target features in §2.2. 2.1. Masked Feature Prediction
Our method performs a masked visual prediction task, motivated by humans’ ability to inpaint masked visual con-tent up to some details. The task first randomly masks out a few space-time cubes of a video, and then predicts the masked ones given the remaining ones. By modeling masked samples, the model attains video understanding in the sense of recognizing parts and motion of objects. For in-stance, to solve the examples in Fig. 1, a model has to first recognize the objects based on the visible area, and also know what the objects typically appear and how they usu-ally move to inpaint the missing area.
One key component of the task is the prediction target.
Masked language modeling tokenizes the corpus with a vo-In contrast, the raw cabulary to serve as the target [21]. visual signal is continuous and high-dimensional and there is no natural vocabulary available. In MaskFeat, we pro-pose to predict features of the masked area. And the super-vision is provided by features extracted from the original, intact sample. We use a wide interpretation of features [12], from hand-crafted feature descriptors, to activations of deep networks. The choice of the target feature largely defines the task and impacts the property of the pre-trained model, which we discuss in §2.2.
Instantiations. We first describe MaskFeat for video input.
A video is first divided into space-time cubes as in typi-cal video Vision Transformers [26, 46]. The cubes are then projected (i.e. convolved) to a sequence of tokens. To per-form masking, some of the tokens in the sequence are ran-domly masked out by being replaced with a [MASK] token.
This is a learnable embedding indicating masked patches. A block of tokens is masked together which we detail in §4.3.
To make a prediction, the token sequence after [MASK] to-ken replacement, with positional embedding added, is pro-cessed by the Transformer. Output tokens corresponding to the masked cubes are projected to the prediction by a linear layer. The prediction is simply the feature of the 2-D spatial patch temporally centered in each masked cube (see discus-sions in Appx. Tab. 11). The number of output channels is adjusted to the specific target feature (e.g., 3×16×16 if pre-dicting RGB colors of pixels in a 16×16 patch). The loss is only operated on the masked cubes. Our instantiation is inspired by BERT [21] and BEiT [2], illustrated in Fig. 2.
MaskFeat can be easily instantiated in the image domain, which can be interpreted as a video with one single frame.
Most operations are shared, except that there is no tempo-ral dimension and each token now represents only a spatial patch instead of a space-time cube. 2.2. Target Features
We consider five different types of target features. The targets are categorized into two groups: 1) one-stage tar-gets that can be directly obtained including pixel colors and
HOG, and 2) other two-stage targets extracted by a trained deep network or teacher. As predicting two-stage targets is effectively learning from a trained deep network teacher, it resembles a form of model distillation [40]; thereby, an ex-tra computational cost of pre-training and inference of the teacher model is inevitable. The five feature types are:
Pixel colors. The most straightforward target is arguably the colors of video pixels. Specifically, we use RGB values that are normalized by the mean and the standard devia-tion of the dataset. We minimize the ℓ2 distance between the model’s prediction and the ground-truth RGB values. A similar idea has been explored in [55] as a image inpainting task and in [2, 23] for masked image prediction. Though simple, pixels as target have a potential downside of over-fitting to local statistics (e.g. illumination and contrast vari-ations) and high-frequency details, which are presumably insignificant [60] for interpretation of visual content.
HOG. Histograms of Oriented Gradients (HOG) [18] is a feature descriptor that describes the distribution of gradient orientations or edge directions within a local subregion. A
HOG descriptor is implemented by a simple gradient filter-ing (i.e. subtracting neighboring pixels) to compute magni-tudes and orientations of gradients at each pixel. The gradi-ents within each small local subregion or cell are then accu-mulated into orientation histogram vectors of several bins, voted by gradient magnitudes. The histogram is normalized to unit length. These features are also used in well-known
SIFT [49] descriptors for detected keypoints or in a dense fashion for classification [12]. Similarly, we extract HOG on a dense grid for the whole image, which suits the predic-tion target for randomly masked patches.
HOG is characteristic of capturing local shapes and appearances while being partially invariant to geometric changes as long as translations are within the spatial cell and rotations are smaller than orientation bin size. Fur-ther, it provides invariance to photometric changes as image gradients and local contrast normalization absorb bright-ness (e.g. illumination) and foreground-background con-trast variation. These invariances are vital for good re-sults when using HOG for pedestrian detection in both im-age [18] and video [19] domains. In accordance to this, our studies (§5.2) reveal local-contrast normalization in HOG is also essential for MaskFeat pre-training.
Finally, HOG computation is cheap and introduces neg-ligible overhead. It can be implemented as a two-channel convolution to generate gradients in x and y axis (or by subtracting neighboring horizontal and vertical pixels) , fol-lowed by histogramming and normalization.
Our method then simply predicts the histograms sum-marizing masked patches. Instead of computing HOG only on masked patches, we first obtain a HOG feature map on the whole image and then split the map into patches.
In this way, we reduce padding on boundaries of each masked patch. The histograms of masked patches are then flattened and concatenated into a 1-D vector as the target feature. Our loss minimizes the ℓ2 distance between the predicted and original HOG feature. We collect HOG in each RGB chan-nel to include color information which can slightly improve its performance (§5.2).
Discrete variational autoencoder (dVAE). To address the continuous high-dimensional nature of visual signals,
DALL-E [58] proposes to compress an image with a dVAE codebook. In particular, each patch is encoded into a token which can assume 8192 possible values using a pre-trained dVAE model. Now the task is to predict the categorical dis-tribution of the masked token by optimizing a cross-entropy loss, as explored in BEiT [2]. However, there is an extra computational cost induced by pre-training the dVAE and tokenizing images alongside masked feature prediction.
Deep features. In comparison to discretized tokens, we consider directly using continuous deep network features as the prediction target. We use a pre-trained model to pro-duce features as a teacher, either a CNN or ViT, and our loss minimizes the cosine distance (i.e. mean squared error of ℓ2-normalized features).
For CNN teachers, we use the last layers’ features cor-responding to the masked patches and for ViT we use the respective output patch tokens. We mainly compare fea-tures from self-supervised models, which are considered to contain more diverse scene layout [9] and preserve more visual details [74] than features from supervised models. (Though, the usage of human annotations makes the pre-training technically not self-supervised.) Supervised fea-tures are expected to be more semantic as they are trained through human annotations. Similar to dVAE, a non-trivial amount of extra computation is involved when using extra model weights for masked feature generation.
Pseudo-label. To explore an even more high-level seman-tic prediction target, we consider predicting class labels of masked patches. We utilize labels provided by Token
Labeling [42], where each patch is assigned an individual location-specific IN-1K pseudo-label. This class label map is generated by a pre-trained high-performance supervised deep network [5] teacher. The masked feature prediction stage is optimized by a cross-entropy loss.
We next study the features discussed in this section. 3. Study: Target Features for MaskFeat
Settings. We use a pre-training and fine-tuning protocol, following BEiT [2]. We pre-train MViTv2-S, 16×4 [46] with MaskFeat on Kinetics-400 (K400) [43] training set for 300 epochs. We also apply MaskFeat on images, where we pre-train ViT-B [23] on the ImageNet-1K (IN-1K) [20] training set for 300 epochs. We report top-1 fine-tuning ac-curacy (%) on both datasets. We pre-train and fine-tune all targets with the same recipe which we find generally good in practice. For targets that involve a teacher model, we use official models released by the authors.
Most features are compared on both video and image do-mains except pseudo-label for which the pseudo-label map feature type scratch pixel image descriptor dVAE unsupervised feature supervised feature one-stage
-✓
✓
✗
✗
✗ variant
MViTv2-S [46]
RGB
HOG [18]
DALL-E [58]
DINO [9], ViT-B
MViT-B [26] top-1 81.1 80.7 82.2 81.7 82.5 81.9
Table 1. Comparing target features for MaskFeat (video). All variants are pre-trained for 300 epochs on MViTv2-S, 16×4 with
MaskFeat. We report fine-tuning top-1 on K400. Default is gray . is only available on IN-1K [42]. Results are summarized in
Tables 1 (video) and 2 (image), analyzed next:
One-stage methods. The fine-tuning accuracy for pixel color prediction in Tables 1 & 2 shows, that compared to the from-scratch baselines, regressing RGB colors produces a slight drop of -0.4% for video classification and a relatively small gain of +0.7% for image. Even though our predicting pixel colors result on IN-1K (82.5%) is better than that re-ported in BEiT [2] (81.0%), we similarly observe that pixel values are not ideal direct targets, presumably because they are considered to be too explicit [58]. In comparison, HOG, by summarizing the local gradient distribution, contributes to large improvements of +1.1% on K400 and +1.8% on IN-1K over the from-scratch baselines without any extra model which is typical in two-stage methods.
Two-stage methods. First, dVAE improves by +0.6% for
K400 and +1.0% for IN-1K over their from-scratch base-lines. This is better than pixel colors, but outperformed by
HOG which does not use an external model.
Next, compared to dVAE, we study MaskFeat to predict continuous, unsupervised features: We compare DINO [9] (with ViT-B) and MoCo [15, 17] (with ResNet50 [38] and
ViT-B), all pre-trained on IN-1K, even for the video pre-training. Unsupervised features contribute a notable gain for both video and image classification: The DINO variant achieves a gain of +1.4% on K400 and +2.2% on IN-1K compared to their baselines. However, this approach has two main drawbacks, (i) the unsupervised feature extractor needs to be pre-trained e.g. worth over thousand epochs in the case of DINO, (ii) the unsupervised features need to be computed on the target data. Still, MaskFeat w/ DINO and
MoCo v3 features boosts their original accuracy [9, 17].
Finally, supervised features (from ResNet50 or ViT-B) as well as token labels, though utilizing human annotations, lag behind unsupervised features and HOG. In fact, we no-tice significant over-fitting during fine-tuning for supervised features and token labels, suggesting that predicting fea-tures learned from class labels is not suitable in MaskFeat. feature type scratch pixel colors image descriptor dVAE token unsupervised feature unsupervised feature unsupervised feature supervised feature supervised feature pseudo-label one-stage
-✓
✓
✗
✗
✗
✗
✗
✗
✗
Table 2. Comparing target features for MaskFeat (image). For all targets, ViT-B is pre-trained with MaskFeat for 300 epochs on IN-1K.
We report 100-epoch fine-tuning accuracy on IN-1K. For two-stage targets, we report the teacher architecture, number of parameters (M), and effective epoch† on IN-1K. The default entry is marked in gray . The plot on the left visualizes the acc/epoch trade-off of the table.
† Different teachers use different training strategies. dVAE is pre-trained on an external 250M dataset, while self-supervised methods require multi-view training. To measure the cost in a unified way, we normalize the number of epochs by the cost of one epoch on IN-1K training set with one 2242 view. variant
DeiT [63]
RGB
HOG [18]
DALL-E [58]
MoCo v2 [15]
MoCo v3 [17]
DINO [9] pytorch [53]
DeiT [63]
Token Labeling [42] NFNet-F6 arch.
---dVAE
ResNet50
ViT-B
ViT-B
ResNet50
ViT-B param.
---54 23 85 85 23 85 438 top-1 81.8 82.5 83.6 82.8 83.6 83.9 84.0 82.6 81.9 78.8 epoch†
---1199 800 600 1535 90 300 360
We hypothesize that class label being invariant to local shapes and textures of the same object disables the ability of MaskFeat to model object’s internal structure.
Discussion. Our results suggest that a broad spectrum of image features can serve as targets in masked visual predic-tion, and provide gains over the train-from-scratch baseline.
We find that although masked language modeling [21] orig-inally predicts the categorical distribution over a pre-defined vocabulary, discretization as in BEiT [2] is not required for vision. We find that continuous unsupervised features and image descriptors can be strong prediction targets, while the latter come without cost compared to the former which also entail a form of model distillation [41, 63]. An interest-ing observation is that supervisedly trained target features produce poor results, which might relate to class-level spe-cific information being present in features [3, 75] that is too global for local mask modeling. Overall, considering the trade-off between performance and computational cost, pre-dicting HOG holds a good balance and therefore we use it as default feature for MaskFeat in the following sections. 4. Experiments: Video Recognition
Settings. We evaluated with both base and large models of
MViTv2 [46]. The models are pre-trained only on video clips in the training set of K400 [20] without labels. Our augmentation includes random resized cropping and hori-zontal flipping. Our models are pre-trained and fine-tuned at 2242 resolution if not specified. We randomly mask out 40% of total space-time cubes with cube masking detailed in §4.3. More implementation details are in Appx. C.1. 4.1. Main Results on Kinetics
Kinetics-400. Table 3 compares MaskFeat with prior work on K400 dataset. From top to bottom, it has three sections.
The first section presents prior work using CNNs, which commonly do not use any pre-training. The second section presents representative Transformer-based methods, most of which are heavily dependent on supervised pre-training on large-scale image datasets.
The third section shows direct comparisons on MViTv2 models. Note that these models are strong baselines and are state-of-the-art for training-from-scratch on their own.
Still, 300 epochs of MaskFeat pre-training improve the scratch MViTv2-S, 16×4 [46] with 81.1% top-1 accuracy by +1.1%. The suffix 16×4 represents that the model takes 16 frames with a temporal stride of 4 as input for training.
Next, we explore larger models for which supervised
IN-21K pre-training is popular. Pre-trained with MaskFeat for 800 epochs on K400, the large model MViTv2-L, 16×4 reaches 84.3% top-1, outperforming its scratch baseline by a large margin of +3.8% and its IN-21K supervised coun-terpart by +0.8%. Similar to the image domain, MaskFeat model
Two-Stream I3D [11]
SlowFast 16×8 +NL [28]
X3D-XL [27]
MoViNet-A6 [44]
MViT-B, 64×3 [26]
ViT-B-TimeSformer [4]
Swin-L, 32×2 [48]
ViViT-L [1]
Swin-L↑384, 32×2 [48]
ViViT-H [1]
TokenLearner [59]
Florence↑384 [72]
SwinV2-G↑384 [47]
-----pre-train top-1 top-5 FLOPs×views Param 216 × NA 25 71.6 90.0 234×3×10 60 79.8 93.9 48×3×10 11 79.1 93.9 386×1×1 31 81.5 95.3 455×3×3 37 81.2 95.1 2380×3×1 121 80.7 94.7
Sup., IN-21K 604×3×4 197
Sup., IN-21K 83.1 95.9 3980×3×1 308
Sup., JFT-300M 83.5 94.3 84.9 96.7 2107×5×10 200
Sup., IN-21K 3981×3×4 654
Sup., JFT-300M 84.9 95.8
Sup., JFT-300M 85.4 N/A 4076×3×4 450
N/A×3×4 647
Text, FLD-900M 86.5 97.3
MIM + Sup.
IN-21K+Ext-70M
-N/A×5×4 3000 86.8 N/A
--71×1×10 71×1×10 71×1×10 377×1×10 377×1×10 377×1×10 377×1×10 2063×3×5 2063×3×5 2063×3×5 2828×3×4 3790×3×4 3790×3×4
MViTv2-S, 16×4 [46] 36 81.1 94.9
MViTv2-S, 16×4 [46] 36
Sup., IN-21K 82.6 95.3
MViTv2-S, 16×4 [46]
MaskFeat, K400 82.2 95.1 36
MViTv2-L, 16×4 [46] 218 80.5 94.1
MViTv2-L, 16×4 [46] 218
Sup., IN-21K 83.5 95.9
MViTv2-L, 16×4 [46]
MaskFeat, K400 84.3 96.3 218
MViTv2-L, 16×4 [46]
MaskFeat, K600 85.1 96.6 218
MViTv2-L↑312, 32×3 [46] 218 82.2 94.7
MViTv2-L↑312, 32×3 [46] Sup., IN-21K 218 85.3 96.6
MViTv2-L↑312, 32×3 [46] MaskFeat, K400 86.3 97.1 218
MViTv2-L↑312, 40×3 [46] MaskFeat, K400 86.4 97.1 218
MViTv2-L↑352, 40×3 [46] MaskFeat, K400 86.7 97.3 218
MViTv2-L↑352, 40×3 [46] MaskFeat, K600 87.0 97.4 218
Table 3. Comparison with previous work on Kinetics-400. We report the inference cost with a single “view” (temporal clip with spatial crop) × the number of views (FLOPs×viewspace×viewtime).
Each “view” consists of T frames with τ temporal stride, T × τ .
Magnitudes are Giga (109) for FLOPs and Mega (106) for Param.
Accuracy of models trained with external data is de-emphasized. is more significant with larger models, showing that our ap-proach is salable to model capacity. The result also suggests that MaskFeat adapts to different model types, as MViTv2 is a Transformer model with convolutions.
We further explore the data scalability of MaskFeat. In particular, we pre-train MViTv2-L, 16×4 with Kinetics-[10] containing ~387K training videos, 600 (K600) 1.6× more than K400. We pre-train for 300 epochs on K600 to use a slightly smaller training budget as the 800 epochs on K400. We again fine-tune on K400 and observe that pre-training on K600, without any labels, contributes to another
+0.8% gain over K400 pre-training to reach 85.1% top-1.
Next, we fine-tune the 84.3% top-1 MViTv2-L, 16×4
MaskFeat model for 30 epochs to larger spatial sizes of 3122 and 3522, as well as longer temporal durations of 32 and 40 frames with a temporal stride of three. The resulting extra large model MViTv2-L↑352, 40×3, without using any external data, achieves a top accuracy of 86.7%. Previously,
Transformer-based video models heavily rely on supervised pre-training on large image datasets to reach high accuracy.
For example, 84.9% top-1 Swin-L↑384 [48] with IN-21K and 84.9% ViViT-H [1] with JFT-300M [61]. MaskFeat opens the door for directly pre-training on unlabeled videos which shows enormous benefits for video understanding, as we can boost the previous best accuracy without external data on K400 (81.5% MoViNet-A6 [44]) by +5.2%.
K400
K600 pre-train center 23.8 27.3 27.5 27.4 28.7 31.0
--model full FLOPs Param
SlowFast R101, 8×8 [28] 53 138
-MViT-B, 64×3 [26] 36 455
-SlowFast 16×8 +NL [28] 59 296
-X3D-XL [27] 11 48
-MViT-B-24, 32×3 [26] 236
-53
Object Transformer [70] 86 244
-ACAR R101, 8×8 +NL [52] 31.4 N/A N/A
ACAR R101, 8×8 +NL [52] 33.3 N/A N/A
MViTv2-L↑312, 40×3 [46], Sup. 2828 218
MViTv2-L↑312, 40×3 [46], MaskFeat 36.3 37.5 2828 218 37.8 38.8 2828 218
MViTv2-L↑312, 40×3 [46], MaskFeat
Table 4. Transferring to AVA v2.2 [35]. We use single center crop inference (center) following MViT [26] and full resolution inference (full) to compare to the 2020 AVA Challenge winner
ACAR [52]. Inference cost is with the center strategy.
IN-21K+K400 31.6
K400
K600
K700
-Our best 87.0% top-1 accuracy is achieved by fine-tuning the 85.1% MViTv2-L, 16×4 pre-trained with MaskFeat on 387K training videos in K600 using no labels.
Our results with just K400 (86.7%) is already similar to recent 86.5% Florence [72] and 86.8% SwinV2-G [47].
Florence uses 900M curated text-image pairs. SwinV2-G utilizes a giant model with three billion parameters, and is first self-supervisedly then supervisedly pre-trained on a large dataset of IN-21K plus 70M in-house images. The ef-ficiency of our approach in terms of parameter count, com-pute cost, data, and annotation suggests again the advantage of MaskFeat directly pre-training on unlabeled videos. 4.2. Transfer Learning
We evaluate downstream transfer learning with the Ki-netics MViTv2-L↑312, 40×3 in Table 3 and Appx. 9a.
Action detection. AVA v2.2 [35] is a benchmark for spa-tiotemporal localization of human actions. We fine-tune the
MViTv2-L↑312, 40×3 Kinetics models on AVA v2.2. De-tails are in Appx. C.2. Table 4 reports mean Average Preci-sion (mAP) of our MaskFeat models compared with prior state-of-the-art. MaskFeat only using K400 contributes to a significant gain of +4.7 mAP over its IN-21K pre-trained counterpart using identical architectures. By uti-lizing a larger video dataset, K600, the model reaches an unprecedented accuracy of 38.8 mAP with full resolution testing, greatly surpassing all previous methods, including
ActivityNet challenge winners. The strong performance of
MaskFeat on AVA suggests a clear advantage of masked modeling on video over supervised classification on image pre-training for this localization-sensitive recognition task.
Human-object interaction classification. We fine-tune the MViTv2-L↑312, 40×3 Kinetics models in Table 3 and
Appx. 9a to Something-Something v2 (SSv2) [33] which focuses on human-object interaction classification. Table 5 presents the results and details are in Appx C.3. In contrast to Kinetics, SSv2 requires fine-grained motion distinctions and temporal modeling to distinguish interactions like pick-ing something up and putting something down.
K400
K600 pre-train top-1 top-5 FLOPs Param model
SlowFast, R101, 8×8 [28] 53 63.1 87.6 106
MViT-B, 64×3 [26] 455 67.7 90.9 37
MViT-B-24, 32×3 [26] 68.7 91.5 236 53.2 68.1 91.2 1185 109
Mformer-L [56] 69.5 91.5 1259 148
ORViT Mformer-L [39]
Swin-B, 32×3 [48] 89 69.6 92.7
IN-21K+K400 73.3 94.1 2828 218
MViTv2-L↑312, 40×3 [46], Sup.
MViTv2-L↑312, 40×3 [46], MaskFeat 74.4 94.6 2828 218
MViTv2-L↑312, 40×3 [46], MaskFeat 75.0 95.0 2828 218
Table 5. Transferring to Something-Something v2 [33]. We report FLOPs with a single “view”. All entries use one temporal clip and three spatial crops (inference cost is FLOPs×3×1).
K400
K600
IN-21K+K400 321
Despite the differences between the supervised tasks of
Kinetics and SSv2, pre-training on Kinetics without super-vised labels using MaskFeat still contributes to a large gain on fine-tuning accuracy of SSv2. Specifically, MaskFeat with only K400 data contributes to +1.1% top-1 over its IN-21K+K400 pre-trained counterpart. By utilizing the larger
K600, the model reaches an unprecedented 75.0% top-1 ac-curacy, surpassing all previous methods. This suggests that
MaskFeat can learn spatiotemporal representations from unlabeled Kinetics data which is known as appearance-biased, through self-supervised masked feature prediction. 4.3. Ablations for Video Recognition
The ablations are with MViTv2-S, 16×4 pre-trained for 300 epochs and fine-tuned for 200 epochs on K400. More ablations (e.g. on masking ratio) are in Appx. A.
Masking strategy. We study the masking strategy for spa-tiotemporal video data. In video, tokens sharing the same spatial position usually also share visual patterns. There-fore, we explore how to handle this redundancy brought by the addition of the temporal dimension. We consider three different ways of masking and present the results in Table 6.
All entries share the same 40% masking ratio. frame 81.0 (-1.2) tube 81.9 (-0.3) masking top-1 cube 82.2
Table 6. Masking strategy. Varying the strategy of masking in spatiotemporal data. The default entry is highlighted in gray .
First, we consider “frame” masking, which indepen-dently masks out consecutive frames. This strategy mostly masks different spatial blocks in consecutive frames, but the model could temporally “interpolate” between frames to solve the task. This strategy only obtains 81.0% top-1.
Second, we consider “tube” masking. Namely, we first sample a 2-D mask map by block-wise masking as for im-ages, and then extend the 2-D map by repeating it in the temporal dimension. Thus, the masked area is a straight tube in a video clip, in which the spatially masked area is the same for every frame. Tube masking refrains from rely-ing on the temporal repetition to predict the masked content in static video. It leads to 81.9% accuracy.
Third, we consider “cube” masking, which includes both spatial and temporal blocks that are masked out together.
extra data
-IN-21K
--DALL-E
-extra model
--momentum ViT momentum ViT dVAE
-pre-train scratch [63] supervised384 [23]
MoCo v3 [17]
DINO [9]
BEiT [2]
MaskFeat (w/ HOG)
Table 7. Comparison with previous work on IN-1K. All entries are pre-trained on IN-1K train split, except supervised384 using
IN-21K. MoCo v3 and DINO use momentum encoder. BEiT uses 250M DALL-E data to pre-train dVAE. All entries are trained and evaluated at image size 2242 except supervised384 at 3842.
ViT-B ViT-L 81.5 81.8 85.2 84.0 84.1 83.2
-82.8 85.2 83.2 85.7 84.0
This is achieved by sampling random “cubes” of tokens un-til a certain masking ratio is reached. Cubes are sampled by first creating a 2-D block at a random time step, then extending in the temporal dimension with a random num-ber of consecutive frames. Therefore, cube masking can be considered as an generalization of tube and frame masking.
It produces 82.2% accuracy when used for pre-training.
Overall, the results in Table 6 show that cube masking performs best, suggesting both spatial and temporal cues are helpful in masked spatiotemporal prediction. 5. Experiments: Image Recognition
Settings. The evaluation protocol is pre-training followed by end-to-end fine-tuning. We use vanilla base and large models in ViT [23] without modification. Our models are pre-trained at 2242 resolution on IN-1K [20] training set without labels. We use minimal data augmentation: ran-dom resized cropping and horizontal flipping. We randomly mask out 40% of total image patches with block-wise mask-ing following BEiT [2]. More details are in Appx. C.1. 5.1. Main Results on ImageNet-1K
In Table 7 we compare MaskFeat to previous work in-cluding from-scratch, IN-21K supervised pre-training, and previous self-supervised methods. We pre-train MaskFeat for 1600 epochs here while for 300 epochs in Table 2.
The fine-tuning schedule is the same everywhere and rather short, 100 epochs for ViT-B and 50 epochs for ViT-L.
We observe that MaskFeat pre-training significantly boosts the scratch baselines for both ViT-B and ViT-L. Our approach at image size 2242 is on par with (ViT-B), or even outperforms (ViT-L) supervised pre-training on IN-21K that has 10×more images and labels at image size 3842. It has been shown [23] that ViT models are data-hungry and re-quire large-scale supervised pre-training, possibly due to the lack of typical CNN inductive biases. Our results sug-gest that MaskFeat pre-training can overcome this without external labeled data by solving our feature inpainting task.
Interestingly, more gains are observed on ViT-L compared with ViT-B, suggesting that it is scalable to larger models.
Compared to self-supervised pre-training approaches,
MaskFeat is more accurate and simpler. DINO [9] and norm. top-1 none 82.2
ℓ1 82.8
ℓ2 83.6 channel top-1 gray 83.2 opp. rgb 83.6 83.5 (a) Contrast normalization. (b) Color channel.
#bins top-1 6 9 83.4 83.6 12 83.5 cell size top-1 4×4 83.2 8×8 83.6 16×16 83.2 (c) Orientation bins. (d) Spatial cell size.
Table 8. HOG implementation. (a) Local contrast normaliza-tion plays a key role, and (b) MaskFeat benefits from color infor-mation; this is in line with HOG/SIFT studies on image recogni-tion [12, 18]. HOG as target is (c) robust to the number of orienta-tion bins, and (d) benefits from 8 × 8 spatial cell. Opp. represents opponent color space [64]. Default entries are marked as gray .
MoCo v3 [17] are contrastive methods that require multi-view training and carefully designed augmentation, while
MaskFeat only uses single-views and minimal augmenta-tion. See Tab. 15 in Appx. for ablation on data augmen-tation of MaskFeat. Compared with BEiT [2], MaskFeat gets rid of the dVAE tokenizer, which introduces both an extra pre-training stage on the 250M DALL-E dataset, and a non-negligible inference overhead during masked predic-tion. While MaskFeat simply calculates HOG features.
MaskFeat in Table 7 is pre-trained for 1600 epochs with a single 2242 view. DINO uses multiple global-local views and an extra momentum encoder, leading to 1535 effective epochs† (Table 2). MoCo v3 saturates after 600 effective epochs [17]. BEiT is pre-trained for 800 epochs on IN-1K but requires another 1199 effective epochs for dVAE.
We also train the best model in Table 2, MaskFeat w/
DINO, for 1600 epochs and it reaches 84.2%; however, this uses a separate ViT-B model that is trained with another
~1535 effective epochs using DINO. MaskFeat w/ HOG can reach 84.0% without extra model. 5.2. Ablations for Image Recognition
We ablate the design choices of MaskFeat in the image domain first. We use ViT-B pre-trained for 300 epochs by default and report fine-tuning top-1 accuracy (%) on IN-1K.
More ablations (e.g. on training epochs) are in Appx. B.
HOG implementation. We ablate HOG implementation details in Table 8. We first investigate the local contrast normalization in HOG, which is key to its performance in image recognition [18]. It is applied by normalizing each histogrammed vector of local 8×8 pixel cells, which leads e.g. to local invariance in illumination change. We show in Table 8a that normalization is essential for MaskFeat.
Compared with default ℓ2 normalization, using ℓ1 normal-ization results in a 0.8% drop and not using any normaliza-tion causes a large -1.4% drop. Similar results are reported in [18] for directly using HOG for image recognition.
We next investigate the effectiveness of color informa-tion in Table 8b. Gray refers to extracting HOG on gray-scale images, which only contains intensity information.
masked input pixel prediction
HOG prediction original image 6.