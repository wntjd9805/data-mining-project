Abstract
Deep neural networks are vulnerable to Trojan attacks.
Existing attacks use visible patterns (e.g., a patch or im-age transformations) as triggers, which are vulnerable to human inspection. In this paper, we propose stealthy and efﬁcient Trojan attacks, BPPATTACK. Based on existing bi-ology literature on human visual systems, we propose to use image quantization and dithering as the Trojan trig-ger, making imperceptible changes.
It is a stealthy and efﬁcient attack without training auxiliary models. Due to the small changes made to images, it is hard to inject such triggers during training. To alleviate this problem, we pro-pose a contrastive learning based approach that leverages adversarial attacks to generate negative sample pairs so that the learned trigger is precise and accurate. The pro-posed method achieves high attack success rates on four benchmark datasets, including MNIST, CIFAR-10, GTSRB, and CelebA. It also effectively bypasses existing Trojan de-fenses and human inspection. Our code can be found in https://github.com/RU- System- Software-and-Security/BppAttack. 1.

Introduction
Deep Neural Networks (DNNs) have achieved superior performance in many computer vision tasks [8, 21, 52]. Re-cent studies show that DNNs are vulnerable to adversarial attacks such as adversarial examples [17, 45], membership inference attacks [54, 58], model stealing [50, 62], etc. In this paper, we focus on Trojan attacks [10, 13, 18, 35, 38, 41, 53]. The adversary injects a secret Trojan behavior dur-ing training, which can be activated at runtime by stamping a Trojan trigger to the image. Such triggers can be image patches [18], watermarks [41], image ﬁlters [1,40] and even learned image transformation models [10, 13, 37].
Trojan attacks [18] are severe threats to the trustworthi-ness of DNN models. Liu et al. [41] demonstrates the pos-sibility of attacking face recognition, speech recognition, and autonomous driving systems. Such attacks are gener-ally feasible in most training scenarios, including federated learning, unsupervised learning, and so on [4, 29, 68]. With the deployment of DNN based computer vision models, it is a critical challenge in our community.
Existing Work: Most existing Trojan attacks leverage in-put patterns as triggers. For example, BadNets [18] uses a yellow pad as its trigger. Recent works [40] try to lever-age image ﬁlters as triggers, which are input dependent and dynamic, making them hard to detect. To further improve the quality of Trojan triggers, Doan et al. [13] train an auxil-iary image transformation model and use the transformation function as its trigger. Other works have adopted similar ideas [10, 37].
One problem of existing attacks is that they are vulnera-ble to human inspections. Once a set of attack inputs are found, it is not difﬁcult to identify the trigger or train a model to simulate the trigger. There are also online de-tection methods to identify such attack samples, such as
STRIP [16]. Even for trained transformations as triggers, it is hard for them to guarantee that the generated images have imperceptible changes. This is because it is hard to formulate human visual systems as a mathematical func-tion, which makes it hard to optimize. Due to the relatively large changes in inputs and limitations of existing poisoning methods, it is also possible for reverse engineering based defense methods [7,40,66] to recover part of the trigger and identify if a model has a Trojan. Moreover, recent works on generating high-quality triggers typically leverage trained auxiliary models, which is time-consuming and inefﬁcient.
Our Work:
In this paper, we propose one new attack,
BPPATTACK. Based on existing literature on human visual systems, we identify that humans inspectors are insensitive to small changes of color depth. In this attack, we try to exploit vulnerabilities in human visual systems. Thus, we propose to reduce the bit-per-pixel (BPP) to conduct an im-perceptible attack, which can also bypass existing defenses mainly because of the small changes made to the input do-main. We achieve our goal by performing a deterministic yet input-dependent image transformation, i.e., image quan-Clean
BadNets
Blend
SIG
Filter
ISSBA
WaNet
Ours t u p n
I l a u d i s e
R
Fig. 1. Comparison of examples generated by different Trojan attacks (i.e., BadNets [18], blending-based attack [9], SIG [2], ﬁlter-based attack [1, 10], ISSBA [37] and WaNet [48]). For each attack, we show the Trojan sample (top) and the magniﬁed (×5) residual (bottom). tization and dithering. Due to the small-sized transforma-tion as triggers, it is more challenging to train the model and inject the trigger. To overcome this issue, we also propose a contrastive learning and adversarial training method based approach to training on the poisoned dataset. By doing so, we do not require training auxiliary models, making the at-tack fast and also input dependent. Moreover, our attack exploits vulnerabilities in human visual systems, making it human imperceptible when the attack settings are properly set. Fig. 1 shows the comparison of our attack and existing attacks using attack samples and residuals.
Our contributions are summered as follows:
• We propose a new attack that exploits human visual systems. We design an effective and efﬁcient attack that leverages image quantization and dithering. It per-forms deterministic input-dependent image transfor-mations, making it fast and dynamic. We also propose a contrastive learning and adversarial training based approach to enhance the poisoning process to inject such human imperceptible triggers.
• We evaluate our prototype on four different datasets and seven different network architectures. Results show that our attack achieves a 99.92% attack suc-cess rate on average. Our human study also conﬁrms that it is 1.60 times better than the SOTA approaches when facing human inspections. Results on existing defenses also conﬁrm that our attack can effectively bypass various types of SOTA defenses. 2.