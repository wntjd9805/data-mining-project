Abstract
Transformers have gained much attention by outper-forming convolutional neural networks in many 2D vision tasks. However, they are known to have generalization problems and rely on massive-scale pre-training and so-phisticated training techniques. When applying to 3D tasks, the irregular data structure and limited data scale add to the difficulty of transformer’s application. We propose Cod-edVTR (Codebook-based Voxel TRansformer), which im-proves data efficiency and generalization ability for 3D sparse voxel transformers. On the one hand, we propose the codebook-based attention that projects an attention space into its subspace represented by the combination of “pro-totypes” in a learnable codebook. It regularizes attention learning and improves generalization. On the other hand, we propose geometry-aware self-attention that utilizes ge-ometric information (geometric pattern, density) to guide attention learning. CodedVTR could be embedded into ex-isting sparse convolution-based methods, and bring consis-tent performance improvements for indoor and outdoor 3D semantic segmentation tasks. 1.

Introduction
Recent advances in deep learning have significantly pushed forward representation learning on 3D point clouds. 3D deep learning models enable autonomous driving and robotic systems to perceive raw 3D sensor data. In this pro-cess, the 3D semantic segmentation task plays a crucial role in real-world scene understanding. It aims to classify each point into pre-defined semantic categories (e.g., car, pedes-trian, table, floor, etc.), which provides point-wise percep-tion information of the whole 3D scene. For 3D seman-tic segmentation on large-scale scenes, point-based meth-ods [20, 22, 30] split the scene into cubical chunks and ap-*Corresponding Author
Figure 1. CodedVTR is a novel transformer-based building block for voxel-based 3D scene understanding tasks. We propose the codebook-based attention to alleviate the transformer’s general-ization problem, which is exacerbated in 3D tasks. Taking 3D point cloud’s unique properties into consideration, we also pro-pose the geometry-aware attention to leverage the geometric in-formation into attention learning, ply point-wise operations to each chunk. Differently, voxel-based approaches [3, 10, 21, 33] directly voxelize the whole scene and apply 3D sparse convolution to voxels, which usually yields superior efficiency and performance. In this paper, we follow the voxel-based scheme and focus on de-signing sparse voxel-based architecture for the 3D semantic segmentation task.
Transformers [24] have received massive attention and achieved state-of-the-art performance in many vision tasks such as image classification and semantic segmenta-It discards the convolution-like inductive bias tion [15]. and adopts more general-purpose self-attention operations.
Although less inductive bias gives transformers potentially
better representative ability, it also poses challenges to its generalization capability [23,31]. Current transformers rely on large-scale data pre-training (ImageNet22K), strong data augmentation, and sophisticated hyper-parameter tuning to outperform convolution neural networks (CNNs). As the
ViT [8] poses: when directly trained on the ImageNet, transformers “yield modest accuracies of a few percent-age points below ResNets of comparable size”. Prior stud-ies [4, 13] prove that transformers have better expressive power compared with CNNs, and conclude that their poor performance on smaller datasets mainly comes from the poor generalization ability.
This problem is further aggravated on 3D tasks. 3D point cloud data has unique properties such as sparse and irregu-lar structure, varying density, implicit geometric features in data locality [12, 14, 16]. For example, 95% of the voxels that locate farther than 5m from the scene center are empty in semanticKITTI [33]. Besides, different voxels have dis-tinct local geometry patterns, e.g., the voxels on the desk and floor mainly have horizontal neighbors. Due to the large variance of density and geometric pattern in 3D data, cap-turing and adapting to these varying situations bring chal-lenges for transformer’s generalization. On the other hand, since high-quality labeled data are hard to acquire for 3D tasks, the dataset sizes are usually restricted, which also poses challenges to the generalization of transformer.
The above-mentioned phenomena stress the vitality of overcoming the generalization problem for transformers, especially for 3D tasks. We seek to address the general-ization problem from the perspective of architecture design.
We propose the codebook-based attention that regular-izes the attention space to improve generalization. Specif-ically, we project self-attention maps into a subspace rep-resented by the combination of several codebook elements (See Fig. 1). In this way, the dimension of attention learning is reduced from the whole attention space to several “proto-types” within the codebook. The projection constrains the attention learning to the subspace and could be viewed as a form of regularization for better generalization.
The above codebook plays a vital role in our design. In-stead of restricting the spatial support of the codebook ele-ments to be regular cubical regions, we in addition propose geometry-aware attention to incorporate inductive bias re-garding the spatial patterns of 3D voxel data. Unlike pix-els that have a regular and dense layout, the voxels have distinct geometric patterns and densities1. Existing sparse voxel CNNs [3] and transformer [17] use a fixed receptive field (spatial support) for all voxels. They solely rely on the learning process to extract geometric features without exploiting the voxel’s geometric pattern.
In contrast, we carefully design geometric regions of different shapes and 1We summarize the geometric pattern and density of voxels as their
“geometric information”. ranges by collecting and clustering the local sparse patterns in the input voxel data. Taking advantage of the codebook design, we assign these geometric regions to each code-book element as its attention spatial support. In this way, we could encourage attention learning to be adaptive to the sparse pattern of voxels.
The contributions of this work could be summarized into three aspects: 1) We address the transformer’s generalization problem for the 3D domain, and propose CodedVTR, a transformer-based 3D backbone. The codebook-based self-attention projects the attention space into its subspace, serving as a regularization for better generalization. 2) We propose geometry-aware attention that exploits the sparse patterns of 3D voxel. It incorporates the inductive bias of geometric properties to guide the attention learning. 3) Our CodedVTR block can be effortlessly embedded into existing sparse convolution-based backbones. Replac-ing the sparse convolution with our CodedVTR brings con-sistent performance improvement on both indoor and out-door 3D semantic segmentation datasets. 2.