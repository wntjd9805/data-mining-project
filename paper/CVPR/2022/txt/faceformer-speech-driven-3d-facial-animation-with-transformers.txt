Abstract
Speech-driven 3D facial animation is challenging due to the complex geometry of human faces and the limited availability of 3D audio-visual data. Prior works typi-cally focus on learning phoneme-level features of short au-dio windows with limited context, occasionally resulting in inaccurate lip movements. To tackle this limitation, we propose a Transformer-based autoregressive model, Face-Former, which encodes the long-term audio context and autoregressively predicts a sequence of animated 3D face meshes.
To cope with the data scarcity issue, we in-tegrate the self-supervised pre-trained speech representa-tions. Also, we devise two biased attention mechanisms well suited to this specific task, including the biased cross-modal multi-head (MH) attention and the biased causal MH self-attention with a periodic positional encoding strategy.
The former effectively aligns the audio-motion modalities, whereas the latter offers abilities to generalize to longer audio sequences. Extensive experiments and a perceptual user study show that our approach outperforms the existing state-of-the-arts. The code and the video are available at: https://evelynfan.github.io/audio2face/. 1.

Introduction
Speech-driven 3D facial animation has become an in-creasingly attractive research area in both academia and in-dustry. It is potentially beneficial to a broad range of ap-plications such as virtual reality, film production, games and education. Realistic speech-driven 3D facial animation aims to automatically animate vivid facial expressions of the 3D avatar from an arbitrary speech signal.
We focus on animating the 3D geometry rather than the 2D pixel values, e.g. photorealistic talking-head anima-tion [12, 15, 42, 52, 63, 67, 69]. The majority of existing works aim to produce 2D videos of talking heads, given the availability of massive 2D video datasets. However, the
∗ Corresponding author
† Work done at HKUST
Figure 1. Concept diagram of FaceFormer. Given the raw au-dio input and a neutral 3D face mesh, our proposed end-to-end
Transformer-based architecture, dubbed FaceFormer, can autore-gressively synthesize a sequence of realistic 3D facial motions with accurate lip movements. generated 2D videos are not directly applicable to applica-tions like 3D games and VR, which need to animate 3D models in a 3D environment. Several methods [27, 47, 60] harness 2D monocular videos to obtain 3D facial parame-ters, which might lead to unreliable results. This is because the quality of the synthetic 3D data is bounded by the accu-racy of 3D reconstruction techniques, which cannot capture the subtle changes in 3D. In speech-driven 3D facial anima-tion, some 3D mesh-based works [8, 17, 39] formulate the input as short audio windows, which might result in ambi-guities in variations of facial expressions. As pointed out by Karras et al. [31], the emotional state can be deduced from a longer-term audio context. While MeshTalk [51] has considered a longer audio context by modeling the audio sequence, training the model with Mel spectral audio fea-tures fails to synthesize accurate lip motions in data-scarce settings. Collecting lots of pairs of speech and 3D mo-tion capture data is also considerably expensive and time-consuming.
To address the issues about long-term context and lack of 3D audio-visual data, we propose a transformer-based au-toregressive model (Fig. 1) which (1) captures longer-term audio context to enable highly realistic animation of the en-tire face, i.e. both upper and lower face expressions, (2) ef-fectively utilizes the self-supervised pre-trained speech rep-resentations to handle the data scarcity issue, and (3) con-siders the history of face motions for producing temporally stable facial animation.
Transformer [58] has achieved remarkable performance in both natural language processing [20, 58] and com-puter vision [13, 21, 44] tasks. The sequential models like LSTM have a bottleneck that hinders the ability to learn longer-term context effectively [45]. Compared to
RNN-based models, transformer can better capture long-range context dependencies based solely on attention mech-anisms [58]. Recently, transformer has also made the en-couraging progress in body motion synthesis [1, 4, 46] and dance generation [36, 37, 57]. The success of transformer is mainly attributed to its design incorporating the self-attention mechanism, which is effective in modeling both the short- and long-range relations by explicitly attending to all parts of the representation. Speech-driven 3D facial animation has not been explored in this direction.
Direct application of a vanilla transformer architecture to audio sequences does not perform well on the task of speech-driven 3D facial animation, and we thus need to ad-dress these issues. First, transformer is data-hungry in na-ture, requiring sufficiently large datasets for training [32].
Given the limited availability of 3D audio-visual data, we explore the use of the self-supervised pre-trained speech model wav2vec 2.0 [2]. Wav2vec 2.0 has learned rich phoneme information, since it has been trained on a large-scale corpus [43] of unlabeled speech. While the limited 3D audio-visual data might not cover enough phonemes, we expect the pre-trained speech representations can bene-fit the speech-driven 3D facial animation task in data-scarce settings. Second, the default encoder-decoder attention of transformer can not handle modality alignment, and thus we add an alignment bias for audio-motion alignment. Third, we argue that modeling the correlation between speech and face motions needs to consider long-term audio context de-pendencies [31]. Accordingly, we do not restrict the atten-tion scope of the encoder self-attention, thus maintaining its ability to capture long-range audio context dependencies.
Fourth, transformer with the sinusoidal position encoding has weak abilities to generalize to sequence lengths longer than the ones seen during training [19, 50]. Inspired by At-tention with Linear Biases (ALiBi) [50], we add a temporal bias to the query-key attention score and design a periodic positional encoding strategy to improve the model’s gener-alization to longer audio sequences.
The main contributions of our work are as follows:
• An autoregressive transformer-based architecture for speech-driven 3D facial animation. FaceFormer encodes the long-term audio context and the history of face motions to autoregressively predict a sequence of
It achieves highly realis-animated 3D face meshes. tic and temporally stable animation of the whole face including both the upper face and the lower face.
• The biased attention modules and a periodic po-sitional encoding strategy. We carefully design the biased cross-modal MH attention to align the differ-ent modalities, and the biased causal MH self-attention with a periodic positional encoding strategy to improve the generalization to longer audio sequences. the self-supervised pre-• Effective utilization of trained speech model.
Incorporating the self-supervised pre-trained speech model in our end-to-end architecture can not only handle the data limita-tion problem, but also notably improve the accuracy of mouth movements for the difficult cases, e.g., the lips are fully closed on /b/,/m/,/p/ phonemes.
• Extensive experiments and the user study to assess the quality of synthesized face motions. The results demonstrate the superiority of FaceFormer over exist-ing state-of-the-art methods in terms of realistic facial animation and lip sync on two 3D datasets [17, 24]. 2.