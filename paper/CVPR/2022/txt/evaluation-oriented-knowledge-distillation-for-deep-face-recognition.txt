Abstract
Knowledge distillation (KD) is a widely-used technique that utilizes large networks to improve the performance of compact models. Previous KD approaches usually aim to guide the student to mimic the teacher’s behavior com-pletely in the representation space. However, such one-to-one corresponding constraints may lead to inflexible knowl-edge transfer from the teacher to the student, especially those with low model capacities.
Inspired by the ulti-mate goal of KD methods, we propose a novel Evaluation-oriented KD method (EKD) for deep face recognition to di-rectly reduce the performance gap between the teacher and student models during training. Specifically, we adopt the commonly used evaluation metrics in face recognition, i.e.,
False Positive Rate (FPR) and True Positive Rate (TPR) as the performance indicator. According to the evalua-tion protocol, the critical pair relations that cause the TPR and FPR difference between the teacher and student models are selected. Then, the critical relations in the student are constrained to approximate the corresponding ones in the teacher by a novel rank-based loss function, giving more flexibility to the student with low capacity. Extensive ex-perimental results on popular benchmarks demonstrate the superiority of our EKD over state-of-the-art competitors.
Figure 1.
Illustration of critical relations of samples. Different colors indicate different models (Teacher T in blue and Student
S in green). Different shapes indicate samples of different sub-jects. The numbers denote the cosine similarities of samples. The relation of the 1st and the 3rd samples is the only one whose sim-ilarities fall on the different side of the threshold in teacher and student models (i.e., 0.6 > 0.55 in teacher while 0.5 < 0.55 in student), and thus leads to the TPR difference. Therefore, in order to pursue the same TPR of the teacher, the student which has lim-ited model capability should pay more attention on the relation (in red) of the 1st and the 3rd samples which is the critical relation.
Similarly, for the negative pairs, the relation of 1st and 5th samples leads to the FPR difference and should be paid more attention. 1.

Introduction
With a large number of recognition systems deployed on mobile and edge devices, compact yet discriminative mod-els are in increasingly high demand. Although some opti-mized neural network architectures for mobile devices [4, 25] are proposed in the recent years, there still exists an enormous performance gap between these compact net-works and the resource-intensive networks which have mil-lions of parameters. In order to narrow the gap, Knowledge
Distillation (KD), which is a widely-used technique that uti-lizes the knowledge of a large network to improve the per-* equal contribution. ‡ corresponding author. formance of the compact models, is proposed.
The seminal works [2,10] introduced the original idea of
KD, which targets on reducing the Kullback–Leibler (KL) divergence between each instance’s probabilities at the out-put layers of the teacher and the student networks. In the past decade, work [13, 24, 33] has continued optimizing
KD methods by extending such instance-wise constraints to the activation of the hidden layers. For example, attention transfer [33] aims to elicit similar response patterns in fea-ture maps. FitNets [24] directly constrains intermediate rep-resentations by using regressions. However, such instance-based methods essentially require the teacher and student to share the same representation space, which is unrealistic
for student networks with low model capacities. As a result, these instance-wised methods bring limited improvement on the performance of student models. Recently, relation-based KD methods [20,23,31] are proposed. Different from the traditional instance-based methods, relation-based ones utilize the correlations between instances as knowledge.
The students in these methods are not required to mimic the teacher’s representation space, but rather to preserve the re-lations of samples in their own representation space. Thus, they can achieve relatively better performance comparing to the instance-based methods. However, the model perfor-mance trained with these methods are still far from perfect as they still have too strict constraint on knowledge transfer.
In particular, they require the student to mimic all relations between samples in a mini-batch, which seriously limits the flexibility and efficiency of the knowledge transfer from the teacher to the student.
Unlike all the KD methods mentioned earlier, we pro-pose a novel Evaluation-oriented Knowledge Distillation (EKD) method for deep face recognition, which draws in-spiration from the ultimate goal of KD, that is, to reduce the performance gap between the teacher and student models.
Specifically, we adopt the commonly used evaluation met-rics in face recognition, i.e., False Positive Rate (FPR), and
True Positive Rate (TPR) as the performance indicator of a face recognition model. By performing these two evalua-tion metrics during the student model training, we can di-rectly obtain the critical pair relations which cause the TPR and FPR difference between the teacher and student mod-els. Naturally, these critical pairs should be mainly focused on during knowledge transfer. Thus, we adopt a novel rank-based loss function to constrain the critical relations in the student to approximate the teacher’s corresponding ones.
Fig. 1 gives a motivational example and illustrates how crit-ical relations cause the difference of TPR and FPR between the teacher and student models. Generally, the thresholds of a face recognition model are determined by target FPRs from the similarities of whole negative pairs and are usu-ally different for different models, even if corresponding to the same FPR. For clarity, we directly give 0.55 and 0.42, which roughly correspond to FPR=1e-5 and FPR=1e-4, as the thresholds of the student and teacher model.
Although both the proposed EKD and the relation-based
KD methods optimize the relations between samples, they differ in two aspects. First, the previous relation-based KD methods require the student to mimic all the relations of the teacher to indirectly reduce the performance gap be-tween the teacher and student models, while our EKD in-troduces the commonly used evaluation protocol, i.e., TPR and FPR, into the training process and optimizes the criti-cal relations that cause the TPR and FPR difference in the student model to reduce these two metrics gap. Second, the previous relation-based KD methods usually constrain the absolute similarity of the corresponding pair between the teacher and student models, while our EKD relaxes the constraint by a novel rank-based loss function, which only requires the similarities of the corresponding pairs on the same side of the thresholds in the teacher and student mod-els.
The contributions of this paper are summarized as fol-lows:
• We propose a novel Evaluation-oriented KD method for deep face recognition. To our best knowledge,
EKD is the first KD method to directly reduce the eval-uation metric difference between the teacher and stu-dent model during training.
• We propose a novel rank-based loss function to opti-mize the student model’s critical relations that cause the TPR and FPR difference between the teacher and student models. By only constraining the similarities of the corresponding pairs are on the same side of the thresholds in the teacher and student models, it gives more flexibility to the student, thereby alleviating the student’s low capacity problem.
• We conduct extensive experiments on popular facial benchmarks, which demonstrate the superiority of the proposed EKD over the SOTA competitors. 2.