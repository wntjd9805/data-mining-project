Abstract
We present an efficient cross-view transformers, attention-based model for map-view semantic segmentation from multiple cameras. Our architecture implicitly learns a mapping from individual camera views into a canonical map-view representation using a camera-aware cross-view attention mechanism. Each camera uses positional embed-dings that depend on its intrinsic and extrinsic calibration.
These embeddings allow a transformer to learn the map-ping across different views without ever explicitly model-ing it geometrically. The architecture consists of a convo-lutional image encoder for each view and cross-view trans-former layers to infer a map-view semantic segmentation.
Our model is simple, easily parallelizable, and runs in real-time. The presented architecture performs at state-of-the-art on the nuScenes dataset, with 4x faster inference speeds.
Code is available at https://github.com/bradyz/ cross_view_transformers. 1.

Introduction
Autonomous vehicles depend on robust scene under-standing and online mapping to navigate the world. To drive safely, these systems not only reason about the se-mantics of their surroundings, but also a spatial under-standing due to the geometric nature of navigation. Many prior approaches directly model geometry and relationships between different view and a canonical map representa-tion [2, 14, 15, 16, 20, 29, 35]. They require an ex-plicit [2, 15, 16, 20, 35] or probabilistic [14, 29] estimate of depth in either image or map-view. However, this explicit modeling can be hard. First, image-based depth estimates are error-prone, as monocular depth estimates scale poorly with the distance to the observer. Second, depth-based pro-jections are a fairly inflexible and rigid bottleneck to map between views. In this work, we take a different approach.
We learn to map from camera-view to a canonical map-view representation using a cross-view transformer archi-tecture. The transformer does not perform any explicit geo-metric reasoning but instead learns to map between views
Figure 1. We introduce an architecture for perception in a map-view frame from multiple views. Our model builds a map-view representation by cross-attending to image features. A camera-aware positional embedding can geometrically link up the camera and map-views. through a geometry-aware positional embedding. Multi-head attention then learns to map features from camera-view into a canonical map-view representation using a learned map-view positional embedding. We learn a single map-view positional embedding for all cameras and per-form attention across all views. The model thus learns to link up different map locations to both cameras and loca-tions within each camera. Our cross-view transformer re-fines the map-view embedding through multiple attention and MLP blocks. The cross-view transformer allows the network to learn any geometric transformation implicitly and directly from data.
It learns an implicit estimate of depth through the camera-dependent map-view positional embedding by performing the downstream task as accu-rately as possible.
The simplicity of our model is a key strength. The model performs at state-of-the-art on the nuScenes [3] dataset for vehicle and road segmentation in the map-view and com-fortably runs in real-time (35 FPS) on a single RTX 2080
Ti GPU. The model is easy to implement and trains within
32 GPU hours. The learned attention mechanism learns ac-curate correspondences between camera and map-view di-rectly from data. 2.