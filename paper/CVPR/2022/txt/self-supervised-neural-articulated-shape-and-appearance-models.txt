Abstract
Learning geometry, motion, and appearance priors of object classes is important for the solution of a large va-riety of computer vision problems. While the majority of approaches has focused on static objects, dynamic objects, especially with controllable articulation, are less explored.
We propose a novel approach for learning a representa-tion of the geometry, appearance, and motion of a class of articulated objects given only a set of color images as in-put. In a self-supervised manner, our novel representation learns shape, appearance, and articulation codes that en-able independent control of these semantic dimensions. Our model is trained end-to-end without requiring any articu-lation annotations. Experiments show that our approach performs well for different joint types, such as revolute and prismatic joints, as well as different combinations of these joints. Compared to state of the art that uses direct 3D supervision and does not output appearance, we recover more faithful geometry and appearance from 2D observa-tions only. In addition, our representation enables a large variety of applications, such as few-shot reconstruction, the generation of novel articulations, and novel view-synthesis.
Project page: https://weify627.github.io/nasam/. 1.

Introduction
Reconstructing articulated 3D objects from image ob-servations in terms of their underlying geometry, kinemat-∗Work done during internship at Reality Labs Research. ics, and appearance is one of the fundamental problems of computer vision with many important applications, e.g. in robotics and augmented/virtual reality. This inverse graph-ics problem is highly challenging and of underconstrained nature, since image formation, i.e., mapping from the 3D world to discrete 2D pixel measurements, tightly entangles all visible properties of an object—and finding non-visible properties, such as kinematics, requires additional informa-tion such as information over time.
Most approaches that tackle this inverse problem rely on object/class-specific priors learned from large datasets with available 3D ground truth, which are challenging and ex-pensive to collect. The learned manifold of shape, appear-ance, and motion is often encoded via a low-dimensional latent space. In the devised approaches, this learned low-dimensional prior is then used to better constrain the inverse reconstruction problem.
The majority of previous techniques in the literature has focused on reconstructing classes of static objects; dynamic objects, especially with controllable articulation, are less explored. For example, occupancy networks condition the decision boundary of a neural classifier on a shape code to represent a class of static objects [32]. Approaches such as DeepSDF follow a similar principle, but employ a learned continuous signed distance field (SDF) [41] to model the object’s surface implicitly as the zero level-set of a coordinate-based neural network. DISN [59] further improves this technique and can recover more details. All the mentioned approaches require dense 3D ground truth geometry for training and do not model object appearance.
One exception is IDR [70] which employs inverse differ-entiable rendering to reconstruct the shape (using an SDF) and view-dependent appearance of a single object, but this approach does not generalize to an entire object class.
Prior work on articulated deformations heavily focuses on humans and animals [9,22,31,34,39,49,67,72] due to the availability of large datasets and readily available (learned) priors. One exception is the A-SDF [36] technique which is focused on general articulated objects. It learns separate codes for shape and articulation and employs an SDF for representing the objects. This approach learns a geometry prior across a class of articulated objects but does not jointly learn an appearance prior. Additionally, it requires dense 3D ground truth for training. We provide comparisons with
A-SDF in our experiments.
Looking at these related works in context raises the ques-tion: Is it possible to jointly learn a prior over the 3D ge-ometry, kinematics, and appearance over an entire class of articulated objects from only photometric 2D observations without requiring access to 3D ground truth?
We propose a novel approach for learning the geometry, kinematics, and appearance manifold of a class of articu-lated objects given only a set of color images as input. Our novel 3D representation of articulated objects is learned in a self-supervised manner from only color observations with-out the need for explicit geometry supervision. It enables in-dependent control of the learned semantic dimensions. Our model is trained end-to-end without requiring any articula-tion annotations. Experiments show that our approach per-forms well on both of the most widespread joint types: rev-olute and prismatic joints, as well as combinations thereof.
We outperform the state of the art, even though these related approaches require access to ground truth geometry for ex-plicit geometry supervision. In addition, our approach han-dles a larger variety of joint types than A-SDF [36]. Fur-thermore, our representation enables various applications, such as few-shot reconstruction, the generation of novel ar-ticulations, and novel view-synthesis. In summary, our con-tributions are:
• A novel approach that learns a representation of the ge-ometry, appearance, and kinematics of a class of artic-ulated objects with only a set of color images as input.
• We introduce an embedding space for geometry, kine-matics, and view-dependent appearance that enables a large variety of applications, such as the generation of new articulations and novel view synthesis.
• Our model, trained only on synthetic data, enables few-shot reconstruction of real-world articulated ob-jects via fine-tuning, as shown in Fig. 1. 2.