Abstract
Open-vocabulary instance segmentation aims at seg-menting novel classes without mask annotations. It is an im-portant step toward reducing laborious human supervision.
Most existing works ﬁrst pretrain a model on captioned images covering many novel classes and then ﬁnetune it on limited base classes with mask annotations. However, the high-level textual information learned from caption pre-training alone cannot effectively encode the details required for pixel-wise segmentation. To address this, we propose a cross-modal pseudo-labeling framework, which generates training pseudo masks by aligning word semantics in cap-tions with visual features of object masks in images. Thus, our framework is capable of labeling novel classes in cap-tions via their word semantics to self-train a student model.
To account for noises in pseudo masks, we design a robust student model that selectively distills mask knowledge by estimating the mask noise levels, hence mitigating the ad-verse impact of noisy pseudo masks. By extensive experi-ments, we show the effectiveness of our framework, where we signiﬁcantly improve mAP score by 4.5% on MS-COCO and 5.1% on the large-scale Open Images & Conceptual
Captions datasets compared to the state-of-the-art.1 1.

Introduction
Instance segmentation is a crucial yet challenging task of segmenting all objects in an image with applications in au-tonomous driving, surveillance systems, and medical imag-ing. Segmentation works have achieved impressive results thanks to advances in training high capacity models with large amounts of mask annotations [1–4]. To be speciﬁc, most methods adopt a two-stage object detection architec-ture [5] for this task by learning an additional mask head to segment objects within box proposals [6–9]. Recent works
*This work was done during Dat Huynh’s internship at Adobe Re-search. 1Code is available at https://github.com/hbdat/cvpr22_ cross_modal_pseudo_labeling.
Figure 1. Conventional pseudo-labeling (top) only segments ob-jects based on visual modality, which produces incorrect labels and misses novel object classes. Our method (bottom) leverages both visual and textual modalities by aligning semantics of cap-tion words with visual features of object masks to correctly label objects and generalize to novel classes without mask annotations. focus on high-quality mask segmentation by increasing the prediction resolutions using dynamic networks [10, 11] or boundary reﬁnement [12–14]. Despite their success, these works all require costly mask annotations of every class. As a result, it is difﬁcult to scale such systems to hundreds or thousands of classes due to their high mask annotation costs for training. In this work, we aim to signiﬁcantly reduce the amount of mask supervision by segmenting novel classes using low-cost captioned images.
One of the most popular ways to increase the number of segmentation classes is partially-supervised learning. It utilizes weak image-level [15–17] or box-level [18–23] su-pervision to segment objects that have no mask annotations, thus lowering the annotation costs. Despite the successes of partially-supervised methods, they can only segment the classes covered by the image/box-level annotation and not a wide general range of novel classes.
Different from previous approaches that are limited to classes with mask annotations, zero-shot instance/semantic segmentation aims to segment novel classes without train-ing samples via high-level semantic descriptions such as word embeddings. However, current zero-shot approaches on both object detection [24–26] and instance segmentation
[27] suffer from low novel-class performances as high-level word embeddings cannot effectively encode ﬁne-grained shape information. To overcome this, the recent OVR [28] work pretrains a visual backbone on captioned images to learn rich visual features. As the backbone of OVR encodes the visual appearances of many novel classes in captions,
ﬁnetuning it on the detection task signiﬁcantly improves the performance of novel classes. Despite its effectiveness for detection, we argue that backbone pretraining has limited effects on instance segmentation since mask predictions are ignored and not learned during caption pretraining.
In this paper, we address instance segmentation of novel classes unknown during training by directly self-training our model to segment objects in captioned images with-out any mask annotations. We introduce a robust cross-modal pseudo-labeling framework that aligns textual and visual modalities in captioned images to create caption-driven pseudo masks and generalize to novel classes be-yond base classes. Speciﬁcally, we train a teacher model on base classes and use this model to select object regions whose visual features are most compatible with the seman-tics of words in captions. The regions are further segmented into pseudo masks for object words in captions. We then distill pseudo masks into a robust student, which jointly learns segmentation and estimates pseudo-mask noise lev-els to downweight incorrect teacher predictions. Finally, we evaluate our segmentation performances on MS-COCO and
Open Images & Conceptual Captions datasets. We qualita-tively demonstrate our generalization ability on truly novel classes, which never appear in most segmentation datasets.
The contributions of this paper are as follows:
• We propose a novel cross-modal pseudo-labeling frame-work to generate caption-driven pseudo masks and fully uti-lize captioned images for segmentation training without re-quiring instance mask annotations.
• Our method is designed to work with novel classes by selecting regions whose visual features are most compatible with the semantics of novel classes and segmenting these regions into pseudo masks to self-train a student model.
• We explicitly capture the reliability of pseudo masks via our robust student model. For pseudo masks with high mask noises, we downweight the loss to avoid error propagation when objects cannot be grounded in images.
• To show the effectiveness of our method, we conduct ex-tensive experiments on MS-COCO and the large-scale Open
Images & Conceptual Captions datasets. 2.