Abstract
Test-time adaptation approaches have recently emerged as a practical solution for handling domain shift without access to the source domain data. In this paper, we pro-pose and explore a new multi-modal extension of test-time adaptation for 3D semantic segmentation. We find that, di-rectly applying existing methods usually results in perfor-mance instability at test time, because multi-modal input is not considered jointly. To design a framework that can take full advantage of multi-modality, where each modal-ity provides regularized self-supervisory signals to other modalities, we propose two complementary modules within and across the modalities. First, Intra-modal Pseudo-label Generation (Intra-PG) is introduced to obtain reli-able pseudo labels within each modality by aggregating information from two models that are both pre-trained on source data but updated with target data at different paces.
Second, Inter-modal Pseudo-label Refinement (Inter-PR) adaptively selects more reliable pseudo labels from different modalities based on a proposed consistency scheme. Exper-iments demonstrate that our regularized pseudo labels pro-duce stable self-learning signals in numerous multi-modal test-time adaptation scenarios for 3D semantic segmenta-tion. Visit our project website at https://www.nec-labs.com/Ëœmas/MM-TTA
Figure 1. We propose a Multi-Modal Test-Time Adaptation (MM-TTA) framework that enables a model to be quickly adapted to multi-modal test data without access to the source domain train-ing data. We introduce two modules: 1) Intra-PG to produce reli-able pseudo labels within each modality via updating two models (batch norm statistics) in different paces, i.e., slow and fast up-dating schemes with a momentum, and 2) Inter-PR to adaptively select pseudo-labels from the two modalities. These two modules seamlessly collaborate with each other and co-produce final cross-modal pseudo labels to help test-time adaptation. 1.

Introduction 3D semantic segmentation is a challenging task that re-quires both geometric and semantic reasoning about the in-put scene, but it can provide rich insights that enable appli-cations like autonomous driving [32, 34], virtual reality and robotics [5, 27]. With the advancement of sensor technol-ogy, multi-modal sensors are considered as the key to ef-fectively tackle this task [6, 16, 17]. In particular, to obtain more accurate 3D point-level semantic understanding, con-textual information in 2D RGB images can be reinforced by the geometric property of 3D points from LiDAR sensors, and vice versa. Therefore, it is of great interest to develop multi-modal approaches for 3D semantic segmentation.
However, multi-modal data is sensitive to a distribution shift at test time when a domain gap exists to the training data [1]. Therefore, it is critical for a model to quickly adapt to the new multi-modal data during testing for ob-taining better performance, i.e., through test-time adapta-tion (TTA) [19, 30]. This is different from the usual domain adaptive semantic segmentation setting [13, 28, 35] that can access both source and target data during training. In TTA, we only have access to model parameters pre-trained on the source data and the unlabeled test data for quick adaptation, which typically (and also in this work) refers to one epoch of training. This is practical for real-world scenarios, but it is also challenging because only the target data is available with a limited budget for adaptation.
In this paper, we study multi-modal 3D semantic seg-mentation in the setting of test-time adaptation, using both image and point cloud as input. Prior works on general test-time adaptation like TENT [30] propose entropy minimiza-tion as a self-training loss to update batch norm parameters.
While TENT [30] is not designed for multi-modality, we show a simple extension that updates parameters in indi-vidual branches for each modality (2D image and 3D point cloud). However, we find that this extension causes instabil-ity during training. One reason is that, since entropy min-imization tends to generate sharp output distributions, us-ing it separately for 2D and 3D branches may increase the cross-modal discrepancy. This would further lead to a sub-optimal model ensemble for 2D and 3D outputs, which is the common scheme for multi-modal semantic segmenta-tion. One way to alleviate this cross-modal discrepancy is to utilize a consistency loss [13] between predictions of 2D and 3D branches, via KL divergence. However, since the test data during adaptation is unlabeled, enforcing the con-sistency across modalities may even worsen predictions if the output of one branch is inaccurate.
To tackle the aforementioned issues and design bet-ter test-time self-supervisory signals, we propose a cross-modal regularized self-training framework that aims to gen-erate reliable and adaptive pseudo labels (see Fig. 1). Our method mainly consists of two modules: 1) Intra-modal
Pseudo-label Generation (Intra-PG), and 2) Inter-modal
Pseudo-label Refinement (Inter-PR). For the intra-modal module, we aim to produce reliable pseudo labels in each modality that alleviate the instability issue in test-time adap-tation, i.e., only updating batch norm parameters by seeing the test data once. To this end, we design a slow-fast mod-eling strategy. Specifically, to maintain the model stability, we initialize one batch norm statistics from the pre-trained source model, and slowly update it with a momentum from another fast-updated batch norm parameter, while this fast-updated model is directly updated by the test data, which is more aggressive but also provides up-to-date statistics. Our model is thus able to fuse predictions from the slow-/fast-updated statistics to enjoy their complementary benefits.
For the inter-modal module, we propose to adaptively select reliable pseudo labels from the individual 2D and 3D branches, because each modality brings its own advantage for 3D semantic segmentation. To this end, we first leverage the Intra-PG module to measure the prediction consisten-cies of each modality separately, and then provide a fused prediction from slow-fast models to the Inter-PR module (Fig. 1). Based on these consistencies, our model adaptively selects reliable pseudo labels from two modalities to form a final cross-modal pseudo label as the self-training signal to update 2D/3D batch norm parameters.
The proposed two modules collaborate with each other for multi-modal test-time adaptation, and thus we name our framework as MM-TTA. We conduct extensive experiments to include several TTA state-of-the-art baselines and show that our MM-TTA framework achieves favorable perfor-mance over different benchmark settings, including cross-dataset with different sensors, synthetic-to-real, and day-to-night scenarios. Moreover, we provide comprehensive anal-ysis to demonstrate the benefits of our two proposed mod-ules (Intra-PG and Inter-PR) and the stability comparisons with existing methods. Here are our main contributions: 1. We explore a new task, test-time adaptation for multi-modal 3D semantic segmentation, and propose a framework that effectively produces cross-modal pseudo labels as self-training signals. 2. We introduce two modules that seamlessly work to-gether: The Intra-PG module produces pseudo labels for each modality separately and the Inter-PR module adaptively selects pseudo labels across modalities. 3. We demonstrate our framework under different adap-tation settings with extensive ablation studies and ex-perimental comparisons against strong baselines and state-of-the-art methods. 2.