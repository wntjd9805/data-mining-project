Abstract
Recent studies show that the state-of-the-art deep neu-ral networks are vulnerable to model inversion attacks, in which access to a model is abused to reconstruct private training data of any given target class. Existing attacks rely on having access to either the complete target model (whitebox) or the model’s soft-labels (blackbox). However, no prior work has been done in the harder but more prac-tical scenario, in which the attacker only has access to the model’s predicted label, without a confidence measure. In this paper, we introduce an algorithm, Boundary-Repelling
Model Inversion (BREP-MI), to invert private training data using only the target model’s predicted labels. The key idea of our algorithm is to evaluate the model’s predicted labels over a sphere and then estimate the direction to reach the target class’s centroid. Using the example of face recogni-tion, we show that the images reconstructed by BREP-MI successfully reproduce the semantics of the private train-ing data for various datasets and target model architec-tures. We compare BREP-MI with the state-of-the-art white-box and blackbox model inversion attacks, and the results show that despite assuming less knowledge about the tar-get model, BREP-MI outperforms the blackbox attack and achieves comparable results to the whitebox attack. Our code is available online. 1 1.

Introduction
Machine learning (ML) algorithms are often trained on private or sensitive data, such as face images, medical records, and financial information. Unfortunately, since ML models tend to memorize information about training data, even when stored and processed securely, privacy informa-tion can still be exposed through the access to the mod-els [20].
Indeed, the prior study of privacy attacks has demonstrated the possibility of exposing training data at different granularities, ranging from “coarse-grained” infor-mation, such as determining whether a certain point partici-1https://github.com/m- kahla/Label- Only- Model-Inversion-Attacks-via-Boundary-Repulsion pates in training [10,14,16,21] or whether a training dataset satisfies certain properties [9,15], to more “fine-grained” in-formation, such as reconstructing the raw data [2, 3, 7, 24].
We focus on model inversion (MI) attacks, which recre-ates training data or sensitive attributes given the access to the trained model. MI attacks cause tremendous harm due to the “fine-grained” information revealed by the attacks.
For instance, MI attacks applied to personalized medicine prediction models result in the leakage of individuals’ ge-nomic attributes [8]. Recent works show that MI attacks could even successfully reconstruct high-dimensional data, such as images. For instance, [3, 7, 23, 24] demonstrated the possibility of recovering an image of a person from a face recognition model given just their name.
Existing MI attacks have either assumed that the attacker has the complete knowledge of the target model or assumed that the attack can query the model and receive model’s output as confident scores. The former and the latter are often referred to as the whitebox and the blackbox threat model, respectively. The idea underlying existing white-box MI attacks [3, 24] is to synthesize the sensitive fea-ture that achieves the maximum likelihood under the target model. The synthesis is implemented as a gradient ascent algorithm. By contrast, existing blackbox attacks [2,19] are based on training an attack network that predicts the sen-sitive feature from the input confidence scores. Despite the exclusive focus on these two threat models, in prac-tice, ML models are often packed into a blackbox that only produces hard-labels when being queried. This label-only threat model is more realistic as ML models deployed in user-facing services need not expose raw confidence scores.
However, the design of label-only MI attacks is much more challenging than the whitebox or blackbox attacks given the limited information accessible to the attacker.
In this paper, we introduce, BREP-MI, a general algo-rithm for MI attack in the label-only setting, where the at-tacker can make queries to the target model and obtain hard labels, instead of confidence scores. Similar to the main idea of whitebox attacks, we still try to synthesize the most likelihood input for the target class under the target model.
However, in the label-only setting, we cannot directly cal-Figure 1. Intuitive explanation of BREP-MI. (A) Query the labels over a sphere and estimate the direction on the sphere that can po-tentially lead to the target label class. (B) Update the synthesized image according to the estimated direction. Alternate between the estimation and update until the sphere fits into the target class. (C)
Increase the radius of the sphere. (D) Repeat the steps above until the attack hits some query budget. culate the gradient information and leverage it to guide the data synthesis. Our key insight to resolve this challenge is that a high-likelihood region for a given class often lies at the center of the class and is far away from any deci-sion boundaries. Hence, we design an algorithm that allows the synthesized image to iteratively move away from the decision boundary, as illustrated in Figure 1. Specifically, we first query the labels over a sphere and estimate the di-rection on the sphere that can potentially lead to the target label class (A). We progressively move according to esti-mated directions until the sphere fits into the target class (B). We then increase the radius of the sphere (C) and re-peat the steps above until the attack hits some query budget (D). We theoretically prove that for linear target models, the direction estimated from hard labels queried on the spheres aligns with the gradient direction. We empirically show that
BREP-MI can also lead to successful attacks against deep neural network-based target models. In particular, the effi-cacy of the attack is even higher than the existing blackbox attacks and comparable to the existing whitebox attacks.
Our contributions can be summarized as follows: (1)
We propose the first algorithm for label-only model inver-(2) We provide theoretical justification for sion attacks. the algorithm in the linear target model case by proving the updates used in our algorithm align with the gradient and also analyze the error of alignment for nonlinear mod-(3) We evaluate the attack on a range of model ar-els. chitectures and datasets, then show that despite exploiting less information about the target model, our attack outper-forms the confidence-based blackbox attack by a large mar-gin and achieves comparable performance to the state-of-the-art whitebox attack. Besides, we will release data, code, and models to facilitate future research. 2.