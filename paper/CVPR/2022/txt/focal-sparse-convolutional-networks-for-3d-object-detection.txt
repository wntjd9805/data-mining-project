Abstract
Non-uniformed 3D sparse data, e.g., point clouds or vox-els in different spatial positions, make contribution to the task of 3D object detection in different ways. Existing ba-sic components in sparse convolutional networks (Sparse
CNNs) process all sparse data, regardless of regular or sub-manifold sparse convolution.
In this paper, we introduce two new modules to enhance the capability of Sparse CNNs, both are based on making feature sparsity learnable with position-wise importance prediction. They are focal sparse convolution (Focals Conv) and its multi-modal variant of focal sparse convolution with fusion, or Focals Conv-F for short. The new modules can readily substitute their plain counterparts in existing Sparse CNNs and be jointly trained in an end-to-end fashion. For the first time, we show that spatially learnable sparsity in sparse convolution is essen-tial for sophisticated 3D object detection. Extensive experi-ments on the KITTI, nuScenes and Waymo benchmarks val-idate the effectiveness of our approach. Without bells and whistles, our results outperform all existing single-model entries on the nuScenes test benchmark. Code and models are at github.com/dvlab-research/FocalsConv. 1.

Introduction
A key challenge in 3D object detection is to learn ef-fective representations from the unstructured and sparse 3D geometric data such as point clouds. In general, there are two ways for this job. The first is to process point clouds [37,51] directly, based on PointNet++ [33] networks.
However, the neighbour sampling and grouping operations are time-consuming. This makes it improper for large-scale autonomous driving scenes that require real-time efficiency.
The The second is to convert point clouds into voxelizations and apply 3D sparse convolutional neural networks (Sparse
CNNs) for feature extraction [11, 36]. 3D Sparse CNNs resemble 2D CNNs in structures, including several feature stages and down-sampling operations. They typically con-sist of regular and submanifold sparse convolutions [15].
Yukangâ€™s work was done during internship in MEGVII Technology.
Although regular and submanifold sparse convolutions have been widely used, they have respective limitations.
It
Regular sparse convolution dilates all sparse features. inevitably burdens models with considerable computations.
That is why backbone networks commonly limit its usage only in down-sampling layers [36, 48]. In addition, detec-tors aim to distinguish target objects from massive back-ground features. But regular sparse convolution reduces the sparsity sharply and blurs feature distinctions.
On the other hand, submanifold sparse convolutions avoid the computation issue by restricting the output fea-ture positions to the input. But it misses necessary informa-tion flow, especially for the spatially disconnected features.
The above issues on regular and submanifold sparse con-volutions limit Sparse CNNs to achieve high representation capability and efficiency. We illustrate the submanifold and regular sparse convolutional operations in Fig. 1.
These limitations originate from the conventional con-volution pattern: all input features are treated equally in the convolution process.
It is natural for 2D CNNs, and yet is improper for 3D sparse features. 2D convolution is designed for structured data. All pixels in the same layer typically share receptive field sizes. But 3D sparse data is with varying sparsity and importance in space.
It is not optimal to handle non-uniform data with uniform
In terms of sparsity, upon the distance to LI-treatment.
In
DAR sensors, objects present large sparsity variance. terms of importance, the contribution of features varies with different locations for 3D object detection, e.g., fore-ground or background. Although 3D object detection is achieved [11, 36, 37, 53], state-of-the-art methods still rely
It corre-on RoI (region-of-interest) feature extraction. sponds to the idea that we should shoot arrows at the target in the feature extraction of 3D detectors.
In this paper, we propose a general format of sparse con-volution by relaxing the conceptual difference between reg-ular and submanifold ones. We introduce two new modules that improve the representation capacity of Sparse CNNs for 3D object detection. The first is focal sparse convolu-tion (Focals Conv). It predicts cubic importance maps for the output pattern of convolutions. Features predicted as important ones are dilated into a deformable output shape,
Figure 1. Process of different sparse convolution types. Submanifold sparse convolution fixes the output position identical to input. It maintains efficiency but disables information flow between disconnected features. Regular sparse convolution dilates all input features to its kernel-size neighbors. It encourages information communication with expensive computation, as it seriously increases feature density.
The proposed focal sparse convolution dynamically determines which input features deserve dilation and dynamic output shapes, using predicted cubic importance. Input and Output are illustrated in 2D features for simplification. This figure is best viewed in color. as shown in Fig 1. The importance is learned via an addi-tional convolutional layer, dynamically conditioned on the input features. This module increases the ratio of valuable information among all features. The second is its multi-modal improved version of Focal sparse Convolution with
Fusion (named as Focals Conv-F). Upon the LIDAR-only
Focals Conv, we enhance importance prediction with RGB features fused, as image features typically contain rich ap-pearance information and large receptive fields.
The proposed modules are novel in two aspects. First,
Focals Conv presents a dynamic mechanism for learning spatial sparsity of features.
It makes the learning pro-cess concentrated on the more valuable foreground data.
With the down-sampling operations, valuable information increases in stages. Meanwhile, the large amount of back-ground voxels are removed. Fig. 2 illustrates the learnable feature sparsity, including the common, crowded, and re-mote objects, where Focals Conv enriches the learned voxel features on the foreground without redundant voxels added in other areas. Second, both modules are lightweight. The importance prediction involves small overhead parameters and computation, as measured in Tab. 1. The RGB feature extraction of Focals Conv-F involves only several layers, instead of heady 2D detection or segmentation models [43].
The proposed modules of Focals Conv and Focals Conv-F can readily replace their original counterparts in sparse
CNNs. To demonstrate the effectiveness, we build the backbone networks on existing 3D object detection frame-works [11,36,53]. Our method enables non-trivial enhance-ment with small model complexity overhead on both the
KITTI [14] and nuScenes [2] benchmarks. These results manifest that learnable sparsity with focal points is essen-tial. Without bells and whistles, our approach outperforms state-of-the-art ones on the nuScenes test split [2].
Convolutional dynamic mechanism adapts the opera-tions conditioned on input data, e.g., deformable convolu-tions [10, 64] and dynamic convolutions [7, 49]. The key difference is that our approach makes use of the intrinsic sparsity of data.
It promotes feature learning to be con-centrated on more valuable information. We deem the non-uniform property as a great benefit. We discuss the relations and differences to previous literature in Sec. 2. 2.