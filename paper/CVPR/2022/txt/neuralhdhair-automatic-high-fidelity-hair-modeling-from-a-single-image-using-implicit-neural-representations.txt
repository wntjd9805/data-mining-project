Abstract
Undoubtedly, high-ﬁdelity 3D hair plays an indispens-able role in digital humans. However, existing monocular hair modeling methods are either tricky to deploy in digital systems (e.g., due to their dependence on complex user in-teractions or large databases) or can produce only a coarse geometry. In this paper, we introduce NeuralHDHair, a ﬂex-ible, fully automatic system for modeling high-ﬁdelity hair from a single image. The key enablers of our system are two carefully designed neural networks: an IRHairNet (Im-plicit representation for hair using neural network) for in-ferring high-ﬁdelity 3D hair geometric features (3D orien-tation ﬁeld and 3D occupancy ﬁeld) hierarchically and a
GrowingNet (Growing hair strands using neural network) to efﬁciently generate 3D hair strands in parallel. Specif-ically, we perform a coarse-to-ﬁne manner and propose a novel voxel-aligned implicit function (VIFu) to represent the global hair feature, which is further enhanced by the local details extracted from a hair luminance map. To improve the efﬁciency of a traditional hair growth algorithm, we adopt a local neural implicit function to grow strands based on the estimated 3D hair geometric features. Extensive ex-periments show that our method is capable of construct-ing a high-ﬁdelity 3D hair model from a single image, both efﬁciently and effectively, and achieves the-state-of-the-art performance. 1.

Introduction
As one of the most distinctive human characteristics, hair plays an indispensable role in digital humans [4, 10, 12, 15, 17]. Undoubtedly, a high-ﬁdelity 3D hair model can sig-niﬁcantly improve the realism of a virtual human. How-ever, the existing single-view-based hair modeling meth-ods [6, 11, 28, 36, 40] cannot sufﬁciently satisfy the require-∗The ﬁrst two authors contributed equally. The authors are afﬁli-ated with the State Key Lab of CAD&CG. †Corresponding author: Youyi
Zheng.
Figure 1. Given a single image, our NeuralHDHair reconstructs a high-ﬁdelity 3D hair model. ments of human digitalization in terms of ﬂexibility, sim-plicity, and realism. On the one hand, data-driven methods
[6, 11] could achieve high-ﬁdelity results, but are complex and not very robust, e.g., entailing a sophisticated search-ing and matching process based on a large hair dataset. On the other hand, deep-learning based methods [28,36,40] are lightweight and ﬂexible to deploy but could only achieve coarse results. Thus, in this work, we consider the problem of automatic high-ﬁdelity 3D hair modeling from a single image utilizing a learning-based method.
Unlike other parts of the human body, the hair structure is more challenging to describe and extract due to remark-ably intricate structures of interweaving strands, leading to extreme difﬁculty in reconstructing a high-ﬁdelity 3D hair model only from a single view. Generally, almost all the existing methods tackle this problem in two steps: ﬁrst es-timating a 3D orientation ﬁeld based on a 2D orientation map extracted from an input image and then synthesizing hair strands from the 3D orientation ﬁeld. However, there are some problems with such a two-step mechanism. First, since an 2D orientation map is only a ﬁltered version of the input hair growing information [24], using the 2D ori-entation map alone to bridge the domain gap between real and synthetic data would unavoidably lose hair details (e.g., the relationship of occlusive strands [33]). Second, the ex-isting methods for inferring 3D orientation ﬁelds are either time-consuming due to the use of a complex searching and
matching process based on a large hair dataset [6, 11], or liable to over-smoothness due to the use of deep networks to directly achieve image-to-voxel inference [36]. Third, the conventional hair growth algorithm [31, 39] to extract strands from the estimated 3D orientation ﬁeld are inefﬁ-cient and not conducive to one-shot hair modeling. Al-though Zhou et al. [40] have attempted to ignore the hair growth procedure by directly regressing 3D hair strands, their reconstruction results are generally unsatisfactory (see
Sec. 4.2). Based on the above observations, we seek to build a fully automatic and efﬁcient hair modeling method that can reconstruct a 3D hair model from a single image with
ﬁne-grained features (Fig. 1) while exhibiting high ﬂexibil-ity, e.g., reconstructing the hair model only needs one for-ward pass of the networks.
We found that implicit functions have excellent perfor-mance and great potential in representing [25] and inferring
[23] 3D shapes. For example, Saito et al. [29] introduced
PIFu to reconstruct a whole human body from a single im-age, including hair. However, the quality of their hair recon-struction results is less satisfactory. We observe that unlike human body modeling, which cares only the surface geom-etry of a human body, 3D hair modeling needs to consider both exterior shape and interior features, which are difﬁcult to represent by a pixel-aligned implicit function [30]. To address this issue, we propose IRHairNet, which imposes a coarse-to-ﬁne strategy to produce a high-ﬁdelity 3D orien-tation ﬁeld. Speciﬁcally, we introduce a novel voxel-aligned implicit function (VIFu) to extract global information from a 2D orientation map in the coarse module. Meanwhile, to supplement the lost local details in the 2D orientation map, we exploit a high-resolution luminance map to extract lo-cal feature and combine it with the gloabl feature in the ﬁne module for high-ﬁdelity hair modeling.
To efﬁciently synthesize a hair-strand model from the 3D orientation ﬁeld, we introduce GrowingNet, a deep learning-based hair growth method by leveraging a local implicit grid representation [13]. This is based on a key observation that although hair geometric shapes and grow-ing directions vary globally, they share similar features at a speciﬁc local scale. Thus, we can extract a high-level latent code for each local 3D orientation patch, and then train a neural implicit function (a decoder) to grow strands inside it based on this latent code. After each growing step, new local patches centered at the ends of the strands will be used to proceed with the growing. After training, it is applicable to 3D orientation ﬁelds of arbitrary resolution.
IRHairNet and GrowingNet form the core of NeuralHD-Hair, a novel automatic monocular hair modeling method.
We conduct extensive experiments, including comparison experiments and ablation study, and the results show that
NeuralHDHair outperforms all existing monocular hair re-construction methods [36, 40].
In summary, the main contributions of our work include:
• We introduce a novel fully-automatic monocular hair modeling framework, signiﬁcantly outperforming the state-of-the-art methods.
• We introduce a coarse-to-ﬁne hair modeling neural network (IRHairNet), where we use a novel voxel-aligned implicit function and a luminance map to en-rich local details for high-quality hair modeling.
• We propose a novel hair growing network (Grow-ingNet) based on a local implicit function to efﬁciently generate strand models with arbitrary resolution, and is an order of magnitude faster than prior methods. 2.