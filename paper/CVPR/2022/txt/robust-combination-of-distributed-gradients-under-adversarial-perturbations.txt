Abstract
We consider distributed (gradient descent-based) learn-ing scenarios where the server combines the gradients of learning objectives gathered from local clients. As individual data collection and learning environments can vary, some clients could transfer erroneous gradients e.g. due to ad-versarial data or gradient perturbations. Further, for data privacy and security, the identities of such affected clients are often unknown to the server. In such cases, na¨ıvely ag-gregating the resulting gradients can mislead the learning process. We propose a new server-side learning algorithm that robustly combines gradients. Our algorithm embeds the local gradients into the manifold of normalized gradients and refines their combinations via simulating a diffusion process therein. The resulting algorithm is instantiated as a compu-tationally simple and efficient weighted gradient averaging algorithm. In the experiments with five classification and three regression benchmark datasets, our algorithm demon-strated significant performance improvements over existing robust gradient combination algorithms as well as the base-line uniform gradient averaging algorithm. 1.

Introduction
The success of deep learning relies on the abundance of training data and computational capability to process such data. When computational resources are limited at centralized servers, distributed learning can be employed, where data is distributed across multiple clients and part of the learning pro-cess is performed within each client. To enhance data privacy and security, and reduce the required communication cost, of-ten in such environments, details of the localized data are hid-den to the server and other clients, and the server aggregates only encapsulated information communicated from clients.
We consider distributed gradient descent-based optimiza-tion as an instance of such learning problems: The main objective is to minimize a differentiable energy function presented as the sum of the client energies defined based on the respective local data (e.g. the average of squared errors or cross-entry losses over clients’ datasets) [23, 27, 41]. Then each client iteratively calculates and transfers the gradient of the local energy with respect to the learner parameters while the server maintains and updates these parameters using the aggregated local gradients.
The nature of individual clients and local data therein can vary significantly. For instance, a client could be a data center securing professionally annotated data while another client might be an edge device containing data labeled by inexperienced annotators. In this case, some clients could exhibit abnormal behavior e.g. due to incorrect labeling or adversarial attacks, and the corresponding local gradients can be unreliable. This can distract the gradient aggregation process at the server leading to suboptimal learning.
Robust distributed learning with such affected clients has only recently come to recognition and existing approaches are limited in that they require a separate training set at the server (to train a detector of affected clients) [34], information on the number of affected clients [32], or access to local training data [12] (see Sec. 2 for details).
We present a new approach that combines local gradients when some clients undergo adversarial perturbation. Our algorithm takes the na¨ıve uniform average of gradients as a noisy observation of an underlying ground-truth combination, and iteratively improves this during the gradient descent steps of the learner parameters. Specifically, our algorithm represents the combined gradient as a convex combination of all local gradients. The corresponding combination weights are uniformly initialized. As the optimization proceeds, the genuine (unaffected) local gradients are automatically iden-tified by adjusting the combination weights via simulating the diffusion of the combined gradient. Our approach does not require a separate training set at the server, and it offers the advantage of refining the gradient combination without having to know the number and identities of affected clients as well as the nature of perturbations therein.
In the experiments with five classification and three regression benchmark datasets, and two different scenarios of client perturbations, our approach demonstrated significant performance improvements over the standard uniform gradient averaging algorithm and existing robust gradient combination approaches.
2.