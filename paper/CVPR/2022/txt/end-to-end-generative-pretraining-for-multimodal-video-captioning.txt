Abstract
Recent video and language pretraining frameworks lack the ability to generate sentences. We present Multimodal
Video Generative Pretraining (MV-GPT), a new pretraining framework for learning from unlabelled videos which can be effectively used for generative tasks such as multimodal video captioning. Unlike recent video-language pretraining frameworks, our framework trains both a multimodal video encoder and a sentence decoder jointly. To overcome the lack of captions in unlabelled videos, we leverage the future utterance as an additional text source and propose a bidirectional genera-tion objective – we generate future utterances given the present mulitmodal context, and also the present utterance given future observations. With this objective, we train an encoder-decoder model end-to-end to generate a caption from raw pixels and transcribed speech directly. Our model achieves state-of-the-art performance for multimodal video captioning on four standard benchmarks, as well as for other video understanding tasks such as VideoQA, video retrieval and action classification. 1.

Introduction
A long-standing goal of the AI community is the devel-opment of conversational multimodal systems that can both reliably perceive the world and effortlessly communicate with humans. An emerging benchmark of progress in this field is the task of multimodal video captioning [17, 32] - which tests both abilities; a successful model must not only accurately understand ‘multimodal’ streams of input video (including the speech and the video frames), but also generate coherent natural language descriptions of the content.
Unsurprisingly, a major challenge in the field of vision and language learning is the lack of large-scale, manually annotated data. Annotating captions for videos is time intensive, expensive and subjective (with low inter-annotator agreement [17]) – this is in contrast to fields such as image classification where fully annotated datasets are orders of magnitude larger [15, 41, 56].
To overcome this limitation, there has been a flurry of recent works that pretrain their video-language models on instructional videos [32, 33, 42, 44, 45], a domain where the speech is particularly well aligned to visual content. Recently introduced datasets such as Cooking312K [45] and HowTo100M [34] leverage such instructional videos with associated captions from ASR (automatic speech recognition) to learn joint video-and-text embeddings [33, 44] or to train multimodal video encoders [27, 42]. However, the models in these works often do not contain a decoder, lacking the ability to generate sentences, and thus only the video encoder is transferred to the downstream tasks – indeed for the case of video captioning, the decoder is often learned from scratch [45, 47, 63]. While one can still initialize the decoder using independently pretrained weights such as those from a GPT-2 [37] model, we observe that this strategy is suboptimal and performance is significantly improved by optimizing the encoder and the decoder jointly.
For the task of multimodal video captioning, we require a
Figure 2. Multimodal Video Generative Pretraining (MV-GPT) framework. During pretraining, our network (which consists of modality specific encoders, a multimodal encoder and a sentence decoder) is trained with a new bi-directional objective. 1) Forward generation (FG, blue ): Given input frames and present utterances from a video clip, we predict a future utterance and 2) Backward generation (BG, red ): Given input frames and a future utterance, predict the current utterances. Both losses are applied to a triplet consisting of video frames, present utterances and a future utterance. To allow our model to recognise the different configurations, we attach distinct special classification tokens CLS1 and CLS2 to the input text for FG and BG respectively, as well as distinct BOS1 and BOS2 (beginning of sentence) tokens to the decoder for sentence generation. model that can both encode multimodal videos (i.e. frames and textual inputs) and generate captions. Using multimodal infor-mation as input can greatly improve the quality of the generated captions (as illustrated in Figure 1a). However, learning such an encoder-decoder model jointly from unlabelled data is par-ticularly challenging, as it requires two streams of textual data – naturally occurring transcribed speech accompanying the video for the encoder, and target sentences for the decoder – whereas unlabelled videos only come with a single stream of speech (Figure 1b). Recent works [17, 23, 32] have attempted to solve this problem with a denoising autoencoder - wherein the input speech to the model is artificially ‘noised’, i.e. random words are masked out [17, 23, 32]. The decoder is then tasked with simply reconstructing either the masked phrases or the original unmasked text, where the supervisory signals are provided only from the masked words. In these frameworks, additional losses are often required to strengthen the pretraining supervision, such as multimodal input alignment [32] and segment ordering [17].
In our framework, we introduce a novel stronger loss. We leverage future utterances as another source of textual data and train a model to generate these entirely unseen sentences as depicted in Figure 1b. To alleviate the problem that future utterances are not temporally aligned, we propose a backward generation objective where present aligned utterances are generated given future utterances. Experimental results show that a model pretrained with this bidirectional generation objective effectively transfers to multimodal video captioning and outperforms the state of the art by a margin.
We make the following contributions: (i) We propose a novel pretraining objective for multimodal video captioning that requires no manually annotated captions, and instead uses utterances sampled at different times in the same video. Our objective is bidirectional in time – i.e. we not only generate future utterances but also the present ones from the future; (ii)
By using two sources of textual data, we are able to jointly train the entire encoder-decoder model. This is unlike previous works which pretrain only the (multimodal) encoder, thereby lacking the ability to generate captions [27, 42, 45]; (iii) Our encoder is trained from raw pixels and words directly, in contrast with existing methods that rely on pre-extracted visual features limiting transfer to new domains [17, 23, 32]; (iv) We achieve state-of-the-art results on four video captioning benchmarks – YouCook2, ViTT, MSR-VTT and ActivityNet-Captions – consistently outperforming existing methods by significant margins; and finally (v) Our pretraining objective yields strong multimodal video representations, which achieve state-of-the-art performance on other video understanding tasks such as
VideoQA, video retrieval and action classification. 2.