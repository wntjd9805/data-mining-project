Abstract
Hierarchical semantic structures naturally exist in an image dataset, in which several semantically relevant im-age clusters can be further integrated into a larger cluster with coarser-grained semantics. Capturing such structures with image representations can greatly benefit the seman-tic understanding on various downstream tasks. Existing contrastive representation learning methods lack such an important model capability. In addition, the negative pairs used in these methods are not guaranteed to be semantically distinct, which could further hamper the structural correct-ness of learned image representations. To tackle these limi-tations, we propose a novel contrastive learning framework called Hierarchical Contrastive Selective Coding (HCSC).
In this framework, a set of hierarchical prototypes are con-structed and also dynamically updated to represent the hi-erarchical semantic structures underlying the data in the latent space. To make image representations better fit such semantic structures, we employ and further improve con-ventional instance-wise and prototypical contrastive learn-ing via an elaborate pair selection scheme. This scheme seeks to select more diverse positive pairs with similar se-mantics and more precise negative pairs with truly distinct semantics. On extensive downstream tasks, we verify the state-of-the-art performance of HCSC and also the effec-tiveness of major model components. We are continually building a comprehensive model zoo (see supplementary material). Our source code and model weights are avail-able at https://github.com/gyfastas/HCSC. 1.

Introduction
In the past few years, self-supervised image represen-tation learning has witnessed great progresses, in which
Figure 1. An image dataset always contains multiple semantic hi-erarchies, e.g. “mammals → dogs → Labradors” in the order from coarse-grained semantics to fine-grained semantics. the traditional methods based on solving informative puz-zles [12, 16, 32, 33, 45] are obviously surpassed by con-trastive learning [6–8, 19, 34]. These contrastive methods succeed in deriving useful and interpretable representations for various downstream tasks. In particular, under the stan-dard linear evaluation protocol [38], they have achieved in-spiring results that approach fully-supervised learning.
Existing contrastive methods mainly lie in two cate-gories, instance-wise contrastive learning [7, 19, 34] and prototypical contrastive learning [6,26]. Instance-wise con-trast seeks to map similar instances nearby in the latent space while mapping dissimilar ones far apart, which guar-antees reasonable local structures among different image representations. Prototypical contrast aims to derive com-pact image representations gathering around corresponding cluster centers, which captures basic semantic structures that can be represented by a single hierarchy of clusters.
However, these approaches lag in representation power when modeling a large-scale image dataset which could always possess multiple semantic hierarchies. For exam-ple, in an extensive species database, the cluster of dogs summarizes the common characters of Labradors, Poodles,
Samoyeds, etc. and should be placed on a higher hierarchy; similarly, dogs together with cats, monkeys, whales, etc. are further summarized by an even higher-level cluster, mam-mals (see Fig. 1 for a more intuitive illustration). Learn-ing image representations that embrace such hierarchical semantic structures can greatly benefit the semantic under-standing on various downstream tasks. How to achieve this by contrastive learning is still an open problem.
In addition, existing contrastive methods commonly con-struct negative pairs by exhaustive sampling from some noise distribution, and all the sampled negative pairs are used without selection. There is no guarantee that the neg-ative pairs obtained in this way own truly distinct seman-tics. Therefore, some samples with similar semantics may be wrongly embedded far apart by these methods, which hampers the quality of learned image representations.
To tackle the limitations above, we propose a novel con-trastive learning framework called Hierarchical Contrastive
Selective Coding (HCSC). In this framework, we propose to capture the hierarchical semantic structures underlying the data with hierarchical prototypes, a set of tree-structured representative embeddings in the latent space. Along the training process, these prototypes are dynamically updated to fit the current image representations. Under the guidance of such hierarchical semantic structures, we seek to im-prove both instance-wise and prototypical contrastive learn-ing by selecting high-quality positive and negative pairs that are semantically correct. Specifically, for each query sample, we search for its most similar prototype on each semantic hierarchy to build more abundant positive pairs.
Moreover, for each candidate of negative pair, we conduct a
Bernoulli sampling to keep/discard it if the semantic corre-lation of the pair is low/high. By using these selected pairs for instance-wise and prototypical contrast, the semantic constraints from hierarchical prototypes can be embedded into the objective of representation learning.
We summarize the contributions of this work as follows:
• We novelly propose to represent the hierarchical se-mantic structures of image representations by dynami-cally maintaining hierarchical prototypes.
• We propose a novel contrastive learning framework,
Hierarchical Contrastive Selective Coding (HCSC), which improves conventional instance-wise and proto-typical contrastive learning by selecting semantically correct positive and negative pairs.
• Our HCSC approach consistently achieves superior performance over state-of-the-art contrastive learning algorithms on various downstream tasks. Also, the ef-fectiveness of key model components are verified by extensive ablation and visualization analysis. 2.