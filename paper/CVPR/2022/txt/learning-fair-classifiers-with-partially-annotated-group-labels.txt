Abstract
Recently, fairness-aware learning have become increas-ingly crucial, but most of those methods operate by assum-ing the availability of fully annotated demographic group labels. We emphasize that such assumption is unrealistic for real-world applications since group label annotations are expensive and can conflict with privacy issues. In this paper, we consider a more practical scenario, dubbed as
Algorithmic Group Fairness with the Partially annotated
Group labels (Fair-PG). We observe that the existing meth-ods to achieve group fairness perform even worse than the vanilla training, which simply uses full data only with tar-get labels, under Fair-PG. To address this problem, we pro-pose a simple Confidence-based Group Label assignment (CGL) strategy that is readily applicable to any fairness-aware learning method. CGL utilizes an auxiliary group classifier to assign pseudo group labels, where random la-bels are assigned to low confident samples. We first theoret-ically show that our method design is better than the vanilla pseudo-labeling strategy in terms of fairness criteria. Then, we empirically show on several benchmark datasets that by combining CGL and the state-of-the-art fairness-aware in-processing methods, the target accuracies and the fairness metrics can be jointly improved compared to the baselines.
Furthermore, we convincingly show that CGL enables to naturally augment the given group-labeled dataset with ex-ternal target label-only datasets so that both accuracy and fairness can be improved. Code is available at https:
//github.com/naver-ai/cgl_fairness. 1.

Introduction
Recent advances of machine learning (ML) models have witnessed promising outcomes even in societal applica-tions, such as credit estimation [28], crime assessment sys-tems [7, 23], automatic job interviews [33], face recogni-tion [8, 38], and law enforcement [17]. However, machines
*Works done while doing an internship at NAVER AI Lab.
†Corresponding authors
Figure 1. Can fair-training methods still learn fair classi-fiers when group labels are partially annotated? We note the state-of-the-art fairness fair-training FairHSIC [34] using only the group-labeled subset (yellow) shows worse fairness criterion (∆M , Eq. (2), lower the better) than the “scratch” (i.e., no consid-eration of a fairness criteria) in the low group label regime (e.g., 10%) on UTKFace [44]. Our CGL (red), on the other hand, can be potentially applied to any fair-training method, and when it is combined with FairHSIC, both the target accuracy and the fairness criteria are significantly improved for the low group label regime. are often more inaccurate to a particular group (e.g., darker-skinned females) than other groups (e.g., lighter-skinned males) [8], i.e., machines are discriminatory. To mitigate the issue, fairness-aware learning has recently emerged; a model should not discriminate against any demographic group with sensitive attributes, e.g., age, gender, or race.
Many existing approaches for group fairness [1, 12, 22, 24, 34, 41, 42] utilize two types of labels: target labels, which are task-oriented (e.g., crime assessment) and group labels, which are defined by socially sensitive attribute groups (e.g., ethnicity or gender). Many existing methods for achieving group fairness rely on the group labels to train fair classifiers. For example, many approaches explic-itly minimize the statistical parity metrics between groups defined by sensitive attributes. However, in many realis-tic applications, e.g., computer vision, assuming that all images have sensitive group labels can be unrealistic and make the existing methods impractical. First, in many im-age datasets, group labels are not explicitly given as in tab-ular datasets [7, 23, 28] but are defined in high-level se-mantics, requiring additional expensive human annotations.
Secondly, the sensitive attributes are usually personal infor-mation protected by laws, such as EU General Data Protec-tion Regulation (GDPR). Hence, in real-world applications, collecting group labels for all data points are impossible without permissions by all users and, furthermore, sensi-tive attributes cannot be persistently stored but should be expired. Thus, the underlying assumption by the previous fair-training methods, i.e., group labels are fully annotated, can limit their usability in real-world applications.
Contribution.
In this work, we propose and investigate a less explored but very practical problem: Algorithmic
Group Fairness with the Partially annotated Group la-bels (Fair-PG). Many existing fair-training methods for group fairness assume all training samples have group la-bels, and optimize fairness constraints by the group labeled training samples. In this case, they cannot be directly ap-plied to the Fair-PG problem. We empirically show that the baseline fair-training methods, which operate only on the group-labeled samples, perform even worse than the vanilla
“scratch” training that use all the training samples, in terms of fairness when the number of group-labeled samples is small (e.g., 10%) – See Fig. 1. Although there exist a few at-tempts to achieve algorithmic fairness without demograph-ics labels [20, 29], they do not directly solve the group fair-ness problem. Also, they do not utilize partially annotated group labels at all, while a small number of labeled data can improve the overall performances. To this end, we propose a simple yet effective strategy for Fair-PG that can be applied to any fair-training methods for group fairness, dubbed as
Confidence-based Group Label assignment (CGL). CGL assigns pseudo group labels to group-unlabeled samples us-ing an auxiliary group classifier, if the predictions are suffi-ciently confident, and random group labels, otherwise.
We provide high-level understandings of how CGL works on the Fair-PG scenario. We theoretically support that (1) the fairness parity computed by our approach ap-proximates the parity of the underlying group label distribu-tion better than the one by the vanilla pseudo-label strategy which totally trusts the predictions of the auxiliary group classifier, (2) assigning a random group label to a data point implies the elimination of the fairness constraint of the sam-ple. In practice, since the existing fair-training methods use a relaxed constraint, CGL can be interpreted as a regulariza-tion method for the low confident group-unlabeled samples.
In our experiments, the combination of CGL with state-of-the-art fair-traning methods (e.g., MFD [24], FairHSIC
[34] and LBC [22]) has consistently and significantly im-proved target accuracies as well as fairness parities even under the low group label regime on facial image [31, 44] and tabular [14, 23] datasets. For example, compared to the
“group-labeled only” baseline, the combination of CGL and
MFD shows +8.23% target accuracy increase and -8.75 dis-parity of equal opportunity (DEO) decrease on UTKFace
[44], when only 10% of data points have group labels. Fur-ther extending this result, by augmenting the full UTKFace training set with extra group-unlabeled dataset in [27], we show that CGL can significantly improve the performance of MFD by +0.92% accuracy and -5.5 DEO. This is promis-ing since it shows CGL can improve both the accuracy and fairness of a baseline method by augmenting the training data with target label-only dataset, which is relatively eas-ier to obtain than jointly requiring the group labels. 2.