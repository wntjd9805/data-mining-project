Abstract
Scene graph generation (SGG) is to detect object pairs with their relations in an image. Existing SGG approaches often use multi-stage pipelines to decompose this task into object detection, relation graph construction, and dense or dense-to-sparse relation prediction.
Instead, from a perspective on SGG as a direct set prediction, this paper presents a simple, sparse, and unified framework, termed as
Structured Sparse R-CNN. The key to our method is a set of learnable triplet queries and a structured triplet detec-tor which could be jointly optimized from the training set in an end-to-end manner. Specifically, the triplet queries encode the general prior for object pairs with their rela-tions, and provide an initial guess of scene graphs for sub-sequent refinement. The triplet detector presents a cas-caded architecture to progressively refine the detected scene graphs with the customized dynamic heads. In addition, to relieve the training difficulty of our method, we propose a relaxed and enhanced training strategy based on knowl-edge distillation from a Siamese Sparse R-CNN. We per-form experiments on several datasets: Visual Genome and
Open Images V4/V6, and the results demonstrate that our method achieves the state-of-the-art performance.
In ad-dition, we also perform in-depth ablation studies to pro-vide insights on our structured modeling in triplet detec-tor design and training strategies. The code and models are made available at https://github.com/MCG-NJU/Structured-Sparse-RCNN . 1.

Introduction
Scene graph generation (SGG) [45] aims at detecting ob-jects with their pairwise relations in an image. This struc-tured representation could serve as an effective and com-pact representation for high-level visual understanding tasks such as image captioning [47, 48] and visual question an-swering [2, 11, 32]. Structure information between visual entities is the key to the success of many SGG methods. (cid:0): Corresponding author.
Figure 1. An example of scene graph generation. The scene graph is relatively sparser than the fully connected graph.
To capture this structure information, most existing meth-ods typically follows a multi-stage pipeline to decompose this complex task into sub-tasks of object detection, fully-connected relation graph construction, dense relation clas-sification [37, 49, 52], or dense-to-sparse relation classifica-tion [46], as shown in Fig. 2. These well-established meth-ods often rely heavily on object detection performance and involve redundant computation for fully-connected relation graph construction.
In addition to structure information, we observe that sparsity is another important property on relation detection in natural images. For example, in Fig. 1, the ground-truth triplets of ⟨ leg, on, woman ⟩ and ⟨ logo, on, shirt ⟩ are more commonly expressed than the relation between logo and leg.
Most existing dense or dense-to-sparse detection methods for SGG fails to well capture the general sparse and seman-tic priors. Accordingly, inspired by the recent sparse object detectors (e.g. DETR [3], Sparse R-CNN [34]), we present a new perspective on SGG by treating it as a direct sparse set prediction problem. However, unlike sparse object de-tection, sparse SGG is much more challenging due to its inherent difficulty in object pairing and relation prediction.
In this paper, we propose a direct sparse scene graph generation framework without explicit object detection and relation graph construction for inference, coined as Struc-tured Sparse R-CNN. As shown in Fig. 2c, the key to our Structured Sparse R-CNN is a set of learnable triplet queries and a structured triplet detector. These learnable triplet queries, composed of two object boxes, two object content vectors and one relation content vector, are respon-sible for capturing the general prior for sparse detection and encoding the spatial and appearance information of ob-jects and their relation. Based on the input of CNN fea-(a) Dense: MOTIFS (b) Dense-to-Sparse: Graph R-CNN (c) Sparse: Ours
Figure 2. Comparison of scene graph generation pipeline. (a) The dense detectors enumerates all object pairs for relation inference, e.g.
MOTIFS [49]. (b) The dense-to-sparse detectors utilize a pruning scheme to remove unreasonable pairs before the relation inference, e.g.
Graph R-CNN [46]. (c) Our network directly generates sparse scene graphs with triplet queries. ture maps and triplet queries, our triplet detector progres-sively detects the visual entities and recognizes their rela-tions. The triplet detector contains two cascaded modules for object pair detection and their relation prediction, re-spectively. Specifically, we devise structured connections for each triplet query to capture the hierarchical context in-formation. These structured connections first leverage the local interaction in object pairs (Pair Fusion) for better de-tection and then utilize the object information (E2R Fu-sion) for better relation prediction. The parameters of triplet queries are jointly optimized with network weights.
In practice, we find it is challenging to directly train our
Structured Sparse R-CNN from scratch. The major chal-lenge comes from the relatively sparse annotations of re-lations in the current datasets. The sparse relation anno-tations contain few related object labels, leading to incom-plete supervision signal for our object pair detection. Fur-thermore, the negative samples are hard to define in the ob-ject pair level. To solve this issue, we propose to build a
Siamese Sparse R-CNN to guide the training of our Struc-tured Sparse R-CNN in a knowledge distillation frame-work [10]. This Sparse R-CNN only generates pseudo-labels [25, 31, 43] for training and is inactivated during test-ing. With the help of these pseudo-labels, we design a new relaxed matching criteria for set prediction loss and enable the training of Structured Sparse R-CNN to be more sta-ble. Finally, to deal with imbalance distribution of object and relation categories in datasets, we propose an adaptive focusing parameter in our focal loss and utilize a post-hoc logit adjustment, to further boost the performance.
To verify the effectiveness of our framework, we perform experiments on several datasets: Visual Genome [15] and
Open Images V4/V6 [17]. The experiment results demon-strate that our model is able to yield new state-of-the-art performance under setting of the same backbone on all
In addition, we conduct in-depth ablation stud-datasets. ies to verify the effectiveness of structure modeling in our design. In summary, our main contribution is threefold:
• We present a new sparse and unified framework for direct scene graph generation, without explicit object detection and preceding graph construction for infer-ence. This new framework equipped with the struc-tured connection proposed by us shares several advan-tages, namely simplicity without multi-stage design, effective context modeling, and high efficiency.
• We present a practical training strategy to overcome the training difficulty of Structured Sparse R-CNN.
The knowledge distilled from a Siamese Sparse R-CNN can generate useful pseudo-labels to guide our training. We also propose an adaptive focusing param-eter and utilize logit adjustment for imbalance distri-bution of objects and relations.
• Experiment results demonstrate that our simple frame-work is able to yield the state-of-the-art performance for scene graph generation on Visual Genome and
Open Images V4/V6. We also perform detailed ab-lation studies to provide insights on our designs. 2.