Abstract
We present a massively parallel Lagrange decomposition method for solving 0â€“1 integer linear programs occurring in structured prediction. We propose a new iterative update scheme for solving the Lagrangean dual and a perturbation technique for decoding primal solutions. For representing subproblems we follow [40] and use binary decision dia-grams (BDDs). Our primal and dual algorithms require little synchronization between subproblems and optimiza-tion over BDDs needs only elementary operations without complicated control flow. This allows us to exploit the par-allelism offered by GPUs for all components of our method.
We present experimental results on combinatorial problems from MAP inference for Markov Random Fields, quadratic assignment and cell tracking for developmental biology. Our highly parallel GPU implementation improves upon the run-ning times of the algorithms from [40] by up to an order of magnitude. In particular, we come close to or outper-form some state-of-the-art specialized heuristics while be-ing problem agnostic. Our implementation is available at https://github.com/LPMP/BDD. 1.

Introduction
Solving integer linear programs (ILP) efficiently on paral-lel computation devices is an open research question. Done properly it would enable more practical usage of many ILP problems from structured prediction in computer vision and machine learning. Currently, state-of-the-art generally ap-plicable ILP solvers tend not to benefit much from paral-lelism [45]. In particular, linear program (LP) solvers for computing relaxations benefit modestly (interior point) or not at all (simplex) from multi-core architectures. In par-ticular generally applicable solvers are not amenable for execution on GPUs. To our knowledge there exists no prac-tical and general GPU-based optimization routine and only a few solvers for narrow problem classes have been made
GPU-compatible e.g. [1,48,58,65]. This, and the superlinear runtime complexity of general ILP solvers has hindered ap-plication of ILPs in large structured prediction problems, ne-cessitating either restriction to at most medium problem sizes
General
Purpose
FastDOG
CPU
GPU several hundred works
[1, 48, 58, 65]
Specialized
Figure 1. Qualitative comparison of ILP solvers for structured prediction. Our solver (FastDOG) is faster than Gurobi [23] and comparable to specialized CPU solvers, but outperformed by spe-cialized GPU solvers. FastDOG is applicable to a diverse set of applications obviating the human effort for developing solvers for new problem classes. or difficult and time-consuming development of specialized solvers as observed for the special case of MAP-MRF [33].
We argue that work on speeding up general purpose ILP solvers has had only limited success so far due to compli-cated control flow and computation interdependencies. We pursue an overall different approach and do not base our work on the typically used components of ILP solvers. Our approach is designed from the outset to only use operations that offer sufficient parallelism for implementation on GPUs.
We argue that our approach sits on a sweet spot between general applicability and efficiency for problems in struc-tured prediction as shown in Figure 1. Similar to general purpose ILP solvers [15, 23], there is little or no effort to adapt these problems for solving them with our approach. On the other hand we outperform general purpose ILP solvers in terms of execution speed for large problems from struc-tured prediction and achieve runtimes comparable to hand-crafted specialized CPU solvers. We are only significantly outperformed by specialized GPU solvers. However, de-velopment of fast specialized solvers especially on GPU is time-consuming and needs to be repeated for every new problem class.
Our work builds upon [40] in which the authors proposed a Lagrange decomposition into subproblems represented by binary decision diagrams (BDD). The authors proposed se-quential algorithms as well as parallel extensions for solving the Lagrange decomposition. We improve upon their solver by proposing massively parallelizable GPU amenable rou-tines for both dual optimization and primal rounding. This results in significant runtime improvements as compared to their approach. 2.