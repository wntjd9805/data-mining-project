Abstract
The goal of open-world compositional zero-shot learn-ing (OW-CZSL) is to recognize compositions of state and objects in images, given only a subset of them during train-ing and no prior on the unseen compositions. In this set-ting, models operate on a huge output space, containing all possible state-object compositions. While previous works tackle the problem by learning embeddings for the com-positions jointly, here we revisit a simple CZSL baseline and predict the primitives, i.e. states and objects, indepen-dently. To ensure that the model develops primitive-specific features, we equip the state and object classifiers with sepa-rate, non-linear feature extractors. Moreover, we estimate the feasibility of each composition through external knowl-edge, using this prior to remove unfeasible compositions from the output space. Finally, we propose a new setting, i.e. CZSL under partial supervision (pCZSL), where either only objects or state labels are available during training, and we can use our prior to estimate the missing labels.
Our model, Knowledge-Guided Simple Primitives (KG-SP), achieves state of the art in both OW-CZSL and pCZSL, sur-passing most recent competitors even when coupled with semi-supervised learning techniques. Code available at: https:// github.com/ ExplainableML/ KG-SP. 1.

Introduction
As humans, we interact with objects depending on their state. For instance, we use ripe lemons rather than moldy ones to prepare a lemonade, and we clean dirty dishes after using them. Algorithms that can recognize objects together with their state are crucial for autonomous agents to show the same high-level interactions capabilities we have. In the literature, this problem is studied under the name of Com-positional Zero-shot Learning (CZSL). In CZSL, we are given a training set with images of objects in a subset of their possible states and, at test time, the goal is to recognize com-positions of the same set of objects and states, even unseen during training. Since an object has a different appearance depending on its state (e.g. dry dog vs wet dog) and a state
Figure 1. We consider the problems of open-world CZSL (OW-CZSL), where we lack priors on unseen compositions at test time, and CZSL under partial supervision (pCZSL) where we also lack compositional labels during training (left). We tackle them by independently predicting object (red) and state (blue) labels and by using external knowledge (bottom) to estimate the feasibility of compositions, reducing the search space during inference and improving pseudo-labeling during training in pCZSL. modifies objects in different ways (e.g. wet dog vs wet car), the challenge of CZSL is modeling how states and objects interact with each other, extrapolating this knowledge from seen to unseen compositions. Under this perspective, mul-tiple works modeled the interactions of objects and states, either through compositional classifiers [26, 31], or a shared embedding space [23, 28, 29].
Despite their effectiveness, [24] showed how the perfor-mance of CZSL methods degrade in the open-world setting (OW-CZSL). In OW-CZSL, there are no priors on the unseen compositions, and models must consider all possible compo-sitions at test time. Due to the large cardinality of the output space, it is difficult to produce discriminative embeddings for the unseen compositions [24]. Inspired by the findings of [24], in this work we explore a completely different direc-tion. Specifically, we design an architecture that disregards the compositional nature of the problem and produces the initial predictions independently for objects and states. The idea is that while discriminating between compositions is hard in OW-CZSL due to the large search space, recognizing primitives (i.e. objects and states) in isolation is easier since
1) the cardinality of the two sets is much lower and 2) the sets are fixed at both training and test time.
Inspired from [26] and [19], we design a simple method that predicts objects and states with two independent clas-sifiers. Since recognizing states requires different features w.r.t. recognizing objects, instead of having a shared fea-ture representation, we train our model with two different non-linear feature extractors. Furthermore, since not all com-positions are equally feasible in reality (e.g. ripe dog) we can refine the predictions of our model by eliminating less feasible compositions from the output space. With this goal, we use external knowledge (i.e. ConceptNet [39]) to estimate the compatibility between a state and an object, using these estimates to remove less feasible compositions at test time.
We name our model Knowledge-Guided Simple Primitives (KG-SP). As our KG-SP method does not require composi-tional labels during training, we explore a new challenging setting, i.e. CZSL under partial supervision (pCZSL). In pCZSL, training samples have either only object or state annotation, but not both. Here we use our prior on feasible compositions to aid pseudo-labeling during training. Experi-ments show that KG-SP is either competitive or surpasses the current state of the art in OW-CZSL and outperforms re-cent CZSL approaches on pCZSL setting. Figure 1 provides an overview of KG-SP and the two tasks.
Contributions. To summarize, 1) inspired by [19, 26], our model predicts state and objects independently while at the same time removing less feasible compositions from the output space based on external contextual information about the feasibility of certain compositions; 2) we explore the problem of CZSL under partial supervision, where either object or state information is missing in the ground-truth; 3) we adapt recent baselines for pCZSL showing that KG-SP outperforms them even when coupled with semi-supervised learning techniques in both OW-CZSL and pCZSL settings. 2.