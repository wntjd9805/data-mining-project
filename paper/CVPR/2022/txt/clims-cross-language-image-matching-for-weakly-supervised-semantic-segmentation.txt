Abstract
It has been widely known that CAM (Class Activation
Map) usually only activates discriminative object regions and falsely includes lots of object-related backgrounds. As only a ﬁxed set of image-level object labels are available to the WSSS (weakly supervised semantic segmentation) model, it could be very difﬁcult to suppress those diverse background regions consisting of open set objects. In this paper, we propose a novel Cross Language Image Match-ing (CLIMS) framework, based on the recently introduced
Contrastive Language-Image Pre-training (CLIP) model, for WSSS. The core idea of our framework is to introduce natural language supervision to activate more complete ob-ject regions and suppress closely-related open background regions. In particular, we design object, background region and text label matching losses to guide the model to excite more reasonable object regions for CAM of each category.
In addition, we design a co-occurring background sup-pression loss to prevent the model from activating closely-related background regions, with a predeﬁned set of class-related background text descriptions. These designs enable the proposed CLIMS to generate a more complete and com-pact activation map for the target objects. Extensive exper-iments on PASCAL VOC2012 dataset show that our CLIMS signiﬁcantly outperforms the previous state-of-the-art meth-ods. Code will be available at https://github.com/CVI-SZU/CLIMS. 1.

Introduction
Semantic segmentation attempts to assign each pixel in an image with a semantic label. Though fully supervised semantic segmentation has achieved remarkable success in recent years, pixel-level annotation is signiﬁcantly time-∗Corresponding Author
†
Equal Contribution (a) Conventional CAM solution.
Figure 1. (b) The proposed
CLIMS. The problem of false-activation of irrelevant background, e.g., railroad and ground, and underestimation of object contents usually exist in conventional CAM method. To solve this prob-lem, we propose a novel text-driven learning framework, CLIMS, which introduces natural language supervision, i.e., an open-world setting, for exploring complete object contents and excluding irrel-evant background regions. Best viewed in color. consuming and labor-intensive. Instead, weakly supervised semantic segmentation (WSSS) tries to mitigate this issue by relying solely on image-level [2, 15, 23, 24], bounding box-level [8, 22], point-level [3], or scribble-based supervi-sion [20, 29]. This work aims to only use image-level labels in the learning of a semantic segmentation model.
Existing WSSS approaches typically follow a three-stage learning process. First, the image-level labels are used as supervision at the feature level to train a classiﬁcation net-work to generate initial class activation maps (CAMs) (as shown in the left of Fig. 1(a)). Then the initial CAMs are re-ﬁned as pseudo ground-truth masks using dense CRF [16], pixel afﬁnity-based methods [1, 2], or additional saliency maps [12, 19, 21]. Finally, the reﬁned pseudo ground-truth masks are used to further train a segmentation network.
However, as only a ﬁxed set of object categories is avail-able during the ﬁrst stage training, i.e., a close-world set-ting, class-related background pixels, e.g., railroad, also
contribute to the prediction of closely related objects, e.g., train. This results in unnecessary activation of background in the generation of initial CAMs, as shown in the right of Fig. 1(a). Besides, conventional CAM solution usually struggles in the underestimation of object contents. Both of them severely limit the quality of initial CAMs for the next two stages.
In this paper, we design a novel Cross Language Image
Matching framework for WSSS, i.e., CLIMS, based on the power of recently introduced Contrastive Language-Image
Pre-training (CLIP) [25] model, to address the aforemen-tioned issues. The CLIP model is pretrained from scratch on a dataset of 400 million image-text pairs (automatically collected from the publicly available sources on the Inter-net), which enables CLIP to associate much wider visual concepts in the image with their text labels in an open-world setting, rather than a ﬁxed set of predetermined object categories. Based on this, the proposed CLIMS has great potentials to generate a high-quality initial activation map for each object category without irrelevant background (as shown in the right of Fig. 1(c)).
In the left of Fig. 1(a), the conventional CAM method performs image-level supervision on the average feature after the global average pooling (GAP) layer. Given the trained model, the class activation maps (CAMs) can be ex-tracted. However, the CLIP model cannot be directly used in this pipeline. Instead, as shown in Fig. 1(b), we replace the GAP and fully connected (FC) layer with convolutional layers to directly generate an activation map for each class under the supervision from CLIP model, where natural lan-guage can be used to guide the model for the activation maps generation.
Details of the proposed CLIMS are depicted in Fig. 2. It mainly consists of a backbone network and a text-driven evaluator including three CLIP-based loss functions, i.e.,
Object region and Text label Matching loss (LOT M ), Back-ground region and Text label Matching loss (LBT M ), and
Co-occurring