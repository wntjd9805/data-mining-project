Abstract
Transformer architecture has been showing its great strength in visual object tracking, for its effective atten-tion mechanism. Existing transformer-based approaches adopt the pixel-to-pixel attention strategy on flattened im-age features and unavoidably ignore the integrity of ob-jects.
In this paper, we propose a new transformer ar-chitecture with multi-scale cyclic shifting window atten-tion for visual object tracking, elevating the attention from pixel to window level. The cross-window multi-scale at-tention has the advantage of aggregating attention at dif-ferent scales and generates the best fine-scale match for the target object. Furthermore, the cyclic shifting strat-egy brings greater accuracy by expanding the window sam-ples with positional information, and at the same time saves huge amounts of computational power by removing redun-dant calculations. Extensive experiments demonstrate the superior performance of our method, which also sets the new state-of-the-art records on five challenging datasets, along with the VOT2020, UAV123, LaSOT, TrackingNet, and GOT-10k benchmarks. Our project is available at https://github.com/SkyeSong38/CSWinTT. 1.

Introduction
Visual object tracking (VOT) is one of the fundamen-tal problems in computer vision research with a wide range of applications in video surveillance, autonomous vehicles, human-machine interaction, and others. It aims to estimate the position of a target object in each video frame, com-monly represented as a bounding box encapsulating the tar-get. The target object is given as a template in the initial frame, and the tracker is required to extract proper fea-tures about the target and localize the target in the follow-ing frames. Most of the popular trackers [1, 22, 23, 37, 41] adopt the Siamese network structure, which conducts track-ing by calculating the similarity between the template and search region in the current frame. The similarity metric
* Corresponding author (a) Cyclic shifting window-level Attn (b) Pixel-level Attn
Figure 1. (a) The proposed approach firstly achieves the window-level attention between query and key through window partition-ing, and then applies cyclic shifts for each window (from the base sample in the red box to generated samples surrounded by orange boxes) to greatly extend the number of window samples, while maintaining the integrity of objects. (b) Previous transformers pro-duce pixel-level attention, which weakens the positional informa-tion between pixels and ignores the integrity of objects. of cross-correlation used in Siamese trackers is prone to lose much semantic information for it is a single-level linear computational process. This deficiency can be well tackled by using the attention mechanism to learn the global con-text. Recently, transformer-based approaches [6, 12, 25, 39] have reported new state-of-the-art performance on image recognition, object detection, and semantic segmentation benchmarks. This is no wonder as transformer [36] has a powerful cross-attention mechanism to reasoning between patches [18]. Particularly, transformer trackers [7,40,45,50] have shown their great strength by introducing the atten-tion mechanism to enhance and fuse the features of the tar-get and the tracked object. However, we observe that these transformer trackers simply put the flattened features of the template and search region into pixel-level attention, each pixel of a flattened feature (Query) matches all pixels of an-other flattened feature (Key) in a complete and disordered manner, as shown in Figure 1b. This pixel-level attention destroys the integrity of the target object and leads to infor-mation loss of relative positions between pixels.
In this paper, we propose a novel multi-scale cyclic shift-ing window transformer for visual object tracking to further
lift pixel-level attention to window-level attention, calculat-ing attention between indivisible windows by treating each window as a whole keeps the location information within the window. The proposed method is inspired by the semi-nal work of the Swin Transformer [25], which adopts a hier-archical transformer structure by starting from small-sized patches and gradually increasing the size through merging to achieve a broader receptive field. Different from the Swin
Transformer, we calculate the cross-window attention be-tween the template and search region directly, which helps to discriminate the target from the background by ensuring the integrity of the object. Further, we propose a multi-head multi-scale attention where each head of the trans-former measures the relevance among partitioned windows at a specific scale. The key idea here is to apply a cyclic shifting strategy on each window, as shown in Figure 1a, for generating more accurate attention results. To address performance drop around boundaries caused by the cyclic shifting operation, we design a spatially regularized atten-tion mask which turns out to be very effective in alleviat-ing the boundary artifacts. Finally, we present some ef-ficient computation strategies to avoid redundant compu-tation introduced by multi-scale cyclic shifting windows, which greatly reduce the time and computational cost. Ex-tensive experiments demonstrate that our tracker performs remarkably better than other state-of-the-art algorithms.
To summarize, our main contributions include: 1. We propose a novel transformer architecture with multi-scale cyclic shifting window attention for visual object tracking, uplifting the original pixel-level atten-tion to the new deliberately designed window-level at-tention. The cross-window attention ensures the in-tegrity of the tracking object, and the cyclic shifts bring greater accuracy by expanding window samples. 2. We design a spatially regularized attention mask and some computational optimization strategies to im-prove the accuracy and speed of the window attention.
Specifically, a spatially regularized attention mask is used to address performance drop around boundaries caused by the cyclic shifts, and we propose three com-putational optimization strategies to remove redundant computations. 2.