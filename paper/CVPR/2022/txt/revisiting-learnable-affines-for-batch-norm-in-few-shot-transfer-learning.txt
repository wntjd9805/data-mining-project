Abstract
Batch normalization is a staple of computer vision models, including those employed in few-shot learning. Batch nor-malization layers in convolutional neural networks are com-posed of a normalization step, followed by a shift and scale of these normalized features applied via the per-channel trainable affine parameters γ and β. These affine param-eters were introduced to maintain the expressive powers of the model following normalization. While this hypoth-esis holds true for classification within the same domain, this work illustrates that these parameters are detrimen-tal to downstream performance on common few-shot trans-fer tasks. This effect is studied with multiple methods on well-known benchmarks such as few-shot classification on miniImageNet, cross-domain few-shot learning (CD-FSL) and META-DATASET. Experiments reveal consistent perfor-mance improvements on CNNs with affine unaccompanied batch normalization layers; particularly in large domain-shift few-shot transfer settings. As opposed to common practices in few-shot transfer learning where the affine pa-rameters are fixed during the adaptation phase, we show fine-tuning them can lead to improved performance. 1.

Introduction
Over the last decade, the growing availability of data has allowed deep neural networks to achieve remarkable per-formance on various visual recognition tasks [10, 12, 13].
However, the size and variability of the dataset can have a huge impact on the effectiveness of these models. Deep neu-ral networks trained on datasets from a specific distribution often fail to generalise their performance to new domains, creating a compelling need for large-scale datasets [33]. De-*∗Equal contributions. †Equal senior author contribution. This research was partially funded by NSERC Discovery Grants [E.B., S.E.K.]; and
CIFAR AI Chair [S.E.K]. [M.C.] is funded by IVADO PRF Grant. We thank Compute Canada and Calcul Quebéc for computational resources.
Moslem.Yazdanpanah@gmail.com,
Correspondence aamer.abdul-rahman.1@ens.etsmtl.net to:
Figure 1. Aggregated distributions of normalization layers from a ResNet10 model, pre-trained on miniImageNet, and fed with samples from EuroSat. Although the input distributions differ, the model with FN appears to accomodate the role of the affine parameters, resulting in a more centered input to the normalization layer (left) with relatively similar output distributions (right). velopments in few-shot learning (FSL) have enabled deep neural networks to draw data representations from target classes with just a few labelled samples [7, 29, 35].
Throughout the literature, batch normalization (BN) [14] layers are ubiquitous in FSL techniques. They speed up model convergence and are believed to add regularization
[22]. Adding BN layers to deep learning models stabilizes the distribution of layer input features by modulating their mean and variance [14]. This results in a smoother optimiza-tion landscape and improved performance across a variety of computer vision problems [28]. Despite these achievements, there persists a poor understanding of the source of effective-ness from BN layers. Moreover, recent work has revealed that these gains may not be the result of alleviating internal covariate shift, as initially believed [28].
BN layers typically consist of two steps. First, the input features are normalized by the mean and standard deviation
over the spatial dimensions of each channel across a mini-batch. These normalized features are then scaled and shifted by the trainable coefficient γ and bias β (the affine param-eters). In this paper, we refer to the initial step as “Feature
Normalization” (FN). The affine parameters acting in the second step serve to preserve the expressive capabilities of the neural network following the normalization of features.
In order to bridge distributional gaps between the source and target datasets, notable efforts have been directed to-wards the area of domain adaptation. Li et al. [21] state that the label information is usually stored in the network’s weight matrix while the statistics of the BN layer represent domain-related knowledge. This interpretation leads to a reasonable question – upon facing a novel target distribution, are the BN’s affine parameters still helpful? Recent work has touched upon the auxiliary benefits of these affine parame-ters towards weight layers [8]. However, the negative effect of this biased adaptation to training labels when facing novel labels of a distant target domain is yet to be explored.
In this work, we investigate the effect of replacing BN with FN layers towards the generalizability of convolu-tional neural networks (CNN) in few-shot transfer learning.
Our experiments on multiple few-shot transfer benchmarks such as miniImageNet [35], cross-domain few-shot learn-ing (CDFSL) [11] and META-DATASET [34] confirm that using batch normalization when learning on the source do-main harms few-shot generalization on the target domain.
We show Feature Normalization achieves significantly better results in similar settings. We hypothesize the decrease in performance in models using BN could be related to BN’s sparsifying effect in conjunction with the ReLU (See sec-tion 4.4 for quantitative evaluation). Ablation studies are conducted to determine the isolated influence of γ and β towards few-shot transfer tasks.
To learn more generalizable features from the source domain and to better adapt to the target domain, we develop a novel methodology for few-shot transfer where we apply
Feature Normalization during representation learning on the source domain (we refer to this learning phase as “base training”) and batch normalization when adapting to the target domain—we refer to this technique as “Fine-Affine”.
With this methodology, we gain from the best of both worlds and achieve an overall better result.
The rest of this paper is structured as follows. Few-shot transfer and normalization-based approaches are reviewed in
Section 2. Formal definitions for Feature Normalization and
Fine-Affine are presented in Section 3, Section 4 describes the benchmarks and experimental setups as well as the eval-uation results. Finally, we draw conclusions in Section 5. 2.