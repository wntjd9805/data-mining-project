Abstract
Current LiDAR-only 3D detection methods inevitably suf-fer from the sparsity of point clouds. Many multi-modal methods are proposed to alleviate this issue, while different representations of images and point clouds make it difﬁ-cult to fuse them, resulting in suboptimal performance. In this paper, we present a novel multi-modal framework SFD (Sparse Fuse Dense), which utilizes pseudo point clouds gen-erated from depth completion to tackle the issues mentioned above. Different from prior works, we propose a new RoI fusion strategy 3D-GAF (3D Grid-wise Attentive Fusion) to make fuller use of information from different types of point clouds. Speciﬁcally, 3D-GAF fuses 3D RoI features from the pair of point clouds in a grid-wise attentive way, which is more ﬁne-grained and more precise. In addition, we pro-pose a SynAugment (Synchronized Augmentation) to enable our multi-modal framework to utilize all data augmentation approaches tailored to LiDAR-only methods. Lastly, we cus-tomize an effective and efﬁcient feature extractor CPConv (Color Point Convolution) for pseudo point clouds. It can explore 2D image features and 3D geometric features of pseudo point clouds simultaneously. Our method holds the highest entry on the KITTI car 3D object detection leader-board†, demonstrating the effectiveness of our SFD. Code will be made publicly available. 1.

Introduction
In recent years, the rise of deep learning and autonomous driving has led to a rapid development of 3D detection. Cur-rent 3D detection methods are mainly based on LiDAR point clouds [1, 3, 6, 22, 23, 29, 30, 42, 43, 50], while the sparsity of point clouds considerably limits their performances. The sparse LiDAR point clouds provide poor information in dis-tant and occluded regions, making it difﬁcult to generate precise 3D bounding boxes. Many multi-modal methods are proposed to address this problem. MV3D [2] introduces
∗Corresponding author
†On the date of CVPR deadline, i.e., Nov.16, 2021 an RoI fusion strategy to fuse features of images and point clouds on the second stage. AVOD [15] proposes to fuse full resolution feature crops from the image feature maps and
BEV feature maps for a high recall. MMF [20] leverages 2D detection, ground estimation and depth completion to assist 3D detection. In MMF, pseudo point clouds are used for backbone feature fusion, and depth completion feature maps are used for RoI feature fusion. Despite their great success, they have two shortcomings.
Coarse RoI Fusion Strategy When fusing RoI features, as shown in Figure 2(a), previous RoI fusion methods con-catenate 2D LiDAR RoI features cropped from BEV LiDAR feature maps and 2D image RoI features cropped from FOV image feature maps. We note that this RoI fusion strategy is coarse. Firstly, 2D image RoI features are usually mixed with features from other objects or backgrounds, which will confuse the model. Secondly, the RoI fusion strategy ignores object part correspondences in 2D images and 3D point clouds. In this paper, we propose a more ﬁne-grained RoI fusion strategy 3D-GAF (3D Grid-wise Attentive Fusion), which fuse 3D RoI features instead of 2D RoI features as shown in Figure 2(b). We elaborate on three advantages of 3D-GAF over previous RoI fusion methods in Section 3.3.
Insufﬁcient Data Augmentation This shortcoming ex-ists in most multi-modal methods. Because 2D image data cannot be operated like 3D LiDAR data, many data aug-mentation approaches are difﬁcult to deploy in multi-modal methods. It is a crucial reason why multi-modal methods are usually inferior to single-modal methods [47]. To this end, we introduce our SynAugment (Synchronized Aug-mentation). We observe that after converting 2D images to 3D pseudo point clouds, the representations of images and raw point clouds are uniﬁed, suggesting that we can operate images just like raw point clouds. However, it is not enough. Some complicated data augmentation approaches such as gt-sampling [41] and local rotation [49] may cause occlusions on the FOV (ﬁeld of view). It is a non-negligible issue because image features need to be extracted on the
FOV. Now, it is time to jump out of the mindset. With 2D images converted to 3D pseudo point clouds, why don’t we
directly extract image features in 3D space? In this way, we no longer need to consider the FOV occlusion issue.
Nevertheless, it is non-trivial to extract features of pseudo point clouds in 3D space. Thus, we present a CPConv (Color
Point Convolution), which searches neighbors of pseudo points on the image domain. It enables us to extract both im-age features and geometric features of pseudo point clouds efﬁciently. Considering the FOV occlusion issue, we cannot project all pseudo points to the image space of current frame for neighbor search. Here we propose an RoI-aware Neigh-bor Search, which projects pseudo points in each 3D RoI to their original image space, as illustrated in Figure 3. Hence, pseudo points that occlude each other on the FOV will not become neighbors when performing neighbor search. In other words, their features will not interfere with each other.
To summarize, our contributions are listed as follows:
• We propose a new RoI feature fusion strategy 3D-GAF to fuse RoI features from raw point clouds and pseudo point clouds in a more ﬁne-grained manner.
• We present a data augmentation method SynAugment to solve the insufﬁcient data augmentation issue that multi-modal methods suffer from.
• We customize an effective and efﬁcient feature extractor
CPConv for pseudo point clouds. It can extract both 2D image features and 3D geometric features.
• We demonstrate the effectiveness of our method with extensive experiments. Specially, we rank 1st on the
KITTI car 3D object detection leaderboard. 2.