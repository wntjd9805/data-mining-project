Abstract
Self-supervised learning (SSL) holds promise in lever-aging large amounts of unlabeled data. However, the suc-cess of popular SSL methods has limited on single-centric-object images like those in ImageNet and ignores the cor-relation among the scene and instances, as well as the se-mantic difference of instances in the scene. To address the above problems, we propose a Unified Self-supervised Vi-sual Pre-training (UniVIP), a novel self-supervised frame-work to learn versatile visual representations on either single-centric-object or non-iconic dataset. The frame-work takes into account the representation learning at three levels: 1) the similarity of scene-scene, 2) the correla-tion of scene-instance, 3) the discrimination of instance-instance. During the learning, we adopt the optimal trans-port algorithm to automatically measure the discrimina-tion of instances. Massive experiments show that Uni-VIP pre-trained on non-iconic COCO achieves state-of-the-art transfer performance on a variety of downstream tasks, such as image classification, semi-supervised learn-ing, object detection and segmentation. Furthermore, our method can also exploit single-centric-object dataset such as ImageNet and outperforms BYOL by 2.5% with the same pre-training epochs in linear probing, and surpass current self-supervised object detection methods on COCO dataset, demonstrating its universality and potential. 1.

Introduction
Deep learning has shown excellent performance on var-ious computer vision tasks [12, 20, 21, 25, 32, 47] using la-bels. Self-supervised learning (SSL) of visual representa-Figure 1. Visualization of different cropped views on COCO dataset. (a) Images from COCO contain multiple instances, thus different random crops might represent different semantic mean-ings and are not satisfied with the semantic consistency assump-tion. (b) Our method for unified self-supervised learning. The two scene views are created with overlapping regions while the over-lapping regions contain multiple instances. tion aims at capturing salient feature representation without relying on human annotations. Recently, contrastive learn-ing [1, 4–7, 17, 18, 23, 41] based SSL has proved impressive results on a number of downstream tasks, largely narrowing the gap between unsupervised and supervised learning and even surpassing the supervised counterpart. These state-of-the-art (SOTA) methods build upon the pretext task called instance discrimination, which regards different views of a single image as the same instance and its objective is simply to learn a feature representation that discriminates among images. Hence, the basic assumption of these methods is that the pre-training data should have the property of se-mantic consistency [3, 27], i.e., the assumption highly re-lies on the single-centric-object data such as those in Ima-geNet [12]. Nevertheless, it is infeasible for complex natu-ral images since they usually consist of multiple instances as shown in Fig 1(a). Some work [17,29,34] naively extend the off-the-shelf SSL methods from ImageNet to other datasets, like MS COCO [26], Places365 [52], and YFCC100M [36], yet they do not acquire satisfactory results.
It is known that multiple instances in the single natu-ral image possess the co-occurrence relationship, and usu-ally have different semantic meanings. Therefore, the mod-els should have the ability to distinguish the semantics of different instances. However, it is still challenging to dis-criminate different instances residing in the single natural image when no instance annotations are available. Several region-level based methods [27, 30, 33] propose to leverage multiple local regions to pre-train models using non-iconic dataset, and achieve the success of the specific downstream task. Nevertheless, these region-level based methods do not explicitly distinguish different instances in the scene. In ad-dition, their results of linear evaluation are inferior to the baseline, i.e., these methods can not obtain versatile visual representations. Moreover, natural images have prior that the scene and instances in the scene have the semantic affin-ity since these instances correlate with the scene. Current
SSL methods are not aware of the prior and do not encode the semantic affinity. Because of the above problems, the application scenario of these methods is limited. It is es-sential to design an effective learning paradigm to obtain versatile visual representations.
In this paper, we introduce a unified self-supervised pre-training framework, named UniVIP, to learn the visual rep-resentations by pre-training on either single-centric-object or non-iconic dataset. Specifically, we first exploit the un-supervised instance proposal method Selective Search [38] to generate candidate instances. Then, for each image, we create two scene views with overlapping regions containing instances to guarantee the global similarity, i.e., the simi-larity of scene-scene, as much as possible, which will ef-fectively alleviate the semantic inconsistency of different scene views. Moreover, to tackle the correlation of scene-instance, the generated instances are grouped to approxi-mate the semantics of the corresponding scene views, guid-ing the network to learn a variety of instances in the im-age. In our UniVIP, the discrimination of instance-instance is formulated as the optimal matching problem among all candidate instances in overlapping regions and uses the op-timal transport algorithm [15] to discriminate different in-stances in the scene. Our objective consists of the above three items, and different views of some scenes and in-stances obtained by our UniVIP are shown in Fig 1(b). It is noted that our framework is specially designed to learn versatile representations from natural images, and is able to fully leverage the prior of semantic affinity among the natural scene and instances in the scene, and explicitly dis-tinguish co-occurrence instances.
Massive experiments in single-centric-object and non-iconic datasets prove that UniVIP can learn the versatile representations. In particular, our method outperforms the state-of-the-art by 2.3% top-1 classification accuracy with pre-training on COCO dataset for the ImageNet [12] linear evaluation protocol. Our 300-epoch UniVIP achieves 42.2 bbox mAP and 38.2 mask mAP using Mask R-CNN [20] on
COCO detection and segmentation with 1× schedule when pre-trained on ImageNet, and even surpasses the popular self-supervised object detection methods.
Overall, we make the following contributions:
• We proposed a Unified Self-supervised Representations
Learning framework to effectively overcome the seman-tic inconsistency of random views in non-iconic images, and it can be pre-trained with any images.
• We proposed to simultaneously leverage the similarity of scene-scene, the correlation of scene-instance, and the discrimination of instance-instance to promote the performance of models effectively.
• Extensive experiments demonstrate the effectiveness and stronger generalization ability of our method.
Specifically, the models pre-trained with UniVIP on single-centric-object and non-iconic datasets all outper-form previous SOTA methods in multiple downstream tasks, such as image classification, semi-supervised learning, object detection and segmentation. 2.