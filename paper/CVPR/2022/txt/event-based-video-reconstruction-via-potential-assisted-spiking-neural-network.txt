Abstract
Neuromorphic vision sensor is a new bio-inspired imag-ing paradigm that reports asynchronous, continuously per-pixel brightness changes called ‘events’ with high tempo-ral resolution and high dynamic range. So far, the event-based image reconstruction methods are based on artiﬁ-cial neural networks (ANN) or hand-crafted spatiotempo-ral smoothing techniques.
In this paper, we ﬁrst imple-ment the image reconstruction work via deep spiking neu-ral network (SNN) architecture. As the bio-inspired neural networks, SNNs operating with asynchronous binary spikes distributed over time, can potentially lead to greater com-putational efﬁciency on event-driven hardware. We pro-pose a novel Event-based Video reconstruction framework based on a fully Spiking Neural Network (EVSNN), which utilizes Leaky-Integrate-and-Fire (LIF) neuron and Mem-brane Potential (MP) neuron. We ﬁnd that the spiking neu-rons have the potential to store useful temporal information (memory) to complete such time-dependent tasks. Further-more, to better utilize the temporal information, we propose a hybrid potential-assisted framework (PA-EVSNN) using the membrane potential of spiking neuron. The proposed neuron is referred as Adaptive Membrane Potential (AM-P) neuron, which adaptively updates the membrane poten-tial according to the input spikes. The experimental results demonstrate that our models achieve comparable perfor-mance to ANN-based models on IJRR, MVSEC, and HQF datasets. The energy consumptions of EVSNN and PA-EVSNN are 19.36(cid:2) and 7.75(cid:2) more computationally ef-ﬁcient than their ANN architectures, respectively. The code and pretrained model are available at https://sites. google.com/view/evsnn. 1.

Introduction
Event cameras [2, 5] are bio-inspired vision sensors that pose a paradigm shift in the way visual information is ac-quired. Compared with standard cameras, event cameras have high temporal resolution, high dynamic range (140 dB (cid:3)Corresponding author. vs. 60 dB of standard cameras), and low power consump-tion. Event cameras work asynchronously, recording the stream of events (t; x; y; p) which includes the timestamp, pixel location and polarity of the brightness changes.
Despite the advantages of the event data, it is not friend-ly to human vision and traditional computer vision [48, 53].
As a solution, image reconstruction bridges the gap between human visualization and events, giving us an intuition of the rich information encoded by events. On other hand, im-age is a useful representation for conventional frame-based computer vision [42]. Reconstructing images from asyn-chronous events has been explored in various researches.
Early works attempt to recover the intensity of an image from events based on hand-crafted priors [46, 47, 3, 30].
Recently, deep neural network based reconstruction model-s [52, 42, 43, 48, 50, 34, 55] have demonstrated impressive performance. The events usually be transformed in to time-surfaces, event images or voxel grids as the input of con-volutional neural network. However, large artiﬁcial neural networks (ANN) can be memory and computationally in-tensive [48], consuming power and hampering the low la-tency of event cameras.
In fact, the sparse event data can be effectively com-bined with neuromorphic hardware for low-power spiking neural network (SNN) applications [14]. Compared with
ANN, SNN is more biologically realistic and its neurons communicate with each other via discrete spikes instead of continuous-valued activations. Visual systems [32, 1] con-structed with SNN and event cameras have demonstrated their capacity in solving visual tasks as well as prominent energy-efﬁciency. However, most of the SNN work has so far been focused on problems like classiﬁcation [10, 59, 62], optical estimation [35, 15], motion segmentation [33], and angular velocity regression [11]. To the best of our knowl-edge, we are the ﬁrst to attempt image reconstruction task based on a deep SNN architecture.
In this paper, we propose a novel Event-based Video re-construction framework based on a fully Spiking Neural
Network (EVSNN), which utilizes the Leaky-Integrate-and-Fire (LIF) neurons and a Membrane Potential (MP) neuron.
To better extract the temporal information, we propose a hy-brid potential-assisted framework (PA-EVSNN) using the
membrane potential of spiking neurons. The main contri-butions of this paper are summarized as follows: 1) We ﬁrst explore a fully spiking neural network (EVSNN) architecture on event-based image reconstruc-tion, which utilizes LIF neuron and MP neuron. This is also the ﬁrst attempt to develop a deep SNN for image re-construction task. 2) We propose a hybrid potential-assisted SNN (PA-EVSNN), which uses adaptive membrane potential (AMP) neurons to improve the temporal receptive ﬁeld of EVSNN.
AMP neurons can adjust the membrane time constant ac-cording to the input spike to adapt to various reconstruction scenes. 3) The experiments on public datasets demonstrate that the proposed models have comparable performance to ex-isting ANN-based models, while the energy consumptions of EVSNN and PA-EVSNN are 19.36(cid:2) and 7.75(cid:2) more computationally efﬁcient than their ANN architectures, re-spectively. Compared to E2VID, the proposed EVSNN and
PA-EVSNN achieve 24.15(cid:2) and 8.76(cid:2) more computation-ally efﬁcient improvement, respectively. 2.