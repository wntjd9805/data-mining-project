Abstract
Amodal completion is a visual task that humans per-form easily but which is difficult for computer vision al-gorithms. The aim is to segment those object boundaries which are occluded and hence invisible. This task is partic-ularly challenging for deep neural networks because data is difficult to obtain and annotate. Therefore, we formu-late amodal segmentation as an out-of-task and out-of-distribution generalization problem. Specifically, we re-place the fully connected classifier in neural networks with a Bayesian generative model of the neural network fea-tures. The model is trained from non-occluded images us-ing bounding box annotations and class labels only, but is applied to generalize out-of-task to object segmentation and to generalize out-of-distribution to segment occluded objects. We demonstrate how such Bayesian models can naturally generalize beyond the training task labels when they learn a prior that models the object’s background con-text and shape. Moreover, by leveraging an outlier process,
Bayesian models can further generalize out-of-distribution to segment partially occluded objects and to predict their amodal object boundaries. Our algorithm outperforms al-ternative methods that use the same supervision by a large margin, and even outperforms methods where annotated amodal segmentations are used during training, when the amount of occlusion is large. Code is publicly available at https://github.com/YihongSun/Bayesian-Amodal. 1.

Introduction
In our everyday life, we often observe partially occluded objects. Humans can reliably recognize the visible parts of an object and use them as cues to estimate the occluded parts. This perception of the object’s complete structure un-der occlusion is referred to as amodal perception [28].
In computer vision, amodal segmentation is important to study, both for its theoretical values and real-world appli-cations. The main limitation of current approaches is the
Figure 1. Our Bayesian model takes the object bounding box as input and estimates the three segmentation masks on the right: the visible object parts in blue, the invisible object parts in red, and the background context in green. The model is fully probabilistic, and the pixel brightness shows the confidence of the model prediction. requirement of detailed supervision of amodal object masks either through human annotation [13, 24, 30] or by generat-ing artificially occluded images [38]. Moreover, these meth-ods assume that the object class of the occluder is known at training time. This is an important limitation in real-world applications, such as autonomous driving, where potential occluders can be any kind of real-world object.
We formulate amodal segmentation as an out-of-task and out-of-distribution generalization problem, where a
Bayesian generative model is trained from non-occluded objects with bounding boxes and class annotations only, but generalizes to amodal segmentation of partially occluded
Intuitively, our model can be under-objects (Figure 1). stood as a convolutional neural network, in which the fully-connected classification head is replaced with a Bayesian generative model of the neural features. During inference, the latent model parameters (i.e. object class and amodal segmentation) are estimated such that the features of the in-put image are explained by the Bayesian model with max-imum likelihood. The invariance properties of the neural features enable us to avoid explicitly modeling nuisances such as small deformations or illumination changes.
Our work builds on recent approaches of learning gen-erative models of neural network features for image classi-fication [19, 20], and extends these in several ways to en-able amodal segmentation. In particular, we extend the net-work architecture with a generative model of the object’s background context, as well as a prior of the object shape.
Unlike standard Deep Network approaches, this makes the notion of the background context and the object shape ex-plicit. Together, these priors enable our model to be trained from bounding box and class supervision only and general-ize out-of-task to object segmentation. The Bayesian model is combined with an outlier process to make it robust to par-tial occlusion. The outlier process enables us to formulate amodal segmentation as an out-of-distribution task, where the model is trained from non-occluded images, but gener-alizes to images with partially occluded objects. We discuss how the full Bayesian model can be learned using maxi-mum likelihood estimation with an EM-type algorithm. We also demonstrate that a joint end-to-end fine-tuning of the
Bayesian model and the convolutional feature extractor fur-ther improves the performance by a steady margin.
Our experiments on all common datasets for amodal segmentation, KITTI Instance dataset (KINS) [30], COCO
Amodal cls. [12] and Occluded-PASCAL3D+ [33], show that our Bayesian approach outperforms related weakly-supervised work by a large margin and even outperforms fully supervised methods when the amount of occlusion is large. In summary, we make several contributions: 1. We formulate amodal instance segmentation as an out-of-task and out-of-distribution generalization problem with a Bayesian generative model. 2. Our Bayesian model is learned from bounding box and class labels only and outperforms alternative weakly-supervised methods by a large margin and even outperforms supervised methods (where anno-tated amodal segmentations are used during training) when the amount of occlusion is large. 3. To the best of our knowledge, our model is the first for amodal segmentation that generalizes to previously unseen occluders. 2.