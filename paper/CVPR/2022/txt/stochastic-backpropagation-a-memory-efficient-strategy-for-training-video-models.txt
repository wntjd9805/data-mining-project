Abstract
We propose a memory efficient method, named Stochas-tic Backpropagation (SBP), for training deep neural net-works on videos.
It is based on the finding that gradi-ents from incomplete execution for backpropagation can still effectively train the models with minimal accuracy loss, which attributes to the high redundancy of video. SBP keeps all forward paths but randomly and independently removes the backward paths for each network layer in each train-ing step.
It reduces the GPU memory cost by eliminat-ing the need to cache activation values corresponding to the dropped backward paths, whose amount can be con-trolled by an adjustable keep-ratio. Experiments show that
SBP can be applied to a wide range of models for video tasks, leading to up to 80.0% GPU memory saving and 10% training speedup with less than 1% accuracy drop on action recognition and temporal action detection. 1.

Introduction
One of the common challenge in training video under-standing models is the limited availability of GPU memory.
Although video models, such as those for action recogni-tion [3, 42, 45, 49] and temporal action detection [6, 13], are known to benefit from capturing context features over the entire span of a video [49, 50], it is not always feasible to end-to-end train them while taking as many input frames (e.g., typically hundreds) as they need. For example, when training a temporal action detector using ResNet-50 [21] as feature extractor, 128 frames as inputs, and batch size 4, the feature extractor itself costs over 40 GB memory that could exceed the limit of most modern GPUs.
On the other hand, video data is highly redundant, which suggests opportunity for optimization. We observe that in video models, the activation maps and gradients of different frames are similar at the bottom layers but become more and
*The work was done during an Amazon internship.
†Corresponding Author.
Figure 1. The activations and gradients are obtained from different layers (e.g., layer 1, 4, 7, and 12) of Video Swin-T [33]. The activations and gradients between frames are similar at the bottom layers and are quite different at the top layers, which matches our assumption that there is much redundancy between the frames at the bottom layers but less at the top layers. more different at the top layers. An example of Video Swin-T [33] on Kinetics-400 [7] dataset is shown in Fig. 1. Dur-ing model training, the tiny difference in the seemly similar features (i.e., fine-grained information) of the bottom lay-ers is crucial for producing the difference in the top layers, however, they may not make much difference in terms of updating the parameters of these bottom layers. Thus, we hypothesize that complete computation is necessary for the activation maps (forward paths) to extract important seman-tic information but could be unnecessary for the gradients (backward paths).
In this paper, we propose a memory saving technique, named Stochastic Backpropagation (SBP). Different from previous work that jointly removes the forward and back-ward paths [1], SBP randomly drops a proportion of the in-action instance. One common practice [4,8,10,17,30,31,55] is to first generate action proposals, then identify their ac-tion classes and temporal boundaries. On the other hand, the
”bottom-up” fashion [40, 57] first makes frame-level dense predictions and groups them as action instances. Online ac-tion detection [13,16,18,19,39,41,52,54,56], however, rec-ognizes actions as soon as each video frame arrives without accessing any future information. Due to the high cost of
GPU memory, most above offline and online methods can-not be trained in end-to-end manner and thus are built upon the extracted features from raw videos. Transformers re-cently achieve convincing results in video tasks, such as ac-tion recognition [2,3,27,33] and detection [37,44,54]. How-ever, the high computational cost and memory demands also limit most of them to be only trained on short video clips.
A few methods [5, 11] focus on designing Transformers to model long-form inputs, but mainly for text.
Memory Saving Techniques. The trade-off between mem-ory and accuracy has been a standing topic in deep learning research. Gradient checkpoint [9] and accumulation, can save a large amount memory (∼50%) but is likely to slow down the training process. Sparse Network [14] is applied to image recognition models but can only save the memory theoretically. Sideways [35, 36] reduce the memory cost by overwriting activations whenever new ones become avail-able but can only be applied to causal models. Regard-ing video specific methods, a popular paradigm for train-ing temporal action detectors is to build the model upon pre-extracted features for temporal modeling and reason-ing (“Freeze Backbone” in Fig. 2). Two recent methods,
AFSD [29] and DaoTAD [48], are end-to-end trained (Dao-TAD also freezes the first two stages of their backbone), but they need to reduce the spatial resolution (i.e. 96 × 96 in
AFSD and 112 × 112 in DaoTAD). However, freezing the backbone or reducing the input size can decrease the ac-curacy and is not an optimal solution. Since video usually contains high redundancy between adjacent frames, several work also tries to reduce the number of input frames, includ-ing frame dropout [1], early pooling [32] and frame selec-tions [20] (“Frame Selection” in Fig. 2). However, aggres-sively subsampling of input frames will unavoidably lose important fine-scale information in the temporal domain, thus have a negative impact on the final accuracy [50, 54].
In addition, these methods cannot be applied to tasks that require per-frame predictions (e.g., online action detection) or involve high-motion (e.g., lip-reading). The compar-isons between SBP and other memory saving techniques are shown in Fig. 2 3. Stochastic Backpropagation
Figure 2. Comparison of different memory saving techniques.
Different from freezing backbone or frame selection, Stochas-tic Backpropagation randomly removes the backward paths when training the spatial model while keep them for the temporal model. dividual backward paths in backpropagation while keeping all the forward paths. Furthermore, we found that this ran-dom removal of backward paths can happen in a layer-wise manner, where we remove immediately connected paths of certain randomly sampled neurons in each layer. Note that, due to the chained dependency in the backpropagation algo-rithm, this would lead to incompletely computed gradients for all network parameters. However, we empirically found that gradients from this incomplete computation could still be sufficient for effectively updating the network parame-ters, as long as the overall computation graph is preserved.
This makes SBP easy to be implemented without the need to account for the change of dependency across multiple layers. The memory saving of SBP is from the avoidance of caching the corresponding activation maps on removed computation paths. Since the memory can be safely re-used in backpropagation, the backward process uses almost no (one layer at most) additional memory1, which makes the activation caching the major source of memory cost dur-ing training. Thus, the saving amount solely depends on the keep-ratio and can be significant when the keep-ratio is low.
SBP is a general technique that can be applied to a wide range of video tasks and models. Experimental re-sults show that training with SBP uses only 0.2×∼0.5× the
GPU memory and speedup 1.1×∼1.2× the training with less than 1% loss of accuracy on Kinetics-400 [7] and Epic-Kitchen-55 [12] for action recognition and less than 1% loss of mAP on THUMOS’14 [22] for temporal action detection. 2.