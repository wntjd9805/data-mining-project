Abstract
We introduce Ev-TTA, a simple, effective test-time adap-tation algorithm for event-based object recognition. While event cameras are proposed to provide measurements of scenes with fast motions or drastic illumination changes, many existing event-based recognition algorithms suffer from performance deterioration under extreme conditions due to significant domain shifts. Ev-TTA mitigates the severe domain gaps by fine-tuning the pre-trained classi-fiers during the test phase using loss functions inspired by the spatio-temporal characteristics of events. Since the event data is a temporal stream of measurements, our loss function enforces similar predictions for adjacent events to quickly adapt to the changed environment online. Also, we utilize the spatial correlations between two polarities of events to handle noise under extreme illumination, where different polarities of events exhibit distinctive noise distri-butions. Ev-TTA demonstrates a large amount of perfor-mance gain on a wide range of event-based object recogni-tion tasks without extensive additional training. Our formu-lation can be successfully applied regardless of input rep-resentations and further extended into regression tasks. We expect Ev-TTA to provide the key technique to deploy event-based vision algorithms in challenging real-world applica-tions where significant domain shift is inevitable. 1.

Introduction
Event cameras are neuromorphic sensors that produce a sequence of brightness changes with high dynamic range and microsecond-scale temporal resolution. The sensor tar-gets conditions where the quality of measurements degrades for standard frame-based cameras. Conventional cameras under extreme measurement conditions produce the promi-nent artifacts of motion blur or pixel saturation, and the per-formance deteriorates for a subsequent perceptual module.
Being able to acquire visual information in challenging en-vironments, event cameras have the potential to overcome
*Young Min Kim is the corresponding author.
Figure 1. Visualization of events from N-ImageNet [17] recorded in various environmental conditions. Positive, negative events are shown in blue and red, respectively. Events in low lighting (b) exhibit noise bursts, where a large number of noisy events are trig-gered from one polarity. Events in extreme motion (c) have denser events triggered along edges compared to normal conditions (a).
Both changes lead to a significant domain gap, deteriorating the recognition performance. the limitations of frame-based cameras.
Despite the myriad of benefits that event cameras can of-fer, there is a clear gap between data acquisition and recog-nition. While event cameras can acquire meaningful infor-mation even in challenging environments, events obtained from these conditions are typically noisy and lack visual features. Figure 1 shows that there exists a stark visual con-trast between events recorded at normal lighting and regular camera motion with those from very low lighting or extreme camera motion. Event-based object recognition algorithms are directly affected by these changes in input and the per-formance becomes very unstable. Figure 3b also shows the perturbation in the feature embedding space due to the do-main shift. Since it is difficult to manually collect labeled data in a wide variety of external conditions, an adaptation strategy is necessary to fully leverage the potential of event cameras.
We propose Ev-TTA, a test-time adaptation algorithm targeted for event-based object recognition. Given a pre-trained event classifier, Ev-TTA adapts the classifier at test phase to new, unseen environments with large domain shifts. Our method does not require labeled data from the target domain and can operate in an online manner. Never-theless, Ev-TTA shows a large amount of performance gain, with more than 10% accuracy increase across all tested rep-resentations in datasets such as N-ImageNet [17]. While we mainly investigate domain shifts caused by external varia-tions in camera trajectories and scene brightness, Ev-TTA is also capable of dealing with other domain shifts such as
Sim2Real gap.
Ev-TTA is composed of two key components that uti-lize the distinctive characteristics of event data in the space-time domain. First of all, our test-time adaptation strategy enforces the consistency of the predictions for temporally adjacent streams. Our novel loss function jointly minimizes the discrepancy between pairs of adjacent event fragments while selectively minimizing the entropy of the predictions.
Secondly, we propose to remove events that lack spatially neighboring events in the opposite polarity. This is based on the observation that under extreme lighting, severe noise in the event streams is exclusively generated on one polar-ity, as shown in Figure 1.
Since Ev-TTA only intervenes with the input event and output probability distribution, it is versatile to various event representations, datasets, or tasks. In Section 4.1, Ev-TTA shows universal improvements across all event rep-resentations tested for a wide range of external conditions.
As there is no consensus in the optimal event representation yet, the flexibility to handle various event representations makes Ev-TTA further suitable for event data. Our formu-lation is general and is also applicable to other vision-based tasks with minor modifications. We demonstrate that Ev-TTA could be used for tasks other than classification such as steering angle regression, suggesting the large applica-bility of Ev-TTA.
To summarize, our main contributions are (i) a novel test-time adaptation objective based on temporal consis-tency, (ii) a noise removal mechanism for low-light condi-tions utilizing spatial consistency, (iii) comprehensive eval-uation of Ev-TTA in event-based object recognition using a wide range of event representations, and (iv) extension of
Ev-TTA to event-based regression tasks. Our experiments demonstrate that Ev-TTA can successfully adapt various event-based vision algorithms to a wide range of external conditions. 2.