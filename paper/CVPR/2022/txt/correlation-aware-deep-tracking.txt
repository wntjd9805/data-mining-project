Abstract
Robustness and discrimination power are two fundamen-tal requirements in visual object tracking. In most tracking paradigms, we find that the features extracted by the pop-ular Siamese-like networks cannot fully discriminatively model the tracked targets and distractor objects, hinder-ing them from simultaneously meeting these two require-ments. While most methods focus on designing robust cor-relation operations, we propose a novel target-dependent feature network inspired by the self-/cross-attention scheme.
In contrast to the Siamese-like feature extraction, our net-work deeply embeds cross-image feature correlation in mul-tiple layers of the feature network. By extensively matching the features of the two images through multiple layers, it is able to suppress non-target features, resulting in instance-varying feature extraction. The output features of the search image can be directly used for predicting target locations without extra correlation step. Moreover, our model can be flexibly pre-trained on abundant unpaired images, lead-ing to notably faster convergence than the existing methods.
Extensive experiments show our method achieves the state-of-the-art results while running at real-time. Our feature networks also can be applied to existing tracking pipelines seamlessly to raise the tracking performance. 1.

Introduction
Visual object tracking (VOT) is a long-standing topic in computer vision. There are two fundamental yet competing goals in VOT: on one hand, it needs to recognize the target undergoing large appearance variations; on the other hand, it needs to filter out the distractors in the background which may be very similar to the target.
Most appearance-based approaches address this chal-lenge in two perspectives: the first is to learn a more expres-sive feature embedding space by Siamese-like extraction
*Work performed when Fei Xie was an intern of MSRA
Figure 1. Comparison with the state-of-the-arts on GOT-10k [19].
We visualize the AO performance with respect to the model size.
All reported trackers follow the official GOT-10k test protocol.
Our SBT tracker achieves superior results while multiple trackers (with suffix “CA”) can benefit from our correlation-aware features. network [22,58]; the second is to develop a more robust cor-relation operation, such as Siamese cropping [23,58], online filter learning [3, 18] and Transformer-based fusion [5, 50].
Since the modern backbones [17, 34] become the main-stream choice in deep era, most trackers devote to the cor-relation operation, hoping to discriminate targets from dis-tractors given their features. Despite their great success, few of these tracking paradigms notice that the two competing goals may put the feature network into a target-distractor dilemma, bringing much difficulties to the correlation step.
The underlying reasons are three folds: 1) The Siamese en-coding process is unaware of the template and search im-ages, which weakens the instance-level discrimination of learned embeddings. 2) There is no explicit modelling for the backbone to learn the decision boundary that separates the two competing goals, leading to a sub-optimal embed-ding space. 3) Each training video only annotates one sin-gle object while arbitrary objects including distractors can be tracked during inference. This gap is further enlarged by 2). Our key insight is that feature extraction should have dy-namic instance-varying behaviors to generate “appropriate”
Figure 2. (a1) standard Siamese-like feature extraction; (a2) our target-dependent feature extraction; (b1) correlation step, such as Siamese cropping correlation [23], DCF [11] and Transformer-based correlation [5] ; (b2) our pipeline removes separated correlation step; (c) prediction stage; (d1)/(d2) are the TSNE [36] visualizations of search features in (a1)/(a2) when feature networks go deeper. embeddings for VOT to ease the dilemma. In more details, it needs to generate coherent features for the same object in all frames of a video in spite of the variations; on the other hand, it needs to generate contrasting features for the target and distractors with similar appearance.
To this end, we present a novel dynamic feature net-work on top of the attention scheme [37]. As shown in
Fig.2 (a2), our Single Branch Transformer (SBT) network allows the features of the two images to deeply interact with each other at the stage of feature extraction. Intuitively, the cross-attention weights gradually filter out target-irrelevant features layer by layer while the self-attention weights en-rich the feature representations for better matching. Thus, the feature extraction process is target-dependent and asym-metrical for image pair, allowing the network to achieve a win-win scenario: it differentiates the target from simi-lar distractors while preserving the coherent characteristics among dissimilar targets. The effectiveness of features from
SBT is validated in Fig. 2 (d2). The features belonging to the target (green) become more and more separated from the background (pink) and distractors (blue) while the search features from Siamese extraction are totally target-unaware.
The overall framework of SBT is shown in Fig. 3. It has three model stages on top of Extract-or-Correlation (EoC) blocks. The patch embedding produces embeddings for the template and search images. Then the embeddings are fed to the stacked EoC blocks. There are two variants of EoC, i.e. EoC-SA and EoC-CA, which use Self-Attention (SA) and Cross-Attention (CA) as its core operator, respectively.
The EoC-SA block fuses features within the same image while the EoC-CA block mixes features across images. The output features of the search image are directly fed to the prediction heads to obtain a spatial score map and a size embedding map. Our key technical innovation is introduc-ing one single stream for template and search image pair processing that jointly extract or correlate through homoge-neous attention-based blocks. Thus, SBT can be pre-trained on abundant unpaired images such as ImageNet [33], lead-ing to a fast convergence in the fine-tune on tracking.
Extensive experiments are conducted to compare dif-ferent SBT network designs. Based on the insights, we summarize a number of general principals. Our method achieves superior performance and improves Siamese, DCF and Transformer-based trackers as can be seen in Fig. 1.
The main contributions of this work are as follows:
• We present a novel tracking framework which allows the features of the search and template image to be deeply fused for tracking. It further improves existing popular tracking pipelines. To our best, we are the first to propose a specialized target-dependent feature network for VOT.
• We conduct a systematic study on SBT tracking both experimentally and theoretically, and summarize several general principles for following works.
The rest of the paper is organized as follows. We discuss related work in Sec. 2. The SBT framework is presented in Sec. 3. Then, we conduct empirical studies and theo-retical analysis on SBT in Sec. 4 and Sec. 5, respectively.
Finally, we provide extensive experimental results in Sec. 6 and conclude the paper in Sec. 7. 2.