Abstract
Generating expressive talking heads is essential for cre-ating virtual humans. However, existing one- or few-shot methods focus on lip-sync and head motion, ignoring the emotional expressions that make talking faces realistic. In this paper, we propose the Granularly Controlled Audio-Visual Talking Heads (GC-AVT), which controls lip move-ments, head poses, and facial expressions of a talking head in a granular manner. Our insight is to decouple the audio-visual driving sources through prior-based pre-processing designs. Detailedly, we disassemble the driving image into three complementary parts including: 1) a cropped mouth that facilitates lip-sync; 2) a masked head that implicitly learns pose; and 3) the upper face which works corpo-rately and complementarily with a time-shifted mouth to contribute the expression.
Interestingly, the encoded fea-tures from the three sources are integrally balanced through reconstruction training. Extensive experiments show that our method generates expressive faces with not only synced mouth shapes, controllable poses, but precisely animated emotional expressions as well. 1.

Introduction
*Equal contribution.
â€ Corresponding authors.
With the rapid development of automatic video genera-tion technology, the task of audio-driven talking head gen-eration has drawn much attention due to its extensive real-world applications such as creating virtual anchors, digi-tal avatars, and animated movies. In order to achieve con-venient deployment with a generalized model, researchers have proposed to drive only a single or a few frames to talk with audios [8, 10, 46, 48, 50, 51]. While accurate lip sync has been almost realized, the ability to control the facial ex-pression, which is crucial for creating human-like talking heads, has not been fully explored.
A great number of previous methods focus only on the lip-sync accuracy with audios [7, 10, 27, 30, 48]. More recently, researchers propose to generate rhythmic [6, 42] or changeable head poses [50] along with talking heads.
However, their methods cannot change detailed expressions such as eyebrows. On the other hand, methods that gen-erate emotional dynamics [18, 21, 35] are basically person-specific, i.e., one model has to be trained for one specific person. Moreover, their models rely on labeled emotional data, thus can only cover limited expressions.
In real-world scenes, people could speak the same con-tent with fixed stress and intonation but flexible expressions and head motions. Inspired by this observation, we argue that the generation of emotional expressions can be divided from mouth movements and poses, that the three of them could be controlled independently. This is technically chal-lenging for nearly all existing models. 1) For methods pre-dicting intermediate structural representation such as 2D or 3D landmarks [6, 8, 51], the above information is inher-ently entangled. Even mainstream 3D face models, such as 3D Face Morphable Model (3DMM) [1], represent mouth movement and facial expression within the same parame-ter. Besides, the accuracy of intermediate representations will be compromised under extreme cases. 2) For latent feature learning methods [3, 10, 48, 50], the expression in-formation can hardly be individually distilled, and current works [3, 50] do not support disentangled expression and mouth control.
In this work, we propose Granularly Controlled
Audio-Visual Talking Heads (GC-AVT), which drives a portrait head from a higher level of granularity. Avoid using any intermediate representation, our method is pure learning-based without specific emotion labels. The most intriguing property of our model is the independent fa-cial control from three complementary perspectives: speech content, head pose, and emotional expression, which makes our talking head more expressive. As shown in Fig 1, while the head pose and expression information are derived from visual sources, the mouth movement can be decided by ei-ther audio or visual information.
Our insight is to explicitly divide the driving informa-tion into granular parts through delicate pre-processing designs. Different from previous methods that learn non-identity representation in a holistic view [3, 50], we argue that all information can be separately extracted in a com-plementary manner. We analyze the key-factors that affect each desired facial area and adopt different types of mask-ing and augmentation schemes. Three functional inputs are thus formulated. Audio input associates explicitly with the mouth shapes, thus the temporal alignment between speech and cropped mouths is leveraged to account for the speech content information. Then we expect that the emotional ex-pressions could be driven by additional visual sources. In particular, we factorize the emotion of a whole face into an upper-face and a time-shifted mouth. The two of them are seamlessly collaborated together to provide precise expres-sions. Finally, an implicit pose code is devised from the whole face. Three encoders are leveraged for the individ-ual information extraction, and a style-based generator pro-cesses them through reconstruction training. Experiments demonstrate that our method manages to generate an ex-pressive talking head with the precise mouth shape, head pose, and emotional expression control.
The contributions of this work are summarized as fol-lows: (1) We propose the Granularly Controlled Audio-Visual Talking Heads (GC-AVT) System, which generates expressive portrait videos from the granular control of pose, audio, and an expression video. (2) We identify three del-icate pre-processing procedures for handling the three dif-ferent control sources. (3) By integrating audio-visual syn-chronization, our system generates accurate mouth move-ments that can be driven by either audio or video. 2.