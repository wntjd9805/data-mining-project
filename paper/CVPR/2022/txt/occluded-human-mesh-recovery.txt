Abstract
Top-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difﬁcul-ties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Oc-cluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human as-sumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disam-biguate between two severely overlapping human bounding-boxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Speciﬁcally, our pro-posed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHu-man datasets, a signiﬁcant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline. 1.

Introduction
Estimating accurate 3D human meshes from single im-ages has diverse applications in modeling human-scene in-teractions, understanding human behaviour, AR/VR and robotics. While recent approaches [4, 11, 25, 32, 43, 48, 50, 66] perform particularly well in images containing a single person, human mesh recovery for complex real-world scenes with multiple occluded people remains a challenging task.
This can be attributed in part to simplifying assumptions made by existing methods. For instance, most top-down approaches expect a single subject in the input image, which affects robustness under in-the-wild scenarios containing severe person-person occlusion, such as crowding. In this paper, we address human mesh recovery in multi-person scenarios by mitigating the limitations of the single-person assumption of top-down approaches.
Current human mesh recovery methods can be catego-rized into top-down and bottom-up methods. Top-down methods [7, 10, 12, 24, 31, 35–37, 67] reduce the problem to a simpler task of single human mesh recovery by rely-ing on a person detector to detect individual bounding box for each person in the image. Since each bounding box is scaled to the same size, top-down methods are less sensitive
to scale variations among subjects and can achieve pixel accurate mesh alignment [10, 67]. In contrast, bottom-up methods [58, 64, 68] simultaneously predict meshes for all subjects in the input image but are limited to a ﬁxed input resolution due to computational constraints. e.g. ROMP [58], a bottom-up method, recovers a limited number of human meshes from a resized 512 × 512 input whereas SPIN [32], a top-down method, scales each bounding box to 224 × 224, retaining higher input resolution per person(see Fig. 1).
This observation has also been discussed by Cheng et al. [6] albeit in the context of 2D human pose estimation. Thus, top-down methods are currently the best performers on various multi-human benchmarks [18, 21, 23, 46, 55, 61]. Despite the advantages, due to the single-human assumption, when presented with multi-human inputs like crowded scenes, top-down methods are forced to select a single plausible mesh per detection bounding box. Bottom-up methods do not have this limitation and typically perform better under occlusion.
A general method should have both traits – be robust to scale variations and person-person occlusions. To this end, we rethink top-down human mesh recovery by predicting multiple meshes from the input bounding box. We condition the top-down model on image spatial-context in the form of body-center maps, refer Fig. 2. Our choice of using center maps for representing humans under occlusion is inspired by crowd-counting literature [44, 56, 62] and recent works in detection [8, 71, 72]. Our method, OCHMR, predicts the output mesh from the input image for the person of interest in the subject-speciﬁc local center-map. Similar to bottom-up methods, we also use information from the global center-map for understanding overall scene context, which is helpful for occlusion reasoning. With this strategy, we obtain the best of both worlds – OCHMR achieves pixel accurate mesh alignment similar to top-down methods and is robust to occlusions similar to bottom-up methods (See Fig. 1).
To design a top-down architecture capable of contextual conditioning using centermaps, we adopt the mechanism of feature normalization [9, 16, 51] and propose a novel Con-text Normalization (CoNorm) block to process the global and local centermaps. The CoNorm blocks are used to inject contextual information at multiple depths in the deep feature backbone network. The spatial context is necessary for 3D occlusion reasoning, and the CoNorm block allows for adap-tive normalization of intermediate features of the network without changing the backbone. We show that unlike early fusion (e.g. channel-wise concatenation) of centermaps with input image I, CoNorm can effectively utilize the contextual information from the image. OCHMR is general and can be extended to other top-down human mesh recovery methods with minimal effort.
While the use of spatial-context allows our method to rea-son about occlusions, our method must also reason about the intersection of a set of 3D human meshes. To address this,
Figure 2. OCHMR leverages image spatial-context for occlusion reasoning by predicting body centermaps. The deep network pre-dicts the mesh output using input image, the subject-speciﬁc local centermap and the scene-speciﬁc global centermap. following CRMH [19], we use an interpenetration loss to penalize intersections among reconstructed meshes and a dif-ferentiable depth-ordering loss for depth-consistent human mesh recovery. Furthermore, we make use of training-time data augmentation like scaling and cropping, which affords
OCHMR the ability to predict meshes from a variety of body-center locations. We show that our proposed method is also robust to errors in estimated body-centers under severe-occlusion. Our empirical results show that OCHMR does not require precise centermaps that correspond to actual body-centers but can also work with any point in its vicinity.
Overall, OCHMR outperforms both top-down and bottom-up methods on various datasets. For challenging datasets such as 3DPW-PC [70], CrowdPose [34] and OCHu-man [69], containing a larger proportion of cluttered scenes (with multiple overlapping people), OCHMR sets a new state-of-the-art for 3D reconstruction error (PMPJPE) and 2D keypoint average precision (AP) achieving 77.1 PM-PJPE, 21.4 AP and 24.8 AP respectively on the val sets outperforming bottom-up methods (Tab. 1). Further, when evaluating using ground-truth bounding boxes, OCHMR dramatically improves SPIN [32] by 20.8 AP and 6.4 AP on the OCHuman and CrowdPose dataset respectively. In summary:
• OCHMR advances top-down human mesh recovery methods by addressing limitations caused by the single-human assumption. Our method leverages spatial-context in the form of centermaps to predict multiple mesh outputs from an input image.
• We introduce novel Context Normalization (CoNorm) blocks to inject global and local centermap information at multiple depths of the top-down network.
• Our approach achieves state-of-the-art results on the oc-cluded 3DPW-PC, CrowdPose and OCHuman datasets.
Empirically, we also show that OCHMR is resilient to noisy body center estimates and demonstrates robust 3D reasoning using multi-person losses. 2.