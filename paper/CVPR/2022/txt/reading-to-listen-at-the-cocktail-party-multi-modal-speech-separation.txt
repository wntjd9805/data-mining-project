Abstract
The goal of this paper is speech separation and en-hancement in multi-speaker and noisy environments using a combination of different modalities. Previous works have shown good performance when conditioning on temporal or static visual evidence such as synchronised lip movements or face identity. In this paper, we present a unified frame-work for multi-modal speech separation and enhancement based on synchronous or asynchronous cues. To that end we make the following contributions: (i) we design a mod-ern Transformer-based architecture tailored to fuse differ-ent modalities to solve the speech separation task in the raw waveform domain; (ii) we propose conditioning on the tex-tual content of a sentence alone or in combination with vi-sual information; (iii) we demonstrate the robustness of our model to audio-visual synchronisation offsets; and, (iv) we obtain state-of-the-art performance on the well-established benchmark datasets LRS2 and LRS3. 1.

Introduction
Humans have the remarkable ability to focus on conver-sations even in a room full of talking people, a phenomenon known as the “cocktail party effect” [29]. Our brains carry out this feat by concentrating their attention to a specific speaker while filtering out the rest of the stimuli originat-ing from interfering voices and other environmental noises.
Although this ability manifests to a limited extent through hearing alone, it is greatly enhanced when simultaneous in-formation from other modalities is available. For exam-ple watching a speaker’s lips can significantly help disam-biguate speech in noise [41], while understanding the nat-ural language context of a sentence enables the listener to anticipate the potential next words of the speaker.
In recent years, progress in audio-visual learning has made it possible for machines to also achieve this ability and very effectively isolate individual voices out of multi-Figure 1. We propose VoiceFormer, a framework for multi-modal speech separation and enhancement, that isolates speech accord-ing to either the text content of the target speaker’s utterance, their lip movements or both. Our framework allows conditioning on cues from multiple modalities, without requiring them to be tem-porally synchronised or have a common temporal rate. This gives it multiple advantages, such as robustness to temporal misalign-ments between the inputs. speaker mixtures of speech or noisy audio [1, 14, 31]. Solv-ing this problem enables a great range of practical applica-tions, such as improving subtitle generation in videos with noisy audio, developing smart audio-visual hearing aids to enhance speech conditioned on visual input, or facilitating teleconferencing in noisy settings such as airports or cars.
Previous works have principally taken two approaches: either using synchronous cues, most commonly the lip movements of the target speaker [1, 14]; or using static (fixed embedding) cues such as the voice [49] or face char-acteristics [11, 21]. The former has the advantage of using dynamic evidence that is very strongly correlated to the de-sired speech output. However, relying on lip movements has several disadvantages. First they may be momentarily disrupted – e.g. from visual occlusions – therefore strong reliance will make the model sensitive to this form of visual noise; and second, they require synchronisation between the audio and visual streams. On the other hand, static cues arising from biometric characteristics of the speaker are more robust to temporary disruptions, however they are not dynamically correlated to the speech (so are a weaker signal) and may be common among different people. For example it may be increasingly harder to separate speech
among individuals with similar voice timbre or appearance.
Recent works have attempted to deal with the inadequacy of conditioning on a single source by either jointly condition-ing on more than one modality using naive fusion of static cues with the lip movement [3] or by learning the separation task jointly with a cross-modal prior [21].
However to-date there is no unified framework for: (i) conditioning on asynchronous information (such as a delay between the audio and visual streams); or (ii) for seamlessly conditioning (and fusing) on multiple sources of informa-tion or on different types of modalities; or (iii) for using a large temporal context so that predictions can be made us-ing a language model.
Our first contribution is to enable conditioning on asyn-chronous visual (lip) streams. Most previous work relies on costly pre-processing steps to synchronise the audio and video streams, and their performance deteriorates in real world situations where out of sync data is a regular occur-rence due to transmission delays, jitter or technical issues.
We show that in our work there are no detrimental effects with timing delays of 5 frames (200 ms) or more. Further-more, the audio and visual streams do not even have to have the same temporal sampling rate.
Our second contribution is to enable enhancement by conditioning on textual input. This new functionality al-lows speech to be enhanced without requiring biometric in-formation or even a visual stream. It is applicable where the textual content of the speech is known in advance, e.g. from a prepared speech or lyrics of a song, or where Automatic
Speech Recognition (ASR) or lip reading [5,10] can be used to transcribe what is said, even imprecisely, and then subse-quently used to isolate the speaker from background noise.
Textual conditioning is asynchronous, as only the order of words (or more precisely the phonemes) is required, but not their precise temporal alignment.
Both of these contributions are facilitated by a new
Transformer-based speech separation and enhancement net-work, where we use the positional encoding of the Trans-former to record the timestamp (of the audio and visual samples) or the ordering (of the words in the text) of the conditioning signal. The network operates directly on the waveform level, without requiring spectrograms as an in-termediate step of the audio processing.
It uses a U-Net [12, 38] architecture to encode noisy audio and then decode it into clean speech, with the Transformer as the network’s bottleneck, where the conditioning information can be visual and/or text. The Transformer also enables modelling a longer temporal context (e.g. compared to an
LSTM) allowing the network to explicitly model structure in natural language. By having the ability to anticipate what follows a certain sequence of words the model can then bet-ter approximate the target speech output.
In summary, we make the following contributions: First we design a modern multi-modal speech enhancement ar-chitecture, VoiceFormer, that uses a Transformer-based bot-tleneck to fuse heterogeneous modality streams, mean-ing that it can simultaneously condition on multiple non-aligned modalities. Second, we introduce text-conditioned speech enhancement as a novel multi-modal task and show that our proposed architecture is well designed to handle it. Third, we demonstrate that our trained models are ro-bust to audio-visual synchronisation offsets. Fourth, we ex-hibit state-of-the art performance, surpassing other audio-only and audio-visual baselines in the tasks of speaker sep-aration and speech enhancement. 2.