Abstract
In this paper, we propose a simple attention mechanism, we call Box-Attention. It enables spatial interaction between grid features, as sampled from boxes of interest, and im-proves the learning capability of transformers for several vision tasks. Speciﬁcally, we present BoxeR, short for Box
Transformer, which attends to a set of boxes by predicting their transformation from a reference window on an input feature map. The BoxeR computes attention weights on these boxes by considering its grid structure. Notably, BoxeR-2D naturally reasons about box information within its attention module, making it suitable for end-to-end instance detection and segmentation tasks. By learning invariance to rotation in the box-attention module, BoxeR-3D is capable of gen-erating discriminative information from a bird’s-eye view plane for 3D end-to-end object detection. Our experiments demonstrate that the proposed BoxeR-2D achieves state-of-the-art results on COCO detection and instance segmenta-tion. Besides, BoxeR-3D improves over the end-to-end 3D object detection baseline and already obtains a compelling performance for the vehicle category of Waymo Open, with-out any class-speciﬁc optimization. Code is available at https://github.com/kienduynguyen/BoxeR. 1.

Introduction
For object detection, instance segmentation, image classi-ﬁcation and many other current computer vision challenges, it may seem a transformer with multi-head self-attention is all one needs [40]. After its success in natural language processing, learning long range feature dependencies has proven an effective tactic in computer vision too, e.g., [1, 7].
Surprisingly, existing transformers for computer vision do not explicitly consider the inherent regularities of the vision modality. Importantly, the image features are vectorized in exactly the same way as language tokens, resulting in the loss of local connectivity among pixels. Once fed with sufﬁ-cient data, a traditional transformer may be powerful enough to compensate for this loss of spatial structure, but in this paper we rather prefer to equip the transformer with spatial
Figure 1. BoxeR with box-attention for object detection and in-stance segmentation. BoxeR-2D perceives an image and generates object bounding boxes and pixel masks. Extended from BoxeR-2D,
BoxeR-3D predicts 3D bounding boxes from point cloud input. image-awareness by design. Recent evidence [5, 39, 47] re-veals that an inductive bias is of crucial importance in both natural language processing and computer vision, and the leading works on image recognition [24] and object detec-tion [47] all utilize “spatial information”. Furthermore, a strong and effective inductive bias enables us to converge faster and generalize better [39].
A solution is to enrich image features with positional encoding, which explicitly encodes the position informa-tion at the feature level. This is already common practice when applying multi-head attention to vision tasks. Both
Carion et al. [1] and Zhu et al. [47] convert absolute 2D positions, while Ramachandran et al. [29] encode relative 2D positions into vectors and sum them up to image fea-tures in the attention computation. However, this approach only acts as a data augmentation to image features. It re-quires the network to infer the spatial information implicitly inside its weight, causing a slow convergence rate during training due to the lack of spatial-awareness in the network architecture. It is well known that an inductive bias in the network architecture delivers a strong ability to learn, which has been proven by well-known architectures such as the convolutional neural network [19] and the long short-term memory [13]. In particular, we postulate a better spatial inductive bias in the transformer’s attention module leads to a better learned representation of image features.
Motivated by this observation, the ﬁrst contribution of this paper is a Box-Attention mechanism for end-to-end vision
representation learning using transformers that we present in
Section 3. Instead of using image features within a region of interest, it treats a set of learnable embeddings representing relative positions in the grid structure as the key vectors in the attention computation. In our second contribution, in Section 4, these computations are encapsulated into a composite network that we call BoxeR-2D, short for Box transformeR, which enables a better prediction in end-to-end object detection and instance segmentation tasks. In
Section 5, the BoxeR-2D and box-attention are then extended into BoxeR-3D to tackle end-to-end 3D object detection without the requirements for 3D-IoU computation, anchors, and a heatmap of object centers. This extension to 3D object detection serves as our third contribution, see Fig. 1.
In Section 6, we show the effectiveness of our contribu-tions by several experimental results on the COCO dataset
[21], achieving leading results in end-to-end object detection.
The proposed method introduces a simple solution for end-to-end instance segmentation that outperforms many well-established and highly-optimized architectures with fewer number of parameters on the challenging COCO instance segmentation dataset. By utilizing only data-independent prior information, our method presents a compelling solu-tion for end-to-end 3D object detection on the Waymo Open dataset [35]. 2.