Abstract
Quantization has been applied to multiple domains in
Deep Neural Networks (DNNs). We propose Depthwise
Quantization (DQ) where quantization is applied to a de-composed sub-tensor along the feature axis of weak statis-tical dependence. The feature decomposition leads to an exponential increase in representation capacity with a linear increase in memory and parameter cost. In addition, DQ can be directly applied to existing encoder-decoder frame-works without modification of the DNN architecture. We use DQ in the context of Hierarchical Auto-Encoders and train end-to-end on an image feature representation. We provide an analysis of the cross-correlation between spatial and channel features and propose a decomposition of the image feature representation along the channel axis. The improved performance of the depthwise operator is due to the increased representation capacity from implicit feature decoupling. We evaluate DQ on the likelihood estimation task, where it outperforms the previous state-of-the-art on
CIFAR-10, ImageNet-32 and ImageNet-64. We progressively train with increasing image size a single hierarchical model that uses 69% fewer parameters and has faster convergence than the previous work. 1.

Introduction
Quantization is an effective lossy compression process that maps a continuous signal to a set of discrete values, also called codes. Quantization is extended to vector feature spaces with learning paradigms such as Vector Quantization (VQ) and with a training objective identical to k-means.
Product Quantization (PQ) decomposes the feature vector and assumes a weak statistical dependence between feature sub-vectors. Additive Quantization (AQ) decomposes the feature vector into a sum of quantized vectors as opposed to the concatenated output in PQ.
Quantization is used in conjunction with Deep Neural Net-works (DNNs) for tasks such as classification [46], incremen-tal learning [46], zero-shot learning [28], generation [33],
Figure 1. The original images (left) are reconstructed by DQ (mid-dle) and VQ (right) with identical models and training setup. The perceptual quality of DQ outperform VQ. compression [29] and data retrieval [4]. The discrete quan-tized feature representations can be used post-hoc [11,33,38] or as a learning objective (i.e. classification) [28, 46]. Our work is motivated by the increasing number of quantization applications to high dimensional feature tensors. We view the quantizer as a density estimator and evaluate it on the task of likelihood estimation for the visual domain.
Likelihood estimation models seek to minimize the diver-gence between the data distribution and the model prior. Ex-plicit likelihood estimation models, including Vector Quanti-Figure 2. DQ (left) apply C1 on the first slice of the sub-tensor and for all sub-vectors and concatenate the quantized vectors. VQ [33] (middle) and PQ (right) quantize the same vector with different codebook and combine the two sub-vectors by addition or concatenation. zation (VQ), Variational Auto-encoders (VAEs), and Auto-Regressive (AR) models directly minimize a divergence. In this work, we focus on explicit likelihood estimation.
AR models do well in likelihood estimation and are ap-plied in multiple domains such as language, vision, and audio. AR models have a recursive dependency on the input during training and inference. Therefore, AR models are computationally inefficient for domains with long sequences, such as pixels of an image. Even with caching [34] during sampling, AR models are still less efficient than VAEs.
The priors of VAEs provide a compressed feature repre-sentation that can be used as a surrogate training objective for the downstream task. In contrast to the discrete prior, a continuous prior can lead to posterior collapse. The repre-sentation is ignored by the downstream task model because it is either too noisy or uninformative. This effect is amplified when the data is discrete, as in the language domain [14].
To that end, we propose the Depthwise Quantization (DQ) method that quantizes each decomposed feature sub-tensor with a different quantizer. We use rate-distortion theory to interpret a quantizer as an encoding function with limited ca-pacity. We provide a theoretical upper bound on the capacity in relation to the quantization cost when DQ is applied on a decoupled feature tensor, as opposed to a coupled feature ten-sor. We evaluate the performance of DQ on the feature space of ImageNet for an image classification backbone. Lastly, we apply DQ to a hierarchical Auto-Encoder with DQ as a bottleneck for different hierarchies and train it end-to-end.
DQ outperforms explicit models in likelihood estimation. In detail:
• We propose Depthwise Quantization (DQ) and decom-pose a feature tensor along the axis of weak statistical dependence.
• We provide a theoretical analysis on the improved quan-tization performance and experimentally corroborate our theoretical results.
• We introduce an improved hierarchical AutoEncoder model Depth-Quantized Auto-Encoder where DQ is applied to the feature representation at different hierar-chies.
• We extend the parametric Mutual Information (MI) quantization estimators for DNNs when the prior is learned. We experimentally verify that the learned prior is implicitly decoupled.
Our approach can be applied to previous works that use quantization. We demonstrate with our experiments that DQ
performs significantly better when the assumption on cross-correlation is strong in both post-hoc analysis and end-to-end training settings. When trained end-to-end, DQ reduces the cross-correlation among the decomposed feature tensors (“implicitly decouple”) and improves reconstruction loss and likelihood estimation. Our code is publicly available1. 2.