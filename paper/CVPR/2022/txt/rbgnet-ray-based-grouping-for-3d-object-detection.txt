Abstract
As a fundamental problem in computer vision, 3D ob-ject detection is experiencing rapid growth. To extract the point-wise features from the irregularly and sparsely dis-tributed points, previous methods usually take a feature grouping module to aggregate the point features to an ob-ject candidate. However, these methods have not yet lever-aged the surface geometry of foreground objects to enhance grouping and 3D box generation.
In this paper, we pro-pose the RBGNet framework, a voting-based 3D detector for accurate 3D object detection from point clouds. In or-der to learn better representations of object shape to en-hance cluster features for predicting 3D boxes, we propose a ray-based feature grouping module, which aggregates the point-wise features on object surfaces using a group of de-termined rays uniformly emitted from cluster centers. Con-sidering the fact that foreground points are more meaning-ful for box estimation, we design a novel foreground biased sampling strategy in downsample process to sample more points on object surfaces and further boost the detection performance. Our model achieves state-of-the-art 3D de-tection performance on ScanNet V2 and SUN RGB-D with remarkable performance gains. Code will be available at https://github.com/Haiyang-W/RBGNet. 1.

Introduction 3D object detection is becoming an active research topic in computer vision, which aims to estimate oriented 3D bounding boxes and semantic labels of objects in 3D scenes.
As a fundamental technique for 3D scene understanding, it plays a critical role in many applications, such as au-tonomous driving [3, 38], augmented reality [2, 4] and do-mestic robots [54, 39]. Unlike the scenarios in the well-*Corresponding author: Shaoshuai Shi.
Figure 1: 3D object detection in point clouds with a ray-based feature grouping module. Given the point clouds of a 3D scene, our model aggregates the point-wise features on object surface by a group of ordered rays to boost the performance of 3D object detection. studied 2D image problems, 3D scenes are generally rep-resented by point clouds, a set of unordered, sparse and ir-regular points captured by depth sensors (e.g., RGB-D cam-eras, LiDAR sensors), which makes it significantly different from traditional regular input data like images and videos.
Previous 3D detection approaches can be coarsely clas-sified into two lines in terms of point representations, i.e., the grid-based methods and the point-based methods. The grid-based methods generally convert the irregular points to regular data structure such as 3D voxels [37, 35, 44, 53, 32, 48, 9] or 2D bird’s eye view maps [6, 16, 19, 45, 46].
Thanks to the great success of PointNet series [30, 31], the point-based methods [29, 34, 28, 49, 20, 7] directly ex-tract the point-wise features from the irregular and points.
These point-wise features are generally enhanced by vari-ous feature grouping modules for predicting the 3D bound-ing boxes. However, these feature grouping strategies have not well explored the fine-grained surface geometry to help improve the performance of 3D box generation.
We argue that feature grouping module plays an impor-tant role in point-based 3D detectors, and how to better incorporate the foreground object geometry features to en-FgSamp mAP@0.25 mAP@0.50 nificantly improves 3D detection performance.
GT-Features (explicit GT-center)
GT-Features (implicit GT-center)
✓
✓ 62.90 39.91 76.21 (+13.31) 53.91 (+14.00) 71.69 (+8.79) 50.71 (+10.80) 71.27 (+8.39) 50.68 (+10.77)
✓
Table 1: Results of VoteNet [28] variants on ScannetV2 [8]. GT-Features: Aggregate the features of ground-truth surface points for the 3D box generation of this object, where the “explicit GT-center” / “implicit GT-center” indicate that the above features are grouped to the ground-truth center / predicted vote centers, respec-tively. FgSamp: FPS is only conducted on foreground points. hance the quality of point-wise features is the key to predict better 3D bounding boxes. As shown in Table 1, for the pop-ular VoteNet [28] point-wise 3D detector, by simply group-ing the features of accurate object surface points to the fea-tures of their correct vote centers, the performance can be improved dramatically with a gain of 13.31 on mAP@0.25 for explicit usage of ground truth labels (2nd row of Ta-ble 1), and a gain of 8.79 for implicit usage of ground truth labels (3rd row of Table 1). Here the “explicit usage” indi-cates that the ground truth labels are not only utilized for grouping the object surface points but also for replacing the vote centers with ground truth centers, while the “im-plicate usage” means the ground truth labels are only used for grouping the object surface points. These facts inspire us to explore on designing a better feature representation for the surface geometry of foreground objects, to help the prediction of 3D bounding boxes.
Hence, we present a new 3D detection framework,
RBGNet, which is a one-stage 3D detector for 3D object detection from raw point clouds. Our RBGNet is built on top of VoteNet [28], and we propose two novel strategies to boost the performance of 3D object detection by implicitly learning from foreground object features.
Firstly, we propose the ray-based feature grouping that could learn better feature representation of the surface ge-ometry of foreground objects. The learned features are uti-lized to augment the cluster features for 3D boxes estima-tion. Specifically, we formulate a ray-based mechanism to capture the object surface points, where a number of rays are uniformly emitted from the cluster center with the de-termined angles (see Fig. 1). The far bounds of the rays are based on our predicted object scale of this cluster. Then a number of anchor points is densely sampled on each ray, where the aggregated local features of each anchor point are utilized to predict whether they are on the object sur-face to learn the geometry shape. Moreover, a coarse-to-fine strategy is proposed to generate different number of an-chor points based on the sparsity of different regions. The learned features from all the anchor points will be finally ag-gregated to boost the features of cluster centers for predict-ing 3D bounding boxes. The experiments (Table 4) show that our ray-based feature grouping strategy can effectively encode the surface geometry of foreground objects and sig-Secondly, we propose the foreground biased sampling strategy to allocate more foreground object points for pre-dicting 3D boxes. We observe that the points on object sur-faces are more useful than those on the background for 3D box estimation (similar observations are also mentioned by
[47, 32]), and 4th row of Table 1 shows that by conducting farthest point sampling only on the ground truth foreground points, the performance of VoteNet [28] could be boosted from 62.90 to 71.27 in terms of mAP@0.25. Therefore, we propose a simple but effective strategy to sample points bi-ased towards object surface while still keeping the coverage rate of the whole scene. Specifically, we append a segmen-tation head to the point-wise features before each farthest point sampling, where the head will predict the confidence of each point being a foreground point. According to the ranking of their foreground scores, the input points are sep-arated into foreground set and background set. And these two sets will apply farthest point sampling separately, where we sample most target points (i.e., 87.5% in our case) from the foreground set and a small number (i.e., 12.5%) from the background set to keep the coverage rate of the whole scene.
Our foreground biased sampling can produce a more infor-mative sampling of points over foreground objects surface for feature extraction, and the performance gains (Table 4) demonstrate its effectiveness.
In a nutshell, our contributions are three-fold: 1) We pro-pose a novel ray-based feature grouping module to encode object surface points with determined rays, which can learn better surface geometry features of objects to boost the per-formance of point-based 3D object detectors. 2) We present foreground biased sampling module to focus feature learn-ing of the network on foreground surface points while also keeping the coverage rate for the whole scene, which can in-corporate more object points to benefit point-based 3D box generation. 3) Equipped with the above two modules, our proposed RBGNet framework outperforms state-of-the-art methods with remarkable margins both on ScanNetV2 [8] and SUN RGB-D [36]. 2.