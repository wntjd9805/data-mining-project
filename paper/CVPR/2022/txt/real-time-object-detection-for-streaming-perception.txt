Abstract
Autonomous driving requires the model to perceive the environment and (re)act within a low latency for safety.
While past works ignore the inevitable changes in the envi-ronment after processing, streaming perception is proposed to jointly evaluate the latency and accuracy into a single metric for video online perception. In this paper, instead of searching trade-offs between accuracy and speed like pre-vious works, we point out that endowing real-time models with the ability to predict the future is the key to dealing with this problem. We build a simple and effective frame-It equips a novel Dual-work for streaming perception.
Flow Perception module (DFP), which includes dynamic and static flows to capture the moving trend and basic detec-tion feature for streaming prediction. Further, we introduce a Trend-Aware Loss (TAL) combined with a trend factor to generate adaptive weights for objects with different mov-ing speeds. Our simple method achieves competitive per-formance on Argoverse-HD dataset and improves the AP by 4.9% compared to the strong baseline, validating its ef-fectiveness. Our code will be made available at https:
//github.com/yancie-yjr/StreamYOLO. 1.

Introduction
One critical factor for autonomous safe driving is to per-ceive its environment and (re)act within a low latency. Re-cently, several real-time detectors [3, 13, 18, 31, 33, 41–43] achieve competitive performance under the low latency re-striction. But they are still explored in an offline setting [26].
In a real-world vision-for-online scenario, no matter how fast the model becomes, the surrounding environment has changed once the model finishes processing the latest frame.
As shown in Fig. 1(a), the inconsistency between perceptive results and the changed state may trigger unsafe decisions for autonomous driving. Thus for online perception, detec-tors are imposed to have the ability of future forecasting.
*Corresponding author (a) baseline (b) ours
Figure 1. Illustration of visualization results of base detector and our method. The green boxes are ground truth, while the red ones are predictions. The red arrows mark the shifts of the prediction boxes caused by the processing time delay while our approach al-leviates this issue.
To tackle this issue, [26] firstly proposes a new metric named streaming accuracy, which integrates latency and ac-curacy into a single metric for real-time online perception.
It jointly evaluates the output of the entire perception stack at every time instant, forcing the perception to forecast the state where the model finishes processing. With this met-ric, [26] shows a significant performance drop of several strong detectors [6, 21, 28] from offline setting to streaming perception. Further, [26] proposes a meta-detector named
Streamer that can incorporate any detector with decision-theoretic scheduling, asynchronous tracking, and future forecasting to recover much of the performance drop. Fol-lowing this work, Adaptive streamer [16] adopts numerous approximate executions based on deep reinforcement learn-ing to learn a better trade-off online. These works focus on searching for a better trade-off policy between speed and ac-curacy for some existing detectors, while a novel streaming perception model design is not well studied.
One more thing ignored by the above works is the ex-isting real-time object detectors [13, 18]. By strong data augmentation and delicate architecture design, they achieve competitive performance and can run faster than 30 FPS.
With these ”fast enough” detectors, there is no space for ac-curacy and latency trade-off on streaming perception as the current frame results from the detector are always matched and evaluated by the next frame. These real-time detectors can narrow the performance gap between streaming per-ception and offline settings. In fact, both the 1st [59] and 2nd [20] place solution of Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) adopt real-time models YOLOX [13] and YOLOv5 [18] as their base detectors. Standing on the shoulder of the real-time models, we find that now the performance gap all comes from the fixed inconsistency between the current process-ing frame and the next matched frame. Thus the key solu-tion for streaming perception is to predict the results of the next frame at the current state.
Unlike the heuristic methods such as Kalman filter [25] adopted in [26], in this paper, we directly endow the real-time detector with the ability to predict the future of the next frame. Specifically, we construct triplets of the last, current, and next frame for training, where the model gets the last and current frames as input and learns to predict the detec-tion results of the next frame. We propose two crucial de-signs to improve the training efficiency: i) For model archi-tecture, we conduct a Dual-Flow Perception (DFP) module to fuse the feature map from the last and current frames. It consists of a dynamic flow and a static flow. Dynamic flow pays attention to the moving trend of objects for forecasting while static flow provides basic information and features of detection through a residual connection. ii) For the training strategy, we introduce a Trend Aware Loss (TAL) to dynam-ically assign different weights for localizing and forecasting each object, as we find that objects within one frame may have different moving speeds.
We conduct comprehensive experiments on Argoverse-HD [5,26] dataset, showing significant improvements in the
In summary, the contributions of stream perception task. this work are as three-fold as follows:
• With the strong performance of real-time detectors, we find the key solution for streaming perception is to pre-dict the results of the next frame. This simplified task is easy to be structured and learned by a model-based algorithm.
• We build a simple and effective streaming detector that learns to forecast the next frame. We propose two adaptation modules, i.e., Dual-Flow Perception (DFP) and Trend Aware Loss (TAL), to perceive the moving trend and predict the future.
• We achieve competitive performance on Argoverse-HD [5, 26] dataset without bells and whistles. Our method improves the mAP by +4.9% compared to the strong baseline of the real-time detector and shows ro-bust forecasting under the different moving speeds of the driving vehicle. 2.