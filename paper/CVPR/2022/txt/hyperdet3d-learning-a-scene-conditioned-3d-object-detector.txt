Abstract
A bathtub in a library, a sink in an office, a bed in a laundry room – the counter-intuition suggests that scene provides important prior knowledge for 3D object detec-tion, which instructs to eliminate the ambiguous detection of similar objects. In this paper, we propose HyperDet3D to explore scene-conditioned prior knowledge for 3D ob-ject detection. Existing methods strive for better represen-tation of local elements and their relations without scene-conditioned knowledge, which may cause ambiguity merely based on the understanding of individual points and object candidates.
Instead, HyperDet3D simultaneously learns scene-agnostic embeddings and scene-specific knowledge through scene-conditioned hypernetworks. More specifi-cally, our HyperDet3D not only explores the sharable ab-†Corresponding author stracts from various 3D scenes, but also adapts the detector to the given scene at test time. We propose a discriminative
Multi-head Scene-specific Attention (MSA) module to dy-namically control the layer parameters of the detector con-ditioned on the fusion of scene-conditioned knowledge. Our
HyperDet3D achieves state-of-the-art results on the 3D ob-ject detection benchmark of the ScanNet and SUN RGB-D datasets. Moreover, through cross-dataset evaluation, we show the acquired scene-conditioned prior knowledge still takes effect when facing 3D scenes with domain gap. 1.

Introduction 3D object detection has gained much attention in recent years, which is fundamental for applications such as au-tonomous driving, robotic navigation and augmented real-ity. Early works adopt sliding window [42] or 2D prior [16]
to locate objects from RGB-D data. However, the order-less and sparse characteristic of point cloud makes it hard to directly employ the recent advances in 2D detection. To tackle this, view-based methods [4] project the points into multiple 2D planes and apply standard 2D detectors. Volu-metric convolution-based methods [18, 23] split points into regular grids, which is feasible for 3D convolutions.
Different from the aforementioned view-based and vol-umetric convolution-based methods, PointNet++ [35] fo-cuses on the local geometries while elegantly consuming raw point cloud, and thus widely used as backbone net-work in 3D detectors. Built on the PointNet++ network,
VoteNet [32] yields outstanding results by regressing off-set votes to object centers from seed coordinates and cor-responding local features. Following works incorporate probabilistic voting [8], multi-level contextual learning [9, 48, 49] and self-attention based transformer [22, 24, 28] to further enhance the local representations. These methods underline the importance of exploiting object-based and relation-based representation of local elements, such as in-dividual points, detection candidates and irregular local ge-ometries in a given point scan.
However, the attributes of similar objects are ambiguous if we only look at themselves or relations.
In this paper, we discover that the scene-level information provides prior knowledge to eliminate such ambiguity. As shown in Fig-ure 1, with the absence of scene-conditioned knowledge, inferring the object-level features or their relations is inad-equate for detecting the object candidate, which may lead to counter-intuitive detection results in the aspect of scene-level understanding. To our best knowledge, the acquisition of such scene-level information among various scenes by 3D detectors is yet to be fully studied.
To this end, we propose HyperDet3D for 3D object de-tection on point cloud which leverages hypernetwork-based structure. Compared with the existing methods that fo-cus on point-wise or object-level representation, our Hy-perDet3D learns the scene-conditioned information as prior and incorporates such scene-level knowledge into network parameters, so that our 3D object detector is dynamically adjusted in accordance with different input scenes. Specif-ically, the scene-conditioned knowledge can be factorized into two levels: scene-agnostic and scene-specific informa-tion. For the scene-agnostic knowledge, we maintain a learnable embedding which is consumed by a hypernetwork and iteratively updated along with the parsing of various in-put scenes during training. Such sharable scene-agnostic knowledge generally abstracts the characteristics of training scenes and can be utilized by the detector at test time. More-over, since conventional detectors maintain the same set of parameters when recognizing objects in different scenes, we propose to incorporate the scene-specific information which adapts the detector to the given scene at test time.
To this end, we attentionally measure how well the cur-rent scene matches a general representation (or how much they differ) by using the specific input data as query. We simultaneously learn the two levels of scene-conditioned knowledge by proposing a Multi-head Scene-Conditioned
Attention (MSA) module. The learned prior knowledge is aggregated with object candidate features by late fusion, therefore providing more powerful guidance to detect the objects. Extensive experiments on the widely used Scan-Net [7] and SUN RGB-D [41] datasets demonstrate that our method surpasses state-of-the-art methods by an obvi-ous margin. Moreover, through cross-dataset evaluation, we show the scene-conditioned prior knowledge acquired by our HyperDet3D still takes effect when faced with domain gap. 2.