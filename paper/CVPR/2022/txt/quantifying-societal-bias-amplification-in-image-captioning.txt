Abstract
We study societal bias amplification in image caption-ing. Image captioning models have been shown to perpetu-ate gender and racial biases, however, metrics to measure, quantify, and evaluate the societal bias in captions are not yet standardized. We provide a comprehensive study on the strengths and limitations of each metric, and propose LIC, a metric to study captioning bias amplification. We argue that, for image captioning, it is not enough to focus on the correct prediction of the protected attribute, and the whole context should be taken into account. We conduct extensive evaluation on traditional and state-of-the-art image cap-tioning models, and surprisingly find that, by only focusing on the protected attribute prediction, bias mitigation models are unexpectedly amplifying bias. 1.

Introduction
The presence of undesirable biases in computer vision applications is of increasing concern. The evidence shows that large-scale datasets, and the models trained on them, present major imbalances in how different subgroups of the population are represented [7, 8, 10, 47]. Detecting and addressing these biases, often known as societal bi-ases, has become an active research direction in our com-munity [1, 11, 21, 30, 32, 37, 44].
Contrary to popular belief, the presence of bias in datasets is not the only cause of unfairness [16]. Model choices and how the systems are trained also have a large impact on the perpetuation of societal bias. This is sup-ported by evidence: 1) models are not only reproducing the inequalities of the datasets but amplifying them [47], and 2) even when trained on balanced datasets, models may still be biased [40] as the depth of historical discrimination is more profound than what it can be manually annotated, i.e., bias is not always evident to the human annotator eye.
The prevalence of accuracy as the single metric to opti-mize in most popular benchmarks [33] has made other as-pects of the models, such as fairness, cost, or efficiency, not a priority (and thus, something to not look into). But
Figure 1. Measuring gender bias in MSCOCO captions [9]. For each caption generated by humans, NIC [36], or NIC+Equalizer
[8], we show our proposed bias score for female and male at-tributes. This bias score indicates how much a caption is biased toward a certain protected attribute. The contribution of each word to the bias score is shown in gray-scale (bold for the word with the highest contribution). Gender revealing words are masked. societal bias is a transversal problem that affects a vari-ety of tasks within computer vision, such as facial recogni-tion, with black women having higher error rates than white men [7]; object classification, with kitchen objects being associated with women with higher probabilities than with men [47]; or pedestrian detection, with lighter skin individ-uals showing higher detection rates than darker skin peo-ple [42]. Although the causes of societal bias in different computer vision systems may be similar, the consequences are particular and require specific solutions for each task.
We examine and quantify societal bias in image caption-ing (Figure 1). Image captioning has achieved state-of-the-art accuracy on MSCOCO captions dataset [9] by means of pre-trained visual and language Transformers [23]. By leveraging very large-scale collections of data (e.g., Google
Conceptual Captions [29] with about 3.3 million image-caption pairs crawled from the Internet), self-attention-based models [34] have the potential to learn world repre-sentations according to the training distribution. However, these large amounts of data, often without (or with min-imal) curation, conceal multiple problems, including the normalization of abuse or the encoding of discrimination
[5,10,27]. So, once image captioning models have achieved outstanding performance on evaluation benchmarks, a ques-tion arises: are these models safe and fair to everyone?
We are not the first to formulate this question.
Image captioning has been shown to reproduce gender [8] and racial [46] bias. By demonstrating the existence of soci-etal bias in image captioning, the pioneering work in [8] set the indispensable seed to continue to investigate this prob-lem, which we believe is far from being solved. We ar-gue that one of the aspects that remains open is the quan-tification and evaluation of societal bias in image caption-ing. So far, a variety of metrics have been applied to as-sess different aspects of societal bias in human and model-generated captions, such as whether the representation of different subgroups is balanced [8, 46] or whether the pro-tected attributes1 values (e.g., female, male) are correctly predicted [8, 31]. However, in Section 3, we show that cur-rent metrics may be insufficient, as they only consider the effects of bias perpetuation to a degree.
With the aim to identify and correct bias in image cap-tioning, in Section 4, we propose a simple but effective met-ric that measures not only how much biased a trained cap-tioning model is, but also how much bias is introduced by the model with respect to the training dataset. This sim-ple metric allows us to conduct a comprehensive analysis of image captioning models in terms of gender and racial bias (Section 5), with an unexpected revelation: the gender equalizer designed to reduce gender bias in [8] is actually amplifying gender bias when considering the semantics of the whole caption. This discovery highlights, even more, the necessity of a standard, unified metric to measure bias and bias amplification in image captioning, as the efforts to address societal inequalities will be ineffective without a tool to quantify how much bias a system exhibits and where this bias is coming from. We conclude the paper with an analysis of the limitation of the proposed metric in Section 6 and a summary of the main findings in Section 7. 2.