Abstract
Gated cameras hold promise as an alternative to scan-ning LiDAR sensors with high-resolution 3D depth that is robust to back-scatter in fog, snow, and rain.
Instead of sequentially scanning a scene and directly recording depth via the photon time-of-ﬂight, as in pulsed LiDAR sensors, gated imagers encode depth in the relative intensity of a handful of gated slices, captured at megapixel resolution.
Although existing methods have shown that it is possible to decode high-resolution depth from such measurements, these methods require synchronized and calibrated LiDAR to supervise the gated depth decoder – prohibiting fast adoption across geographies, training on large unpaired datasets, and exploring alternative applications outside of automotive use cases.
In this work, propose an entirely self-supervised depth estimation method that uses gated in-tensity proﬁles and temporal consistency as a training sig-nal. The proposed model is trained end-to-end from gated video sequences, does not require LiDAR or RGB data, and learns to estimate absolute depth values. We take gated slices as input and disentangle the estimation of the scene albedo, depth, and ambient light, which are then used to learn to reconstruct the input slices through a cyclic loss.
We rely on temporal consistency between a given frame and neighboring gated slices to estimate depth in regions with shadows and reﬂections. We experimentally validate that the proposed approach outperforms existing super-vised and self-supervised depth estimation methods based on monocular RGB and stereo images, as well as super-vised methods based on gated images. Code is available at https://github.com/princeton-computational-imaging/Gated2Gated. 1.

Introduction
Depth sensing has become a cornerstone imaging modal-ity for 3D scene understanding. Depth is used directly as in-put to a vision module, or indirectly in training datasets, to supervise models relying on other modalities across a wide range of applications such as perception and planning in au-tonomous driving, robotics, remote sensing, augmented and virtual reality [40, 56].
The most successful methods for depth estimation are ei-* These authors contributed equally to this work. ther based on pulsed scanning LiDAR [43], or passive RGB sensors, i.e., monocular [14,17,48] and stereo RGB [26,49].
LiDAR depth sensors [51] measure the time-of-ﬂight of pulses of light emitted into the scene and returned to the sen-sor along a coaxial path that is sequentially scanned across a scene. As a result, these sensors deliver precise depth with high spatial resolution at short distances. However, they are expensive, suffer from quadratic decreasing spatial resolu-tion at longer distances, e.g., resulting in a few measure-ment points for pedestrians at 100 m distance [51], and they fail in the presence of strong back-scatter. Monocular and stereo RGB methods offer a substantially cheaper alterna-tive, but they struggle to achieve depth precision compara-ble to time-of-ﬂight imaging, struggle in low-light scenar-ios, and at long distances that map to small disparities.
Recently, gated imaging has been proposed as an alter-native sensor modality for depth estimation and 3D detec-tion [22, 32] which promises to overcome the spatial reso-lution limitation of scanning LiDAR while providing com-parable depth precision. Gated cameras combine low-cost
CMOS sensors with analog gated readout and active ﬂash illumination, allowing to capture a sequence of gated im-age slices that each encode time-resolved illumination via their relative intensity and, as such, provide depth cues not present in RGB cameras. Thanks to this active gated ﬂash acquisition mode, gated imaging methods have shown to be more robust in low-light scenarios and in the presence of strong back-scatter that can be suppressed during acquisi-tion [22]. For a comprehensive review of Gated Sensors see Sec. 3. Gated images provide dense depth informa-tion at megapixel resolution of the gated camera, allow-ing for long-range perception where LiDAR-based meth-ods fail [32]. However, all of these existing methods re-quire calibrated and synchronized LiDAR data for train-ing supervision. Although commercial gated cameras have become available [20], the requirement of such a multi-modal capture system prohibits the rapid adoption of gated depth imaging not only in automotive applications but also in other robotic use cases. Moreover, large unpaired se-quences of gated imagery cannot be exploited in training existing gated depth estimation methods. In this work, we propose the ﬁrst self-supervised method for depth estima-tion from gated cameras. The proposed method takes gated
slices as input and predicts the scene albedo, depth, and ambient illumination. We reconstruct the input slices us-ing calibrated gated proﬁles enforcing measurement cycle consistency and warping from the nearby slices in a tem-poral window enforcing temporal consistency. To this end, we introduce a differentiable gated image formation model that uses depth-dependent calibrated gating proﬁles for self-supervised measurement cycle loss. To learn in the absence of reliable gated measurements due to shadows, we utilize temporal depth consistency using differentiable structure-from-motion and the proposed image formation model.
Speciﬁcally, we make the following main contributions:
• We propose a novel self-supervised method that uses measurement cycle consistency and temporal consis-tency as training signals.
• The proposed model is trained end-to-end, and by ex-ploiting calibrated gating proﬁles, the method is able to accurately estimate metric depth using the cycle con-sistency component.
• We validate that the proposed method outperforms self-supervised and supervised depth estimation us-ing monocular and stereo RGB images and supervised gated depth estimation methods.
We have released models and code used to reproduce the results from this work. 2.