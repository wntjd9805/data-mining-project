Abstract
We present GeoNeRF, a generalizable photorealistic novel view synthesis method based on neural radiance
ﬁelds. Our approach consists of two main stages: a ge-ometry reasoner and a renderer. To render a novel view, the geometry reasoner ﬁrst constructs cascaded cost volumes for each nearby source view. Then, using a Transformer-based attention mechanism and the cascaded cost volumes, the renderer infers geometry and appearance, and ren-ders detailed images via classical volume rendering tech-niques. This architecture, in particular, allows sophis-ticated occlusion reasoning, gathering information from consistent source views. Moreover, our method can eas-ily be ﬁne-tuned on a single scene, and renders com-petitive results with per-scene optimized neural render-ing methods with a fraction of computational cost. Ex-periments show that GeoNeRF outperforms state-of-the-art generalizable neural rendering models on various syn-thetic and real datasets. Lastly, with a slight modiﬁca-tion to the geometry reasoner, we also propose an alter-native model that adapts to RGBD images. This model di-rectly exploits the depth information often available thanks to depth sensors. The implementation code is available at https://www.idiap.ch/paper/geonerf. 1.

Introduction (NeRF) [37] made a signiﬁcant impact on this research area by implicitly representing the 3D structure of the scene and rendering high-quality novel images. Our work addresses the main drawback of NeRF, which is the requirement to train from scratch for every scene separately. The per-scene optimization of NeRF is lengthy and requires densely cap-tured images from each scene.
Approaches like pixelNeRF [61], GRF [51], MINE [29],
SRF [8], IBRNet [54], MVSNeRF [6], and recently intro-duced NeRFormer [43] address this issue and generalize
NeRF rendering technique to unseen scenes. The common motivation behind such methods is to condition the NeRF renderer with features extracted from source images from a set of nearby views. Despite the generalizability of these models to new scenes, their understanding of the scene ge-ometry and occlusions is limited, resulting in undesired ar-tifacts in the rendered outputs. MVSNeRF [6] constructs a low-resolution 3D cost volume inspired by MVSNet [58], which is widely used in the Multi-View Stereo research, to condition and generalize the NeRF renderer. However, it has difﬁculty rendering detailed images and does not deal with occlusions in a scene. In this work, we take MVSNeRF as a baseline and propose the following improvements.
• We introduce a geometry reasoner in the form of cas-caded cost volumes (Section 3.1) and train it in a semi-supervised fashion (Section 3.4) to obtain ﬁne and high-resolution priors for conditioning the renderer.
Novel view synthesis is a long-standing task in com-puter vision and computer graphics. Neural Radiance Fields
• We combine an attention-based model which deals with information coming from different source views
at any point in space, by essence permutation invariant, with an auto-encoder network which aggregates infor-mation along a ray, leveraging its strong Euclidean and ordering structure (Section 3.3).
• Thanks to the symmetry and generalizability of our ge-ometry reasoner and renderer, we detect and exclude occluded views for each point in space and use the re-maining views for processing that point (Section 3.3).
In addition, with a slight modiﬁcation to the architecture, we propose an alternate model that takes RGBD images (RGB+Depth) as input and exploits the depth information to improve its perception of the geometry (Section 3.5).
Concurrent to our work, the followings also introduce a generalizable NeRF: RGBD-Net [38] builds a cost volume for the target view instead of source views, NeuralMVS [45] proposes a coarse to ﬁne approach to increase speed, and
NeuRay [32] proposes a method to deal with occlusions. 2.