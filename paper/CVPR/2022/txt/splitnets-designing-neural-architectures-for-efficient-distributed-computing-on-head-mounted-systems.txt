Abstract
We design deep neural networks (DNNs) and corre-sponding networks’ splittings to distribute DNNs’ workload to camera sensors and a centralized aggregator on head mounted devices to meet system performance targets in in-ference accuracy and latency under the given hardware re-source constraints. To achieve an optimal balance among computation, communication, and performance, a split-aware neural architecture search framework, SplitNets, is introduced to conduct model designing, splitting, and com-munication reduction simultaneously. We further extend the framework to multi-view systems for learning to fuse inputs from multiple camera sensors with optimal performance and systemic efﬁciency. We validate SplitNets for single-view system on ImageNet as well as multi-view system on 3D classiﬁcation, and show that the SplitNets framework achieves state-of-the-art (SOTA) performance and system latency compared with existing approaches. 1.

Introduction
Virtual Reality (VR) and Augmented Reality (AR) are becoming increasingly prevailing as one of the next-generation computing platforms [3]. Head mounted devices (HMD) for AR/VR feature multiple cameras to support var-ious computer vision (CV) / machine learning (ML) pow-ered human-computer interaction functions, such as object classiﬁcation [40, 61], hand-tracking [19] and SLAM [38].
Due to the recent advances of camera technologies, tiny multi-layer stacked cameras with AI computing capabil-ity arise [32, 33], as depicted in Figure 1. Because of the small form factor, these intelligent cameras have highly constrained resources compared with general purpose mo-bile or data center grade AI processors. However, each cam-era is still capable of performing pre-processing directly af-ter image acquisition, signiﬁcantly reducing expensive raw image data movement.
In a modern HMD system, the distributed intelligent stacked image sensors and a central
AR/VR processor (aggregator) form the hardware backbone to realize complex CV/ML functions on device [3, 13, 14].
Within such systems, it’s natural to split the machine learning workload for an application between the sensors and the centralized computer (aggregator). As is shown in Figure 2, for an application that requires multiple lay-ers of convolutional neural networks and fusion of multiple input sources, the early layers can be distributed to the on-sensor processing. The feature fusion and rest of process-ing are on the aggregator. This way, overall system latency can be improved by leveraging direct parallel processing on sensors and reduced sensor-aggregator communication.
The success of distributed computing for DNNs between sensors and aggregator heavily relies on the network ar-chitecture to satisfy the application and hardware system constraints such as memory, communication bandwidth, la-tency target and etc.. Prior work [10, 27, 28, 39] searches network partitions (i.e., the splitting points) for existing models in either exhaustive or heuristic manners. Some work [4, 11, 36, 45, 46] manually injects a bottleneck mod-Advantages
↓ Comm. Cost
↓ Peak Sensor Mem.
↑ Parallelism ↑ Hardware Utilization
Privacy Preserving
All on sensors
All on aggregator
Distributed computing (cid:51) (cid:55) (cid:51) (cid:55) (cid:51) (cid:51) (cid:51) (cid:55) (cid:51) (cid:55) (cid:55) (cid:51) (cid:51) (cid:55) (cid:51)
Table 1. Comparison of different DNN computing ofﬂoad paradigms.
Components
System factors to be considered 2 Sensor (cid:3)(cid:88) Computation capability (cid:3)(cid:88) Peak memory constraint (cid:3)(cid:88) Number of sensors (i.e., parallelism) (cid:3)(cid:88) Communication bandwidth
X Comm.
< Aggregator (cid:3)(cid:88) Computation capability
Whole system (cid:3)(cid:88) Task performance (e.g., accuracy) (cid:3)(cid:88) Overall latency = on-sen. latency
+ comm. latency + on-agg. latency
Table 2. Factors need to be balanced when distributing DNN com-putation. The listed factors are interwoven with each other and have to be considered holistically. ule into the model to reduce communication. However, these methods sometimes obtain naive splitting results (i.e., splitting after the last layer) [28] and lead to performance degradation [35].
The challenge of splitting DNNs comes from the compli-cated mutual-impact of many model and hardware factors.
For example, existing (hand-crafted and searched) model architectures are designed without considering distributed computing over multiple computing modelities, and thus not suitable for splitting in the ﬁrst place. In addition, the position of the splitting point as well as the inserted com-pression module will simultaneously impact model perfor-mance, computation ofﬂoad, communication, and hardware utilization in different directions [37]. Heuristic and rule-based methods are limited in this context.
In this work, we adapt the neural architecture search (NAS) approach to automatically search split-aware model architectures targeting the distributed computing systems on AR/VR glasses. We propose the SplitNets framework that jointly optimizes the task performance and system efﬁ-ciency subject to resource constraints of the mobile AR/VR system featuring smart sensors. We speciﬁcally answer the following two questions: 1. Can we jointly search for optimal network architectures and their network splitting solution between sensors and the aggregator while satisfying resource constraints im-posed by the underlining hardware? 2. Can we learn an optimal network architecture to com-press and fuse features from multiple sensors to the ag-gregator and achieve SOTA performance and efﬁciency compared with conventional centralized models?
We design SplitNets, a split-aware NAS framework, for efﬁcient and ﬂexible searching of the splitting module in the network in the distributed computing context where the splitting module is able to split the network, compress fea-tures along the channel dimension for communication sav-ing as well as view fusion for multi-view tasks.
We introduce a series of techniques for module initializa-tion and sampling to stabilize the training with information bottlenecks and mitigate introduced accuracy degradation.
We further extend SplitNets to support searching of split-ting modules with view fusion for multi-view tasks. To our best knowledge, it is the ﬁrst framework supporting the po-sition searching of information compression / fusion in a multi-input neural network. Overall, our contributions are summarized as follows:
• We propose SplitNets, a split-aware NAS framework, for efﬁcient and ﬂexible position searching of splitting mod-ules for single / multi-view task.
• We introduce splitting modules for single- and multi-view tasks which can achieve model splitting, feature compression, as well as view fusion simultaneously. To search the optimal position of the splitting module, we propose to use separate supernets for sensors and the ag-gregator respectively, and stitch them together to form the split-aware model of interest, using the shared splitting module as the joint point. In addition, we combine the compression- / recovery- / fusion- based splitting mod-ule design with custom weights initialization, and a novel candidate networks sampling strategy to mitigate the ac-curacy drop due to model partition.
• We evaluate SplitNets with single-view classiﬁcation and multi-view 3D classiﬁcation. Empirical observations val-idate the importance of joint model and splitting posi-tion search to improve both task and system performance.
Our results show that optimized network architectures and model partitions discovered by SplitNets signiﬁcantly outperform existing solutions and ﬁt the distributed com-puting system well on AR/VR glasses. 2.