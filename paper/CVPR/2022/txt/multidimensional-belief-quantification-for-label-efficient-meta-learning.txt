Abstract
Optimization-based meta-learning offers a promising di-rection for few-shot learning that is essential for many real-world computer vision applications. However, learn-ing from few samples introduces uncertainty, and quanti-fying model conﬁdence for few-shot predictions is essen-tial for many critical domains. Furthermore, few-shot tasks used in meta training are usually sampled randomly from a task distribution for an iterative model update, leading to high labeling costs and computational overhead in meta-training. We propose a novel uncertainty-aware task se-lection model for label efﬁcient meta-learning. The pro-posed model formulates a multidimensional belief measure, which can quantify the known uncertainty and lower bound the unknown uncertainty of any given task. Our theoreti-cal result establishes an important relationship between the conﬂicting belief and the incorrect belief. The theoretical result allows us to estimate the total uncertainty of a task, which provides a principled criterion for task selection. A novel multi-query task formulation is further developed to improve both the computational and labeling efﬁciency of meta-learning. Experiments conducted over multiple real-world few-shot image classiﬁcation tasks demonstrate the effectiveness of the proposed model. 1.

Introduction
Deep learning (DL) models have achieved state-of-the-art performance for many computer vision applications.
However, the effectiveness of DL models is challenged by some specialized domains (e.g. medicine, biology, and se-curity intelligence), in which labeled data for model train-ing may be scarce. Unlike the DL models, human beings can learn efﬁciently from limited training samples by using the prior knowledge stored in their brains and applying it to new tasks. For example, once a child learns how to distin-guish between lions and tigers, s/he can quickly generalize the concept to distinguish lions and cats with little or no ad-ditional training. Inspired by such human learning, various few-shot learning techniques [6, 24, 37] have been devel-oped, which provide a promising approach to address the label scarcity problem for DL models.
In recent successful few-shot learning approaches, the model is trained from multiple few-shot tasks comprised of few labeled examples instead of one large dataset as in the traditional setting. By learning from many similar tasks, the model can accumulate the shared knowledge among tasks.
After training, it uses the knowledge gained from similar tasks as the prior knowledge to perform well on new un-seen few-shot tasks. Meta-learning is one popular approach for few-shot learning where the model learns at two stages: rapid learning within a new task, which is guided by prior knowledge gained from gradual learning across tasks [30].
In meta-learning, the model is trained on a large number of few-shot tasks to learn the shared inter-task knowledge.
The learned model is evaluated based on its generalization capabilities on unseen few-shot tasks.
Few-shot tasks have limited data to learn from (in some cases just 1 example/class). So, some model predictions may not be reliable. For critical applications (e.g. au-tonomous driving), it is essential to quantify the prediction uncertainty. Some existing approaches indirectly provide uncertainty information of few-shot tasks by learning a pos-terior predictive distribution for testing data samples [7, 10, 12, 17, 28]. However, they usually suffer from a high com-putational cost and rely on assumptions/approximations that may be invalid in practice.
Additionally, few-shot tasks used in meta training are usually sampled randomly from a task distribution formed using a large pool of labeled data samples. Thus, meta-training for many optimization-based meta-learning ap-proaches is computationally expensive, requiring evaluation of the second-order derivative (i.e., Hessian) of the (global) model parameters over each of the sampled tasks. Further-more, the large number of tasks leads to high labeling costs in many real-world problems. However, not all the tasks contribute equally to the learning of the (global) model pa-rameters, and evaluating the Hessian over these tasks can signiﬁcantly slow down the meta training process.
In this paper, we present a novel Uncertainty-aware task selection model for efﬁcient meta-learning (referred to as
Units-ML) that provides uncertainty estimation to quan-tify the model conﬁdence in few-shot predictions. Build-ing upon the theory of subjective logic [13], we formulate a multidimensional belief measure, including vacuous, con-ﬂicting, and incorrect beliefs, which can quantify both the known uncertainty (KUN) and unknown uncertainty (UUN) of a given task. However, evaluating incorrect belief re-lies on the labels of a query set in a task, making the UUN not accessible during task selection. We address this issue by proving a novel relationship between conﬂicting belief and incorrect belief, which allows us to bridge the gap to
UUN. A novel task selection function is designed accord-ingly that integrates both KUN and UUN for belief-oriented label-efﬁcient meta-learning.
We summarize our key contributions below: (1) a novel computationally and label-efﬁcient meta-learning model that can estimate uncertainty in few-shot tasks, (2) a multi-dimensional belief measure to quantify the KUN and lower bound the UUN of a given task, (3) theoretical justiﬁca-tion that conﬂicting belief lower bounds incorrect belief, which allows UUN estimation without label information, and (4) an uncertainty-aware task selection criterion and a novel multi-query task formulation to improve both compu-tation and label efﬁciency of meta-learning. We conduct in-tensive experiments over multiple real-world image datasets to demonstrate the effectiveness of the proposed Units-ML model in terms of accurate uncertainty estimation, computa-tionally effective task selection, and label-efﬁcient learning under a limited annotation budget. 2.