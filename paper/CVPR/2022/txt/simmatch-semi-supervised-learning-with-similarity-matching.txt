Abstract
Learning with few labeled data has been a longstanding problem in the computer vision and machine learning re-search community. In this paper, we introduced a new semi-supervised learning framework, SimMatch, which simulta-neously considers semantic similarity and instance similar-ity. In SimMatch, the consistency regularization will be ap-plied on both semantic-level and instance-level. The differ-ent augmented views of the same instance are encouraged to have the same class prediction and similar similarity re-lationship respected to other instances. Next, we instanti-ated a labeled memory buffer to fully leverage the ground truth labels on instance-level and bridge the gaps between the semantic and instance similarities. Finally, we pro-posed the unfolding and aggregation operation which al-lows these two similarities be isomorphically transformed with each other.
In this way, the semantic and instance pseudo-labels can be mutually propagated to generate more high-quality and reliable matching targets. Extensive ex-perimental results demonstrate that SimMatch improves the performance of semi-supervised learning tasks across dif-ferent benchmark datasets and different settings. Notably, with 400 epochs of training, SimMatch achieves 67.2%, and 74.4% Top-1 Accuracy with 1% and 10% labeled examples on ImageNet, which significantly outperforms the baseline methods and is better than previous semi-supervised learn-ing frameworks. 1.

Introduction
Benefiting from the availability of large-scale annotated datasets and growing computational resources in the last decades, deep neural networks have demonstrated their suc-cess on a variety of visual tasks [19, 20, 22, 23, 27, 38, 50, 58–60]. However, a large volume of labeled data is very expensive to collect in a real-world scenario. Learning with few labeled data has been a longstanding problem in
Figure 1. A sketch of SimMatch. The Fully-Connected layer vec-tors can be viewed as a semantic representative or class center for each category. However, due to the limited labeled samples, the semantic-level information is not always reliable. In SimMatch, we consider the instance-level and semantic-level information si-multaneously and adopt a labeled memory buffer to fully leverage the ground truth label on instance-level. the computer vision and machine learning research com-munity. Among various methods, semi-supervised learning (SSL) [12, 46, 53, 67] serves as an effective solution by dint of the help of massive unlabeled data, and achieves remark-able performance.
A simple but very effective semi-supervised learning method is to pretrain a model on a large-scale dataset and then transfer the learned representation by fine-tuning the pretrained model with a few labeled samples. Thanks to the recent advances in self-supervised learning [10, 14, 15, 21, 25, 26, 30, 54], such pretraining and fine-tuning pipeline have demonstrated its promising performance in SSL. Most self-supervised learning frameworks focus on the design of pretext tasks. For example, instance discrimination [55] en-courages different views of the same instance to share the same features, and different instances should have distinct features. Deep clustering based methods [3, 9, 10] expect different augmented views of the same instance should be classified into the same cluster. However, most of these pre-text tasks are designed in a completely unsupervised man-ner, without considering the few labeled data at hand.
*Correspondence to: Shan You <youshan@sensetime.com>.
Instead of standalone two-stage pretraining and fine-tuning, current popular methods directly involve the la-beled data in a joint feature learning paradigm with pseudo-labeling [35] or consistency regularization [47]. The main idea behind these methods is to train a semantic classifier with labeled samples and use the predicted distribution as the pseudo label for the unlabeled samples. In this way, the pseudo-labels are generally produced by the weakly aug-mented views [5, 48] or the averaged predictions of mul-tiple strongly augmented views [6]. The objective will be constructed by the cross entropy loss between an different strongly augmented views and the pseudo-labels. It might also be noted that the pseudo-labels will generally be sharp-ened or operated by argmax since every instance is ex-pected to be classified into a category. However, when there are only very limited annotated data, the semantic classi-fier is no longer reliable; applying the pseudo-label method will cause the “overconfidence” issue [13,63], which means the model will fit on the confident but wrong pseudo-labels, resulting in poor performance.
In this paper, we introduce a novel semi-supervised learning framework, SimMatch, which has been shown in
Figure 1. In SimMatch, we bridge both sides and propose to match the similarity relationships of both semantic and instance levels simultaneously for different augmentations.
Specifically, we first require the strongly augmented view to have the same semantic similarity (i.e. label prediction) with a weak augmented view; besides, we also encourage the strong augmentation to have the same instance charac-teristics (i.e. similarity between instances) with the weak one for more intrinsic feature matching. Moreover, different from the previous works that simply regard the predictions of the weakly augmented views as pseudo-labels. In Sim-Match, the semantic and instance pseudo-labels are allowed to interact by instantiating a memory buffer that keeps all
In this way, these two similarities the labeled examples. can be isomorphically transformed with each other by in-troducing aggregating and unfolding techniques. Thus the semantic and instance pseudo-labels can be mutually prop-agated to generate more high-quality and reliable matching targets. Extensive experiments demonstrate the effective-ness of SimMatch across different settings. Our contribu-tion can be summarized as follows:
• We proposed SimMatch, a novel semi-supervised learning framework that simultaneously considers se-mantics similarity and instance similarity.
• To channel both similarities, we leverage a labeled memory buffer so that semantic and instance pseudo-labels can be mutually propagated with the aggregat-ing and unfolding techniques.
• SimMatch establishes a new state-of-the-art perfor-mance for semi-supervised learning. With only 400 epochs of training, SimMatch achieves 67.2% and 74.4% Top-1 accuracy with 1% and 10% labeled ex-amples on ImageNet. 2.