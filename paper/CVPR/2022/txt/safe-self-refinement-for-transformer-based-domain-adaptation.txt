Abstract
Unsupervised Domain Adaptation (UDA) aims to lever-age a label-rich source domain to solve tasks on a related unlabeled target domain. It is a challenging problem espe-cially when a large domain gap lies between the source and target domains. In this paper we propose a novel solution named SSRT (Safe Self-Refinement for Transformer-based domain adaptation), which brings improvement from two aspects. First, encouraged by the success of vision trans-formers in various vision tasks, we arm SSRT with a trans-former backbone. We find that the combination of vision transformer with simple adversarial adaptation surpasses best reported Convolutional Neural Network (CNN)-based results on the challenging DomainNet benchmark, show-ing its strong transferable feature representation. Second, to reduce the risk of model collapse and improve the effec-tiveness of knowledge transfer between domains with large gaps, we propose a Safe Self-Refinement strategy. Specifi-cally, SSRT utilizes predictions of perturbed target domain data to refine the model. Since the model capacity of vi-sion transformer is large and predictions in such challeng-ing tasks can be noisy, a safe training mechanism is de-signed to adaptively adjust learning configuration. Ex-tensive evaluations are conducted on several widely tested
UDA benchmarks and SSRT achieves consistently the best performances, including 85.43% on Office-Home, 88.76% on VisDA-2017 and 45.2% on DomainNet. 1.

Introduction
Deep neural networks have achieved impressive per-formance in a variety of machine learning tasks. How-ever, the success often relies on a large amount of labeled training data, which can be costly or impractical to ob-tain. Unsupervised Domain Adaptation (UDA) [36] han-dles this issue by transferring knowledge from a label-rich source domain to a different unlabeled target domain.
Over the past years, many UDA methods have been pro-posed [4, 12, 14, 24, 44]. Among them, adversarial adapta-tion [4, 14, 44] that learns domain-invariant feature repre-sentation using the idea of adversarial learning has been a prevailing paradigm. Deep UDA methods are usually ap-plied in conjunction with a pretrained Convolutional Neural
Network (CNN, e.g., ResNet [8]) backbone in vision tasks.
On medium-sized classification benchmarks such as Office-Home [33] and VisDA [20], the reported state-of-the-arts are very impressive [12]. However, on large-scale datasets like DomainNet [19], the most recent results in the litera-ture by our submission report a best average accuracy of 33.3% [10], which is far from satisfactory.
With the above observations, we focus our investigation on challenging cases from two aspects:
• First, from the representation aspect, it is desirable to use a more powerful backbone network. This directs our at-tention to the recently popularized vision transformers, which have been successfully applied to various vision tasks [2, 3, 42]. Vision transformer processes an image as a sequence of tokens, and uses global self-attention to refine this representation. With its long-range dependen-cies and large-scale pre-training, vision transformer ob-tains strong feature representation that is ready for down-stream tasks. Despite this, its application in UDA is still under-explored. Hence we propose to integrate vision transformer to UDA. We find that by simply combining
ViT-B/16 [3] with adversarial adaptation, it can achieve 38.5% average accuracy on DomainNet, better than the current arts using ResNet-101 [8,10]. This shows that the feature representation of vision transformer is discrimi-native as well as transferable across domains.
• Second, from the domain adaptation aspect, a more re-liable strategy is needed to protect the learning process from collapse due to large domain gaps. As strong back-bones with large capacity like vision transformer increase the chance of overfitting to source domain data, a regu-larization from target domain data is desired. A common practice in UDA is to utilize model predictions for self-training or enforce clustering structure on target domain data [12, 24, 43]. While this helps generally, the supervi-sions can be noisy when the domain gap is large. There-fore, an adaptation method is expected to be Safe [11] enough to avoid model collapse.
Motivated by the above discussions, in this paper, we propose a novel UDA solution named SSRT (Safe Self-Refinement for Transformer-based domain adaptation).
SSRT takes a vision transformer as the backbone network and utilizes predictions on perturbed target domain data to refine the adapted model. Specifically, we add random off-sets to the latent token sequences of target domain data, and minimize the discrepancy of model’s predicted prob-abilities between the original and perturbed versions us-ing the Kullback Leibler (KL) divergence. This imposes a regularization on the corresponding transformer layers in effect. Moreover, SSRT has several important compo-nents that contribute to its excellent performance, including multi-layer perturbation and bi-directional supervision.
To protect the learning process from collapse, we pro-pose a novel Safe Training mechanism. As UDA tasks vary widely even when they are drawn from the same dataset, a specific learning configuration (e.g., hyper-parameters) that works on most tasks may fail on some particular ones.
The learning configuration is thus desired to be automati-cally adjustable. For example, for perturbation-based meth-ods [17, 25], a small perturbation may under-exploit their benefits while a large one may result in collapse. Recent works [1, 29] apply a manually defined ramp-up period at the beginning of training. However, this cannot solve the issue when its maximum value is improper for the current task. In contrast, we propose to monitor the whole training process and adjust learning configuration adaptively. We use a diversity measure of model predictions on the tar-get domain data to detect model collapse. Once it occurs, the model is restored to a previously achieved state and the configuration is reset. With this safe training strategy, our
SSRT avoids significant performance deterioration on adap-tation tasks with large domain gaps. The code is available at https://github.com/tsun/SSRT.
In summary, we make the following contributions:
• We develop a novel UDA solution SSRT, which adopts a vision transformer backbone for its strong transferable feature representation, and utilizes the predictions on per-turbed target domain data for model refinement.
• We propose a safe training strategy to protect the learning process from collapse due to large domain gaps. It adap-tively adjusts learning configuration during the training process with a diversity measure of model predictions on target domain data.
• SSRT is among the first to explore vision transformer for domain adaptation. Vision transformer-based UDA has shown promising results, especially on large-scale datasets like DomainNet.
• Extensive experiments are conducted on widely tested benchmarks. Our SSRT achieves the best performances, including 85.43% on Office-Home, 88.76% on VisDA-2017 and 45.2% on DomainNet. 2.