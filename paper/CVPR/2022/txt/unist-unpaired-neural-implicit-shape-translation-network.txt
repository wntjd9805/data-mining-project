Abstract
We introduce UNIST, the first deep neural implicit model for general-purpose, unpaired shape-to-shape translation, in both 2D and 3D domains. Our model is built on au-toencoding implicit fields, rather than point clouds which represents the state of the art. Furthermore, our translation network is trained to perform the task over a latent grid rep-resentation which combines the merits of both latent-space processing and position awareness, to not only enable dras-tic shape transforms but also well preserve spatial features and fine local details for natural shape translations. With the same network architecture and only dictated by the in-put domain pairs, our model can learn both style-preserving content alteration and content-preserving style transfer. We demonstrate the generality and quality of the translation re-sults, and compare them to well-known baselines. Code is available at https://qiminchen.github.io/unist/. 1.

Introduction
Unpaired image-to-image translation has become one of the most extensively studied problems in computer vision since the advent of CycleGAN [29], DualGAN [26], and
UNIT [16] in 2017. Somewhat surprisingly, there have been much fewer works on the same problem for shapes, i.e., unpaired shape-to-shape translation. To date, most image translation networks have been designed for style transfers that are localized, without large structural alterations. For shape translations, however, one may naturally expect more of the latter, e.g., to change the shape of a letter ‘R’ to that of a ‘G’, or a table to a chair; see Figures 1(a-d).
Recently, Yin et al. [27] proposed LOGAN, an unpaired shape translation network that can be trained to execute both style and content, i.e., shape- and structure-level, trans-forms. However, their network was designed to operate on low-resolution point clouds (up to 2,048 points), which can severely limit the quality of the reconstructed and translated shapes, especially in the 3D case. In addition, the transla-tion network is trained to operate on ªholisticº latent codes which encode global information that is multi-scale but not position-aware. A consequence of such a lack of positional information in the encoding is losing control of spatial fea-tures, as well as local details, during shape translation. For example, when the translation is supposed to only italicize a letter shape, which is a pose change, the local details of a source shape (e.g., thickness/sharpness of certain tips of the shape) may be unexpectedly altered as well, as shown in the second row of Figure 4 on the letter A translation.
In this paper, we present a method for unpaired shape-to-shape translation that is built on autoencoding neural im-plicit fields [3, 18, 20], rather than point clouds [27]. In re-cent years, advantages of learning continuous implicit func-tions over discrete representations such as voxels, mesh
Figure 2. Overview of our framework for unpaired neural implicit shape-to-shape translation, which consists of two separately trained networks. The autoencoding network (top) learns to encode and decode binary voxel occupancies for shapes from both the source and target domains, where the encoder maps an input shape to a latent grid representation Z. In the 2D case, Z ∈ Rk×k×m, where the k × k grid is obtained via spatial convolution over the n × n input image, and m is the length of the latent code. The latent feature at any query point p is obtained via bilinear interpolation over the latent codes stored in Z. In the 3D case, the grid is three-dimensional and obtained via volumetric convolution and trilinear interpolation is performed to extract latent features for the decoder. The translation network (bottom) employs the pre-trained autoencoder network above to transform the translation problem into a latent space. In that space, a generator learns two tasks: 1) translating source-domain codes (Zχ1 ) into target-domain codes (Zχ1→2 ); 2) preserving target-domain codes, from
Zχ2 to Zχ2→2 . Zχ1→2 is passed to the pre-trained implicit decoder to obtain the final target shape resulting from the generator network. patches, and point clouds have been demonstrated predom-inantly for reconstructive tasks including neural rendering, shape completion, and single-view 3D reconstruction. Our work shall show that the same advantages of neural implicit models can be carried over to domain translation.
Furthermore, our translation network is trained to per-form the task over a latent grid representation whose grid structure is spatially correlated with that of the input shapes, via convolution, while its remaining dimension en-codes the latent features. Hence, our approach combines the merits of both latent-space processing and position aware-ness, with the former facilitating more drastic shape transla-tions [16,27] and the latter resulting in better preservation of spatial features and details [4, 5, 22] during the translation.
Our model, coined UNIST for unpaired neural implicit shape translation, consists of two separately trained net-works, as illustrated in Figure 2. Given two unpaired do-mains of shapes, e.g., chairs and tables or various letters in different fonts (see Figure 1 for several examples of domain pairs), the autoencoding network learns to encode and de-code shapes (in the form of binary voxel occupancies) from both domains, using latent grids. The network training is self-supervised with the typical reconstruction loss.
The translation network is based on the LOGAN [27] ar-chitecture which consists of a latent generator that is trained to perform two tasks: one is to translate the code of a source shape to that of a target shape under the adversarial set-ting, and the other is to turn the code of a target shape to itself based on a feature preservation loss. Differently from the original LOGAN, inputs to the UNIST generator are no longer the holistic and overcomplete latent codes; they are replaced by the latent grid features produced by the en-coder of the pre-trained autoencoder network (top in Fig-ure 2). The translation network is trained with the same set of losses as LOGAN, while the outputs from the generator would go through the pre-trained decoder (top in Figure 2) to produce the final shapes in the target domain.
Our work represents the first deep implicit model for general-purpose, unpaired shape-to-shape translation. With the same network architecture and only dictated by the in-put domain pairs, our model can learn both style-preserving content alteration and content-preserving style transfer, as shown in Figure 1. We demonstrate the generality and quality of the translation results, and compare them to LO-GAN [27] and other baselines. We show that clear qual-ity improvements on both shape reconstruction and trans-Latent Vector vs. Latent Grid
Latent Vector vs. Latent Grid lation are obtained merely by autoencoding implicit fields rather than point clouds. Further, adding position awareness through latent grids leads to more natural shape translation, with better preservation of spatial features and fine details.
Query point p through concatenation
Query point p through concatenation
Query point p through bilinear interpolation
Query point p through bilinear interpolation
Encoder
Encoder
Implicit 
Decoder
Implicit 
Decoder
Encoder
Encoder
Implicit 
Decoder
Implicit 
Decoder m
... m
... 2.