Abstract 1.

Introduction
While NeRF [28] has shown great success for neural re-construction and rendering, its limited MLP capacity and long per-scene optimization times make it challenging to model large-scale indoor scenes. In contrast, classical 3D reconstruction methods can handle large-scale scenes but do not produce realistic renderings. We propose NeRFu-sion, a method that combines the advantages of NeRF and
TSDF-based fusion techniques to achieve efficient large-scale reconstruction and photo-realistic rendering. We pro-cess the input image sequence to predict per-frame local radiance fields via direct network inference. These are then fused using a novel recurrent neural network that incremen-tally reconstructs a global, sparse scene representation in real-time at 22 fps. This global volume can be further fine-tuned to boost rendering quality. We demonstrate that NeR-Fusionachieves state-of-the-art quality on both large-scale indoor and small-scale object scenes, with substantially faster reconstruction than NeRF and other recent methods.1
*Research partially done during Xiaoshuai’s internship at Adobe Re-search 1https://jetd1.github.io/NeRFusion-Web/
Reconstructing and rendering large-scale indoor scenes from RGB images is challenging but crucial for various applications in computer vision and graphics, including
AR/VR, e-commerce, and robotics. While truncated signed distance function (TSDF) fusion techniques [29, 42] can achieve efficient reconstruction, these methods often use depth sensors and focus on geometric reconstruction only, and cannot synthesize realistic images. Recently, NeRF
[28] proposed optimizing scene radiance fields, represented using global MLPs, from RGB images to achieve photo-realistic novel view synthesis. However, NeRF cannot han-dle large-scale scenes well due to its limited MLP network capacity and impractical slow per-scene optimization.
In this work, we aim to achieve fast, large-scale scene-level radiance field reconstruction to make neural scene re-construction and rendering more practical. As opposed to small-scale object-centric scenes, we use “large-scale” to refer to full-size indoor scenes, like ScanNet scenes [11]), with multiple rooms and objects with complex scene geom-etry and appearance. To achieve fast radiance field recon-struction on such challenging scenes, we propose a novel neural framework that uses recurrent neural modules to in-1
crementally reconstruct a large sparse radiance field from a long RGB image sequence. Unlike NeRF [28], that re-quires per-scene optimization, our network is generalizable, pre-trained across scenes, and able to efficiently reconstruct large-scale radiance fields via direct network inference. As shown in Fig. 1, our framework can successfully reconstruct a large indoor scene from from an input monocular RGB video from ScanNet [11], to create a high-quality radiance field with photo-realistic novel view synthesis results.
Our reconstructed radiance field is represented by a sparse volume grid with per-voxel neural features; these voxel features are tri-linearly interpolated at any scene location, and used to regress volume density and view-dependent radiance through an MLP decoder for differen-tiable volume rendering. In contrast to previous methods
[14, 24] that reconstruct similar representations using slow per-scene optimization, we present a novel deep neural net-work that can be trained across scenes and generalize on unseen novel scenes to achieve fast radiance field recon-struction, bypassing per-scene fitting.
Given an input sequence of RGB images with known camera poses (that can be registered by SLAM or SfM tech-niques), our framework reconstructs a radiance field as a sparse neural volume. Our pipeline is inspired by the clas-sical TSDF fusion workflow [29, 30, 42] that starts from per-view geometry (depth) and fuses the per-view recon-struction across key frames to obtain a global sparse TSDF volume. This workflow is widely used to reconstruct large-scale scenes, but only focuses on geometric reconstruction.
Instead, we propose novel neural modules to reconstruct ra-diance fields as sparse voxels for photo-realistic rendering.
We first reconstruct local radiance fields for each input key frame. We leverage deep MVS techniques and ap-ply sparse 3D convolutions on a world-space cost-volume built from unprojected 2D images features (regressed from a deep 2D CNN) of neighboring key frames. This recon-structs sparse neural voxels that represent a local radiance field. Once estimated, this field can already be used to ren-der realistic images locally, though only for partial scene content seen by the local frames. We propose a recurrent neural fusion module to sequentially fuse multiple local fields across frames. Our fusion module recurrently takes a newly estimated local field as input and learns to incorpo-rate the local voxels to progressively reconstruct a global ra-diance field modeling the entire scene, by adding new vox-els and updating existing voxels. Our full model is trained from end to end, learning to reconstruct radiance fields with arbitrary scene scales from an arbitrary number of input im-ages. We show that our direct network output can already render high-quality images; moreover, our neural field can be effectively fine-tuned by optimizing the predicted voxel features per scene in a short period to achieve better render-ing quality (see Fig. 1 and 4).
We train our full framework from end to end with only rendering losses on a combination of scenes from the Scan-Net [11], DTU [16], and Google Scanned Object [34] datasets. These datasets contain a large variety of differ-ent objects and scenes, allowing for our method to work properly with any scene scale. We demonstrate, on vari-ous datasets, that our approach performs better than prior arts, including IBRNet [41] that also designs networks that generalize across scenes. Especially on large-scale indoor scenes, our results from real-time direct network inference can even be on par with NeRF’s results from long per-scene optimization. Moreover, after only one hour of per-scene fine-tuning, our quality can be further boosted to the state-of-the-art, outperforming NeRF [27] and NVSF [24] that require much longer per-scene optimization times. Our ap-proach significantly improves the efficiency and scalability of radiance field reconstruction. We believe this is an impor-tant step towards making neural scene reconstruction and rendering practical. 2.