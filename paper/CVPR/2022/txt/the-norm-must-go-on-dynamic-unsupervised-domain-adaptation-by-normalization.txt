Abstract
Domain adaptation is crucial to adapt a learned model to new scenarios, such as domain shifts or changing data distributions. Current approaches usually require a large amount of labeled or unlabeled data from the shifted do-main. This can be a hurdle in fields which require continu-ous dynamic adaptation or suffer from scarcity of data, e.g. autonomous driving in challenging weather conditions. To address this problem of continuous adaptation to distribu-tion shifts, we propose Dynamic Unsupervised Adaptation (DUA). By continuously adapting the statistics of the batch normalization layers we modify the feature representations of the model. We show that by sequentially adapting a model with only a fraction of unlabeled data, a strong per-formance gain can be achieved. With even less than 1% of unlabeled data from the target domain, DUA already achieves competitive results to strong baselines. In addi-tion, the computational overhead is minimal in contrast to previous approaches. Our approach is simple, yet effective and can be applied to any architecture which uses batch normalization as one of its components. We show the utility of DUA by evaluating it on a variety of domain adaptation datasets and tasks including object recognition, digit recog-nition and object detection. 1.

Introduction
Present day Deep Neural Networks (DNNs) show promising results when both training and testing data be-long to the same distribution [16, 26, 68]. However, if there is a domain shift, i.e. when the testing data comes from a different domain, neural networks struggle to general-ize [4, 10, 38]. In fact, even if there is only a slight distribu-tion shift, the performance of neural networks is reported to already degrade significantly [18, 47].
One way to overcome the performance drop during do-main shifts is to obtain labeled data from the shifted domain (a) CIFAR-10C results. (b) Detections in foggy weather without (top) and with (bottom) DUA.
We overlay the detection results (blue) and the ground truth (orange).
Figure 1. Exemplary DUA results. a) Mean classification error over 15 different corruption types (at the most severe level 5) on
CIFAR-10C [18]. We outperform the state-of-the-art NORM [42, 54] and TTT [60], while using less than 1% of unlabeled data from the corrupted test set only. Our proposed adaptive momentum scheme leads to both fast and stable improvements in contrast to fixing the momentum parameter ρ. b) Qualitative results for object detection in degrading weather conditions: our DUA (bottom) sig-nificantly improves the performance of a KITTI [11] pre-trained
YOLOv3 [48] on KITTI-Fog [14]. Best viewed in color. and re-train the network. However, manual labeling of large amounts of data imposes significant human and monetary costs. These issues are addressed by Unsupervised Domain
Adaptation (UDA) approaches, e.g. [4, 10, 12, 17, 24, 33, 34,
52, 67, 75]. For UDA, the goal is to modify the network parameters in such a way that it can adapt in an unsuper-vised manner to out-of-distribution testing data. Tradition-ally, these approaches require labeled training data along with a large amount of unlabeled testing data.
In many practical scenarios, the traditional requirements, i.e. access to both labeled training and large amounts of un-labeled testing data can often not be fulfilled. For exam-ple, in the medical domain, pre-trained models are often provided without access to the training data (which is kept private due to privacy regulations). Likewise, some applica-tion domains benefit from dynamic adaptation to a chang-ing environment. For example, consider object detectors for autonomous vehicles, which are usually trained on mostly clear weather images, e.g. [11,15,58]. In real-world scenar-ios, however, weather can suddenly deteriorate, resulting in significant performance degradation [40, 41]. In such cases, it is not feasible to obtain labeled training data captured in degrading weather and re-train the detector from scratch. A better solution is to dynamically adapt the detector, given only a few (unlabeled) bad weather examples.
In this work, we highlight that one hindrance in domain generalization is the statistical difference in mean and vari-ance between train and (shifted) test data. Thus, during inference, we adapt the running mean and variance which are calculated during training by the batch normalization layer [21]. Moreover, we adapt the statistics dynamically in an online manner on a tiny fraction of test data. For adapta-tion, we form a small batch by augmenting each incoming sample. In order to ensure stable adaptation and fast con-vergence we propose an adaptive update schema.
Related approaches [28, 42, 54, 63] typically ignore the training statistics and recalculate the batch statistics from scratch for the test data. This, however, requires large batches of test data. We argue that a large batch of test data might not always be available in real-world applica-tions, e.g. autonomous cars adapting to challenging weather (see Figure 1). We show that a strong performance gain can be achieved by adapting the running mean and variance in an online manner (one sample at a time). In particular, we require only a small number of sequential samples from the out-of-distribution data.
Our contributions can be summarized as follows:
• We show that online adaptation of batch normaliza-tion parameters on a tiny fraction of unlabeled out-of-distribution test data can provide a strong performance gain. With even less than 1% of unlabeled test data,
DUA already performs competitively to strong base-lines which use the entire test set for adaptation.
• DUA is simple, unsupervised, dynamic and requires no back propagation [50] to work. Since the computa-tional overhead is also negligible, it is perfectly suited for real-time applications.
• We evaluate DUA on a variety of domain shift bench-marks, demonstrating its beneficial performance. We achieve state-of-the-art results on most benchmarks while being competitive on the remaining.
• We show that our dynamic adaptation method works on a variety of different tasks and different architec-tures. To the best of our knowledge, we are the first to show dynamic adaptation for object detection. 2.