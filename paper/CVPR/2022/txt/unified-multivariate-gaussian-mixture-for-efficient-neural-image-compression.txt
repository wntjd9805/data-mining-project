Abstract
Modeling latent variables with priors and hyperpriors is an essential problem in variational image compression.
Formally, trade-off between rate and distortion is handled well if priors and hyperpriors precisely describe latent vari-ables. Current practices only adopt univariate priors and process each variable individually. However, we ﬁnd inter-correlations and intra-correlations exist when observing la-tent variables in a vectorized perspective. These ﬁndings reveal visual redundancies to improve rate-distortion per-formance and parallel processing ability to speed up com-pression. This encourages us to propose a novel vectorized prior. Speciﬁcally, a multivariate Gaussian mixture is pro-posed with means and covariances to be estimated. Then, a novel probabilistic vector quantization is utilized to effec-tively approximate means, and remaining covariances are further induced to a uniﬁed mixture and solved by cascaded estimation without context models involved. Furthermore, codebooks involved in quantization are extended to multi-codebooks for complexity reduction, which formulates an efﬁcient compression procedure. Extensive experiments on benchmark datasets against state-of-the-art indicate our model has better rate-distortion performance and an im-pressive 3.18× compression speed up, giving us the ability to perform real-time, high-quality variational image com-pression in practice. Our source code is publicly available at https://github.com/xiaosu-zhu/McQuic. 1.

Introduction
As a crucial technique in image processing, lossy im-age compression has been studied for an extended pe-riod [17, 20, 30, 38]. The goal is to achieve high perceptual reconstruction performance, extreme compression rate, and efﬁcient processing pipeline. Classical lossy image com-pression standards, e.g., JPEG [33,39], BPG [6], HEIF [35],
VVC [8], have been widely applied and adopted as fun-*Corresponding author. (a) y1 (b) y2 = (cid:2) y1 − y1 (cid:3)
↓
Figure 1. UMAP [24] projection of 128-d latent vectors with a toy 2-level 32-codeword model from 24 Kodak images. Left: Latent vectors extracted from analysis transform are correlated and can be described by multivariate Gaussian mixture. Right: Next level’s latents are under similar distribution. damental components in almost all image processing soft-ware. However, the explosion of multimedia content in the digital era still raises urgent requests to ﬁnd an effective and efﬁcient compressor to tackle storage costs.
Distinct from the above traditional codecs, learnable neural image compression is proposed by exploiting advan-tages of deep neural networks. It adopts neural networks as nonlinear transforms to extract binaries from images and restore them, while essential research problem is to han-dle the trade-off between rate and distortion [7]. Recent studies propose variational image compression and arrange above trade-off as a Lagrange multiplier for joint optimiza-tion [3, 4, 9, 25, 26]. They introduce univariate priors and hyperpriors to describe latent variables and make a break-through to control rate. We summarize advances in this task as a series of operational diagrams in Figs. 2(a) to 2(c).
To design an effective compressor in variational image compression, an appropriate prior that precisely describes quantized latent variables is needed [4, 9, 26]. Fig. 1(a) demonstrates observation of latent variables grouped by channels. This vectorized perspective reveals correlations of latents that help us to ﬁnd a prior. Note that latent vec-tor comes from a speciﬁc region of an image and repre-sents this region’s visual appearance, correlations between
(cid:2208) (cid:2207) (cid:1860)(cid:3028) (cid:1859)(cid:3028) (cid:2206) (cid:24) (cid:513) (cid:20) (cid:1827) (cid:3556)(cid:2208) (cid:513) (cid:3548)(cid:2208) (cid:1860)(cid:3046) (cid:1868) (cid:3549)(cid:2207) (cid:817)(cid:1840) (cid:2246)(cid:481) (cid:2252)(cid:2870) (cid:24) (cid:513) (cid:20) (cid:3557)(cid:2207) (cid:513) (cid:3549)(cid:2207) (cid:1859)(cid:3046) (cid:3557)(cid:2206) (cid:513) (cid:3549)(cid:2206) (cid:2208) (cid:2207) (cid:1860)(cid:3028) (cid:1859)(cid:3028) (cid:2206) (cid:24) (cid:513) (cid:20) (cid:1827) (cid:3556)(cid:2208) (cid:513) (cid:3548)(cid:2208) (cid:1860)(cid:3046) (cid:1868) (cid:3549)(cid:2207) (cid:817)(cid:557)(cid:3038)(cid:2205) (cid:3038) (cid:1840) (cid:2246) (cid:3038) (cid:481) (cid:2252)(cid:2870) (cid:3038) (cid:24) (cid:513) (cid:20) (cid:3557)(cid:2207) (cid:513) (cid:3549)(cid:2207) (cid:1859)(cid:3046) (cid:3557)(cid:2206) (cid:513) (cid:3549)(cid:2206) (b) Hyperprior [4, 26, 29]. (c) Discretized Gaussians [9]. (cid:2737)(cid:3013)(cid:2879)(cid:2869) (cid:3013) (cid:20) (cid:482) (cid:2159)(cid:3012) (cid:3557)(cid:2207)(cid:3013) (cid:2737)(cid:2869) (cid:2870) (cid:20) (cid:482) (cid:2159)(cid:3012) (cid:2869) (cid:20) (cid:482) (cid:2159)(cid:3012) (cid:2207) (cid:1859)(cid:3028) (cid:3557)(cid:2207)(cid:2870) (cid:1868) (cid:3557)(cid:2207)(cid:954) (cid:817)(cid:557)(cid:3038)(cid:2230)(cid:3038)(cid:1840) (cid:2159)(cid:3038) (cid:3557)(cid:2207) (cid:954) (cid:481) (cid:3557)(cid:2207)(cid:954)(cid:2878)(cid:2778)(cid:2779) (cid:1859)(cid:3046) (cid:3557)(cid:2206) (cid:2206) (d) Vectorized prior. (cid:2207) (cid:1859)(cid:3028) (cid:24) (cid:513) (cid:20) (cid:3557)(cid:2207) (cid:513) (cid:3549)(cid:2207) (cid:1859)(cid:3046) (cid:2206) (cid:3557)(cid:2206) (cid:513) (cid:3549)(cid:2206) (a) Factorized prior [3].
Figure 2. Operational diagrams of different methods. We generalize prior as a uniﬁed multivariate Gaussian mixture. vectors can be summarized as inter-correlation and intra-Inter-correlation comes from facts that im-correlation. ages have spatial redundancy [29] i.e. vectors extracted from visually-similar regions or patches are closed together.
Meanwhile, similar regions still have differences in details, resulting in intra-correlation i.e. covariances. Two proper-ties guide us to ﬁnd a vectorized prior which could describe two correlations by means and covariances.
Univariate priors previous works adopt may not be sufﬁ-cient to describe above observations, because they process each scalar value individually and lack a whole view over vectors. In other words, adopting a vectorized prior mainly has two impacts. Firstly, it treats latents as vectors along channels other than scalars, helping to summarize inter- and intra-correlations. Secondly, vectorized processing has the potential to speed up compression procedure. Therefore in this paper, we propose a novel vectorized prior for varia-tional image compression. Speciﬁcally, a uniﬁed multivari-ate Gaussian mixture is proposed to describe latents. Then, a probabilistic vector quantization with cascaded estima-tion is designed to effectively and efﬁciently estimate means and covariances without context models involved. Multi-codebooks are further incorporated into quantization to re-duce complexity and enable ﬂexible rate control. The whole procedure is demonstrated in Fig. 2(d) and our contribution is summarized below: 1. We propose a new vectorized perspective for vari-ational image compression. Unlike previous works, ours considers correlations between latent vectors and formu-lates a uniﬁed multivariate Gaussian mixture. We further propose a probabilistic vector quantization with cascaded estimation to estimate means and covariances. 2. A multi-codebook structure is further incorporated into quantization to reduce complexity and enable ﬂexible rate control. Overall framework is able to perform effective and efﬁcient compression with the help of vectorized prior. 3. Extensive experiments on benchmark datasets re-veal impacts of vectorized prior. Compared to state-of-the-art, our method achieves better rate-distortion performance with an impressive 3.18× speed up for compression latency.
These results reveal possibility to provide practical varia-tional image compression with vectorized prior. 2.