Abstract
Neural network quantization aims at reducing bit-widths of weights and activations for memory and computational efﬁciency. Since a linear quantizer (i.e., round(·) function) cannot well ﬁt the bell-shaped distributions of weights and activations, many existing methods use pre-deﬁned func-tions (e.g., exponential function) with learnable parame-ters to build the quantizer for joint optimization. How-ever, these complicated quantizers introduce considerable computational overhead during inference since activation quantization should be conducted online. In this paper, we formulate the quantization process as a simple lookup op-eration and propose to learn lookup tables as quantizers.
Speciﬁcally, we develop differentiable lookup tables and in-troduce several training strategies for optimization. Our lookup tables can be trained with the network in an end-to-end manner to ﬁt the distributions in different layers and have very small additional computational cost. Compari-son with previous methods show that quantized networks us-ing our lookup tables achieve state-of-the-art performance on image classiﬁcation, image super-resolution, and point cloud classiﬁcation tasks. 1.

Introduction
Deep neural networks have achieved huge success in computer vision, natural language processing and many other ﬁelds. However, high computational cost and mem-ory footprint limit their applications on edge devices. Many efforts have been made to address this problem, includ-ing efﬁcient architecture designs [1–3], network pruning
[4–6], weight decomposition [7, 8], knowledge distillation
[9–11], and network quantization [12–14]. Among these approaches, network quantization can signiﬁcantly reduce the computational cost and is widely applied in real-world applications (e.g., inference framework like TF-lite [15]).
Neural network quantization aims at obtaining low-bit networks to reduce memory footprint and computational cost. The decrease in bit-width naturally introduces quanti-zation errors, which in turn leads to accuracy loss. Network
Figure 1. An illustration of a linear quantizer and our learned lookup table (LUT) for 2-bit quantization of activations. quantization can be divided into uniform approaches and non-uniform approaches. In this paper, we focus on uni-form ones since they can be directly deployed on off-the-shelf hardwares with the support of integer arithmetic [16].
Early uniform quantization methods [16–19] use ﬁxed linear quantizers (i.e., round(·) function) to quantize ﬂoat values. However, these quantizers cannot ﬁt the bell-shaped and long-tailed distributions of weights and activations well
[20]. Moreover, the distributions in different layers can vary signiﬁcantly. Consequently, ﬁxed quantizers have limited capability to adapt to various layers.
To ﬁt various distributions of weights and activations in different layers, several works [14,21] adopt trainable quan-tizers for joint optimization. These methods use a combi-nation of pre-deﬁned functions (e.g., exponential function and sigmoid function) with learnable parameters to repre-sent the quantization function. Since activation quantization needs to be conducted online during inference, these com-plicated quantizers introduce considerable computational overhead.
In this paper, we propose to use learnable lookup tables (LLTs) to map ﬂoat values to quantized values for network quantization (Fig. 1). Speciﬁcally, the quantization function is formulated as a lookup table (Fig. 2(a)) and then trans-formed to one-hot distributions (Fig. 2(d)) for optimization.
During training, we soften one-hot distributions to temper-atured softmax distributions (Fig. 2(e)) to update both the lookup tables and the network in an end-to-end manner. In addition, we introduce several training strategies to promote the convergence of our lookup tables, including an exponen-tial formulation of scale parameters and a gradient rescaling scheme. As temperatured softmax distributions converge to one-hot distributions, our lookup tables learn an adap-tive mapping from continuous ﬂoat values to the quantiza-tion levels. During inference, online activation quantiza-tion can be achieved by a simple lookup operation with very small computational cost. Extensive experiments on image classiﬁcation, image super-resolution (SR), and point cloud classiﬁcation tasks demonstrate the state-of-the-art perfor-mance of our method. 2.