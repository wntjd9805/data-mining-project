Abstract
Pseudo-label-based semi-supervised learning (SSL) has achieved great success on raw data utilization. However, its training procedure suffers from confirmation bias due to the noise contained in self-generated artificial labels. Moreover, the model’s judgment becomes noisier in real-world applications with extensive out-of-distribution data. To address this issue, we propose a general method named Class-aware Contrastive Semi-Supervised Learn-ing (CCSSL), which is a drop-in helper to improve the pseudo-label quality and enhance the model’s robustness in the real-world setting. Rather than treating real-world data as a union set, our method separately handles reliable in-distribution data with class-wise clustering for blending into downstream tasks and noisy out-of-distribution data with image-wise contrastive for better generalization. Fur-thermore, by applying target re-weighting, we successfully emphasize clean label learning and simultaneously reduce noisy label learning. Despite its simplicity, our proposed
CCSSL has significant performance improvements over the state-of-the-art SSL methods on the standard datasets
CIFAR100 [18] and STL10 [8]. On the real-world dataset
Semi-iNat 2021 [27], we improve FixMatch [25] by 9.80% and CoMatch [19] by 3.18%. Code is available https://github.com/TencentYoutuResearch/Classification-SemiCLS. 1.

Introduction
Raw data utilization is becoming a research focal point for its high accessibility and high affordability. The def-inition of raw data is the union set of in-distribution data (known classes and balanced distribution) and out-of-distribution data [32] (unknown classes or unbalanced dis-tribution), as shown in Fig. 1. For in-distribution datasets, semi-supervised learning (SSL) has achieved excellent per-formance with the help of pseudo labeling [25] [3] [2] [13].
The primary process for the pseudo-label based SSL is it-*Equal contribution.
†Corresponding author. (a) Real-World Data With In-Distribution and Out-of-Distribution Data (b) Pseudo-Label-Based SSL (c) Class-Aware Contrastive SSL (ours)
Figure 1.
Intuition graph for class-aware contrastive semi-supervised learning. (a) represents real-world unlabeled data con-taining in-distribution and out-of-distribution data (unbalanced distribution or unknown classes). Unlike pseudo-label-based semi-supervised learning in (b), which wrongly clusters noisy out-of-distribution data, CCSSL in (c) reduces noise by image-level contrastive learning on out-of-distribution data while maintaining the class-aware clustering ability for in-distribution data. erative training: 1) creating self-generated pseudo labels on raw data, 2) training on pseudo labels, 3) repeating 1 and 2. The underlying assumption for training on pseudo labels is that the distribution of the labeled data is close to the unlabeled, and the unlabeled dataset does not con-tain any novel categories. This assumption often does not hold in the real-world applications with extensive out-of-distribution data, which contains unbalanced distribution or unknown classes. SSL’s training and labeling loop collapses in the real-world because of the enormous noise introduced by pseudo labeling on out-of-distribution data. The confir-mation bias [1] induced by noisy pseudo labels deteriorates
SSL performance by a large margin [26].
SSL has used many techniques to alleviate the confirma-tion bias and achieved state-of-the-art results on standard in-distribution benchmarks, like CIFAR [18] and STL10 [8].
Some methods achieve this goal by using the model’s self-correcting capability. In [25] [3] [2] [25], a high confidence threshold is used to filter incorrectly pseudo-labeled data.
[34] [20] [29] use a self-ema teacher to generate pseudo la-bels with the assumption that the weighted averaged model produces more stable and less noisy predictions. Some methods [28] [31] try to narrow the distribution gap be-tween raw data and labeled data by predictions’ uncertainty.
However, justifying a model’s prediction by its output with-out introducing other information still suffers from confir-mation bias [21]. Especially on the real-world data with unbalanced distribution or unknown classes, the effect of a model’s self-correcting is facing a huge challenge. By ex-plicitly introducing contrastive information, our CCSSL al-leviates the confirmation bias in the feature space and shows great de-noise ability on the real-world data.
Another trend is using prior or posterior information to co-rectify a model’s predictions. Both [11] [5] use a pre-trained model as a universal prior information. In [21], by knowledge embedding graph, semantic information is used to regularize feature learning. Although the above meth-ods are proved helpful, fixed prior is hard to be blended into downstream tasks, resulting in inferior performance in prac-tice. [19] utilizes posterior information by constructing a prediction graph and introducing contrastive learning from self-supervised learning (Self-SL). However, directly com-bining image-level feature repulsion interferes with SSL’s class-clustering ability. To solve this conflict, in our pro-posed CCSSL, a class-aware contrastive module is specifi-cally explored for reliable in-distribution data clustering and noisy out-of-distribution data contrasting to better integrate with downstream classification tasks.
To this end, we propose a class-aware contrastive semi-supervised learning (CCSSL) method. CCSSL consists of a semi-supervised module and a class-aware contrastive module. Any end-to-end pseudo-label-based SSL can re-place the semi-supervised module to benefit from the class-aware contrastive module’s confirmation bias alleviation ability. Unlike self-correcting methods that try to allevi-ate noise by model’s own output, CCSSL regularizes the training process by introducing information in the feature space. As shown in the Fig. 1, CCSSL constructs the fea-ture space with high dimensional vectors from two strong augmented views. Rather than directly combining con-trastive learning, CCSSL uses class-aware clustering on in-distribution data to maintain SSL’s clustering ability and image-level contrasting on out-of-distribution data for noise alleviation. Furthermore, to reduce the implicit noise in-troduced from the class-aware clustering, we incorporate a target re-weighting module that emphasizes learning on high probable in-distribution samples and reduces the ef-fect of uncertain noisy samples. We found that with the help of the re-weighting module, the prediction confidence of learned models is robust enough for roughly discriminat-ing in-distribution and out-of-distribution data.
Our contributions can be described in three folds:
• We propose a novel SSL learning method CCSSL, which takes advantage of both SSL’s effective learn-ing and Self-SL’s noise alleviation ability by class-wise clustering on in-distribution data and image-wise con-trasting on out-of-distribution data.
• CCSSL is a drop-in helper that can improve upon any end-to-end pseudo-label-based SSL method for confir-mation bias alleviation and faster convergence. CC-SSL’s noise alleviation capability makes SSL methods more practical in the real-world setting.
• We thoroughly analyzed the effect of CCSSL with vari-ous state-of-the-art SSL methods on both in-distribution data and real-world data. By simply combining CCSSL, current state-of-the-art SSL methods can be further im-proved. 2.