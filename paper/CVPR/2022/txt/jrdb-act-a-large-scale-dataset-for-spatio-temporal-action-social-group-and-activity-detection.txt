Abstract
The availability of large-scale video action understand-ing datasets has facilitated advances in the interpretation of visual scenes containing people. However, learning to recognise human actions and their social interactions in an unconstrained real-world environment comprising numer-ous people, with potentially highly unbalanced and long-tailed distributed action labels from a stream of sensory data captured from a mobile robot platform remains a sig-nificant challenge, not least owing to the lack of a reflective large-scale dataset. In this paper, we introduce JRDB-Act, as an extension of the existing JRDB, which is captured by a social mobile manipulator and reflects a real distribution of human daily-life actions in a university campus environ-ment. JRDB-Act has been densely annotated with atomic actions, comprises over 2.8M action labels, constituting a large-scale spatio-temporal action detection dataset. Each human bounding box is labeled with one pose-based action label and multiple (optional) interaction-based action la-bels. Moreover JRDB-Act provides social group annota-tion, conducive to the task of grouping individuals based on their interactions in the scene to infer their social ac-tivities (common activities in each social group). Each an-notated label in JRDB-Act is tagged with the annotators’ confidence level which contributes to the development of re-liable evaluation strategies. In order to demonstrate how one can effectively utilise such annotations, we develop an end-to-end trainable pipeline to learn and infer these tasks, i.e. individual action and social group detection. The data and the evaluation code will be publicly available at https://jrdb.erc.monash.edu/. 1.

Introduction
Understanding and predicting human actions and inten-tions are essential tasks in tackling many real-world prob-lems such as autonomous driving, robot navigation safety,
*Work done while at the Australian National University (ANU).
Figure 1. An illustration of a single frame of the JRDB-Act dataset. As shown, the data captured with a 2D and 3D multi-modal sensory platform is accompanied with a new set of anno-tations including individual actions and social group formation leading to infer social activities (common activities in each social group) to further complement the 2D and 3D detection and track-ing annotation in the JRDB. human-robot interaction, and detection of perilous behav-iors in surveillance systems. Developing an AI model per-forming these tasks is challenging due to the high variations of human actions in an unconstrained real-world environ-ment. Moreover, dealing with daily actions which resem-bles a highly unbalanced, long-tailed distribution poses new challenges for many existing approaches.
Recently, great progress has been made to create large-scale video datasets for human activity understanding [3, 9, 20, 26, 45]. While these popular datasets have contributed significantly to the recent advances in human activity un-derstanding from visual data, their primary application is not targeting robotics domain and therefore rarely reflect the challenges in problems such as human-robot interaction and robot navigation in human crowded environments, e.g.
shopping malls, university campus, etc. Such environments include not only many individuals, but also often groups of people connected to each other through some form of inter-action, e.g. engaging in common activities or goals, which form the concept of social groups and activities. Moreover, in many robotics problem, e.g. for safe navigation and colli-sion risk prediction in human environments, it is essential to anticipate every individual’s action and intention way ahead of time, considering their social interactions. To this end, the availability of a spatio-temporally dense annotated hu-man action data is indispensable for the development and evaluation of a robotic perception system.
With this motivation, we introduce JRDB-Act, a large-scale dataset captured from a mobile robot platform, containing dense spatio-temporal individual action and social group annotation. JRDB-Act is an extension of the recently in-troduced JRDB [35,43]. We now elaborate the unique char-acteristics of JRDB-Act and our proposed method.
New Annotations. We provide a set of atomic action la-bels for each person at each frame from the three categories of human pose, human-human, and human-object interac-tions, as shown in Fig. 1. Our action vocabulary contains common daily human actions including 11 human pose, 3 human-human, and 12 human-object interaction classes.
Since these action labels are densely annotated over space and time, JRDB-Act contains over 2.8M action labels, mak-ing it one of the large-scale spatio-temporal action datasets publicly available. Furthermore, the dataset provides new unique annotations, i.e. social group labels, by assigning a group ID to each person in each frame such that individuals with the same ID represent a social group. We further pro-vide social activity annotation for each group by inferring it from the annotated individual actions and social groups.
Another novel aspect of JRDB-Act is the difficulty level an-notation, e.g. easy, moderate, and difficult, for each anno-tated label which reflects the confidence level of annotators for the corresponding label. The provided difficulty level can be conducive to more reliable evaluation paradigms.
Unique Challenges. The sequences in JRDB-Act are cap-tured from human daily-life in different indoor and outdoor places of a university campus as an unconstrained environ-ment [35] by a mobile robotic platform. Thus, they reflect the highly unbalanced distribution of human actions in real-world scenarios. Moreover, the sequences naturally include diverse levels of human population density. The average number of people per frame in JRDB-Act is 30, which is significantly higher than most popular action datasets. Fur-ther, the robot motion and the perspective view of the cap-tured sequences makes this dataset challenging. Consid-ering the aforementioned compelling attributes, dense an-notations, and natural complexities, JRDB-Act introduces means to study new problems and challenges in human un-derstanding for computer vision and robotics community.
Our Proposed Method. In order to showcase the potential research directions and challenges required to be tackled in
JRDB-Act, we develop an end-to-end trainable pipeline for both individual action and social group detection tasks. Our method uses the panoramic video clips as input and adopts a similar backbone as [13] to extract spatio-temporal individ-ual features. However, we fuse additional pair-wise geomet-rical features and incorporate a novel eigenvalue-based loss function to improve the social group detection performance compared to [13]. We also suggest a simple, yet effective strategy to handle the unbalanced nature of action labels by partitioning and balancing action loss functions based on the occurring frequency of action classes in the dataset. 2.