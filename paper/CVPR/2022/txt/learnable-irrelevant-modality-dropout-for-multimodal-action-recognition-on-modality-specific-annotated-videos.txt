Abstract
With the assumption that a video dataset is multimodal-ity annotated in which auditory and visual modalities both are labeled or class-relevant, current multimodal methods apply modality fusion or cross-modality attention. How-ever, effectively leveraging the audio modality in vision-specific annotated videos for action recognition is of par-ticular challenge.
To tackle this challenge, we propose a novel audio-visual framework that effectively leverages the audio modality in any solely vision-specific annotated dataset. We adopt the language models (e.g., BERT) to build a semantic audio-video label dictionary (SAVLD) that maps each video la-bel to its most K-relevant audio labels in which SAVLD serves as a bridge between audio and video datasets. Then,
SAVLD along with a pretrained audio multi-label model are used to estimate the audio-visual modality relevance dur-ing the training phase. Accordingly, a novel learnable ir-relevant modality dropout (IMD) is proposed to completely drop out the irrelevant audio modality and fuse only the rel-evant modalities. Moreover, we present a new two-stream video Transformer for efficiently modeling the visual modal-ities. Results on several vision-specific annotated datasets including Kinetics400 and UCF-101 validated our frame-work as it outperforms most relevant action recognition methods. 1.

Introduction
One of the deep neural network (DNN) learning schemes to improve video understanding is to leverage as many input modalities as available, such as audio, RGB frames, motion, textual data, the visible text on videos, and human skeleton joints. Therefore, multimodal learning has shown a remark-able improvement in video-based action recognition. These methods process either each modality with an independent
∗Corresponding author
Figure 1. Conceptual Overview: Vision-Audio Label Mapping.
In an overlapped manner, our method performs a cross-dataset textual labels mapping in which audio labels are mapped to their most closely relevant video-based human activity labels by using language models, e.g., BERT. The resulting clusters compose a
SAVLD dictionary that serves as a bridge between video and audio datasets. Since our framework trains human activity multimodal models on vision-specific datasets, we use SAVLD to leverage the auditory modality in videos.
DNN or all with a single shared DNN. The modalities or their feature representations are fused either in an early fu-sion, middle fusion, or late fusion manner [1–3]. Another fusion scheme is proposed by adopting cross-modality at-tention or gated units [4–7].
Motivation. Most of the attention and fusion methods
[2] boost the audio-visual models’ performance on audio-vision correspondence-based videos in which auditory and visual modalities are corresponding or at least relevant as in
Kinetics-Sounds [8] and EPIC-Kitchen dataset [9]. How-ever, these fusion methods do not provide similar perfor-mance boost on the vision-specific annotated datasets, such as Kinetics400 [10] and UCF-101 [11]. This condition
is due to the high irrelevance and non-correspondence be-tween visual and auditory modalities as concluded in [1]. In this case, multimodal methods do not effectively gain max-imum benefit from unlabeld audio modality with noisy rel-evance on vision-specific datasets. For example, music can largely indicate that the human activity is dancing. How-ever, music audio can be associated with several other hu-man activities, such as car driving, or simply can be found as background audio in any edited video. This interprets the findings in an interesting study [7], which showed that unimodal models consistently outperform multimodal DNN in modality-specific datasets because multimodal networks are more prone to overfitting with their increase in capacity.
This finding motivated us to look for a method that lever-ages the relevant audio modality while completely dropping out the irrelevant modality in training and inference phases.
Contributions. To tackle the aforementioned challenge, we present a novel multimodal training framework that trains action recognition networks with the best audio-visual modality combination on visual modality-specific datasets.
In our method, we automatically estimate the audio-visual modalities relevance by leveraging the pre-trained audio classification models on large audio-specific datasets such as AudioSet [12] and VGGSound [13]. This task is effectively achieved in two phases.
In the first phase, we map audio labels of an audio-specific dataset to their most semantically relevant labels in the vision-specific dataset. This process is achieved using the language pre-trained models, e.g., BERT [14] or GloVe [15]. After ob-taining the semantic sentence-based embedding of each la-bel, we perform cross-dataset label matching and generate overlapped label clusters each of which contains a single label from the video dataset along with a set of its most semantically similar labels in the audio dataset as shown in Fig. 1. For example, Dancing label in Kinetics400 and Music, Singing, and Clapping labels in AudioSet are in the same cluster. Overall, the resulting overlapped clusters compose a semantic audio-video label dictionary (SAVLD) that serves as a bridge between video and audio datasets for the training supervision in our next phase. In the second phase, as a transfer learning task, our framework adopts a pretrained audio Transformer to generate highly semantic audio features and multi-label audio predictions.
The audio dataset used for building the SAVLD should be the same dataset on which the audio Transformer has been trained. Overall, the outputs of the audio Transformer and the SAVLD are then used by our proposed framework to annotate audio, as depicted in Fig.2.
After obtaining the audio modality predictions using any audio pretrained model (e.g., AST), our framework guided by the SAVLD drops the irrelevant audio modality. This is performed by using a novel trainable irrelevant modality dropout (IMD) that consists of two main modules: The first module is a neural network termed relevance network (RN) that receives auditory and visual modalities and decides whether they are relevant. The output of this network is sig-moidal predictions representing the relevance level that is used by the second module to decide whether to fuse the two modalities for the final video classification or to drop the au-dio modality. We further improve our framework learning by proposing a new intra-class cross-modality augmenta-tion in which it randomly pairs auditory and visual modali-ties of the same class from different videos. This augmen-tation method may show a negative effect on some applica-tions of audio-visual networks when the audio and video are required to be corresponding as the case of speech recogni-tion. However, in our case, the audio modality is required to be relevant but is not necessarily to be aligned or accurately correspond to the visual modality because we tackle the problem of human activity recognition in this work. This motivates us to propose this augmentation method that em-pirically provides a reasonable performance boost.
Generally, our framework is an entirely convolution-free
Transformer-based network. It leverages three video modal-ities: RGB frames, optical flow, and audio. The RGB and optical flow modalities are processed by using our efficient two-stream video Transformer, whereas the audio modality is processed by any audio pretrained Transformer.
The key contributions of this work are as follows:
• A novel multimodal human activity recognition frame-work that leverages the power of the NLP BERT model and the pretrained audio classification models is pro-posed to automatically annotate audio modality. It ef-fectively trains audio-visual action recognition models on any vision-specific annotated dataset.
• A novel learnable IMD network is proposed to com-pletely drop out the irrelevant audio modality, whereas the relevant modalities are fused on the basis of their relevance level.
• An efficient two-stream video Transformer is designed to learn the visual modality with few parameters as compared to the relevant video Transformers.
• An intra-class cross-modality augmentation method is proposed for generating more training samples by al-lowing each audio modality sample to be paired with any visual modality sample among the same class. 2.