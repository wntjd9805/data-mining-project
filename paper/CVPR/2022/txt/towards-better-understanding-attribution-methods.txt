Abstract 1.

Introduction
Deep neural networks are very successful on many vision tasks, but hard to interpret due to their black box nature. To overcome this, various post-hoc attribution methods have been proposed to identify image regions most influential to the models’ decisions. Evaluating such methods is chal-lenging since no ground truth attributions exist. We thus propose three novel evaluation schemes to more reliably measure the faithfulness of those methods, to make compar-isons between them more fair, and to make visual inspec-tion more systematic. To address faithfulness, we propose a novel evaluation setting (DiFull) in which we carefully control which parts of the input can influence the output in order to distinguish possible from impossible attributions.
To address fairness, we note that different methods are ap-plied at different layers, which skews any comparison, and so evaluate all methods on the same layers (ML-Att) and discuss how this impacts their performance on quantita-tive metrics. For more systematic visualizations, we pro-pose a scheme (AggAtt) to qualitatively evaluate the meth-ods on complete datasets. We use these evaluation schemes to study strengths and shortcomings of some widely used attribution methods. Finally, we propose a post-processing smoothing step that significantly improves the performance of some attribution methods, and discuss its applicability.
Deep neural networks (DNNs) are highly successful on many computer vision tasks. However, their black box na-ture makes it hard to interpret and thus trust their decisions.
To shed light on the models’ decision-making process, sev-eral methods have been proposed that aim to attribute im-portance values to individual input features (see Sec. 2).
However, given the lack of ground truth importance values, it has proven difficult to compare and evaluate these attri-bution methods in a holistic and systematic manner.
In this work, we take a three-pronged approach towards addressing this issue. In particular, we focus on three im-portant components for such evaluations: reliably measur-ing the methods’ model-faithfulness, ensuring a fair com-parison between methods, and providing a framework that allows for systematic visual inspections of their attributions.
First, we propose an evaluation scheme (DiFull), which allows distinguishing possible from impossible importance attributions. This effectively provides ground truth annota-tions for whether or not an input feature can possibly have influenced the model output. As such, it can highlight dis-tinct failure modes of attribution methods (Fig. 1, left).
Second, a fair evaluation requires attribution methods to be compared on equal footing. However, we observe that different methods explain DNNs to different depths (e.g., full network or classification head only). Thus, some meth-ods in fact solve a much easier problem (i.e., explain a much
shallower network). To even the playing field, we propose a multi-layer evaluation scheme for attributions (ML-Att) and provide a thorough evaluation of commonly used meth-ods across multiple layers and models (Fig. 1, left). When compared on the same level, we find that performance dif-ferences between some methods essentially vanish.
Third, relying on individual examples for a qualitative comparison is prone to skew the comparison and cannot fully represent the evaluated attribution methods. To over-come this, we propose a qualitative evaluation scheme for which we aggregate attribution maps (AggAtt) across many input samples. This allows us to observe trends in the per-formance of attribution methods across complete datasets, in addition to looking at individual examples (Fig. 1, right). (1) We propose a novel evaluation set-Contributions. in which we control which regions can-ting, DiFull, not possibly influence a model’s output, which allows us to highlight definite failure modes of attribution methods. (2) We argue that methods can only be compared fairly when evaluated on the same layer. To do this, we in-troduce ML-Att and evaluate all attribution methods at multiple layers. We show that, when compared fairly, apparent performance differences between some methods (3) We propose a novel aggregation effectively vanish. method, AggAtt, to qualitatively evaluate attribution meth-ods across all images in a dataset. This allows to quali-tatively assess a method’s performance across many sam-ples (Fig. 1, right), which complements the evaluation on individual samples. (4) We propose a post-processing smoothing step that significantly improves localization performance on some attribution methods. We observe significant differences when evaluating these smoothed attributions on different architectures, which highlights how architectural design choices can influence an attri-bution method’s applicability. Our code is available at https://github.com/sukrutrao/Attribution-Evaluation. 2.