Abstract
Text spotting end-to-end methods have recently gained attention in the literature due to the benefits of jointly opti-mizing the text detection and recognition components. Ex-isting methods usually have a distinct separation between the detection and recognition branches, requiring exact an-notations for the two tasks. We introduce TextTranSpotter (TTS), a transformer-based approach for text spotting and the first text spotting framework which may be trained with both fully- and weakly-supervised settings. By learning a single latent representation per word detection, and using a novel loss function based on the Hungarian loss, our method alleviates the need for expensive localization an-notations. Trained with only text transcription annotations on real data, our weakly-supervised method achieves com-petitive performance with previous state-of-the-art fully-supervised methods. When trained in a fully-supervised manner, TextTranSpotter shows state-of-the-art results on multiple benchmarks. 1.

Introduction
Text spotting, i.e., detecting and reading text in images, is a key capability for machines to operate in the real world.
Applications include vehicle navigation in buildings and cities, indexing of image collections and video, automated handling of packages, and prosthetics for blind and visually impaired people. This challenge was recognized early in the computer vision literature [6, 19, 40] and is currently under-going a deep learning revival [14,20], with most researchers focusing on two issues: architectures and data.
Early systems [3, 13] use separate architectures for text detection and recognition, without sharing any component.
More recent approaches take a leap forward towards a uni-fied end-to-end architecture by sharing a convolutional fea-ture backbone [5, 20] and employing a feature cropping mechanism to extract the relevant area of interest for the recognition head. Such architectures are still not ideal, since
Figure 1. Weakly-supervised text spotting. Top: Visualization of ’fully’ (left) and ’weakly’ (right) supervised ground-truth (GT) annotations. Bottom: Text spotting methods results on the Total-Text dataset (higher is better) vs. the time cost per word to annotate the datasets used for training (Sec. 4.5, lower is better). Even when using weaker annotations only, our method surpasses state-of-the-art fully-supervised methods. the recognition head is usually trained using the detection ground-truth and thus it is not optimized for the predictions of the detection head. Furthermore, the detection head is trained as a standard object detection model, without re-gard to the additional supervision given by the text tran-scription or to the downstream recognition task. Other than
mutually optimizing the backbone, the tasks are separate, requiring both transcription annotations for the recognition head, and polygons or bounding box annotations for the de-tection head. Recently, more sophisticated methods forgo the two-stage approach by directly localizing and classify-ing the characters in the text [1, 30], which further requires character-level annotations.
The datasets in the field of text spotting consist of syn-thetic and real data. Real data annotation is an expensive task, however relying solely on synthetic data leads to poor results. Most of the annotation time is dedicated to the detection ground-truth, while the transcription annotation alone requires less than half of the time, as discussed in Sec. 4.5. State-of-the-art-methods explicitly segment the text area, allowing the recognizer to cope with rotated, curved, or densely located text and ignore background noise [5,22].
A disadvantage of such methods is that they require expen-sive polygonal annotations [7, 16].
In this work, we suggest a new text spotting approach,
TextTranSpotter (TTS), which can forgo the expensive spa-tial annotations and use only transcript annotations for real data. This setting is weakly supervised, in the sense that only partial information about the text in the image is used for training. At inference time, the model outputs both the detection and the transcription of the text in the image. The weakly supervised setting has many use-cases, especially in situations where annotation resources are limited or there is an existing dataset with only text transcription annota-tions [15]. Furthermore, TTS can be trained in both a fully-or a weakly-supervised manner, thus allowing a trade-off between model performance and annotation cost (Fig. 1).
To allow the weakly-supervised setting, we depart from existing text spotting methods which treat text detection and recognition as related but independent tasks. Our approach includes a novel architecture and loss function which bet-ter entangle the two tasks, taking a step further towards a unified end-to-end system. TextTranSpotter takes advan-tage of recent developments in transformers [4, 10, 38] to create a multitask network (see Fig. 2), learning a single object query embedding for both detection and recognition heads. The task heads are very simple and lean; the detec-tion head is a linear feed-forward network and the recogni-tion head is a Recurrent Neural Network (RNN) [34]. This indicates that the majority of the computation is performed in the shared transformer, unlike most approaches which use more intricate recognition and detection networks (see supplementary for comparison of methods). The input to the recognition head is the transformer output, which allows it to learn the relevant areas of interest for the given query instead of being given this area explicitly as input. There-fore, it does not require accurate segmentation of the text to perform even in challenging scenarios, such as rotated text, arbitrary-shaped text, or text with overlapping bound-ing boxes (Sec. 4.4). If the segmentation output is desired, a mask head can be added similarly to the detection and recognition head using a simple deconvolutional decoder.
Our weakly-supervised training scheme is obtained by introducing a new loss function based on the Hungarian matching loss [4] that simultaneously optimizes the detec-tion and recognition tasks. The Hungarian loss, which has shown promise in the field of object detection [4, 9, 29, 44], is meaningful in our setting, where the matching explic-itly uses the text content for the detection optimization.
Our Hungarian loss, which we call Text Hungarian Loss, replaces the detection cost with a recognition cost in the matching criteria. The shared embedding that is opti-mized in this manner allows for a significant benefit com-pared to training on synthetic data only, without using any spatial information about the real data. Our weakly-supervised model reaches results comparable to existing fully-supervised methods.
Our main contributions are: 1. A weakly-supervised training scheme using only the text annotations without any spatial ground-truth for real data, utilizing a novel text-based Hungarian matching loss. 2. The first multi-task transformer-based approach for text spotting, in which a single representation is be-ing learned per word for both detection and recognition predictions. 3. Extensive quantitative benchmarks showing our fully-supervised method achieves state-of-the-art results on common text spotting benchmarks, and our weakly-supervised method achieves results competitive with previous fully-supervised methods. 4. The first text spotting framework to offer both a fully-supervised training scheme and a weakly-supervised one for the same architecture, presenting a trade-off between model accuracy and annotation cost. 2.