Abstract
We propose a method for compressively acquiring a dynamic light ﬁeld (a 5-D volume) through a single-shot coded image (a 2-D measurement). We designed an imag-ing model that synchronously applies aperture coding and pixel-wise exposure coding within a single exposure time.
This coding scheme enables us to effectively embed the orig-inal information into a single observed image. The ob-served image is then fed to a convolutional neural network (CNN) for light-ﬁeld reconstruction, which is jointly trained with the camera-side coding patterns. We also developed a hardware prototype to capture a real 3-D scene moving over time. We succeeded in acquiring a dynamic light ﬁeld with 5×5 viewpoints over 4 temporal sub-frames (100 views in total) from a single observed image. Repeating capture and reconstruction processes over time, we can acquire a dy-namic light ﬁeld at 4× the frame rate of the camera. To our knowledge, our method is the ﬁrst to achieve a ﬁner temporal resolution than the camera itself in compressive light-ﬁeld acquisition. Our software is available from our project webpage.1 1.

Introduction
A light ﬁeld is represented as a set of multi-view im-ages, where dozens of views are aligned on a 2-D grid with tiny viewpoint intervals. This representation contains rich visual information of a target scene and thus can be used for various applications such as 3-D display [14, 38], view synthesis [20,58], depth estimation [34,51], synthetic refo-cusing [13, 25], and object recognition [17, 45]. The scope of applications will further expand if the target scene is able to move over time. However, a light ﬁeld varying over time, i.e., a dynamic light ﬁeld, is challenging to acquire due to the huge data rate, which is proportional to both the number of views and frame rate.
Several approaches to acquire light ﬁelds have been in-vestigated as summarized in Fig. 1. The most straightfor-ward approach is to construct an array of cameras [5,37,49], which requires bulky and costly hardware. The second ap-1https://www.fujii.nuee.nagoya-u.ac.jp/Research/CompCam2
Figure 1. Our achievement compared with representative previous works (camera array [49], lens-array camera [24], coded-aperture camera [12], and coded exposure camera [54]). Axes are in rela-tive scales w.r.t. camera’s spatial resolution and frame rate. proach is to insert a micro-lens array in front of an image sensor [1, 2, 24, 25, 29, 46], which enables us to capture a light ﬁeld in a single-shot image. However, the spatial reso-lution of each viewpoint image is sacriﬁced for the angular resolution (number of views). In the above two approaches, the frame rate of the acquired light ﬁeld is at most equiva-lent to that of the cameras. Moreover, the data rate is not compressed because each light ray is sampled individually.
The third approach aims to acquire a light ﬁeld compres-sively by using a single camera equipped with a coded mask or aperture [3, 6, 7, 12, 16, 18, 22, 23, 39, 41, 43]. This kind of camera was used to obtain a small number of coded im-ages, from which a light ﬁeld with the full-sensor spatial resolution can be reconstructed. For static scenes, taking more images with different coding patterns is beneﬁcial to achieve higher reconstruction quality. However, for moving scenes, the use of multiple coded images involves additional complexities related to scene motions. Hajisharif et al. [8] used a high dimensional light-ﬁeld dictionary that spanned several temporal frames. However, their dictionary-based light-ﬁeld reconstruction required a prohibitively long com-putation time. Sakai et al. [31] handled scene motions by alternating two coding patterns over time and by training their CNN-based algorithm on dynamic scenes. However, the light ﬁeld was reconstructed only for every two temporal
frames (at 0.5× the frame rate of the camera).
In this paper, we advance the compressive approach sev-eral steps further to innovate the imaging method for a dy-namic light ﬁeld. As shown in Fig. 1, our method pursues the full-sensor spatial resolution and a faster frame rate than the camera itself. To this end, we design an imaging model that synchronously applies aperture coding [12, 16, 23] and pixel-wise exposure coding [9,30,48,54] within a single ex-posure time. This coding scheme enables us to effectively embed the original information (a 5-D volume of a dynamic light ﬁeld) into a single coded image (a 2-D measurement).
The coded image is then fed to a CNN for light-ﬁeld re-construction, which is jointly trained with the camera-side coding patterns. We also develop a hardware prototype to capture real 3-D scenes moving over time. As a result, we succeeded in acquiring the dynamic light ﬁeld with 5×5 viewpoints over 4 temporal sub-frames (100 views in total) from a single coded image. Repeating capture and recon-struction processes over time, we acquired a dynamic light
ﬁeld at 4× the frame rate of the camera. To our knowledge, our method is the ﬁrst to achieve a ﬁner temporal resolution than the camera itself in compressive light-ﬁeld acquisition. 2.