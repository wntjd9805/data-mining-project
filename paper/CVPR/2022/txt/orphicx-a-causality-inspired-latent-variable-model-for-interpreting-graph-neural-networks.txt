Abstract
This paper proposes a new eXplanation framework, called OrphicX, for generating causal explanations for any graph neural networks (GNNs) based on learned la-tent causal factors. Speciﬁcally, we construct a distinct generative model and design an objective function that en-courages the generative model to produce causal, compact, and faithful explanations. This is achieved by isolating the causal factors in the latent space of graphs by maximizing the information ﬂow measurements. We theoretically analyze the cause-effect relationships in the proposed causal graph, identify node attributes as confounders between graphs and
GNN predictions, and circumvent such confounder effect by leveraging the backdoor adjustment formula. Our frame-work is compatible with any GNNs, and it does not require access to the process by which the target GNN produces its predictions. In addition, it does not rely on the linear-independence assumption of the explained features, nor re-quire prior knowledge on the graph learning tasks. We show a proof-of-concept of OrphicX on canonical classiﬁ-cation problems on graph data. In particular, we analyze the explanatory subgraphs obtained from explanations for molecular graphs (i.e., Mutag) and quantitatively evaluate the explanation performance with frequently occurring sub-graph patterns. Empirically, we show that OrphicX can ef-fectively identify the causal semantics for generating causal explanations, signiﬁcantly outperforming its alternatives1. 1.

Introduction
Graph neural networks (GNNs) have found various ap-plications in many scientiﬁc domains, including iamge classiﬁcation [10], 3D-shape analysis [17], video analy-sis [36], speech recognition [6], and social information sys-1This project is supported by the Internal Research Fund at The Hong
Kong Polytechnic University P0035763. HW is partially supported by NSF
Grant IIS-2127918 and an Amazon Faculty Research Award. tems [9, 12]. The decisions of powerful GNNs for graph-structural data are difﬁcult to interpret. In this paper, we focus on providing post-hoc explanations for any GNN by parameterizing the process of generating explanations.
Speciﬁcally, given a pre-trained GNN of interest, an expla-nation model, or called explainer, is trained for generating compact subgraphs, leading to the model outcomes. How-ever, learning the explanation process can be difﬁcult as no ground-truth explanations exist. If an explanation high-lights subjectively irrelevant subgraph patterns of the input instance, this may correctly reﬂect the target GNN’s unex-pected way of processing the data, or the explanation may be inaccurate.
Recently, a few recent works have been proposed to explain GNNs via learning the explanation process.
XGNN [34] was proposed to investigate the graph patterns that lead to a speciﬁc class by learning a policy network.
PGExplainer [14] was proposed to learn a mask predictor to obtain the edge masks for providing explanations. However,
XGNN fails to explain individual instances and therefore lacks local ﬁdelity [22], while PGExplainer heavily relies on the learned embeddings of the target model, and has the restrictive assumption of having domain knowledge over the learning tasks (e.g., the explicit subgraph patterns are pro-vided). The closest to ours is Gem [11], wherein an explainer is learned based on the concept of Granger causality. The distillation process of ground-truth explanation naturally im-plies the independent assumptions of the explained features2, which might be problematic as the graph-structured data is inherently interdependent [31].
In this work, we deﬁne a distinct generative model as an explainer that can provide interpretable explanations for any
GNNs through the lens of causality, in particular from the notion of the structural causal model (SCM) [19, 29]. In principle, generating causal explanations require reasoning about how changing different concepts of the input instance 2We are aware of the drawbacks of reusing the term “feature.” Speciﬁ-cally, nodes and edges are the explained features in an explanatory subgraph.
— which can be thought of as enforcing perturbations or inter-ventions on the input — affects the decisions over the target model (or the response of the system) [15]. Different from prior works quantifying the causal inﬂuence from the data space (e.g., Gem [11]), we propose to identify the underlying causal factors from latent space. By doing so, we can avoid working with input spaces with complex interdependency.
The intuition is that if the latent features3 can untwist the causal factors and the spurious factors between the input instance and the corresponding output of the target GNN, generating causal explanations is possible.
For this purpose, we ﬁrst present a causal graph that mod-els both causal features and spurious features to the GNN’s prediction. The causal features causing the prediction might be informative to generate a graph-structural mask for the explanation. Our causal analysis shows that there exists a confounder from the data space while considering the cause-effect relationships between the latent features and the GNN outcome [4, 27, 28]. Speciﬁcally, when interpreting graph-structural data, node features/attributes can be a confounder that affects both the generated graph structures and corre-sponding model outcomes. The existence of the confounder represents a barrier to causal quantiﬁcation [19]. To this end, we adopt the concept of information ﬂow [1], along with the backdoor adjustment formula [20], to bypass the confounder effect and measure the causal information transmission from the latent features to the predictions.
Then we instantiate our explainer with a variational graph auto-encoder (VGAE) [8], which consists of an inference network and a generative network (shown in Figure 1). The inference network seeks a representation of the input, in which the representation is learned in such a way that a sub-set of the factors with large causal inﬂuence, i.e. the causal features, can be identiﬁed. The generative network is to map the causal features into an adjacency mask for the explana-tion. Importantly, the generative network ensures that the learned latent representations (the causal features and the spurious features together) are within the data distribution.
In a nutshell, our main contributions are highlighted as follows. We propose a new explanation technique, called
OrphicX, that eXplains the predictions of any GNN by iden-tifying the causal factors in the latent space. We utilize the notion of information ﬂow measurements to quantify the causal information ﬂowing from the latent features to the model predictions. We theoretically analyze the causal-effect relationships in the proposed causal model, identify a confounder, and circumvent it by leveraging the backdoor adjustment formula. We empirically demonstrate that the learned features with causal semantics are indeed informa-tive for generating interpretable and faithful explanations for any GNNs. Our work improves model interpretability and 3Features and factors are used interchangeably, e.g., causal features are equivalent to causal factors. increases trust in GNN model explanation results. 2. Method 2.1. Notations and Problem Setting
V
V
G
Y
|⇥|
G!Y
, where
Notations. Given a pre-trained GNN (the target model to be explained), denoted as f : is the space of input graphs to the model and is the label space. Speciﬁcally, the input graph G = (V, E) of the
GNN includes the corresponding adjacency matrix (A 2
D). We
R| 2 (Dc+Ds) to denote the latent use Z = [Zc, Zs]
|⇥ feature matrix, where Zc is the causal feature sub-matrix and
Zs is the spurious feature sub-matrix. Correspondingly for each node, we denote its node attribute vector by x (one row of X), its causal latent features by zc, and its spurious latent features by zs.
|) and a node attribute matrix (X
V
R|
R| 2
|⇥
V
The desiderata for GNN explanation methods. An essential criterion for explanations is ﬁdelity [22]. A faithful explanation/subgraph should correspond to how the target
GNN behaves in the vicinity of the given graph of interest.
Stated differently, the outcome of feeding to the explanatory subgraph to the target GNN should be similar to that of the graph to be explained. Another essential criterion for explanations is human interpretability, which implies that the generated explanations should be sparse/compact in the context of graph-structured data [21].
In other words, a human-understandable explanation should highlight the most important part of the input while discarding the irrelevant part. In addition, an explainer should be able to explain any
GNN model, commonly known as “model-agnostic” (i.e., treat the target GNN as a black box).
F
Problem setting. Therefore, our ultimate goal is to ob-, that tain a generative model as an explainer, denoted as can identify which part of the input causes the GNN predic-tion, while achieving the best possible performance under the above criteria. Consistent with prior works [11, 14, 34], we focus on explanations on graph structures. We consider the black-box setting where we do not have any informa-tion about the ground-truth labels of the input graphs and we speciﬁcally do not require access to, nor knowledge of, the process by which the target GNN produces its output.
Nevertheless, we are allowed to retrieve different predic-tions by performing multiple queries, and we assume that the gradients of the target GNN are provided. 2.2. OrphicX
Overview. In this paper, we propose a generative model as an explainer, called OrphicX, that can generate causal explanations by identifying the causal features leading to the GNN outcome. In particular, we propose to isolate the causal features and the spurious features from the latent space. For this purpose, we ﬁrst propose a causal graph
latent space causal explanation
Inference 
Network causal features  spurious features
Generative 
Network
Target 
GNN
Y
Y
Figure 1. Illustration of OrphicX. We instantiate our explainer with a variational graph auto-encoder (VGAE), which consists of an inference network and a generative network. The causal features along with the spurious features can be used to reconstruct the graph structure within the data distribution, while the causal features are mapped to a graph-structured mask for the causal explanation. The target GNN is pre-trained, and the parameters would not be changed during the training of OrphicX. reconstruction node attribute X
I (Zc; y).
Zc causal factors spurious factors
Zs
Y adjacency matrix A
Figure 2. Illustration of the causal graph. The causal features are a set of factors in the latent space. The causal features and the spurious features together form the representation of the input graph. The graph structure is reconstructed based on the latent representation; it forms the input of the target GNN, along with the feature matrix. y denotes the predicted label of the GNN target. to model the relationships among the causal features, the spurious features, the input graph, and the prediction of the target model. Then we show how to train OrphicX with a faithful causal-quantiﬁcation mechanism based on the notion of information ﬂow along with the backdoor adjustment formula. With the identiﬁed causal features, we are able to generate a graph-structured mask for the explanation.
Information ﬂow for causal measurements. Recall that, our objective is to generate compact subgraphs as the explanations for the pre-trained GNN. The explanatory sub-graph is causal in the sense that it tends to be independent of the spurious aspects of the input graph while holding the causal portions contributing to the prediction of the target GNN. One challenge, therefore, is how to quantify the causal inﬂuence of different data aspects in the latent space, so as to identify the portion with large causal inﬂu-ence, denoted by Zc. To address this issue, we leverage recent work on information-theoretic measures of causal in-ﬂuence [1]. Speciﬁcally, we measure the causal inﬂuence of
Zc on the model prediction y using the information ﬂow, de-y), between them. Here information ﬂow noted as I (Zc ! can be seen as the causal counterpart of mutual information
Succinctly, our framework attempts to isolate a subset of the representation from the hidden space, denoted as Zc, such that the information ﬂow from Zc to y is maximized.
In what follows, we will show how to quantify this term corresponding to our causal model.
Causal analysis. Throughout this paper, we assume the causal model in Figure 2. Speciﬁcally, the causal features and the spurious features together form the representation of the input graph, which can be used to reconstruct the graph structure, denoted as A. This ensures that the learned latent features still reﬂect the same data distribution as the one captured by the target GNN. The graph structure A, along with the node attribute X, contributes to the model prediction y. Stated differently, X is a confounder when we consider the cause-effect relationships between the latent features (i.e. causal features and spurious features) and the model prediction. Consequently, directly ignoring X can lead to inaccurate estimates of the causal features. To ad-dress this issue, we leverage the classic backdoor adjustment formula [20] and have:
P (y do(Zc)) =
P (y
Zc, X)P (X). (1)
|
|
XX
Eqn. 1 is crucial to circumvent the confounder effect in-troduced by node attributes and compute the informa-y), which is the causal counter-tion ﬂow I(Zc ! part of mutual information [1].
Intuitively, Eqn. 1 goes through different versions of X while keeping Zc ﬁxed to estimate the causal effect Zc has on y. Note that
Zc, X)P (X) is different from
P (y
P (y
Zc); the former samples from the marginal distribution P (X), while the latter sam-ples X from the conditional distribution P (X
Zc). In causal theory, P (y
Zc, X)P (X) is referred
X P (y to as the backdoor adjustment formula [20]. Our Theo-rem 2.1 below provides a way of computing the information
ﬂow I(Zc !
X P (y
|
Zc, X)P (X do(Zc)) =
Zc) = do(Zc)) =
X P (y
P y).
P
P
|
|
|
|
|
|
|
Theorem 2.1 (Information ﬂow between Zc and y) The information ﬂow between the causal factors Zc and the prediction y can be computed as
Put together, we have
I(Zc y)
!
P (Zc)
P (Zc)
=
=
ZZc
ZZc y
X y
X
P (y
| do(Zc)) log
P (y
P (y do(Zc)) do(Zc))dZc
|
|
Zc dZc log
⇣
P (y
R
Zc, X)P (X)
|
·
 
XX
X P (y
X P (y log
Zc
P
P
R
Zc, X)P (X)
Zc, X)P (X)dZc
|
| dZc y) is
Note that due to the confounder X, I(Zc ! not equal to the mutual information I(Zc; y). The term
Zc, X) comes from Eqn. 1 and can be estimated
X P (y
| efﬁciently. Speciﬁcally, we have
P
P (y
| do(Zc)) =
=
⇡
XX XA ZZs 1
NxNsNz
P (y
|
Zc, X)P (X) (2)
A, X)P (A
|
|
Zs, Zc)P (Zs
|
Zc, X)P (X)dZs
XX
P (y
Nx
Xk=1
Ns
Nz
Xj=1
Xn=1
A(kjn), X(k)).
P (y
| (3)
⇠
Here k indexes the Nx sampled node attribute matrices
X(k) from the dataset; j indexes the Ns samples for each
X(k), i.e., Z(kj)
Zc, X(k)); n indexes the Nz sam-P (Zs| s pled graphs for each Z(kj)
Zc, Z(kj)
, i.e., A(kjn)
). s
Note that in practice we use the variational distribution
A, X(k)) to approximate the true posterior distribu-q(Zs|
Zc, X(k)), and that in Eqn. 3, X, Zc, and Zs tion P (Zs| do not necessarily belong to the same graph in the original dataset. Intuitively this is to remove the confounding effect of X on Zc and Zs. Consequently we have p(A
⇠
| s
P (Zc)P (y
| do(Zc))dZc
ZZc
= (4) (5)
Zs, Zc)
|
P (y
A, X)P (A
ZZc
XX XA ZZs
|
P (Zs|
Nc
Nx 1
NcNxNsNz
⇡
Zc, X)P (X)P (Zc)dZsdZc
Ns
Nz
A(ikjn), X(k)), (6)
P (y
| i=1
X
Xk=1 j=1
X n=1
X
⇠
⇠
Similarly, i indexes the Nc samples from Zc’s marginal distribution, i.e., Z(i)
P (Zc); k indexes the Nx sam-c pled node attribute matrices from X’s marginal distribu-tion X(k)
P (X); j indexes the Ns samples of Zs for each pair (Z(i) c , X(k)), i.e., Z(ikj) c , X(k)); n indexes the Nz sampled graphs for each pair (Z(i) c , Z(kj)
), s c , Z(kj) i.e., A(ikjn)
). Note that in practice we p(A s
|
A, Z(i) c , X(k)) to ap-use the variational distribution q(Zs| c , X(k)). proximate the true posterior distribution P (Zs|
P (Zs|
Z(i)
Z(i)
Z(i)
⇠
⇠ s
I(Zc ! y) = 1
NcNxNsNz h
Nc
Nx
Ns
Nz y ⇣
Xi=1 X
Ns
Nx
Xk=1
Nz
Xj=1 n=1
X
A(ikjn), X(k))
P (y
|
·
⌘ 1
NxNsNz
Xk=1
Ns n=1
X
Xj=1
Nz
Nc
Nx
A(ikjn), X(k))
P (y
|
A(ikjn), X(k))
P (y
|
⌘
·
⌘ y ⇣
X
Xk=1
Xi=1 1
NcNxNsNz log n=1
X
Xj=1
Nc
Nx
Ns
Nz
P (y
|
A(ikjn), X(k))
.
⇣
⌘i n=1
X
Xj=1
Xi=1
Xk=1
Graph generative model as an explainer. Our frame-work, OrphicX, leverages the latent space of a variational graph auto-encoder (VGAE) to avoid working with input spaces with complex interdependency. Speciﬁcally, our
VGAE-based framework (shown in Figure 1) consists of an inference network and a generative network. The former is instantiated with a graph convolutional encoder and the latter is a multi-layer perceptron equipped with an inner prod-uct decoder. More concretely, the inference network seeks a representation — a latent feature matrix Z of the input graph, of which the causal features Zc, a sub-matrix with large causal inﬂuence, can be isolated. The generative network serves two purposes: (1) it maps the causal sub-matrix into an adjacency mask, which is used as the causal explanation, and (2) it ensures that the causal features, merged with the spurious features, can reconstruct the graphs within the data distribution characterized by the target GNN.
Learning OrphicX. Learning of OrphicX can be cast as the following optimization problem: y) +   min
I (Zc !
 
LVGAE, (7) where
LVGAE is the negative evidence lower bound (ELBO) loss term that encourages the latent features Z to stay in the data manifold [8], and Zc is the causal sub-matrix of Z. A detailed description of the ELBO term of the VGAE is provided in Appendix. Our empirical results suggest that the ELBO term helps learn a sub-matrix that embeds more relevant information leading to the GNN prediction.
Recall that, our objective is to generate explanations that can provide insights into how the target GNN truly computes its predictions. An ideal explainer should fulﬁll the three desiderata presented in Section 2.1: high ﬁdelity (faithful), high sparsity (compact), and model agnostic. Therefore, apart from the objective function Eqn. 7, we further enforce the ﬁdelity and sparsity criteria through regularization specif-ically tailored to such explainers. Concretely, we denote the generated explanatory subgraph as Gc and the corresponding adjacency matrix as Ac. The sparsity criterion is measured
Ac||1
|| · ||1 denotes the l1 norm of the adjacency by ||
A
||1
|| matrix. The ﬁdelity criterion implies that the GNN outcome corresponding to the explanatory subgraph should be ap-f (G), proximated to that of the target instance, i.e. f (Gc)
, where
⇡
·
) is the probability distribution over the classes — where f ( the outcome of the target GNN. For this purpose, we intro-duce a Kullback–Leibler (KL) divergence term to measure how much the two outputs differ.
Therefore, the optimization problem can be reformulated as: min
I (Zc !
 
Ac||1 y) +  1LVGAE +  2 ||
A
||1
||
+ 3KL (f (Gc), f (G)) , 2{ where  i (i 1, 2, 3
) controls the associated regularizer
} terms. To understand OrphicX comprehensively, a series of ablation studies for the loss function are performed. Note that, the parameters of the target GNN (shown in Figure 1) are pre-trained and would not be changed during the training of OrphicX. OrphicX only works with the model inputs and the outputs, rather than the internal structure of speciﬁc models. Therefore, our framework can be used to explain any GNN models as long as their gradients are admitted. 3. Experiments 3.1. Datasets and Settings
Datasets. We conducted experiments on benchmark datasets for interpreting GNNs: 1) For the node classiﬁcation task, we evaluate different methods with synthetic datasets, including BA-shapes and Tree-cycles, where ground-truth explanations are available. We followed data processing in the literature [32]. 2) For the graph classiﬁcation task, we use two datasets in bioinformatics, MUTAG [2] and NCI1 [26].
Note that the model architectures for node classiﬁcation [5] and graph classiﬁcation [30] tasks are different (more de-tails of the dataset descriptions and corresponding model architectures are provided in Appendix A.2).
Comparison methods. We compare our approach against various powerful interpretability frameworks for
GNNs. They are GNNExplainer [32], PGExplainer [14], and
Gem [11]4. Among others, PGExplainer and Gem explain the target GNN via learning an explainer. As for GNNEx-plainer, there is no training phase, as it is naturally designed for explaining a given instance at a time. Unless otherwise stated, we set all the hyperparameters of the baselines as reported in the corresponding papers.
Hyperparameters in OrphicX. For all datasets on differ-ent tasks, the explainers share the same model structure [8].
For the inference network, we applied a three-layer GCN with output dimensions 32, 32, and 16. The generative model is equipped with a two-layer MLP and an inner product decoder. We trained the explainers using the Adam opti-mizer [7] with a learning rate of 0.003 for 300epochs. For all experiments, we set Nx = 5, Nz = 2, Nc = 25, Ns = 100,
Dc = 3,  1 = 0.1,  2 = 0.1, and  3 = 0.2. The results reported in the paper correspond to the best hyperparame-ter conﬁgurations. With this testing setup, our goal is to fairly compare best achievable explanation performance of the methods. Detailed implementations, including our hy-perparameter search space are given in Appendix A.2.
Evaluation metrics. We evaluate our approach with two criteria. 1) Faithfulness5/ﬁdelity: are the explanations indica-tive of “true” model behaviors? 2) Sparsity: are the explana-tions compact and understandable? Below, we address these criteria, proposing quantitative metrics for evaluating ﬁdelity and sparsity and qualitative assessment via visualizing the explanations.
To evaluate ﬁdelity, we generate explanations for the test set6 according to OrphicX, Gem, PGExplainer, and GN-NExplainer, respectively. We then evaluate the explanation accuracy of different methods by comparing the predicted labels of the explanatory subgraphs with the predicted labels of the input graphs using the pre-trained GNN [11]. An explanation is faithful only when the predicted label of the explanatory subgraph is the same as the corresponding in-put graph. To evaluate sparsity, we use different evaluation metrics. Speciﬁcally, in Mutag, the type and the size of explainable motifs are various. We measure the fraction of edges (i.e., edge ratio denoted as R) selected as “important” by different explanation methods for Mutag and NCI1. For the synthetic datasets, we use the number of edges (denoted as K), as did in prior works [11, 32]. A smaller fraction of edges or a smaller number of edges selected implies a more compact subgraph or higher sparsity.
To further check the interpretability, we use the visual-ized explanations to analyze the performance qualitatively.
However, we do not know the ground-truth explanations for the real-world datasets. For Mutag7, we ask an expert from
Biochemical Engineering to label the explicit subgraph pat-terns as our explanation ground truth (i.e., carbon rings with chemical groups such as the azo N=N, NO2 and NH2 for the mutagenic class). Speciﬁcally, 739/933 instances con-taining the subgraph patterns fall into the mutagenic class in the entire dataset, which corroborates that these patterns are sufﬁcient for the ground-truth explanation. Figure 3 de-scribes the detailed distribution of instances with various occurring subgraph patterns. With these occurring subgraph patterns, we can evaluate the explanation performance on
Mutag with edge AUC. The evaluation intuition is elaborated in the Section 3.2. 3.2. Empirical Results
Explanation performance. We ﬁrst report the expla-nation performance for synthetic datasets and real-world 5In the context of model interpretability, “faithfulness” means high
ﬁdelity [13], which is different from the meaning used in causal discovery. 6The detailed data splitting is provided in the Appendix. 7As we cannot obtain the ground-truth explanations for NCI1, we focus 4We use the source code released by the authors. on the quantitative evaluation for this dataset.
Table 2. Explanation Accuracy on Real-World Datasets (%).
Mutag
NCI1
R edge ratio 0.5 0.6 0.7 0.8 0.9 0.5 0.6 0.7 0.8 0.9
OrphicX 71.4 71.2 77.2 78.8 83.2 66.9 72.7 77.1 81.3 85.4 66.4 67.7 71.4 76.5 81.8 61.8 68.6 70.6 74.9 83.9
Gem
GNNExp. 65.0 66.6 66.4 71.0 78.3 64.2 65.7 68.6 75.2 81.8 59.3 58.9 65.1 70.3 74.7 57.7 60.8 65.2 69.3 71.0
PGExp. around 0, which indicates OrphicX can well capture the most relevant subgraphs towards the predictions by the pre-trained
GNNs. As OrphicX exhibits a similar performance trend on other datasets, we present corresponding evaluation results in the Appendix. (cid:335)
O
N
H
C
Br
S
Mutagenic
Non-mutagenic g n i r r u c c
O f o y c n e u q e r
F s n r e t t a
P h p a r g b u
S 448 315 90 73 35
N=N 83
NO2
NH2
Figure 3. The frequency of occurring subgraph patterns indicates that it is reasonable to treat the labeled motifs/subgraph patterns as the explanation ground truth, i.e., carbon rings with chemical groups such as N=N, NO2, and NH2 for the mutagenic class.
Table 1. Explanation Accuracy on Synthetic Datasets (%). (a) BA-shapes 5
BA-SHAPES 8 7 6
K
#of edges 10
OrphicX 82.4 97.1 97.1 97.1 100 85.7 91.4 100 100 100 64.7 94.1 91.2 91.2 91.2 74.3 88.6 100 100 100
Gem
GNNExp. 67.6 67.6 82.4 88.2 85.3 20.0 54.3 74.3 88.6 97.1 59.5 59.5 59.5 59.5 64.3 76.2 81.5 91.3 95.4 97.1
PGExp.
TREE-CYCLES 8 6 9 7 9 datasets. In particular, we evaluate the explanation accu-racy under various sparseness constraints (i.e., various R for the real-world datasets and various K for the synthetic datasets). Table 1 and Table 2 report the explanation accu-racy of different methods speciﬁcally. A smaller number of edges (denoted as K) or a smaller value of edge ratio (denoted as R) indicates that the explanatory subgraphs are more compact. As observed, OrphicX consistently outper-forms baselines across various sparseness constraints over all datasets. As the model architectures for node and graph classiﬁcation [30] tasks are different, the performance cor-roborates that our framework is model architecture-agnostic (see the model architectures in the Appendix).
Following existing works [11, 18], we also evaluate the
Log-odds difference to illustrate the ﬁdelity of generated ex-planations in a more statistical view. Log-odds difference describes the resulting change in the pre-trained GNNs’ out-come by computing the difference (the initial graph and the explanation subgraph) in log odds. The detailed deﬁnition of Log-odds difference is elaborated in Appendix A.2. Fig-ure 4 depicts the distributions of log-odds difference over the entire test set for synthetic datasets. We can observe that the log-odds difference of OrphicX is more concentrated (b) Tree-cycles
Figure 4. Explanation Performance with Log-Odds Difference.
OrphicX consistently achieves the best performance overall (denser distribution around 0 is better).
For fair comparisons, we also report the explanation ﬁ-delity of different methods in terms of edge AUC in Ta-ble 3. We follow the experimental settings of GNNExplainer and PGExplainer8, where the explanation problem was for-malized as a binary classiﬁcation of edge. The mean and standard deviation are calculated over 5 runs. This metric works for the datasets with ground-truth explanations (i.e., the “house”-structured pattern/motif of BA-shapes and the labeled subgraph patterns in Mutag). The intuition is that a good explanation method assigns higher weights to the edges within the ground-truth subgraphs/motifs. Regarding edge 8We use PGExp. and GNNExp. to represent PGExplainer and GNNEx-plainer for simplicity.        
Table 3. Explanation Accuracy with Edge AUC (* means the rounded estimate of 0.9995 0.0006).
±
DATASETS
BA-SHAPES 0.988
TREE-CYCLES 0.988 1.000
MUTAG
OrphicX
GEM
GNNEXP.
PGEXP. 0.008 0.597 0.001 0.761 0.001⇤ 0.988
±
±
± 0.001 0.956 0.002 0.961 0.013 0.998 0.001 0.924 0.003 0.952 0.001 0.998
±
±
±
±
±
±
±
±
± 0.042 0.000 0.001 0.686 0.098
±
ATT 0.815 0.824
Original
OrphicX
Gem
GNNExplainer
PGExplainer p=0.9924 p=0.9993 p=0.4142 p=0.9668 p=0.9716 p=0.9781 p=0.9721 p=0.9309 p=0.7137 p=0.0058 p=0.8679 p=0.8634 p=0.8634 p=0.9991 p=0.0322
O
N
H
C
Br
S
Figure 5. Explanation Visualization (MUTAG): p is the corresponding probability of being classiﬁed as Mutagenic class by the pre-trained
GNN. The graphs in the ﬁrst column are the target instances to be explained. The solid edges in other columns are identiﬁed as ‘important’ by corresponding methods. The closer the probability to that of the target instance, the better the explanation is. importance, one might naturally consider the self-attention mechanism as a feasible solution. Prior works have shown its performance for model explanations. For clarity, we also re-port the experimental results of the self-attention mechanism denoted as ATT in Table 3. The results of synthetic datasets are from GNNExplainer and PGExplainer. For Mutag, we evaluate the subgraph patterns labeled by the domain expert.
As might be expected, OrphicX exhibits its superiority in identifying the most important edges captured by the pre-trained GNNs. We also observe that the prior causality-based approach, Gem, does not perform well evaluating with edge
AUC. We conjecture that the explainable subgraph patterns are destroyed due to the distillation process [11]. Though the generated subgraphs with Gem can well reﬂect the classiﬁ-cation pattern captured by the pre-trained GNN, it degrades the human interpretability of the generated explanations.
Explanation visualization. Figure 5 plots the visualized explanations of different methods. In particular, we focus on the visualization on Mutag, which can reﬂect the in-terpretability quantitatively and qualitatively. The ﬁrst col-umn shows the initial graphs and corresponding probabilities of being classiﬁed as “mutagenic” class by the pre-trained
GNN, while the other columns report the explanation sub-graphs. Associated probabilities belonging to the “muta-genic” class based on the pre-trained GNN are reported below the subgraphs. Speciﬁcally, in the ﬁrst case (the ﬁrst row), OrphicX can identify the essential subgraph pattern — a complete carbon ring with a NO2 — leading to its label (
“mutagenic”). Nevertheless, prior works, particularly Gem, fail to recognize the explainable motif. In the second instance (the second row), OrphicX can well identify a complete car-bon ring with a NH2. At the same time, PGExplainer fails to recognize the NH2, leading to a high probability of being classiﬁed into the wrong class — “non-mutagenic” — by the target GNN, with a probability of 0.9942. In the third in-stance (the third row), a complete carbon ring with a N=N is the essential motif, consistent with the criterion from the do-main expert. Overall, OrphicX can identify the explanatory subgraphs that best reﬂect the predictions of the pre-trained
GNN. The visualization of synthetic datasets and more visu-alization plots on Mutag are provided in Appendix A.3.
Information ﬂow measurements. To validate Theo-rem 2.1, we evaluate the information ﬂow of the causal fac-tors (Zc) and the spurious factors (Zs) corresponding to the model prediction, respectively. Figure 10a in Appendix A.3 shows that, as desired, the information ﬂow from the causal factors to the model prediction is large while the information
ﬂow from the spurious factors to the prediction is small. We also evaluate the prediction performance while adding noise (mean is set as 0) to the causal factors and the spurious fac-Table 4. Prediction Accuracy of the Pre-trained GNN on Mutag with Various Perturbation (mean is set as 0).
PERTURBATION STD 0.0 1.3 0.3
CAUSAL FACTORS 0.935 0.926 0.926 0.887 0.860 0.826
SPURIOUS FACTORS 0.935 0.936 0.936 0.935 0.934 0.926 1.0 0.8 0.5 tors, respectively. From Table 4, we can observe that adding perturbations to the causal factors degrades the prediction performance of the pre-trained GNN signiﬁcantly with the increase of the standard deviation of the noise (mean is set as 0) while adding the perturbations on the spurious coun-terparts does not. These insights, in turn, verify the efﬁcacy of applying the concept of information ﬂow for the causal inﬂuence measurements.
Ablation studies. An ablation study for the information
ﬂow in the hidden space was performed by removing the causal inﬂuence term. From Figure 10b in Appendix A.3, we can observe that without the causal inﬂuence term, the causal inﬂuence to the model prediction is distributed across all hidden factors. In addition, we also inspect the explanation performance for our framework as an ablation study for the loss function proposed. We empirically prove the need for different forms of regularization leveraged by the OrphicX loss function. Due to space constraints, the empirical results are provided in the Appendix. 4.