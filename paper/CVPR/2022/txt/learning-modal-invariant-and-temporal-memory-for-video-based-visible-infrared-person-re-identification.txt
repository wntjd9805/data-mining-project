Abstract
Thanks for the cross-modal retrieval techniques, visible-infrared (RGB-IR) person re-identiﬁcation (Re-ID) is achieved by projecting them into a common space, allow-ing person Re-ID in 24-hour surveillance systems. How-ever, with respect to the probe-to-gallery, almost all ex-isting RGB-IR based cross-modal person Re-ID methods focus on image-to-image matching, while the video-to-video matching which contains much richer spatial- and temporal-information remains under-explored. In this pa-per, we primarily study the video-based cross-modal per-son Re-ID method. To achieve this task, a video-based
RGB-IR dataset is constructed, in which 927 valid iden-tities with 463,259 frames and 21,863 tracklets captured by 12 RGB/IR cameras are collected.
Based on our constructed dataset, we prove that with the increase of frames in a tracklet, the performance does meet more enhancement, demonstrating the signiﬁcance of video-to-video matching in RGB-IR person Re-ID. Additionally, a novel method is further proposed, which not only projects two modalities to a modal-invariant subspace, but also ex-tracts the temporal-memory for motion-invariant. Thanks to these two strategies, much better results are achieved on our video-based cross-modal person Re-ID. The code and dataset are released at: https://github.com/VCM-project233/MITML. 1.

Introduction
Person re-identiﬁcation (Re-ID) [17, 25, 43, 54] focuses on matching probe pedestrian images with the gallery sets.
Due to multiple views which are non-overlapped, there are signiﬁcant changes in human body postures, illumina-tion and backgrounds, leading a large challenge to Re-ID.
Thanks to the rapid development of deep learning, various (cid:0) Jinxing Li is the Corresponding Author.
Figure 1. Advantages of video-based cross-modal person Re-ID.
If two persons enjoy similar appearances, video data can also pro-vide discriminative temporal-information that image data is un-available. Speciﬁcally, the person wearing the black T-shirt is quite similar to the person wearing the blue T-shirt under the IR camera (shown in the red box), while their speciﬁc arm postures in the motions give the discriminative features (shown in the green box). deep end-to-end approaches [3, 22–24] have been studied, greatly enhancing the Re-ID performance.
Despite the achievement of aforementioned methods, most of them are heavily dependent on the RGB images, so that the lighting for the cameras is essential. However, this constraint is too strict, especially at night, making the collected RGB data uninformative and failing to achieve person Re-ID. Fortunately, most surveillance cameras can automatically switch from RGB to the infrared (IR) mode if the lighting is unavailable. In contrast to RGB images,
IR images are capable of preserving the information under invisible lighting and showing pedestrians clearly. Thus, in order to achieve person Re-ID in 24-hour surveillance sys-tems, the RGB-IR based visible-infrared (cross-modal) per-son Re-ID [6, 25, 39] provides a promising strategy. For in-stance, Wu et al. [39] ﬁrst collected an RGB-IR dataset and proved the feasibility for these two modalities matching. In-spired by this work, various cross-modal Re-ID works were
then studied.
Although the cross-modal person Re-ID methods ﬁll the gap between RGB images and IR images, they are only single-image based tasks. In the data collection, pedestrians originally appear in the video databases, containing mul-tiple frames in each tracklet.
Intuitively, the video-based data contains much richer visual information than a single image [53].
In some speciﬁc cases, it is indeed difﬁcult to identify two persons with similar appearances if only a single image is given. This case is more difﬁcult for the infrared modality, and even the human beings cannot guar-antee the correctness. In contrast to still images, the video is an image sequence containing the spatial and temporal-information, so that the beneﬁcial motion information can be exploited for the discriminative identiﬁcation. For in-stance, as displayed in Fig. 1, two images captured from two persons enjoy similarity under the IR camera. However, the person wearing the black T-shirt has the speciﬁc arm pos-ture in the motion, compared with the person wearing the blue T-shirt. Thanks to such motion characteristics, more discriminative information is provided for us to achieve a more robust and accurate identiﬁcation model. Thus, it is quite signiﬁcant to replace the still-images with videos in cross-modal person Re-ID.
To address this problem, in this paper, the video-based
In comparison to image-cross-modal Re-ID is studied. based cross-modal Re-ID, the video-based cross-modal Re-ID further aims to exploit the temporal-information for ro-bust feature extraction.
In contrast to the existing video-based RGB Re-ID methods, our focused work additionally extracts the consistency between RGB and IR modalities.
In order to achieve the video-based cross-modal Re-ID, an associated database is inevitable. Although Wu et al.
[39] has presented an RGB-IR dataset, it only focuses on the image-based retrieval, being far away from our video-based requirement. To substantiate our task, we primarily construct a video-based RGB-IR database named HITSZ
Video Cross-Modal (HITSZ-VCM) Re-ID dataset. The comparison between our collected dataset and existing Re-ID datasets is listed in Tab. 1. Different from SYSU-MM01
[39] which only collected the RGB images and IR images via 4 RGB cameras and 2 IR cameras, we set 12 cameras to capture both RGB and IR videos and much more valid identities are collected. Totally, 927 valid identities includ-ing 11,785 / 10,078 tracklets and 251,452 / 211,807 images with or free from masks for RGB and IR modalities are ob-tained, respectively.
For the video-based cross-modal Re-ID, the spatial- and temporal-information among each tracklet do contribute to the performance improvement.
In this paper, a base-line method is ﬁrst applied to our constructed dataset, demonstrating the signiﬁcance of video-based cross-modal
Re-ID. Speciﬁcally, we follow Ye et al.’s [49] base-line on image-based cross-modal Re-ID and add a mod-ule to utilize the temporal-information. Additionally, we also propose a novel method named Modal-Invariant and
Temporal-Memory Learning (MITML). Two modalities are transformed to get modal-invariant but id-related features through an adversarial strategy, so that the gap between
RGB and IR modalities is relieved. Referring to the motion information in a tracklet, we also propose a temporal mem-ory reﬁnement module to extract the temporal-information.
Thanks to these two strategies, the Re-ID performance on our dataset is further improved.
Overall, the main contributions of this paper are:
• We construct a video-based RGB-IR database, allow-ing the study on video-based cross-modal person Re-ID. Different from existing Re-ID works, to the best of our knowledge, this is the ﬁrst work which jointly takes cross modalities and videos into ac-count, deﬁning a challenging task.
• We introduce a baseline to prove the signiﬁcance of video-based cross-modal person Re-ID. In detail, by embedding a temporal-information exploitation mod-ule, the cross-modal person Re-ID performance meets a continuous increase when the number of images in a tracklet rises.
• A novel method named Modal-Invariant and
Temporal-Memory Learning (MITML) addi-tionally proposed by more efﬁciently removing modal-variance and exploiting motion information.
Experimental results substantiate the superiority of our proposed method. is 2.