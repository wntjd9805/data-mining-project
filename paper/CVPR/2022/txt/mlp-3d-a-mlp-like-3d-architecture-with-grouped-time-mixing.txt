Abstract
Convolutional Neural Networks (CNNs) have been re-garded as the go-to models for visual recognition. More re-cently, convolution-free networks, based on multi-head self-attention (MSA) or multi-layer perceptrons (MLPs), become more and more popular. Nevertheless, it is not trivial when utilizing these newly-minted networks for video recognition due to the large variations and complexities in video data.
In this paper, we present MLP-3D networks, a novel MLP-like 3D architecture for video recognition. Speciﬁcally, the architecture consists of MLP-3D blocks, where each block contains one MLP applied across tokens (i.e., token-mixing
MLP) and one MLP applied independently to each token (i.e., channel MLP). By deriving the novel grouped time mixing (GTM) operations, we equip the basic token-mixing
MLP with the ability of temporal modeling. GTM divides the input tokens into several temporal groups and linearly maps the tokens in each group with the shared projection matrix. Furthermore, we devise several variants of GTM with different grouping strategies, and compose each vari-ant in different blocks of MLP-3D network by greedy ar-chitecture search. Without the dependence on convolutions or attention mechanisms, our MLP-3D networks achieves 68.5%/81.4% top-1 accuracy on Something-Something V2 and Kinetics-400 datasets, respectively. Despite with fewer computations, the results are comparable to state-of-the-art widely-used 3D CNNs and video transformers. 1.

Introduction
During the past decade, the advances in Convolutional
Neural Networks (CNNs) have successfully pushed the lim-its and improved the state-of-the-art technologies for image and video understanding [5, 11, 12, 14, 19, 25, 28, 30, 45, 46, 48–51, 57, 64, 69]. Besides achieving the top performances across tasks, the highly optimized implementation of convo-lution on various hardware makes CNNs continue to domi-nate computer vision research. Nevertheless, motivated by the success of attention model in Natural Language Process-ing (NLP) [60], vision transformers [9, 21, 33, 38, 55, 63]
Figure 1. A schematic diagram of MLP-3D block. The block originates from the MLP-mixer layer in [54], and decomposes the original token-mixing MLP into three sub-operations along height, width and time dimensions, respectively. For time dimension, the novel grouped time mixing operation is exploited. become an alternative choice for image recognition by us-ing multi-head self-attention (MSA) and multi-layer per-ceptrons (MLPs). More recently, the models solely with
MLPs (i.e., MLP-like networks) without convolutions or self-attention layers are also shown able to perform well on ImageNet classiﬁcation, and are more efﬁcient for both training and inference [6, 17, 54, 70].
Despite having these impressive progresses on image recognition, devising MLP-like architecture on video data is seldom studied and remains challenging. The video data is more complex due to large variations in motion and rich content in visual details. Capturing useful information from such information-intensive media requires exhaustive com-puting resources. This property inherently poses difﬁculties for developing MLP-like 3D architecture from two aspects: 1) how to capture the complex temporal dynamics in videos via MLP-like operations? 2) how to reduce the expensive computations for space-time modeling?
To address the issues, in this work, we start from the design of basic MLP-style operations to model temporal sequence, and next study how to construct an efﬁcient
MLP-like architecture on the utilization of these opera-tions. To this end, we propose MLP-3D networks - a novel
MLP-like 3D architecture to model spatio-temporal depen-dency in videos. In MLP-3D networks, an input video clip is divided into overlapped tubelets (i.e., sequences of as-sociated frame patches across time), and each tubelet is mapped into a visual token through a tubelet embedding layer. These tokens are then fed into several stacked MLP-3D blocks, where each block abstracts inter-token informa-tion by token-mixing MLP and intra-token information by channel MLP, as shown in Figure 1. The channel MLP, which shares the similar structure as the feed-forward layer in transformer [60], is applied to each token independently.
The token-mixing MLP is the weighted summation of three sub-operations applied across different tokens along the height, width and time dimension, respectively. The sub-operations on the spatial dimensions (height and width) fol-low the recipe of Cycle Fully-Connected Layer (Cycle FC) in [6]. For the time dimension, we devise a novel Grouped
Time Mixing (GTM) operation, i.e., a group-based token mixing operation across tokens at different time points. By only mixing the information within each group indepen-dently, the computational complexity and the number of pa-rameters are effectively reduced. Furthermore, based on dif-ferent grouping strategies, we derive four variants of GTM operations and compose each in different MLP-3D blocks by greedy architecture search.
The main contributions of this work are summarized as follows. First, GTM is a novel family of MLP-style operations to model temporal dynamics in an economic and effective way. Second, MLP-3D networks is a new
MLP-like 3D architecture by utilizing GTM operations in the decomposed token-mixing MLP. Extensive exper-iments conducted on Something-Something and Kinetics datasets demonstrate that MLP-3D networks achieve supe-rior or comparable performances to widely-used 3D CNNs (e.g., SlowFast networks [12]) and computationally expen-sive video transformers (e.g., TimeSformer [3]). Moreover,
MLP-3D networks show a great potential in developing the
MLP-like architecture for video understanding. 2.