Abstract
Gaze object prediction is a newly proposed task that aims to discover the objects being stared at by humans. It is of great application significance but still lacks a unified so-lution framework. An intuitive solution is to incorporate an object detection branch into an existing gaze prediction method. However, previous gaze prediction methods usually use two different networks to extract features from scene im-age and head image, which would lead to heavy network ar-chitecture and prevent each branch from joint optimization.
In this paper, we build a novel framework named GaTec-tor to tackle the gaze object prediction problem in a unified way. Particularly, a specific-general-specific (SGS) feature extractor is firstly proposed to utilize a shared backbone to extract general features for both scene and head images. To better consider the specificity of inputs and tasks, SGS intro-duces two input-specific blocks before the shared backbone and three task-specific blocks after the shared backbone.
Specifically, a novel Defocus layer is designed to generate object-specific features for the object detection task without losing information or requiring extra computations. More-over, the energy aggregation loss is introduced to guide the gaze heatmap to concentrate on the stared box. In the end, we propose a novel wUoC metric that can reveal the differ-ence between boxes even when they share no overlapping area. Extensive experiments on the GOO dataset verify the superiority of our method in all three tracks, i.e. object de-tection, gaze estimation, and gaze object prediction. 1.

Introduction
Gaze estimation (GE) aims at determining the direction and point that a person is staring at [30]. As gaze behavior is an essential aspect of human social behavior [7, 19], we can infer potential information based on the staring object.
For example, in front of the bus station, a person looking at his watch may indicate that he has something urgent to do.
Customers staring at the product may want to purchase it in the shopping mall. The stared object can generally reveal our state, e.g. what they are doing, or what they plan to do.
*Corresponding author.
Figure 1. Illustration of the gaze object prediction approaches. (a)
Previous gaze prediction methods use two separate backbones to tackle scene image and head image, respectively. (b) The proposed
SGS feature extractor can produce features from scene image and head image in a unified manner.
In the gaze estimation community, researchers usually employ two separated backbones to process the entire scene image and the head image. This scene-head separated struc-ture was first proposed by Recasens et al. [30], where one backbone captures holistic cues from the entire scene im-age, and the other backbone analyzes the details from the head image. Later, Lian et al. [21] proposed the multi-scale gaze direction fields to analyze the head image precisely.
Recently, Chong et al. [4] elaborately designed interactions between the head branch and the scene branch and utilized deconvolutional layers to produce a fine-grained heatmap.
In general, the gaze estimation performance has kept im-proving in recent years while the network architecture grad-ually gets complex. Existing models only predict the gaze area that people may stare at rather than precisely predict-ing the location of the object been be stared at. Tomas et al. [38] recently pointed out that it has significant practical usage to identify the stared thing. As shown in Fig. 1, from the bounding box of the stared item, we can infer that the person is likely to buy the Locally Mango product, which is beyond the scope of the traditional gaze estimation task.
However, performing gaze object prediction is non-trivial and faces the following challenges, i.e. heavy net-work architecture, inconsistent requirements about image
size. First of all, although it is an intuitive solution that adds an additional object detection branch to the existing two-branch gaze estimation models [4, 11, 21, 25, 30, 31, 36, 51], this approach would undoubtedly increase the number of calculations and parameters of the entire network. Besides, gaze estimation models usually employ an image of ordi-nary size (e.g. 224 × 224) to capture a global receptive field.
In contrast, objects in the retail scenario are generally small and dense, requiring an enlarged image to detect bounding boxes precisely. Moreover, compared with individuals car-rying out gaze estimation and object detection, a more suit-able way should employ a unified framework and achieve joint optimization.
To alleviate the above issues, we make the following three designs. (1) Different from previous gaze estimation works [4, 21, 30] that use two independent branches (see in Fig. 1 (a)), we propose a specific-general-specific (SGS) mechanism to extract task-specific features from the scene and head image with only one backbone (see in Fig. 1 (b)), which can help to reduce the parameters and computational burdens, and also make it possible to joint optimization for different inputs. (2) To assist precise object detection, we develop a Defocus layer to generate object-specific features.
In particular, the input image with ordinary size can not pro-duce a feature map with sufficient resolution for detecting small and dense retail objects. The proposed Defocus layer zooms the spatial size via shrinking the channel size, which can produce a feature of high-resolution without losing in-formation or bringing extra computations. (3) To tackle the performance bottleneck of imprecise gaze heatmap, we pro-pose the energy aggregation loss that measures the percent-age of energy within the stared box, and use the ground truth bounding box to guide the gaze estimation process.
In our work, we propose a unified framework, namely
GaTector, to estimate gaze heatmap, detect retail objects and conduct gaze object detection, as shown in Figure 2.
The scene and head images are first jointly tackled by the
SGS feature extractor. Then, the object detection head dis-covers bounding boxes, and the gaze prediction head pre-dicts gaze heatmap, so we can jointly consider gaze pre-diction and object detection results to carry out gaze object prediction. Also, a novel wUoC metrics proposed to better reveal the difference between boxes even when they share no overlapping area. Our contributions can be summarized as follows:
• We proposed a unified method GaTector with a novel wUoC evaluation metric to make an early exploration on the gaze object detection task.
• We propose a novel SGS mechanism that can ex-tract task-specific features by a single backbone while maintaining satisfactory performance. A Defocus layer is introduced in SGS to prepare high-resolution feature maps for small retail object detection, and the energy aggregation loss guides the gaze heatmap to be concentrated.
• On the large-scale GOO dataset, we consistently im-prove the performance on two traditional tracks, i.e. gaze estimation, and object detection, while reducing the model parameter and computational costs. In ad-dition, we build a solid baseline for the gaze object detection task to promote future research. 2.