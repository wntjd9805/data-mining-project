Abstract
Compared to convolutional layers, fully-connected (FC) layers are better at modeling the long-range dependencies but worse at capturing the local patterns, hence usually less favored for image recognition. In this paper, we propose a methodology, Locality Injection, to incorporate local priors into an FC layer via merging the trained parameters of a parallel conv kernel into the FC kernel. Locality Injection can be viewed as a novel Structural Re-parameterization method since it equivalently converts the structures via transforming the parameters. Based on that, we propose a multi-layer-perceptron (MLP) block named RepMLP Block, which uses three FC layers to extract features, and a novel architecture named RepMLPNet. The hierarchical design distinguishes RepMLPNet from the other concurrently pro-posed vision MLPs. As it produces feature maps of differ-ent levels, it qualifies as a backbone model for downstream tasks like semantic segmentation. Our results reveal that 1) Locality Injection is a general methodology for MLP models; 2) RepMLPNet has favorable accuracy-efficiency trade-off compared to the other MLPs; 3) RepMLPNet is the first MLP that seamlessly transfer to Cityscapes seman-tic segmentation. The code and models are available at https://github.com/DingXiaoH/RepMLP. 1.

Introduction
The locality of images (i.e., a pixel is more related to its neighbors than the distant pixels) makes Convolutional
*This work is supported by the National Natural Science Foundation of
China (Nos.61925107, U1936202, 62021002) and the Beijing Academy of Artificial Intelligence (BAAI). This work is done during Xiaohan Ding and Honghao Chen’s internship at MEGVII Technology.
†Corresponding author.
Neural Network (CNN) successful in image recognition, as a conv layer only processes a local neighborhood. In this paper, we refer to this inductive bias as the local prior.
Besides, we also desire the ability to build up long-range dependencies, which is referred to as the global capacity in this paper. Traditional CNNs model the long-range de-pendencies by deep stacks of conv layers [40]. However, repeating local operations may cause optimization difficul-ties [1, 20, 40]. Some prior works enhance the global ca-pacity with self-attention-based modules [18,39,40], which have no local prior. For example, due to the lack of lo-cal prior, ViT [18] requires an enormous amount of training data (3 × 108 images in JFT-300M) to converge. On the other hand, a fully-connected (FC) layer can also directly model the dependencies between any two input points, which is as simple as flattening the feature map as a vec-tor, linearly mapping it into another vector, and reshap-ing the resultant vector back into a feature map. How-ever, this process has no locality either. Without such an important inductive bias, the concurrently proposed
MLPs [26, 36, 37, 43] usually demand a huge amount of training data (e.g., JFT-300M), more training epochs (300 or 400 ImageNet [8] epochs) or special training techniques (e.g., a DeiT-style distillation method [37, 38]) to learn the inductive bias from scratch. Otherwise, they may not reach a comparable level of performance with traditional CNNs.
We desire an MLP model that is 1) friendly to small-data, 2) trainable with ordinary training methods, and 3) effective in visual recognition. To this end, we make contributions in three aspects: methodology, component and architecture.
Methodology We propose a novel methodology, Locality
Injection, to provide an FC layer with what it demands the locality. Specifi-for effective visual understanding: cally, we place one or more conv layers parallel to the FC and add up their outputs. Though the FC simply views
Figure 1. RepMLP Block, where n, c, h, w are the batch size, number of input channels, height and width of the feature map, s is the number of share-sets, p is the padding. This example assumes n = 1, c = 4, s = 2. 1) The Global Perceptron aggregates the information across all the spatial locations and establishes the relations among channels. 2) In parallel, the input feature map is split into s share-sets and fed into the Channel Perceptron, which simply reshapes the features into vectors, linearly maps it to the output vectors, and reshapes them back. 3) The Local Perceptron takes the same inputs as the Channel Perceptron but convolve with small kernels to capture the local patterns. This example uses 1×1 and 3×3 so that the padding should be p = 0 and 1, respectively, to maintain the feature map size.
Through batch normalization (BN) [22], the outputs of Local Perceptron and Channel Perceptron and added up. Finally, we combine the global and channel-wise information by merging the outcomes of the Global Perceptron. After training, the conv layers are absorbed into the FC3 kernel via Locality Injection, so that the training-time block is equivalently converted into a three-FC block and used for inference. the feature maps as vectors, completely ignoring the local-ity, the conv layers can capture the local patterns. How-ever, though such conv layers bring only negligible parame-ters and compute, the inference speed may be considerably slowed down because of the reduction of degree of paral-lelism on high-power computing devices like GPUs [30].
Therefore, we propose to equivalently merge the conv lay-ers into the FC kernels after training to speed up the infer-ence. By doing so, we obtain an FC layer that is structurally identical to a normally trained FC layer but is parameter-ized by a special matrix with locality. Since Locality Injec-tion converts the training-time structure (FC + conv) to the inference-time (a single FC) via an equivalent transforma-tion on the parameters, it can be viewed as a novel Struc-tural Re-parameterization [11, 13–16] technique. In other words, we equivalently incorporate the inductive bias into a trained FC layer, instead of letting it learn from scratch.
The key to such an equivalent transformation is converting an arbitrary conv kernel to an FC kernel (i.e., a Toeplitz ma-trix). In this paper, we propose a simple, platform-agnostic and differentiable approach (Sec. 3).
Component Based on Locality Injection, we propose
RepMLP Block as an MLP building block. Fig. 1 shows a training-time RepMLP Block with FC, conv and batch nor-malization [22] (BN) layers can be equivalently converted into an inference-time block with only three FC layers.
Architecture The hierarchical design has been shown to benefit visual understanding [20,27,34]. Therefore, we pro-pose a hierarchical MLP architecture composed of RepMLP
Blocks. In other words, as the feature map size reduces, the number of channels increases. We reckon that the major ob-stacle for adopting hierarchical design in a ResMLP- [37] or MLP-Mixer-style [36] model is that the number of pa-rameters is coupled with the feature map size 1, so that the number of parameters of the lower-level FC layers would be several orders of magnitude greater than the higher-level layers. For example, assume the lowest-level feature maps are of 56×56, an FC layer requires 564 = 9.8M parame-ters to map a channel to another, without any cross-channel correlations (i.e., like depth-wise convolution). We may let all the channels share the same set of parameters, so that the layer will have a total of 9.8M parameters. However, let the highest-level feature maps be of 7×7, the parame-ter count is only 74 = 2.4K but the number of channels is large. Predictably, sharing so few parameters among so many channels restricts the representational capacity hence results in inferior performance. We solve this problem by a set-sharing linear mapping (Sec. 4.1) so that we can in-dependently control the parameter count of each layer by letting the channels share a configurable number of param-1In this paper, an MLP refers to a model that mostly uses FC layers to linearly map features from a vector to another, so that the number of parameters must be proportional to the input size and output size. By our definition, another model, CycleMLP [3], is not referred to as an MLP.
eter sets s. With a smaller s for the lower-level layers and a larger s for the higher-level ones, we can balance the model size and the representational capacity.
Another drawback of the concurrently proposed MLPs is the difficulty of transferring to the downstream tasks like semantic segmentation. For example, MLP-Mixer [36] demonstrates satisfactory performance on ImageNet but does not qualify as the backbone of a segmentation frame-work like UperNet [41] as it aggressively embeds (i.e., downsamples) the images by 16× and repeatedly trans-forms the embeddings, so that it cannot produce multi-scale feature maps with different levels of semantic information.
In contrast, our hierarchical design produces semantic in-formation on different levels, so that it can be readily used as the backbone of the common downstream frameworks.
In summary, with Locality Injection, RepMLP Block and a hierarchical architecture, RepMLPNet achieves favorable accuracy-efficiency trade-off with only 100 training epochs on ImageNet, compared to the other MLP models trained in 300 or 400 epochs. We also show the universality of Lo-cality Injection as it improves the performance of not only
RepMLPNet but also ResMLP [37]. Moreover, RepMLP-Net shows satisfactory performance as the first attempt to transfer an MLP-style backbone to semantic segmentation. 2.