Abstract
The development of online economics arouses the demand of generating images of models on product clothes, to dis-play new clothes and promote sales. However, the expensive proprietary model images challenge the existing image vir-tual try-on methods in this scenario, as most of them need to be trained on considerable amounts of model images accom-panied with paired clothes images. In this paper, we propose a cheap yet scalable weakly-supervised method called Deep
Generative Projection (DGP) to address this specific sce-nario. Lying in the heart of the proposed method is to imitate the process of human predicting the wearing effect, which is an unsupervised imagination based on life experience rather than computation rules learned from supervisions. Here a pretrained StyleGAN is used to capture the practical expe-rience of wearing. Experiments show that projecting the rough alignment of clothing and body onto the StyleGAN space can yield photo-realistic wearing results. Experiments on real scene proprietary model images demonstrate the superiority of DGP over several state-of-the-art supervised methods when generating clothing model images.
Figure 1. Given a set of commercial models with underwear and different clothing images, the proposed method can generate realis-tic clothing model results with clear pattern reconstruction. 1.

Introduction
Taking pictures of models on product clothes is an es-sential demand for online apparel retails to display clothes and promote sales. However, it is highly expensive to hire models and professional studios to actually take those pic-tures of wearing each clothing. Thus image virtual try-on [11, 12, 15, 22, 24, 30, 42, 47, 53, 56], the technique that generates wearing results from clothing and model images, has rapidly aroused broad academic and industrial interests.
*Corresponding author
A practical algorithm, however, must consider the cost to apply it in industrial scenarios. Existing image virtual try-on (VTO) methods are highly expensive to train. Most of them [22, 28, 30, 41, 53, 56] are trained on paired image data of clothing and a person on that clothing. Such paired data consume considerable labor cost thus is infeasible to collect at scale. Also, the testing data of clothing model generation should be proprietary model images as only they can be legally used in the final display on e-shop websites.
Those images are highly expensive due to hiring commercial models and buying all necessary rights. Thus algorithms should avoid relying on those proprietary model images
during training, while managing to bear the non-negligible performance drop incurred by discrepancy in testing and training.
In response to those challenges, we propose Deep Genera-tive Projection (DGP), a powerful weakly-supervised method to yield realistic try-on results while training on cheap un-paired data collected from the Web, which is motivated by the procedure of people predicting how they will look like while picking clothes. It is an imagination based on life experience rather than rules learned from paired annotations.
People may pick up the clothes and align the clothes to their shoulders or necks, from which they then imagine the pic-ture of wearing those clothes. Following this idea, the DGP method reproduces this process in the clothing model gen-eration scenario. Given a clothing image and a proprietary model image, we first align the clothing to the model’s body dominated by a simple perspective transformation [40] of four body key points. Then we project this rough alignment onto the synthesis space of a pretrained StyleGAN [32, 33].
The StyleGAN is trained on abundant unsupervised fashion images collected from the Web. Thus, it represents real-world knowledge of wearing. A couple of semantic and pattern searches will then yield the realistic clothing model image, as is shown in Fig. 1. The whole algorithm needs no paired data or proprietary model images during training, thus it is practical for the industrial scenario.
In conclusion, the contributions of this paper include:
• We propose the first framework to generate clothing model images for online clothing shops, which has not received enough attention in the virtual try-on commu-nity;
• The proposed method consumes only unpaired data, and no proprietary model images during the training, which is more practical for industrial applications than most existing methods;
• Our weakly-supervised method significantly outper-forms some state-of-the-art supervised competitors in both numerical and visual quality, and demonstrates good robustness under preprocessing mistakes. 2.