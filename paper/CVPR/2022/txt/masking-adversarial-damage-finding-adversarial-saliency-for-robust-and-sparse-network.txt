Abstract
Adversarial examples provoke weak reliability and po-tential security issues in deep neural networks. Although adversarial training has been widely studied to improve adversarial robustness, it works in an over-parameterized regime and requires high computations and large memory budgets. To bridge adversarial robustness and model com-pression, we propose a novel adversarial pruning method,
Masking Adversarial Damage (MAD) that employs second-order information of adversarial loss. By using it, we can accurately estimate adversarial saliency for model parame-ters and determine which parameters can be pruned without weakening adversarial robustness. Furthermore, we reveal that model parameters of initial layer are highly sensitive to the adversarial examples and show that compressed fea-ture representation retains semantic information for the tar-get objects. Through extensive experiments on three pub-lic datasets, we demonstrate that MAD effectively prunes adversarially trained networks without loosing adversarial robustness and shows better performance than previous ad-versarial pruning methods. 1.

Introduction
Deep neural networks (DNNs) have achieved impressive performances in a wide variety of computer vision tasks (e.g., image classification [19,26], object detection [30,40], and semantic segmentation [5, 18]). Despite the break-through outcomes, DNNs are easily deceived from ad-versarial attacks with carefully crafted perturbations [4, 9, 32, 50].
Injecting these perturbations into benign images generates adversarial examples which hinder the decision-making process of DNNs. Although the perturbations are imperceptible to humans, they easily induce vulnerable fea-tures in DNNs [22, 23]. Due to such fragility, various deep learning applications have suffered from potential security issues that induce weak reliability of DNNs [2, 44, 55].
*Equal contribution. † Corresponding author.
Accordingly, to achieve robust and reliable DNNs, a lot of adversarial research have been dedicated to presenting powerful adversarial attack and defense algorithms in the sense of cat-and-mouse games. Among various methods, adversarial training (AT) [3, 9, 27, 32] has been widely stud-ied to improve adversarial robustness so far, where DNNs are trained with adversarial examples. Madry et al. [32] have shown that adversarially trained models are robust against several white-box attacks with the knowledge of model parameters. Besides, recent studies [56,62] have fur-ther enhanced the robustness by adding various regulariza-tions to fully utilize not only the adversarial examples but also benign examples for model generalization.
Orthogonal to the adversarial issue, most of the AT-based methods work in an over-parameterized regime to achieve the robustness, thus they induce higher compu-tations and require larger memory budgets than benign classifiers [32]. Thereby, applying AT-based methods to resource-constrained devices is burdensome and known as a critical limitation. To bridge adversarial robustness and model compression, several studies [37, 45, 47, 52] have introduced adversarial pruning mechanisms to reduce its model capacity while preserving the adversarial robustness.
In standard training procedure, a promising prun-ing method is to remove the lowest weight magnitudes (LWM) [14, 15], assuming that small magnitudes affect the least changes of the model prediction. Under its assump-tion, Sehwang et al. [45] have proposed a 3-step pruning method (pre-training, pruning, and fine-tuning) for the ad-versarial examples. Several works [12, 59] further have enhanced LWM pruning methods by employing alternat-ing direction method of multipliers (ADMM) [63] or Beta-Bernoulli dropout [31] to eliminate unuseful model param-eters. Recently, Sehwag et al. [46] have argued that despite successful results of LWM pruning, such heuristic pruning methods cause performance degradation when integrated with adversarial training. Accordingly, they have formu-lated a way of finding importance scores [39] and removed model parameters with low importance scores for adversar-ial training loss. We further focus on how to reflect corre-lations of each model parameter to prune multiple param-eters at once and theoretically formulate local geometry of adversarial loss to identify which combinatorial model pa-rameters affect model prediction in adversarial settings.
In this paper, we present a novel adversarial pruning method, namely Masking Adversarial Damage (MAD) that uses second-order information (Hessian) of the adversarial loss with masking model parameters. Motivated by Op-timal Brain Damage (OBD) [29] and Optimal Brain Sur-geon (OBS) [17] employing Hessian of the loss function, we devise a way of estimating adversarial saliency for model parameters by considering their correlated conjunc-tions, which can more precisely represent the connectivity of model parameters for the adversarial prediction.
In recent works [49, 53], approximating Hessian of mul-tiple parameters is regarded as computationally prohibitive.
Alternatively, computing each parameter’s importance and sorting it with pruning statistics is a practical approach in standard pruning. Bringing in such combinatorial problem into adversarial settings, we aim to prune adversarially less salient parameters that cannot arouse adversarial vulnera-bility. To that end, we first optimize masks for DNNs to be predictive to adversarial examples and approximate Hes-sian of the adversarial loss by utilizing optimized masks. To effectively compute Hessian, we introduce a block-wise K-FAC approximation that can consider the local geometry of the adversarial loss at multiple parameters points. Based on the change of the adversarial loss along with mask-applied parameters, we can track how sensitively specific conjunc-tions of model parameters respond to adversarial examples and compress DNNs without weakening the robustness.
For the proposed method, we thoroughly analyze ad-versarial saliency and compressed feature representation, and we reveal that: i) model parameters of initial layer are highly sensitive to adversarial perturbation, ii) MAD can retain semantic information of target objects even with the high pruning ratio. Through extensive experiments on
MAD in three major datasets, we corroborate that MAD can effectively compress DNNs without losing adversarial ro-bustness and show better adversarial defense performance than the previous adversarial pruning methods.
The major contributions of this paper are as follows:
• We present a novel adversarial pruning method, Mask-ing Adversarial Damage (MAD) which can precisely estimate adversarial saliency for model parameters us-ing second-order information.
• By analyzing MAD, we investigate that the model pa-rameters of initial layer are highly sensitive to adver-sarial examples, and compressed feature representa-tion retains semantic information of target objects.
• Through extensive experiments, we demonstrate the effectiveness of compression capability as well as the adversarial robustness for the proposed method. 2.