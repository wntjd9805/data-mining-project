Abstract
Supervised deep learning methods require a large repos-itory of annotated data; hence, label noise is inevitable.
Training with such noisy data negatively impacts the gener-alization performance of deep neural networks. To combat label noise, recent state-of-the-art methods employ some sort of sample selection mechanism to select a possibly clean subset of data. Next, an off-the-shelf semi-supervised learning method is used for training where rejected samples are treated as unlabeled data. Our comprehensive analysis shows that current selection methods disproportionately se-lect samples from easy (fast learnable) classes while reject-ing those from relatively harder ones. This creates class imbalance in the selected clean set and in turn, deterio-rates performance under high label noise.
In this work, we propose UNICON, a simple yet effective sample selec-tion method which is robust to high label noise. To address the disproportionate selection of easy and hard samples, we introduce a Jensen-Shannon divergence based uniform se-lection mechanism which does not require any probabilistic modeling and hyperparameter tuning. We complement our selection method with contrastive learning to further com-bat the memorization of noisy labels. Extensive experimen-tation on multiple benchmark datasets demonstrates the ef-fectiveness of UNICON; we obtain an 11.4% improvement over the current state-of-the-art on CIFAR100 dataset with a 90% noise rate. Our code is publicly available.1 1.

Introduction
Deep neural networks (DNNs) have proven to be highly effective in solving various computer vision tasks [9, 18, 22, 36, 43, 48, 49, 53, 62]. Most state-of-the-art (SOTA) meth-ods require supervised training with a large pool of anno-tated data [4, 8, 27, 28, 57]. Collecting and manually an-1https : / / github . com / nazmul - karim170 / UNICON -Noisy-Label
Figure 1. UNICON training overview: At each iteration, we em-ploy a uniform selection technique to partition the training set into clean and noisy sets. Upon separation, we perform SSL-training with an additional contrastive loss function. The uniform selection and subsequent SSL-training is repeated until convergence.
Noise Rate (%) 90% 92% 95% 98%
DMix [25]
UNICON (Ours) 76.08 90.81 57.62 87.61 51.28 80.82 17.18 50.63
Table 1. Classification performance (%) of the proposed method on CIFAR10 under severe label noise. notating such data is challenging and oftentimes very ex-pensive. Most large-scale data collection techniques rely on open-source web data that can be automatically anno-tated using search engine queries and user tags [33, 52].
This annotation scheme inevitably introduces label noise
[27, 57]. Training with such noisy labels is challenging since DNNs can effectively memorize arbitrary (noisy) la-bels over the course of training [2]. Combating label noise is one of the fundamental problems in deep learn-ing [15, 24, 38, 46, 54, 54, 58, 60, 61, 65], and is the focus of this study.
Training with noisy label data has been the subject of
many recent studies [12, 16, 31, 42, 45, 70]. Existing tech-niques can be categorized into two dominant groups: i) la-bel correction, [11,40] and ii) sample separation [12,25,66].
The former approach requires the estimation of noise tran-sition matrix, which is hard to estimate for high number of classes and in high noise scenarios. The latter approach tries to filter out the noisy samples from the clean ones based on the small-loss criterion [25], where the low-loss samples are assumed to have clean labels. Next, an off-the-shelf semi-supervised learning (SSL) technique [3,47] is used for train-ing where the selected noisy samples are treated as unla-beled data. However, the selection process is usually biased towards easy classes as clean samples from the hard classes (e.g. cats and dogs can be considered as hard classes in CI-FAR10 [21]) may produce high-loss values. This is more prominent at the early stage of training and can introduce class-disparity among the selected clean samples. Severe class-imbalance may lead to poor precision of sample se-lection, hence, sub-par classification performance.
In this work, we revamp the selection process from a more fundamental perspective. Our goal is to simplify the selection process by introducing an effective and scalable
Jensen-Shannon divergence based sample separation mech-anism. To address the disproportionate selection of easy and hard samples, we enforce a class-balance prior by se-lecting an equal number of clean samples from each class.
Such a prior improves the overall quality of pseudo-labels, and hence, significantly boosts the performance of subse-quent semi supervised learning-based training. In addition, we opt to employ unsupervised contrastive learning (CL) because of its inherent resistance (as labels are not required for training) to label noise memorization. We empirically show that unsupervised feature learning lowers memoriza-tion risk and improves the sample separation performance; especially under severe noise levels. We call this combined technique of UNIform selection and CONtrastive learning
UNICON (shown in Fig. 1), which is found to be effective even in the presence of very high label noise (see Table 1).
Our contributions are summarized as follows:
• We propose a simple yet effective uniform selection mechanism that ensures class-balancing among the selected clean samples. Through empirical analy-sis, we observe that class-uniformity helps in gener-ating higher quality pseudo-labels for samples from all classes irrespective of their difficulty level.
• We further minimize the risk of label noise memoriza-tion by performing unsupervised feature learning using contrastive loss. This in turn boosts the sample separa-tion performance.
• Our extensive experimentation demonstrates that UNI-CON achieves significant performance improvement over state-of-the-art methods, especially on datasets with severe label noise. 2.