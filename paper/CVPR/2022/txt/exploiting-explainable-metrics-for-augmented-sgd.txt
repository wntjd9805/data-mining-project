Abstract
Explaining the generalization characteristics of deep learning is an emerging topic in advanced machine learning.
There are several unanswered questions about how learning under stochastic optimization really works and why certain strategies are better than others. In this paper, we address the following question: can we probe intermediate layers of a deep neural network to identify and quantify the learning quality of each layer? With this question in mind, we pro-pose new explainability metrics that measure the redundant information in a network’s layers using a low-rank factor-ization framework and quantify a complexity measure that is highly correlated with the generalization performance of a given optimizer, network, and dataset. We subsequently exploit these metrics to augment the Stochastic Gradient De-scent (SGD) optimizer by adaptively adjusting the learning rate in each layer to improve in generalization performance.
Our augmented SGD – dubbed RMSGD – introduces min-imal computational overhead compared to SOTA methods and outperforms them by exhibiting strong generalization characteristics across application, architecture, and dataset. 1.

Introduction
The task of predicting network generalization perfor-mance using some measure of complexity based on training data is an emerging topic in the field of machine learning.
Development of such “explainability” metrics is vitally im-portant in order to understand and better explain the learn-ing mechanisms involved in training of a given optimizer, network, and dataset. Identifying the causal relationship between some metric and generalization gap (or even testing accuracy directly) in order to select optimal network topolo-gies or tune hyper-parameters is an important problem and actively researched today [10, 22, 23, 29, 36].
While the field of metric development for predicting gen-eralization performance is growing (see related works in
*Equally major contribution section 2), our interest in this work is to exploit such ex-plainability metrics to augment the training of deep neural networks (DNNS). We achieve this by defining new metrics – stable rank, condition number, and a quality measure – de-rived from the intermediate layers of DNNS during training to access the true knowledge in the underlying weights and express how well the network layers are functioning as high-quality autoencoders for knowledge representation. Using these metrics, we exploit their ability to quantify learning in order to augment stochastic gradient descent (SGD) by dynamically adjusting the learning rate.
From a different lens, this work also sheds light on the be-haviour of commonly practiced hyper-parameter tuning tech-niques like learning rate scheduling through decay methods
[12, 14, 17, 32, 33, 57] or functional methods [16, 32, 41, 42].
Little is understood about such methods and why they really work and their use becomes more like alchemy rather than analytical/empirical reasoning. We highlight how our met-rics provide reasonable explanation to these strategies and also how they can be used in a simple optimization frame-work to augment SGD and gain performance. Our main contributions are as follows:
[C1] We introduce new explainability metrics derived using training data that quantify layer-level learning of neural networks and are robust to the randomness of initialization.
[C2] We use these metrics to explain the training mecha-nisms of neural networks for various optimizers and datasets, and predict generalization characteristics in deep learning.
[C3] We exploit these metrics to augment SGD and intro-duce our new RMSGD optimizer, which gains considerable performance improvements at minimal computational cost and generalizes well across experimental configuration. 2.