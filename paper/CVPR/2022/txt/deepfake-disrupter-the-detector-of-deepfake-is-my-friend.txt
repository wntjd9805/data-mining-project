Abstract
In recent years, with the advances of generative mod-els, many powerful face manipulation systems have been developed based on Deep Neural Networks (DNNs), called
DeepFakes.
If DeepFakes are not controlled timely and properly, they would become a real threat to both celebri-ties and ordinary people. Precautions such as adding per-turbations to the source inputs will make DeepFake results look distorted from the perspective of human eyes. How-ever, previous method doesn’t explore whether the disrupted images can still spoof DeepFake detectors. This is critical for many applications where DeepFake detectors are used to discriminate between DeepFake data and real data due to the huge cost of examining a large amount of data man-ually. We argue that the detectors do not share a similar perspective as human eyes, which might still be spoofed by the disrupted data. Besides, the existing disruption meth-ods rely on iteration-based perturbation generation algo-rithms, which is time-consuming. In this paper, we propose a novel DeepFake disruption algorithm called “DeepFake
Disrupter”. By training a perturbation generator, we can add the human-imperceptible perturbations to source im-ages that need to be protected without any backpropaga-tion update. The DeepFake results of these protected source inputs would not only look unrealistic by the human eye but also can be distinguished by DeepFake detectors eas-ily. For example, experimental results show that by adding our trained perturbations, fake images generated by Star-GAN [5] can result in a 10 ∼ 20% increase in F1-score evaluated by various DeepFake detectors. 1.

Introduction
Face Manipulation has raised significant concerns within
It is a kind of technique that allows our digital society. people to modify the face’s identity, expression, and at-*Equal Contribution
Figure 1. (a) shows that advanced DeepFake manipulation mod-els can easily spoof human naked eye and DeepFake detector. (b) shows that after DeepFake Disruption, fake outputs become apparently distorted from the perspective of human eye, but can (c) shows that our proposed still spoof the DeepFake detector. method DeepFake disrupter can invalidate the DeepFake manipu-lation process from both human end and machine end. tributes in a given image or video. With the development and implementation of Deep Neural Networks (DNN), the recent manipulation methods could produce verisimilar re-sults that might fool human eyes. These DNN based meth-ods, called DeepFakes [3, 4, 11, 24, 25, 28], have attracted much attention from the public and researchers because the high-quality DeepFake results could lead to social and se-curity problems. For example, the public might think the victims did somethings that they never did due to the pres-ence of their identities in the DeepFake videos, thus ruining their reputations. The forgery data could also fool the secu-rity protocol by verifying the payment authorization system through fake personal information so that putting the vic-tims’ wealth at great risk.
As a response to the increasing concern of DeepFake, many defense methods are proposed. The first way is using detection models to distinguish Real and DeepFake data.
Multiple detection algorithms are introduced, including us-ing traditional DNN models for detection [7,18,21], analyz-ing the inconsistent within the DeepFake data [12, 30], and
extracting the synthesis signal as the evidence for discrim-ination [27]. On the other hand, recent research provides a new direction for defense, preventing attackers from syn-thesizing DeepFake images. These methods, called Deep-Fake Disruption, attempt to add small perturbations to the original images such that the corresponding DeepFake re-sults might be heavily distorted in visualization. Disrupt-ing DeepFakes [22] is a related work on image translation disruption framework to make image manipulation models generates fake images with human-perceptible distortions.
Although the recent DeepFake Disruption methods could prevent the DeepFake models from generating realistic re-sults, these kinds of methods still have some problems. In the real-world multi-media systems, it is extremely expen-sive to employ human observers to defend the DeepFake by manually examining every input image in the large vol-ume of vision data, even though the defects in the image produced by DeepFake are obvious. Instead, to automate the defense of DeepFake, it is prevalent and preferable to develop DeepFake detectors. However, although the ex-isting disruption methods could make the DeepFake’s out-put become distorted from the human eye, our experiment demonstrates that these visually unnatural samples can still spoof the DeepFake detectors since the human eye and neu-ral network share a different decision logic. What’s more, these recent disruption methods rely on iteration-based ad-versarial attack algorithms, e.g.
Iterative Fast Gradient
Sign Method(I-FGSM) [10] and Projected Gradient De-scent (PGD) [14], to find out the perturbation for each data, which is normally time-consuming.
We argue that we also need to consider the loss of the
DeepFake detector, such that the generated DeepFake re-sults of protected data are not only being recognized by the human eye but also can be detected by the detectors, and at the same time, the original data injected with perturba-tions can still be recognized as the real one. We should also use a perturbation generator to generate perturbation, which provides an end-to-end protection algorithm that can save time. Figure 1 shows the development of DeepFake disruption methods.
In this work, we propose a novel framework, called
DeepFake Disrupter, to defend against DeepFake with the help of the DeepFake detector. The DeepFake Disrupter is a perturbation generator that takes as input real images and outputs a human-imperceptible perturbation so as to make the data generated by the DeepFake models be identified as fake by DeepFake detector and human eyes; meanwhile, the original real inputs injected with perturbations can still be identified as real by DeepFake detector. We show that just making DeepFake outputs distorted from the human eye’s view is insufficient because the DeepFake detector may still be fooled by classifying the fake videos as real.
Experimental results on CelebA [13] and VoxCeleb1 [16] datasets demonstrate that the proposed DeepFake disrupter can effectively protect original real images/videos from be-ing used as a source for making DeepFake data. 2.