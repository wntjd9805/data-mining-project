Abstract
When coming up with phrases of movement, choreogra-phers all have their habits as they are used to their skilled dance genres. Therefore, they tend to return certain pat-terns of the dance genres that they are familiar with. What if artificial intelligence could be used to help choreogra-phers blend dance genres by suggesting various dances, and one that matches their choreographic style? Numer-ous task-specific variants of autoregressive networks have been developed for dance generation. Yet, a serious limita-tion remains that all existing algorithms can return repeated patterns for a given initial pose sequence, which may be in-ferior. To mitigate this issue, we propose MNET, a novel and scalable approach that can perform music-conditioned pluralistic dance generation synthesized by multiple dance genres using only a single model. Here, we learn a dance-genre aware latent representation by training a conditional generative adversarial network leveraging Transformer ar-chitecture. We conduct extensive experiments on AIST++ along with user studies. Compared to the state-of-the-art methods, our method synthesizes plausible and diverse out-puts according to multiple dance genres as well as gen-erates outperforming dance sequences qualitatively and quantitatively. 1.

Introduction
Dance has long been considered as a universal language that can share emotions more effectively than words. Nowa-days, many people share their life-log via short-form video apps such as TikTok and Youtube Shorts [27,54]. However, dancing is a highly creative and artistic process, hence pro-fessional training is often followed to express a feeling of elegant and rhythmic own story in a short-form video. For
*Corresponding author. this reason, the music-conditioned dance generation, de-spite significant progress, is a challenging task that should capture high kinematic complexity rhythmically.
Recently, deep autoregressive networks have been used to synthesize dance motions learning long-range dependen-cies with the input music. Most state-of-the-art (SOTA) methods [21, 29, 37, 38] exploit RNN or Transformer archi-tectures and generate dance for a given initial pose sequence with music. While previous studies produce temporally co-herence sequence according to music, we find that all exist-ing algorithms remain severely limited diversity by extend-ing a given initial pose sequence into a repeated patterns.
Although skilled dancers and choreographers often repeat dance patterns, they try to subtly diversify their dance lines.
So, the lack of diversity issue is critical to the dance synthe-sizing.
In this paper, we tackle the problem of the music-conditioned pluralistic dance generation synthesized by multiple dance genres. The key challenge of pluralis-tic dance generation is to produce perceptually realistic and various motions aligning to musical beats. To over-come this challenge, we propose a generic new approach that bridges the gap between music-conditional sequence-to-sequence learning and recent unconditional generative architectures via Transformer Conditional GAN.
Specifically, we leverage the generative capability from transformer decoder, embedding both conditional and stochastic representations via self-attention module.
By injecting a latent code and querying a certain duration of music and initial pose sequence, the proposed model en-ables the synthesis of diverse and consistent dance as shown in Figure 1.
However, to synthesize dance controlled by multiple dance genres, injecting latent code with conditions is an ineffective process in such multi-domain translation task, which can not be scalable to the increasing number of do-mains. As discussed in multi-domain image-to-image trans-Figure 1. Goal: We learn a Music-conditioned transformer NETwork (MNET) to generate diverse dance motions following beats. Given music, our model not only generates diverse sequences within one dance genre (left two rows) but also synthesizes various dance genres during the music (right two rows). lation literature [4,22,28,42,46], given k domains, k(k-1) generators are required to sufficiently handle translations between each and every domain, limiting their practical us-age. To address the scalability, we employ two modules, a mapping network and a multi-task discriminator, to the sequence-to-sequence generative learning inspired by [10].
The mapping network learns to transform random Gaussian noise into each dance genre code, which is termed style code for a specific domain. For the multi-task discrimi-nator, we take the role of the classifier to the transformer encoder, where the module performs per-style clssification as in the standard GAN setting. Considering multiple do-mains, both modules have multiple output branches. Fi-nally, our generator learns to successfully synthesize diverse dance motions over multiple domains with a single model utilizing an adversarial framework.
The main contribution of this work is three-fold: (1) We newly introduce MNET, a novel Transformer-based condi-tional GAN framework, and train it to generate pluralistic dance motions by sampling from each latent representation of multiple dance genres. (2) We demonstrate that it is pos-sible to learn to generate realistic and diverse dance motions which scalable to multiple dance genres in terms of visual quality and empirical metrics. (3) We present a comprehen-sive ablation studies of the architecture and loss components outperforming state-of-the-art performance on the AIST++ dataset, which contains 3D motions reconstructed from real dancers paired with music and multiple dance genres. Code will be available for research purposes. Project page 2.