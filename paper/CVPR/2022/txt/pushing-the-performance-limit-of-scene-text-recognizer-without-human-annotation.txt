Abstract
Scene text recognition (STR) attracts much attention over the years because of its wide application. Most method-s train STR model in a fully supervised manner which re-quires large amounts of labeled data. Although synthetic data contributes a lot to STR, it suffers from the real-to-synthetic domain gap the restricts model performance. In this work, we aim to boost STR models by leveraging both synthetic data and the numerous real unlabeled images, ex-empting human annotation cost thoroughly. A robust con-sistency regularization based semi-supervised framework is proposed for STR, which can effectively solve the instabili-ty issue due to domain inconsistency between synthetic and real images. A character-level consistency regularization is designed to mitigate the misalignment between character-s in sequence recognition. Extensive experiments on stan-dard text recognition benchmarks demonstrate the effective-ness of the proposed method. It can steadily improve exist-ing STR models, and boost an STR model to achieve new state-of-the-art results. To our best knowledge, this is the
ﬁrst consistency regularization based framework that ap-plies successfully to STR. 1.

Introduction
Scene text recognition (STR) is to recognize text in nat-ural scenes and is widely used in many applications such as image retrieval, robot navigation and instant translation.
Compared to traditional OCR, STR is more challenging be-cause of multiple variations from the environment, various
∗Part of the work was done when C.Zheng was an intern at SRCX.
†P. Wang is the corresponding author. (a) cross-domain. (b) in-domain.
Figure 1. Scene text recognition test accuracy by using supervised training, existing consistency regularization SSL (UDA [50] and
FixMatch [43]) and our method. Cross-domain means the labeled and unlabeled training data are from different domains (e.g. syn-thetic labeled vs. real unlabeled in our setting), while in-domain means they are from similar condition. UDA and FixMatch are feasible in in-domain condition but fail in cross-domain setting.
It is observed that the test accuracy drops drastically during the training process, and the highest accuracy is even lower than that obtained by supervised training. By contrast, our method is able to stabilize the training process and improve test performance in both in-domain and cross-domain conditions. font styles and complicated layouts.
Although STR has made great success, it is mainly re-searched in a fully supervised manner. Real labeled datasets in STR are usually small because the annotation work is ex-pensive and time-consuming. Hence, two large synthetic datasets MJSynth [16, 17] and SynthText [15] are common-ly used to train STR models and produce competitive result-s. However, there exists domain gap between synthetic and real data which restricts the effect of synthetic data. Brieﬂy speaking, synthetic dataset can improve STR performance, but STR model is still hungry for real data.
Considering that it is easy to obtain a large scale of unla-beled data in real world, many researchers intend to lever-age unlabeled data and train models in a Semi-Supervised
Learning (SSL) manner. Baek et al. [3] and Fang et al. [9] introduced self-training methods to train STR models and receive improved performance. Nevertheless, self-training requires a pre-trained model to predict pseudo-labels for un-labeled data and then re-trains the model, which affects the training efﬁciency. By contrast, Consistency Regularization (CR), another important component of state-of-the-art (SO-TA) SSL algorithms, has not been well exploited in STR.
In this paper, we would like to explore a CR-based SS-L approach to improve STR models, where only synthetic data and unlabeled real data are used for training, exempt-ing human annotation cost thoroughly. CR assumes that the model should output similar predictions when fed perturbed versions of the same image [38]. It tends to outperform self-training on several SSL benchmarks [1,36]. Nevertheless, it is non-trivial to utilize existing CR methods to STR directly.
We attempt to two representative CR approaches, UDA [50] and FixMatch [43]. Neither of them is feasible in our set-ting. As shown in Figure 1a, the models are quite unstable during the training process. Compared with experiments on image classiﬁcation where they show big superiority, we as-sume the reasons lie in the following two aspects. 1) Our labeled images are synthetic while unlabeled im-ages are from real scenarios. The domain gap between syn-thetic and real images affects the training stability. Actually, it is found that the collapsed models recognize synthetic in-puts with a reasonable accuracy, but generate nearly iden-tical outputs for all real inputs. We conjecture that they incorrectly utilize the domain gap to minimize the overal-l loss: they learn to distinguish between synthetic and real data, and learn reasonable representations for synthetic data to minimize the supervised loss, but simply project real data to identical outputs such that the consistency loss is zero. To validate this conjecture, we perform another experiment by using training images all from real. As shown in Figure 1b, the training processes of UDA and FixMatch become stable in such a setting. However, we aim to relieve human label-ing cost. The introduced domain gap becomes an issue. 2) Different from image classiﬁcation, STR is a kind of sequence prediction task. The alignment between character sequences brings another difﬁculty to consistency training.
To address the aforementioned problems, we propose a robust character-level consistency regularization based framework for STR. Firstly, inspired by BYOL [14] that prevents model collapse without using negative samples in contrastive learning, we propose an asymmetric consistency training structure for STR. Secondly, a character-level CR unit is proposed to ensure the character-level consistency during training process. Thirdly, some techniques are sub-tly adopted in training process, such as weight decay and domain adaption, which improve STR model furthermore.
The main contributions are summarized as follows: 1) We propose a robust consistency regularization based semi-supervised framework for STR. It is capable of tack-ling the cross-domain setting, thus more easily beneﬁtting from labelled synthetic data and unlabeled real data. Com-pared with self-training approaches, our method is more ef-ﬁcient, without iteratively predicting and re-training. 2) Considering the sequential property of text, we pro-pose a character-level consistency regularization (CCR) u-nit to ensure better sequence alignment between the outputs of two siamese models. 3) Extensive experiments are performed to analyze the effectiveness of the proposed framework. It boosts the per-formance of a variety of existing STR models. Despite free of human annotation, our method achieves new SOTA per-formance on several standard text recognition benchmarks for both regular and irregular text. 2.