Abstract
We propose Continuous Scene Representations (CSR), a scene representation constructed by an embodied agent navigating within a space, where objects and their rela-tionships are modeled by continuous valued embeddings.
Our method captures feature relationships between objects, composes them into a graph structure on-the-ﬂy, and situ-ates an embodied agent within the representation. Our key insight is to embed pair-wise relationships between objects in a latent space. This allows for a richer representation compared to discrete relations (e.g., [SUPPORT], [NEXT-TO]) commonly used for building scene representations.
CSR can track objects as the agent moves in a scene, up-date the representation accordingly, and detect changes in room conﬁgurations. Using CSR, we outperform state-of-the-art approaches for the challenging downstream task of visual room rearrangement, without any task speciﬁc train-ing. Moreover, we show the learned embeddings capture salient spatial details of the scene and show applicability to real world data. A summery video and code is available at prior.allenai.org/projects/csr. 1.

Introduction
To operate within a scene, embodied agents require a comprehensive representation of their surroundings. Such perceptual understanding should not be limited to determin-ing object identities, but should rather capture relationships between objects and between the agent and its surround-ings. An expressive scene representation should also allow for downstream task completion of interactive tasks without additional training (i.e., zero-shot inference).
Towards building such representations, scene graphs
[2, 29, 65] are a candidate as they provide a compact and explicit description of a scene.
In a typical scene graph pipeline, it is common to deﬁne a set of relationship labels, use them to manually annotate connections between objects in frames, and train a model to infer target graphs. However,
*Work done while SYG was a research intern at AI2. Correspondence to sy@cs.columbia.edu (a) (b)
Figure 1. Representing relationships. Our work, Continuous
Scene Representations, encodes object relationships into a graph with nodes and edges represented as feature vectors. (a) If an ob-ject is shufﬂed between two trajectories, different embeddings al-low for change detection. (b) As the agent moves within a trajec-tory, new nodes and edges are populated. once labels are deﬁned, there are inherent limitations. Once trained, models are restricted to a ﬁxed set of relationships.
Even if all relationships are modeled, how well can pre-deﬁned discrete symbols represent the complex relation-ships between objects? Consider [SUPPORT], a common semantic relation used in the literature [32, 62, 64]. The re-lationship “table supports mug” indicates the mug is on the table, but it does not capture where. In practice, the agent behavior may depend on this information.
Beyond the inherent limitations of modeling relation-ships as discrete symbols, scene graphs used in the literature are typically static [28, 34, 61] (i.e., they represent a snap-shot of a scene or a bundle of frames). In embodied settings, new objects are observed as an agent explores, and the scene representation should update on-the-ﬂy. If an agent returns to a position, it should determine if objects have moved.
Inspired by these observations, we develop a scene rep-resentation that is more suitable for embodied AI tasks. We propose Continuous Scene Representations (CSR), a novel approach to construct a scene representation from egocen-tric RGB images as an embodied agent explores a scene.
To address limitations of traditional scene graphs, our goal is to represent relations between objects as continuous vec-tors and update the representation on-the-ﬂy as the agent moves. To enable downstream interactive task execution without additional training, we also present a simple strat-egy for planning with respect to the representation.
Constructing such a scene representation interactively poses various challenges. The graph should accommodate the new objects, and relationships with other objects should be inferred. A successful algorithm should also determine when detections correspond to the same object even in dif-ferent views. To tackle these challenges, our idea is to learn object relational embeddings via a contrastive loss to repre-sent the nodes and edges of a scene representation. In this scheme, modeling is not constrained to a pre-deﬁned set of symbols. The agent maintains a memory of previously encountered embeddings. As the agent extracts new em-beddings from egocentric observations, it compares them to the memory to determine which embeddings are new and which already exist. This can be used to detect changes in the scene and update the representation as shown in Fig. 1.
We perform experiments using the AI2-THOR [30] framework and the YCB-Video dataset [59]. We support experimentally that (1) without any task speciﬁc training, a simple planning approach that employs CSR as the underly-ing representation, outperforms a map-based, reinforcement and imitation learning baseline trained directly on the task of room rearrangement [4, 52]. (2) CSR is able to capture commonly used discrete relations that can be extracted via linear probes. (3) CSR effectively captures spatial relation-ship between objects within a scene. (4) Without any ﬁne-tuning or hyperparameter tuning, a CSR trained on AI2-THOR is able to track objects over time in real world YCB-Video [59], which contains objects unseen during training. 2.