Abstract
Recently, video transformers have shown great success in video understanding, exceeding CNN performance; yet existing video transformer models do not explicitly model objects, although objects can be essential for recognizing
In this work, we present Object-Region Video actions.
Transformers (ORViT), an object-centric approach that ex-tends video transformer layers with a block that directly incorporates object representations. The key idea is to fuse object-centric representations starting from early lay-ers and propagate them into the transformer-layers, thus af-fecting the spatio-temporal representations throughout the network. Our ORViT block consists of two object-level
In the appearance streams: appearance and dynamics. stream, an “Object-Region Attention” module applies self-attention over the patches and object regions. In this way, visual object regions interact with uniform patch tokens and enrich them with contextualized object information.
We further model object dynamics via a separate “Object-Dynamics Module”, which captures trajectory interactions, and show how to integrate the two streams. We evalu-ate our model on four tasks and five datasets: composi-tional and few-shot action recognition on SomethingElse, spatio-temporal action detection on AVA, and standard ac-tion recognition on Something-Something V2, Diving48 and
Epic-Kitchen100. We show strong performance improve-ment across all tasks and datasets considered, demonstrat-ing the value of a model that incorporates object repre-sentations into a transformer architecture. For code and pretrained models, visit the project page at https:// roeiherz.github.io/ORViT/ 1.

Introduction
Consider the simple action of “Picking up a coffee cup”
Intuitively, a human recognizing this action in Figure 1. would identify the hand, the coffee cup and the coaster, and perceive the upward movement of the cup. This high-lights three important cues needed for recognizing actions: what/where are the objects? how do they interact? and how do they move? The above perception process allows easy generalization to different compositions of actions. For ex-ample, the process of “picking up a knife” shares some of the components with “picking up a coffee cup”, namely, the way the object and hand move together. More broadly, representing image semantics using objects facilitates com-positional understanding, because many perceptual compo-nents remain similar when one object is swapped for an-other. Thus, a model that captures this compositional aspect potentially requires less examples to train.
It seems intuitively clear that machine vision models
should also benefit from such object-focused representa-tions, and indeed this has been explored in the past [29, 69] and more recently by [3,63,83], who utilize bounding boxes for various video understanding tasks. However, the central question of what is the best way to process objects informa-tion still remains. Most object-centric methods to video un-derstanding take a post-processing approach. Namely, they compute object descriptors using a backbone and then re-estimate those based on other objects via message passing or graph networks without propagating the object informa-tion into the backbone. Unlike these approaches, we argue that objects should influence the spatio-temporal represen-tations of the scene throughout the network, starting from the early layers (i.e., closer to the input). We claim that self-attention in video transformers is a natural architecture to achieve this result by enabling the attention to incorporate objects as well as salient image regions.
Video transformers have recently been introduced as powerful video understanding models [2, 7, 30, 65], moti-vated by the success of transformers in language [17] and vision [10,19]. In these models, each video frame is divided into patches, and a self-attention architecture obtains a con-textualized representation for the patches. However, this approach has no explicit representation of objects. Our key observation is that self-attention can be applied jointly to object representations and spatio-temporal representations, thus offering an elegant and straightforward mechanism to enhance the spatio-temporal representations by the objects.
Motivated by the above, our key goal in this paper is to explicitly fuse object-centric representations into the spatio-temporal representations of video-transformer archi-tectures [2], and do so throughout the model layers, start-ing from the earlier layers. We propose a general approach for achieving this by adapting the self-attention block [19] to incorporate object information. The challenge in build-ing such an architecture is that it should have components for modeling the appearance of objects as they move, the interaction between objects, and the dynamics of the ob-jects (irrespective of their visual appearance). An additional desideratum is that video content outside the objects should not be discarded, as it contains important contextual infor-mation. In what follows, we show that a self-attention ar-chitecture can be extended to address these aspects. Our key idea is that object regions can be introduced into trans-formers in a similar way to that of the regular patches, and dynamics can also be integrated into this framework in a natural way. We refer to our model as an “Object-Region
Video Transformer” (or ORViT).
We introduce a new ORViT block, which takes as input bounding boxes and patch tokens (also referred to as spatio-temporal representations) and outputs refined patch tokens based on object information. Within the block, the infor-mation is processed by two separate object-level streams: an “Object-Region Attention” stream that models appear-ance and an “Object-Dynamics Module” stream that models trajectories.1 The appearance stream first extracts descrip-tors for each object based on the object coordinates and the patch tokens. Next, we append the object descriptors to the patch tokens, and self-attention is applied to all these to-kens jointly, thus incorporating object information into the patch tokens (see Figure 1). The trajectory stream only uses object coordinates to model the geometry of motion and performs self-attention over those. Finally, we re-integrate both streams into a set of refined patch tokens, which have the same dimensionality as the input to our ORViT block.
This means that the ORViT block can be called repeatedly.
See Figure 2 and Figure 4 for visualizations.
We evaluate ORViT on several challenging video-understanding tasks: compositional and few-shot action recognition on SomethingElse [63], where bounding boxes are given as part of the input; spatio-temporal action de-tection on AVA [28], where the boxes are obtained via an off-the-shelf detector that was provided by previous methods; and in a standard action recognition task on
Something-Something V2 [26], Diving48 [54] and Epic-Kitchen100 [16], where we use class-agnostic boxes from an off-the-shelf detector.
Through extensive empirical study, we show that integrating the ORViT block into the video transformer architecture leads to improved results on all tasks. These results confirm our hypothesis that incorpo-rating object representations starting from early layers and propagating them into the spatio-temporal representations throughout the network, leads to better performance. 2.