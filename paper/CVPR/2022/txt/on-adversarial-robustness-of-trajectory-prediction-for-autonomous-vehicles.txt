Abstract
Trajectory prediction is a critical component for au-tonomous vehicles (AVs) to perform safe planning and navi-gation. However, few studies have analyzed the adversarial robustness of trajectory prediction or investigated whether the worst-case prediction can still lead to safe planning. To bridge this gap, we study the adversarial robustness of tra-jectory prediction models by proposing a new adversarial attack that perturbs normal vehicle trajectories to maxi-mize the prediction error. Our experiments on three models and three datasets show that the adversarial prediction in-creases the prediction error by more than 150%. Our case studies show that if an adversary drives a vehicle close to the target AV following the adversarial trajectory, the AV may make an inaccurate prediction and even make unsafe driving decisions. We also explore possible mitigation tech-niques via data augmentation and trajectory smoothing. 1.

Introduction
Autonomous vehicles (AVs) are transforming the trans-portation systems. AV is a complex system integrating a pipeline of modules such as perception of obstacles, plan-ning of driving behaviors, and controlling of the physical vehicle [1, 3]. Specifically, trajectory prediction in the per-ception module predicts the future trajectories of nearby moving objects. The prediction is essential for the plan-ning module and affects AV’s driving behavior. Therefore, accurate trajectory prediction is critical for safe AV driving.
Many studies propose trajectory prediction models based on deep neural networks [8, 10, 11, 13, 21, 23, 24, 27, 32, 33, 35, 38, 41, 42]. They evaluate the models on benchmarks collected from real world using the average ℓ2 distance be-tween ground truth and predicted trajectories as the key metric. However, few studies evaluate trajectory prediction models from the perspective of security or analyze the ro-bustness against adversarial examples. For trajectory pre-diction, if the adversary can control the position of a vehi-cle close to the target AV, e.g., by driving the vehicle along a crafted trajectory, the adversary can influence the AV’s trajectory prediction and driving behaviors.
To bridge this gap, we propose new white/black box ad-versarial attacks on trajectory prediction, which adds minor perturbation on normal trajectories to maximize the pre-diction error. Compared with adversarial attacks on im-age/video classification, attacking trajectory prediction is unique in two aspects. First, the attack requires natural-ness [40] of the adversarial examples. Adversarial trajec-tories are natural if they obey physical rules and are possi-ble to happen in the real world. With naturalness, the tra-jectories can be reproduced by the attacker-controlled ve-hicle in the real world and cannot be easily classified as anomaly by AVs. To realize naturalness, we enforce con-straints on physical properties (e.g., velocity and accelera-tion) of the perturbed trajectory during optimization solv-ing. Second, we need to define optimization objectives that are semantically-attractive for attackers targeting trajectory prediction. To this end, we find multiple attractive attack dimensions can co-exist even for the same scenario (e.g., causing the predicted trajectory to deviate laterally or lon-gitudinally are both of interest to attackers in AV context).
Thus, in our attack design we consider different metrics of prediction error as optimization objectives, e.g., average lat-eral/longitudinal deviation to four different directions.
We evaluate the proposed attacks on 10 different com-binations of prediction models [18, 20, 30] and trajectory datasets [2, 5, 16]. The results show that the adversarial per-turbation can substantially increase the prediction error by around 150%. 62.2% of attacks cause prediction to devi-ate by more than half of the lane width, which are likely to significantly alter AV’s navigation decisions. In addition, we thoroughly analyze how various factors impact the at-tack results and make recommendations for improvements of implementing trajectory prediction such as leveraging map information and driving rules. We also explore miti-gation mechanisms against adversarial trajectories through data augmentation and trajectory smoothing, which reduce the prediction error under attacks by 28%. In general, our work exposes the necessity of evaluating the worst-case per-formance of trajectory prediction. The hard cases involving natural but adversarial trajectories generated by attacks have critical safety concerns (e.g., causing hard brakes or even
collisions) as demonstrated by our case studies.
Our main contributions are summarized as follows:
• We propose the first adversarial attack and adversarial ro-bustness analysis on trajectory prediction for AVs consid-ering real-world constraints and impacts.
• We report a thorough evaluation of adversarial attacks on various prediction models and trajectory datasets.
• We explore mitigation methods against adversarial exam-ples via data augmentation and trajectory smoothing. 2.