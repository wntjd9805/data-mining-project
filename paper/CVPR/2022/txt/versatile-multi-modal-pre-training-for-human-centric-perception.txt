Abstract are available at https://github.com/hongfz16/
HCMoCo.
Human-centric perception plays a vital role in vision and graphics. But their data annotations are prohibitively ex-pensive. Therefore, it is desirable to have a versatile pre-train model that serves as a foundation for data-efficient downstream tasks transfer. To this end, we propose the
Human-Centric Multi-Modal Contrastive Learning frame-work HCMoCo that leverages the multi-modal nature of hu-man data (e.g. RGB, depth, 2D keypoints) for effective rep-resentation learning. The objective comes with two main challenges: dense pre-train for multi-modality data, effi-cient usage of sparse human priors. To tackle the chal-lenges, we design the novel Dense Intra-sample Contrastive
Learning and Sparse Structure-aware Contrastive Learning targets by hierarchically learning a modal-invariant latent space featured with continuous and ordinal feature distri-bution and structure-aware semantic consistency. HCMoCo provides pre-train for different modalities by combining het-erogeneous datasets, which allows efficient usage of exist-ing task-specific human data. Extensive experiments on four downstream tasks of different modalities demonstrate the ef-fectiveness of HCMoCo, especially under data-efficient set-tings (7.16% and 12% improvement on DensePose Estima-tion and Human Parsing). Moreover, we demonstrate the versatility of HCMoCo by exploring cross-modality super-vision and missing-modality inference, validating its strong ability in cross-modal association and reasoning. Codes (cid:0) Corresponding author 1.

Introduction
As a long-standing problem, human-centric perception has been studied for decades, ranging from sparse predic-tion tasks, such as human action recognition [8, 27, 42, 50], 2D keypoints detection [2, 26, 43, 48] and 3D pose estima-tion [22, 31, 40], to dense prediction tasks, such as human parsing [7, 11, 12, 25] and DensePose prediction [14]. Un-fortunately, to train a model with reasonable generalizabil-ity and robustness, an enormous amount of labeled real data is necessary, which is extremely expensive to collect and an-notate. Therefore, it is desirable to have a versatile pre-train model that can serve as a foundation for all the aforemen-tioned human-centric perception tasks.
With the development of sensors, the human body can be more conveniently perceived and represented in multi-ple modalities, such as RGB, depth, and infrared. In this work, we argue that the multi-modality nature of human-centric data can induce effective representations that trans-fer well to various downstream tasks, due to three major advantages: 1) Learning a modal-invariant latent space through pre-training helps efficient task-relevant mutual in-formation extraction. 2) A single versatile pre-train model on multi-modal data facilitates multiple downstream tasks using various modalities. 3) Our multi-modal pre-train set-ting bridges heterogeneous human-centric datasets through their common modality, which benefits the generalizability of pre-train models.
We mainly explore two groups of modalities as shown in
Fig. 1 a): dense representations (e.g. RGB, depth, infrared) and sparse representations (e.g. 2D keypoints, 3D pose).
Dense representations can provide rich texture and/or 3D geometry information. But they are mostly low-level and noisy. On the contrary, sparse representations obtained by off-the-shelf tools [4, 9] are semantic and structured. But the sparsity results in insufficient details. We highlight that it is non-trivial to integrate these heterogeneous modalities into a unified pre-training framework for the following two main challenges: 1) learning representations suitable for dense prediction tasks in the multi-modality setting; 2) us-ing weak priors from sparse representations effectively for pre-training.
Challenge 1: Dense Targets. Existing methods [21,30] perform contrastive learning densely on pixel-level features to achieve view-invariance for dense prediction tasks. How-ever, those methods require multiple views of a static 3D scene [10], which is inapplicable for human-centric appli-cations with only single view. Furthermore, it is preferable to learn representations that are continuously and orderly distributed over the human body. In light of this, we gener-alize the widely used InfoNCE [33] and propose a dense intra-sample contrastive learning objective that applies a soft pixel-level contrastive target, which can facilitate learn-ing ordinal and continuous dense feature distributions.
Challenge 2: Sparse Priors. To employ priors in con-trastive learning, previous works [3, 23, 46] mainly use the supervision to generate semantically positive pairs. How-ever, these methods only focus on the sample-level con-trastive learning, which means each sample is encoded to a global embedding. It is not optimal for human dense predic-tion tasks. To this end, we propose a sparse structure-aware contrastive learning target, which uses semantic correspon-dences across samples as positive pairs to complement pos-itive intra-sample pairs. Particularly, leveraging sparse hu-man priors leads to an embedding space where semantically corresponding parts are aligned more closely.
To sum up, we propose HCMoCo, a Human-Centric multi-Modal Contrastive learning framework for versatile multi-modal pre-training. To fully leverage multi-modal observations, HCMoCo effectively utilizes both dense mea-surements and sparse priors using the following three-levels hierarchical contrastive learning objectives: 1) sample-level modality-invariant representation learning; 2) dense intra-sample contrastive learning; 3) sparse structure-aware contrastive learning. As an effort towards establishing a comprehensive multi-modal human parsing benchmark dataset, we label human segments for RGB-D images from
NTU RGB+D dataset [42], and contribute the NTURGBD-Parsing-4K dataset. To evaluate HCMoCo, we trans-fer our pre-train model to four human-centric downstream tasks using different modalities, including DensePose es-timation (RGB) [14], human parsing using RGB [22] or depth frames, and 3D pose estimation (depth) [16]. Un-der full set and data-efficient training settings, HCMoCo constantly achieves better performance than training from scratch or pre-train on ImageNet. To name a few, as shown in Fig. 1 b), we achieve 7.16% improvement in terms of GPS AP on 10% training data of DensePose estima-tion; 12% improvement in terms of mIoU on 20% training data of Human3.6M human parsing. Moreover, we eval-uate the modal-invariance of the latent space learned by
HCMoCo for dense prediction on NTURGBD-Parsing-4K with two settings: cross-modality supervision and missing-modality inference. Compared against conventional con-trastive learning targets, our method improves the segmen-tation mIoU by 29% and 24% for the two settings, respec-tively. To the best of our knowledge, we are the first to study multi-modal pre-training for human-centric perception.
The main contributions are summarized below: 1) As the first endeavor, we provide an in-depth analysis for human-centric pre-training, which is formulated as a chal-lenging multi-modal contrastive learning problem. 2) To-gether with the novel hierarchical contrastive learning ob-jectives, a comprehensive framework HCMoCo is pro-posed for effective pre-training for human-centric tasks. 3)
Through extensive experiments, HCMoCo achieves supe-rior performance than existing methods, and meanwhile shows promising modal-invariance properties. 4) To bene-fit multi-modal human-centric perception, we contribute an
RGB-D human parsing dataset, NTURGBD-Parsing-4K. 2.