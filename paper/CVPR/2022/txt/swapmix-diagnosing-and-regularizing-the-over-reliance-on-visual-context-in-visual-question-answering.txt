Abstract
While Visual Question Answering (VQA) has progressed rapidly, previous works raise concerns about robustness of current VQA models.
In this work, we study the robust-ness of VQA models from a novel perspective: visual con-text. We suggest that the models over-rely on the visual context, i.e., irrelevant objects in the image, to make pre-dictions. To diagnose the models’ reliance on visual con-text and measure their robustness, we propose a simple yet effective perturbation technique, SwapMix. SwapMix perturbs the visual context by swapping features of irrele-vant context objects with features from other objects in the dataset. Using SwapMix we are able to change answers to more than 45% of the questions for a representative VQA model. Additionally, we train the models with perfect sight and find that the context over-reliance highly depends on the quality of visual representations. In addition to diag-nosing, SwapMix can also be applied as a data augmen-tation strategy during training in order to regularize the context over-reliance. By swapping the context object fea-tures, the model reliance on context can be suppressed ef-fectively. Two representative VQA models are studied us-ing SwapMix: a co-attention model MCAN and a large-scale pretrained model LXMERT. Our experiments on the popular GQA dataset show the effectiveness of SwapMix for both diagnosing model robustness, and regularizing the over-reliance on visual context. The code for our method is available at https://github.com/vipulgupta1011/swapmix 1.

Introduction
Visual Question Answering (VQA) is a challenging task that requires a model to answer open-ended questions based on images.
In recent years, VQA performance is greatly boosted by different techniques including intra- and inter-modality attentions [7,49], large scale multi-modal pretrain-ing [26, 27, 42], etc. However, previous works study the robustness of VQA models and show that the models may
Figure 1. VQA models over-rely on visual context. By swapping features of irrelevant context objects, we can perturb the model prediction. Here the tennis ball (in yellow box) is an irrelevant context object for the question. Changing feature of the tennis ball to feature of soccer ball results in change in model prediction. exploit language prior [2, 33, 34], statistical bias [1, 16] or dataset shortcuts [20, 21] to answer questions.
While previous works studied VQA robustness from the perspective of language context, in this work, we study the robustness of VQA models from a different view: visual context. The visual context refers to the background in the image or the irrelevant objects that are not needed during the reasoning process to answer the question. For example, in
Figure 1, the tennis ball is irrelevant for the question ”What color is the women’s dress”, so we say it is a context object.
Ideally, a model with real perception and reasoning ability should be robust to the irrelevant context. However, in our work, we find that VQA models are vulnerable to context changes, which suggests the models’ over-reliance on the irrelevant context in the image.
To study the role of visual context, we propose a simple perturbation strategy named SwapMix, which perturbs the visual context by swapping features of context object with features from another object in the dataset. We first identify the visual features corresponding to irrelevant objects in the image, then randomly swap them with feature vectors of an-other similar object from the dataset. For example, in Figure 1, the tennis ball is a context object for the given question,
so we swap tennis ball feature vector with a feature vector of soccer ball. The swapping confuses the model to mis-recognize the color of the dress. In the swapping process, we carefully control the swapped objects to ensure that the new object is compatible to the scene (e.g., we don’t want to change the ball into a car).
Surprisingly, by perturbing the irrelevant context, more than 45% of the correct answers get changed. This reveals that VQA models highly rely on the context in the image, thus are vulnerable to context perturbations. The model may utilize shortcut correlations in the visual context to make predictions. We diagnose two representative VQA models: MCAN [49] as representative for attention-based models, and LXMERT [42] as representative for large-scale pretrained models. Our experiments show that LXMERT is much more robust to context perturbations, which indicates that large-scale pretraining may increase model robustness.
We further find that the context over-reliance highly de-pends on the quality of visual representations: a perfect sighted model relies much less on context. We achieve this by replacing the visual representations 1 with the ground-truth object and attribute encoding, which can be viewed as gold visual representation that provides the model the perfect sight. By studying this perfectly sighted model, we can exclude the influence of imperfect visual percep-tion, thus purely focus on the reliance on relevant objects in the reasoning process. Our results shows that by pro-viding VQA models with the perfect visual encoding, the answer changes are greatly reduced from 45.0% to 16.4% (for MCAN model). This suggests that models trained with perfect visual representations are more robust and that the context over-reliance largely comes from the imperfection of visual perception features.
In addition to diagnosing context over-reliance, Swap-Mix can also be used as a data augmentation technique dur-ing training. In training, we randomly swap a part of the context features with other object features from the dataset.
This forces the model to focus more on relevant objects in the image and less on irrelevant context. Our empirical re-sults show that by applying SwapMix in training, the model robustness improves by more than 40% and effective accu-racy improves by more than 5% on GQA dataset [19].
Our main contributions in this paper are three-fold. First, we are the first to study VQA robustness from the perspec-tive of visual context. With our simple context perturbation strategy named SwapMix, we benchmark robustness of two representative VQA models and find their over-reliance on visual context. Second, we find that a perfect sighted model relies much less on visual context. We provide models with perfect visual encodings and observe the improvement in model robustness. Third, we define 2 metrics, context re-1Majority of VQA models use object features extracted by pretrained object detectors as visual representation. liance and effective accuracy and shows improvement by using SwapMix as data augmentation technique. 2.