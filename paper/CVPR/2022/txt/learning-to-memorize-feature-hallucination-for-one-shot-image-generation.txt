Abstract
This paper studies the task of One-Shot image Gener-ation (OSG), where generation network learned on base dataset should be generalizable to synthesize images of novel categories with only one available sample per novel category. Most existing methods for feature transfer in one-shot image generation only learn reusable features implic-itly on pre-training tasks. Such methods would be likely
In this paper, we propose to overfit pre-training tasks. a novel model to explicitly learn and memorize reusable features that can help hallucinate novel category images.
To be specific, our algorithm learns to decompose im-age features into the Category-Related (CR) and Category-Independent(CI) features. Our model learning to memo-rize class-independent CI features which are further utilized by our feature hallucination component to generate target novel category images. We validate our model on several benchmarks. Extensive experiments demonstrate that our model effectively boosts the OSG performance and can gen-erate compelling and diverse samples. 1.

Introduction
As humans, our knowledge of concepts and the rich imagination ability may allow us to visualize or ‘halluci-*Corresponding author. This work was supported in part by NSFC un-der Grant (No. 62076067), and SMSTM Project (2018SHZDZX01). nate’ what the given image of the novel object would look like in other poses, viewpoints, or background, as shown in
Fig. 1. Essentially, humans can robustly learn novel con-cepts with very little supervision, benefiting from the well-known ability of learning to learn. Inspired by such ability, previous works [6, 26, 28] study the recognition task in the low-data regime. In contrast, this paper addresses the task of One-Shot image Generation (OSG), which is defined as learning to synthesize images of a novel category with only one training example. Especially, the newly synthesized im-ages should be visually similar to the given example. For example, given a new example of a novel target category in
Fig. 1, the OSG task aims at generating new possible ani-mal images by implicitly varying their key attributes, such as poses, viewpoints, and actions while crucially not chang-ing the category of the example image.
Extensive efforts have been devoted to the one-shot im-age generation task. Specifically, some few-shot recogni-tion models [31, 35] explore the generative models as data-augmentation methods, while these methods do not neces-sitate generating images of good visual quality. Then, to reduce the cost, researchers [16, 24] study training GANs using only a few images and produce high-quality images of good texture yet lacking semantic information. On the other hand, there are many transfer learning-based meth-ods [14, 21, 33, 34] that transfer the pre-training model to the target task with only a few training samples. In these works, the models pre-trained on large datasets are adapted to some specific novel tasks or domains.
tures with supervision from the category label, while the CI encoder projects CI features onto a memory from the given image. The memory serves as a dictionary of the CI fea-tures. To efficiently utilize the memory, a novel addresser network is presented in our work. Note that since there is no directly labeled supervision for the CI encoder, we intro-duce an implicit supervision strategy at the pairwise level.
Particularly, given two different images from the same cat-egory, we assume these two images have the same CR, and yet different CI features. In the training stage, we randomly select two CI features from memory and combine them with the same CR feature; and we encourage the generator to synthesize the image differently. Simultaneously, we en-force the classifier to predict the label of the reconstructed image the same as the original category. We thus define such pairwise relationships as diversity loss to supervise our
MFH, which is learned in an end-to-end manner. In the test-ing stage, we use the CR features from the input image and sample the CI features from the memory. Then we employ the generator to hallucinate the new images. Extensive ex-periments on two benchmarks validate the efficacy of our model.
Contributions We highlight several key contributions here: (i) We propose a novel method of learning to memorize fea-ture hallucination for the task of OSG. (ii) Our MFH has the component of L2M and FeaHa. The L2M learns how to disentangle image features and repurpose the memory struc-ture to preserve the CI features. By sampling from memory, our feature hallucination component can produce new im-ages. (iii) To efficiently learn the class-independent CI fea-tures, we present a novel pairwise supervision strategy to help model explicitly learn features that can be reused in one-shot generation tasks. The learned CI features can con-sistently represent interpretable and meaningful concepts of various categories. (iv)Interestingly, we show that the newly synthesized images by our MFH can be directly employed as additional training instances, thus can boost the perfor-mance of one-shot classification. 2.