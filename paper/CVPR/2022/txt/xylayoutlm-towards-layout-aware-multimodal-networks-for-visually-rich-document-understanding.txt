Abstract
Recently, various multimodal networks for Visually-Rich Document Understanding(VRDU) have been pro-posed, showing the promotion of transformers by integrat-ing visual and layout information with the text embeddings.
However, most existing approaches utilize the position em-beddings to incorporate the sequence information, neglect-ing the noisy improper reading order obtained by OCR tools. In this paper, we propose a robust layout-aware mul-timodal network named XYLayoutLM to capture and lever-age rich layout information from proper reading orders pro-duced by our Augmented XY Cut. Moreover, a Dilated Con-ditional Position Encoding module is proposed to deal with the input sequence of variable lengths, and it additionally extracts local layout information from both textual and vi-sual modalities while generating position embeddings. Ex-periment results show that our XYLayoutLM achieves com-petitive results on document understanding tasks. 1.

Introduction
While significant progress has been made in natural lan-guage processing and visual understanding [5, 7, 8, 20], less attention has been paid to their challenging variant in the multimodal document understanding domain. The
Visually-Rich Document Understanding (VRDU) [28] task requires combining the abundant image, text, and layout information from scanned/digital-born documents (images,
PDFs, etc.) Such technology can benefit a great variety of scenarios such as report/receipt understanding, automatical form filling, and document relation extraction. As a result, it is in great need of effective and efficient VRDU approaches.
To this end, researchers have developed sophisticated
*Corresponding author. pipelines for tackling this task [2, 10, 16, 18, 28–30]. Gen-erally speaking, early attempts can be divided into the cate-gories of textual-based [4, 6, 10], convolution-based [12, 15, 24, 26, 34] and GCN-based [19] methods. Text-based meth-ods, e.g., XLM-RoBERT [6] and InfoXLM [4], usually rely on the representation ability of self-supervised models like
Bert [7] pretrained on large datasets. Convolution-based method Chargrid [15] utilized a fully convolutional network that predicted a segmentation mask and bounding boxes for document representation. More recently, [19] introduces a
Graph Convolutional Networks based model to fuse the tex-tual and visual feature from scanned documents. attempts
Although like LayoutLM [28],
Lay-outLMv2 [30] and LayoutXLM [29] have been made to tackle document understanding in a multimodal manner, they still confront two limitations: (1) They rely on the tokens and boxes from OCR [31] tools without exploring the effect of reading orders. The proper reading orders refer to the well-organized readable token sequences, which may not be unique.
Intuitively, the reading order of input tokens is crucial to many tasks such as language translation [27] and VQA [33]. For example, the meaning of a sentence may be changed when we shuffle the words, resulting in mistakes during language translation. A common solution is to use position embeddings to denote such sequential order of input tokens. However, we find that multimodal models with widely-used relative position embeddings still suffer improper reading order. Proper reading orders implicitly include the layout information, (2) They which is essentially needed in VRDU tasks. usually leverage fixed-length absolute/relative position embeddings in transformers. Once the model is trained, it can not deal with the test data with longer token sequences.
Although bilinear interpolation on position embeddings can be applied to the longer sequence, the performance is not satisfying. Recently, Conditional Position Encoding
tracting local layouts. We demonstrate that the XYLay-outLM can lead to better performance than previous Lay-outLMs [28–30], which will benefit a great variety of real-world document understanding applications. We summa-rize our contributions as follows.
• For the first time, Augmented XY Cut is proposed and utilized to sort the input tokens for generating different proper reading orders in VRDU tasks. It extracts and leverages the layout information to achieve competi-tive performances.
• To deal with input sequences of variable lengths, we propose a Dilated Conditional Position Encoding as the position embedding generator to adaptively process the 1D textual and 2D visual tokens. Benefitting from proper reading orders, DCPE can further extract rich local layouts of input tokens with dilated convolutions.
• Comprehensive experiments are conducted on VRDU datasets. Our XYLayoutLM achieves competitive per-formance among all listed VRDU approaches on se-mantic entity recognition and relation extraction tasks. 2.