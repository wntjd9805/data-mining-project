Abstract
Video frame interpolation (VFI) is currently a very ac-tive research topic, with applications spanning computer vision, post production and video encoding. VFI can be extremely challenging, particularly in sequences contain-ing large motions, occlusions or dynamic textures, where existing approaches fail to offer perceptually robust inter-polation performance. In this context, we present a novel deep learning based VFI method, ST-MFNet, based on a
Spatio-Temporal Multi-Flow architecture. ST-MFNet em-ploys a new multi-scale multi-ﬂow predictor to estimate many-to-one intermediate ﬂows, which are combined with conventional one-to-one optical ﬂows to capture both large and complex motions.
In order to enhance interpolation performance for various textures, a 3D CNN is also em-ployed to model the content dynamics over an extended temporal window. Moreover, ST-MFNet has been trained within an ST-GAN framework, which was originally devel-oped for texture synthesis, with the aim of further improving perceptual interpolation quality. Our approach has been comprehensively evaluated – compared with fourteen state-of-the-art VFI algorithms – clearly demonstrating that ST-MFNet consistently outperforms these benchmarks on var-ied and representative test datasets, with signiﬁcant gains up to 1.09dB in PSNR for cases including large motions and dynamic textures. Our source code is available at https://github.com/danielism97/ST-MFNet. 1.

Introduction
Video frame interpolation (VFI) has been extensively employed to deliver an improved user experience across a wide range of important applications. VFI increases the temporal resolution (frame rate) of a video through synthe-sizing intermediate frames between every two consecutive original frames.
It can mitigate the need for costly high frame rate acquisition processes [27], enhance the rendering of slow-motion content [26], support view synthesis [16] and improve rate-quality trade-offs in video coding [56].
Figure 1. High-level architecture of ST-MFNet, which employs a two-stage workﬂow to interpolate an intermediate frame.
In recent years, deep learning has empowered a variety of VFI algorithms. These methods can be categorized as
ﬂow-based [26, 59] or kernel-based [30, 41]. While ﬂow-based methods use the estimated optical ﬂow maps to warp input frames, kernel-based methods learn local or shared convolution kernels for synthesizing the output. To han-dle challenging scenarios encountered in VFI applications, various techniques have been employed to enhance these methods, including non-linear motion models [43, 48, 59], coarse-to-ﬁne architectures [9, 42, 48, 64], attention mecha-nisms [11, 27], and deformable convolutions [20, 30]. compared with conventional VFI
Although these methods have signiﬁcantly improved performance ap-proaches [3], their performance can still be inconsistent, especially for content exhibiting large motions, occlusions
Large motion typically means and dynamic textures. large pixel displacements, which are difﬁcult to capture using Convolutional Neural Networks (CNNs) with limited receptive ﬁelds [40, 41].
In the case of occlusion, pixels relating to occluded objects will not appear in all input frames, thus preventing interpolation algorithms from accurately estimating the intermediate locations of those
pixels [11, 27]. Finally, dynamic textures (e.g. water, ﬁre, foliage, etc.) exhibit more complex motion characteristics compared to the movements of rigid objects [13, 63]. Typi-cally, they are spatially irregular and temporally stochastic, causing most existing VFI methods to fail, especially those based on optical ﬂow [26, 32].
To solve these problems, we propose a novel video frame interpolation model, the Spatio-Temporal Multi-Flow Net-work (ST-MFNet), which decouples the handling of large and complex motions using single- and multi-ﬂows respec-tively in a multi-branch structure to offer improved inter-polation performance across a wide range of content types.
Speciﬁcally, ST-MFNet employs a two-stage architecture, as shown in Figure 1. In Stage I, the Multi-InterFlow Net-work (MIFNet) ﬁrst predicts multi-interﬂows [12, 30] at multiple scales (including an up-sampling scale simulating sub-pixel motion estimation), using a customized CNN ar-chitecture, UMSResNext, with variable kernel sizes. The multi-ﬂows here correspond to a many-to-one mapping which enables more ﬂexible transformation, facilitating the modeling of complex motions. To further improve the per-formance for large motions, a Bi-directional Linear Flow
Network (BLFNet) is employed to linearly approximate the intermediate ﬂows based on the bi-directional ﬂows be-tween input frames, which are estimated using a coarse-to-ﬁne architecture [51]. In the second stage, inspired by recent work on texture synthesis [57, 62], we integrate a 3D CNN, Texture Enhancement Network (TENet) that per-forms spatial and temporal ﬁltering to capture longer-range dynamics and to predict textural residuals. Finally, we trained our model based on the ST-GAN [62] methodology, which was originally proposed for texture synthesis. This ensures both spatial consistency and temporal coherence of interpolated content. Extensive quantitative and qualitative studies have been performed which demonstrate the supe-rior performance of ST-MFNet over current state-of-the-art
VFI methods on a wide range of test data including large and complex motions and dynamic textures.
The primary contributions of this work are:
• A novel VFI method where multi-ﬂow based (MIFNet) and single-ﬂow based warping (BLFNet) are combined to enhance the capturing of complex and large motions.
• A new CNN architecture (UMSResNext) for the MIFNet, which predicts multiple intermediate ﬂows at various scales, including an up-sampling scale for high precision sub-pixel motion estimation.
• The use of a spatio-temporal CNN (TENet) and ST-GAN, which were originally designed for texture synthesis, to enhance the interpolation of complex textures.
• Validation, through comprehensive experiments, that our model consistently outperforms state-of-the-art VFI methods on various scenarios, including large and com-plex motions and various texture types. 2.