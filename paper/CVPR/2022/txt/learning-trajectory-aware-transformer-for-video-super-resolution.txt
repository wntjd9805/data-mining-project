Abstract
Target
Frame 57
Frame 61
Frame 64
Video super-resolution (VSR) aims to restore a sequence of high-resolution (HR) frames from their low-resolution (LR) counterparts. Although some progress has been made, there are grand challenges to effectively utilize temporal de-pendency in entire video sequences. Existing approaches usually align and aggregate video frames from limited ad-jacent frames (e.g., 5 or 7 frames), which prevents these approaches from satisfactory results.
In this paper, we take one step further to enable effective spatio-temporal learning in videos. We propose a novel Trajectory-aware
Transformer for Video Super-Resolution (TTVSR). In par-ticular, we formulate video frames into several pre-aligned trajectories which consist of continuous visual tokens. For a query token, self-attention is only learned on relevant vi-sual tokens along spatio-temporal trajectories. Compared with vanilla vision Transformers, such a design signiﬁ-cantly reduces the computational cost and enables Trans-formers to model long-range features. We further pro-pose a cross-scale feature tokenization module to over-come scale-changing problems that often occur in long-range videos. Experimental results demonstrate the supe-riority of the proposed TTVSR over state-of-the-art mod-els, by extensive quantitative and qualitative evaluations in four widely-used video super-resolution benchmarks.
Both code and pre-trained models can be downloaded at https://github.com/researchmm/TTVSR. 1.

Introduction
Video super-resolution (VSR) aims to recover a high-resolution (HR) video from a low-resolution (LR) counter-part [39]. As a fundamental task in computer vision, VSR is usually adopted to enhance visual quality, which has great value in many practical applications, such as video surveil-lance [48], high-deﬁnition television [10], and satellite im-agery [6, 27], etc. From a methodology perspective, unlike image super-resolution that usually learns on spatial dimen-*This work was done while Chengxu Liu was a research intern at Mi-crosoft Research Asia.
MuCAN
IconVSR
TTVSR(Ours)
GT
Figure 1. A comparison between TTVSR and other SOTA meth-ods: MuCAN [24] and IconVSR [4]. We introduce ﬁner textures for recovering the target frame from the boxed areas (indicated by yellow) tracked by the trajectory (indicated by green). sions, VSR tasks pay more attention to exploiting temporal information.
In Fig. 1, if detailed textures to recover the target frame can be discovered and leveraged at relatively distant frames, video qualities can be greatly enhanced.
To solve this challenge, recent years have witnessed an increasing number of VSR approaches, which can be cate-gorized into two paradigms. The former makes attempts to utilize adjacent frames as inputs (e.g., 5 or 7 frames), and
[18, 23] or explicit align temporal features in an implicit manners [34, 39]. One of the classic works is EDVR that adopts deformable convolutions to capture features within a sliding window [39]. However, larger window sizes will dramatically increase computational costs which makes this paradigm infeasible to capture distant frames. The lat-ter investigates temporal utilization by recurrent mecha-nisms [4, 32, 44]. One of the representative works is Icon-VSR that uses a hidden state to convey relevant features from entire video frames [4]. Nonetheless, recurrent net-works usually lack long-term modeling capability due to vanishing gradient [12], which inevitably leads to unsatis-ﬁed results as shown in Fig. 1.
Inspired by the recent progress of Transformer in nat-ural language processing [36], signiﬁcant progresses have been made in both visual recognition [3, 8] and generation tasks [43, 46]. For example, MuCAN proposes to use at-tention mechanisms to aggregate inter-frame features [24]
for VSR tasks. However, due to the high computational complexity in a video, it only learns from a narrow tem-poral window, which results in sub-optimal performance as shown in Fig. 1. Therefore, exploring proper ways of utiliz-ing Transformers in videos remains a big challenge.
In this paper, we propose a novel Trajectory-aware
Transformer to enable effective video representation learn-ing for Video Super-Resolution (TTVSR). The key insight of TTVSR is to formulate video frames into pre-aligned trajectories of visual tokens, and calculate Q, K, and V in the same trajectory. In particular, we learn to link relevant visual tokens together along temporal dimensions, which forms multiple trajectories to depict object motions in a video (e.g., the green trajectory in Fig. 1). We update to-ken trajectories by a proposed location map that online ag-gregates pixel motions around a token by average pooling.
Once video trajectories have been learned, TTVSR calcu-lates self-attention only on the most relevant visual tokens that are located in the same trajectory. Compared with Mu-CAN that calculates attention across visual tokens in space and time [24], the proposed TTVSR signiﬁcantly reduces the computational cost and thus makes long-range video modeling practicable.
To further deal with the scale-changing problem that of-ten occur in long-range videos (e.g., the yellow boxes in
Fig. 1), we devise a cross-scale feature tokenization module and enhance feature representations from multiple scales.
Our contributions are summarized as follows:
• We propose a novel trajectory-aware Transformer, which is one of the ﬁrst works to introduce Trans-former into video super-resolution tasks. Our method signiﬁcantly reduces computational costs and enables long-range modeling in videos.
• Extensive experiments demonstrate that the proposed
TTVSR can signiﬁcantly outperform existing SOTA methods in four widely-used VSR benchmarks.
In the most challenging REDS4 dataset, TTVSR gains 0.70db and 0.45db PSNR improvements than Ba-sicVSR and IconVSR, respectively. 2.