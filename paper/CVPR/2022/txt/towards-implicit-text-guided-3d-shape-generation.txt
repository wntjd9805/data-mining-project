Abstract
In this work, we explore the challenging task of gener-ating 3D shapes from text. Beyond the existing works, we propose a new approach for text-guided 3D shape gener-ation, capable of producing high-fidelity shapes with col-ors that match the given text description. This work has several technical contributions. First, we decouple the shape and color predictions for learning features in both texts and shapes, and propose the word-level spatial trans-former to correlate word features from text with spatial features from shape. Also, we design a cyclic loss to en-courage consistency between text and shape, and introduce the shape IMLE to diversify the generated shapes. Fur-ther, we extend the framework to enable text-guided shape manipulation. Extensive experiments on the largest ex-isting text-shape benchmark [10] manifest the superiority of this work. The code and the models are available at https://github.com/liuzhengzhe/Towards- Implicit-Text-Guided-Shape-Generation. 1.

Introduction 3D shape creation has a wide range of applications, e.g.,
CAD, games, animations, computational design, augmented reality, etc. Significant progress has been made in recent years by exploiting neural networks and generative models to learn to produce 3D shapes. Yet, existing works [7, 12, 13, 22, 34, 35, 47, 49, 70, 76, 78] focus mostly on generating the overall shapes, whereas the more recent ones [11, 14, 23, 44, 59, 79, 80] attempt to generate shapes with more details.
In this work, we are interested in the challenging task of text-guided 3D shape generation—Given a sentence, e.g.,
“A comfortable red color chair with four legs,” we aim to develop a method to automatically generate a 3D shape that follows the text description; see Figure 1 (a) for our example results. This research direction has great potential for effi-cient 3D shape production, say by taking user speech/text input to guide or condition the process of generating 3D shapes. By this means, we can assist users to readily gener-ate and edit 3D models for diverse applications.
*: Corresponding authors
Figure 1. (a) Chairs of different structures and appearances gen-erated by our method from the same given sentence. Our method also allows text-based manipulation in color (b) and in shape (c).
While many methods [40,61–64,67,72,73,77,82,83] have been developed for generating 2D images from text, the task of generating 3D shapes from text is rather under-explored.
Chen et al. [10] generate 3D shapes from natural language descriptions by learning joint text and shape embeddings, but the performance and visual quality are highly limited by the low-resolution 3D representations. Another very recent work [33] leverages semantic labels to guide the shape generation, but it requires predefined semantic labels and cannot directly deal with natural language inputs.
To enhance 3D shape generation from text, we pro-pose a new solution by leveraging the implicit represen-tation [13, 48, 54] to predict an occupancy field. Yet, several inherited challenges have not been addressed in the early works for properly adopting the implicit representation for the text-to-shape task. First, the above works generate shapes typically without colors, which are crucial in text-guided 3D shape generation, since text descriptions often contain col-ors; we empirically found that directly predicting shape and color with a single implicit decoder often lead to shape distor-tion and color blur. Second, text contains a large amount of spatial-relation information, e.g., “a wooden table on a metal base.” Still, spatial-relation local features are ignored in ex-isting works, since the implicit decoder generally considers only the global feature from the auto encoder as input [13].
Third, the generated shapes are not all consistent with the
input texts, largely due to the semantic gap between text and 3D shape and also the lack of effective learning constraints.
Last, text-to-shape generation is inherently one-to-many, i.e., diverse results may match the same input text. Yet, the exist-ing regression-based approach outputs only a single shape.
This work presents a new approach for high-fidelity text-guided 3D shape generation. First, we decouple the shape and color predictions for feature learning in both texts and shapes to improve the generation fidelity; this strategy also aids the text-guided shape manipulation. Also, we introduce a word-level spatial transformer to learn to correlate the word features with the spatial domain in shapes. In addition, we design a cyclic loss to encourage the consistency between the generated 3D shape and the input text. Further, we propose a novel style-based latent shape-IMLE generator for producing diversified shapes from the same given text. Last, we extend the framework for text-guided 3D shape manipulation with a two-way cyclic loss. As shown in Figure 1 (b), we may modify the original text and our framework can produce new colored shapes according to the edited text, while keeping the other attributes unchanged.
Extensive experiments on the largest existing text-shape dataset [10] demonstrate the superiority of our approach over the existing works, both qualitatively and quantitatively. 2.