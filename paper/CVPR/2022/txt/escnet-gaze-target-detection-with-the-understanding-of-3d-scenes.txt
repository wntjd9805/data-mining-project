Abstract
This paper aims to address the single image gaze target detection problem. Conventional methods either focus on 2D visual cues or exploit additional depth information in a very coarse manner. In this work, we propose to explic-itly and effectively model 3D geometry under challenging scenario where only 2D annotations are available. We first obtain 3D point clouds of given scene with estimated depth and reference objects. Then we figure out the front-most points in all possible 3D directions of given person. These points are later leveraged in our ESCNet model. Specifi-cally, ESCNet consists of geometry and scene parsing mod-ules. The former produces an initial heatmap inferring the probability that each front-most point has been looking at according to estimated 3D gaze direction. And the latter further explores scene contextual cues to regulate detec-tion results. We validate our idea on two publicly available dataset, GazeFollow and VideoAttentionTarget, and demon-strate the state-of-the-art performance. Our method also beats the human in terms of AUC on GazeFollow. Our code can be found here https://github.com/bjj9/ESCNet. 1.

Introduction
Gaze target detection is important to understand human’s intention. Therefore, it plays an important role in appli-cations such as human computer interface [26] and social awareness tracking [27]. Though physical equipment such as wearable eye trackers [10] is available to perform gaze estimation, they are not desired due to location or calibra-tion limitations. A more general setting takes third person view image as well as a given person in this scene as input and aims to locate where this person is looking in 2D image space [31]. Conventional methods typically leverage 2D vi-sual cues to regulate gaze predictions by not only salient objects but also estimated gaze orientation [6, 31]. More re-cent approach [9] proposes to incorporate 3D gaze estima-tion and depth cues. Though demonstrating advanced per-formance,it requires additional human annotations [18] to
*Corresponding author.
Figure 1. We propose to explicitly model 3D geometry in 2D gaze target detection task. We propose to reconstruct the scene with 3D point cloud in single-image setting in (1) and demonstrate that our
ESCNet is able to effectively exploit such information in (2). explicitly model 3D gaze and due to coarse depth represen-tation, it lacks the ability to handle more general scenarios, e.g. multiple salient objects lie in the same depth layer and field of view. To this end, we propose to perform gaze target detection by complete understanding and explicit modelling of 3D scenes, with 2D gaze annotations only.
Promising as it sounds, lacking of 3D information in ex-isting dataset makes our task hard. Also, effectively repre-senting such information remains a vital problem.
We address these challenges through two key insights.
Firstly, 3D geometry can be reconstructed by absolute depth and camera parameters, which can be estimated with rel-ative depth and certain assumptions of reference objects.
Specifically, we use ”person” as our reference category as humans occur most frequently in images collected for gaze estimation task and their sizes are of certain distributions.
With the assumption of human sizes, we can estimate the absolute depth and focal length of each image, leading to 3D point clouds (See (1) in Fig. 1). Second, occlusion plays an important role in gaze estimation given the fact that one cannot see through occluders. Such fact provides strong pri-ors to regulate where one person may look at. Inspired by this, we propose to represent the 3D geometry with front-most points, or occluders. This is achieved by modelling points in all possible 3D directions of a given person, and then leaving only the front-most one in each direction.
Further, we propose a novel model ESCNet that consists of gEometry and SCene parsing modules. The former lever-ages geometric cues, e.g. 3D gaze direction and 3D geom-etry, and outputs an initial heatmap as an intermediate rep-resentation, inferring the probability that each front-most point being looking at (See (2) in Fig. 1). The latter further incorporates scene contextual cues such as saliency in RGB image and generates final heatmap predictions.
We test on GazeFollow [31] and VideoAttentionTar-get [6], and obtain state-of-the-art (SOTA) performance.
Our intermediate representation is not only visually and conceptually meaningful, but also allows deep supervision, leading to performance boost. Finally, our method even out-performs human performance on AUC metric.
To summarize, our key contributions are:
• A novel method that explicitly models full 3D geome-try, especially occlusion, in 2D gaze target detection.
• An end-to-end deeply supervised model ESCNet that explores 3D geometry, 2D/3D gaze and scene contex-tual cues, with gaze annotations only available in 2D.
• State-of-the-art results on publicly available datasets and superior performance over human. 2.