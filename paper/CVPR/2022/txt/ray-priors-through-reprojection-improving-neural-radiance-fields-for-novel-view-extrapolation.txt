Abstract generation.
Neural Radiance Fields (NeRF) [22] have emerged as a potent paradigm for representing scenes and synthesizing photo-realistic images. A main limitation of conventional
NeRFs is that they often fail to produce high-quality render-ings under novel viewpoints that are significantly different from the training viewpoints. In this paper, instead of ex-ploiting few-shot image synthesis, we study the novel view extrapolation setting that (1) the training images can well describe an object, and (2) there is a notable discrepancy between the training and test viewpoints’ distributions. We present RapNeRF (RAy Priors) as a solution. Our insight is that the inherent appearances of a 3D surface’s arbitrary visible projections should be consistent. We thus propose a random ray casting policy that allows training unseen views using seen views. Furthermore, we show that a ray atlas pre-computed from the observed rays’ viewing direc-tions could further enhance the rendering quality for ex-trapolated views. A main limitation is that RapNeRF would remove the strong view-dependent effects because it lever-ages the multi-view consistency property. 1.

Introduction
A primary target of the computer graphics community is to enable photo-realistic rendering of virtual worlds effi-ciently. Physics-inspired graphics techniques well approach real-time rendering and photo-realistic imagery creation but suffer from expensive manual content generations of ge-ometries, materials, and other aspects of scenes. The past several years have seen an explosion of interest in neu-ral rendering [17, 18, 21, 38, 39, 47], which models phys-ical knowledge in deep networks to address reconstruction and rendering in a single formulation for controllable image
*These authors contribute equally to this work.
²Corresponding author.
Leveraging neural volume rendering, a recent advance
Neural Radiance Fields (NeRF [22]) learn to represent 3D scenes from images and impressively support photo-realistic image synthesis. The visual quality of the gen-erated images is even competitive with ones produced by physically-based rendering pipelines. One of NeRF’s main limitations is that it requires many images to reconstruct a scene’s geometry and texture details. Thereby, several sub-sequent works focus on investigating few-shot or unsuper-vised radiance fields reconstruction [4,11,52]. These works assume that we only observe several images of a scene. In an extreme setting, some geometries and appearances of the scene are never observed.
In this paper, we investigate NeRF from an object re-construction perspective like [9, 33, 51], and restrict our focus on solid and non-transparent objects. We find that, even with enough images that can well describe an object, conventional NeRFs often fail to produce high-quality ren-derings for novel viewpoints that are significantly different from the training viewpoints. This observation motivates us to study the novel view extrapolation setting as explained in
Figure 1. We take inspiration from the insight that the inher-ent appearances of a 3D surface’s arbitrary visible projec-tions should be consistent. It has been well studied in unsu-pervised 3D object reconstruction and texture optimization works [10, 15, 41, 55]. We dig into the multi-view consis-tency property on NeRF’s formulation and present RapN-eRF as a solution.
In specific, we propose a random ray casting (RRC) pol-icy that randomly generates rays within a cone for each training ray in an online fashion. This training strategy is simple yet effective in creating supervisions for potential unseen views using seen views. RRC relies on the target ob-jects’ rough 3D meshes (R3DMs), which can be extracted from their pre-trained NeRFs. Furthermore, we prudently rethink the tradeoff between strong view-dependent effects
Figure 1. Observation & Setting. Left: We study the novel view extrapolation setting that (1) the training images can well describe the objects, and (2) the test viewpoints are significantly different from the training viewpoints. We take a specific object as an example to illustrate the training (Red) and test (Blue) viewpoints in MobileObject. The viewpoints labeled in ªGrayº are discarded. Right: For novel view extrapolation, NeRF [22] produces images that usually contains artifacts, while RapNeRF can generate high-quality renderings. and multi-view consistent renderings. We show that a ray atlas (RA) computed from the training ray’s viewing direc-tions could further improve the rendering quality of extrap-olated views. RapNeRF is empowered by both RRC and
RA in a unified formulation.
To study the novel view exploration setting, we resplit the synthetic training and test images of NeRF’s objects to construct the Synthetic-NeRF∗ [22]. We also capture eight scenes with real objects via a mobile phone to build a MobileObject dataset. A sample is illustrated in Figure 1 (right). Experiments demonstrate the superiority of Rap-NeRF in synthesizing promising novel views compared to state-of-the-art approaches. We conduct various ablation studies to discuss the core components of RapNeRF. Last but not least, a major limitation of RapNeRF is that it trades some view-dependent effects for better novel view explo-ration performance. We provide it a remedy by studying the deferred NeRF architecture in [9]. 2.