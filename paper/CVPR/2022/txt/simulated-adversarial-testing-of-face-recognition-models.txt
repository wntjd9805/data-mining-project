Abstract
Most machine learning models are validated and tested on fixed datasets. This can give an incomplete picture of the capabilities and weaknesses of the model. Such weaknesses can be revealed at test time in the real world. The risks in-volved in such failures can be loss of profits, loss of time or even loss of life in certain critical applications. In order to alleviate this issue, simulators can be controlled in a fine-grained manner using interpretable parameters to explore the semantic image manifold. In this work, we propose a framework for learning how to test machine learning algo-rithms using simulators in an adversarial manner in order to find weaknesses in the model before deploying it in crit-ical scenarios. We apply this method in a face recognition setup. We show that certain weaknesses of models trained on real data can be discovered using simulated samples.
Using our proposed method, we can find adversarial syn-thetic faces that fool contemporary face recognition mod-els. This demonstrates the fact that these models have weak-nesses that are not measured by commonly used validation datasets. We hypothesize that this type of adversarial exam-ples are not isolated, but usually lie in connected spaces in the latent space of the simulator. We present a method to find these adversarial regions as opposed to the typical ad-versarial points found in the adversarial example literature. 1.

Introduction
Evaluating a machine learning model can have many pit-falls. Ideally, we would like to know (1) when the model will fail (2) in which way it will fail and (3) how badly it will fail. In other words, we would like to be able to accu-rately estimate the model’s risk on the true test data distribu-*Equal senior contribution. tion as well as know what specific factors induce the model to failure. We would like to know how these failures will manifest themselves. For example, whether a face verifica-tion model will generate a false-positive or false-negative error. And finally, when this failure happens, we would like to know how confident was the incorrect decision by the model. Testing models is no longer a purely academic endeavour [60], with many high profile bad societal conse-quences being revealed in recent years due to insufficient testing particularly with respect to racial and gender bias in face analysis systems [5, 15, 19].
These three desiderata are very hard to achieve in prac-tice. There are major philosophical and theoretical obsta-cles to achieve perfect knowledge of model failures a priori.
Nevertheless, partial knowledge of model weaknesses and predictions of model failures are possible. Yet, there are still major hurdles that stand in our way.
One such hurdle is the fact that testing data is limited, due to the fact that it is expensive to gather and label. It is not uncommon for a model to perform well on an assigned test set and fail to generalize to specific obscure examples when it is deployed. A second important hurdle is the fact that testing data is unruly. There are latent factors that gen-erate the testing data, which are hard to control or even to fully understand. For example, a known factor that is hard to control is the lighting of a scene. Most datasets have been captured without controlling for this variable, and thus present an insufficient amount of variability in this respect.
Testing a model in one environment could yield perfect per-formance, yet fail on an environment with more lighting variability. Even if a test dataset with carefully controlled lighting were assembled, the dataset would be very expen-sive and time-consuming to collect and there is no guarantee that the full variability would be explored.
A way to tackle these problems is to use simulators to generate test data. Such an approach can cheaply generate a large quantity of data spanning a large spectrum. Also, sim-ulators are fully controllable and the generative parameters are known. This allows for careful exploration of situations where models fail. This includes the possibility to find in-tepretable factors that generate failures, to study the way these failures manifest themselves (is the model classifying a cat as a jaguar when there is green in the background?) and to examine the degrees of certainty of the model in these failure modes.
When simulating test data, we have full control over sim-ulator parameters. Thus, we are able to explore the mani-fold generated by the simulator in the space of the simula-tor parameters. We call this manifold the semantic image manifold, in contrast to the adversarial image manifold that is explored in the traditional adversarial attack literature.
A random exploration of this manifold is both inefficient and not the most informative approach.
In this work we propose to test machine learning models using simulation in an adversarial manner by finding simulator parameters that generate samples that fool the model. We are inspired by the literature on adversarial examples that fool machine learning models, yet in contrast to this body of work, the adversarial examples that our simulator generates are se-mantically realistic in the sense that we are not adding low magnitude noise to an image in order to fool the model but finding semantically sensible image configurations that gen-erate model failure. In this way, we are not investigating the well-known weakness of gradient-based models to unrealis-tic targeted noise but to plausible scenes that might be rare, yet mislead the model. We present a method that finds ad-versarial samples efficiently using a continuous policy that searches the high-dimensional space of possibilities.
A limitation of this type of work is that, in general there exists domain shift between the distribution described by the simulator and the real world distribution [7,14,20,39,54, 55]. Nevertheless, in our work we are able to show that in some situations, real model weaknesses can be found using simulated data. This gives credence to the hypothesis that, even though there is domain shift, simulated samples can be informative. Also, simulators are rapidly improving in terms of realism [11, 30, 36, 48]. This allows for greater opportunities to use these ideas in the future as simulated and real data distributions become more and more aligned.
We hypothesize that these adversarial examples are not isolated points in space, but instead are regions of this man-ifold.
In prior work on traditional adversarial examples, optimization procedures find adversarial samples that are points in image space [6, 18, 33, 37, 49, 53]. In contrast to this body of work we propose a method to find these adver-sarial regions instead. This is valuable because ideally we would like to be able to fully describe the machine learning model’s regions of reliability, where model predictions will tend to be correct. With this knowledge a user would be able to avoid performing inference on a model outside of its scope in order to minimize failures.
Contributions of this work are three-fold. We summarize them as follows:
• We show that weaknesses of models trained on real data can be discovered using simulated samples. We perform experiments on face recognition networks showing that we can diagnose the weakness of a model trained on biased data.
• We present a method to find adversarial simulated sam-ples in the semantic image manifold by finding adver-sarial simulator parameters that generate such samples.
We present experiments on contemporary face recog-nition networks showing that we can efficiently find faces that are incorrectly recognized by the network.
• We present a method to find regions that are adversar-ial, in order to locate danger zones where a model’s predictions are more liable to be incorrect. To the best of our knowledge, we are the first to explore the exis-tence of these adversarial regions in the interpretable latent space of a simulator. 2. A Framework for Simulated Adversarial
Testing
Here we formalize adversarial testing using a simulator.
We postulate some assumptions on the data generation pro-cess in the real and simulator world. Then we give the risks for a machine learning model and the mathematical formu-lation to find adversarial parameters that yield samples that fool machine learning models. We then present some par-allels between our scenario and the literature on learning across domains. Finally, we describe our proposed algo-rithm to find such adversarial simulator parameters and ad-versarial samples.
Let us assume the real world data (x, y) (where x is the data and y is the label) is generated by the distribution p(x, y|ψ) where ψ is a latent variable that causally controls the data generation process. For example, ψ includes the object type in the image and the angle of view of such an ob-ject, as well as all other parameters that generate the scene and image. The risk for a discriminative model f is:
Eψ∼a[E(x,y)∼p(x,y|ψ)[L(f (x), y)]], (1) where a is the distribution of ψ and L is the loss. We can search for ψ∗ that maximizes this risk:
[E(x,y)∼p(x,y|ψ)[L(f (x), y)]] max
ψ∈A (2) where A is the set of all possible ψ. Let us assume that we have ψ = (ψu, ψk), a decomposition of ψ into two latent variables ψu and ψk. Furthermore, let us assume that ψu
Figure 1. Our method applied to the face verification scenario. The simulator is conditioned on parameters generated by the policy. An image pair of the same identity is generated. Face verification is run on this image pair using the face recognition network that is to be diagnosed. A reward is computed based on the correct or incorrect prediction of the network and policy parameters are updated accordingly. controls for unknown features of the image, and ψk controls for known features of the image such as the camera pose, or the object position with respect to the camera. We can write the average risk as:
We also introduce a hypothesis that is a function h :
X → {0, 1}. We can write the risk of this hypothesis on p as:
ϵp(h, gp) = Ex∼p[|h(x) − gp(x)|] (5)
Eψu∼a[Eψk∼b[E(x,y)∼p(x,y|ψu,ψk)[L(f (x), y)]]], (3) where b is the distribution of ψk. In most scenarios, we do not have access to the real data distribution p and cannot sample from it at will. Additionally, it is very difficult to control the known latent variable ψk when generating data, and we do not even know what factors are hidden in the variable ψu, much less how to control it. Using simulated data we are able to fully control the generative process.
A simulator samples data (x, y) ∼ q(x, y|ρ), where q is the simulated data distribution and we have complete knowledge over the latent variable ρ. We are able to search for adversarial examples and compute estimates of the mean and worst-case risks using this simulator. For example, the parameter ρ∗ that maximizes the risk is written as follows:
[E(x,y)∼q(x,y|ρ)[L(f (x), y)]] max
ρ∈C (4) where C is the set of all possible ρ. We can find ˆρ∗, an estimate of ρ∗, by sampling (albeit inefficiently).
In our case we are working in a less restrictive scenario since we do not try to find the global maximum ρ∗, instead we try to find any ρ where E(x,y)∼q(x,y|ρ)[L(f (x), y)] is above the misclassification threshold.
If we assume that the distributions p and q are similar enough we can use the knowledge gathered in simulation to understand the possibilities of failure in the real world.
Essentially, this is a different kind of domain shift prob-In a traditional setting of transfer learning between lem. domains, we are concerned about minimizing the risk on a target domain by training on a source domain. In the binary classification case, let us define a domain as a pair consist-ing of a distribution p on inputs X and a labeling function gp : X → [0, 1]. We consider the real domain and the sim-ulated domain denoted by (p, gp) and (q, gq) respectively.
In traditional domain adaptation from simulation to re-ality, we seek to learn on distribution q and generalize to distribution p. We want to find a hypothesis that minimizes the risk on the target real world distribution ϵp(h, gp) by training on samples from q.
In our setting, we do not train on synthetic samples. In-stead we want to find a relationship between testing a hy-pothesis h on samples from distribution q and testing h on samples from p. There exist bound results for the risks
ϵp(h, gp) and ϵq(h, gq) in the work of Ben-David et al. [4]:
ϵp(h, gp) < ϵq(h, gq) + d1(q, p)+ min{Ep[|gq(x) − gp(x)|], Eq[|gq(x) − gp(x)|]}, (6) where d1 is the variation divergence. The second term of the right hand side quantifies the difference between distri-butions q and p, and the third term of the right hand side is the difference between the labeling functions across do-mains, which is expected to be small.
Since this bound characterizes the cross-domain general-ization error and ϵq(h, gq) will usually be minimized by the learning algorithm, it is useful for studying transfer learning between domains. There are some differences in our sce-nario since for us h is a fixed function that has been trained on the target domain and we would like to talk about in-dividual examples instead of overall risk over distributions.
Also, the bound is proven for a binary classification prob-lem, whereas our target scenario can be multi-class classifi-cation or regression.
Assume there exists a mapping τ : C → A, that maps the simulated latent variables to real latent variables
ψ = τ (ρ). In order for adversarial examples in the simula-tor domain to be informative in the real domain, we want to
have a simulator such that:
P(xs,ys)∼q,(xr,yr)∼p[|L(xs, ys)−L(xr, yr)| < ϵ] > θ. (7)
We denote p(xr, yr|τ (ρ)) as p and q(xs, ys|ρ) as q in the equation above for succinctness. Here ϵ is small and θ ∈
[0, 1] is large. This way, high-loss examples found in the se-mantic image manifold using simulation have a high prob-ability of transferring to the real world. Since the simulator and real domain are different, this is a moderately strong as-sumption. Nevertheless, we show cases where this assump-tion holds in our experimental evaluations in Section 4.3.
Finding Adversarial Parameters Our task is then to find
ρ such that the loss over samples generated with this la-tent variable is above the misclasification threshold T . One main difficulty in searching for latent variables that fulfill this condition is that in general the simulator q is non-differentiable. Thus, we turn to black-box optimization methods to search for adversarial parameters. Specifically, we use a policy gradient method [57].
We define a policy πω parameterized by ω that can sam-ple simulator parameters ρ ∼ πω(ρ). We train this policy to generate simulator parameters that generate samples that obtain high loss when fed to the machine learning model f .
For this we define a reward R that is equal to the negative loss L and we want to find the parameters ω that maximize
J(ω) = Eρ∼πω [R]. Following the REINFORCE rule we obtain gradients for updating ω as
∇ωJ(ω) = Eρ∼πω (cid:2)∇ω log(πω)R(ρ)(cid:3) .
An unbiased, empirical estimate of the above quantity is
L(ω) = 1
K
K (cid:88) k=1
∇ω log(πω) ˆAk , (8) (9) where ˆAk = R(ωk) − β is the advantage estimate, β is a baseline, K is the number of different parameters ρ sampled in one policy forward pass and R(ρk) designates the reward obtained by evaluating f on (xk, yk) ∼ q(xk, yk|ρk). We show all of the steps of our method in Algorithm 1 and we show an illustration of our method applied to the face veri-fication scenario in Figure 1. 3. Finding Adversarial Regions
Here we describe our method to find adversarial regions.
Once an adversarial simulator latent vector ρadv ∈ Rn have been found using Algorithm 1 we define a graph G = (V, E). V are the vertices of the graph, obtained by dis-cretizing the space around the adversarial point in grid with spacing ν between vertices. The edges E of the graph con-nect neighboring vectors, with each vector having 2n neigh-bors. We find the connected space of adversarial examples
Radv that is seeded by ρadv by following Algorithm 2.
Algorithm 1: Our adversarial testing approach us-ing a policy gradient method.
Result: adversarial simulator parameters ρk and adversarial sample xk for iteration=1,2,... do
Generate K simulator parameters ρk ∼ πω(ρk);
Generate K samples (xk, yk) ∼ q(xk, yk|ρk)
Test the discriminative model and obtain K losses L(f (xk), yk) if ∃k ∈ {1, ..., K}; L(f (xk), yk) > T then
Terminate and yield adversarial sample xk and adversarial simulator parameters ρk end
Compute rewards R(ρk)
Compute the advantage estimate
ˆAk = R(ρk) − β
Update ω via equation 9 end
In essence, our method follows the general idea of an area flooding algorithm [31, 52] with two main differ-ences. First, that we discretize a continuous space that is n-dimensional instead of working on binary 2-dimensional image, and second, that we check for sample membership of Radv by testing whether the model loss is higher than the adversarial threshold L(f (x), y) > T .
Algorithm 2: Finding connected spaces of adver-sarial examples.
Result: connected space of adversarial examples
Radv
Data: seed adversarial simulator parameters ρadv
Radv = {ρadv}
Initialize a stack χ.
Push 2n neighbors of ρadv to χ. for i=1,2,... do
Pop ρi from χ
Sample (xi, yi) ∼ q(xi, yi|ρi)
Test the discriminative model and obtain loss
L(f (xk), yk) if L(f (xk), yk) > T then
Radv = Radv ∪ {ρi}
Push all neighbors of ρi that have not been visited to χ end end
4. Experimental Results 4.1. Controllable Face Simulation
We use the FLAME face model [29] as a controllable face simulator with the Basel texture model [38]. FLAME uses a linear shape space trained from 3,800 3D scans of human heads and combines this linear shape space with an articulated jaw, neck, and eyeballs, pose-dependent correc-tive blendshapes, and additional global expression blend-shapes. In this way, using shape and texture components we can generate faces with different identities. The syn-thetic faces that are generated in our work are new and do not mimic any existing person’s features. By changing the pose and expression components we can add variability to these faces. Moreover, we have full control over the scene lighting and the head and camera pose and position. In or-der to render our scene we use the PyTorch3D rendering framework [41]. We extract the corresponding shape, tex-ture and expression components from the real faces of the
CASIA WebFace dataset using DECA [10]. 4.2. Models, Datasets and Infrastructure
In our experiments we use the CASIA WebFace [59] dataset for training the face recognition models and the
LFW [23] dataset for real-world data testing. We use a Con-volutional Block Attention Module (CBAM) [58] ResNet50 with the ArcFace [8] loss as our base face recognition model. We also test our method on MobileNet [21] and
CBAM-Squeeze-Excitation-ResNet [22] architectures and the CosFace [56] loss. We use a multivariate Gaussian pol-icy π(ρ) = N (µπ, σ2
π = 0.05 × I and µπ is learned. For the random optimization baseline we use one Gaussian for each parameter type with standard deviation σrs = wp 10 × I, where wp is the width of the parameter domain. For the Gaussian random sampling baseline we use a standard deviation σg = wp 2 . We use a
GeForce RTX 2080 GPU with 11GB of memory to perform all of our experiments.
π) where the variance is fixed σ2 4.3. Testing Weakened Models
We present a way to verify that knowledge from simu-lated weaknesses translates to real-world weaknesses. We weaken two networks by training on the CASIA Web-Face dataset with images that exhibit a yaw parameter
[−∞, −0.5] and [0.5, +∞] filtered out. We extract the these the Nega-yaw parameter using DECA. We call tive Yaw Filtered (NYF) and Positive Yaw Filtered (PYF) datasets/networks, respectively. Both datasets have roughly the Negative Yaw Filtered the same number of samples: dataset has ∼ 440k training samples and the Positive Yaw
Filtered dataset has ∼ 449k samples. We also train a Nor-mal network on all of the ∼ 491k samples of the unfiltered
CASIA WebFace dataset. We then test both the normal net-work and the yaw-weakened networks on simulated sam-ples. We do this by generating two images of a same person, by fixing the shape, texture and expression parameters. The first image is a frontal image of the person. We vary the yaw component of the second image in the [−1, 1] range, where
−1 and 1 in the yaw component indicate a fully-profile face on the negative and positive sides, and compute the cosine similarity between the embeddings of the two images. This cosine similarity should be large given that the two images presented are of the same identity. A low cosine similarity means that the network has less confidence that the images show the same person.
We plot this in Figure 2, and observe that each yaw-weakened network makes less accurate predictions for im-ages presenting high yaw in their respective weakness in-tervals. Note that all networks perform almost identically with frontal samples. Also, note that the normal network is almost always superior to the two weakened networks.
This is a natural result of having 10% more training data.
This plot is an average over 25 different identities that we obtain by grid-sampling the first texture and shape compo-nents over the range [−σ, σ].
We compute the area between the curves for the
[−1.0, −0.5], [−0.5, 0.5] and [0.5, 1.0] intervals. We ob-serve in Table 1 (left) that in the [−1.0, −0.5] yaw range, precisely where the NYF network has been weakened, the area between the Normal-NYF curves is large and the area between the Normal-PYF curves is small. Conversely, in the [0.5, 1.0] range, where PYF has been weakened, we see that the difference between the Normal-PYF curves is large and the Normal-NYF difference is smaller. Also, we ob-serve near identical differences between Normal-NYF and
Normal-PYF in the [−0.5, 0.5], which is a consequence of the lesser amount of training data of NYF and PYF net-works. We also compute pairwise mean differences for the different populations of Normal, NYF and PYF networks and present them in Table 1 (right). We highlight in blue the statistically significant differences. We have similar re-sults as in Table 1 (left).
This evidence indicates that when a weakness is purpose-fully created in a network by filtering out key samples in the real training dataset, we can retrieve this weakness us-ing our face simulator. This gives credence to the idea that we are able to find simulated adversarial examples in the semantic image manifold that will give us knowledge about adversarial examples in the real world. 4.4. Simulated Adversarial Testing of Face Recog-nition Models
In this section we evaluate adversarial testing of face recognition models for face verification. Specifically, we generate samples using the FLAME face model and use our proposed search algorithm to fool face recognition models.
↓ Models / Yaw Interval → [-1.0, -0.5]
Area Between Curves
[-0.5, 0.5]
[0.5, 1.0]
↓ Models / Yaw → -1.0
Mean Difference 0.0 1.0
Normal:NYF
Normal:PYF 8.69 2.71 2.83 2.76 4.68 8.46
Normal-NYF
Normal-PYF
NYF-PYF 0.18 0.01
-0.17 0.01 0.00
-0.01 0.10 0.16 0.06
Table 1. Quantitative differences between evaluation of the purposefully weakened Negative Yaw Filtered (NYF) and Positive Yaw Filtered (PYF) and the Normal on synthetic faces (bold values for emphasis). Blue values in the table on the right mean the differences are statistically significant with p < 0.01. periments on several combinations of network backbones (CBAM-ResNet50, CBAM-SE-ResNet50 [22], MobileNet) and face recognition losses (ArcFace, CosFace) trained on
CASIA WebFace for 20 epochs. All networks achieve ac-curacies in the (98.85%, 99.1%) range on the LFW test set.
We vary 30 shape parameters, 30 texture parameters ranging from −2σ and 2σ where σ is the standard deviation of each parameter. We also vary the yaw pose parameter within
[−1, +1], corresponding to variations of [−π/2, +π/2] de-grees and the pitch pose parameter from [−1/4, +1/4] cor-responding to variations within [−π/8, +π/8]. Thus, in this case our algorithm has to learn 62 parameters. This is a more challenging scenario due to the larger dimensionality of the policy output.
We perform 100 runs of our adversarial testing algorithm (200 maximum iterations), 100 runs of Random Optimiza-tion using a Gaussian sampling distribution and 1,000 iter-ations of uniform random sampling and Gaussian random sampling. We compare these testing methods in Table 3 and we show that the networks achieve very high accura-cies for both random sampling regimes and for testing us-ing random optimization. Using adversarial testing, all net-works exhibit a marked drop in verification performance.
There is also a large increase in the average cosine simi-larity between pairs, showing that adversarial testing gener-ates highly adversarial samples (below success thresholds
T = (0.298, 0.237, 0.292, 0.294) respectively), whereas other methods generate “easy” samples on average.
Further, for example, for ArcFace CBAM-ResNet50, adversarial testing achieves 51 adversarial samples over 12,587 iterations while random sampling achieves only one adversarial sample over 1,000 iterations. This makes ad-versarial testing 400% more sample efficient than random sampling in this specific scenario. In some of our tested sce-narios and depending on the number of iterations, random sampling was not able to find any adversarial samples. This is reflected by a 100% face verification accuracy. In Fig-ure 4, we show several successful adversarial testing runs (orange/red) and one random sampling run (green). Unsuc-cessful optimization attempts usually converge to low co-sine similarity without becoming adversarial and remain in the high-dimensional local minima. Finally, we show an example of adversarial testing in action where all 30 shape, 30 texture and 2 pose parameters are being learned jointly in Figure 5. The algorithm finds an adversarial sample that
Figure 2. Recognition cosine similarity between two simulated pairs (frontal and variable yaw) of the same identity (avg. over 25 different identities). The Negative Yaw Filtered network exhibits less accurate predictions for highly negative yaw images than both the Positive Yaw Filtered and Normal networks. The Positive Yaw
Filtered network exhibits less accurate predictions for highly pos-itive yaw images than both other networks.
We train an ArcFace CBAM-ResNet50 on CASIA Web-Face for 20 epochs. This network achieves a 99.1% accu-racy on the LFW test set for the face verification task. The evaluation task is face verification between two synthetic images of a same person’s face, one frontal and one pro-file image. We vary the first 15 shape parameters as well as the first 15 texture parameters for our generated identities, ranging from −2σ and 2σ where σ is the standard deviation of each parameter in question.
We propose testing the network using 100 identities ob-tained by random sampling these parameters following a uniform distribution. We also test the network using 100 runs of our adversarial testing algorithm (200 maximum it-erations). In Table 2, we show that the random sampling testing regime achieves an accuracy of 99%, which is very close to the 99.1% real-world accuracy of the network on the LFW test set. Using adversarial testing, the network exhibits an accuracy of 36%, which is a marked drop in verification performance. We also compute the average co-sine similarity between pairs, showing that adversarial test-ing generates highly adversarial samples (success thresh-old T = 0.298) whereas random samples are highly non-adversarial on average. In Figure 3 we show a subset of the generated samples for both the adversarial testing (above) and random sampling (below).
We perform further simulated adversarial testing ex-Table 2. CBAM-ResNet50 face verification accuracy over syn-thetic datasets generated by uniform random sampling or by ad-versarial testing (Adv. Testing). We vary the identity by varying 15 shape parameters and 15 texture parameters.
Method
Accuracy ↓ Avg. Cosine Similarity ↓
Uniform Random
Adv. Testing 99% 36% 0.518 0.263
Figure 4. Cosine similarity for successful adversarial testing (red) and random parameter sampling (green).
Figure 3. Face models obtained using adversarial testing (above) and random parameter sampling (below). A green border denotes pairs that are successfully verified as the same identity, whereas a red border denotes failed verification (model failure). We obtain adversarial samples using our adversarial testing method more consistently than with random parameter sampling. Some recur-ring features of adversarial faces are ambiguous frontal/profile features (e.g. long nose, tucked jaw), pale/dark skin colors and left/right asymmetries. reveals model weaknesses such as vulnerability to unusual poses, exaggerated facial features and distinct skin color. 4.5. Finding Adversarial Regions of Face Recogni-tion Models
We use our method described in Algorithm 2 to find ad-versarial regions in the simulator latent space for face recog-nition models. We do this in the face verification scenario between a frontal image with neutral expression and a pro-file image with an open jaw. We vary the first shape and texture parameters to find an adversarial sample, and then find the connected spaces to those seed parameters. We also grid sample both parameters in order to plot the syn-thetic sample surface. We show the surface of all synthetic samples (blue), along with the adversarial region (red) and the adversarial threshold plane (orange) in Figure 6.
We are successful in finding the adversarial regions when they exist. We discover a surprising fact when plotting the synthetic loss landscape (Figure 7) of all the tested net-works. In this configuration with only 2 variable parame-ters, the only network with an adversarial region is ArcFace
CBAM-ResNet50. Even though all networks have been
Figure 5. A sequence of generated synthetic samples undergo-ing adversarial testing (left to right, top to bottom). Our method searches through all 30 shape, 30 texture and 2 pose parameters jointly to find an adversarial face. The border line colors denote whether the face recognition network can successfully verify the pairs, with red denoting a failed verification and green denoting a successful verification. trained in the same manner on the same dataset, the net-work backbone and the loss function change the loss land-scape substantially. Some networks have a similar down-ward slope from negative shape towards positive shape, but some particularities arise in some. Strikingly, ArcFace Mo-bileNet is the most robust of all the networks in this scenario with a landscape far above the misclassification threshold plane. The landscape shape is also completely different from the other networks. 5.