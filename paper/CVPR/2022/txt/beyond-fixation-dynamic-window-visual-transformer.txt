Abstract
Recently, a surge of interest in visual transformers is to reduce the computational cost by limiting the calculation of self-attention to a local window. Most current work uses a fixed single-scale window for modeling by default, ignoring the impact of window size on model performance. How-ever, this may limit the modeling potential of these window-based models for multi-scale information.
In this paper, we propose a novel method, named Dynamic Window Vi-sion Transformer (DW-ViT). The dynamic window strategy proposed by DW-ViT goes beyond the model that employs a fixed single window setting. To the best of our knowl-edge, we are the first to use dynamic multi-scale windows to explore the upper limit of the effect of window settings on model performance.
In DW-ViT, multi-scale informa-tion is obtained by assigning windows of different sizes to different head groups of window multi-head self-attention.
Then, the information is dynamically fused by assigning dif-ferent weights to the multi-scale window branches. We con-ducted a detailed performance evaluation on three datasets,
ImageNet-1K, ADE20K, and COCO. Compared with re-lated state-of-the-art (SoTA) methods, DW-ViT obtains the best performance. Specifically, compared with the current
SoTA Swin Transformers [31], DW-ViT has achieved con-sistent and substantial improvements on all three datasets with similar parameters and computational costs.
In ad-dition, DW-ViT exhibits good scalability and can be easily inserted into any window-based visual transformers.1 1.

Introduction
In computer vision (CV) tasks, the visual transformer represented by Vision Transformer (ViT) [12] has shown great potential. These methods have achieved impres-*Corresponding author. 1Code release: https://github.com/pzhren/DW-ViT. This work was done when the first author interned at Dark Matter AI.
Figure 1. Performance comparison of DW-ViT, Swin [31] and
Swin with multi-scale window (MSW-Swin) on ImageNet-1K [10] as the window size increases. We use a purple broken line (l) to indicate the performance and FLOPs changes of Swin-T [31] with a single-scale window (win ∈ [7, 14, 21, 23]). The multi-scale windows used by MSW-Swin and DW-T are all set to [7, 14, 21]. sive performance on tasks such as image classification
[37, 49], semantic segmentation [30, 51] and object detec-tion [31, 55, 57].
In ViT, the complexity of the self-attention operation is proportional to the square of the number of image patches.
This is unfriendly to most tasks in the CV field. Swin [31] thus proposed to limit the calculation of self-attention to a local window to reduce the computational complexity and achieved some promising results. This local window self-attention quickly attracted a significant amount of atten-tion [7, 28, 50]. However, most of these methods [7, 28, 50] use a fixed single-scale window (e.g., win = 7) by default.
The following questions accordingly arise: Is this window size optimal? Does a bigger window entail better perfor-mance? Is a multi-scale window more advantageous than a single-scale window? Furthermore, will dynamic multi-scale windows yield better results? To answer these ques-tions, we evaluate the impact of window sizes on the model performance. In Fig. 1, we report the change curve (l) of top-1 accuracy and FLOPs (G) of Swin-T [31] under four single-scale windows (win ∈ [7, 14, 21, 23]) on ImageNet-(a) DW-ViT (ours) (b) Swin Transformer
Figure 2. Comparison of DW-ViT’s multi-scale window (e.g., win1 = 6 and win2 = 3) and Swin-based single-scale win-dow (e.g., win = 9). The number of patches in the local win-dow is win × win. A dynamic multi-scale window (DMSW) is a dynamic adaptive window module we designed for multi-scale window multi-head self-attention (MSW-MSA). α is a learnable parameter of the DMSW module. α1 and α2 are a possible weight distribution scheme of DMSW. 1K [10]. In Swin [31], the window size has a very small effect on the amount of model parameters.
As shown in Fig. 1, as the window size increases, the performance of the model is found to be significantly im-proved, but this is not absolutely monotonous. For exam-ple, when the window size is increased from 21 to 23, the performance of the model hardly improves or even drops.
Therefore, it is not feasible to simply increase the window to improve the performance of the model. In addition, it is difficult to choose the best window size from multiple alter-native window sizes. And the optimal window settings of different layers may also be different. A natural idea is to mix information from windows of different scales for pre-diction tasks. Based on this idea, we design a multi-scale window multi-head self-attention (MSW-MSA) mechanism for the window-based ViT. In Fig. 1, as shown in the re-sults of Swin-T with MSW (MSW-Swin) and Swin-T with a single-scale window, simply introducing the MSW mech-anism for the W-MSA of the transformer cannot further ef-fectively improve the performance of the model. For exam-ple, the performance of MSW-Swin (win = [7, 14, 21]) is lower than that of Swin-T with single-scale windows when win = 21. It may be caused by suboptimal window settings that impairs the performance of the model. This shows that it may require more effort to protect ViT with MSW from suboptimal window settings while retaining the advantages of multi-scale windows. On the other hand, the dynamic neural network [17] has been favored by a large number of researchers because of its ability to adjust the structure and parameters of the model adaptively according to the input.
Moreover, the dynamic network has been successfully ap-plied in CNN [27, 40, 43, 44, 53, 62] and ViT [4, 50, 55].
Based on the above observations, in this paper, we pro-pose a novel method, named Dynamic Window Vision
Transformer (DW-ViT). As far as we know, it is the first method to use dynamic multi-scale windows to explore the upper limit of the impact of window settings on model per-Figure 3. In the visual transformer, a schematic diagram of the window self-attention calculation process. Assume that the num-ber of pixels in the input image is H ×W (e.g. 36×36). The image is first split into ⌈ H p ⌉ fixed-size patches (e.g. p = 6), and then the self-attention calculation is limited to a fixed-size window (i.e. each window has M × M patches, e.g. M = win = 3). For simplicity, patch and position embeddings are omitted here. p ⌉ × ⌈ W formance. In DW-ViT, we first obtain multi-scale informa-tion by assigning different scale windows to different head groups of multi-head self-attention in transformer. Then, we realize the dynamic fusion of information by assigning weights to the multi-scale window branches. In Fig. 2, we present a comparison of DW-ViT’s multi-scale window and single-scale window approaches based on Swin [31] class methods. More specifically, in DW-ViT, MSW-MSA is re-sponsible for the extraction of multi-scale window informa-tion, while DMSW is responsible for the dynamic enhance-ment of these multi-scale information. Through the above two parts, DW-ViT can improve the model’s multi-scale in-formation modeling capabilities dynamically while ensur-ing relatively low computational complexity. As shown in
Fig. 1, the performance of DW-T with a dynamic window is significantly better than that of Swin-T with a single fixed-scale window, which we call ”beyond fixed”. Our main con-tributions can be summarized as follows:
• The recently popular window-based ViT mostly ig-nores the influence of window size on model perfor-mance. This severely limits the upper limit of the model’s performance. As far as we know, we are the first to challenge this problem.
• We propose a novel plug-and-play module with a dynamic multi-scale window for multi-head self-attention in transformer. DW-ViT is superior to all other ViTs that use the same single-scale window and can be easily embedded into any window-based ViT.
• Compared with the state-of-the-art methods, DW-ViT achieves the best performance on multiple CV tasks with similar parameters and FLOPs. 2.