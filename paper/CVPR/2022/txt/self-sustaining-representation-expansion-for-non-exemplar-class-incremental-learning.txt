Abstract
Non-exemplar class-incremental learning is to recog-nize both the old and new classes when old class sam-ples cannot be saved.
It is a challenging task since rep-resentation optimization and feature retention can only be achieved under supervision from new classes. To address this problem, we propose a novel self-sustaining representa-tion expansion scheme. Our scheme consists of a structure reorganization strategy that fuses main-branch expansion and side-branch updating to maintain the old features, and a main-branch distillation scheme to transfer the invari-ant knowledge. Furthermore, a prototype selection mecha-nism is proposed to enhance the discrimination between the old and new classes by selectively incorporating new sam-ples into the distillation process. Extensive experiments on three benchmarks demonstrate significant incremental per-formance, outperforming the state-of-the-art methods by a margin of 3%, 3% and 6%, respectively. 1.

Introduction
Since deep neural networks have made great advances in fully supervised conditions, research attention is increas-ingly turning to other aspects of learning. An important as-pect is the ability to continuously learn new tasks as the input stream is updated, which is often the case in real ap-plications. In recent years, class-incremental learning (CIL)
[11, 27], a difficult type in continual learning, has attracted much attention, which aims to recognize new classes with-out forgetting the old ones that have been learned.
In this case, re-training the old and new class samples jointly in each phase is time-consuming and laborious, not to mention that the old class samples may not be fully avail-able. A simple alternative is to fine-tune the network using the new class, however, it will cause the catastrophic forget-ting problem [8]. That is, during the optimization process,
â€ Corresponding Author
Figure 1. The t-SNE visualization. Compared to the baseline in
Section 4.1, (1) the representations of the old classes in our method are better maintained (circular area), (2) and the novel class is more discriminating from the old classes (rectangular area). the entire representation and the classifier become biased toward the new class, resulting in a sharp drop in the perfor-mance for the old class. To deal with it, recent CIL methods maintain the past knowledge by preserving some represen-tative samples (i.e., exemplars [27]) and introducing various distillation losses [7], and correct the bias caused by number imbalance by calibrating the classifier [11].
However, most of the existing methods [21, 30] assume that a certain number (e.g., 2000) of exemplars can be stored in memory, which is usually difficult to satisfy in practice due to user privacy or device limitations. This fact poses great difficulties to incremental learning, because the opti-mization of the representation and the correction of the clas-sifier will degenerate directly from the imbalance between the old and new classes. To this end, this paper focuses on this ability of incrementally learning new classes where old class samples cannot be preserved, which is called non-Figure 2. Motivation of our method. In NECIL, the rehearsal-based and structure-based methods suffer from the unreliability of distillation in the absence of exemplars and continuously expanding structure, respectively. DSR is proposed to drive network to expand from a structurally recoverable direction, thus maintaining the discrimination during the new optimization process. On this foundation, we utilize
MBD to exploit the ability of distillation-based methods to balance old and new class knowledge. exemplar class-incremental learning (NECIL [44]).
A natural idea for this problem is to directly transfer the existing CIL framework (i.e., rehearsal-based and structure-based methods in Section 2.1) to NECIL, but the exper-imental results show that this way leads to performance degradation and parameter explosion. On one hand, in rehearsal-based methods, due to the lack of old class sam-ples, the distillation that the new class samples participate in is the only one that can help maintain the representation of old classes. However, for new samples, it is impractical to provide the same complete old class distribution as the ex-emplars, so it is difficult to effectively promote the knowl-edge transfer in the distillation process. Consequently, rep-resentative features learned in the old phase are lost phase by phase with the decrease of relevance to the new class.
On the other hand, the idea of structure-based methods is to leave the old model for inference and expand a new model for training at each new phase [28,35]. Although this strategy maintains the performance of the old class com-pletely, demonstrating strong performance [35], the net-work parameters that increase linearly with phase (i.e., 5, 10 and 20 in this paper) during training are discouraging. Be-sides, although a large amount of data can be used to learn the discriminative features among new classes, it is easy to confuse with similar ones from the old distribution. The augmentation of prototypes [44] can only improve the se-lection of the optimal boundary for the classifier, but cannot essentially improve the discrimination of the old and new classes in the feature representation. As shown in Fig. 1, the representations of old classes obtained by the standard CIL method are more confused compared to the initial phase, be-cause they may gradually overlap with similar classes due to the lack of effective supervision. At the same time, the new class may directly overlap with the old cluster, resulting in serious confusion to the subsequent optimization process.
To address this problem, we propose a self-sustaining representation expansion scheme to learn a structure-cyclic representation, promoting the optimization from the ex-panded direction while integrating the overall structure at the end of each phase. As shown in Fig. 2, the preservation of the old classes is reflected in both the structure and fea-ture aspects. First, we adopt a dynamic structure reorgani-zation (DSR) strategy, which leaves structured space for the learning of new class while stably preserving the old class space through maintaining heritage at the main-branch and fusing update at the side-branch. Second, on the basis of the expandable structure, we employ a main-branch distillation (MBD) to maintain the discrimination of the new network with respect to the old features by aligning the invariant dis-tribution knowledge on the old classes.
Specifically, we insert a residual adapter in each block of the old feature extractor to map the old representation to a high-dimensional embedding space, forcing the optimiza-tion flow to only pass through the expanding branches unre-lated to the old class. After the optimization, we adopt the structural re-parameterization technique to fuse the old and new features and map them back to the initial space loss-lessly. Furthermore, to reduce the confusion between the newly incremental classes and the original classes, we add a prototype selection mechanism (PSM) during the distilla-tion process. The normalized cosine is first used to measure the similarity between the new representation and the old prototype. Then samples similar to the old classes are used for distillation, maintaining the old knowledge with a soft label that retains the old class statistical information, while those samples dissimilar to the old classes are used for new
class training. This mechanism improves the performance of forward transfer and mitigates the lack of joint optimiza-tion to some extent. Our main contributions are as follows: 1) A self-sustaining representation expansion scheme is proposed for non-exemplar incremental learning, in which a cyclically expanding optimization is accomplished by a dynamic structure reorganization strategy, resulting in a structure-invariant representation. 2) A prototype selection mechanism is proposed, which combinatorially co-uses the preserved invariant knowledge and the incoming new supervision to reduce the feature con-fusion among the similar classes. 3) Extensive experiments are performed on bench-marks including CIFAR-100, TinyImageNet and ImageNet-Subset, and the results demonstrate the superiority of our method over the state of the art. 2.