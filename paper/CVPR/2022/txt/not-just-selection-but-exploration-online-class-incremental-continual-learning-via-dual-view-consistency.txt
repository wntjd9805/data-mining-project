Abstract
Online class-incremental continual learning aims to learn new classes continually from a never-ending and single-pass data stream, while not forgetting the learned knowledge of old classes. Existing replay-based method-s have shown promising performance by storing a subset of old class data. Unfortunately, these methods only fo-cus on selecting samples from the memory bank for re-play and ignore the adequate exploration of semantic in-formation in the single-pass data stream, leading to poor classification accuracy. In this paper, we propose a novel yet effective framework for online class-incremental con-tinual learning, which considers not only the selection of stored samples, but also the full exploration of the data stream. Specifically, we propose a gradient-based sample selection strategy, which selects the stored samples whose gradients generated in the network are most interfered by the new incoming samples. We believe such samples are beneficial for updating the neural network based on back gradient propagation. More importantly, we seek to ex-plore the semantic information between two different views of training images by maximizing their mutual informa-tion, which is conducive to the improvement of classifica-tion accuracy. Extensive experimental results demonstrate that our method achieves state-of-the-art performance on a variety of benchmark datasets. Our code is available on https://github.com/YananGu/DVC. 1.

Introduction
Intelligent systems [24, 30, 33, 49, 53, 56] based on con-volutional neural networks have achieved excellent perfor-mance on various tasks, some of which even exceed human-level performance. However, these intelligent systems, which need to restart the training process when new data is available, lack the ability to accumulate knowledge over time as humans do. Actually, such a restart practice is of-∗ Corresponding author ten not applicable in real scenarios because of training costs and privacy concerns. In order to achieve a higher level in-telligent system, Continual Learning [17, 35, 38] studies the problem of learning from a never-ending data stream. A sig-nificant problem such a never-ending learning process faces is catastrophic forgetting—the inability to retain previously learned knowledge when learning new tasks.
Existing methods [1, 32, 42, 55] often relax the problem of continual learning to a more accessible task-incremental setting. In a task-incremental setting, the data stream is di-vided into several tasks with clear boundaries, and each task is learned offline. However, this setting lacks flexibility, be-cause data in real world scenarios is often streamed online without task identity.
In this paper, we focus on a more general online class-incremental continual setting, where a stream of samples is seen only once and task identity is un-available during the training and testing phases.
There are already many types of methods proposed for task-incremental setting [1, 32, 34, 37], which can be pri-marily divided into three categories: regularization-based,
In these parameter-isolation, and replay-based methods. methods, replay-based methods [2,13,45] have been proved to be simple yet efficient compared to other methods in the online class-incremental continual setting. Such methods usually keep previous performance by recording some sam-ples of old classes for replay. Specifically, the recorded and incoming samples are put together to train the model, which makes the model preserve the previous knowledge to the greatest extent and learn new knowledge simultaneously.
However, these methods only focus on finding the optimal recorded samples for replay and lack a full exploration of semantic information in the single-pass data stream, lead-ing to poor classification accuracy.
In this paper, we propose a novel yet effective framework for online class-incremental continual learning to address the deficiencies observed above. Specifically, we propose a Maximally Gradient Interfered (MGI) retrieval strategy, which selects the stored samples whose gradients generat-ed in the network are most interfered by the new incoming samples. We believe such samples are beneficial for updat-ing the neural network based on back gradient propagation.
More importantly, we propose a Dual View Consistency (D-VC) strategy to fully leverage the data stream, including the incoming and retrieved images. Besides learning the tradi-tional input-output mapping, the network is also forced by
DVC to explore a consistent mapping between the represen-tations of two transformed inputs with the same label (dual view image pairs). More specifically, we maximize the mu-tual information among dual view image pairs, which con-strains the network to mine the common semantic informa-tion between image pairs. In this way, the model can learn the invariant image representations, which is beneficial for improving classification accuracy. Extensive experiments demonstrate both MGI and DVC improve the performance of the proposed method, which is found to achieve state-of-the-art performance on three commonly used datasets.
To sum up, our contributions are as follows:
• Unlike existing methods that only focus on selecting samples from the memory bank for replay, we pro-pose a novel yet effective framework for online class-incremental continual learning, which simultaneously considers optimal samples selection and sufficient ex-ploration of the single-pass data stream.
• We propose a Maximally Gradient Interfered retrieval strategy to better maintain the performance of old classes, and offer a Dual View Consistency strategy to further improve the classification accuracy.
• Extensive empirical results demonstrate our method performs significantly better than existing methods on several benchmark datasets. 2.