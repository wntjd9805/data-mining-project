Abstract
Fully unsupervised 3D representation learning has gained attention owing to its advantages in data collec-tion. A successful approach involves a viewpoint-aware ap-proach that learns an image distribution based on genera-tive models (e.g., generative adversarial networks (GANs)) while generating various view images based on 3D-aware models (e.g., neural radiance fields (NeRFs)). However, they require images with various views for training, and consequently, their application to datasets with few or lim-ited viewpoints remains a challenge. As a complementary approach, an aperture rendering GAN (AR-GAN) that em-ploys a defocus cue was proposed. However, an AR-GAN is a CNN-based model and represents a defocus independently from a viewpoint change despite its high correlation, which is one of the reasons for its performance. As an alternative to an AR-GAN, we propose an aperture rendering NeRF (AR-NeRF), which can utilize viewpoint and defocus cues in a unified manner by representing both factors in a com-mon ray-tracing framework. Moreover, to learn defocus-aware and defocus-independent representations in a disen-tangled manner, we propose aperture randomized training, for which we learn to generate images while randomizing the aperture size and latent codes independently. During our experiments, we applied AR-NeRF to various natural image datasets, including flower, bird, and face images, the results of which demonstrate the utility of AR-NeRF for un-supervised learning of the depth and defocus effects. 1.

Introduction
Natural images are 2D projections of the 3D world.
Solving the inverse problem, i.e., understanding the 3D world from natural images, is a principal challenge in com-puter vision and graphics and has been actively studied in various fields owing to its diverse applications, such as en-vironmental understanding in robotics, content creation in advertisements, and photo editing in the arts.
After collecting pairs of 2D and 3D data or sets of mul-tiview images, a successful approach is to learn the 3D predictor using direct or photometric-driven supervision.
This approach demonstrates promising results in terms of fidelity. However, the collection of such data is often diffi-cult or impractical. To reduce the collection costs, learning from single images (i.e., from a dataset that includes a sin-gle image per training instance) has been actively studied.
To obtain clues under such setting, several studies [19, 32, 75, 77, 93] have introduced object-specific shape mod-els, including 3DMM [5] and SMPL [51], and searched
for solutions within the shape model constraints. Other studies have utilized auxiliary information such as 2D key-points [33, 87] or 2D silhouettes [10, 22, 27, 44] to sim-plify the problem by aligning the object parts or separating the target objects from the background. These studies also demonstrate remarkable results; however, the construction of the shape model is not always easy and narrows the appli-cable objects, and auxiliary information incurs extra costs in terms of data collection.
To alleviate such restrictions, a fully unsupervised ap-proach, which learns 3D representations from single im-ages without any additional supervision (including auxil-iary information and pre-trained models), has gained atten-tion. Under this setting, the viewpoint is a principal clue, which typical methods utilize by learning an image distri-bution using a generative model (e.g., a generative adver-sarial network (GAN) [23]) while generating various view-point images based on viewpoint-aware 3D models, such as voxels [27, 58, 59], primitives [46], and neural radiance fields (NeRFs) [9, 24, 56, 60, 61, 76]. This allows learning a viewpoint-aware 3D representation; however, owing to the diverse viewpoints needed, the application to a dataset in which viewpoint cues are limited or unavailable without the use of a preprocessing (e.g., natural flower or bird images, as shown in Figure 1) remains a challenge.
As a complement to a viewpoint cue, an aperture render-ing GAN (AR-GAN) [34] was proposed to exploit a defo-cus cue by equipping the aperture rendering [83] on top of the CNN GANs. This constraint allows the learning of both depth and depth-of-field (DoF) effects in an unsupervised manner. However, as a limitation, an AR-GAN employs the defocus cue independently from the viewpoint cue and cannot utilize both factors jointly despite these two factors being highly correlated with the ability to help each other.1
Consequently, the quality of the depth prediction when us-ing AR-GAN remains limited.
We thus aim to construct a unified model that can lever-age defocus and viewpoint cues jointly by considering the application of unsupervised learning of the 3D representa-tion (particularly depth and defocus effects) from natural un-structured (and view-limited) images (Figure 1). To achieve this, we propose a new extension of NeRF called aperture rendering NeRF (AR-NeRF), which can represent defocus effects and viewpoint changes in a unified manner by repre-senting both factors through a common ray-tracing frame-work. More precisely, in contrast to the standard NeRF, which represents each pixel using a single ray under the pin-hole camera assumption, AR-NeRF employs an aperture camera [79] that represents each pixel using a collection of rays that converge at the focus plane and whose scale is de-in
[34], 1More precisely, the combinations of an AR-GAN and viewpoint-aware GANs (particularly, HoloGAN [58] and RGBD-GAN [65]) are provided. These models can learn the defocus and viewpoint-aware representations simultaneously but individually; there-fore, such models cannot utilize the learning of one representation for the learning of another. termined according to the aperture size. Through such mod-eling, we can represent both viewpoint changes and defocus effects by simply changing the inputs and the integration of the implicit function (multilayer perceptron (MLP)), which converts the point position and view direction into the RGB color and volume density. Consequently, through training, we can optimize the MLP while reflecting both factors.
Moreover, to disentangle defocus-aware and defocus-independent representations in an unsupervised manner, we introduce aperture randomized training, in which we learn to generate images in a GAN framework while changing the aperture size and latent codes both randomly and indepen-dently. A similar technique is commonly used in viewpoint-aware representation learning [9, 24, 27, 46, 56, 58–61, 76], and this training is useful for disentangling the effect of the corresponding factor from latent codes.
We applied AR-NeRF to natural image datasets, includ-ing view-limited (Oxford Flowers [64] (flower) and CUB-200-2011 [90] (bird)) datasets and datasets with various views (FFHQ [39] (face)), and demonstrated that AR-NeRF is better than or comparable to the baseline models, includ-ing a state-of-art fully unsupervised depth-learning model (i.e., AR-GAN [34]) and generative NeRF (particularly pi-GAN [9]), in terms of the depth prediction accuracy. We also demonstrated that AR-NeRF can manipulate the defo-cus effects (i.e., defocus strength and focus distance) intu-itively and continuously while retaining the image quality, whereas AR-GAN has difficulty doing so.
Overall, our contributions can be summarized as follows:
• To achieve an unsupervised learning of the depth and defocus effects, we propose a new extension of NeRF called AR-NeRF, which can employ viewpoint and de-focus cues in a unified manner by representing both factors in a common ray-tracing framework.
• To and disentangle defocus-aware representations defocus-independent unsupervised conditions, we introduce aperture randomized train-ing, by which we learn to generate images while changing the aperture size and latent codes both randomly and independently. under
• We empirically demonstrate the utility of AR-NeRF for the unsupervised learning of the depth and defocus effects using various natural image datasets, including view-limited (flower and bird) datasets and datasets of various views (face). We provide detailed analyses and extended results in the supplementary material.2 2.