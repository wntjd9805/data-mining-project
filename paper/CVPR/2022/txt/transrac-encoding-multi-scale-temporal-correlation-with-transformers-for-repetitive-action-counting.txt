Abstract
Counting repetitive actions are widely seen in human ac-tivities such as physical exercise. Existing methods focus on performing repetitive action counting in short videos, which is tough for dealing with longer videos in more re-alistic scenarios. In the data-driven era, the degradation of such generalization capability is mainly attributed to the lack of long video datasets. To complement this margin, we introduce a new large-scale repetitive action counting dataset covering a wide variety of video lengths, along with more realistic situations where action interruption or action inconsistencies occur in the video. Besides, we also pro-vide a fine-grained annotation of the action cycles instead of just counting annotation along with a numerical value.
Such a dataset contains 1,451 videos with about 20,000 an-notations, which is more challenging. For repetitive action counting towards more realistic scenarios, we further pro-pose encoding multi-scale temporal correlation with trans-formers that can take into account both performance and ef-ficiency. Furthermore, with the help of fine-grained annota-tion of action cycles, we propose a density map regression-based method to predict the action period, which yields bet-ter performance with sufficient interpretability. Our pro-posed method outperforms state-of-the-art methods on all datasets and also achieves better performance on the un-seen dataset without fine-tuning. The dataset and code are available 1. 1.

Introduction
Planetary motion, the change of seasons, and heartbeats, these periodic movements that are everywhere in our lives.
*These authors contributed equally to this work.
†Corresponding authors. 1https://svip- lab.github.io/dataset/RepCount_ dataset.html
They can be modeled by the Newtonian mechanics, or de-tected with the aid of sensors for the understanding of the world or our bodies.
In computer vision, the detection of repetitive/periodic motions also plays an important role, such as in human activity, where the counting of some phys-ical exercise movements benefits people in fitness detec-tion and planning. Although one can use some sensors (e.g., gravity sensors) on the human body, vision-based ap-proaches enable non-invasive and thus make third-view-based video analysis possible and promising. Repetitive action counting in computer vision is also useful as an aux-iliary cue for other human-centric video analysis applica-tions, such as pedestrian detection [26] and 3D reconstruc-tion [17, 28].
Despite this importance, repetitive action counting meth-ods in computer vision has rarely been explored. Previ-ous papers [5, 38] tend to count repetitive actions in short videos, such as some simple videos grabbed from the Ki-netics dataset [10]. However, these videos lack some realis-tic scenarios, which limits the application of the method in more realistic scenarios due to the following two points:
• Restricted video length. The previous datasets [5, 16, 29, 38] typically contain only short videos (e.g., 0.4-30 s), however, methods are likely to be deployed to long videos in real scenarios. For instance, we count push-ups or jump-jacks with a video length of 60 s.
Counting actions in such long videos is more challeng-ing because there might exist various anomalies in real scenarios, such as the action being interrupted with in-ternal or external reasons (Fig. 1(a) ), or the incon-sistency between action periods (Fig. 1(b) ). These anomalies might cause the previous algorithm to fail or obtain sub-optimal performance, affecting the gen-eralization of the algorithm to real scenarios.
• Inadequate annotations. In previous datasets [5, 16, 29, 38], the number of repetitive actions in a video
(a) Interruption during the actions (squats) (b) Inconsistent action cycles (push up) (c) Long video with numerical cycles (60 seconds) (punch jacks) (d) Annotations in the form of start and end of each cycle (front raise)
Figure 1. The features of our proposed dataset RepCount: (a) anomaly case (interruption during actions); (b) anomaly case (inconsistent action cycles); (c) long video that consists of numerical action cycles; (d) the fine-grained labeling protocol. is simply labeled as a numerical value. Although the count number serves as an ultimate predictive goal, such coarse-grained annotation deprives the in-terpretability of the algorithm. The model only pre-dicts a numerical value during training or inference, which makes it difficult to evaluate the model more completely. As argued in some crowd counting pa-pers [20, 27, 36, 40], the total number of repetitive ac-tions is correct, but the position of the intermediate cy-cles might be wrong.
In data-driven deep learning approaches, the dataset is the key to algorithmic innovation. To tackle the above prob-lems, we collect a large-scale human-centric dataset, which is closer to the real one. As shown in Fig. 1(c), there are a large number of variations in video length, while the inter-ruptions or inconsistent action cycles occur in some videos.
For more accurate performance evaluation and model in-terpretability, we provide a more fine-grained annotation of the action cycles, such as Fig. 1(d). Further, we also col-lect a part containing student activity videos captured in a fully realistic scenario (in local school), which is signifi-cantly different from the previous datasets where the videos are crawled from YouTube. Fig. 2 provides an overview of our dataset. Such a dataset is more challenging and has the potential to become a new benchmark for repetitive action counting.
To perform repetitive action counting, previous meth-ods [5] generally take a fixed number of frames for pre-diction. Such an approach might be reasonable in relatively short videos. For example, TSN [37] extracts three frames for action recognition of trimmed videos, where the infor-mation characterizing the action is concentrated on certain keyframes. However, for long videos in the real scenario, extracting fixed frames will result in sub-optimal perfor-mance. Since the video duration varies very much (e.g. from 4s to 88s) and if the number of selected frames is too small, high-frequency actions will be neglected. On the contrary, if too many frames are selected, it might cause a waste of computational resources. Another alternative is to sample the video with the same frequency for both long and short videos. However, some actions are very fast (e.g., jumping rope) and some are very slow (e.g., push-ups).
Sampling with a fixed frequency would either lead to perfor-mance degradation or would not be efficient enough. To bal-ance performance and efficiency, we propose a multi-scale temporal correlation encoding network with transformers that can take care of not only high and low-frequency ac-tions but also long and short videos. This approach allows the model to automatically select its adapted scale to com-pute the correlation matrix for final count prediction. Fur-thermore, thanks to the fine-grained annotation of action cy-cles in our dataset (see Fig. 1(d) ), we also propose a den-sity map regression-based method to predict action periods, which not only yields better performance but is also more beneficial for the interpretability of the model.
We summarize our contributions in three-fold:
• We introduce a new dataset, named RepCount, which consists of 1,451 videos and about 20,000 fine-grained annotations. Such a dataset allows for a large number of video length variations and contains anomaly cases, thus is more challenging.
• A new multi-scale temporal correlation encoding network with transformers, which can take care of not only high and low frequency actions, but also long and short videos, is designed for repetitive action counting.
• The proposed method outperforms state-of-the-art meth-ods on our proposed dataset and all other datasets. Fur-thermore, we also achieve better performance on the un-seen dataset without fine-tuning. 2.