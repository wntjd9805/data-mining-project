Abstract
Nowadays, cameras equipped with AI systems can cap-ture and analyze images to detect people automatically.
However, the AI system can make mistakes when receiving deliberately designed patterns in the real world, i.e., phys-ical adversarial examples. Prior works have shown that it is possible to print adversarial patches on clothes to evade
DNN-based person detectors. However, these adversarial examples could have catastrophic drops in the attack suc-cess rate when the viewing angle (i.e., the camera’s an-gle towards the object) changes. To perform a multi-angle attack, we propose Adversarial Texture (AdvTexture). Ad-vTexture can cover clothes with arbitrary shapes so that people wearing such clothes can hide from person detec-tors from different viewing angles. We propose a generative method, named Toroidal-Cropping-based Expandable Gen-erative Attack (TC-EGA), to craft AdvTexture with repetitive structures. We printed several pieces of cloth with AdvTex-ure and then made T-shirts, skirts, and dresses in the physi-cal world. Experiments showed that these clothes could fool person detectors in the physical world. 1.

Introduction
Recent works have shown that Deep Neural Networks (DNNs) are vulnerable to the adversarial examples crafted by adding subtle noise to the original images in the dig-ital world [5, 8, 10, 18, 22–24, 31], and that the DNNs can be attacked by manufactured objects in the physical world [1, 4, 9, 29]. These manufactured objects are called physical adversarial examples. Recently, some methods based on patch attacks [29] have been proposed to evade person detectors [14,15,32,34,35,37]. Specifically, Thys et
∗Corresponding author. (a) (b) (c) (d)
Figure 1.
Illustration of the attacks at different viewing angles. (a) The camera captures different parts (P1, P2, P3) of the clothes when set to different viewing angles (C1, C2, C3). (b-d) The boxes are the possible areas that the camera may capture. The blue ones indicate the most effective areas for attack, while the red ones are less effective. al. [32] proposed to attach a patch to a cardboard. By hold-ing the cardboard in front of the camera, the person cannot be detected by the person detectors. Xu et al. [35] proposed an adversarial T-shirt printed with adversarial patches. The person wearing the T-shirt can also evade person detectors.
These works impose considerable threats to the widely de-ployed deep learning-based security systems. It urges re-searchers to re-evaluate the safety and reliability of these systems.
However, the person detector attack methods mentioned above are effective only when the adversarial patches face the camera. Apparently, a single adversarial patch on a piece of clothing is hard to attack detectors at multiple view-ing angles, as the camera may only capture a segment of the heavily deformed patch (Fig. 1a and Fig. 1b). We call this the segment-missing problem. A naive extension is to cover the clothing with multiple patches (e.g., tiling the patches tightly on the clothing; see Fig. 1c). However, it cannot totally solve the segment-missing problem, because the camera will capture several segments belonging to dif-ferent patch units, making the attack inefficient. Another straightforward solution is to build a 3D model of a human
2.