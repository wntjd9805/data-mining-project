Abstract
Knowledge distillation has been applied to image clas-sification successfully. However, object detection is much more sophisticated and most knowledge distillation meth-ods have failed on it.
In this paper, we point out that in object detection, the features of the teacher and stu-dent vary greatly in different areas, especially in the fore-ground and background. If we distill them equally, the un-even differences between feature maps will negatively af-fect the distillation. Thus, we propose Focal and Global
Distillation (FGD). Focal distillation separates the fore-ground and background, forcing the student to focus on the teacher’s critical pixels and channels. Global distilla-tion rebuilds the relation between different pixels and trans-fers it from teachers to students, compensating for missing global information in focal distillation. As our method only needs to calculate the loss on the feature map, FGD can be applied to various detectors. We experiment on vari-ous detectors with different backbones and the results show that the student detector achieves excellent mAP improve-ment. For example, ResNet-50 based RetinaNet, Faster
RCNN, RepPoints and Mask RCNN with our distillation method achieve 40.7%, 42.0%, 42.0% and 42.1% mAP on COCO2017, which are 3.3, 3.6, 3.4 and 2.9 higher than the baseline, respectively. Our codes are available at https://github.com/yzd-v/FGD. 1.

Introduction
Recently, deep learning has achieved great success in various domains [8, 9, 22, 24]. To get better performance, we usually use a larger backbone, which needs more com-pute resources and inferences more slowly. To get over this, knowledge distillation has been proposed [11]. Knowl-*This work was performed while Zhendong worked as an intern at
ByteDance.
†Corresponding author
Figure 1. Visualization of the spatial and channel attention map from the teacher detector (RetinaNet-ResNeXt101) and the stu-dent detector (RetinaNet-ResNet50).
RetinaNet
Res101-Res50 distillation area fg bg split
× ×
✓ ×
× ✓
✓ ✓
✓ ✓
×
✓ mAP mAR 37.4 39.3 39.2 38.9 39.4 53.9 55.6 55.8 55.1 56.1 fg: fore-Table 1. Comparisons of different distillation areas. ground. bg: background. split: split the foreground and back-ground and distill them with different weights. edge distillation is a method to inherit the information from a large teacher network to a compact student net-work and achieve strong performance without extra cost during inference time. However, most distillation meth-ods [10, 27, 33, 34] are designed for image classification, which lead to trivial improvements for object detection.
It is well acknowledged that the extreme foreground-background class imbalance is a key point in object detec-tion [17]. The imbalanced ratio also harms the distillation for object detection. There are some efforts for this prob-lem. Chen et al. [3] distributes a weight to suppress the background. Mimick [15] distills the positive area proposed by region proposal network of the student. FGFI [28] and
TADF [25] use the fine-grained and Gaussian Mask to se-lect the distillation area, respectively. Defeat [7] distills the foreground and background separately. However, where is the key area for distillation is still not clear.
In order to explore the difference between the features of students and teachers, we do the visualization of the spa-tial and channel attention. As the Fig. 1 shows, the differ-ence between student’s attention and teacher’s attention in the foreground is quite significant, while that in the back-ground is relatively small. This may lead to different diffi-culties in learning the foreground and background. In this paper, we further explore the influence of the foreground and background in knowledge distillation on object detec-tion. We design experiments by decoupling the foreground and background in the distillation. Surprisingly, as shown in
Tab. 1, the performance of distillation on the foreground and background together is the worst, even worse than only us-ing foreground or background. This phenomenon suggests that the uneven differences in the feature map can negatively affect distillation. Besides, as shown in Fig. 1, the attention between each channel is also very different. Thinking one step deeper, not only are there negative influences between the foreground and the background, but also between the pixels and the channels. Therefore, we propose focal distil-lation. While separating the foreground and background, focal distillation also calculates the attention of different pixels and channels in teacher’s feature, allowing the stu-dent to focus on teacher’s crucial pixels and channels.
However, just focusing on key information is not enough.
It is well known that global context also plays an impor-tant role in detection. A lot of relation modules have been successfully applied into detection, such as non-local [29],
GcBlock [2], relation network [12], which have greatly im-proved the performance of detectors. In order to compen-sate for the missing global information in focal distillation, we further propose global distillation. In global distillation, we utilize GcBlock to extract the relation between different pixels and then distill them from teachers to students.
As we analyzed above, we propose Focal and Global
Distillation (FGD), combining focal distillation and global distillation, as shown in Fig. 2. All loss functions are only calculated on features, so that FGD can be used directly on various detectors, including two-stage models, anchor-based one-stage models and anchor-free one-stage models.
Without bells and whistles, we achieve state-of-the-art per-formances in object detection with FGD. In a nutshell, the contributions of this paper are:
• We present that the pixels and channels that teacher and student pay attention to are quite different. If we distill the pixels and channels without distinguishing them, it will result in a trivial improvement.
• We propose focal and global distillation, which en-ables the student not only to focus on the teacher’s crit-ical pixels and channels, but also to learn the relation between pixels.
• We verify the effectiveness of our method on various detectors via extensive experiments on the COCO [18], including one-stage, two-stage, anchor-free methods, achieving state-of-the-art performance. 2.