Abstract
Federated semi-supervised learning (FSSL) aims to de-rive a global model by training fully-labeled and fully-unlabeled clients or training partially labeled clients. The existing approaches work well when local clients have in-dependent and identically distributed (IID) data but fail to generalize to a more practical FSSL setting, i.e., Non-IID setting. In this paper, we present a Random Sampling
Consensus Federated learning, namely RSCFed, by con-sidering the uneven reliability among models from fully-labeled clients, fully-unlabeled clients or partially labeled clients. Our key motivation is that given models with large deviations from either labeled clients or unlabeled clients, the consensus could be reached by performing random sub-sampling over clients. To achieve it, instead of di-rectly aggregating local models, we first distill several sub-consensus models by random sub-sampling over clients and then aggregating the sub-consensus models to the global model. To enhance the robustness of sub-consensus models, we also develop a novel distance-reweighted model aggre-gation method. Experimental results show that our method outperforms state-of-the-art methods on three benchmarked datasets, including both natural and medical images. The code is available at https://github.com/XMed-Lab/RSCFed. 1.

Introduction
The core idea of federated learning (FL) is to train ma-chine learning models on separate datasets that are dis-tributed across different places or devices, which can pre-serve local data privacy to a certain extent. Over the past few years, FL has emerged as an important research area and attracted many researchers’ attention to study its appli-cation in medical image diagnosis [10, 14, 28], image clas-sification [16] and object detection [22].
Considerable efforts have been devoted to design vari-ous FL methods, such as FedAvg [23], SCAFFOLD [12]
‡Project lead and corresponding author.
Illustration of the existing FSSL method, i.e., [28]
Figure 1. and our RSCFed. Existing methods simply perform the standard model aggregation in FedAvg [23], while our RSCFed distills mul-tiple sub-consensus models from local models and updates the global model via aggregating sub-consensus models. and MOON [16]. Although the results are quite promising, these methods require fully labeled images on each local client, limiting its application in real practice.
Recently, federated semi-supervised learning (FSSL) [8, 19, 21, 28] is becoming a new research topic, aiming at uti-lizing the unlabeled images to enhance the global model de-velopment. One line of the research studies FSSL by con-sidering each client has partially labeled and unlabeled im-ages. For example, Jeong et al. [8] introduced inter-client consistency loss to improve the global model by encourag-ing the consistent outputs from multiple clients. Another line of FSSL [21, 28] assumes that some local clients have fully labeled images while some clients contain unlabeled images, which we denote as labeled clients and unlabeled clients respectively. However, existing methods have two main limitations. First, they do not consider not indepen-dent and identically distributed data (Non-IID) among lo-cal clients, which is a key problem for FL and can cause a deterioration in accuracy [9, 15]. Second, some solu-tions [21] share the correlation matrix among local clients, which might cause information leakage.
This paper studies the FSSL with two widely used set-tings: (1) jointly training fully-labeled and fully-unlabeled clients; (2) jointly training partially-labeled clients. A straightforward solution is to extend existing FSSL meth-Figure 2. Comparisons of test accuracy curve of our RSCFed with
FedIRM [21] and Fed-Consist [28] under original setting (origi.) and under our weight adjusting setting (weight adju.). ods [21, 28] to the Non-IID setting. However, both
FedIRM [21] and Fed-Consist [28] fail to generalize to the
Non-IID setting. This is because FedIRM [21] proposed to share an inter-class correlation matrix among clients based on the assumption that each client has the same class rela-tionship. However, the class relationship can not be cor-rectly learned due to the heterogeneous data among lo-cal clients in the Non-IID setting, thus hurting the model performance. Fed-Consist [28] proposed to equally aver-age the model weights from labeled and unlabeled clients.
However, the performance significantly decreases when un-labeled clients increases since the global model may be dominated by the unlabeled clients. Adjusting aggregation weights for labeled and unlabeled clients is one solution, i.e., increasing the weights for labeled clients while decreas-ing the weights for unlabeled ones. Nevertheless, this result achieves limited performance; see (weight adju.) in Fig. 2.
To this end, we present Random Sampling Consensus
Federated learning, namely RSCFed, by considering the uneven reliability among models either from fully-labeled and fully-unlabeled clients or from several partially-labeled clients under the Non-IID setting without any informa-tion leakage among clients. For example, labeled clients are easily biased to local data, while unlabeled clients are difficult to achieve the high accuracy, leading to uneven model reliability among clients. On the other hand, train-ing with several partially-labeled clients may also cause un-even model reliability because images in each client are het-erogeneously distributed in quantity skew and label skew.
To achieve a robust global model, our key idea is to regard the local models as noisy models and distill several consen-sus models via random sampling before aggregating to the global model, as shown in Fig. 1. Specifically, in each syn-chronization round, we randomly sub-sample clients and record the averaged weights from the sub-sampled models as a sub-consensus model. By performing the operation multiple times, we update the global model via aggregat-ing multiple sub-consensus models. To distill a robust sub-consensus model from randomly sampled local clients, we introduce a distance-reweighted model aggregation (DMA) module, which dynamically increases the weights for mod-els that are close to the sub-consensus model and vice versa.
The idea shares a similar spirit with random sample con-sensus (RANSAC) [5], which identifies points as outliers if they are far from the model. We conduct extensive experi-ments on natural image classification datasets (e.g., SVHN and CIFAR-100) and medical dataset (i.e., ISIC 2018 Skin) to demonstrate the effectiveness of RSCFed. Overall, our main contributions can be summarized as follows:
• In this paper, we present a novel FSSL method, named
RSCFed, to address the uneven reliability of Non-IID local clients. Unlike existing FSSL frameworks that directly aggregate local clients, RSCFed proposes the concept of updating the global model via aggregating multiple sub-consensus models.
• To improve the sub-consensus model, we introduce a novel distance-reweighted model aggregation (DMA) module, which dynamically adjusts the weights of each sampled local client to the sub-consensus model.
• Experiments on three public datasets demonstrate that our RSCFed significantly outperforms the other state-of-the-art FSSL methods. We further show that with larger ratio of unlabeled data involved, the better im-provement RSCFed can achieve. 2.