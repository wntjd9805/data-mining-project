Abstract
Comprehending the rich semantics in an image and or-dering them in linguistic order are essential to compose a visually-grounded and linguistically coherent description for image captioning. Modern techniques commonly cap-italize on a pre-trained object detector/classiﬁer to mine the semantics in an image, while leaving the inherent lin-guistic ordering of semantics under-exploited. In this pa-per, we propose a new recipe of Transformer-style struc-ture, namely Comprehending and Ordering Semantics Net-works (COS-Net), that novelly uniﬁes an enriched seman-tic comprehending and a learnable semantic ordering pro-cesses into a single architecture. Technically, we initially utilize a cross-modal retrieval model to search the relevant sentences of each image, and all words in the searched sen-tences are taken as primary semantic cues. Next, a novel semantic comprehender is devised to ﬁlter out the irrele-vant semantic words in primary semantic cues, and mean-while infer the missing relevant semantic words visually grounded in the image. After that, we feed all the screened and enriched semantic words into a semantic ranker, which learns to allocate all semantic words in linguistic order as humans.
Such sequence of ordered semantic words are further integrated with visual tokens of images to trig-ger sentence generation. Empirical evidences show that
COS-Net clearly surpasses the state-of-the-art approaches on COCO and achieves to-date the best CIDEr score of 141.1% on Karpathy test split. Source code is available at https://github.com/YehLi/xmodaler/tree/ master/configs/image_caption/cosnet. 1.

Introduction
The ability to describe visual content with a descrip-tive utterance is a fundamental human capability that chil-dren are taught from childhood. To formalize such unique capability, the task of image captioning [7, 11, 21, 33] is developed to simulate the human-like interaction between vision and language. The ultimate target of this task is to produce a visually-grounded and linguistically coher-E n c o d e r
I m a g e
R e t r i e v a l
E n c o d e r
T e x t
C o m p r e h e n d e r
S e m a n t i c parking  man lot cow
.
.
. rides
R a n k e r
S e m a n t i c
S e n t e n c e s i
T r a n n g i man cars woman walking parking 
... lot parked horse (b) man->rides->cow->parking->lot (c)
Figure 1. Semantics produced by (a) pre-trained object detector, (b) cross-modal retrieval model (CLIP), and (c) our semantic com-prehender & ranker for image captioning. ent sentence, which covers most semantics in an image that are worthy of mention and meanwhile describes them in linguistic order. Modern image captioning techniques generally focus on the former aspect of enhancing vision-language alignment by ﬁrst capturing ﬁne-grained seman-tics (e.g., attributes [40, 41], objects [2, 14, 37], or scene graph [36,38,39]) via pre-trained image encoder (object de-tector/classiﬁer). Then, a series of innovations that employ visual attention over these ﬁne-grained semantics [6,10] are present to strengthen vision-language interaction. However, the capability of semantic comprehending in pre-trained de-tector/classiﬁer is severely limited by the pre-deﬁned se-mantic/class labels. the pre-trained detec-In addition, tor/classiﬁer is not optimized along with sentence decoding process, thereby hardly to be tuned for emphasizing visually salient semantics in output sentence. As shown in Figure 1 (a), the pre-trained object detector (Faster R-CNN) solely captures one major semantic word (“man”), while the other mined semantic words are either irrelevant (e.g., “horse”) or trivial (e.g., “sky” and “bushes”).
To enhance the scalability and generalization of image encoder, a recent pioneering practice [29] is to leverage
CLIP model (i.e., image encoder and text encoder [24]) that is trained on diverse and large-scale data. In this work, we regard CLIP model as a powerful cross-modal retrieval model that retrieves relevant sentences from the human-annotated sentence pool. Such way naturally accumulates more salient semantic words that tend to be mentioned in vi-sually similar images, while more irrelevant semantic words are also introduced (see Figure 1 (b)). To alleviate this issue, we uniquely design a semantic comprehender that further reﬁnes the primary semantic cues in the searched sentences based on visual content. By doing so, the semantic compre-         
hender (see Figure 1 (c)) not only ﬁlters out the irrelevant semantic words (e.g., “horse”), but also learns to infer the missing relevant semantic words (e.g., “cow” and “rides”), pursuing an enriched and accurate semantic understanding.
In pursuit of the linguistical coherence of the output sentence, the recent advances directly capitalize on the
RNN/Transformer based sentence decoder for language modeling. Unfortunately, such paradigm overly relies on the language priors, and sometimes leans to hallucinate semantic words that are not actually in an image, a phe-nomenon known as “object hallucination” [27]. Here we propose to mitigate the issue from the viewpoint of exploit-ing the inherent linguistic ordering of semantics as addi-tional supervisory signals to guide sentence decoding pro-cess. Technically, a semantic ranker (see Figure 1 (c)) is leveraged to rank all the reﬁned semantic words derived from semantic comprehender in linguistic order, yielding a sequence of ordered semantic words. This semantic word sequence manifests the emphasis of the relative linguistic position of each semantic word in a sequence. As such, the sequence acts as the inherent skeleton of the descriptive sen-tence, and thus can be exploited to encourage the generation of relevant words at each decoding timestep.
In this work, we design a novel Transformer-style encoder-decoder structure for image captioning, namely
Comprehending and Ordering Semantics Networks (COS-Net). Our launching point is to unify the above-mentioned two processes of semantic comprehending and ordering into a single scheme, so that both semantic comprehender and ranker can be jointly optimized to better suit the sentence decoding procedure. Speciﬁcally, we ﬁrst take the off-the-shelf CLIP as cross-modal retrieval model to retrieve se-mantically similar sentences for the input image. All se-mantic words in searched sentences are initially regarded as the primary semantic cues. Next, based on the output grid features of image encoder in CLIP, a visual encoder is utilized to contextually encode each grid feature into vi-sual token via self-attention. By taking the primary seman-tic cues and visual tokens as inputs, semantic comprehen-der ﬁlters out irrelevant semantic words in primary seman-tic cues and meanwhile reconstructs the missing relevant semantic words through cross-attention mechanism. After that, semantic ranker learns to allocate all the reﬁned se-mantic words in a linguistic order by upgrading each seman-tic word with the encoding of its estimated linguistic posi-tion. Finally, both the visual tokens and the ordered seman-tic words are dynamically integrated via attention to auto-regressively decode the output sentence word-by-word.
The main contribution of this work is the proposal of jointly comprehending and ordering the semantics in an im-age to boost image captioning. This also leads to the elegant views of how to nicely capture the richer relevant semantics that are worthy of mention from visual content, and how to explore the inherent linguistic ordering of them to fur-ther facilitate sentence generation. Extensive experiments on COCO demonstrate the effectiveness of our COS-Net. 2.