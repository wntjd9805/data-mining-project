Abstract
It is challenging to annotate large-scale datasets for su-pervised video shadow detection methods. Using a model trained on labeled images to the video frames directly may lead to high generalization error and temporal inconsis-In this paper, we address these challenges tent results. by proposing a Spatio-Temporal Interpolation Consistency
Training (STICT) framework to rationally feed the unla-beled video frames together with the labeled images into an image shadow detection network training. Specifically, we propose the Spatial and Temporal ICT, in which we de-fine two new interpolation schemes, i.e., the spatial interpo-lation and the temporal interpolation. We then derive the spatial and temporal interpolation consistency constraints accordingly for enhancing generalization in the pixel-wise classification task and for encouraging temporal consistent predictions, respectively. In addition, we design a Scale-Aware Network for multi-scale shadow knowledge learn-ing in images, and propose a scale-consistency constraint to minimize the discrepancy among the predictions at dif-ferent scales. Our proposed approach is extensively val-idated on the ViSha dataset and a self-annotated dataset.
Experimental results show that, even without video labels, our approach is better than most state of the art supervised, semi-supervised or unsupervised image/video shadow de-tection methods and other methods in related tasks. Code and dataset are available at https://github.com/ yihong-97/STICT.
*Corresponding author.
Figure 1. Shadow maps produced by our image shadow detection network SANet (a) trained on labeled images and (b) trained on both labeled images and unlabeled videos with STICT. 1.

Introduction
Shadow detection is an important problem for many com-puter vision and graphics tasks, and has drawn interest in a wide range of vision applications [4, 20, 40, 41], such as object recognition [13, 23â€“25], virtual reality scene gen-eration, light position estimation and object shape per-ception. Recently, shadow detection has achieved signif-icant progress [6, 7, 12, 19, 35, 43, 45] on image bench-mark datasets [33, 35, 44] due to the development of deep
Convolutional Neural Networks (CNNs), while lacking of large-scale annotated datasets is the main reason impending the applications of deep learning-based methods in video shadow detection (VSD).
How to rationally feed the unlabeled video samples into the network training, and transfer knowledge from labeled images to videos efficiently is critical for promoting the ca-pability of deep learning-based methods on unsupervised data. However, it is rare and challenging for existing semi-supervised methods to transfer the shadow patterns in im-ages (supervised) to videos (unsupervised) with end-to-end training, small generalization error, and temporal-consistent predictions meanwhile.
In this paper, we propose a Spatio-Temporal Interpo-lation Consistency Training (STICT) framework for the image-to-video shadow knowledge transfer task, in which the unlabeled video frames together with the labeled im-ages can be rationally fed into a Scale-Aware shadow detec-tion Network (SANet) for an end-to-end training. Accord-ingly, we propose a spatial interpolation consistency con-straint, a temporal interpolation consistency constraint, and a scale consistency constraint to guide the network training for improving generalization, producing temporal smooth and scale consistent results. As seen from Fig.1, the detec-tion results can be largely improved with the STICT.
To enhance the model generalization ability in our pixel-wise classification task, we propose the Spatial ICT inspired by the semi-supervised image classification method Inter-polation Consistency Training (ICT) [34]. ICT encourages the prediction at a random interpolation of two unlabeled images to be consistent with the interpolation of the predic-tions at those two images. As proved in [34], the samples lying near the class boundary are beneficial to enforce the decision boundary to traverse the low-density distribution regions for better generalization. Unlike the random inter-polation between images in RGB space that used in [34], we propose the spatial interpolation that is the interpolation of two uncorrelated pixels in the feature space. Our spatial interpolation is motivated by the intuitions 1) the interpola-tions of uncorrelated samples are more likely to locate near the class boundary to smooth the decision boundaries; 2) the interpolations of semantic pixels are more meaningful for pixel-wise classification task. Then, we derive a spatial in-terpolation consistency constraint accordingly to guide the network training for generalization improvement.
To encourage the temporal consistent predictions, we propose the Temporal ICT to track the prediction of the same pixel among sequential frames, in which we propose to use the temporal interpolation between two consecutive frames along the time-axis via optical flow. Then, we de-rive a temporal interpolation consistency constraint to guide the network training for producing temporal smooth results.
Comparing with other methods that use multi-frame fea-tures or correlations among frames for temporal consis-tency, our method guides the network training by this ex-tra constraint, and processes each frame independently for inference without introducing computation overhead. We would highlight that the spatial and temporal interpolations are conducted during training process, which makes our framework quite simple for inference.
Considering that the shadows in videos usually exhibit large changes in scale, we design a Scale-Aware Network (SANet) as the single-frame network for image shadow knowledge learning in the STICT framework. Unlike the traditional encoder-decoder network for shadow feature learning, SANet is designed as a encoder-decoder-refiner structure with a feature fusion module and a detail atten-tive module, to learn image shadow knowledge at different scales. We also propose a scale-consistency constraint ac-cordingly to minimize the discrepancy among the predic-tions at different scales.
We summarize our contributions as following: (1) We propose a STICT framework for image-to-video shadow detection task, which is rarely considered in the ex-isting semi-supervised methods. All the labeled images and unlabeled video frames can be rationally fed into an image shadow detection network for an end-to-end training, which guarantees a compact and real-time model for inference. (2) We propose the Spatial and Temporal ICT, in which we define two new interpolation schemes, the spatial inter-polation and the temporal interpolation, for better general-ization in the pixel-wise classification task and for tempo-ral consistency, respectively. We design the SANet as the single-frame network in STICT for multi-scale shadow fea-ture learning, and propose a scale consistency constraint ac-cordingly for obtaining accurate shadow maps. (3) We annotate a challenging dataset for VSD task. Ex-perimental results on ViSha and our self-annotated dataset show that our approach performs better than most of the ex-isting SOTA supervised/semi-supervised/unsupervised im-age and video methods. 2.