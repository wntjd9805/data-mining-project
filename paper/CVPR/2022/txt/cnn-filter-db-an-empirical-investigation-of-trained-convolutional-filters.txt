Abstract
Currently, many theoretical as well as practically rele-vant questions towards the transferability and robustness of
Convolutional Neural Networks (CNNs) remain unsolved.
While ongoing research efforts are engaging these problems from various angles, in most computer vision related cases these approaches can be generalized to investigations of the effects of distribution shifts in image data.
In this context, we propose to study the shifts in the learned weights of trained CNN models. Here we focus on the prop-erties of the distributions of dominantly used 3 × 3 convo-lution filter kernels. We collected and publicly provide a dataset with over 1.4 billion filters from hundreds of trained
CNNs, using a wide range of datasets, architectures, and vi-sion tasks. In a first use case of the proposed dataset, we can show highly relevant properties of many publicly available pre-trained models for practical applications: I) We ana-lyze distribution shifts (or the lack thereof) between trained filters along different axes of meta-parameters, like visual category of the dataset, task, architecture, or layer depth.
Based on these results, we conclude that model pre-training can succeed on arbitrary datasets if they meet size and vari-ance conditions. II) We show that many pre-trained models contain degenerated filters which make them less robust and less suitable for fine-tuning on target applications.
Data & Project website: https://github.com/ paulgavrikov/cnn-filter-db 1.

Introduction
Despite their overwhelming success in the application to various vision tasks, the practical deployment of convolu-tional neural networks (CNNs) is still suffering from sev-eral inherent drawbacks. Two prominent examples are I) the dependence on very large amounts of annotated train-ing data [1], which is not available for all target domains
*Funded by the Ministry for Science, Research and Arts, Baden-Wuerttemberg, Grant 32-7545.20/45/1 (Q-AMeLiA).
The authors also thank Margret Keuper for her support and encouragement to submit this work.
Figure 1. First 3 × 3 filters extracted of each convolution layer in a ResNet-18 trained on CIFAR-10. The filters show a clear loss of diversity and increasing sparsity with depth. The colormap range is determined layer-wise by the absolute peak weight of all filters in that layer. and is expensive to generate; and II) still widely unsolved problems with the robustness and generalization abilities of
CNNs [2] towards shifts of the input data distributions. One can argue that both problems are strongly related, since a common practical solution to I) is the fine-tuning [3] of pre-trained models by small datasets from the actual tar-get domain. This results in the challenge to find suitable pre-trained models based on data distributions that are “as close as possible” to the target distributions. Hence, both cases (I+II) imply the need to model and observe distribu-tion shifts in the contexts of CNNs.
In this paper, we propose not to investigate these shifts in the input (image) domain, but rather in the 2D filter-kernel distributions of the CNNs themselves. We argue that e.g. the distributions of trained convolutional filters in a CNN, which implicitly reflect the sub-distributions of the input image data, are more suitable and easier accessible repre-sentations for this task. In order to foster systematic inves-tigations of learned filters, we collected and publicly pro-vide a dataset of over 1.4 billion filters with meta data from hundreds of trained CNNs, using a wide range of data sets, architectures, and vision tasks. To show the scientific value
of this new data source, we conduct a first analysis and re-port a series of novel insights into widely used CNN mod-els. Based on our presented methods we show that many publicly provided models suffer from degeneration. We show that overparameterization leads to sparse and/or non-diverse filters (Fig. 1), while robust training increases fil-ter diversity, and reduces sparsity. Our results also show that learned filters do not significantly differ across models trained for various tasks, except for extreme outliers such as
GAN-Discriminators. Models trained on datasets of differ-ent visual categories do not significantly drift either. Most shifts in studied models are due to degeneration, rather than an actual difference in structure. Therefore, our results im-ply that pre-training can be performed independent of the actual target data, and only the amount of training data and its diversity matters. This is inline with recent findings that models can be pre-trained even with images of fractals [4].
For classification models we show that the most variance in learned filters is found in the beginning and end of the model, while object/face detection models only show sig-nificant variance in early layers. Also, the most specialized filters are found in the last layers. We summarize our key contributions as follows:
• Publication of a diverse database of over 1.4B 3 × 3 con-volution filters alongside with relevant meta information of the extracted filters and models [5].
• Presentation of a data-agnostic method based on sparsity and entropy of filters to find “degenerated” convolution layers due to overparameterization or non-convergence of trained CNN models.
• Showing that publicly available models often contain de-generated layers and can therefore be questionable candi-dates for transfer tasks.
• Analysis of distribution shifts in filters over various groups, providing insights that formed filters are fairly similar across a wide-range of examined groups.
• Showing that the model-to-model shifts that exist in clas-sification models are, contrary to the predominant opin-ion, not only seen in deeper layers but also in the first layers.
Paper organization. We give an overview of our dataset and its collection process in Sec. 3, followed by an introduc-tion of methods studying filter structure, distributions shifts, and layer degeneration such as randomness, low variance in filter structure, and high sparsity of filters. Then in Sec. 4 we apply these methods to our collected data. We show the impact of overparameterization and robust training on filter degeneration and provide intuitions for threshold find-ing. Then we analyze filter structures by determining a suit-able filter basis and looking into reproducibility of filters in training, filter formation during training, and an analysis of distribution shifts for various dimensions of the collected meta-data. We discuss limitations of our approach in Sec. 5 and, finally, draw conclusions in Sec. 6. 2.