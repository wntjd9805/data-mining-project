Abstract
Action recognition models have shown a promising ca-pability to classify human actions in short video clips. In a real scenario, multiple correlated human actions commonly occur in particular orders, forming semantically meaning-ful human activities. Conventional action recognition ap-proaches focus on analyzing single actions. However, they fail to fully reason about the contextual relations between adjacent actions, which provide potential temporal logic for understanding long videos. In this paper, we propose a prompt-based framework, Bridge-Prompt (Br-Prompt), to model the semantics across adjacent actions, so that it si-multaneously exploits both out-of-context and contextual information from a series of ordinal actions in instruc-tional videos. More specifically, we reformulate the indi-vidual action labels as integrated text prompts for super-vision, which bridge the gap between individual action se-mantics. The generated text prompts are paired with cor-responding video clips, and together co-train the text en-coder and the video encoder via a contrastive approach.
The learned vision encoder has a stronger capability for ordinal-action-related downstream tasks, e.g. action seg-mentation and human activity recognition. We evaluate the performances of our approach on several video datasets:
Georgia Tech Egocentric Activities (GTEA), 50Salads, and the Breakfast dataset. Br-Prompt achieves state-of-the-art on multiple benchmarks. Code is available at: https:
//github.com/ttlmh/Bridge-Prompt. 1.

Introduction
Recent years have witnessed the flourish of video anal-ysis. Understanding human actions is the key to analyz-ing massive amounts of video data, which is conducive to a wide range of applications including video retrieval [8], video captioning [28] and video summarization [2]. Among the many sub-topics in action analysis, action recognition is
†Corresponding author.
Figure 1. Comparisons of conventional representations and
Bridge-Prompt representations for ordinal actions. The human ac-tivity of making cheese sandwich contains four actions. Suppose the final action putting the bread on cheese and bread is unseen in training set. Conventional approaches in (a) and (b) are unable to depict the intra-semantics and inter-relations of all four actions, while our Bridge-Prompt representations in (c) is able to capture the full semantic information. a basic and core issue, which has made remarkable progress under various well-designed models [3, 5, 11].
Meanwhile, the current research trend of video analy-sis is experiencing a transition from understanding single-semantics short video clips to longer and more complex videos [38]. The increased attention on instructional video analysis has shown the significance of understanding se-mantically rich video contents [29, 38, 46]. From the per-spective of action analysis, conventional action recognition approaches focus on classifying the single action being per-formed in a short video clip [5, 36].
In contrast, instruc-tional video analysis methods need to study a series of ac-tions being performed in longer time duration. In order to analyze instructional videos, we do not only need to un-derstand the semantics of individual actions, but are also
required to learn the semantic relations between contextual actions. Recently, some works have studied the mutual in-formation between correlated actions in instructional videos using graph-based models [15, 30, 43]. The common ap-proach is to regard each kind of action as a single node on a graph, where the edges between the nodes represent the contextual relations between adjacent actions.
However, the graph-based approaches are transductive, which are limited by the prior knowledge of input nodes and/or edges. Therefore, graph-based approaches are un-able to address unknown types of nodes and thus are hard to extend and transfer. Moreover, under the existing frame-work of action recognition, the current way of depicting hu-man actions is to allocate individual annotations to every single action, where different actions are treated as sepa-rate class IDs. This is practicable for recognizing separate actions, yet it is unable to depict the contextual relations between ordinal actions since individual class IDs cannot provide contextual information. The example of (a) and (b) in Figure 1 further illustrates the limitations of conventional class-ID-based approaches.
In this paper, we discover that human language is a pow-erful tool to depict the ordinal semantics between correlated actions. Human language is able to describe multiple se-quentially occurred events based on ordinal numerals and specific sentence patterns. For example, ordinal relations between taking bottle and pouring water can be described in:“the person firstly takes (the) bottle, and then pours wa-ter (into it)”. The language naturally bridges the semantics between ordinal actions. In certain circumstances, even the textual descriptions of actions themselves can provide con-textual information. For example, the ordinal relationship between actions of taking bread, putting cheese on bread and putting bread on cheese and bread is easy to be deduced literally. Moreover, language can intuitively extrapolate to unknown types of action. Given a new expression putting bread on bread, its semantics can be inferred from the ex-pressions of known types of action. Figure 1(c) illustrates the effectiveness of language representations.
To this end, we propose a text-based learning method,
Bridge-Prompt, for instructional video analysis. Motivated by the recent advances of prompt-based learning approach in Natural Language Processing (NLP) [25] and visual recognition [31], we introduce a three-plus-one-level design of text prompts to analyze the video clips containing a series of ordinal actions. Figure 1 shows the comparisons between conventional and Bridge-Prompt representations of ordi-nal actions. More specifically, we develop a prompt-based learning framework to jointly co-train the video and text encoders based on a specially designed video-text fusion module, so that we simultaneously exploit out-of-context and contextual action information towards a more compre-hensive understanding of instructional videos. Our work digs deeper into the further potential of prompt-based learn-ing approaches towards ordinal action understanding and instructional video analysis. Extensive experimental re-sults on three benchmark datasets illustrate that the Bridge-Prompt-based approaches have achieved promising perfor-mances, and reach state-of-the-art on several benchmarks with the help of the prompt-based learning framework. 2.