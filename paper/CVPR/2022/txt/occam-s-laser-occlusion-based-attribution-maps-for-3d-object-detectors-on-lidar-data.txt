Abstract
While 3D object detection in LiDAR point clouds is well-established in academia and industry, the explainability of these models is a largely unexplored ﬁeld.
In this paper, we propose a method to generate attribution maps for the detected objects in order to better understand the behav-ior of such models. These maps indicate the importance of each 3D point in predicting the speciﬁc objects. Our method works with black-box models: We do not require any prior knowledge of the architecture nor access to the model’s internals, like parameters, activations or gradients.
Our efﬁcient perturbation-based approach empirically es-timates the importance of each point by testing the model with randomly generated subsets of the input point cloud.
Our sub-sampling strategy takes into account the special characteristics of LiDAR data, such as the depth-dependent point density. We show a detailed evaluation of the attribu-tion maps and demonstrate that they are interpretable and highly informative. Furthermore, we compare the attribu-tion maps of recent 3D object detection architectures to pro-vide insights into their decision-making processes. 1.

Introduction
Driven by the race to autonomous driving, the interest in 3D object detection is increasing in both, industry and the research community. Especially the detection of objects in point clouds acquired with LiDAR (Light Detection and
Ranging) sensors is of great interest. On the one hand, this can be seen by the number and volume of published datasets that include LiDAR data, e.g. [3, 4, 8, 12, 42] and, on the other hand, by the number of recent publications that deal with this topic, e.g. [11, 14, 18, 33, 35, 36, 46, 49–54, 60]. In contrast, the explainability of these detectors based on deep neural networks is a rather unexplored area. For a safety-relevant area such as autonomous driving, however, it is of particular importance to make the decisions of these inher-Figure 1. Our attribution maps show the importance of individual
LiDAR points for the black-box detection model’s decisions. ently opaque models more transparent.
An established technique for the analysis of models based on deep neural networks operating on image data is the generation of attribution or saliency maps, e.g. [30, 39, 41,43,55,55]. The goal is to visualize the importance of in-put pixels on the model’s decision-making process. These have been proven useful for interpreting detections or ana-lyzing the learned features. However, for models operating on point clouds there are only a few approaches that address the generation of point-level attribution maps, e.g. [10, 58].
These focus exclusively on the analysis of point cloud clas-siﬁcation models. The analyzed models operate on artiﬁ-cially generated point clouds derived from CAD models, such as in the ModelNet dataset [48]. Moreover, these attri-bution map approaches assume prior knowledge of the ar-chitecture and require access to the gradients of the model.
Thus, they are only applicable for the analysis of white-box classiﬁcation models. These properties prevent using them for the analysis of 3D object detectors on LiDAR point clouds. First, for 3D object detection, not only the classi-ﬁcation of the object is important, but also its localization.
These tasks usually go hand in hand and cannot be analyzed separately. Second, the detectors typically consist of com-plex architectures in which different parts contribute to the
ﬁnal detection, e.g. [33, 36], making gradient-based meth-ods difﬁcult or infeasible. In addition, a non-differentiable pre-processing step used in some models, e.g. [14, 50], pre-vents the backpropagation to the input points.
To the best of our knowledge, we present the ﬁrst method to generate point-level attribution maps for 3D object de-tectors on LiDAR point clouds. For each detected object a separate map is created, as shown in Figure 1, which re-ﬂects the inﬂuence of each 3D point on the classiﬁcation and localization of the object. Our method is applicable to black-box models and thus, in principle, to any detector, since neither prior knowledge of the model’s architecture nor access to its internals is required.
Inspired by perturbation approaches to generate saliency maps for image-based black-box models [23, 24, 55], we leverage the principle of analysis by occlusion. We propose
OccAM: Occlusion-based Attribution Maps for 3D object detectors on LiDAR data. We estimate the importance of points by testing the model with randomly generated sub-sets of the input point cloud. The underlying assumption is that an object will be detected less accurately or not at all if areas of the input that are important for the detection have been removed or perturbed. However, the special character-istics of point clouds, like the unstructured nature and the depth-dependent point density in LiDAR data, poses new challenges compared to the analysis of image-based mod-els. Thus, we propose a voxel-based sub-sampling strategy in which the sampling probability is adapted to the point density to challenge the detector appropriately. To evalu-ate the inﬂuence on the detections as precisely as possible, we further use a similarity metric that is optimized for the properties of 3D bounding boxes.
In a detailed analysis for PointPillars [14], we demon-strate that our attribution maps are interpretable and in-formative, e.g. allowing us to analyze the potential source of false detections. We show that by averaging multiple maps of a class, regions of particular importance can be derived. By comparing the average attribution maps of different state-of-the-art 3D object detectors, i.e. PointPil-lars, SECOND [49] and PV-RCNN [33], we further analyze whether differences can be detected. 2.