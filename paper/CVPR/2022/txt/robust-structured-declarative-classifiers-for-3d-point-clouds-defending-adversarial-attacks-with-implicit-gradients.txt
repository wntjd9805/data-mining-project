Abstract
Deep neural networks for 3D point cloud classiﬁcation, such as PointNet, have been demonstrated to be vulnera-ble to adversarial attacks. Current adversarial defenders often learn to denoise the (attacked) point clouds by recon-struction, and then feed them to the classiﬁers as input. In contrast to the literature, we propose a family of robust structured declarative classiﬁers for point cloud classiﬁ-cation, where the internal constrained optimization mech-anism can effectively defend adversarial attacks through implicit gradients. Such classiﬁers can be formulated using a bilevel optimization framework. We further propose an effective and efﬁcient instantiation of our approach, namely,
Lattice Point Classiﬁer (LPC), based on structured sparse coding in the permutohedral lattice and 2D convolutional neural networks (CNNs) that is end-to-end trainable. We demonstrate state-of-the-art robust point cloud classiﬁca-tion performance on ModelNet40 and ScanNet under seven different attackers. For instance, we achieve 89.51% and 83.16% test accuracy on each dataset under the recent
JGBA attacker that outperforms DUP-Net and IF-Defense with PointNet by ∼70%. The demo code is available at https://zhang-vislab.github.io. 1.

Introduction
Point clouds are unstructured data which is widely used in real-world applications such as autonomous driving. To recognize them using deep neural networks, point clouds can be represented as points [36], images [30], voxels [61], or graphs [56]. Recent works [74, 63, 31, 14, 62, 58, 28] have demonstrated that such deep networks are vulnerable to (gradient-based) adversarial attacks. Accordingly, several adversarial defenders [75, 60, 29] have been proposed for robust point cloud classiﬁcation. The basic ideas are often to
*Joint ﬁrst author
Figure 1: Illustration of robust structured declarative classi-ﬁers, where our defender is optimized in a structural space. denoise the (attacked) point clouds before feeding them into the classiﬁers as input to preserve their prediction accuracy.
Obfuscated gradients. In the white-box adversarial attacks, the attackers are assumed to have full access to both classi-ﬁers and defenders. To defend such attacks, one common way is to break the gradient over the input data in the back-propagation (either inadvertently or intentionally, e.g., a de-fender is non-differentiable or prevents gradient signal from
ﬂowing through the network) so that the attackers fail to be optimized. Such scenarios are called obfuscated gradients.
In [4] Athalye et al. have discussed the false sense of se-curity in such defenders and proposed new methods, such as Backward Pass Differentiable Approximation (BPDA), to attack them successfully. Take DUP-Net [75] for exam-ple, where a non-differentiable Statistical Outlier Removal (SOR) defense strategy was proposed. In [31] Ma et al. pro-posed Joint Gradient Based Attack (JGBA) that can compute the gradient with a linear approximation (an instantiation of
BPDA) of the SOR defense to attack DUP-Net successfully.
Implicit gradients. Now let us consider the scenarios where both defenders and classiﬁers are differentiable. Then in order to defend the adversarial attacks, one way is to make the calculation of the gradient challenging. To this end, implicit gradients [40] may be more suitable for designing the defenders. An implicit gradient, ∂y
∂x , is deﬁned by a differentiable function h that takes x, y as its input, i.e.,
∂y
∂x = h(x, y). Such an equation can be also considered as a ﬁrst-order ordinary differential equation (ODE), which is
solvable (approximately) using Euler’s Method [19]. Here we assume that the gradients through the classiﬁers can be easily computed, which often holds empirically. To our best knowledge, so far there is no work on designing adversarial defenders for 3D point clouds based on implicit gradients.
Declarative networks. Implicit gradients require equations that contain both the input and output of a defender. One po-tential solution for this is to introduce optimization problems as the defenders, where the ﬁrst-order optimality conditions provide such equations. In the literature, there have been some works [3, 2, 25, 40] that proposed optimization as net-work layers in deep neural networks. Recently in [12] Gould et al. generalized these ideas and proposed deep declarative networks. A declarative network node is introduced where the exact implementation of the forward processing function is not deﬁned; rather the input-output relationship (x (cid:55)→ ˜x) is deﬁned in terms of behavior speciﬁed as the solution to an optimization problem ˜x ∈ arg minz∈Z f (x, z; θ). Here f is an objective function, θ denotes the node parameters, and Z is the feasible solution space. In [12] a robust pooling layer was proposed as a declarative node using unconstrained minimization with various penalty functions such as Huber or Welsch, which can be efﬁciently solved using Newton’s method or gradient descent. The effectiveness of such pool-ing layers was demonstrated for point cloud classiﬁcation.
Our approach. Motivated by the methods above, in this paper we propose a novel robust structured declarative clas-siﬁers for 3D point clouds by embedding a declarative node into the networks, as illustrated by Fig. 1. Different from robust pooling layers in [12], our declarative defender is designed to reconstruct each point cloud in a (learnable) structural space as a means of denoising. To this end, we borrow the idea from structured sparse coding [49, 20, 57] by representing each point as a linear combination of atoms in a dictionary. Together with the backbone networks, the training of our robust classiﬁers can lead to a bilevel op-timization problem. Considering the inference efﬁciency, one plausible instantiation of our classiﬁers, as illustrated in Fig. 2, is to deﬁne the structural space using the permu-tohedral lattice [1, 47, 13], project each point cloud onto the lattice, generate a 2D image based on the barycentric weights, and feed the image to a 2D convolutional neural network (CNN) for classiﬁcation. We call this instantiation
Lattice Point Classiﬁer (LPC).
Our contributions. We summarize our contributions below:
• We propose a family of novel robust structured declarative classiﬁers for 3D point clouds where the declarative nodes defend the adversarial attacks through implicit gradients.
To the best of our knowledge, we are the ﬁrst to explore implicit gradients in robust point cloud classiﬁcation.
• We propose a bilevel optimization framework to learn the network parameters in an end-to-end fashion.
Figure 2: Illustration of our Lattice Point Classiﬁer (LPC).
• We propose an effective and efﬁcient instantiation of our robust classiﬁers based on the structured sparse coding in the permutohedral lattice and 2D CNNs.
• We demonstrate superior performance of our approach by comparing with the state-of-the-art adversarial defenders under the state-of-the-art adversarial attackers. 2.