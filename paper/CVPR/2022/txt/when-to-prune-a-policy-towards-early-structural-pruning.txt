Abstract
Pruning enables appealing reductions in network mem-ory footprint and time complexity. Conventional post-training pruning techniques lean towards efficient inference while overlooking the heavy computation for training. Re-cent exploration of pre-training pruning at initialization hints on training cost reduction via pruning, but suffers noticeable performance degradation. We attempt to com-bine the benefits of both directions and propose a policy that prunes as early as possible during training without hurting performance. Instead of pruning at initialization, our method exploits initial dense training for few epochs to quickly guide the architecture, while constantly evaluat-ing dominant sub-networks via neuron importance ranking.
This unveils dominant sub-networks whose structures turn stable, allowing conventional pruning to be pushed earlier into the training. To do this early, we further introduce an
Early Pruning Indicator (EPI) that relies on sub-network architectural similarity and quickly triggers pruning when the sub-network’s architecture stabilizes. Through exten-sive experiments on ImageNet, we show that EPI empow-ers a quick tracking of early training epochs suitable for pruning, offering same efficacy as an otherwise “oracle” grid-search that scans through epochs and requires orders of magnitude more compute. Our method yields 1.4% top-1 accuracy boost over state-of-the-art pruning counterparts, cuts down training cost on GPU by 2.4×, hence offers a new efficiency-accuracy boundary for network pruning dur-ing training. 1.

Introduction
The success of convolutional neural networks (CNNs) fuels the recent progress in computer vision, boosting up performance for classification, detection, and segmentation tasks [16, 35, 37]. While enjoying the accuracy benefits
CNNs bring, a simultaneous increase in network complex-ity imposes higher memory footprint and computing power consumption, making deployment of CNNs on resource-constrained devices a challenging task [7, 29, 30]. In lieu of
Figure 1. Pruning paradigm overview. Train-Prune-Finetune prunes after training, effective but costs additional training time;
Prune-at-Initialization prunes right before training towards a smaller network, cuts down on training time but suffers notable performance degradation; Pruning-aware-Training (ours) prunes during training aiming at benefits from two worlds. It governs on post-pruning performance while aiming to minimize training time, via a new policy around Early Pruning Indicator (EPI) that signals an early optimal point to start pruning during training. computation-intensive networks, recent work turn to com-pression techniques for efficient models leveraging prun-ing [3, 15, 25, 29], quantization [5, 43, 46], knowledge dis-tillation [19, 31, 45], neural architecture search [38, 41, 44], and architecture redesigns [21, 28, 39]. Among these, prun-ing demonstrates to be a widely adopted method that com-presses pre-trained models before deployment. The primary goal of pruning aims to remove insignificant network pa-rameters without impacting accuracy. In particular, struc-tural pruning removes entire filters (or neurons) as such the resulting structural sparsity benefits legacy off-the-shelf platforms, e.g., CPUs, DSPs, and GPUs.
In general, network pruning involves three key steps: (i) original training of a dense model for high accuracy, (ii) pruning away insignificant weights to remove redundancy, and finally (iii) fine-tuning the pruned model to recover per-formance [15, 26, 30]. Despite remarkable compactness de-livered by the last two steps, the original training of an over-parameterized network remains mostly untouched. Such approaches require a twice long time (resource) as an origi-nal training recipe given similar required computes for fine-tuning, making the entire pipeline slow, sometimes infeasi-ble. For example, a very recent breakthrough in language modeling, a GPT-3 model [4], requires millions of dollars (more than 300 NVIDIA V100 GPU years) just for the ini-tial training. Already aware of post-training redundancy, an interesting question arises - can we somehow prune a net-work during its initial training, as such the resulting sparsity can (i) immediately benefit training and (ii) save us from the costly additional fine-tuning upon training ends?
One intuitive and ideal solution to this problem is prun-ing the network right at initialization even before training starts. The intriguing observation from the Lottery Ticket
Hypothesis [9] hints potential to this task: it shows the (i) existence of small sub-models, identifiable via prun-ing, within a large dense model, that can (ii) be trained in isolation to achieve the same accuracy as its dense coun-terpart [9, 25]. This field has quickly evolved and recent approaches have enhanced policy for optimal sub-network at the initialization by preserving the loss or the gradient flow [8, 24, 42]. Despite rapid progress, the approaches of sub-network identification at the initialization remain chal-lenging and still suffer noticeable accuracy loss [10, 12].
Instead of zero training, in this work, we showcase the benefits and practicability of pruning early during training.
Doing so allows one to (i) save compute by training only pruned models most of the time, (ii) alleviate any extra fine-tuning by aligning the process with original training, while (iii) suppressing accuracy loss by moving slightly later into the training regime for pruning guidance. We name this ap-proach pruning-aware-training (PaT). As shown in Fig. 1, unlike pruning-at-initialization, PaT takes full advantage of early-stage dense model training that is beneficial for rapid learning and optimal architecture exploration [1, 11], while aiming to identify the best sub-network as early as possi-ble, rather than waiting till training ends as in conventional pruning.
The key of benefiting from the training efficiency of PaT and saving training time relies on finding an early yet eli-gible point during training to start pruning. Existing meth-ods that perform pruning during training [3, 10, 27, 33] have shown the efficacy of this direction by reducing the turn-around time. However, in most cases a fixed initial interval for pruning is set heuristically, or post-training statistics are required. In this work we focus on understanding how the starting point of pruning can be set automatically.
We start by analyzing in depth the evolution of pruned architectures via performing trimming across all epochs rig-orously and compare their suitability for pruning. Though laborious, this oracle estimate offers key insights on pruning during training. We observe an important property: agnos-tic of magnitude or gradient criterion, (i) pruning at early epochs results in different final architectures, but (ii) domi-nant architecture emerges within just a few epochs and sta-bilizes thereafter till training ends, allowing conventional pruning to be pushed earlier into the training.
Amid such property we further propose a novel met-ric, called Early Pruning Indicator (EPI), that estimates the structure similarity between networks resulted in pruning at consecutive epochs of the same base model. Given intrinsic access to model weights and gradients during training, EPI can be calculated very efficiently alongside initial training without bells and whistles, while helping avoid the other-wise lengthy grid search for starting epochs. Once the re-sulting pruning structure will not vary between epochs we argue and demonstrate it is safe to prune. As prior work [25] and we observe, structural pruning acts as an architecture search and tries to find the optimal number of neurons per layer. Therefore, we hypothesize that pruning can be per-formed as soon as the architecture of the dominant sub-network becomes stable. We demonstrate that the proposed metric works across varying network architectures, pruning ratios, delivering consistent reductions in training time.
Our main contributions are as follows:
• We propose a novel metric called Early Pruning Indi-cator (EPI) that indicates an early point to start pruning during training. Our metric enables training to benefit from sparsity, significantly reducing training resources with minor accuracy drop.
• We demonstrate that for structural pruning (output channel pruning), initial dense training fuels accuracy boosts. Augmented by EPI, our pruning-aware-training outperforms pruning-at-initialization alternatives by a large margin.
• We show that EPI is agnostic to the pruning method used by showing efficacy for both magnitude-based and gradient-based pruning, enabling a new state-of-the-art boundary for training speedup through in-situ pruning. 2.