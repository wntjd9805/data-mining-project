Abstract
Temporal modeling is crucial for video super-resolution.
Most of the video super-resolution methods adopt the op-tical ﬂow or deformable convolution for explicitly motion compensation. However, such temporal modeling techniques increase the model complexity and might fail in case of oc-clusion or complex motion, resulting in serious distortion and artifacts. In this paper, we propose to explore the role of explicit temporal difference modeling in both LR and HR space. Instead of directly feeding consecutive frames into a VSR model, we propose to compute the temporal differ-ence between frames and divide those pixels into two subsets according to the level of difference. They are separately processed with two branches of different receptive ﬁelds in order to better extract complementary information. To fur-ther enhance the super-resolution result, not only spatial residual features are extracted, but the difference between consecutive frames in high-frequency domain is also com-puted. It allows the model to exploit intermediate SR results in both future and past to reﬁne the current SR output. The difference at different time steps could be cached such that in-formation from further distance in time could be propagated to the current frame for reﬁnement. Experiments on sev-eral video super-resolution benchmark datasets demonstrate the effectiveness of the proposed method and its favorable performance against state-of-the-art methods. 1.

Introduction
Super-resolution (SR) is an important vision task that aims at recovering high-resolution (HR) images from low-resolution (LR) observations. Single image super-resolution (SISR) [2, 7, 10, 18, 19, 21, 31, 41, 46] methods reply much on image prior learned from large datasets or self-similarity within images to synthesize high frequency contents, while
*Corresponding author
Figure 1. VSR performance comparison on Vid4 [27] in terms of
PSNR (dB) and runtime (ms). Our proposed ETDM outperforms the previous methods with high efﬁciency. video super-resolution (VSR) methods [1, 15, 16, 22, 25, 28, 37, 43] are expected to extract valuable complementary de-tails from neighbouring frames, which could provide more information to alleviate the ill-posed problem. Both of these tasks have achieved remarkable progress thanks to the de-velopment of deep learning techniques. As more and more videos are recorded, VSR has become a key component in many applications such as video remastering, live streaming and surveillance.
To effectively explore abundant spatial-temporal informa-tion within frames, some methods [4, 23, 36, 37, 40, 42, 45] at-tempt to model temporal information across frames through explicit or implicit motion compensation. However, explicit motion compensation [40, 42] would increase the model complexity, and inevitable errors in motion estimation may cause distortion and degrade super-resolution results. The methods with implicit motion compensation, e.g., 3D con-volutional layer [17, 29], bet all on model capacity, ignoring
the valuable temporal priors. Another kind of method ex-plores temporal information following a recurrent fashion in either uni-direction or bi-direction. They can accumu-late rich history information in the hidden state, either only from past [9, 13, 15, 28, 34], or from both future and past to extract beneﬁcial complementary information for detail recovery [3, 5, 43]. However, they suffer from either unbal-anced historical accumulation at each frame or large memory caching.
Although many techniques have been proposed to extract complementary information, the differences among frames and SR results of different time steps have not been explicitly explored yet. Very recently, the idea of explicit temporal dif-ference modeling was explored and was successfully applied to video-related tasks either to improve its performance or efﬁciency. In [38, 39], authors proposed to exploit the RGB difference between frames as an efﬁcient alternative to opti-cal ﬂow to model motion. The temporal difference network is able to effectively capture both short-term and long-term information, which is essential in the action recognition task.
In this work, we explore the role of explicit temporal difference modeling in both LR and HR space. The VSR is conducted in a uni-directional recurrent way for efﬁciency and avoiding large memory caching of bi-direction. Instead of directly feeding consecutive frames into a VSR model, we propose to compute the temporal difference between the reference frame and the neighboring frame. The neighboring frames are divided into two subsets according to the level of difference, with smaller ones as low variance regions and larger ones as high variance regions. They are separately fed together with the reference frame into two branches with different receptive ﬁelds. The outputs of these two branches are combined and fed to the Spatial-Residual Head to reconstruct the initial SR results. Additionally, the Future-Residual Head and Past-Residual Head are respectively used to model temporal difference in HR space based on the initial spatial residual features at future and past time steps. In this way, the result at the current step would be further enhanced in HR space by allowing the model to look back and forth at intermediate estimation in both future and past. In addition, the temporal difference between spatial residual features at different time steps would be cached. Therefore, informa-tion from further time steps could all be propagated to the current time step for comprehensive reﬁnement. Compared to the bi-directional based methods, the proposed method could enjoy both the efﬁciency of uni-directional network and the power of bi-directional information propagation, but with a ﬂexible cache. The proposed method achieves favor-able performance against state-of-the-art methods on several benchmark datasets. Several ablation studies are conducted to examine the effectiveness of its components.
Our main contributions are as follows: (1) A new frame-work to explicitly explore the temporal difference in both LR and HR for the VSR task; (2) A novel back-and-forth reﬁne-ment strategy to boost performance; (3) Favorable perfor-mance against state-of-the-arts on several VSR benchmarks. 2.