Abstract
Monocular 3D object detection is an essential task in autonomous driving. However, most current methods con-sider each 3D object in the scene as an independent training sample, while ignoring their inherent geometric relations, thus inevitably resulting in a lack of leveraging spatial con-straints. In this paper, we propose a novel method that takes all the objects into consideration and explores their mutual relationships to help better estimate the 3D boxes. More-over, since 2D detection is more reliable currently, we also investigate how to use the detected 2D boxes as guidance to globally constrain the optimization of the corresponding predicted 3D boxes. To this end, a differentiable loss func-tion, termed as Homography Loss, is proposed to achieve the goal, which exploits both 2D and 3D information, aim-ing at balancing the positional relationships between differ-ent objects by global constraints, so as to obtain more ac-curately predicted 3D boxes. Thanks to the concise design, our loss function is universal and can be plugged into any mature monocular 3D detector, while significantly boosting the performance over their baseline. Experiments demon-strate that our method yields the best performance (Nov. 2021) compared with the other state-of-the-arts by a large margin on KITTI 3D datasets. 1.

Introduction
Monocular 3D object detection is a fundamental task in computer vision, where the goal is to localize and estimate 3D bounding boxes, parameterized by location, dimension, and orientation, of objects from a single image. It can be ap-plied to various scenes, such as autonomous driving, robotic navigation, etc. However, it is an ill-posed and challenging problem since a single image cannot provide explicit depth information. To acquire such resources, most existing meth-ods resort to LiDAR sensors to obtain accurate depth mea-surements [29], or stereo cameras for stereo depth estima-tion [15], but they will increase the cost of practical usages.
In comparison, the monocular camera is cost-effective.
*Corresponding author: ustcbjwu@gmail.com
Figure 1. (a) Most of existing methods consider each object as a single training sample, (b) our proposed homography loss estab-lishes connections between objects, and applies 2D detection as guidance to help constrain 3D localization in (c) Bird’s Eye View.
Most of the existing monocular 3D object detection methods have already achieved remarkable high accuracy with fixed camera settings. However, in their training strate-gies, each 3D object in the scene is treated as an indi-vidual sample without considering the mutual relationships with other neighboring objects, for example, as shown in
Fig. 1(a). Assuming that, if the predicted 3D box of a single object obviously deviates from its ground truth, without ad-ditional constraints, it is usually hard for the network to re-fine and correct the estimated position of this specific sam-ple. To handle this, apart from the regression loss defined by minimizing the discrepancies between the predicted 3D boxes and the ground truths, many algorithms propose pro-jection loss [15, 17, 25, 26] to constrain the optimization of 3D boxes with the supervision of corresponding projected 2D ground truth boxes. However, the 3D localization of a single object is still independent of the others. Differently,
MonoPair [7] exploits the object relationships and builds scene graph to enhance the mutual connections of objects during training and inference. They fully leverage the spa-tial relationships between close-by objects instead of indi-vidually focusing on the information-constrained single ob-ject. An obvious drawback is that an object can only locally connect with its nearest neighbor.
On the other hand, a large percent of approaches are ef-fective for normal objects. In reality, only the foreground
objects can be detected easily, because they are fully vis-ible and have rich recognizable features. Therefore, these approaches still struggle to handle the occluded objects or small ones that are far away from the camera, and those objects usually occupy a higher proportion in the scene.
Limited improvement is achieved since little information is helpful to solve the problem. A straightforward way to im-prove the 3D detection is to correct the results by the fore-ground objects or even the 2D detection results. The most relevant work, MonoFlex [42], which leverages the distribu-tion of different objects and proposes a flexible framework to decouple the truncated objects and adaptively combine multiple approaches for 3D detection. However, it is also limited to training the network for each individual sample.
Moreover, due to the perspective projection, objects with different depths may block each other in image space. Thus,
OFTNet [33] and ImVoxelNet [34] propose to regress 3D positions on Bird’s Eye View (BEV), since objects on the projected BEV plane do not intersect with each other and can be distinguished.
In general, to be concrete as shown in Fig. 1, our core idea is to build the connections between all the objects and globally optimize their 3D positions. Besides, we also asso-ciate BEV with image view through inverse projective map-ping and apply 2D detection results as guidance to improve the 3D localization in BEV. To achieve the goal, we propose
Homography Loss to combine 2D and 3D information and globally balance the mutual relationships to obtain more ac-curate 3D boxes. By doing so, our loss function is able to effectively encode necessary geometric information in both 2D and 3D space, and the network will be enforced to ex-plicitly capture the global geometric relationships between objects which are demonstrated to be helpful for 3D detec-tion. Because of the differentiability and interpretability, our loss function can be plugged into any mature monocu-lar 3D detector. Practically, we take ImVoxelNet [34] and
MonoFlex [42] as examples, and integrate the novel homog-raphy loss during training phase, experiments demonstrate that our method outperforms the state-of-the-arts by a large margin on KITTI 3D detection benchmark (Nov. 2021).
The main contributions can be summarized as follows:
• We propose a novel loss function, termed as homog-raphy loss, to exploit geometric relationships of all the objects in the scene and globally constrain their mutual locations, by using the homography between the image view and the Bird’s Eye View. At the same time, the geometric consistency in both 2D and 3D space will be well preserved. To the best of our knowledge, this is the first work that fully leverages the global geometric constraints in monocular 3D object detection.
• The proposed monocular 3D detector based on homog-raphy loss achieves the state-of-the-art performance on
KITTI 3D detection benchmark, and surpasses the re-sults of all the other monocular 3D detectors, which implies the superiority of our loss.
• We apply this loss function to several popular monoc-ular 3D detectors. Without any additional inference cost, the training is more stable and easier to converge, achieving higher accuracy and performance. It can be a plug-and-play module and be adapted to any monoc-ular 3D detector. 2.