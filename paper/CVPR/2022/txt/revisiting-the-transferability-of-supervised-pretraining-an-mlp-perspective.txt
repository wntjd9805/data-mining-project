Abstract
The pretrain-finetune paradigm is a classical pipeline in visual learning. Recent progress on unsupervised pre-training methods shows superior transfer performance to their supervised counterparts. This paper revisits this phe-nomenon and sheds new light on understanding the trans-ferability gap between unsupervised and supervised pre-training from a multilayer perceptron (MLP) perspective.
While previous works [6, 8, 17] focus on the effectiveness of MLP on unsupervised image classification where pre-training and evaluation are conducted on the same dataset, we reveal that the MLP projector is also the key factor to better transferability of unsupervised pretraining methods than supervised pretraining methods. Based on this ob-servation, we attempt to close the transferability gap be-tween supervised and unsupervised pretraining by adding an MLP projector before the classifier in supervised pre-training. Our analysis indicates that the MLP projector can help retain intra-class variation of visual features, decrease the feature distribution distance between pretraining and evaluation datasets, and reduce feature redundancy. Ex-tensive experiments on public benchmarks demonstrate that the added MLP projector significantly boosts the transfer-ability of supervised pretraining, e.g. +7.2% top-1 accuracy on the concept generalization task, +5.8% top-1 accuracy for linear evaluation on 12-domain classification tasks, and
+0.8% AP on COCO object detection task, making super-vised pretraining comparable or even better than unsuper-vised pretraining. 1.

Introduction
While Supervised Learning with the cross-entropy loss1 (SL) were the de facto pretraining paradigm in computer vi-sion [14, 20, 26, 39] for a long period, recent unsupervised 1In the paper, we specifically use the notation “SL” to indicate the con-ventional supervised learning with the cross-entropy loss. learning methods [3–9, 15, 17, 18, 49, 52] show better trans-fer learning performance on various visual tasks [17,22,53].
This raised the question of why unsupervised pretraining surpasses supervised pretraining even though supervised pretraining uses annotations with rich semantic informa-tion.
Several works have attempted to explain the better trans-ferability of unsupervised pretraining than supervised pre-training by the following two reasons: (1) Learning without semantic information in annotations [16, 37, 45, 53], which makes the backbone less overfit to semantic labels to pre-serve instance-specific information which may be useful in transfer tasks, and (2) Special design of the contrastive loss [22, 23, 53], which helps the learned features to con-tain more low/mid-level information for effective transfer to downstream tasks. Starting from the perspective of su-pervision and loss design, these works provide intuitive ex-planations for better transferability.
In this paper, we shed new light on understanding trans-ferability by considering the multilayer perception (MLP) projector. While previous works [6, 8, 17] verified its effec-tiveness on the unsupervised image classification task: un-supervised training and evaluating the model on the same
ImagNet-1K dataset, they did not explore its effective-ness on transfer tasks thoroughly and rigorously. It is not straightforward to extend the effectiveness of MLP on the unsupervised image classification task to downstream tasks if not supported by rigorous experiments or theoretical anal-ysis, because the performance on the pretraining task is not always predictive of the performance on transfer tasks when there exists a large semantic gap [16, 35, 43]. To our best knowledge, we are the first to identify the MLP projector as the core factor for the transferability with deep empiri-cal and theoretical analysis. With this new viewpoint, we find that a simple yet effective method, adding an MLP pro-jector, can promote the transferability of the conventional
∗ The work was done during an internship at SenseTime.
† Equal Contribution.
‡ Corresponding author.
supervised pretraining methods with the cross-entropy loss (SL) to be comparable or even better than representative un-supervised pretraining methods.
Specifically, we use the concept generalization task [37] on ImageNet-1K, where the pretraining and the evaluation datasets have a large semantic distance, as a probe to ana-lyze the transferability of different models. Our experimen-tal results and corresponding analysis indicate that the MLP projector in unsupervised pretraining methods is important for their better transferability. Motivated by this observa-tion, we insert an MLP projector before the classifier in SL, forming SL-MLP. The added MLP can improve the trans-ferability of supervised pretraining, making supervised pre-training comparable or even better than unsupervised pre-training. Experimental results on SL and SL-MLP show three interesting findings: 1) The added MLP preserves the intra-class variation on the pretraining dataset. 2) The added
MLP decreases the feature distribution distance between the pretraining and the evaluation dataset; 3) The added MLP decreases the feature redundancy in the pretraining dataset.
We also provide theoretical analysis on how the preserved intra-class variation and the decreased feature distribution distance improve the performance on the target dataset, by adding an MLP projector.
Extensive experimental results confirm that adding an
MLP projector into the supervised pretraining method (SL) can consistently improve the transferability of the model on various downstream tasks. Specifically, on the concept generalization task [37], SL-MLP boosts the top-1 accu-racy compared to SL (55.9%→63.1%) by +7.2%. It also achieves better performance (64.1%) than Byol (62.3%) by
+1.8% on the 300-epochs pretraining setting. In classifica-tion tasks on 12 cross-domain datasets [22], SL-MLP im-proves SL by +5.8% accuracy on average. Moreover, SL-MLP shows better transferability than SL on COCO object detection [25] by +0.8% AP. These improvements brought by the MLP projector can largely bridge the transferabil-ity gap between supervised and unsupervised pretraining as detailed in Sec. 5.2.
The main contributions of our paper are three-fold. (1)
We reveal that the MLP projector is the main factor for the transferability gap between existing unsupervised and su-pervised learning methods. (2) We empirically demonstrate that, by adding an MLP projector, supervised pretraining methods can have comparable or even better transferabil-ity than representative unsupervised pretraining methods. (3) We theoretically prove that the MLP projector can im-prove transferability of pretrained models by preserving intra-class feature variation. 2.