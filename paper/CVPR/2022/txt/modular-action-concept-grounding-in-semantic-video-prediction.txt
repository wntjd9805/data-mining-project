Abstract
Recent works in video prediction have mainly focused on passive forecasting and low-level action-conditional pre-diction, which sidesteps the learning of interaction between agents and objects. We introduce the task of semantic action-conditional video prediction, which uses semantic action labels to describe those interactions and can be regarded as an inverse problem of action recognition. The challenge of this new task primarily lies in how to effectively inform the model of semantic action information. Inspired by the idea of Mixture of Experts, we embody each abstract label by a structured combination of various visual concept learn-ers and propose a novel video prediction model, Modular
Action Concept Network (MAC). Our method is evaluated on two newly designed synthetic datasets, CLEVR-Building-Blocks and Sapien-Kitchen, and one real-world dataset called Tower-Creation. Extensive experiments demonstrate that MAC can correctly condition on given instructions and generate corresponding future frames without need of bound-ing boxes. We further show that the trained model can make out-of-distribution generalization, be quickly adapted to new object categories and exploit its learnt features for object detection, showing the progression towards higher-level cognitive abilities. More visualizations can be found at http://www.pair.toronto.edu/mac/. 1.

Introduction
Recently, video prediction has drawn a lot of attention due to its ability to capture meaningful representations through self-supervision [37, 42]. Although modern video predic-tion methods have made significant progress in improving predictive accuracy, most of their applications are limited in the scenarios of passive forecasting [5, 18, 33, 36], meaning models can only passively observe a short period of dynam-ics and accordingly make a short-term extrapolation. Such settings neglect the fact that the observer can also become an active participant in the environment.
To model the movements of active manipulators, several low-level action-conditional video prediction models have been proposed in the community [2, 8, 15, 25, 26, 27]. In
Figure 1. Concept Grounding in Semantic Video Prediction. Af-ter observing the scene, an agent predicts future frames conditioned on a series of semantic actions describing agent-object interactions.
Neither bounding boxes nor key points are provided. Conditioning on different action labels leads to Counterfactual generations. this work, we go one step further by introducing the task of semantic action-conditional video prediction which em-phasizes the modeling of interactions between agents and environment. Instead of using low-level single-entity ac-tions such as action vectors of robot arms as done in prior works [9, 22], our new task provides semantic descriptions of interactive actions, e.g. "Open the door", and asks the model to imagine "What if I open the door" in the form of future frames. This task requires the model to recognize the object identity, assign correct affordances to objects and envision the long-term expectation by planning a reasonable trajectory toward the goal, which resembles how humans might imagine conditional futures. The ability to predict correct and semantically consistent future perceptual infor-mation is indicative of conceptual grounding of actions, in a manner similar to object grounding in image-based detection and generation tasks.
The challenge of action-conditional video prediction pri-marily lies in how to correctly inform the model of more abstract semantic action information. Existing low-level counterparts usually achieve this by employing a naive con-catenation [2, 9] with action vector of each timestep. While this implementation might enable model to move the desired objects, it fails to produce consistent long-term predictions
toward target locations in the multi-entity settings because it was originally designed to only encode the motion infor-mation of a single entity. If we take "put A on B" as an example, it turns out to be difficult to make the model learn what and where B is, because the main self-supervisory sig-nals in the framework of video prediction are pixel changes and B is not moving in this case. In order to distinguish and locate instances in the scene, other related works heavily rely on pre-trained object detectors or ground-truth bounding boxes [3, 14, 17, 40]. However, we argue that utilizing a pre-trained detector actually simplifies the task since such a detector already solves the major difficulty by mapping high-dimension inputs to low-dimension groundings. Fur-thermore, bounding boxes cannot effectively describe com-plex visual changes including rotations and occlusions. Thus, a more flexible way of representing objects and actions is required.
We present a new video prediction model, MAC, short for Modular Action Concept Network. Inspired by the idea of Mixture of Experts, MAC embodies each semantic label by a structured combination of various concept slots, each of which encodes the spatial representation of a specific concept. This design allows MAC to reuse and integrate the knowledge learnt from different scenarios so that it can perceive the locations of motionless objects and extrapolate to unseen cases, showing the progression towards higher-level cognitive abilities. The contributions of this work are summarized as follows: 1. We introduce a new task, semantic action-conditional video prediction as illustrated in Fig 1, which can be viewed as an inverse problem of action recognition. 2. We create two new synthetic video datasets, CLEVR-Building-blocks and Sapien-Kitchen, and label one real-world dataset called Tower-Creation for evaluation. 3. We propose a novel video prediction model, Modular
Action Concept Network, in which aggregation of visual concept slots is directly controlled by action labels. We show that MAC can successfully depict the long-term counterfactual evolution without need of bounding boxes. 4. We demonstrate that the trained MAC can make out-of-distribution generalization, be adapted for new object categories with a small number of samples and exploit its learnt features for detection. 2. Approach
We begin with defining the task of semantic action-conditional video prediction. Given an initial frame x0 and a sequence of action labels a1:T , the model is required to predict the corresponding future frames x1:T . Each action label is a pre-defined semantic description of a spatiotempo-ral movement that involves multiple objects in a scene and spans over multiple frames such as "take the yellow cup on the table" from t = 0 to t = 10. So technically, one can regard this task as an inverse problem of action recognition.
It should also be pointed out that our semantic task is dif-ferent from common dense video prediction and generation tasks in the sense that it focuses on predicting time-agnostic events. Hence, we design the corresponding datasets as videos capturing sufficient key frames of entire actions. In future practices, we can further apply video interpolation methods in CV or motion planner algorithms in RL to make up the intermediate process if needed. 2.1. Motivation
The design of our new task is necessary for studying compositional generalization as it detaches the definition of object from its specific location. However, it also re-quires a successful model to figure out where the desired object is through leveraging abstract labels. Our main idea is that we create a large number of small, specialized and relatively independent learners called concept slots for each word in the dictionary of action labels to capture their corre-sponding spatial representations from observations. During training, action labels will be translated as constituency trees to control the activations of all related concept slots and to assemble the representations of given actions for next-frame prediction. As a result, this language-guided gating mecha-nism embeds the syntactic structures into the learning system and enables the proposed model to dynamically recombine its learnt concepts so that it can understand the combinatorial complexity of the world. In this paper, we demonstrate that our method possesses many key characteristics of system-2 learning [10, 11], including concept grounding, sample efficiency, counterfactual generations, out-of-distribution generalization and fast transfer. 2.2. Modular Action Concept Network
The MAC model is composed of 4 modules including encoder E, decoder D, concept slot module C and recurrent predictor P. The goal of our model is to learn the following mapping:
ˆxt = D(P(C(E(xt−1)|at)|ht−1)) (1) where xt, at and ht are video frame, action labels and hidden states at time t. The overall architecture of our method is illustrated in Fig 2. In the case of stochastic video generation, another two modules, prior p(z) and posterior q(z) , will be added to help estimate the latent distribution of trajectories.
Encoder and Decoder: At each timestep t − 1, the en-coder E receives visual input xt−1 and extracts a set of multi-scale feature maps. In the deterministic setting, we employ a convolutional neural network with an architecture similar to DCGAN [28]. The matching decoder D is a mir-rored version of the encoder with down-sampling operations
Figure 2. The pipeline of MAC in which the computation of concept slot module is elaborated (Better viewed in color). Feature maps extracted by encoder are mapped into the concept slot tensors. Concept slot module receives an action label that controls the collection of concept slot tensors and outputs representations encapsulating this action. A recurrent predictor updates representations before sending them to decoder to predict the next frame. replaced with spatial up-sampling and additional sigmoid output layer. It aggregates the updated latent representations produced by predictor and multi-scale feature maps from encoder to predict the next frame ˆxt.
In the stochastic setting, we use invertible autoencoder in-troduced in CrevNet [42] instead as we find this information-preserving architecture can better preserve the attributes of randomly moving objects. The corresponding decoder is the backward pass, i.e. inverse computation, of the same network of the encoder. Readers can find more details about invertible autoencoder and coupling layer in Appendix B.
Concept Slot Module: The concept slot module C is the core module of MAC. It resembles the mixture of experts as each slot focuses on only one concept in the space of action labels and will be activated and assembled to represent the given actions through the language-guided gating functions.
Each atomic action label will first be decomposed into several constituents of sentence. A constituent is a verb or object phrase, like “pick” or "large red bowl". Since we are mostly dealing with manipulation videos, atomic actions are usually divided into 3 constituents, verb, object1, object2, and more complex multiple-entity actions can be expanded into temporal sequences of several atomic two-object actions. For single-entity actions, object2 will be filled with all zero tensors. Each constituent will have its own dictionary recording all pre-defined words or concepts and gating functions can be derived based on these dictionaries to establish bottom-up connections from concept slots. The computation of concept slot module is given as follows: wi = Ψi(f), cj = Φj(Concat({wi| ∀i, δj(i) = 1})) (2) where w and c are concept and constituent representations and δj is the indicator function for gating function of jth constituent. More specifically, after the feature maps f are extracted from the input image, they are fed into K convo-lutional units Ψi, i.e. the concept slot layer, to create K concept slot tensors of dimension Nd. Here, K is the total number of possible concepts we pre-defined in the dictionary of action labels. Since verbs can be interpreted as spatiotem-poral changes of relationships between objects, not only slots for objects but also slots for verbs, like ’take’ or ’put on’, are computed from the extracted feature maps.
Next, a gating function will collect all involved concept slot tensors and create an ensemble as input for each con-stituent. This assembly process simulates the formation of simplified constituency parse trees. Constituent slot layer Φj can either be resolution-preserving or upsampling operators as spatial information is important for our new task. Finally, outputs of all constituent slots are concatenated pixel-wisely to obtain the representation of actions before sending them to predictor. It is worth noticing that MAC is allowed to have multiple concurrent actions in a scene at inference time. In this case, we copy additional groups of trained constituent slots to represent other actions.
Hence, a training strategy called scheduled sampling [4] is adopted to alleviate the discrepancy between training and inference. 3. Datasets
In this study, we create two new synthetic datesets,
CLEVR-Building-blocks and Sapien-Kitchen, and label one real-world dataset called Tower-Creation from Roboturk [24] for evaluation. This is because most existing video datasets either don’t come with semantic action labels [2] or fail to provide necessary visual information in their first frames due to egomotions and occlusions [16]. Although there are several candidate datasets like Penn Action [44], BAIR [9] and KTH [30] for multi-modal learning, they all adopt the same single-entity setting which actually indicates they can be solved by a much simpler model. To tackle the above issues, we design each video in our datatsets as a depiction of certain atomic action performed by an agent with objects which are observable in the starting frame. Furthermore, we add functions to generate bounding boxes of all objects for both synthetic datasets in order to train AG2Vid. It is worth noting that all three of these domains exhibit a key property named combinatorial explosion, resulting in factorial com-plexity growth in both spatial and temporal dimensions even with a small object set. For instance, a sequence with 6 (out of 32) objects and 6 actions can have 333,396,000 possibili-ties without considering any continuous factor. Hence, our model only sees a small fraction of these potential scenarios during training. 3.1. CLEVR-Building-blocks Dataset
CLEVR-Building-blocks dataset is built upon CLEVR en-vironment [19]. For each video, the data generator initializes the scene with 4 - 6 randomly positioned and visually dif-ferent objects. There are totally 32 combinations of shapes, colors and materials of objects and at most one instance of each combination is allowed to appear in a video sequence.
The agent can perform one of the following 8 actions on objects OA and OB: Pick OA, Pick and Rotate OA trans-versely / longitudinally, Put OA on OB, Put OA on the left / right side of OB, Put OA in the front of / behind OB. Each training sample contains a video of three consecutive Pick-and Put- action pairs and a sequence of semantic action labels of every frame. 3.2. Sapien-Kitchen Dataset
Sapien-Kitchen Dataset describes a more complicated environment in the sense that: (a). It contains deformable ac-tions like "open" and "close"; (b). The structures of different objects in the same category are highly diverse; (c). Objects can be initialized with randomly assigned relative positions like "along the wall" and "on the dishwasher". We collect totally 21 types of small movable objects in 3 categories,
Figure 3. Learned prior. Two recurrent inference modules are deployed to estimate the latent distribution of trajectories. The posterior inference network q(z) can access to the representations of target frames to estimate a true distribution that we expect its prior counterpart p(z) to mimic at test time.
Learned Prior: We leverage a technique called learned prior (Fig 3) from SVG [7] to model the stochastic move-ments in videos. In particular, we build two additional re-current inference networks, prior and posterior respectively, to capture the randomness of motions. During training, the posterior inference network q(z) can access to the repre-sentations of target frames to estimate a true distribution of trajectory that we expect its prior counterpart p(z) to mimic at test time. Codes of motions zt estimated by posterior during training (or by prior during testing) will then be con-catenated with latent representations before sent to predictor.
Predictor: The recurrent predictor P, implemented as a stack of residual ConvLSTM layers [31], calculates the spatiotemporal evolution for each action label respectively.
The memory mechanism of ConvLSTM is essential for MAC to remember its previous actions and to recover the occluded objects. To prevent interference between concurrent actions, hidden states are not shared between actions. The outputs of predictor for all action labels are added point-wisely.
Training: In the deterministic setting, we train our model by minimizing the mean squared error the between the target frames and the predictions. In the stochastic setting, we opti-mize the following variational lower bound (ELBO) using re-parameterization trick [21]:
Lθ,ϕ,ψ(x1:T ) =
T (cid:88)
[Eqϕ(z1:t|x1:t) log pθ(xt|z1:t, x1:t−1) t=1
− βDKL(qϕ(zt|x1:t)||pψ(zt|x1:t−1)] (3) where pθ is the future frame generator, zt represents the latent codes of motion, pψ(zt|x1:t−1) is the prior distribution, qϕ(zt|x1:t) is the posterior distribution and DKL denotes the
Kullback–Leibler (KL) divergence which forces the posterior to approximate the prior distribution. Since pθ is modeled by conditional Gaussian, the likelihood term reduces to MSE measure between the ground truth frames and the predictions.
The full derivation of ELBO is provided in the Appendix A.
At the inference phase, the model will use its previous predictions as visual inputs instead except for the first pass.
4. Experimental Evaluation 4.1. Action-conditonal video prediction
Baselines and setup: We evaluate the proposed model on CLEVR-Building-blocks and Sapien-Kitchen Datasets.
AG2Vid [3] is re-implemented as the baseline model be-cause it is the most related work. Every action graph used in
AG2Vid can be equivalently translated into our cases because each atomic action graph in AG2Vd also involves at most two objects. But unlike our method which only needs visual input and action sequence, AG2Vid also requires bounding boxes of all objects for training and testing and it can only handle deterministic prediction. Furthermore, we conduct an ablation study by replacing concept slot module with the concatenation of features and tiled action vector, which is commonly used in low-level action-conditional video predic-tion [9], to show the effectiveness of our module.
Metrics: To estimate the fidelity of action-conditional video prediction, MSE, SSIM [38], PSNR and LPIPS [43] are calculated between the predictions and groundtruths. FID
[13] and FVD [32] are not appropriate for this task because they cannot tell how faithfully the model obeys the given instructions. However, these metrics still may not effectively tell if actions are successfully completed due to the small sizes of the moving objects. Hence, we also perform a human study to assess the accuracy of performing the correct action in generated videos for each model. The human judges annotate whether the model can identify the desired objects, perform actions specified by action labels and maintain the consistent visual appearances of all objects in its generations and only videos meeting all three criterions are scored as correct. Also, we find it is technically infeasible to train an action recognition model to estimate the accuracy due to the innumerable action labels caused by the property of combinatorial explosion.
Results: The quantitative comparisons of all methods are summarized in Table 1. The MAC achieves the best scores on all metrics without access to additional information like bounding boxes, showing the superior performance of our concept slot module. The qualitative analysis in Fig 4 fur-ther reveals the drawbacks of other baselines. For CLEVR-Building-blocks, the concatenation-based variant fails to recognize the right objects due to its limited inductive bias.
Although AG2Vid has no difficulty in identifying the desired objects, assumptions made by flow warping are too strong to handle rotation and occlusion. Consequently, the adversarial loss enforces AG2Vid to fix these errors by converting them to wrong poses or colors. These limitations of AG2Vid will be further amplified in a more complicated environment, i.e.
Sapien-Kitchen. The same architecture used for CLEVR can only learn to remove the moving objects from their starting positions in Sapien-Kitchen because rotation and occlusion occur more often. The concatenation baseline performs bet-Figure 4. The qualitative comparison on CLEVR-Building-blocks and Sapien-Kitchen. The first row of each figure is the groundtruth sequence. The red and green boxes highlight the quality of pre-dictions by each method.
In contrast to the success of MAC, concatenation-based method fails to find the correct destinations or to preserve attributes of moving objects. Also, bounding boxes used in AG2Vid cannot portray visual changes like rotations correctly. bottle, kettle and kitchen pot, and 19 types of large openable appliances in another 3 categories, oven, refrigerator and dishwasher, from Sapien engine [41]. The agent can perform one of the following 6 atomic actions on small object Os and large appliance Ol: Take Os on Ol, Take Os in Ol, Put
Os on Ol, Put Os in Ol, Open Ol and Close Ol. Composite action sequences are defined as follows: "Take_on–Put_on",
"Take_on–Open–Put_in–Close", "Open–Take_in–Close". 3.3. Tower-Creation Dataset
Each video in Tower-Creation Dataset depicts a robotic arm building a tower with flatware present on the table. We have labeled 524 videos in total since semantic descriptions are not provided and produce 1867 samples consists of two actions: Pick OA and Put OA on OB. We use 1536 video clips for training and 331 for evaluation. It should be pointed out that the size of Tower-Creation dataset is small compared with commonly used datasets such as BAIR [9] which has 59k videos in total. Thus, our experiments can also tell whether evaluated methods are data efficient.
Model
CLEVR-Building-blocks
SSIM↑ MSE↓
LPIPS↓
Accuracy↑
SSIM↑ MSE↓
Sapien-Kitchen
LPIPS↓
Accuracy↑
Copy-First-Frame
Concatenation Baseline
AG2Vid
MAC 0.962 0.961 0.956 0.983 251.38 0.1320
-226.53 58.67 43.52 0.1301 0.0399 0.0303 50.8% 78.8% 95.2% 0.951 0.962 0.947 0.971 152.87 0.0393
-23.13 270.87 11.16 0.0232 0.0684 0.0178 52.4% 5.2% 86.4%
Table 1. Quantitative evaluation on CLEVR-Building-blocks and Sapien-Kitchen. All metrics are averaged frame-wisely except for accuracy.
Figure 5. Counterfactual video generation: Condi-tioning on the same initial frame and different action labels, MAC can produce high-quality imaginations of counterfactual futures.
Various visual outcomes present in the final frames are highlighted with red boxes and enlarged in the final column.
Top: Generative results on
CLEVR-Building-blocks. 34 frames are generated.
Bottom: Generative results on Sapien-Kitchen dataset. 35 frames are generated. ter by showing correct generation of open and close actions on large appliance. Yet, it still fails to produce long-term consistent predictions as the visual appearances of moving objects are altered. On the contrary, MAC can authentically depict the correct actions specified by action labels on both datasets. 4.2. Counterfactual generation
Counterfactual generation: The most intriguing applica-tion of MAC is counterfactual generation. More specifically, counterfactual generation means that our model will observe the same starting frame but receive different valid action labels to produce the corresponding future frames.
Results: The visual results of counterfactual generations on each dataset are displayed in Fig 5. As we can see, our model successfully identifies the desired objects, plans correct trajectories toward the target places and generates high-quality imaginations of counterfactual futures. It is also worth noticing that all displayed generations are long-term generations , i.e. more than 30 frames are predicted for each sequence. Our recurrent predictor plays an very important role in sustaining the spatiotemporal consistency and in reconstructing the fully-occluded objects. 4.3. Stochastic video generation
Baselines and setup: We continue to evaluate the stochas-tic version of MAC (sMAC) on Tower-Creation dataset.
SVG-LP was extended to action-conditional version in [34] so that we can adopt it as the baseline model to demonstrate the effectiveness of concept slot module.
Results: The qualitative and frame-wise quantitative com-parison between sMAC and action-conditional SVG-LP is provided in Fig 6. Although SVG-LP can partially under-stand the given action labels, it often fails to locate and manipulate the desired objects. Consequently, it will gen-erate the moving object out of nowhere and often place it on a wrong target object. In contrast, sMAC can success-fully simulate the trajectory of robotic arms and correctly animate the "Pick" and "Put" actions thanks to the concept slot module. Row 3 and 5 in Fig 6 show that sMAC is also capable of producing diverse future frames and predicting counterfactual results following different action instructions.
The overall accuracy of sMAC estimated by human study on
Tower-Creation is 65.3% compared with 31.8% of SVG-LP. 4.4. Compositional Generalization
We further explore other interesting features of our MAC.
We first demonstrate that MAC is capable of making out-of-distribution generalization by designing two experiments.
We evaluate how quickly our model can be adapted to new
Figure 6. Left: Visual comparison between sMAC and SVG-LP on Tower-Creation. The supposed completions of Pick and Put in the final frames are highlighted by red and yellow boxes while incorrect completions in SVG-LP generations are labelled by grey boxes. The last two rows are counterfactual generations in which models are given different action labels. Right: Quantitative comparison per-frame. Higher SSIM and PSNR indicate better performance.
Figure 7. Compositional gen-eralization and feature reuse.
Top: Unobserved scenarios.
All red cubes are removed from the tranining data, but the trained model can still manipu-late red cube at test time.
Middle: Concurrent actions. In-putting two action sequences at the same time. Both actions are depicted correctly.
Bottom Left: New-object adap-tation. Even with a few train-ing samples, MAC can be fast adapted for generation of new objects. Red arrows point to new objects present in images
Bottom Right: Object detec-tion. Learnt features can be di-rectly applied for detection. objects. It turns out for each new object, the trained MAC only requires a few training video examples to generate decent results. Finally, to verify that our model encodes the spatial information, we add SSD [23] head after the frozen encoder and concept slot layer to conduct object detection.
Unobserved scenarios: We design an interesting experi-ment where only a subset of CLEVR-Building-blocks data are used for training and check what will happen if we input the unobserved action labels to the trained model. More pre-cisely, we exclude all videos manipulating red cubes in the training sets and send the instructions involving red cubes at test time. Note that we only remove one object to avoid high correlation among concept slots, otherwise it will violate the relative independence assumption. Since failure cases will not manipulate the correct objects and will produce very large pixelwise losses. We can set a threshold of MSE to cal-culate the accuracy of performing the correct actions, which is 75.6%. The visualization of this experiment can be found
in Fig 7. As we can see, MAC can still identify and manipu-late red cubes correctly, showing its ability to recombine the learnt concept to comprehend new objects.
Concurrent actions: Concurrent actions means multiple action inputs at the same time. It can be considered as out-of-distribution generalization because our model only observes single-action videos during training. Generating concurrent-action videos needs to employ copied constituent slots and parallel hidden states. As illustrated in Fig 7, MAC can linearly integrate the action information in the latent space and correctly portray 2 concurrent actions in the same scene.
Adaptation: We add a new openable category "safe" and a new movable category "dispenser" into Sapien-Kitchen and generate 100 video sequences for each new object showing its interaction with other objects. Approximately, there are about 5 new sequences created for each new action pair between 2 objects. Blank concept slots for new categories are attached to trained MAC and we finetune it on this small new training set. Visualization in Fig 7 shows that even with a few training samples, MAC is accurately adapted for video generation of new objects. This is because, with the help of concept slots, MAC can disentangle actions into relatively independent grounded concepts. When it learns new concepts, MAC reuses and integrates prior knowledge learnt from different cases.
Object detection: The quantitative results of object de-tection and more visualizations can be found in Appendix
D. We observe that the features learnt by MAC can be eas-ily transferred for detection as our video prediction task is highly location-dependent. This result indicates that utilizing bounding boxes might be a little redundant for some video tasks because videos already provide rich motion informa-tion that can be used for salient object detection. 5.