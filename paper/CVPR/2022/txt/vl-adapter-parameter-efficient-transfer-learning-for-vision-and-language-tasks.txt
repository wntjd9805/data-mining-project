Abstract
Recently, ﬁne-tuning language models pre-trained on large text corpora have provided huge improvements on vision-and-language (V&L) tasks as well as on pure lan-guage tasks. However, ﬁne-tuning the entire parameter set of pre-trained models becomes impractical since the model size is growing rapidly. Hence, in this paper, we introduce adapter-based parameter-efﬁcient transfer learn-ing techniques to V&L models such as VL-BART and VL-T5. We evaluate our methods in a uniﬁed multi-task setup on both image-text and video-text benchmarks. For the image-text tasks, we use four diverse V&L datasets:
VQAv2, GQA, NLVR2, and MSCOCO image captioning.
For video-text tasks, we use TVQA, How2QA, TVC, and
YC2C. With careful training and thorough experiments, we benchmark three popular adapter-based methods (Adapter,
Hyperformer, Compacter) against the standard full ﬁne-tuning and the recently proposed prompt-tuning approach.
We also enhance the efﬁciency and performance of adapters by sharing their weights to attain knowledge across tasks.
Our results demonstrate that training the adapter with the weight-sharing technique (4.18% of total parameters for image-text tasks and 3.39% for video-text tasks) can match the performance of ﬁne-tuning the entire model. Lastly, we present a comprehensive analysis including the combina-tion of adapter and task-speciﬁc prompts and the impact of
V&L pre-training on adapters.1 1.

Introduction
Following the success in the language domain [4, 8, 25, 30, 39, 40], large-scale pre-training of vision-and-language (V&L) models has become a standard framework to tackle
V&L tasks [6, 7, 18, 32, 45, 48]. In such frameworks, V&L models, which are usually the combination of vision en-coders and language models, are ﬁrst pre-trained on large-1The code for our CVPR 2022 paper is available at: https:// github.com/ylsung/VL_adapter.
Figure 1. Comparison of (a) full ﬁne-tuning and our (b) adapter training for V&L tasks. By updating only a small set of adapter pa-rameters, we can achieve similar performance to full ﬁne-tuning.
We experiment with our adapter training on diverse image-text and video-text benchmarks, and here we show VQA as an example. scale unlabeled data, then ﬁne-tuned for downstream V&L tasks. This is the standard strategy to fuse the knowledge of vision-and-language to the language model. However, given that such models’ size grows very rapidly nowadays, either pre-training or ﬁne-tuning of the V&L model can still contribute to an unignorable, large memory and stor-age cost. For instance, if we use GPT-3 [4] with 175B pa-rameters as a backbone of V&L model, we would need 700
GB of memory to store its entire parameters.2 To address this problem, recently, several parameter-efﬁcient training methods have been proposed [12, 16, 17, 20, 28, 33, 47, 52].
Among them, adapter [16] and its variants [20, 33] are widely used in the NLP domain and applied to different ar-chitectures. Adapter is a small module added to interme-diate layers of the model (which is illustrated in Figure 2), which allows to achieve as high performance as full ﬁne-tuning (i.e., updating all parameters), by ﬁne-tuning only a small set of parameters. Moreover, this also shows that it is possible to use a few parameters to learn the information fusion of vision and language without losing performance.
Despite adapters having achieved success in text classiﬁca-tion [16, 20, 33] and image-text alignment [2], to the best of 2(175 × 109) × 4(bytes) × 1 10−9 (GB/bytes) = 700(GB)
our knowledge, no work has utilized this efﬁcient method for more challenging downstream V&L problems, such as visual/video question answering and image/video caption-ing. Besides, V&L models often come with expensive com-putations by combining the knowledge of two input modali-ties. Hence, we investigate the application of adapter-based parameter-efﬁcient training techniques to V&L tasks.
We aim to efﬁciently tune language models on diverse downstream V&L tasks while achieving performance com-parable to full ﬁne-tuning. For this, we analyze these parameter-efﬁcient training techniques in a uniﬁed multi-task learning setup, and we benchmark different adapter
[16, 20, 33] and prompt-based methods [24]. For our V&L model, following Cho et al. [7], we adopt encoder-decoder language models (BART [25] and T5 [40]) that tackle mul-tiple V&L tasks as text generation to avoid designing task-speciﬁc architectures. We use CLIP [37], a pretrained image-text alignment model, as our visual encoder for the ease of doing V&L pre-training. To inform the model about which task it is going to perform, we follow [7, 40] to add task-speciﬁc (text) prompts to the front of the in-put sentence (e.g., “vqa: [Q]” for VQA). We then insert
Adapter [16] and its variants, Hyperformer [20] and Com-pacter [33], into the model to perform parameter-efﬁcient training. Hyperformer and Compacter are recently pro-posed state-of-the-art approaches: Hyperformer improves the efﬁciency of adapters by generating their weights via a hyper-network, while Compacter reduces the parameters by utilizing Kronecker products and low-rank parameterization for the adapters’ weights. We also compare adapter-based approaches with prompt tuning [24], which adds trainable prompts to the input. We show the high-level concept of our work in Figure 1. Practically, adapter training involves parameter updates of adapter modules, layer normalization layers, and the visual projection layer (see Section 3.1 and
Figure 2 for more details). Since we tackle multiple tasks simultaneously [20, 36], we also explore taking advantage of the sharing of information between tasks on adapters and prompts. Speciﬁcally, we make some of the trainable parameters to be shareable to learn cross-task information while reserving the rest of them for task-speciﬁc informa-tion. With this technique, the number of trainable parame-ters can be reduced even further.
We conduct our experiments and analysis on four diverse image-text tasks: VQAv2 [10], GQA [19], NLVR2 [46], and MSCOCO captioning [5]. For completeness, we also verify the effectiveness of our framework on four video-text tasks: TVQA [22], How2QA [26], TVC [23], and
YC2C [56]. Overall, the performance of the three adapter-based approaches closes the gap between which of full ﬁne-tuning. In our experiments, Compacter does not stand out in terms of efﬁciency, since we remove the low-rank ap-proximation for trading performance. Hyperformer is more efﬁcient than adapters, but we eventually show our adapter training with the weight-sharing technique can achieve the same performance as full ﬁne-tuning while only updating 4.18% of the entire parameters for image-text tasks (and 3.39% for video-text tasks). Next, we compare the ﬁne-tuning and freezing of the CLIP parameters, where the lat-ter achieves a better trade-off between performance and parameter efﬁciency. We also present a detailed analy-sis to understand the contribution of each component in adapters, as well as the different parameter-sharing mech-anisms. We ﬁnd that using a single set of adapter mod-ules across all tasks achieves the best results and accuracy-efﬁciency trade-off, showing the possibility of pursuing ef-ﬁciency with simplicity (Fig. 4). Since most of the exper-iments are based on the pre-trained weights accompanied with the models (e.g., CLIP pre-trained weights for ResNet and BART pre-trained weights), we also demonstrate that the results of training adapters on top of V&L pre-trained weights can match or even exceed which of the full ﬁne-tuning counterpart. While we conduct most of our experi-ments with the V&L generation framework, we also extend the adapter training to CLIP-ViL [45], which is one of the
SOTA discriminative V&L approaches. Lastly, we report the results of comprehensive hyperparameter search in Ap-pendix, hoping that they will be useful for related research on parameter-efﬁcient training.
Our contributions could be summarized as: (1) the ﬁrst work benchmarking different types of parameter-efﬁcient training techniques (Adapter, Hyperformer and Compacter) for various challenging downstream image-text and video-text tasks; (2) empirical demonstration of adapters reach-ing the performance of full ﬁne-tuning while updating only 3.39-4.18% of the parameters; (3) comprehensive analysis on the design of freezing CLIP, impact of different architec-tural components, weight-sharing techniques, task-speciﬁc prompts, and vision-language pretraining. 2.