Abstract
CWT
Federated learning is an emerging research paradigm enabling collaborative training of machine learning models among different organizations while keeping data private at each institution. Despite recent progress, there remain fun-damental challenges such as the lack of convergence and the potential for catastrophic forgetting across real-world heterogeneous devices. In this paper, we demonstrate that self-attention-based architectures (e.g., Transformers) are more robust to distribution shifts and hence improve feder-ated learning over heterogeneous data. Concretely, we con-duct the ﬁrst rigorous empirical investigation of different neural architectures across a range of federated algorithms, real-world benchmarks, and heterogeneous data splits. Our experiments show that simply replacing convolutional net-works with Transformers can greatly reduce catastrophic forgetting of previous devices, accelerate convergence, and reach a better global model, especially when dealing with heterogeneous data. We release our code and pretrained models to encourage future exploration in robust architec-tures as an alternative to current research efforts on the op-timization front. 1.

Introduction
Federated Learning (FL) is an emerging research paradigm to train machine learning models on private da-ta distributed over multiple heterogeneous devices [47]. FL keeps data on each device private and aims to train a glob-al model that is updated only via communicated parameters instead of the data itself. Therefore, it provides an opportu-nity for collaborative machine learning across multiple in-stitutions without risking leakage of private data [25,36,54].
This has proved especially useful in domains such as health-care [4, 7, 15, 40], learning from mobile devices [17, 38], s-∗Equal contribution
FedAVG
Figure 1. Prediction test accuracy on highly heterogeneous da-ta partitions (Split-3) of CIFAR-10 dataset versus model size1.
Vision Transformers (ViTs and Swin Transformers) signiﬁcantly outperform CNNs (ResNets and EfﬁcientNets) on highly hetero-geneous data partitions. mart cities [25], and communication networks [49], where preserving privacy is crucial. Despite the rich opportu-nities afforded by FL, there remain fundamental research problems to be tackled before FL can be readily applica-ble to real-world data distributions. Most current methods that aim to learn a single global model across non-IID de-vices encounter challenges such as non-guaranteed conver-gence and model weight divergence for parallel FL meth-ods [35,37,68], and severe catastrophic forgetting problems for serial FL methods [7, 16, 57].
While most research efforts focus on improving the op-timization process in FL, our paper aims to provide a new perspective by rethinking the choice of architectures in fed-erated models. We hypothesize that Transformer architec-tures [12, 61] are especially suitable for heterogeneous da-ta distributions due to their surprising robustness to distri-bution shifts [3]. This property has led to the prevalence of Transformers in self-supervised learning where hetero-geneity is manifested via distribution shifts between unla-beled pretraining data and labeled test data [11], as well as in multimodal learning over fundamentally heterogeneous input modalities such as image and text [24, 60]. To s-tudy this hypothesis, we conduct the ﬁrst large-scale em-pirical benchmarking of several neural architectures across a suite of federated algorithms, real-world benchmarks, and heterogeneous data splits. To represent Transformer net-works, we use a standard implementation of Vision Trans-formers [12, 41] on image tasks spanning image classiﬁca-tion [31, 42] and medical image classiﬁcation [27].
Our results suggest that VIT-FL (Federated Learning with Vision Transformers) performs especially well in set-tings with most heterogeneous device splits, with the gap between VIT-FL and FL with ResNets [19] increasing sig-niﬁcantly as heterogeneity increases. To understand these results, we ﬁnd that the main source of improvement lies in the increased robustness of Transformer models to hetero-geneous data which reduces catastrophic forgetting of pre-vious devices when trained on substantially different new ones. Together, Transformers converge faster and reach a better global model suitable for most devices. Through comparisons to FL methods designed speciﬁcally to combat heterogeneous data, we ﬁnd that VIT-FL provides immedi-ate improvements without using training heuristics, addi-tional hyperparameter tuning, or additional training. More-over, it is noteworthy that our VIT-FL is orthogonal to ex-isting optimization based FL methods, and can be easily applied to improve their performance. To this end, we con-clude that Transformers should be regarded as a natural s-tarting point for FL problems in future research. 2.