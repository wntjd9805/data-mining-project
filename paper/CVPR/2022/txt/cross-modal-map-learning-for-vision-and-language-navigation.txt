Abstract
We consider the problem of Vision-and-Language Navi-gation (VLN). The majority of current methods for VLN are trained end-to-end using either unstructured memory such as LSTM, or using cross-modal attention over the egocen-tric observations of the agent. In contrast to other works, our key insight is that the association between language and vision is stronger when it occurs in explicit spatial representations.
In this work, we propose a cross-modal map learning model for vision-and-language navigation that first learns to predict the top-down semantics on an egocentric map for both observed and unobserved regions, and then predicts a path towards the goal as a set of way-points. In both cases, the prediction is informed by the lan-guage through cross-modal attention mechanisms. We ex-perimentally test the basic hypothesis that language-driven navigation can be solved given a map, and then show com-petitive results on the full VLN-CE benchmark. 1.

Introduction
For mobile robots to be able to operate together with hu-mans, they must be able to execute tasks that are defined not in the form of machine-readable scripts but rather in the form of human instructions. A very basic but challenging task is going from A to B. While robots have been quite successful in executing this task using metric representa-tions, it has been more challenging for robots to execute semantic tasks like “go to the kitchen sink” or follow in-structions that describe a path and associate actions with natural language, defined as the Vision-and-Language Nav-igation (VLN) task [4, 32, 33]. In VLN, the robot is given instructions and has to reach a goal making use of images of the environment that it can acquire along the way.
The dominant approach for VLN tasks has been using end-to-end pipelines from images and instructions to ac-tions [17, 23, 31, 32]. While they can be attractive due to their simplicity, they are expected to implicitly learn end-to-end all navigation components such as mapping, planning,
Figure 1. We approach the task of vision-and-language navigation as a two-stage procedure which learns to semantically and spa-tially ground the instruction on egocentric maps. and control, and thus often require considerable amounts of training data. This approach to designing navigation sys-tems is in direct contrast to research on human spatial nav-igation, which has shown that humans and other species build map-like representations of the environment to ac-complish way-finding [41, 51]. However, multiple findings have shown that the ability to build cognitive maps and ac-quire spatial knowledge deteriorates when humans exclu-sively use ready to drive or walk paths to a goal [6]. On the other hand, studies have shown that humans build bet-ter spatial representations when presented with landmark-based navigation instructions rather than full paths [54].
Such spatial representations enable the recall of landmarks on an egocentric map weeks after the experiment. While this does not prove that humans build a map during wayfind-ing when following semantic instructions, it is a strong indi-cation that they can anchor landmarks and other semantics to a map that they easily recall. Research in learning of mapping and planning in computer vision and robotics [22] has also shown that an end-to-end system encompasses se-mantic maps that naturally emerge in the learning process.
We propose Cross-modal Map Learning (CM2), a novel navigation system for the VLN task in continuous environ-ments, that learns a language-informed representation for both map and trajectory prediction by applying twice cross-modal attention, hence CM2. Our method decomposes the problem in the two paths of semantic and spatial grounding as illustrated in Figure 1. First, we use a cross-modal atten-tion network to semantically ground the instruction through an egocentric map prediction task that learns to hallucinate information outside the field-of-view of the agent. This is followed by another cross-modal attention network that is responsible for spatially grounding the instruction by learn-ing to predict the path on the egocentric map. Our analysis shows that through these two subtasks, the attended repre-sentations learn to focus on instruction-relevant objects and locations on the map.
The main difference between our method and existing image-language attentional mechanisms that generate ac-tions is that in our approach, the robot is building a cogni-tive map that encodes the environmental priors and follows instructions based on this map. The motivation to use this representation is based on our finding that when the robot is given a local ground-truth (“correct”) map of the envi-ronment, then the robot outperforms all approaches on the
VLN task by a large margin. This map is still local, more like a crop of the blueprint than a global map of the envi-ronment, but can still hallucinate what is behind the walls so that it can align better the map with the language instruction.
This differentiates us from approaches such as [12] that first build a topological map of the environment by exploring the whole scene and then executing the task having access to a global map. We further argue that by learning the layout priors through cross-modal attention, we can leverage the spatial and semantic descriptions from natural language and decrease the uncertainty over the hallucinated areas. As op-posed to recent work [31] that outputs a single waypoint, we learn to predict the whole trajectory, while our way-points are determined by the alignment between language and egocentric maps rather than the distance to the goal.
In summary, our contributions are as follows:
• A novel system for the VLN task that learns maps as an explicit intermediate representation.
• Semantic grounding of language to those maps by ap-plying cross-modal attention when learning to predict semantic maps.
• Spatial grounding of instructions when learning to pre-dict paths by applying cross-modal attention on se-mantic maps and language.
• An analysis over the learned representation that demonstrates the effectiveness of using egocentric maps for the VLN task.
• Competitive results in the VLN-CE [32] dataset against current state-of-the-art methods. 2.