Abstract
Learning under a continuously changing data distribu-tion with incorrect labels is a desirable real-world problem yet challenging. A large body of continual learning (CL) methods, however, assumes data streams with clean labels, and online learning scenarios under noisy data streams are yet underexplored. We consider a more practical CL task setup of an online learning from blurry data stream with corrupted labels, where existing CL methods strug-gle. To address the task, we first argue the importance of both diversity and purity of examples in the episodic mem-ory of continual learning models. To balance diversity and purity in the episodic memory, we propose a novel strat-egy to manage and use the memory by a unified approach of label noise aware diverse sampling and robust learn-ing with semi-supervised learning. Our empirical valida-tions on four real-world or synthetic noise datasets (CI-FAR10 and 100, mini-WebVision, and Food-101N) exhibit that our method significantly outperforms prior arts in this realistic and challenging continual learning scenario. Code and data splits are available in https://github.com/ clovaai/puridiver. 1.

Introduction
Continual learning sce-learning (CL) is a practical nario under a continuous and online stream of annotated data [5,24,27]. Due to continuously changing data distribu-tion, the CL methods are known to suffer from the stability-plasticity dilemma due to catastrophic interference and re-sistance to learning new information [12, 25]. In addition, when the CL model is deployed in real-world such as e-commerce applications, the annotated labels are often un-reliable due to less controlled data curation process, e.g., crowd-sourcing [36]. Although data labels are likely to be contaminated in real-world due to human errors, existing
*: corresponding author.
CL studies have largely assumed that the given training data has no annotation error [19, 27], which might hinder practi-cal usages of many CL methods in real-world applications.
A very recent work [17] relaxes this assumption by proposing an online CL setup under data stream with less reliable labels. However, they assume a disjoint class in-cremental scenario such that there is no class overlapping between task streams, which is argued as less practical for real-world applications in the literature [5, 27]. To step for-ward in addressing continuously changing data distribution that can be falsely labeled, we first design a novel CL task of online continual learning on a contaminated data stream on blurry task boundaries [27] as a more realistic and prac-tical continual learning scenario.
Sample selection strategy is arguably important for episodic memory-based CL methods [1, 5, 17, 19, 27].
Specifically, diversity-aware sample selection policy is shown to be effective in the online blurry CL scenarios [5].
However, on the contaminated data stream, diversity-based memory construction may promote to include many exam-ples with corrupted labels in the episodic memory, leading to poor performance thanks to very high capacity of deep models to overly fit all the falsely labeled examples [40].
To address this issue, we propose a unified framework of memory sampling policy with robust learning with the episodic memory and semi-supervised learning with unre-liable data, named PuriDivER (Purity and Diversity aware
Episode Replay). Fig. 1 illustrates an overview of the pro-posed method to address the new CL task set-up.
Specifically, for the memory sampling policy, we pro-pose to construct a robust episodic memory that preserves a set of training examples that are diverse and pure. Unfor-tunately, maintaining the information diversity and purity is in trade-off ; clean examples mostly exhibit smaller losses (less diverse) than noise examples due to the memorization effect of deep neural networks [4, 33]. Thus, emphasizing purity in memory sampling does not promote sample diver-sity and vice versa. To address the dilemma, we define a
Figure 1. Overview of PuriDivER for the online blurry continual learning with noisy labels, where all the tasks share classes (i.e., blurry tasks [27]) and examples might be falsely labeled. PuriDivER updates episodic memory by balancing examples’ diversity and purity, and unifies a robust training scheme based on semi-supervised learning not to be interfered by unreliable labels in the episodic memory. score function of promoting the label purity with an addi-tional term to promote diversity by optimizing the sample distribution to be similar to the one with noisy examples.
In addition, we incorporate a novel robust learning scheme to further promote both purity and diversity when we use the samples from the episodic memory (Sec. 4.2). Thus, we enforce high diversity and purity around the memory twice; by its construction and usage.
Our empirical validations show that the proposed method outperforms combinations of prior arts to address the com-bined challenges by noticeable margins on multiple eval-uations using CIFAR-10/100, WebVision and Food-101N.
Our contributions are summarized as follows:
• Proposing the first online blurry CL set-up with contami-nated data stream, which is realistic and challenging, and establishing a strong baseline for it.
• Proposing a unified framework of considering diversity and purity in memory updates and usage strategy.
• Proposing an adaptive balancing scheme for dynamic trade-off between diversity and purity in memory sam-pling for addressing various noise ratio in CL.
• Proposing a semi-supervised robust learning scheme with label cleaned up samples and unreliable samples regarded as unlabeled samples. 2.