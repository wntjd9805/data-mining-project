Abstract
Current methods for object detection, segmentation, and tracking fail in the presence of severe occlusions in busy ur-ban environments. Labeled real data of occlusions is scarce (even in large datasets) and synthetic data leaves a domain gap, making it hard to explicitly model and learn occlu-sions.
In this work, we present the best of both the real and synthetic worlds for automatic occlusion supervision using a large readily available source of data: time-lapse imagery from stationary webcams observing street intersec-tions over weeks, months, or even years. We introduce a new dataset, Watch and Learn Time-lapse (WALT), consisting of 12 (4K and 1080p) cameras capturing urban environments over a year. We exploit this real data in a novel way to au-tomatically mine a large set of unoccluded objects and then composite them in the same views to generate occlusions.
This longitudinal self-supervision is strong enough for an amodal network to learn object-occluder-occluded layer representations. We show how to speed up the discovery of unoccluded objects and relate the confidence in this discov-ery to the rate and accuracy of training occluded objects.
After watching and automatically learning for several days, this approach shows significant performance improvement in detecting and segmenting occluded people and vehicles, over human-supervised amodal approaches. 1.

Introduction
While there has been strong progress in data-driven methods for object detection [10,14,20,40], tracking [7,58, 59, 62], segmentation [4, 22, 30, 39, 50] and reconstruction
[25,27,29,53] with limited occlusions, most methods under-perform in severely occluded scenarios. Severe occlusions are common in busy intersections and crowded places. Even in less dense scenes, pedestrians and vehicles often pass each other or pass behind other objects. As a result, objects are either not detected at all, or the 2D bounding boxes and segments are truncated and produce errors in downstream processes such as 3D reconstruction [5, 6, 25, 41, 42, 45].
Much of this state of affairs can be attributed to the fact that occlusions are treated as noise that must be over-Figure 1. We visualize the prediction of amodal representation of vehicles and people under severe occlusions trained using our longitudinal self-supervision framework. The method shows sig-nificant improvement in amodal detection and segmentation with images captured from different cameras. come by robust measures [16, 17, 23, 36, 52, 57]. There are several challenges that make this strategy hard to succeed.
First, it is much harder to label object bounding boxes or segments that are occluded, even for humans [47, 49, 63].
Thus, even large datasets like COCO [38] and ImageNet
[34] have relatively few objects labeled that are severely oc-cluded [47, 63]. This creates a strong bias against learning robustness to occlusions [11,46,56]. Further, the evaluation metrics are often reported on the entire datasets [9, 18, 38] that could hide problems in occluded scenarios.
As a result, there is growing recognition that occlusions must be explicitly modeled and learned [15, 19, 30, 30, 48, 61]. This has led to new efforts in labeling occlusions ex-plicitly in multiple datasets [21, 47, 63]. Using such super-vision, amodal, or holistic, representations (e.g. segmenta-tions and bounding boxes) of objects are learned from par-tially occluded observations [28, 54, 60]. While producing significantly better results than before, these commendable efforts are still plagued by the same challenges - difficulty for humans to label occlusions in real scenes and the lim-ited dataset size. To supplement such limited data, focus has turned toward synthesizing objects in occluded scenar-ios using synthetic inpainting [30, 31, 60] using computer graphics [1,13,24]. CG can generate a large amount of data for supervision (given today’s cloud computing resources)
but even the best renderers [8,12,44] leave a notable domain gap to the real data, which needs to be bridged [33, 51].
In this work, we present the best of both the real and synthetic worlds for automatic occlusion supervision us-ing a large source of hitherto unexploited data: time-lapse imagery from stationary cameras observing street intersec-tions over weeks, months, and even years1. We exploit this data in a novel way to first mine a large dataset of real un-occluded objects over time and then use them to synthe-size a large number of occlusion scenarios. We develop a new method to classify unoccluded objects based on the idea that when objects on the same ground plane occlude one another, their bounding boxes overlap in a particular common configuration. Once unoccluded objects are dis-covered, they are composited in layers back into the same scene. These compositions have artifacts that perhaps do not make them too useful for visualization. But they are close enough to real data to reduce the domain gap for a deep network that explicitly predicts the object, its occluder, and the occluded.
Being patient pays off here. Over time, our method dis-covers tens of thousands of unoccluded objects at diverse positions, orientations, and appearances due to lighting and weather conditions, even in busy scenes. We speed up this discovery by combining sparse time sampling of the data with burst local tracking. This step reduces the required ob-servation period from many months to several days (images captured every few mins.). The data enables us to analyze the performance of our approach over different durations and confidences of self-supervision. Specifically, we re-late the confidence in unoccluded object prediction to the rate and accuracy of training occluded objects. In the be-ginning, including lower confidence predictions increases more supervision to speed up training, but is quickly passed by training only on high confidence supervisions.
We introduce a new dataset, Watch and Learn Time-lapse (WALT), consisting of 12 (4K or 1080p) cameras capturing urban environments over a year. The cameras view a di-verse set of scenes from traffic intersections to boardwalks.
The performances of pedestrian and vehicle detection and segmentation improve significantly on all cameras. Like in [32, 49, 57], we report performances at different levels of occlusion and show that the performance drops more slowly as occlusion increases, compared to methods that do not use longitudinal self-supervision. Because of this, we achieve strong results in detecting and tracking objects as they pass each other - a common failure mode of existing approaches.
The methods we present are simple but provide an effective baseline to inspire future work on exploiting longitudinal supervision for computer vision under strong occlusions. 1In the past decades, much analysis on time-lapse data was conducted for illumination and weather understanding [35] [43], object insertion and rendering, from thousands of webcams all over the world [2, 26, 37].
Figure 2. Illustrating the region used to classify unoccluded (Blue) and occluded objects (Red) using planar based IOU (Green) for different categories of objects like vehicles and people. 2. Watch and Learn Amodal Representation
We address the problem of layer representation of objects in a scene under severe occlusions. We propose a continuous learning framework to resolve occlusion ambiguities from images. Initially, given a time-lapse stream of data from a stationary camera, we detect and mine all the unoccluded objects over a long duration of time. These unoccluded ob-jects collected over time automatically act as supervision that we term longitudinal self-supervision. We follow a clip art-based integration method to place these unoccluded ob-jects within the scene at the same detected location but over-lapping with another unoccluded object from the database.
This generates many realistic occlusion configurations for training a network to disentangle holistic object segmenta-tion from a cluttered scene. We further show how to speed up the training for learning amodal representations by track-ing around unoccluded detections. 2.1. Unoccluded Object Mining
We exploit the time-lapse data in a novel way to mine a large dataset of real unoccluded objects over time. We develop a new method to classify unoccluded objects based on the idea that when objects on the same ground plane occlude one another, their bounding boxes overlap in a particular common configuration.
Preprocessing Videos: On the time lapse feed from a cam-era, we run instance segmentation [40] on each frame. We use Intersection-Over-Union based tracker [3] to track the detected bounding box and segmentation. We represent the detections as Dt0....tN m=0,...,M , where tN represents time, while
N represents the number of images and m corresponds to the index of the object from a total of M detections.
Occlusion Classification: We locate and segment unoc-cluded objects in the scene from time lapse video se-quences. The unoccluded objects are detected by exploiting overlap between objects detected in an image as shown in
Fig 2. For every detection Di at time instance tj, we com-Figure 3. We illustrate generated training images(top) from Clip Art WALT dataset. The synthesized Ground-Truth amodal segmentation map(bottom) captures multiple layers(darker represents higher order of occlusion) of occlusions for training. The Clip Art images have realistic occlusions because the inpainting is performed by superimposing the object at the same location as it was observed but from varying time instances. pute the occlusion indicator O(Dtj i ) using (cid:40)
O(Dtj i ) = i ∩ Dtj = 0 or B(Dtj i ) ∩ Dtj < δ if Dtj 0, 1, otherwise. (1)
We use two hypotheses to classify the detected objects as occluded or fully visible. The first constraint is that the bounding box should not intersect any other detected ob-jects Dtj from the same time instance. Secondly, for every overlapping bounding box, we disentangle the occluded ob-ject and the occluder assuming planar constraints. When both objects are on the same plane, we observe that the bot-tom of the occluded bounding box always intersects with another bounding box from the scene. We exploit this ob-servation and find the intersection of the occluding bound-ing box with the bottom of the occluded bounding box
B(Dtj i ). If the intersection is larger than a threshold δ, we classify the object as occluded. This classification is com-puted iteratively over all the detections Dt0....tN m=0,...,M and un-occluded object detections and segmentations are extracted. 2.2. Clip-Art based Self-Supervision
Once unoccluded objects are discovered, they are compos-ited in layers back into the same scene as shown in Fig 3.
These are close enough to real data to reduce the domain gap for a deep network that explicitly predicts the object, its occluder, and the occluded.