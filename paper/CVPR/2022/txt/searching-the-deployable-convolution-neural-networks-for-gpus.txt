Abstract
Customizing Convolution Neural Networks (CNN) for production use has been a challenging task for DL practi-tioners. This paper intends to expedite the model customiza-tion with a model hub that contains the optimized models tiered by their inference latency using Neural Architecture
Search (NAS). To achieve this goal, we build a distributed
NAS system to search on a novel search space that con-sists of prominent factors to impact latency and accuracy.
Since we target GPU, we name the NAS optimized models as GPUNet, which establishes a new SOTA Pareto frontier in inference latency and accuracy. Within 1ms, GPUNet is 2x faster than EfficientNet-X and FBNetV3 with even better accuracy. We also validate GPUNet on detection tasks, and
GPUNet consistently outperforms EfficientNet-X and FB-NetV3 on COCO detection tasks in both latency and ac-curacy. All of these data validate that our NAS system is effective and generic to handle different design tasks. With this NAS system, we expand GPUNet to cover a wide range of latency targets such that DL practitioners can deploy our models directly in different scenarios. 1.

Introduction
The progress of neural networks has decoupled from the actual deployment for a long time. Deep Learning (DL) researchers have been dedicated to inventing new building blocks, while DL engineers deploy these building blocks in real-world tasks, painstakingly recombine them to find architectures that meet the design requirements. Most of the time, we can simplify these requirements to find the best-performing architecture on the target device (e.g.,
GPUs) within a specific latency budget. Though there are many exciting advancements in the neural network designs, e.g., the residual connection [13], Inverted Residual Block (IRB) [28] and the attention [32], deploying these network designs remains challenging and laborious; and this is the problem to be addressed in this paper.
Our solution to alleviate the gap between the DL research
Figure 1. GPUNet establishes the new SOTA Pareto frontier in the accuracy and inference latency. and the actual deployment is to propose a set of optimized
Convolution Neural Networks for each type of GPUs tiered by their optimized inference latency (e.g., post-processed by TensorRT [3] or OpenVINO [2]). Specifically, we de-liver a table of models, an entry of which is the result of model optimization from maximizing the accuracy subject to the limit of inference latency on a GPU. This table en-ables DL engineers to directly query the optimized neural architecture w.r.t the design requirements to expedite the customization process on expensive models.
We resort to Neural Architecture Search (NAS) to design models in this table. Recently NAS has shown promising results to automate the design of network architectures in many tasks [19, 36, 37]. Therefore, NAS can be a handy tool since we need to design many models for many latency limits for different GPUs. When models are ready to de-ploy, we measure post-processed TensorRT engine latency, i.e., including quantization, layer/tensor fusion, kernel tun-ing, and other system side model optimizations. Finally, we design our model toward NVIDIA enterprise GPU products for their broad adoption by the community today.
We built a novel distributed NAS system to achieve our goal. Following the prior works, our NAS consists of 3 modules, a search space, an evaluation module, and a search method. The search space provisions networks following the predefined patterns; the search method proposes the most promising network based on the priors. The evalua-tion module returns the performance of the proposed net-work either by training or estimation from a supernet [45].
Our search space constructs a network by stacking convolu-tion layers, IRBs, and Fused-IRBs used in EfficientNet [31].
However, our search space is the most comprehensive by far that includes filter numbers (#filters), kernel sizes, the number of layers (#layers) or IRBs (#IRBs) in a stage, and the input resolution. Within an IRB or Fused-IRB, we also search for the expansion ratio, the activation type, with or without the Squeeze-Excitation (SE) layer. All of these fac-tors are identified as prominent factors to affect latency and accuracy. Therefore, this search space enables us to bet-ter leverage the accuracy and latency than prior works, e.g., the fixed filter pattern in NASNet [46] and the fixed ac-tivation and SE pattern in FBNetV3 [10]; and the search also enables us to find a better policy than the fixed scaling strategy in EfficientNet [18, 30]. To support such a com-plex search space, we choose to evaluate a network candi-date by training. Although this approach is far more ex-pensive than the supernet approach, the evaluation is more accurate in ranking the architectures [42, 45]. And we can avoid many unresolved problems in building a supernet for our search space, e.g., supporting multiple types of activa-tion, activating/deactivating SE, and variable filter sizes. We built a client-server-style distributed system to tackle the computation challenges, and it has robustly scaled to 300
A100 GPUs (40 DGX-A100 nodes) in our experiments. Fi-nally, we adopt the LA-MCTS guided Bayesian Optimiza-tion (BO) [35] as the search method for its superior sample efficiency demonstrated in the recent black-box optimiza-tion challenges [1].
We name the NAS optimized CNNs as GPUNet, and
GPUNet has established a new SOTA Pareto front in the latency and accuracy in Fig. 1. We measure the latency of
GPUNet using TensorRT, so GPUNet is directly reusable to DL practitioners. Particularly, GPUNet-1 is nearly 2x faster and 0.5% better in accuracy than FBNetV3-B and
EfficientNet-X-B2-GPU, respectively. We also validate
GPUNet on COCO detection tasks, and GPUNet still con-sistently outperforms EfficientNet-X and FBNetV3. All of these data validate that our NAS system is effective and generic in designing various tasks. Although this paper only shows a few GPUNet for comparisons, the complete model hub tiered by the inference latency is still ongoing, and we will release them with the paper. 2.