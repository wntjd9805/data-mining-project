Abstract
We propose a neural inverse rendering pipeline called
IRON that operates on photometric images and outputs high-quality 3D content in the format of triangle meshes and material textures readily deployable in existing graphics pipelines. Our method adopts neural representations for geometry as signed distance fields (SDFs) and materials dur-ing optimization to enjoy their flexibility and compactness, and features a hybrid optimization scheme for neural SDFs: first, optimize using a volumetric radiance field approach to recover correct topology, then optimize further using edge-aware physics-based surface rendering for geometry refine-ment and disentanglement of materials and lighting. In the second stage, we also draw inspiration from mesh-based differentiable rendering, and design a novel edge sampling algorithm for neural SDFs to further improve performance.
We show that our IRON achieves significantly better inverse rendering quality compared to prior works. (a) Reconstruction of real-world objects, rendered under global illumination. 1.

Introduction
Inverse rendering—the reconstruction of shape and ap-pearance of real-world objects from a set of 2D input images—can enable accessible, high-quality digitization of our world. One way to formulate this problem is as the in-version of rendering algorithms used in computer graphics.
Recent advances in graphics have led to fully differentiable
Monte Carlo path tracing methods for jointly optimizing ge-ometry and BRDFs represented as triangle meshes and ma-terial textures. However, meshes can be difficult to optimize, because it is non-trivial to modify their topology or maintain regularity during optimization. On the other hand, recently developed neural representations for shape [22, 31] and radi-ance fields [23, 29] demonstrate impressive success in view synthesis [23] and shape reconstruction [26, 30, 34, 38, 39] tasks. But these representations entangle material and light-ing, and cannot be directly used for applications like relight-ing or material editing. Recent methods for decoupling neu-ral radiance fields are either limited to simple shapes [6, 41], (b) Scene editing by modifying illumination and materials, and inserting objects.
Figure 1. Real-world objects reconstructed by our IRON pipeline.
We show (a) re-renderings under novel global illumination via
Mitsuba path tracing [14], and (b) scene edits that include changing lighting, material BRDFs, and the scale/orientation of existing objects, and inserting new virtual objects or participating media such as the metal Van Gogh head and the heterogeneous smoke. or have neural components [4, 43] that are incompatible with standard 3D rendering and editing tools.
We address the inverse rendering problem with the ob-jective of embracing both the flexibility and compactness of neural representations, and the convenience of meshes with material textures in downstream applications. We present a pipeline we call IRON—Inverse Rendering by Optimiz-ing Neural scene components, including neural signed dis-tance fields (SDF) and neural materials. IRON performs an inverse rendering optimization starting from multi-view images captured by co-locating a flashlight with a moving
camera [4, 5, 21, 24] (called photometric images in recent work), while allowing for the export of the optimized 3D content in mesh and material texture formats readily usable by traditional graphics renderers and AR/VR applications, as shown in Fig. 1. At the core of IRON are four compact neural networks representing the neural SDF and materials, which we optimize with a hybrid optimization scheme: first, a volumetric radiance field optimization to recover object topology, followed by physics-based surface rendering to refine geometric details and disentangle materials and light-ing. We demonstrate superior inverse rendering quality over baseline methods targeting photometric images.
In addition, recent differentiable rendering methods in the graphics community emphasize the importance of carefully considering occlusion boundaries when performing inverse rendering with meshes, e.g., with special edge sampling algorithms that compute accurate gradients at such edges [2, 8, 17, 18, 28, 40]. In addition to our system as a whole, we propose an edge sampling algorithm defined for neural SDF representations, rather than meshes as in prior work. We show that our new edge sampling algorithm significantly improves reconstruction quality around edges.
Contributions. To summarize, our key contributions are:
• IRON, a neural inverse rendering pipeline for high-quality reconstruction of object shape and spatially varying materials, outperforming existing methods for photometric images.
• A hybrid optimization scheme for neural SDFs and ma-terials that first optimizes a volumetric radiance field then performs edge-aware physics-based surface render-ing for improved performance and compatibility with meshes and material textures.
• An edge-aware rendering optimization featuring a novel edge sampling algorithm that generates unbiased gradi-ent estimates for better optimizing neural SDFs. 2.