Abstract
Despite the recent advances in multi-task learning of dense prediction problems, most methods rely on expensive labelled datasets. In this paper, we present a label efficient approach and look at jointly learning of multiple dense pre-diction tasks on partially annotated data (i.e. not all the task labels are available for each image), which we call multi-task partially-supervised learning. We propose a multi-task training procedure that successfully leverages task relations to supervise its multi-task learning when data is partially annotated. In particular, we learn to map each task pair to a joint pairwise task-space which enables sharing information between them in a computationally efficient way through an-other network conditioned on task pairs, and avoids learning trivial cross-task relations by retaining high-level informa-tion about the input image. We rigorously demonstrate that our proposed method effectively exploits the images with unlabelled tasks and outperforms existing semi-supervised learning approaches and related methods on three standard benchmarks. 1.

Introduction
With the recent advances in dense prediction computer vision problems [16, 24, 37, 42, 46, 53, 56, 63, 64, 67, 71, 72], where the aim is to produce pixel-level predictions (e.g. se-mantic and instance segmentation, depth estimation), the interest of the community has started to shift towards the more ambitious goal of learning multiple of these problems jointly by multi-task learning (MTL) [7]. Compared to the standard single task learning (STL) that focuses on learning an individual model for each task, MTL aims at learning a single model for multiple tasks with a better efficiency and generalization tradeoff while sharing information and computational resources across them.
Recent MTL dense prediction methods broadly focus on designing MTL architectures [4, 5, 18, 23, 36, 38, 43, 48, 57, 59, 65, 75–77] that enable effective sharing of information across tasks and improving the MTL optimization [11,12,20, 22,29,32,33,36,50,66] to balance the influence of each task-Figure 1. Multi-task partially supervised learning. We look at the problem of learning multiple tasks from partially annotated data (b) where not all the task labels are available for each image, which generalizes over the standard supervised learning (a) where all task labels are available. We propose a MTL method that employs a shared feature extractor (fϕ) with task-specific heads (hψ) and exploits label correlations between each task pair by mapping them into a joint pairwise task-space and penalizing inconsistencies between the provided ground-truth labels and predictions (c). specific loss function and to prevent interference between the tasks in training. We refer to [58] for a more comprehensive review. One common and strong assumption in these works is that each training image has to be labelled for all the tasks (Fig. 1(a)). There are two main practical limitations to this assumption. First, curating multi-task image datasets (e.g.
KITTI [19] and CityScapes [14]) typically involves using multiple sensors to produce ground-truth labels for several tasks, and obtaining all the labels for each image requires very accurate synchronization between the sensors, which is a challenging research problem by itself [60]. Second, imagine a scenario where one would like to add a new task to an existing image dataset which is already annotated for another task and obtaining the ground-truth labels for the new task requires using a different sensor (e.g. depth camera) to the one which is used to capture the original data. In this case,
labelling the previously recorded images for the new task will not be possible for many visual scenes (e.g. uncontrolled outdoor environments). Such real-world scenarios lead to obtaining partially annotated data and thus ask for algorithms that can learn from such data.
In this paper, we look at a more realistic and general case of the MTL dense prediction problem where not all the task labels are available for each image (Fig. 1(b)) and we call this setting multi-task partially supervised learning. In particular, we assume that each image is at least labelled for one task and each task at least has few labelled images and we would like to learn a multi-task model on them. A naive way of learning from such partial supervision is to train a multi-task model only on the available labels (i.e. by setting the weight of the corresponding loss function to 0 for the missing task labels). Though, in this setting, the MTL model is trained on all the images thanks to the parameter sharing across the tasks, it cannot extract the task-specific information from the images for the unlabelled tasks. To this end, one can extend existing single-task semi-supervised learning methods to MTL by penalizing the inconsistent predictions of images over multiple perturbations for the unlabelled tasks (e.g. [13, 28, 31, 35, 55]). While this strategy ensures consistent predictions over various perturbations, it does not guarantee consistency across the related tasks.
An orthogonal information that has recently been used in
MTL is cross-task relation [39, 49, 68] which aims at produc-ing consistent predictions across task pairs. Unfortunately existing methods are not directly applicable for learning from partial supervision, as they require either each train-ing image to be labelled with all the task labels [49, 68] or cross-task relations that can be analytically derived [39]. In our setting, compared to [39, 49, 68], there are fewer training images available with ground-truth labels of each task pair and thus it is harder to learn the relationship. In addition, unlike [39], we focus on the general setting where one task label cannot be accurately obtained from another (e.g. from semantic segmentation to depth) and hence learning exact mappings between two task labels is not possible.
Motivated by these challenges, we propose a MTL ap-proach that shares a feature extractor between tasks and also learns to relate each task pair in a learned joint pairwise task-space (illustrated in Fig. 1(c)), which encodes only the shared information between them and does not require the ill-posed problem of recovering labels of one task from an-other one. There are two challenges to this goal. First, a naive learning of the joint pairwise task-spaces can lead to trivial mappings that take all predictions to the same point such that each task produces artificially consistent encodings with each other. To this end, we regulate learning of each mapping by penalizing its output to retain high-level infor-mation about the input image. Second, the computational cost of modelling each task pair relation can get exponen-tially expensive with the number of tasks. To address this challenge, we use a single encoder network to learn all the pairwise-task mappings, however, dynamically estimate its weights by conditioning them on the target task pair.
The main contributions of our method are as following.
We propose a new and practical setting for multi-task dense prediction problems and a novel MTL model that penal-izes cross-task consistencies between pairs of tasks in joint pairwise task-spaces, each encoding the commonalities be-tween pairs, in a computationally efficient manner. We show that our method can be incorporated to several architectures and significantly outperforms the related baselines in three standard multi-task benchmarks. 2.