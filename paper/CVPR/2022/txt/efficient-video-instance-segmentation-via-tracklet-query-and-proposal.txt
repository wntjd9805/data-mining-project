Abstract
Video Instance Segmentation (VIS) aims to simultane-ously classify, segment, and track multiple object instances in videos. Recent clip-level VIS takes a short video clip as input each time showing stronger performance than frame-level
VIS (tracking-by-segmentation), as more temporal context from multiple frames is utilized. Yet, most clip-level meth-ods are neither end-to-end learnable nor real-time. These limitations are addressed by the recent VIS transformer (VisTR) [25] which performs VIS end-to-end within a clip.
However, VisTR suffers from long training time due to its frame-wise dense attention. In addition, VisTR is not fully end-to-end learnable in multiple video clips as it requires a hand-crafted data association to link instance tracklets between successive clips. This paper proposes EfﬁcientVIS, a fully end-to-end framework with efﬁcient training and in-ference. At the core are tracklet query and tracklet proposal that associate and segment regions-of-interest (RoIs) across space and time by an iterative query-video interaction. We further propose a correspondence learning that makes track-lets linking between clips end-to-end learnable. Compared to VisTR, EfﬁcientVIS requires 15× fewer training epochs while achieving state-of-the-art accuracy on the YouTube-VIS benchmark. Meanwhile, our method enables whole video instance segmentation in a single end-to-end pass without data association at all. 1.

Introduction
Video Instance Segmentation (VIS) is a challenging video task recently introduced in [28]. It aims to predict a tracklet segmentation mask with a class label for every appeared ob-ject instance in a video as illustrated in Fig. 1. Existing meth-ods typically solve the VIS problem at either frame-level or clip-level. The frame-level methods [10, 14, 16, 24, 26, 29] follow a tracking-by-segmentation paradigm, which ﬁrst performs image instance segmentation and then links the current masks with history tracklets via data association as shown in Fig. 1 (a). This paradigm typically requires com-Figure 1. Overview of real-time VIS pipelines. Compared to (a) or (b), our method features 1) Efﬁcient training thanks to the RoI-wise design; 2) Efﬁcient framework that achieves whole video instance segmentation in a single end-to-end pass without any data association and post-processing; 3) Strong performance thanks to the clip-by-clip processing and fully end-to-end paradigm. plex data association algorithms and limits temporal context, making it susceptible to object occlusions. By contrast, the clip-level methods [2, 3, 15, 25] jointly perform segmentation and tracking clip-by-clip. Within each clip, object informa-tion is propagated back and forth. Such a paradigm usually performs stronger than the frame-level methods thanks to larger temporal receptive ﬁeld. However, most clip-level methods are not end-to-end and require elaborated inference that causes slow speed. These issues are addressed by the recent VIS transformer (VisTR) [25] which extends image object detector DETR [7] to the VIS task. VisTR generates
VIS predictions within each video clip in one end-to-end pass, which greatly simpliﬁes the clip-level paradigm and makes VIS within a clip end-to-end trainable.
However, the VIS transformer is confronted with two issues: (i) The convergence speed of VisTR is slow since the frame-wise dense attention weights in the transformer need long training epochs to search for properly attended regions among all video frame pixels.
It is difﬁcult for researchers to experiment with this algorithm as it requires long development cycles. (ii) When a video is too long to ﬁt into GPU memory in one forward-pass, it has to be divided into multiple successive clips for sequential processing. In
this case, VisTR becomes partially end-to-end. As shown in
Fig. 1 (b), VisTR requires a hand-crafted data association to link tracklets between clips. Such a hand-crafted scheme is not only more complicated but also less effective than an end-to-end learnable association as evidenced in Table 1d.
In this paper, we propose a new clip-level VIS frame-work, coined as EfﬁcientVIS. EfﬁcientVIS delivers a fully end-to-end framework that can achieve fast convergence and inference with strong performance. Our work is inspired by the recent success of the query-based R-CNN architec-ture [10, 20] in image object detection. EfﬁcientVIS extends the spirit of sparse object proposal [20] to the video domain to model the complex space-time interactions. Speciﬁcally,
EfﬁcientVIS uses tracklet query paired with tracklet pro-posal to represent an individual object instance in video.
Tracklet query is latent embeddings that encode appearance information for a target instance, while tracklet proposal is a space-time tube that locates the target in the video. Our track-let query collects the target information by interacting with video in a clip-by-clip fashion through a designed temporal dynamic convolution. This way enriches temporal object context that is an important cue for handling object occlu-sion and motion blur in videos. We also design a factorised temporo-spatial self-attention allowing tracklet queries to exchange information over not only space but also time. It enables one query to correlate a target across multiple frames so as to end-to-end generate target tracklet mask as a whole in a video clip. Compared to prior works, EfﬁcientVIS en-joys three remarkable properties: (i) Fast convergence: In EfﬁcientVIS, tracklet queries interact with video CNN features only in the region of the space-time RoIs deﬁned by the tracklet proposal. This is different from VisTR [25] that is interacting with all video pixels on the transformer [22] encoded features using dense attention. Our RoI-wise design drastically reduces video redundancies and therefore allows EfﬁcientVIS for faster convergence than transformer as shown in Table 1g. (ii) Fully end-to-end learnable framework: EfﬁcientVIS goes beyond a short clip and is fully end-to-end learnable over the whole video. When there are multiple successive clips, one needs to link instance tracklets between clips. In contrast to prior clip-level works [3, 18, 23, 25] that manually stitch the tracklets, we design a correspondence learning that enables tracklet query to be shared among clips for seamlessly associating a same instance. In other words, the tracklet query output from one clip is enabled to be fed into the next clip to associate and segment the same instance.
Meanwhile, the query is dynamically updated in terms of the next clip content so as to achieve a continuous tracking for the future. Such a scheme makes EfﬁcientVIS fully end-to-end, without any explicit data association for either inner-clip or inter-clip tracking as shown in Fig. 1 (c). (iii) Tracking in low frame rate videos: Tracking object instances that have dramatic movements is a great challenge for motion-based trackers [18, 23, 31], as they suppose in-stances move smoothly over time. In contrast, our method retrieves a target instance in a frame conditioned on its query representation regardless of where it is in nearby frames.
Thus, EfﬁcientVIS is robust to dramatic object movements and can track instances in low frame rate videos as shown in
Fig. 5 and Table 1h.
We summarize our major contributions as follows:
• EfﬁcientVIS is the ﬁrst RoI-wise clip-level VIS framework that runs in real-time. The RoI-wise de-sign enables a fast convergence by drastically reducing video redundancies. Fully end-to-end learnable track-ing and rich temporal context of the clip-by-clip work-ﬂow together bring a strong performance. EfﬁcientVIS
ResNet-50 achieves 37.9 AP on Youtube-VIS [28] in 36 FPS by training 33 epochs, which is 15× training epochs fewer and 2.3 AP higher than VIS transformer.
• EfﬁcientVIS is the ﬁrst fully end-to-end neural net for VIS. Given a video as input despite its length, Efﬁ-cientVIS directly produces VIS predictions without any data association or post-processing. We will demon-strate by diagnostic experiments that this fully end-to-end paradigm is not only simpler but also more effective than the previous partially/non end-to-end frameworks. 2.