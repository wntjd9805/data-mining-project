Abstract
This paper presents a new approach to boost a single-modality (LiDAR) 3D object detector by teaching it to sim-ulate features and responses that follow a multi-modality (LiDAR-image) detector. The approach needs LiDAR-image data only when training the single-modality detector, and once well-trained, it only needs LiDAR data at inference.
We design a novel framework to realize the approach: re-sponse distillation to focus on the crucial response samples and avoid most background samples; sparse-voxel distilla-tion to learn voxel semantics and relations from the esti-mated crucial voxels; a fine-grained voxel-to-point distilla-tion to better attend to features of small and distant objects; and instance distillation to further enhance the deep-feature consistency. Experimental results on the nuScenes dataset show that our approach outperforms all SOTA LiDAR-only 3D detectors and even surpasses the baseline LiDAR-image detector on the key NDS metric, filling ∼72% mAP gap be-tween the single- and multi-modality detectors. 1.

Introduction
State-of-the-art 3D object detectors, e.g., [11, 33, 48, 51, 54], widely adopt LiDAR-produced point clouds as the ma-jor input modality, since point clouds offer precise depth in-formation and are robust to varying weather condition and illumination. Yet, due to the laser-ray divergence, the spar-sity of point clouds increases with distance. So, there are only few points in small and distant objects, making it very hard to predict their object boundaries and semantic classes.
On the other hand, camera-produced images are a pop-ular modality for monocular and stereo 3D object detec-tion [7,16,21,22,31]. As images offer clear appearance and texture with dense pixels, image-based detectors can easily recognize the object boundaries and classify even small and distant objects. Yet, images have no depth information and the visibility of objects depends on the environment condi-tions, e.g., lighting, so image-based detectors usually cannot be as accurate and robust as the LiDAR-based ones.
Figure 1. Overview of our Simulated Single-to-Multi-Modality
Single-Stage 3D object Detector (S2M2-SSD) framework, by which we can train a single-modality SSD to learn from a multi-modality SSD and to achieve a high precision close to the multi-modality SSD but with only single-modality input at inference.
Some recent works [17, 27, 41, 44, 52] started to explore the fusion of 3D point clouds and 2D RGB images for im-proving the feature quality. While higher precisions can of-ten be attained, multi-modality detectors unavoidably sacri-fice the inference efficiency for processing the extra modal-ity. Also, it is tedious to calibrate and synchronize different sensors, spatially and temporally, for high-quality data fu-sion. Last, a breakdown of any modality sensor will cause a detector failure, thus reducing the system’s fault tolerance.
In this work, we propose to teach a single-modality net-work to produce simulated multi-modality features and re-sponses from only LiDAR input by training the network to learn from a multi-modality LiDAR-image detector. Our approach needs multi-modality data only when training the single-modality network, and once it is well-trained, it can detect 3D objects without image inputs. This approach per-fectly meets the sheer practical need of autonomous driv-ing and boosts single-modality 3D object detection for (i) high efficiency, since our approach needs to process only
LiDAR data at inference; (ii) high precision, since our net-work outperforms the SOTA LiDAR-only detectors; and
Figure 2. Even with only LiDAR input, our S2M2-SSD (c) is able to predict accurate bounding boxes (green) that well match the ground truths (red). Its precision is close to that of the multi-modality SSD (a) and outperforms the SOTA LiDAR-only SSD [25] (b). (iii) high robustness, since our approach is capable of simu-lating LiDAR-image features for detecting objects in vary-ing lighting conditions, even at night time.
To realize the approach, the single-modality network has to effectively learn from the multi-modality network. Yet, there are several technical challenges. First, the vast 3D space is dominated by background samples, which hinder the transfer of foreground knowledge. Second, 3D detectors involve massive points and/or voxels; naively distilling all pairs of voxel/point features from multi- to single-modality is computationally infeasible. Last, it remains challenging to effectively transfer knowledge for objects of various sizes and shapes, particularly for the small and distant ones.
We address the challenges by designing a novel Simu-lated Single-to-Multi-Modality Single-Stage 3D object De-tector (S2M2-SSD) framework (see Figure 1) to effectively train the single-modality network to learn from the multi-modality one. This work has the following technical contri-butions. First, we design the crucial response mining strat-egy and formulate the response distillation for focusing on the crucial responses while avoiding most background ones.
Second, we extend the strategy to voxels and formulate con-sistency constraints on voxel features and voxel relations to enhance the single-modality intermediate features. Third, we formulate a fine-grained voxel-to-point distillation on the crucial foreground points for enhancing the features of objects with sparse points or of small sizes. Last, we fur-ther correct the single-modality predictions by learning on the last-layer bird’s eye view (BEV) features to improve the instance-level consistency in the deep-layer features.
The above techniques enable us to train and produce a single-modality network that takes only point clouds as input, yet capable of achieving a high performance close to a multi-modality LiDAR-image network; see Figure 2.
The evaluation on the nuScenes test set also shows that our
S2M2-SSD outperforms all state-of-the-art single-modality 3D object detectors; our NDS metric even surpasses the multi-modality SSD and our mAP fills more than 70% of the gap between the single- and multi-modality SSDs. 2.