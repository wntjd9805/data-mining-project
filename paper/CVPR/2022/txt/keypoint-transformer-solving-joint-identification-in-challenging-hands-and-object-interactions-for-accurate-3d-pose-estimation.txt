Abstract
We propose a robust and accurate method for estimat-ing the 3D poses of two hands in close interaction from a single color image. This is a very challenging problem, as large occlusions and many confusions between the joints may happen. State-of-the-art methods solve this problem by regressing a heatmap for each joint, which requires solv-ing two problems simultaneously: localizing the joints and recognizing them.
In this work, we propose to separate these tasks by relying on a CNN to first localize joints as 2D keypoints, and on self-attention between the CNN features at these keypoints to associate them with the correspond-ing hand joint. The resulting architecture, which we call
“Keypoint Transformer”, is highly efficient as it achieves state-of-the-art performance with roughly half the number of model parameters on the InterHand2.6M dataset. We also show it can be easily extended to estimate the 3D pose of an object manipulated by one or two hands with high performance. Moreover, we created a new dataset of more than 75,000 images of two hands manipulating an object fully annotated in 3D and will make it publicly available. 1.

Introduction 3D hand pose estimation has the potential to make vir-tual reality, augmented reality, and interaction with com-puters and robots much more intuitive. Recently, signifi-cant progress has been made for single-hand pose estima-tion using depth maps and even single RGB images. Be-ing able to deal with RGB images is particularly attrac-tive as it does not require a power-hungry active sensor.
Many approaches have been proposed, mostly based on di-rect prediction with different convolutional network archi-tectures [15, 18, 29, 36, 44, 49, 61] of the 3D joint locations or angles, or relying on rendering for fine pose estimation and tracking [2, 12, 32, 40, 50].
In contrast to single-hand pose estimation, two-hand
Figure 1. Our approach accurately predicts 3D hand and object poses from a single RGB image in challenging scenarios including complex hand interactions (top) and 2 hands interacting with an object where the hands can be severely occluded (bottom). The bottom example is from the H2O-3D dataset we also introduce in this paper, which contains challenging, fully and accurately 3D-annotated, video sequences of two hands manipulating objects. pose estimation has received much less attention. This problem is indeed significantly harder: The appearance sim-ilarities between the joints of the two hands make their iden-tification extremely challenging. Moreover, in close inter-action, some of the joints of a hand are likely to be occluded by the other hand or itself. Thus, first detecting the left and right hands and then independently predicting their 3D poses [13, 36] performs poorly in close interaction scenar-ios. Bottom-up approaches [30,54] directly estimate the 2D joint locations and their depths using one heatmap per joint.
However, as shown in Fig. 2, the similarity in appearances of the joints and severe occlusions degrade the quality of heatmaps failing to localize the joints accurately. More re-Input image
Heatmap from [30] Right hand pose Right hand pose for the
Right Index Tip recovered by [30] recovered by our method
Figure 2. Similar appearances between joints and partial oc-clusions make previous methods prone to failure. The Inter-Net state-of-the-art method [30] predicts a heatmap for each joint but the predicted heatmaps can become ambiguous, resulting in failures when predicting the hand pose (the hand on the back in this example). Our approach explicitly models the relationship be-tween keypoints resulting in more accurate poses. More examples can be found in the supplementary material. cent works [10,22,58] have attempted to alleviate this prob-lem by exploiting joint-segmentation, joint-visibility, or by adding more refinement layers increasing the overall com-plexity of the network. By exploiting only keypoints, our method outperforms these methods by a large margin with a significantly smaller model.
As shown in Fig. 3, instead of aiming to localize and rec-ognize the hand joints simultaneously, we estimate the 3D poses of the hands in three stages: (1) We first detect “key-points”, which are potential joint locations in the image, by predicting a single heatmap. These keypoints do not have to exactly match all the hand joints: The 3D poses we predict are still correct if some joints are not detected as keypoints, and if some keypoints do not correspond to joints. (2) Then, we associate the keypoints with the corresponding joint or to the background in the case of false positives, on the ba-sis of the keypoint locations and their image features. This is done for all the keypoints simultaneously to exploit mu-tual constraints, using the self-attention mechanism. (3) Fi-nally, we predict the 3D hand poses using a cross-attention module, which selects keypoints associated with each of the hand joints. Our approach is agnostic to the parameteriza-tion of the pose and we consider three different hand pose representations.
Our architecture, which we call “Keypoint Trans-former”, is therefore designed to explicitly disambiguate the identity of the keypoints and performs very well even on complex configurations. Fig. 1 shows its output on two challenging examples, using the MANO [41] mesh as the output representation. Our architecture is related to the “De-tection Transformer” (DETR) [8] architecture. DETR uses all the spatial features from a low-resolution CNN feature map, combined with learned location queries to detect ob-jects in an image. The high computational complexity of the Transformer restricts DETR from using higher resolu-tion CNN feature maps. As we show in our experiments, using the DETR-style architecture for hand pose estimation results in lower accuracy and we hypothesize that this is due to the use of lower resolution feature maps and features from the entire image.
We train and evaluate our architecture on the recent In-terHand2.6M hand-hand [30] and HO-3D hand-object [12] interaction datasets. We also introduce a challenging dataset of videos with two hands interacting with an object with complete and accurate 3D annotations without markers.
This dataset is based on the work of [12], and we call it
H2O-3D. Our method achieves state-of-the-art performance on existing hand-interaction datasets and serves as a strong baseline for the H2O-3D dataset. Our experiments show that on InterHand2.6M, our method achieves state-of-the-art performance with roughly half the number of model pa-rameters. We carry out several ablation studies and compare with strong baselines to prove the efficacy of our approach. 2.