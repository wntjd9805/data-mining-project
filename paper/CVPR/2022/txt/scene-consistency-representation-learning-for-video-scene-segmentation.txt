Abstract
A long-term video, such as a movie or TV show, is com-posed of various scenes, each of which represents a series of shots sharing the same semantic story. Spotting the cor-rect scene boundary from the long-term video is a challeng-ing task, since a model must understand the storyline of the video to figure out where a scene starts and ends. To this end, we propose an effective Self-Supervised Learning (SSL) framework to learn better shot representations from unlabeled long-term videos. More specifically, we present an SSL scheme to achieve scene consistency, while explor-ing considerable data augmentation and shuffling meth-ods to boost the model generalizability. Instead of explic-itly learning the scene boundary features as in the previ-ous methods, we introduce a vanilla temporal model with less inductive bias to verify the quality of the shot features.
Our method achieves the state-of-the-art performance on the task of Video Scene Segmentation. Additionally, we sug-gest a more fair and reasonable benchmark to evaluate the performance of Video Scene Segmentation methods. The code is made available.1 1.

Introduction
In the process of video creation, to make the story more compelling, the editor will use various editing techniques, such as montage, one shot to the end, etc. Quickly switch-ing between stories and scenes makes the movie plot tighter, e.g. inserting outdoor battle scenes into indoor dialogue scenes, as shown in Fig. 1 (b), making the scene tran-1https://github.com/TencentYoutuResearch/Scen
∗Equal Contribution
†Corresponding Author eSegmentation-SCRL
sition more intriguing and unpredictable, thus the task of
Video Scene Segmentation turns out to be rather challeng-ing. Hence, it is essential to understand the high-level se-mantic information of each scene in the long-term video.
There has been extensive studies dealing with video un-derstanding tasks on datasets where the individual video clip is typically short, while requiring a lot of labor to segment uncurated videos into short videos by category.
Although some studies focus on splitting the long video into smaller segments, e.g., the methods of Action Spot-ting [2–5] aim to locate the positions of the beginning and ending of the action, however, they are the category-aware approaches. By contrast, Video Scene Segmentation is a category-agnostic task that only the scene boundary label is available, and it’s very confusing to classify a scene frag-ment taxonomically.
Since a long-term video is inherently structured in a spe-cific way, a sequence of frames can be divided into shots or scenes in terms of the granularity of semantics [6] [7].
More specifically, a shot contains only continuous frames taken by the camera without interruption, and a scene is composed of successive shots and describes the same short story. For detecting shot boundaries, [8] [7] split a video into many separate shots using lower-level visual context.
Based on this, many mainstream approaches of Video Scene
Segmentation [9] [10] [6] [1] determine scene boundaries by exploring semantic correlations among the adjacent shots.
While computer vision tasks suffer from the high cost of manual annotation, Self-Supervised Learning (SSL) based methods [11–18] are proposed to train a general feature ex-tractor using unlabeled data. By leveraging a small amount of annotated data for training, these SSL methods can achieve appealing feature representation to even rival some supervised learning methods. For Video Scene Segmenta-tion, [1] proposes to narrow the feature representation dis-tance of the most similar shot pair in a local region, it sig-nificantly surpasses the supervised learning method [6] by employing a mere MLP classifier. However, in current SSL methods on the task of Video Scene Segmentation, the strat-egy of positive sample selection, pretraining protocol, eval-uation metric and downstream model are not well discussed or addressed.
To achieve this goal, we propose a self-supervised learn-ing scheme to learn better representations, as well as the evaluation metric for the task of Video Scene Segmentation.
The contributions of this paper are summarized as follows:
• A representation learning scheme based on Scene Con-sistency is proposed to obtain better shot representa-tions on the unlabeled long-term video.
• A simple yet effective temporal model with less induc-tive bias is proposed to assess the quality of the shot representation for the downstream Video Scene Seg-mentation task.
• A benchmark that is more fair and reasonable is intro-duced for both pretraining and evaluation. More im-portantly, the proposed method outperforms the state-of-the-art methods under all the protocols, and can sig-nificantly improve the performance of existing super-vised methods without bells and whistles. 2.