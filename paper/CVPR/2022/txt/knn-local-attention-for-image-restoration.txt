Abstract
Recent works attempt to integrate the non-local opera-tion with CNNs or Transformer, achieving remarkable per-formance in image restoration tasks. The global similar-ity, however, has the problems of the lack of locality and the high computational complexity that is quadratic to an input resolution. The local attention mechanism alleviates these issues by introducing the inductive bias of the local-ity with convolution-like operators. However, by focusing only on adjacent positions, the local attention suffers from an insufﬁcient receptive ﬁeld for image restoration. In this paper, we propose a new attention mechanism for image restoration, called k-NN Image Transformer (KiT), that rec-tiﬁes the above mentioned limitations. Speciﬁcally, the KiT groups k-nearest neighbor patches with locality sensitive hashing (LSH), and the grouped patches are aggregated into each query patch by performing a pair-wise local atten-tion. In this way, the pair-wise operation establishes non-local connectivity while maintaining the desired properties of the local attention, i.e., inductive bias of locality and lin-ear complexity to input resolution. The proposed method outperforms state-of-the-art restoration approaches on im-age denoising, deblurring and deraining benchmarks. The code will be available soon. 1.

Introduction
Image restoration aims to recover a clean image from various type of degradations (e.g. noise, blur, rain, and compression artifacts), which has a huge impact on the performance of downstream tasks such as image classiﬁca-tion [14, 56], object detection [22, 46], segmentation [4, 10], and to name a few. It is a highly ill-posed inverse problem as there may exist multiple number of solutions for a sin-gle degraded image. Recent restoration works [17, 36, 76] attempt to establish a mapping relation between clean and
*This work was supported by the Mid-Career Researcher Pro-gram through the NRF of Korea (NRF-2021R1A2C2011624 and NRF-2021R1A2C2006703) and the Yonsei University Research Fund of 2021 (2021-22-0001).
† Corresponding author: dbmin@ewha.ac.kr (a) (b) (c)
Figure 1. Comparisons of different attention approaches: (a)
Global attention [18, 45, 57] computes self-similarity between patches globally, (b) Local attention [33, 59] measures self-similarity within a single patch at the pixel-level, and (c) the pro-posed method aggregates similar k patches with a pair-wise local attention at the pixel-level. degraded images by leveraging the representation power of the convolutional neural networks (CNNs). A series of local operations used in the CNNs is, however, inher-ently less capable of capturing a long-range dependency, exhibiting certain limitations in deliberating global infor-mation over an entire image. To enlarge the receptive ﬁeld, increasing network depth [51], dilated convolution [66], and hierarchical architecture [40] have been proposed, but the receptive ﬁeld still does not secure global information as it is limited to local regions. Recently, non-local op-eration, which mostly contributed to non-learning based restoration approaches [5, 15], has again emerged as a promising solution with the success of non-local neural net-works [58]. As similar patterns tend to repeat within a nat-ural image, non-local self-similarity of computing the re-sponse at a single position by weighted sum of all posi-tions has served as an important cue for an image restora-tion [16, 28, 32, 37, 38, 43, 53, 77, 78]. A non-local self-similarity of [58] could capture the long-range dependency within deep networks, but the quadratic complexity with re-spect to the input feature resolution limits the network ca-pacity. Consequently, it is employed only in relatively low-resolution feature maps of speciﬁc layers [16, 32, 77].
More recently, Vision Transformer (ViT) [18] proposed a new approach to apply the global attention mechanism, which can be viewed as the non-local operation, of the
Transformer [55] to vision tasks by splitting an image into
a set of non-overlapping patches, embedding into the fea-ture space, and feeding them into multiple transformer lay-ers to model global self-similarities among patches (Fig. 1 (a)). ViT achieved a pleasing trade-off between accuracy and computational complexity in the image classiﬁcation task, but the quadratic complexity with respect to the input feature resolution still makes it nearly infeasible to apply the transformer to dense prediction tasks. To overcome this lim-itation, different from ViT that maintains feature resolutions across the entire network, some approaches [45, 57] pro-posed a hierarchical architecture to exploit multi-scale fea-ture maps that are suitable for dense prediction tasks. How-ever, they focus only on capturing global self-similarity, and their capability in exploring locality that is essential for im-age restoration is inferior to that of CNNs.
In this context, numerous methods have been proposed to introduce the inductive bias of locality into transformer ar-chitectures [30, 33, 59, 61, 63]. Among them, local attention is considered in recent works [31,33,54,59,67] at the cost of restricting the receptive ﬁeld in the transformer. These ap-proaches propose the local self-attention module, achieving a linear complexity to the input feature resolution (Fig. 1 (b)). Since they constrain the self-attention computation only within a local patch, a shifting approach [31, 33, 59] is additionally applied to exchange information across non-overlapping patches. However, it considers only neighbor-ing patches and thus still has insufﬁcient receptive ﬁeld.
In this paper, we propose a novel non-local image restoration method, called k-NN Image Transformer (KiT), that successfully captures locality while explicitly establish-ing non-local connectivity by considering the local attention of k nearest neighbor (k-NN) patches. To remedy the lack of the long-range dependency inherent in the local atten-tion, the proposed method considers k matched patches that generate non-local connectivity between patches of differ-ent positions. To be speciﬁc, the KiT ﬁrst searches a set of similar patches for each base patch with k-NN matching, and then sets the base patch as query and k matched patches as key and value for applying pair-wise attention locally, as shown in Fig. 1 (c). This enables our method to apply the lo-cal attention over an entire image while maintaining a linear complexity with respect to the feature resolution. Addition-ally, the inductive bias of locality enhances local feature ex-traction capability. As shown in Fig. 2, our method consists of a series of k-NN transformer block (KTB), and adopts
U-shaped hierarchical architecture for efﬁciently leveraging multi-scale features. Comprehensive experiments on var-ious image restoration tasks demonstrate the effectiveness of the proposed method over state-of-the-art methods. 2.