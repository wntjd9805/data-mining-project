Abstract
Event cameras are novel bio-inspired sensors, which asynchronously capture pixel-level intensity changes in the form of “events”. Due to their sensing mechanism, event cameras have little to no motion blur, a very high temporal resolution and require signiﬁcantly less power and mem-ory than traditional frame-based cameras. These charac-teristics make them a perfect ﬁt to several real-world appli-cations such as egocentric action recognition on wearable devices, where fast camera motion and limited power chal-lenge traditional vision sensors. However, the ever-growing
ﬁeld of event-based vision has, to date, overlooked the po-tential of event cameras in such applications. In this pa-per, we show that event data is a very valuable modality for egocentric action recognition. To do so, we introduce
N-EPIC-Kitchens, the ﬁrst event-based camera extension of the large-scale EPIC-Kitchens dataset. In this context, we propose two strategies: (i) directly processing event-camera data with traditional video-processing architectures (E2(GO)) and (ii) using event-data to distill optical ﬂow in-formation (E2(GO)MO). On our proposed benchmark, we show that event data provides a comparable performance to RGB and optical ﬂow, yet without any additional ﬂow computation at deploy time, and an improved performance of up to 4% with respect to RGB only information. The N-EPIC-Kitchens dataset is available at https://github. com/EgocentricVision/N-EPIC-Kitchens. 1.

Introduction
Egocentric vision has introduced a variety of new chal-lenges to the computer vision community, such as human-object interaction [18,65], action anticipation [1,30,39,64], action recognition [52], and video summarization [23, 57, 58]. With the advent of novel large-scale datasets [14, 15], new tasks are being proposed, such as wearer’s pose es-timation [105] and egocentric videos anonymization [95].
This trend will grow in the next years thanks to the very recent release of Ego4D [41], a massive-scale egocentric
*The authors equally contributed to this work.
Figure 1. N-EPIC-Kitchens: the ﬁrst event-based dataset for egocentric action recognition. From RGB images, we generate a stream of events (bottom). Positive polarity is represented by red events, whereas blue events represent negative polarity. Events focus on motion, similarly to optical ﬂow (top). With their low latency, high temporal resolution, and low-power consumption, event data are a perfect ﬁt for egocentric action recognition. video dataset offering more than 3,000 hours of daily-life activity videos accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and multi-view videos.
Among all, RGB sensors provide by far the richest source of visual information. However, the performance of RGB-based deep models drastically decrease when the training and test data do not share the same distribution [20].
This issue, known as environmental bias [53, 72, 78, 85, 89], originates from RGB-based networks’ tendency to rely on the environment in which activities are recorded, affect-ing their ability to recognize actions when they are per-formed in unfamiliar (unseen) surroundings. This is mainly caused by appearance-based networks’ tendency to primar-ily focus on background cues and objects texture, which are typically uncorrelated with the action being performed and thus largely varying in different environments. As a result, appearance-free modalities, such as motion, have become the favored choice in current egocentric vision systems, as
testiﬁed by the results of recent EPIC-Kitchens challenges
[16, 17, 19]. However, the optical ﬂow used in this setting is computed from RGB frames by solving expensive op-timization problems (TV-L1 algorithm [108]), introducing signiﬁcant test-time computations [12].
Event-based cameras, on the other hand, have been shown to be particularly suitable for online settings [24,31].
Their high pixel bandwidth results in reduced motion blur, and the extremely low latency and low power consumption make these novel sensors particularly good in egocentric scenarios, where fast motion often impacts RGB-based sys-tems negatively. Moreover, as they only convey differential information, event sequences reveal more information about the dynamic of the scene than its appearance, making them a valid alternative to RGB frames when learning to focus on motion. Still, despite these advantages, no prior research has looked at how to exploit their sensitivity to motion in egocentric vision, where these devices remain unused.
As a ﬁrst step in this direction, we propose N-EPIC-Kitchens, a novel dataset that enables, for the ﬁrst time, the use of event data in this context. It consists in the exten-sion of the large-scale EPIC-Kitchens dataset [14] under the setup proposed in [72]. The latter is particularly appealing for both the availability of multiple environments (kitchens) and multiple modalities, i.e., RGB, optical ﬂow, and audio.
These characteristics allow for the analysis of the afore-mentioned environmental bias as well as the comparison of event data to well-established modalities. On the proposed
N-EPIC-Kitchens, we introduce two approaches to exploit the intrinsic motion characteristics of event data in this con-text. The ﬁrst, which we call E2(GO), consists in extend-ing traditional 2D and 3D action recognition architectures with layer variations aimed at exploiting the motion-rich features of event data. The second, E2(GO)MO, extends motion reasoning by distilling motion information from op-tical ﬂow to event data. This is accomplished following a teacher-student approach that allows taking full advantage of expensive ofﬂine TV-L1 ﬂow during training only, while avoiding its computation at test time. We summarize our contributions as follows:
• We release N-EPIC-Kitchens, the ﬁrst event-based egocentric action recognition dataset, which unlocks the possibility to explore event data in this context;
• We benchmark N-EPIC-Kitchens on popular action recognition architectures, showing performance of both event data alone and combined with RGB and op-tical ﬂow modalities. Moreover, we demonstrate the robustness of event data to environment changes;
• We propose E2(GO) and E2(GO)MO, two event-based approaches tailored at emphasizing motion in-formation captured by event data in egocentric action recognition;
• We show that event data can outperform RGB in chal-lenging unseen environments and are competitive with them in known environments, suggesting that using event data is a viable option and more research should be performed in this direction. 2.