Abstract
The task of video virtual try-on aims to fit the target clothes to a person in the video with spatio-temporal con-sistency. Despite tremendous progress of image virtual try-on, they lead to inconsistency between frames when applied to videos. Limited work also explored the task of video-based virtual try-on but failed to produce visually pleasing and temporally coherent results. Moreover, there are two other key challenges: 1) how to generate accurate warp-ing when occlusions appear in the clothing region; 2) how to generate clothes and non-target body parts (e.g. arms, neck) in harmony with the complicated background; To ad-dress them, we propose a novel video virtual try-on frame-work, ClothFormer, which successfully synthesizes realistic, harmonious, and spatio-temporal consistent results in com-plicated environment. In particular, ClothFormer involves three major modules. First, a two-stage anti-occlusion warping module that predicts an accurate dense flow map-ping between the body regions and the clothing regions.
Second, an appearance-flow tracking module utilizes ridge regression and optical flow correction to smooth the dense flow sequence and generate a temporally smooth warped clothing sequence. Third, a dual-stream transformer ex-tracts and fuses clothing textures, person features, and en-vironment information to generate realistic try-on videos.
Through rigorous experiments, we demonstrate that our method highly surpasses the baselines in terms of synthe-sized video quality both qualitatively and quantitatively†. 1.

Introduction
The task of video virtual try-on aims to synthesize a co-herent video that preserves the appearance of one target clothes and the original person’s pose and body shape in the source video. This task has attracted much attention in recent years because of the prospects of its wide application in e-commerce and short video industry.
*Work done in iQIYI Inc.
†The code and all demos are available at https://github.com/ luxiangju-PersonAI/ClothFormer.
Figure 1. Examples of our video virtual try-on results on the VVT dataset and our dataset (first row).
Previous virtual try-on methods usually focus on image-based operations [8, 12, 17, 18, 22, 28, 36, 44]. Among them,
CP-VTON [36] proposed a geometric matching module to learn the parameters of TPS transformation, which greatly improves the accuracy of deformation. WUTON [22] and
PFAFN [13] proposed parser-free methods to reduce the de-pendency of using accurate masks, and VITON-HD [8] fur-ther increased the resolution of the generated image. These methods have achieved great success in different aspects.
However, image virtual try-on is far from video virtual try-on in terms of immersion, and it often lead to inconsistent results between frames when image-based methods applied to videos.
There are a few attempts of designing video virtual try-on. FW-GAN [9] is the first proposed method, which in-troduces the optical flow prediction module proposed in
Video2Video [38] to warp the past frames to synthesize coherent future frames. Similarly, FashionMirror [7] also predicts optical flow, however, it warps the past frames to future frames at the feature level instead of the pixel level, which enabled it can generates clothes in differ-ent views. MV-TON [48] further adds a memory refine-ment module to memorize the features of past frames. Al-though the above methods have made some progress in video spatio-temporal consistency, however, their generated frames are flickering and has a distance from achieving a
spatio-temporally smooth video. We argue that there are two stems: on one hand, the above approaches only pay attention to try-on module while ignoring that the inputs deformed by the warping module are inconsistent. On the other hand, the images are synthesized in a frame by frame manner, which suffers from inconsistent attention results along spatio-temporal dimensions and often leads to blurri-ness and temporal artifacts in videos. Besides, the above ap-proach is difficult to meet the demands in practical scenarios due to the presence of occlusions (e.g., hair, hands, bags) appearing in the clothing area. Lastly, the above methods are designed for simple datasets with pure background, like
VVT [9]. Thus they cannot deal with complex environment, nor to be in harmony with the natural background.
To address the challenges mentioned above, we pro-pose a novel video virtual try-on framework, called Cloth-Former. Firstly, inspired by VITON-HD [8], we intro-duce a clothing-agnostic person representation that elimi-nates clothing information thoroughly and preserves back-ground and occlusion. Next, we novelly employ frame-level TPS-based warp method to predict and mask the oc-then feed the pro-clusion region of the target clothes, cessed target clothes to an appearance-flow-based meth-ods to get a accurate and anti-occlusion dense flow map-ping (appearance-flow) between the body regions and the clothing regions. Moreover, we enforce output to be spatio-temporally consistent in both warp module and try-on mod-ule. In warp module, we carry out two steps (ridge regres-sion and optical flow correction) on appearance-flow se-quence to produce the temporally smooth warped clothes
In try-on module, we propose a Multi-scale sequence.
Patch-based Dual-stream Transformer (MPDT) generator with multi-input and multi-output to synthesize the final re-alistic video based on the outputs from the previous stages, which simultaneously optimizes all the output frames in a single feed-forward process. Lastly, MPDT generator em-ploys MPDT block to extract the clothes color and texture spatio-temporal features from warped clothing sequence and extract preserved person features and environmental information in agnostic-clothing sequence, which aims to generate clothes, non-target body(including arms, neck et al.) and fill the background in agnostic region. Then, the integration between the clothing features and background content features adopted in MPDT block to generate more harmonious results. To validate the performance of our framework, we collected a wild virtual try-on dataset with occlusion and complicated background for our research pur-pose. Our experiments demonstrate that ClothFormer sig-nificantly outperforms the existing methods in generating videos, both quantitatively and qualitatively.
Our contributions can be summarized as below:
• A novel warp module that combines the advantages of
TPS-based methods and appearance-flow-based meth-ods is designed to address the problem of inaccurate warp due to occlusions appear in clothing region.
• A tracking module based on ridge regression and op-tical flow correction are proposed to deforme a tem-porally smooth warped clothing sequence, which pro-vides a prerequisite for the try-on module to generate coherent videos.
• The MPDT generator is designed carefully in the try-on module, which can extract and fuse clothing tex-tures, person features and environment information to generate realistic try-on videos. To the best of our knowledge, this is the first time that transformer has been applied to the video virtual try-on. 2.