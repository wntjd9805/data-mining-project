Abstract
Input ours (f = 4)
PSNR: 27.4 R-FID: 0.58
DALL-E (f = 8)
PSNR: 22.8 R-FID: 32.01
VQGAN (f = 16)
PSNR: 19.9 R-FID: 4.98
By decomposing the image formation process into a se-quential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation al-lows for a guiding mechanism to control the image gen-eration process without retraining. However, since these models typically operate directly in pixel space, optimiza-tion of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evalu-ations. To enable DM training on limited computational resources while retaining their quality and ﬂexibility, we apply them in the latent space of powerful pretrained au-toencoders. In contrast to previous work, training diffusion models on such a representation allows for the ﬁrst time to reach a near-optimal point between complexity reduc-tion and detail preservation, greatly boosting visual ﬁdelity.
By introducing cross-attention layers into the model archi-tecture, we turn diffusion models into powerful and ﬂexi-ble generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for im-age inpainting and class-conditional image synthesis and highly competitive performance on various tasks, includ-ing unconditional image generation, text-to-image synthe-sis, and super-resolution, while signiﬁcantly reducing com-putational requirements compared to pixel-based DMs. 1.

Introduction
Image synthesis is one of the computer vision ﬁelds with the most spectacular recent development, but also among those with the greatest computational demands. Espe-cially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based mod-els, potentially containing billions of parameters in autore-gressive (AR) transformers [64,65]. In contrast, the promis-ing results of GANs [3, 26, 39] have been revealed to be mostly conﬁned to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [79], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive
∗The ﬁrst two authors contributed equally to this work.
Figure 1. Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excel-lent inductive biases for spatial data, we do not need the heavy spa-tial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at 5122 px. We denote the spatial down-sampling factor by f . Reconstruction FIDs [28] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8. results in image synthesis [29,82] and beyond [7,44,47,56], and deﬁne the state-of-the-art in class-conditional image synthesis [15,30] and super-resolution [70]. Moreover, even unconditional DMs can readily be applied to tasks such as inpainting and colorization [82] or stroke-based syn-thesis [52], in contrast to other types of generative mod-els [19, 45, 67]. Being likelihood-based models, they do not exhibit mode-collapse and training instabilities as GANs they can and, by heavily exploiting parameter sharing, model highly complex distributions of natural images with-out involving billions of parameters as in AR models [65].
Democratizing High-Resolution Image Synthesis DMs belong to the class of likelihood-based models, whose mode-covering behavior makes them prone to spend ex-cessive amounts of capacity (and thus compute resources) on modeling imperceptible details of the data [16, 71]. Al-though the reweighted variational objective [29] aims to ad-dress this by undersampling the initial denoising steps, DMs are still computationally demanding, since training and evaluating such a model requires repeated function evalu-ations (and gradient computations) in the high-dimensional space of RGB images. As an example, training the most powerful DMs often takes hundreds of GPU days (e.g. 150 -1000 V100 days in [15]) and repeated evaluations on a noisy version of the input space render also inference expensive,
so that producing 50k samples takes approximately 5 days
[15] on a single A100 GPU. This has two consequences for the research community and users in general: Firstly, train-ing such a model requires massive computational resources only available to a small fraction of the ﬁeld, and leaves a huge carbon footprint [63, 83]. Secondly, evaluating an al-ready trained model is also expensive in time and memory, since the same model architecture must run sequentially for a large number of steps (e.g. 25 - 1000 steps in [15]).
To increase the accessibility of this powerful model class and at the same time reduce its signiﬁcant resource con-sumption, a method is needed that reduces the computa-tional complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility.
Departure to Latent Space Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a perceptual compression stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and concep-tual composition of the data (semantic compression). We thus aim to ﬁrst ﬁnd a perceptually equivalent, but compu-tationally more suitable space, in which we will train diffu-sion models for high-resolution image synthesis.
Following common practice [11, 23, 64, 65, 93], we sep-arate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efﬁcient) representational space which is perceptu-ally equivalent to the data space. Importantly, and in con-trast to previous work [23,64], we do not need to rely on ex-cessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complex-ity also provides efﬁcient image generation from the latent space with a single network pass. We dub the resulting model class Latent Diffusion Models (LDMs).
A notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [78]. This enables efﬁ-cient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the lat-ter, we design an architecture that connects transformers to the DM’s UNet backbone [69] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3.
In sum, our work makes the following contributions: (i) In contrast to purely transformer-based approaches
[23, 64], our method scales more graceful to higher dimen-sional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efﬁciently
Figure 2. Illustrating perceptual and semantic compression: Most bits of a digital image correspond to imperceptible details. While
DMs allow to suppress this semantically meaningless information by minimizing the responsible loss term, gradients (during train-ing) and the neural network backbone (training and inference) still need to be evaluated on all pixels, leading to superﬂuous compu-tations and unnecessarily expensive optimization and inference.
We propose latent diffusion models (LDMs) as an effective gener-ative model and a separate mild compression stage that only elim-inates imperceptible details. Data and images from [29]. applied to high-resolution synthesis of megapixel images. (ii) We achieve competitive performance on multiple tasks (unconditional image synthesis, inpainting, stochastic super-resolution) and datasets while signiﬁcantly lowering computational costs. Compared to pixel-based diffusion ap-proaches, we also signiﬁcantly decrease inference costs. (iii) We show that, in contrast to previous work [90] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not re-quire a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space. (iv) We ﬁnd that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of ∼ 10242 px. (v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models. (vi) Finally, we release pretrained latent diffusion and autoencoding models at https : / / github . com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [78]. 2.