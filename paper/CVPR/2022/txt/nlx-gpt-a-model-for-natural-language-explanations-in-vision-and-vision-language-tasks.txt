Abstract
Natural language explanation (NLE) models aim at ex-plaining the decision-making process of a black box sys-tem via generating natural language sentences which are human-friendly, high-level and fine-grained. Current NLE models1 explain the decision-making process of a vision or vision-language model (a.k.a., task model), e.g., a VQA model, via a language model (a.k.a., explanation model), e.g., GPT. Other than the additional memory resources and inference time required by the task model, the task and ex-planation models are completely independent, which disas-sociates the explanation from the reasoning process made to predict the answer. We introduce NLX-GPT, a gen-eral, compact and faithful language model that can si-multaneously predict an answer and explain it. We first conduct pre-training on large scale data of image-caption pairs for general understanding of images, and then for-mulate the answer as a text prediction task along with the explanation. Without region proposals nor a task model, our resulting overall framework attains better eval-uation scores, contains much less parameters and is 15× faster than the current SoA model. We then address the problem of evaluating the explanations which can be in many times generic, data-biased and can come in several forms. We therefore design 2 new evaluation measures: (1) explain-predict and (2) retrieval-based attack, a self-evaluation framework that requires no labels. Code is at: https://github.com/fawazsammani/nlxgpt. 1.

Introduction
Deep learning models have enabled extraordinary break-throughs in a variety of vision tasks (such as image classifi-cation [15,17,26]) and vision-language tasks (such as visual question answering [1, 3, 57], visual entailment [56], image 1Throughout this paper, we refer to NLE models as Natural Language
Explanation models aimed for vision and vision-language tasks.
Figure 1. A comparison between previous models (left) and ours (right). Our model solely requires a visual encoder and a language model. We model the answer as a text prediction task along with the explanation. Best viewed in color. captioning [11, 34, 43, 53], and more), achieving promising performance. However, they are black box systems. For these models to be deployed in everyday life, explaining their decision-making process becomes critical for several reasons such as trust, accountability, and model bias under-standing and correctness. Different from visual or textual explanations which highlight regions or tokens in an image or sentence that lead to a specific prediction [5, 45, 48, 49], natural language explanation models [8, 33] explain the decision-making process of a model through natural lan-guage sentences. These sentences are easy to understand by humans and are much more detailed than highlighted regions or tokens. Recently, NLE for vision and vision-language (VL) tasks has been introduced [20, 32, 36, 54].
In this work, we focus on explaining models aimed for vision and vision-language tasks. Current NLE models
[20, 32, 36, 54] first utilize a VL-model to get an answer for the task at hand (e.g., a visual question answering (VQA) model). The outputs of the VL-model (answer and mul-timodal features) along with the question are then fed to language model (e.g., LSTM or Transformer) to get an ex-planation for the answer (see Figure 1). At training time, the language model is trained to produce explanations for the ground-truth answers with a NLE dataset. At test time, the output of a VL-model is utilized to predict an answer which is fed to the language model to get the explanation.
This paradigm has two disadvantages. First, the addition of the task model requires higher storage and memory re-quirements (typically, approx. 80M and 300M parameters for small and large models, respectively). Second, the VL-model and language model are completely independent of each other, which disconnects the explanations from the reasoning process made to predict the answer. Finally, au-tomatic NLE measures that evaluate the generated explana-tion do not always reflect the correctness, reasoning and se-mantic meaning of the explanations since explanations can come in different forms and may learn correlations and bias in the dataset.
In summary, we make the following contributions:
• We propose NLX-GPT, a model which can simultane-ously predict an answer and explain it, by formulating the answer prediction as a text generation task along with the explanation. This eliminates the need for a
VL-model to provide an answer and associates the ex-planation with the reasoning process made to predict the answer.
• Our method outperforms previous works on most met-rics while being 15× faster and requiring less memory resources. We further present an ablation analysis of the steps and components of our model, demonstrating that each aspect contributes non-trivially to the final performance of the model.
• We present two new evaluation frameworks for NLE which can reflect the correctness, reasoning, semantic meaning and the degree of biasness of the generated explanations 2.