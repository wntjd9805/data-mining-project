Abstract
Deep neural networks (DNNs) have witnessed great suc-cesses in semantic segmentation, which requires a large number of labeled data for training. We present a novel learning framework called Uncertainty guided Cross-head
Co-training (UCC) for semi-supervised semantic segmen-tation. Our framework introduces weak and strong aug-mentations within a shared encoder to achieve co-training, which naturally combines the benefits of consistency and self-training. Every segmentation head interacts with its peers and, the weak augmentation result is used for su-pervising the strong. The consistency training samples’ di-versity can be boosted by Dynamic Cross-Set Copy-Paste (DCSCP), which also alleviates the distribution mismatch and class imbalance problems. Moreover, our proposed
Uncertainty Guided Re-weight Module (UGRM) enhances the self-training pseudo labels by suppressing the effect of the low-quality pseudo labels from its peer via model-ing uncertainty. Extensive experiments on Cityscapes and
PASCAL VOC 2012 demonstrate the effectiveness of our
UCC. Our approach significantly outperforms other state-of-the-art semi-supervised semantic segmentation methods.
It achieves 77.17%, 76.49% mIoU on Cityscapes and PAS-CAL VOC 2012 datasets respectively under 1/16 proto-cols, which are +10.1%, +7.91% better than the supervised baseline. 1.

Introduction
Image semantic segmentation is an important and hot topic in the computer vision field, which can be applied to autonomous driving [24], medical image processing and smart city.
In the past few years, semantic segmentation methods based on deep neural networks (DNNs) have made tremendous progress, such as [3, 5, 20, 30]. However, most of these methods involve pixel-level manual labeling, which is quite expensive and time-consuming.
To effectively utilize unlabeled images, the consistency regularization based methods have been widely used in
*Corresponding Author: jianglihui1@huawei.com semi-supervised learning [11, 18, 27]. It promotes the net-work to generate similar predictions for the same unlabeled image with different augmentations by calculating the dif-ference between outputs as the loss function. Among them, data augmentation is commonly used in consistency regu-larization, and [29, 36] serve a pool of data augmentation policies by designing the search space. Additionally, Fix-Match [26] shows its effectiveness by enforcing consistency constraints on predictions generated by weak and strong augmentation. Despite the success of consistency regular-ization, we find that the network performance tends to reach its bottleneck in the high-data regime.
Another semi-supervised learning method, self-training,
It incorporates pseudo la-could fully use massive data. bels on the unlabeled images obtained from the segmen-tation model to guide its learning process and then re-trains the segmentation model with both labeled and un-labeled data. However, the traditional self-training proce-dure has its inherent drawbacks: the noise of pseudo la-bels may accumulate and influence the whole training pro-cess. As an extension of self-training, co-training [23, 42] lets multiple individual learners learn from each other in-stead of collapsing to themselves. To fully take the merit of consistency regularization and co-training, we propose a Cross-head Co-training learning framework incorporated with weak and strong augmentation. Comparing multiple models, we could achieve co-training with minimal addi-tional parameters by the shared encoder, which enforces constraints on different learners and avoids them from con-verging in the opposite direction.
Our method also benefits from the learners’ diversity.
Co-training will fall into self-training without diversity, and consistency training on the same prediction will also be meaningless for the lack of diversity. The diversity inher-ently comes from the randomness in the strong augmenta-tion function (the unlabeled examples for two heads are dif-ferently augmented and pseudo-labeled) and different learn-ers’ initialization. Copy-Paste (CP) is also an alternative way to boost training samples’ diversity, and recent work
[42] has proved its effectiveness. However, the plain CP has its inherent drawback caused by two problems. The first is the distribution mismatch between labeled data and
unlabeled data. The second is the class imbalance prob-lem, most current semantic segmentation datasets [7,9] con-tain long-tailed categories. By extending Copy-Paste (CP) into Dynamic Cross-Set Copy-Paste (DCSCP), our method could not only boost consistency training samples’ diversity but also reduce the misalignment between two sets’ sam-ples and address the class imbalance problem via preserv-ing long-tailed samples. Meanwhile, to reduce the negative effect of noisy predictions brought by self-training, Uncer-tainty Guided Re-weight Module (UGRM) is applied to un-supervised loss to dynamically give more weight to reliable samples while suppressing the noisy pseudo labels in self-training.
By combining data-augmentation strategies followed by the uncertainty re-weight module, we develop Uncertainty guided Cross-head Co-training, which combines consis-tency and co-training naturally. Experimental results with various settings on two benchmarks, Cityscapes and PAS-CAL VOC 2012, show that the proposed approach achieves the state-of-the-art semi-supervised segmentation perfor-mance.
Our contributions can be summarised as follow:
•We propose a novel framework UCC, which introduces weak and strong augmentations into the cross-head co-training framework. Through the shared module, we can further improve the generalization ability and learn a more compact feature representation from two different views.
• A method called DCSCP is proposed to boost consis-tency training samples’ diversity while simultaneously re-ducing distribution misalignment and addressing the class imbalance problem. Furthermore, we propose UGRM to tackle the noise of pseudo labels brought by self-training.
• We validate the proposed method on Cityscapes and
PASCAL VOC 2012 dataset, which significantly outper-forms other state-of-the-art methods with all the labeled data ratios. 2.