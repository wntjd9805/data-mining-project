Abstract
Deep Neural Networks (DNNs) usually have a large number of parameters and consume a huge volume of storage space, which limits the application of DNNs on memory-constrained devices. Network quantization is an appealing way to compress DNNs. However, most of exist-ing quantization methods require the training dataset and a fine-tuning procedure to preserve the quality of a full-precision model. These are unavailable for the confiden-tial scenarios due to personal privacy and security prob-lems. Focusing on this issue, we propose a novel data-free method for network compression called PNMQ, which em-ploys the Parametric Non-uniform Mixed precision Quan-tization to generate a quantized network. During the com-pression stage, the optimal parametric non-uniform quan-tization grid is calculated directly for each layer to mini-mize the quantization error. User can directly specify the re-quired compression ratio of a network, which is used by the
PNMQ algorithm to select bitwidths of layers. This method does not require any model retraining or expensive cal-culations, which allows efficient implementations for net-work compression on edge devices. Extensive experiments have been conducted on various computer vision tasks and the results demonstrate that PNMQ achieves better perfor-mance than other state-of-the-art methods of network com-pression. 1.

Introduction
DNNs have achieved impressive results in a large num-ber of problems from image classification to various gener-ation tasks, etc. But with an increasing number of problems to be solved the models grow in size significantly, architec-tures become more complex, the number of parameters is estimated in hundreds of millions. Unfortunately, in the real world, the use and storage of such models is difficult, espe-cially on various peripheral and mobile devices for which computing and memory resources are a bottleneck. Since the demand for the use of neural networks in various mo-bile processes and applications is growing, there is a great need for matching neural models to the technical param-eters of devices. Many researchers try to offer some ap-proaches to solve this problem, such as making changes to existing model structures or applying various compression techniques to the original structure.
Popular approaches include techniques such as pruning, quantization, encoding methods, knowledge distillation and their various combinations. Now there are a lot of well-known methods and ready-made frameworks that can sig-nificantly reduce the model size, but to keep a high quality of the compressed model, many of these methods require a lot of training data and expensive calculations such as model retraining, which is a significant drawback. Methods that do not use expensive calculations for compression often pro-duce a significant quality drop of the compressed models, since the use of uniform quantization without model train-ing leads to a low quality of approximation from quantiza-tion for most of the weights.
In this paper, we propose a novel method of network compression called PNMQ, which does not use expensive calculations such as gradient descent training or clustering, and at the same time allows us to achieve better results com-pared to existing data-free and training-free compression methods, and even to some methods that use data, model training or other additional compression techniques. First of all, we propose a parametric family of non-uniform quan-tization grids that depends on a single scalar parameter and is therefore easily optimized. The use of the proposed non-uniform quantization grids significantly improves the qual-ity of quantization approximation relative to the uniform quantization. We provide an optimization pipeline, which allows to tune these non-uniform grids for each model layer, both without using data and using a small amount of data.
In addition, we demonstrate that different layers of models require different bitwidths for a high degree of quantiza-tion approximation, and propose a universal data-free algo-rithm for selecting the sufficient bitwidths of layers based on comparison of the quantization errors of different layers and bitwidths. This algorithm automatically selects the op-timal bitwidths for different layers to achieve the required compression ratio that the user can specify directly, and this is a very convenient feature of the proposed method. PNMQ can be applied to any model, does not depend on the layer types and does not require changing the network structure.
Moreover, PNMQ is compatible with most existing weight compression techniques such as weights pruning, transfor-mations or lossless coding. 2.