Abstract
The capability of the traditional semi-supervised learn-ing (SSL) methods is far from real-world application due to severely biased pseudo-labels caused by (1) class imbal-ance and (2) class distribution mismatch between labeled and unlabeled data. This paper addresses such a rela-tively under-explored problem. First, we propose a general pseudo-labeling framework that class-adaptively blends the semantic pseudo-label from a similarity-based classifier to the linear one from the linear classifier, after making the observation that both types of pseudo-labels have comple-mentary properties in terms of bias. We further introduce a novel semantic alignment loss to establish balanced fea-ture representation to reduce the biased predictions from the classifier. We term the whole framework as Distribution-Aware Semantics-Oriented (DASO) Pseudo-label. We con-duct extensive experiments in a wide range of imbalanced benchmarks: CIFAR10/100-LT, STL10-LT, and large-scale long-tailed Semi-Aves with open-set class, and demonstrate that, the proposed DASO framework reliably improves SSL learners with unlabeled data especially when both (1) class imbalance and (2) distribution mismatch dominate. 1.

Introduction
Semi-supervised learning (SSL) [7] has shown to be promising for leveraging unlabeled data to reduce the cost of constructing labeled data [4, 5, 36, 40, 57] and even boost the performance at scale [29, 49, 67, 68]. The common approach of these algorithms is to produce pseudo-labels for unlabeled data based on modelâ€™s predictions and uti-lize them for regularizing model training [29, 38, 57]. Al-though adopted in a variety of tasks, these algorithms often assume class-balanced data, while many real-world datasets exhibit long-tailed distributions [3, 18, 31, 32]. With class-imbalanced data, the class distribution of pseudo-labels from unlabeled data becomes severely biased to the ma-jority classes due to confirmation bias [2]. Such biased pseudo-labels can further bias the model during training.
Figure 1. Glimpse of the DASO framework. DASO reduces the overall bias in pseudo-labels (PL) from unlabeled data by blend-ing two complementary PLs from different classifiers. Note that bias is conceptually illustrated as relative PL size (Rel. PL size), meaning that pseudo-label size is normalized by actual label size.
Many methods of handling class-imbalanced labels have been proposed in the supervised learning community, but little interest has been made in re-balancing pseudo-labels in SSL. Recent studies have explored this imbalanced SSL setting, where as a reference to the class distribution of unla-beled data, it is often assumed that it is the same as the class distribution of labels [33,64], or a separate distribution esti-mate is required [33]. However, the actual class distribution of unlabeled data is unknown without the labels. For ex-ample, unlabeled data may have large class distribution gap from labeled data, including many samples in novel classes not defined in the label set [58]. As we elaborate in Sec. 4, the bias of pseudo-labels also depends on such class distri-bution mismatch between labeled and unlabeled data, and using inaccurate estimates or wrong assumptions about the unlabeled data cannot be helpful under imbalanced SSL.
In this work, we present a new imbalanced SSL method specifically tailored for alleviating the bias in pseudo-labels under class-imbalanced data, while discarding the common assumption that the class distribution of unlabeled data is the same with the label distribution. To this end, as shown in Fig. 1, we observe that semantic pseudo-labels [22] ob-tained from a similarity-based classifier [56] are biased to-wards minority classes as opposed to linear classifier-based
pseudo-labels [38, 57] being biased towards head classes.
As illustrated in Sec. 3.2, we draw the key inspiration from those complementary properties of two different types of pseudo-labels to develop a new pseudo-labeling scheme.
In this regard, we introduce a generic imbalanced SSL framework termed Distribution-Aware Semantics-Oriented (DASO) Pseudo-label in Sec. 3.3. Building upon the exist-ing SSL learner, we propose to blend the linear and seman-tic pseudo-labels in different proportions for each class to reduce the overall bias. This blending strategy can provide a more balanced supervision than simply using either of the pseudo-label. The primary novelty comes from the schedul-ing of the weights for mixing the pseudo-labels. Specifi-cally, we dynamically adjust the relative weights of seman-tic pseudo-labels to be blended so that linear pseudo-labels are less biased according to the current class distribution of pseudo-labels. By virtue of such mechanism, without re-sorting to any class priors for the unlabeled data, DASO reliably brings performance gain even with substantial class distribution mismatch between labeled and unlabeled data.
We further propose a simple yet effective semantic align-ment loss to establish balanced feature representation via balanced class prototypes, which is the extension of the consistency regularization framework in [57, 66] onto fea-ture space. We align the unlabeled data onto each of the similar prototypes, by consistently assigning two different views of an unlabeled sample in feature space to the same prototype. These enhanced feature representations not only help linear classifier produce less biased predictions, but can also be reused for semantic pseudo-labels from similarity-based classifier. We validate the semantic alignment loss is useful under imbalanced SSL, especially helpful for DASO.
The efficacy of DASO is extensively justified with the imbalanced versions of benchmarks: CIFAR-10/100 [35] and STL-10 [12] in Sec. 4. We even test DASO with large-scale long-tailed Semi-Aves [58] with open-set classes in unlabeled data, closely related to real-world scenarios. As such, DASO consistently benefits under various distribu-tions of unlabeled data and degrees of imbalance, demon-strating to be a truly generic framework that works well on top of diverse frameworks such as existing SSL learners and even other re-balancing frameworks for labels and SSL.
The key contributions in our work can be summa-rized as follows: (1) We propose a novel pseudo-labeling framework, DASO, for debiasing pseudo-labels by class-adaptively blending two complementary types of pseudo-labels observing current class distribution of pseudo-labels. (2) DASO introduces semantic alignment loss to further al-leviate the bias from high-quality feature representation, by aligning each unlabeled example to the similar prototype. (3) DASO readily integrates with other frameworks to show significant performance improvements under diverse imbal-anced SSL setup, including the most practical scenario. 2.