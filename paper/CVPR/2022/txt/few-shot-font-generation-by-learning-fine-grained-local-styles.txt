Abstract
Few-shot font generation (FFG), which aims to generate a new font with a few examples, is gaining increasing at-tention due to the significant reduction in labor cost. A typ-ical FFG pipeline considers characters in a standard font library as content glyphs and transfers them to a new tar-get font by extracting style information from the reference glyphs. Most existing solutions explicitly disentangle con-tent and style of reference glyphs globally or component-wisely. However, the style of glyphs mainly lies in the local details, i.e. the styles of radicals, components, and strokes together depict the style of a glyph. Therefore, even a sin-gle character can contain different styles distributed over spatial locations. In this paper, we propose a new font gen-eration approach by learning 1) the fine-grained local styles from references, and 2) the spatial correspondence between the content and reference glyphs. Therefore, each spatial location in the content glyph can be assigned with the right fine-grained style. To this end, we adopt cross-attention over the representation of the content glyphs as the queries and the representations of the reference glyphs as the keys and values.
Instead of explicitly disentangling global or component-wise modeling, the cross-attention mechanism can attend to the right local styles in the reference glyphs and aggregate the reference styles into a fine-grained style representation for the given content glyphs. The experi-ments show that the proposed method outperforms the state-of-the-art methods in FFG. In particular, the user studies also demonstrate the style consistency of our approach sig-nificantly outperforms previous methods. 1.

Introduction
In the modern era, both computer systems and humans process huge amounts of text information. Fonts, the rep-resentations of text, have thus played critical roles in many
*Equal contribution.
†Corresponding author.
Figure 1. Our proposed fine-grained local style extraction and style aggregation process. Our proposed module enables fine-grained style extraction from references and learns the correspon-dence between content and reference, thus aggregate correspond-ing local styles into correct locations in content with high-fidelity. applications. Therefore, the stylish font generation has its unique commercial and artistic values. However, building commercial font libraries is costly and labor-intensive. The cost is even higher for those languages with a huge amount of characters (Chinese, Japanese Kanji, Korean, Thai, etc.).
Due to the expert’s high cost of building fonts, automatic font generation with deep learning has drawn rising atten-tion.
It aims at generating a brand new font library with only a few characters as a reference. With the development of Generative Adversarial Networks (GANs) [10, 20], there have been many classic works of font generation. Early at-tempts, such as zi2zi [26], use Pix2Pix [14] like networks with a plug-in font category embedding conditions to learn multiple font styles with a single model. However, these methods require a large number of glyphs to train each un-seen font.
In recent years, some works tried to tackle few-shot font generation (FFG) with a few-shot Image-to-Image transla-tion (I2I) scheme [4,8,22,23,31]. Unlike zi2zi, the font style representation is learned from a few reference exemplars, rather than learning embeddings from different font labels.
One popular strategy of these works is to explicitly disen-tangle the content and style representations from given con-tent images and reference exemplars, and two representa-tions are then combined and decoded into the target glyphs.
With the advance of these works, the generated glyphs’
quality is significantly improved when the number of ref-erences is limited. Based on the explicit disentanglement ideas, the research of FFG can be divided into two different categories, i.e. global style representation and component-wise style representation. The former one models the glyph style as universal representation for each font [8, 18, 31], while the latter one utilizes component-wise style repre-sentation from different reference exemplars in the same font [4, 13, 22, 23].
However, in a commercial font, multiple levels of styles need to be considered. An expert would carefully design ev-ery possible detail. The detailed styles among components, strokes and, even edges are designed to be consistent. Pre-vious works mostly focus on component-wise styles, while largely ignoring the finer-grained styles. Meanwhile, since the content and style are highly entangled, the commonly used explicit disentanglement can hardly assure the con-sistency of component-wise styles between the reference glyphs and the generated glyphs. To this end, we employ an references encoder to learn the fine-grained local styles (FLS) without explicit disentangled representation learning.
Instead of regarding the overall reference map as style, we consider each spatial location of the feature map as a FLS representation of reference glyph. After learning the spatial correspondence between references and content, we further acquire the target style map by aggregating the correspond-ing FLSs from references. Each feature vector of the target style map also represents FLSs for the target glyph.
In this paper, we propose a novel approach shown in Figure 1, named FSFont for few-shot font generation.
The reference glyph images are encoded into reference maps, which represent the FLSs of the references. Our proposed cross-attention based style aggregation module (SAM) learns the spatial correspondence between refer-ences and the content glyph. The spatial correspondence is not only on the component level but also on the granular level, which contains more detailed local styles. Afterward,
SAM aggregates the FLSs of references into the target style map, where each spatial location can be assigned to the right fine-grained style. Moreover, to enhance the model to recover details of the references better and learn corre-spondence more effectively, we adapt a self-reconstruction branch that takes the target character as the input of the ref-erence encoder, and the generated result is supervised by it-self. This branch makes learning the correspondence more easily, and helps to produce highly consistent output. Last but not least, we develop a strategy to select the references for each glyph automatically. After analyzing the compo-sitional rules, we design a breadth-first search-based algo-rithm to search for the reference set and find the optimal references for each character.
In summary, the contribution of this paper is threefold:
• We devise a novel model for few-shot generation. The model extracts the FLS of the reference glyph images, and a cross-attention based style aggregation module aggregates the reference styles into a target style map.
The details from reference glyphs are thereby trans-ferred to the target glyph.
• We propose a unified training framework with a newly designed self-reconstruction branch. This branch sig-nificantly boosts the detail capture ability of the model and improves the output images’ quality. As a result, the proposed full model achieves state-of-the-art font generation results.
• We analyze the relationship between characters and se-lect a fairly small set of characters as references. Then we develop a rule to map each character with the ele-ments in the reference set. With the proposed rule, the model’s ability to extract component features is better exploited. 2.