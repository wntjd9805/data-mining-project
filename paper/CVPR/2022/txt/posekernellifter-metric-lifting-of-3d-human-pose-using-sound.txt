Abstract
Reconstructing the 3D pose of a person in metric scale from a single view image is a geometrically ill-posed prob-lem. For example, we can not measure the exact distance of a person to the camera from a single view image without additional scene assumptions (e.g., known height). Exist-ing learning based approaches circumvent this issue by re-constructing the 3D pose up to scale. However, there are many applications such as virtual telepresence, robotics, and augmented reality that require metric scale reconstruc-tion.
In this paper, we show that audio signals recorded along with an image, provide complementary information to reconstruct the metric 3D pose of the person. The key insight is that as the audio signals traverse across the 3D space, their interactions with the body provide metric in-formation about the body’s pose. Based on this insight, we introduce a time-invariant transfer function called pose kernel—the impulse response of audio signals induced by the body pose. The main properties of the pose kernel are that (1) its envelope highly correlates with 3D pose, (2) the time response corresponds to arrival time, indicating the metric distance to the microphone, and (3) it is invariant to changes in the scene geometry configurations. There-fore, it is readily generalizable to unseen scenes. We design a multi-stage 3D CNN that fuses audio and visual signals
* Work performed solely as a member of Samsung AI Center NY despite some authors being affiliated with universities. and learns to reconstruct 3D pose in a metric scale. We show that our multi-modal method produces accurate met-ric reconstruction in realworld scenes, which is not possible with state-of-the-art lifting approaches including paramet-ric mesh regression and depth regression. 1.

Introduction
Since the projection of the 3D world onto an image loses scale information, 3D reconstruction of a human’s pose from a single image is an ill-posed problem. To address this limitation, human pose priors have been used in existing lifting approaches [6,20,31,34,43,60,75] to reconstruct the plausible 3D pose given the 2D detected pose by predicting relative depths. The resulting reconstruction, nonetheless, still lacks metric scale, i.e., the metric scale cannot be re-covered without making an additional assumption such as known height or ground plane contact. This fundamen-tal limitation of 3D pose lifting precludes applying it to realworld downstream tasks, e.g., smart home facilitation, robotics, and augmented reality, where the precise metric measurements of human activities are critical, in relation to the surrounding physical objects.
In this paper, we study a problem of metric human pose reconstruction from a single view image by incorporating a new sensing modality—audio signals from consumer-grade speakers (Figure 1). Our insight is that while traversing a 3D environment, the transmitted audio signals undergo a
characteristic transformation induced by the geometry of re-flective physical objects including human body. This trans-formation is subtle yet highly indicative of body pose ge-ometry, which can be used to reason about the metric scale reconstruction. For instance, the same music playing in a room sounds differently based on the presence or absence of a person, and more importantly, as the person moves.
We parametrize this transformation of audio signals us-ing a time-invariant transfer function called pose kernel—an impulse response of audio induced by a body pose, i.e., the received audio signal is a temporal convolution of the trans-mitted signal with the pose kernel. Three key properties of pose kernel enables metric 3D pose lifting in a gener-alizable fashion: (1) metric property: its impulse response is equivalent to the arrival time of the reflected audio, and therefore, it provides metric distance from the receiver (mi-crophone); (2) uniqueness: the envelope of pose kernel is strongly correlated with the location and pose of the target person; (3) invariance: it is invariant to the geometry of sur-rounding environments, which allows us to generalize it to unseen environments.
While highly indicative of pose and location of the per-son in 3D, the pose kernel is a time-domain signal. Integrat-ing it with spatial-domain 2D pose detection is non-trivial.
Further, generalization to new scenes requires precise 3D reasoning where existing audio-visual learning tasks such as source separation in an image domain and image repre-sentation learning [13, 16, 42, 59] are not applicable.
We address this challenge in 3D reasoning of visual and audio signals, by learning to fuse the pose kernels from mul-tiple microphones and the 2D pose detected from an image, using a 3D convolutional neural network (3D CNN): (1) we project each point in 3D onto the image to encode the like-lihood of landmarks (visual features); and (2) we spatially encode the time-domain pose kernel in 3D to form audio features. Inspired by the convolutional pose machine archi-tecture [70], a multi-stage 3D CNN is designed to predict the 3D heatmaps of the joints given the visual and audio features. This multi-stage design increases effective recep-tive field with a small convolutional kernel (e.g., 3 × 3 × 3) while addressing the issue of vanishing gradients.
In addition, we present a new dataset called PoseKer-nel dataset. The dataset includes more than 10,000 poses from six locations with more than six participants per lo-cation, performing diverse daily activities including sitting, drinking, walking, and jumping. We use this dataset to evaluate the performance of our metric lifting method and show that it significantly outperforms state-of-the-art lift-ing approaches including mesh regression (e.g., FrankMo-cap [49]) and joint depth regression (e.g., Tome et al. [60]).
Due to the scale ambiguity of state-of-the-art approaches, the accuracy is dependent on the heights of target persons.
In contrast, our approach can reliably recover 3D poses re-gardless the heights, applicable to both adults and minors.
Why Metric Scale? Smart home technology is poised to enter our daily activities, in particular, for monitoring frag-ile populations including children, patients, and the elderly.
This requires not only 3D pose reconstruction but also holis-tic 3D understanding in the context of metric scenes, which allows AI and autonomous agents to respond in a situation-aware manner. While multiview cameras can provide metric reconstruction, the number of required cameras to cover the space increases quadratically as area increases. Our novel multi-modal solution can mitigate this challenge by leverag-ing multi-source audios (often inaudible) generated by con-sumer grade speakers (e.g., Alexa).
Contributions This paper makes a major conceptual con-tribution that sheds a new light on a single view pose esti-mation by incorporating with audio signals. The technical contributions include (1) a new formulation of the pose ker-nel that is a function of the body pose and location, which can be generalized to a new scene geometry, (2) the spatial encoding of pose kernel that facilitates fusing visual and audio features, (3) a multi-stage 3D CNN architecture that can effectively fuse them together, and (4) a strong perfor-mance of our method, outperforming state-of-the-art lifting approaches with meaningful margin. 2.