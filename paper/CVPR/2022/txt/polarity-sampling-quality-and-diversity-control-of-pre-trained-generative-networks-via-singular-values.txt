Abstract
We present Polarity Sampling, a theoretically justified plug-and-play method for controlling the generation quality and diversity of any pre-trained deep generative network (DGN). Leveraging the fact that DGNs are, or can be ap-proximated by, continuous piecewise affine splines, we de-rive the analytical DGN output space distribution as a func-tion of the product of the DGN’s Jacobian singular val-ues raised to a power ρ. We dub ρ the polarity param-eter and prove that ρ focuses the DGN sampling on the modes (ρ < 0) or anti-modes (ρ > 0) of the DGN output-space probability distribution. We demonstrate that nonzero polarity values achieve a better precision-recall (quality-diversity) Pareto frontier than standard methods, such as truncation, for a number of state-of-the-art DGNs. We also present quantitative and qualitative results on the improve-ment of overall generation quality (e.g., in terms of the
Fr´echet Inception Distance) for a number of state-of-the-art DGNs, including StyleGAN3, BigGAN-deep, NVAE, for different conditional and unconditional image generation tasks. In particular, Polarity Sampling redefines the state-of-the-art for StyleGAN2 on the FFHQ Dataset to FID 2.57,
StyleGAN2 on the LSUN Car Dataset to FID 2.27 and Style-GAN3 on the AFHQv2 Dataset to FID 3.95. Colab Demo. 1.

Introduction
Deep Generative Networks (DGNs) have emerged as the go-to framework for generative modeling of high-dimensional datasets, such as natural images. Within the realm of DGNs, different frameworks can be used to pro-duce an approximation of the data distribution, e.g., Gen-erative Adversarial Networks (GANs) [18], Variational Au-toEncoders (VAEs) [31] or flow-based models [40]. But despite the different training settings and losses that each of these frameworks aim to minimize, the evaluation met-ric of choice that is used to characterize the overall quality of generation is the Fr´echet Inception Distance (FID) [22].
∗equal contribution
) y t i l a u q ( n o i s i c e r p recall (diversity)
ρ ≪ 0
ρ < 0
ρ > 0
ρ ≫ 0
Figure 1. First row: Evolution of generation quality and diversity for varying truncation [29] ψ and polarity ρ. Polarity Sampling achieves a better Pareto trade-off than truncation, e.g., polarity can be used to achieve a specified recall at higher precision or a spec-ified precision at higher recall, compared to truncation. For addi-tional Pareto examples, see Fig. 3. Second, Third, and Fourth row: Samples obtained from BigGAN-deep on Golden Retriever,
Tiger and House Finch classes of Imagenet with samples of greater quality (ρ < 0) and greater diversity (ρ > 0). For examples with
LSUN [54], see Fig. 4.
The FID is obtained by taking the Fr´echet Distance in the
InceptionV3 [48] embedding space between two distribu-tions; the distributions are usually taken to be the training dataset and samples from a DGN trained on the dataset.
It has been established in prior work [45] that FID non-linearly combines measures of quality and diversity of the samples, which has inspired further research into disentan-glement of these quantities as precision and recall [32, 45] metrics respectively.
Recent state-of-the-art DGNs such as BigGAN [8],
StyleGAN2/3 [28, 30], and NVAE [53], have reached FIDs nearly as low as one could obtain when comparing sub-sets of real data with themselves. This has led to the deployment of DGNs in a variety of applications, such as real-world high-quality content generation and data-augmentation. However, it is clear that, depending on the domain of application, generating samples from the best
FID model could be suboptimal. For example, realistic content generation might benefit more from high-quality (precision) samples, while data-augmentation might ben-efit more from samples of high-diversity (recall), even if in each case, the overall FID slightly diminishes [16, 25].
Therefore, a number of state-of-the-art DGNs have intro-duced a controllable parameter to trade-off between the pre-cision and recall of the generated samples, e.g., truncated latent space sampling [8], interpolating truncation [29, 30].
However, these methods do not always work “out-of-the-box” [8], e.g., BigGAN requires orthogonal regularization of the DGN’s parameters during training. These methods also lack a clear theoretical understanding which can limit their deployment for sensitive applications.
In this paper, we propose a principled solution to con-trol the quality (precision) and diversity (recall) of DGN samples that does not require retraining nor specific conditioning of model training. Our method, termed Po-larity Sampling, builds on our previous work on the analyt-ical form of the learned DGN sample distribution [24] and introduces a new hyperparameter, that we dub the polarity
ρ ∈ R, that adapts the latent space distribution for post-training control. The polarity parameter provably forces the latent distribution to concentrate on the modes of the DGN distribution, i.e., regions of high probability (ρ < 0), or on the anti-modes, i.e., regions of low-probability (ρ > 0); with ρ = 0 recovering the original
DGN distribution. The Polarity Sampling process depends only on the top singular values of the DGN’s output Jaco-bian matrices evaluated at each input sample and can be im-plemented to perform online sampling. A crucial benefit of Polarity Sampling lies in its theoretical derivation from the analytical DGN data distribution [24] where the product of the DGN Jacobian matrices singular values – raised to the power ρ – provably controls the DGN samples distribu-tion as desired. See Fig. 1 for an initial example of Polarity
Sampling in action.
Our main contributions are as follows:
[C1] We first provide the theoretical derivation of Polarity
Sampling based on the singular values of the generator Ja-cobian matrix. We provide pseudocode for Polarity Sam-pling and an approximation scheme to control its computa-tional complexity as desired (Sec. 3).
[C2] We demonstrate on a range of DGNs and datasets that
Polarity Sampling not only enables one to move on the precision-recall Pareto frontier (Sec. 4.1), i.e., it controls the quality and diversity efficiently, but it also reaches im-proved FID scores for each model (Sec. 4.2).
[C3] We leverage the fact that negative Polarity Sam-pling provides access to the modes of the learned DGN distribution, which enables us to explore several timely and important questions regarding DGNs. We provide visualization of the modes of trained GANs and VAEs (Sec. 5.1) and assess the perceptual smoothness around the modes (Sec. 5.2). 2.