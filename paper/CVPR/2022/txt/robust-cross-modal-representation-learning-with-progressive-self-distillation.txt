Abstract
The learning objective of vision-language approach of
CLIP [63] does not effectively account for the noisy many-to-many correspondences found in web-harvested image captioning datasets, which contributes to its compute and data inefﬁciency. To address this challenge, we introduce a novel training framework based on cross-modal con-trastive learning that uses progressive self-distillation and soft image-text alignments to more efﬁciently learn robust representations from noisy data. Our model distills its own knowledge to dynamically generate soft-alignment targets for a subset of images and captions in every minibatch, which are then used to update its parameters. Extensive evaluation across 14 benchmark datasets shows that our method consistently outperforms its CLIP counterpart in multiple settings, including: (a) zero-shot classiﬁcation, (b) linear probe transfer, and (c) image-text retrieval, with-out incurring extra computational cost. Analysis using an
ImageNet-based robustness test-bed [70] reveals that our method offers better effective robustness to natural distribu-tion shifts compared to both ImageNet-trained models and
CLIP itself. Lastly, pretraining with datasets spanning two orders of magnitude in size shows that our improvements over CLIP tend to scale with number of training examples. 1.

Introduction
The convergence of self-supervised pretraining techniques in natural language processing and computer vision have brought about a renaissance of cross-modal representation learning methods [1, 19, 30, 39, 52, 63, 68, 75] where large-scale weakly correlated multimodal data (e.g., image-text pairs) is used to learn cross-modal representations using contrastive learning techniques. In particular, the recently proposed CLIP [63] model has garnered signiﬁcant atten-tion due to its impressive zero-shot recognition ability and excellent transfer performance on downstream tasks.
However, despite their recent success, multimodal pre-training methods like CLIP [63] are data and compute in-*This work was done when the author was an intern at Amazon.
P
I
L
C s r u
O
V1
V2
V3
V1
V2
V3
T1
T2
T3
T1
T2
T3 test image ground truth: goldfish  guinea pig beer glass cockroach pomeranian axolotl 0.04 0.04 0.04 0.18 0.11
C
L
I
P an out-of-distribution  image from ImageNet-R  dataset showing a  goldfish rendered in a  stained glass pane goldfish clown fish axolotl puffer fish snail 0.17 0.01 0.01 0.01 0.67
O u r s pretrained models test image model outputs
Figure 1.
Illustrative Comparison with CLIP – CLIP [63] learns a joint vision-language embedding space by bringing cor-responding image-text representation together (green links), while repelling unpaired instances away from each other (red links).
This formulation does not account for potential semantic similar-ity between negative samples. We address this issue by learning to predict a distribution of soft-alignment targets (dotted blue edges) in a given minibatch, thereby allowing our model to learn more robust representations. This robustness is evident when compar-ing predicted distributions on an out-of-distribution image from
ImageNet-R dataset [25], where unlike CLIP, our method can cor-rectly classify a goldﬁsh rendered in a stained glass pane. efﬁcient. Much of CLIP’s success can be attributed to its voracious appetite for training data, utilizing 400M image-text pairs and an estimated 3, 584 GPU days for pretraining.
As the scale of data increases, pretraining requirements of these methods become increasingly expensive, thereby lim-iting their widespread adoption in a sustainable manner.
This data and compute inefﬁciency of CLIP [63] can be partially attributed to the underlying assumptions it makes about the web-harvested data it uses for training. Sev-eral mainstream vision-language datasets utilize the alt-text HTML attribute of images scraped from archived web pages [9, 67, 71] where captions can often have words un-related to their corresponding image-content [67]. How-ever, CLIP [63] models the caption for each image to be accurately and exclusively related to only that image (see
Figure 1). Moreover, when using larger batch sizes (32K used for CLIP), the likelihood of observing negatives with
high semantic similarity increases which can further de-grade the learned representations especially those associ-ated with shared semantics between faulty negatives [2].
To address this challenge, we propose to model the many-to-many relationships between images of web-harvested datasets and their corresponding captions more accurately using soft probabilities rather than hard pair-ing labels. Speciﬁcally, we propose a simple yet effective framework for robust contrastive language-image pretrain-ing that uses progressive self-distillation and soft image-text alignment targets to more efﬁciently learn from noisy data.
Instead of explicitly ﬁnding, correcting or even pruning noisy correspondences [75, 88], our joint student-teacher model dynamically generates a new set of soft-alignments for a random subset of images and captions in every mini-batch. This enables our method to model many-to-many re-lationships while simultaneously re-calibrating potentially poorly matched instances without needing to identify them.
Over the course of training, our network generates soft-alignments for increasingly large subsets of a minibatch, ef-fectively becoming its own teacher. We identify several key elements that allow the student network to predict its targets without representation collapse or reinforcing its mistakes.
We use multiple pretraining datasets to extensively com-pare our approach to CLIP [63] evaluated on 14 bench-mark datasets, where our approach consistently outper-forms CLIP under multiple settings. Analysis using an
ImageNet-based robustness test-bed [70] shows that our method offers better effective robustness to natural distri-bution shifts compared to both ImageNet-trained models as well as CLIP. Pretraining with datasets spanning two orders of magnitude in size shows that our improvements over CLIP tend to scale with number of training examples.
Lastly, the simplicity of our approach allows it to be readily incorporated into existing and future methods. 2.