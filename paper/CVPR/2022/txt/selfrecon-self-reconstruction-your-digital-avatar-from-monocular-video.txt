Abstract
We propose SelfRecon, a clothed human body recon-struction method that combines implicit and explicit repre-sentations to recover space-time coherent geometries from a monocular self-rotating human video. Explicit methods require a predeﬁned template mesh for a given sequence, while the template is hard to acquire for a speciﬁc subject.
Meanwhile, the ﬁxed topology limits the reconstruction ac-curacy and clothing types. Implicit representation supports arbitrary topology and can represent high-ﬁdelity geometry shapes due to its continuous nature. However, it is difﬁcult to integrate multi-frame information to produce a consistent registration sequence for downstream applications. We pro-pose to combine the advantages of both representations. We utilize differential mask loss of the explicit mesh to obtain the coherent overall shape, while the details on the implicit surface are reﬁned with the differentiable neural rendering.
Meanwhile, the explicit mesh is updated periodically to ad-just its topology changes, and a consistency loss is designed to match both representations. Compared with existing methods, SelfRecon can produce high-ﬁdelity surfaces for arbitrary clothed humans with self-supervised optimization.
Extensive experimental results demonstrate its effectiveness on real captured monocular videos. The source code is available at https://github.com/jby1993/SelfReconCode. 1.

Introduction
Clothed body reconstruction has been an important and challenging research topic in the community for years. In the ﬁlm and gaming industry, high-ﬁdelity human recon-struction usually requires pre-captured templates, multi-camera systems, controlled studios, and long-term works of talented artists. However, these requirements exceed the application scenarios of general customers, such as person-alized avatars for telepresence, AR/VR, anthropometry, and virtual try-on, etc. Therefore, directly reconstruction high-ﬁdelity digital avatar from monocular video will have sig-niﬁcant practical application value.
*Corresponding author
The state-of-the-art marker-less monocular human per-formance capture approaches [17, 18, 52] are mainly de-signed based on explicit mesh representation. They re-quire actor-speciﬁc rigged templates and utilize detected 2D/3D joints and silhouettes to estimate per frame’s pos-ture and non-rigid deformation. DeepCap [18] addition-ally uses multi-view information during training to resolve deep ambiguity and improve tracking accuracy for monoc-ular inference. The explicit representation has some ad-vantages, including space-time coherence and compatibility with existing graphics control pipelines, like texture edit-ing and reposing. Moreover, skinning deformation is suit-able under this paradigm to model the body’s large-scale ar-ticulated deformations. However, actor-speciﬁc templates limit the extension of these methods to unseen human se-quences. For videos of self-rotation humans under rough A-pose, VideoAvatar [2] can estimate general clothed humans with the SMPL+D parametric representation [1–3,6,31,49], while it can not recover folds and loose clothing, like skirts.
Recently, some neural implicit representation based monocular human reconstruction approaches have demon-strated compelling results [10, 20, 21, 23, 41, 42, 46, 50, 55, 56]. These methods can handle various topologies, and thus can represent various clothing and hairstyles. How-ever, they require high-quality 3D data for supervision, and they only reconstruct for a speciﬁc frame and can not keep the space-time coherence of surface vertices for the whole sequence. A simple solution to guarantee the coherence and correct body structure is to maintain an implicit tem-plate surface in the canonical space, and then utilize back-ward deformation ﬁelds to map current points to canonical space to assist their implicit function queries. The back-ward deformation strategy has been widely applied recently and works well for small-scale deformations [29,36,40,45].
However, it is not very suitable for articulated skinning de-formation due to its irreversibility in some parts of current space [11, 24]. To this end, technologies such as pose-related skinning weights prediction [24, 38, 47] and speciﬁc inverse articulated deformation design [13] are proposed at the cost of high complexity and poor generalization.
In this work, we propose SelfRecon, which combines the explicit and implicit representations together to reconstruct
high-ﬁdelity digital avatar from a monocular video. Specif-ically, SelfRecon utilizes a learnable signed distance ﬁeld (SDF) rather than a template with ﬁxed topology to repre-sent the canonical shape. To improve the generalization of the deformation and reduce the optimization difﬁculty, we adopt the forward deformation to map canonical points to the current frame space [11, 54]. During optimization, we periodically extract the explicit canonical mesh and warp it to each frame with the deformation ﬁelds. For these meshes, we utilize mask loss and smooth constraints to recover the overall shape. For the implicit part, a differential formula-tion is designed to intersect the deformed surface and fol-low IDR’s neural rendering [53] to reﬁne the geometry. A consistency loss is designed to match both geometric repre-sentations as close as possible.
SelfRecon alleviates the dependence on actor-speciﬁc templates and extracts a space-time coherent mesh sequence from a monocular video. Extensive evaluations on self-rotating human videos demonstrate that it outperforms ex-isting methods. We believe that SelfRecon will inspire more studies on combining implicit and explicit representations for 3D reconstruction for articulated object. 2.