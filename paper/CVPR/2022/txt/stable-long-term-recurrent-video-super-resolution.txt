Abstract
Recurrent models have gained popularity in deep learn-ing (DL) based video super-resolution (VSR), due to their increased computational efficiency, temporal receptive field and temporal consistency compared to sliding-window based models. However, when inferring on long video se-quences presenting low motion (i.e. in which some parts of the scene barely move), recurrent models diverge through recurrent processing, generating high frequency artifacts.
To the best of our knowledge, no study about VSR pointed out this instability problem, which can be critical for some real-world applications. Video surveillance is a typical ex-ample where such artifacts would occur, as both the camera and the scene stay static for a long time.
In this work, we expose instabilities of existing recur-rent VSR networks on long sequences with low motion. We demonstrate it on a new long sequence dataset Quasi-Static
Video Set, that we have created. Finally, we introduce a new framework of recurrent VSR networks that is both sta-ble and competitive, based on Lipschitz stability theory. We propose a new recurrent VSR network, coined Middle Re-current Video Super-Resolution (MRVSR), based on this framework. We empirically show its competitive perfor-mance on long sequences with low motion. 1.

Introduction
Video super-resolution (VSR) is an inverse problem that extends single-image super-resolution (SISR). While SISR aims to generate a high-resolution (HR) image from its low-resolution (LR) version, in VSR the goal is to reconstruct a sequence of HR images from the sequence of their LR coun-terparts. The idea behind VSR, which makes it fundamen-tally different from SISR, is that the fusion of several LR images produces an HR image. Therefore, VSR requires to accumulate information over a number of LR frames as (a) GT (b) Bicubic (c) RLSP (d) MRVSR (Ours)
Figure 1. A comparison between a state-of-the-art recurrent VSR network (RLSP) and our proposed network. The former generates high frequency artifacts on long sequences with low motion. The proposed network does not. large as possible. Classical VSR methods based on the im-age sequence formation model, knowledge on motion and iterative algorithms [2,12] could fill this requirement. How-ever, these iterative algorithms are relatively slow and not suitable for real-world applications. Moreover, they per-form poorly when the image sequence formation model and the assumptions on motion are too simplified.
VSR has recently benefited from DL methods [3,7–9,20, 26, 29] that can overcome some of the drawbacks of clas-sical methods. Deep VSR networks can efficiently learn complex spatio-temporal statistics from a training dataset of natural videos, and once trained the reconstruction is faster. There are broadly two classes of deep VSR methods.
The first one groups sliding-window based models. These models [8, 9, 13, 26, 29] take a batch of multiple LR frames as input to fuse them and reconstruct an HR frame. In most cases, this batch contains 5 to 7 LR frames. Therefore, the temporal receptive field—i.e. the number of LR frames that are used in order to super-resolve a frame—is limited to 7.
In contrast, methods introduced in [3, 7, 20], that build upon recurrent models, enable a larger temporal receptive field.
In these networks, to super-resolve a frame at time step t, the hidden states and/or output computed in previous time step t − 1 are taken as input, in addition to a batch of 1 to 3 LR frames. This recursion allows to propagate informa-tion through a large number of frames. As their input batch contains less LR frames and their network structures are mostly simpler, recurrent methods are faster than sliding-window based methods. Moreover, an inference of a recur-rent model presents less redundant computations than the one of a sliding-window based model because each frame is processed only once. Finally, sliding-window based meth-ods generate independent output HR frames, which reduces temporal consistency of the produced HR frames, result-ing in flickering artifacts. This is not the case for recurrent
VSR, in which information about previously super-resolved frame is part of the input at each time step. These consid-erations make recurrent methods more interesting from a realistic application-oriented point of view.
Because of computational and memory constraints, as well as vanishing and exploding gradients, recurrent VSR models are usually trained on sequences of 7 to 12 im-ages.
They are then deployed to super-resolve a se-quence of any length. Some applications, such as video-surveillance, would require to super-resolve sequences of arbitrary length. However, recurrent models are not trained on these long sequences. Hence, there is no guarantee that they optimally perform on long sequences. In this study, we show that recurrent VSR networks generate high frequency artifacts when inferring on long video sequences presenting low motion. Such sequences contain parts of the scene that barely move, for instance when the camera is quasi-static.
The super-resolution process creates high-frequency infor-mation which is accumulated in the long-term recurrence, creating artifacts and causing divergence. Fig. 1 illustrates this phenomenon. To the best of our knowledge, this work is the first study about VSR that raises this instability is-sue. This unexpected behavior can be critical for some real-world applications, like video surveillance in which both the camera and the scene stay static for a long time.
The structure of the article is the following. First, we re-view studies related to VSR and instabilities of recurrent networks. Then, based on Lipschitz stability theory, we propose a new framework of recurrent VSR network that is both stable and competitive on long sequences with low motion. After this, we introduce a new recurrent VSR net-work MRVSR as an implementation of this framework. Fi-nally, we empirically analyze instabilities of existing recur-rent VSR models on long sequences with low motion and show the stability and superior performance of the proposed network. A new long sequence dataset has been created for our experiments. We make it publicly available. 2.