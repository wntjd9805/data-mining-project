Abstract
Estimating 3D human poses from monocular videos is a challenging task due to depth ambiguity and self-occlusion.
Most existing works attempt to solve both issues by exploiting spatial and temporal relationships. However, those works ignore the fact that it is an inverse problem where multi-ple feasible solutions (i.e., hypotheses) exist. To relieve this limitation, we propose a Multi-Hypothesis Transformer (MHFormer) that learns spatio-temporal representations of multiple plausible pose hypotheses. In order to effectively model multi-hypothesis dependencies and build strong rela-tionships across hypothesis features, the task is decomposed into three stages: (i) Generate multiple initial hypothesis representations; (ii) Model self-hypothesis communication, merge multiple hypotheses into a single converged represen-tation and then partition it into several diverged hypotheses; (iii) Learn cross-hypothesis communication and aggregate the multi-hypothesis features to synthesize the ﬁnal 3D pose.
Through the above processes, the ﬁnal representation is en-hanced and the synthesized pose is much more accurate.
Extensive experiments show that MHFormer achieves state-of-the-art results on two challenging datasets: Human3.6M and MPI-INF-3DHP. Without bells and whistles, its perfor-mance surpasses the previous best result by a large margin of 3% on Human3.6M. Code and models are available at https://github.com/Vegetebird/MHFormer. 1.

Introduction 3D human pose estimation (HPE) from monocular videos is a fundamental vision task with a wide range of appli-cations, such as action recognition [23, 24, 39], human-computer interaction [7], and augmented/virtual reality [31].
This task is typically solved by dividing it into two decoupled subtasks, i.e., 2D pose detection to localize the keypoints
*Corresponding author: hongliu@pku.edu.cn. This work is supported by National Key R&D Program of China (No. 2020AAA0108904), Science and Technology Plan of Shenzhen (No. JCYJ20200109140410340).
Figure 1. Given a frame with occluded body parts (right arm and elbow), a recent state-of-the-art 3D HPE method, PoseFormer [46], outputs a single solution that is inconsistent with the 2D input. In contrast, our MHFormer generates multiple plausible hypotheses (different colors) consistent with the 2D evidence and ﬁnally syn-thesizes a more accurate 3D pose (green). For easy comparison, the input frame is shown at a novel viewpoint. on the image plane, followed by 2D-to-3D lifting to infer joint locations in the 3D space from 2D keypoints. Despite their impressive performance [4, 9, 29, 34], it remains an inherently ill-posed problem because of self-occlusion and depth ambiguity in 2D representations.
To alleviate such issues, most methods [2, 11, 38, 46] focus on exploring spatial and temporal relationships. They either employ graph convolutional networks to estimate 3D poses with a spatio-temporal graph representation of human skeletons [2,11,38] or apply a pure Transformer-based model to capture spatial and temporal information from 2D pose sequences [46]. Yet, the 2D-to-3D lifting from monocular videos is an inverse problem [1] where multiple feasible solutions (i.e., hypotheses) exist due to its ill-posed nature given the missing depth [17]. Those approaches ignore this problem and only estimate a single solution, which often leads to unsatisfactory results, especially when the person is severely occluded (see Figure 1).
Recently, a couple of methods [14, 16, 35, 40] that gener-ate multiple hypotheses have been proposed for the inverse problem. They often rely on the one-to-many mapping by adding multiple output heads to an existing architecture with a shared feature extractor, while failing to build the relation-ships among the features of different hypotheses. That is an important shortcoming, as such ability is vital to improve the
Figure 2. The proposed MHFormer constructs a three-stage framework by starting from generating multiple initial representations and then communicating among them in both independent and mutual ways to synthesize a more precise estimation. For easy illustration, we only show the process of a single-frame 2D pose as input. expressiveness and the performance of the model. In view of the ambiguous inverse problem of 3D HPE, we argue that it is more reasonable to conduct a one-to-many mapping ﬁrst and then a many-to-one mapping with various intermediate hypotheses, as this way can enrich the diversity of features and produce a better synthesis for the ﬁnal 3D pose.
To this end, we present Multi-Hypothesis Transformer (MHFormer), a novel Transformer-based method for 3D
HPE from monocular videos. The key insight is to allow the model to learn spatio-temporal representations of di-verse pose hypotheses. To accomplish this, we introduce a three-stage framework that starts from generating multiple initial representations and gradually communicates across them to synthesize a more accurate prediction, as shown in
Figure 2. This framework more effectively models multi-hypothesis dependencies while also building stronger rela-tionships among hypothesis features. Speciﬁcally, in the
ﬁrst stage, a Multi-Hypothesis Generation (MHG) module is built to model the intrinsic structure information of human joints and generate several multi-level features in the spatial domain. Those features contain diverse semantic informa-tion in different depths from shallow to deep and hence can be regarded as initial representations of multiple hypotheses.
Next, we propose two novel modules to model temporal consistencies and enhance those coarse representations in the temporal domain, which have not been explored by the existing works that generate multiple hypotheses. In the second stage, a Self-Hypothesis Reﬁnement (SHR) mod-ule is proposed to reﬁne every single-hypothesis feature.
The SHR consists of two new blocks. The ﬁrst block is a multi-hypothesis self-attention (MH-SA) which models single-hypothesis dependencies independently to construct self-hypothesis communication, enabling message passing within each hypothesis for feature enhancement. The second block is a hypothesis-mixing multi-layer perceptron (MLP) that exchanges information across hypotheses. The multiple hypotheses are merged into a single converged representa-tion, and then this representation is partitioned into several diverged hypotheses.
Although those hypotheses are reﬁned by SHR, the con-nections across different hypotheses are not strong enough since the MH-SA in the SHR only passes intra-hypothesis information. To address this issue, in the last stage, a
Cross-Hypothesis Interaction (CHI) module models inter-Its key com-actions among multi-hypothesis features. ponent is the multi-hypothesis cross-attention (MH-CA), which captures mutual multi-hypothesis correlations to build cross-hypothesis communication, enabling message passing among hypotheses for better interaction modeling. Subse-quently, a hypothesis-mixing MLP is used to aggregate the multiple hypotheses to synthesize the ﬁnal prediction.
With the proposed MHFormer, multi-hypothesis spatio-temporal feature hierarchies are explicitly incorporated into
Transformer models, where the multiple hypothesis infor-mation of body joints can be independently and mutually processed in an end-to-end manner. As a result, the repre-sentation ability is potentially enhanced and the synthesized pose is much more accurate. Our contributions are summa-rized as follows:
• We present a new Transformer-based method, called
Multi-Hypothesis Transformer (MHFormer), for 3D
HPE from monocular videos. MHFormer can effec-tively learn spatio-temporal representations of multiple pose hypotheses in an end-to-end manner.
• We propose to communicate among multi-hypothesis features both independently and mutually, providing powerful self-hypothesis and cross-hypothesis message passing, and strong relationships among hypotheses.
• Our MHFormer achieves state-of-the-art performance on two challenging datasets for 3D HPE, signiﬁcantly outperforming PoseFormer [46] by 3% with 1.3 mm error reduction on Human3.6M [12]. 2.