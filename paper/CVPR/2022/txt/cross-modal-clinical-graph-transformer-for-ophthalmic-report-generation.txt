Abstract
Automatic generation of ophthalmic reports using data-driven neural networks has great potential in clinical prac-tice. When writing a report, ophthalmologists make infer-ences with prior clinical knowledge. This knowledge has been neglected in prior medical report generation meth-ods. To endow models with the capability of incorporat-ing expert knowledge, we propose a Cross-modal clinical
Graph Transformer (CGT) for ophthalmic report genera-tion (ORG), in which clinical relation triples are injected into the visual features as prior knowledge to drive the de-coding procedure. However, two major common Knowl-edge Noise (KN) issues may affect modelsâ€™ effectiveness. 1) Existing general biomedical knowledge bases such as the
UMLS may not align meaningfully to the specific context and language of the report, limiting their utility for knowl-edge injection. 2) Incorporating too much knowledge may divert the visual features from their correct meaning. To overcome these limitations, we design an automatic infor-mation extraction scheme based on natural language pro-cessing to obtain clinical entities and relations directly from in-domain training reports. Given a set of ophthalmic im-ages, our CGT first restores a sub-graph from the clini-cal graph and injects the restored triples into visual fea-tures. Then visible matrix is employed during the encod-ing procedure to limit the impact of knowledge. Finally, reports are predicted by the encoded cross-modal features via a Transformer decoder. Extensive experiments on the large-scale FFA-IR benchmark demonstrate that the pro-posed CGT is able to outperform previous benchmark meth-ods and achieve state-of-the-art performances. 1.

Introduction
Fundus Fluorescein Angiography (FFA) is one of the es-sential ophthalmic imaging examinations in clinical prac-*Corresponding author. tice. However, writing reports to summarize findings from dozens of ophthalmic images during an examination is time-consuming and error-prone, especially for inexperi-enced ophthalmologists. With the success of data-driven neural networks [11, 19, 22, 25, 42] in many real-life sce-narios, researchers and ophthalmologists start to investigate how to apply artificial intelligent (AI) models in clinical ophthalmic practice and acquire significant achievements
[3]. Automatic generation of ophthalmic reports offers the possibility of reducing the heavy workloads of ophthalmol-ogists. Furthermore, the predicted reports can highlight ab-normalities for the ophthalmologists and provide a ratio-nale for disease diagnosis; hence, automatic ophthalmic re-port generation has attracted increasing research interest for
AI-based clinical decision support, as well as presenting a meaningful opportunity to explore the integration of vision and language modalities in neural network models.
Despite significant progress in generic image caption-ing models [2, 8], when transferring them into medical knowledge-driven tasks, they fail to achieve promising and competitive performance due to a lack of prior medical knowledge. When describing ophthalmic images, ordinary people can only recognize the common visual information, such as the shape and color, while ophthalmologists make inferences with their prior clinical knowledge. For models to achieve this capability, recent work explores the incor-poration of medical knowledge to enhance diagnostic mod-els [20, 23, 26, 41].
On the one hand, researchers [20, 23] have explored graph structure weights as posterior knowledge to allevi-ate the textual bias. In each graph, the nodes are observed abnormalities selected from prior knowledge, such as exter-nal medical corpus, and the edges are the predicted weights correlating each pair abnormalities. However, the weight graph limits the effectiveness of the knowledge graph from two aspects. Firstly, some entities are extracted from the external medical corpus or knowledge graph database sepa-rated from the training corpus. These entities will bring in a
heterogeneous embedding space [27] which makes the em-bedding vectors inconsistent. Secondly, there are no ground truth weights to supervise the message passing procedure, and the model is still prone to be distracted by the visual bias in medical images [26]. On the other hand, a uni-versal graph is proposed with prior knowledge on 20 chest findings [41] to enhance models. Since these findings are not always depicted in one report, incorporating all this knowledge may divert the visual features from their orig-inal meaning.
To address these issues, we propose a Cross-modal clin-ical Graph Transformer (CGT) for ophthalmic report gen-eration (ORG). In particular, we first invoke an information extraction scheme based on a natural language processing pipeline, including named entity recognition and entity link-ing, to obtain a clinical knowledge graph. More details will be introduced in Section 3.2. As discussed in [15], the struc-tured clinical information behind the free-text reports can enhance the diagnostic methods.
In addition, the entities and relations in our clinical graph are in the homogeneous embedding space with the training corpus. Given a set of ophthalmic images, the extracted visual features are trans-formed to a compressed visual token and a sub-graph with relevant restored triples. Since the sub-graph is not guaran-teed to be a completely accurate representation of the given images and natural noise exists in the clinical graph, we adopt a cross-modal encoder to encode the universal fea-ture token and sub-graph information. To avoid influence from unrelated entities, a visible matrix is introduced dur-ing the cross-modal encoding process. Finally, reports are generated via a Transformer [34] decoder.
We conduct extensive experiments on the publicly avail-able FFA-IR benchmark [21]. Experiments show that our
CGT achieves the state-of-the-art performance of predicted reports under four automatic evaluation metrics and high
AUC scores for the restored triples, providing a solid ratio-nale for the explanation. 2.