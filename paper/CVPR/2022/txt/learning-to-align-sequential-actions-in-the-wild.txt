Abstract
State-of-the-art methods for self-supervised sequential action alignment rely on deep networks that find correspon-dences across videos in time. They either learn frame-to-frame mapping across sequences, which does not leverage temporal information, or assume monotonic alignment be-tween each video pair, which ignores variations in the or-der of actions. As such, these methods are not able to deal with common real-world scenarios that involve background frames or videos that contain non-monotonic sequence of actions.
In this paper, we propose an approach to align sequential actions in the wild that involve diverse temporal variations.
To this end, we propose an approach to enforce tempo-ral priors on the optimal transport matrix, which leverages temporal consistency, while allowing for variations in the order of actions. Our model accounts for both monotonic and non-monotonic sequences and handles background frames that should not be aligned. We demonstrate that our approach consistently outperforms the state-of-the-art in self-supervised sequential action representation learning on four different benchmark datasets. Code is publicly avail-able at https://github.com/weizheliu/VAVA. 1.

Introduction
Understanding human activities in video sequences is important for applications such as human-computer inter-action, video analysis, robot learning, and surveillance. In recent years, a significant amount of research has focused on supervised, coarse-scale action understanding. Most of the work focuses on predicting explicit classes for clips cor-responding to a certain limited set of action categories in a supervised fashion [8,11,33,52,53,55]. While giving a cat-egorical understanding of human behavior, such techniques do not provide a fine-grained analysis of human action.
Furthermore, the dependence on per-frame labels requires a large amount of human effort that does not scale up to many different types of subjects, environments, and scenar-*This work was completed during an internship at Microsoft Mixed
Reality & AI Lab. (a) (b) (c)
Figure 1. Temporal Variations [15]. (a)