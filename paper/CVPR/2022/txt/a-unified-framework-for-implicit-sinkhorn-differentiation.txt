Abstract
The Sinkhorn operator has recently experienced a surge of popularity in computer vision and related fields. One major reason is its ease of integration into deep learning frameworks. To allow for an efficient training of respec-tive neural networks, we propose an algorithm that obtains analytical gradients of a Sinkhorn layer via implicit differ-entiation. In comparison to prior work, our framework is based on the most general formulation of the Sinkhorn op-erator. It allows for any type of loss function, while both the target capacities and cost matrices are differentiated jointly.
We further construct error bounds of the resulting algorithm for approximate inputs. Finally, we demonstrate that for a number of applications, simply replacing automatic differ-entiation with our algorithm directly improves the stability and accuracy of the obtained gradients. Moreover, we show that it is computationally more efficient, particularly when resources like GPU memory are scarce.1 1.

Introduction
Computing matchings and permutations is a fundamen-tal problem at the heart of many computer vision and ma-chine learning algorithms. Common applications include pose estimation, 3D reconstruction, localization, informa-tion transfer, ranking, and sorting, with data domains rang-ing from images, voxel grids, point clouds, 3D surface meshes to generic Euclidean features. A popular tool to address this is the Sinkhorn operator, which has its roots in the theory of entropy regularized optimal transport [9]. The 1Our implementation is available under the following link: https:
//github.com/marvin-eisenberger/implicit-sinkhorn
Sinkhorn operator can be computed efficiently via a simple iterative matrix scaling approach. Furthermore, the result-ing operator is differentiable, and can therefore be readily integrated into deep learning frameworks.
A key question is how to compute the first-order deriva-tive of a respective Sinkhorn layer in practice. The stan-dard approach is automatic differentiation of Sinkhornâ€™s al-gorithm. Yet, this comes with a considerable computational burden because the runtime of the resulting backward pass scales linearly with the number of forward iterations. More importantly, since the computation graph needs to be main-tained for all unrolled matrix-scaling steps, the memory de-mand is often prohibitively high for GPU processing.
A number of recent works leverage implicit gradients as an alternative to automatic differentiation [7, 11, 17, 22, 26] to backpropagate through a Sinkhorn layer. Although such approaches prove to be computationally inexpensive, a downside is that corresponding algorithms are less straight-forward to derive and implement. Hence, many application works still rely on automatic differentiation [13, 25, 39, 46, 47]. Yet, the computational burden of automatic differen-tiation might drive practitioners to opt for an insufficiently small number of Sinkhorn iterations which in turn impairs the performance as we experimentally verify in Sec. 5.
To date, existing work on implicit differentiation of
Sinkhorn layers suffers from two major limitations: (i) Most approaches derive gradients only for very specific settings, i.e. specific loss functions, structured inputs, or only a sub-set of all inputs. Algorithms are therefore often not trans-ferable to similar but distinct settings. (ii) Secondly, beyond their empirical success, there is a lack of an in-depth theo-retical analysis that supports the use of implicit gradients.
Our work provides a unified framework of implicit dif-ferentiation techniques for Sinkhorn layers. To encourage practical adaptation, we provide a simple module that works out-of-the-box for the most general formulation, see Fig. 2.
We can thus recover existing methods as special cases of our framework, see Tab. 1 for an overview. Our contribu-tion can be summarized as follows: 1. From first principles we derive an efficient algorithm for computing gradients of a generic Sinkhorn layer. 2. We provide theoretical guarantees for the accuracy of the resulting gradients as a function of the approxima-tion error in the forward pass (Theorem 5). 3. Our PyTorch module can be applied in an out-of-the-box manner to existing approaches based on automatic differentiation. This often improves the quantitative results while using significantly less GPU memory. 2.