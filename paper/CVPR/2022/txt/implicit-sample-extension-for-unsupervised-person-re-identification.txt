Abstract
Most existing unsupervised person re-identification (Re-ID) methods use clustering to generate pseudo labels for model training. Unfortunately, clustering sometimes mixes different true identities together or splits the same identity into two or more sub clusters. Training on these noisy clusters substantially hampers the Re-ID accuracy. Due to the limited samples in each identity, we suppose there may lack some underlying information to well reveal the accurate clusters. To discover these information, we pro-pose an Implicit Sample Extension (ISE) method to gener-ate what we call support samples around the cluster bound-aries. Specifically, we generate support samples from ac-tual samples and their neighbouring clusters in the em-bedding space through a progressive linear interpolation (PLI) strategy. PLI controls the generation with two crit-ical factors, i.e., 1) the direction from the actual sample towards its K-nearest clusters and 2) the degree for mix-ing up the context information from the K-nearest clusters.
Meanwhile, given the support samples, ISE further uses a label-preserving loss to pull them towards their corre-sponding actual samples, so as to compact each cluster.
Consequently, ISE reduces the “sub and mixed” cluster-ing errors, thus improving the Re-ID performance. Exten-sive experiments demonstrate that the proposed method is effective and achieves state-of-the-art performance for un-supervised person Re-ID. Code is available at: https:
//github.com/PaddlePaddle/PaddleClas. 1.

Introduction
Unsupervised person re-identification (Re-ID) aims to learn person appearance features without annotations.
It gains increasing attention and popularity due to its wide
*Equal contribution.
†Corresponding author.
Figure 1. Clustering behaviour changes with our implicit sample extension (ISE). Different shapes represent different ground-truth identities, while different colors stand for different pseudo labels (best viewed in color). Usually, clustering may produce (a) sub clusters that samples share the same true identity but are split apart as two or more clusters; and (b) mixed clusters that sam-ples with different identities are mixed to the same cluster. Our support samples help to merge the sub clusters as one and split the mixed clusters apart, thus improving the performance. practical applications in the real world. One kind of works
[2, 8, 13, 25, 43, 44, 49] attempt to transfer knowledge from existing labeled data to unlabeled target data, known as unsupervised domain adaptation (UDA). Another kind of works rely on unsupervised learning (USL) to learn rep-resentations purely from unlabeled images, which is more data-friendly than UDA. Most USL Re-ID methods [4,9,14] follow an iterative two-stage training procedure: 1) using clustering [12, 22] to generate pseudo labels as the super-vision for the next step; 2) training the Re-ID model using the pseudo labels. Here we focus on investigating the USL
Re-ID, and follow the two-stage pipeline.
Clustering is critical in the aforementioned USL and has attracted great attention from existing USL methods. For instance, ACT [38] utilizes two models that try to refine pseudo labels for each other. SpCL [14] employs a self-paced learning to gradually generate more reliable clusters for training. Despite the progresses, these methods still suf-fer from noisy pseudo labels, especially the sub and mixed clusters shown in the left part of Figure 1. Based on this observation, we suppose that some underlying information may be lost in the existing data distribution due to the lim-ited samples in each identity. If we can compensate for the missing information, the clustering quality can be improved in the next step. For example, if there exists intermediate variations among two sub clusters with the same identity, they can be correctly merged into one cluster.
To achieve this goal, we propose an implicit sample ex-tension (ISE) method, progressively synthesizing supple-mentary samples to improve the context representation for each cluster. We name these supplementary samples as support samples, since they often patrol around the clus-ter boundaries.
Inspired by [3, 28, 32] that deep features are usually linearized, we propose a progressive linear inter-polation operation (PLI) to guide the generation of support samples with two factors, i.e., direction and degree. Specif-ically, the direction factor controls that the generation of support samples is from a actual sample to its K-nearest neighbor clusters in the embedding space. Support sam-ples in these directions are more meaningful, since neighbor clusters are more likely to have the above clustering prob-lems. The degree factor decides how much context infor-mation from K-nearest clusters should be incorporated by support samples. We increases the degree progressively to fill the cluster gap with farther and farther support samples.
This strategy can prevent the training collapse caused by aggressive support samples, especially in the early training stages. Except for PLI, we also propose a label-preserving (LP) contrastive loss to enforce support samples close to their originals to compact each cluster. After the sample generation, we regard support samples as actual ones to let them participate in the model training.
We observe that with the help of support samples, the data distribution is refined, and spurious clustering be-haviours, including the sub and mixed clusters, can be al-leviated (see experiments for more details).
In addition, different from GAN (General Adversarial Network) based methods [11, 51], ISE is parameter-free and performs the sample generation implicitly in the embedding space in-stead of explicitly in the image pixel space. Therefore, the support samples can be readily utilized in loss func-tions without any feature extraction procedure. These mer-its make the proposed ISE a very efficient method. In sum-mary, our contributions are as follows:
• We present a novel progressive linear interpolation (PLI) strategy and a label-preserving contrastive loss (LP) for the support sample generation;
• We conduct comprehensive experiments and analyses to show the effectiveness of ISE, and ISE outperforms the current state-of-the-arts by a large margin. 2.