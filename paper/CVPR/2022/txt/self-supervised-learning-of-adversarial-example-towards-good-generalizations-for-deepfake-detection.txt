Abstract
Recent studies in deepfake detection have yielded promising results when the training and testing face forg-eries are from the same dataset. However, the problem remains challenging when one tries to generalize the de-tector to forgeries created by unseen methods in the train-ing dataset. This work addresses the generalizable deep-fake detection from a simple principle: a generalizable representation should be sensitive to diverse types of forg-eries. Following this principle, we propose to enrich the
“diversity” of forgeries by synthesizing augmented forg-eries with a pool of forgery configurations and strengthen the “sensitivity” to the forgeries by enforcing the model to predict the forgery configurations. To effectively ex-plore the large forgery augmentation space, we further pro-pose to use the adversarial training strategy to dynami-cally synthesize the most challenging forgeries to the cur-rent model. Through extensive experiments, we show that the proposed strategies are surprisingly effective (see Fig-ure 1), and they could achieve superior performance than the current state-of-the-art methods. Code is available at https://github.com/liangchen527/SLADD. 1.

Introduction
The realistic image generation brought by the generative adversarial network (GAN) raises a security issue that hu-man portraits can be easily substituted to provide malicious bioinformatics [11, 45, 15, 44, 51, 56, 40, 54, 48]. This forgery becomes a threat to subject identifications which have been extensively utilized in digital payment, video surveillance, and social media. To reduce these risks, there is an emerging investigation on deepfake detectors to iden-tify face forgeries. Formulated as a binary classification problem, current detectors [26, 3, 2, 32, 39] perform well
∗Corresponding authors. This work is done when L. Chen is an intern in Tencent AI Lab.
Figure 1. Performance improvements of the proposed strategies for the baseline model (i.e. Xception [41]). The models are trained on the four types of data from Faceforensics++ dataset [41] and tested on CelebDF [28], DFDC [12], and DF1.0 [20] datasets. when training and testing forgeries are synthesized from the same dataset and same forgery methods. However, in prac-tice the testing forgeries are usually from unseen datasets and synthesized by unseen methods. Discrepancies between training and testing data lead to inferior performance of de-tectors. There is a performance drop when detectors rec-ognize forgeries outside the training dataset, which brings challenges to deepfake detectors for practical usage.
Attempts have been made in recent arts to improve the generalizations. For example, to overcome dataset bias, some studies [24, 58] suggest data augmentation is an ef-fective tool against poor generalization. These methods augment training data by synthesizing new face forgeries with their empirically designed augmentations. However, their augmentations are with a limited choice of strat-egy. The lacking of variety may jeopardize the generaliza-tion. Meanwhile, intrinsic forgery attributes shared between forgeries, such as detail discrepancies [30], and frequency features [39, 29], are also mined broadly with hand-crafted representations to distinct forgeries. But these attributes mainly rely on imperceptible image patterns, which are sen-sitive to post-processing steps, such as compression. They vary significantly in different datasets, thus leading to large detection bias and limiting their generalization [60].
In this paper, we address the deepfake detection from a simple heuristic principle: a generalizable representation should be sensitive to the various types of forgeries. Train-ing a model by following this principle could potentially avoid the “blind spot” of the model or avoid relying on pat-terns specific to a dataset, since otherwise the model may not be capable of identifying a variety of forgeries. Push-ing this idea to the limit, we propose to enrich the “diversity of forgeries” by synthesizing forgery images from a large pool of configurations 1. Specifically, given a pristine, we randomly choose a reference image 2 from training data, our synthesizer network (i.e. generator) produces forgery configurations that specify the forgery region, the blending type, and the blending ratio. Then based on these configu-rations, a synthesized forgery is generated (some examples are shown in Figure 2). To enhance the “sensitivity to the forgeries”, our detector network (i.e. discriminator) is re-quired to predict the configurations of an input in addition to judging if it is a forgery or not. Further, to effectively explore the large space of forgery augmentation, we adopt an adversarial training strategy to dynamically construct the augment that is most challenging to the current detector net-work. Different from the empirically designed augmenta-tions [24, 58, 6], our adversarial augmentation strategy en-joys a large variety, and it can be dynamically constructed by the performance of the discriminator.
Through our experimental studies, we demonstrate the effectiveness of employing both adversarial augmentation and self-supervised tasks. A significant improvement over the baseline approach is observed, and our method also per-forms favorably against other state-of-the-art detectors. 2.