Abstract
End-to-end scene text spotting has attracted great at-tention in recent years due to the success of excavat-ing the intrinsic synergy of the scene text detection and recognition. However, recent state-of-the-art methods usu-ally incorporate detection and recognition simply by shar-ing the backbone, which does not directly take advan-tage of the feature interaction between the two tasks.
In this paper, we propose a new end-to-end scene text spot-ting framework termed SwinTextSpotter. Using a trans-former encoder with dynamic head as the detector, we unify the two tasks with a novel Recognition Conversion mechanism to explicitly guide text localization through recognition loss. The straightforward design results in a concise framework that requires neither additional rec-tiﬁcation module nor character-level annotation for the arbitrarily-shaped text. Qualitative and quantitative experi-ments on multi-oriented datasets RoIC13 and ICDAR 2015, arbitrarily-shaped datasets Total-Text and CTW1500, and multi-lingual datasets ReCTS (Chinese) and VinText (Viet-namese) demonstrate SwinTextSpotter signiﬁcantly outper-forms existing methods. Code is available at https:
//github.com/mxin262/SwinTextSpotter. 1.

Introduction
Scene text spotting, which aims to detect and recognize the entire word or sentence in natural images, has raised a lot of attention due to its wide range of applications in au-tonomous driving [64], intelligent navigation [42, 50], and key entities recognition [51, 65], etc. Traditional scene text spotting methods treat detection and recognition as two sep-arate tasks and adopt a pipeline that ﬁrst localizes and crops the text regions on the input images and then predicts the
†Equal contribution.
∗Corresponding author. (a) Without Recognition Conversion (b) With Recognition Conversion (c) The recognition loss with and without Recognition Conversion.
Figure 1. Effectiveness of Recognition Conversion. The proposed
Recognition Conversion explicitly guides the detection, leading to better text spotting performance. text sequence by feeding the cropped regions into text rec-ognizer [9, 14, 16, 23, 35]. Such a pipeline may have some limitations, such as (1) error accumulation between these two tasks, e.g., imprecise detection result may heavily hin-der the performance of text recognition; (2) separate opti-mization of the two tasks might not maximize the ﬁnal per-formance of text spotting; (3) intensive memory consump-tion and low inference efﬁciency.
Therefore, many methods [12,20,27,32] attempt to solve text spotting in end-to-end systems, i.e., optimizing detec-tion and recognition jointly in uniﬁed architectures. The recognizer can improve the performance of the detector by eliminating the false positive detection results [20, 21]. In turn, even if the detection is not precise, the recognizer can still correctly predict the text sequence by the large recep-tive ﬁeld of the feature map [21, 27]. Another advantage is that an end-to-end system is easier to maintain and transfer to new domains compared to a cascaded pipeline where the model is coupled with the data and thus requires substantial engineering efforts [30, 55].
However, there are two limitations in most of the exist-ing end-to-end scene text spotting systems [8, 22, 28, 32, 38, 39, 49]. First, if the detection is simply based on the vi-sual information in the input features, the detector is prone to be distracted by background noise and proposes incon-sistent detection, as depicted in Figure 1(a). The interac-tion between texts in the same image is the crucial factor to eliminate the impact of background noise, since differ-ent characters of the same word may contain strong sim-ilarities, such as the backgrounds and text styles. Using
Transformer [48] can learn rich interactions between text instances. For example, Yu et al. [62] use transformer to make texts interact with each other at semantic level. Fang et al. [7] and Wang et al. [57] further adopt transformer to model the visual relationship between texts. Second, the in-teractions between detection and recognition is not enough by sharing backbone because neither the recognition loss optimizes the detector nor the recognizer utilizes the de-tection features. To jointly improve detection and recog-nition, a character segmentation map is designed by Mask
TextSpotter [21], which simultaneously optimizes the de-tection and recognition results in the same branch; ABC-Net v2 [30] proposes Adaptive End-to-End Training (AET) strategy using the detection results to extract recognition features instead of only using the ground truths; ARTS [67] improves the performance of the end-to-end text spotting by back-propagating the loss from the recognition branch to the detection branch using a differentiable Spatial Trans-form Network (STN) [15]. However, these three methods assume the detector proposes text features structurally, e.g. in the reading order. The overall performance of the text spotting is thereafter bounded by the detector.
We propose SwinTextSpotter, an end-to-end trainable
Transformer-based framework, stepping toward better syn-ergy between the text detection and recognition. To better distinguish the densely scattered text instances in crowded scenes, we use Transformer and a two-level self-attention mechanism in SwinTextSpotter, stimulating the interactions between the text instances. Addressing the challenge in arbitrarily-shaped scene text spotting, inspired by [13, 45], we regard text detection task as a set-prediction problem and thus adopt a query-based text detector. We further pro-pose Recognition Conversion (RC), which implicitly guides the recognition head through incorporating the detection features. RC can back-propagate recognition information to the detector and suppress the background noise in the features for recognition, leading to the joint optimization of the detector and recognizer. Empowered by the proposed
RC, SwinTextSpotter has a concise framework without the character-level annotation and rectiﬁcation module used in previous works to improve the recognizer. SwinTextSpot-ter has superior performance in both the detection and the recognition. As illustrated in Figure 1(b), the detector of
SwinTextSpotter can accurately localize difﬁcult samples.
On the other hand, more accurate detection features can im-prove the recognizer and result in faster convergence and better performance, as shown in Figure 1(c).
We conduct extensive experiments on six benchmarks, including multi-oriented dataset RoIC13 [22] and ICDAR 2015 [18], arbitrarily-shaped dataset Total-Text [6] and
SCUT-CTW1500 [29], and multilingual dataset ReCTS (Chinese) [66] and VinText (Vietnamese) [36]. The results demonstrate the superior performance of the SwinTextSpot-ter: (1) SwinTextSpotter achieves 88.0% F-measure for the detection task on SCUT-CTW1500 and Total-Text, exceed-ing previous methods by a large margin; (2) SwinTextSpot-ter signiﬁcantly outperforms ABCNet v2 [30] by 9.8% in terms of 1-NED for the text spotting task in ReCTs dataset.
Additionally, without using character-level annotation on
ReCTs, SwinTextSpotter outperforms previous state-of-the-art methods MaskTextSpotter [21] and AE TextSpotter [54] that use such annotation; (3) SwinTextSpotter shows better robustness for the extremely rotated instances on RoIC13 dataset compared to MaskTextSpotter v3 [21]. The main contributions of this work are summarized as follows.
• SwinTextSpotter groundbreakingly shows that Trans-former and the set-prediction scheme are effective in end-to-end scene text spotting.
• SwinTextSpotter adopts the Recognition Conversion to exploit the synergy of text detection and recognition.
• SwinTextSpotter is a concise framework that does not require character-level annotation as well as specif-ically designed rectiﬁcation module for recognizing arbitrarily-shaped text.
• SwinTextSpotter achieves state-of-the-art performance on multiple public scene text benchmarks. 2.