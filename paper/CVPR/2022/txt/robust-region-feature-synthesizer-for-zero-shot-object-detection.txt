Abstract
Zero-shot object detection aims at incorporating class semantic vectors to realize the detection of (both seen and) unseen classes given an unconstrained test image. In this study, we reveal the core challenges in this research area: how to synthesize robust region features (for unseen ob-jects) that are as intra-class diverse and inter-class sepa-rable as the real samples, so that strong unseen object de-tectors can be trained upon them. To address these chal-lenges, we build a novel zero-shot object detection frame-work that contains an Intra-class Semantic Diverging com-ponent and an Inter-class Structure Preserving component.
The former is used to realize the one-to-more mapping to obtain diverse visual features from each class semantic vec-tor, preventing miss-classifying the real unseen objects as image backgrounds. While the latter is used to avoid the synthesized features too scattered to mix up the inter-class and foreground-background relationship. To demonstrate the effectiveness of the proposed approach, comprehensive experiments on PASCAL VOC, COCO, and DIOR datasets are conducted. Notably, our approach achieves the new state-of-the-art performance on PASCAL VOC and COCO and it is the first study to carry out zero-shot object detec-tion in remote sensing imagery. 1.

Introduction
With the rapid development of the deep learning tech-nologies, such as CNN [14, 34, 48] and Transformer [6, 26], great progresses have been made in the research field of object detection. Although the detection performance achieved by existing methods looks promising and encour-aging, it exists a hidden drawback for applying them in real-world scenarios—The mainstream detection approaches have the strict constraint on the category to detect. Once the model is trained, it can only recognize objects that ap-pear in the training data, whereas other objects appearing in the test images but unseen during training would confuse
*Corresponding author. the model dramatically, leading to avoidless faults in detec-tion results. To address this problem, the task of zero-shot object detection (ZSD) [4, 17, 33, 51] was raised in recent years. The goal is to enable the detection models to pre-dict unseen objects which are without any available samples during training.
Earlier efforts on zero-shot object detection (ZSD) [4,33] focus on mapping function-based methods, which learn mapping functions from the visual space to the semantic space. With the learned mapping functions, unseen object categories can be predicted by mapping their visual fea-tures into the semantic space and then performing the near-est neighbor search in the semantic space. However, due to that the mapping functions are learned all upon the seen cat-egories provided by the training data, the models would get significantly biased towards the seen categories when deal-ing with the visual features in testing [17]. Recently, gen-erative model-based methods [17, 51] are presented as an alternative solution. Usually, these methods utilize genera-tive models to synthesize visual features from the provided semantic embeddings [2, 30] corresponding to each object category. The synthesized visual features can then be used for training a standard detector for unseen classes. Genera-tive model-based methods show stronger performance com-pared with mapping function-based methods in solving the bias problem as, although the samples corresponding to the unseen objects are still absent, the detectors are trained with synthesized visual features for the unseen objects.
However, the current generative model-based methods mainly follow the ideas presented in zero-short classifica-tion frameworks, such as [31, 41], where the synthesized visual features may perform well in the less complex classi-fication scenarios but are not robust enough to obtain satis-fying results in the complicated detection scenarios. To our best knowledge, there are two-fold challenges for synthe-sizing visual features for detection scenarios:
• Intra-class diversity: Objects in real-world detection scenarios present high variation in pose, shape, tex-ture, etc., and one object instance may be covered by
Figure 1. Illustrations of the problem studied in this work. In real cases, the feature space built by the samples show high intra-class diversity but still with inter-class separability like in (a), whereas the spaces of the synthesized visual features learned by existing approaches either have insufficient intra-class diversity, as shown in (b), or have excess intra-class diversity to make inter-class inseparable, as shown in (c). several bounding boxes with different sizes and loca-tions. This leads to the high diversity in their feature representations.
• Inter-class separability: Though having such varia-tions, each object category still has easy-to-recognized characteristics that are distinct from other object cate-gories as well as the image backgrounds, making the feature representations from different classes (includ-ing the background class) highly separable.
Although some existing approaches have recognized the importance of intra-class diversity [17, 49], without jointly considering the inter-class separability, these methods would either impose insufficient diversity to the synthesized visual features, leading to miss-classify the real unseen ob-jects as image backgrounds (see Fig 1 (b)), or go too far to make the visual features synthesized for different class se-mantics mixed together, thus making the learned detection models obtain inaccurate object categories for foreground regions or suffer from errors in dealing with the image back-grounds (see Fig 1 (c)).
To overcome the feature synthesizing problems toward real-world detection scenarios, we build a novel zero-shot object detection framework as shown in Fig 2. Specifi-cally, we design two components for learning robust region features. To enable the model to synthesize diverse visual features, we propose an Intra-class Semantic Diverging (In-traSD) component which can diverge the semantic vector of a single class into a set of visual features. To prevent the intra-class diversity of the synthesized features goes too far to mix up the inter-class relationship, we further pro-pose an Inter-class Structure Preserving (InterSP) compo-nent that utilizes real visual samples from different object categories to constrain the separability of the synthesized visual features.
It is also worth mentioning that in the design of InterSP, we fully leverage the region features sampled from the real image scenes for detection instead of implementing it on the synthesized visual features. This enables our model to syn-thesize visual features as separable as in real cases and ob-tain much better performance when compared to the afore-mentioned counterpart (see experiments in Section 4.2).
To sum up, this paper mainly has the following three-fold contributions:
• We reveal the key challenges, i.e., the intra-class diver-sity and inter-class separability, for feature synthesiz-ing in real-world object detection scenarios.
• With the goal to synthesize robust region features for ZSD, we build a novel framework that contains an Intra-class Semantic Diverging component and an
Inter-class Structure Preserving component.
• Comprehensive experiments on three datasets, includ-ing PASCAL VOC, COCO, and DIOR, demonstrate the effectiveness of the proposed approach. Notably, this is also the first attempt for implementing zero-shot object detection in remote sensing imagery. 2.