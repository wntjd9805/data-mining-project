Abstract
Training
Inference
Conventional deep learning based methods for 3D object detection require a large amount of 3D bounding box anno-tations for training, which is expensive to obtain in practice.
Sparsely annotated object detection, which can largely re-duce the annotations, is very challenging since the missing-annotated instances would be regarded as the background during training.
In this paper, we propose a sparsely-supervised 3D object detection method, named SS3D. Aim-ing to eliminate the negative supervision caused by the missing annotations, we design a missing-annotated in-stance mining module with strict ﬁltering strategies to mine positive instances. In the meantime, we design a reliable background mining module and a point cloud ﬁlling data augmentation strategy to generate the conﬁdent data for it-eratively learning with reliable supervision. The proposed
SS3D is a general framework that can be used to learn any modern 3D object detector. Extensive experiments on the KITTI dataset reveal that on different 3D detectors, the proposed SS3D framework with only 20% annotations re-quired can achieve on-par performance comparing to fully-supervised methods. Comparing with the state-of-the-art semi-supervised 3D objection detection on KITTI, our SS3D improves the benchmarks by signiﬁcant margins under the same annotation workload. Moreover, our SS3D also out-performs the state-of-the-art weakly-supervised method by remarkable margins, highlighting its effectiveness. 1.

Introduction
Three-dimensional (3D) object detection, aiming to lo-calize and categorize objects from 3D sensor data (e.g.,
LiDAR point cloud), has attracted increasing attention due to its diversiﬁed applications in autonomous driving,
*Corresponding author.
Input
Input
Input
Input
PV-RCNN
Our Model
PV-RCNN
Our Model
Fully-supervised
Sparsely-supervised
Annotated Ground Truth
Predicted Bounding Box
Predictions
Predictions
Figure 1. Demonstration of the required annotations of the fully-supervised method and our method. The left case shows the train-ing stage of PV-RCNN [16] which is a high-performance detector with full annotations as input, while our model only annotates one instance for each scene. The right case shows the prediction results of PV-RCNN and our model, indicating that our model achieves comparable performance of the fully-supervised method. augmented/virtual reality, and indoor robotics. Recently, a number of approaches [1, 17, 18, 34, 35] based on ei-ther voxel-wise or point-wise features have been proposed and achieved high performance on large-scale benchmark datasets [2, 21]. However, most of the proposed 3D ob-ject detectors require fully supervised learning, implying a fully annotated dataset is required for the model learning.
Compared to 2D image objects, annotating 3D point cloud objects is more labor-intensive: annotators have to switch viewpoints or zoom in and out throughout a 3D scene care-fully for labeling each 3D object. Therefore, developing 3D detectors with on-par detection performance, while only re-quiring light-weighted object annotations, is a meaningful problem to tackle for practical applications.
Recently, few works [10, 15, 24, 26, 33] have been pro-posed to address this issue. In [10], the weakly-supervised learning strategy was adopted. Speciﬁcally, the point an-notation scheme was used to reduce the burden of annotat-ing bounding boxes. However, the supervision information provided by the point annotation is weak, so that a certain amount of full annotations have to be provided addition-ally, in order to achieve optimal performance. In [24, 33], the semi-supervised learning strategy was used, where just part of the dataset was annotated with the rest unlabeled.
The teacher-student framework was leveraged to transfer information from labeled data to unlabeled data. Neverthe-less, the information transfer tends to be ineffective when the gap between labeled and unlabeled data is large. Be-sides, although just part of the dataset is annotated, it still takes non-negligible labour to label an individual scene, es-pecially for crowded scenarios with many 3D objects, as shown as Fig. 1.
In this paper, we adopt the sparse annotation strategy and just annotate one 3D object in a scene, as illustrated in the left of the Fig. 1.
In this way, we are capable of obtain-ing full supervision information of one 3D object for each scene. Intuitively, this facilitates the learning of information on unlabeled objects, since infra-scene information transfer is much easier than cross-scene knowledge transfer. How-ever, sparsely annotated object detection also raises new challenges: missing-annotated instances will bring incor-rect supervision signals (i.e., as negative samples) to dis-turb the training of the network. During training, due to that the missing-annotated instances and the region near those instances could be incorrectly marked as background, the weight updated of the network will be misguided sig-niﬁcantly when gradients back-propagated. This challenge has been tackled in 2D sparse object detection methods
[11, 27] by utilizing overlap or hierarchical relation infor-mation among 2D objects. However, such information may be absent in 3D datasets e.g., in KITTI [2], which impedes directly applying such methods to 3D applications.
To address the challenge, we propose a novel and ef-fective method for sparsely annotated 3D object detection, namely SS3D, which can be applied to any advanced 3D detector. The main idea of our SS3D is to iteratively mine positive instances and background with high conﬁdence, and further use these generated data to train the 3D de-tector. We design two effective modules, namely missing-annotated instance mining module and reliable background mining module, to mine reliable missing positive instances and background, respectively. This ensures the 3D detector to be trained with conﬁdent supervision data. By this de-sign, compared with the 3D detector trained with the fully annotated dataset, our SS3D can achieve comparable per-formance, where only 20% annotation is required for the sparsely annotated dataset.
To summarize, our contributions are as follows:
• We propose a novel method for sparsely annotated 3D object detection from point cloud which can be used as a general framework to train any existing 3D fully-supervised detector. To the best of our knowledge, this is the ﬁrst work to explore the sparsely annotated strat-egy for the 3D object detection task.
• We design two effective modules to mine reliable miss-ing positive instances and background, respectively, which ensures the 3D detector to be trained with con-ﬁdent supervision data.
• Experimental results show that our method with sparse annotations can achieve comparable perfor-mance with fully-supervised methods and highly out-performs state-of-the-art semi-supervised and weakly-supervised 3D object detection methods. 2.