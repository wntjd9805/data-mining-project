Abstract
Existing work on VQA explores data augmentation to achieve better generalization by perturbing images in the dataset or modifying existing questions and answers. While these methods exhibit good performance, the diversity of the questions and answers are constrained by the available im-ages.
In this work we explore using synthetic computer-generated data to fully control the visual and language space, allowing us to provide more diverse scenarios. We quantify the effectiveness of leveraging synthetic data for real-world VQA. By exploiting 3D and physics simulation platforms, we provide a pipeline to generate synthetic data to expand and replace type-speciﬁc questions and answers without risking exposure of sensitive or personal data that might be present in real images. We offer a comprehensive analysis while expanding existing hyper-realistic datasets to be used for VQA. We also propose Feature Swapping (F-SWAP) – where we randomly switch object-level features during training to make a VQA model more domain invari-ant. We show that F-SWAP is effective for improving VQA models on real images without compromising on their ac-curacy to answer existing questions in the dataset. 1.

Introduction
Data augmentation is an effective way to achieve better generalization on several visual recognition and natural lan-guage understanding tasks. Existing work on Visual Ques-tion Answering (VQA) has explored augmenting the pool of questions and answers, e.g. by perturbing or masking some parts of the images [1, 6, 29, 56]. Moreover, curating large-scale datasets is a laborious task and sourcing images is an expensive process that needs to account for practical issues such as copyright and privacy. Augmenting existing datasets with synthetically generated data offers a path to enhance our existing data-driven models at a lower cost.
Our work focuses on leveraging synthetically gener-ated data through the use of modern 3D generated com-puter graphics using a couple of novel resources – Hyper-In the past, leveraging sim [45] and ThreeDWorld [14].
∗Work partially done while interning at the MIT-IBM Watson AI Lab
Figure 1. Training samples for VQA from real and synthetic datasets. The ﬁrst row shows existing examples from the VQA 2.0 dataset. The second row shows examples from Hypersim [45], a hyper-realistic synthetic dataset we extend for VQA. The third row shows some examples we generate using ThreeDWorld [14]. We show type-speciﬁc questions for each dataset, i.e., counting ques-tions, color related questions, and yes/no questions. synthetic data has proven challenging due the particularly wide domain gap between synthetic images and real im-ages. However, there have been some successes in tasks such as eye gaze estimation [53], embodied agent naviga-tion [10, 49, 50], and autonomous driving [39, 44]. There have also been some synthetic datasets for visual question answering such as CLEVR [26] CLEVRER [64], and VQA
Abstract [4]. However these VQA datasets build a closed world that is not designed to generalize to real world im-ages. Remarkably, some recent work has managed to show domain transfer from cartoon images to real images [67], but there is still a limitation on how much could be learned from these existing resources. Our proposed Hypersim-VQA and ThreeDWorld-VQA datasets provide a promis-ing alternative that more realistically captures real world settings and offers a path forward in this direction. Fig-ure 1 shows synthetic image samples along with the VQA 2.0 dataset [18].
Our work also proposes feature swapping (F-SWAP) as a simple yet effective method to augment a currently ex-isting VQA dataset with computer graphics generated ex-amples. Existing methods for domain adaptation rely on the assumption that adaptation can be addressed by mak-ing the out-of-domain samples match the distribution of the in-domain samples. However current work often op-erationalizes this assumption by making the input images themselves look more like the real images e.g. [22, 46, 58].
While there has been success in applying these techniques in domain adaptation for a number scenarios, we claim that perhaps adapting the input space is a harder problem that needs to be solved in order to have effective domain adapta-tion. Feature Swapping relies instead on swapping random object-level intermediate feature representations. We posit that unless realistic style-transfer is desired from the input domain to the target domain, as long as the two domains are matched at the feature level – domain adaptation can take place. We explain and compare our F-SWAP approach with other methods such as adversarial domain adaptation and demonstrate superior results.
Our contributions can be summarized as follows:
• Dataset generation: We are providing an extension of the Hypersim dataset for VQA, and automatically cre-ating a synthetic VQA dataset using ThreeDWorld.
• Feature swapping (F-SWAP): We propose a surpris-ingly simple yet effective new technique for incorpo-rating synthetic images in our training while mitigating domain shift. Our method does not rely on GANs or adversarial losses which could be difﬁcult to train.
• Experimental results: We provide an empirical analy-sis, using well known techniques such as adversarial augmentation, domain independent fusion, and maxi-mum mean discrepancy matching to alleviate the vi-sual domain gap vs our proposed approach – and anal-ysis on knowledge transfer between skills.
We ﬁrst introduce related work (Sec. 2), then we describe our proposed synthetic dataset generation process (Sec. 3), then we explain the motivation and details of our feature swapping method (Sec. 4), then we describe and discuss our experiments(Sec. 5), and ﬁnally we conclude the paper (Sec. 6). Our synthetic datasets and code are available at https://simvqa.github.io. 2.