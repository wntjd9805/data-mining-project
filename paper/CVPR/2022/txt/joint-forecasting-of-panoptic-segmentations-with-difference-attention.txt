Abstract
Forecasting of a representation is important for safe and effective autonomy. For this, panoptic segmentations have been studied as a compelling representation in recent work.
However, recent state-of-the-art on panoptic segmentation forecasting suffers from two issues: first, individual object instances are treated independently of each other; second, individual object instance forecasts are merged in a heuris-tic manner. To address both issues, we study a new panoptic segmentation forecasting model that jointly forecasts all ob-ject instances in a scene using a transformer model based on ‘difference attention.’ It further refines the predictions by taking depth estimates into account. We evaluate the pro-posed model on the Cityscapes and AIODrive datasets. We find difference attention to be particularly suitable for fore-casting because the difference of quantities like locations enables a model to explicitly reason about velocities and acceleration. Because of this, we attain state-of-the-art on panoptic segmentation forecasting metrics. 1.

Introduction
Forecasting is needed for safe and effective autonomous systems [9, 26]. For this reason, forecasting has been stud-ied in many different domains from computer vision and robotics to machine learning. In common across domains is the discussion about what representations are useful for forecasting. Representations which have been studied range from trajectories [10,11,13,32,53] and bounding boxes [29, 30, 39, 50, 51] to semantic segmentation [6, 25, 28, 34, 36], instance segmentation [8, 19, 27], images [12, 24, 52] and recently also panoptic segmentations [14, 38].
Each representation has applications which benefit from their use. We focus on panoptic segmentations as they natu-rally disentangle 1) objects which change position in an im-age due to observer motion; from 2) object instances which change position due to both observer and instance motion.
However, the state-of-the-art on panoptic segmentation forecasting [14] is challenged by two key issues. First, fore-ground predictions of individual instances are made inde-pendently of each other. This is suboptimal because the
Figure 1. Our panoptic segmentation forecasting. We jointly rea-son about every instance in a scene to predict instance masks (top), and then reason about the relative depth of foreground and back-ground components (middle) to produce an output (bottom). movements of instances are clearly correlated, e.g., when considering traffic patterns like the ones in the Cityscapes dataset [7]. Second, the method opted for a simple strategy to merge individual object instance segmentation forecasts with the background forecast. Specifically, in [14], object instance segmentation forecasts are always placed in front of the background segmentation forecast. This assumes that no background objects are located closer to the camera than any foreground entity, which is not true in practice.
In this work, we study a new method to address these two issues: 1) To jointly forecast object instance segmentations, we develop a modified attention module for transformer models. Specifically, instead of the inner-product attention in classical transformers, we propose “difference attention.”
This developed difference attention fits tasks like forecast-ing because it enables reasoning about velocities and ac-celeration, which is non-trivial with classical inner-product attention (see Fig. 1 top). 2) To properly reason about object and background placement, we develop a refinement head
which denoises background depth estimates and compares them against foreground predictions (see Fig. 1 middle).
We assess our method on the challenging Cityscapes [7] and AIODrive [46] datasets. We find difference attention and refinement to provide accurate results (see Fig. 1 bot-tom) which yield a new state-of-the-art of 37.6 PQ for mid-term forecasting on Cityscapes and 48.5 PQ on AIO-Drive. Code to reproduce results is available via https:
//github.com/cgraber/psf-diffattn. 2.