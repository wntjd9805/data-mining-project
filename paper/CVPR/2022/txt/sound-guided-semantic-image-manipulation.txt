Abstract 1.

Introduction
The recent success of the generative model shows that leveraging the multi-modal embedding space can manipu-late an image using text information. However, manipulat-ing an image with other sources rather than text, such as sound, is not easy due to the dynamic characteristics of the sources. Especially, sound can convey vivid emotions and dynamic expressions of the real world. Here, we propose a framework that directly encodes sound into the multi-modal (image-text) embedding space and manipulates an image from the space. Our audio encoder is trained to pro-duce a latent representation from an audio input, which is forced to be aligned with image and text representations in the multi-modal embedding space. We use a direct latent op-timization method based on aligned embeddings for sound-guided image manipulation. We also show that our method can mix different modalities, i.e., text and audio, which en-rich the variety of the image modification. The experiments on zero-shot audio classification and semantic-level image classification show that our proposed model outperforms other text and sound-guided state-of-the-art methods.
Image manipulation has been widely studied in the field of computer vision due to its usefulness in photo-realistic manipulation applications, social media image sharing, and image-based advertisement. An image can be used to trans-fer its style into the target image [14, 13]. Also, modifying specific parts in the human face image, such as hairstyle or color, is useful in image manipulation applications [49, 34].
The purpose of semantic image manipulation is to generate a novel image that contains both source image identifica-tion and semantic information of user intention. In this pa-Acknowledgement. This work was supported by the National Research Foun-dation of Korea grant (NRF-2021R1G1A1093855) and partially supported by Insti-tute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No. 2019-0-00079, Artificial Intelli-gence Graduate School Program(Korea University)). J. Kim is partially supported by the National Research Foundation of Korea grant (NRF-2021R1C1C1009608), Ba-sic Science Research Program (NRF-2021R1A6A1A13044830), and ICT Creative
Consilience program (IITP-2022-2022-0-01819). S. Yoon is supported by KAIST grant (G04210059). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agency.
∗Corresponding authors: Jinkyu Kim (jinkyukim@korea.ac.kr) and Sangpil
Kim (spk7@korea.ac.kr). Code and more diverse examples are available at https:
//kuai-lab.github.io/cvpr2022sound. 1
per, we tackle the semantic image manipulation task, which is the task of modifying an image with user-provided se-mantic cues. To apply the user intention into the image, a mixture of sketches and text is used to perform image ma-nipulation and synthesis [33, 49]. User intention can be ap-plied by drawing a paint [33] or writing text with semantic meanings [49, 13].
Text-based image manipulation methods are proposed to edit the image conditionally [11, 21, 26, 29, 49]. These works modify target contents in the image based on the text information. Among the text-based image manipula-tion methods, StyleCLIP [34] considered leveraging the representational power of Contrastive Language-Image Pre-training (CLIP) [37] models to produce text-relevant ma-nipulations with given text input. StyleCLIP maintains high quality image generation ability using StyleGAN [20] while allowing insertion of semantic text into the image.
However, text-based image manipulation has an inherent limitation when applying sound semantics into the image.
It is impossible to fully express vivid sound with discrete text since sound has continuous and dynamic properties. For example, every “thunder” generates different loudness and characteristic of “sound of thunder”. The infinite range of sound variations is not interchangeable with discrete text expressions. Therefore, the text-based image manipulation methods suffer from the discreteness of the text expression, limiting the transfer of specific and vivid sound semantics into the source image in image manipulation with sound se-mantics.
Several studies [7, 18, 30, 36, 47, 52] have attempted to visualize the meaning of sound, but it is still challeng-ing to reflect sound events in high-resolution images due to two reasons. The first reason is the lack of a suitable high-resolution audio-visual dataset. Audio-visual bench-mark video datasets [5, 24, 42] for GAN training has gener-ally lower resolution than high-resolution image datasets in-cluding Flickr-Faces-HQ (FFHQ) [22] and The Large-scale
Scene Understanding Challenge (LSUN) [50]. There is no dataset with as many audio-visual pairs as the number of image-text pairs used for CLIP training. CLIP uses 400 mil-lion image-text pair data to learn the relationship between very large and diverse image and text modalities, whereas audio-visual pair data is still insufficient. Secondly, it is difficult to discover potential correlations between auditory and visual modalities [52]. Extracting appropriate temporal context, tone, and theme from the sound is difficult.
To overcome these challenges of manipulating images with sound semantics, we introduce a novel image manipu-lation method driven by sound semantics (see Fig. 2). As shown in Fig. 1, an image of an old car is manipulated into an old car with a fire truck-like exterior appearance when adding a siren sound. Our model consists of two main stages: (i) the CLIP-based Multi-modal Representa-tion Learning, where an audio encoder is trained to pro-duce a latent representation aligned with textual and visual semantics by leveraging the representation power of pre-trained CLIP models. (ii) the Sound-Guided Image Manip-ulation, where we use the direct latent code optimization to produce a semantically meaningful image in response to a user-provided sound.
Our experimental results show that the proposed method supports a variety of sound sources with a better reflection of given audio information when transferring image styles.
The sound-based approach supports more diverse and de-tailed information related to scenes compared to text-based image manipulation methods. We illustrate diverse exam-ples in the supplemental material and project website.
Our main contributions are listed as follows:
• We propose multi-modal contrastive losses to expand the CLIP-based embedding space. Moreover, we in-troduce contrastive learning on augmented audio data, which helps to learn a more robust representation.
• We propose semantic-level image manipulation solely based on the given audio features, including temporal context, tone, and volume.
• We propose the sound-guided code optimization steps with adaptive layer masking for putting sound meaning into images, enhancing the realism of the output. 2.