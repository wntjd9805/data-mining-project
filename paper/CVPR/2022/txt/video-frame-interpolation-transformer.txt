Abstract
Existing methods for video interpolation heavily rely on deep convolution neural networks, and thus suffer from their intrinsic limitations, such as content-agnostic kernel weights and restricted receptive ﬁeld. To address these issues, we propose a Transformer-based video interpo-lation framework that allows content-aware aggregation weights and considers long-range dependencies with the self-attention operations. To avoid the high computational cost of global self-attention, we introduce the concept of local attention into video interpolation and extend it to the spatial-temporal domain. Furthermore, we propose a space-time separation strategy to save memory usage, which also improves performance. In addition, we develop a multi-scale frame synthesis scheme to fully realize the po-tential of Transformers. Extensive experiments demonstrate the proposed model performs favorably against the state-of-the-art methods both quantitatively and qualitatively on a variety of benchmark datasets. The code and models are released at https://github.com/zhshi0816/
Video-Frame-Interpolation-Transformer. 1.

Introduction
Video frame interpolation aims to temporally upsample an input video by synthesizing new frames between existing ones. It is a fundamental problem in computer vision that involves the understanding of motions, structures, and nat-ural image distributions, which facilitates numerous down-stream applications, such as image restoration [5, 52], vir-tual reality [1], and medical imaging [22].
Most of the state-of-the-art video frame interpolation methods are based on deep convolution neural networks (CNNs) [3, 20, 25, 29, 30, 32, 37, 53]. While achieving the state-of-the-art performance, these CNN-based archi-tectures usually suffer from two major drawbacks. First,
*These authors contributed equally.
†Corresponding author.
Figure 1. Comparison of performance and model size using the
Vimeo-90K dataset [54]. VFIT outperforms the state-of-the-art methods with fewer parameters. VFIT-S and VFIT-B denote the proposed small and base models. the convolution layer is content-agnostic, where the same kernels are used to convolve with different locations of different inputs. While this design can serve as a desir-able inductive bias for image recognition models to ac-quire translational equivalence [24], it is not always suitable for video interpolation which involves a complex motion-compensation process that is spatially-variant and content-dependent. Thus, adopting CNN backbones may restrict the ability of adaptive motion modeling and potentially limits further development of video interpolation models.
Second, capturing long-range dependencies is of central importance in video interpolation for which large motion
ﬁelds pose the most prominent challenges. However, most
CNNs [25, 53] usually employ small convolution kernels (typically 3×3 as suggested by VGG [39]), which is inef-ﬁcient in exploiting long-range information and thus less effective in synthesizing high-quality video frames. While it seems an easy ﬁx to use larger kernels in the convolu-tion layer, it signiﬁcantly increases the number of model parameters and computational cost, thereby leading to poor local minimums in training without proper regularizations.
Moreover, simply stacking multiple small kernel layers for a larger receptive ﬁeld does not fully resolve this problem either, as distant dependencies cannot be effectively learned in a multi-hop fashion [45].
On the other hand, Transformers [43], which are ini-tially designed for natural language processing (NLP) to efﬁciently model long-range dependencies between input and output, naturally overcome the above drawbacks of
CNN-based algorithms, and are in particular suitable for the task of video interpolation. Motivated by the success in
NLP, several methods recently adapt Transformers to com-puter vision and demonstrate promising results on various tasks, such as image classiﬁcation [13, 41], semantic seg-mentation [44], object detection [8], and 3D reconstruc-tion [51]. Nevertheless, how to effectively apply Trans-formers to video interpolation that involves an extra tem-poral dimension remains an open yet challenging problem.
In this work, we propose the Video Frame Interpolation
Transformer (VFIT) for effective video interpolation. Com-pared with typical Transformers [8, 9, 13] where the ba-sic modules are largely borrowed from the original NLP model [43], there are three distinguished designs in the proposed VFIT to generate photo-realistic and temporally-coherent frames. First, the original Transformer [43] is based on a self-attention layer that interacts with the in-put elements (e.g., pixels) globally. As this global op-eration has quadratic complexity with regard to the num-ber of elements, directly applying it to our task leads to extremely high memory and computational cost due to the high-dimensionality nature of videos. Several meth-ods [7, 9] circumvent this problem by dividing the feature maps into patches and treating each patch as a new element in the self-attention. However, this strategy cannot model
ﬁne-grained dependencies between pixels inside each patch which are important for synthesizing realistic details. More-over, it may introduce edge artifacts around patch borders.
In contrast, we introduce the local attention mechanism of
Swin [27] into VFIT to address the complexity issue while retaining the capability of modeling long-range dependen-cies with its shift-window scheme. We demonstrate that with proper development and adaptation, the local attention mechanism originally used for image recognition can effec-tively improve the video interpolation performance with a smaller amount of parameters as shown in Figure 1.
Second, the original local attention mechanism [27] is only suitable for image input and cannot be easily used for the video interpolation task where an extra temporal dimen-sion is involved. To address this issue, we generalize the concept of local attention to spatial-temporal domain, which leads to the Spatial-Temporal Swin attention layer (STS) that is compatible with videos. However, this simple exten-sion could lead to memory issues when using large window sizes. To make our model more memory efﬁcient, we fur-ther devise a space-time separable version of STS, called
Sep-STS, by factorizing the spatial-temporal self-attention.
Interestingly, Sep-STS not only effectively reduces mem-ory usage but also considerably improves video interpola-tion performance.
To exploit the full potential of our Sep-STS, we propose a new multi-scale kernel-prediction framework which can better handle multi-scale motion and structures in diverse videos, and generates high-quality video interpolation re-sults in a coarse-to-ﬁne manner. The proposed VFIT is concise, ﬂexible, light-weight, high-performing, fast, and memory-efﬁcient. As shown in Figure 1, a small model (VFIT-S) already outperforms the state-of-the-art FLAVR method [21] by 0.18 dB with only 17.7% of its parameters, while our base model (VFIT-B) achieves 0.66 dB improve-ment with 68.4% of its parameters. 2.