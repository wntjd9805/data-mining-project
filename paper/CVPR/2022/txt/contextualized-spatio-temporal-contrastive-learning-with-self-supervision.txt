Abstract
Modern self-supervised learning algorithms typically enforce persistency of instance representations across views. While being very effective on learning holistic image and video representations, such an objective becomes sub-optimal for learning spatio-temporally ﬁne-grained fea-tures in videos, where scenes and instances evolve through space and time. In this paper, we present Contextualized
Spatio-Temporal Contrastive Learning (ConST-CL) to ef-fectively learn spatio-temporally ﬁne-grained video repre-sentations via self-supervision. We ﬁrst design a region-based pretext task which requires the model to transform in-stance representations from one view to another, guided by context features. Further, we introduce a simple network de-sign that successfully reconciles the simultaneous learning process of both holistic and local representations. We evalu-ate our learned representations on a variety of downstream tasks and show that ConST-CL achieves competitive re-sults on 6 datasets, including Kinetics, UCF, HMDB, AVA-Kinetics, AVA and OTB. Our code and models will be avail-able at https://github.com/tensorflow/models/ tree/master/official/projects/const_cl. 1.

Introduction
Self-supervised learning (SSL) has revolutionized nat-ural language processing [12, 32] and computer vision [3, 4, 9, 23] due to strong representations learned from a vast amount of unlabeled data. The key breakthroughs that paved the way for SSL’s success in computer vision come from the instance discrimination pretext task [16] and the contrastive objective [39], with which for the ﬁrst time the self-supervised pretraining surpasses the supervised pre-training on downstream visual tasks [26]. For videos, many self-supervised contrastive learning approaches [3,4,18,43] directly extend established image-based methods [9, 26] to the spatio-temporal domain. Most of them, however, do not explicitly exploit the temporal evolutions of multiple in-stances and scene context in videos.
∗Work done as a student researcher at Google. (a) (b)
Figure 1. (a) A typical contrastive learning algorithm draws two augmented views {x, x(cid:48)} from one source s and trains an en-coder network f (·) to construct representations h and h(cid:48). A pro-jection function g(·) is trained to project representations into a shared space and to maximize the agreement between two views. (b) Contextualized Spatio-Temporal Contrastive Learning uses a binary projection function g(·, ·) to transform a representation h from one view to the other, guided by context features F (cid:48) c from the other view. The contrastive objective encourages the transformed representation z to agree with its correspondence h(cid:48).
Self-supervised learning methods typically enforce se-mantic consistency across views to construct instance rep-resentations [9, 26]. This assumption is particularly true in the image domain because two views are typically gen-erated from the same image. As shown in Fig. 1a, the goal is to enforce the representations of these two views to be as close as possible in the feature space. In the video domain, these view-based contrastive approaches [18, 43] may be less effective as the visual appearance of an in-stance frequently and drastically changes across frames.
For example, one person in a video can have different poses and perform different activities over time, indicating the states and semantics of an instance are likely to change across space and time. Enforcing spatio-temporal persis-tency throughout the video [18] would lead to representa-tions only encoding minimally shared information across frames, which may negatively impact spatio-temporally
ﬁne-grained downstream tasks.
Furthermore, existing self-supervised methods typically
focus on learning representations for holistic visual under-standing tasks [9,43], such as image classiﬁcation and video action recognition. For dense prediction tasks, such as ob-ject detection, action localization and tracking, those mod-els are enhanced by adding task speciﬁc heads. On the other hand, several approaches are designed to learn discrimina-tive local features for dense prediction tasks [22, 53, 60, 61], but their performances on holistic visual understandings are often compromised [58]. In light of this, we are interested in learning representations that can be applied to both holistic and local video tasks.
We propose Contextualized Spatio-Temporal Con-trastive Learning (ConST-CL), illustrated in Fig. 1b, to cir-cumvent the undesirable strong spatio-temporal persistency enforced by the global contrastive objective. ConST-CL learns semantically consistent but discriminative local rep-resentations for various video downstream tasks, ranging from spatio-temporal action localization and object tracking to action recognition. Speciﬁcally, we design a projection function g(·, ·) to take not only the instance feature but also the context feature into account, where the instance feature is extracted from the source view of a video, and the context feature is sampled from the target view. This task enforces the model to be context-aware in a video and thus is a good proxy to learn discriminative local representations.
To address the imbalanced capability of learning holis-tic and local video representations, we design a simple two-branch module to facilitate the network to learn high-quality video representations both globally and locally in one uni-ﬁed self-supervised learning scheme.
For
We evaluate the learned representations on a variety of downstream tasks. For holistic representations, we eval-uate with video action recognition on Kinetics400 [30], lo-UCF101 [47] and HMDB51 [31] datasets. cal representations, we conduct experiments with spatio-temporal action localization on AVA-Kinetics [35] and
AVA [24] datasets, and the single object tracking on the
OTB2015 [56] dataset. Our experimental results show that by pretraining with ConST-CL, the learned representations adapt well across all studied datasets, surpassing recently proposed methods that use either supervised pretraining or the self-supervised pretraing [18, 21, 43, 60].
The main contributions of this work are:
• A region-based contrastive learning framework for
ﬁne-grained spatio-temporal representation learning.
• A contextualized region prediction task that facilitates learning semantically consistent while locally discrin-imative video features.
• A simple network design to effectively reconcile si-multaneous holistic and local representation learning.
• Competitive performance on 6 benchmarks, includ-ing spatio-temporal action localization, object track-ing, and video action recognition. 2.