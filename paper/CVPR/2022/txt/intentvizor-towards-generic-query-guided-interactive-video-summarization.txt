Abstract
The target of automatic video summarization is to cre-ate a short skim of the original long video while preserv-ing the major content/events. There is a growing interest in the integration of user queries into video summarization or query-driven video summarization. This video summa-rization method predicts a concise synopsis of the original video based on the user query, which is commonly repre-sented by the input text. However, two inherent problems exist in this query-driven way. First, the text query might not be enough to describe the exact and diverse needs of the user. Second, the user cannot edit once the summaries are produced, while we assume the needs of the user should be subtle and need to be adjusted interactively. To solve these two problems, we propose IntentVizor, an interactive video summarization framework guided by generic multi-modality queries. The input query that describes the user’s needs are not limited to text but also the video snippets. We further represent these multi-modality ﬁner-grained queries as user ‘intent’, which is interpretable, interactable, ed-itable, and can better quantify the user’s needs. In this pa-per, we use a set of the proposed intents to represent the user query and design a new interactive visual analytic in-terface. Users can interactively control and adjust these mixed-initiative intents to obtain a more satisfying summary through the interface. Also, to improve the summariza-tion quality via video understanding, a novel Granularity-Scalable Ego-Graph Convolutional Networks (GSE-GCN) is proposed. We conduct our experiments on two benchmark datasets. Comparisons with the state-of-the-art methods verify the effectiveness of the proposed framework. Code and dataset are available at https://github.com/ jnzs1836/intent-vizor. 1.

Introduction
With the online explosive video content, an increas-ing need has been identiﬁed for automatic video summa-*equal contribution
Figure 1. Illustration of our IntentVizor framework. We take query
“Table” as an example. Generic queries, including text/video snip-pets related to “Table” are inputs of the model. The intent module transforms these queries into a probability distribution over the basis intents, followed by the summary module, which generates a video summary by combing the basis intents and their probability values. As the user can ﬁnd the underlying visual semantic mean-ing of each basis intents (e.g., in the ﬁgure, basis intent #1: the dining table; #2: the working table), they can adjust the distribu-tion of these basis intents through our proposed interface (Fig. 4) to satisfy their needs, and the ﬁnal generated summaries can be updated accordingly/iteratively. rization in recent years. Traditional video summariza-tion methods usually generate concise/representative sum-mary that contains the entities and events with high pri-ority from the video and with low repetition and redun-dancy using unsupervised [5, 15, 16, 20, 25, 38, 45], super-vised [7, 26, 27, 44, 47, 48, 51] and reinforcement learning ways [2, 49]. However, such a summary cannot satisfy the needs of users and be of low practical value. As the elon-gated video, especially when captured in the realistic sce-nario, may cover a wide range of topics, only fractional content of speciﬁc topics will meet the user’s needs. Based
on this observation, the user query-driven summarization model, which considers the user’s preference, has gradually attracted researchers’ attention. backbone, the two modules each has an intent head and a summary head separately.
To sum up, we structure our contributions as follows:
The basic idea for query-driven summarization is to use the text query to guide the generation of video sum-maries. A popular dataset for this query-driven summariza-tion was the textual query dataset, proposed by Sharghi et al. [29]. The summarization model proposed in the paper was trained to predict a subset of the video shots (5 sec-onds per shot) closely related to the textual query. For the follow-up works, the attention mechanism [22, 35, 36] and generative adversarial networks [46] based summarization models are also introduced to achieve better summarization performance. However, the performance of these models was still not satisfying as the textual query is not enough to represent the users’ preferences. To be more speciﬁc, ﬁrst, the user cannot express their detailed needs with few ﬁxed input textual queries at the very beginning of summariza-tion. They may have multiple needs and want to adjust the priority of different needs. Second, the textual query can be ambiguous. People can have different understandings of a word in the communication, let alone the model trained on a ﬁxed word dictionary. Therefore, the model should be interactive to loop users into the summarization, and other query formats (e.g., visual query) should be considered to better represent the user preference with lower ambiguity.
To propose a generic model for queries from different modalities and allow users to interact during the summa-rization process, in this paper, we propose a novel frame-work named as IntentVizor. We borrow the concept In-tent from the Information Retrieval (IR) community to de-ﬁne the users’ need, independent of the query modalities
[3, 17, 43]. However, our intent differs from the traditional deﬁnition in IR with different representation and extraction: (1) We represent the intent by an adjustable distribution over the basis intents rather than the pre-deﬁned categories [3] , taxonomies [4, 40] or in a distributed representation space
[12, 41]; The basis intents are deﬁned as the learned and basic components of the user’s needs. Compared with the traditional deﬁnitions [3, 4, 12, 40, 41], our method enables interactive manipulation, satisfying the user’s diverse and subtle needs. (2) We extract a uniﬁed intent from the queries of different modalities instead of only the textual query to avoid the ambiguity problem as mentioned before.
The intentVizor framework consists of two modules, i.e., the intent module for extracting the intent from the query and the summary module for summarizing the video with the intent. To effectively correlate the video fea-tures with the generic query/intent in the two modules, we design a ﬂexible network structure named Granularity-Scalable Ego-Graph Convolutional Network (GSE-GCN).
This GSE-GCN will work as a shared backbone for both the summary module and the intent module. Besides this
• To the best of our knowledge, our IntentVizor frame-work is the ﬁrst attempt to introduce generic queries to better satisfy the user’s diverse needs. We also pro-pose a novel dataset for the visual-query-guided video summarization based on UTE videos.
• We formulate the video summarization as an interac-tive process, where the user can ﬁne-tune its intent it-eratively with our proposed novel interface.
• We propose a novel GSE-GCN structure to effectively correlate the generic queries of multi-modalities with the input video. 2.