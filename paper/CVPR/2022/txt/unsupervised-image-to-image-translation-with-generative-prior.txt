Abstract
Unsupervised image-to-image translation aims to learn the translation between two visual domains without paired data. Despite the recent progress in image translation mod-els, it remains challenging to build mappings between com-plex domains with drastic visual discrepancies.
In this work, we present a novel framework, Generative Prior-guided UNsupervised Image-to-image Translation (GP-UNIT), to improve the overall quality and applicability of the translation algorithm. Our key insight is to lever-age the generative prior from pre-trained class-conditional
GANs (e.g., BigGAN) to learn rich content correspondences across various domains. We propose a novel coarse-to-fine scheme: we first distill the generative prior to capture a ro-bust coarse-level content representation that can link ob-jects at an abstract semantic level, based on which fine-level content features are adaptively learned for more accu-rate multi-level content correspondences. Extensive exper-iments demonstrate the superiority of our versatile frame-work over state-of-the-art methods in robust, high-quality and diversified translations, even for challenging and dis-tant domains. Code is available at https://github. com/williamyang1991/GP-UNIT. 1.

Introduction
Unsupervised image-to-image translation (UNIT) aims to translate images from one domain to another without paired data. Mainstream UNIT methods assume a bijec-tion between domains and exploit cycle-consistency [43] to build cross-domain mappings. Though good results are achieved in simple cases like horse-to-zebra translations, such assumption is often too restrictive for more general heterogeneous and asymmetric domains in the real world.
The performance of existing methods often degrades dra-matically in translations with large cross-domain shape and appearance discrepancies such as translating human faces to animal faces, limiting their practical applications.
Translating across domains with large discrepancies re-quires one to establish the translation at a higher semantic level [40]. For instance, to translate a human face to a cat face, one can use the more reliable correspondence of facial components such as the eyes between a human and a cat rather than on the local textures. In the more extreme case of distant domains, such as animals and man-made objects, a translation is still possible if their correspondence can be determined at a higher abstract semantic level, for example through affirming the frontal orientation of an object or the layout of an object within the image.
Establishing translations at different semantic levels de-mands a UNIT model’s ability to find accurate correspon-dences of different semantic granularity. This requirement is clearly too stringent since training a translation model with such a capability requires complex ground truth correspon-dences that either do not exist or are infeasible to collect.
In particular, we propose a versatile Gen-translation. erative Prior-guided UNsupervised Image-to-image Trans-lation framework (GP-UNIT) to expand the application scenarios of previous UNIT methods that mainly handles close domains. Our framework shows positive improve-ments over previous cycle-consistency-guided frameworks in: 1) capturing coarse-level correspondences across var-ious heterogeneous and asymmetric domains, beyond the ability of cycle-consistency guidance; 2) learning fine-level correspondences applicable to various tasks adaptively; and 3) retaining essential content features in the coarse-to-fine stages, avoiding artifacts from the source domain com-monly observed in cycle reconstruction.
In summary, our contributions are threefold:
• We propose a versatile GP-UNIT framework that pro-motes the overall quality and applicability of UNIT with BigGAN generative prior.
• We present an effective way of learning robust corre-spondences across non-trivially distant domains at a high semantic level via generative prior distillation.
• We design a novel coarse-to-fine scheme to learn cross-domain correspondences adaptively at different semantic levels. 2.