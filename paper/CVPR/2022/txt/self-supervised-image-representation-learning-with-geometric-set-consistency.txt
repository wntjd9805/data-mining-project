Abstract
Pre-training with Geometric Set Consistency
We propose a method for self-supervised image represen-tation learning under the guidance of 3D geometric consis-tency. Our intuition is that 3D geometric consistency priors such as smooth regions and surface discontinuities may im-ply consistent semantics or object boundaries, and can act as strong cues to guide the learning of 2D image represen-tations without semantic labels. Specifically, we introduce 3D geometric consistency into a contrastive learning frame-work to enforce the feature consistency within image views.
We propose to use geometric consistency sets as constraints and adapt the InfoNCE loss accordingly. We show that our learned image representations are general. By fine-tuning our pre-trained representations for various 2D image-based downstream tasks, including semantic segmentation, object detection, and instance segmentation on real-world indoor scene datasets, we achieve superior performance compared with state-of-the-art methods. 1.

Introduction
Self-supervised image representation learning is an im-portant problem in the field of computer vision and has been rapidly developed in recent years. Existing works in this area mainly focus on designing various pretext tasks to learn general and intrinsic image features in self-supervised manners [14, 18, 37]. Those pretext tasks are usually low-level and can capture general image properties that favor many downstream tasks, like image classification, semantic segmentation, object detection, instance segmentation, etc.
Due to its ability to learn from a large amount of unlabeled data, self-supervised representation learning has already be-come a standard training regime in many real-world appli-cations [2, 3, 15, 55].
Recently, researchers have started to use 3D data, usually
*This work was done when Nenglun Chen was an intern at Microsoft
Research Asia.
Object Detection
Instance Segmentation Semantic Segmentation
Figure 1. We introduce geometric consistency set to enforce the consistency within images for self-supervised image representa-tion learning. We show that, the image representations pre-trained in this way can improve the performance of some 2D downstream tasks including semantic segmentation, object detection and in-stance segmentation. represented by point clouds, meshes, or voxel grids, as guid-ance for learning image representations [18, 30]. Compared with 2D images, 3D data has complementary advantages for learning discriminative image features. Since 3D data is usually acquired by real-world scanning and reconstruction, and has the same dimension as the real-world scenes, learn-ing geometric structures from 3D data is much easier than 2D images. Moreover, complex occlusions, as well as the texture of objects will also affect the performance of image perception methods. Meanwhile, 3D data is occlusion-free, and geometric cues like smooth regions and sharp edges can be strong priors for semantic understanding. Thus it is nat-ural to use 3D geometric cues to favor the learning of image
representations.
Pioneering works, like Pri3D [18], mainly rely on multi-view pixel-level consistency or 2D-3D pixel to point consis-tency for learning image representations in a self-supervised manner. The image representations learned in this way are proven to have significantly better performance than learn-ing purely from 2D images for downstream tasks. Despite these great successes, the geometric consistency priors in 3D data (i.e. smooth regions or depth gaps) are not directly employed, which we demonstrate are strong cues and can significantly enhance the learning of image semantics.
In this paper, we propose to use geometric consistency to promote the learning of image representations. Our in-tuition is that 3D points within the same smooth or even planar regions may share similar semantics, while the dis-continuities or depth gaps in 3D space may imply seman-tic changes. Such geometric cues are directly observable in unlabeled 3D data. However, due to complex textures, these cues can be hardly learned from purely unlabeled 2D images. Based on the above intuition, we design a sim-ple yet effective method to learn the geometric consistency priors described above. Specifically, we leverage the con-tinuities and discontinuities of 3D data, and use the clus-tering method to cluster the 3D data into many local small segments, termed geometric consistency sets, to guide the learning of image representations in a self-supervised con-trastive way. Our method is simple and can be easily imple-mented. We show that by exploring geometric consistency in the self-supervised pre-training stage, the performance of downstream tasks can be significantly improved.
In the following, we summarize our main contributions: 1) We introduce geometric consistency into a contrastive learning framework for self-supervised image representa-tion learning. 2) We propose a simple yet effective multi-view con-trastive loss with geometric consistency sets to leverage the consistency within images. 3) We demonstrate superior performance on several downstream tasks compared with SOTA methods. 2.