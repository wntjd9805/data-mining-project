Abstract
Nowadays, robotics, AR, and 3D modeling applications attract considerable attention to single-view depth estima-tion (SVDE) as it allows estimating scene geometry from a single RGB image. Recent works have demonstrated that the accuracy of an SVDE method hugely depends on the diversity and volume of the training data. However,
RGB-D datasets obtained via depth capturing or 3D re-construction are typically small, synthetic datasets are not photorealistic enough, and all these datasets lack diver-sity. The large-scale and diverse data can be sourced from stereo images or stereo videos from the web. Typically being uncalibrated, stereo data provides disparities up to unknown shift (geometrically incomplete data), so stereo-trained SVDE methods cannot recover 3D geometry.
It was recently shown that the distorted point clouds obtained with a stereo-trained SVDE method can be corrected with additional point cloud modules (PCM) separately trained on the geometrically complete data. On the contrary, we propose GP2, General-Purpose and Geometry-Preserving training scheme, and show that conventional SVDE mod-els can learn correct shifts themselves without any post-processing, benefiting from using stereo data even in the geometry-preserving setting. Through experiments on dif-ferent dataset mixtures, we prove that GP2-trained mod-els outperform methods relying on PCM in both accuracy and speed, and report the state-of-the-art results in the general-purpose geometry-preserving SVDE. Moreover, we show that SVDE models can learn to predict geometrically correct depth even when geometrically complete data com-prises the minor part of the training set. 1.

Introduction
Single-view monocular depth estimation (SVDE) aims at estimating a dense depth map corresponding to an input
RGB image. Being a fundamental problem in computer vi-sion and visual understanding, it has numerous applications in autonomous driving, robotics navigation and manipula-tion, and augmented reality. Most applications request for approaches able to process an arbitrary RGB image and es-timate a depth map that allows recovering the 3D geometry of a scene (e.g. a point cloud can be reconstructed given camera calibration).
Early SVDE methods were designed to work either in indoor scenes [19] or in autonomous driving scenarios [8], thus being domain-specific. Accordingly, these methods demonstrate poor generalization ability in cross-domain ex-periments [25]. Overall, it is empirically proved that the generalization ability of SVDE models heavily depends on the diversity of the training data [4, 15, 25, 31, 35].
Recently, efforts have been made to obtain depth data from new sources: from computer simulation [2, 26, 28], via 3D reconstruction [15], or from stereo data. Among them, stereo videos [25, 31] and stereo images [10, 32, 33] collected from the web are the most diverse and accessible data sources. However, the depth data obtained from stereo videos is geometrically incomplete since stereo cameras’ intrinsic and extrinsic parameters are typically unknown.
Respectively, disparity from a stereo pair can be computed up to unknown shift and scale coefficients (UTSS). Such depth data can be used as a proxy for a ground truth depth map, but is insufficient to restore 3D geometry.
As a result, modern general-purpose SVDE methods trained on stereo data output predictions that cannot be used to recover 3D geometry [16, 25, 33]. Hence, we describe these methods as not geometry-preserving and their predic-tions as geometrically incorrect. The only SVDE method that is both general-purpose and geometry-preserving is
LeReS [35].
It addresses geometry-preserving depth es-timation with a multi-stage pipeline that recovers shift and focal length through predicted depth post-processing.
This post-processing is performed using trainable point-cloud modules (PCM), thus resulting in significant over-head. Moreover, training PCM requires geometrically com-plete data with known camera parameters which limits the possible sources of training data.
In this work, we introduce GP2, an end-to-end training
scheme and show that SVDE models can learn correct shift themselves and benefit from training on geometrically in-complete data while being geometry-preserving.
To obtain geometrically correct depth predictions, we do not employ additional models like PCM in LeReS [35]. In-stead, we use geometrically complete data to encourage an
SVDE model to predict geometrically correct depth maps while taking advantage of diverse and large-scale UTSS data. Thus, the proposed training scheme can be applied to make any SVDE model general-purpose and geometry-preserving. To prove this, we train different SVDE models using GP2 and evaluate them on unseen datasets.
We demonstrate that the LeReS SVDE model trained with our scheme outperforms the original LeReS+PCM
[35] trained on the same data. Then, we improve this re-sult by switching to the LRN-based [21] SVDE model, thus pushing the boundaries of general-purpose geometry-preserving SVDE even further.
Overall, our contribution is three-fold:
• In this paper, we present an end-to-end training scheme
GP2 that allows training a general-purpose geometry-preserving SVDE method;
• Through multiple zero-shot cross-dataset trials, we show that our training scheme improves both depth es-timates and point cloud reconstructions obtained with the existing SVDE methods. As a result, we set a new state-of-the-art in the general-purpose geometry-preserving SVDE with an SVDE model being much faster on inference than competing methods that use
PCM;
• In ablation studies, we show that an SVDE model learns to predict geometrically correct depth even when trained mainly on UTSS data with a small por-tion of the geometrically complete data.
The rest of the paper is organized as follows: in Sec. 2, we discuss the relationship between depth data and 3D ge-ometry and give an overview of existing general-purpose
SVDE approaches. Our training scheme is introduced in
Sec. 3. Sec. 4 is dedicated to experiments: there, we de-scribe our experimental protocol, report quantitative results, and demonstrate visualizations. 2.