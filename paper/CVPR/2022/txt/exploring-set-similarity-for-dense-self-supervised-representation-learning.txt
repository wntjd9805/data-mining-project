Abstract
By considering the spatial correspondence, dense self-supervised representation learning has achieved superior performance on various dense prediction tasks. However, the pixel-level correspondence tends to be noisy because of many similar misleading pixels, e.g., backgrounds. To address this issue, in this paper, we propose to explore set similarity (SetSim) for dense self-supervised representation learning. We generalize pixel-wise similarity learning to set-wise one to improve the robustness because sets contain more semantic and structure information. Specifically, by resorting to attentional features of views, we establish the corresponding set, thus filtering out noisy backgrounds that may cause incorrect correspondences. Meanwhile, these at-tentional features can keep the coherence of the same image across different views to alleviate semantic inconsistency.
We further search the cross-view nearest neighbours of sets and employ the structured neighbourhood information to enhance the robustness. Empirical evaluations demonstrate that SetSim surpasses or is on par with state-of-the-art meth-ods on object detection, keypoint detection, instance segmen-tation, and semantic segmentation. 1.

Introduction
Pretraining has become a widely-used paradigm in vari-ous computer vision tasks. Generally, models are first pre-trained on large-scale datasets (e.g., ImageNet [11]) and fine-tuned on the specific tasks. Recently, self-supervised pretraining has broken the dominance of the supervised Ima-geNet [11] pretraining on almost all downstream tasks includ-ing image classification [21], object detection [34], semantic segmentation [14, 15], etc.
In particular, state-of-the-art self-supervised representation learning methods [3, 5, 8, 17] mainly adopt the instance discrimination formulation as their pretext task to obtain transfer learning ability for downstream
*Corresponding author.
Figure 1. The comparison of existing pixel-wise correspondence with our proposed method. (a) Pixel-based: compare all combina-tions of pixel-wise features and maximize the most similar pairs. (b)
Geometry-based: force features in overlapping regions to remain constant and distinguish features from different locations. (c) Set-based: considering misleading features and semantic inconsistency in the former methods, we propose to explore set similarity across two views for dense self-supervised representation learning. By resorting to attentional features, SetSim constructs corresponding sets across two views (blue point & green point), which can filter out misleading features and keep the coherence of the same image across views. Furthermore, SetSim searches the cross-view nearest neighbours (blue point with red circle) to enhance the structured neighbourhood information. tasks. The main idea of these methods is to maximize the similarity of two data-augmented views of the same image, while minimizing the similarity of views generated from different images.
Since most of the self-supervised methods are designed for image-level tasks like image classification, they are sub-optimal for dense prediction tasks, e.g., object detection and semantic segmentation. To narrow this performance gap, dense self-supervised learning has been explored re-cently [32, 40, 46]. A popular way is to leverage intrinsic spatial information that a matching pair of pixel-wise fea-tures should remain constant over different viewing condi-tions. DenseCL [40] compares all combinations of pixel-wise features across two views and picks out the most similar pairs as spatial correspondences. Besides, VADeR [32] and
PixPro [46] adopt geometry-based correspondence, which means that two views’ pixel-wise features from the same lo-cation of the same image are treated as positive pairs, while features obtained from different locations are treated as neg-ative pairs.
Nonetheless, in general, the pixel-wise correspondence based on similarity or geometry information is more likely to be noisy. Specifically, if there is a pixel-wise feature vector, there would be many misleading features similar to it, thus causing incorrect correspondences, which are illustrated in
Figure 1(a). Besides, in Figure 1(b), the overlapping region of two views splits similar semantic features into two parts, which are pushed far in the embedding space by a learn-ing objective, resulting in spatial semantic inconsistency.
Although recently proposed methods have shown promis-ing transfer performance improvements on dense prediction tasks, the issue of establishing robust correspondence re-mains unsolved, therefore, we are motivated to seek further exploration on this issue in order to apply it to our research.
In this paper, we propose to explore set similarity (Set-Sim) across views for dense self-supervised representation learning. Considering that a set of pixel-wise features can represent more semantic and structure information than indi-vidual counterpart, we generalize pixel-wise similarity learn-ing to set-wise one to improve the robustness. In particular, based on the attention map, we construct the corresponding set, which contains pixels of two different views with similar semantic information. As shown in Figure 1(c), attention maps are able to reveal the salient objects, i.e., castle, in two views of the input image, which effectively keep the coherence of the input image. Since certain useful pixels are excluded from the set, we further search the cross-view nearest neighbours of one view’s set and enhance the struc-tured neighbourhood information. Finally, the model maps the pixels in the corresponding set to similar representations in the embedding space by a contrastive [18, 24] or cosine similarity [8] optimization function.
The proposed SetSim outperforms or is on par with the state-of-the-art methods on various dense prediction tasks, including object detection, keypoint detection, instance seg-mentation, and semantic segmentation. Compared to the
MoCo-v2 baseline, our method can significantly improve the localization and classification abilities on dense prediction tasks: +2.1AP b on VOC object detection, +1.3AP b on
COCO object detection, +0.4AP kp on COCO keypoint de-tection, +1.0AP m, +2.2AP m on COCO and Cityscapes in-stance segmentation, +2.6mIoU on VOC object segmenta-tion, +1.7mIoU, +1.6mIoU on ADE20K and Cityscapes semantic segmentation, respectively. 2.