Abstract
Batch normalization (BN) is a milestone technique in deep learning. It normalizes the activation using mini-batch statistics during training but the estimated population statis-tics during inference. This paper focuses on investigating the estimation of population statistics. We deﬁne the estimation shift magnitude of BN to quantitatively measure the differ-ence between its estimated population statistics and expected ones. Our primary observation is that the estimation shift can be accumulated due to the stack of BN in a network, which has detriment effects for the test performance. We fur-ther ﬁnd a batch-free normalization (BFN) can block such an accumulation of estimation shift. These observations mo-tivate our design of XBNBlock that replace one BN with BFN in the bottleneck block of residual-style networks. Experi-ments on the ImageNet and COCO benchmarks show that
XBNBlock consistently improves the performance of different architectures, including ResNet and ResNeXt, by a signiﬁ-cant margin and seems to be more robust to distribution shift. 1.

Introduction
Input normalization is extensively used in training neu-ral networks for decades [19] and shows good theoretical properties in optimization for linear models [20, 47]. It uses population statistics for normalization that can be calculated directly from the available training data. A natural idea is to extend normalization for the activation in a network. How-ever, normalizing activation is more challenging since the distribution of internal activation varies, which leads to the estimation of population statistics for normalization inaccu-rate [7, 28]. A network with activation normalized by the population statistics shows the training instability [12].
Batch normalization (BN) [16] addresses itself to normal-ize the activation using mini-batch statistics during train-ing, but the estimated population statistics during infer-ence/test. BN ensures the normalized mini-batch output standardized over each iteration, enabling stable training,
*Corresponding author. E-mail: huangleiAI@buaa.edu.cn (cid:28629)(cid:28641) (cid:28629)(cid:28641) (cid:28629)(cid:28641) (cid:28629)(cid:28641)
Dataset
ESM
Input
Layer 1
Layer 2
Layer 3
Layer 4 (a) Network with BN in each layer (cid:28629)(cid:28633)(cid:28641) (cid:28629)(cid:28641) (cid:28629)(cid:28633)(cid:28641) (cid:28629)(cid:28641)
Dataset
ESM
Input
Layer 1
Layer 2
Layer 3
Layer 4 (b) Network with BN and BFN mixed in different layers
Figure 1. Illustration of the main observations. The red rectangle and green round represent a linear and non-linear transformation, respectively. Given the training and test data with distribution shift, we show the distributions of normalized output after each BN layer during training and test, and calculate the magnitude of difference between the estimated population statistics and expected ones (refer to as ESM, and see Section 4.2 for details). efﬁcient optimization [1, 13, 16, 36] and potential general-ization [3, 13, 48]. It has been extensively used in varieties of architectures [9, 11, 44, 45, 50, 54], and successfully prolif-erated throughout various areas [12, 24, 35].
Despite the common success of BN, it still suffers from problems when applied in certain scenarios [4, 12]. One notorious limitation of BN is its small-batch-size problem
— BN’s error increases rapidly as the batch size becomes smaller [41,48]. Besides, a network with a naive BN gets sig-niﬁcantly degenerated performance, if there exists covariate shift between the training and test data [2, 22, 29, 37]. While these problems raise across different scenarios and contexts, the estimated population statistics of BN used for inference seems to be the link between them: 1) the small-batch-size problem of BN can be relieved if its estimated populations statistics are corrected during test [41, 43]; 2) and a model
is more robust for unseen domain data (corrupted images) if the estimated population statistics of BN are adapted based on the available test data [2, 22, 37].
This paper investigates the estimation of population statis-tics in a systematic way. We introduce expected popula-tion statistics of BN, considering the ill-deﬁned population statistics of the activation with a varying distribution during training (see Section 4.2 for details). We refer to as esti-mation shift of BN if its estimated population statistics do not equal to its expected ones, and design experiments to quantitatively investigate how the estimation shift affects a batch normalized network.
Our primary observation is that the estimation shift of BN can be accumulated in a network (Figure 1 (a)). This obser-vation provides clues to explain why a network with BN has signiﬁcantly degenerated performance under small-batch-size training, and why the population statistics of BN need to be adapted if there exists distribution shift for input data during test. We further ﬁnd that a batch-free normalization (BFN)—normalizing each sample independently without across batch dimension—can block the accumulation of the estimation shift of BN. This relieves the performance degen-eration of a network if a distribution shift occurs.
These observations motivate our design of XBNBlock that replaces one BN with BFN in the bottleneck of residual-style networks [9, 50]. We apply the proposed XBNBlock to ResNet [9] and ResNeXt [50] architectures and conduct experiments on the ImageNet [35] and COCO [24] bench-marks. XBNBlock consistently improves the performance for both architectures, with absolute gains of 0.6% ∼ 1.1% in top-1 accuracy for ImageNet, 0.86% ∼ 1.62% in bound-ing box AP for COCO using Faster R-CNN [34], and 0.56% ∼ 2.06% ( 0.22% ∼ 1.18%) in bounding box AP (mask AP) for COCO using Mask R-CNN [8]. Besides,
XBNBlock seems to be more robust to the distribution shift. 2.