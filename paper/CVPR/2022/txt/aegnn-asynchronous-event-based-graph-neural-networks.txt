Abstract
The best performing learning algorithms devised for event cameras work by first converting events into dense representations that are then processed using standard
CNNs. However, these steps discard both the sparsity and high temporal resolution of events, leading to high compu-tational burden and latency. For this reason, recent works have adopted Graph Neural Networks (GNNs), which pro-cess events as “static” spatio-temporal graphs, which are inherently ”sparse”. We take this trend one step further by introducing Asynchronous, Event-based Graph Neural Net-works (AEGNNs), a novel event-processing paradigm that generalizes standard GNNs to process events as “evolving” spatio-temporal graphs. AEGNNs follow efficient update rules that restrict recomputation of network activations only to the nodes affected by each new event, thereby signif-icantly reducing both computation and latency for event-by-event processing. AEGNNs are easily trained on syn-chronous inputs and can be converted to efficient, ”asyn-chronous” networks at test time. We thoroughly validate our method on object classification and detection tasks,
*these authors contributed equally where we show an up to a 200-fold reduction in compu-tational complexity (FLOPs), with similar or even better performance than state-of-the-art asynchronous methods.
This reduction in computation directly translates to an 8-fold reduction in computational latency when compared to standard GNNs, which opens the door to low-latency event-based processing.
Multimedia Material
For videos, code and more, visit our project page https://uzh-rpg.github.io/aegnn/. 1.

Introduction
Compared to standard frame-based cameras, which mea-sure absolute intensity at a synchronous rate, event-cameras only measure changes in intensity, and do this indepen-dently for each pixel, resulting in an asynchronous and bi-nary stream of events (Figure 1 (a)). These events mea-sure a highly compressed representation of the visual sig-nal and are characterized by microsecond-level latency and temporal resolution, a high dynamic range of up to 140
dB, low motion blur, and low power (milliwatts instead of watts). Due to these outstanding properties, event cam-eras are indispensable sensors in challenging application domains—such as robotics [9, 21,47,52], autonomous driv-ing [18, 51, 56], and computational photography [3, 44, 53, 54]—characterized by frequent high-speed motions, low-light and high-dynamic-range scenes, or in always-on ap-plications, where low power is needed, such as IoT video surveillance [23, 36]. A survey about applications and re-search in event-based vision can be found in [14].
The output of event cameras is inherently sparse and asynchronous, making them incompatible with traditional computer-vision algorithms designed for standard images.
This prompts the development of novel algorithms that optimally leverage the sparse and asynchronous nature of events.
In doing so, existing algorithms designed for event cameras have traded off latency and prediction per-formance. Filtering-based [39] [28] approaches process events sequentially, and, thus, can provide low-latency pre-dictions and a high temporal resolution. However, these ap-proaches usually rely on handcrafted filter equations, which do not scale to more complex tasks, such as object detec-tion or classification. Spiking Neural Networks (SNNs) are one instance of filtering-based models, which seek to learn these rules in a data-driven fashion, but are still in their in-fancy, lacking general and robust learning rules [19, 29, 49].
As a result, SNNs typically fail to solve more complex high-level tasks [2, 39, 41, 51]. Many of the challenges above can be avoided by processing events as batches. In fact, recent progress has been made by converting batches of events into dense, image-like representations and pro-cessing them using methods designed for images, such as convolutional neural networks (CNNs). By adopting this paradigm, learning-based methods using CNNs have made significant strides in solving computer vision tasks with events [17, 22, 34, 42, 44, 53, 57, 58].
However, while easy to process, treating events as image-like representations discards their sparse and asyn-chronous nature and leads to wasteful computation. This wasteful computation directly translates to higher power consumption and latency [1, 23, 37]. A recent line of work
[16] showed on an FPGA that by reducing the computa-tional complexity by a factor of 5, they could reduce the latency by a factor of 5 while reducing the power consump-tion by a factor of 4. Therefore, by eliminating waste-ful computation, we can expect significant decreases in the power consumption and latency of learning systems.
Currently, this wasteful computation is caused by two factors: On the one hand, due to the working principle of event cameras, they trigger predominantly at edges, while large texture-less or static regions remain without events.
Image representations typically encode these regions as ze-ros, which are then unnecessarily processed by standard neural networks. On the other hand, for each new event, standard methods would need to recompute all network activations. However, events only measure single pixel changes and, thus, leave most of the activations unchanged, leading to unnecessary recomputation of activations.
A recent line of work seeks to address both of these challenges by reducing the computational complexity of learning-based approaches while maintaining the high tem-poral resolution of events. A key ingredient to keeping high performance in this setting was the adoption of geometric learning methods, such as recursive point-cloud process-ing [48] or Asynchronous Sparse Convolutions [35]. In both works, standard neural networks were trained using batches of events, leveraging well-established learning techniques such as backpropagation, and then deploying them in an event-by-event fashion at test time, thus minimizing com-putation. However, both of these methods suffer from limi-tations: While [48] does not perform hierarchical learning, limiting scalability to complex tasks, [35], relies on a spe-cific type of input representation, which discards the tem-poral information of events.
In this work, we introduce Asynchronous, Event-based
Graph Neural Networks (AEGNN), a neural network archi-tecture geared toward processing events as graphs in a se-quential manner (Fig. 1). For each new event, our method only performs local changes to the activations of the GNN, and propagates these asynchronously to lower layers. Sim-ilar to [35, 48], AEGNNs can be trained on batches of events—thus leveraging backpropagation—and can later be deployed in an asynchronous mode, generating the identical output. However, they address the key limitations of previ-ous work: (i) They allow hierarchical learning using stan-dard graph neural networks and (ii) model events as spatio-temporal graphs, thus retaining their temporal information, instead of discarding it. This leads to significant computa-tional savings. We summarize our contributions as follows:
• We introduce AEGNN, a novel paradigm for process-ing events sparsely and asynchronously as temporally evolving graphs. This allows us to process events effi-ciently, without sacrificing their sparsity and high tem-poral resolution.
• (ii) We derive efficient update rules, which allow us to simply train AEGNNs on synchronous event-data, and then deploy them in an asynchronous mode during test-time. These rules are general and can be applied to most existing graph neural network architectures.
• (iii) We apply AEGNNs on object recognition and ob-ject detection benchmarks. For object detection, we show similar performance to state-of-the-art methods, while requiring up to 200 times less compute, while for object detection we show a 21-fold computation reduc-tion with an up to 3.4% increase in terms of mAP.
2.