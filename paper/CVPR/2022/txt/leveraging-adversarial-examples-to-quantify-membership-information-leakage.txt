Abstract
The use of personal data for training machine learning systems comes with a privacy threat and measuring the level of privacy of a model is one of the major challenges in ma-chine learning today.
Identifying training data based on a trained model is a standard way of measuring the pri-vacy risks induced by the model. We develop a novel ap-proach to address the problem of membership inference in pattern recognition models, relying on information provided by adversarial examples. The strategy we propose consists of measuring the magnitude of a perturbation necessary to build an adversarial example. Indeed, we argue that this quantity reﬂects the likelihood of belonging to the training data. Extensive numerical experiments on multivariate data and an array of state-of-the-art target models show that our method performs comparable or even outperforms state-of-the-art strategies, but without requiring any additional training samples. 1.

Introduction
With the deluge of data and increase of computational power within the last decades, performance of modern ma-chine learning shows dramatic improvement in a wide range of applications such as computer vision [15,23,42] and nat-ural language processing [6, 13, 62]. Along with this im-provements new methods emerge, leading to remarkable change in societal applications ranging from industry [7,25] to modern medicine [24, 35, 59] to art [16, 22], all of which may be considered sensitive domains, given the nature of the data.
While the beneﬁts of machine learning are set upfront,
*Equal contribution. other societal aspects such as fairness [38, 57] or safety [5] should not be trampled on [4]. It is common consensus that models require vast amounts of training data [2,12] to reach state-of-the-art performance; meanwhile they do not neces-sarily guarantee the anonymity of the data provider [20].
This represents a serious privacy issue and controlling such leakage of information with modern regulation presents a new challenge [18, 37]. With recent data protection regula-tions [1,43] personal data are required to be protected while being used by machine learning models [54]. To improve safety in machine learning, the study of attack strategies that exploit models in order to infer training data, or even corrupt them, has become an active area of research.
In this paper we investigate membership inference at-tacks (MIAs) [32–34,46,48–51,55,63], in which an attacker tries to determine whether or not a sample was part of the training set of a target model [49]. By leveraging adversar-ial attacks, we propose an MIA strategy that achieves sim-ilar performance to the state-of-the-art, but without using training samples to construct the attack. We only require accessing the target model and the testing sample.
Adversarial attacks maximize the loss function of a model with respect to an input sample, in order to ﬁnd a per-turbation that changes the class predicted by the model. In-terestingly, we empirically observed that changing the pre-dicted class requires a larger perturbation for samples that are part of the training set since the model was tuned to minimize the empirical loss function computed using these samples. Hence the idea is to measure this perturbation, i.e., the distance between an adversarial example and its origi-nal counterpart, and test whether it is lager than a certain threshold. We call this measure Adversarial Distance. Fig-ure 1 provides an overview of our strategy1. 1The illustrations for the pipeline’s input and adversarial noise are pro-vided by [17]. The noise illustrated in Figure 1 is obtained with a fast
In contrast to other recent works [40, 44, 49], which pro-vide the attacker with a subset of the training set of the tar-get model, our approach does not require any training data.
Intuitively, if a model is susceptible to MIAs without re-sorting to training resources, we would expect it to be even more vulnerable in presence of additional data. As a matter of fact, we will show that in many cases additional sam-ples are not necessary in order to accurately determine the membership of target samples. 1.1. Contributions
Below we list the contributions of this work:
• We propose a novel MIA (Sec. 3) that performs con-sistently well regardless of the architecture of the tar-get model, and does not require training samples. This strategy exploits the distance between adversarial ex-amples and the corresponding raw inputs.
• We perform a thorough revision of MIA strategies pre-viously proposed in the literature and evaluate their performance (Sec. 5). Through this evaluation we show that several well-known machine learning mod-els are vulnerable to MIAs.
• Empirically, we show that in most of the investigated scenarios the proposed MIA outperforms, or it is at least competing with, state-of-the-art methods that rely on a large amount of data samples to perform the attack. On the other hand, for large models (e.g.,
DenseNet), we observe that training samples can grant a signiﬁcant advantage to the attacker. 1.2.