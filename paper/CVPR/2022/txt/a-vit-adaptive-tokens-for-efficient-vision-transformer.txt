Abstract 1.

Introduction
We introduce A-ViT, a method that adaptively adjusts the inference cost of vision transformer (ViT) for images of different complexity. A-ViT achieves this by automatically reducing the number of tokens in vision transformers that are processed in the network as inference proceeds. We refor-mulate Adaptive Computation Time (ACT [17]) for this task, extending halting to discard redundant spatial tokens. The appealing architectural properties of vision transformers enables our adaptive token reduction mechanism to speed up inference without modifying the network architecture or inference hardware. We demonstrate that A-ViT requires no extra parameters or sub-network for halting, as we base the learning of adaptive halting on the original network parame-ters. We further introduce distributional prior regularization that stabilizes training compared to prior ACT approaches.
On the image classiﬁcation task (ImageNet1K), we show that our proposed A-ViT yields high efﬁcacy in ﬁltering in-formative spatial features and cutting down on the overall compute. The proposed method improves the throughput of
DeiT-Tiny by 62% and DeiT-Small by 38% with only 0.3% accuracy drop, outperforming prior art by a large margin.
Transformers have emerged as a popular class of neural network architecture that computes network outputs using highly expressive attention mechanisms. Originated from the natural language processing (NLP) community, they have been shown effective in solving a wide range of problems in
NLP, such as machine translation, representation learning, and question answering [2, 9, 22, 35, 44]. Recently, vision transformers have gained an increasing popularity in the vi-sion community and they have been successfully applied to a broad range of vision applications, such as image classiﬁca-tion [11, 16, 32, 43, 48, 55], object detection [3, 7, 39], image generation [20, 21], and semantic segmentation [28, 52]. The most popular paradigm remains when vision transformers form tokens via splitting an image into a series of ordered patches and perform inter-/intra-calculations between tokens to solve the underlying task. Processing an image with vision transformers remains computationally expensive, pri-marily due to the quadratic number of interactions between tokens [36, 40, 53]. Therefore, deploying vision transformers on data processing clusters or edge devices is challenging amid signiﬁcant computational and memory resources.
Project page at https://a-vit.github.io/.
The main focus of this paper is to study how to automati-                           
cally adjust the compute in visions transformers as a function of the complexity of the input image. Almost all mainstream vision transformers have a ﬁxed cost during inference that is independent from the input. However, the difﬁculty of a prediction task varies with the complexity of the input image.
For example, classifying a car versus a human from a single image with a homogeneous background is relatively simple; while differentiating between different breeds of dogs on a complex background is more challenging. Even within a single image, the patches that contain detailed object fea-tures are far more informative compared to those from the background. Inspired by this, we develop a framework that adaptively adjusts the compute used in vision transformers based on the input.
The problem of input-dependent inference for neural net-works has been studied in prior work. Graves [17] proposed adaptive computation time (ACT) to represent the output of the neural module as a mean-ﬁeld model deﬁned by a halting distribution. Such formulation relaxes the discrete halting problem to a continuous optimization problem that minimizes an upper bound on the total compute. Recently, stochastic methods were also applied to solve this problem, leveraging geometric-modelling of exit distribution to en-able early halting of network layers [1]. Figurnov et al. [13] proposed a spatial extension of ACT that halts convolutional operations along the spatial cells rather than the residual layers. This approach does not lead to faster inference as high-performance hardware still relies on dense computa-tions. However, we show that the vision transformer’s uni-form shape and tokenization enable an adaptive computation method to yield a direct speedup on off-the-shelf hardware, surpassing prior work in efﬁciency-accuracy tradeoff.
In this paper, we propose an input-dependent adaptive inference mechanism for vision transformers. A naive ap-proach is to follow ACT, where the computation is halted for all tokens in a residual layer simultaneously. We observe that this approach reduces the compute by a small margin with an undesirable accuracy loss. To resolve this, we propose
A-ViT, a spatially adaptive inference mechanism that halts the compute of different tokens at different depths, reserv-ing compute for only discriminative tokens in a dynamic manner. Unlike point-wise ACT within convolutional fea-ture maps [13], our spatial halting is directly supported by high-performance hardware since the halted tokens can be efﬁciently removed from the underlying computation. More-over, entire halting mechanism can be learnt using existing parameters within the model, without introducing any extra parameters. We also propose a novel approach to target dif-ferent computational budgets by enforcing a distributional prior on the halting probability. We empirically observe that the depth of the compute is highly correlated with the object semantics, indicating that our model can ignore less relevant background information (see quick examples in Fig. 1 and more examples in Fig. 3). Our proposed approach signiﬁ-cantly cuts down the inference cost – A-ViT improves the throughput of DEIT-Tiny by 62% and DEIT-Small by 38% with only 0.3% accuracy drop on ImageNet1K.
Our main contributions are as follows:
• We introduce a method for input-dependent inference in vision transformers that allows us to halt the compu-tation for different tokens at different depth.
• We base learning of adaptive token halting on the exis-tent embedding dimensions in the original architecture and do not require extra parameters or compute for halting.
• We introduce distributional prior regularization to guide halting towards a speciﬁc distribution and average token depth that stabelizes ACT training.
• We analyze the depth of varying tokens across different images and provide insights into the attention mecha-nism of vision transformer.
• We empirically show that the proposed method im-proves throughput by up to 62% on hardware with mi-nor drop in accuracy. 2.