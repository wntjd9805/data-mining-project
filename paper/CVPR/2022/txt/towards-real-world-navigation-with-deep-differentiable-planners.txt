Abstract
We train embodied neural networks to plan and navigate unseen complex 3D environments, emphasising real-world deployment. Rather than requiring prior knowledge of the agent or environment, the planner learns to model the state transitions and rewards. To avoid the potentially hazardous trial-and-error of reinforcement learning, we focus on dif-ferentiable planners such as Value Iteration Networks (VIN), which are trained offline from safe expert demonstrations.
Although they work well in small simulations, we address two major limitations that hinder their deployment. First, we observed that current differentiable planners struggle to plan long-term in environments with a high branching complexity. While they should ideally learn to assign low rewards to obstacles to avoid collisions, these penalties are not strong enough to guarantee collision-free operation. We thus impose a structural constraint on the value iteration, which explicitly learns to model impossible actions and noisy motion. Secondly, we extend the model to plan exploration with a limited perspective camera under translation and fine rotations, which is crucial for real robot deployment. Our proposals significantly improve semantic navigation and ex-ploration on several 2D and 3D environments, succeeding in settings that are otherwise challenging for differentiable planners. As far as we know, we are the first to successfully apply them to the difficult Active Vision Dataset, consisting of real images captured from a robot.1 1.

Introduction
Continuous advances in robotics have enabled robots to be deployed to a wide range of scenarios, from manufactur-ing in factories and cleaning in households, to the emerging applications of autonomous vehicles and delivery drones [2].
Improving their autonomy is met with many challenges, due to the difficulty of planning from uncertain sensory data. In classical robotics, the study of planning has a long tradi-1Code available: https://github.com/shuishida/calvin (1st column) Input images seen during a run of our
Figure 1. method on AVD (Sec. 5.2.2). This embodied neural network has learned to efficiently explore and navigate unseen indoor environ-ments, to seek objects of a given class (highlighted in the last image). (2nd-3rd columns) Predicted rewards and values (resp.), for each spatial location (higher for brighter values). The unknown optimal trajectory is dashed, while the robot’s trajectory is solid. tion [44], using detailed knowledge of a robot’s configura-tion and sensors, with little emphasis on learning from data.
An almost orthogonal approach is to use deep learning, an intensely data-driven, non-parametric approach [11]. Mod-ern deep neural networks excel at pattern recognition [40], although they do not offer a direct path to planning applica-tions. While one approach would be to parse a scene into pre-defined elements (e.g. object classes and their poses) to be passed to a more classical planner, an end-to-end ap-proach where all modules are learnable has the chance to improve with data, and be adaptable to novel settings with no manual tuning. Because of the data-driven setup, a deep network has the potential to learn behaviour that leverages the biases of the environment, such as likely locations for certain types of rooms. Value Iteration Networks (VINs) [43] emerged as an elegant way to merge classical planning and data-driven deep networks, by defining a differentiable plan-ner. Being sub-differentiable, like all other elements of a deep network, allows the planner to include learnable ele-ments, trained end-to-end from example data. For example, it can learn to identify and avoid obstacles, and to recognise and seek classes of target objects, without explicitly labelled examples. However, there are gaps between VIN’s idealised formulation and realistic robotics scenarios, which some works address [12, 21]. The CNN-based VIN [43] considers that the full environment is visible and expressible as a 2D grid. As such, it does not account for embodied (first-person) observations in 3D spaces, unexplored and partially-visible environments, or the mismatch between egocentric observa-tions and idealised world-space discrete states.
In this paper we address these challenges, and close the gap between current differentiable planners and realistic robot navigation applications. Our contributions are: 1. A constrained transition model for value iteration, fol-lowing a rigorous probabilistic formulation, which explicitly models illegal actions and task termination (Sec. 4.1). This is our main contribution. 2. A 3D state-space for embodied planning through the robot’s translation and rotation (Sec. 4.2.1). Planning through fine-grained rotations is often overlooked (Sec. 2), requiring better priors for transition modelling (Sec. 4.1.2). 3. A trajectory reweighting scheme that addresses the unbal-anced nature of navigation training distributions (Sec. 4.1.5). 4. We demonstrate for the first time that differentiable plan-ners can learn to navigate in both complex 3D mazes, and the challenging Active Vision Dataset [1], with images from a real robot platform (Sec. 5.2.2). Thus our method can be trained with limited data collected offline, as opposed to limitless data from a simulator as in prior work.
Sec. 2 discusses related work, while Sec. 3 gives a short introduction to differentiable planning. Sec. 4 presents our technical proposals, and Sec. 5 evaluates them. 2.