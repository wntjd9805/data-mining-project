Abstract
Group activity recognition is the task of understanding the activity conducted by a group of people as a whole in a multi-person video. Existing models for this task are often impractical in that they demand ground-truth bounding box labels of actors even in testing or rely on off-the-shelf object detectors. Motivated by this, we propose a novel model for group activity recognition that depends neither on bound-ing box labels nor on object detector. Our model based on Transformer localizes and encodes partial contexts of a group activity by leveraging the attention mechanism, and represents a video clip as a set of partial context embed-dings. The embedding vectors are then aggregated to form a single group representation that reflects the entire context of an activity while capturing temporal evolution of each par-tial context. Our method achieves outstanding performance on two benchmarks, Volleyball and NBA datasets, surpass-ing not only the state of the art trained with the same level of supervision, but also some of existing models relying on stronger supervision. 1.

Introduction
Group activity recognition (GAR) is the task of classify-ing the activity that a group of people are doing as a whole in a given video clip. It has attracted increasing attention due to a variety of its applications including sports video analysis, video surveillance, and social scene understand-ing. Unlike the conventional action recognition that focuses on understanding individual actions [9, 16, 18, 26, 31, 35, 44, 46, 50, 52], GAR demands comprehensive and precise un-derstanding of interactions between multiple actors, which introduces inherent challenges such as localization of actors and modeling their spatio-temporal relations.
Due to the difficulty of the task, most of existing meth-ods for GAR [15,17,21,23,29,36,53,55,57] require ground-truth bounding boxes of individual actors for both training and testing, and their action class labels for training. In par-ticular, the bounding box labels are used to extract features of individual actors (e.g., RoIPool [39] and RoIAlign [19])
Figure 1. Visualization of partial contexts captured by a token across time. The token in this example focuses on how players behave after they conceded a goal. (a) Right after the event 3p-succ, the timer has been reset to 24 secs and a defender stares at the ball. (b) Players prepare for the next attack, while a referee and a cameraman point at who takes the ball. (c) A player initiates the next attack. With such tokens each representing different pieces of the whole group activity, our model acquires the encapsulated semantics of the target activity. and discover their spatio-temporal relations precisely; such actor features are aggregated while considering the relations between actors to form a group-level video representation, which is in turn fed to a group activity classifier. Though these methods have demonstrated impressive performance on the challenging task, their dependence on the heavy an-notations, especially bounding boxes at inference, is im-practical and in consequence restricts their applicability sig-nificantly.
One way to resolve this issue is to jointly learn group ac-tivity recognition and person detection using bounding box labels [6, 60] to estimate bounding boxes of actors at in-ference. This approach however still requires ground-truth bounding boxes of individual actors for training videos. To further reduce the annotation cost, Yan et al. [56] introduced weakly supervised GAR (WSGAR) that does not demand actor-level labels at both training and inference. They ad-dress the lack of bounding box labels by generating actor box proposals through a detector pretrained on an external dataset and learning to prune irrelevant proposals.
The detector-based WSGAR however has several draw-backs as follows. First of all, a detector often suffers
from occlusion and background clutter, and thus frequently causes missing and false detections that degrade GAR accu-racy. Second, the detector-based approach loses contextual information that are useful for GAR since it concentrates only on people; in sports video analysis, for example, enti-ties other than people, such as a ball and a scoreboard, may provide crucial information for the task. Third, object de-tection is costly to itself and imposes additional overheads in both computation and memory.
In this paper, we propose a detector-free model for WS-GAR that depends neither on ground-truth bounding boxes nor on object detector. It bypasses explicit object detection by drawing attention on entities involved in a group activ-ity through a Transformer encoder [48] placed on top of a convolutional neural network (CNN) backbone. Specif-ically, we define learnable tokens as input to the encoder so that each of them learns to localize partial contexts of a group activity through the attention mechanism of the en-coder; the tokens capture not only key actors but also other useful clues as shown in Fig. 1. Since a set of learnable tokens are shared for all frames, a predefined number of token embeddings are computed by the encoder for every frame. A video clip is then represented as a bag of token embeddings, which are aggregated into a group representa-tion in two steps: Those computed from the same token at different frames are first aggregated to capture the temporal evolution of each token, then the results are fused to form a single feature vector for group activity classification.
In addition, for further performance improvement, the backbone of our model is designed to compute motion-augmented features. Unlike previous work on GAR [4, 17, 29, 36], it does not rely on off-the-shelf optical flow that is prohibitively expensive and thus has been a computational bottleneck. Instead, inspired by recent video representation architectures [16, 26, 27, 35, 49], it learns to capture motion information in feature levels by embedding local correlation between the feature maps of two adjacent frames.
We evaluate the proposed framework on two datasets,
Volleyball [23] and NBA [56]. Our framework achieves the state-of-the-art performance on the two benchmarks in the weakly supervised learning setting, and is as competitive as existing methods relying on stronger supervision such as ground-truth bounding boxes and individual action class labels. The contribution of this paper is three-fold:
• We present the first detector-free method dedicated to
WSGAR, which demands neither ground-truth bound-ing box labels nor object detector.
• We propose a novel Transformer-based model that cap-tures key actors and objects involved in a group activity through the attention mechanism. Moreover, our model is carefully designed to capture their temporal dynamics to produce a rich group-level video feature.
• On the two benchmarks, the proposed method largely outperforms existing WSGAR models. Also, it even beats early GAR models that depend on stronger super-vision than ours. 2.