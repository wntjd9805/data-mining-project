Abstract
This paper aims to address the problem of pre-training for person re-identification (Re-ID) with noisy labels. To setup the pre-training task, we apply a simple online multi-object tracking system on raw videos of an existing un-labeled Re-ID dataset “LUPerson” and build the Noisy
Labeled variant called “LUPerson-NL”. Since theses ID labels automatically derived from tracklets inevitably con-tain noises, we develop a large-scale Pre-training frame-work utilizing Noisy Labels (PNL), which consists of three learning modules: supervised Re-ID learning, prototype-based contrastive learning, and label-guided contrastive learning. In principle, joint learning of these three mod-ules not only clusters similar examples to one prototype, but also rectifies noisy labels based on the prototype as-signment. We demonstrate that learning directly from raw videos is a promising alternative for pre-training, which utilizes spatial and temporal correlations as weak super-vision. This simple pre-training task provides a scalable way to learn SOTA Re-ID representations from scratch on
“LUPerson-NL” without bells and whistles. For example, by applying on the same supervised Re-ID method MGN, our pre-trained model improves the mAP over the unsu-pervised pre-training counterpart by 5.7%, 2.2%, 2.3% on
CUHK03, DukeMTMC, and MSMT17 respectively. Under the small-scale or few-shot setting, the performance gain is even more significant, suggesting a better transferability of the learned representation. Code is available at https:
//github.com/DengpanFu/LUPerson-NL. 1.

Introduction
A large high-quality labeled dataset for person re-identification (Re-ID) is labor intensive and costly to cre-ate. Existing fully labeled datasets [25, 52, 58, 61] for per-son Re-ID are all of limited scale and diversity compared to other vision tasks. Therefore, model pre-training be-*Corresponding author. (a) Market1501 with MGN (b) Market1501 with IDE (c) DukeMTMC with MGN (d) DukeMTMC with IDE
Figure 1. Comparing person Re-ID performances of three pre-trained models on two methods (IDE [59] and MGN [51]). Re-sults are reported on Market1501 and DukeMTC, with different scales under the small-scale setting. IN.sup. refers to the model supervised pre-trained on ImageNet, LUP.unsup. is the model un-supervised pre-trained on LUPserson, and LUPnl.pnl. is the model pre-trained on our LUPerson-NL dataset using our proposed PNL. comes a crucial approach to achieve good Re-ID perfor-mance. However, due to the lack of large-scale Re-ID dataset, most previous methods simply use the models pre-trained on the crowd-labeled ImageNet dataset, resulting in a limited improvement because of the big domain gap be-tween generic images in ImageNet and person-focused im-ages desired by the Re-ID task. To mitigate this problem, the recent work [12] has demonstrated that unsupervised pre-training on a web-scale unlabeled Re-ID image dataset
“LUPerson” (sub-sampled from massive streeview videos) surpasses that of pre-training on ImageNet.
In this paper, our hypothesis is that scalable ReID pre-training methods that learn directly from raw videos can generate better representations. To verify it, we propose the noisy labels guided person Re-ID pre-training, which lever-ages the spatial and temporal correlations in videos as weak
supervision. This supervision is nearly cost-free, and can be achieved by the tracklets of a person over time derived from any multi-object tracking algorithm, such as [56]. In par-ticular, we track each person in consecutive video frames, and automatically assign the tracked persons in the same tracklet to the same Re-ID label and vice versa. Enabled by the large amounts of raw videos in LUPerson [12], publicly available data of this form on the internet, we create a new variant named “LUPerson-NL” with derived pseudo Re-ID labels from tracklets for pre-training with noisy labels. This variant totally consists of 10M person images from 21K scenes with noisy labels of about 430K identities.
We demonstrate that contrastive pre-training of Re-ID is an effective method of learning from this weak supervision at large scale. This new Pre-training framework utilizing
Noisy Labels (PNL) composes three learning modules: (1) a simple supervised learning module directly learns from
Re-ID labels through classification; (2) a prototype-based contrastive learning module helps cluster instances to the prototype which is dynamically updated by moving aver-aging the centroids of instance features, and progressively rectify the noisy labels based on the prototype assignment. and (3) a label-guided contrastive learning module utilizes the rectified labels subsequently as the guidance. In contrast to the vanilla momentum contrastive learning [7,12,19] that treats only features from the same instance as positive sam-ples, our label-guided contrastive learning uses the rectified labels to distinguish positive and negative samples accord-ingly, leading to a better performance. In principle, joint learning of these three modules make the consistency be-tween the prototype assignment from instances and the high confident (rectified) labels, as possible as it can.
The experiments show that our PNL model achieves re-markable improvements on various person Re-ID bench-marks. Figure 1 indicates that the performance gain from our pre-trained models is consistent on different scales of training data. For example, upon the strong MGN [51] baseline, our pre-trained model improves the mAP by 4.4%, 4.9% on Market1501 and DukeMTMC over the Im-ageNet supervised one, and 0.9%, 2.2% over the unsuper-vised pre-training baseline [12]. Moreover, the gains are even larger under the small-scale and few-shot settings, where the labeled Re-ID data are extremely limited. To the best of our knowledge, we are the first to show that large-scale noisy label guided pre-training can significantly ben-efit person Re-ID task.
Our key contributions can be summarized as follows:
• We propose noisy label guided pre-training for person Re-ID, which incorporates supervised learning, prototype-based contrastive learning, label-guided contrastive learn-ing and noisy label rectification to a unified framework.
• We construct a large-scale noisy labeled person Re-ID dataset “LUPerson-NL” as a new variant of “LUPerson”.
It is by far the largest noisy labeled person Re-ID dataset without any human labeling effort.
• Our models pre-trained on LUPerson-NL push the state-of-the-art results on various public benchmarks to a new limit without bells and whistles. 2.