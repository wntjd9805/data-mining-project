Abstract
We study the automatic generation of navigation instruc-tions from 360◦ images captured on indoor routes. Existing generators suffer from poor visual grounding, causing them to rely on language priors and hallucinate objects. Our
MARKY-MT5 system addresses this by focusing on visual landmarks; it comprises a ﬁrst stage landmark detector and a second stage generator–a multimodal, multilingual, mul-titask encoder-decoder. To train it, we bootstrap grounded landmark annotations on top of the Room-across-Room (RxR) dataset. Using text parsers, weak supervision from
RxR’s pose traces, and a multilingual image-text encoder trained on 1.8b images, we identify 971k English, Hindi and
Telugu landmark descriptions and ground them to speciﬁc regions in panoramas. On Room-to-Room, human wayﬁnd-ers obtain success rates (SR) of 71% following MARKY-MT5’s instructions, just shy of their 75% SR following hu-man instructions—and well above SRs with other genera-tors. Evaluations on RxR’s longer, diverse paths obtain 61-64% SRs on three languages. Generating such high-quality navigation instructions in novel environments is a step to-wards conversational navigation tools and could facilitate larger-scale training of instruction-following agents. 1.

Introduction
Wayﬁnding–navigating to a destination–is an everyday task. We study the automatic generation of navigation in-structions that effectively guide people. Template-based language generators that use cardinal directions and street names are commonly used in outdoor mapping applica-tions, and some more ﬂexible generation approaches rely on databases containing information about maps, roads and landmarks [17, 50, 51]. In contrast, instructions for indoor wayﬁnding require egocentric movement guidance and ref-erence to the visual environment (e.g. notable objects).
Systems for generating indoor wayﬁnding instructions assume access to pre-existing ﬂoorplans and landmark databases [42], but recent work attempts to generate novel
Figure 1. We generate grounded navigation instructions from a sequence of 360◦ images captured along a route in a previously unseen building. Our two-stage approach ﬁrst detects landmarks and then generates instructions conditioned on these landmarks. instructions directly from visual inputs [22,39,58]. Progress toward this goal will enable navigation aids that are conver-sational rather than map-based—and it could provide a vir-tually unlimited supply of high-quality synthetic navigation instructions for training instruction-following robots. De-scribing navigation paths is also a key capability for human-robot communication, equipping robots to answer questions such as where did you go? or where should I meet you?.
We seek to generate accurate and ﬂuent navigation instructions–in multiple languages–directly from visual representations and actions taken to traverse a path. Previ-ous work assumed that the input to the instruction generator is a sequence of 360◦ panoramic (henceforth, pano) images captured at intervals on a path, typically training on instruc-tions from Room-to-Room (R2R) [5] using Matterport3D environments [9]. These models’ instructions have proven
valuable as additional training data for vision-and-language navigation (VLN) agents [22]. However, people struggle to follow them [64]: human wayﬁnding success rates on
R2R are 36% for Speaker-Follower [22] and 42% for En-vDrop [58] in unseen environments. The generated text is stylistically correct, but frequently references non-existent objects and confuses spatial terms such as left and right. for our model trained on full 360◦ panos. When it comes to selecting visual inputs for the generator, less is more. On the more challenging RxR paths, human wayﬁnders obtain a 62% SR using MARKY-MT5 vs. 78% for human instruc-tions. We release our silver landmark data and over one million navigation instructions generated by MARKY-MT5, as data augmentation for training VLN agents.2
A challenge for visually-oriented instruction generators
In many other is dealing with irrelevant visual inputs. image-to-text generation tasks (e.g., image captioning), much of the visual information in the input is reﬂected in the output text. This is not the case when generating navigation instructions. Human annotators look at less than 30% of the environment [36], and the instructions reference only a fraction of the objects that they look at. This makes learning a precise mapping between visual inputs and text outputs much harder. Perversely, access to more information can degrade performance [15], as models happily learn spuri-ous correlations that cause hallucinations during inference.
To solve this, we exploit the spatiotemporal grounding in the Room-across-Room (RxR) dataset [36]. Instead of writing instructions, RxR annotators spoke while traversing paths. Every RxR instruction thus comes with pose traces that align the words spoken (and later transcribed) with what annotators were looking at. We use these pose traces and instructions to derive a new silver annotated dataset1 that contains bounding boxes over visual landmarks com-bined with their multilingual descriptions (English, Hindi and Telugu). Speciﬁcally, we bootstrap landmark annota-tions using text parsers to identify landmark phrases in in-structions. We then use powerful image-text co-embedding models [32] combined with weak supervision from pose traces to ground those landmarks in the environment. (landmark and multilingual T5 [62]) instruction generation by improving how visual landmarks are selected and mentioned. Given a path-connected sequence of panoramic views, the ﬁrst stage landmark detector infers a sequence of landmarks that a person might select for describing the path. E.g., in Fig. 1 eight landmarks are selected, each represented by an image. This sequence, plus interleaved descriptions of navigation actions, is passed to the second stage instruction generator – a multimodal extension of the multilingual T5 (mT5) model [62] similar to VL-T5 [13] – to produce the instruction in Fig. 1. two-stage MARKY-MT5 system enhances
Our
In human wayﬁnding experiments on R2R paths,
MARKY-MT5 trained with silver landmarks (a subset of the visual inputs from the full environment) almost elimi-nates the gap between model-generated and human-written instructions – achieving a 71% success rate (SR) vs. 75% for human instructions, 42% for previous models, and 58% 1The term silver data refers to high-quality annotations–not created by people–that are derived by combining models and constraints [26, 49, 61]. 2.