Abstract
Existing symmetric contrastive learning methods suffer from collapses (complete and dimensional) or quadratic complexity of objectives. Departure from these methods which maximize mutual information of two generated views, along either instance or feature dimension, the proposed paradigm introduces intermediate variables at the feature level, and maximizes the consistency between variables and representations of each view. Specifically, the proposed in-termediate variables are the nearest group of base vectors to representations. Hence, we call the proposed method
ARB (Align Representations with Base). Compared with other symmetric approaches, ARB 1) does not require neg-ative pairs, which leads the complexity of the overall ob-jective function is in linear order, 2) reduces feature redun-dancy, increasing the information density of training sam-ples, 3) is more robust to output dimension size, which out-performs previous feature-wise arts over 28% Top-1 accu-racy on ImageNet-100 under low-dimension settings. 1.

Introduction
One major bottleneck in deep learning is the scarcity of labeled data, and much attention has been paid to unsuper-vised learning [15, 17, 30, 33] and self-supervised learning
[4, 12, 14, 16, 25, 40]. Among the mainstream approaches, most fall into one of three classes: generative, pretext-tasks-based, and contrastive methods. Generative based methods
[15, 30, 39] mainly use pixel-level reconstruction to learn the backbone. However, the backbone usually learns a se-mantic feature, so it’s unnecessary to record the information of each pixel. Therefore, more attempts about discrimina-tive approaches are proposed to train encoders by providing
*Junchi Yan is the correspondence author. Shaofeng Zhang, Lyn Qiu,
Junchi Yan, Xiaokang Yang are also with MoE Key Lab of Artificial In-telligence, Artificial Intelligence Institute, Shanghai Jiao Tong University.
Rui Zhao is also with Qing Yuan Research Institute, Shanghai Jiao Tong
University. 1 and zA
Figure 1. Comparison of direct alignment (a) and the proposed
ARB (b). zA 2 mean the first and second dimensional fea-ture of view A. Direct alignment will easily cause dimensional collapse (green dash line) and redundancy (gray area). In ARB, we solve these caveats by introducing intermediate variables—nearest group of base (bA 2 ) to representations. Then, the redun-dancy reduces to zero without dimensional collapse. 1 and bB pretext tasks [10, 17, 31], which gain obviously better per-formance, such as rotation angle [10] and spot artifacts [39].
Among these, contrastive-based methods [4, 5, 19] are the mainstream of current research. The incentive is that views of the same images generated by random augmentation re-tain similar semantic information. Hence, aligning the two-view embeddings is the key to success. However, directly aligning the embeddings usually causes degenerated solu-tions [35], which means different samples are mapped to the same points in feature space, as shown on the left side of Fig. 1. This is often due to the lack of proper objective functions or architectures [6, 32].
Therefore, one way is to design a suitable objective func-tion. SimCLR [4] takes each two embeddings as one pair, where positive pairs are views of the same images and nega-tive pairs are composed of views of the different images. By expanding the consistency between positive pairs and the difference between negative pairs, models can avoid map-ping different samples to the same points. However, such a strategy calcuates the similarity of every two samples, and this brings quadratic complexity. Another way is to build a proper architecture: BYOL [16] and SimSiam [6] propose asymmetric structures, e.g., Stop-Gradient to avoid negative sampling. Despite their promising results and linear com-plexity (since no pair-wise distance is required), the ratio-nales behind these approaches are still unclear1.
To this end, we propose a novel method named ARB to fill the gap. In detail, we maximize the mutual information between the immediate variable generated from one view and representations of the other view, and give a theoretical explanation about why it could avoid degenerate solutions.
Technically, we propose to shuffle the feature and divide the output space into several groups on feature dimension to further reduce the complexity. In a nutshell, the highlights of this paper are summarized as: 1) We propose a new method named ARB to avoid collapses in contrastive learning, which is straightforward, comprehensible and efficient (shuffle and divide groups).
Compared with other symmetric architecture, ARB only re-quires linear order complexity of objective (negative-free). 2) We theoretically analyze the relationship between the proposed ARB (maximize the mutual information between the proposed immediate variables and representations) and previous feature-wise methods [37] (maximize the consis-tency of two views). Furthermore, we give a theoretical analysis of how ARB could avoid degenerate solutions. 3) The experiments results on CIFAR-10, CIFAR-100, and ImageNet show that our method can achieve higher or be on par with previous methods. Compared with other feature-wise contrastive methods like Barlow Twins [1, 37], our methods are much more robust to dimension size. 2.