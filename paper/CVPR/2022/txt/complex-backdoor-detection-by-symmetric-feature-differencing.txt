Abstract
Many existing backdoor scanners work by finding a small and fixed trigger. However, advanced attacks have large and pervasive triggers, rendering existing scanners
It less effective. We develop a new detection method. first uses a trigger inversion technique to generate triggers, namely, universal input patterns flipping victim class sam-ples to a target class. It then checks if any such trigger is composed of features that are not natural distinctive fea-tures between the victim and target classes. It is based on a novel symmetric feature differencing method that identi-fies features separating two sets of samples (e.g., from two respective classes). We evaluate the technique on a number of advanced attacks including composite attack, reflection attack, hidden attack, filter attack, and also on the tradi-tional patch attack. The evaluation is on thousands of mod-els, including both clean and trojaned models, with various architectures. We compare with three state-of-the-art scan-ners. Our technique can achieve 80-88% accuracy while the baselines can only achieve 50-70% on complex attacks.
Our results on the TrojAI competition rounds 2-4, which have patch backdoors and filter backdoors, show that exist-ing scanners may produce hundreds of false positives (i.e., clean models recognized as trojaned), while our technique removes 78-100% of them with a small increase of false negatives by 0-30%, leading to 17-41% overall accuracy improvement. This allows us to achieve top performance on the leaderboard. 1.

Introduction
Backdoor attack (or trojan attack) on deep learning models injects malicious behaviors such that a compro-mised model behaves normally on clean inputs and mis-classifies inputs stamped with a trigger to a target label
[6, 20, 36, 40, 41]. It becomes a prominent threat due to the low complexity of launching such attacks, the devastating consequences especially in safety/security critical applica-*Equal contribution
Figure 1. Backdoor detection by trigger inversion tions, and the difficulty of defense.
There are a body of existing defense techniques such as trigger inversion [39, 66], attribution analysis [16, 25], tro-janed input detection [14, 64], backdoor removal [34, 37], and model certification [65]. According to [3, 48], trigger inversion is an effective technique that can determine if a model contains backdoor without assuming the availabil-ity of any input with trigger. For example, Neural Cleanse (NC) [66], Artificial Brain Stimulation (ABS) [39], and K-Arm [55] make use of optimization to invert triggers and determine if a model is trojaned. They consider each label in the model as a potential target and use optimization to check if a small and fixed input pattern, i.e., a trigger, can be found to cause any input to be misclassified to the la-bel. The intuition is that attackers tend to use small triggers for attack stealthiness. Figure 1 illustrates trigger inversion.
An input pattern (the circular pattern or the rectangular one on the bottom) is generated by gradient back-propagation to flip cat samples to bird. If the subject model is clean, a large pattern that exhibits a lot of bird features is gener-ated (e.g., the rectangular pattern with the “clean” tag).
In contrast, when the model is trojaned (with a red circu-lar patch), a small pattern containing the trigger features is inverted (e.g., the circular pattern with the “trojaned” tag). The size difference of the patterns is critical for these scanners, that is, a model is flagged as trojaned only when a small trigger can be found. Observe that inverted patterns are usually noisy and may not be human interpretable.
While existing techniques are effective for attacks with small and static triggers, advanced attacks proposed re-Our Method. Figure 2 illustrates our method. It takes a model and a small set of clean samples (e.g., 10 for each class). It first leverages an existing trigger inversion method to derive a trigger that can flip a set of clean samples of the victim class (cat) to the target class (bird). It feeds a set of clean victim samples to the model, and extracts the internal feature maps at a selected layer (the first row in the figure).
It then injects the inverted trigger (the butterfly-like pattern) to the clean victim samples and extracts the corresponding feature maps (the second row). A novel feature compari-son technique called symmetric feature differencing (SFD) is then applied to the two sets of feature maps, to determine the distinctive features between the two sets of samples (the first rectangular map on the right with red cells), which es-sentially denotes the trigger features. The map is also called a feature difference mask or just mask. A red cell in the mask indicates a feature map that is distinctive. It further feeds a set of clean target class (bird) samples to the model and extracts the feature maps (the third row). Applying SFD to the target class and the victim class feature maps yields the natural features distinguishing the two classes, that is, the second mask on the right. A model is considered tro-janed when the two masks do not have similarity.
The key enabling technique of our method is SFD, which is a novel differential analysis. It is based on counter-factual causality [33]. Given two sets of samples, like the victim and victim+trigger samples mentioned above, it computes a smallest set of feature maps such that exchanging their acti-vation values across the two sets entails exchanged classifi-cation results. They are considered the distinctive features.
The formal definition and the computation algorithm can be found in Section 3.
Our contributions are summarized as follows.
• We develop a new scanning technique that can detect large and complex backdoors which are difficult for existing techniques.
• The technique is based on a novel symmetric feature differencing method that can identify the distinctive features of two sets of given examples.
• We implement a prototype EX-RAY(“DEtecting
CompleX BackdooR in NeurAl Networks by SYmmetr-ic Feature Differencing”). is general and can leverage different upstream trigger inversion meth-ods. EX-RAY is publicly available at https:// github.com/PurduePAML/Exray
It
• We evaluate EX-RAY on 4246 models (2081 benign and 2165 trojaned) with 23 structures and 7 datasets, and four attacks that have large/pervasive and dynamic triggers (reflection, composite, hidden, and filter at-tacks). We compare with three state-of-the-art trig-ger inversion based scanners, NC, ABS, and K-Arm.
Our results show that EX-RAY can achieve 80-88% ac-curacy while the baselines can only achieve 50-70%.
Figure 2. Ex-ray overview cently [36, 41, 52] have large and dynamic triggers: the in-put level differences before and after injecting a trigger are substantial and the differences vary across different inputs.
Composite attack [36] injects a backdoor by mixing benign features from two or more classes. For example, a butter-fly appearing in a cat image causes the model to predict bird. Triggers may be large (e.g., a butterfly could be much larger than a typical patch trigger) and have different pixel level manifestations (e.g., various kinds of butterfly). Re-flection backdoor [41] uses the reflection of an image as the trigger. Reflection occurs when pictures are taken behind a glass window. Reflection could be as large as a whole image. Hidden trigger backdoor [52] introduces perturba-tions on the training images of target label such that the per-turbed images have similar inner activations to the trigger and forces the model to learn the correlations between the trigger and the target label. Since the trigger is not explicit in the training, the trojaning process is more difficult and re-quires larger triggers. Existing scanners such as NC, ABS, and K-arm have difficulties detecting these backdoors. They only achieve 0.5-0.7 accuracy (see the evaluation section).
Essence of Backdoor Attack. We observe the essence of backdoor attack is that model misclassification (to the target class) is induced by features not in the target class. For ex-ample, in a composite attack, a cat image is misclassified to bird when a butterfly is also present in the image. The mis-classification is essentially induced by the features of cat and butterfly (not bird’s). Our overarching idea is hence to determine if the features of an inverted trigger are the nat-ural features distinguishing the victim and target classes. If so, the model is clean. Otherwise, it is considered trojaned.
Note that in our method small sizes and fixed triggers are no longer essential properties. One may argue that the attacker could craft a trigger with the target class’s natural features.
We will discuss such an adaptive attack in Section 4.3.
We also use model interpretation techniques to show that EX-RAY indeed captures natural feature differ-ences between classes. EX-RAY can also be used to remove false positives in backdoor scanning (i.e., clean models are considered trojaned), which are usu-ally due to small triggers found between clean labels.
EX-RAY can determine that such triggers essentially denote natural features of the target label and should be precluded. We test EX-RAY (with ABS as the up-stream inversion technique) on TrojAI1 rounds 2-4 and show that EX-RAY can reduce false warnings by 78-100%, with the cost of a small increase in false neg-atives (0-30%), i.e., trojaned models are considered
It can improve multiple upstream scanners’ clean. overall accuracy including ABS (by 17-41%), NC (by 25%), and the Bottom-up-Top-down backdoor scan-ner [1] (by 2-15%) in the competition. Our method also outperforms a number of other false positive re-moval methods that compare L2 distances and leverage attribution/interpretation techniques. EX-RAY will be released upon publication.
• On the TrojAI leaderboard, ABS+EX-RAY achieves top performance in 2 out of the 4 rounds for image classification, including the most challenging round 4, with an average cross-entropy (CE) loss around 0.322 and an average AUC-ROC3 around 0.90.
It is the only technique that successfully reached the round target (for both the training sets and the test sets remotely evaluated by IARPA), i.e., a CE loss lower than 0.3465, for all the 4 rounds. As far as we know, a large number of state-of-the-art scanning techniques have been evaluated in the competition, including NC [66], ABS [39], Meta neural analy-sis [73], ULP [29], DeepInspect [11], SCAn [60], K-Arm backdoor scanning [55], Noise analysis backdoor detection [16] and attribution based backdoor detec-tion [25, 57].
Threat Model. Our threat model is consistent with that in existing works [3,39]. Given a set of models, including both trojaned and clean models, and a small set of clean samples for each model (covering all labels), we aim to identify the models with injected backdoor(s) that can flip clean samples to the target class. These samples may belong to one or many victim class(es). The former is label-specific attack and the latter is universal attack. (cid:50) 1TrojAI is a backdoor scanning competition organized by IARPA [3].
Rounds 1-4 are for image classification. Round 1 dataset is excluded due to simplicity. 2The smaller the better. 3An accuracy metric used by TrojAI, the larger the better. 2.