Abstract
Deep cross-modal hashing has become an essential tool for supervised multimodal search. These models tend to be optimized with large, curated multimodal datasets, where most labels have been manually verified. Unfortunately, in many scenarios, such accurate labeling may not be avail-In contrast, datasets with low-quality annotations able. may be acquired, which inevitably introduce numerous mis-takes or label noise and therefore degrade the search per-formance. To address the challenge, we present a general robust cross-modal hashing framework to correlate distinct modalities and combat noisy labels simultaneously. More specifically, we propose a proxy-based contrastive (PC) loss to mitigate the gap between different modalities and train networks for different modalities jointly with small-loss samples that are selected with the PC loss and a mu-tual quantization loss. The small-loss sample selection from such joint loss can help choose confident examples to guide the model training, and the mutual quantization loss can maximize the agreement between different modalities and is beneficial to improve the effectiveness of sample selec-tion. Experiments on three widely-used multimodal datasets show that our method significantly outperforms existing state-of-the-arts. 1.

Introduction
By transforming high-dimensional data from multiple modalities into compact binary hash codes in a common
Hamming space, cross-modal hashing offers remarkable efficiency for large-scale multi-modal data storage and search. Recently, supervised deep learning-based cross-modal hashing methods have achieved promising results and been applied to many multi-modal learning tasks [4– 6, 43, 49]. These models usually rely on a large number of training instances with clean and intact labels. How-*Corresponding author. ever, noisy labels, which are systematically corrupted, are ubiquitous and unavoidable in our daily life, such as social-network tagging [7], crowdsourcing [38], medical diagno-sis [13], and financial analysis [1]. As deep networks have large model capacities, they can easily memorize and even-tually overfit these noisy labels, which correspondingly de-generates the model generalization [51].
To combat the impact of noisy labels, numerous studies have been conducted, such as correction method [14], Men-torNet [19], Co-teaching [15], and T-revision [40]. These methods can learn robust continuous representations for unimodal learning tasks. However, they cannot simultane-ously tackle multi-modal inputs, such as real-world multi-media data. Moreover, continuous representations are in-efficient for storage and computation, and it is non-trivial to binarize continuous representations. Improper binariza-tion may introduce large quantization errors and severely degrade the model performance. Therefore, it is important to explore how to learn robust binary codes for cross-modal search with noisy labels. While this is rarely touched in previous works.
We perform an empirical study on a general deep cross-modal hashing framework trained with noisy labels. Fig-ure 1a illustrates the mean average precision (mAP) values in different epochs for the training datasets evaluated with noisy labels. We can see that the performance continues to increase during the training stage, which suggests that the model will keep memorizing noisy labels during the whole training procedure. Figure 1b shows the performance of testing data evaluated with clean labels, which also demon-strates that the models will fast overfit to noisy labels, thus degrading the search performance. Furthermore, from Fig-ure 1a, we can see that there exists diversity in the per-formance of different modalities since representations from different modalities may exist in entirely different spaces with heterogeneity, making learning from noisy data more difficult. Lastly, wrongly labeled data can confuse the dis-criminative connections across distinct modalities, resulting in challenges mitigating the heterogeneous gap. Therefore,
(a) Train results with noisy labels (b) Test results (c) Train results with clean labels
Figure 1. We train a basic cross-modal hashing model with binary cross-entropy loss on MIRFlickr-25k dataset with 0.6 asymmetric noise.
The mean average precision (mAP) values with different epochs are reported in the figures: (a) mAP values based on noisy labels for the training dataset; (b) mAP values for the testing dataset; (c) mAP values based on clean labels for the training dataset, where “T T2I mAP” and “T I2T mAP” are for the clean training dataset, and “F T2I mAP” and “F I2T mAP” are for the corrupted training dataset. For each line, we run five times and report the mean values. The error bar for STD in each sub-figure is highlighted as a shade. it is more challenging and complex to simultaneously con-sider both noisy data and inter-modal discrepancy.
To address the above problems, we first provide a more in-depth analysis of the impact of noisy labels on deep cross-modal search models. Then we propose our method to combat the effect of noisy labels. Specifically, accord-ing to the ground-truth labels, we split the noisy training dataset into clean dataset that are accurately labeled and cor-rupted dataset that are wrongly labeled. Then, we show the mAP values for these two datasets based on correct labels with different training epochs, respectively. The results are shown in Figure 1c. As been revealed by previous studies on image classifications, there exists a memorization effect for deep neural networks (DNNs): DNNs tend first to memo-rize and fit majority (clean) patterns and then overfit minor-ity (noisy) patterns. From Figure 1b and Figure 1c, we can also obtain the following critical findings for cross-modal search tasks: (1) the performance evaluated with correct la-bels for both clean cross-modal training data and corrupted cross-modal training data will first increase and then de-crease, which show that, during the early learning stage, the cross-modal search model can fit clean data and also gener-alize well to corrupted data; (2) the performance of test data first increase and then decrease, suggesting that the learning from clean data can dominate the early learning stage and then be overwhelmed by the overfitting to noisy labels.
Based on the above analysis, we propose a robust cross-modal hashing framework called Cross-Modal Mu-tual Quantization (CMMQ) to combat the impact of noisy labels and narrow the heterogeneous gap simultaneously.
Firstly, to correlate different modalities, we design to gener-ate proxy codes for each class based on the Hadamard ma-trix [34] and adopt a proxy-based contrastive (PC) loss to push examples from different modalities to the correspond-ing shared proxy codes. Secondly, to excavate the discrim-inative information from noisy labels, we exploit the mem-orization effect of deep cross-modal models and preferen-tially select examples with small PC losses to train the net-work confidently. Thirdly, different models are unlikely to agree on noisy examples. Hence we adopt a mutual quan-tization loss to maximize the agreement of networks from different modalities, which can further improve the effec-tiveness of sample selection. The overall learning frame-work is illustrated in Figure 2.
Before delving into details, we clearly emphasize our contributions as follows.
• We propose a proxy-based contrastive (PC) loss, which can push examples from different modalities to their corresponding shared proxy codes and narrow the het-erogeneous gap effectively.
• By preferentially selecting examples with small losses, our method can effectively exploit the memorization effect of deep cross-modal networks and combat the impact of noisy labels.
• A mutual quantization loss is proposed to maximize the agreement of models from different modalities, thus can improve the quality of the predicted codes.
• Experiments on three cross-modal benchmark datasets clearly demonstrate that the method can outperform many state-of-the-art approaches in various settings.
In Sec-The rest of the paper is organized as follows. tion 2, we briefly review some closely related works.
In
Section 3, we propose our cross-modal mutual quantization paradigm. Section 4 shows the experimental results. Fi-nally, concluding remarks are provided in Section 5.
2.