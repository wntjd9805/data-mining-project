Abstract
State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety of downstream tasks. Gener-ally, such models are often either cross-modal (contrastive) or multi-modal (with earlier fusion) but not both; and they often only target speciﬁc modalities or tasks. A promising direction would be to use a single holistic universal model, as a “foundation”, that targets all modalities at once—a true vision and language foundation model should be good at vision tasks, language tasks, and cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate impressive performance on a wide range of 35 tasks spanning these target modalities. 1.

Introduction
Large-scale pre-training of vision and language trans-formers has led to impressive performance gains in a wide variety of downstream tasks. In particular, contrastive meth-ods such as CLIP [82] and ALIGN [50] have shown that natural language supervision can lead to very high quality visual models for transfer learning.
Purely contrastive methods, however, also have impor-tant shortcomings. Their cross-modal nature does not make them easily usable on multimodal problems that require dealing with both modalities at the same time. They re-quire large corpora, which for both CLIP and ALIGN have not been made accessible to the research community and the details of which remain shrouded in mystery, notwith-standing well-known issues with the construction of such datasets [9].
In contrast, the recent literature is rich with transformer models that explicitly target the multimodal vision-and-language domain by having earlier fusion and shared self-attention across modalities. For those cases, however, the unimodal vision-only or language-only performance of the model is often either glossed over or ignored completely.
If the future of our ﬁeld lies in generalized “foundation models” [10] or “universal” transformers [72] with many
*Equal contribution.
Figure 1. We present FLAVA, a language and vision alignment model that learns strong representations from multimodal (image-text pairs) and unimodal data (unpaired images and text) and can be applied to target a broad scope of tasks from three domains (visual recognition, language understanding, and multimodal rea-soning) under a common transformer model architecture. different capabilities, then the following limitation should be overcome: a true foundation model in the vision and lan-guage space should not only be good at vision, or language, or vision-and-language problems–it should be good at all three, at the same time.
Combining information from different modalities into one universal architecture holds promise not only because it is similar to how humans make sense of the world, but also because it may lead to better sample efﬁciency and much richer representations.
In this work, we introduce FLAVA, a foundational lan-guage and vision alignment model that explicitly targets vision, language, and their multimodal combination all at once. FLAVA learns strong representations through joint pretraining on both unimodal and multimodal data while en-compassing cross-modal “alignment” objectives and multi-modal “fusion” objectives. We validate FLAVA by applying it to 35 tasks across vision, NLP, and multimodal domains and show impressive performance. An important advantage of our approach is that it was trained on a corpus of openly available datasets that is an order of magnitude smaller than datasets used in comparable models. Our models and code are available in https://flava-model.github.io/.
Method
CLIP [82]
ALIGN [50]
SimVLM [107]
UniT [43]
VinVL [115]
ViLT [54]
ALBEF [62]
FLAVA (ours)
Multimodal Pretraining data size dataset(s) public (cid:55) WebImageText 400M (cid:55) 1.8B (cid:55) 1.8B – – (cid:51) Combination 9M (cid:51) Combination 10M (cid:51) Combination 5M (cid:51) PMD (Tbl. 2) 70M
JFT
JFT
None
Pretraining Objectives
ITM Masking
Unimodal
Target Modalities
V CV&L MV&L
Contr. (cid:51) (cid:51) – – (cid:51) – (cid:51) (cid:51) – – – – – (cid:51) (cid:51) (cid:51) – –
PreﬁxLM –
MLM
MLM
MLM
MMM – –
CLM – – – – (cid:51) (cid:51)
*
* – – –
MLM+MIM (cid:51) (cid:51) (cid:51) (cid:51) – (cid:51) (cid:51) (cid:51) (cid:51) – – (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51)
L – – (cid:51) (cid:51) – – – (cid:51)
Table 1. Comparison of recent models in different modalities. CV&L and MV&L stands for cross-modal and multi-modal vision-and-language. * means the modality is partially targeted (SimVLM [107] and UniT [43] include ImageNet and object detection, respectively). 2.