Abstract
Systems that can efficiently search collections of sign language videos have been highlighted as a useful applica-tion of sign language technology. However, the problem of searching videos beyond individual keywords has received limited attention in the literature. To address this gap, in this work we introduce the task of sign language retrieval with free-form1 textual queries: given a written query (e.g. a sentence) and a large collection of sign language videos, the objective is to find the signing video that best matches the written query. We propose to tackle this task by learning cross-modal embeddings on the recently introduced large-scale How2Sign dataset of American Sign Language (ASL).
We identify that a key bottleneck in the performance of the system is the quality of the sign video embedding which suf-fers from a scarcity of labelled training data. We, therefore, propose SPOT-ALIGN, a framework for interleaving itera-tive rounds of sign spotting and feature alignment to expand the scope and scale of available training data. We validate the effectiveness of SPOT-ALIGN for learning a robust sign video embedding through improvements in both sign recog-nition and the proposed video retrieval task. 1.

Introduction
Sign languages are the primary means of communica-tion among deaf communities. They are visual, complex, evolved languages that employ combinations of manual and non-manual markers such as movements of the face, body and hands to convey information [56].
Recent developments in automatic speech recognition (ASR) for spoken languages [14,15,65,70] have enabled au-tomatic captioning of vast swathes of video content hosted 1The terminology “natural language query” is commonly used to de-scribe unconstrained textual queries in spoken languages. However, since sign languages are also natural languages, we adopt for the term “free-form textual query” instead.
Figure 1. Text-based sign language video retrieval: In this work we introduce sign language video retrieval with free-form textual queries, the task of searching collections of sign language videos to find the best match for a free-form textual query, going beyond individual keyword search. on platforms such as YouTube. In addition to rendering the videos more accessible, this captioning yields a second im-portant benefit: it allows the content of the videos to be indexed and efficiently searched with text queries. By con-trast, the same automatic captioning capability (and hence searchability) does not exist for sign language content. In-deed, recent work has drawn attention to the pressing need to develop systems that can index archives of sign language videos to render them searchable [6]. Without these tools, sign language video creators must type the spoken language translation of their content if they want to reach the same discoverability as their spoken language counterparts.
One solution might appear to be to use sign language translation systems to perform video captioning, analogous to ASR cascading in spoken content retrieval [34]. Un-fortunately, while promising translation results have been demonstrated in constrained domains of discourse (such as
weather forecasts) [9, 10, 37], it has been widely observed that these systems are unable to achieve functional per-formance across the multiple domains of discourse [6, 29, 62] required for open-vocabulary video indexing (see Ap-pendix D). An alternative solution would be to employ ex-isting methods for sign spotting to perform keyword search.
However, such approaches are fundamentally brittle—they work best when the user knows exactly which signs of in-terest were used in the video. Moreover, to build an ac-curate index of such signs using recent sign spotting tech-niques [1, 26, 46] requires a list of appropriate query candi-dates, which to date have often been obtained from subtitles corresponding to speech transcriptions of the translation, for example from an ASR engine. We focus on sign language videos produced by and for signers, that may not contain any speech track, so producing such speech transcriptions is not an option.
In this work, we address the task of sign language video retrieval with free-form textual queries by learning a joint embedding space between text and video as illustrated in
Fig. 1. Cross-modal embeddings target only the task nec-essary to enable search (i.e. ranking a finite pool of sign language videos), rather than the more involved task of full sign language translation. As we demonstrate through experiments, this renders their practical application even across multiple topics. Moreover, cross-modal embeddings enable extremely efficient search (with the potential to scale up to collections of billions of videos thanks to mature approximate nearest neighbour algorithms for embedding spaces [27]).
The task of sign language video retrieval is extremely challenging for several reasons: (1) Translation mappings between sign languages and spoken languages are highly complex [57], with differing modalities and grammar struc-tures (ordering is typically not preserved between signed and spoken languages, for example); (2) In contrast to the datasets used to train text-video retrieval models (mil-lions of paired examples of videos with corresponding sen-tences [5, 41]) sign language datasets are orders of magni-tude smaller in scale; (3) In addition to a paucity of paired data, the annotated data available for learning robust sign embeddings is also extremely scarce (with sign recognition datasets also considerably smaller than their counterparts for action recognition [11, 25], for example).
In this work, we propose to study sign language video retrieval on the recently released How2Sign American Sign
Language (ASL) dataset [20]. To the best of our knowledge, this dataset represents the largest public source of sign lan-guage videos with aligned captions. In order to address the first and second challenges highlighted above, we construct cross-modal embeddings that leverage pretrained language models to reduce the burden of data required to learn the mapping between signing sequences and sentences. To ad-dress the third annotation scarcity challenge, we propose
SPOT-ALIGN, a framework for automatic annotation that integrates multiple sign spotting methods to automatically annotate significant fractions of the How2Sign dataset. By training on the resulting annotations, we obtain more robust sign embeddings for the downstream retrieval task.
In summary, we make the following contributions: (1)
We introduce the task of sign language video retrieval with free-form textual queries; (2) We provide several baselines for this task, demonstrating the value of cross modal em-beddings and the benefits of incorporating additional re-trieval cues from a sign recognition method (whose predic-tions provide a basis for text-based similarity search) on the
How2Sign and PHOENIX2014T datasets; (3) We propose the SPOT-ALIGN framework for automatic annotation and demonstrate its efficacy in producing more robust sign em-beddings; (4) We contribute a new manually annotated test set for the How2Sign benchmark. 2.