Abstract
Given a composite image, image harmonization aims to adjust the foreground to make it compatible with the background. High-resolution image harmonization is in high demand, but still remains unexplored. Conventional image harmonization methods learn global RGB-to-RGB transformation which could effortlessly scale to high reso-lution, but ignore diverse local context. Recent deep learn-ing methods learn the dense pixel-to-pixel transformation which could generate harmonious outputs, but are highly constrained in low resolution. In this work, we propose a high-resolution image harmonization network with Collab-orative Dual Transformation (CDTNet) to combine pixel-to-pixel transformation and RGB-to-RGB transformation coherently in an end-to-end network. Our CDTNet con-sists of a low-resolution generator for pixel-to-pixel trans-formation, a color mapping module for RGB-to-RGB trans-formation, and a reﬁnement module to take advantage of both. Extensive experiments on high-resolution bench-mark dataset and our created high-resolution real compos-ite images demonstrate that our CDTNet strikes a good balance between efﬁciency and effectiveness. Our used datasets can be found in https://github.com/bcmi/CDTNet-High-Resolution-Image-Harmonization. 1.

Introduction
Image composition [26] combines foreground and back-ground from different images into a composite image. The quality of composite image may be degraded by the appear-ance (e.g., tone, illumination) inconsistency between fore-ground and background. To address this issue, image har-monization adjusts the foreground appearance to make it compatible with the background. Deep image harmoniza-tion methods [7, 8, 15, 24, 31, 34] have achieved remarkable progress by learning the dense pixel-to-pixel transformation between composite images and ground-truth harmonized
*Corresponding author. images. However, they only performed low-resolution (e.g., 256 × 256) image harmonization, and a na¨ıve upsampling can merely lead to a large yet blurry output (see Figure 1a).
Though directly training with high-resolution images could seemingly address this issue, the computational cost is very expensive. For example, it will cost more than 950G
FLOPs (ﬂoating point operations) and more than 20 GB memory for [31] to harmonize a 2048×2048 composite im-age. Besides, the high-resolution network may be weak in capturing long-range dependencies due to local convolution operations [36] (see Section 4.3 and the Supplementary).
Prior to deep image harmonization, conventional har-monization methods [21, 29, 32, 38, 42] mainly used hand-crafted statistical features (e.g., illumination, color tempera-ture, contrast, saturation) to determine color-to-color trans-formation for foreground adjustment. Color-to-color trans-formation could be achieved in different color spaces, and we narrow the scope to RGB-to-RGB transformation in this work. RGB-to-RGB transformation is a global transforma-tion of color values. Therefore, it is barely constrained by the number of pixels and could effortlessly scale to high resolution. However, global transformation disregards the local context for each pixel, prone to generate harmoniza-tion results with local inharmony.
Our key insight for high-resolution image harmonization is to combine the advantages of both pixel-to-pixel trans-formation and RGB-to-RGB transformation. We name our high-resolution image harmonization network with Collab-orative Dual Transformations (CDT) as CDTNet. CDT-Net consists of a low-resolution generator for pixel-to-pixel transformation, a color mapping module for RGB-to-RGB transformation, and a reﬁnement module to combine the best of two worlds. The low-resolution generator is a U-Net [30] structure which takes in a low-resolution compos-ite image and outputs a low-resolution harmonized result.
Meanwhile, the encoder feature is used to learn RGB-to-RGB transformation, unlike previous hand-crafted methods
[29, 32]. Speciﬁcally, we learn several basis lookup tables (LUTs) [10–12, 37] shared by all images and a weight pre-dictor based on the encoder feature to predict image-speciﬁc
(a) pixel-to-pixel transformation. (b) RGB-to-RGB transformation. (c) collaborative dual transformations.
Figure 1. In (a), deep harmonization network [31] learns dense transformation for each individual pixel and outputs low-resolution harmo-nious results, which would be blurry if upsampled. In (b), a 3D lookup table (LUT) in our method learns global color transformation and outputs high-resolution results without considering local context, which might lead to inharmonious local regions. In (c), our full method combines two transformations to achieve the most plausible results. Best viewed by zooming in. combination coefﬁcients. RGB-to-RGB transformation is performed on high-resolution composite images using the combined LUT. With the RGB-to-RGB result as guidance, the reﬁnement module utilizes both the harmonized result and decoder feature map from the low-resolution generator to compensate for ﬁne-grained local information.
Considering the efﬁciency of CDTNet, pixel-to-pixel transformation only operates on low-resolution inputs and the reﬁnement module is light-weighted, so the memory cost and computational cost are well-suppressed. Consid-ering the effectiveness of CDTNet, RGB-to-RGB transfor-mation could provide a holistic understanding of the whole image and the sharp edges of the transformed image, while pixel-to-pixel transformation could provide ﬁne-grained lo-cal information. As shown in Figure 1b, RGB-to-RGB transformation can obtain a globally reasonable tone and illumination. However, it may produce inharmonious local regions (i.e., the left and right borders of the foreground building) without considering local context.
In contrast, our CDTNet can yield visually plausible and harmonious results of high resolution (see Figure 1c). Our contributions could be summarized as follows:
• To the best of our knowledge, this is the ﬁrst work fo-cusing on high-resolution image harmonization.
• We are the ﬁrst to achieve deep learning based color-to-color transformation for image harmonization.
• We unify pixel-to-pixel transformation and color-to-color transformation coherently in an end-to-end net-work named CDTNet.
• Extensive experiments demonstrate that our CDTNet achieves state-of-the-art results with less resource con-sumption. 2.