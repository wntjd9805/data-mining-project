Abstract
Deep learning models are known to suffer from the problem of catastrophic forgetting when they incrementally learn new classes. Continual learning for semantic seg-mentation (CSS) is an emerging field in computer vision.
We identify a problem in CSS: A model tends to be con-fused between old and new classes that are visually similar, which makes it forget the old ones. To address this gap, we propose REMINDER - a new CSS framework and a novel class similarity knowledge distillation (CSW-KD) method.
Our CSW-KD method distills the knowledge of a previ-ous model on old classes that are similar to the new one.
This provides two main benefits: (i) selectively revising old classes that are more likely to be forgotten, and (ii) better learning new classes by relating them with the previously seen classes. Extensive experiments on Pascal-VOC 2012 and ADE20k datasets show that our approach outperforms state-of-the-art methods on standard CSS settings by up to 7.07% and 8.49%, respectively. 1.

Introduction
Semantic segmentation, which aims to assign each pixel of an image to its semantic class, is a fundamental task in computer vision. Segmentation models are critical to many real-world applications, such as self-driving cars [1,17] and medical image diagnostics [14, 40]. In most practical cases, the model needs to continuously learn new data and adapt to the changes in the operating environment. However, contin-ually learning new classes leads to catastrophic forgetting of old knowledge [12, 34]. In other words, the performance of newly retrained models degrades significantly on old tasks.
Research on continual learning for semantic segmenta-tion (CSS) only emerged recently in medical imaging [27, 28] and general scene understanding [2, 9]. Besides forget-ting, CSS also faces the background shift problem, where (a) Animal group (b) Vehicle group
Figure 1. Performance drop (degree of forgetting) on old classes in the (a) animal and (b) vehicle group as a model learns a new class. Our method forgets less when learning a new similar class. object classes from previous steps are shifted to the back-ground at the current step [2].
There are two main problems causing catastrophic for-getting in continual learning (CL). First, the model has a strong bias toward new classes [36]. In other words, ob-jects of old classes are mispredicted as new ones. Second, the model tends to forget old classes that are visually sim-ilar to newly added classes. To investigate this problem, we divide classes in the Pascal-VOC 2012 dataset into two groups: animal and vehicle, and evaluate the degree of for-getting in each group. Fig. 1 shows the performance drop1 in each group as the model learns a new class. The perfor-mance on the animal group drops the most when the model learns sheep. Similarly, the result on the vehicle group re-duces the most when it learns train.
Recent CSS methods [2, 9, 23, 24] distill the knowledge of a previous model on old classes to a current model.
Knowledge distillation prevents the model from diverging from what it previously learned. This continual learning paradigm gains high research interests because of its com-putational efficiency. They do not require storing exemplars of old classes to re-learn old knowledge. Despite the recent 1The performance drop measures how much mIoU a model drops in percentage point(%) when it learns a new class.
success, modern distillation based methods [2,23,24] distill the knowledge on all old classes equally even though some are more likely to be forgotten than others. They may put less emphasis on revising the affected old knowledge. This overlooking makes the model more vulnerable to forgetting visually similar old classes.
To resolve the current research gap, this paper proposes a novel class similarity weighted knowledge distillation (CSW-KD) method. Our CSW-KD emphasizes revising the knowledge of old classes that are likely to be forgotten, i.e., the classes that are similar to a new one. In particular, when learning a new class, the proposed method computes its similarity to the old ones. It then reweighs the predictions of a previous model on old classes based on their similarity scores. The class similarity weighted knowledge is distilled to the current model.
The proposed approach has three benefits. First, our method is more resilient to forgetting when learning new vi-sually similar classes (as shown in Fig. 1). The model iden-tifies the group of old classes that is more likely to be for-gotten, i.e., the group to which a new class belongs. It then selectively reinforces the knowledge of this group. Second, our method better learns new tasks. Via CSW-KD, we en-force the model to capture the similarity between classes.
Thus, it can relate the new with the previously learned knowledge. The model then transfers what it previously learned to facilitate the learning of new classes. Third, the prior knowledge about class similarity enables the model to learn an underlying class hierarchy. Using this learned hier-archy, the model can identify groups of old knowledge that are being affected.
We introduce REMINDER - a CSS framework that con-sists of two components. First, the class similarity weighted knowledge distillation (CSW-KD) transfers the reweighted outputs of an old model based on their similarity to the new class. Second, a feature knowledge distillation (FKD) mod-ule distills the features of the previous model to encourage feature reuse among different tasks.
Our main contributions can be summarized as follows.
• We propose to use semantic similarity between classes as a prior for continual learning. To the best of our knowl-edge, this is the first work that explores hierarchical learn-ing to reduce catastrophic forgetting in CL.
• We propose a novel CSW-KD method that leverages class similarity to reduce the forgetting of similar old classes (rigidity) and promote learning of new classes (plastic-ity). We then propose REMINDER - a CSS framework that uses CSW-KD to remind the model of old knowledge based on the similarity between new and old classes.
• We show that our method achieves a better rigidity-plasticity trade-off than strong baselines via extensive experiments. REMINDER outperforms state-of-the-art methods on Pascal-VOC 2012 and ADE20k datasets by up to 7.07% and 8.49%. 2.