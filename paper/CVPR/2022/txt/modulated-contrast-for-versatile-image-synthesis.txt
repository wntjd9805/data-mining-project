Abstract
Perceiving the similarity between images has been a long-standing and fundamental problem underlying various visual generation tasks. Predominant approaches measure the inter-image distance by computing pointwise absolute deviations, which tends to estimate the median of instance distributions and leads to blurs and artifacts in the gener-ated images. This paper presents MoNCE, a versatile met-ric that introduces image contrast to learn a calibrated met-ric for the perception of multifaceted inter-image distances.
Unlike vanilla contrast which indiscriminately pushes neg-ative samples from the anchor regardless of their similar-ity, we propose to re-weight the pushing force of negative samples adaptively according to their similarity to the an-chor, which facilitates the contrastive learning from infor-mative negative samples. Since multiple patch-level con-trastive objectives are involved in image distance measure-ment, we introduce optimal transport in MoNCE to modu-late the pushing force of negative samples collaboratively across multiple contrastive objectives. Extensive experi-ments over multiple image translation tasks show that the proposed MoNCE outperforms various prevailing metrics substantially. The code is available at MoNCE. 1.

Introduction
Multifarious image generation tasks [26, 27, 30, 45, 46, 50, 56, 57] often entail multifaceted metrics to measure the inter-image similarity with regard to different properties such as image structures, image semantics and image per-ceptual realism, etc. Defining generic metrics to fulfil mul-tiple objectives is challenging as different visual properties are usually entangled in pixels and the notion of visual sim-ilarity is often subjective.
Image similarity measurement remains a very open research challenge in visual generation tasks.
To measure and minimize the content variation in un-paired image translation, Zhu et al. [56] design a cycle-consistency loss to ensure that input images can be recov-ered from the output images. Different from unpaired image
*Corresponding author, E-mail: shijian.lu@ntu.edu.sg
Figure 1. Comparison of different contrastive objectives: For the contrastive objective of a single image patch, vanilla con-trastive objective repels all negative samples indiscriminately. The introduced weighted contrastive objective adaptively adjusts the weights of negative pairs according to the pair similarity. With inverse weighting strategies for unpaired and paired translation tasks, the weighted objective can be applied to improve the gener-ation performance substantially. The modulated contrastive objec-tive introduces optimal transport to modulate the learning objec-tives of all image patches as a whole. translation, paired image translation entails certain metrics to measure the perceptual similarity between output images and ground truth. Among various distance metrics [36, 37], perceptual loss [17] emerges as a powerful metric in line with human perception by leveraging the internal activation of pre-trained networks. However, above metrics are de-signed based on point-wise deviations, which undesirably minimize the average deviation to all possible instances.
For example, a semantic map corresponds to numerous real images, minimizing the average deviation to all possible real images tends to produce blurred generation results.
Instead of minimizing the point-wise deviation, the pre-vailing contrastive learning [5, 14, 41] aims to pull positive samples towards an anchor and push negative samples far away from it. It has recently been adopted in image gen-eration tasks for preserving image contents in unpaired im-age translation [26], perceiving image similarity in paired image translation [2], or serving as a contrastive regulariza-tion term in image dehazing [38]. However, all these studies adopt the vanilla contrast that shares a critical constraint â€“ negative samples are indiscriminately pushed away from the anchor regardless of their similarity to the anchor.
In this work, we formulate contrastive learning as a ver-satile metric for various image translation tasks as shown in
Fig. 1. In unpaired image translation, contrastive learning allows to preserve image contents by maximizing the mu-tual information of corresponding patches [26]. In paired image translation, contrastive learning is employed to mea-sure the perceptual similarity between images in line with human judgement, by leveraging pre-trained networks for feature extraction. However, vanilla contrastive objective repels all negative samples indiscriminately, which is ap-parently sub-optimal as negative samples usually have dif-ferent similarity with the anchor. Certain weighting strategy is desired to formulate more effective contrast by adaptively adjusting the pushing force of negative samples.
Aiming to boost the translation performance, we com-prehensively investigated different weighting strategies for negative samples and some non-trivial conclusions are drawn for the selection of weighting strategies in different scenarios. Intuitively, hard negative samples (i.e., with high similarity to the anchor) should be assigned higher weights (referred as hard weighting), complying with the rationale of hard negative sampling [29]. It is true for unpaired im-age translation where negative samples can be easily pushed apart as illustrated in the similarity distributions of negative
& positive pairs in Fig. 2. However, for paired image trans-lation, negative samples are hard to be pushed apart from the anchor (or positive pairs) as there is severe overlap for the similarity distribution of negative & positive pairs as in Fig. 2. In this scenario, we surprisingly find that the intuitive hard weighting strategy tends to impair the performance, and an inverse weighting strategy as shown in Fig. 1 allows to improve the performance. In addition, as in PatchNCE loss [26], contrastive learning for measuring image simi-larity involves several sub-objectives as each image patch is associated with a contrastive objective. Re-weighting each sub-objective separately without overall coordination tends to be sub-optimal. We propose a Modulated Noise
Contrastive Estimation (MoNCE) loss that employs opti-mal transport [28] to modulate the re-weighting of all nega-tive samples collaboratively across the multiple objectives.
With a cost matrix designed based on the similarity of neg-ative pairs, optimal transport allows to retrieve an optimal transport plan which serves as the weights for negative sam-ples to reach an overall optimal objective.
The contributions of this work can be summarized in three aspects. First, we formulate contrastive learning as a versatile metric in multifarious image translation tasks. Sec-ond, we extensively investigate the effect of negative pair weighting in contrastive learning and propose to adopt dif-ferent weighting strategies according to the similarity dis-tribution of negative pairs. Third, we propose a modulated contrast that exploits optimal transport to modulate the re-weighting of all negative pairs collaboratively across multi-ple contrastive objectives. 2.