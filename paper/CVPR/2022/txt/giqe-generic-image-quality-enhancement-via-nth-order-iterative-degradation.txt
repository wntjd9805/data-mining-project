Abstract
Visual degradations caused by motion blur, raindrop, rain, snow, illumination, and fog deteriorate image qual-ity and, subsequently, the performance of perception algo-rithms deployed in outdoor conditions. While degradation-specific image restoration techniques have been extensively studied, such algorithms are domain sensitive and fail in real scenarios where multiple degradations exist simulta-neously. This makes a case for blind image restoration and reconstruction algorithms as practically relevant. However, the absence of a dataset diverse enough to encapsulate all variations hinders development for such an algorithm. In this paper, we utilize a synthetic degradation model that recursively applies sets of random degradations to gener-ate naturalistic degradation images of varying complexity, which are used as input. Furthermore, as the degradation intensity can vary across an image, the spatially invariant convolutional filter cannot be applied for all degradations.
Hence to enable spatial variance during image restoration and reconstruction, we design a transformer-based archi-tecture to benefit from the long-range dependencies. In ad-dition, to reduce the computational cost of transformers, we propose a multi-branch structure coupled with modifi-cations such as a complimentary feature selection mecha-nism and the replacement of a feed-forward network with lightweight multiscale convolutions. Finally, to improve restoration and reconstruction, we integrate an auxiliary decoder branch to predict the degradation mask to ensure the underlying network can localize the degradation infor-mation. From empirical analysis on 10 datasets covering rain drop removal, deraining, dehazing, image enhance-ment, and deblurring, we demonstrate the efficacy of the proposed approach while obtaining SoTA performance. 1.

Introduction
Image quality plays an important role in the performance of vision-based algorithms designed for tasks such as object
*Co-corresponding authors. Listed in alphabetical order. (a) (b) (c) (d) (e)
Images generated by proposed Nth order degradation
Figure 1. (top) along with corresponding spatial distortion masks (bottom) for a given input having (a) natural rainy droplets and synthetic (b) motion blur, (c) snow, (d) rain, and (e) rain with snow. detection, semantic segmentation, depth estimation, etc.
Hence, an image affected by environmental degradations such as motion blur, illumination variations, rain, fog, snow, and water droplets results in an undesirable performance drop [22, 31]. Despite the nature of degradations, they can be modeled using a common mask-based approach consid-ering that they affect the spatial properties of an image to reduce its quality. However, since the intensity and com-binations of degradations co-occurring can be non-uniform, some regions are bound to be affected more than others.
Hence, a generic image restoration algorithm should be able to localize and be robust towards spatially varying degra-dations. While perception algorithms can be made robust to diverse weather conditions either by extending the train-ing dataset [25,37,43,46] or utilizing restoration algorithms as preprocessing step [15, 24, 40] to generate clean images.
However, these approaches have their shortcomings as con-structing a labeled dataset for high-level perception, diverse enough to account for all variations, is time-consuming and expensive.
In contrast, image restoration algorithms are presently degradation-specific (dehazing, deraining, rain-drop removal, desnowing, etc.) and do not perform well outside the distribution of training set [11, 16, 42, 44]. Fur-thermore, as SoTA image restoration algorithms are built upon convolutional neural networks (CNNs), utilization of the same convolution filter for the complete feature would result in weak restoration owing to the co-occurrence of multiple spatially-varying degradations across the image.
Yet, from a practical standpoint, having an generic image restoration algorithm would be highly desirable as it would avert extending the dataset for all perception tasks to make them robust to environment variations. Thus in this paper, we focus on blind image restoration as a preprocessing step to ensure the robust performance of perception algorithms in varying environmental conditions.
Due to the inability of CNNs to capture long-range de-pendencies and their fixed convolutional filters being inap-propriate for varying degradations, standard convolutional filters cannot be utilized. Recently, Swin transformers [28] were proposed that can use the advantages of both CNNs and Transformers. Categorically their ability to handle large image resolutions and capture long-range dependencies via a shifted window scheme respectively presents an oppor-tunity to utilize such a mechanism for developing generic image restoration and reconstruction algorithm. However, naively replacing convolutional blocks with transformer modules would result in a substantial increase in redundant computations. Hence in its current form, they cannot be utilized for processing a degraded image. Thus to lower computational cost without reducing performance efficacy, we propose a multiscale architecture that extracts features from different scales representing different feature granular-ity and subsequently uses transformer modules with various repetitions on each scale. Specifically, we use CSWin [8] wherein the computations of self-attention is decreased by using horizontal and vertical stripes. During experiments, we observe simply concatenating the features extracted by horizontal and vertical filters to be inefficient. Instead, we propose a feature selection module (FSM) that aggregates relevant features and suppresses irrelevant ones. As we deal with image restoration, we observe a self-attention mech-anism to result in a high computational cost that isn’t ob-served for high-level perception tasks due to the absence of decoder blocks or utilization of spatially large feature maps.
To overcome this, we propose a spatial compression mech-anism to replace the multi-head attention.
While image restoration is an extensively studied topic, prior works under-utilize the paired samples by proposing an end-to-end architecture. Concretely, a spatial distortion mask that represents the location of affected pixels isn’t uti-lized. We highlight that designing the restoration algorithm that also predicts the spatial distortion mask as auxiliary output during training would aid the network in identify-ing locations that are affected. One of the challenges faced in training and evaluating practical image restoration algo-rithms is the absence of paired datasets having multiple co-occurring degradations involving motion blur, illumination variations, fog, rain, and water droplets. Thus, we utilize the Cityscapes [7] and its synthetic variants containing fog
[39], and rain [17] degradation for training and evaluating restoration quality as well as its effect on downstream tasks such as semantic segmentation. We utilize Pix2PixHD [48] to consider distortions caused by water droplets. We sum-marize our contributions as follows,
• We propose an image restoration and reconstruction architecture that is able to recover images affected by blind distortion combination.
• To ensure degradation is accurately localized, we in-tegrate an auxiliary degradation prediction branch to enhance the restoration performance.
• To enable realistic distortions we propose a Nth or-der degradation model that recursively applies a set of degradations.
• We propose a Feature Selection Module and Spatial
Compression Mechanism to reduce the computations of the CSWin Transformer module.
• We examine the effect of image restoration vis-a-vis extended training on downstream tasks towards achieving robust performance. 2.