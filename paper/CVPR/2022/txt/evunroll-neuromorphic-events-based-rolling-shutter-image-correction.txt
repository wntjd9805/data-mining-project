Abstract 1.

Introduction
This paper proposes to use neuromorphic events for cor-recting rolling shutter (RS) images as consecutive global shutter (GS) frames. RS effect introduces edge distortion and region occlusion into images caused by row-wise read-out of CMOS sensors. We introduce a novel computa-tional imaging setup consisting of an RS sensor and an event sensor, and propose a neural network called EvUn-roll to solve this problem by exploring the high-temporal-resolution property of events. We use events to bridge a spatio-temporal connection between RS and GS, estab-lish a ﬂow estimation module to correct edge distortions, and design a synthesis-based restoration module to restore occluded regions. The results of two branches are fused through a reﬁning module to generate corrected GS images.
We further propose datasets captured by a high-speed cam-era and an RS-Event hybrid camera system for training and testing our network. Experimental results on both public and proposed datasets show a systematic performance im-provement compared to state-of-the-art methods.
# Contributed equally to this work as ﬁrst authors
Corresponding author: shiboxin@pku.edu.cn
Project page: https://github.com/zxyemo/EvUnroll
CMOS imaging sensors are the mainstream choice for mobile phones and machine vision cameras due to low power consumption and cost [15]. However, commonly row-by-row readout scheme of CMOS sensors will always cause the rolling shutter (RS) effect (also known as the jelly effect) for captured images in scenes with camera or local object motion. Compared with the global shutter (GS) sen-sor that synchronizes the exposure period of each pixel, RS effects limit the applicability of CMOS sensors in consumer or industrial applications due to edge distortion and region occlusion [14, 22, 24, 54]. As such, the RS correction is a way to make up for such deﬁciencies.
A well-known challenge for RS correction is to estimate the transformation between RS and GS images [22, 26, 54].
Unlike many image restoration tasks (such as video frame interpolation [16, 32, 36] and image deblurring [17, 23]) which assume that the edge structures of local areas remain unchanged, RS correction needs to deal with the edge dis-tortion. To address this problem, geometric model based methods [2, 12, 12, 29, 39] simplify the RS to GS (RS2GS) transformation via different assumptions, such as the scene is static [29, 30] and straight lines keep straight [43], and employ homography mixture or camera pose estimation to
achieve RS correction [12]. However, these simpliﬁed as-sumptions lead to poor compatibility with complex mo-tions, and the computational cost of such optimization prob-lems is expensive [26]. Deep neural network since ﬁrstly demonstrated in [42] have revealed its effectiveness in RS correction by learning camera motion parameters [42, 57], optical ﬂow maps [7], or direct mappings of RS2GS [26,54] from single or multiple consecutive RS frames. Nonethe-less, even multi-frame images lack the ability to provide motion within the inter-frame period, which makes the problem still ill-posed.
Another bottleneck for RS correction is the intra-frame region occlusion, which is caused by hybrid models of global and local motion, or depth differences in 3D scenes.
The depth-dependent RS distortion could be handled by modeling a 3D scene as layers of planar, and jointly esti-mating the depth and camera motion from more than three frames [49] at the cost of solving a complicated optimiza-tion problem. Deep neural networks could also be em-ployed to learn the underlying camera motion properties and depth maps to restore intra-frame occluded regions
[57], but it mainly deals with small occlusion artifacts due to the challenging nature of the single-image problem.
Neuromorphic event cameras are novel visual sensors that enable each pixel to work asynchronously to com-pare current/subsequent light intensity states and trigger a binary event whenever the log-intensity variation exceeds the preset thresholds [1, 10, 25, 47]. Thanks to their high-temporal-resolution property with microseconds-level sen-sitivity, event cameras are able to address several limita-tions of traditional frame-based tasks for dynamic scenes with fast motion. A particular body of the past methods have attended to event-based image reconstruction tasks
[3, 5, 6, 35, 40, 51], and a branch of literatures prove the beneﬁt that events could bring to high-frame rate video re-construction [13, 48]. Hence, the bottlenecks of image-only based RS correction and the beneﬁts of events motivate us to think about: Can we synergize the RS frame and event sig-nals and make use of the high-speed characteristic of events to assist RS correction?
To answer this question, we propose EvUnroll, a neural network that synchronizes and fuses event signals to cor-rect RS images as well as recover consecutive GS frames.
Events encode the pixel-wise motion information and in-tensity change, so we use them to bridge a ﬂow-based con-nection and a synthesis-based connection between RS and
GS frames, and correspondingly establish a ﬂow estimation module to correct edge distortions and a restoration module to restore occluded regions. These two branches are paral-lel and their outputs are fused through a reﬁnement module to ﬁnally restore the GS image at any timestamp in the ex-posure period of the input RS image. An optional module for dealing with blurry RS images is designed to handle real scenes with blurs. We further collect a new training dataset generated from real videos captured at 5700 fps, and a test-ing dataset captured with an RS-event hybrid camera sys-tem. Overall, this paper makes the following contributions:
• EvUnroll is the ﬁrst trial to improve RS correction with motion estimation and occlusion region restoration by involving event signals.
• We build a GS-event-RS triplet dataset called Gev-RS using RS-distortion-free frames from a high-speed camera to train the network, and build an RS-event hy-brid camera (Fig. 1 left) to collect a real testing dataset.
• EvUnroll outperforms state-of-the-art RS correction methods on commonly used datasets, and obtains a nu-merical gain of 2.98 dB for PSNR, accompanied by visual quality improvements (Fig. 1 right). 2.