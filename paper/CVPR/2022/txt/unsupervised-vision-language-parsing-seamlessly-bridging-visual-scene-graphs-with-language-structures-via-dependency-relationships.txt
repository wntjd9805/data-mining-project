Abstract
Understanding realistic visual scene images together with language descriptions is a fundamental task towards generic visual understanding. Previous works have shown compelling comprehensive results by building hierarchical structures for visual scenes (e.g., scene graphs) and natural languages (e.g., dependency trees), individually. However, how to construct a joint vision-language (VL) structure has barely been investigated. More challenging but worthwhile, we introduce a new task that targets on inducing such a joint
VL structure in an unsupervised manner. Our goal is to bridge the visual scene graphs and linguistic dependency trees seamlessly. Due to the lack of VL structural data, we start by building a new dataset VLParse. Rather than us-ing labor-intensive labeling from scratch, we propose an automatic alignment procedure to produce coarse struc-tures followed by human refinement to produce high-quality ones. Moreover, we benchmark our dataset by proposing a contrastive learning (CL)-based framework VLGAE, short for Vision-Language Graph Autoencoder. Our model ob-tains superior performance on two derived tasks, i.e., lan-guage grammar induction and VL phrase grounding. Abla-tions show the effectiveness of both visual cues and depen-dency relationships on fine-grained VL structure construc-tion. 1.

Introduction
Visual scene understanding has long been considered a primal goal for computer vision. Going beyond the success
*Equal contribution. Author orders are coin clipped. This work was conducted when Chao Lou and Yuhuan Lin were research interns at BI-GAI.
†Corresponding author.
Figure 1. Task illustration of VLParse. Different node types are identified by their background colors and the yellow areas indicate first-order relationships (§3.1). of high-accurate individual object detection in complicated environments, various attempts have been made for higher-order visual understanding, such as predicting an explain-able, structured, and semantically-aligned representation from scene images [18, 22, 41]. Such representations not only provide fine-grained visual cues for low-level recog-nition tasks, but have further demonstrated their applica-tions on numerous high-level visual reasoning tasks, e.g., visual question answering (VQA) [34, 50], image caption-ing [3, 44], and scene synthesis [18, 21].
Scene graph (SG), one of the most popular visual struc-tures, serves as an abstraction of objects and their com-plex relationships within scene images [22, 26]. Conven-tional scene graph generation models recognize and pre-dict objects, attributes, relationships, and their correspond-ing semantic labels purely from natural images in a fully-supervised manner [33, 43]. Despite the promising perfor-mance achieved on large-scale SG benchmarks, these meth-ods suffer from limitations on existing datasets and task set-ting [13]. First, a comprehensive scene graph requires dif-ferent semantic levels of visual understanding [27], whilst most current datasets only capture a small portion of acces-sible semantics for classification [13], which will cause the prediction model bias towards those most-frequent labels.
Second, building such datasets requires exhaustive label-ing of bounding boxes, relations, and corresponding seman-tics, which are time-consuming and inefficient. Third, it is typically hard to induce a semantically-consistent graph-ical structure solely from visual inputs, which typically requires an extra visual relation recognition module with heavy manually-labeled supervision.
Different from dense and noisy visual information, natu-ral language directly provides symbolic and structured in-formation (e.g., grammar) to support the comprehension process. Researches on language structure induction can date back to early computational linguistic theories [4, 5, 6].
Empowered by advances in deep learning techniques, a va-riety of neural structured prediction algorithms were pro-posed to analyze more complicated structure information and apply them to natural language tasks [9, 10, 23].
Dependency tree (DT) parsing, as one essential branch of language structured prediction, aims to generate a parse tree that is composed of vertices representing each word’s semantic and syntactic meanings, and directed edges rep-resenting the dependency relationships among them. Of note, such tree structure shares a similar idea as in SG.
However, the ground truth structure (commonly referred to as “gold structure”) requires professional linguists’ label-ing. To mitigate the data issue, pioneer works have also demonstrated the success of DT learning in an unsupervised schema [19, 23].
In this work, we leverage the best of both modalities and introduce a new task – unsupervised vision-language (VL) parsing (short for VLParse) – aiming to devise a joint
VL structure that bridges visual scene graphs with linguis-tic dependency trees seamlessly. By “seamless”, we mean that each node in the VL structure shall present the well-aligned information of some node in SG and DT, so are their relationships, as shown in Figure 1. To the best of our knowledge, this is the first work that formally defines the joint representation of VL structure with dependency relationships. Respecting the semantic consistency and in-dependent characteristics, the joint VL structure consid-ers both the shared multimodal instances and the indepen-dent instances for each modality. In such a heterogeneous graph, semantically consistent instances across two graphs (DT and SG) are aligned in different levels, which maxi-mizes the retention of the representation from two modal-ities. Some previous attempts have shown the benefits of exploring multi-modality information for structured under-standing. For example, Shi et al. [31] first proposes a vi-sually grounded syntax parser to induce the language struc-ture. [46, 48] further exploit visual semantics to improve the structure for language. These structures, however, are still for language syntactic parsing rather than for joint vision-language understanding. One closest work to us is VL-Grammar [17], which builds separate image structures and language structures via compound PCFG [23]. However, the annotations (i.e., segmentation parts) are provided in ad-vance.
VLParse aims to conduct thoughtful cross-modality understanding and bridge the gap between multiple sub-tasks: structure induction for the image and language sep-arately and unsupervised visual grounding. As a complex task, it is comprised of several instances, such as objects, attributes, and different levels of relationships. The interac-tions among different instances and subtasks can provide rich information and play a complementary or restrictive role during identification and understanding.
To address this challenging task, we propose a novel contrastive learning (CL)-based architecture, Vision-Language Graph Autoencoder (VLGAE), aiming at con-structing a multimodal structure and aligning VL informa-tion simultaneously. The VLGAE is comprised of fea-ture extraction, structure construction, and cross-modality matching modules. The feature extraction module extracts features from both modalities and builds representations for all instances in DT and SG. The structure construc-tion module follows the encoder-decoder paradigm, where the encoder obtains a compressed global VL representa-tion from image-caption pair using attention mechanisms; the decoder incorporates the inside algorithm to construct the VL structure recursively as well as compute the pos-teriors of spans. The VL structure induction is optimized by Maximum Likelihood Estimation (MLE) with a nega-tive likelihood loss. For cross-modality matching, we com-pute the vision-language matching score between visual im-age regions and language contexts. We further enhance the matching score with posterior values achieved from the structure construction module. This score is used to pro-mote the cross-modality fine-grained correspondence with the supervisory signal of the image-caption pairs via a CL strategy; see Figure 3 and Section 5 for details.
In summary, our contributions are five-fold: (i) We de-sign a joint VL structure that bridges visual scene graph and linguistic dependency tree; (ii) We introduce a new task VLParse for better cross-modality visual scene un-derstanding (§4); (iii) We present a two-step VL dataset creation paradigm without labor-intensive labelling and de-liver a new dataset (§3); (iv) We benchmark our dataset with a novel CL-based framework VLGAE (§5); (v) Empir-ical results demonstrate significant improvements on single modality structure induction and cross-modality alignment with the proposed framework. 2.