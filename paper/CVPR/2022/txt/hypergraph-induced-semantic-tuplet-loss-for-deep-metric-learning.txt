Abstract
In this paper, we propose Hypergraph-Induced Seman-tic Tuplet (HIST) loss for deep metric learning that lever-ages the multilateral semantic relations of multiple samples to multiple classes via hypergraph modeling. We formulate deep metric learning as a hypergraph node classification problem in which each sample in a mini-batch is regarded as a node and each hyperedge models class-specific seman-tic relations represented by a semantic tuplet. Unlike pre-vious graph-based losses that only use a bundle of pair-wise relations, our HIST loss takes advantage of the mul-tilateral semantic relations provided by the semantic tuplets through hypergraph modeling. Notably, by leveraging the rich multilateral semantic relations, HIST loss guides the embedding model to learn class-discriminative visual se-mantics, contributing to better generalization performance and model robustness against input corruptions. Extensive experiments and ablations provide a strong motivation for the proposed method and show that our HIST loss leads to improved feature learning, achieving state-of-the-art re-sults on three widely used benchmarks. Code is available at https://github.com/ljin0429/HIST.
Figure 1. Our HIST loss utilizes multilateral semantic relations between every sample and class (marked by color) for a given mini-batch. A semantic tuplet is defined for a class (e.g., green) and represents the sample’s semantic relations to the class. Inside the semantic tuplet, positive samples have definite relation values (= 1), and negative samples have soft relation values (≤ 1) based on their likelihood of belonging to the class. Each semantic tuplet is then modeled as a hyperedge. In this hypergraph, we formulate a node classification objective. By leveraging multilateral seman-tic relations, HIST loss enables the embedding network to capture important visual semantics suitable for deep metric learning. 1.

Introduction
Deep metric learning has been extensively studied for a variety of visual tasks, such as image retrieval [29, 37, 46], face recognition [25,34,48], person re-identification [4,50], and few-shot learning [36, 39, 42]. The aim of deep metric learning is to train a deep embedding network to yield dis-criminative features whereby the embedded features from semantically similar images are close to each other while those from dissimilar ones are far apart. This discerning capability of the embedding network is mainly achieved through loss functions, and many attempts have been made to design optimal loss functions for deep metric learning.
Conventionally, pair-based losses (e.g., Contrastive [5, 14], Triplet [19, 34], and N-pair [37] losses) have been employed. These minimize the feature distances of posi-tive pairs while maximizing those of negative pairs. How-ever, because not all data pairs are informative, pair-based losses often result in bad convergence [21, 29]. For reli-able performance, pair-based losses require elaborate sam-ple mining [15, 17, 49, 53], adding a computational bur-den. Alternative options are proxy-based [1, 13, 21, 29, 40, 60] and classification-based [26, 32, 44, 45, 48, 54] losses, which have demonstrated fast convergence and good perfor-mance. However, as they associate each data sample only with representative parameters (i.e., proxies or classifica-tion weights), neither proxy-based nor classification-based losses can leverage relations between data samples, which can limit the quality of the learned features.
Recently, to resolve the above limitations, several graph-based losses [7,35,55,60] have been proposed that leverage relations between data samples via graph modeling. These
methods construct a graph between data samples within a mini-batch and then formulate graph-based learning objec-tives. Although they have shown promising performance improvements, these graph-based losses have inherent lim-itations. Since each edge in the graph can only connect two nodes, lessons of graph-based losses are limited to a bun-dle of pairwise relations. Furthermore, each edge is defined by the feature distance or self-attention [41] and is deter-mined regardless of the classes of the two samples. That is, graph-based losses only consider pairwise feature relations and cannot take advantage of class semantic relations. Intu-itively, learning from multilateral relations between sample and class, i.e., relations among samples from the same class and similar-looking samples from different classes, must be helpful for understanding class-discriminative visual se-mantics, leading to improved feature learning.
In this work, we propose Hypergraph-Induced Semantic
Tuplet (HIST) loss, a novel loss function for deep metric learning that leverages multilateral semantic relations be-tween every sample and every class within a mini-batch via hypergraph modeling1. Concretely, such semantic relations are given by the proposed semantic tuplets. As shown in
Figure 1, the semantic tuplets are expressed by a seman-tic relation matrix with learnable elements where each row indicates the relation of each sample to every class in the mini-batch, and each column represents the relation of each class to every sample in the mini-batch. Thus, the semantic tuplets represent the multilateral semantic relations between every sample and every class by the learnable matrix. To fully exploit these multilateral semantic relations, we intro-duce hypergraph modeling whereby each semantic tuplet is modeled by a hyperedge. In this hypergraph, we formulate a node classification problem employing a hypergraph neu-ral network (HGNN) [8] and define HIST loss as the node classification loss. This formulation utilizing HGNN allows our HIST loss to benefit from the rich multilateral semantic relations provided by the proposed semantic tuples beyond pairwise feature relations.
We validate our method on three public benchmarks for deep metric learning, CUB-200-2011 [43], CARS-196 [22], and Stanford Online Products [31]. In the experiments, we present extensive ablation studies and parameter analyses to demonstrate the effectiveness of the proposed components.
In particular, we show that our HIST loss directs the em-bedding model to attend to meaningful object regions rather than background or distracting noises, contributing to better generalization performance and model robustness against input corruptions. The main results show that a standard embedding network trained with our HIST loss significantly outperforms state-of-the-art methods for all benchmarks. 1A hypergraph is a generalization of a graph where each hyperedge can connect more than two nodes. 2.