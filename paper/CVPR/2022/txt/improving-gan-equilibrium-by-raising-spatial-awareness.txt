Abstract
The success of Generative Adversarial Networks (GANs) is largely built upon the adversarial training between a generator (G) and a discriminator (D). They are expected to reach a certain equilibrium where D cannot distinguish the generated images from the real ones. However, such an equilibrium is rarely achieved in practical GAN training, instead, D almost always surpasses G. We attribute one of its sources to the information asymmetry between D and
G. We observe that D learns its own visual attention when determining whether an image is real or fake, but G has no explicit clue on which regions to focus on for a particular
To alleviate the issue of D dominating the synthesis. competition in GANs, we aim to raise the spatial awareness of G. Randomly sampled multi-level heatmaps are encoded into the intermediate layers of G as an inductive bias. Thus
G can purposefully improve the synthesis of certain image regions. We further propose to align the spatial awareness of G with the attention map induced from D. Through this way we effectively lessen the information gap between D and G. Extensive results show that our method pushes the two-player game in GANs closer to the equilibrium, leading to a better synthesis performance. As a byproduct, the intro-duced spatial awareness facilitates interactive editing over the output synthesis. Demo video and code are available at https://genforce.github.io/eqgan-sa/. 1.

Introduction
Generative Adversarial Network (GAN) has made huge progress toward image synthesis [3, 9, 15, 16, 21]. GAN is formulated as a two-player game between a generator (G) and a discriminator (D) [9], where G targets at reproducing the distribution of observed data through synthesizing new samples, and D competes with G by distinguishing the generated images from the real ones. In principle, they are expected to reach an equilibrium where D cannot tell the real and fake images apart [1, 9].
In practice, it turns out to be difficult to achieve such an equilibrium when training modern GAN variants [3,
Figure 1. Discriminator’s output scores for real and generated samples during training. A higher number indicates more realistic. We use StyleGAN2 [16] as the baseline and implement the proposed method over it on the LSUN Cat dataset. For a clear comparison, we report the minimum of the scores for real samples and the maximum for generated samples, which are supposed to be close to each other. However, the curve for the real is always above the curve for the generated. Our method can reduce the gap toward the equilibrium as well as improve the synthesis quality. 14–16], despite their appealing synthesis quality. Taking
StyleGAN2 [16] as an example, D almost always assigns a higher score to real images than to fake ones throughout the entire training process, as the blue lines shown in Fig. 1.
It suggests that D can easily beat G in the competition.
Increasing the model capacity of G barely helps mitigate this issue [3]. From this point of view, we can hold that
G fails to fool D even after the model converges, leaving a gap between the real and synthesized distributions. Such a disequilibrium remains much less explored in recent years, in spite of the rapid development of new GAN models.
To investigate the source of the aforementioned dise-quilibrium, we analyze the behaviour of D to figure out its advantage over G. With the help of GradCAM [23] as a neural network interpretation tool, we visualize the intermediate feature maps produced by D. As shown in
Fig. 2, given a target image, either real or generated, D holds its own visual attention, even it is merely learned
2. Analyzing GAN Equilibrium
Although GANs [1, 6, 9, 15, 16] are supposed to reach an equilibrium between G and D, this is barely fulfilled in practice. Typically, D wins over G most of the time. In this section, we attempt to investigate the cause of such a phenomenon. Sec. 2.1 briefly reviews the formulation of
GANs and describes the disequilibrium between G and D.
Sec. 2.2 interprets the visual attention learned by D, which could be one of the key sources for the disequilibrium. 2.1. Learning Objective of GAN
GAN training [9] is formulated as a two-player game, where a generator is trained to recover the real distribution pr over data {x}, while a discriminator is optimized to differentiate between pr and the generated distribution pg.
The overall objective function is min
G max
D
V (D, G) = Ex∼pr [log D(x)]
+ Ez∼pz [log(1 − D(G(z)))], (1) where z is a randomly sampled vector, subject to a prior distribution, pz. In particular, Goodfellow et al. [9] point out that the minimax game defined by GAN will reach an equilibrium point, where G recovers the training data distribution while D fails to distinguish the real and fake distributions. In that case, D is supposed to assign same realness scores to the real and synthesized samples.
Observation of Disequilibrium. We observe that such an equilibrium is seldom achieved in most GAN variants [3, 15, 16]. It appears that D almost always dominates the G-D competition, assigning a higher score to real data. An example of training StyleGAN2 [16] on LSUN Cat [26] is shown in Fig. 1. We can spot an obvious gap even between
D’s minimum score for real images and the maximum score for synthesized images. Such a phenomenon implies that the generated distribution pg is far from the real distribution pr, affecting the synthesis quality. 2.2. Visual Attention of Discriminator
After observing the disequilibrium in the interplay be-tween D and G, we would like to investigate the behavior of D to see how it manages to outperform G. Prior work on network interpretability, like CAM [28], has found that a classifier tends to focus on some discriminative regions to categorize a given image to the proper class. However, the discriminator in GANs is trained with the relatively i.e., only having real or fake labels. weak supervision,
Whether it can learn the attentive property from such a bi-classification task remains unknown. To look under the hood, we apply GradCAM [23] as an interpretability tool on the well-trained discriminator of StyleGAN2 [16].
Specifically, for a certain layer and a certain class,
GradCAM calculates the importance weight of each neuron
Figure 2. Spatial visual attention at the intermediate layers of the discriminator, visualized by GradCAM. A bright color indi-cates a strong contribution to the final score. ‘64 × 64’ indicates being upsampled from a 64 × 64 feature map. The samples are the real images and the images generated by StyleGAN2 [16]. from a bi-classification task (i.e., differentiating the real and fake domains). In other words, D is aware of which flawed regions to pay attention to when making the real/fake decision. Such an attentive property eases its competition to G because D can simply focus on the regions that are poorly synthesized by G. On the contrary, to produce an image, G takes a randomly sampled latent code as the input and has no explicit clue about which regions to focus on, let alone knowing the spatial preference of D. Due to such an information asymmetry, G may omit some important cues picked by D and hence get defeated in the two-player game.
In this paper, we propose a training method, termed as
EqGAN-SA, to improve the Equilibrium of GAN through raising Spatial Awareness. Concretely, we strive to lessen the information asymmetry between G and D by raising the spatial awareness of G. We design a hierarchical heatmap sampling strategy to match the coarse-to-fine synthesis mechanism [15, 25]. The sampled multi-level heatmaps are integrated into the per-layer feature maps of G. Meanwhile, to make sure G utilize the input heatmap adequately, we involve D as a regularizer to spatially supervise the generation process, through aligning the spatial awareness of G with the visual attention induced from D.
We evaluate the proposed method on various datasets.
As the orange lines shown in Fig. 1, we push the competi-tion between G and D closer to the equilibrium compared to the baseline method [16]. Consequently, our EqGAN-SA learns a distribution that is more identical to the real one, leading to a substantial improvement in the synthesis performance. For example, with Fr´echet Inception Distance (FID) [10] as the metric, we improve the baseline from 3.66 to 2.96 on FFHQ dataset [15] under 256×256 resolution. In addition, we can achieve interactive editing over the output image by altering the spatial heatmaps fed into G.
Figure 3. Illustration of EqGAN-SA. We conduct spatial encoding in G and align its spatial awareness with D attention maps. Specifically, we randomly sample spatial heatmaps and encode them into G via the spatial encoding layer (SEL). To implement the alignment during training, we calculate D attention maps over the generated samples via GradCAM. by average-pooling the gradients back-propagated from the final classification score, over the width and height. It then computes the attention map as a weighted combination of the importance weight and the forward activation maps, followed by a ReLU [8] activation. The attention map has the same spatial shape as the corresponding feature map.
In this work, we report the GradCAM attention maps all using gradients computed via maximizing the output of D.
It reflects the spatial preference of D in making a ‘real’ decision. In practice we find the attention maps are almost the same if instead minimizing the output of D, which indicates the areas that largely contribute to the decision are the same for a discriminator, no matter positively or negatively. The region with higher response within the attention map contributes more to the decision.
Fig. 2 visualizes some GradCAM results under multiple feature resolutions. They are obtained from the discrimina-tors of two StyleGAN2 models, trained on LSUN Cat and
FFHQ respectively. We have following observations: (1) D learns its own visual attention on both real and generated
It suggests that D makes the real/fake decision images. (2) by paying more attention to some particular regions.
The visual attention emerging from D shows a hierarchical
In the shallow layers (like 64 × 64 and 32 × property. 32 resolutions), D is attentive to local structures such as edge lines in the image. As the layer goes deeper,
D progressively concentrates on the overall location of discriminative contents, e.g., the face of a cat. (3) The hierarchical attention maps have fewer ‘local peaks’ at more abstract feature layers with a lower resolution. For example, there is only one peak in the 4 × 4 attention maps. 3. Improving GAN Equilibrium
As shown in Sec. 2, the discriminator of GANs has its own visual attention when determining real or fake image.
However, when learning to transform a latent vector into a realistic image, the generator receives no explicit clue about which regions to focus on. Specifically, for a particular synthesis, G has to decode all the needed information from the input latent code. Furthermore, G has no idea about the spatial preference of D on making the real/fake decisions. Such an information asymmetry puts G at a disadvantage when competing with D. In this section, we propose to raise the spatial awareness of G to lessen the information gap between G and D. The overall framework is illustrated in Fig. 3, which mainly consists of two steps, (1) explicitly encoding spatial awareness into G with a hierarchical heatmap sampling strategy and (2) aligning the spatial awareness of G with the visual attention from D via a feedback regularizer. Sec. 3.1 and Sec. 3.2 introduce these two techniques respectively. 3.1. Encoding Spatial Awareness in Generator
Hierarchical Heatmap Sampling. To improve the aware-ness of G on spatial regions, we propose a hierarchical heatmap sampling algorithm. This heatmap is responsible for teaching G which regions to pay more attention to.
Inspired by the visual attention induced from D as in
Sec. 2.2, we abstract our heatmap as a combination of several sub-regions and a background. Taking the heatmap at the 4 × 4 resolution as an example (leftest in Fig. 3), it tells G there is one region to focus on, whose center locates at the black dot. We formulate each sub-region as a 2D map,
Hi, which is sampled subject to a Gaussian distribution
Hi ∼ N (ci, cov), (2) where ci and cov denote the mean and the covariance.
According to the definition of 2D Gaussian distribution, ci just represents the coordinates of the region center. The final heatmap can be written as the sum of all sub-maps,
H = (cid:80)n i=1 Hi, where n denotes the total number of local regions for G to focus on.
As pointed out in the prior works [15, 25], the generator in GANs learns image synthesis in a coarse-to-fine manner, where the early layers provide a rough template and the latter layers refine the details. To match such a mechanism, we design a hierarchical heatmap sampling algorithm. Con-cretely, we first sample a spatial heatmap with Eq. (2) for the most abstract level (i.e., with the lowest resolution), and derive the heatmaps for other resolutions based on the initial one. The number of centers, n, and the covariance, cov, adapt accordingly to the feature resolution.
Heatmap Encoding. We incorporate the spatial heatmaps into G to raise its spatial awareness.
It generally can be conducted in two ways, via feature concatenation or feature normalization [12, 20]. We use a spatial encoding layer (SEL), respectively trying these two variants, de-noted as SELconcat and SELnorm. Specifically, inspired by
SPADE [20], the variant SELnorm integrates the hierarchical heatmaps into the per-layer feature maps of G with normal-ization and denormalization operations, as
Figure 4. Spatial Encoding Layer. The left shows how the layer works over StyleGAN2 at each resolution, and the right describes the internal of the SELnorm. The symbol ‘S’ represents the style in StyleGAN2, ‘N’ is the noise, and ‘H’ indicates the spatial heatmap. Learning from [20], we incorporate the spatial heatmaps into G via normalization and denormalization. supervision signal. Besides competing with D, G is further trained to minimize the distance between the attention map induced from D and the input heatmap H. The loss function can be written as
SELnorm(F, H) = ϕσ(H)
F − µ(F )
σ(F )
+ ϕµ(H), (3)
Lalign = || GradCAMD[G(H, z)] − H ||1. (4) where F denotes an intermediate feature map produced by
G, which is with the same resolution as H. µ(·) and σ(·) respectively stands for the functions of computing channel-wise mean and standard deviation. ϕµ(·) and ϕσ(·) are two learnable functions, whose outputs are point-wise and with a shape of (h, w, 1). Besides, as shown in Fig. 4, we use a residual connection to stabilize the intermediate features.
If not particularly specified, this paper adopts the variant
SELnorm since it shows a slightly better performance.
It is worth noting, although we learn the SELnorm archi-tecture from SPADE [20], these two methods are clearly different since SPADE targets at synthesizing images based on a given semantic segmentation mask, whose training re-quires paired ground-truth data, while our model is trained with completely unlabeled data. Meanwhile, SELnorm is just a replaceable component of our approach. 3.2. Aligning Spatial Awareness with Discriminator
Encoding heatmaps into G can explicitly raise its spatial awareness, but it is not enough to make G competitive with
D. The reason is that, D learns its own visual attention based on the semantically meaningful image contents, but the heatmaps fed into G are completely arbitrary. Without further guidance, how G is supposed to utilize the heatmaps is ambiguous. For example, G has no idea about “whether to pay more or less attention on the highlighted regions in the heatmap”. To make the best usage of the introduced spatial awareness, we propose to involve D as a regularizer to supervise G, to properly leverage the spatial knowledge.
Specifically, at each optimization step of G, we use D to generate the visual attention map via GradCAM as a self-We truncate the Lalign values if smaller than a constant τ , since the sampled heatmaps are not expected to perfectly match the real attention maps shaped by semantics. The threshold τ is set as 0.25 for all the experiments. Note that
D is not updated in the process above and only used as a supervision signal to train G. Such a regularization loss aligns the spatial awareness of G with the spatial preference of D, narrowing the information gap between them. 4. Experiments
We evaluate the proposed EqGAN-SA on multiple benchmarks. Sec. 4.1 provides the implementation de-tails. The main comparison and experimental results are presented in Sec. 4.2. Our EqGAN-SA could improve the spatial attentive property in G and mitigates the disequilib-rium to some extent. Sec. 4.3 includes the comprehensive ablation studies on the role of each proposed component. 4.1. Implementation Details
Datasets. We conduct the experiments on the LSUN
Cat [26], FFHQ [15], and LSUN Church [26] datasets.
The LSUN Cat dataset contains 1600K real-world images regarding different cats. Following the setting of
[14], we take 200K image samples from the LSUN Cat dataset for training. The FFHQ dataset consists of 70K high-resolution (1024 × 1024) images of human faces, under
Creative Commons BY-NC-SA 4.0 license [24]. Usually, the images are horizontally flipped to double the size of training samples. The LSUN Church dataset includes 126K images with visually complex church scenes. It is worthy noting that all images are resized to 256 × 256 resolution.
Figure 5. Qualitative results on LSUN Cat dataset and the demonstration of spatial awareness via varying the spatial heatmaps of the generator. Row (a) shows several generated samples of a model trained through EqGAN-SA. Rows (b) and (c) illustrate the spatial awareness of G: we keep the latent codes unchanged and move the spatial heatmap at the 4 × 4 level. The arrows indicate the movement direction, where the cat moves along with the varied heatmap. To further show the hierarchical structure, we move the heatmap at the finer level in the Row (d). Different from the body movement, the change in 8 × 8 heatmap (two centers) mainly moves the cat eyes, and the change in 16 × 16 heatmap (four centers) leads to subtle movement of the cat ears. It is worth noting that, as the content is being manipulated, our G knows to adjust the nearby regions to make everything coherent.
Spatial Heatmap Sampling and Encoding.
In practice, we find the GradCAM maps on the fine resolutions are too sensitive to semantic cues. Therefore, we only conduct encoding on the level 0, 1, 2 of G, i.e., resolution 4 × 4, 8 × 8, and 16 × 16. We heuristically generate 1, 2, 4 centers (in other words, sub-heatmaps) on these three levels.
We sample the level 0 heatmap center c0 0 by a Gaussian distribution with a mean of ( h 2 ), and a standard deviation of ( h 3 ). To keep the heatmaps consistent at various levels, we sample the level 1, 2 centers over the level 0
It indicates the mean of Gaussian distribution c1 center. n and c2 n is the sampled c0 0. Their standard deviations are 6 , w ( h
If we shift the level 0 center, the heatmaps of 6 ). other levels will move correspondingly. Following the coarse-to-fine manner, we decrease each center’s influence 3 , w 2 , w area level by level. Besides, we drop the sampling if the level 0 center is outside the image.
In our observation, the results of the proposed method are robust to these hyperparameters for heatmap sampling. Therefore, we use the same hyperparameters for heatmap sampling on all the datasets. More implementation details are provided in
Supplementary Material.
Training. We implement our EqGAN-SA on the official implementation of StyleGAN2, such that the state-of-the-art image generation method StyleGAN2 [16] serves as our baseline. We follow the default training configuration of [14] for the convenience of reproducibility, and keep the hyperparameters unchanged to validate the effectiveness of our proposed framework. For example, we train all the models with a batch size of 64 on 8 GPUs and continue
Figure 6. Qualitative results on the FFHQ dataset (top) and the LSUN Church dataset (bottom). Each row uses the same spatial heatmap but different latent codes, and each column uses the same latent code. We can see that the spatial heatmap roughly controls the pose of the face and the viewpoint of the church building, which facilitates the interactive spatial editing of the output image. the training until 25M images have been shown to the discriminator. Our method increases the training time by around 30% compared with the baseline.
Evaluation. We use Fr´echet Inception Distance (FID) [10] between 50K generated samples and all the available real samples as the image generation quality indicator. We utilize a specific approximation of Wasserstein distance to quantify the degree of disequilibrium, the distance between (a) the minimum discriminator scores for real samples and (b) the maximum scores for generated samples.
We term it as Disequilibrium Indicator (DI), where DI = min(sr) − max(sg).
It indicates if D can distinguish between the hardest real image and most realistic generated image. To keep the result stable, we compute DI over 128 randomly sampled images (64 real and 64 fake) for 200 times and take the mean value. We also discuss the validity of DI in Supplementary Material. i.e., 4.2. Main Results
Spatial Awareness is Raised in Generator. As discussed in Sec. 3.1, we propose to encode spatial awareness into
G. Here we provide the qualitative results in Fig. 5 and
Fig. 6 to verify that G indeed learns to focus on the regions specified by input heatmaps. Specifically, we keep the latent vector unchanged and move the spatial heatmaps. As we move the level 0 heatmap of the sample (b) and (c) in
Fig. 5, the cat bodies move under the guidance of heatmap movement (indicated by red arrows). We can observe the same phenomenon by watching each column of Fig. 6.
Furthermore, as illustrated by each row of Fig. 6, the human images generated with the same heatmap will put faces on the same location. In addition, as desired by our hierarchical design, moving level 1 and 2 heatmaps would affect local structures. For example, in the sample (d) of Fig. 5, the change in level 1 heatmap leads to a movement in cat eyes.
As we slightly push the top two centers of level 2 heatmap to the right, the cat ears subtly turn right while other parts, even the cat whiskers, remain unchanged. These verify the effect of our hierarchical spatial encoding. We also notice
G could adaptively modify the nearby texture and structure to give a reasonable image. Additionally, we visualize the generator intermediate features to investigate whether it has spatial awareness and the effect of our method, as shown in Supplementary Material. Overall, the moved contents depict the spatial awareness of G, which shows a hierarchical style and matches our design target.
Equilibrium is Improved. The quantitative results on the three datasets are provided in Tab. 1. On all the datasets, the metric DI shows a drop after encoding spatial awareness into G, and a further decrease with the help of
Table 1. Quantitative results on LSUN Cat, FFHQ, and LSUN
Church datasets, all trained with 25M images shown to dis-criminator. The baseline uses the architecture of StyleGAN2 [15].
We use FID as the metric for image generation quality. We also formulate a metric Disequilibrium Indicator (DI), a specific form of Wasserstein distance [1], to quantify the disequilibrium.
DI is calculated as min(sr) − max(sg), where s indicates the discriminator outputs before the activation. We discuss the validity of DI and include the results of other metrics in Supplementary
Material. ↓ denotes smaller is better.
Method
Baseline
+ SEL
+ Lalign
Cat [26] 256 × 256
FID ↓ DI ↓ 3.64 8.36 3.12 7.82 2.39 6.81
FFHQ [15] 256 × 256
FID ↓ DI ↓ 1.62 3.66 1.38 3.39 0.73 2.96
Church [15] 256 × 256
FID ↓ DI ↓ 3.01 3.73 2.59 3.55 2.07 3.11
Lalign. For example, DI reduces from 3.64 to 3.12 and finally 2.39 on the LSUN Cat dataset. This observation verifies the hypothesis that the aforementioned information asymmetry is a source of GAN disequilibrium, and our proposed approach can mitigate the imbalance.
With the improved equilibrium, the image synthesis quality also becomes better. We observe that there are consistent improvements over the FID on three datasets, outperforming the baseline StyleGAN2. We also validate our idea on the basis of SN-DCGAN (DCGAN [21] with spectral normalization [18]) on the CIFAR-10 [17] dataset, as shown in Tab. 2. 4.3. Ablation Study
How Important is the Type of Spatial Heatmap Sam-pling? Different sampling strategies are applied here to validate our choice, as provided in Tab. 3. Specifically, 2D Gaussian noise is first considered as a straightforward baseline experiment since it provides non-structured spatial information. Accordingly, 2D Gaussian noise introduces indicates, merely feeding a no performance gains. 2D heatmap but without any region to be emphasized is insufficient to raise spatial awareness and mitigate the disequilibrium.
It
Besides, we also use multiple-resolution spatial heatmaps but discard the hierarchical constraint, referred as
Non-Hie in Tab. 3. Namely, spatial heatmaps at different resolutions are independently sampled. Obviously, the baseline is improved by this non-hierarchical spatial heatmap, demonstrating the effectiveness of the spatial awareness of G. Moreover, when the hierarchical sampling is adopted, we observe further improvements over the synthesis quality and equilibrium.
How Important is the Way of Spatial Encoding? In order to raise the spatial awareness of G, there exist several alternatives to implement. Therefore, we conduct
Table 2. Quantitative results on the CIFAR-10 dataset over the baseline SN-DCGAN, with conditional or unconditional image synthesis.
Method
SN-DCGAN
+ Ours
Unconditional
DI ↓
FID ↓ 1.85 23.72 0.96 16.93
Conditional
FID ↓ 19.89 13.56
DI ↓ 1.61 0.78
Table 3. Spatial Heatmap Sampling. With other parts un-changed, we separately throw random Gaussian noise, spatial heatmap without hierarchical sampling, and our spatial heatmap as the input to the spatial encoding layer.
FID ↓
DI ↓
Baseline 8.36 3.64
Gau. Noise 8.31 3.67
Non-Hie 7.29 2.70
Hie 6.81 2.39
Table 4. Ablation study on spatial encoding. We flatten the spatial heatmap and incorporate the vectorized one into latent
It destroys the 2D space structure, code, denoted as ‘Flatten’. and hence cannot improve over the baseline.
Instead, encoding heatmaps in the spatial domain is beneficial. The two variants of
SEL show a similar result, where SELnorm is slightly better.
Method
Baseline
Flatten
SELconcat
SELnorm
Cat [26]
FFHQ [15]
FID ↓ 8.36 8.63 7.02 6.81
DI ↓ 3.64 3.71 2.47 2.39
FID ↓ 3.66 3.78 3.11 2.96
DI ↓ 1.62 1.63 0.90 0.73 an ablation study on LSUN Cat and FFHQ datasets to test various methods. For example, the first way of feeding the spatial heatmap is to flatten the 2D heatmap as a vector, and then concatenate it with the original latent code. This setting aims at validating whether maintaining 2D structure of spatial heatmap is necessary. Besides, we also use two different SEL modules (i.e., SELconcat and
SELnorm) mentioned in Sec. 3.1. Their details are available in Supplementary Material. For a fair comparison, all the ablation studies use Lalign.
Tab. 4 presents the results. Apparently, simply feeding the spatial heatmap but without the explicit 2D structure leads to no gains compared to the baseline. It might imply that it is challenging to use a vector (like the original latent code) to raise the spatial awareness of the generator.
Instead, the proposed SEL module could introduce the substantial improvements, demonstrating the effectiveness of the encoding implementation.
Whether Visual Attention of D is Robust and Con-sistent? As discussed in Sec. 3.2, the alignment loss (Lalign) uses the D attention maps to guide G. It assumes the attention map from D is stable enough to serve as a supervision signal and valid over the whole training. To validate the design, we first explore the robustness of D.
As shown in the left top of Fig. 7, we add random Gaussian noise to a real image from the LSUN Cat dataset, destroying its texture. As the noise amplitude increasing, we can visually see the noise pattern and the local appearance has been over smoothed. D is still attentive to the original important regions, e.g., the human and cat faces. We then test its response to terrible samples generated by a poorly-trained G, illustrated in the right top of Fig. 7. The samples contain distorted human, cat and background. That is, the visual attention of D is sufficiently robust to the noise perturbation and the generated artifacts. Furthermore, as indicated in the bottom of Fig. 7, we validate whether the visual attention is consistent throughout the entire training process. At a very early stage of training, D has already localized the discriminative regions. The focus of such visual attention is consistently maintained till the end of the training. The robustness and consistency property of
D attention could successfully provide a support for Lalign. 5. Discussion