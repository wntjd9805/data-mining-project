Abstract
We present techniques for scaling Swin Transformer [35] up to 3 billion parameters and making it capable of train-ing with images of up to 1,536×1,536 resolution. By scal-ing up capacity and resolution, Swin Transformer sets new records on four representative vision benchmarks: 84.0% top-1 accuracy on ImageNet-V2 image classification, 63.1 / 54.4 box / mask mAP on COCO object detection, 59.9 mIoU on ADE20K semantic segmentation, and 86.8% top-1 accu-racy on Kinetics-400 video action classification.
We tackle issues of training instability, and study how to effectively transfer models pre-trained at low resolutions to higher resolution ones. To this aim, several novel technolo-gies are proposed: 1) a residual post normalization tech-nique and a scaled cosine attention approach to improve the stability of large vision models; 2) a log-spaced contin-uous position bias technique to effectively transfer models pre-trained at low-resolution images and windows to their higher-resolution counterparts. In addition, we share our crucial implementation details that lead to significant sav-ings of GPU memory consumption and thus make it feasi-ble to train large vision models with regular GPUs. Using these techniques and self-supervised pre-training, we suc-cessfully train a strong 3 billion Swin Transformer model and effectively transfer it to various vision tasks involving high-resolution images or windows, achieving the state-of-the-art accuracy on a variety of benchmarks. Code is avail-able at https://github.com/microsoft/Swin-Transformer. 1.

Introduction
Scaling up language models has been incredibly success-ful. It significantly improves a model’s performance on lan-*Equal. †Project lead. Ze, Yutong, Zhuliang, Zhenda, Yixuan, Jia are long-term interns at MSRA.
Figure 1. To better scale up model capacity and window resolu-tion, several adaptions are made on the original Swin Transformer architecture (V1): 1) A res-post-norm to replace the previous pre-norm configuration; 2) A scaled cosine attention to replace the original dot product attention; 3) A log-spaced continuous relative position bias approach to replace the previous parameterized ap-proach. Adaptions 1) and 2) make it easier for the model to scale up capacity. Adaption 3) makes the model to be transferred more effectively across window resolutions. The adapted architecture is named Swin Transformer V2. guage tasks [12, 16, 37, 38, 40, 41] and the model demon-strates amazing few-shot capabilities similar to that of hu-man beings [5]. Since the BERT large model with 340 mil-lion parameters [12], language models are quickly scaled up by more than 1,000 times in a few years, reaching 530 billion dense parameters [38] and 1.6 trillion sparse param-eters [16]. These large language models are also found to possess increasingly strong few-shot capabilities akin to hu-man intelligence for a broad range of language tasks [5].
On the other hand, the scaling up of vision models has been lagging behind. While it has long been recognized that larger vision models usually perform better on vision tasks [19,48], the absolute model size was just able to reach
about 1-2 billion parameters very recently [10,18,28,44,66].
More importantly, unlike large language models, the exist-ing large vision models are applied to the image classifica-tion task only [10, 44, 66].
To successfully train large and general vision model, we need to address a few key issues. Firstly, our experiments with large vision models reveal an instability issue in train-ing. We find that the discrepancy of activation amplitudes across layers becomes significantly greater in large models.
A closer look at the original architecture reveals that this is caused by the output of the residual unit directly added back to the main branch. The result is that the activation values are accumulated layer by layer, and the amplitudes at deeper layers are thus significantly larger than those at early lay-ers. To address this issue, we propose a new normalization configuration, called res-post-norm, which moves the LN layer from the beginning of each residual unit to the back-end, as shown in Figure 1. We find this new configuration produces much milder activation values across the network layers. We also propose a scaled cosine attention to replace the previous dot product attention. The scaled cosine at-tention makes the computation irrelevant to amplitudes of block inputs, and the attention values are less likely to fall into extremes. In our experiments, the proposed two tech-niques not only make the training process more stable but also improve the accuracy especially for larger models.
Secondly, many downstream vision tasks such as object detection and semantic segmentation require high resolu-tion input images or large attention windows. The win-dow size variations between low-resolution pre-training and high-resolution fine-tuning can be quite large. The current common practice is to perform a bi-cubic interpolation of the position bias maps [15, 35]. This simple fix is some-what ad-hoc and the result is usually sub-optimal. We in-troduce a log-spaced continuous position bias (Log-CPB), which generates bias values for arbitrary coordinate ranges by applying a small meta network on the log-spaced co-ordinate inputs. Since the meta network takes any coor-dinates, a pre-trained model will be able to freely transfer across window sizes by sharing weights of the meta net-work. A critical design of our approach is to transform the coordinates into the log-space so that the extrapolation ratio can be low even when the target window size is sig-nificantly larger than that of pre-training. The scaling up of model capacity and resolution also leads to prohibitively high GPU memory consumption with existing vision mod-els. To resolve the memory issue, we incorporate several important techniques including zero-optimizer [42], activa-tion check pointing [6] and a novel implementation of se-quential self-attention computation. With these techniques, the GPU memory consumption of large models and resolu-tions is significantly reduced with only marginal effect on the training speed.
With the above techniques, we successfully trained a 3 billion Swin Transformer model and effectively transferred it to various vision tasks with image resolution as large as 1,536×1,536, using Nvidia A100-40G GPUs. In our model pre-training, we also employ self-supervised pre-training to reduce the dependency on super-huge labeled data. With 40× less labelled data than that in previous practice (JFT-3B), the 3 billion model achieves the state-of-the-art accu-racy on a broad range of vision benchmarks. Specifically, it obtains 84.0% top-1 accuracy on the ImageNet-V2 image classification validation set [43], 63.1 / 54.4 box / mask AP on the COCO test-dev set of object detection, 59.9 mIoU on ADE20K semantic segmentation, and 86.8% top-1 accu-racy on Kinetics-400 video action classification, which are
+NA%, +4.4/+3.3, +6.3 and +1.9 higher than the best num-bers in the original Swin Transformers [35, 36], and surpass previous best records by +0.8% ( [66]), +1.8/+1.4 ( [61]),
+1.5 ( [3]) and +1.4% ( [45]).
By scaling up both capacity and resolution of vision models with strong performance on general vision tasks, just like a good language model’s performance on general
NLP tasks, we aim to stimulate more research in this di-rection so that we can eventually close the capacity gap be-tween vision and language models and facilitate the joint modeling of the two domains. 2.