Abstract
Few-sample compression aims to compress a big redun-dant model into a small compact one with only few sam-ples. If we fine-tune models with these limited few samples directly, models will be vulnerable to overfit and learn al-most nothing. Hence, previous methods optimize the com-pressed model layer-by-layer and try to make every layer have the same outputs as the corresponding layer in the teacher model, which is cumbersome. In this paper, we pro-pose a new framework named Mimicking then Replacing (MiR) for few-sample compression, which firstly urges the pruned model to output the same features as the teacher’s in the penultimate layer, and then replaces teacher’s layers be-fore penultimate with a well-tuned compact one. Unlike pre-vious layer-wise reconstruction methods, our MiR optimizes the entire network holistically, which is not only simple and effective, but also unsupervised and general. MiR outper-forms previous methods with large margins. Codes is avail-able at https://github.com/cjnjuwhy/MiR. 1.

Introduction
Convolutional neural networks (CNNs) with millions of parameters can only be utilized by high-performance de-vices, even when we only care about the inference stage. In order to put deep models into small devices and decrease the latency and memory consumption, network compression [5] is widely used in model deployment. To compress a model, network pruning methods [14,16,20,22,23,30] try to prune less useful weights or channels, while quantization meth-ods [7] aim at quantizing the weights and activations with fewer bits, and knowledge distillation methods [15, 24] try to distill the dark knowledge from a potentially redundant big model into a more compact small one.
These compression methods have been very successful in reducing computations and accelerating inference speed.
*Corresponding author. This research was partly supported by the Na-tional Natural Science Foundation of China under Grant 61772256 and
Grant 61921006.
But, they all assume full access to the training data, and unfortunately this assumption does not hold in many cases, especially in non-academic scenarios. When handling sen-sitive data (e.g., medical or commercial data), data security issues are of special importance. As a response to this is-sue, the few-sample or few-shot compression problem aims to compress models with limited samples, which is a practi-cal way to protect data privacy by using only non-sensitive data.
To tackle this few-sample compression problem, recent methods try to obtain a compact model in a layer-wise man-ner. Li et al. proposed FSKD [18], which adds a 1 × 1 conv. after each layer and optimizes the weights by minimizing the reconstruction error for each layer. Bai et al. intro-duced a cross distillation operation to better alleviate the er-ror accumulation in each layer [1]. Furthermore, Shen et al. tried to distill a compact model by grafting layers from the teacher model to the student model progressively [27]. All these methods tried to reconstruct the representation ability layer-wisely, which is not only cumbersome but may also cause error accumulation. Moreover, this layer-wise frame-work needs a one-to-one relationship between the pruned and the original model, therefore imposing heavy restric-tions to the pruned model’s structure.
Instead, we advocate pruning and optimizing the entire network holistically instead of following this cumbersome layer-wise reconstruction framework, and recover the rep-resentation abilities of the pruned model globally instead of training the layers locally. In this regard, we propose a new framework, Mimicking then Replacing (MiR), which first urges the pruned model to output the same features as the teacher’s (i.e., mimicking features), and following
LSHKD [29] we can mimic the features in the penultimate layer. Then, while keeping the (classification, detection, etc.) head intact, we replace all the other layers with the trained compact model after mimicking. The features in the penultimate layer in LSHKD [29] are obtained after a pool-ing layer. We reveal that mimicking the features before the pooling layer can boost the accuracy without extra compu-tation.
Figure 1. Illustration of different pruning schemes. We use a rectangle with notation N1 × N2 × K to represent a conv. with N1 output channels, N2 input channels, and kernel size K. Blue rectangles represent features (activation maps). In this figure: a) A residual block in
ResNet-34 contains two conv., and we omit batch normalization and non-linear layers; b) In the ‘Normal’ pruning scheme, only channels within residual blocks are pruned, in which the light blue color indicates channels that are pruned; c) the ‘Residual’ pruning scheme, which not only prunes channels within blocks, but also prunes coupled channels across different residual blocks; d) In the ‘CD’ pruning scheme, only dashed and transparent residual blocks are pruned, and these blocks are pruned with the ‘Normal’ scheme. Best viewed in color.
MiR is simple to use (simple algorithm and zero ex-tra hyperparameters), general (suitable for many scenarios), unsupervised (not even use labels for the few-sample train-ing set) and highly accurate (outperforming current state-of-the-art methods by a large margin). With MiR, we can train an accurate compressed model within dozens of min-utes and only hundreds of samples. To sum up, our contri-butions are:
• We propose a simple but effective framework, Mimick-ing then Replacing (MiR), for few-sample compression.
MiR contains no extra hyperparameters to tune but out-performs state-of-the-art methods with a large margin.
• MiR is general to use. It can be used for different prun-ing schemes and is effective in different network archi-tectures. Moreover, it has no restriction on the model’s structure, and can avoid error accumulation because we are the first to optimize the weights holistically in few-shot compression.
• LSHKD [29] mimics the features in the penultimate layer, but we find that mimicking features before pool-ing helps a lot, at least in few-shot compression. It brings 0.5 to 2.0 percentage points without extra computation compared with mimicking features after the final pooling layer (i.e., the penultimate layer). 2.