Abstract
We propose an end-to-end network that takes a single perspective RGB image of a complex road scene as input, to produce occlusion-reasoned layouts in perspective space as well as a parametric bird’s-eye-view (BEV) space. In contrast to prior works that require dense supervision such as semantic labels in perspective view, our method only requires human annotations for parametric attributes that are cheaper and less ambiguous to obtain. To solve this challenging task, our design is comprised of modules that incorporate inductive biases to learn occlusion-reasoning, geometric transformation and semantic abstraction, where each module may be supervised by appropriately transform-ing the parametric annotations. We demonstrate how our design choices and proposed deep supervision help achieve meaningful representations and accurate predictions. We validate our approach on two public datasets, KITTI and
NuScenes, to achieve state-of-the-art results with consider-ably less human supervision. 1.

Introduction
Understanding road layout from images is essential for real-world applications such as autonomous driving or path planning [5, 8, 13, 31], where, besides the usual perspective space outputs, top-view representations of geometry and semantics have been popular. Non-parametric representa-tions such as pixel-level semantics [31] generally require labor-intensive and potentially ambiguous supervision in the top-view, for example, when dealing with occluded regions.
On the other hand, parametric representations for top-view layouts are desirable for their interpretability, which is ben-eficial for higher-level reasoning and decision-making in downstream applications.
Parametric attributes such as presence of side roads or number of lanes may be easily annotated by humans given sensor inputs, and require less effort than pixel-level seman-tic annotations. However, besides parametric annotations1 in 1Parametric and attribute-level annotations are used interchangeably in our paper.
Figure 1. We propose an end-to-end model that inputs perspective image and outputs parametric layouts in top-view. Compared to ex-isting methods, ours requires only the parametric layout annotations during training and achieves SOTA performance under complex road scenarios. Moreover, it generates occlusion-reasoned (see the predicted semantics on regions occluded by cars) pixel-level semantics in both perspective and top view. the bird’s-eye-view (BEV), i.e. top-view, previous works that estimate parametric BEV layouts also require pixel-level su-pervision in perspective images [23, 46] or handle only very simple road layouts [35]. This paper seeks to obtain para-metric BEV maps as well as pixel-level semantics in both the perspective and top views, but using only the cheaper parametric supervision on attributes.
While relying on cheap supervision only is undoubtedly a goal worth pursuing, removing the dense perspective super-vision makes the problem harder. This is non-trivial, since there exists a large gap between sparse parametric supervi-sion and dense pixel-level semantic supervision. To bridge the gap, one must reason about the underlying geometry to map the parametric supervision to top-view and get the correct semantics, even in occluded regions.
We address this challenge through two key insights. First, rather than directly regressing the parametric BEV layout from RGB image space, we introduce two intermediate steps – a perspective semantics (PS) module and a top-view se-mantics (TS) module – to predict intermediate occlusion-reasoned per-pixel perspective and BEV layouts (Fig. 1).
Second, to obtain supervision for PS/TS, a simple renderer can convert the parametric annotations to occlusion-reasoned per-pixel semantic annotations in both the BEV and perspec-tive view, with the help of geometric transformation. This allows meaningful deep supervision [18, 19] of intermediate modules without additional annotation costs, thus weakly supervised. The weakly but deeply supervised PS and TS modules together lead to accurate parametric BEV layout by introducing inductive biases on the type of reasoning the network should perform, thereby facilitating complex tasks such as occlusion reasoning, geometric transformation and semantic abstraction that correspond to the parametric supervision.
The above insights make our method simple yet highly effective, even outperforming previous methods that rely on perspective-view dense supervision for semantic segmenta-tion. We validate our choices through state-of-the-art (SOTA) accuracies on both KITTI [9] and NuScenes [28] datasets, achieving 47.3% and 13.0% F1 score. In extensive ablation experiments, we establish the value of the inductive biases introduced by the PS and TS modules, as well as the deep supervision through transformed parametric annotations.
To summarize, our key contributions are:
• An end-to-end model for occlusion-reasoned perspec-tive and top-view parametric layout in complex scenes.
• Intermediate module design that incorporates inductive biases to learn occlusion-reasoning, geometric transfor-mation and semantic abstraction.
• Deep supervision with cheap parametric annotations in top view only, rather than requiring additional expen-sive per-pixel labeling in either perspective or top view.
• State-of-the-art results on publicly available datasets. 2.