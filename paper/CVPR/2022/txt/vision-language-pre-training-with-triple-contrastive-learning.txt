Abstract
Vision-language representation learning largely benefits from image-text alignment through contrastive losses (e.g.,
InfoNCE loss). The success of this alignment strategy is attributed to its capability in maximizing the mutual informa-tion (MI) between an image and its matched text. However, simply performing cross-modal alignment (CMA) ignores data potential within each modality, which may result in de-graded representations. For instance, although CMA-based models are able to map image-text pairs close together in the embedding space, they fail to ensure that similar inputs from the same modality stay close by. This problem can get even worse when the pre-training data is noisy. In this pa-per, we propose triple contrastive learning (TCL) for vision-language pre-training by leveraging both cross-modal and intra-modal self-supervision. Besides CMA, TCL introduces an intra-modal contrastive objective to provide complemen-tary benefits in representation learning. To take advantage of localized and structural information from image and text input, TCL further maximizes the average MI between lo-cal regions of image/text and their global summary. To the best of our knowledge, ours is the first work that takes into account local structure information for multi-modality repre-sentation learning. Experimental evaluations show that our approach is competitive and achieves the new state of the art on various common down-stream vision-language tasks such as image-text retrieval and visual question answering. 1.

Introduction
Self-supervision is an active research topic both in vision and language representation learning. Numerous methods have been proposed with an impressive performance on chal-lenging tasks [5, 7, 10, 17, 19, 40]. A typical approach is to pre-train a model on massive amounts of unlabeled data in a self-supervised manner, then fine-tune it for downstream 1https://github.com/uta-smile/TCL 2This work was done while Jinyu Yang was interning at Amazon tasks (e.g., zero-shot learning and transfer learning) of inter-est. In the vision, self-supervision can be carried out using exemplars [13], predicting the relative position between two random patches [11] or via solving jigsaw [32]. In language, masked language modeling (MLM) is widely used as the method of choice for self-supervision.
Inspired by the success of self-supervision in individ-ual modalities, there is a surging interest in self-supervised vision-language pre-training (VLP) [4, 14], which is essen-tial for multi-modal tasks such as visual question answering (VQA), image-text retrieval, and visual entailment. These tasks heavily rely on joint multi-modal embeddings which are typically obtained by modeling interactions between vi-sion and language features. To achieve this goal, various
VLP frameworks are proposed by exploiting massive image-text pairs in the past few years [8, 16, 27, 28], where the key insight is to apply a fusion encoder to the concatenation of vision and language features to learn joint representa-tions. Although simple and effective, this strategy suffers from the problem that vision and language features lie in different embedding spaces, which makes the feature fusion quite challenging [26]. To mitigate this problem, the most recent state of the art [26] disentangles the learning process into two stages: i) first align cross-modal features by using a contrastive loss (i.e., InfoNCE [33]) to pull the embed-dings of matched image-text pairs together while pushing those of non-matched pairs apart; then ii) apply a fusion en-coder to the aligned image and text representations to learn joint embeddings. Specifically, stage 1 aims to maximize the mutual information (MI) between matched image-text pair (I, T ) through InfoNCE loss, which is spurred by the fact that I and T represent two ”views” of the same se-mantic [46]. However, the limitation of stage 1 lies in that: simply performing cross-modal alignment (CMA) cannot fully guarantee the expressiveness of the learned features that is essential for joint multi-modal representation learn-ing. The main reason is that I and T are unable to fully describe each other. For instance, the text in (Figure 1 A) only focus on salient objects in the paired image, while over-looking other detailed and fine-grained information. To align
I and T , only co-occurring features are captured by CMA.
This is also evidenced by [23], where the performance of
CMA-based features on image-text retrieval is far greater than intra-modal retrieval (image-image and text-text). Fur-thermore, the pre-training datasets are usually collected from the web and are inherently noisy. This leads to learning de-graded representations, where cross-modal features fail to capture certain key concepts.
As transformer became increasingly popular in both vi-sion and language tasks, existing VLP methods adopted transformer architecture for extracting visual and linguis-tic features. Specifically, [CLS] tokens from the vision transformer (e.g., ViT [12]) and the text transformer (e.g.,
BERT [10]) are used to represent the global information of the input. For instance, ALBEF [26] maximizes MI between vision [CLS] and text [CLS]. However, global MI maximiza-tion fails to consider localized and structural information in the input [1, 20]. One potential side-effect is that it encour-ages the encoder to mainly extract information from certain unrelated/noisy image patches or text tokens that dominate
MI.
In this paper, we introduce a novel VLP framework called triple contrastive learning (or TCL for short). The key idea is to learn desirable representations by leveraging both cross-modal and intra-modal self-supervision, aiming to make it easier for the fusion encoder to learn multi-modal interac-tions. To achieve this goal, TCL introduces three contrastive modules: cross-modal alignment (CMA), intra-modal con-trastive (IMC), and local MI maximization (LMI), all of which rely on MI maximization. Specifically, i) CMA pulls the embeddings of matched image-text pairs together while pushing those of non-matched pairs apart by maximizing global MI between matched image and text; ii) complemen-tary to CMA, IMC maximizes agreement between differ-ently augmented views of the same data example through maximizing their global MI; iii) LMI encourages high MI between the global representation and every local region (e.g., image patches and text tokens) of the input, which is designed to remedy the side-effects that are introduced by the global MI maximization. The combination of these three modules allows us to i) learn representations that are seman-tically meaningful not only for cross-modal image-text pairs but also for intra-modal inputs; ii) capture the structural and localized information by extracting relevant features that are shared across local patches/tokens.
Our main contributions can be summarized as
• We leverage both cross-modal and intra-modal self-supervision to provide complementary benefits in repre-sentation learning, which facilitates modeling of better joint multi-modal features in the fusion encoder;
• Rather than simply relying on global information for multi-modal contrastive learning, we propose to take advantage of localized and structural information in both image and text input by maximizing local MI maximization between local regions and their global summary;
Comprehensive empirical studies demonstrate that TCL achieves a new state of the art on a wide range of vi-sion+language benchmarks, such as image-text retrieval and
VQA. Specifically, on zero-shot image-text retrieval tasks, our method achieves significant improvement than ALIGN
[23] (a mean recall of 79.5% vs 70.9% on MSCOCO). It is noteworthy that ALIGN is pre-trained on 1.8B image-text pairs, which is approximately 350× larger than TCL (5M). By pre-training TCL on a large-scale dataset with 14M image-text pairs, we observe a significant performance boost, implying its potential for further improvement with larger datasets. To investigate the effectiveness of each component in TCL, comprehensive ablation studies are also carried out with detailed analyses. 2.