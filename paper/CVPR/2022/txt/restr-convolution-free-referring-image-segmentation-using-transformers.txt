Abstract
: Visual features
: Class seed embedding
: Linguistic features
: Inner product
Patch-level prediction
Referring image segmentation is an advanced semantic segmentation task where target is not a predeﬁned class but is described in natural language. Most of existing meth-ods for this task rely heavily on convolutional neural net-works, which however have trouble capturing long-range dependencies between entities in the language expression and are not ﬂexible enough for modeling interactions be-tween the two different modalities. To address these issues, we present the ﬁrst convolution-free model for referring im-age segmentation using transformers, dubbed ReSTR. Since it extracts features of both modalities through transformer encoders, it can capture long-range dependencies between entities within each modality. Also, ReSTR fuses features of the two modalities by a self-attention encoder, which en-ables ﬂexible and adaptive interactions between the two modalities in the fusion process. The fused features are fed to a segmentation module, which works adaptively accord-ing to the image and language expression in hand. ReSTR is evaluated and compared with previous work on all public benchmarks, where it outperforms all existing models. 1.

Introduction
Throughout the recent years, there have been witnessed remarkable advances in semantic segmentation in terms of both efﬁcacy and efﬁciency [4, 5, 15, 28, 33, 51, 52]. How-ever, its application to real-world downstream tasks is still limited. Since the task is designed to deal with only a pre-deﬁned set of classes (e.g., “car”, “person”), semantic seg-mentation models are hard to address undeﬁned classes and speciﬁc entities of user interest (e.g., “a red Ferrari”, “a man wearing a blue hat”).
Referring image segmentation [12] has been studied to resolve this limitation by segmenting an image region cor-responding to a natural language expression given as query.
As this task is no longer restricted by predeﬁned classes, it enables a large variety of applications such as human-robot interaction and interactive photo editing. Referring image segmentation is however more challenging than semantic segmentation since it demands to comprehend individual
Composition of Transformers
A white van follows right behind an orange sedan
Upsampling and Linear Layers
Input image
Pixel-level prediction
Figure 1. Our convolution-free architecture for Referring image
Segmentation using TRansformer (ReSTR) takes a set of non-overlapped image patches and that of word embeddings, and cap-tures intra- and inter-modality interactions by transformers. Then,
ReSTR takes a class seed embedding to produce an adaptive clas-siﬁer which examines whether each image patch contains a part of target entity. Finally, a series of upsampling and linear layers computes a pixel-level prediction in a coarse-to-ﬁne manner. entities and their relations expressed in the language expres-sion (e.g., “a car behind the taxi next to the building”), and to fully exploit such structured and relational information in the segmentation process. For this reason, models for the task should be capable of capturing interactions between se-mantic entities in both modalities as well as joint reasoning over the two different modalities.
Existing methods for referring image segmentation [3, 11, 12, 13, 14, 16, 22, 25, 31, 37, 46] have adopted convo-lutional neural networks (CNNs) and recurrent neural net-works (RNNs) to extract visual and linguistic features, re-spectively.
In general, these features are integrated into a multimodal feature map through convolution layers ap-plied to a concatenation of the two features, so-called concatenation-convolution operation. On top of the multi-modal feature map, recent methods [11, 13, 14, 16, 46] fur-ther employ attention mechanisms [40, 43] so that the fea-ture map effectively captures interactions between semantic entities. The ﬁnal multimodal features are then fed as input to a segmentation module.
Although these methods have shown remarkable results on the challenging task, they share the following limitations.
First, they have trouble handling long-range interactions be-tween semantic entities within each modality. Referring im-age segmentation requires to capture such interactions since language expressions often involve complicated relations between entities to precisely indicate target region. In this aspect, both of CNNs and RNNs are limited due to the lo-cality of their basic building blocks. Second, existing mod-els have difﬁculty in modeling sophisticated interactions be-tween the two modalities. They aggregate visual and lin-guistic features through the concatenation-convolution op-eration, which is a ﬁxed and handcrafted way of feature fu-sion and thus could not be sufﬁciently ﬂexible and effective to handle a large variety of referring image segmentation scenarios.
To overcome the aforementioned limitations, we propose the ﬁrst convolution-free model for Referring image Seg-mentation using TRansformers, dubbed ReSTR. Its overall pipeline is illustrated brieﬂy in Fig. 1. First of all, ReSTR extracts visual and linguistic features through transformer encoders [40]. The two encoders, namely vision encoder and language encoder, take a set of non-overlapped image patches and that of word embeddings as input, respectively, and extract their features while considering their long-range interactions within each modality. By using transformers for both modalities, we take advantage of capturing global context from the beginning of feature extraction and unify-ing network topology for the two modalities [32].
Next, a self-attention encoder aggregates the visual and linguistic features into a patch-wise multimodal features.
This multimodal fusion encoder enables sophisticated and
ﬂexible interactions between features of the two modalities thanks to its self-attention layers. Moreover, the fusion en-coder takes a class seed embedding as another input. The class seed embedding is transformed adaptively by the fu-sion encoder to a classiﬁer for the target entity described in the language expression.
Finally, the outputs of the multimodal fusion encoder, i.e., the patch-wise multimodal features and the adaptive classiﬁer, are fed as input to the segmentation decoder. The decoder computes the ﬁnal segmentation map in a coarse-to-ﬁne manner. The adaptive classiﬁer is ﬁrst applied to each multimodal feature as a classiﬁer to examine whether each image patch contains a part of target entity. The coarse, patch-level prediction is then converted into a pixel-level segmentation map by a series of upsampling and linear lay-ers. Thanks to the powerful transformer encoders, this sim-ple and efﬁcient decoder is able to produce accurate seg-mentation results, achieving the state of the art on four pub-lic benchmarks for referring image segmentation.
In summary, the contribution of this work is three-fold:
• Our network is the ﬁrst convolution-free architecture for referring image segmentation.
It captures long-range interactions between vision and language modal-ities and uniﬁes the network topology for the two dif-ferent modalities by transformers.
• To encode the ﬁne comprehension of the two modal-ities, we carefully design the multimodal fusion en-coder with the class seed embedding which is trans-formed to an adaptive classiﬁer for referring image segmentation.
• ReSTR achieves the state of the art on four public benchmarks without bells and whistles. 2.