Abstract
Objects play a crucial role in our everyday activities.
Though multisensory object-centric learning has shown great potential lately, the modeling of objects in prior work is rather unrealistic. OBJECTFOLDER 1.0 is a recent dataset that introduces 100 virtualized objects with visual, acoustic, and tactile sensory data. However, the dataset is small in scale and the multisensory data is of limited quality, hampering generalization to real-world scenarios.
We present OBJECTFOLDER 2.0, a large-scale, multisen-sory dataset of common household objects in the form of implicit neural representations that signiﬁcantly enhances
OBJECTFOLDER 1.0 in three aspects. First, our dataset is 10 times larger in the amount of objects and orders of magnitude faster in rendering time. Second, we signiﬁ-cantly improve the multisensory rendering quality for all three modalities. Third, we show that models learned from virtual objects in our dataset successfully transfer to their real-world counterparts in three challenging tasks: ob-ject scale estimation, contact localization, and shape re-construction. OBJECTFOLDER 2.0 offers a new path and testbed for multisensory learning in computer vision and robotics. The dataset is available at https://github. com/rhgao/ObjectFolder. 1.

Introduction
Our everyday activities involve perception and manipu-lation of a wide variety of objects. For example, we begin the morning by ﬁrst turning off the alarm clock on the night-stand, slowly waking up. Then we may put some bread on a plate and enjoy our breakfast with a fork and knife to kick off the day. Each of these objects has very different physi-cal properties—3D shapes, appearance, and material types, leading to their distinctive sensory modes: the alarm clock looks round and glossy, the plate clinks when struck with the fork, the knife feels sharp when touched on the blade.
However, prior work on modeling real-world objects is
*indicates equal contribution.
Figure 1. OBJECTFOLDER 2.0 contains 1,000 implicitly repre-sented objects each containing the complete multisensory proﬁle of a real object. We virtualize each object by encoding its intrinsics (texture, material type, and 3D shape) with an Object File implicit neural representation. Then we can render its visual appearance, impact sound, and tactile readings based on any extrinsic parame-ters. We successfully transfer the models learned from our virtu-alized objects to three challenging tasks on their real-world coun-terparts. This opens a new path for multisensory learning in com-puter vision and robotics, where OBJECTFOLDER 2.0 serves as a rich and realistic object repository for training real-world models. rather limited and unrealistic. In computer vision, objects are often modeled in 2D with the focus of identifying and locating them in static images [15, 24, 39]. Prior works on shape modeling build 3D CAD models of objects [11, 72], but they tend to focus purely on geometry, and the visual textures of the objects are of low-quality. Moreover, most works lack the full spectrum of physical object properties and focus on a single modality, mostly vision.
Our goal is to build a large dataset of realistic and mul-tisensory 3D object models such that learning with these virtualized objects can generalize to their real-world coun-terparts. As shown in Fig. 1, we leverage existing high-quality scans of real-world objects and extract their physical properties including visual textures, material types, and 3D shapes. Then we simulate the visual, acoustic, and tactile data for each object based on their object intrinsics, and use an implicit neural representation network—Object File—to
encode the simulated multisensory data. If the sensory data is realistic enough, models learned with these virtualized objects can then be transferred to real-world tasks involving these objects.
To this end, we introduce OBJECTFOLDER 2.0, a large dataset of implicitly represented multisensory replicas of real-world objects. It contains 1,000 high-quality 3D ob-jects collected from online repositories [1, 2, 10, 14]. Com-pared with OBJECTFOLDER 1.01 that is slow in rendering and of limited quality in multisensory simulation, we im-prove the acoustic and tactile simulation pipelines to render more realistic multisensory data. Furthermore, we propose a new implicit neural representation network that renders vi-sual, acoustic, and tactile sensory data all in real-time with state-of-the-art rendering quality. We successfully transfer models learned on our virtualized objects to three challeng-ing real-world tasks, including object scale estimation, con-tact localization, and shape reconstruction.
OBJECTFOLDER 2.0 enables many applications, includ-ing 1) multisensory learning with vision, audio, and touch; 2) robot grasping of diverse real objects on various robotic platforms; and 3) applications that need on-the-ﬂy multi-sensory data such as on-policy reinforcement learning.
In summary, our main contributions are as follows: First, we introduce a new large multisensory dataset of 3D ob-jects in the form of implicit neural representations, which is 10 times larger in scale compared to existing work. We signiﬁcantly improve the multisensory rendering quality for vision, audio, and touch, while being orders of magnitude faster in rendering time. Second, we show that learning with our virtualized objects can successfully transfer to a series of real-world tasks, offering a new path and testbed for mul-tisensory learning for computer vision and robotics. 2.