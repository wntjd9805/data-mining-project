Abstract
The main challenge of Temporal Action Localization is to retrieve subtle human actions from various co-occurring ingredients, e.g., context and background, in an untrimmed video. While prior approaches have achieved substantial progress through devising advanced action detectors, they still suffer from these co-occurring ingredients which often dominate the actual action content in videos. In this paper, we explore two orthogonal but complementary aspects of a video snippet, i.e., the action features and the co-occurrence features. Especially, we develop a novel auxiliary task by decoupling these two types of features within a video snippet and recombining them to generate a new feature representa-tion with more salient action information for accurate action localization. We term our method RefactorNet, which first explicitly factorizes the action content and regularizes its co-occurrence features, and then synthesizes a new action-dominated video representation. Extensive experimental results and ablation studies on THUMOS14 and ActivityNet v1.3 demonstrate that our new representation, combined with a simple action detector, can significantly improve the action localization performance.
Figure 1. Visualization of the action component and the co-occurrence component decoupled by our method from a snippet representation. Co-occurrence components help reduce action am-biguity and uncertainty, but they often dominate the action compo-nents in a video, and thus adversely affect action detection. How to balance these two components in a snippet representation is an important yet under-explored problem. 1.

Introduction
Temporal Action Localization (TAL) aims to locate the start and end times of action instances from long untrimmed videos as well as to classify their categories. As a funda-mental task in video understanding, it has attracted great attention in recent years and facilitated various applications such as security surveillance [36,43,50] and human behavior analysis [9, 39, 53].
A common first step of current TAL approaches [12, 33, 46,52] is to extract features from each video snippet via a pre-trained two-stream network. They are then aggregated, e.g., via max pooling, to obtain the representation of a proposal
∗Corresponding author. or an anchor for action classification and temporal boundary regression. These snippet-level features characterize two aspects of a video snippet, and we term them the action component and the co-occurrence component, respectively.
The action component refers to features characterizing the action occurring in a snippet, including the motion pattern of one or more persons and their interaction with objects. The co-occurrence component refers to features not characteriz-ing any actions but often co-occurring with them in a frame or a snippet. This includes class-specific context, which only co-occurs frequently with certain actions, e.g., a track field, and class-agnostic background, whose occurrence is less relevant to the action categories, e.g., the sky. Figure 1 illustrates that an untrimmed video of High Jump contains a
subtle action component and a richer co-occurrence compo-nent surrounding the action.
It is critical to treat both the action and co-occurrence components carefully to achieve robust TAL. On the one hand, while the action component directly characterizes an action, it can be ambiguous and uncertain because the mo-tion and appearance of humans and their interactions with each other and with objects are complex. Thus, only retain-ing the action component from the snippet-level features is insufficient. On the other hand, while some co-occurrence components are useful to reduce the action ambiguity and uncertainty, e.g., a swimming pool distinguishing “diving” from “trampoline”, some others are more like nuisance noise, e.g., audience and random persons in the scene, and over-relying on the co-occurrence component also blurs the action boundaries. Therefore, it is necessary to find an appropriate balance between the action component and the co-occurrence component in a feature representation, especially as the latter often dominates the former in a video. This problem has been largely ignored by prior work.
This paper investigates feature refactoring for TAL. It means to decouple the snippet-level features as the action component and the co-occurrence component, and then re-combine them into a more appropriate representation to achieve effective TAL.
We propose a novel Feature Refactoring Network or
RefactorNet to achieve this goal. It consists of a feature de-coupling module and a feature recombining module. Since the annotations of co-occurrence components are unavail-able, we collect action samples and their coupling samples from the whole video for the decoupling process. Coupling samples refer to any video snippets containing co-occurring elements of the action but do not involve the actual action. In other words, an action sample and its coupling sample share similar co-occurrence components but differ in whether the action component is present. Taking the action samples and coupling samples as supervision, the feature decoupling module is trained to separate action and co-occurrence com-ponents. Then, the feature recombining module synthesizes these two components into a new snippet representation con-taining a more salient action component and a more suitable co-occurrence component for accurate action detection.
Both quantitative and qualitative results demonstrate that our RefactorNet can effectively decouple the action and co-occurrence components, and the recombined snippet rep-resentations improve both action classification and temporal boundary regression. Combined with a simple action detec-tor, our RefactorNet achieves state-of-the-art performance on two benchmarks, THUMOS14 and ActivityNet v1.3.
Our contributions can be summarized as follows:
• Co-occurrence components help reduce action ambigu-ity and uncertainty, but they often dominate the action components in a video, and thus adversely affect TAL.
How to balance these two components in a snippet rep-resentation is an important yet under-explored problem.
Our proposed RefactorNet is the first approach to ex-plicitly refactor, i.e., decouple and recombine, these two components to obtain a new snippet representation containing a more salient action component and a more suitable co-occurrence component for accurate action detection.
• Decoupling the two components is indeed very chal-lenging because they co-occur frequently, and more severely, their annotations are unavailable. To address these difficulties, we carefully design the learning ob-jective, and introduce action samples and their coupling samples, which can be obtained from standard TAL annotations, to supervise the decoupling process. They together help our feature decoupling module separate the two components effectively.
• Our RefactorNet outperforms all state-of-the-art meth-ods on two benchmark datasets. Extensive ablation study and visualizations are provided to show in-depth analysis of the decoupling process and validate how it improves TAL. 2.