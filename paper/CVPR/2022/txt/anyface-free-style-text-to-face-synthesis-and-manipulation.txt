Abstract
Existing text-to-image synthesis methods generally are only applicable to words in the training dataset. However, human faces are so variable to be described with limited words. So this paper proposes the ﬁrst free-style text-to-face method namely AnyFace enabling much wider open world applications such as metaverse, social media, cos-metics, forensics, etc. AnyFace has a novel two-stream framework for face image synthesis and manipulation given arbitrary descriptions of the human face. Speciﬁcally, one stream performs text-to-face generation and the other con-ducts face image reconstruction. Facial text and image fea-tures are extracted using the CLIP (Contrastive Language-Image Pre-training) encoders. And a collaborative Cross
Modal Distillation (CMD) module is designed to align the linguistic and visual features across these two streams. Fur-thermore, a Diverse Triplet Loss (DT loss) is developed to model ﬁne-grained features and improve facial diver-sity. Extensive experiments on Multi-modal CelebA-HQ and
*Equal contribution
†Corresponding author
CelebAText-HQ demonstrate signiﬁcant advantages of Any-Face over state-of-the-art methods. AnyFace can achieve high-quality, high-resolution, and high-diversity face syn-thesis and manipulation results without any constraints on the number and content of input captions. 1.

Introduction
A picture is worth a thousand words. Human face, as one of the most important visual signal for social interac-tions, deserves at least 10,000 words to describe the great diversity in shape, color, hair, expression, and intention, etc.
Therefore, it is highly desirable to generate variable face im-ages from arbitrary text descriptions for increasing demands of metaverse, social media, cosmetics, advertisement, etc.
This problem is ﬁrstly deﬁned as a free-style Text-to-Face task in this paper because current technology only supports one or two facial captions in the training dataset. This pa-per aims to explore the ﬁrst plausible solution for free-style
Text-to-Face synthesis with successful applications on face image synthesis and manipulation.
Existing face image synthesis [18, 23, 24, 29, 33, 34] and
manipulation [3, 14, 40] methods can synthesize impressive results with the powerful unconditional generative model (e.g., StyleGAN [10] and NVAE [27]), but vividly generat-ing faces according to speciﬁc requirements is still a chal-lenging problem. Thus, more and more researchers tend to explore text-to-image (T2I) synthesis [1,5,13,15,16,28,31, 39]. In earlier research, text embedding is directly concate-nated with encoded image features and then adopted to gen-erat semantically consistent images by learning the relation-ship between text and image. However, such concatenation operation can only synthesize images from a single caption, limiting the utility of these models in real-life scenarios. In practice, one caption can only describe limited information for target images while multiple captions provide ﬁne de-tails and accurate representation. As shown in Figure 1(a), compared with one caption, the face synthesized from 5-caption is more consistent with the source face.
To handle this issue, several methods [1, 24] attempt to implement multi-caption text-to-image synthesis. They usually use multi-stage architecture and introduce multi-caption embedding to each stage with special modules.
Such caption fusion modules extract the features of multi-ple captions at the cost of external computational resources.
And more importantly, since an image-text matching net-work in these methods is pre-trained on the training set, they are still failed in out-of-dataset text descriptions.
Existing text descriptions for image synthesis and ma-nipulation are only limited to a ﬁxed style (e.g. ﬁxed num-ber of sentences, formatted grammar, and existing words in datasets), severely limiting the user’s creativity and imag-ination.
In real-world applications, any style of text de-scription is more in line with the user’s operating needs.
Free-style text descriptions include three perspectives: 1) any number of captions (i.e., one or more captions are al-lowed to describe an image); 2) any content of captions (allowing users to explore the content or concepts of real-world scenarios); 3) any format of captions (allowing users to describe an image in their own sentences, rather than fol-lowing a ﬁxed template).
In this paper, free-style text descriptions are explored, leveraging the power of the recently introduced Contrastive
Language-Image Pre-training (CLIP) model and a free-style text-to-face method is proposed namely AnyFace for open-world applications. Due to CLIP’s ability to learn visual concepts from text-guided natural language, feature extrac-tion of out-of-dataset text descriptions is possible for open-world scenarios (see Figure 1(b)). AnyFace is a two-stream framework that utilizes the interaction between face im-age synthesis and faces image reconstruction. The face image synthesis stream generates target images consistent with given texts encoded by the Cross Modal Distillation
Module (CM D). The face image reconstruction stream reconstructs the face images paired with the given texts.
These two streams are trained independently, whose mu-tual information is interacted through a cross-modal transfer loss. To align the linguistic and visual features, previous at-tempts [29, 30] often leverage a pairwise loss to strictly nar-row the distance between text and image features. We argue that there is no need to restrict the text embedding to the corresponding image features, as the relationship between text and image is a one-to-many problem. The one-to-one constraint may even produce unique results. As a remedy, a new Diverse Triplet Loss is proposed to encourage diverse text embedding along with correct visual semantics.
Overall, the key contributions of this paper are summa-rized as follows:
• To the best of our knowledge, it is the ﬁrst deﬁnition, solution, and application of the free-style text-to-face problem, which is a breakthrough to remove the con-straints in face image synthesis.
• A novel two-stream framework, which consists of a face synthesis stream and a face reconstruction stream, is proposed for accurate and high ﬁdelity text-to-face synthesis and manipulation. Our method provides a good fusion solution of CLIP visual concepts learning and StyleGAN high-quality image generation.
• Extensive experiments are conducted to demonstrate the advantages of our method in synthesizing and ma-nipulating high ﬁdelity face images. Our work will deﬁnitely inspire more creative and wider applications of text-to-face technology. 2.