Abstract
In response to the explosively-increasing requirement of annotated data, Novel Class Discovery (NCD) has emerged as a promising alternative to automatically recognize un-known classes without any annotation. To this end, a model makes use of a base set to learn basic semantic discrim-inability that can be transferred to recognize novel classes.
Most existing works handle the base and novel sets using separate objectives within a two-stage training paradigm.
Despite showing competitive performance on novel classes, they fail to generalize to recognizing samples from both base and novel sets. In this paper, we focus on this gen-eralized setting of NCD (GNCD), and propose to divide and conquer it with two groups of Compositional Experts (ComEx). Each group of experts is designed to character-ize the whole dataset in a comprehensive yet complemen-tary fashion. With their union, we can solve GNCD in an efficient end-to-end manner. We further look into the draw-back in current NCD methods, and propose to strengthen
ComEx with global-to-local and local-to-local regulariza-tion. ComEx1 is evaluated on four popular benchmarks, showing clear superiority towards the goal of GNCD. 1.

Introduction
A key to the success of deep learning is huge amounts of curated data with elaborate annotations [12, 25, 31]. De-spite being expensive and cumbersome to collect, the anno-tated data plays an indispensable role since deep models are notoriously known to be data-hungry. In this regard, Semi-Supervised Learning (SSL) [4,5,39] sheds some light on the dilemma. Requiring only a small amount of annotations,
SSL addresses unannotated data using pseudo-labeling or consistency regularization, yet limited to known classes from existing annotations. To recognize unknown classes never seen in training, Zero-Shot Learning (ZSL) [9, 28, 37] resorts to extra annotations to learn transferable attributes among known and unknown classes, which in turn exacer-bates the need for annotations.
We consider a new task setting that naturally addresses the limitations of both SSL and ZSL, namely Novel Class
Discovery (NCD) [15, 16]. As shown in Fig. 1a, NCD as-sumes two sets of samples with disjoint classes — a base set containing labeled samples of base classes, and a novel set containing unlabeled samples of novel classes. The goal of NCD, besides correctly classifying base2 samples, is to recognize novel classes out of unlabeled samples using no extra knowledge. In practice, the base and novel sets are of-ten subsets of a same dataset, and thus share similar visual knowledge that can be transferred to discover novel classes.
In view of this, most existing NCD methods [15, 16, 24, 46–48] adopt a two-stage paradigm: a supervised training stage on the base set to learn basic semantic discriminabil-ity, and a fine-tuning stage on the novel set to discover novel classes by clustering unlabeled samples. To compensate the lack of supervised information, the two aforementioned
SSL techniques are often employed to strengthen clustering performance — using pseudo labels estimated in the novel set as clustering targets, and enforcing consistency between different transformations of a same input. Despite being competitive on discovering novel classes, these methods in-evitably suffer from performance degradation on the base set due to separate objectives for base and novel classes.
This can be problematic for deployment on a system han-dling data from both sets. A most recent work [13] identi-fies this problem and proposes a unified objective for NCD.
Although yielding promising results on either base or novel set, this method struggles to generalize to the union of the two sets, i.e., testing on both base and novel samples, show-ing it is still not yet ready for real-world deployment.
In this paper, we focus on the generalized setting of
NCD (GNCD), aiming at designing a unified model that works well on both base and novel sets, especially on their union. This is challenging due to the uneven properties of
*Corresponding authors. 1Code: https://github.com/muliyangm/ComEx. 2We interchangeably use base / labeled, and novel / unlabeled.
Figure 1. (a) A training batch for Novel Class Discovery (NCD). (b) A batch-class view of NCD. We can decompose a batch of training samples with batch-wise and class-wise perspectives. “Cls.” is short for “Classes”. (c) Batch-wise experts, dealing with respective sub-batches, yet aware of both base and novel classes. (d) Class-wise experts, handling a whole batch of samples, but with class-wise expertise. the two sets. As shown in Fig. 1b, we seek to leverage the compositional nature of base and novel sets by viewing
GNCD within a batch-class perspective. By respectively decomposing base and novel sets according to batches and classes, we propose to divide and conquer GNCD with two groups of Compositional Experts (ComEx). As illustrated in Figs. 1c and 1d, each group of experts characterizes the whole dataset in a comprehensive yet complementary way — with batch-wise experts (Fig. 1c) capturing separa-bility between base and novel classes, and class-wise ex-perts (Fig. 1d) modeling discriminability within each set of classes. With their union, we can achieve our goal of
GNCD in an end-to-end manner (see Sec. 3.1). Inside the experts, we regard the weights of each clustering head as a series of global cluster centers, such that our experts can be powered by sophisticated pseudo-labeling technique [1, 8], which actually induces a global-to-local alignment between global cluster centers and local training samples. We argue that this pure global-to-local formulation can be vulnerable to local changes (e.g., color, background), resulting in unfa-vorable clustering performance. To this end, we introduce local consistency into pseudo labels by aggregating neigh-borhood information according to soft similarities, i.e., a local-to-local aggregation (see Sec. 3.2), which offers clear performance boost on novel classes.
To sum up, our contributions are threefold:
• Our work is among the first attempts that focus on the generalized setting of NCD, and proposes to divide and conquer it using compositional experts that character-ize the data in a unique yet complementary manner;
• We interpret pseudo-labeling as a global-to-local alig-nment between cluster centers and training samples, and propose to strengthen pseudo labels with local-to-local aggregation among neighborhood samples;
• We show in extensive experiments the superiority of our proposed method against several state of the arts, and push the limits of NCD into the challenging gen-eralized setting that is of greater practical significance. 2.