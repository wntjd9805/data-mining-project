Abstract
Online Feature Extraction
Similarity Map Refinement
Temporal contexts among consecutive frames are far from being fully utilized in existing visual trackers. In this work, we present TCTrack1, a comprehensive framework to fully exploit temporal contexts for aerial tracking. The tem-poral contexts are incorporated at two levels: the extraction of features and the reﬁnement of similarity maps. Speciﬁ-cally, for feature extraction, an online temporally adaptive convolution is proposed to enhance the spatial features us-ing temporal information, which is achieved by dynamically calibrating the convolution weights according to the previ-ous frames. For similarity map reﬁnement, we propose an adaptive temporal transformer, which ﬁrst effectively en-codes temporal knowledge in a memory-efﬁcient way, be-fore the temporal knowledge is decoded for accurate ad-justment of the similarity map. TCTrack is effective and efﬁ-cient: evaluation on four aerial tracking benchmarks shows its impressive performance; real-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX Xavier. 1.

Introduction
Visual tracking is one of the most fundamental tasks in computer vision. Owing to the superior mobility of un-manned aerial vehicles (UAVs), tracking-based applications are experiencing rapid developments, e.g., motion object analysis [57], geographical survey [61], and visual local-ization [47]. Nevertheless, aerial tracking still faces two difﬁculties: 1) aerial conditions inevitably introduce special challenges including motion blur, camera motion, occlu-sion, etc; 2) the limited power of aerial platforms restricts the computational resource, impeding the deployment of time-consuming state-of-the-art methods [6]. Hence, an ideal tracker for aerial tracking must be robust and efﬁcient.
Most existing trackers adopt the standard tracking-by-∗Corresponding author 1https://github.com/vision4robotics/TCTrack
#1
#k-1 e n i l e m
T i
#1
#k
TAdaCNN
Temporal 
Contexts
AT-Trans
Temporal
Contexts
For Prediction
TAdaCNN
AT-Trans
For Prediction (a) (b)
Figure 1. Overview of our framework namely TCTrack.
It ex-ploits temporal information at two levels: (a) the extraction of features by the temporally adaptive convolutional neural networks (TAdaCNN) and (b) the reﬁnement of similarity maps by the adap-tive temporal transformer (AT-Trans). detection framework and perform detection for each frame independently. Among these trackers, discriminative cor-relation ﬁlter (DCF)-based methods are widely applied on aerial platforms because of their high efﬁciency and low resource requirements originated from the operations in the Fourier domain [16, 31, 38]. However, these trackers struggle when there are fast motions and severe appear-ance variations. Recently, the Siamese-based network has emerged as a strong framework for accurate and robust tracking [2, 4, 11, 41, 42]. Its efﬁciency is also optimized in [7,21,22] for the real-time deployment of Siamese-based trackers on aerial platforms.
However, the strong correlations inherently existing among consecutive frames, i.e., the temporal information, are neglected by these frameworks, which makes it difﬁ-cult for these approaches to perceive the motion informa-tion of the target objects. Therefore, those trackers are more likely to fail when the target undergoes severe appear-ance change caused by different complex conditions such as large motions and occlusions. This has sparked the re-cent research into how to make use of temporal information for visual tracking. For DCF-based approaches, the varia-tion in the response maps along the temporal dimension is penalized [33, 47], which guides the current response map
by previous ones. In Siamese-based networks, which is the focus of this work, temporal information is introduced in most works through dynamic templates, which integrates historical object appearance in the current template through concatenation [72], weighted sum [74], graph network [24], transformer [68], or memory networks [23, 73]. Despite their success in introducing temporal information into the visual tracking task, most of the explorations are restricted to only a single stage, i.e., the template feature, in the whole tracking pipeline.
In this work, we present a comprehensive framework for exploiting temporal contexts in Siamese-based networks, which we call TCTrack. As shown in Fig. 1, TCTrack in-troduces temporal context into the tracking pipeline at two levels, i.e., features and similarity maps. At the feature level, we propose an online temporally adaptive convolution (TAdaConv), where features are extracted with convolu-tion weights dynamically calibrated by the previous frames.
Based on this operation, we transform the standard convo-lutional networks to temporally adaptive ones (TAdaCNN).
Since the calibration in the online TAdaConv is based on the global descriptor of the features in the previous frames,
TAdaCNN only introduces a negligible frame rate drop but notably improves the tracking performance. At the sim-ilarity map level, an adaptive temporal transformer (AT-Trans) is proposed to reﬁne the similarity map according to the temporal information. Speciﬁcally, AT-Trans adopts an encoder-decoder structure, where (i) the encoder produces the temporal prior knowledge for the current time step, by integrating the previous prior with the current similarity map, and (ii) the decoder reﬁnes the similarity map based on the produced temporal prior knowledge in an adaptive way. Compared to [23,24,68], AT-Trans is memory efﬁcient and thus edge-platform friendly since we keep updating the temporal prior knowledge at each frame. Overall, our ap-proach provides a holistic temporal encoding framework to handle temporal contexts in Siamese-based aerial tracking.
Extensive evaluations of TCTrack show both the ef-fectiveness and the efﬁciency of the proposed framework.
Competitive accuracy and precision are observed on four standard aerial tracking benchmarks in comparison with 51 state-of-the-art trackers, where TCTrack also has a high frame rate of 125.6 FPS on PC. Real-world deployment on
NVIDIA Jetson AGX Xavier shows that TCTrack maintains impressive stability and robustness for aerial tracking, run-ning at a frame rate of over 27 FPS. plex aerial tracking conditions. Recently, Siamese-based trackers have stood out attributing to their SOTA accuracy and attractive efﬁciency [2, 3, 9, 26, 41, 42, 78]. For meeting the aerial tracking requirement, some works propose efﬁ-cient tracking methods [7, 21, 22].
Despite achieving SOTA performance, those trackers above disregard the temporal contexts in the tracking sce-narios, thereby blocking the performance improvement.
Differently, our tracker can effectively model the histori-cal temporal contexts during the tracking for increasing the discriminability and robustness.
Temporal-based tracking methods. Previously, many works are devoted to exploiting the temporal information in tracking scenarios for raising the tracking performance [10, 33, 43, 47]. Recently, many DL-based temporal tracking methods focus on dynamic templates based on transformer integration [68], template memory update [23,27,73], graph network [24], weighted sum [74], and explicit template up-date [72]. They try to update the template features in an explicit way or implicit way based on the pre-deﬁned pa-rameters. Then, based on the transformed template features, those trackers exploit the discrete temporal information in tracking sequences.
Despite superior tracking performance, they introduce temporal information via only a single level in the whole tracking pipeline, blocking further improvement of tracking performance. To fully exploit the temporal contexts, in this work, we propose a comprehensive framework for explor-ing the temporal contexts via two levels, i.e., features level and similarity maps level.
Temporal modelling in videos. Modelling the temporal dynamics is essential for a genuine understanding of videos.
Hence, it is widely explored in both supervised [20, 35, 48, 49, 63, 70] and self-supervised paradigm [28, 29, 34, 36, 39].
Self-supervised approaches learns temporal modelling by solving various pre-text tasks, such as dense future predic-tion [28, 29], jigsaw puzzle solving [36, 39], and pseudo motion classiﬁcation [34], etc. Supervised video recogni-tion explores various connections between different frames, such as 3D convolutions [62], temporal convolution [63], and temporal shift [48], etc. Closely related to our work is the temporally adaptive convolutions [35], which is applied for temporal modeling in videos. In this work, to adapt to the tracking task, we propose an online CNN which can extract spatial features according to temporal contexts for enriching the temporal information comprehensively. 2.