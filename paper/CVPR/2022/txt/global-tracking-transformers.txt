Abstract
We present a novel transformer-based architecture for global multi-object tracking. Our network takes a short sequence of frames as input and produces global trajecto-ries for all objects. The core component is a global track-ing transformer that operates on objects from all frames in the sequence. The transformer encodes object features from all frames, and uses trajectory queries to group them into trajectories. The trajectory queries are object features from a single frame and naturally produce unique trajec-tories. Our global tracking transformer does not require intermediate pairwise grouping or combinatorial associa-tion, and can be jointly trained with an object detector. It achieves competitive performance on the popular MOT17 benchmark, with 75.3 MOTA and 59.1 HOTA. More im-portantly, our framework seamlessly integrates into state-of-the-art large-vocabulary detectors to track any objects.
Experiments on the challenging TAO dataset show that our framework consistently improves upon baselines that are based on pairwise association, outperforming published work by a significant 7.7 tracking mAP. Code is available at https://github.com/xingyizhou/GTR. 1.

Introduction
Multi-object tracking aims to find and follow all objects in a video stream. It is a basic building block in applica-tion areas such as mobile robotics, where an autonomous system must traverse dynamic environments populated by other mobile agents. In recent years, tracking-by-detection has emerged as the dominant tracking paradigm, powered by advances in deep learning and object detection [20, 36].
Tracking-by-detection reduces tracking to two steps: de-tection and association. First, an object detector indepen-dently finds potential objects in each frame of the video stream. Second, an association step links detections through time. Local trackers [4, 5, 54, 55, 60, 66] primarily con-sider pairwise associations in a greedy way (Figure 1a).
They maintain a status of each trajectory based on loca-tion [5, 68] and/or identity features [55, 66], and associate current-frame detections with each trajectory based on its (a) Local trackers (b) Our global tracker
Figure 1. Local trackers (top) vs. our global tracker (bot-tom). Local trackers associate objects frame-by-frame, optionally with a external track status memory (not show in the figure). Our global tracker take a short video clip as input, and associates ob-jects across all frames using global object queries. last visible status. This pairwise association is efficient, but lacks an explicit model of trajectories as a whole, and sometimes struggles with heavy occlusion or strong appear-ance change. Global trackers [3, 6, 44, 63, 65] run offline graph-based combinatorial optimization over pairwise as-sociations. They can resolve inconsistently grouped detec-tions and are more robust, but can be slow and are usually detached from the detector.
In this work, we show how to represent global track-ing (Figure 1b) as a few layers in a deep network (Fig-ure 2). Our network directly outputs trajectories and thus sidesteps both pairwise association and graph-based opti-mization. We show that detectors [20, 36, 70] can be aug-mented by transformer layers to turn into joint detectors and trackers. Our Global TRacking transformer (GTR) encodes detections from multiple consecutive frames, and uses tra-jectory queries to group them into trajectories. The queries are detection features from a single frame (e.g., the current frame in an online tracker) after non-maximum suppression, and are transformed by the GTR into trajectories. Each tra-jectory query produces a single global trajectory by assign-ing to it a detection from each frame using a softmax dis-tribution. The outputs of our model are thus detections and their associations through time. During training, we explic-itly supervise the output of our global tracking transformer
Figure 2. Overview of our joint detection and tracking framework. An object detector first independently detects objects in all frames.
Object features are concatenated and fed into the encoder of our global Tracking transformer (GTR). The GTR additionally takes trajectory queries as decoder input, and produces association scores between each query and object. The association matrix links objects for each query. During testing, the trajectory queries are object features in the last frame. The structure of the transformer is shown in Figure 3. using ground-truth trajectories and their image-level bound-ing boxes. During inference, we run GTR in a sliding win-dow manner with a moderate temporal size of 32 frames, and link trajectories between windows online. The model is end-to-end differentiable within the temporal window.
Our framework is motivated by the recent success of transformer models [49] in computer vision in general [14, 25, 47, 67] and in object detection in particular [8, 53]. The cross-attention structure between queries and encoder fea-tures mines similarities between objects and naturally fits the association objective in multi-object tracking. We per-form cross-attention between trajectory queries and object features within a temporal windows, and explicitly super-vise it to produce a query-to-detections assignment. Each assignment directly corresponds to a global trajectory. Un-like transformer-based detectors [8, 30, 40, 53] that learn queries as fixed parameters, our queries come from exist-ing detection features and adapt with the image content.
Furthermore, our transformer operates on detected objects rather than raw pixels [8]. This enables us to take full ad-vantage of well-developed object detectors [20, 69].
Our framework is end-to-end trainable, and easily inte-grates with state-of-the-art object detectors. On the chal-lenging large-scale TAO dataset, our model reaches 20.1 tracking mAP on the test set, significantly outperforming published work, which achieved 12.4 tracking mAP [32].
On the MOT17 [31] benchmark, our entry achieves com-petitive 75.3 MOTA and 59.1 HOTA, outperforming most concurrent transformer-based trackers [30, 61, 64], and on-par with state-of-the-art association-based trackers. 2.