Abstract
Deep image matting methods have achieved increasingly better results on benchmarks (e.g., Composition-1k/alpha-matting.com). However, the robustness, including robust-ness to trimaps and generalization to images from different domains, is still under-explored. Although some works pro-pose to either refine the trimaps or adapt the algorithms to real-world images via extra data augmentation, none of them has taken both into consideration, not to mention the significant performance deterioration on benchmarks while using those data augmentation. To fill this gap, we propose an image matting method which achieves higher robustness (RMat) via multilevel context assembling and strong data augmentation targeting matting. Specifically, we first build a strong matting framework by modeling ample global in-formation with transformer blocks in the encoder, and fo-cusing on details in combination with convolution layers as well as a low-level feature assembling attention block in the decoder. Then, based on this strong baseline, we analyze current data augmentation and explore simple but effective strong data augmentation to boost the baseline model and contribute a more generalizable matting method.
Compared with previous methods, the proposed method not only achieves state-of-the-art results on the Composition-1k benchmark (11% improvement on SAD and 27% improve-ment on Grad) with smaller model size, but also shows more robust generalization results on other benchmarks, on real-world images, and also on varying coarse-to-fine trimaps with our extensive experiments.1 1.

Introduction
Image matting, as a fundamental computer vision task, aims to obtain the high-quality alpha matte of the fore-ground object given an input image. Mathematically, image matting is formulated as:
Ii = αiFi + (1 − αi)Bi , (1) where Ii, αi, Fi and Bi are observed colour value, al-pha value, foreground value and background value of pixel i, respectively.
It is an ill-posed problem because there are 7 unknowns given 3 equations. With recent success of deep learning, deep matting methods [4, 11, 14, 18, 23, 29, 35] achieve promising results on benchmarks such as
Composition-1k [35] and alphamatting.com [26]. While 1This work was in part done when YD was an intern at Adobe and CS was with The University of Adelaide. CS is the corresponding author.
Project page: https://dongdong93.github.io/RMat/
increasingly higher accuracy have been promised on bench-marks, due to the limited training/test data, robustness of these methods is still under explored.
First, robustness to the trimap, the commonly used prior input, is important for a matting algorithm. In real appli-cations, trimaps are labeled by users, with unpredictable precision of unknown regions. However, as shown in
Fig. 2, existing matting methods [11,29] are sensitive to the shape/size of the given trimap so that it requires users’ more time to accurately brush the trimap. A main reason why existing methods are sensitive to the precision of trimap is they focus more on detailed cues, where robustness to trimap with varing precision, which relies more on context information, is less cared about. One possible solution is to optimize the trimap to be a more detailed one. This was proposed in [1], where an extra branch was used to generate a more precise trimap. Though multi-task learning is lever-aged in this method to adapt the trimap, its context modeling is still limited, which restricts its robustness in applications.
Therefore, we wonder whether it is possible to enhance the context modeling ability (robustness) of a matting algorithm with a simpler and more effective approach.
Meanwhile, it has been known that deep matting models trained on synthetic data undertake the risk of poor gener-ation to real-world domains [14, 29, 39] (Fig. 1). However, due to the difficulty of obtaining ground-truth annotations for real-world images, only synthetic datasets are available to train the matting algorithms, so some works attempted to narrow the domain gap. For example, [14,39] leverage extra data augmentation to adapt the models to real-world images, while significant performance degradation on the synthetic benchmark happens at the same time. Although better pre-diction on real-world images is appreciated, it is desirable that the model can be generalized to broader scenes with-out sacrificing too much performance on images from one domain such as the benchmark data, because it is hard to confirm which domain a test image comes from, and not to mention that the real-world test images in [14, 39] can only cover a tiny part of real scenes. Therefore, a model showing better domain generalization ability is in demand.
Motivated by these demands, we present a more robust matting method (RMat), which achieves higher robustness to diverse trimap precision and better generalization to var-ious domains. In detail, two steps are designed. The first step is to build a strong baseline model with multilevel con-text assembling.
It is implemented by combining trans-former blocks with convolution layers, where global con-text is learned via self-attention modules and local context is emphasized by convolution layers. Considering the unique-ness of matting that needs local context information and original test resolutions to capture details, we explore de-signs and implementations aiming at this task to build an efficient model. Further, founded on this strong baseline model, we investigate strong data augmentation for matting.
We analyze the problems behind current augmentation and propose strong augmentation strategies specifically for mat-ting. Finally, to verify robustness of the model, a series of experiments and visualizations are carried out in compari-son with state-of-the-art methods.
In summary, our main contributions are: 1) A strong matting framework with multilevel context assembling; 2)
Strong augmentation strategies targeting matting; 3) De-signs of experiments and visualizations to verify generaliza-tion capability of matting models; 4) State-of-the-art results on benchmarks (w/ and w/o fitting the training sets), higher robustness to varying trimap precision, and better general-ization to real-world images. 2.