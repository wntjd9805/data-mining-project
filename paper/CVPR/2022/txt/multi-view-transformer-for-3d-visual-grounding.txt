Abstract
The 3D visual grounding task aims to ground a natural language description to the targeted object in a 3D scene, which is usually represented in 3D point clouds. Previous works studied visual grounding under specific views. The vision-language correspondence learned by this way can easily fail once the view changes.
In this paper, we propose a Multi-View Transformer (MVT) for 3D visual grounding. We project the 3D scene to a multi-view space, in which the position information of the 3D scene under different views are modeled simultaneously and aggregated together. The multi-view space enables the network to learn a more robust multi-modal representation for 3D visual grounding and eliminates the dependence on specific views.
Extensive experiments show that our approach significantly outperforms all state-of-the-art methods. Specifically, on
Nr3D and Sr3D datasets, our method outperforms the best competitor by 11.2% and 7.1% and even surpasses recent work with extra 2D assistance by 5.9% and 6.6%. Our code is available at https://github.com/sega-hsj/MVT-3DVG. 1.

Introduction
Visual Grounding (VG) aims to ground a natural lan-The field has guage description to the target object. made tremendous progress in 2D images [18, 22, 30, 35].
With the rapid development of 3D sensors and 3D vision technology, 3D visual grounding has recently attracted more attention, with wide applications including vision-language navigation [31, 39], intelligent agents [26, 32], and autonomous vehicles [11, 21]. Compared to 2D visual grounding, the 3D task has more complex input data (e.g., sparse point clouds) and more variant spatial relationships, switching the output from grounding to 2D regions to 3D objects.
Recent works [1, 3, 14, 25, 33, 36] mainly follow a two-stage scheme, i.e., first generating all candidate objects in 3D scene and then selecting the most matched one. These two-stage approaches in 3D visual grounding are tailored from 2D visual grounding methods [20, 34], which have
Figure 1. An example from Nr3D [1]: when the view is changed, the position information in 3D scene can be quite different. i.e., 3D
The mentioned “pillow” exhibits changed positions, coordinates, when the view is changed from the front to the back. not considered the unique property of 3D data. Therefore, in this paper, we bring the community’s attention back to studying the intrinsic property of 3D data to break through the 3D visual grounding research.
Position information is essential to locate an object.
The visual grounding aligns visual position information in the scene (e.g., coordinates of objects) with the location described in natural language (e.g., “left” or “right”).
Unlike 2D images with static views, 3D scenes can freely rotate to different views. These dynamic view changes make 3D visual grounding more challenging. The view changes will directly affect the position encoding, e.g., thus bringing more difficulty in coordinates of objects, representing 3D objects. For example, as shown in Fig. 1, the rightmost pillow in the front view becomes the leftmost one in the back view. This discrepancy commonly exists in real applications: when a robot or an intelligent agent is navigating in the 3D scene, its view is usually different from the speaker’s or commander’s view. Such view discrepancy can bring more ambiguity in identifying the location of targets. To alleviate this view issue, when Referit3D [1] were collecting data, they split the data into view-dependent and view-independent categories according to whether the description was constrained to the speaker’s view. They
have provided additional view guidance, e.g., “Facing the bed” in the example of Fig. 1, to those view-dependent data. In practice, the view constraint is usually implicit, and we cannot always rely on speakers to provide augmented descriptions to solve the problem. Especially, the robot or the intelligent agent should be able to ground to the target object no matter how its view is different from the speaker’s. Thus, developing a robust approach to view changes becomes imperative in 3D visual grounding.
Existing methods [1, 10, 14, 25, 33, 36, 38] have paid little attention to the issue as mentioned above and mainly learned the grounding task under specific views. The vision-language correspondence learned in this way can easily fail once the view changes. An intuitive solution to alleviating the problem is to add augmented rotation data under different views during training. However, as we tried and tested (see Tab. 4 in our experimental section), such a straightforward method does not bring the improvement.
This result can be understood, i.e., although novel views of the 3D scene can be augmented in the training stage, visual features under specific views are not accurately aligned with the position information embedded in the language description. An alternative is to ask the model to predict the view attached to the language query, say, the speaker’s view, and then align the query with the visual features under the predicted view. However, this will incur a lot more costs for view prediction, and if the model predicts the wrong view, the alignment between the two modalities will inevitably be damaged. Experiments provided by LanguageRefer [25] confirmed this point, i.e., even if they manually correct the view of all training data to the speaker’s view, the performance of the model on unseen test data with view discrepancy still cannot be improved. Therefore, in this paper, we turn to the exploration of learning better multi-modal representations for 3D visual grounding, which are robust to view changes, instead of augmenting view data or predicting views.
In this paper, we propose a Multi-View Transformer (MVT) for 3D visual grounding to learn such a view-robust representation. Given the 3D scene under a specific view, our MVT projects it to a multi-view space by rotating the original view with equal angles. Our approach simul-taneously models the position information within the 3D scene under different views and aggregates all information to learn a view-robust representation by eliminating the dependence on the starting view. Furthermore, to reduce the computational cost of modeling multi-view space, we decouple the 3D object representation computation into computing point clouds features and object coordinates separately to share the point clouds features across views.
Besides, with the rich and fine-grained category labels provided by Nr3D [1] and Sr3D [1], we make full use of these language priors and propose a language-guided object classification task to enhance the object encoder further. Extensive experiments have been conducted on three mainstream benchmark datasets Nr3D [1], Sr3D [1], and ScanRefer [3]. Our MVT outperforms all state-of-the-art methods by a large margin. Specifically, on Nr3D and Sr3D, MVT not only outperforms the best competitor by 11.2% and 7.1% respectively but also surpasses the method [33] with extra 2D assistance by 5.9% and 6.6%. 2.