Abstract
We address weakly supervised point cloud segmentation by proposing a new model, MIL-derived transformer, to mine additional supervisory signals. First, the transformer model is derived based on multiple instance learning (MIL) to explore pair-wise cloud-level supervision, where two clouds of the same category yield a positive bag while two of different classes produce a negative bag. It leverages not only individual cloud annotations but also pair-wise cloud semantics for model optimization. Second, Adaptive global weighted pooling (AdaGWP) is integrated into our trans-former model to replace max pooling and average pooling.
It introduces learnable weights to re-scale logits in the class activation maps. It is more robust to noise while discovering more complete foreground points under weak supervision.
Third, we perform point subsampling and enforce feature equivariance between the original and subsampled point clouds for regularization. The proposed method is end-to-end trainable and is general because it can work with different backbones with diverse types of weak supervision signals, including sparsely annotated points and cloud-level labels. The experiments show that it achieves state-of-the-art performance on the S3DIS and ScanNet benchmarks.
The source code will be available at https://github. com/jimmy15923/wspss_mil_transformer. 1.

Introduction
Point clouds capture geometric characteristics and sur-face context, and hence serve as an essential representation for many 3D vision applications such as scene understand-ing [6, 22, 28], autonomous vehicles [4, 5], and robotics [9].
Point cloud segmentation aims to identify points belong-ing to semantic categories of interest. It offers point-level recognition, thereby being an intrinsic component of point cloud analysis. However, learning a segmentation model re-lies on training data with point-level annotations. The high annotation cost poses an obstacle to this task. To address this issue, existing weakly supervised methods derive the segmentation model with different weak supervisory sig-nals, such as partially labeled points [26, 42, 46, 47], sub-cloud level annotations [38] or scene level annotations [31].
To compensate for the lack of complete annotations, weakly supervised point cloud segmentation methods [26, 31, 38, 42, 46, 47] make the most of weakly labeled data by different techniques such as graph propagation, permutation consistency, and object proposals. Despite effectiveness, these methods use only intra-cloud information: The super-visory signals are grabbed from point clouds independently.
Inspired by image co-segmentation [13,45] and cross-image pattern mining [32], we aim to explore inter-cloud seman-tics to supervise segmentation model training. To this end, we generalize the transformer model [34] to work on paired point clouds and formulate the problem as a multiple in-stance learning (MIL) [27] task. It follows that our method can use both intra-cloud and inter-cloud information to bet-ter accomplish weakly supervised segmentation.
Speciﬁcally, we develop an MIL-derived transformer where MIL addresses the uncertainty of weak labels. As shown in Figure 1, we apply the transformer to two point clouds of the same category. One cloud is treated as an anchor with each of its points being a query in the trans-former. The other cloud serves as a reference where each of its points forms a key-value pair. The transformer encoder and decoder are applied to the reference and the anchor re-spectively. Through the cross-attention [34] mechanism of the decoder, each query (from the anchor) is expressed as a weighted sum of the values (from the reference). The resul-tant feature vectors of all points (i.e., queries) of the anchor yield a positive bag in MIL for the common category of the two clouds. MIL via this positive bag encourages the model to attend to the foreground points of the anchor.
In contrast, we consider another reference point cloud whose category is different from the anchor. This time, the feature vectors of all queries of the anchor generate a negative bag because the feature vector of each query is a weighted sum of values of this reference where the target category is not present. The model via this negative bag is enforced to suppress irrelevant points. With the proposed
MIL-transformer, each pair of point clouds, forming either a positive or a negative bag, produces extra signal to su-pervise model training. In addition, long-range dependency can be taken into account via the transformer.
Max or average pooling is widely used to aggregate in-1
reference backbone a positive bag anchor
… transformer encoder
… point embeddings
… key-value
… query
… transformer decoder
… point embeddings backbone
Figure 1: Given two point clouds (anchor and reference) of the same category (chair), a backbone network is applied to compute point embeddings. The transformer encoder and decoder are applied to the two clouds respectively. Self-attention captures long-range dependency. In the cross-attention module of the decoder, the points (tokens) from the anchor serve as the queries, while those from the reference act as key-value pairs. The transformer maps each query to a weighted sum of values. The outputs of the queries produce a positive bag for multiple instance learning. Once the reference is changed to another cloud without covering any chairs, the outputs of the queries then yield a negative bag for the chair category. formation in weakly supervised segmentation, and hence is crucial to the performance. Max pooling considers only peak points, typically leading to incomplete object seg-ments. In addition, it is sensitive to noise. Average pool-ing for weakly supervised segmentation often suffers from performance degradation caused by irrelevant points such as those belonging to other classes or background. More-over, stuff categories, e.g. ﬂoor or wall, in point cloud seg-mentation bring the class imbalance problem, which makes the aforementioned issues worse. We address these issues by proposing adaptive global weighted pooling (AdaGWP), which introduces learnable weights, one for each class.
These class-speciﬁc weights are derived so that the model
It turns out that can attend to points of relevant classes.
AdaGWP suppresses irrelevant points while recovering ob-ject points more completely.
We also consider cross-scale consistency of point clouds to regularize weakly supervised feature extraction. Random point sampling is applied to subsample a point cloud. Sub-sampling does not change point labels even if the labels are unknown in weakly supervised learning. Thus, a consis-tency loss is imposed to enforce the similarity between the features of the original and subsampled point clouds, acting as extra supervision signals for network training.
The main contribution of this work is the MIL-derived transformer, which explores additional inter-cloud seman-tics for weakly supervised segmentation.
In addition, a class-speciﬁc, learnable pooling technique AdaGWP and multi-scale feature equivariance are utilized to enhance model training. Our method is ﬂexible to work with differ-ent point cloud networks, and with diverse types of weak su-pervisory signals, including sparsely annotated points [42], subcloud-level [38] and scene-level [31] annotations. It per-forms favorably against existing methods on the S3DIS [1] and ScanNet [7] benchmarks. 2.