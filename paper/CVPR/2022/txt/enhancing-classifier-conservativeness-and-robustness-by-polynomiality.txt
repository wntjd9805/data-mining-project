Abstract
We illustrate the detrimental effect, such as overconﬁdent decisions, that exponential behavior can have in methods like classical LDA and logistic regression. We then show how polynomiality can remedy the situation. This, among others, leads purposefully to random-level performance in the tails, away from the bulk of the training data. A directly related, simple, yet important technical novelty we subse-quently present is softRmax: a reasoned alternative to the standard softmax function employed in contemporary (deep) neural networks. It is derived through linking the standard softmax to Gaussian class-conditional models, as employed in LDA, and replacing those by a polynomial alternative. We show that two aspects of softRmax, conservativeness and inherent gradient regularization, lead to robustness against adversarial attacks without gradient obfuscation. 1.

Introduction
Models that show some form of exponential behavior are ubiquitous in machine learning: from the Gaussian class conditional distribution in linear discriminant anal-ysis (LDA) [11, 15] to sigmoid activation for logistic regres-sion [19, 27], and the softmax activation function in deep neural networks [23, 37]. Models with such use of exponen-tiality can, however, have unwanted behavior. We describe and illustrate such behavior, examine its reason, and propose a partial remedy by switching to models that behave poly-nomially. Like [6, 17], we consider the distribution tail and show that samples in the tails receive overconﬁdent posterior predictions [21]. This renders the model sensitive to outliers and causes overﬁtting, especially in the case of distribution shift. Moreover, we link overconﬁdent predictions to the lack of robustness against gradient based adversarial attacks.
A model should not be certain about a sample that de-viates too much from the training data. Overconﬁdent pre-dictions on samples in the distribution tails should often be avoided, e.g. an atypical patient may otherwise be classi-ﬁed to be healthy or diseased with strong conﬁdence. We want what we call conservativeness, which expresses the fact that we are uncertain. Speciﬁcally, we deﬁne it to be random guess-level prediction for samples in the tail of the distribution and show that this can be achieved by moving from exponential to polynomial behavior both in LDA and logistic regression. In addition, for logistic regression and deep learning, studies into the standard softmax activation have shown that it is not necessarily the best choice in many settings [8, 18, 40]. We propose a polynomial form of soft-max posterior estimation that we coin softRmax. For this, we exploit the connection between the standard softmax func-tion and LDA [4] and adopt a modiﬁed Cauchy distribution as the substitute for the (super)exponential Gaussian term.
Besides overconﬁdent predictions, the use of exponen-tiality is also linked to vulnerability to adversarial attacks.
Such attacks aim to cause malicious prediction changes by adding an unnoticeable perturbation to the original input.
Robustness is the ability to maintain performance under ad-versarial attacks [5]. We demonstrate that a higher robustness of neural networks can be obtained by simply substituting the standard softmax with our softRmax. We show that the robustness can be linked back to the conservativeness of soft-Rmax and inherent gradient regularization. The ﬁrst factor, conservativeness, mainly brings robustness against gradient based attacks. The second leads to an enlarged margin be-tween samples and the decision boundary, thereby boosting robustness against attacks as well. The effectiveness of vari-ous strategies countering adversarial attacks can be attributed to gradient obfuscation [2, 10]. We show that our inherent gradient regularization does not rely on such obfuscation.
We sketch the beneﬁts of conservativeness under covari-ate shift [14,38,39] and show it when a model is under attack.
We verify the robustness of our polynomial substitutes em-pirically on toy and public datasets. We further propose a semi-black-box attack, which we call an average-sample at-tack, to conﬁrm that the robustness of our softRmax indeed comes from the above two factors. We also introduce a scale-invariant metric, the magnitude-margin ratio, for comparing the robustness of different models under the same level of attack.
2.