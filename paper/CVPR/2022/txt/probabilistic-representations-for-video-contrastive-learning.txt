Abstract
This paper presents Probabilistic Video Contrastive
Learning, a self-supervised representation learning method that bridges contrastive learning with probabilistic repre-sentation. We hypothesize that the clips composing the video have different distributions in short-term duration, but can represent the complicated and sophisticated video distribution through combination in a common embedding space. Thus, the proposed method represents video clips as normal distributions and combines them into a Mixture of
Gaussians to model the whole video distribution. By sam-pling embeddings from the whole video distribution, we can circumvent the careful sampling strategy or transformations to generate augmented views of the clips, unlike previous deterministic methods that have mainly focused on such sample generation strategies for contrastive learning. We further propose a stochastic contrastive loss to learn proper video distributions and handle the inherent uncertainty from the nature of the raw video. Experimental results verify that our probabilistic embedding stands as a state-of-the-art video representation learning for action recognition and video retrieval on the most popular benchmarks, including
UCF101 and HMDB51. 1.

Introduction
Video is the vitality of the Internet, which means that understanding video content is essential for the most mod-ern artificial intelligence (AI) agents. Alongside this, learn-ing enriched spatiotemporal representations from unlabeled videos (i.e., self-supervised or unsupervised video repre-sentation learning) [59, 61, 62] has become a crucial re-search topic for the computer vision community. The in-terest in this topic is to learn deep features representing general visual contents, which has proven essential to im-proving performance on downstream tasks such as action
*Corresponding author.
This research was supported by R&D program for Advanced
Integrated-intelligence for Identification (AIID) through the National Re-search Foundation of KOREA(NRF) funded by Ministry of Science and
ICT (NRF-2018M3E3A1057289) and the Yonsei University Research
Fund of 2021 (2021-22-0001). (a) Deterministic embedding (b) Probabilistic embedding (Ours)
Figure 1. Contrary to (a) the deterministic point embedding meth-ods, which estimate the subset of the video distribution, (b) the proposed method estimates the whole video distribution through the mixture of probabilistic embeddings. recognition [10, 28, 82], action detection [86, 88], video retrieval [22, 80], and even multi-modal event recogni-tion [3, 55]. However, self-supervised video representation learning has still remained challenging due to the inherent difficulty caused by the nature of the videos in comparison to static images.
Recent breakthroughs in self-supervised video repre-sentation learning have been developed with two different branches: (1) leveraging pretext tasks related to the coher-ence of videos and (2) using contrastive learning [26] for in-stance discrimination. Specifically, video coherence is em-pirically modeled through sub-properties of video contents associated with temporal ordering [21, 34, 45, 54, 83], opti-cal flow [24], spatiotemporal statistics [39,51,75], and play-back rate [76, 85]. Even though they have shown that spa-tiotemporal representations can be learned from unlabeled
videos, the learned representations inevitably contain task-specific information.
In contrast, instance discrimination methods [20, 47, 49, 59, 62, 68] have attempted to learn video representations by incorporating contrastive learning [26], which aims to dis-criminate different instances without using sub-properties of data [18, 81]. Concretely, to learn spatiotemporal repre-sentations from videos, existing works treat each video as an “instance” and embed video clips to deterministic points on the embedding space, as shown in Fig. 1(a). Based on contrastive objectives [15,63,77,79], positive point pairs are pulled together and negative point pairs are pushed away.
The positive pairs are composed of clips from the same video [62] or different views (augmented versions) of the same clip [20, 59], and the negative pairs are composed of clips from different videos. To make such training pairs, several works have introduced carefully designed spatial and temporal transformations in the form of data augmen-tation, including a temporal mask [59] and temporally con-sistent spatial augmentation [62].
However, deterministic representations for video con-trastive learning have critical limitations in three respects:
First, representing the complicated and sophisticated video distribution as a set of deterministic points is insufficient to learn discriminative video representations. Unlike static images, videos are a collection of noisy temporal dynamics and contain a lot of redundant information, that makes the uncertainty of data high [52]. Therefore, an alternative to deterministic representations is required to describe overall video distribution. Second, improper sampling and transfor-mation techniques to generate different views can cause per-formance fluctuation according to downstream tasks [89].
Moreover, improper temporal transformations (e.g. shuf-fling) that can harm the video contents weaken the effec-tiveness of contrastive learning [5]. Third, they often ne-glect common components that are likely to contain valid correspondences between semantically adjacent instances (e.g. same category, but different videos), leading to lim-ited discrimination performance of learned representations, as demonstrated in [27].
To overcome these limitations while maximizing the ad-vantages of contrastive learning, we propose probabilis-tic representations for video contrastive learning, named
ProViCo, in which video clips are represented as random variables in a stochastic embedding space. As shown in
Fig. 1(b), clips sampled from a video are represented as dis-tinct normal distributions and the distribution of the whole video is approximated by a Mixture of Gaussians (MoG) of clip distributions. We construct the positive and nega-tive pairs based on the probabilistic distance between em-beddings sampled from each video distribution. Moreover, we propose an uncertainty-based stochastic contrastive loss that incorporates uncertainty (i.e., the inherent noise of videos) into the soft contrastive loss [57]. By leveraging uncertainty, we can reduce the effect of noisy samples or improper training pairs on self-supervised representation learning, and can make useful applications such as estima-tion of difficulty or chance of failure on test data.
To sum up, our contributions are as follows: (1) We pro-pose a novel ProViCo to effectively represent the probabilis-tic video embedding space. To the best of our knowledge, this is the first attempt to leverage probabilistic embeddings for self-supervised video representation learning. (2) We in-troduce the probabilistic distance-based positive mining to exploit semantic relations between videos and present the stochastic contrastive loss to weaken the adverse impact of unreliable instances. (3) We demonstrate the effectiveness of the proposed probabilistic approach through the uncer-tainty estimation and extensive experiments on downstream tasks, including action recognition and video retrieval. 2.