Abstract
Vision-Language Navigation (VLN) is a challenging task that requires an embodied agent to perform action-level modality alignment, i.e., make instruction-asked actions se-quentially in complex visual environments. Most existing
VLN agents learn the instruction-path data directly and cannot sufficiently explore action-level alignment knowl-edge inside the multi-modal inputs. In this paper, we pro-pose modAlity-aligneD Action PrompTs (ADAPT), which provides the VLN agent with action prompts to enable the explicit learning of action-level modality alignment to pur-sue successful navigation. Specifically, an action prompt is defined as a modality-aligned pair of an image sub-prompt and a text sub-prompt, where the former is a single-view observation and the latter is a phrase like “walk past the chair”. When starting navigation, the instruction-related action prompt set is retrieved from a pre-built action prompt base and passed through a prompt encoder to obtain the prompt feature. Then the prompt feature is concatenated with the original instruction feature and fed to a multi-layer transformer for action prediction. To collect high-quality action prompts into the prompt base, we use the Contrastive
Language-Image Pretraining (CLIP) model which has pow-erful cross-modality alignment ability. A modality align-ment loss and a sequential consistency loss are further in-troduced to enhance the alignment of the action prompt and enforce the agent to focus on the related prompt sequen-tially. Experimental results on both R2R and RxR show the superiority of ADAPT over state-of-the-art methods. 1.

Introduction
In the Vision-Language Navigation (VLN) task [1,4], an embodied agent is required to navigate through complex scenes following a given language instruction. To accom-plish successful navigation, the agent needs to implement
*Part of this work was done during an internship in Huawei Noah’s Ark
Lab.
†Corresponding Author
Figure 1. The action decision comparison between a baseline [14] and our ADAPT. With the help of action prompts related to “walk to the staircase” in the instruction, our ADAPT successfully makes correct action from the current observation. both object-level and action-level modality alignment ac-curately given the instruction and visual observations. For example, given an instruction of “exit the bedroom”, the agent should not only locate the “bedroom” in its observa-tion but also find the door of the bedroom to make the action of “exit”. With great potential in the applications such as in-home robots and personal assistants, VLN has received wide spread attention in the robotic visual applications.
Early VLN approaches explore diverse data augmenta-tion strategies [8,9,27,38], efficient learning paradigms [15, 24, 40, 46, 47] and useful model architecture [7, 13, 29, 40] to improve the agent performance. Motivated by the signif-icant progress made by large-scale cross-modal pre-trained models in vision-language tasks [6,21,23,25,37], more and more works attempt to introduce the pretraining paradigms and models into the VLN task. PREVALENT [11] pretrains the model on a large amount of image-text-action triplets in a self-supervised learning manner. VLN⟳BERT [14] in-troduces a recurrent function into the pretrained models to make the VLN agent time-aware. Although the object-level alignment ability may be significantly enhanced through the
pretraining process, these VLN agents still learn the action-level modality alignment in an implicit way, which largely limits the robust action decision under different scenes.
Recently, the prompt engineering paradigm has shown great potential in endowing pretrained models with di-verse capabilities through simply providing prompts de-signed by experts or optimized with task-specific objectives
[20,28,39,43,45]. Inspired by this, we propose to introduce the prompt into the VLN task to improve the action-level modality alignment ability of the pretrained VLN agents.
To this end, we propose modAlity-aligneD Action PrompTs (ADAPT), where the agent is provided with explicit action prompts to make action decision. An action prompt con-tains a pair of multi-modal sub-prompts, where the image sub-prompt is a single-view observation indicating a salient visual object or location, and the paired text sub-prompt is an object-related action phrase like “go to the staircase”.
Before navigating, the instruction-related action prompts are retrieved from a pre-constructed action prompt base.
Then the action prompts are passed through a prompt en-coder and the output feature is concatenated with the orig-inal instruction feature. The prompt-based instruction fea-ture, together with the visual feature are fed to a multi-layer transformer for making action decision. Note that differ-ent from the common prompt engineering methods which change the output prediction form of a downstream task by introducing the prompt [28], in this work, we keep the same form of the action prediction as the baseline model and fo-cus on the design of the prompts. Through these provided action prompts, the agent can learn the action-level modal-ity alignment explicitly and make robust actions in different scenes. To enhance the discriminative power of the action prompts and enforce the agent to attend to related action prompts at each timestep, a modality alignment loss and a sequential consistency loss are further introduced into the training. Fig. 1 presents an action decision comparison be-tween the baseline agent [14] and our ADAPT. As shown in
Fig. 1, with the help of the action prompts related to “walk to the staircase”, our ADAPT can choose the correct action in the given observations to navigate successfully.
To collect high-quality action prompts into the ac-tion prompt base, we resort to the recently devel-oped Contrastive Language-Image Pretraining (CLIP) [32] model which has powerful cross-modal object/location-level alignment ability. Concretely, the image sub-prompt is obtained by retrieving object/location-related images us-ing CLIP from the action image sequence where each image contains the action information itself. The text sub-prompt is derived through a simple nearest-verb-search scheme.
Experimental results on both Room-to-Room (R2R) [1] and Room-across-Room (RxR) [19] benchmarks show the superiority of our proposed ADAPT over the state-of-the-art methods, demonstrating that introducing explicit action prompts is promising for improving the agent navigation performance. Our ablation study indicates the effective-ness of each method component and the good generalization ability of ADAPT. The visualization analysis also shows its good interpretability.
To summarize, the main contributions of this paper are: 1) We propose modality-aligned action prompts (ADAPT) to enforce the VLN agent to learn cross-modal action knowledge explicitly for improving action decision during navigation. To the best of our knowledge, this is the first attempt to develop prompt-based agents in the VLN task. 2) We develop a modality alignment loss and a sequen-tial consistency loss for enabling efficient learning of ac-tion prompts. The Contrastive Language-Image Pretraining (CLIP) model is employed to ensure the quality of the ac-tion prompts. 3) ADAPT establishes new state-of-the-art results on both R2R and RxR. It also shows good inter-pretability and generalization ability. 2.