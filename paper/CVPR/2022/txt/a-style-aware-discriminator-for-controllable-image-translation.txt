Abstract
Current image-to-image translations do not control the output domain beyond the classes used during training, nor do they interpolate between different domains well, lead-ing to implausible results. This limitation largely arises be-cause labels do not consider the semantic distance. To miti-gate such problems, we propose a style-aware discriminator that acts as a critic as well as a style encoder to provide conditions. The style-aware discriminator learns a con-trollable style space using prototype-based self-supervised learning and simultaneously guides the generator. Experi-ments on multiple datasets verify that the proposed model outperforms current state-of-the-art image-to-image trans-lation methods. In contrast with current methods, the pro-posed approach supports various applications, including style interpolation, content transplantation, and local im-age translation. The code is available at github.com/ kunheek/style-aware-discriminator. 1.

Introduction
Image-to-image (I2I) translation aims to manipulate the style of an existing image, where style refers to generic at-tributes that can be applied to any image in a dataset (e.g., texture or domain). Content generally refers to the remain-ing information, such as the pose and structure. This task has shown significant progress with generative adversarial network (GAN) [15] developments. Recent studies have ex-panded the functionality to multi-modal and multi-domains using domain-specific discriminators and latent injection
[18], enabling the direct manipulation of existing images using domain labels or reference images [10, 11, 31, 32, 40].
However, despite promising functionality advances, there remains considerable room for development in terms of controllability. For example, users can only control the classes used for training. Although a reference image can be used to control output but this can often lead to er-roneous results, particularly for misrecognition within the same class; and another common problem is inability to fine-tune the output. Since the label space does not con-sider the semantic distance between classes, the learned style space cannot reflect these semantic distances, which leads to unrealistic images when controlling the results by manipulating the style code [32].
This study investigates I2I translation controllability, i.e., to be able to edit the result as desired using the style code, without being limited to the previously defined label space.
The proposed model learns the style space using prototype-based self-supervised learning [6] with carefully chosen augmentations. Although the current domain-specific dis-criminators are not designed for an external continuous space, this is possible if the discriminator knows the style internally. Therefore, we propose a Style-aware Discrimi-nator, combining a style encoder and a discriminator into a single module. Thus, the proposed model is somewhat lighter by reducing one module and achieves better perfor-mance because of to the better representation space of the discriminator. We used the style code sampled from pro-totypes during training to improve the controllability; and feature-level and pixel-level reconstructions to improve the consistency. Thus, the proposed model goes beyond image translation to support various applications, including style interpolation and content transplantation. Finally, we pro-pose feedforward local image translation by exploiting spa-tial properties of the GAN feature space.
We evaluated the model on several challenging datasets:
Animal Faces HQ (AFHQ) [11], CelebA-HQ [22], LSUN churches [41], Oxford-102 [34], and FlickrFaces-HQ (FFHQ) [24]. Extensive experiments confirm that the pro-posed method outperforms current state-of-the-art models in terms of both performance and efficiency without seman-tic annotations. The proposed model can also project an im-age into the latent space faster than baselines while achiev-ing comparable reconstruction results.
The contributions from this study are summarized as fol-lows: (i) We propose an integrated module for style en-coding and adversarial losses for I2I translation, as well as a data augmentation strategy for the style space. The proposed method reduces the parameter count significantly and does not require semantic annotations. (ii) We achieve state-of-the-art results in truly unsupervised I2I translation
in terms of the FrÂ´echet Inception Distance (FID) [17]. The proposed method shows similar or better performance com-pared with supervised methods. (iii) We extend image translation functionality to various applications, including style interpolation, content transplantation, and local image translation. 2.