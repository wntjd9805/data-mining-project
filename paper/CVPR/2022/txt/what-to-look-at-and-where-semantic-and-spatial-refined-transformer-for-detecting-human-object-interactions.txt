Abstract
We propose a novel one-stage Transformer-based se-mantic and spatial refined transformer (SSRT) to solve the
Human-Object Interaction detection task, which requires to localize humans and objects, and predicts their interac-tions. Differently from previous Transformer-based HOI ap-proaches, which mostly focus at improving the design of the decoder outputs for the final detection, SSRT introduces two new modules to help select the most relevant object-action pairs within an image and refine the queries’ representation using rich semantic and spatial features. These enhance-ments lead to state-of-the-art results on the two most popu-lar HOI benchmarks: V-COCO and HICO-DET. 1.

Introduction
Human-object interaction (HOI) detection is an im-portant building block for complex visual reasoning, such as scene understanding [11, 51] and action recog-nition [50, 55], and its goal is to detect all HOI triplets
⟨human, object, action⟩ in each image. Fig. 1 shows an example of a HOI detection, where the person (i.e., the hu-man) is denoted with a red bounding box, the sports ball (i.e., the object) with a yellow bounding box, and the action kick is what that human is performing with that object.
The HOI literature can be divided into two-stage and one stage approaches. Two-stage approaches [12–14, 18, 19, 27, 30, 31, 33, 34, 39, 43, 46–48, 53, 54, 56] first use off-the-shelf detectors to localize all instances of people and objects in-dependently. For each person and object bounding box pair, an interaction class is then predicted in the second stage.
This sequential process has two main drawbacks [7, 25, 26]: (1) off-the-shelf object detectors are agnostic to the concept of interactions; and (2) enumerating over all pairs of person and object bounding boxes to predict an interaction class
In contrast, one-stage is time-consuming and expensive.
*Equal contribution.
†Work done during an internship at Amazon.
Figure 1. Conceptual workflow of SSRT. Instead of just feeding the encoded image to the decoder, we pre-select object-action (OA) prediction candidates and encode them to semantic and spatial features. These features then refine the learnt queries in decoding to enable them to attend to more relevant HOI predictions. approaches detect all the components of an HOI triplet di-rectly in an end-to-end fashion. Some earlier one-stage ap-proaches used intermediate representations based on inter-action points [32, 49] and union boxes [25] to predict these.
However, such methods fail when the interacting human and object are far away from each other and when multi-ple interactions overlap (e.g., crowd scenes) [7, 42].
More recently, a new trend of one-stage approaches [7, 26, 42, 58] based on Transformer architectures [5, 10, 35] have been proposed to overcome these problems and im-prove the HOI detection performance. This paper belongs to this category of works (Fig. 1). At a high-level, these ap-proaches first use a CNN backbone to extract image features and then feed them into an encoder-decoder architecture.
Some approaches use two decoders to detect instances and interactions in parallel [7, 26]. while others follow a sim-pler design that directly predicts all the elements of an HOI triplet with a single decoder [42, 58]. While successful, this design suffers from two limitations: (i) not all object-action pairs are meaningful (e.g., a person cannot be ‘cutting a pizza’ when the pizza is far away from the person’s loca-tion; and it is unusual for a person to be ‘cutting a football’),
simply relying on the one-shot network to reduce them may not be effective; and (ii) each simple query is decoded for all the rich elements of an HOI triplet (i.e., person location, object class and location, and interaction class), which is challenging, especially considering how HOI detection re-quires reasoning about complex relational structures in im-ages. We address both of these limitations in our work.
For this, we propose Semantic and Spatial Refined
Transformer (SSRT), that solves the aforementioned limita-tions by predicting what subset of object-action pairs is rel-evant for an image, and using explicit semantic and spatial information to support and guide the queries, so that they can be decoded more reliably and are more aligned with the final detection. In details, SSRT improves the Transformer design of previous HOI detector by introducing two novel modules: a Support Feature Generator (SFG) and a Query
Refiner (QR) (Fig. 2). The former generates semantic and spatial features from a set of pre-selected object-action (OA) pairs, while the latter integrates these features for decoding.
Our approach achieves state-of-the-art results on both V-COCO [16] and HICO-DET [6] datasets, showing the im-portance and effectiveness of our semantic and spatial guid-ance for HOI detection. Finally, in an extensive ablation study, we also evaluate our model design and our parameter choices, to further highlight the SSRT’s contributions. 2.