Abstract
In this paper, we study the problem of jointly estimat-ing the optical ﬂow and scene ﬂow from synchronized 2D and 3D data. Previous methods either employ a complex pipeline that splits the joint task into independent stages, or fuse 2D and 3D information in an “early-fusion” or “late-fusion” manner. Such one-size-ﬁts-all approaches suffer from a dilemma of failing to fully utilize the characteris-tic of each modality or to maximize the inter-modality com-plementarity. To address the problem, we propose a novel end-to-end framework, called CamLiFlow.
It consists of 2D and 3D branches with multiple bidirectional connec-tions between them in speciﬁc layers. Different from pre-vious work, we apply a point-based 3D branch to better ex-tract the geometric features and design a symmetric learn-able operator to fuse dense image features and sparse point features. Experiments show that CamLiFlow achieves bet-ter performance with fewer parameters. Our method ranks 1st on the KITTI Scene Flow benchmark, outperforming the previous art with 1/7 parameters. Code is available at https://github.com/MCG-NJU/CamLiFlow. 1.

Introduction
Optical ﬂow and scene ﬂow are the motion ﬁeld in 2D and 3D space respectively. Through them, we can gain insights into the dynamics of the scene, which are criti-cal to some high-level scene understanding tasks. In this work, we focus on the joint estimation of optical ﬂow and scene ﬂow, which addresses monocular camera frames with sparse depth measurements from LiDAR.
Previous methods [3, 31, 55, 56] construct a modular net-work that decomposes the estimation of ﬂow into multiple subtasks. These submodules are independent of each other, making it impossible for utilizing their complementarity.
Moreover, the limitations of any submodule will hurt the overall performance, since the whole pipeline depends on its results.
* Corresponding author (liuhs@smail.nju.edu.cn).
Figure 1. Results of the KITTI Scene Flow Benchmark. Marker size indicates model size. Models with unknown sizes and conven-tional approaches are marked as hollow. Our method outperforms all existing approaches [3, 23, 31, 33, 41, 45, 46, 55, 56] with much fewer parameters.
Other methods [42, 45] build an end-to-end architecture, consisting of multiple stages (including feature extraction, computing correlation, feature decoding, etc). RAFT-3D
[45] concatenates images and dense depth maps to RGB-D frames and feeds them into a uniﬁed 2D network to predict pixel-wise 3D motion. This kind of “early fusion” (Fig. 2a) makes 2D CNNs fail to use most 3D structural information provided by depth. DeepLiDARFlow [42] takes images and
LiDAR point clouds as input, where points are projected onto the image plane for densiﬁcation and are fused with images in a “late-fusion” manner (Fig. 2b). However, some errors generated in the early stage lack the opportunity to be corrected by the other modality and are accumulated to the subsequent stages. Thus the complementarity between the two modalities is not fully exploited.
In general, single-stage fusion suffers from a dilemma of failing to fully utilize the characteristic of each modal-ity or to maximize the inter-modality complementarity. To address the problem, we propose a multi-stage and bidi-rectional fusion pipeline (see Fig. 2c), which achieves bet-ter performance with fewer parameters. Within each stage, the two modalities are learned in separate branches using modality-speciﬁc architecture. At the end of each stage, a
learnable bidirectional bridge connects the two branches to pass complementary information. Moreover, recent point-based methods [28,29,36–38,48,51,52] achieve remarkable progress for 3D computer vision. This inspires us to process point clouds with a point-based branch, which can extract the ﬁne 3D geometric information without any voxelization or projection.
It is worth noting that there are two challenges for the fusion of the image branch and the point branch. First, im-age features are organized in a dense grid structure, while point clouds do not conform to the regular grid and are sparsely distributed in the continuous domain. As a result, there is no guarantee of one-to-one correspondence between pixels and points. Second, LiDAR point cloud possesses the property of varying density, where nearby regions have much greater density than farther-away regions. To tackle the ﬁrst problem, we propose a new learnable fusion op-erator, named bidirectional camera-LiDAR fusion module (Bi-CLFM), which fuses image/point features for both di-rections via learnable interpolation and sampling. As for the second problem, we propose a new transformation op-erator, named inverse depth scaling (IDS), which balances the distribution of points by scaling them non-linearly ac-cording to the inverse depth.
Experiments demonstrate that our approach achieves better performance with much fewer parameters. On Fly-ingThings3D [32], we achieve up to a 48.4% reduction in end-point-error over RAFT-3D with only 1/6 parameters.
On KITTI [33], CamLiFlow achieves an error of 4.43%, outperforming the previous art [56] with only 1/7 parame-ters. The leaderboard is shown in Fig. 1. 2.