Abstract
In this paper, we propose a novel and practical mecha-nism to enable the service provider to verify whether a sus-pect model is stolen from the victim model via model ex-traction attacks. Our key insight is that the proﬁle of a DNN model’s decision boundary can be uniquely characterized by its Universal Adversarial Perturbations (UAPs). UAPs belong to a low-dimensional subspace and piracy models’ subspaces are more consistent with victim model’s subspace compared with non-piracy model. Based on this, we pro-pose a UAP ﬁngerprinting method for DNN models and train an encoder via contrastive learning that takes ﬁnger-prints as inputs, outputs a similarity score. Extensive stud-ies show that our framework can detect model Intellectual
Property (IP) breaches with conﬁdence > 99.99 % within only 20 ﬁngerprints of the suspect model. It also has good generalizability across different model architectures and is robust against post-modiﬁcations on stolen models. 1.

Introduction
In the past few years, deep learning has emerged as a promising approach and the foundation for a wide range of real-world applications. As network architecture becomes more and more sophisticated and training costs rise, well-trained models become lucrative targets for the adversary looking to “steal” them. By querying the publicly available
APIs of these models, an adversary can collect the outputs to train a piracy model, dubbed model extraction attacks [4, 6, 7, 16, 31, 37, 42].
Existing works on mitigating model extraction attacks and protecting the Intellectual Property (IP) of trained mod-els fall into two groups [8, 18]. The ﬁrst group is based on watermarking techniques [2, 14, 17, 26, 29, 32, 35]. The idea is that the model owner introduces into her IP model a backdoor (i.e., a watermark), which would persist during
*Equal contribution. the model extraction. By checking whether a suspect model contains the injected watermark, a defender can determine whether this model is pirated.
The other category is based on ﬁngerprinting techniques that leverage inherent information (i.e., decision boundary) of the trained models. Observing that a DNN model can be uniquely proﬁled by its decision boundary which is also likely to be inherited by piracy models, a model extraction attack can be identiﬁed by examining whether a suspect model has (almost) the same decision boundary as the vic-tim model. A line of research [5, 13, 24] adopts adversarial examples to represent the decision boundaries.
However, the effectiveness of existing mitigation schemes were challenged. Watermarking based solutions suffer from utility drop caused by watermarks. Another concern is that an attacker can illegally inject a backdoor to argue the ownership which violates the non-forgeable de-mand [17]. Adversarial examples can only capture the local geometry, particularly, orientations of the decision bound-ary in local regions surrounding the adversarial examples, which may fail to be transferred to the suspect model due to decision boundary variation during the extraction [21, 30].
In this paper, we explore methods to capture the global characteristics of the decision boundary. As demonstrated in Figure 1, we propose a more effective model extraction detection scheme based on Universal Adversarial Perturba-tions (UAPs) [27]. A carefully selected UAP vector v can fool the model on almost all datapoints. We ﬁnd that UAPs are drawn from a low-dimensional subspace that contains most of the normal vectors of decision boundary. Due to decision boundary dependency, UAP subspaces of piracy models are more consistent with that of the victim model, which enables us to give a similarity score.
There are two challenges in applying UAPs for detect-ing model extraction. First, since the calculation of UAPs usually requires the knowledge of model parameters (i.e., white-box access) which model owners are not willing to provide, it is intractable for the defender to obtain UAPs of suspect models via black-box access. The second challenge
Figure 1. Illustrations of local and universal adversarial perturbations. Left: local adversarial perturbations are less robust to point-to-point decision boundary modiﬁcation due to extraction. Right: our framework relies on the stable correlation of decision boundaries proﬁled by universal adversarial perturbations (UAPs). is how to reliably distinguish between a piracy model and a homologous model (i.e., model trained on the same training data rather than the victim model’s outputs and should be not considered “stolen”).
To tackle the ﬁrst challenge, we propose a ﬁngerprinting function which is obtained by querying the suspect model with a number of datapoints, added by victim’s UAPs. A more informative ﬁngerprints need to capture as many parts of decision boundaries as possible. We therefore adopt K-means clustering on the last layer of the victim model to en-sure that the datapoints are uniformly selected from differ-ent source classes and move towards different target classes.
To address the second challenge, we design an encoder to map ﬁngerprints of the victim model, piracy models, and homologous models into a joint representation space. We adopt contrastive learning [9] (which aims to shorten the distances of the samples in the same classes and push away any samples of other classes) to project homologous models farther away from the victim model than the piracy models.
In summary, we propose a more accurate, robust and general IP protection framework against the model extrac-tion attacks. Our main contributions are:
• We present one of the ﬁrst attempts to leverage UAP dis-tribution dependency to measure the decision boundary similarity between models. We show that UAP outper-forms adversarial perturbation for model ﬁngerprinting.
• We propose a novel model ownership veriﬁcation frame-work based on UAP ﬁngerprinting that achieves a highly competitive detection rate in terms of AUC.
• Compared with prior ﬁngerprinting works, we demon-strate the capability of our framework for detecting post-modiﬁcated piracy models.
• We adopt contrastive learning in encoder training to ad-dress the similarity gap between homologous models and piracy models. A new data augmentation approach is proposed to create “views” for ﬁngerprints. 2.