Abstract
Advances in speech driven animation techniques allow the creation of convincing animations for virtual characters solely from audio data. Many existing approaches focus on facial and lip motion and they often do not provide realis-tic animation of the inner mouth. This paper addresses the problem of speech-driven inner mouth animation.
Obtaining performance capture data of the tongue and jaw from video alone is difficult because the inner mouth is only partially observable during speech. In this work, we introduce a large-scale speech and mocap dataset that fo-cuses on capturing tongue, jaw, and lip motion. This dataset enables research using data-driven techniques to generate realistic inner mouth animation from speech.
We then propose a deep-learning based method for ac-curate and generalizable speech to tongue and jaw anima-tion, and evaluate several encoder-decoder network archi-tectures and audio feature encoders. We find that recent self-supervised deep learning based audio feature encoders are robust, generalize well to unseen speakers and content, and work best for our task.
To demonstrate the practical application of our ap-proach, we show animations on high-quality parametric 3D face models driven by the landmarks generated from our speech-to-tongue animation method. 1.

Introduction
Virtual human characters with strikingly realistic facial animation are possible through advances in facial perfor-mance capture and speech-driven animation. However, vir-tual characters often lack realistic representation and motion of the inner mouth, in particular for the tongue. Tongue ani-mation is often subdued and unnatural, breaking the illusion of realism and contributing to an uncanny valley experience.
Accurately animating the tongue is a challenging task.
Typical optical performance capture approaches fail be-cause the inner mouth articulators are only partially observ-able even when the mouth is open. Manually animating the tongue and jaw requires a skilled artist familiar with speech articulation, and is time consuming due to the rapid and complex motions required for speech production. Animat-ing manually is clearly not an option for any real-time or interactive application. In all cases, a low-latency and real-time automatic animation solution is preferred.
In practice, inner mouth animations for movies and video games often make use of rule-based or procedural anima-tion approaches. The result is to broadly match the utter-ance of specific sounds, such as dental consonants, open mouth vowels, or articulation that moves the tongue from the mouth floor to the palate and vice-versa. In many cases, the inner mouth region is simply intentionally poorly lit while speech articulation is approximated by placing the tongue in a neutral position.
In this paper we consider the problem of automatic speech-driven tongue and jaw animation using a data-driven sequence to sequence approach. Sequence to sequence models have shown impressive results on a diverse set of regression and forecasting problems, and they have been applied in a wide variety of research areas. In our task, the input is streaming speech audio waveform, and the output is the temporally corresponding set of 3D landmark locations of motion captured speech articulators. We recorded and release a new dataset, comprising over 2.5 hours of labelled speech, that includes ground truth landmarks tracked using an Electromagnetic Articulograph (EMA) [5], a specialized speech motion capture system. The dataset is publicly avail-able for further research1.
We leverage recent work in deep-learning based speech audio feature representations and compare ML-based ap-proaches with traditional features based on phonetic or fre-quency based representations. Our experiments show that deep learning speech representations greatly improve gen-eralization and resiliency to noise over traditional features. 1https://salmedina.github.io/tongue-anim
The landmark locations predicted by our model may be used to drive any facial animation rig. We demonstrate rig solving using a professional FACS-inspired [11] MetaHu-man facial rig based on the capture subject. General retar-geting is shown on further MetaHuman characters that can be customized using the MetaHuman Creator Tool [12]. To animate a rig, we perform an optimization that minimizes the distance between the predicted landmarks and its cor-responding locations on the facial mesh and solve for the parametric representation of the animation. This approach means the results can be readily used in game engines or any digital content creation (DCC) software.
In summary, our main contributions in this work are as follows: (1) We introduce a framework for speech-driven tongue animation that trains a high-quality speech-to-animation model for the tongue and jaw. (2) We thor-oughly analyze and compare a diverse set of audio represen-tations by introducing self-supervised deep learning audio features for the task of speech-to-animation. (3) We present a pipeline approach that drives a high-quality parametric 3D face model from a few 3D landmark constraints through a fast optimization method. (4) We release code and a novel large-scale speech-to-tongue mocap dataset to train tongue and jaw speech animation models. 2.