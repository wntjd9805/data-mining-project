Abstract
Image segmentation is usually addressed by training a model for a fixed set of object classes. Incorporating ad-ditional classes or more complex queries later is expen-sive as it requires re-training the model on a dataset that encompasses these expressions. Here we propose a sys-tem that can generate image segmentations based on ar-bitrary prompts at test time. A prompt can be either a text or an image. This approach enables us to create a unified model (trained once) for three common segmenta-tion tasks, which come with distinct challenges: referring expression segmentation, zero-shot segmentation and one-shot segmentation. We build upon the CLIP model as a backbone which we extend with a transformer-based de-coder that enables dense prediction. After training on an extended version of the PhraseCut dataset, our system gen-erates a binary segmentation map for an image based on a free-text prompt or on an additional image expressing the query. We analyze different variants of the latter image-based prompts in detail. This novel hybrid input allows for dynamic adaptation not only to the three segmentation tasks mentioned above, but to any binary segmentation task where a text or image query can be formulated. Finally, we find our system to adapt well to generalized queries involving affordances or properties. Code is available at https://eckerlab.org/code/clipseg 1.

Introduction
The ability to generalize to unseen data is a fundamental problem relevant for a broad range of applications in artifi-cial intelligence. For instance, it is crucial that a household robot understands the prompt of its user, which might in-volve an unseen object type or an uncommon expression for an object. While humans excel at this task, this form of inference is challenging for computer vision systems.
Image segmentation requires a model to output a predic-tion for each pixel. Compared to whole-image classifica-tion, segmentation requires not only predicting what can be seen but also where it can be found. Classical semantic seg-mentation models are limited to segment the categories they
§timo.lueddecke@uni-goettingen.de
Figure 1. Our key idea is to use CLIP to build a flexible zero/one-shot segmentation system that addresses multiple tasks at once. have been trained on. Different approaches have emerged that extend this fairly constrained setting (see Tab. 1):
• In generalized zero-shot segmentation, seen as well as unseen categories needs to be segmented by putting unseen categories in relation to seen ones, e.g. through word embeddings [1] or WordNet [2].
• In one-shot segmentation, the desired class is provided in form of an image (and often an associated mask) in addition to the query image to be segmented.
• In referring expression segmentation, a model is trained on complex text queries but sees all classes dur-ing training (i.e. no generalization to unseen classes).
To this work, we introduce the CLIPSeg model (Fig. 1), which is capable of segmenting based on an arbitrary text query or an example image. CLIPSeg can address all three tasks named above. This multi-modal input format goes beyond existing multi-task benchmarks such as Visual De-cathlon [3] where input is always provided in form of im-ages. To realize this system, we employ the pre-trained
CLIP model as a backbone and train a thin conditional segmentation layer (decoder) on top. We use the joint text-visual embedding space of CLIP for conditioning our model, which enables us to process prompts in text form as well as images. Our idea is to teach the decoder to relate activations inside CLIP with an output segmentation, while permitting as little dataset bias as possible and maintaining the excellent and broad predictive capabilities of CLIP.
We employ a generic binary prediction setting, where a foreground that matches the prompt has to be differen-tiated from background. This binary setting can be adapted
unseen classes free form prompt no fixed targets negative samples
Our setting
Classic
Referring Expression
Zero-shot
One-shot
✓
--✓
✓
✓
-✓
--✓
-✓
✓
✓
✓
✓
-✓
-Table 1. Comparison of different segmentation tasks. Negative means samples that do not contain the target (or one of the tar-gets in multi-label segmentation). All approaches except classic segmentation adapt to new targets dynamically at inference time. to multi-label predictions which is needed by Pascal zero-shot segmentation. Although the focus of our work is on building a versatile model, we find that CLIPSeg achieves competitive performance across three low-shot segmenta-tion tasks. Moreover, it is able to generalize to classes and expressions for which it has never seen a segmentation.
Contributions Our main technical contribution is the
CLIPSeg model, which extends the well-known CLIP trans-former for zero-shot and one-shot segmentation tasks by a proposing a lightweight transformer-based decoder. A key novelty of this model is that the segmentation target can be specified by different modalities: through text or an image.
This allows us to train a unified model for several bench-marks. For text-based queries, unlike networks trained on
PhraseCut, our model is able to generalize to new queries involving unseen words. For image-based queries, we ex-plore various forms of visual prompt engineering – analo-gously to text prompt engineering in language modeling.
Furthermore, we evaluate how our model generalizes to novel forms of prompts involving affordances. 2.