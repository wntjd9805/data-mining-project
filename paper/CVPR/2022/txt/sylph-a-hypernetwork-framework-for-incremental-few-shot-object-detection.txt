Abstract
We study the challenging incremental few-shot object de-tection (iFSD) setting. Recently, hypernetwork-based ap-proaches have been studied in the context of continuous and finetune-free iFSD with limited success. We take a closer look at important design choices of such methods, leading to several key improvements and resulting in a more accurate and flexible framework, which we call Sylph.
In particular, we demonstrate the effectiveness of decou-pling object classification from localization by leveraging a base detector that is pretrained for class-agnostic local-ization on large-scale dataset. Contrary to what previous results have suggested, we show that with a carefully de-signed class-conditional hypernetwork, finetune-free iFSD can be highly effective, especially when a large number of base categories with abundant data are available for meta-training, almost approaching alternatives that undergo test-time-training. This result is even more significant consider-ing its many practical advantages: (1) incrementally learn-ing new classes in sequence without additional training, (2) detecting both novel and seen classes in a single pass, and (3) no forgetting of previously seen classes. We benchmark our model on both COCO and LVIS, reporting as high as 17% AP on the long-tail rare classes on LVIS, indicating the promise of hypernetwork-based iFSD. 1.

Introduction
While advances in deep learning have led to significant progress in computer vision [18, 23, 24, 31], much of this success has relied upon large-scale data collection and an-notation [7, 20, 22, 36], a process that is both labor-intensive and time-consuming, and does not scale well with the num-ber of categories. This is especially true for object detec-tion [18, 23, 37], particularly for the long tail of object cat-egories, where data may be scarcer [22]. As a result, few-shot learning of object detectors (FSD) [28, 65, 70, 72] has become a recent topic of interest.
While learning a novel class from only a few samples alone is a challenging problem, the task can be made sim-pler by leveraging known classes with abundant data (com-monly referred to as base classes), whose structure can be used as a prior for knowledge transfer. The few pre-vious FSD works have approached this primarily in two ways. The first is fine-tuning [47, 65], where a model is first pretrained on the base classes and then fine-tuned on a small balanced set of data from both the base and novel classes, a form of test-time training [59]. Although sim-ple, it has difficulty scaling to many real-world applications due to its computational and memory requirements. An alternate strategy is taking a meta-learning approach [72].
Meta-learning approaches frame the problem as “learning to learn” [4,10,32,44,61,69,72], training the model episod-ically to induce fast adaptation to novel classes.
However, many FSD methods focus on the limited set-up where only novel categories are to be detected. These meth-ods often fail to preserve the original detector performance on base categories [4, 10, 32, 72] or forget about the ones it was initially trained on [65]. Given the ever-changing nature of the real-world, a desirable property of machine learning systems is the ability to incrementally learn new concepts without revisiting previous ones and not forgetting them [40, 42]. Humans are able to achieve such feat, learn-ing novel concepts not only without forgetting but reusing such knowledge [45]. Conventional supervised learning struggles with incrementally presented data, tending to suf-fer catastrophic forgetting [39, 51]. An alternative is study-ing all the available data every time new concepts arrive, commonly referred to as “joint training” [22], but such a paradigm imposes a slow development cycle, requiring sig-nificant data collection efforts for the new concepts and ex-pensive large-scale training (and re-training).
Instead, we seek an object detection model capable of learning new classes from a few shots in a fast, scalable manner without forgetting previously seen classes, a set-ting commonly referred to as incremental few-shot detec-tion (iFSD). ONCE [44], a meta-learning approach to FSD, is of particular interest due to its hypernetwork-based class-conditional design. ONCE is able to enroll novel categories
without affecting its ability to remember base classes. We use a base detector and hypernetwork architecture similar to
ONCE, but with a few key design differences: (1) ONCE, along several other recent works [28, 65, 72], attempts to di-rectly produce (via training or hypernet) the parameters of a localization regression model that transforms the query sample feature maps into the output bounding boxes, all from the few available training samples. We find this to be unnecessary and potentially harmful, as the task can be sig-nificantly simplified by decoupling localization from clas-sification. To achieve this goal, we leverage a base detec-tor with class-agnostic localization capability pretrained on abundant base class data. (2) We study the class-conditional hypernetwork’s behavior, making some key changes to the structure and adding normalization to the predicted param-eters, resulting in much higher accuracy.
With an architecture that can swiftly adapt to the long tail of classes from few shots, we name our framework Sylph, after the nimble long-tailed hummingbird (Figure 1). We present extensive evaluations that empirically demonstrate the benefits of our design, showing that Sylph is more effec-tive than ONCE [44] (our main baseline) across all the re-ported datasets and evaluation regimes. On the challenging
LVIS few-shot learning benchmark in particular, we show that Sylph is superior by a margin of 8% points. 2.