Abstract
While deep face recognition (FR) systems have shown amazing performance in identification and verification, they also arouse privacy concerns for their excessive surveil-lance on users, especially for public face images widely spread on social networks. Recently, some studies adopt adversarial examples to protect photos from being identified by unauthorized face recognition systems. However, exist-ing methods of generating adversarial face images suffer from many limitations, such as awkward visual, white-box setting, weak transferability, making them difficult to be ap-plied to protect face privacy in reality.
In this paper, we propose adversarial makeup transfer
GAN (AMT-GAN)1, a novel face protection method aim-ing at constructing adversarial face images that preserve stronger black-box transferability and better visual quality simultaneously. AMT-GAN leverages generative adversar-ial networks (GAN) to synthesize adversarial face images with makeup transferred from reference images. In particu-lar, we introduce a new regularization module along with a joint training strategy to reconcile the conflicts between the adversarial noises and the cycle consistence loss in makeup transfer, achieving a desirable balance between the attack strength and visual changes. Extensive experiments verify that compared with state of the arts, AMT-GAN can not only preserve a comfortable visual quality, but also achieve a higher attack success rate over commercial FR APIs, in-1https://github.com/CGCL-codes/AMT-GAN
Figure 1. Comparison with existing adversarial attacks on FR sys-tems in the black-box setting. The images in (a) are directly ex-tracted from their papers. The numbers listed below images are the verification confidence of the target identity given by commercial
FR APIs, and a higher score represent a stronger attack ability. The blue is from Face++, the green is from Aliyun, and the red is from
Microsoft Azure (The same color code will be used hereinafter). cluding Face++, Aliyun, and Microsoft. 1.

Introduction
Recent years have witnessed the fast development of face recognition (FR) based on deep neural networks (DNNs).
The powerful face recognition systems, however, also pose a great threat to personal privacy. For example, it has been shown that FR systems can be used to identify social me-dia profiles and track user relationships through large-scale photo analysis [16, 31]. Such kind of excessive surveillance on users urgently demands an effective approach to help in-dividuals protect their face images against unauthorized FR systems.
Launching data poisoning attacks towards the training dataset or the gallery dataset of malicious FR models is a promising solution to protect facial privacy [3, 29]. How-ever, these schemes require that the adversary (i.e., the users in our scenario) can inject poisoned face images into the datasets. Once the target model is trained or makes infer-ence over clean datasets, they are likely to become invalid.
Another strategy is to make use of adversarial examples to launch evasion attacks and protect face images from be-ing illegally identified [26, 32, 40]. Adversarial face images are more suitable for protecting user privacy in real-world scenarios since they only need to modify users’ own data re-gardless of the settings of the target model. Unfortunately, existing approaches of generating adversarial face examples suffer from several limitations when they are considered to protect face privacy on social media: (1) Accessibility to target models. Most existing schemes belong to white-box attack (i.e., the adversary has full knowledge of the target model) [11, 24], or query-based black-box attack (i.e., the adversary can arbitrarily query the target model) [9]. They are infeasible for protecting users’ privacy since the users have no idea which kind of DNNs the third-party tracker is running; (2) Poor visual quality. As shown in Fig. 1, ex-isting adversarial attacks on FR system fail to preserve the image quality in the black-box setting. Patch-based adver-sarial attacks [22, 37] often cause a fairly bizarre and con-spicuous change on source images, and the state-of-the-art perturbation-based method [40] makes the modified face fill with awkward noises; (3) Weak transferability. Fig. 1 also demonstrates that the state of the art has a relatively low attack success rate on commercial APIs. In summary, it is still challenging to balance the trade-off between the visual quality and attack ability of adversarial face images in the black-box setting.
In this paper, we solve this problem from a new perspec-tive. Different from existing works trying to place multi-farious restrictions on perturbations and then dig a better gradient-based algorithm to construct adversarial examples, we focus on organizing the perturbations, although exten-sive and visible, in a reasonable way such that they appear natural and comfortable, under the condition that a high at-tack ability is maintained. We therefore leverage makeup as the key idea for arranging perturbations. Specifically, we propose a new framework called adversarial makeup transfer GAN (AMT-GAN) to generate adversarial face im-ages with the natural appearance and stronger black-box attack strength. ATM-GAN first exploits a set of gener-ative adversarial networks to construct adversarial exam-ples that can inherit makeup styles from a reference im-age. In order to reconcile the conflicts between the adver-sarial noises and the cycle consistence loss in makeup trans-fer, we incorporate a newly designed regularization module by exploiting the disentanglement function of the encoder-decoder architecture and the residual dense blocks in the image super-resolution. As a result, the adversarial toxic-ity can be compatibly alleviated in the cycle reconstruction phase, making the generator focus on building robust map-pings between the source domain and the target style do-main with adversarial features. In addition, we introduce a joint training strategy which integrates the traditional G-D game in the GAN training and the newly designed regular-ization module, as well as the transferability enhancement process to encourage the generator to catch, imitate, and reconstruct the common adversarial features which can ef-fectively transfer between different models.
To the best of our knowledge, we propose the first joint training framework to address the collapse phenomenon of the cycle consistency and the domain mappings of the gen-erator when the image-to-image translation GANs are used to craft adversarial examples. Our joint training frame-work can be extended to other security-sensitive fields when
GAN is considered, such as Deepfake [35]. In summary, we make the following contributions:
• We propose AMT-GAN, a more practical approach for protecting face images against unauthorized FR sys-tems, by constructing adversarial examples with out-standing black-box attack performance and natural ap-pearance that derive cosmetic styles from any chosen reference images.
• We design a regularization module based on feature disentanglement to improve the visual quality of adver-sarial images, and then develop a joint training pipeline to train the generator, the discriminator, and the reg-ularization module, such that the generator can ac-complish two jobs (i.e., makeup-transfer and adversar-ial attack) simultaneously and build robust mappings among different data manifolds.
• Our extensive experiments on multiple benchmark datasets verify that AMT-GAN is highly effective at attacking various deep FR models, including commer-cial face verification APIs such as Face++ 2, Aliyun3, and Microsoft4, where we outperform state of the arts about 4% ∼ 60%. 2https://www.faceplusplus.com 3https://vision.aliyun.com 4https://azure.microsoft.com
Figure 2. The architecture of AMT-GAN 2.