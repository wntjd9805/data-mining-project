Abstract
Recent advances of deep learning-based approaches have achieved remarkable performance on appearance-based gaze estimation. However, due to the shortage of target domain data and absence of target labels, general-izing gaze estimation algorithm to unseen environments is still challenging.
In this paper, we discover the rotation-consistency property in gaze estimation and introduce the
‘sub-label’ for unsupervised domain adaptation. Conse-quently, we propose the Rotation-enhanced Unsupervised
Domain Adaptation (RUDA) for gaze estimation. First, we rotate the original images with different angles for train-ing. Then we conduct domain adaptation under the con-straint of rotation consistency. The target domain images are assigned with sub-labels, derived from relative rotation angles rather than untouchable real labels. With such sub-labels, we propose a novel distribution loss that facilitates the domain adaptation. We evaluate the RUDA framework on four cross-domain gaze estimation tasks. Experimental results demonstrate that it improves the performance over the baselines with gains ranging from 12.2% to 30.5%. Our framework has the potential to be used in other computer vision tasks with physical constraints. 1.

Introduction
Gaze is one of the most important cues for human inten-tion prediction. It has been used in a variety of applications such as virtual/augmented reality [21,30], human-computer interaction [18, 35, 37], and medical analysis [3, 20]. To ob-tain accurate gaze estimations, various systems have been developed. Appearance-based gaze estimation is one of the most promising approaches, since it has the lowest hard-ware requirements.
With the advancement of deep learning techniques, Con-volutional Neural Networks (CNN) have achieved signif-icant performance improvement in many computer vision
*Corresponding Author. This work was supported by the National Nat-ural Science Foundation of China (NSFC) under Grant 61972012.
Adaptation
Output
Sub-label
Images
Rotation Consistency
Gaze direction labels
Pretrain model
Source domain
RUDA framework (proposed)
Before UDA
After UDA r o r r
E e z a
G
User
Target domain
Figure 1. The overall structure of the proposed rotation-enhanced unsupervised domain adaptation (RUDA) framework for gaze es-timation. RUDA adapts the pre-trained model to target domain without requiring any gaze labels in target domain. tasks. Gaze estimation task is no exception, various CNN-based gaze estimation methods have been proposed over the last decades [8]. These systems usually have different in-puts: eye images [9,24,29,39,42], face images [19,22,43] or both face/eye images [1, 7, 23]. However, existing methods suffer from severe performance degradation when adapting to new domains, which is mainly caused by the difference between the domains, e.g., subject appearance, image qual-ity, shooting angle and illumination.
One of the major challenge of gaze domain adaptation is that we usually do not have access to target domain labels in real world scenarios, and we cannot directly train the gaze estimator in target domain. To address this problem, unsu-pervised domain adaptation approaches aim to ﬁnd a gaze-relevant constraint generalizing the model to target domain without label. Kellnhofer et al. propose to supervise gaze estimation model with an domain discriminator by adver-sarial learning [19]. Similarly, Wang et al. employ an ap-pearance discriminator and a head pose classiﬁer for adap-tation [39]. More recently, Liu et al. propose to guide the model with outliers [25]. Although some unsupervised do-main adaptation approaches for gaze estimation have been proposed, it is still a challenging task.  
To build a gaze-relevant constraint to supervise the model without requiring ground truth labels, we dive into the physical nature of gaze. We ﬁnd that the human gaze, as a 3D direction vector, is rotation-consistent. Rotating the face image results in the same rotation angle of the gaze direction, we call this rotation-consistency property. And we deﬁne the relative rotation angle as the sub-label, mean-ing that it is not an absolute angle, but the relative differ-ence angle before and after the rotation. This rotation con-sistency could serve as the desired gaze-relevant constraint without ground truth. Although training with rotated im-ages in source domain does not improve gaze estimation accuracy because user faces are already aligned by normal-ization [42], we argue that the rotation consistency property provides a gaze-relevant optimization target for adaptation.
In light of this, we present the Rotation-enhanced Unsu-pervised Domain Adaptation (RUDA) framework for gaze estimation. Our approach creates sub-labels between origi-nal and randomly rotated images. The estimator is general-ized to target domain via rotation consistency of estimation results with no target domain label required and low com-putation cost. The contributions of this work are as follow:
• We propose the Rotation-enhanced Unsupervised Do-main Adaptation (RUDA) framework for gaze estima-tion. The RUDA ﬁrst trains a rotation-augmented model in source domain, then adapts the model to target domain using the synthesized images with physically-constrained gaze directions.
• We found the rotation consistency property, which can be used to generate sub-labels for unsupervised gaze adap-tation tasks. To facilitate adaptation, we design a novel distribution loss which supervise the model with rotation consistency and sub-labels.
• Experimental results demonstrate that the RUDA frame-work achieves consistent improvement over the baseline model on four cross-domain gaze estimation tasks, rang-ing from 12.2% to 30.5%. It achieves surprisingly good results, even outperforms some state-of-the-art methods trained on target domain with labels. 2.