Abstract
A compositional question refers to a question that con-tains multiple visual concepts (e.g., objects, attributes, and relationships) and requires compositional reasoning to an-swer. Existing VQA models can answer a compositional question well, but cannot work well in terms of reasoning consistency in answering the compositional question and its sub-questions. For example, a compositional question for an image is: “Are there any elephants to the right of the white bird?” and one of its sub-questions is “ Is any bird visible in the scene?”. The models may answer “yes” to the compositional question, but “no” to the sub-question. This paper presents a dialog-like reasoning method for main-taining reasoning consistency in answering a compositional question and its sub-questions. Our method integrates the reasoning processes for the sub-questions into the reason-ing process for the compositional question like a dialog task, and uses a consistency constraint to penalize incon-sistent answer predictions. In order to enable quantitative evaluation of reasoning consistency, we construct a GQA-Sub dataset based on the well-organized GQA dataset. Ex-perimental results on the GQA dataset and the GQA-Sub dataset demonstrate the effectiveness of our method. 1.

Introduction
Compositional visual question answering (VQA) [3, 9, 27] is the task of providing an answer to a compositional question about an image based on the content of the image.
A compositional question refers to a question that contains multiple visual concepts (e.g., objects, attributes, and rela-tionships). The task requires a comprehensive understand-ing of the multi-modal inputs and compositional relational reasoning based on the understanding.
∗corresponding author
Figure 1. Qualitative examples showing the inconsistency of ex-isting compositional VQA models. For each example, we show an image on the left side. A compositional question (Q) and its sub-questions (Sub-Q) with ground-truth answers and predicted answers of two reasoning models (i.e., the LCGN [9] and the
MMN [4]) are shown on the right side.
Existing reasoning models for compositional VQA can answer a compositional question well, but cannot work well in terms of reasoning consistency in answering the com-positional question and its sub-questions. A compositional question usually contains various known visual concepts in the image. To answer the question, reasoning models are supposed to infer unknown concepts based on the known concepts. For a compositional question “ Are there any ele-phants to the right of the white bird?” (shown in Fig. 1 (a)), the known concepts are “white” and “bird”, while the unknown concepts are “elephants” and “to the right of”.
Obviously, the compositional question reveals answers to sub-questions about these known concepts, such as “ Is any bird visible in the scene?”, and “What the white animal is called?”. The question requires a stronger reasoning abil-ity to answer than its sub-questions. However, although a model accurately answers the compositional question, it may fail to provide correct answers for its sub-questions as shown in Fig. 1. The inconsistency indicates errors in the reasoning process for the compositional question.
This paper proposes a dialog-like reasoning method that integrates the reasoning processes for sub-questions into the reasoning process for a compositional question to main-tain the reasoning consistency in compositional VQA. The method represents an input image as a structured visual graph and performs iterative language-guided graph convo-lution to learn contextual visual representations for compo-sitional reasoning. The number of required iterations for each question is determined in advance. In the reasoning process of a compositional question, if the current num-ber of iterations equals the required number of iterations of a sub-question, we use an answer classifier to answer the corresponding sub-question using current visual repre-sentations. A consistency constraint is further introduced to penalize inconsistent answer predictions. By answering sub-questions with the guidance of the compositional ques-tion, the method is supposed to capture the correlations of the question and its sub-questions for better consistency.
The answering process for the sub-questions can also be regarded as the intermediate supervision of the reasoning process for the compositional question.
To enable the quantitative evaluation of reasoning con-sistency in compositional VQA, we build a GQA-Sub dataset based on the GQA dataset [11], a well-organized large-scale dataset for compositional VQA. We automat-ically decompose compositional questions of the GQA into sub-questions, and carefully balance the obtained sub-questions to avoid biases. Given a compositional question and its sub-questions, reasoning consistency can be quan-titatively measured by evaluating whether predictions for these questions are contradictory. Experimental results on the GQA and the GQA-Sub demonstrate the effectiveness of our method. The data and code are publicly available at https://github.com/jingchenchen/ReasoningConsistency-VQA.
The contributions of this paper are two-fold: 1. We propose a dialog-like reasoning method that inte-grates reasoning processes for the sub-questions into the reasoning process for a compositional question to ensure reasoning consistency in compositional VQA. tradiction [19]. Specifically, a visual fact is defined as a triplet in a scene graph of an image. Recent work has pro-posed various datasets to measure the consistency of VQA models. Ribeiro et al. [20] first studied the inconsistency of VQA models and argue that we need to consider the re-lationship between predictions to measure true understand-ing. They automatically generate implications for questions in the VQA v1 dataset [3] to evaluate the consistency. The implications are questions about the same fact as the origi-nal question. Given a question-answer pair as “What room is this? bathroom”, they generate implications such as “Is this a bathroom?” and “Is there a bathroom in the pic-ture?”. Ray et al. [19] generate entailed questions for ques-tions in the VQA v2 dataset [5]. The entailed questions are also about the same fact as the original question. Selvaraju et al. [22] distinguish reasoning questions and perception questions and build datasets based on the VQA v2 to test the consistency of models for reasoning questions and percep-tion questions. Yuan et al. [28] transform counting question of the VQA v2 by performing object-oriented partition, re-ordering, or reversion, to test the perception ability of VQA models. Shah et al. [23] and Whitehead et al. [26] rephrase questions in the VQA v2 to evaluate the VQA models’ ro-bustness for linguistic variations. Previous work generates questions about one visual fact in the original question to measure the consistency of VQA models. By contrast, we focus on generating sub-questions about known visual facts in the compositional questions to test whether the VQA models are really capable of compositional reasoning.
Hudson and Manning [11] devise a consistency metric by generating entailed questions for compositional ques-tions in the GQA [11].
In Sec. 4, we analyze the differ-ences between entailed questions of the GQA and our sub-questions in detail.
Compositional VQA. Existing methods for composi-tional VQA can be mainly divided into two categories: modular and holistic. Modular methods [2, 4, 8, 24] as-semble various modules according to an input question and execute the modules for reasoning. Holistic methods
[9, 10, 12, 13, 16, 29] use a single model for different inputs to achieve reasoning. Our work aims to ensure the reason-ing consistency for compositional VQA and is orthogonal to previous work. 3. Method 2. We present a GQA-Sub dataset to evaluate the reason-3.1. Overview ing consistency of compositional VQA models. 2.