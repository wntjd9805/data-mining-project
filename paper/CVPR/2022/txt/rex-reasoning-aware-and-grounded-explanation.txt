Abstract
Effectiveness and interpretability are two essential prop-erties for trustworthy AI systems. Most recent studies in visual reasoning are dedicated to improving the accuracy of predicted answers, and less attention is paid to explain-ing the rationales behind the decisions. As a result, they commonly take advantage of spurious biases instead of ac-tually reasoning on the visual-textual data, and have yet developed the capability to explain their decision making by considering key information from both modalities. This paper aims to close the gap from three distinct perspec-tives: first, we define a new type of multi-modal explana-tions that explain the decisions by progressively traversing the reasoning process and grounding keywords in the im-ages. We develop a functional program to sequentially ex-ecute different reasoning steps and construct a new dataset with 1,040,830 multi-modal explanations. Second, we iden-tify the critical need to tightly couple important compo-nents across the visual and textual modalities for explain-ing the decisions, and propose a novel explanation genera-tion method that explicitly models the pairwise correspon-dence between words and regions of interest. It improves the visual grounding capability by a considerable margin, resulting in enhanced interpretability and reasoning perfor-mance. Finally, with our new data and method, we perform extensive analyses to study the effectiveness of our explana-tion under different settings, including multi-task learning and transfer learning. Our code and data are available at https://github.com/szzexpoi/rex. 1.

Introduction
One of the fundamental goals in artificial intelligence is to develop intelligent systems that are able to reason and explain with the complexity of real-world data to make de-cisions. While explaining decisions is an integral part of human communication, understanding and reasoning, ex-isting visual reasoning models typically answer questions without explaining the rationales behind their answers. As a
Figure 1. Illustration of our explanation that is derived from the reasoning process (with different reasoning steps color coded) and explicitly grounds key objects in the image. result, despite the significantly increased accuracy achieved by powerful deep neural networks [2, 16, 21, 23, 26, 35], ex-isting methods commonly take advantage of spurious data biases [27] and it is difficult to understand if they make de-cisions by truly understanding the causal relationships be-tween multi-modal inputs and the answers.
An important line of research to tackle the issues is to improve the interpretability of visual reasoning models with multi-modal explanations [7, 22, 28, 31, 39, 40, 43]. While showing usefulness in highlighting important visual regions and providing user-friendly textual descriptions, these ap-proaches suffer from two major limitations: (1) Existing explanations are typically defined in the forms of attention maps or free-formed natural language. Attention maps cap-ture the salient regions for generating the answers but fall short of explaining how different regions contribute to the decision-making process. On the other hand, unconstrained textual explanations could be highly diverse and often in-consistent when explaining the same decision. Both of them lack the capability to illustrate the reasoning process be-hind a decision. (2) The explanations of different modal-ities are loosely connected and modeled with separate pro-cesses [22, 31, 40]. This not only undermines the capability of explaining models’ rationales with multiple modalities, but can also result in contradictory explanations [40]. For
instance, textual explanations “The apple is above the pear” and “The pear is above the apple” have opposite meanings but could share the same attention map. We address the aforementioned challenges from two distinct perspectives (i.e., data and model), and propose an integrated framework that consists of a new type of explanations, a functional pro-gram, and a novel explanation generation method.
From the data perspective, instead of independently modeling explanations of a single modality without consid-ering the reasoning process, we introduce a new Reasoning-aware and grounded EXplanation (REX) that is derived by traversing the reasoning process and tightly coupling key components across the visual and textual modalities. As shown in Figure 1, it is constructed based on the consecutive reasoning steps (e.g., select, common) for decision making, and explicitly grounds key objects (e.g., comb, heart) with visual regions to elaborate how they contribute to the an-swer. The structured reasoning process also naturally allevi-ates the variance of natural language, and enables models to pay focused attention to important information for reason-ing instead of the language structure. To automatically con-struct our explanations, we develop a functional program to progressively execute the reasoning steps and query key information from scene graphs [14, 19], and collect a new dataset with 1,040,830 multi-modal explanations.
From the model perspective, unlike existing methods
[7, 22, 28, 31, 40] that model key components in different modalities with separate processes, we propose a novel ex-planation generation method that explicitly models the cor-respondence between important words and regions of inter-est. It takes into account the semantic similarity between features of the two modalities, and incorporates an adaptive gate to ground words in the visual scene. Our method im-proves the visual grounding by a large margin, resulting in enhanced interpretability and reasoning performance.
To summarize, our contributions are as follows: (1) We present REX, a new type of reasoning-aware and visually grounded explanations. Our explanation differen-tiates itself with its strong correlation with the reasoning process and the tight coupling between different modalities.
We develop a functional program to automatically construct our new dataset with 1,040,830 multi-modal explanations. (2) We propose a novel explanation generation method that goes beyond the conventional paradigm of indepen-dently modeling multi-modal explanations [7, 28, 40], and leverages an explicit mapping to ground words in the visual regions based on their correlation. (3) We demonstrate the effectiveness of our data and method with extensive experiments under different settings, including multi-task learning and transfer learning. We also analyze different visual skills and their correlation with the reasoning performance. 2.