Abstract
We propose a framework that can deform an object in a 2D image as it exists in 3D space. Most existing meth-ods for 3D-aware image manipulation are limited to (1) only changing the global scene information or depth, or
In this (2) manipulating an object of specific categories. paper, we present a 3D-aware image deformation method with minimal restrictions on shape category and deforma-tion type. While our framework leverages 2D-to-3D recon-struction, we argue that reconstruction is not sufficient for realistic deformations due to the vulnerability to topologi-cal errors. Thus, we propose to take a supervised learning-based approach to predict the shape Laplacian of the under-lying volume of a 3D reconstruction represented as a point cloud. Given the deformation energy calculated using the predicted shape Laplacian and user-defined deformation handles (e.g., keypoints), we obtain bounded biharmonic weights to model plausible handle-based image deforma-tion. In the experiments, we present our results of deforming 2D character and clothed human images. We also quanti-tatively show that our approach can produce more accurate deformation weights compared to alternative methods (i.e., mesh reconstruction and point cloud Laplacian methods). 1.

Introduction
The capability of photo editing, which had been con-fined in 2D space, has recently been popped out into 3D
* equal contributions, † corresponding author space. For example, predicting depth from a 2D image enables the composition of objects in an image [41, 57].
Object segmentation allows the projection of an image to a new view [27, 36, 52]. Lighting and ground plane estima-tion make it possible to relight objects and generate a new shadow in an image [9,14,15,54]. Such techniques for 3D-aware image editing have allowed the user to manipulate an image in a more intuitive manner — as if the object exists in 3D space — and opened new opportunities in downstream applications.
As a missing piece of the existing 3D-aware image ma-nipulation methods, we focus on 3D-aware image defor-mation. Unlike the aforementioned techniques, 3D-aware deformation does not just alter the scene information (e.g., camera parameters, lighting conditions) or modify the 2.5D information. Instead, it allows the user to directly manip-ulate the 3D geometry and appearance of an object. More relevant topics to 3D-aware deformation would be (1) hu-man pose transfer [6, 29, 33], which works only for human bodies, (2) novel view synthesis [13, 37, 51], which is lim-ited to altering a viewpoint of an image, and (3) 3D model-based manipulation [26], which requires the exact 3D model of the object in an image. To address these limitations, we aim to enable 3D-aware image deformation with minimal restrictions on shape category and deformation type.
For 3D-aware deformation, it is necessary to reconstruct the object in a 2D image to 3D space; however, it is not sufficient in general. Deformation requires either a surface or volume information [21, 48]. However, most of existing
methods of image-based 3D reconstruction do not directly output a surface or volume [8, 45] or produce a surface without proper consideration about intrinsic shape proper-ties [12, 23, 28, 39, 40, 46, 49, 53] – which can largely affect the deformation result. See an example in the red branch of
Figure 1. The inaccurate topological prediction connecting legs causes undesired visual artifacts in the deformation. In-deed, aiming for topological correctness in 3D reconstruc-tion is a difficult task due to its nature defined with both continuous and discrete quantities.
In this paper, given a 3D point cloud of the object in an input image (whose 2D-to-3D reconstruction is performed by PIFu [39]), we propose to enable 3D-aware image defor-mation through learning the additional intrinsic geometric property: the shape Laplacian. The shape Laplacian is the essential information encoding the geometry intrinsics. In particular, bounded biharmonic weights [21] – which has been widely used as a standard technique for deformation in computer graphics – compute the linear blending weights associated with deformation handles as the minimizers of the deformation energy defined using the shape Laplacian.
In our framework, we utilize the estimated shape Lapla-cian of a 3D reconstruction to obtain bounded biharmonic weights to plausibly model handle-based image deforma-tion.
To this end, we introduce a neural network that can pre-dict the shape Laplacian of the underlying volume of a 3D point cloud reconstructed from a 2D image — without di-rectly converting the point cloud to a volume. Considering that the deformation energy can be discretized with the stan-dard linear FEM Laplacian LM −1L (where L is a symmet-ric cotangent Laplacian matrix and M is a diagonal lumped mass matrix), we design our network to learn the matrices L and M −1 from the supervision obtained from a ground truth 3D mesh. The elements in the inverse mass matrix M −1 are predicted for each individual point, while the elements of the cotangent Laplacian matrix L are predicted by taking pairs of the input points. We use a symmetric feature aggre-gation function for such pairs and also a weight module to enforce the output matrix L to be symmetric and sparse. In test time, we recover the deformation energy from the pre-dicted L and M −1 to compute bounded biharmonic weights with user-specified deformation handles. Since our method learns the shape Laplacian instead of the handle-dependent deformation weights, it can generalize well to arbitrary han-dle configurations.
In the experiments, we show our results of 3D-aware de-formation on 2D character and clothed human images. We also showcase an user-interactive image editing scenario, where the user produces intuitive 3D deformations based on the specified control points. For quantitative evaluation, we test our method on a large-scale 3D point cloud dataset (i.e., DFAUST [4]), in which our method is shown to pro-duce more accurate deformation weights compared to the alternative methods on mesh reconstruction and point cloud
Laplacian.
Our main contributions can be summarized as follows:
• We propose a method for 3D-aware deformation of 2D images, which can be applied with minimal restrictions on shape category and deformation type.
• We introduce a novel network architecture that can learn the shape Laplacian with several desired prop-erties (i.e., positive semi-definiteness, symmetry and sparsity) from a 3D reconstruction. To the best of our knowledge, this is the first study to demonstrate that a learning-based approach can be effective in predict-ing the shape Laplacian of the underlying volume of a point cloud.
• We empirically demonstrate that our learning-based approach leads to more plausible deformations com-pared to the alternative cases of calculating the approx-imation of the shape Laplacian using mesh reconstruc-tion or point cloud Laplacian methods. 2.