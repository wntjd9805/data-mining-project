Abstract
It is a mystery which input features contribute to a neu-ral network’s output. Various explanation (feature attribu-tion) methods are proposed in the literature to shed light on the problem. One peculiar observation is that these ex-planations (attributions) point to different features as being important. The phenomenon raises the question, which ex-planation to trust? We propose a framework for evaluating the explanations using the neural network model itself. The framework leverages the network to generate input features that impose a particular behavior on the output. Using the generated features, we devise controlled experimental se-tups to evaluate whether an explanation method conforms to an axiom. Thus we propose an empirical framework for axiomatic evaluation of explanation methods. We evaluate well-known and promising explanation solutions using the proposed framework. The framework provides a toolset to reveal properties and drawbacks within existing and future explanation solutions.1 1.

Introduction
Considering a neural network function, how do we know which features (patterns) within the input are im-portant for its output? The problem is called feature at-tribution [16, 35], and the solutions are commonly known as explanation, attribution, or saliency methods. There is an extensive list of explanation methods in the literature
[8,9,13,15,16,19,26,27,29,32,33,35,39,41]. One peculiar observation is that these solutions point to different features as being important. Though they are solutions to the same problem, feature attribution, the resulting explanations are curiously dissimilar. The phenomenon raises the question, which explanation is correct? Or are the explanations cor-rect but revealing the problem in a different light?
*denotes equal contribution 1https : / / github . com / CAMP - eXplain - AI / Do -Explanations-Explain
One approach is to compare the explanations against ground truth (e.g., bounding box) annotations on the dataset
[27,38,41]. But how do we know what is important for a hu-man is also important for the model? There is no guarantee (or reason) that the model would use the same features as humans. To resolve this issue, we need to take a step back and ask what it means for a feature to be “important” for an output. The intuitive approach is to remove the feature and observe the output behavior [8, 11, 25]. Such removal of evidence is indeed the foundation of many explanation approaches [8, 9, 16, 26, 35]. However, such a conception could lead to ambiguities. Consider the scenario of hav-ing equivalent features (e.g., repeated features), where the existence of each feature alone sufﬁces for a speciﬁc out-put value. Add to the scenario that the removal of any of these features does not affect the output value. In this case, the conception based on removal assigns zero importance to each feature. However, a desirable property, in this case, could be assigning equal importance to each feature.
The concept of importance can thus be further chiseled by specifying desirable properties that an importance as-signment method ought to satisfy. Such desirable proper-ties are formalized via axioms [16, 34, 35]. The axiomatic view provides a complementary framework for evaluating feature attribution solutions. Explanation methods can be evaluated whether they conform to an axiom. The axiomatic view has the advantage that the methods can be mathemat-ically proven to comply with a particular axiom. For in-stance, solutions such as the Shapley value [16, 28] and in-tegrated gradients [34, 35] are proven to conform with par-ticular axioms. However, proofs can be broken in practi-cal implementations. For instance, [34] show that inherent assumptions within methods that approximate the Shapley value result in methods not conforming with the axioms.
Moreover, certain conditions might be overlooked in proofs.
Thus experiments are required to test whether ﬁnal solu-tions comply with the axioms. Even if methods are accom-panied by elegant and solid mathematical derivations and proofs, they must comply with the axioms in observations
in designed experiments. If they do not comply with ax-ioms in experiments, we may revisit our assumptions and methodologies. Such is the way of the scientiﬁc method.
This work lays out an experimental framework for eval-uating attribution solutions axiomatically. We set up each experiment such that the solution can be tested whether it complies with a speciﬁc axiom. We generate input fea-tures that impose a particular behavior on the network’s input/output relationship. Features are generated via opti-mization on the input space while the network parameters are kept constant. Using optimization, we can impose the desired relationship between the generated input and the output. We can thus engineer setups to evaluate axioms. For instance, one axiom that attribution methods are required to conform to is the Null-player axiom. The null-player axiom requires the following; If removal of a feature in all pos-sible coalitions with other features does not affect the out-put, it should be assigned zero importance. With our pro-posed framework, we can generate a null player feature for the neural network function. Subsequently, we can test dif-ferent feature attributions solutions and check whether they assign importance to the null player feature. Thus we can test whether a solution conforms to the Null-player axiom.
We also devise experiments to evaluate the explanations in terms of other desirable properties; The class-sensitivity and the feature-saturation. With our framework, we evaluate well-known and recently introduced promising solutions.
With our experiments, we intend to reveal properties and drawbacks within existing explanations. 2.