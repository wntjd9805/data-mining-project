Abstract posed method produces superior head swapping results on a variety of scenes.
The head swapping task aims at flawlessly placing a source head onto a target body, which is of great impor-tance to various entertainment scenarios. While face swap-ping has drawn much attention, the task of head swapping has rarely been explored, particularly under the few-shot setting. It is inherently challenging due to its unique needs in head modeling and background blending. In this paper, we present the Head Swapper (HeSer), which achieves few-shot head swapping in the wild through two delicately de-signed modules. Firstly, a Head2Head Aligner is devised to holistically migrate pose and expression information from the target to the source head by examining multi-scale in-formation. Secondly, to tackle the challenges of skin color variations and head-background mismatches in the swap-ping procedure, a Head2Scene Blender is introduced to si-multaneously modify facial skin color and fill mismatched gaps on the background around the head. Particularly, seamless blending is achieved with the help of a Semantic-Guided Color Reference Creation procedure and a Blend-ing UNet. Extensive experiments demonstrate that the pro-*Corresponding authors. 1.

Introduction
Human cognition of identity appearance is profoundly affected by not only facial structures but also head shapes and hairstyles. Head swapping, the ability to seamlessly replace the head in a target image with a source one (as shown in Fig. 1) would be of great importance to a variety of scenarios such as movie and advertisement composition, virtual humans creation, and deepfake video detection, etc.
While face swapping has long been a topic of inter-est [3,17,21,23], only a few studies have been carried out on the task of head swapping. DeepFaceLab [23] requires large manual intervention to generate head swapping results, and they are totally incapable of handling mismatched regions.
StylePoseGAN [24] tends to change the color of body skin and background in the target image in an undesired man-ner. Moreover, both methods fail to address few-shot head swapping, particualrly for in-the-wild scenes.
We identify several properties that make few-shot head swapping more challenging than face swapping: 1) Head
swapping requires not only perfect facial identity and ex-pression modeling, but also capturing the structural infor-mation of a whole head and the non-rigid hair. Thus previ-ous identity extraction strategies for face swapping [3, 21] cannot be directly applied to head swapping. 2) There would be a huge region mismatch between swapped head edges and backgrounds caused by the editing of head shapes and hairstyles. Such a problem does not exist in the face swapping setting. 3) Moreover, similar to face swapping, the color difference between source and target skins needs to be handled carefully.
In this paper, we propose a framework called Head
Swapper (HeSer), which generates high fidelity head swapping results in the wild based on a few frames. Our key insight is to positionally and emotionally align the source head with the target in a unified blender that seamlessly handles both color and background mismatches. Two mod-ules, namely the Head2Head Aligner and the Head2Scene
Blender are devised. The Head2Head Aligner is responsible for finding a latent representation of a whole head as well as the facial details. It aligns the source head to the same pose and expression as the target image in a holistic manner. The identity, expression, and pose information are prominently balanced in a style-based generator by encoding multi-scale local and global information from both images. Moreover, a subject-specific fine-tuning procedure could further im-prove identity preservation and pose consistency.
To further blend the aligned head into the target scene, we devise a module named Head2Scene Blender, which provides both 1) color guidance for facial skins and 2) padding priors for inpainting the gaps on the background around the head. Thus both the skin color and edge-backgrounds mismatches between source and target can be handled within one unified sub-module. It efficiently cre-ates colored references by building the correlations between pixels of the same semantic regions. Then with a blending
UNet, seamless and realistic head swapping results can be rendered.
We summarize our main contributions as follows: 1) We introduce a Head2Head Aligner that holistically migrates position and expression information from the target to the source head by examining multi-scale information. 2) We design a Head2Scene Blender to simultaneously handle fa-cial skin color and background texture mismatches. 3) Our proposed Head Swapper (HeSer) produces photo-realistic head swapping results on different scenes. To the best of our knowledge, this is one of the earliest methods to achieve few-shot head swapping in the wild. 2.