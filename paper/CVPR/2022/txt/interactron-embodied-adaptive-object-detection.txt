Abstract
Standard Detector
Remote
Table
?
?
Box
TV
?
Train Test
Painting
?
Bin
Cup
Over the years various methods have been proposed for the problem of object detection. Recently, we have wit-nessed great strides in this domain owing to the emer-gence of powerful deep neural networks. However, there are typically two main assumptions common among these approaches. First, the model is trained on a ﬁxed training set and is evaluated on a pre-recorded test set. Second, the model is kept frozen after the training phase, so no further updates are performed after the training is ﬁnished. These two assumptions limit the applicability of these methods to real-world settings. In this paper, we propose Interactron, a method for adaptive object detection in an interactive set-ting, where the goal is to perform object detection in images observed by an embodied agent navigating in different envi-ronments. Our idea is to continue training during inference and adapt the model at test time without any explicit super-vision via interacting with the environment. Our adaptive object detection model provides a 11.8 point improvement in
AP (and 19.1 points in AP50) over DETR [5], a recent, high-performance object detector. Moreover, we show that our object detection model adapts to environments with com-pletely different appearance characteristics, and its per-formance is on par with a model trained with full super-vision for those environments. The code is available at: https://github.com/allenai/interactron. 1.

Introduction
Object detection has been a central problem in computer vision since the inception of the ﬁeld. There has been an extensive literature over the past decades proposing vari-ous methods ranging from constellation [13,14,17], region-based [21, 49, 50], and hierarchical [24, 44, 60] models to the more recent powerful CNN [18,19,42] and Transformer
[5, 6, 61] based models to tackle this problem. Typically, there are two main assumptions in these works: (1) There is a ﬁxed training set and a test set. (2) The model is frozen after the training stage (i.e., it cannot be updated) and is evaluated on the pre-deﬁned test set.
These assumptions pose certain limitations for object de-Detections
Detections
Detections
Detections
Object 
Detector
Object 
Detector
Object 
Detector
Object 
Detector
Interactron
TV
Fridge
Box
Remote
Detections
Cup
Detections
Object 
Detector
Adaptive Feedback
Object 
Detector
Adaptive Feedback
Training Continues at Test Time
Figure 1. We introduce INTERACTRON a novel method for object detection. The idea is to adapt the detection model in an interactive setting during inference without any explicit supervision. The top row shows a standard detector that is kept frozen during inference.
The bottom row shows our model that is updated during inference without any supervision by using future observations. tection in real world applications. First, in many applica-tions (e.g., autonomous driving or home assistant robots), the model continuously receives new observations from the environment. The new observations might help the model correct its belief. For example, a partially occluded ob-ject might not be detected conﬁdently in the current frame, but there might be a better (unoccluded) view of that ob-ject in later observations. The model should use this sig-nal to improve its conﬁdence in similar situations in the fu-ture. Second, freezing the weights after training inhibits further improvement and adaptation of the model. We be-lieve there are strong self-supervised signals in the infer-ence phase that an embodied agent can leverage via inter-acting with its environment to adapt the model. There has been work to adapt object detector in an unsupervised way (e.g., [10,46,51,54]). However, they assume a pre-recorded set of observations during inference.
The idea of the proposed method is to continue train-ing during inference while interacting with an environment.
Our hypothesis is that interacting with the environment en-ables the embodied agent to capture better observations dur-ing inference leading to better adaptation and higher perfor-mance. In stark contrast to common object detection works, there is no distinct boundary between training and inference phases, and the model learns to take actions and adapt with-out any explicit supervision during inference. More specif-ically, there is an agent that interacts within indoor envi-ronments and relies on an object detector trained fully su-pervised to recognize objects. Our goal is to improve the object detection performance by adapting the model during inference while the agent interacts with the environment ac-cording to a learned policy (Figure 1). During training, the agent learns a loss function using the supervised data, i.e. it learns to mimic the gradients produced during training using the labeled data. During inference, there is no super-vision available for object detection. However, the model can generate gradients for the input images. Therefore, the model can be updated at inference time using the generated gradients. Basically, the model is updated without any ex-plicit supervision at test time.
We evaluate our adaptive object detection model, re-ferred to as Interactron1, using the AI2-THOR [26] frame-work which includes 125 different object categories that ap-pear in 120 indoor scenes. The task is to detect objects in all frames that the agent observes while navigating in a scene. Our experiments show that by learning to adapt, the DETR [5] model, which is a recent, high-performance object detection model, improves by 11.8 points in mAP.
In addition to this strong result, we show that our adaptive model trained on AI2-THOR achieves on par results with a model trained with full supervision for the Habitat [43] framework, which includes scenes with completely differ-ent appearance characteristics.
In summary, we propose an embodied adaptive object detection approach, where the network is updated during both training and inference. This approach is in contrast to the traditional object detection frameworks, where the net-work is frozen after training. The model learns to adapt dur-ing inference via interaction with the environment and with-out any explicit supervision. We show our model signiﬁ-cantly outperforms strong non-adaptive baselines and gen-eralizes well to environments with different appearance dis-tributions. 2.