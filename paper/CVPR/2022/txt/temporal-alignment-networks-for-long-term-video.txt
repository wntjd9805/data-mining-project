Abstract
The objective of this paper is a temporal alignment net-work that ingests long term video sequences, and associ-ated text sentences, in order to: (1) determine if a sentence is alignable with the video; and (2) if it is alignable, then determine its alignment. The challenge is to train such networks from large-scale datasets, such as HowTo100M, where the associated text sentences have significant noise, and are only weakly aligned when relevant.
Apart from proposing the alignment network, we also make four contributions: (i) we describe a novel co-training method that enables to denoise and train on raw instructional videos without using manual annotation, de-spite the considerable noise; (ii) to benchmark the align-ment performance, we manually curate a 10-hour subset of
HowTo100M, totalling 80 videos, with sparse temporal de-scriptions. Our proposed model, trained on HowTo100M, outperforms strong baselines (CLIP, MIL-NCE) on this alignment dataset by a significant margin; (iii) we ap-ply the trained model in the zero-shot settings to mul-tiple downstream video understanding tasks and achieve including text-video retrieval on state-of-the-art results,
YouCook2, and weakly supervised video action segmenta-tion on Breakfast-Action. (iv) we use the automatically-aligned HowTo100M annotations for end-to-end finetuning of the backbone model, and obtain improved performance on downstream action recognition tasks. 1.

Introduction
The recent CLIP and ALIGN papers [30, 53] have demonstrated that a combination of large scale paired image-caption data, and a simple noise contrastive learning loss can be used to learn powerful image-text embeddings from scratch. The image-caption data can be crawled from the internet at scale, for example from image alt-text, and the resulting embeddings demonstrate strong “zero-shot” generalization abilities.
In the video domain, there also exists large-scale sources of text supervision, e.g. narrated instructional videos such as the HowTo100M [47] dataset, where demonstrators explain their actions while perform-ing a complex task. The narrations are unconstrained and can be combinatorially complex, including information on
“what”, “where” and “when”, such as the actions, the ob-jects, human-object interactions, etc.
However, these instructional videos pose additional fun-damental challenges over the image-caption scenario due to the temporal alignment problem (illustrated in Figure 1): (i) the demonstrator often makes statements that are unre-lated to the visual signal, such as describing food taste or explaining the consequence of actions. These texts are not visually alignable. (ii) the demonstrator might explain their action before or after performing it, and their statements of-ten do not follow the same order as their actions, resulting in the text and visual entities being asynchronous. These texts are not temporally aligned to the visual signal. Addition-ally, unlike spatial segmentation in images, where objects boundaries are often formed by a discontinuity between re-gions with strong gradients, temporal actions in videos are often continuous, making it difficult to clearly define the start and end points for the temporal interval. Last but not the least, there is additional noise coming from the imper-fect Automatic Speech Recognition (ASR) systems on the spoken narrations. Note that the image-caption data does not face these problems since captions are provided by hu-man annotators for that image; although they may be in-complete, there is no temporal alignment issue.
The extent of these alignment challenges is signifi-cant [46, 47]. In 10 hours of instructional videos (sourced from HowTo100M) that we annotated for this work, only 30% of the narration sentences are visually alignable, and only 15% are naturally well-aligned. This means that the demonstrator is describing their action synchronously with the video only 15% of the time. If the alignment issues are resolved then the benefits of learning from such narrated in-struction videos can potentially be substantial: with the ex-tra time axis alignment, models can be trained to deal with fine-grained tasks, and predict temporal action localization and segmentation.
In this paper, we tackle the sentence-to-video tempo-ral alignment problem, and propose a Temporal Alignment
Network (TAN) that ingests a video sequence and its asso-ciated narrative sentences, attends to a large temporal con-Figure 1: An example of visual-textual mis-alignment in a raw instructional video. The presenter’s narration can be not visually relevant at all, e.g. describ-ing a flavor; or asynchronous with visual content by a time difference. The ✓ and ✗ indicate visually alignable and non-alignable text, respectively (by human judgement). The colored bar shows the start-end timestamp of narration. Example from https://www.youtube.com/watch?v=M8OGXmLTTiI?t=30. text in both, and is able to: (1) determine if a sentence is alignable with the video; and (2) if it is alignable, then de-termine its temporal alignment. Given all the challenges described above, training such a network on raw instruc-tional videos, e.g., HowTo100M, is clearly a non-trivial task. To this end, we propose a novel method for denois-ing, by co-training TAN with an auxiliary dual encoder net-work. By design, these two networks use complementary architectures: TAN iteratively attends to temporal context from both visual and textual modalities, establishing accu-rate alignment for sentences that are alignable; while the dual encoder processes visual and textual modalities inde-pendently, which enables it to spot unalignable sentences at ease, e.g., sentences that emit low alignment score to all frames within the video. The output from these two net-works can be treated as two different views for alignment, and their mutual agreements are adopted for co-training.
In addition to introducing the model and training methodology, we make the following contributions: (1)
We manually annotate an 80-video subset of HowTo100M, named HTM-Align, by assigning the visually related sen-tences to their corresponding timestamps and annotating vi-sually unrelated ones. This aligned subset is used to eval-uate the model’s performance and is released publicly; (2)
We train the model on the HowTo100M dataset, and demon-strate a significant improvement in alignment over prior work (MIL-NCE approach of [46] in particular); (3) We ap-ply the trained model in both the zero-shot and fine-tuned settings to multiple downstream video tasks and achieve state of the art results on both settings. This includes text-video retrieval on YouCook2 [75] and weakly supervised video action segmentation on Breakfast-Action [34]; (4) We use the automatically-aligned HowTo100M annotations to finetune the backbone model, and observe improved perfor-mance on downstream action classification tasks. 2.