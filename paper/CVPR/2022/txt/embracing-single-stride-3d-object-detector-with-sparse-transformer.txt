Abstract
In LiDAR-based 3D object detection for autonomous driving, the ratio of the object size to input scene size is sig-niﬁcantly smaller compared to 2D detection cases. Over-looking this difference, many 3D detectors directly follow the common practice of 2D detectors, which downsample the feature maps even after quantizing the point clouds.
In this paper, we start by rethinking how such multi-stride stereotype affects the LiDAR-based 3D object detectors.
Our experiments point out that the downsampling opera-tions bring few advantages, and lead to inevitable infor-mation loss. To remedy this issue, we propose Single-stride Sparse Transformer (SST) to maintain the original resolution from the beginning to the end of the network.
Armed with transformers, our method addresses the prob-lem of insufﬁcient receptive ﬁeld in single-stride architec-tures.
It also cooperates well with the sparsity of point clouds and naturally avoids expensive computation. Even-tually, our SST achieves state-of-the-art results on the large-scale Waymo Open Dataset. It is worth mentioning that our method can achieve exciting performance (83.8 LEVEL 1
AP on validation split) on small object (pedestrian) detec-tion due to the characteristic of single stride. Our codes will be public soon. 1.

Introduction
LiDAR-based 3D object detection for autonomous driv-ing has been beneﬁting from the progress of image-based object detection. The mainstream 3D detectors quantize the 3D space into a stack of pseudo-images from Bird (cid:0) Corresponding author.
Figure 1. Compared with previous multi-stride 3D detectors, our model is single-stride and operates sparsely on the non-empty vox-els. We paint the vehicle bounding boxes on the input point cloud to show the tiny object size compared to the input scene size.
Eye’s View (BEV), which makes it convenient to bor-row advanced techniques from the 2D counterparts. Many works [13, 19, 57, 60] are proposed under this paradigm and achieve competitive performance. However, 3D and 2D spaces have intrinsic distinction in their relative object scales, where the objects in 3D spaces have much smaller relative sizes (See Fig. 2). For example, in Waymo Open
Dataset [48], the perception range is usually 150m × 150m, while a vehicle is only about 4m long, even a pedestrian
occupies as little as 1m in length. Such a tiny pedestrian equivalently translates to an object of size 8 × 8 pixels in a 1200×1200 image, suggesting that object detection on such a tiny scale is one of the challenges in 3D object detection.
Different from the above challenge of small scales in the 3D space, 2D detectors have to consider the handling of the objects with varied scales. It is observed in Fig. 2 that the scales of objects in 2D images exhibit a long-tail distri-bution, while in 3D space they are quite concentrated due to the non-projective transformation used in voxelization.
To handle the varied scales, 2D detectors [23, 25, 46, 47] usually build multi-scale features with a series of down-sampling and upsampling operations. Such multi-scale ar-chitecture is also widely inherited in 3D detectors (See
Fig. 1) [13, 19, 57, 60, 65]. Since the object size in 3D ob-ject detectors is usually tiny while no large objects exist, a question naturally arises: do we really need downsampling in 3D object detectors ?
With this question in mind, we make an exploratory at-tempt on the single-stride architecture with no downsam-pling operators. The single-stride network maintains the original resolution throughout the network. However, it is challenging to make such a design feasible. The discard of downsampling operators leads to two issues: 1) the increase of computation cost; 2) the decrease of receptive ﬁeld. The former constrains the applicability to the real-time system and the latter hinders the capability of object recognition.
For the issue of computation, sparse convolution seems to be a solution, but the sparse connectivity between voxels2 makes the decrease of receptive ﬁeld even more severe (See
Table 7). For the issue of receptive ﬁeld, we experimentally show that some commonly adopted techniques do not meet our needs (See Table 1): the dilated convolution [5, 61] is not friendly to small objects, and the larger kernel leads to unaffordable computational overhead in the single stride ar-chitecture. Therefore, we are getting into a dilemma, where it is difﬁcult to design a convolutional network simultane-ously satisfying the three aspects: single stride architecture, sufﬁcient receptive ﬁeld, and acceptable computation cost.
These difﬁculties naturally lead us to think out of the paradigm of CNN, and the attention mechanism emerges as a better option because of the following two reasons: 1)
The attention-based model is better at capturing large con-text and build sufﬁcient receptive ﬁeld. 2) Due to the capa-bility of modeling dynamic data, the attention-based model
ﬁts well into the sparse voxelized representation of point clouds, where only a small portion of voxels are occupied.
This property guarantees the efﬁciency of our single stride network. Although the attention mechanism is efﬁcient on sparse data, computing attentions on a global scale is still unaffordable and undesirable. So we partition the voxelized 3D space into many local regions and apply self-attention 2We provide a clear illustration for this in our supplementary materials.
Figure 2. Distribution of the relative object size Srel in COCO dataset [27] and Waymo Open Dataset (WOD). Srel is deﬁned as (cid:112)Ao/As, where Ao denotes the area of 2D objects (COCO) and the BEV area of 3D objects (WOD). As is the image area in
COCO, and 150m × 150m in WOD. In COCO 73.03% objects in
COCO have a Srel larger than 0.04, while only 0.54% objects in
WOD have a Srel larger than 0.04. inside each of them. Eventually, this local attention mecha-nism, named as Sparse Regional Attention (SRA), enjoys the best of two worlds. By stacking SRA layers, we make the single-stride network feasible and obtain a transformer-style network, called Single-stride Sparse Transformer (SST). Extensive experiments are conducted on the large-scale Waymo Open Dataset [48]. We summarize our con-tributions as follows:
• We rethink the architecture of current mainstream
LiDAR-based 3D detectors. With pilot experiments, we point out that the network stride is an overlooked design factor for LiDAR-based 3D detectors.
• We propose the Single-stride Sparse Transformer (SST). With its local attention mechanism and capa-bility of handling sparse data, we overcome receptive
ﬁeld shrinkage in the single-stride setting and avoid heavy computational overhead.
• Our method achieves state-of-the-art performance on the large-scale Waymo Open Dataset. Thanks to the characteristic of single stride, our method obtains ex-citing results on tiny objects like pedestrians (83.8
LEVEL 1 AP on the validation split). 2.