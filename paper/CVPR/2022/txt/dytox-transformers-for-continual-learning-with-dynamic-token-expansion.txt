Abstract
Deep network architectures struggle to continually learn new tasks without forgetting the previous tasks. A recent trend indicates that dynamic architectures based on an ex-pansion of the parameters can reduce catastrophic forget-ting efﬁciently in continual learning. However, existing ap-proaches often require a task identiﬁer at test-time, need complex tuning to balance the growing number of param-eters, and barely share any information across tasks. As a result, they struggle to scale to a large number of tasks with-out signiﬁcant overhead.
In this paper, we propose a transformer architecture based on a dedicated encoder/decoder framework. Critically, the encoder and decoder are shared among all tasks. Through a dynamic expansion of special tokens, we specialize each forward of our decoder network on a task distribution. Our strategy scales to a large number of tasks while having neg-ligible memory and time overheads due to strict control of the expansion of the parameters. Moreover, this efﬁcient strategy doesn’t need any hyperparameter tuning to control the network’s expansion. Our model reaches excellent re-sults on CIFAR100 and state-of-the-art performances on the large-scale ImageNet100 and ImageNet1000 while having fewer parameters than concurrent dynamic frameworks.1 1.

Introduction
Most of the deep learning literature focuses on learning a model on a ﬁxed dataset. However, real-world data con-stantly evolve through time, leading to ever-changing dis-i.e., new classes or domains appeared. When tributions: a model loses access to previous classes data (e.g., for pri-vacy reasons) and is ﬁne-tuned on new classes data, it catas-trophically forgets the old distribution. Continual learning models aim at balancing a rigidity/plasticity trade-off where old data are not forgotten (rigidity to changes) while learn-ing new incoming data (plasticity to adapt). Despite recent 1Code is released at https://github.com/arthurdouillard/dytox
Figure 1: DyTox’s continual learning performance on
ImageNet1000: for each task, 100 new classes are learned while previously learned classes are not fully accessible but shouldn’t be forgotten. Our strategy DyTox (in red) is state-of-the-art by a large margin. Note that at the initial step be-fore the continual process begins (denoted by a dashed rect-angle
), our model has performance comparable to other baselines: the performance gain is achieved by reducing catastrophic forgetting. Moreover, we have systematically fewer parameters than previous approaches. advances, it is still an open challenge.
A growing amount of efforts have emerged to tackle catastrophic forgetting [49, 34, 63, 29, 18, 64]. Recent works [65, 39, 30, 21, 24, 54] dynamically expand the net-work architectures [65, 39] or re-arrange their internal struc-tures [21, 54, 30, 24]. Unfortunately at test-time, they re-quire to know the task to which the test sample belongs — in order to know which parameters should be used. More re-cently, DER [64] and Simple-DER [41] discarded the need for this task identiﬁer by learning a single classiﬁer on the concatenation of all produced embeddings by different sub-1
sets of parameters. Yet, these strategies induce dramatic memory overhead when tackling a large number of tasks, and thus need complex pruning as post-processing.
To improve the ease of use of continual learning frame-works for real-world applications, we aim to design a dy-namically expandable representation (almost) ‘for free’ by having the three following properties: #1 limited memory overhead as the number of tasks grows, #2 limited time overhead at test time and #3 no setting-speciﬁc hyper-parameters for improved robustness when faced to an un-known (potentially large) number of tasks.
To this end, we leverage the computer vision trans-former ViT [15]. Transformers [60] offer a very interest-ing framework to satisfy the previously mentioned con-straints. Indeed, we build upon this architecture to design a encoder/decoder strategy: the encoder layers are shared among all members of our dynamic network; the unique de-coder layer is also shared but its forward pass is specialized by a task-speciﬁc learned token to produce task-speciﬁc embeddings. Thus, the memory growth of the dynamic net-work is extremely limited: only a 384d vector per task, val-idating property #1. Moreover, this requires no hyperpa-rameter tuning (property #3). Finally, the decoder is explic-itly designed to be computationally lightweight (satisfying property #2). We nicknamed our framework, DyTox, for
DYnamic TOken eXpansion. To the best of our knowl-edge, we are the ﬁrst to apply the transformer architecture to continual computer vision.
Our strategy is robust to different settings, and can easily scale to a large number of tasks. In particular, we validate the efﬁciency of our approach on CIFAR100, ImageNet100, and ImageNet1000 (displayed on Fig. 1) for multiple set-tings. We reach state-of-the-art results, with only a small overhead thanks to our efﬁcient dynamic strategy. 2.