Abstract
Universal domain adaptation (UniDA) aims to transfer the knowledge learned from a label-rich source domain to a label-scarce target domain without any constraints on the label space. However, domain shift and category shift make UniDA extremely challenging, which mainly lies in how to recognize both shared “known” samples and pri-vate “unknown” samples. Previous works rarely explore the intrinsic geometrical relationship between the two do-mains, and they manually set a threshold for the overconfi-dent closed-world classifier to reject “unknown” samples.
Therefore, in this paper, we propose a Geometric anchor-guided Adversarial and conTrastive learning framework with uncErtainty modeling called GATE to alleviate these issues. Specifically, we first develop a random walk-based anchor mining strategy together with a high-order atten-tion mechanism to build correspondence across domains.
Then a global joint local domain alignment paradigm is de-signed, i.e., geometric adversarial learning for global dis-tribution calibration and subgraph-level contrastive learn-ing for local region aggregation. Toward accurate target private samples detection, GATE introduces a universal incremental classifier by modeling the energy uncertainty.
We further efficiently generate novel categories by mani-fold mixup, and minimize the open-set entropy to learn the
“unknown” threshold adaptively. Extensive experiments on three benchmarks demonstrate that GATE significantly out-performs previous state-of-the-art UniDA methods. 1.

Introduction
Deep neural networks have achieved impressive progress in image recognition tasks given abundant labeled data, however, they do not generalize well to novel unlabeled do-main [28]. Annotating massive data from various domains is an expensive and time-consuming task, and these do-mains often sample from different distributions against each other. Such so-called domain shift would degenerate the
*This work is completed in Huawei Technologies.
†Corresponding Author. model performance heavily [15]. To alleviate this issue, do-main adaptation (DA) transfers knowledge from the label-rich source domain to the label-scarce target domain by di-minishing the feature discrepancy across two domains [10].
Suppose that the label spaces of two domains are Cs and Ct, respectively, traditional unsupervised DA usually assumes
Cs = Ct, i.e., close-set DA (CDA) [38]. In complex real-world scenarios, however, this assumption may not be eas-ily satisfied. There are several situations we may encounter:
Ct ⊂ Cs, i.e., partial DA (PDA) [2]; Cs ⊂ Ct, i.e., open-set DA (ODA) [29]; Cs ∩ Ct ̸= ∅, Cs ∪ Ct ̸= Cs or Ct, i.e., open-partial DA (OPDA) [16]. Although these variants have recently been resolved independently, the downside is that a method that is applicable to one variant may not be ap-plicable to another. More difficultly, we cannot know which of these variants will occur in advance.
To better tackle these general situations, universal DA (UniDA) is raised to account for both domain shift and cat-egory shift, allowing two domains to own their private cat-egories [44]. It assumes that we have no prior knowledge about their label space differences, thus being very prac-tical in the real world. In UniDA, we aim to classify the target samples into either one of the “known” labels or the
“unknown” label. However, there exist two main techni-cal challenges in UniDA. First, we should constrain the do-main bias removal into the common categories between two domains, and need to separate their respective private cat-egories simultaneously. Second, we need to detect the po-tential “unknown” samples in the target domain without any target label supervision or other prior information. These two challenges are complementary and mutually constrain-ing, since better removal of feature discrepancy can help identify the “unknown” sample, whereas, in turn, the model can help separate the common and private categories once it learns the concept of “unknown”.
Recently, several efforts have been devoted to settling the UniDA task. For the first challenge, UAN [44] exploits both the domain similarity and the prediction uncertainty of each sample to incorporate a weighting mechanism into the adversarial network for promoting common-class adap-tation. DANCE [32] moves each target sample either to a
source “known” class prototype or to its target neighbors, and uses an entropy separation loss to encourage domain alignment. These methods rarely explore the intrinsic struc-tural relationship between the two domains in the geometric manifold space, making it difficult to achieve fine-grained class-specific alignment and separation. For the second challenge, existing methods like CMU [9] manually set a threshold for confidence or entropy score outputted by the closed-world classifier to reject “unknown” samples. How-ever, the class competition nature may cause the neural net-work to generate overconfident predictions for “unknown” instances, making the threshold hard to tune [43]. Besides, since the semantic information of “known” categories varies in different domains and tasks, it is hard to acquire an opti-mal threshold that suits all open-set recognition tasks [47].
To address these two challenges, here we propose a Geometric anchor-guided Adversarial and conTrastive learning framework with uncErtainty modeling called
GATE for UniDA task. First, we retrieve the mutual nearest neighbors (MNN) intra- and inter-domains as the geometric anchors to build the intrinsic structure correspondence be-tween two domains. To better uncover the complete distri-bution of samples from the common categories, we develop a novel within-domain random walk-based strategy to ex-pand the MNN anchor pair list. We also introduce a shared nearest neighbor (SNN) scoring mechanism to minimize the influence of incorrectly constructed anchors. Based on this fine-grained affinity knowledge across domains, we design a multi-anchor-constrained geometric adversarial learning paradigm for global distribution alignment, and propose a subgraph-level cross-domain contrastive learning objective to facilitate local region aggregation. Such a global joint local learning strategy better realizes the matching of com-mon categories and the separation of private categories.
Second, to enable the model known “unknown”, we transform the closed-world classifier into a universal formu-lation by adding extra dimension to model the energy-based uncertainty. Such an augmented classifier component also stands for a learnable threshold between “known” and “un-known”. To efficiently anticipate the distribution of novel categories in training, we consider generating synthesized instances by manifold mixup technique, which mimics the target private categories with a limited computation cost.
In this way, the open-set classification has in turn changed back to a generalized closed-set classification, thus adap-tively estimating the “unknown” threshold. We also pro-pose to apply open-set entropy minimization for unlabeled target samples, allowing the model to align them to either
“known” or “unknown” categories. Given this insight, the idea of our method is simple yet powerful.
Our contribution can be summarized as follows:
• We propose a novel UniDA framework called GATE based on geometric anchor mining and uncertainty modeling. GATE designs a global joint local domain alignment strategy, i.e., geometric adversarial learning for global distribution calibration and subgraph-level contrastive learning for local region aggregation.
• We propose an energy-based universal classification paradigm that learns the “unknown” threshold adap-tively. The open-set entropy minimization allows us to label the “known” samples and detect “unknown” samples in the target domain.
• We conduct extensive experiments on various UniDA benchmarks, and the empirical results show that GATE outperforms other state-of-the-art UniDA methods.
Deeper analyses validate the effectiveness of individ-ual components proposed in GATE. 2.