Abstract
This paper proposes using a sparse-structured multivari-ate Gaussian to provide a closed-form approximator for the output of probabilistic ensemble models used for dense im-age prediction tasks. This is achieved through a convolu-tional neural network that predicts the mean and covari-ance of the distribution, where the inverse covariance is parameterised by a sparsely structured Cholesky matrix.
Similarly to distillation approaches, our single network is trained to maximise the probability of samples from pre-trained probabilistic models, in this work we use a fixed en-semble of networks. Once trained, our compact represen-tation can be used to efficiently draw spatially correlated samples from the approximated output distribution. Impor-tantly, this approach captures the uncertainty and struc-tured correlations in the predictions explicitly in a formal distribution, rather than implicitly through sampling alone.
This allows direct introspection of the model, enabling vi-sualisation of the learned structure. Moreover, this formu-lation provides two further benefits: estimation of a sample probability, and the introduction of arbitrary spatial condi-tioning at test time. We demonstrate the merits of our ap-proach on monocular depth estimation and show that the advantages of our approach are obtained with comparable quantitative performance. 1.

Introduction
Single prediction neural networks are ubiquitous in com-puter vision and have demonstrated extensive capability for a variety of tasks. However, researchers are increasingly interested in capturing the uncertainty in estimation tasks to combat over-confidence and ambiguity; such concerns are important when building robust systems that connect computer vision approaches to down-stream applications.
The deployment of neural networks for safety-critical tasks, such as autonomous driving, requires an accurate measure of uncertainty. While Bayesian Neural Networks [17] are often a model of choice for uncertainty estimation, ensem-bles [14] have been proposed as a simple alternative. Em-Figure 1. Our method is trained to approximate the output of an ensemble, by using structured uncertainty prediction networks (SUPN) to predict a mean and covariance for a multivariate Gaus-sian distribution. This explicit distribution enables a variety of tasks including: sampling, conditioning and model introspection. pirically, ensembles have been shown to produce good mea-sures of uncertainty for vision tasks [15,19] and allow prac-titioners to exploit associated application-specific inductive biases, for example established architectures, directly.
Limitations of implicit approaches Despite their pop-ularity, ensembles have a number of drawbacks that we group into three themes. Firstly, they come at an increased cost compared with deterministic networks. At training time, they require training multiple deep models, while at test time, multiple inference passes are required. MC-dropout [5] saves computation at training time, but it still requires multiple passes at inference time. Secondly, these approaches only provide an implicit distribution over prob-able model outputs. Any uncertainty captured is only acces-sible through ancestral sampling. Accordingly, one cannot
calculate conditional samples, or assess the likelihood of a new sample given the learned model. Finally, and of in-creasing importance to the community, introspection of the trained models is very difficult.
When combining computer vision with larger systems, there is virtue to summarising the posterior distribution in a formal and compact form that can be visualised and ap-propriately used to inform downstream tasks. The computa-tional challenges have prompted work on producing a single model to approximate the output of an ensemble; so called
“ensemble distillation” [2, 15, 18, 21].
Lack of structure in distillation methods Previous meth-ods focus on: classification problems [15, 18], approximat-ing only the mean of the ensemble [2], or modelling inde-pendent per-pixel variance [21]. In contrast, while we also adopt a single model to reduce the computational cost, we propose to learn a model that approximates the ensemble by formally capturing structure in the output space; this is more appropriate for dense prediction tasks. When mak-ing per-pixel predictions, it is common to use models that capture spatial correlation in the output space. In particular, models such as Markov or Conditional Random Fields [20], which capture correlations between neighbouring pixels, have been extensively used in computer vision. However, capturing the structure of the output space is less explored in the context of modelling uncertainty. Previous work has focused on per-pixel heteroscedastic uncertainty, by using a
Gaussian [11, 21] or Laplace [13] likelihood model with di-agonal covariance. Since these models do not capture corre-lations between pixels, samples suffer from salt-and-pepper (independent) noise.
Capturing structure explicitly Previously, adopting per-pixel uncertainty representations usually follows from the expectation that direct estimation of a full covariance struc-ture is intractable in both storage O(pixels2) and com-putation O(pixels3). Recently, however, Dorta et al. [4] introduced Structured Uncertainty Pprediction Networks (SUPN) for generative models. The paper extended a Vari-ational Auto Encoder (VAE) [12] with a likelihood model that is Gaussian with a full covariance matrix. The authors show how this can be predicted efficiently by using a sparse approximation of the Cholesky decomposition of the preci-sion matrix. Working in the domain of the precision allows a dense covariance structure to be obtained whilst also re-specting our prior that long range structure is derived from the propagation of local image statistics. By encoding a full covariance matrix, the samples obtained from such a model capture these long range correlations in the image domain and are free from salt-and-pepper (independent) noise.
Contributions In this work, we build on SUPN [4] and show that a deep network can be trained in a regression setting to predict a structured Gaussian distribution that approximates the output distribution of methods that cap-ture model uncertainty, such as ensembles [14] and MC-dropout [5]. We introduce a novel efficient approach to drawing conditioned or unconditioned samples from a structured multivariate Gaussian distribution with a sparsely structured precision matrix. By taking full advantage of the closed form nature of the Gaussian distribution, our method allows introspection and enables conditioning at test time, which proves cumbersome for other methods. Importantly, our approach is not limited to Gaussian likelihoods over the prediction space (see § 3.4).
Evaluation We demonstrate the efficacy of our method for the task of depth estimation. Experiments show that the new advantages can be obtained without sacrificing quanti-tative performance, with results comparable to the original ensemble; we consider metrics over both accuracy and the capture of uncertainty. The samples are found to follow the ensembles without being limited in the number that can be drawn. The compact representation is capable of encod-ing a rich distribution with only a modest increase in com-putation over a single deterministic network. Furthermore, we demonstrate using our explicit representation to perform conditional sampling and illustrate the ability to inspect the model and visualise the correlation structure learned. 2.