Abstract
Since facial actions such as lip movements contain sig-nificant information about speech content, it is not sur-prising that audio-visual speech enhancement methods are more accurate than their audio-only counterparts. Yet, state-of-the-art approaches still struggle to generate clean, realistic speech without noise artifacts and unnatural dis-tortions in challenging acoustic environments. In this pa-per, we propose a novel audio-visual speech enhancement framework for high-fidelity telecommunications in AR/VR.
Our approach leverages audio-visual speech cues to gen-erate the codes of a neural speech codec, enabling effi-cient synthesis of clean, realistic speech from noisy signals.
Given the importance of speaker-specific cues in speech, we focus on developing personalized models that work well for individual speakers. We demonstrate the efficacy of our ap-proach on a new audio-visual speech dataset collected in an unconstrained, large vocabulary setting, as well as existing audio-visual datasets, outperforming speech enhancement baselines on both quantitative metrics and human evalua-tion studies. Please see the supplemental video for qualita-tive results1. 1.

Introduction
Humans have the remarkable ability to extract speech content from visual information such as lip movement.
Studies show that viewing speakers’ faces improves human listening in noisy environments [41, 57], and that individu-als naturally learn to read lip movements when their hear-ing is impaired [24]. Inspired by these observations, audio-visual speech enhancement methods leverage the visual in-put of a speaker to isolate their voice in a noisy environment
[35]. By integrating facial frames of a target speaker with a noisy audio spectrogram, for example, recent deep learn-ing models can generate a mask for the spectrogram that suppresses irrelevant voices and background sounds from 1https://github.com/facebookresearch/facestar/ releases/download/paper_materials/video.mp4
Figure 1. Audio-Visual Speech Codecs. Our model performs speech enhancement by leveraging audio-visual speech cues to synthesize the discrete codes of a neural speech codec. (a) During training, we first learn a codebook of natural speech for a target speaker by training a neural speech codec to compress and decode their clean speech signal. We then train an auto-regressive proba-bilistic model over the codes conditioned on noisy audio and visual inputs. (b) During inference, we use the auto-regressive model to generate a sequence of speech codes, which are then synthesized into speech using the decoder module of the speech codec. the output [1, 11, 18]. These models prove useful for reduc-ing noise and improving speech intelligibility of videos for downstream applications.
However, there are a growing number of telecommuni-cations applications where the quality and realism of the output speech, beyond speech intelligibility, are paramount.
One example is social telepresence in AR/VR, which aims to enable realistic face-to-face conversations between peo-ple in a virtual setting [34, 62]. Immersive virtual conver-sations require extremely high-quality speech signals: each speaker’s voice must sound clean and realistic when ren-dered in the virtual environment, as if a real conversation were taking place there. Current state-of-the-art methods fall short of these applications for two main reasons. First,
they generate speech by using the noisy audio as a template
[1,11,18] rather than by explicitly modeling the distribution of speech, which can lead to bleed-through noise and other unnatural distortions that disrupt the sense of immersion.
Second, they focus on learning audio-visual speech cues that generalize well across a large population, but that may fail to capture speaker-specific cues needed for a higher-fidelity model [45].
Main Contributions. In this work, we take a different ap-proach from existing work that overcomes these two limita-tions. Our main contributions are the following: (1) We propose audio-visual (AV) speech codecs, a novel framework for AV speech enhancement. Rather than us-ing noisy audio input as a template for producing enhanced output, AV speech codecs explicitly model the distribution of speech and re-synthesize clean speech conditioned on audio-visual cues. Our approach is summarized in Figure 1. During training, we first learn the building blocks of natural speech by training a neural speech codec to com-press and decode clean speech signal through a discrete codebook. Subsequently, we learn an auto-regressive prob-abilistic model over the codes conditioned on noisy audio and visual inputs. At test time, we obtain speech codes from the auto-regressive model and use the decoder mod-ule of the neural speech codec to synthesize clean speech.
Our approach is analogous to high-quality, two-stage image generation techniques that learn a probabilistic prior over a pre-trained vocabulary of image components [13, 38]. (2) Rather than adopting a speaker-agnostic framework as done in most recent work, we focus on personalized models that leverage speaker-specific audio-visual cues for higher fidelity speech enhancement. To this end, we introduce
Facestar2, a high-quality audio-visual dataset containing 10 hours of speech data from two speakers. Existing audio-visual datasets used for vision-based speech synthesis tasks are either captured in a clean, controlled environment with a small and constrained vocabulary [7, 21] or curated from
“in-the-wild” videos with variable audio quality and unre-In contrast, Facestar contains un-liable lip motion [45]. constrained, large vocabulary natural speech recorded with high audio and visual quality and enables development of high-quality personalized speech models. (3) Empirically, our personalized AV speech codecs outper-form audio-visual speech enhancement baselines on quanti-tative metrics and human evaluation studies while operating on only 2kbps transmission rate from transmitter to receiver.
To the best of our knowledge, our work is the first to enable audio-visual speech enhancement at the quality required for high-fidelity telecommunications in AR/VR, even when the transmitter is in a highly noisy and reverberant environment.
Addressing Scalability. Beyond introducing high-quality personalized AV speech codecs, we also take steps towards 2https://github.com/facebookresearch/facestar addressing their scalability. While personalized models are commonly used in high-fidelity applications– for example, personalized visual avatars enable extremely photo-realistic visual representations of humans in VR that overcome the uncanny valley [34, 62] – a downside is that they typically require training on hours of data from the target individual.
The question naturally arises of how we can obtain high-quality personalized models with less data, in order to scale high-fidelity telecommunications to a large volume of users.
As a first step, we propose a simple strategy for personaliz-ing AV speech codecs to new individuals with minimal new data, based on a similar approach used to scale personal-ized text-to-speech models [4]. Specifically, we introduce a multi-speaker extension of AV speech codecs that features a speaker identity encoder, which can be pre-trained on a multi-speaker dataset and then fine-tuned for a new speaker with only a small sample of their data. We demonstrate this personalization strategy on the GRID dataset [7]. An addi-tional benefit of our multi-speaker model is that it enables voice conversion from one speaker to another, and thereby opens up creative applications in AR/VR. 2.