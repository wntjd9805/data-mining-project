Abstract
Multi-Camera Multi-Object Tracking is currently draw-ing attention in the computer vision ﬁeld due to its supe-rior performance in real-world applications such as video surveillance with crowded scenes or in wide spaces. In this work, we propose a mathematically elegant multi-camera tracking approach based on a spatial-multiple object temporal lifted multicut formulation. Our model utilizes state-of-the-art tracklets produced by single-camera track-ers as proposals. As these tracklets may contain ID-Switch errors, we reﬁne them through a novel pre-clustering ob-tained from 3D geometry projections. As a result, we derive a better tracking graph without ID switches and more precise afﬁnity costs for the data association phase.
Tracklets are then matched to multi-camera trajectories by solving a global lifted multicut formulation that incorpo-rates short and long-range temporal interactions on track-lets located in the same camera as well as inter-camera ones. Experimental results on the WildTrack dataset yield near-perfect performance, outperforming state-of-the-art trackers on Campus while being on par on the PETS-09 dataset. We will release our implementations at this link https://github.com/nhmduy/LMGP. 1.

Introduction
Multiple object tracking (MOT), i.e., extracting motions of objects moving through a scene, is a fundamental prim-itive for high-level understanding information in videos.
The most common approach to MOT is the tracking-by-assignment paradigm, in which ﬁrst detection boxes are computed for the objects of interest in each timeframe, and second, a data association is performed by linking detec-tions of the same objects to each other. In the most popular setting, a single camera faces a scene and the data associ-ation links detections in different timeframes to each other
[1, 3, 9, 43]. However, even though a large body of research has been devoted to MOT with a single camera, large and
Figure 1. Multi-camera tracking with four overlapping cameras.
A target object (red rectangle) is occluded at Cam 3 but is still ob-served at Cam 1, Cam 2, and Cam 4. Taking this correspondence into account (red arrow), we can recover a missing bounding box at Cam 3 (red dashed arrow). crowded scenes still cannot be tracked faithfully, and errors occur mainly in the data association step. These errors most often are caused by partial visibility (or even occlusion) and indistinguishability of objects.
One possibility of improving performance has been to use multiple cameras facing the same scene but from dif-ferent angles (Figure 1).
In this setting, partial visibility and indistinguishability are less severe since an object may be occluded in a single camera but may still be fully ob-served by another camera. Leveraging this property, recent papers have pursued two principal approaches: single view-based and centralized representation methods. In the ﬁrst strategy [16, 28, 33, 45], a two-step procedure is followed: 1) generating local tracklets of all the targets within each camera; 2) matching local tracklets that belong to the same target across cameras through computing afﬁnity costs and using a global optimization framework. While this frame-work brings beneﬁts via a reduced hypothesis space and al-lows to design motion-based features, its main drawbacks lie in ID-Switch errors contained inside local tracklets, i.e., detections of distinct objects are grouped into the same tra-jectory (Figure 3a). As a result, these errors will propa-gate throughout the tracking graph, affecting the total per-formance. The centralized representation approach [47, 49]
on the other hand, is not plagued by such obstacles since each node in the tracking graph is an occupancy map (not a tracklet), which is estimated from all detections at each timeframe. Unfortunately, the cost of the data association step is increased due to a huge state space of variables and integrating advances from single-camera methods is more complicated.
In this work, we propose a method that follows the sin-gle view-based approach but integrates concepts from the centralized representation paradigm. Our motivation for this design choice is to harness the great progress made in single-camera tracking while at the same time effectively addressing limitations encountered in prior studies such as
ID-Switch errors by exploiting centralized representation ideas through our novel pre-clustering step. Speciﬁcally, the corresponding images from the pre-clustering step (i.e. our occupancy map) allow us to break up initial tracklets generated by single-camera trackers at ID-Switch errors and establish precise afﬁnity costs for both temporal and spatial afﬁnities (Subsection 4.5). On top of that, a novel spatial-temporal optimization model for the data association is em-ployed, which takes into account both short- and long-range temporal interactions of objects detected by a single camera as well as spatial interactions between cameras in a single framework (Table 4). The experimental ﬁndings show that, given the right conditions, with a multiple-camera environ-ment and precise boundary detections, our method leads to a nearly optimal solution for multiple object tracking using multiple cameras (Table 1).
Contributions Our main contributions can be summarized as follows. First, we introduce a new pre-clustering algo-rithm driven by 3D geometry projections to group detec-tions at each timestep across cameras. This effectively elim-inates tracklet errors from single-camera trackers and pro-vides highly accurate afﬁnity costs for the data association step. Second, we propose a novel spatial-temporal lifted multicut formulation for the multi-camera setting, jointly optimizing both intra- and inter-camera as well as short-and long-range interaction in a single global formulation.
Finally, we obtain nearly perfect performance on the large-scale WILDTRACK [7] dataset, outperform state-of-the-art on Campus [44] and are on par with the PETS-09 [12] dataset. 2.