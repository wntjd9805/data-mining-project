Abstract
Understanding 2D computer-aided design (CAD) draw-ings plays a crucial role for creating 3D prototypes in ar-chitecture, engineering and construction (AEC) industries.
The task of automated panoptic symbol spotting, i.e., to spot and parse both countable object instances (windows, doors, tables, etc.) and uncountable stuff (wall, railing, etc.) from
CAD drawings, has recently drawn interests from the com-puter vision community. Unfortunately, the highly irreg-ular ordering and orientations set major roadblocks for this task. Existing methods, based on convolutional neural networks (CNNs) and/or graph neural networks (GNNs), regress instance bounding boxes in the pixel domain and
In this paper, then convert the predictions into symbols. we present a novel framework named CADTransformer, that can painlessly modify existing vision transformer (ViT) backbones to tackle the above limitations for the panop-tic symbol spotting task. CADTransformer tokenizes di-rectly from the set of graphical primitives in CAD draw-ings, and correspondingly optimizes line-grained seman-tic and instance symbol spotting altogether by a pair of prediction heads. The backbone is further enhanced with a few plug-and-play modifications, including a neighbor-hood aware self-attention, hierarchical feature aggregation, and graphic entity position encoding, to bake in the struc-ture prior while optimizing the efficiency. Besides, a new data augmentation method, termed Random Layer, is pro-posed by the layer-wise separation and recombination of a CAD drawing. Overall, CADTransformer significantly boosts the previous state-of-the-art from 0.595 to 0.685 in the panoptic quality (PQ) metric, on the recently released
FloorPlanCAD dataset. We further demonstrate that our model can spot symbols with irregular shapes and arbi-trary orientations. Our codes are available in https:
//github.com/VITA-Group/CADTransformer. 1.

Introduction 1.1. A Primer for CAD Panoptic Symbol Spotting
Symbol spotting [42, 43, 45, 47] refers to a particular application of pattern recognition, in which symbols with (a) GT (b) CADTransformer (c) Results of [17] (d) GT (e) CADTransformer (f) Results of [17]
Figure 1. CADTransformer makes predictions on graphic entities, without converting the predicted 2D bounding boxes on pixel im-ages to the label on graphical entities. (a) and (d): Panoptic anno-tations. (b) and (e): Results from the proposed CADTransformer. (c) and (f): Results from PanCADNet [17]. Red arrows indicate spotting results of adjacent symbols, bounding box in (c) shows one of predicted boxes of [17] for stair symbol. domain-specific semantics are localized and recognized to predefined symbol types. Symbols with simple line seg-ment groups with an engineering, electronics or architec-tural flair, which constitute some examples of symbols.
Therefore, symbol spotting plays a crucial role for docu-ment image analysis community [42] and architecture, engi-neering and construction (AEC) industries [17]. In architec-ture, a 2D computer-aided design (CAD) drawing typically contains accurate geometric and rich semantic information of a cross-section of a 3D design [17]. With the perception of such CAD drawings, 3D prototypes and the according 3D model can be efficiently and precisely reconstructed.
However, unlike images which are arranged on regular pixel grids, a CAD drawing is composed of graphical prim-itives (e.g., arc, circle, polyline, etc.). It is non-trivial to spot each symbol (set of graphical primitives) in CAD drawing due to the presence of occlusion, cluster, appearance vari-ations, and large unbalanced distribution of the categories.
Traditional symbol spotting methods are typically carried
out as query-by-example [31, 32, 45], and are impractical for real-world datasets since they can not cope with the tremendous graphical notation variability [17, 43] caused by the producer. Recent learning-based symbol spotting methods [19, 43] proposed to apply convolutional neural networks (CNNs) to real-world symbol spotting datasets.
However, they formulate symbol spotting as symbol de-tection [19, 41, 43, 65] and simply treat vector CAD data as pixel images, leaving the gap between images and vec-tor graphics and leading to inaccurate predictions for real-world applications.
The latest work [17] proposed a large-scale FloorPlan-CAD dataset from industry with annotations for graphical entities (visualization on several CAD drawing examples from the dataset, with irregular shapes and slanting orien-tation, can be seen in Figure 1.). Similar to the panoptic segmentation task [7] which integrates instance and seman-tic segmentation as one visual recognition task, the panoptic symbol spotting task was formulated to unify the spotting of countable instances (e.g., a single door, a window, a table, etc.) and the recognition of uncountable stuff (e.g., wall, railing, etc.) in [17] . The authors [17] address the panoptic symbol spotting task by introducing Graph Convolutional
Networks (GCNs) [26] to reason the stuff semantics. Par-allel to the GCN head, another CNN-based detection head also predicts the 2D box information of each countable in-stance. However, the CAD graphical primitives come with irregular ordering, arbitrary scales and orientations, that re-main to challenge the standard CNNs. Moreover, the GCN module requires a pre-specified graph topology for infor-mation propagation;
[17] uses an ad-hoc graph manually crafted by rules, but that might be inaccurate and subject to structural noise. 1.2. Why Transformer, and Our Contributions
Transformers [10, 12, 23, 40, 51, 54, 55, 59, 61, 64] rea-son global relationships across tokens without pre-defined graph connectivity, by instead learning with self-attention.
That makes transformer a promising replacement for GCN to tackle the panoptic symbol spotting task. However, a standard transformer is not immediately ready for this task due to the following challenges: 1). Tokenization and po-sition encoding of graphical symbols. The standard Vi-sion Transformers (ViT) [13] splits each image into 14 × 14 or 16 × 16 patches (a.k.a., tokens) with fixed length over entire dataset. But line segments, as the highly structured minimum units in CAD drawings, are an unordered set of vectors represented in the continuous coordinate space, which differ drastically from raster images. 2). Immense set of primitives in certain scenes. ViT conducts global self-attention among tokens which leads to quadratic com-plexity with respect to the token number. That becomes intractable for processing CAD drawings, whose maxi-mum primitive number can be explosively high, e.g., up to 5 × 104. The number of primitives also varies a lot across drawings. 3). Training data limitation. ViTs are freed from the inherent inductive biases to CNNs. While that leads to more flexibility, it also makes the ViT training par-ticularly data-hungry [13]. Although the latest dataset [17] contains over 10,000 floor plans, it is still unclear whether that suffices for training ViTs to generalize.
In view of those roadblocks, we propose the first transformer-based framework for panoptic symbol spotting, called CADTransformer. CADTransformer is designed to be a general framework that can be painlessly plugged into existing ViT backbones.
It is well motivated since trans-formers can reason the hidden relationships among graphi-cal primitives without any handcrafted graph topology.
Firstly, in contrary to the common token embeddings from image pixel patches, CADTransformer tokenizes di-rectly from the set of graphical primitives in CAD draw-ings. Correspondingly, to optimize line-grained seman-tic and instance predictions, we design a semantic head that predicts the categories of graphical primitives, in par-allel with another offset head to shift to their respective ground-truth instance centroids. Further, to enhance the efficiency-accuracy trade-off, CADTransformer embraces a few “plug-and-play” improvements over the standard trans-former backbone, including injecting neighborhood aware-ness to self-attention to bake-in the structure drawing prior; and hierarchical feature aggregation, along with graphic en-tity position encoding. Additionally, we introduce a novel
Random Layer data augmentation approach, which lever-ages CAD domain knowledge to augment new drawings.
We perform the panoptic symbol spotting via CAD-Transformer, by plugging our proposed design into two ViT backbones: ViT [13] and Point Transformer [62]. CAD-Transformer significantly boosts the previous state-of-the-art from 0.595 to 0.689 in the panoptic quality (PQ) metric, on the FloorPlanCAD dataset. We also report a compre-hensive set of ablation studies to demonstrate the benefit of each proposed design component. 2.