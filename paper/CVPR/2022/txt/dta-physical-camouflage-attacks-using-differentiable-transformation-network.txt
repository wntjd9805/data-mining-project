Abstract
To perform adversarial attacks in the physical world, many studies have proposed adversarial camouflage, a method to hide a target object by applying camouflage pat-terns on 3D object surfaces. For obtaining optimal physi-cal adversarial camouflage, previous studies have utilized the so-called neural renderer, as it supports differentiabil-ity. However, existing neural renderers cannot fully rep-resent various real-world transformations due to a lack of control of scene parameters compared to the legacy photo-realistic renderers. In this paper, we propose the Differen-tiable Transformation Attack (DTA), a framework for gen-erating a robust physical adversarial pattern on a target ob-ject to camouflage it against object detection models with a wide range of transformations.
It utilizes our novel Dif-ferentiable Transformation Network (DTN), which learns the expected transformation of a rendered object when the texture is changed while preserving the original properties of the target object. Using our attack framework, an ad-versary can gain both the advantages of the legacy photo-realistic renderers including various physical-world trans-formations and the benefit of white-box access by offering differentiability. Our experiments show that our camou-flaged 3D vehicles can successfully evade state-of-the-art object detection models in the photo-realistic environment (i.e., CARLA on Unreal Engine). Furthermore, our demon-stration on a scaled Tesla Model 3 proves the applicability and transferability of our method to the real world. 1.

Introduction
Deep neural networks (DNNs), despite their renowned capability for solving computer vision tasks [10, 12, 19], have been proven vulnerable to adversarial examples [28].
‡ Equal contribution
* Corresponding author (a) (b) (c) (a) Neural renderer used in prior work (Dual At-Figure 1. tention Suppression (DAS)) [32]. Although it is differentiable, it possesses a limitation in physical properties representation (e.g., transparency for windshield) and lacks background blending (e.g., shadowing) because the object and the background scene are ren-dered separately. (b) Our Differential Transformation Network (DTN). DTN considers both differentiability and photo-realistic aspects by learning the correct transformation when the object’s texture is changed. As shown, transparency for the windshield and shadowing at the bottom and the top of the car are rendered correctly. (c) Comparison of detection results using different textures (normal, DAS, random, DTA) on photo-realistic envi-ronment. Unlike other examples, the adversarial camouflage gen-erated by our DTA framework successfully evades detection.
That is, carefully crafted inputs may cause DNN models to misrepresent a seemingly obvious image to the human eye, giving incorrect prediction results. A deliberate act by an adversary to take advantage of this weakness, namely the adversarial attack, has captured the attention of many in the
Its potential applicability in not only the past few years. digital domain but also the physical domain has drawn sig-nificant interest.
Compared to digital attacks, physical adversarial attacks are more difficult to launch since they must account for vari-ous physical constraints and conditions (e.g., lighting, cam-era pose, and occlusion). However, the fully physical at-tack experiments in the real world such as [2, 4, 15, 30] are extremely time consuming and expensive. Therefore, vari-ous studies have been conducted through simulation of the physical world in the digital environment by using legacy photo-realistic rendering software, such as Unreal Engine
[8] and AirSim [26], facilitating parameter control. Exam-ples of methods to craft physical adversarial camouflage, i.e., adversarial attack variant that focuses on hiding an ob-ject by fully covering the target object, in simulators can be found in [34, 35]. However, since such simulators are non-differentiable, the attacks employ a black-box approach such as utilizing a clone network [35] or a genetic algo-rithm [34], yielding an inevitably lower attack performance than the white-box counterpart.
To obtain the advantage of differentiability, more recent methods [7, 16, 32] have proposed the use of neural render-ers for generating adversarial camouflage. However, the ex-isting neural renderers (e.g., [17]) can only support the gen-eration of foreground objects; hence, background images are still handled by the legacy photo-realistic renderers.
As a result, they simply attach the generated target object to the background image, yielding inaccurate foreground-background blending effects, such as shadow casting and light reflection, as shown in Fig. 1a. Although workaround efforts, such as masking for handling occlusion [16], have been proposed, the overall adversarial camouflage results using existing neural renderers are still inferior in terms of photo-realistic attributes.
Motivated by the challenge faced in prior works, we develop an attack framework that takes advantage of the differentiability in neural renderers without compromising the photo-realistic properties of the target object.
In par-ticular, the framework leverages our novel neural render-ing technique, which learns the representation of various scene properties (e.g., object material, lighting effects, and shadows) from legacy photo-realistic renderers. As a result, truly robust physical adversarial camouflage, verified from our experiment using both a photo-realistic simulation and a real-world example, can be obtained.
Our contributions can be summarized as follows:
• We present the Differentiable Transformation Attack (DTA), a framework for generating robust physical ad-versarial camouflage on 3D objects. It combines the advantages of a photo-realistic rendering engine with the differentiability of our novel rendering technique.
• We propose the Differentiable Transformation Net-work (DTN), a brand-new neural renderer that learns the transformations of an object when the texture is changed while preserving its original parameters for a realistic output that resembles the original.
• Our DTN can be embedded as an extension to pro-vide differentiability to any rendering software (e.g.,
Unreal Engine [8]), enabling the use of any gradient-based method.
• We demonstrate that the adversarial camouflage gen-erated from DTA is robust and applicable for evad-ing pre-trained object detection models under various transformations in both simulations and the real world.
• Our attack method, DTA, outperforms previous works in terms of our evaluation of target object detection models and transferability to other models. 2.