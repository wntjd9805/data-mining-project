Abstract 1.

Introduction
Breakthroughs in transformer-based models have revo-lutionized not only the NLP field, but also vision and mul-timodal systems. However, although visualization and in-terpretability tools have become available for NLP models, internal mechanisms of vision and multimodal transform-ers remain largely opaque. With the success of these trans-formers, it is increasingly critical to understand their inner workings, as unraveling these black-boxes will lead to more capable and trustworthy models. To contribute to this quest, we propose VL-InterpreT, which provides novel interactive visualizations for interpreting the attentions and hidden representations in multimodal transformers. VL-InterpreT is a task agnostic and integrated tool that (1) tracks a vari-ety of statistics in attention heads throughout all layers for both vision and language components, (2) visualizes cross-modal and intra-modal attentions through easily readable heatmaps, and (3) plots the hidden representations of vision and language tokens as they pass through the transformer layers. In this paper, we demonstrate the functionalities of
VL-InterpreT through the analysis of KD-VLP, an end-to-end pretraining vision-language multimodal transformer-based model, in the tasks of Visual Commonsense Rea-soning (VCR) and WebQA, two visual question answering benchmarks. Furthermore, we also present a few interest-ing findings about multimodal transformer behaviors that were learned through our tool.
*Equal Contributions
Since transformers were introduced in Vaswani et al.
[30], not only have they seen massive success in NLP ap-plications, their impact on computer vision and multimodal problems has also become increasingly disruptive. How-ever, the internal mechanisms of transformers that lead to such successes are not well understood. Although efforts have been made to interpret the attentions [8] and hidden states [22] of transformers for NLP, such as BERT [12], in-vestigations in the mechanisms of vision and multimodal transformers are relatively scarce, and tools for probing such transformers are also limited. Given the fast-growing number of successful vision and multimodal transformers (e.g., ViT [13] and CLIP [25]), enhanced interpretability of these models is needed to guide better designs in the future.
Past research has shown the importance of interpreting the inner mechanisms of transformers. For example, Clark et al. [8] found certain BERT attention heads specialized in handling certain syntactic relations, as well as interesting ways in which BERT attention utilizes special tokens and punctuation marks. Additionally, Lin et al. [21] showed that the linguistic information encoded in BERT becomes increasingly abstract and hierarchical in later layers. These studies provide valuable insights into the functions of vari-ous elements in transformer architecture for NLP, and shed light on their limitations.
This paper presents VL-InterpreT1, which is an interac-tive visualization tool for interpreting the attentions and hid-1A screencast of our application is available at https://www. youtube.com/watch?v=4Rj15Hi_Pdo. Source code and a link to a live demo: https://github.com/IntelLabs/VL-InterpreT
den representations of vision-language (VL) transformers.
Importantly, it is a single system that analyzes and visu-alizes several aspects of multimodal transformers: first, it tracks behaviors of both vision and language attention com-ponents in attention heads throughout all layers, as well as the interactions across the two modalities. Second, it also visualizes the hidden representations of vision and language tokens as they pass through transformer layers.
The main contributions of our work are:
• Our tool allows interactive visualization for probing hidden representations of tokens in VL transformers.
• Our tool allows systematic analysis, interpretation, and interactive visualization of cross- and intra-modal components of attention in VL transformers.
• As an application of VL-InterpreT, we demonstrate multimodal coreference in two analyses: 1) how con-textualized tokens in different modalities referring to the same concept are mapped to proximate represen-tations, and 2) how attention components capture the conceptual alignment within and across modalities. 2.