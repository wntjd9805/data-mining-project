Abstract power consumption.
Spiking Neural Network (SNN) is a promising energy-efficient AI model when implemented on neuromorphic hardware. However, it is a challenge to efficiently train
SNNs due to their non-differentiability. Most existing meth-ods either suffer from high latency (i.e., long simulation time steps), or cannot achieve as high performance as Ar-tificial Neural Networks (ANNs). In this paper, we propose the Differentiation on Spike Representation (DSR) method, which could achieve high performance that is competitive to
ANNs yet with low latency. First, we encode the spike trains into spike representation using (weighted) firing rate cod-ing. Based on the spike representation, we systematically derive that the spiking dynamics with common neural mod-els can be represented as some sub-differentiable mapping.
With this viewpoint, our proposed DSR method trains SNNs through gradients of the mapping and avoids the common non-differentiability problem in SNN training. Then we an-alyze the error when representing the specific mapping with the forward computation of the SNN. To reduce such error, we propose to train the spike threshold in each layer, and to introduce a new hyperparameter for the neural models.
With these components, the DSR method can achieve state-of-the-art SNN performance with low latency on both static and neuromorphic datasets, including CIFAR-10, CIFAR-100, ImageNet, and DVS-CIFAR10. 1.

Introduction
Inspired by biological neurons that communicate using spikes, Spiking Neural Networks (SNNs) have recently re-ceived surging attention. This promise depends on their en-ergy efficiency on neuromorphic hardware [7,31,35], while deep Artificial Neural Networks (ANNs) require substantial
*Corresponding author.
Specifically,
However, the training of SNNs is a major challenge
[43] since information in SNNs is transmitted through the non-non-differentiable spike trains. differentiability in SNN computation hampers the effec-tive usage of gradient-based backpropagation methods. To tackle this problem, the surrogate gradient (SG) method
[15,33,40,46,53] and the ANN-to-SNN conversion method
[4, 10, 38, 39, 50] have been proposed and yielded the best performance. In the SG method, an SNN is regarded as a re-current neural network (RNN) and trained by the backprop-agation through time (BPTT) framework. And during back-propagation, gradients of non-differentiable spike functions are approximated by some surrogate gradients. Although the SG method can train SNNs with low latency (i.e., short simulation time steps), it cannot achieve high performance comparable to leading ANNs. Besides, the adopted BPTT framework needs to backpropagate gradients through both the layer-by-layer spatial domain and the temporal domain, leading to a long training time and high memory cost of the SG method. The high training costs further limit the usage of large-scale network architectures. On the other hand, the ANN-to-SNN conversion method directly deter-mines the network weights of an SNN from a corresponding
ANN, relying on the connection between firing rates of the
SNN and activations of the ANN. The conversion method enables the obtained SNN to perform as competent as its
ANN counterpart. However, intolerably high latency is typ-ically required, since only a large number of time steps can make the firing rates closely approach the high-precision ac-tivation values of ANNs [21, 38]. Overall, SNNs obtained by the two widely-used methods either cannot compete their
ANN counterparts, or suffer from high latency.
In this paper, we overcome both the low performance and high latency issues by introducing the Differentiation on
Table 1. Comparison of the ANN-to-SNN conversion, surrogate gradient (SG), and DSR method with respect to latency, perfor-mance with low latency, and applicability on neuromorphic data.
Conversion
High
Latency
Performace w/
Low Latency
Neuromorphic Non-appli-Low
Data cable
SG
Low
DSR
Low
Medium High
Appli-cable
Appli-cable
Spike Representation (DSR) method to train SNNs. First, we treat the (weighted) firing rate of the spiking neurons as spike representation. Based on the representation, we show that the forward computation of an SNN with common spik-ing neurons can be represented as some sub-differentiable mapping. We then derive the backpropagation algorithm while treating the spike representation as the information
In this way, our method encodes the temporal carrier. information into spike representation and backpropagates through sub-differentiable mappings of it, avoiding calcu-lating gradients at each time step. To effectively train SNNs with low latency, we further study the representation error due to the SNN-to-mapping approximation, and propose to train the spike thresholds and introduce a new hyperparam-eter for the spiking neural models to reduce the error. With these methods, we can train high-performance low-latency
SNNs. And the comparison of the properties between the
DSR method and other methods is illustrated in Tab. 1. For-mally, our main contributions are summarized as follows: 1. We systematically study the spike representation for common spiking neural models, and propose the DSR method that uses the representation to train SNNs by backpropagation. The proposed method avoids the non-differentiability problem in SNN training and does not require the costly error backpropagation through the temporal domain. 2. We propose to train the spike thresholds and introduce a new hyperparameter for the spiking neural models to reduce the representation error. The two techniques greatly help the DSR method to train SNNs with high performance and low latency. 3. Our model achieves competitive or state-of-the-art (SOTA) SNN performance with low latency on the
CIFAR-10 [26], CIFAR-100 [26], ImageNet [8], and
DVS-CIFAR10 [29] datasets. Furthermore, the exper-iments also prove the effectiveness of the DSR method under ultra-low latency or deep network structures. 2.