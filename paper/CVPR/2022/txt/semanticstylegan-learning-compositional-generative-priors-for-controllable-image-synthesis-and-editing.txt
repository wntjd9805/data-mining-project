Abstract 1.

Introduction
Recent studies have shown that StyleGANs provide promising prior models for downstream tasks on image syn-thesis and editing. However, since the latent codes of Style-GANs are designed to control global styles, it is hard to achieve a ﬁne-grained control over synthesized images. We present SemanticStyleGAN, where a generator is trained to model local semantic parts separately and synthesizes im-ages in a compositional way. The structure and texture of different local parts are controlled by corresponding latent codes. Experimental results demonstrate that our model provides a strong disentanglement between different spatial areas. When combined with editing methods designed for
StyleGANs, it can achieve a more ﬁne-grained control to edit synthesized or real images. The model can also be ex-tended to other domains via transfer learning. Thus, as a generic prior model with built-in disentanglement, it could facilitate the development of GAN-based applications and enable more potential downstream tasks.
Recent studies on Generative Adversarial Networks (GANs) have made impressive progress on image synthesis, where photo-realistic images can be generated from random codes in a latent space [11, 35–37]. These models provide powerful generative priors for downstream tasks by serving as neural renderers. However, their synthesis procedure is usually stochastic and no user control is naturally promised.
Thus, it is still a challenging problem to achieve controllable image synthesis and editing utilizing generative priors.
One of the most famous work among such generative priors is the StyleGAN series [35–37], where each gener-ated image is conditioned on a set of coarse-to-ﬁne latent codes (See Fig. 2). However, the meanings of these la-tent codes are still relatively ambiguous. Thus, a plethora of studies have attempted to further investigate into the latent space of StyleGAN to improve controllability.
It is shown that by learning a linear boundary or a neu-ral network in the latent space of StyleGAN, one could control the global attributes [4, 26, 59, 60] or 3D struc-ture [64] of the generated images. Furthermore, by using an optimization/encoder-based method, real images can also be embedded into the latent space to create a uniﬁed synthe-sis/editing model [2, 3, 5, 55, 65, 67, 75]. However, as pure learning-based methods, these solutions inevitability suffer from the biases in the StyleGAN latent space. For example, since different attributes could be correlated in StyleGAN, it often happens that unexpected attributes or local parts are changed while one wants to edit a certain attribute or area.
To obtain a more precise control, another solution is to train a new GAN model from scratch by introducing ad-ditional supervision or inductive biases. For example, by using 3D rendered faces, CONFIG [39] and DiscoFace-GAN [18] aim to build a GAN where pose, 3D informa-tion are factorized in the latent space. GAN-Control [61] disentangles the latent space by incorporating pre-trained attribute models for contrastive learning. Given the re-cent progress on neural rendering, it has also been shown that 3D-controllable GANs can be trained from images by injecting volumetric rendering into the synthesis proce-dure [13, 25, 46, 58, 74]. However, a major limitation of above-mentioned models is that they are designed for holis-tic attributes and there is no ﬁne-grained local editability.
In this work, we propose SemanticStyleGAN, which in-troduces a new type of generative prior for controllable im-age synthesis. Unlike prior work, the latent space of Se-manticStyleGAN is factorized based on semantic parts de-ﬁned by semantic segmentation masks (Fig. 2 (b)). Each semantic part is modulated individually with correspond-ing local latent codes and an image is synthesized by com-posing local feature maps. Different from layout-to-image translation methods [14, 69, 76], our local latent codes are able to control both the structure and texture of seman-tic parts (See Fig. 1). Compared to attribute-conditional
GANs [18, 39, 61], our model is not designed for any spe-ciﬁc task and can serve as a generic prior like StyleGAN.
Thus, it can be combined with latent manipulation methods designed for StyleGAN to edit output images while provid-ing more precise local controls. The contributions of this work can be summarized as follows:
• A compositional generator architecture that disentan-gles the latent space into different semantic areas to control the structure and texture of local parts.
• A GAN training framework that learns the joint mod-eling of image and semantic segmentation masks.
• Experiments showing that our generator can be com-bined with existing latent manipulation methods to edit images in a more controllable fashion.
• Experiments showing that our generator can be adapted to other domains with only limited images while preserving spatial disentanglement. (a) StyleGAN (b) SemanticStyleGAN
Figure 2. Abstract illustration of our method. Unlike StyleGAN, whose latent codes are associated with different granularity. The latent space of SemanticStyleGAN is factorized over different re-gions, which controls both local shape and texture. 2.