Abstract
Continual learning methods aim at training a neural net-work from sequential data with streaming labels, reliev-ing catastrophic forgetting. However, existing methods are based on and designed for convolutional neural networks (CNNs), which have not utilized the full potential of newly emerged powerful vision transformers.
In this paper, we propose a novel attention-based framework Lifelong Vision
Transformer (LVT), to achieve a better stability-plasticity trade-off for continual learning. Specifically, an inter-task attention mechanism is presented in LVT, which implicitly absorbs the previous tasks’ information and slows down the drift of important attention between previous tasks and the current task. LVT designs a dual-classifier structure that independently injects new representation to avoid catas-trophic interference and accumulates the new and previous knowledge in a balanced manner to improve the overall per-formance. Moreover, we develop a confidence-aware mem-ory update strategy to deepen the impression of the previ-ous tasks. The extensive experimental results show that our approach achieves state-of-the-art performance with even fewer parameters on continual learning benchmarks. 1.

Introduction
Humans can continuously learn novel concepts through-out their lifetime and accumulate visual knowledge from past experiences [5, 69]. In contrast, artificial neural net-works forget the information learned in the previous tasks while learning new ones, resulting in a drastic drop in performance on the previous tasks. This phenomenon, known as catastrophic forgetting or catastrophic interfer-ence [52, 59], stems from changes in the input data dis-tribution that cause the new input information to interfere severely with the previously learned knowledge [8, 51]. To address this challenge, the field of continual learning (also called lifelong or incremental learning) [17,61,62,72] stud-ies the problem of learning from a non-stationary stream of data, with the goal of maintaining and extending the ac-quired knowledge over time.
Figure 1.
Incremental accuracy and forgetting evaluated on all tasks observed so far during continual learning. We compare our model with vision transformers (ViT [21], LeViT [25], CvT [83], and CCT [32]) and prior continual learning methods (GEM [47], iCaRL [61], DER++ [10], HAL [14], ERT [11], and RM [7]) on the experiment CIFAR100 of 10 splits with memory size 500. [↑] higher is better, [↓] lower is better.
Continual learning requires neural networks to be sta-ble to prevent forgetting, but also plastic to learn new streaming labels, which is referred to as the stability-plasticity dilemma [27, 53]. Most of the early works in continual learning focus on the task-incremental learning (task-IL), where oracle knowledge of the task identity is available at inference time for selecting the corresponding classifier [2, 17, 44, 65, 68]. For example, regularization-based methods penalize the changes of important param-eters during the learning process of new tasks and typ-ically assign a separate output layer (classifier) for each task [13, 40, 64, 91]. Recently, various works have focused on the more difficult and realistic class-incremental learn-ing (class-IL) [3, 9, 14, 20, 36, 61, 75, 78, 89, 94], where the network is evaluated on all classes observed during the training, without requiring the task identity. Among them, rehearsal-based methods [4, 7, 10] storing a small portion of observed data in a limited memory for replaying have shown promising results; besides, distillation-based meth-ods [12,61,93] alleviate deterioration in later tasks by using knowledge distillation [34] to maintain the representation.
However, existing methods are based on and designed
for convolutional neural networks (CNNs) [33], which have not taken the full utilization of the potential from newly emerged powerful vision transformers [31, 39]. Vision transformers, recently, have shown superiority on certain computer vision tasks based on the self-attention mech-anism [16, 21, 25, 46, 55, 83, 90]. The merits of vision transformers bring a new perspective to the development of continual learning. Nevertheless, current vision trans-formers are not directly applicable to modeling a stream of tasks [31,39], since transformers lack the mechanism to pre-vent catastrophic forgetting on previous tasks. As shown in Figure 1, vision transformers [21, 25, 32, 83] with re-hearsal strategy suffer from catastrophic forgetting and per-formance degradation on previous tasks. Thus, it is chal-lenging to incorporate transformers well for further improv-ing continual learning.
In this work, we propose a novel framework, Lifelong
Vision Transformer (LVT), which plays the strengths of the attention mechanism in continual learning, achieving a better stability-plasticity trade-off. Unlike vanilla self-attention in vision transformers [21, 25, 32, 74] that de-rives the attention map by computing similarity between self-queries and self-keys, we propose an inter-task atten-tion mechanism to obtain attention maps by computing the affinities between self-queries and a learnable external key with an attention bias, which implicitly absorbs the previous tasks information. Also, inter-task attention saves the num-ber of parameters compared to self-attention. Besides, we consolidate the important attention weights by preventing them from changing in future tasks, thereby avoiding catas-trophic forgetting of past tasks. Different from the existing rehearsal-based methods [7, 9, 10, 14, 17, 47, 64, 75, 78] that use the same classifier for learning new tasks and replay-ing previous data, LVT proposes to utilize two classifiers: an injection classifier is used to inject new task represen-tation into the model, mitigating interference with previous tasks; and an accumulation classifier focuses on integrating the previous and new knowledge in a balanced manner to improve the overall performance.
Moreover, we propose a simple and effective confidence-based memory update strategy to store impressive exem-plars in the limited memory. The impressive exemplars have the distinctive characteristics of their classes. Like memo-ries in the brain [23, 26, 51], recalling these impressive ex-emplars is more beneficial for the model to consolidate pre-vious knowledge and thus reduce forgetting. We systemati-cally compare state-of-the-art and well-established methods for the continual learning problem in both the class-IL and task-IL settings. Experimental results show that the pro-posed framework significantly outperforms other methods in terms of accuracy and forgetting even with fewer param-eters. Using various ablation experiments, we validate the components of our approach.
The main contributions of this paper are four-fold:
• We propose a novel attention-based framework Lifelong
Vision Transformer (LVT), to achieve a better stability-plasticity trade-off for continual learning. LVT contains an inter-task attention mechanism that consolidates pre-vious knowledge and alleviates the forgetting on previ-ous tasks.
• LVT presents a novel dual-classifier structure to inde-pendently inject new task representation avoiding catas-trophic interference, and accumulate the new and previ-ous knowledge in a balanced manner.
• We develop a confidence-aware memory update strategy to deepen the impression of the previous tasks.
• The extensive experimental results show that our ap-proach achieves state-of-the-art performance with even fewer parameters on continual learning benchmarks. 2.