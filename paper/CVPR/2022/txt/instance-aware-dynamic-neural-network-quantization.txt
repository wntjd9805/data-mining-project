Abstract
Quantization is an effective way to reduce the memory and computational costs of deep neural networks in which the full-precision weights and activations are represented using low-bit values. The bit-width for each layer in most of existing quantization methods is static, i.e., the same for all samples in the given dataset. However, natural im-ages are of huge diversity with abundant content and us-ing such a universal quantization conﬁguration for all sam-ples is not an optimal strategy. In this paper, we present to conduct the low-bit quantization for each image individ-ually, and develop a dynamic quantization scheme for ex-ploring their optimal bit-widths. To this end, a lightweight bit-controller is established and trained jointly with the given neural network to be quantized. During inference, the quantization conﬁguration for an arbitrary image will be determined by the bit-widths generated by the controller, e.g., an image with simple texture will be allocated with lower bits and computational complexity and vice versa.
Experimental results conducted on benchmarks demon-strate the effectiveness of the proposed dynamic quanti-zation method for achieving state-of-art performance in terms of accuracy and computational complexity. The code will be available at https://github.com/huawei-noah / Efficient - Computing and https : / / gitee.com/mindspore/models/tree/master/ research/cv/DynamicQuant. 1.

Introduction
Deep convolutional neural networks (CNNs) have achieved remarkable results in a wide range of intelligent applications including image processing [14, 27], video understanding [5], natural language processing [15] and speech recognition [49]. However, these models have ex-cessive demands on storage and computational resources to achieve satisfactory performance, which prohibits their de-ployment on mobile and embedded devices. Thus, how to effectively compress and accelerate the CNNs is urgently
Figure 1. Results of applying DQNet for ResNet50 on the Ima-geNet dataset. The proposed DQNet can be easily embedded with mainstream quantization frameworks for better performance, such as DoReFa [51] and PACT [8].1 required for real-world applications.
Admittedly, there have been extensive explorations on model compression and acceleration methods such as prun-ing [16, 18, 30, 39], tensor decomposition [24, 38] and knowledge distillation [20, 26], which aims to shrink the original network architectures while remaining their per-formance. Quantization is another effective approach for reducing the complexity by representing weights and acti-vations in the network using low-bit integer values, which is convenient for applications since it does not change the network architecture and is easily to deploy. For example,
Zhou et al. [51] proposed to constrain weights and activa-tions to low-bit values and approximated the sign function with ”hard tanh” function [2] in the backward process to avoid the zero-gradient problem. The subsequent methods in [8, 12, 48, 50] utilized ﬂexible scale factors and training strategy to optimize the quantized models for better perfor-mance. 1The Bit-FLOPs is calculated as the product of the weight bit-width, the activation bit-width and the FLOPs (the multiplication and add operations).
Although the aforementioned approach have made tremendous efforts for enhancing the performance of the low-bit quantized network, the diversity of each instance in the given dataset is usually ignored. In fact, how to effec-tively handle hard and easy examples is a widespread prob-lem in computer vision, especially in visual recognition task as discussed in [35, 37]. To this end, Wu et al. [44] propose to select a small proportion of layers from the pre-trained network during inference and reduce the overall compu-tational complexity. Cheng et al. [7] point out that a sin-gle network architecture is not representative enough for all samples in the given dataset and exploit an instance-level network variation algorithm. These methods are mainly de-veloped based on the fact that the required resources for processing different samples using deep neural networks are often various. For example, a well trained network is very easy to identify an image containing only one dog, but it is hard to recognize an obscured bicycle in the street view.
Thus, we are motivated to explore an instance-aware dy-namic approach that can provide better trade-off of perfor-mance and computational complexity.
In this paper, we propose a novel network quantization scheme, which dynamically allocates bit-widths in quan-tized neural networks conditioned on each input samples as shown in Fig 2. Speciﬁcally, a great number of hidden sub-networks with various bit-width conﬁgurations will be derived from the given network architecture. During the inference, an image which is hard to be accurately recog-nized will be assigned with a larger network and vice versa.
For an arbitrary image, a bit-controller is utilized for pre-dicting its optimal bit-width sequence for weights and ac-tivations of all layers. The bit-controller is designed with a lightweight architecture so that its additional computa-tional costs can be ignored. The quantized neural network is trained together with the bit-controller in an end-to-end manner for better performance. Since our dynamic quan-tized network (DQNet) can provide the optimal bit-width conﬁguration according to different input images, the com-putational cost for processing each sample can be largely reduced. The dynamic computation resource allocation can achieve a better trade-off between computational cost and accuracy than those of conventional static quantization methods. Extensive experiments conducted on CIFAR-10 and ImageNet datasets demonstrate that DQNet can achieve similar or even better classiﬁcation accuracy than that of static quantization methods while consuming less computa-tion resources. 2.