Abstract
Videos from edited media like movies are a useful, yet under-explored source of information, with rich variety of appearance and interactions between humans depicted over a large temporal context. However, the richness of data comes at the expense of fundamental challenges such as abrupt shot changes and close up shots of actors with heavy truncation, which limits the applicability of existing 3D hu-man understanding methods. In this paper, we address these limitations with the insight that while shot changes of the same scene incur a discontinuity between frames, the 3D structure of the scene still changes smoothly. This allows us to handle frames before and after the shot change as multi-view signal that provide strong cues to recover the 3D state of the actors. We propose a multi-shot optimization framework that realizes this insight, leading to improved 3D reconstruction and mining of sequences with pseudo-ground truth 3D human mesh. We treat this data as valuable supervision for models that enable human mesh recovery from movies; both from single image and from video, where we propose a transformer-based temporal encoder that can naturally handle missing observations due to shot changes in the input frames. We demonstrate the importance of our insight and proposed models through extensive experiments.
The tools we develop open the door to processing and an-alyzing in 3D content from a large library of edited me-dia, which could be helpful for many downstream applica-tions. Code, models and data are available at: https:
//geopavlakos.github.io/multishot/
Figure 2. Multi-shot reasoning. Frames before and after the shot change depict the same 3D scene and provide a multi-view signal which helps reconstruct the underlying 3D pose of humans, particularly in cases of close-up, heavily truncated images of people. Blue triangles correspond to estimated camera locations in the scene. Each person is reconstructed independently. 1.

Introduction
Movies are a treasure trove of human “behavior episodes” [4]. They are produced in many different coun-tries in multiple genres, giving us tremendous cultural di-versity and range. Datasets, most prominently, AVA [14] have emerged, which provide a rich annotation of spatio-temporally localized human actions in movies. This would seem like ideal data on which to train systems for video un-derstanding, and furthermore use that as a stepping stone for acquiring “common sense” from observations of diverse human behavior. This “visual” route could be complemen-tary to the “linguistic” route to capturing common sense and arguably more fundamental.
But before we go too far with our wishful thinking, we must confront a fundamental challenge of video data de-rived from movies – the complication of “shots”. Film has a grammar [2]. Stories are communicated through a jux-taposition of shots, typically from different camera angles viewing the same scene. Alfred Hitchcock’s Rope and Sam
Mendes’s 1917 are noteworthy precisely because they are presented as a single take, without any discernible breaks corresponding to shot boundaries.
These shot changes manifest as sudden discontinuities in video as illustrated in Figure 1. Current temporal 3D human mesh and motion recovery methods, as well as most action classification algorithms, treat these shots as independent scenes, which reduce the rich potential of a film to a se-ries of short independent temporal sequences. Furthermore, shot changes often manifest in close up shots of actors and most state-of-the-art human mesh recovery models struggle to handle such heavily truncated images of people as shown in Figure 7. These two issues prevent applying such models to analyze 3D human behaviors in movies.
In this work, we propose a solution that addresses both of these challenges. First, we recognize that shot changes often depict a coherent underlying 4D scene from differ-ent viewpoints, despite the temporal discontinuities at the frame level. Thus, when handled properly, shot changes can be used as a multi-view signal of the underlying dynamic scene. This can be a powerful cue in disambiguating the 3D pose and motion of humans, which is particularly helpful for close-up, heavily truncated images of people (Figure 2).
Specifically, we build on this novel and unexplored idea and propose a multi-shot optimization method that allows re-covery of a consistent 3D human motion sequence across shot changes, simultaneously addressing both challenges of temporal fragmentation and partial humans.
The proposed multi-shot optimization allows recovery of long and reliable 3D human motion sequences from movies.
This data can be treated as pseudo-ground truth and used for training regression models that predict human mesh directly from pixels in a feed-forward manner from images [22] or videos [23]. This workflow is illustrated in Figure 3. We show that high quality output from our multi-shot optimiza-tion is crucial for improving the performance of these mod-els as multi-shot reasoning provides both longer and more accurate 3D pseudo-ground truth. Notably, unlike many previous works, the resulting direct prediction models are robust enough to perform human mesh recovery on movie data. Moreover, to further push the applicability on films, we propose a transformer-based architecture (t-HMMR) for our temporal encoder. A common challenge in edited me-dia is that a person may not be consecutively depicted in the scene due to shot changes to another character or the background, often referred to as B-rolls (e.g., sequence of
Figure 3). Transformers can easily address this by explic-itly not attending to frames that do not contain the person of interest and ignore them, while still processing a larger tem-poral context before and after the irrelevant input frames.
We experiment on AVA [14], a large scale dataset of movies with atomic action annotations. Applying our multi-shot optimization on AVA results in over 350k frames with pseudo-ground truth 3D. We treat this as training data to su-pervise regression models for human mesh recovery, from single image or video. Simultaneously, we curate a subset of AVA and use it for evaluation. We demonstrate the im-portance of our multi-shot optimization and the benefit on the downstream models through extensive experimentation
Figure 3. Overview of our workflow. We reconstruct 3D human mesh sequences from movies using our multi-shot optimization. The resulting reconstructions can be used as training data for both single-view human mesh recovery and temporal human mesh motion recovery. on AVA and common benchmarks.
In summary, our contributions are:
• We introduce the problem of human mesh recovery from multiple shots and we propose an optimization approach that is applicable in multi-shot sequences.
This results in high-quality 3D pseudo-ground truth that proves to be particularly effective at supervising direct regression models for human mesh recovery.
• We demonstrate that the resulting regression models can be applied successfully on movies, and we validate the importance of multi-shot reasoning at getting more accurate and longer pseudo-ground truth for training.
• To further push the applicability of regression models on movie data, we propose a temporal model with a pure transformer-based temporal encoder that is more suitable for inference on multi-shot sequences. 2.