Abstract
Efficient photorealistic scene capture is a challenging task. Current online reconstruction systems can operate very efficiently, but images generated from the models cap-tured by these systems are often not photorealistic. Recent approaches based on neural volume rendering can render novel views at high fidelity, but they often require a long time to train, making them impractical for applications that require real-time scene capture. In this paper, we propose a system that can reconstruct photorealistic models of com-plex scenes in an efficient manner. Our system processes images online, i.e. it can obtain a good quality estimate of both the scene geometry and appearance at roughly the same rate the video is captured. To achieve the efficiency, we propose a hierarchical feature volume using VDB grids.
This representation is memory efficient and allows for fast querying of the scene information. Secondly, we introduce a novel optimization technique that improves the efficiency of the bundle adjustment which allows our system to converge to the target camera poses and scene geometry much faster.
Experiments on real-world scenes show that our method outperforms existing systems in terms of efficiency and cap-ture quality. To the best of our knowledge, this is the first method that can achieve online photorealistic scene cap-ture.
Figure 1. Our method in operation. We present an approach for online scene capture from a stream of monocular RGB im-ages. The scene is represented using a neural volumetric dynamic
B+Tree (nVDB) which stores a hierarchy of spatial features. The dynamic topology of the tree allows it to grow as the camera ex-plores the scene. The camera and the volume parameters are opti-mized online using a novel volumetric bundle adjustment method (VBA). This allows our method to efficiently capture scene ap-pearance and geometry. See the supplementary video for an online demo. 1.

Introduction
Many applications require accurate online estimation of the geometry and appearance of 3D scenes.
In aug-mented reality, for example, accurate geometry is necessary for proper handling of occlusions and physics interactions, while accurate appearance is necessary for rendering con-vincing novel views.
There are two main paradigms for achieving this goal.
The first paradigm consists of visual simultaneous local-ization and mapping (SLAM) systems which are able to construct a dense model of a scene. These methods typ-ically obtain depth maps either from a depth sensor, in the case of RGB-D methods [13, 15], or by estimating depth keyframes from multiple RGB images, in the case of monocular SLAM systems [11, 11, 14, 17]. These depth measurements are then fused into a consistent model in the form of a voxel grid representing a signed distance function (SDF) to surfaces which encodes the geometry of the scene.
The colors from the RGB images are projected to the vol-ume to model the appearance. These approaches are very efficient and operate in real time1. However, their main dis-advantage is that they make simplified assumptions about scene appearance and can only reconstruct solid surfaces. 1For the purposes of this paper we define real time to mean the process-ing time for the reconstruction is comparable to the time taken to capture the image sequence. We define online to mean allowing for incremental processing as images arrive from the sensor.
Therefore they cannot capture photorealistic models of real-world scenes which are needed for real-time applications.
The second paradigm which has recently become very popular consists of neural volumetric rendering approaches
[9,10]. These methods model the scene as a volume and use raymarching to render novel views. They estimate the vol-ume parameters by directly optimizing photo-metric consis-tency using stochastic gradient descent (SGD) or momen-tum optimizers such as Adam [6]. The volume rendering, combined with the direct optimization of photometric error, allows these approaches to capture scenes in a photorealis-tic manner. However, the main disadvantage of these ap-proaches is that they require a long per-scene training time.
In the case of NeRF [10] this is up to a few days due to the neural network scene parameterisation.
In this paper, we aim to achieve photorealistic scene cap-ture with high efficiency to get closer to real-time perfor-mance. This is accomplished though two novel compo-nents. The first is a tree-based scene representation that improves memory efficiency because information is only stored near occupied areas. The tree is very efficient to query which speeds up rendering and therefore also opti-mization. The second component is our volumetric bun-dle adjustment (VBA) approach that optimizes the pose and volume parameters to achieve photo consistency. VBA is based on a Levenberg-Marquadt type approach which en-ables much faster convergence by utilizing the curvature of the objective function. This allows it to traverse valleys of the objective function more quickly. By combining the nVDB scene representation and the VBA optimizer our sys-tem achieves more accurate results than RGB-D systems.
To summarize, the main contribution of in this paper is a system for online photorealistic reconstruction. Key to this system are two novel components: 1. A neural volumetric dynamic B+Tree, called nVDB, that can efficiently represent 3D scenes and can grow as more areas of the scene are explored 2. An efficient method for optimizing the volume proper-ties, called volumetric bundle adjustment (VBA), that given a stream of input images can estimate the opti-mal nVDB parameters 2.