Abstract
Automated generation of 3D human motions from text is a challenging problem. The generated motions are ex-pected to be sufficiently diverse to explore the text-grounded motion space, and more importantly, accurately depict-ing the content in prescribed text descriptions. Here we tackle this problem with a two-stage approach: text2length sampling and text2motion generation. Text2length involves sampling from the learned distribution function of motion lengths conditioned on the input text. This is followed by our text2motion module using temporal variational autoen-coder to synthesize a diverse set of human motions of the sampled lengths. Instead of directly engaging with pose se-quences, we propose motion snippet code as our internal motion representation, which captures local semantic mo-tion contexts and is empirically shown to facilitate the gen-eration of plausible motions faithful to the input text. More-over, a large-scale dataset of scripted 3D Human motions,
HumanML3D, is constructed, consisting of 14,616 motion clips and 44,970 text descriptions. 1.

Introduction
Given a short textual description of a character’s move-ment as for example, an excerpt from a novel or a script, we are capable of visualizing the motions in our minds or even in drawings. The question is, how to automate this pro-cess by a machine, or in paraphrase, to generate realistic 3D human motions from text? This is the problem we tackle with in this paper. As illustrated in Fig. 1, given the in-put feed of “the figure rises from a lying position and walks in a counterclockwise circle, and then lays back down the ground”, our goal is to generate a diverse set of plausible 3D human motion dynamics following precisely the action types, directions, speeds, timing and styles as prescribed by the text.This automation process could bring a broad range of application impacts in AR/VR content creation, gaming, robotics, and human-machine interaction, to name a few. 1
Meanwhile, existing efforts in generating 3D human mo-tions from descriptions [1, 4, 21, 32, 44] are sporadic and the results are far from being satisfactory. Several common shortfalls are observed: the input text is usually one short sentence; the task is invariably formulated as determinis-tic sequence-to-sequence generation, with the synthesized motions tending to be stationary and lifeless; moreover, the generated motions are restricted to have the same length; fi-nally, the sole dataset relied on by existing methods, KIT
Motion-Language (KIT-ML) [31], consists of only 3,010 motion sequences focusing on locomotion actions. In par-ticular, there are three inherit challenges yet to be addressed. 1Project webpage: https://ericguo5513.github.io/text-to-motion
First, motions generated from text by the same model are expected to possess variable lengths. Second, there are usu-ally multiple ways for a character to behave following the same textual description. Third, from natural language per-spective, the input descriptions may have a wide range of forms, from being short & simple to very long & complex.
To address the aforementioned shortfalls and challenges, we propose a two-stage pipeline consisting of text2length sampling and text2motion generation. Text2length esti-mates the distribution function of visual motion length grounded on the input text. The role of text2motion is to generate distinct 3D motions from the input text and the sampled motion length; this is realized by engaging the tem-poral variational autoencoder (VAE) framework in its triplet form of prior, posterior, and generator networks; moreover, motion snippet code is introduced as the internal represen-tation in VAE code and throughout our pipeline to charac-terize the temporal motion semantics, with its role empiri-cally examined in later ablation studies. Finally, a dedicated dataset (HumanML3D) is constructed, consisting of 44,970 textual descriptions for 14,616 3D human motions. It cov-ers a wide range of action types including but not limited to locomotive actions. Empirical evaluations on both Hu-manML3D and KIT-ML datasets demonstrate the superior performance of our approach over existing methods.
Our key contributions are summarized as follows. First, our work is to our knowledge the first in stochastically gen-erating 3D motions from text, capable of generating diverse 3D human motions of variable lengths that are realistic-looking and faithful to the text input. Second, our approach is flexible to work with input text ranging from simple to complex forms. This is made possible by the text2length
& text2motion modules, and the proposed motion snippet codes that are to be detailed in later sections. Finally, a large-scale human motion dataset is constructed. It contains a wide range of actions, with each motion sequence paired with three textual descriptions. 2.