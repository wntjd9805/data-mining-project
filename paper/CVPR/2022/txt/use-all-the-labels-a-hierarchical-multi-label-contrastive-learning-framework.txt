Abstract
Current contrastive learning frameworks focus on lever-aging a single supervisory signal to learn representations, which limits the efficacy on unseen data and downstream tasks. In this paper, we present a hierarchical multi-label representation learning framework that can leverage all available labels and preserve the hierarchical relationship between classes. We introduce novel hierarchy preserv-ing losses, which jointly apply a hierarchical penalty to the contrastive loss, and enforce the hierarchy constraint.
The loss function is data driven and automatically adapts to arbitrary multi-label structures. Experiments on several datasets show that our relationship-preserving embedding performs well on a variety of tasks and outperform the base-line supervised and self-supervised approaches. Code is available at https://github.com/salesforce/ hierarchicalContrastiveLearning. 1.

Introduction
In the real world, hierarchical multi-labels occur natu-rally and frequently. Biological classification of organisms is structured in a taxonomic hierarchy. In e-commerce web-sites, retail spaces and grocery stores, products are orga-nized by several levels of categories. The hierarchical rep-resentation is a natural categorization of classes, and serves to efficiently represent the relationship between different classes. However, this relationship is seldom utilized in learning tasks, with traditional supervised approaches pre-ferring to organize their classes in a flat list. In single task learning problems, where a model is learned for one ob-jective only, a flat list of classes is a reasonable approach.
However, in representation learning frameworks, where a single embedding function can be used in a variety of down-stream tasks, utilizing all of the supervisory signal available is vital. In order to generalize to unknown downstream tasks and unseen data, the embedding function must represent the data concisely and accurately, which includes preserving the hierarchical categorization in the embedding space.
However, representation learning approaches that exploit this hierarchical relationship between labels have received very little attention. In recent years, several unsupervised
[5, 13, 16] and supervised [17, 18, 30, 42] metric learning frameworks have been proposed. These approaches typ-ically rely on minimizing the distance between represen-tations of a positive pair and maximizing the distance be-tween negative pairs. In the unsupervised (self-supervised) setting [5, 13, 33, 46], the positive pairs are different views of the same image, most typically obtained by random aug-mentations of the anchor image [5, 13, 33]. In the super-vised setting, labels are used to construct a wider variety of positive pairs, from different images of the same class and their augmentations [17, 18]. Positive pairs constructed from augmentations of the anchor image, and pairs con-structed from the anchor image and other images of the same class are considered to be equivalent, and the learning process attempts to minimize the distance between images in all of these positive pairs to the same degree. While repre-sentations learned in this paradigm may be satisfactory for a downstream task based on the supervisory label such as cat-egory prediction, other tasks such as instance prediction, re-trieval, attribute prediction and clustering can suffer due to the absence of direct supervision for these tasks. Addition-ally, these approaches do not support multi-label learning and are unable to utilize information about the relationship between labels.
Formally, in the hierarchical multi-label setting, each data point has multiple dependent labels, and the relation-ship between labels is best represented in a hierarchy. See
Figure 1(b) for a sample representation in a tree struc-ture. For example, in the DeepFashion dataset [21], each data point has 3 hierarchically structured labels: category (Denim, Cardigan, Shirts etc), product (identified by prod-uct id) and variation (typically color / pattern variations).
For the anchor image in Figure 1(a), which belongs to a specific product in the Denim category , the sub-category image is a different sample from the same product, and the category image is from a different product in the same cate-Figure 1. Hierarchical multi-label contrastive learning overview. A positive pair is constructed by pairing the anchor image with images drawn from all levels in the hierarchy. The learning objective of this work is to force positive pairs closer together, but the magnitude of the force is dependent on the common ancestry of the pairâ€™s labels. (a) The anchor image and corresponding positive pairings (blue) and negative pairings(red), visualized on a unit sphere. Different shades of blue nodes indicate their relationship to the anchor image, with darkness in shades of blue corresponding to increasing distance (both in the label space and representation space) from the anchor image.
The red data points are from different categories in the dataset and hence form negative pairs with the anchor image. (b) Representation of the sample images from (a) in the label hierarchy. A tree data structure is used to to visualize the multi-labels. gory. All the negative images are from different categories.
Our proposed approach leverages all the available labels to learn an embedding function that can preserve the label hierarchy in the embedding space. We develop a general representation learning framework that can utilize available ground truth and learn embeddings that generalize to a vari-ety of downstream tasks. We present two novel losses (and their combination) that exploit the relationship between hi-erarchical multi-labels and learn representations that can re-tain the label relationship in the representation space. The
Hierarchical Multi-label Contrastive Loss (HiMulCon) en-forces a penalty that is dependent on the proximity between the anchor image and the matching image in the label space.
In this setting, we define proximity in the label space as the overlap in ancestry in the tree structure. The Hierarchical
Constraint Enforcing Loss (HiConE) prevents the hierarchy violation, that is, it ensures that the loss from pairs farther apart in the label space is never smaller than the loss from pairs that are closer. Models learned under this framework can be used exactly like traditional representation learning frameworks, a model is trained with our novel loss func-tions to learn an efficient encoder network, and embeddings generated from this approach can be used in a variety of downstream tasks.
Our framework is not limited to the hierarchical multi-label scenario. It reduces to the supervised contrastive ap-proach [18] when only single level labels are available, and to the SimCLR [5] approach when no labels are available.
We demonstrate the efficacy of our framework in compar-ison to Khosla et al. [18], Chen et al. [5] and a standard cross entropy based approach on downstream tasks such as category prediction, sub-category retrieval and cluster-ing NMI [38]. These tasks also show that our approach preserves the hierarchical relationship between labels in the representation space, and generalizes to unseen data as well. 2.