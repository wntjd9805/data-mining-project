Abstract
Observing that the 3D captioning task and the 3D grounding task contain both shared and complementary in-formation in nature, in this work, we propose a unified framework to jointly solve these two distinct but closely re-lated tasks in a synergistic fashion, which consists of both shared task-agnostic modules and lightweight task-specific modules. On one hand, the shared task-agnostic modules aim to learn precise locations of objects, fine-grained at-tribute features to characterize different objects, and com-plex relations between objects, which benefit both caption-ing and visual grounding. On the other hand, by casting each of the two tasks as the proxy task of another one, the lightweight task-specific modules solve the captioning task and the grounding task respectively. Extensive ex-periments and ablation study on three 3D vision and lan-guage datasets demonstrate that our joint training frame-work achieves significant performance gains for each indi-vidual task and finally improves the state-of-the-art perfor-mance for both captioning and grounding tasks. 1.

Introduction
There is increasing research interest in the intersection field between 3D visual understanding and natural language processing, such as 3D dense captioning [9] and 3D visual grounding [1, 7, 21, 50]. These two tasks push the advance of the intersection field along different directions (i.e., from vision to language versus from language to vision), and en-couraging progress has been achieved by separately solving each task. It still remains an open issue on whether it is pos-sible to develop a unified framework to jointly solve the two closely related tasks in a synergistic fashion.
We observe that the two 3D vision-language tasks con-tain both shared and complementary information in nature, and it is possible to enhance the performance of both tasks if we treat one task as a proxy task of the other. On one
â€  Corresponding author: Jing Zhang. hand, each of the two tasks can be decomposed into several sub-tasks, and some of these sub-tasks share the common objectives and network structures. For example, as shown in the previous vision-language works [1,7,9,21,44,47,50] on RGB-D scans, both 3D dense captioning and 3D vi-sual grounding require: 1) a 3D object detector to detect the salient object proposals in a 3D scene, 2) a relation modeling module to model complex 3D relations among these detected objects, and 3) a multi-modal learning mod-ule to learn fused information from both visual features and textual features to generate sentences or produce bounding boxes based on each input sentence. On the other hand, the opposite procedures are also used to separately solve the two problems, namely, the captioning task is to gener-ate a meaningful textual description from the detected boxes (i.e., from vision to language), while the grounding task is to locate the desired box by understanding a given textual description (i.e., from language to vision).
Moreover, the 3D point clouds generated from RGB-D scans often contain rich and complex relations among dif-ferent objects, while the corresponding RGB data provides more fine-grained attribute information, such as color, tex-ture, and materials. Thus, the RGB-D scans intrinsically contain rich and abundant attribute and relation information for enhancing both 3D captioning and 3D grounding tasks.
However, we empirically observe that the 3D dense caption-ing task is more object-oriented, which tends to learn more attribute information of the target objects (i.e., the objects of interest) in a scene and only the primary relationship be-tween the target object and its surrounding objects. In con-trast, the 3D visual grounding task is more relation-oriented, which focuses more on the relations between objects and distinguishes different objects (especially the objects from the same class) based on their relations. Thus, it is desir-able to develop a joint framework to unify both 3D dense captioning and 3D visual grounding tasks and take advan-tage of each other for improving the performance of both tasks.
To this end, in this work, we propose a joint frame-work by unifying the distinct but closely related 3D vision-language tasks of 3D dense captioning and 3D visual grounding. Specifically, the proposed framework consists of three main modules: (1) a 3D object detector, (2) an at-tribute and relation-aware feature enhancement module, and (3) a task-specific grounding or captioning head. Specifi-cally, the 3D object detector and the feature enhancement module are task-agnostic, which are designed for collab-oratively supporting both captioning and grounding tasks.
The two modules output the object proposals as the initial localization results of the potential objects in a scene, as well as the improved features within the proposals by inte-grating both attribute information from each object proposal and the complex relations between multiple proposals. With the strong task-agnostic modules, the task-specific caption-ing head and grounding head are designed as lightweight networks for dealing with each task, which consist of a lightweight transformer-based module together with simple preprocessing modules (i.e., the Query/Key/Value genera-tion modules) and lightweight postprocessing modules (i.e., the word prediction or bounding box selection module). In this way, the 3D captioning and 3D visual grounding tasks can be cast as the proxy task of each other. In other words, the more object-oriented captioning task can provide more attribute information to potentially improve the grounding performance, while the more relation-oriented grounding task can help improve the captioning results by enhancing the captioning task with more relation information. More-over, our joint framework also inspires the insights of the design of each individual captioning network and ground-ing network.
The contribution of this work is two-fold: (1) By an-alyzing both 3D dense captioning and 3D visual ground-ing tasks, we propose a unified framework to jointly solve the two distinct but closely related tasks by using our simple and strong network structure, which consists of a task-agnostic module with a 3D object detector and an attribute and relation-aware feature enhancement module, and two lightweight task-specific modules (i.e., a caption-(2) Extensive exper-ing head and a grounding head). iments conducted on three benchmark datasets ScanRe-fer [7], Scan2Cap [9], and Nr3D dataset [1] demonstrate our joint framework achieves the state-of-the-art results for both 3D dense captioning and 3D visual grounding tasks. 2.