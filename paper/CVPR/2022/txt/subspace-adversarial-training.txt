Abstract
Single-step adversarial training (AT) has received wide attention as it proved to be both efficient and robust. How-ever, a serious problem of catastrophic overfitting exists, i.e., the robust accuracy against projected gradient descent (PGD) attack suddenly drops to 0% during the training. In this paper, we approach this problem from a novel perspec-tive of optimization and firstly reveal the close link between the fast-growing gradient of each sample and overfitting, which can also be applied to understand robust overfitting in multi-step AT. To control the growth of the gradient, we propose a new AT method, Subspace Adversarial Training (Sub-AT), which constrains AT in a carefully extracted sub-It successfully resolves both kinds of overfitting space. and significantly boosts the robustness.
In subspace, we also allow single-step AT with larger steps and larger ra-dius, further improving the robustness performance. As a result, we achieve state-of-the-art single-step AT perfor-mance. Without any regularization term, our single-step AT can reach over 51% robust accuracy against strong PGD-50 attack of radius 8/255 on CIFAR-10, reaching a compet-itive performance against standard multi-step PGD-10 AT with huge computational advantages. The code is released at https://github.com/nblt/Sub-AT. 1.

Introduction
Adversarial training (AT) [23], which aims to minimize the model’s risk under the worst-case perturbations, is cur-rently the most effective approach for improving the robust-ness of deep neural networks. For a given neural network f (x, w) with parameters w, the optimization objective of
AT can be formulated as follows:
E(x,y)∼D min w (cid:20) max
δ∈B(x,ϵ)
L (f (x + δ, w), y)
, (cid:21) where B(x, ϵ) is the norm ball with radius ϵ and L is the loss function. The key issue of AT lies in solving the in-ner worst-case problem by generating adversarial examples.
Figure 1. Catastrophic overfitting in single-step AT. The ex-periments are conducted on CIFAR-10 with PreAct ResNet18 model for adversarial robustness against ℓ∞ perturbations of ra-dius 8/255. The robust accuracy of single-step Fast AT on the validation set against PGD-20 attack abruptly drops to 0 in one single epoch, characterized by a rapid explosion of the average gradient norm of each sample.
Presently the most efficient way to generate adversarial ex-amples is the fast gradient sign method (FGSM) [9], i.e., xadv = x + ϵ · sgn (∇xL(f (x, w), y)) .
Since the adversarial examples above are generated by one-step gradient propagation, the corresponding AT is called single-step AT. In Fig. 1, we demonstrate a standard single-step AT process where the training robust accuracy against
FGSM attack keeps increasing. However, the generaliza-tion capability, i.e., the robust accuracy on the validation set under projected gradient descent (PGD) attack [23], can suddenly drop to zero, which is a typical overfitting phe-nomenon referred as catastrophic overfitting [40].
Many works [1, 16, 17, 32, 37, 40] are devoted to resolv-ing such an intriguing overfitting problem. One approach to tackle the overfitting is to use a judiciously designed learn-ing rate schedule as well as appropriate regularizations. For example, Wong et al. [40] proposed to add a random step to FGSM and introduce cyclic learning rates [30] to over-come the overfitting. Andriushchenko et al. [1] proposed a novel regularization term called GradAlign to further im-prove the quality of single-step AT solutions. However,
these methods highly rely on specifically designed learning rate schedules, which need to be tuned carefully for differ-ent tasks. Another approach is to generate more precise ad-versarial examples. For example, Kim et al. [17] suggested verifying the inner interval along the adversarial direction and searching for appropriate step size. PGD AT, a typical multi-step AT which generates adversarial examples using multiple iterations, can also help avoid catastrophic over-fitting. However, these methods require multiple forward propagations. More seriously, overfitting can still promi-nently occur in multi-step AT (known as robust overfitting) as demonstrated by Rice et al. [28]. i (cid:80)n
, w), y)(cid:13) i=1 ∇wL(f (xadv
In order to understand this interesting phenomenon, let us investigate what happens at the 64-th epoch in Fig. 1 when catastrophic overfitting occurs. Before the overfit-ting, the training robust accuracy has already stepped into a stable stage, indicating the small norm of batch gradi-ent (cid:13) (cid:13) 1 (cid:13)2 (n denotes the batch n size). There are two possibilities for the small batch gradi-ent: i) the gradient of each sample is small; ii) the gradients of samples does not converge, but they cancel each other, resulting in an overall balanced state. We then plot the av-erage norm of each sample’s gradient on one fixed training (cid:13) batch (i.e., 1 (cid:13)∇wL(f (xadv (cid:13)2) in red. An n interesting thing is that before the overfitting, the average norm stays almost constant. However, it abruptly increases in the moment when the overfitting occurs. Intuitively, at that time, the balance of gradient is broken — the network tries to capture each sample’s label with huge fluctuations, namely large gradients, a significant signal of overfitting.
This phenomenon also coincides with the recent discussion on the connection between the gradient variance and gener-alization capability [12, 15, 25].
, w), y)(cid:13) (cid:80)n i=1 i
Inspired by the link between large gradients and overfit-ting, we propose to resolve the overfitting by controlling the magnitude of the gradient. A possible way is to restrict the gradient descent in a subspace instead of the whole parame-ter space, to prevent the excessive growth of the gradient.
The key challenge lies in keeping the network’s capabil-ity in such a subspace, which has been recently discussed in [20] showing that, optimizing parameters in a tiny sub-space extracted from training dynamics could keep the per-formance. Based on this discovery, we propose a new AT method called Subspace Adversarial Training (Sub-AT), which identifies such an effective subspace and conducts
AT in it. From the training statistics of Sub-AT in Fig. 2a, we observe that it successfully controls the average gradient norm under a low level (the yellow dotted curve), thus re-solving the catastrophic overfitting. Meanwhile, the robust accuracy is significantly improved from 0.4 to nearly 0.5 (the yellow solid curve). The sensitivity to learning rates is also fundamentally overcome as we only use a constant learning rate, and the results remain similar across a wide range of choices. As a direct extension, Sub-AT can be ap-plied to mitigate the robust overfitting (Fig. 2b) in multi-step
AT, implying the similar essence behind these two phenom-ena. Thus for the first time, the two overfittings, which were previously treated separately [1], are now connected and re-solved in a unified approach.
Since training in subspace controls the gradient mag-nitude and hence fundamentally resolves the catastrophic overfitting, we now can allow larger steps and radius, which previously requires the assistance of delicate regulariza-tions, e.g. GradAlign [1].
It brings further improvement on robustness, from which it follows that pure single-step-based AT (without regularization terms) achieves competi-tive robustness with standard multi-step PGD AT with great computational benefits, answering a long-existing question:
Can single-step AT achieve comparable robustness against iterative attacks than multi-step AT?
Our Sub-AT uncovers the long-neglected potential of single-step AT and can enlighten more efficient and pow-erful AT algorithms.
Our main contributions can be summarized as follows:
• We approach the catastrophic overfitting in single-step
AT from a novel view of optimization and firstly re-veal the close link between the fast-growing gradient of each sample and overfitting, which can also be ap-plied to explain the robust overfitting in multi-step AT.
• We propose an efficient AT method, Sub-AT, which constrains AT in a carefully extracted subspace, to control the growth of gradient. It uniformly resolves both kinds of overfitting, significantly improves the ro-bustness, and successfully overcomes the sensitivity to learning rates.
It is also very easy to combine with other AT methods to bring consistent improvements.
• Our Sub-AT achieves state-of-the-art adversarial ro-bustness on single-step AT and can successfully train with larger steps and larger radius, which brings fur-ther improvements. Notably, our pure single-step AT achieves over 51% robust accuracy against PGD-50 at-tack of ϵ = 8/255 on CIFAR-10, competitive to the multi-step PGD-10 AT with great time benefits. 2.