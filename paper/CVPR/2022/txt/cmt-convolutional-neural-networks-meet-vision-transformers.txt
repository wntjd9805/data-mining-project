Abstract
Vision transformers have been successfully applied to image recognition tasks due to their ability to capture long-range dependencies within an image. However, there are still gaps in both performance and computational cost between transformers and existing convolutional neural networks (CNNs). In this paper, we aim to address this issue and de-velop a network that can outperform not only the canonical transformers, but also the high-performance convolutional models. We propose a new transformer based hybrid network by taking advantage of transformers to capture long-range dependencies, and of CNNs to extract local information.
Furthermore, we scale it to obtain a family of models, called
CMTs, obtaining much better trade-off for accuracy and efficiency than previous CNN-based and transformer-based models. In particular, our CMT-S achieves 83.5% top-1 accuracy on ImageNet, while being 14x and 2x smaller on
FLOPs than the existing DeiT and EfficientNet, respectively.
The proposed CMT-S also generalizes well on CIFAR10 (99.2%), CIFAR100 (91.7%), Flowers (98.7%), and other challenging vision datasets such as COCO (44.3% mAP), with considerably less computational cost. 1.

Introduction
The past decades have witnessed the extraordinary contri-bution of CNNs [16, 46, 47, 52, 53] in the field of computer vision due to its ability of extracting deep discriminative fea-tures. Meanwhile, self-attention based transformers [9, 58] has become the de facto most popular models for natural language processing (NLP) tasks, and shown excellent ca-pability of capturing long-distance relationships. Recently, many researchers attempt to apply the transformer-based architectures to vision domains, and achieve promising re-sults in various tasks such as image classification [10, 57], object detection [2, 70], and semantic segmentation [69]. Vi-sion transformer (ViT) [10] is the first work to replace the
*Corresponding author. Mindspore [23] implementation can be found in: https://gitee.com/mindspore/models/tree/master/research/cv/CMT. Py-torch [43] implementation: https://github.com/huawei-noah/CV-Backbones. conventional CNN backbone with a pure transformer. Input images (224×224×3) are first split into 196 non-overlapping patches (with a fixed size of 16×16×3 per patch), which are analogous to the word tokens in NLP. The patches are then fed into stacked standard transformer blocks to model global relations and extract feature for classification. The design paradigm of ViT has heavily inspired the following trans-former based models for computer vision, such as IPT [3] for low-level vision and SETR [69] for semantic segmentation.
Despite that transformers have demonstrated excellent ca-pabilities when migrated to vision tasks, their performances are still far inferior to similar-sized convolutional neural net-work counterparts, e.g., EfficientNets [53]. We believe the reason of such weakness is threefold. Firstly, images are split into patches in ViT [10] and other transformer-based models such as IPT [3] and SETR [69]. Doing so can greatly simplify the process of applying transformer to image-based tasks. And the sequence of patches can be directly fed into a standard transformer where long-range dependencies be-tween patches can be well captured. However, it ignores the fundamental difference between sequence-based NLP tasks and image-based vision tasks, e.g., the 2D structure and spatial local information within each patch. Secondly, trans-former is difficult to explicitly extract low-resolution and multi-scale features due to the fixed patch size, which poses a big challenge to dense prediction tasks such as detection and segmentation. Thirdly, the computational and memory cost of self-attention modules in transformers are quadratic (O(N 2C)) to the resolution of inputs, compared to O(N C 2) of convolution-based CNNs. High resolution images are prevalent and common, e.g., 1333×800 in COCO [35] and 2048×1024 in Cityscapes [7]. Using transformers to pro-cess such images would inevitably cause the problem of insufficient GPU memory and low computation efficiency.
In this paper, we stand upon the intersection of CNNs and transformers, and propose a novel CMT (CNNs meet trans-formers) architecture for visual recognition. The proposed
CMT takes the advantages of CNNs to compensate for the aforementioned limitations when utilizing pure transformers.
As shown in Figure 2(c), input images first go through the convolution stem for fine-grained feature extraction, and are
Model
DeiT-Ti [57]
CPVT-Ti-GAP [6]
CeiT-T [67]
EfficientNet-B3 [53]
CMT-XS
ResNet-50 [16]
DeiT-S [57]
RegNetY-4GF [44]
T2T-ViT-14 [68]
ResNeXt-101 [64]
PVT-M [60]
Swin-T [36]
CPVT-S-GAP [6]
CeiT-S [67]
CvT-13-NAS [63]
EfficientNet-B4 [53]
CMT-S
Top1 Acc. 72.2% 74.9% 76.4% 81.6% 81.7% 76.2% 79.8% 80.0% 80.6% 80.9% 81.2% 81.3% 81.5% 82.0% 82.2% 82.9% 83.5%
# Params 5M 6M 5M 12M 14M 26M 22M 21M 21M 84M 44M 29M 23M 24M 18M 19M 25M
# FLOPs 1.3B 1.3B 1.2B 1.8B 1.5B 4.1B 4.6B 4.0B 4.8B 32B 6.7B 4.5B 4.6B 4.5B 4.1B 4.2B 4.0B
Model
ConT-M [66]
ResNet-101 [16]
RelationNet++ [4]
ResNeXt-101-64x4d [64]
Swin-T [36]
PVT-M [60]
Twins-SVT-S [5]
CMT-S
# FLOPs mAP 217B 37.9% 315B 38.5% 266B 39.2% 473B 41.0% 245B 41.5% 283B 42.0% 42.3% 209B 44.3% 230B (a) ImageNet Accuracy vs. FLOPs (b) COCO mAP vs. FLOPs
Figure 1. Performance comparison between CMT and other models. (a) Top-1 accuracy on ImageNet [8]. (b) Object detection results on
COCO val2017 [35] of different backbones using RetinaNet framework, all numbers are for single-scale, “1x” training schedule. then fed into a stack of CMT blocks for representation learn-ing. Specifically, the introduced CMT block is an improved variant of transformer block whose local information is en-hanced by depth-wise convolution. Compared to ViT [10], the features generated from the first stage of CMT can main-tain higher resolution, i.e., H/4×W/4 against H/16×W/16 in ViT, which are essential for other dense prediction tasks.
Furthermore, we adopt the stage-wise architecture design similar to CNNs [16,46,53] by using four convolutional layer with stride 2, to gradually reduce the resolution (sequence length) and increase the dimension flexibly. The stage-wise design helps to extract multi-scale features and alleviate the computation burden caused by high resolution. The local perception unit (LPU) and inverted residual feed-forward net-work (IRFFN) in CMT block can help capture both local and global structure information within the intermediate features and promote the representation ability of the network. Fi-nally, the average pooling is used to replace the class token in
ViT for better classification results. In addition, we propose a simple scaling strategy to obtain a family of CMT variants.
Extensive experiments on ImageNet and other downstream tasks demonstrate the superiority of our CMT in terms of accuracy and FLOPs. For example, our CMT-S achieves 83.5% ImageNet top-1 with only 4.0B FLOPs, while being 14x and 2x less than the best existing DeiT [57] and Efficient-Net [53], respectively. In addition to image classification,
CMT can also be easily transferred to other vision tasks and serve as a versatile backbone. Using CMT-S as the backbone,
RetinaNet [34] can achieve 44.3% mAP on COCO val2017, outperforming the PVT-based RetinaNet [60] by 3.9% with less computational cost. 2.