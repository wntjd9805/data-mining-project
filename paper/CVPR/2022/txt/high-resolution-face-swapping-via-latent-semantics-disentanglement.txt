Abstract 1.

Introduction
We present a novel high-resolution face swapping method using the inherent prior knowledge of a pre-trained GAN model. Although previous research can leverage generative priors to produce high-resolution results, their quality can suffer from the entangled semantics of the latent space. We explicitly disentangle the latent semantics by utilizing the progressive nature of the generator, deriving structure at-tributes from the shallow layers and appearance attributes from the deeper ones. Identity and pose information within the structure attributes are further separated by introducing a landmark-driven structure transfer latent direction. The disentangled latent code produces rich generative features that incorporate feature blending to produce a plausible swapping result. We further extend our method to video face swapping by enforcing two spatio-temporal constraints on the latent space and the image space. Extensive experiments demonstrate that the proposed method outperforms state-of-the-art image/video face swapping methods in terms of hallucination quality and consistency. Code can be found at: https://github.com/cnnlstm/FSLSD_HiRes.
*Corresponding author (hesfe@scut.edu.cn).
Face swapping aims at transferring the identity from a source face image to a target face image, while preserving attributes in the target image such as facial expression, head pose, illumination and background. It has received extensive attention in the computer vision and graphics community due to its wide range of potential applications, such as computer games, special effects and privacy protection [13, 22, 41, 42].
The main challenge for face swapping is to identify the highly entangled target facial attributes and source identity information for a natural-looking swapping. Early works such as [8] replace the pixels of face regions and rely on similarities between the source and the target in pose and illumination. 3D-based approaches [14,28,37] ﬁt a 3D model to the faces and can handle large pose variation, although the
ﬁtting can be largely inﬂuenced by the environment and thus unstable. Other works introduce the generative adversarial networks (GANs) for hallucinating target attributes [24, 26, 35] due to its strong generative capability.
Though much progress has been made, many existing
GAN-based approaches do not work well on high-resolution faces, due to the compressed representation of end-to-end frameworks [7, 26, 34], the instability of adversarial train-ing [5], and the limitation in GPU memory size. Recently,
Zhu et al. [55] utilize the inherent prior knowledge of a pre-trained high-resolution GAN model and propose MegaFS for high-resolution face swapping in the latent space of Style-GAN [20, 21]. It learns to assemble the inverted latent codes of the source and target images, and directly feed the fused code to a StyleGAN generator to produce the swapped result.
However, since the identity and attributes are highly entan-gled in the latent space, assembling two latent codes without explicit guidance cannot guarantee the transfer of source identity and the preservation of target attributes simultane-ously. Moreover, ﬁne details embedded in the latent codes are easily diluted after the assembly, hence, their swapped face tends to have a blurred appearance with some loss of
ﬁne details (see Fig. 1c and Fig. 1f for examples). To obtain disentangled semantics in the latent space, we argue that face features should be transferred in a class-speciﬁc manner.
Intuitively, the structure attributes of a face, such as the facial shape, pose, and expression, should be treated differently than the appearance attributes such as illumination and skin tones. The swapped face should retain the source identity while hallucinating the appearance attributes of the target image. Such separate treatments would require proper disen-tanglement between the structure and appearance features.
In this paper, we delve into the latent semantics of Style-GAN. StyleGAN is a noise-to-image, coarse-to-ﬁne gen-eration process, and we leverage its progressive nature to disentangle key factors in swapping such as pose, expression, and appearance. Given the inverted source latent code, we decouple the structure (pose and expression) attributes from identity by deriving a structure transfer latent direction. The direction is determined by source and target landmarks, and serves as a latent space operation that transfers the structure.
On the other hand, the appearance attributes are controlled in the deeper layers. Therefore, we regroup the target’s ap-pearance codes with the structure-transferred source code in the deep layers. In this way, the integrated latent code retains the identity attribute from the source, while having the appearance and structure of the target. This disentangled latent code is fed to the StyleGAN generator to produce generative features. This rich prior knowledge is aggregated with the target features in all scales, effectively eliminating the noticeable blending artifacts.
Furthermore, we extend our model to video face swap-ping, where we generate a swapped face video from a source face image and a target face video. Since directly applying our model to each video frame can lead to incoherence ar-tifacts, we enforce two spatio-temporal constraints on the disentangled structure and appearance semantics. First, we require the inter-frame structural changes of the swapped faces to be consistent with those in the target faces. This is achieved by enforcing similarity between the swapped and target faces in terms of the latent offsets in the shallow layers (representing structural changes). Secondly, we adopt a linear assumption for the changes of image content be-tween neighboring frames in the output video. These two constraints effectively ensure inter-frame coherence. Exten-sive experiments on several benchmarks for face swapping methods demonstrate superior performance against state-of-the-arts. As far as we are aware, this is the ﬁrst feasible solution for high-resolution video face swapping.
In summary, our contributions are three-fold:
• We propose a novel framework for high-resolution face swapping. We disentangle the latent semantics of a pre-trained StyleGAN, enabling the transfer of source identity while preserving the appearance and structure of the target.
• We tailor two novel constraints to enforce the coherence of swapped face videos, including a code trajectory constraint that limits the offset between the latent codes of neighbor-ing frames, and a ﬂow trajectory constraint that works in the RGB space to guarantee the video smoothness.
• Experiments on several datasets demonstrate state-of-the-art results from our method, which can potentially serve as new high-resolution test cases for face forgery detection. 2.