Abstract
We propose Styleformer, a generator that synthesizes im-age using style vectors based on the Transformer structure.
In this paper, we effectively apply the modified Transformer structure (e.g., Increased multi-head attention and Pre-layer normalization) and introduce novel Attention Style
Injection module which is style modulation and demodu-lation method for self-attention operation. The new gen-erator components have strengths in CNNâ€™s shortcomings, handling long-range dependency and understanding global structure of objects. We present two methods to generate high-resolution images using Styleformer. First, we apply
Linformer in the field of visual synthesis (Styleformer-L), enabling Styleformer to generate higher resolution images and result in improvements in terms of computation cost and performance. This is the first case using Linformer to im-age generation. Second, we combine Styleformer and Style-GAN2 (Styleformer-C) to generate high-resolution compo-sitional scene efficiently, which Styleformer captures long-range dependencies between components. With these adap-tations, Styleformer achieves comparable performances to state-of-the-art in both single and multi-object datasets.
Furthermore, groundbreaking results from style mixing and attention map visualization demonstrate the advantages and efficiency of our model. 1.

Introduction
Generative Adversarial Network (GAN) [21] is one of the widely used generative model. Since the appear of DC-GAN [43], convolution operations have been considered essential for high-resolution image generation and stable training. Convolution operations are created under the as-sumption of the locality and stationarity of the image (i.e., inductive bias), which is advantageous for image process-ing [37]. Through convolution neural networks (CNNs) with this strong inductive bias, GAN have efficiently gen-*Equal contribution. The order was determined randomly. erated realistic, high-fidelity images.
However, drawbacks of CNNs clearly exist. Local re-ceptive field of CNNs makes model difficult to capture long-range dependency and understanding global structure of ob-ject. Stacking multiple layers can solve this problem, but this leads to another problem of losing spatial information and fine details [55]. Moreover, sharing kernel weights across locations leads to unstable training when the pattern or styles differ by location in the image [56]. This is also related to the poor quality of generated structured images or compositional scenes (e.g., outdoor scenes), unlike the generation of a single object (e.g., faces)
In this paper, we propose Styleformer, a generator that uses style vectors based on the Transformer structure. Un-like CNNs, Styleformer utilizes self-attention operation to capture long-range dependency and understand global structure of objects efficiently. Furthermore, we overcome computation problem of Transformer and show superior performance not only in low-resolution but also in high res-olution images. Specifically, we introduce the following three models: 1) Styleformer - The basic block of Styleformer is based on Transformer encoder, so we introduce components that need to be changed for stable learning.
Inspired by Mo-bileStyleGAN [3], we enhance the multi-head attention in original Transformer by increasing the number of heads, al-lowing model to generate image efficiently. We also modify layer normalization, residual connection, and feed-forward network (Section 3.2). Moreover, we introduce novel atten-tion style injection module, suitable style modulation, and demodulation method for self-attention operation (Section 3.3). This design allows Styleformer to generate image sta-bly, and enables model to handle long-range dependency and understand global structures. 2) Styleformer-L - We sidestep scalability limitation arising from the quadratic mode of attention operation by applying Linformer [50] (Styleformer-L). As such,
Styleformer-L can generate high-resolution images with linear computational costs. This paper is the first case to ap-ply Linformer in the field of visual synthesis (Section 3.4).
3) Styleformer-C - We further combine Styleformer and
StyleGAN2, applying Styleformer at low resolution and style block of StyleGAN2 at high resolution (Styleformer-C). As can be seen from our experiments and analysis (e.g., style mixing and visualizing attention map), we show that
Styleformer-C with the structure above can generate com-positional scenes efficiently, and showing flexibility of our model. In detail, we prove that Styleformer in low resolu-tion help model to capture long-range dependency between components, and style block in high resolution help model to refine the details of each components such as color or texture. This novel blending structure enables fast training, which is the advantage of StyleGAN2, while maintaining the advantages of Styleformer that can generate structured images.(Section 4).
Styleformer achieves comparable performances to state-of-the-art in both single and multi-object datasets. We record FID 2.82 and IS 10.00 at the unconditional setting on CIFAR-10. These results outperform all GAN-based models including StyleGAN2-ADA [32] which recently recorded state-of-the-art. As can be expected, Styleformer show strength especially in multi-object images or com-positional scenes generation (e.g., CLEVR, Cityscapes).
Styleformer-C records FID 11.67, IS 2.27 in CLEVR, and
FID 5.99, IS 2.56 in Cityscapes, showing better perfor-mance than pure StyleGAN2. 2.