Abstract
We present Playable Environments—a new representa-tion for interactive video generation and manipulation in space and time. With a single image at inference time, our novel framework allows the user to move objects in 3D while generating a video by providing a sequence of desired actions. The actions are learnt in an unsupervised manner.
The camera can be controlled to get the desired viewpoint.
Our method builds an environment state for each frame, which can be manipulated by our proposed action mod-ule and decoded back to the image space with volumetric rendering. To support diverse appearances of objects, we extend neural radiance ﬁelds with style-based modulation.
Our method trains on a collection of various monocular videos requiring only the estimated camera parameters and 2D object locations. To set a challenging benchmark, we in-troduce two large scale video datasets with signiﬁcant cam-era movements. As evidenced by our experiments, playable environments enable several creative applications not at-tainable by prior video synthesis works, including playable 3D video generation, stylization and manipulation1. 1.

Introduction
What would you change in the last tennis match you saw? The actions of the player? The style of the ﬁeld, or, perhaps, the camera trajectory to observe a highlight more dramatically? To do so interactively, the geometry and the style of the ﬁeld and the players need to be reconstructed.
Players’ actions need to be understood and the outcomes of future actions anticipated. To enable these features one needs to reconstruct the observed environment in 3D and
*This work was partially done while interning at MPI for Informatics
†Equal senior contribution 1willi-menapace.github.io/playable-environments-website
Figure 1. Given a single initial frame, our method creates playable environments that allow the user to interactively generate different videos by specifying discrete actions to control players, manipu-lating the camera trajectory and indicating the style for each object in the scene. provide simple and intuitive interaction, offering an experi-ence similar to playing a video game. We call these repre-sentations Playable Environments (PE).
Such a representation enables multiple creative applica-tions, such as 3D- and action-aware video editing, camera trajectory manipulation, changing the action sequence, the agents and their styles, or continuing the video in time, be-yond the observed footage. Fig. 1 shows a playable envi-ronment for tennis matches. In it, the user speciﬁes actions to move the players, controls the viewpoint and changes the style of the players and the ﬁeld. The environment can be played, akin to a video game, but with real objects.
In this work, we propose a method to construct PEs of complex scenes that supports a large set of interactive ma-nipulations. Trained on a dataset of monocular videos, our method presents six core characteristics listed in Tab. 1 that enable the creation of such PEs. Our framework allows the user to interactively generate videos by providing discrete actions ⟨1⟩ and controlling the camera pose ⟨2⟩. Further-Name
Description
⟨1⟩ Playability The user can control generation with discrete actions.
⟨2⟩ Camera
The camera pose is explicitly controlled at test time. control
⟨3⟩ Multi-object Each object is explicitly modeled.
⟨4⟩ Deformable objects
⟨5⟩ Appearance changes
The model handles deformable object such as human bodies
The model handles objects whose appearance is not constant is the training set
⟨6⟩ Robustness The model is robust to calibration and localization er-rors.
Table 1. Characteristics of our method for Playable Environments.
Each row is referred in the text with ⟨·⟩ symbols. more, it can represent environments with multiple objects
⟨3⟩ with varying poses ⟨4⟩ and appearances ⟨5⟩ and is ro-bust to imprecise inputs ⟨6⟩. In particular, we do not require ground-truth camera intrinsics and extrinsincs, but assume they can be estimated for each frame. Neither do we assume ground-truth object locations, but rely on an off-the-shelf object detector [27] to locate the agents in 2D, such as both tennis players. No other supervision is required.
Playable Environments encapsulate and extend represen-tations built by several prior image or video manipulation methods. Novel view synthesis and volumetric rendering methods support re-rendering of static scenes. However, while some methods support moving or articulated objects
[24, 26, 34, 39], it is challenging for them to handle dy-namic environments and they do not allow user interaction, making them undesirable for modeling compelling environ-ments. Video synthesis methods manipulate videos by pre-dicting future frames [15, 16, 33, 35], animating [30–32] or playing videos [18], but environments modeled with such methods typically lack camera control and multi-object sup-port. Consequently, these methods limit interactivity as they do not take into account the 3D nature of the environment.
Our method consists of two components. The ﬁrst one is the synthesis module.
It extracts the state of the environment—location, style and non-rigid pose of each object—and renders the state back to the image space. Re-cently introduced Neural Radiance Fields (NeRFs) [19] rep-resent an attractive tool for their ability to render novel views. In this work, we introduce a style-based modiﬁca-tion of NeRF to support objects of different appearances.
Furthermore, we propose a compositional non-rigid volu-metric rendering approach handling the rigid parts of the scene and non-rigid objects. The second component—the action module—enables playability. It takes two consecu-tive states of the environment and predicts an action with respect to the camera orientation. We train our framework using reconstruction losses in the image space and the state space, and a novel loss for action consistency. Finally, to improve temporal dynamics, we introduce a temporal dis-criminator that operates on sequences of environment states.
To thoroughly evaluate ⟨1−6⟩, we introduce two com-plementary large-scale datasets for the training of playable environments, a synthetic and a real one. The ﬁrst is in-tended to evaluate ⟨1−5⟩, with a particular focus on cam-era control thanks to the synthetic ground truth, the second to evaluate ⟨1−6⟩, with a particular focus on ⟨4−6⟩ given the high diversity present in this dataset. We propose an extensive evaluation of our method with several baselines derived from existing NeRF and video generation methods.
These experiments show that our method is able to generate high-quality videos and outperforms all baselines in terms of playability, camera control and video quality.
In summary, the primary contributions of this work are as follows: a new framework for the creation of com-pelling Playable Environments with the characteristics in
Tab. 1, featuring a new compositional NeRF that handles deformable objects with different visual styles and an ac-tion module that operates in the latent space of our NeRF model; two challenging large-scale datasets for training and evaluating PEs to stimulate future research in this area. 2.