Abstract
In contrast,
Most self-supervised video representation learning ap-proaches focus on action recognition. in this paper we focus on self-supervised video learning for movie understanding and propose a novel hierarchical self-supervised pretraining strategy that separately pretrains each level of our hierarchical movie understanding model (based on [37]). Specifically, we propose to pretrain the low-level video backbone using a contrastive learning ob-jective, while pretrain the higher-level video contextualizer using an event mask prediction task, which enables the us-age of different data sources for pretraining different levels of the hierarchy. We first show that our self-supervised pre-training strategies are effective and lead to improved perfor-mance on all tasks and metrics on VidSitu benchmark [37] (e.g., improving on semantic role prediction from 47% to 61% CIDEr scores). We further demonstrate the effective-ness of our contextualized event features on LVU tasks [54], both when used alone and when combined with instance features, showing their complementarity. 1.

Introduction
Most of the latest research on self-supervised video rep-resentation learning (SSL) focuses on the task of action recognition [4, 9, 13, 17, 32, 34, 55]. This priority has largely influenced the design of these methods, as well as the type of datasets used to learn their representations. For example, they propose models that encourage the learning of short-term appearance and motion cues, as these are the most in-formative for action recognition. At the same time, they mostly focus on pretraining on the Kinetics [20] dataset, which consists of hundreds of thousands of short YouTube clips with diverse motion and semantic patterns. Unlike these works, we are interested in learning self-supervised video representations to understand movies.
Movies are however very complex and they require rea-soning at many levels: from the simple understanding of low-level actions to the interpretation of high-level semantic narratives, which require knowledge of the characters, their
*Work done while at Amazon, now at Meta AI
Figure 1. Hierarchical self-supervised pretraining. We pretrain the low-level video feature backbone using contrastive learning objectives on large collections of YouTube-style action clips; while pretrain the higher-level feature contextualizer using mask predic-tion on movies with rich temporal plots. histories, relationships, behaviours, etc. Towards building rich models for movie understanding, [37] recently pro-posed a hierarchical movie understanding model that learns in a fully supervised fashion. However, it is extremely dif-ficult to annotate large-scale video datasets, even for a rel-atively simple task like action classification, not to men-tion for complex movie tasks (e.g. labeling actor relation graphs [45]). To overcome this bottleneck, we propose a novel hierarchical self-supervised pretraining strategy that separately pretrains each level of this hierarchical model.
In details, the hierarchical movie model of [37] consists of two levels: a low-level video backbone encoder and a higher-level transformer contextualizer (Fig. 1). We design our hierarchical learning strategy to sequentially pretrain the backbone and the transformer encoder as they specialize in different aspects of movie understanding. The backbone is responsible for the heavy-lifting work to extract low-level appearance and motion cues for people, objects and scenes from raw pixels. Therefore, it needs to be high in capacity and can be trained on a large amount of YouTube videos (e.g., Kinetics [21]). Once we obtain an appropriate feature abstraction from the video backbone, we can treat such rep-resentations as visual “word tokens” and learn to contextu-alize the neighboring visual tokens. The contextualizer can be lightweight and trained on a small amount of training data with stronger semantic and temporal structures (i.e., movies).
In details, we propose to pretrain the video backbone using a contrastive learning objective, which helps mod-els learning the intra-instance invariances from visual cues.
This pretraining paradigm has shown to be very effective for action recognition [13, 17, 32, 34, 55]. Furthermore, we pretrain the higher level transformer model to produce con-textualized semantic representations using the mask predic-tion task, which has been shown effective for pretraining language models that take in word tokens for contextualiza-tion [7, 25]. These hierarchical self-supervised pretraining strategies bring two data advantages: they enable the use of different data sources for pretraining the different levels of the hierarchy, and they do not require any annotation, which are inherently expensive to collect.
We evaluate the impact of our pretrainings on the re-cently released VidSitu [37] and LVU [54] datasets. These are movie datasets that have been annotated for various tasks, ranging from low-level verb prediction (i.e., actions) to high-level semantic role prediction or event relation clas-sification (i.e., “A causes B”). Our results show that our self-supervised pretraining strategies are effective and lead to improved performance on all tasks and metrics. For exam-ple, on the task of Semantic Role Prediction, we improve
CIDEr [44] metric performance over the previous, fully-supervised, state-of-the-art [37] from 47% to 61%. Finally, we also ablate the design choices of our pretraining recipes. 2.