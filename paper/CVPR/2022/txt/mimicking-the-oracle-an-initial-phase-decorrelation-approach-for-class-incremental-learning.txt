Abstract
Class Incremental Learning (CIL) aims at learning a classifier in a phase-by-phase manner, in which only data of a subset of the classes are provided at each phase. Previous works mainly focus on mitigating forgetting in phases after the initial one. However, we find that improving CIL at its initial phase is also a promising direction. Specifically, we experimentally show that directly encouraging CIL Learner at the initial phase to output similar representations as the model jointly trained on all classes can greatly boost the
CIL performance. Motivated by this, we study the differ-ence between a na¨ıvely-trained initial-phase model and the oracle model. Specifically, since one major difference be-tween these two models is the number of training classes, we investigate how such difference affects the model rep-resentations. We find that, with fewer training classes, the data representations of each class lie in a long and narrow region; with more training classes, the representations of each class scatter more uniformly. Inspired by this obser-vation, we propose Class-wise Decorrelation (CwD) that ef-fectively regularizes representations of each class to scatter more uniformly, thus mimicking the model jointly trained with all classes (i.e., the oracle model). Our CwD is simple to implement and easy to plug into existing methods. Ex-tensive experiments on various benchmark datasets show that CwD consistently and significantly improves the per-formance of existing state-of-the-art methods by around 1% to 3%. Code: https://github.com/Yujun-Shi/CwD. 1.

Introduction
The ability to continually acquire new knowledge is a key to achieve artificial intelligence. To enable this ability for classification models, [8, 12, 19, 20, 26, 29, 33] introduce
*Work done when interning with Song Bai at ByteDance.
Figure 1. Visualization of representations (normalized to the unit sphere) in a two-phase CIL setting (learning 2 classes for each phase). (a) Na¨ıve training at the initial phase (a.k.a., phase 0). The data representations of each class lie in a long and narrow region. (b) Joint training on all 4 classes (oracle model). The data repre-sentations of each class scatter more uniformly. (c) Directly mim-icking the oracle model at the initial phase, i.e., training the CIL learner with a regularization term that enforces the learner to out-put representation that is similar to the oracle model. This makes the representations of each class scatter more uniformly (like (b)). (d) Training at the initial phase with our CwD regularizer, which also yields uniformly-scattered representations (like (b) and (c)).
Best viewed in color. and study Class Incremental Learning (CIL). In CIL, train-ing is conducted phase-by-phase, and only data of a subset of classes are provided at each phase. The goal of CIL is to perform well on classes learned at the current phase as well as all previous phases.
The major challenge of CIL is that the model per-formance on previously learned classes usually degrades catastrophic seriously after learning new classes, a.k.a.
forgetting [9, 23]. To reduce forgetting, most previous works [8,12,18,26,29] focus on phases after the initial one, e.g. introducing forgetting-reduction regularization terms that enforce the current-phase model and the previous-phase model to produce similar outputs of the same input.
However, the role of the initial phase in CIL (the phase before the CIL learner begins incrementally learning new classes) is largely neglected and much less understood.
We argue that the initial phase is of critical importance, since the model trained at this phase implicitly affects model learning in subsequent CIL phases (e.g., through the forgetting-reduction regularization term). In this work, we thus study whether and how we can boost CIL performance by improving the representations of the initial phase.
To start with and to motivate our method, we conduct an exploratory experiment to investigate the potential of im-proving CIL at its initial phase. Specifically, at the ini-tial phase, we regularize the CIL learner to produce sim-ilar representations as the model trained with data of all classes (i.e., the oracle model), since the upper bound of
CIL is the oracle model. According to our results, this addi-tional regularization drastically improves CIL performance.
In addition, as we experimentally show that, although this term is used in the initial phase, it yields little performance gain in the initial phase. In contrast, it significantly benefits
CIL performance in subsequent phases. This demonstrates that the performance improvements are not simply due to a higher accuracy at the initial phase, but because this regu-larization makes the initial-phase representations more fa-vorable for incrementally learning new classes.
Inspired by this, we consider improving CIL from a novel perspective—encouraging the CIL learner to mimic the oracle model in the initial phase. To achieve this, we first need to understand the difference between representa-tions produced by a na¨ıvely-trained initial-phase model and the oracle model. Specifically, since the oracle model is trained with more classes, we investigate how representa-tions are affected by the number of training classes. To this end, we compute and analyze the eigenvalues of the covari-ance matrix of representations of each class. Interestingly, we find that when training with fewer classes, the top eigen-values of the covariance matrix of representations of each class dominate, indicating that the representations of each class lie in a long and narrow region (see Fig. 1 (a) for ex-ample). On the other hand, for models trained with more classes (particularly, the oracle model), the top eigenvalues become less dominant, indicating that the representations of each class scatter more uniformly (see Fig. 1 (b)).
We are thus motivated to enforce data representations of each class to be more uniformly scattered at the initial phase, which mimics the representations produced by the oracle model. To this end, we first theoretically show that, a group of embeddings will scatter more uniformly in the space if its correlation matrix has smaller Frobenius norm.
We then propose to minimize the Frobenius norm of the correlation matrix of the data representations for each class.
We refer to our regularization term as Class-wise Decor-relation (CwD). We provide a visualization to summarize our motivation and methodology in Fig. 1. Our proposed
CwD regularizer can serve as a generic plug-in to other CIL methods and can be easily implemented.
Extensive experiments on various benchmark datasets show that our CwD regularizer works well with state-of-the-art CIL methods, yielding significant and consistent perfor-mance gain in different settings. In addition, we also per-form detailed ablation studies on how the effectiveness of
CwD is influenced by factors such as the number of classes at the initial CIL phase, the number of exemplars for each class and regularization coefficient of the CwD term.
The contributions of this paper are as follows: 1) We em-pirically discover that encouraging the CIL learner to mimic the oracle model in the initial phase can boost the CIL per-formance. 2) We find that compared with na¨ıvely-trained initial-phase model, data representations of each class pro-duced by the oracle model scatter more uniformly, and that mimicking such representations at the initial phase can ben-efit CIL. 3) Based on our findings, we propose a novel
Class-wise Decorrelation (CwD) regularization technique to enforce representations of each class to scatter more uni-formly at the initial CIL phase. 4) Extensive experiments show that our proposed CwD regularization yields consis-tent improvements over previous state-of-the-art methods. 2.