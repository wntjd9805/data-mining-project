Abstract
Adversarial training (AT) is always formulated as a min-imax problem, of which the performance depends on the inner optimization that involves the generation of adver-sarial examples (AEs). Most previous methods adopt Pro-jected Gradient Decent (PGD) with manually specifying at-tack parameters for AE generation. A combination of the attack parameters can be referred to as an attack strategy.
Several works have revealed that using a ﬁxed attack strat-egy to generate AEs during the whole training phase lim-its the model robustness and propose to exploit different at-tack strategies at different training stages to improve robust-ness. But those multi-stage hand-crafted attack strategies need much domain expertise, and the robustness improve-ment is limited. In this paper, we propose a novel frame-work for adversarial training by introducing the concept of
“learnable attack strategy”, dubbed LAS-AT, which learns to automatically produce attack strategies to improve the model robustness. Our framework is composed of a tar-get network that uses AEs for training to improve robust-ness, and a strategy network that produces attack strate-gies to control the AE generation. Experimental evalua-tions on three benchmark databases demonstrate the su-periority of the proposed method. The code is released at https://github.com/jiaxiaojunQAQ/LAS-AT. 1.

Introduction
Although deep neural networks (DNNs) have achieved great success in academia and industry, they could be easily
*The ﬁrst two authors contribute equally to this work. † Work done during an internship at Tencent AI Lab. ‡ Correspondence to: Baoyuan Wu (wubaoyuan@cuhk.edu.cn) and Xiaochun Cao (caoxiaochun@iie.ac.cn).
Hand-crafted
Attack Strategy 
Target Net training r o t a r e n e
G
E
A (cid:2196) (cid:2183) (cid:2187) (cid:2194) (cid:2185) (cid:2206) (cid:2204) (cid:2186) (cid:2183) (cid:2206) (a) Conventional AT
Strategy Net 
Automatic 
Attack Strategy 
Target Net training (b) LAS-AT r o t a r e n e
G
E
A (cid:2196) (cid:2183) (cid:2187) (cid:2194) (cid:2185) (cid:2206) (cid:2204) (cid:2186) (cid:2183) (cid:2206)
Figure 1. The difference between conventional AT and LAS-AT. (a) Conventional AT methods use a hand-crafted attack strategy to generate AEs. (b) The proposed LAS-AT uses a strategy network to automatically produce sample-dependent attack strategies. fooled by adversarial examples (AEs) [15,44] generated via adding indistinguishable perturbations to benign images.
Recently, many studies [2, 3, 12, 13, 19, 25, 32, 47] focus on generating AEs. It has been proven that many real-world applications [14, 30] of DNNs are vulnerable to AEs, such as image classiﬁcation [15, 22], object detection [26, 50], neural machine translation [23, 62], etc. The vulnerability of DNNs makes people pay attention to the safety of artiﬁ-cial intelligence and brings new challenges to the applica-tion of deep learning [17,18,54,55,61]. Adversarial training (AT) [33, 35, 43, 52] is considered as one of the most effec-tive defense methods to improve adversarial robustness by injecting AEs into the training procedure through a mini-max formulation. Under the minimax framework, the gen-eration of AEs plays a key role in determining robustness.
Several recent works improve the standard AT method from different perspectives. Although existing methods [4, 9,10,16,27,39] have made signiﬁcant progress in improving robustness, they rarely explore the impact of attack strategy on adversarial training. First, as shown in Fig. 1a, most existing methods leverage a hand-crafted attack strategy to generate AEs by manually specifying the attack parameters, e.g., PGD attack with the maximal perturbation of 8, itera-tion of 10, and step size of 2. A hand-crafted attack strategy    
Strategy Net
REINFORCE Gradients (cid:4668)(cid:1853)(cid:2869)(cid:481) (cid:1853)(cid:2870)(cid:481)(cid:1668)(cid:1668)(cid:1668)(cid:481) (cid:1853)(cid:3014)(cid:4669)
Attack Strategy (cid:4668)(cid:2205)(cid:4669)
Parameters (cid:2206)(cid:2185)(cid:2194)(cid:2187)(cid:2183)(cid:2196)
AE Generator (cid:2189)(cid:4666)(cid:2206)(cid:481) (cid:2183)(cid:481) (cid:2205)(cid:4667)
Target Net
Training (cid:2206)(cid:2183)(cid:2186)(cid:2204) (cid:2188)(cid:2205)(cid:4666)(cid:2206)(cid:4667)
Figure 2. The framework of proposed LAS-AT. It consists of a tar-get network and a strategy network. Given a clean image, the strat-egy network generates an attack strategy. The AE generator takes the strategy as well as the target network to generate an AE which is used to train the target network. Some non-differentiable op-erations (e.g. choosing the iteration times) related to attack break gradient ﬂow from the target network to the strategy network. As an alternative approach, REINFORCE algorithm [51] is applied to optimize the strategy network and we utilize the so-called “REIN-FORCE gradient” to update the strategy network. lacks ﬂexibility and might limit the generalization perfor-mance. Second, most methods use only one attack strat-egy. Though some works [5, 48, 58] have realized that ex-ploiting different attack strategies at different training stages could improve robustness, i.e., using weak attacks at the early stages and strong attacks at the late stages, they use manually designed metrics to evaluate the difﬁculty of AEs and still use one strategy at each stage. However, they need much domain expertise, and the robustness improvement is limited. They use sample-agnostic attack strategies that are hand-crafted and independent of any information of speciﬁc samples. There exist statistical differences among samples, and attack strategy should be designed according to the in-formation of the speciﬁc sample, i.e., sample-dependent.
To alleviate these issues, we propose a novel adversarial training framework by introducing the concept of “learnable attack strategy”, dubbed LAS-AT, which learns to automat-ically produce sample-dependent attack strategies for AE generation instead of using hand-crafted ones (see Fig. 1b).
Our framework consists of two networks, i.e., a target net-work and a strategy network. The former uses AEs for training to improve robustness, while the latter produces at-tack strategies to control the generation of AEs. The two networks play a game where the target network learns to minimize the training loss of AEs while the strategy net-work learns to generate strategies to maximize the training loss. Under such a gaming mechanism, at the early train-ing stages, weak attacks can successfully attack the target network. As the robustness improves, the strategy network learns to produce strategies to generate stronger attacks.
Unlike [5]
[48], and [58] that use designed metrics and hand-crafted attack strategies, we use the strategy network to automatically produce an attack strategy according to the given sample. As the strategy network updates according to the robustness of the target model and the given sample, the strategy network ﬁgures out to produce different strategies accordingly at different stages, rather than setting up any manually designed metrics or strategies. We propose two loss terms to guide the learning of the strategy network. One evaluates the robustness of the target model updated with the AEs generated by the strategy. The other evaluates how well the updated target model performs on clean samples.
Our main contributions are in three aspects: 1) We propose a novel adversarial training framework by introducing the concept of “learnable attack strategy”, which learns to au-tomatically produce sample-dependent attack strategies to generate AEs. Our framework can be combined with other state-of-the-art methods as a plug-and-play component. 2)
We propose two loss terms to guide the learning of the strat-egy network, which involve explicitly evaluating the robust-ness of the target model and the accuracy of clean samples. 3) We conduct experiments and analyses on three databases to demonstrate the effectiveness of the proposed method. 2.