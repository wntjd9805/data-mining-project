Abstract
In this paper, we take an early step towards video repre-sentation learning of human actions with the help of large-scale synthetic videos, particularly for human motion rep-resentation enhancement. Specifically, we first introduce an automatic action-related video synthesis pipeline based on a photorealistic video game. A large-scale human ac-tion dataset named GATA (GTA Animation Transformed
Actions) is then built by the proposed pipeline, which in-cludes 8.1 million action clips spanning over 28K ac-tion classes. Based on the presented dataset, we design a contrastive learning framework for human motion rep-resentation learning, which shows significant performance improvements on several typical video datasets for ac-tion recognition, e.g., Charades, HAA 500 and NTU-RGB.
Besides, we further explore a domain adaptation method based on cross-domain positive pairs mining to alleviate the domain gap between synthetic and realistic data. Extensive properties analyses of learned representation are conducted to demonstrate the effectiveness of the proposed dataset for enhancing human motion representation learning. 1.

Introduction
Spatiotemporal semantic features are important for video understanding. Early works [41, 44] adopt two-stream net-works to extract appearance features and motion informa-tion separately. However, the extraction of optical flow is expensive in both time and space. And the flow of objects and background are also retained, which introduce scene
*Equal Contribution.
Figure 1. The proposed contrastive learning framework for our
GATA dataset. Unlike previous methods, we construct positive pairs offline, which are different views rendered using the same semantic 3D skeleton action sequence with diverse backgrounds, human appearances and camera views. Our method samples real views rather than simple traditional data augmentation for con-trastive unsupervised learning. bias and thus affect the human motion representation learn-ing. [20] adopts 3D convolutions to capture the spatiotem-poral features directly from raw videos. However, stacked 3D convolutions require tremendous parameters and the motion dynamics are capture implicitly. Recently, many re-searchers [21, 25, 49] attempts to design elaborate architec-tures to extract motion features explicitly through neighbor-ing feature-level difference, which provide complementary features for action recognition and achieve convincing per-formance. Therefore, learning a strong motion representa-tion is essential for human action understanding. In contrast to the task-specific architecture designs for motion model-ing, we try to solve this problem from the data perspective.
To analyze the human motion extraction process, a large-scale motion-oriented human action dataset is essential.
However, existing public datasets, e.g., Kinetics [7, 8] and
YouTube-8M [3], fail to effectively support the motion rep-resentation learning due to the overwhelming bias of scene context [11]. That is said, correct action predictions can be made merely based on scene context instead of human actors. For example, a classroom environment or a white-board usually indicates the action of giving a lecture, while the actual activity in the scene is underrepresented. There are also some motion-oriented datasets, e.g., Charades [40] and NTU-RGB [39]. However, the size and diversity of these datasets are limited. Therefore, with the help of a high-performance automatic data collection pipeline based on GTA, we collect a large-scale synthetic video dataset named GATA, which contains ∼ 8.1M action instances cov-ering ∼28k classes. In this dataset, an action class is defined by a specific character animation or a pose sequence. Ran-domized human characters are controlled to play this action under diverse scene settings with random camera views. In short, scene bias is weakened or even eliminated in the pro-posed dataset. Figure 2 illustrates some examples of our
GATA.
Based on the proposed GATA dataset, a general and ro-bust human motion representations can be learned using a ready-made action recognition model (e.g., SlowFast [13],
TSM [26], etc.). And the encoded knowledge can be easily transferred to down-stream action understanding tasks. As shown in Figure 1, we design a contrastive learning frame-work for GATA, where a skeleton sequence is euqivalent to a sample, and the action instance is equivalent to a view in the tranditional contrastive learning setting. However, this view is not generated by simple online data augmentation but is rendered and stored offline by the CG pipeline. Fur-thermore, we analyze the learned representations through confusion matrix, Nearest-Neighbor retrieval and Class Ac-tivation Maps (CAMs) [50]. And we can find that our model tends to focus on human motion to recognize actions, while the model trained with Kinetics [8] is more inclined to rec-ognize actions through scenes and surrounding objects. Sur-prisingly, we also find that our synthetic GATA and web-crawled videos are complementary. By jointly training with
Kinetics and HAA500, the model can learn a more compre-hensive representation for scenes, objects, and human mo-tion. Furthermore, we propose a domain adaptation method based on cross-domain positive pair mining to alleviate the domain gap between synthetic and realistic data.
In summary, our contributions are three folds:
• We introduce an automatic high-performance data col-lection pipeline and synthesize a large-scale human ac-tion dataset. The videos of an action class are trans-formed from a specific character animation with the help of modern graphics technology, which is essen-tial for human motion modeling.
• We formalize the GATA training process with a con-trastive learning framework and design a joint con-trastive learning strategy together with realistic videos for a more comprehensive video representation.
• Detailed experiments are conducted to learn and an-alyze the human motion representation with the help of proposed GATA, which shows considerable perfor-mance improvement on downstream tasks and evident enhancement of motion modeling by training solely or jointly with our proposed GATA. 2.