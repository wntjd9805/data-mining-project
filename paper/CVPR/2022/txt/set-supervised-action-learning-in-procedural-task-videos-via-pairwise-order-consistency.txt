Abstract
We address the problem of set-supervised action learn-ing, whose goal is to learn an action segmentation model using weak supervision in the form of sets of actions occur-ring in training videos. Our key observation is that videos within the same task have similar ordering of actions, which can be leveraged for effective learning. Therefore, we pro-pose an attention-based method with a new Pairwise Order-ing Consistency (POC) loss that encourages that for each common action pair in two videos of the same task, the at-tentions of actions follow a similar ordering. Unlike ex-isting sequence alignment methods, which misalign actions in videos with different orderings or cannot reliably sep-arate more from less consistent orderings, our POC loss efﬁciently aligns videos with different action orders and is differentiable, which enables end-to-end training. In addi-tion, it avoids the time-consuming pseudo-label generation of prior works. Our method efﬁciently learns the actions and their temporal locations, therefore, extends the existing attention-based action localization methods from learning one action per video to multiple actions using our POC loss along with video-level and frame-level losses. By experi-ments on three datasets, we demonstrate that our method signiﬁcantly improves the state of the art. We also show that our method, with a small modiﬁcation, can effectively ad-dress the transcript-supervised action learning task, where actions and their ordering are available during training.1 1.

Introduction
Learning actions by partitioning long and untrimmed procedural videos into action segments has recently drawn increasing attention in video understanding.
Fully-supervised methods [23, 26, 46, 55, 57, 62] require dense annotation of training videos with framewise action la-bels, which is costly and unscalable. Therefore, weakly-supervised methods [5, 9, 13, 30, 31, 33, 37, 44, 45, 52, 58] learn from weak labels, in the form of action transcripts, 1Code available at https://github.com/ZijiaLewisLu/
CVPR22-POC.
Figure 1. Our attention-based framework with pairwise ordering consis-tency for set-supervised action learning. i.e., ordered lists of actions in videos, or action sets, i.e., sets of unique actions in videos (obtained from video narrations, captions or meta-tags). For action transcripts/sets, learning faces the major challenge of ﬁnding the temporal regions of actions in videos during the training. Learning from ac-tion sets, referred to as set-supervised action learning, addi-tionally faces the challenge of not knowing the ground-truth action orderings in training videos.
Most prior works on set-supervised action learning [31, 33, 44] alternate between three steps: i) generating action-transcripts from action-sets, ii) generating pseudo-labels from transcripts using Viterbi decoding, iii) training the model using pseudo-labels. However, generating transcripts and pseudo-labels is costly, slowing down the training and inference speeds, while the often erroneous pseudo-labels in early training iterations degrade the performance. [13] pro-poses a two-branch CNN that co-supervise each other and directly predicts the label and length of each video segment.
Despite faster training/inference speed, [13] uses a ﬁxed ra-tio of the number of segments to the video length, which cannot handle dense action regions, i.e., it may underseg-ment short videos that contain many actions. More im-portantly, it predicts the labels of segments independently, which ignores the ordering between actions and similar transcripts of training videos of each task.
In fact, videos of the same task often have similar tran-scripts. For example, in videos of ‘make fried-egg’, the step of ‘cracking egg’ is followed by ‘frying egg’. While such similar ordering has been the key in the develop-ment and success of unsupervised action learning meth-ods [1, 11, 12, 15, 25, 53, 64], it has not been exploited for set-supervised action learning. On the other hand, temporal attention is a powerful mechanism for ﬁnding temporal re-gions of actions in videos. However, existing works based on attention [27, 28, 38–40, 54, 63], assume one action is present in the video, which must be distinguished from the background frames. Therefore, extending temporal atten-tion to videos of complex tasks with multiple actions and regulating it according to ordering consistency within and across videos remains a major challenge.
Paper Contributions. In this paper, we propose a tempo-ral attention-based method for set-supervised action learn-ing by leveraging the similarity of action ordering of videos of each task, see Figure 1. We design a new loss function, referred to as Pairwise Ordering Consistency (POC) loss, which encourages that for each common pair of actions in videos of the same task, attention predictions follow the same ordering across videos. Our work has the following advantages with respect to the state of the art: – Our new POC loss resolves drawbacks of Dynamic Time
Warping (DTW) [48] and Edit Distance (ED) [29]. Un-like DTW, which misaligns actions when applied to videos with different transcripts (action orderings), our method correctly aligns actions and brings the representations of the same action closer to each other. Unlike ED, which may not properly distinguish different inconsistent order-ings from one another, our method efﬁciently distinguishes different consistency levels of action orderings by compar-ing the percentage of inconsistent action pairs. Addition-ally, our POC loss is differentiable, enabling feature learn-ing, and its computational complexity is linear in the num-ber of videos, thereby is scalable to large training sets. – Unlike three-step approaches [31, 33, 44], our method has a single step of minimizing a new loss, hence, enjoys faster training/inference and does not suffer from error ampliﬁ-cation. Unlike [13], our method uses similarity of a task’s videos, gives frame predictions, handles short videos with many actions and has much smaller number of parameters. – We extend the existing works on attention-based action lo-calization, from one to multiple actions in each videos. Our
POC loss penalizes the overlap between attentions of dif-ferent actions for better localization and enforces consistent ordering between actions, which has not been addressed in prior works on attention-based action localization. Un-like existing works, our method does not assume that the length of each action is a ﬁxed ratio of the total number of video frames and alternatively learns it via video-level ac-tion recognition, where the attention of an action is enforced to cover every possible frame of it. – Last, but not least, by extensive experiments on three datasets, we show that our proposed method outperforms existing set-supervised algorithms. We show that, with a small modiﬁcation, our method can also effectively address transcript-supervised action learning, where the actions and their orderings are available during training. 2.