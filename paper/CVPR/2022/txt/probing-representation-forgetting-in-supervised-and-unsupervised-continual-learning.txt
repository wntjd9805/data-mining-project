Abstract
Continual Learning (CL) research typically focuses on tackling the phenomenon of catastrophic forgetting in neu-ral networks. Catastrophic forgetting is associated with an abrupt loss of knowledge previously learned by a model when the task, or more broadly the data distribution, be-ing trained on changes.
In supervised learning problems this forgetting, resulting from a change in the model’s rep-resentation, is typically measured or observed by evaluating the decrease in old task performance. However, a model’s representation can change without losing knowledge about prior tasks. In this work we consider the concept of rep-resentation forgetting, observed by using the difference in performance of an optimal linear classiﬁer before and after a new task is introduced. Using this tool we revisit a num-ber of standard continual learning benchmarks and observe that, through this lens, model representations trained with-out any explicit control for forgetting often experience small representation forgetting and can sometimes be comparable to methods which explicitly control for forgetting, especially in longer task sequences. We also show that representation forgetting can lead to new insights on the effect of model ca-pacity and loss function used in continual learning. Based on our results, we show that a simple yet competitive ap-proach is to learn representations continually with standard supervised contrastive learning while constructing proto-types of class samples when queried on old samples.1 1.

Introduction
Continual Learning (CL) is concerned with methods for learners to manage changing training data distributions. The goal is to acquire new knowledge from new data distribu-*Equal contribution, name order randomized.
This research was partially funded by NSERC Discovery Grant RGPIN-2021-04104 and RGPIN-2019-05729. We acknowledge resources provided by Compute Canada and Calcul Quebec. Correspondence to: m davari@live.concordia.ca, nader.asadi@concordia.ca, eugene.belilovsky@concordia.ca results 1The code to reproduce our is publicly available at: https://github.com/rezazzr/Probing-Representation-Forgetting tions while not forgetting previous knowledge. A common scenario is CL in the classiﬁcation setting, where the class labels presented to the learner change over time. In this sce-nario, a phenomenon known as catastrophic forgetting has been observed [11,31]. This phenomenon is often described as a loss of knowledge about previously seen data and is ob-served in the classiﬁcation setting as a decrease in accuracy.
Deep learning has been traditionally motivated as an ap-proach, which can automatically learn representations [4], forgoing the need to design handcrafted features.
Indeed representation learning is at the core of deep learning meth-In the ods in supervised and unsupervised settings [10]. case of many practical scenarios we may not simply be in-terested in the ﬁnal performance of the model, but also the usefulness of the learned features for various downstream tasks [42]. Although a model’s representation may change, sometimes drastically at task boundaries [5], this does not necessarily imply a loss of useful information and may in-stead correspond to a simple transformation. For example, consider a standard multi-head CL setting, where each task shares a representation and only differs through task spe-ciﬁc “heads”. A permutation of the features leading into the classiﬁcation heads leads to total catastrophic forgetting as measured by standard approaches as the task heads no longer match with the representations. However, this does not correspond to a loss of knowledge about the data, nor a less useful representation. Indeed recent works have high-lighted the importance of fast remembering versus catas-trophic forgetting [14, 17], a looser continual learning re-quirement where in the task performance may decrease but the agent is able to recover rapidly upon observing a few samples from the previous task. In this light, maintaining a useful representation, which facilitates rapid recovery, is as important as maintaining high performance for the task.
CL envisions having learners operate over long time horizons while continually maintaining old knowledge and integrating new knowledge. Hence, in addition to directly measuring the performance on previous tasks using the last layer classiﬁers, it is sensible to consider the usefulness of
In this paper we their representations for previous tasks. highlight that traditional approaches of evaluating forget-ting are unable to properly disambiguate trivial changes in the features (e.g. permutation) from abrupt losses of useful representations. We instead use optimal Linear Probes (LP), commonly used to study unsupervised representations [8] and intermediate layer representations [36, 48], to evaluate
CL algorithms and their effectiveness.
We revisit several CL settings and benchmarks and mea-sure forgetting using LP. Our focus is particularly on re-evaluating ﬁnetuning approaches that do not apply explicit control for the non-iid nature of continual learning. We ob-serve that in many commonly studied cases of catastrophic forgetting, the representations under naive ﬁnetuning ap-proaches, undergo minimal forgetting, without losing criti-cal task information.
Our major contributions in this work are as follows. First we bring three new signiﬁcant insights obtained and demon-strated through extensive experimental analysis: 1. In a number of CL settings the observed accuracy can be a misleading metric for studying forgetting, partic-ularly when compared to ﬁnetuning approaches 2. Naive training with SupCon [21] or SimCLR (in the unsupervised case) have advantageous properties for continual learning, particularly in longer sequences. 3. By using LP based evaluation, forgetting is clearly de-creased for wider and deeper models, which is not seen that clearly from earlier observed accuracy.
Secondly, we suggest a simple approach to facilitate fast remembering, which does not require using a large memory during training; it relies only on a small memory combined with SupCon based ﬁnetuning. 2.