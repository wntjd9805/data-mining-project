Abstract
Segmenting or detecting objects in sparse Lidar point clouds are two important tasks in autonomous driving to allow a vehicle to act safely in its 3D environment. The best performing methods in 3D semantic segmentation or object detection rely on a large amount of annotated data.
Yet annotating 3D Lidar data for these tasks is tedious and costly.
In this context, we propose a self-supervised pre-training method for 3D perception models that is tailored to autonomous driving data. Specifically, we leverage the availability of synchronized and calibrated image and Li-dar sensors in autonomous driving setups for distilling self-supervised pre-trained image representations into 3D mod-els. Hence, our method does not require any point cloud nor image annotations. The key ingredient of our method is the use of superpixels which are used to pool 3D point features and 2D pixel features in visually similar regions.
We then train a 3D network on the self-supervised task of matching these pooled point features with the correspond-ing pooled image pixel features. The advantages of con-trasting regions obtained by superpixels are that: (1) group-ing together pixels and points of visually coherent regions leads to a more meaningful contrastive task that produces features well adapted to 3D semantic segmentation and 3D object detection; (2) all the different regions have the same weight in the contrastive loss regardless of the number of 3D points sampled in these regions; (3) it mitigates the noise produced by incorrect matching of points and pixels due to occlusions between the different sensors. Extensive experiments on autonomous driving datasets demonstrate the ability of our image-to-Lidar distillation strategy to pro-duce 3D representations that transfer well on semantic seg-mentation and object detection tasks. 1.

Introduction
Lidar sensors deliver rich information about the 3D world, and making sense of this kind of information is cru-Code available at https://github.com/valeoai/SLidR cial for an autonomous driving vehicle to properly act in its environment, across different external conditions. State-of-the-art methods for semantic segmentation or object de-tection in Lidar point clouds rely on deep neural networks trained on large collections of annotated point clouds. Yet, annotating 3D Lidar point clouds is a long and costly task [3, 20]. Self-supervision reduces the burden of anno-tating large datasets by exploiting a large amount of non-annotated data to pre-train neural networks, which are sub-sequently fine-tuned on a smaller set of annotated data.
The current best performing self-supervised techniques for 3D neural networks working on real point clouds are mostly adapted to indoor scenes with dense point clouds.
These methods suffer from several shortcomings when deal-ing with sparse point clouds, such as those acquired outdoor by a moving vehicle. For example, PointContrast [66] re-quires pairs of registered point clouds and a list of match-ing points between them. While multiple reliable match-ing points can be found in a densely sampled static scene, the number of such pairs of points is much lower in au-tonomous driving datasets, in particular on objects of inter-est (cars, trucks, pedestrians, etc.) as they are sparsely sam-pled and likely to move between two acquisitions. Depth-Contrast [71] avoids the need of finding pairs of correspond-ing points as it only requires a single representation for each scene. This representation is computed by global pooling and therefore loses information on small objects. These design choices limit significantly the performance of these methods in our experiments on autonomous driving scenes.
Our goal is to design a self-supervised method for tasks such as semantic segmentation or object detection in Lidar point clouds, and tailored to autonomous driving data. Most autonomous driving vehicles are equipped with an array of cameras and Lidar sensors that are synchronized and cali-brated, offering rich surround-view information. These data are a lot easier to acquire than to annotate, and we propose to leverage them to distill self-supervised pre-trained image representations into a 3D network. This whole pre-training process does not require any annotation of the images nor of the point clouds. Self-supervised pre-training on images has proven very successful for learning generic representations
Figure 1. SLidR distillates the knowledge of a pre-trained and fixed 2D network into a 3D network. It uses superpixels to pool features of visually similar regions together, both on the images, and on the point clouds through superpixels back-projection. The superpixel-driven contrastive loss aligns the pooled point and image features. The visualized segments proposed in this figure have been manually generated and are intentionally over-sized for illustrative purposes. Superpixels actually used can be observed on Fig. 2. that transfer well to various complex downstream tasks in 2D, often surpassing supervised pre-training [8, 22, 24, 28].
In this work, we show that these powerful representations can also be used to pre-train 3D networks for autonomous driving. We call this setting self-supervised 2D-to-3D rep-resentation distillation.
We propose a distillation loss suited to tasks such as se-mantic segmentation and object detection by forcing the networks to produce object-aware representations. Inspired by [31], we use superpixels [1, 18] which group visually similar regions that are likely to belong to the same ob-ject. We then use these superpixels as pooling masks for 3D point features and 2D pixel features, and enforce pairs of corresponding pooled features to match each other us-ing a contrastive loss, as illustrated in Fig. 1. This pool-ing strategy naturally mitigates two drawbacks encountered in autonomous driving data: (1) It reduces the noise in-duced by incorrect matching of points and pixels (which is performed automatically), e.g., caused by occlusions for one of the sensors; (2) It balances asymmetries between ar-eas with denser coverage of points and sparser areas, that would otherwise have different weights in the contrastive loss. The latter is particularly important for objects such as cars, pedestrians and cyclists, that are sampled more sparsely than the road near the ego-vehicle.
Finally, we examine key elements of our image-to-Lidar distillation method. This includes a careful design of the image feature projection head to avoid degenerate cases where no useful information is transferred to the 3D net-work.
In summary, our contributions are the following.
• We propose a novel self-supervised 2D-to-3D repre-sentation distillation approach based on a superpixel-to-superpoint contrastive loss and a carefully designed image feature upsampling architecture that allows high resolution image features to be distilled without suf-fering from degenerate solutions. We call this method
SLidR, for Superpixel-driven Lidar Representations.
• To the best of our knowledge, this work provides the first study on the self-supervised image-to-Lidar rep-resentation distillation problem for autonomous driv-ing data. This includes extensively evaluating our method for the downstream tasks of semantic segmen-tation on nuScenes [6] and SemanticKITTI [3] and ob-ject detection on KITTI [19], and comparing it against strong baselines. The latter were produced by adapting and optimizing several existing self-supervised pre-training methods for the autonomous driving setting.
• We demonstrate that our image-to-Lidar pre-training strategy surpasses, in all evaluation settings, state-of-the-art 3D self-supervised pre-training methods and prior 2D-to-3D distillation methods, devised for dense point clouds captured in indoor scenes.
2.