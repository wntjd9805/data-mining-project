Abstract
Deep learning researchers have a keen interest in proposing new novel activation functions that can boost neural network performance. A good choice of activation function can have a significant effect on improving network performance and training dynamics. Rectified Linear Unit (ReLU) is a popular hand-designed activation function and is the most common choice in the deep learning commu-nity due to its simplicity though ReLU has some drawbacks.
In this paper, we have proposed two new novel activation functions based on approximation of the maximum function, and we call these functions Smooth Maximum Unit (SMU and SMU-1). We show that SMU and SMU-1 can smoothly approximate ReLU, Leaky ReLU, or more general Maxout family, and GELU is a particular case of SMU. Replac-ing ReLU by SMU, Top-1 classification accuracy improves by 6.22%, 3.39%, 3.51%, and 3.08% on the CIFAR100 dataset with ShuffleNet V2, PreActResNet-50, ResNet-50, and SeNet-50 models respectively. Also, our experimental evaluation shows that SMU and SMU-1 improve network performance in a variety of deep learning tasks like im-age classification, object detection, semantic segmentation, and machine translation compared to widely used activa-tion functions. 1.

Introduction
Deep Neural network has emerged a lot in recent years and has significantly impacted our real-life applications.
Neural networks are the backbone of deep learning. An ac-tivation function is the brain of the neural network, which plays a central role in the effectiveness & training dynamics of deep neural networks. Hand-designed activation func-tions are quite a common choice in neural network models.
ReLU [36] is a widely used hand-designed activation func-tion. Despite its simplicity, ReLU has a major drawback, known as the dying ReLU problem in which up to 50% neu-rons can be dead during network training. To overcome the shortcomings of ReLU, a significant number of activations have been proposed in recent years, and Leaky ReLU [33],
Parametric ReLU [12], ELU [6], Softplus [56], Randomized
Leaky ReLU [53] are a few of them though they marginally improve performance of ReLU. Swish [41] is a non-linear activation function proposed by the Google brain team, and it shows some good improvement of ReLU. GELU [14] is an another popular smooth activation function.
It can be shown that Swish and GELU both are a smooth approxima-tion of ReLU. Recently, a few non-linear activations have been proposed which improves the performance of ReLU,
Swish or GELU. Some of them are either hand-designed or smooth approximation of Leaky ReLU function, and Mish
[34], ErfAct [2], Pad´e activation unit [35], Orthogonal Pad´e activation unit [1] are a few of them. 2.