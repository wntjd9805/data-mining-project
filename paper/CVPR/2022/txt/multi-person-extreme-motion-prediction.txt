Abstract
Human motion prediction aims to forecast future poses given a sequence of past 3D skeletons. While this prob-lem has recently received increasing attention, it has mostly been tackled for single humans in isolation. In this paper, we explore this problem when dealing with humans per-forming collaborative tasks, we seek to predict the future motion of two interacted persons given two sequences of their past skeletons. We propose a novel cross interaction attention mechanism that exploits historical information of both persons, and learns to predict cross dependencies be-tween the two pose sequences. Since no dataset to train such interactive situations is available, we collected ExPI (Extreme Pose Interaction) dataset, a new lab-based per-*Equal contribution.
†This research was supported by ANR-3IA MIAI (ANR-19-P3IA-0003), ANR-JCJC ML3RI (ANR-19-CE33-0008-01), H2020 SPRING (funded by EC under GA #871245), by the Spanish government with the project MoHuCo PID2020-120049RB-I00 and by an Amazon Research
Award. We also thank the Kinovis platform at Inria Grenoble, Laurence
Boissieux and Julien Pansiot at Inria Grenoble for their help, and Nvidia for hardware donation under the Academic Hardware Grant Program. And we thank Yuming Du, Yang Xiao, Meng Guo, Anand Ballou, Louis Airale for their kind suggestions and discussions. son interaction dataset of professional dancers performing
Lindy-hop dancing actions, which contains 115 sequences with 30K frames annotated with 3D body poses and shapes.
We thoroughly evaluate our cross interaction network on
ExPI and show that both in short- and long-term pre-dictions, it consistently outperforms state-of-the-art meth-ods for single-person motion prediction. Our code and dataset are available at: https://team.inria.fr/ robotlearn/multi-person-extreme-motion-prediction/ 1.

Introduction
The goal of human motion prediction is to predict future motions from previous observations. With the successful development of deep human pose estimation from single image [9, 18, 27, 37, 51, 52, 55, 56, 58, 59, 67], motion pre-diction begins to draw an increasing attention [3, 8, 16, 22, 23,26,29,33,38,43,47,49,50,60]. Most existing works for-mulate motion prediction as a sequence-to-sequence task, where past observations of 3D skeleton data are used to forecast future skeleton movements. A common denomi-nator of all these approaches is that they treat each pose
sequence as an independent and isolated entity: the motion predicted for one person relies solely on her/his past motion.
However, in real world scenarios people interact with each other, and the motion of one person is typically dependent on or correlated with the motion of other people. Thus, we could potentially improve the performance of motion pre-diction by exploiting such human interaction.
Based on this intuition, in this paper we present a novel task: collaborative motion prediction, which aims to jointly predict the motion of two persons strongly involved in an interaction. To the best of our knowledge, previous pub-licly available datasets for 3D human motion prediction like 3DPW [67] and CMU-Mocap [25] that involve multiple persons only include weak human interactions, e.g., talk-ing, shaking hands etc. Here we move a step further and analyse situations where the motion of one person is highly correlated to the other person, which is often seen in team sports or collaborative assembly tasks in factories.
With the goal to foster research on this new task, we col-lected the ExPI (Extreme Pose Interaction) dataset, a large dataset of professional dancers performing Lindy Hop aerial steps.1 To perform these actions, the two dancers perform different movements that require a high level of synchroni-sation. These actions are composed of extreme poses and require strict and close cooperation between the two per-sons, which is highly suitable for the study of human inter-actions. Some examples of this highly interacted dataset are shown in Figure 2. Our dataset contains 115 sequences of 2 professional couples performing 16 different actions. It is recorded in a multiview motion capture studio, and the 3D poses and 3D shapes of the two persons are annotated for all the 30K frames. We have carefully created train/test splits, and proposed two different extensions of the pose evalua-tion metrics for collaborative motion prediction task.
To model such strong human-to-human interactions, we introduce a novel Cross-Interaction Attention (XIA) mod-ule, which is based upon a standard multi-head atten-tion [64] and exploits historical motion data of the two persons simultaneously. For a pair of persons engaging in the same activity, XIA module extracts the spatial-temporal motion information from both persons and uses them to guide the prediction of each other.
We exhaustively evaluate our approach and compare it with state-of-the-art methods designed for single human motion prediction. Note that in our dataset of dancing ac-tions, movements are performed at high speed. The long term predictions are very challenging in this case. Never-theless, the results demonstrate that our approach consis-tently outperforms these methods by a large margin, with 10 ∼ 40% accuracy improvement for short (≤ 500 ms) and 5 ∼ 30% accuracy improvement for long term prediction 1The Lindy Hop is an African-American couple dance born in the 1930’s in Harlem, New York, see [54]. (500 ms ∼ 1000 ms).
Our key contributions can be summarized as follows:
• We introduce the task of collaborative motion predic-tion, to focus on the estimation of future poses of peo-ple in highly interactive setups.
• We collect and make publicly available ExPI, a large dataset of highly interacted extreme dancing poses, annotated with 3D joint locations and body shapes.
We also define the benchmark with carefully selected train/test splits and evaluation protocols.
• We propose a method with a novel cross-interaction attention (XIA) module that exploits historical motion of two interacted persons to predict their future move-ments. Our model can be used as a baseline method for collaborative motion prediction. 2.