Abstract
We present a visual localization system that learns to estimate camera poses in the real world with the help of synthetic data. Despite significant progress in recent years, most learning-based approaches to visual localization tar-get at a single domain and require a dense database of geo-tagged images to function well. To mitigate the data scarcity issue and improve the scalability of the neural lo-calization models, we introduce TOPO-DataGen, a versa-tile synthetic data generation tool that traverses smoothly between the real and virtual world, hinged on the geo-graphic camera viewpoint. New large-scale sim-to-real benchmark datasets are proposed to showcase and evalu-ate the utility of the said synthetic data. Our experiments reveal that synthetic data generically enhances the neural network performance on real data. Furthermore, we intro-duce CrossLoc, a cross-modal visual representation learn-ing approach to pose estimation that makes full use of the scene coordinate ground truth via self-supervision. With-out any extra data, CrossLoc significantly outperforms the state-of-the-art methods and achieves substantially higher real-data sample efficiency. Our code and datasets are all available at crossloc.github.io . 1.

Introduction
Due to vulnerabilities in the reception of satellite posi-tioning (GNSS) signals, on which aerial systems rely for navigation and controls, alternative methods are in demand for absolute large-scale localization. The dead-reckoning navigation systems have improved significantly in recent years; however, residual drift is always a challenge for long-term applications [21, 30]. In contrast, the availabil-ity of small and low-cost cameras has made them popular sensors for capturing information on the surrounding land-1
scape. When features of a known environment are rec-ognized in the captured images, these can be used to de-termine the absolute camera poses. Current state-of-the-art machine-learning-based methods for absolute localiza-tion [11, 13, 15, 41, 42, 52] show promising performance but typically focus on single-domain operation and on be-spoke datasets collected for indoor or outdoor city street localization [36]. However, open-source datasets for posi-tioning airborne platforms or workflows to generate geospa-tial learning data are scarce. This poses a severe barrier in adapting the algorithms to real-world aerial scenarios, as they typically require dense datasets consisting of accu-rately geo-referenced photos in the area of interest [26]. De-veloping inclusive datasets for real-life navigation around the globe and employing state-of-the-art visual algorithms for aerial applications can currently be considered econom-ically and technically difficult, if not unfeasible.
In this work, we first present a synthetic data generation scheme called TOPO-DataGen (Figure 1, left), that lever-ages the topographic information to produce geo-referenced data with rich modalities for subsequent training. Given the designated camera poses, this scheme renders the sim-ulated RGB images accompanied by 2D and 3D modali-ties such as semantics, scene coordinates, depth, and sur-face normal. In an area of interest with available geodata, one may adopt a stochastic sampling strategy such as Latin hypercube sampling (LHS) [22, 35] or use real geo-tagged photos to attain the camera viewpoints and create the corre-sponding synthetic labels. The real geo-tagged data can be designer sourced, such as data acquisition by drones or from crowd sourced campaigns [3, 4]. Our method hinges on the geospatial location of the camera viewpoint and traverses between reality and simulation smoothly.
To mitigate the data-scarcity issue for learning-based vi-sual localization methods via sim-to-real transfer, we cu-rated two large-scale benchmark datasets at [19] using the proposed data generation workflow on urban and natural sites. They are comprised of primarily synthetic data and a small fraction of accurately geo-tagged real data, with both sections containing dense 3D and semantic labels. Unlike the existing datasets focusing on localization in a single do-main [26, 29, 45], the provided benchmark datasets show-case and evaluate the use of synthetic data to assist localiza-tion in the real world using significantly less real data.
In addition, we introduce a cross-modal visual repre-sentation learning approach CrossLoc (Figure 1, right) for absolute localization via scene coordinate regression.
CrossLoc exploits the rich information contained in the scene coordinates through self-supervision to achieve im-proved performance. We start from the scene coordinate ground-truth to impose geometrically less complex pretext tasks such as depth estimation without any extra labels.
The visual representations learned from the tightly-coupled tasks [39, 59, 60] jointly improve the downstream coordi-nate regression. We find that this approach consistently out-performs the state-of-the-art baselines in our benchmark.
Our main contributions are summarized as follows: 1. TOPO-DataGen: an open-source multimodal synthetic data generation tool tailored to aerial scenes. 2. Large-scale benchmark datasets for sim-to-real visual localization, including synthetic and real images with 3D and semantic labels on urban and natural sites. 3. CrossLoc: a cross-modal visual representation learn-ing method via self-supervision for absolute localiza-tion, which outperforms the state-of-the-art baselines. 2.