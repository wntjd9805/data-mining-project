Abstract
Generalized zero-shot learning (GZSL) aims to recog-nize samples whose categories may not have been seen at training. Recognizing unseen classes as seen ones or vice versa often leads to poor performance in GZSL. There-fore, distinguishing seen and unseen domains is naturally an effective yet challenging solution for GZSL. In this pa-per, we present a novel method which leverages both vi-sual and semantic modalities to distinguish seen and un-seen categories. Speciﬁcally, our method deploys two vari-ational autoencoders to generate latent representations for visual and semantic modalities in a shared latent space, in which we align latent representations of both modalities by
Wasserstein distance and reconstruct two modalities with the representations of each other. In order to learn a clearer boundary between seen and unseen classes, we propose a two-stage training strategy which takes advantage of seen and unseen semantic descriptions and searches a threshold to separate seen and unseen visual samples. At last, a seen expert and an unseen expert are used for ﬁnal classiﬁca-tion. Extensive experiments on ﬁve widely used benchmarks verify that the proposed method can signiﬁcantly improve the results of GZSL. For instance, our method correctly rec-ognizes more than 99% samples when separating domains and improves the ﬁnal classiﬁcation accuracy from 72.6% to 82.9% on AWA1. 1.

Introduction
Conventional visual classiﬁcation tasks deal with the same object categories in training and testing stage, i.e., samples in the training set and testing set have the same label space. Generally, methods for these tasks cannot cor-rectly recognize samples which did not appear in training categories. Unfortunately, unseen categories are often in-volved in many real-world applications since the training dataset is ﬁnite. Zero-shot learning (ZSL) [10, 11, 14, 26, 32, 43] aims to handle unseen or novel instances by lever-aging shared representations of visual and semantic modal-ities. In conventional ZSL [1, 25], a model recognizes sam-ples only from unseen domain. Generalized zero-shot learn-ing (GZSL) is a more challenging task which handles visual samples from both seen and unseen domains.
Early ZSL methods focus on embedding visual and se-mantic representations into a shared space [2, 3, 14, 17, 36, 38], e.g., mapping visual features into semantic space, or vice versa, and measuring similarity between two modali-ties. Recently, generating synthetic unseen visual features is widely adopted [8,27,28,33,37,41,43]. Generative meth-ods ﬁrstly train a generative model such as GAN [18] or
VAE [23] and synthesize a batch of features with unseen semantic attributes. Then a classiﬁer is trained with seen samples and synthesized unseen samples to distinguish dif-ferent classes. Since GZSL involves both seen and unseen categories, separating seen and unseen domains [5,7,12,31] is a reasonable solution. Once seen and unseen domains are separated, the GZSL problem is decomposed to a conven-tional zero-shot learning task and arbitrary seen and unseen experts can be adopted to accomplish classiﬁcation.
Although separating seen and unseen domains is promis-ing, it is quite challenging to distinguish seen and unseen visual features. For instance, as illustrated in Figure 1, killerwhale and humpbackwhale are categories can be accessed during the training phase, dolphin is the unseen category for testing. These three species share a large number of common visual features and semantic attributes.
Then generator is prone to generate features of dolphin highly similar to killerwhale and humpbackwhale for lacking visual prior of category dolphin in the training phase. Therefore, in the testing phase, a sample of dolphin is prone to be wrongly recognized as killerwhale or humpbackwhale. This phenomenon results in low clas-siﬁcation accuracy on unseen classes. Since the supervision information of training is from seen categories, it is a dis-aster for unseen categories with similar samples in seen do-mains. Therefore, distinguishing seen and unseen samples is essential to promote GZSL performance.
*Jingjing Li is the corresponding author. Email: lijin117@yeah.net
In this paper, we focus on accurately separating seen and
on a more realistic yet challenging setting of ZSL named generalized zero-shot learning (GZSL). Different from con-ventional ZSL, the testing set of GZSL contains both seen and unseen categories.
In GZSL, a majority of previous studies focus on clas-siﬁcation tasks with visual samples. The solutions of this task can mainly be divided into three categories, i.e., em-bedding methods [2, 3, 14, 30, 38, 48], generative meth-ods [9,27,33,37,43,45] and domain-aware methods [7,31].
With respect to embedding methods, Ivan et al. [38] pro-pose a method that maps semantic descriptions into visual space with class-wise normalization and signiﬁcantly out-performs other methods. Alternatively, generative methods synthesize samples for unseen categories with correspond-ing semantic attributes, then inject these samples into train-ing data. For instance, f-CLSWGAN [43] is a representative
GAN-based method that deploys a Wasserstein GAN [4] to generate visual features. Domain-aware methods aim to ex-plicitly distinguish seen and unseen domains. For instance,
DVBE [31] learns to distinguish seen and unseen visual fea-tures in a semantic-free space and semantic-aligned space, respectively. DVBE embeds semantic descriptions into vi-sual space to distinguish both seen and unseen categories with a single model. COSMO [5] designs a conﬁdence-based gating to separate seen and unseen samples. This model consists of three parts, i.e., a seen classiﬁer, an un-seen classiﬁer and a gating binary classiﬁer. The gating classiﬁer takes predictions of seen and unseen classiﬁer as input and predicts gating scores for the two classiﬁers. An-other representative method is proposed by Chen et al. [7], which embeds visual features and semantic descriptions in a latent space to split seen and unseen samples.
Our method falls into the domain-aware group. Specif-ically, we employ two VAE models to align visual features and semantic descriptions in a latent space and conduct a two-stage training strategy. Meanwhile, we learn a classi-ﬁer in the latent space to distinguish seen and unseen repre-sentations. Notice that although our method shares the sim-ilar idea with OOD [7], the formulations of our method and
OOD are signiﬁcantly different. On one hand, our method conducts a two-stage training and generates synthetic fea-tures with unseen semantic attributes. However, OOD can-not leverage unseen knowledge for lacking unseen visual features. On the other hand, we propose to leverage a novel
ﬁctitious class to distinguish similar seen and unseen visual representations, which is shown capable of distinguishing seen and unseen samples accurately even they share quite similar visual and semantic characteristics. Furthermore, we propose an unseen expert with attention mechanism to classify unseen classes while OOD directly adopts an un-seen classiﬁer trained in f-CLSWGAN [43]. In the experi-ments, we will show that our method can signiﬁcantly out-perform OOD on all evaluated datasets, and much more de-Figure 1. Illustration of similar species in AWA2 (best viewed in color). Samples of dolphin locate near in killer-whale cluster and humpback whale cluster. unseen categories in cases when similar categories exist. To this end, we propose to generate a special class, called ﬁc-titious class, to separate similar visual features in a latent space.
In our method, the latent representations of both visual and semantic modalities are embedded class-wisely on a latent space. Then we analyze the embedding bound-ary of each class and search a threshold to split seen and unseen samples. Speciﬁcally, we deploy hyperspherical
VAE [13] models for both visual and semantic modalities and align the latent representations of the two modalities at the category-level. To leverage ﬁctitious class, we pro-pose a two-stage training scheme. Speciﬁcally, we ﬁrstly train both visual and semantic VAE models with seen sam-ples and corresponding semantic attributes. Then we gener-ate ﬁctitious classes and train semantic VAE with ﬁctitious samples and unseen semantic attributes. We measure sim-ilarity between the latent representations of two modalities and search a threshold to distinguish seen and unseen do-mains. By this, seen and unseen samples can be success-fully distinguished. Further, we propose an unseen expert which is regularized by attention mechanism to classify un-seen visual samples.
To summarize, the main contributions of this paper are threefold: (1) We propose a novel method to distinguish seen and unseen domains for GZSL. We design a two-stage training scheme which signiﬁcantly improves the model performance by leveraging both seen and unseen seman-tic attributes. (2) We propose to leverage a novel ﬁctitious class to separate similar visual representations. With ﬁcti-tious class, we can successfully separate indistinguishable seen and unseen samples. In addition, we propose an un-seen expert with attention mechanism to recognize unseen samples. It is worth noting that the unseen expert can be trained less than a minute in all tested datasets. (3) We con-duct extensive experiments on ﬁve open benchmarks. The results verify that the proposed method can signiﬁcantly im-prove the result of previous state-of-the-art approaches. 2.