Abstract
Knowledge distillation aims to compress a powerful yet cumbersome teacher model into a lightweight student model without much sacrifice of performance. For this purpose, various approaches have been proposed over the past few years, generally with elaborately designed knowledge rep-resentations, which in turn increase the difficulty of model development and interpretation.
In contrast, we empiri-cally show that a simple knowledge distillation technique is enough to significantly narrow down the teacher-student performance gap. We directly reuse the discriminative clas-sifier from the pre-trained teacher model for student infer-ence and train a student encoder through feature alignment with a single ℓ2 loss. In this way, the student model is able to achieve exactly the same performance as the teacher model provided that their extracted features are perfectly aligned.
An additional projector is developed to help the student en-coder match with the teacher classifier, which renders our technique applicable to various teacher and student archi-tectures. Extensive experiments demonstrate that our tech-nique achieves state-of-the-art results at the modest cost of compression ratio due to the added projector. 1.

Introduction
Given a powerful teacher model with large numbers of parameters, the goal of knowledge distillation (KD) is to help another less-parameterized student model gain a simi-lar generalization ability as the larger teacher model [4, 24].
A straightforward way to achieve this goal is by aligning their logits or class predictions given the same inputs [2,24].
Due to its conceptual simplicity and practical effectiveness,
KD technique has achieved great success in a variety of ap-plications, such as object detection [8], semantic segmenta-tion [32] and the training of transformers [45].
One limitation of the vanilla KD is that the performance
∗Corresponding author
Figure 1. An overview of our proposed SimKD. A simple ℓ2 loss is adopted for feature alignment in the preceding layer of the final classifier. Only the student feature encoder and dimension projec-tor are updated during training (boxes with the black border). The pre-trained teacher classifier is reused for student inference. gap between the original teacher model and the distilled stu-dent model is still significant. To overcome this drawback, a bunch of approaches have been proposed in the last few years [19, 48]. Most of them benefit from exploiting addi-tional supervision from the pre-trained teacher model, es-pecially the intermediate layers [1, 6, 39, 44, 46, 50, 53]. Be-sides aligning the plain intermediate features [6, 39, 50], the existing efforts are typically based on elaborately designed knowledge representations, such as mimicking spatial at-tention maps [53], pairwise similarity patterns [36, 37, 46] or maximizing the mutual information between teacher and student features [1, 44, 55]. Although we indeed see con-stant improvements of these works in student performance, neither effective representations nor well-optimized hyper-parameters ensuring their success are easily achievable in practice. Furthermore, the diversity of transferred knowl-edge hinders the emergence of a unified and clear interpre-tation of the final improvement in student performance.
In this paper, we present a simple knowledge distillation technique and demonstrate that it can significantly bridge the performance gap between teacher and student models with no need for elaborate knowledge representations. Our proposed “SimKD” technique is illustrated in Figure 1. We
argue that the powerful class prediction ability of a teacher model is credited to not only those expressive features but just as importantly, a discriminative classifier. Based on this argument, which is empirically supported later on, we train a student model through feature alignment in the preceding layer of the classifier and directly copy the teacher classifier for student inference. In this way, if we could perfectly align the student features with those of the teacher model, their performance gap will just disappear. That is to say, the fea-ture alignment error alone accounts for the accuracy of stu-dent inference, which makes our knowledge transfer more comprehensible. According to our experimental results, a single ℓ2 loss for feature alignment already works surpris-ingly well. Such a simple loss saves us from carefully tun-ing hyper-parameters as previous works do in order to bal-ance the effect of multiple losses [1, 6, 24, 39, 44, 46, 50, 53].
As the dimensions of extracted features from teacher and student models usually differ from each other, a projector is thus added after the student feature encoder to remedy this dimension mismatch. This projector generally incurs a less than 3% cost to the pruning ratio in teacher-to-student com-pression, but it makes our technique applicable to arbitrary model architectures. The pruning ratio could be even en-larged in a few cases where the parameter number of the added projector plus the reused teacher classifier is less than that of the original student classifier (see Figure 7).
We conduct extensive experiments on standard benchmark datasets and observe that our SimKD consistently outper-forms all compared state-of-the-art approaches with a vari-ety of teacher-student architecture combinations. We also show that our simple technique generalizes well in different scenarios such as multi-teacher knowledge distillation and data-free knowledge distillation. 2.