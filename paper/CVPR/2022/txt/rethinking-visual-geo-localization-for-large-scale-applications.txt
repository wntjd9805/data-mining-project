Abstract
Visual Geo-localization (VG) is the task of estimating the position where a given photo was taken by compar-ing it with a large database of images of known locations.
To investigate how existing techniques would perform on a real-world city-wide VG application, we build San Fran-cisco eXtra Large, a new dataset covering a whole city and providing a wide range of challenging cases, with a size 30x bigger than the previous largest dataset for visual geo-localization. We find that current methods fail to scale to such large datasets, therefore we design a new highly scalable training technique, called CosPlace, which casts the training as a classification problem avoiding the ex-pensive mining needed by the commonly used contrastive learning. We achieve state-of-the-art performance on a wide range of datasets and find that CosPlace is robust to heavy domain changes. Moreover, we show that, compared to the previous state-of-the-art, CosPlace requires roughly 80% less GPU memory at train time, and it achieves bet-ter results with 8x smaller descriptors, paving the way for city-wide real-world visual geo-localization. Dataset, code and trained models are available for research purposes at https://github.com/gmberton/CosPlace. 1.

Introduction
Visual geo-localization (VG), also known as visual place recognition [1] or image localization [30], is a staple of computer vision [1, 15, 21, 44–46, 52] and robotics research
[9–11, 17, 22, 26] and it is defined as the task of coarsely recognizing the geographical location where a photo was taken, usually with a tolerance of few meters [1,5,18,24,27, 30, 48]. This task is commonly approached as an image re-trieval problem where the query to be localized is compared to a database of geo-tagged images: the most similar im-ages retrieved from the database, together with their meta-data, represent the hypotheses of the query’s geographical location. In particular, all recent VG methods are learning-based and use a neural network to project the images into an embedding space that well represents the similarity of their
Figure 1. Map of various datasets on the city of San Francisco.
Previous datasets only cover a sector of the city (green points) or are sparse (red points). SF-XL densely covers the whole city and provides a realistic case-study for large-scale applications. locations, and that can be used for the retrieval.
So far, research on VG has focused on recognizing the location of images in moderately sized geographical areas (e.g., a neighborhood). However, real-world applications of this technology, such as autonomous driving [15] and as-sistive devices [12], are posed to operate at a much larger scale (e.g., cities or metropolitan areas), thus requiring mas-sive databases of geo-tagged images to execute the retrieval.
Having access to such massive databases, it would be advis-able to use them also to train the model rather than just for the execution of the retrieval (inference). This idea requires us to rethink VG, addressing the two following limitations.
Non-representative datasets. The current datasets for VG are not representative of realistic large-scale applications, because they are either too small in the geographical cover-age [1,7,19,32,44] or too sparse [2,13,31,48] (see Fig. 1 for an example of these limitations). Moreover, current datasets follow the common practice of splitting the collected im-ages into geographically disjoint sets for training and infer-ence. However, this practice does not find a correspondence in the real world where one would likely opt to use images
from the target geographical area to train the model. Con-sidering also the cost of collecting the images, it would be advisable to use the whole database also for training.
Scalability of training. Having access to a massive amount of data raises the question of how to use it effectively for training. All the recent state-of-the-art methods in VG use contrastive learning [1,5,18,21,24,27,30,34,35,48] (mostly relying on a triplet loss), which heavily depends on mining of negative examples across the training database [1]. This operation is expensive, and it becomes prohibitive when the database is very large. Lightweight mining strategies that explore only a small pool of samples can reduce the dura-tion of the mining phase [48], but they still result in a slow convergence and possibly less effective use of the data.
Contributions. In this paper, we address these two limita-tions with the following contributions:
• A new large-scale and dense dataset, called San Fran-cisco eXtra Large (SF-XL), that is roughly 30x big-ger than what is currently available (see Fig. 1). The dataset includes crowd-sourced (i.e., multi-domain) queries that make for a challenging problem.
• A procedure that uses a classification task as a proxy to train the model that is used at inference to extract discriminative descriptors for the retrieval. We call this method CosPlace. CosPlace is remarkably simple, it does not necessitate to mine negative examples, and it can effectively learn from massive collections of data.
Through extensive experimental validation, we demon-strate that not only CosPlace requires roughly 80% less
GPU memory at train time than current SOTA, but also that a simple model trained with CosPlace on SF-XL surpasses the SOTA while using 8x smaller embeddings. Addition-ally, we show that this model generalizes far better to other datasets. 2.