Abstract
As acquiring pixel-wise annotations of real-world im-ages for semantic segmentation is a costly process, a model can instead be trained with more accessible synthetic data and adapted to real images without requiring their annota-tions. This process is studied in unsupervised domain adap-tation (UDA). Even though a large number of methods pro-pose new adaptation strategies, they are mostly based on outdated network architectures. As the inﬂuence of recent network architectures has not been systematically studied, we ﬁrst benchmark different network architectures for UDA and newly reveal the potential of Transformers for UDA se-mantic segmentation. Based on the ﬁndings, we propose a novel UDA method, DAFormer. The network architecture of
DAFormer consists of a Transformer encoder and a multi-It is enabled level context-aware feature fusion decoder. by three simple but crucial training strategies to stabilize the training and to avoid overﬁtting to the source domain:
While (1) Rare Class Sampling on the source domain im-proves the quality of the pseudo-labels by mitigating the conﬁrmation bias of self-training toward common classes, (2) a Thing-Class ImageNet Feature Distance and (3) a learning rate warmup promote feature transfer from Ima-geNet pretraining. DAFormer represents a major advance in UDA. It improves the state of the art by 10.8 mIoU for
GTA→Cityscapes and 5.4 mIoU for Synthia→Cityscapes and enables learning even difﬁcult classes such as train, bus, and truck well. The implementation is available at https://github.com/lhoyer/DAFormer. 1.

Introduction
In the last few years, neural networks have achieved overwhelming performance on many computer vision tasks.
However, they require a large amount of annotated data in order to be trained properly. For semantic segmentation, annotations are particularly costly as every pixel has to be labeled. For instance, it takes 1.5 hours to annotate a single
Figure 1. Progress of UDA over time on GTA→Cityscapes.
Most previous UDA methods are evaluated using the outdated
DeepLabV2 architecture. We rethink the design of the network architecture as well as its training strategies for UDA and propose
DAFormer, signiﬁcantly outperforming previous methods. image of Cityscapes [12] while, for adverse weather con-ditions, it is even 3.3 hours [58]. One idea to circumvent this issue is training with synthetic data [55, 57]. However, commonly used CNNs [38] are sensitive to domain shift and generalize poorly from synthetic to real data. This is-sue is addressed in unsupervised domain adaptation (UDA) by adapting the network trained with source (synthetic) data to target (real) data without access to target labels.
Previous UDA methods mostly evaluated their contribu-tions using a DeepLabV2 [6] or FCN8s [46] network archi-tecture with ResNet [24] or VGG [60] backbone in order to be comparable to previously published works. However, even their strongest architecture (DeepLabV2+ResNet101) is outdated in the area of supervised semantic segmentation.
For instance, it only achieves a supervised performance of 65 mIoU [68] on Cityscapes while recent networks reach up to 85 mIoU [64, 86]. Due to the large performance gap, it stands to question whether using outdated network archi-tectures can limit the overall performance of UDA and can also misguide the benchmark progress of UDA. In order to answer this question, this work studies the inﬂuence of the network architecture for UDA, compiles a more sophisti-cated architecture, and successfully applies it to UDA with a few simple, yet crucial training strategies. Naively using a
more powerful network architecture for UDA might be sub-optimal as it can be more prone to overﬁtting to the source domain. Based on a study of different semantic segmen-tation architectures evaluated in a UDA setting, we com-pile DAFormer, a network architecture tailored for UDA (Sec 3.2). It is based on recent Transformers [14,83], which have been shown to be more robust than the predominant
CNNs [3]. We combine them with a context-aware multi-level feature fusion, which further enhances the UDA per-formance. To the best of our knowledge, DAFormer is the
ﬁrst work to reveal the signiﬁcant potential of Transformers for UDA semantic segmentation.
Since more complex and capable architectures are more prone to adaptation instability and overﬁtting to the source domain, in this work, we introduce three training strategies to UDA to address these issues (Sec. 3.3). First, we pro-pose Rare Class Sampling (RCS) to consider the long-tail distribution of the source domain, which hinders the learn-ing of rare classes, especially in UDA due to the conﬁr-mation bias of self-training toward common classes. By frequently sampling images with rare classes, the network can learn them more stably, which improves the quality of pseudo-labels and reduces the conﬁrmation bias. Second, we propose a Thing-Class ImageNet Feature Distance (FD), which distills knowledge from diverse and expressive Im-ageNet features in order to regularize the source training.
This is particularly helpful as the source domain is limited to only a few instances of certain classes (low diversity), which have a different appearance than the target domain (domain shift). Without FD this would result in learning less expressive and source-domain-speciﬁc features. As Im-ageNet features were trained for thing-classes, we restrict the FD to regions of the image that are labeled as a thing-class. And third, we introduce learning rate warmup [22] newly to UDA. By linearly increasing the learning rate up to the intended value in the early training, the learning pro-cess is stabilized and features from ImageNet pretraining can be better transferred to semantic segmentation.
DAFormer outperforms previous methods by a large margin (see Fig. 1) supporting our hypothesis that the network architecture and appropriate training strategies play an important role for UDA. On GTA→Cityscapes, we improve the mIoU from 57.5 [88] to 68.3 and on
Synthia→Cityscapes from 55.5 [88] to 60.9. In particular,
DAFormer learns even difﬁcult classes that previous meth-ods struggled with. For instance, we improve the class train from 16 to 65 IoU, truck from 49 to 75 IoU, and bus from 59 to 78 IoU on GTA→Cityscapes. Overall, DAFormer rep-resents a major advance in UDA. Our framework can be trained in one stage on a single consumer RTX 2080 Ti GPU within 16 hours, which simpliﬁes its usage compared to pre-vious methods such as ProDA [88], which requires training multiple stages on four V100 GPUs for several days. 2.