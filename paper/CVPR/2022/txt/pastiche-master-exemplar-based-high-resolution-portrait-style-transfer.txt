Abstract
Recent studies on StyleGAN show high performance on artistic portrait generation by transfer learning with limited data. In this paper, we explore more challenging exemplar-based high-resolution portrait style transfer by introduc-ing a novel DualStyleGAN with flexible control of dual styles of the original face domain and the extended artistic portrait domain. Different from StyleGAN, DualStyleGAN provides a natural way of style transfer by characterizing the content and style of a portrait with an intrinsic style path and a new extrinsic style path, respectively. The del-icately designed extrinsic style path enables our model to modulate both the color and complex structural styles hi-erarchically to precisely pastiche the style example. Fur-thermore, a novel progressive fine-tuning scheme is intro-duced to smoothly transform the generative space of the model to the target domain, even with the above modifica-tions on the network architecture. Experiments demonstrate the superiority of DualStyleGAN over state-of-the-art meth-ods in high-quality portrait style transfer and flexible style control. Code is available at https://github.com/ williamyang1991/DualStyleGAN . 1.

Introduction
Artistic portraits are popular in our daily lives and espe-cially in industries related to comics, animations, posters, and advertising. In this paper, we focus on exemplar-based portrait style transfer, a core problem that aims to trans-fer the style of an exemplar artistic portrait onto a target face. Its potential application is appealing in that it allows any novice to easily transform their photograph into a stun-ning pastiche based on the style of their favourite artworks, which otherwise would have required highly professional skills for manual creation.
Automatic portrait style transfer based on image style transfer [23, 24, 31] and image-to-image translation [6, 19, 22] has been extensively studied. Recently, Style-GAN [17, 18], the state-of-the-art face generator, has been very promising for high-resolution artistic portrait genera-tion via transfer learning [29]. Specifically, StyleGAN can
be effectively fine-tuned, usually only requiring hundreds of portrait images and hours of training time, to translate its generative space from the face domain to the artistic portrait domain. It shows great superiority in quality, image resolu-tion, data requirement, and efficiency compared to image style transfer and image-to-image translation models.
The strategy above, while effective, only learns an over-all translation of the distribution, incapable of performing exemplar-based style transfer. For a StyleGAN that has been transferred for generating a fixed caricature style, a laughing face will be largely mapped to its nearest one in the caricature domain, i.e., a portrait with an exaggerated mouth. Users have no means of shrinking the face to pas-tiche their preferred artworks like in Fig. 1(c). Although
StyleGAN provides inherent exemplar-based single-domain style mixing by latent swapping [1,17], such single-domain-oriented operation is counter-intuitive and incompetent for style transfer involving a source domain and a target do-main. This is because misalignment between these two do-mains may lead to unwanted artifacts during style mixing, especially for domain-specific structures. However, impor-tantly, a professional pastiche, should imitate how an artist handles face structures, e.g., abstraction in cartoons and de-formation in caricatures.
To tackle these challenges, we propose a novel Dual-StyleGAN to realize effective modelling and control of dual styles for exemplar-based portrait style transfer. DualStyle-GAN retains an intrinsic style path of StyleGAN to control the style of the original domain, while adding an extrinsic style path to model and control the style of the target ex-tended domain, which naturally correspond to the content path and style path in the standard style transfer paradigm.
Moreover, the extrinsic style path inherits the hierarchical architecture from StyleGAN to modulate structural styles in coarse-resolution layers and color styles in fine-resolution layers for flexible multi-level style manipulations.
Adding an extrinsic style path to the original StyleGAN architecture is non-trivial for our task as it risks altering the generative space and behavior of the pre-trained StyleGAN.
To overcome this challenge, we present effective ways and insights to design the extrinsic style path and train Dual-StyleGAN. 1) Model design: based on the analysis on the fine-tuning behavior of StyleGAN, we propose to introduce the extrinsic style in a residual manner to the convolution layers, which can well approximate how fine-tuning affects the convolution layers of StyleGAN. We show that such de-sign enables DualStyleGAN to effectively modulate the key structural styles. 2) Model training: we introduce a novel progressive fine-tuning methodology, where the extrinsic style path is first elaborately initialized so that DualStyle-GAN retains the generative space of StyleGAN for seam-less transfer learning. Then, we start out training DualStyle-GAN with an easy style transfer task and then gradually in-creases the task difficulty, to progressively translate its gen-erative space to the target domain. In addition, we present a facial destylization method to provide face-portrait pairs, serving as supervision to promote the model to learn diverse styles and avoid mode collapse.
With the novel formulation above, the proposed Dual-StyleGAN offers high-quality and high-resolution pastiches and provides flexible and diverse control over both color styles and complicated structural styles, as shown in Fig. 1.
In summary, our contributions are threefold:
• We propose a novel DualStyleGAN to characterize and control the intrinsic and extrinsic styles for exemplar-based high-resolution portrait style transfer, requiring only a few hundred style examples, which achieves superior performance over state-of-the-art methods in high-quality and diverse artistic portrait generation.
• We design a principled extrinsic style path to introduce style features from external domains via fine-tuning and to provide hierarchical style manipulation in terms of both color and structure.
• We propose a novel progressive fine-tuning scheme for robust transfer learning over networks with architec-ture modifications. 2.