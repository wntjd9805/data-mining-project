Abstract
Scene ﬂow is a powerful tool for capturing the motion
ﬁeld of 3D point clouds. However, it is difﬁcult to directly apply ﬂow-based models to dynamic point cloud classiﬁ-cation since the unstructured points make it hard or even impossible to efﬁciently and effectively trace point-wise correspondences. To capture 3D motions without explic-itly tracking correspondences, we propose a kinematics-inspired neural network (Kinet) by generalizing the kine-matic concept of ST-surfaces to the feature space. By unrolling the normal solver of ST-surfaces in the fea-ture space, Kinet implicitly encodes feature-level dynam-ics and gains advantages from the use of mature back-bones for static point cloud processing. With only minor changes in network structures and low computing over-head, it is painless to jointly train and deploy our frame-work with a given static model. Experiments on NvGes-ture, SHREC’17, MSRAction-3D, and NTU-RGBD demon-strate its efﬁcacy in performance, efﬁciency in both the num-ber of parameters and computational complexity, as well as its versatility to various static backbones. Noticeably,
Kinet achieves the accuracy of 93.27% on MSRAction-3D with only 3.20M parameters and 10.35G FLOPS. The code is available at https://github.com/jx-zhong-for-academic-purpose/Kinet. 1.

Introduction
Due to continued miniaturization and mass production, 3D sensors are becoming less esoteric and increasingly prevalent in geometric perception tasks. These sensors typi-cally represent scene geometry through a point cloud, which is an unordered and irregular data structure consisting of distinct spatial 3D coordinates. As a fundamental prob-lem in point cloud understanding, classiﬁcation of static scenes [11, 30, 72] or objects [7, 58, 59] has witnessed rapid advances over the past few years. Whilst impressive, these techniques do not directly account for the fact that the real 3D world is also changing, through egocentric and/or al-(a) Vanilla two-stream framework based on physical scene ﬂow. (b) Kinematic two-stream framework guided by feature-level ST-surfaces.
Figure 1. Comparison between the ﬂow-based framework and ours. With neither explicit point-wise correspondence estimation nor the stand-alone temporal branch, our framework is lightweight and efﬁcient. locentric motion. To better understand our time-varying world, a handful of recent works [15–17,44,47,48,80] have been applied to dynamic point cloud classiﬁcation, a task in which the model is required to output a video-level category for a given sequence of 3D point clouds.
As a natural extension of 2D optical ﬂow, 3D scene ﬂow captures the motion ﬁeld of point clouds. Based on optical
ﬂow, two-stream networks [6, 19, 68, 76, 89] have already proven to be successful in image-based video classiﬁcation.
Hence, it should be a natural choice to classify dynamic point clouds with the help of scene ﬂow. However, to the best of our knowledge, scene ﬂow has not been utilized in point cloud sequences despite the prevalence of mature scene ﬂow estimators [3, 22, 24, 43, 50, 56, 73, 82].
What then hampers us from applying scene ﬂow to dy-namic point cloud classiﬁcation? Although scene ﬂow is a powerful tool, it is difﬁcult to estimate it efﬁciently and effectively from sequential point clouds - the computation of 3D scene ﬂow inevitability has higher time expenditure,
larger memory consumption, and lower accuracy than that of 2D optical ﬂow. These challenges are mainly caused by the irregular and unordered nature of dynamic point clouds.
This unstructured nature makes it difﬁcult to track the point-wise correspondences of the moving point sets across differ-ent frames.
Why not extract dynamic information without explicitly
ﬁnding the point-wise correspondences? If this were possi-ble, researchers could gain advantages from the decoupled motion representations but not suffer from the painful com-putational process of scene ﬂow. Similar to the gains seen in two-stream networks in image-based models, we would be able to preserve the beneﬁt of mature static solutions in inference and training, such as well-benchmarked network architectures, transferable pre-trained weights, and ready-to-use source code. At the same time, the pain of scene ﬂow estimation would be signiﬁcantly relieved, only with minor network modiﬁcation and low computational overhead.
For this purpose, we get inspiration from kinematics and propose a neural network (Kinet) to bypass direct scene
ﬂow estimation by generalizing the kinematic concept of space-time surfaces [55] (ST-surfaces) from the physical domain of point clouds to the feature space. In this way, normal vectors w.r.t. these ST-surfaces (ST-normals) es-tablish the representation ﬁeld of dynamic information as shown in Figure 1b. Thus, motions are implicitly repre-sented by feature-level ST-surfaces without explicitly com-puting point-wise correspondences.
Inspired by iterative normal reﬁnement [49], we unroll the solver for ST-normals and make it jointly trainable alongside the static model in an end-to-end manner. Inheriting intermediate features from static network layers, Kinet is lightweight in parameters and efﬁcient in computational complexity, compared with the vanilla ﬂow-based framework depicted in Figure 1a which requires extra scene ﬂow estimation and the independent temporal branch.
Experiments are conducted on four datasets (NvGes-ture [52], SHREC’17 [14], MSRAction-3D [40] and NTU-RGBD [66]) for two tasks (gesture recognition and action classiﬁcation) with three typical static backbones (MLP-based PointNet++ [60], graph-based DGCNN [78] and convolution-based SpiderCNN [83]). Noticeably, 1) in ges-ture recognition, our framework outperforms humans for the ﬁrst time with the accuracy of 89.1% on NvGesture; 2) in action classiﬁcation, it achieves a new record of 93.27% on 24-frame MSRAction-3D with only 3.20M parameters and 10.35G FLOPS.
In summary, our main contribution is as follows:
• By introducing Kinet, we decouple temporal infor-mation from spatial features, thereby easily extending static backbones to dynamic recognition and entirely preserving the merits of these mature backbones.
• Without the pain of tracking point-wise correspon-dences, we encode point cloud dynamics by unrolling the ST-normal solver in the feature space. This method is jointly trainable alongside the static model, with mi-nor structural changes and low computing overhead.
• Extensive experiments on various datasets, tasks, and static backbones show its efﬁcacy in performance, ef-ﬁciency in parameters and computational complexity, as well as versatility to different static backbones. The code is available at https://github.com/jx-zhong-for-academic-purpose/Kinet. 2.