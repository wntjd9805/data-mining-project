Abstract
A camera begins to sense light the moment we press the shutter button. During the exposure interval, relative mo-tion between the scene and the camera causes motion blur, a common undesirable visual artifact. This paper presents E-CIR, which converts a blurry image into a sharp video rep-resented as a parametric function from time to intensity. E-CIR leverages events as an auxiliary input. We discuss how to exploit the temporal event structure to construct the para-metric bases. We demonstrate how to train a deep learning model to predict the function coefficients. To improve the appearance consistency, we further introduce a refinement module to propagate visual features among consecutive frames. Compared to state-of-the-art event-enhanced de-blurring approaches, E-CIR generates smoother and more realistic results. The implementation of E-CIR is available at https://github.com/chensong1995/E-CIR. 1.

Introduction
The shutter speed, or the length of the exposure in-terval, controls how much light reaches the image sensor from the environment. If the exposure interval is too short, the camera only has the time to capture very few pho-tons. Consequently, the resulting image is not only unil-luminated but also lacks fine details. On the other hand, if the exposure interval is too long, the relative motion be-tween the scene and the camera may potentially be very significant. The resulting image is then the temporal aver-age of a moving trajectory, causing blurry artifacts. Tra-ditionally, it is presumed that any motion during the ex-posure interval, including both camera shake and subject movement, is unwanted and should therefore be removed.
Over the past several decades, researchers have studied extensively how to convert a blurry image into a sharp one [1, 5, 6, 10, 11, 13â€“16, 27, 32, 34, 41, 42]. It is only until recently when several works that reconstruct the complete motion trajectory have received profound attention [9, 28].
These works introduce algorithms that convert a blurry im-age into a sharp video describing the exact movement that causes the blurry artifact.
Figure 1. Problem Description. In this imaginary scene, we place a white square along the edge of a black disk. The image taken by the conventional camera is blurry because the disk rotates at a fast speed. It is as if the perimeter of the disk somehow grows into a gray collar. During the exposure interval, the event sensor produces a spiral of events. Our approach takes the blurry frame and the events as input and produces a sharp video sequence as output. The output video explains the motion blur by entailing the complete motion trajectory of the rotating disk.
Sharp video reconstruction is an ill-posed problem be-cause there are infinitely many motion trajectories whose temporal averages correspond to the same blurry frame. To compensate for the ambiguity, previous works [7, 8, 20, 24, 25, 35, 39, 40, 43] exploit event data as an auxiliary input, which provides additional information during the exposure interval at a finer temporal resolution, as shown in Figure 1.
Even with the event input, difficult challenges remain. The events fail to capture the complete motion information. The video reconstruction quality is determined not only by the appearance of each individual frame but also the temporal smoothness. The immense density of events creates another obstacle for effective and efficient processing. The success of video deblurring depends on how the blurry image, the events, and priors about video sequences are integrated to-gether. This calls for suitable video representations and pre-diction algorithms.
This paper makes fundamental contributions in video representations and methodologies for recovering accurate and temporally consistent videos. Specifically, we propose a continuous video representation whose coefficients are highly interpretable and easy to learn, due to their strong correlation to the events. For every pixel (x, y), we rep-resent its intensity as a parametric polynomial function
Lxy(t), allowing us to render the sharp image at any given timestamp t during the exposure interval. We show how to choose the polynomial bases such that the derivative of
Lxy(t) interpolates the significant intensity changes. We also demonstrate how to train a deep neural network that regresses the polynomial coefficients. Instead of processing the video as a volume and implicitly encoding motions in convolutional filters, our approach explicitly asks the model to elaborate the motions that have already been described by the events. To further polish the frame quality, we in-troduce a refinement module that propagates the visual fea-tures among consecutive frames, which can be trained in an end-to-end manner with the rest of the model. The proposed regress-and-refine paradigm nicely combines the strength of recurrent modules for enforcing temporal smoothness and the strength of regression for drifting avoidance.
We quantitatively evaluate our method on the synthetic
REDS dataset [21]. In terms of reconstruction quality, E-CIR achieves an MSE of 0.114, representing a 37.4% im-provement from state-of-the-art algorithms. We also present a qualitative evaluation on the real captures provided by Pan et al. [25]. Compared with baseline approaches, our method is less noisy, more realistic, and temporally smoother.
In summary, our key contributions are: 1. We represent a video by per-pixel parametric polyno-mials. We discuss why this representation integrates easily with the event mechanism by showing the par-allelism between function derivatives and events. 2. From a blurry image and its associated events in the exposure interval, we demonstrate how to use a deep learning model to predict a sharp video represented by the proposed parametric polynomials. 3. To overcome the limitations of the polynomial repre-sentation, we discuss how to formulate a refinement objective and encourage the temporal propagation of sharp visual features. 4. We provide source code and documentation for con-verting the original REDS dataset into the event for-mat. This clears the vagueness of the evaluation dataset in previous works and establishes an open-source benchmark for future comparisons. 2.