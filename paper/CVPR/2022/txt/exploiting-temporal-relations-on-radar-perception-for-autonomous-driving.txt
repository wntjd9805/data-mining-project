Abstract
We consider the object recognition problem in au-tonomous driving using automotive radar sensors. Com-paring to Lidar sensors, radar is cost-effective and robust in all-weather conditions for perception in autonomous driving.
However, radar signals suffer from low angular resolution and precision in recognizing surrounding objects. To en-hance the capacity of automotive radar, in this work, we exploit the temporal information from successive ego-centric bird-eye-view radar image frames for radar object recogni-tion. We leverage the consistency of an object’s existence and attributes (size, orientation, etc.), and propose a tempo-ral relational layer to explicitly model the relations between objects within successive radar images. In both object detec-tion and multiple object tracking, we show the superiority of our method compared to several baseline approaches. 1.

Introduction
Autonomous driving utilizes sensing technology for ro-bust dynamic object perception, and sequentially uses the perception for reliable and safe vehicle decision-making [34].
Among various perception sensors, camera and Lidar are the two dominant ones exploited for surrounding object recogni-tion. The camera provides semantically rich visual features of traffic scenarios, while Lidar provides high-resolution point clouds that can capture the reflection from objects.
Compared with camera and Lidar, radar enjoys the following unique advantages when applied in automotive applications.
Primarily operating at 77 GHz, radar transmits electromag-netic waves at a millimeter wavelength to estimate the range, velocity, and angle of objects. At such a wavelength, it can penetrate or diffract around tiny particles in conditions such as rain, fog, snow, and dust, and offer long-range perception in these adverse weather conditions [35]. In contrast, laser sent by Lidar at a much shorter wavelength may bounce off these tiny particles, which leads to a significantly reduced operating range. Compared with the camera, radar is also resilient to light conditions, e.g., night and sun glare. Fur-thermore, radar offers a cost-effective and reliable option
*Work done during the internship at MERL
Figure 1. Showcasing of two successive radar images and the corresponding camera recording from Radiate dataset [20]. From top to bottom, we display examples in the normal, foggy, and snowy weather. The bounding boxes are the ground-truth annotations of objects where its color implies the object ID. The plotted arrows show the consistency of the object’s appearance and attributes within a short time period, e.g., length, width, and orientation. to complement other sensors. For the cost of Lidar, accord-ing to an aggressive estimate by Luminar, is expected to be the range of $500 - $1000 [1]. In contrast, automotive radar is expected to be less than $100 in 2022 [8]. However, as a disadvantage of radar-assisted automotive perception, a high angular resolution in the azimuth and elevation do-mains are indispensable. In recent open-access automotive radar datasets, an azimuth resolution of 1◦ becomes avail-able, while the elevation resolution is still lagging behind.
With 1◦ azimuth resolution, semantic features for objects in a short range, e.g., corners and shapes, can be observed, while an object at far distances can still be blurred due to the cross-range resolution. In summary, the capability of localizing and identifying objects for radar is still falling behind from full-level autonomous driving.
Some recent efforts have been taken to leverage and en-hance automotive radar for object recognition from an algo-rithmic perspective. [14] proposes a deep-learning approach using range-azimuth-doppler measurement. [16] detects ob-jects via synchronous radar and Lidar signals. Similarly,
[12, 30] exploit the multi-modal sensing fusion. Besides deep learning, Bayesian learning has also attempted to solve extended object tracking with radar point clouds [28, 31].
The above works mainly focus on multi-modal sensing fu-sion for robust perception [12, 16, 30]. Differently, in this paper, we take our attempt to enhance the perception only using radar information, which requires fewer perception resources and avoids a complicated synchronized process for signals among multi-modal sensors.
In this paper, we consider ego-centric bird-eye-view radar point clouds presented in a Cartesian frame, where pixel values indicate the strength of reflections. We develop an approach to enhance radar perception using temporal in-formation. Based on the observation in Fig. 1, we assume that the same objects detected by radar within successive frames are consistent and share almost the same attributes, such as the object’s existence, length, orientation, etc. As a result, the detection at one frame can be facilitated by a previous/future frame through object-level correlations. To compensate for the blurriness and low angular resolution raised by radar sensors, we involve temporality and incor-porate customized temporal relational layers to explicitly handle the object-level relations across successive frames.
The temporal relational layer takes feature vectors at the potential object’s centers and conducts a temporal as well as a self-attention over the object features which are wrapped with their locality. Colloquially, this layer links temporally similar objects and transmits their representations, and is akin to feature smoothing. Hence, temporal relational layers could insert the inductive bias from object temporal consis-tency. Afterward, the object heatmap (indicating the center of objects) and relevant attributes are inferred upon the up-dated feature representation from temporal relational layers.
In this work, we consider the object recognition problem using radar in autonomous driving, which is a crucial alter-native sensing technology that owes unique advantages. We underline major contributions of our work as follows:
• We facilitate the radar perception with additional tem-poral information to compensate for the blurriness and low angular resolution raised by radar sensors.
• We design a customized temporal relational layer, where the networks are inserted with an inductive bias that the same object in successive frames should share consistent appearance and attributes.
• We evaluate our method in object detection and multiple object tracking on Radiate dataset. With the compre-hensive comparison to baseline methods, we show the consistent improvements brought by our method. 2. Radar Perception: