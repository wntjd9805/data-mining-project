Abstract
Recent self-supervised learning (SSL) methods have shown impressive results in learning visual representations from unlabeled images. This paper aims to improve their performance further by utilizing the architectural advan-tages of the underlying neural network, as the current state-of-the-art visual pretext tasks for SSL do not enjoy the ben-efit, i.e., they are architecture-agnostic. In particular, we fo-cus on Vision Transformers (ViTs), which have gained much attention recently as a better architectural choice, often out-performing convolutional networks for various visual tasks.
The unique characteristic of ViT is that it takes a sequence of disjoint patches from an image and processes patch-level representations internally.
Inspired by this, we design a simple yet effective visual pretext task, coined SelfPatch, for learning better patch-level representations. To be specific, we enforce invariance against each patch and its neigh-bors, i.e., each patch treats similar neighboring patches as positive samples. Consequently, training ViTs with Self-Patch learns more semantically meaningful relations among patches (without using human-annotated labels), which can be beneficial, in particular, to downstream tasks of a dense prediction type. Despite its simplicity, we demonstrate that it can significantly improve the performance of existing SSL methods for various visual tasks, including object detection and semantic segmentation. Specifically, SelfPatch signif-icantly improves the recent self-supervised ViT, DINO, by achieving +1.3 AP on COCO object detection, +1.2 AP on
COCO instance segmentation, and +2.9 mIoU on ADE20K semantic segmentation. 1.

Introduction
Recently, self-supervised learning (SSL) has achieved successful results in learning visual representations from unlabeled images with a variety of elaborate pretext tasks, including contrastive learning [5, 6, 14], clustering [3], and pseudo-labeling [4, 7, 13]. The common nature of their de-signs is on utilizing different augmentations from the same image as the positive pairs, i.e., they learn representations to be invariant to the augmentations. The SSL approaches without utilizing human-annotated labels have been com-petitive with or even outperformed the standard supervised learning [16] in various downstream tasks, including image classification [6], object detection [3], and segmentation [3].
Meanwhile, motivated by the success of Transformers
[29] in natural language processing [1, 10], Vision Trans-formers (ViTs) [11, 27, 28] have gained much attention as an alternative to convolutional neural networks (CNNs) with superior performance over CNNs in various visual tasks [25,27]. For example, ViT-S/16 [27] has a 1.8 faster throughput than ResNet-152 [16] with higher accuracy in the ImageNet [9] benchmark.
×
There have been several recent attempts to apply existing self-supervision techniques to ViTs [4, 8, 36]. Although the techniques have shown to be also effective with ViTs, they do not fully utilize the architectural advantages of ViTs, i.e., their pretext tasks are architecture-agnostic. For ex-ample, ViTs are able to process patch-level representations, but pretext tasks used in the existing SSL schemes only use the whole image-level self-supervision without considering learning patch-level representations. As a result, existing self-supervised ViTs [4, 8] may fail to capture semantically meaningful relations among patches; e.g., collapsed self-attention maps of among patches as shown in the second row of Fig. 1. This limitation inspires us to investigate the following question: how to utilize architectural characteris-tics of ViTs for improving the quality of learned patch-level representations without human-annotated supervision?
Contribution. In this paper, we propose a simple yet effec-tive SSL scheme for learning patch-level representations, coined SelfPatch, which can be beneficial to various visual downstream tasks. Our patch-level SSL scheme, SelfPatch, can be incorporated into any image-level self-supervised
ViT, e.g., DINO [4], MoCo-v3 [8] and MoBY [36], for learning both global (i.e., image-level) and local (i.e., patch-level) information simultaneously. Fig. 1 shows that Self-Patch enhances the quality of self-attention maps of DINO, which is evidence that SelfPatch encourages to learn better patch-level representations.
Our key idea is to treat semantically similar neighboring
Figure 1. Visualization of top-10% patches obtained by thresholding the self-attention maps of query patches (top) in the last layer of
ViT-S/16 trained with DINO (middle) and DINO + SelfPatch (bottom). While the selected patches obtained by DINO are not semantically correlated with its query patch, SelfPatch encourages the model to learn semantic correlations among patches. patches as positive samples motivated by the following prior knowledge: adjacent patches often share a common seman-tic context. Since there might be multiple positive patches (but we do not know exactly which patches are positive), we first select a fixed number of adjacent patches for pos-itive candidates using the cosine similarity between patch representations of the current model. Here, some of them might still be noisy (e.g., not positive), and for the purpose of denoising, we summarize their patch representations by utilizing an additional attention-based aggregation module on the top of ViT. Then, we minimize the distance between each patch representation and the corresponding summa-rized one. We provide an overall illustration of the proposed scheme in Fig. 2a.
To demonstrate the effectiveness of our method, we pre-train ViT-S/16 [27] on the ImageNet [9] dataset and evaluate transferring performance on a wide range of dense predic-tion downstream tasks: (a) COCO object detection and in-stance segmentation [20], (b) ADE20K semantic segmen-tation [38], and (c) DAVIS 2017 video object segmenta-tion [24]. Specifically, our method improves the state-of-the-art image-level self-supervision, DINO [4], with a large margin, e.g., +1.3 APbb (i.e., 40.8 42.1) on COCO de-tection (see Tab. 1), and +2.9 mIoU (i.e., 38.3 41.2) on
ADE20K segmentation (see Tab. 2). As a result, our method outperforms all the SSL baselines [3, 6, 31, 33, 34, 36] such as DenseCL [31]. Furthermore, we demonstrate high com-patibility of SelfPatch by incorporating with various objec-tives (e.g., MoBY [36]), architectures (e.g., Swin Trans-former [21]), and patch sizes (e.g., 8 8). For example, Self-Patch improves MoBY [36] for both ViT and Swin Trans-)m, respectively, on the former by +4.8 and +5.6 (
J
DAVIS video segmentation benchmark [24] (see Tab. 5).
→
→
×
&
F 2. Method
In this section, we introduce a simple yet effective vi-sual pretext task, coined SelfPatch, for learning better patch-level representations, which is tailored to Vision Trans-formers [11] for utilizing their unique architectural advan-tages. We first review Vision Transformers with recent self-supervised learning schemes [4, 8, 36] in Sec. 2.1 and then present details of SelfPatch in Sec. 2.2. Fig. 2a illustrates the overall scheme of our method, SelfPatch. 2.1. Preliminaries
RH
×
×
W
∈ x(i)
{
C be an image
Vision Transformers. Let x where (H, W ) is the resolution of x and C is the number of channels. Vision Transformers (ViTs) [11] treat the im-age x as a sequence of non-overlapping patches
∈
RP 2C
N i=1 (i.e., tokens) where each patch has a fixed reso-} lution (P, P ). Then, the patches are linearly transformed to
D-dimensional patch embeddings e(i) = Ex(i) + E(i) pos
∈
P 2C is a linear projection and E(i)
RD where E pos
∈
RD is a positional embedding for the patch index i. ViTs also prepend the [CLS] token, which represents the entire patches (i.e., the given image x), to the patch sequence with
RD. The resulting input a learnable embedding e[CLS] sequence e is e = [e[CLS]; e(1); e(2); . . . ; e(N )]. Then, ViTs take the input e and output all the patch-level and image-level (i.e., [CLS] token) representations with a transformer encoder.1 For conciseness, we use fθ to denote the whole process of a ViT parameterized by θ2 as follows:
RD
∈
∈
× fθ(x) = fθ([e[CLS]; e(1); e(2); . . . ; e(N )]) (x); f (1) (x); f (2)
= [f [CLS]
θ
θ
θ (x); . . . ; f (N )
θ (x)], (1)
Overall, our work highlights the importance of learn-ing patch-level representations during pre-training ViTs in a self-supervised manner. We hope that ours could inspire researchers to rethink the under-explored problem and pro-vide a new direction of patch-level self-supervised learning. 1We omit the details of the transformer encoder [29] of which each layer consists of the self-attention module, skip connection, and multi-layer perceptron (MLP). 2Note that θ contains all the transformer encoder parameters and em-bedding parameters E, Epos, and e[CLS].
(a) Illustration of the proposed patch-level self-supervision (SelfPatch) (b) Positive matching
Figure 2. (a) Top: image-level self-supervision, which minimizes the distance between the final representations of two differently aug-mented images. Bottom: patch-level self-supervision, SelfPatch, which minimizes the distance between the final representations of each patch and its positives. We use both types of self-supervision for learning image-level and patch-level representations simultaneously. (b)
For a given query patch, we find semantically similar patches from its neighborhood using the cosine similarity on the representation space.
θ
θ
{ f (i)
θ (x) (x) and f (i) where f [CLS]
θ (x) are the final representations of the [CLS] token and the i-th patch, respectively. Re-mark that f [CLS] (x) is utilized for solving image-level down-stream tasks such as image classification [11, 27, 28] while
N the patch-level representations i=1 are done for
} dense prediction tasks, e.g., object detection [2] and seman-tic segmentation [35].
Self-supervised learning with ViTs. The recent litera-ture [4, 8, 36] has attempted to apply self-supervised learn-ing (SSL) schemes to ViTs. They commonly construct a positive pair (x, x+) by applying different augmentations to the same image, and then learn their representations to be similar, i.e., invariant to augmentations. We here pro-vide a generic formulation of this idea.3 To this end, we denote two ViT backbone networks as fθ and f˜θ, and their projection heads as gθ and g˜θ are parametrized by θ and
˜θ, respectively. Then, the generic form of image-level self-3Built upon this formulation, one can additionally consider negative pairs for contrastive learning or asymmetric architectures such as a predic-tion head [8, 36]. supervision
SSL(
{
L x, x+
; θ, ˜θ) can be written as follows:
}
SSL := D(gθ(f [CLS]
θ (x)), sg(g˜θ(f [CLS]
˜θ (x+)))), (2)
L where D is a distance function and sg is the stop-gradient operation. Note that the choices of distance D and archi-tecture g depend on a type of self-supervision; for example,
Caron et al. [4] update ˜θ by the exponential moving average of θ, and use Kullback-Leibler (KL) divergence as D, where the projection heads g˜θ and gθ are designed to produce a probability distribution over the final feature dimension.
We remark that the idea of constructing a positive pair (x, x+) of augmented images is architecture-agnostic, which means it does not fully utilize the architectural bene-fits of ViTs. For example, ViTs can handle patch-level rep-resentations
, but the recent SSL schemes use only f [CLS] (x) as described in Eq. (2). This motivates us to ex-θ plore the following question: how to construct patch-level self-supervision, i.e., positive pairs of patches? f (i)
θ (x)
}
{
2.2. Patch-level self-supervision objectives,
SSL(x, x+) and
L
SelfPatch(x), as follows:
L
P
∈P
N
}j (i) from its neighborhood (i) as self-supervision for x(i) (see Fig. 2a).
Recall that our goal is to learn better patch-level repre-sentations, which can be beneficial to various downstream tasks. Our key idea is to consider neighboring patches as positive samples based on the continuous nature of image patches. Specifically, SelfPatch aims to learn patch-level representations via predicting self-supervision constructed by the following procedure: for each patch x(i), Positive
Matching first finds a set of candidates for positive patch (i) (see Fig. 2b), and indices then Aggregation Module aggregates their representations f (j)
θ (x)
{
Neighboring patches. Given a query patch x(i), we as-sume that there exists a semantically similar patch x(j) (i), because neighboring patches in its neighborhood x(j) (i) often share a semantic context with the query
}j
{
∈N x(i). Let (i) be a set of neighboring patches, (i) is a set of patch indices in the neighborhood. where
N (i) to be adjacent patches (i.e.,
We simply use
= 8), and empirically found that this choice is important for selecting candidates for positive patches (see Sec. 5.1 for analysis on the importance of neighboring patches).
Matching positives from the neighborhood. To sample positive (i.e., semantically similar) patches from the neigh-(i), we measure the semantic closeness between borhood the query patch x(i) and its neighboring patch x(j) for all
θ (i). To this end, we use the cosine similarity on the j representation space, i.e., x(j)
{
∈ N
|N
}j
∈N
N
N
N (i)
|
∈P
}j (3)
||2||
θ (x)/ f (j)
θ (x) s(i, j) = f (i)
θ (x)⊤f (j)
||2. x(j) (i) based on the
{ (i) is a set of patch indices
P (i). We use k =
= 4 in our f (i)
θ (x)
||
We take top-k positive patches similarity scores s(i, j), where of top-k patches in experiments (see Sec. 5.1 for analysis on the effect of k).
Aggregation module. We aggregate positive patches x(j) (i) using an aggregation module hθ to construct
{ patch-level self-supervision y(i) for each patch x(i). We
θ utilize hθ for not only patch representations (i.e., y(i)
θ ), but also the image-level representation (i.e., yθ). Formally,
}j
|P
N
∈P (i)
| yθ := hθ( y(i)
θ
:= hθ( f (j)
N
θ (x) j=1),
}
{ f (j)
θ (x)
}j
{
∈P (i)). (4) (5)
We follow Touvron et al. [28] to implement the aggrega-tion module hθ. To be specific, hθ is a small Transformer that takes a sequence of representations as an input and out-puts a representation of the [CLS] token. We found that the aggregation module hθ is crucial for generating better self-supervision (see Sec. 5.1 for ablation experiments). total(x, x+)
Training objective. Our training objective consists of the image-level and patch-level self-supervision
L
SSL(x, x+) := D(gθ(yθ), sg(g˜θ(y˜θ))),
L
N
SelfPatch(x) :=
L 1
N total(x, x+) :=
L
L i=1 (cid:88)
SSL(x, x+) + λ
L
D(g′θ(f (i)
θ (x)), sg(g′˜θ(y(i)
˜θ
))), (7)
SelfPatch(x), (8) (6)
L
L
∈P
}j
SSL and
L x(j)
{ where D is a distance function, sg is the stop-gradient operation, λ is a hyperparameter, g and g′ are projection
SelfPatch, respectively. Here, our patch-heads for
SelfPatch(x) enforces a query patch x(i) and level objective its positives (i) to be similar by minimizing the distance between the patch and aggregated representations, f (i)
θ (x) and y(i)
SelfPatch(x)
˜θ has an asymmetric form: a query representation f (i)
θ (x) does not use the aggregation module hθ while its (aggre-gated) target y(i) does. We empirically found that this ar-˜θ chitectural asymmetry with the stop-gradient operation ef-fectively avoids the mode collapse issue which comes from minimizing the distance between patch representations.
, respectively. We remark that
L 3.