Abstract
Only parts of unlabeled data are selected to train mod-els for most semi-supervised learning methods, whose confi-dence scores are usually higher than the pre-defined thresh-old (i.e., the confidence margin). We argue that the recog-nition performance should be further improved by making full use of all unlabeled data. In this paper, we learn an
Adaptive Confidence Margin (Ada-CM) to fully leverage all unlabeled data for semi-supervised deep facial expres-sion recognition. All unlabeled samples are partitioned into two subsets by comparing their confidence scores with the adaptively learned confidence margin at each training epoch: (1) subset I including samples whose confidence scores are no lower than the margin; (2) subset II includ-ing samples whose confidence scores are lower than the margin. For samples in subset I, we constrain their pre-dictions to match pseudo labels. Meanwhile, samples in subset II participate in the feature-level contrastive objec-tive to learn effective facial expression features. We ex-tensively evaluate Ada-CM on four challenging datasets, showing that our method achieves state-of-the-art perfor-mance, especially surpassing fully-supervised baselines in a semi-supervised manner. Ablation study further proves the effectiveness of our method. The source code is avail-able at https://github.com/hangyu94/Ada-CM . 1.

Introduction
Facial expression recognition (FER) aims to make com-puters understand visual emotion. Recently, the advance-ment of deep FER is largely promoted by large-scale la-beled datasets, e.g., RAF-DB [16] and AffectNet [22].
However, the collection of large-scale labels is quite ex-pensive and difficult. Besides, existing labels often fail to
*Corresponding author
Figure 1. Confidence scores by 30 volunteers on ten faces annotated with seven classes, including surprise, fear, disgust, happiness, sadness, anger and neutral. The upper left corner of each face is tagged with its confidence score. All faces are divided into three groups based on the confidence score. The results pro-vide insights that the confidence scores may be inconsistent among different classes and even the confidence gap between intra-class expressions may be large, e.g., faces annotated with Sadness. satisfy actual fine-grained needs and the re-labeling data re-quires human experts. Therefore, it is urgent to develop a powerful method for training models on a large amount of data without corresponding labels, i.e., semi-supervised deep facial expression recognition (SS-DFER).
Most recent semi-supervised learning (SSL) algorithms achieve competitive performance by predicting artificial la-bels of unlabeled data. For example, pseudo-labeling meth-ods [12, 14, 24, 35] utilize the model predictions as artifi-cial labels to retrain CNN models. Typically, FixMatch
[28] explores weakly-augmented and strongly-augmented data pairs and selects only unlabeled samples with high-confidence predictions, whose confidence scores are above the pre-defined fixed threshold (e.g., 0.95).
Despite excellent performance on common classification tasks, the threshold-based pseudo-labeling strategy is still challenging for SS-DFER mainly due to two reasons: (1)
The fixed threshold for all categories. Facial expressions from different categories are classified with varying degrees of difficulty. To better understand this, we randomly pick
several images from RAF-DB [16] and conduct a user study.
As shown in Figure 1, for the face annotated with Happi-ness, the confidence score is much higher than other facial expressions. Especially, the confidence gap between the most and least possibles is up to 28%. Therefore, the fixed threshold is unfair to different facial expressions. In other words, the fixed threshold (e.g., 0.95) may lead to select-ing too many expressions with high confidence scores (e.g., happiness) and too few expressions with low or lower con-fidence scores (e.g., disgust). Moreover, the fixed setting is not adaptive enough at each training epoch. (2) Ineffi-cient data utilization. There is a large gap between the con-fidence scores of different intra-class samples. For example, the confidence gap between faces annotated with Sadness is as large as 25% (see Figure 1). This issue may cause that some intra-class samples with low confidence scores can-not be selected for training models, e.g., the Sadness with the confidence score of 0.71. This inspires us to consider that how samples with low confidence scores contribute to feature learning. Hence, to fully leverage unlabeled data with the adaptive threshold is crucial for SS-DFER.
To this end, we propose a semi-supervised DFER algo-rithm with an Adaptive Confidence Margin (Ada-CM) to enjoy its adaptivity in terms of the learning on all unlabeled data. Specifically, the proposed Ada-CM firstly runs over all given labeled data and adaptively updates the confidence margin based on the learning difficulty of different facial ex-pressions. Importantly, the confidence margin is gradually improved over training epochs. Then, it predicts confidence scores of weakly-augmented unlabeled data, which are compared with the learned confidence margin to partition all unlabeled samples into two subsets: subset I including samples with high confidence scores (i.e., whose confidence scores are not lower than the margin) and subset II including samples with low confidence scores (i.e., whose confidence scores are lower than the margin). For samples in subset I,
Ada-CM leverages strongly-augmented unlabeled samples and pseudo labels from their weakly-augmented versions to calculate the cross-entropy loss. Moreover, for subset II, we conduct a feature-level contrastive objective to learn effec-tive features by applying the InfoNCE loss [4]. Overall, our main contributions can be summarized as follows:
• We propose a novel end-to-end semi-supervised DFER method by adaptively learning the confidence margin.
To the best of our knowledge, this is the first solution to explore the dynamic confidence margin in SS-DFER.
• An adaptive confidence margin is designed to dynam-ically learn on all unlabeled data for the model’s train-ing. More importantly, samples with low confidence scores are leveraged to enhance the feature-level simi-larity.
• Extensive experiments on four challenging datasets show the effectiveness of our proposed Ada-CM.
Especially, our method achieves superior perfor-mance, surpassing fully-supervised baselines in a semi-supervised manner. 2.