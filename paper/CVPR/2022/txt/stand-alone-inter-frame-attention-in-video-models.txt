Abstract
Motion, as the uniqueness of a video, has been critical to the development of video understanding models. Mod-ern deep learning models leverage motion by either ex-ecuting spatio-temporal 3D convolutions, factorizing 3D convolutions into spatial and temporal convolutions sepa-rately, or computing self-attention along temporal dimen-sion. The implicit assumption behind such successes is that the feature maps across consecutive frames can be nicely aggregated. Nevertheless, the assumption may not always hold especially for the regions with large defor-mation.
In this paper, we present a new recipe of inter-frame attention block, namely Stand-alone Inter-Frame At-tention (SIFA), that novelly delves into the deformation across frames to estimate local self-attention on each spa-tial location. Technically, SIFA remoulds the deformable design via re-scaling the offset predictions by the difference between two frames. Taking each spatial location in the cur-rent frame as the query, the locally deformable neighbors in the next frame are regarded as the keys/values. Then,
SIFA measures the similarity between query and keys as stand-alone attention to weighted average the values for temporal aggregation. We further plug SIFA block into Con-vNets and Vision Transformer, respectively, to devise SIFA-Net and SIFA-Transformer. Extensive experiments con-ducted on four video datasets demonstrate the superiority of SIFA-Net and SIFA-Transformer as stronger backbones.
More remarkably, SIFA-Transformer achieves an accuracy of 83.1% on Kinetics-400 dataset. Source code is available at https://github.com/FuchenUSTC/SIFA. 1.

Introduction
Video is an electronic representation of moving visual images and naturally forms the motion, which signifies a continuous change in position of objects or persons with time. Modeling such temporal dynamics is essential to the
Figure 1. Illustration of (a) temporal convolution, (b) self-attention along temporal dimension, and (c) our inter-frame attention. extension from understanding still images to videos. The recent advances generally suggest to leverage motion along two directions. One involves utilization of temporal con-volutions by being integrated into space-time 3D convolu-tions [18, 50] or explicitly co-working with spatial convolu-tions [3, 52, 62]. The other measures self-attention of each location over the temporal neighbors at the same spatial po-sition across frames. Figure 1(a) and (b) conceptually de-pict the implementation of temporal convolution and self-attention along temporal dimension, respectively. The un-derlying spirit behind these operations originates from the foundation that the feature maps across frames should be well aligned. This assumption nevertheless may not always be valid in practice. Taking the three consecutive frames in
Figure 1 as an example, the same positions across frames highlighted in the circles correspond to different objects (person and track in the case) due to the motion of the ath-lete in pole vault. As such, performing temporal convolu-tion or computing attention over these positions might be suboptimal for temporal feature aggregation.
To alleviate this issue, we propose to take the changes
in video content caused by motion into account to enhance the alignment of feature maps across frames and eventu-ally improve temporal aggregation. Technically, we develop inter-frame attention as shown in Figure 1(c) to character-ize richer inter-frame correlation within a local neighboring region rather than only the same spatial location in consec-utive frames. By doing so, inter-frame attention, on one hand, is beneficial more with large receptive fields, and on the other, manifests the emphasis of each location in the region to better achieve feature alignment. In an effort to nicely support the regions with large deformation, we fur-ther capitalize on the deformable design and estimate the offset to each spatial location. Moreover, we uniquely ex-ploit the motion cues across frames to act as motion supervi-sory signal and re-scale the deformable feature re-sampling.
By delving into the deformation across frames to in-fer temporal attention within locally deformable region for temporal modeling, we present a novel Stand-alone Inter-Frame Attention (SIFA) block in video models. Specifi-cally, we take each spatial location in the current frame as the query, and its temporal neighbors within the local re-gion of the next frame are treated as keys/values accordingly to trigger the inter-frame attention learning. Note that in view of the irregular geometric transformations of objects, we sample the keys/values of temporal neighbors in a spa-tial deformation, which is learnt with additional guidance of the motion cues across frames. After that, SIFA block re-gards the estimated inter-frame attention of each temporal neighbor as its temporal correlation against query. Finally, we aggregate all temporal neighbors of nearby frames with inter-frame attention weights to further strengthen the query feature in current frame via temporal aggregation.
The SIFA block can be viewed as a stand-alone attention primitive for temporal modeling, and is readily pluggable to any 2D CNN or Vision Transformer backbones for video representation learning. By directly inserting SIFA block in ResNet [17] and Swin Transformer [32], we construct two new video backbones, named as SIFA-Net and SIFA-Transformer, respectively. Through extensive experiments on a series of action recognition benchmarks, we demon-strate that our SIFA-Net and SIFA-Transformer outperform several state-of-the-art video backbones. 2.