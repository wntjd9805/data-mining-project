Abstract
Unsupervised methods have showed promising results on monocular depth estimation. However, the training data must be captured in scenes without moving objects. To push the envelope of accuracy, recent methods tend to increase their model parameters.
In this paper, an unsupervised learning framework is proposed to jointly predict monocu-lar depth and complete 3D motion including the motions of moving objects and camera. (1) Recurrent modulation units are used to adaptively and iteratively fuse encoder and de-coder features. This improves the single-image depth infer-ence without overspending model parameters. (2) Instead of using a single set of ﬁlters for upsampling, multiple sets of ﬁlters are devised for the residual upsampling. This fa-cilitates the learning of edge-preserving ﬁlters and leads to the improved performance. (3) A warping-based network is used to estimate a motion ﬁeld of moving objects without using semantic priors. This breaks down the requirement of scene rigidity and allows to use general videos for the unsupervised learning. The motion ﬁeld is further regular-ized by an outlier-aware training loss. Despite the depth model just uses a single image in test time and 2.97M pa-rameters, it achieves state-of-the-art results on the KITTI and Cityscapes benchmarks. 1.

Introduction
Visual perception is an important ability for human to understand and perceive the world. As a consequence, re-search work on scene geometry has attracted a lot of atten-tion over several decades. This promotes the deployment of technology to numerous applications such as autonomous vehicle, interactive robot, virtual and augmented reality, and more. The problem of scene geometry generally involves estimating depth, camera motion1, and optical ﬂow from an image sequence. The above computer vision tasks are often recovered together since they are coupled through geomet-ric constraints [34, 44].
Unlike depth from triangulation, single-image depth es-timation is inherently ill-posed because there are multiple possible 3D points along each light ray towards the camera center. Convolutional neural networks have demonstrated the ability to exploit the relationship between a captured image and the corresponding scene depth [9, 25]. Recently, unsupervised methods [13, 14, 34, 44, 47] have achieved ap-pealing performance than the early supervised counterparts.
Their successes primarily rely on the use of structure from motion. Given at least two images, a novel view gener-ated from an image will be consistent with another image in the pair if depth and camera motion are correctly esti-mated. This strictly requires the training data to be captured in static scenes without moving objects, i.e. scene rigidity.
To get rid of this requirement, stereo image sequences [13] and masking out dynamic objects [34, 47] are commonly adopted. Recent works tend to devise a multi-image ap-proach [42], a large amount of model parameters [16], and semantic priors [39] for improving the depth accuracy.
In this paper, an unsupervised learning framework of re-current monocular depth, dubbed RM-Depth, is proposed to jointly predict depth, camera motion, and motion ﬁeld of moving objects without requiring static scenes. RM-Depth requires neither a large number of parameters nor semantic prior. Particularly, image pairs are used in training while only a single image is used for depth inference at test time.
The contributions of this work are summarized as follows: 1. Recurrent modulation unit – Fusion of feature maps across encoder and decoder often appears in top-down
I propose to iteratively reﬁne the approach [14, 35]. fusion by adaptive modulating of the encoder features using the hidden state of the decoder. This in turn im-proves the single-image depth inference.
*This research work is not for commercial use unless a prior arrange-ment has been made with the author. 1The words, ego-motion, camera motion and pose, are interchangeably used throughout the paper. 2. Residual upsampling – Conventionally, feature maps are upsampled using a single set of ﬁlters [37, 45]. I propose to use multiple sets of ﬁlters such that each
set of them is speciﬁcally trained for upsampling some of the spectral components. This effectively improves upsampling along edges. 3. Motion ﬁeld of moving objects – Besides camera mo-tion, I propose to estimate a 3D motion ﬁeld of moving objects in a coarse-to-ﬁne framework through a warp-ing approach. This breaks down the scene rigidity as-sumption and allows to use general videos for the un-supervised learning. The unsupervised learning of mo-tion ﬁeld is further improved by introducing an outlier-aware regularization loss.
With the above innovations, RM-Depth achieves state-of-the-art results on the KITTI and Cityscapes benchmarks.
The depth model only requires 2.97M parameters, while it achieves 4.8 and 44 times reduction in model size com-paring to Monodepth2 [14] and PackNet [16], respectively.
Code and trained models are made publicly available at https://github.com/twhui/RM-Depth. 2.