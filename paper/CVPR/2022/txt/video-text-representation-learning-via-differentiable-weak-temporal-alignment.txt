Abstract
Learning generic joint representations for video and text by a supervised method requires a prohibitively substan-tial amount of manually annotated video datasets. As a practical alternative, a large-scale but uncurated and nar-rated video dataset, HowTo100M, has recently been intro-duced. But it is still challenging to learn joint embed-dings of video and text in a self-supervised manner, due to its ambiguity and non-sequential alignment. In this paper, we propose a novel multi-modal self-supervised framework
Video-Text Temporally Weak Alignment-based Contrastive
Learning (VT-TWINS) to capture signiﬁcant information from noisy and weakly correlated data using a variant of
Dynamic Time Warping (DTW). We observe that the stan-dard DTW inherently cannot handle weakly correlated data and only considers the globally optimal alignment path. To address these problems, we develop a differentiable DTW which also reﬂects local information with weak temporal alignment. Moreover, our proposed model applies a con-trastive learning scheme to learn feature representations on weakly correlated data. Our extensive experiments demon-strate that VT-TWINS attains signiﬁcant improvements in multi-modal representation learning and outperforms var-ious challenging downstream tasks. Code is available at https://github.com/mlvlab/VT-TWINS. 1.

Introduction
Learning video-text representations is an important prob-In recent years, it has recently lem in computer vision. drawn increasing attention due to a large amount of video data and various applications. Previous works [32, 52, 57] have achieved exciting results by learning mappings be-tween video clips and texts but they usually require a large amount of manual annotations such as MSR-VTT [55],
*is the corresponding author.
DiDeMo [3], EPIC-KITCHENS [13]. However, since la-beling videos is expensive and time-consuming, it does not scale well for sufﬁciently large datasets which are es-sential to learning generic video-text representations that are readily applicable to a wide range of downstream tasks that include text-to-video retrieval or video-text re-trieval [27, 50, 51, 56], text-based action localization [3, 11], action segmentation [29, 43] and video question answer-ing [34, 46, 56]. Recent studies suggest that multi-modal self-supervised learning with a huge amount of data is a promising alternative to fully supervised methods [15, 54].
To this extent, HowTo100M [36] has been introduced, which is composed of 100 million pairs of video clips and captions from 1.22M narrated instructional videos.
The HowTo100M is one of the largest video datasets but it comes with several challenges.
It is uncurated and its video-text pairs are weakly correlated meaning that given a video clip the caption depicting the visual content may appear before/after the clip or not even exist (Fig-ure 1). To handle the weakly correlated video-text pairs,
MIL-NCE [35] has proposed a multiple instance learn-ing (MIL)-based contrastive learning adopting Noise Con-trastive Learning (NCE) loss [19]. MIL-NCE treats the multiple captions which are temporally close to one clip as positive samples allowing one-to-many correspondence.
But this strong assumption often leads to suboptimal repre-sentation learning.
In this paper, to address the problem, we develop a new weak temporal alignment algorithm building upon Dy-namic Time Warping (DTW) [41]. In contrast to the stan-dard DTW which is limited to sequential alignment, our proposed alignment algorithm allows ﬂexibility by skip-ping irrelevant pairs and starting/ending at arbitrary time points. Also, it takes into account a globally optimal path as well as locally optimal paths by introducing local neigh-borhood smoothing. More importantly, our alignment algo-rithm is differentiable so we incorporate it into represen-tation learning as a distance measure. We then propose a novel multi-modal self-supervised learning framework to learn a joint video and text embedding model named as Video-Text Temporally Weak Alignment-based Con-trastive Learning (VT-TWINS) that automatically handles the correspondence between noisy and weakly correlated captions and clips.
Our extensive experiments on ﬁve benchmark datasets demonstrate that our learned video and text representations generalize well on various downstream tasks including ac-tion recognition, text-to-video retrieval, and action step lo-calization. Moreover, ablation studies and qualitative anal-ysis show that our framework effectively aligns the noisy and weakly correlated multi-modal time-series data.
Our contributions are threefold:
• We propose a novel self-supervised learning frame-work with differentiable weak temporal alignment that automatically handles the noisy and weakly correlated multi-modal time-series data.
• We analyze the local neighborhood smoothing in our alignment algorithm showing that unlike DTW the alignment takes into account local optimal paths as well as global optimal path.
• Our experiments show that the proposed method con-siderably improves joint representations of video and text an is adapted well on various downstream tasks. paper, we focus on extending contrastive learning to tempo-rally align two time-series modalities, i.e., clips and cap-tions from videos without any additional crossmodal en-coders.
Sequence Alignment. Sequence alignment is crucial in
ﬁelds related to the time-series data due to the temporal information. In particular, the lack of manually annotated video datasets makes it harder to align clips and captions temporally. Dynamic Time Warping (DTW) [41] measures the distance with strong temporal constraints between two sequences. [7] uses global sequence alignment as a proxy task by relying on the DTW. [12, 20] extended the DTW for end-to-end learning with differentiable approximations of the discrete operations (e.g., the ‘min’ operator) in the
DTW. Chang et al. [6] proposed the frame-wise alignment loss using the DTW in weakly supervised action alignment in videos. Drop-DTW [14] proposed a variant of the DTW algorithm which automatically drops the outlier elements from the pairwise distance to handle the noisy data. How-ever, using the DTW alone can cause feature collapsing which leads all the feature embeddings to be concentrated to a single point. To address this problem, [6] and [22] use the subsidiary regularization loss term with the DTW. 3. Preliminaries
We brieﬂy summarize the basic concepts of dynamic time warping and the characteristics of an uncurated nar-rated video dataset HowTo100M. 2.