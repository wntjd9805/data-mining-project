Abstract
Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn an avatar from only 2D images of people in uncon-strained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair and clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (“Implicit Clothed humans Obtained from
Normals”), which, instead, uses local features. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at in-ference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use a modified ver-sion of SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruc-tion, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruc-tion from in-the-wild images. This enables avatar creation directly from video with personalized pose-dependent cloth deformation. Models and code are available for research at https://icon.is.tue.mpg.de. 1.

Introduction
Realistic virtual humans will play a central role in mixed and augmented reality, forming a key foundation for the
“metaverse” and supporting remote presence, collaboration, education, and entertainment. To enable this, new tools are needed to easily create 3D virtual humans that can be readily animated. Traditionally, this requires significant artist effort and expensive scanning equipment. Therefore, such approaches do not scale easily. A more practical approach would enable individuals to create an avatar from one or more images. There are now several methods that take a single image and regress a minimally clothed 3D human model [4, 5, 13, 16, 28, 40]. Existing parametric body models, however, lack important details like clothing and hair [24, 32, 40, 45, 56]. In contrast, we present a method that robustly extracts 3D scan-like data from images of people in arbitrary poses and uses this to construct an animatable avatar.
We base our approach on implicit functions (IFs), which go beyond parametric body models to represent fine shape details and varied topology. IFs allow recent methods to infer detailed shape from an image [17, 19, 46, 47, 57, 62]. Despite promising results, state-of-the-art (SOTA) methods struggle with in-the-wild data and often produce humans with broken or disembodied limbs, missing details, high-frequency noise, or non-human shape; see Fig. 2 for examples.
The issues with previous methods are twofold: (1) Such methods are typically trained on small, hand-curated, 3D human datasets (e.g. Renderpeople [1]) with very limited pose, shape and clothing variation. (2) They typically feed their implicit-function module with features of a global 2D image or 3D voxel encoder, but these are sensitive to global pose. While more, and more var-ied, 3D training data would help, such data remains limited.
Hence, we take a different approach and improve the model.
Specifically, our goal is to reconstruct a detailed clothed 3D human from a single RGB image with a method that is training-data efficient and robust to in-the-wild images and out-of-distribution poses. Our method, called ICON, stands for Implicit Clothed humans Obtained from Normals. ICON replaces the global encoder of existing methods with a more data-efficient local scheme; Fig. 3 shows a model overview.
ICON takes as input an RGB image of a segmented clothed human and a SMPL body estimated from the image [27].
The SMPL body is used to guide two of ICON’s modules: one infers detailed clothed-human surface normals (front and back views), and the other infers a visibility-aware implicit surface (iso-surface of an occupancy field). Errors in the initial SMPL estimate, however, might misguide inference.
Thus, at inference time, an iterative feedback loop refines
SMPL (i.e., its 3D shape, pose, and translation) using the inferred detailed normals, and vice versa, leading to a refined implicit shape with better 3D details.
We evaluate ICON quantitatively and qualitatively on challenging datasets, namely AGORA [39] and CAPE [35], as well as on in-the-wild images. Results show that the state of the art:
ICON has two advantages w.r.t. (1) Generalization. ICON’s locality helps it generalize to in-the-wild images and out-of-distribution poses and clothes better than previous methods. Representative cases are shown in Fig. 2; notice that, although ICON is trained on full-body images only, it can handle images with out-of-frame cropping, with no fine tuning or post processing. (2) Data efficacy. ICON’s locality helps it avoid spurious correlations between pose and surface shape. Thus, it needs less data for training. ICON significantly outperforms base-lines in low-data regimes, as it reaches SOTA performance when trained with as little as 12% of the data.
We provide an example application of ICON for creating an animatable avatar; see Fig. 1 for an overview. We first apply ICON on the individual frames of a video sequence,
Figure 2. SOTA methods for inferring 3D humans from in-the-wild images, e.g., PIFu, PIFuHD, PaMIR, and ARCH++, struggle with challenging poses and out-of-frame cropping (E), resulting in var-ious artifacts including non-human shapes (A,G), disembodied parts (B,H), missing body parts (C,D), missing details (E), and high-frequency noise (F). ICON deals with these challenges and produces high-quality results, highlighted with a green shadow
Front view (blue) and rotated view (bronze). to obtain 3D meshes of a clothed person in various poses.
We then use these to train a poseable avatar using a modi-fied version of SCANimate [48]. Unlike 3D scans, which
SCANimate takes as input, our estimated shapes are not equally detailed and reliable from all views. Consequently, we modify SCANimate to exploit visibility information in learning the avatar. The output is a 3D clothed avatar that moves and deforms naturally; see Fig. 1-right and Fig. 8b.
ICON takes a step towards robust reconstruction of 3D clothed humans from in-the-wild photos. Based on this, fully textured and animatable avatars with personalized pose-aware clothing deformation can be created directly from video frames. Models and code are available at https:
//icon.is.tue.mpg.de. 2.