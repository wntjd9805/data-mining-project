Abstract
Video understanding requires reasoning at multiple spa-tiotemporal resolutions – from short fine-grained motions to events taking place over longer durations. Although transformer architectures have recently advanced the state-of-the-art, they have not explicitly modelled different spa-tiotemporal resolutions. To this end, we present Multiview
Transformers for Video Recognition (MTV). Our model con-sists of separate encoders to represent different views of the input video with lateral connections to fuse informa-tion across views. We present thorough ablation stud-ies of our model and show that MTV consistently per-forms better than single-view counterparts in terms of ac-curacy and computational cost across a range of model sizes. Furthermore, we achieve state-of-the-art results on six standard datasets, and improve even further with large-scale pretraining. Code and checkpoints are available at: https://github.com/google-research/scenic. 1.

Introduction
Vision architectures based on convolutional neural net-works (CNNs), and now more recently transformers, have made great advances in numerous computer vision tasks.
A central idea, that has remained constant across classi-cal methods based on handcrafted features [9, 14, 38] to
CNNs [42, 43, 84] and now transformers [11, 44, 73], has been to analyze input signals at multiple resolutions.
In the image domain, multiscale processing is typically performed with pyramids as the statistics of natural images are isotropic (all orientations are equally likely) and shift in-variant [30, 66]. To model multiscale temporal information in videos, previous approaches such as SlowFast [23] have processed videos with two streams, using a “Fast” stream operating at high frame rates and a “Slow” stream at low frame rates, or employed graph neural networks to model long-range interactions [4, 76].
When creating a pyramidal structure, spatio-temporal in-*This work was done while the first author was an intern at Google.
Figure 1. Overview of our Multiview Transformer. We create mul-tiple input representations, or “views”, of the input, by tokenizing the video using tubelets of different sizes (for clarity, we show two views here). These tokens are then processed by separate en-coder streams, which include lateral connections and a final global encoder to fuse information from different views. Note that the tokens from each view may have different hidden sizes, and the encoders used to process them can vary in architecture too. formation is partially lost due to its pooling or subsampling operations. For example, when constructing the “Slow” stream, SlowFast [23] subsamples frames, losing temporal information. In this work, we propose a simple transformer-based model without relying on pyramidal structures or sub-sampling the inputs to capture multi-resolution temporal context. We do so by leveraging multiple input representa-tions, or “views” of the input video. As shown in Fig. 1, we extract tokens from the input video over multiple temporal durations. Intuitively, tokens extracted from long time in-tervals capture the gist of the scene (such as the background where the activity is taking place), whilst tokens extracted from short segments can capture fine-grained details (such as the gestures performed by a person).
We propose a multiview transformer (Fig. 1) to pro-cess these tokens, and it consists of separate transformer
encoders specialized for each “view”, with lateral connec-tions between them to fuse information from different views to each other. We can use transformer encoders of vary-ing sizes to process each view, and find that it is better (in terms of accuracy/computation trade-offs) to use a smaller encoder (e.g. smaller hidden sizes and fewer layers) to rep-resent the broader view of the video (Fig. 1 left) while an encoder with larger capacity is used to capture the details (Fig. 1 right). This design therefore poses a clear contrast to pyramid-based approaches where model complexity in-creases as the spatio-temporal resolution decreases. Our design is verified by our experiments which show clear ad-vantages over the former approach.
Our proposed method, of processing different “views” of the input video is simple, and in contrast to previous work [23] generalizes readily to a variable number of views.
This is significant, as our experiments show that accuracy increases as the number of views grows. Although our pro-posed architecture increases the number of tokens processed by the network according to the number of input views, we show that we can consistently achieve superior accu-racy/computation trade-offs compared to the current state of the art [3], across a spectrum of model sizes, ranging from “Small” to “Huge”. We show empirically that this is because processing more views in parallel enables us to achieve larger accuracy improvements than increasing the depth of the transformer network. We perform thorough ablation studies of our design choices, and achieve state-of-the-art results on six standard video classification datasets.
Moreover, we show that these results can be further im-proved with large-scale pretraining. 2.