Abstract
Generic event boundary detection (GEBD) is an impor-tant yet challenging task in video understanding, which aims at detecting the moments where humans naturally per-ceive event boundaries. The main challenge of this task is perceiving various temporal variations of diverse event boundaries. To this end, this paper presents an effective and end-to-end learnable framework (DDM-Net). To tackle the diversity and complicated semantics of event boundaries, we make three notable improvements. First, we construct a feature bank to store multi-level features of space and time, prepared for difference calculation at multiple scales.
Second, to alleviate inadequate temporal modeling of pre-vious methods, we present dense difference maps (DDM) to comprehensively characterize the motion pattern. Fi-nally, we exploit progressive attention on multi-level DDM to jointly aggregate appearance and motion clues. As a re-sult, DDM-Net respectively achieves a significant boost of 14% and 8% on Kinetics-GEBD and TAPOS benchmark, and outperforms the top-1 winner solution of LOVEU Chal-lenge@CVPR 2021 without bells and whistles. The state-of-the-art result demonstrates the effectiveness of richer mo-tion representation and more sophisticated aggregation, in handling the diversity of GEBD. The code is made available at https://github.com/MCG-NJU/DDM . 1.

Introduction
With the explosive growth of online videos, video un-derstanding has drawn tremendous attention from both academia and industry. Cognitive science [43] suggests that humans naturally divide a video into meaningful units by perceiving event boundaries. To this end, a task termed as
Generic Event Boundary Detection [35] (GEBD) is re-cently proposed to localize the generic event boundaries in videos, which is expected to facilitate the development of (cid:66): Corresponding author.
Figure 1. Comparisons of sparse motion representation (black lines, optical flow) and dense motion representation (green lines, some are omitted for clarity, dense feature differences).
Numbers on lines indicate the magnitude of motion between two frames. Dense motion representation provides more holistic tem-poral cues to better distinguish boundaries and non-boundaries. video understanding.
Generic event boundaries in GEBD task are taxonomy-free and related to a broad range of temporal changes, in-cluding changes of action, subject and environment. The primary challenge in GEBD task is to model diverse pat-terns of generic event boundaries: a) Spatial diversity is dominantly characterized by the change of appearance, which normally comprises low-level changes (e.g., change in color or brightness) and high-level changes (e.g., the dominant subject appears or disappears). b) Temporal di-versity is mainly relevant to actions, such as change of ac-tion (e.g., walk to run) or change of object of interaction.
Notably, different actions usually exhibit inconsistent speed and duration, which further increases the temporal diversi-ties of event boundaries. As a result, the spatio-temporal diversities lead to overly complicated variations in videos, which impedes the accurate detection of event boundaries.
Since GEBD task is highly correlated with changes in temporal dimension, motion information is the key to per-ceiving temporal variations and detecting event boundaries.
Previous methods wildly use optical flow [24, 25, 45] as al-ternative motion representation to learn temporal clues in videos. However, they model the semantics in a single feature level and focus on local motion cues between two consecutive frames (Figure 1), which is insufficient to per-ceive diverse event boundaries. In addition, previous two-stream methods [36, 45] commonly resort to simple fusion schemes, short of interaction across appearance and motion modalities. Hence, they are less effective for learning com-plex semantics of diverse event boundaries.
To address the above issues, we present a method (DDM-Net) that progressively aggregates dense motion informa-tion along with appearance cues to perceive event bound-aries, as illustrated in Figure 2. We make three notable improvements, including Multi-Level Feature Bank, Dense
Difference Map and Progressive Attention. First, we build a Multi-Level Feature Bank where the features are collected in different spatial and temporal scales respectively, which empowers the subsequent modules to thoroughly perceive different levels of changes in videos.
Second, based on aforementioned feature bank, we pro-pose a Dense Difference Map (DDM) to model rich tem-poral contexts. Technically, we calculate pairwise feature differences between every two frames in a clip of length T , and obtain a T × T dense difference map. The main advan-tage of DDM is to exploit the difference of each feature pair and provide holistic motion information. As shown in Fig-ure 1, our proposed DDM is able to provide more holistic and salient temporal clues than optical flow, which is cal-culated between two consecutive frames. Furthermore, in-stead of directly being operated on raw frames, our DDM is built on the features collected from different layers of back-bone network, and thus ought to be more robust to temporal noise (e.g., camera blur in the second row of Figure 1).
Third, as event boundaries show their spatio-temporal di-versities and complexities, we argue that simple linear fu-sion in two-stream methods is insufficient to aggregate the appearance and motion clues. We thus exploit Progressive
Attention to mine important clues hidden in RGB features and DDM. In order to align the shape of DDM to RGB fea-tures, we design map-squeezed attention to squeeze DDM.
Then, in intra-modal attention, key features of two modal-ities are respectively enhanced through two sets of learn-able queries, prepared for cross-modal attention. Cross-modal attention is leveraged to perform feature interaction across modalities, enabling appearance and motion features to query and guide each other. As a result, DDM-Net can more effectively aggregate spatio-temporal clues and im-prove the discrimination of event boundaries.
Our DDM-Net exploits multi-level dense differences to perceive diverse temporal variations, and leverages progres-sive attention to effectively aggregate appearance and mo-tion clues. To prove the effectiveness of DDM-Net, we perform extensive experiments on two datasets: Kinetics-GEBD [35] and TAPOS [34]. Evaluation results demon-strate that our DDM-Net outperforms the existing state-of-the-art methods by a large margin on all evaluation metrics.
Particularly, DDM-Net obtains a superior 76.4% F1@0.05 on Kinetics-GEBD, with a significant boost of 14 percent.
On TAPOS, we improve F1 score@0.05 from 52.2% to 60.4%. In addition, our DDM-Net is superior to winners of LOVEU Challenge@CVPR 2021 [35] on the testing set of Kinetics-GEBD, demonstrating the effectiveness of our method. In summary, our main contributions are as follows:
• We propose dense difference maps equipped with multi-level feature bank to leverage richer temporal clues for detection of diverse event boundaries.
• Instead of simple feature fusion methods, progressive attention is employed to aggregate appearance and mo-tion clues from RGB features and DDM, enabling
DDM-Net to generate more discriminative representa-tions and learn more complicated semantics.
• Extensive experiments and studies demonstrate that our DDM-Net achieves the state-of-the-art perfor-mance on Kinetics-GEBD and TAPOS benchmark, un-der the setting of the same backbone. 2.