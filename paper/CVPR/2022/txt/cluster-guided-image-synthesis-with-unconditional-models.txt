Abstract
Generative Adversarial Networks (GANs) are the driv-ing force behind the state-of-the-art in image generation.
Despite their ability to synthesize high-resolution photo-realistic images, generating content with on-demand condi-tioning of different granularity remains a challenge. This challenge is usually tackled by annotating massive datasets with the attributes of interest, a laborious task that is not always a viable option. Therefore, it is vital to introduce control into the generation process of unsupervised gener-ative models. In this work, we focus on controllable image generation by leveraging GANs that are well-trained in an unsupervised fashion. To this end, we discover that the rep-resentation space of intermediate layers of the generator forms a number of clusters that separate the data accord-ing to semantically meaningful attributes (e.g., hair color and pose). By conditioning on the cluster assignments, the proposed method is able to control the semantic class of the generated image. Our approach enables sampling from each cluster by Implicit Maximum Likelihood Estimation (IMLE). We showcase the efﬁcacy of our approach on faces (CelebA-HQ and FFHQ), animals (Imagenet) and objects (LSUN) using different pre-trained generative models. The results highlight the ability of our approach to condition image generation on attributes like gender, pose and hair style on faces, as well as a variety of features on different object classes. 1.

Introduction
Generative Adversarial Nets (GANs) [8] have demon-strated photo-realistic generation quality by utilizing the rich corpus of available image datasets. Despite their success, the value they can add as data generation tools is currently limited by the lack of control in the synthesized content. In the typical GAN setting an image is synthesized by sampling a vector from a latent distribution and performing a forward pass through a generator network. However, random sam-pling from the latent distribution provides no control over semantic attributes in the image space. Such control over the generated characteristics is vital for tasks like autonomous driving [39] or (inverse) reinforcement learning [12].
A common solution to the problem of controllable gen-eration is to introduce supervision in the form of class la-bels [1, 5, 28]. This process requires the annotation of the training set, which can be a resource-intensive task, in addi-tion to being impractical for a continually-growing number of attributes of interest. Additionally, even with a rich and diverse annotated dataset, training a conditional generative model that can balance control and photo-realism is a non-trivial task that requires tailored engineering tricks (e.g., truncation trick [1]).
In this work, we introduce a method that can be imple-mented on top of any pretrained GAN to introduce control without the need for labels and supervision. The method relies on the clusters that are formed in the intermediate representation space of a generator. We posit that the rep-resentational capacity of the network allows for semantic attributes, like hair color and pose, to be disentangled in this representation space. Hence, each of the formed clusters cor-responds to a different semantic attribute. This assumption enables us to control image generation by conditioning on the cluster assignment. Latent sampling from these clusters is achieved via Implicit Maximum Likelihood Estimation (IMLE). The proposed framework is summarized in Figure 1. We benchmark the method against GANs that learn clus-tering in the latent space as well as methods for interpretable directions in pretrained GANs. The results highlight the efﬁcacy of our method in consistently generating images of desired attributes.
Figure 1. Conditional generation using an unsupervised generator. The training phase (depicted on the left) includes the following steps: (a) latent codes are sampled from the latent space of the generator, and then (b) passed through the ﬁrst n layers of the generator. The resulting representations are then clustered using k-means. Thus, we can assign each sampled latent code to a cluster (in the representation space). (c)
Sequentially, we can learn a mapping from an auxiliary distribution ec to the subspace of each cluster in the latent space of the generator. In the testing phase (depicted on the right), we can sample from the auxiliary distributions and use the corresponding mappings (T1 or T2) to synthesize images that have speciﬁc semantic attributes, e.g., male or female. 2.