Abstract 1.

Introduction
As 3D facial avatars become more widely used for com-munication, it is critical that they faithfully convey emo-tion. Unfortunately, the best recent methods that regress parametric 3D face models from monocular images are unable to capture the full spectrum of facial expression, such as subtle or extreme emotions. We find the stan-dard reconstruction metrics used for training (landmark re-projection error, photometric error, and face recognition loss) are insufficient to capture high-fidelity expressions.
The result is facial geometries that do not match the emo-tional content of the input image. We address this with
EMOCA (EMOtion Capture and Animation), by introduc-ing a novel deep perceptual emotion consistency loss dur-ing training, which helps ensure that the reconstructed 3D expression matches the expression depicted in the input im-age. While EMOCA achieves 3D reconstruction errors that are on par with the current best methods, it significantly out-performs them in terms of the quality of the reconstructed expression and the perceived emotional content. We also di-rectly regress levels of valence and arousal and classify ba-sic expressions from the estimated 3D face parameters. On the task of in-the-wild emotion recognition, our purely geo-metric approach is on par with the best image-based meth-ods, highlighting the value of 3D geometry in analyzing hu-man behavior. The model and code are publicly available at https://emoca.is.tue.mpg.de.
Teaching computers to see humans and understand their behavior is a long-standing goal of computer vision. To ac-complish this, computers need to understand how humans look, how they move, and what they feel. Faces and their emotional expressions provide an important source of infor-mation about a person’s internal emotional state. To sup-port automated analysis of emotional state, we capture a person’s face, including its 3D shape, pose, and facial ex-pression, given a single RGB image. To do so, we go be-yond prior work to extract 3D geometry that carries rich emotional content. We focus on parametric methods (i.e., animatable and model-based) due to their wide applicability for 3D avatar creation [36], image synthesis [32, 81], video editing [41, 84] and face recognition [9, 68].
The field of 3D face reconstruction has rapidly advanced over the last two decades; see Egger et al. [22] for a review.
Existing methods that estimate 3D face models struggle to capture facial expressions in detail and often produce 3D shapes that do not carry the emotional content of the input image. This has several causes. First, some 3D face models lack sufficient expressiveness to capture subtle or extreme expressions. Second, reconstruction metrics like landmark reprojection loss [8], photometric loss [10], face recogni-tion loss [30], or multi-image consistency losses [73, 80], are either not affected by facial expressions, or require per-fect image alignment to capture subtle cues. Subtle changes
in geometry, however, can result in large differences in the perceived emotion. We argue that, to recover 3D expres-sion accurately, we need a new reconstruction metric that measures differences in expressions between the 3D recon-struction and the input image.
To that end, we describe EMOCA (EMOtion Capture and Animation), a neural network that learns an animatable face model from in-the-wild images without 3D supervi-sion. The design of our method is inspired by advances in the field of facial emotion recognition, which has made tremendous progress to date on estimating affect (or emo-tion) from in-the-wild-images [50]. Specifically, we train a state-of-the-art emotion recognition model, and leverage this during training of EMOCA as supervision. EMOCA introduces a novel perceptual emotion consistency loss that encourages the similarity of emotional content between the input and rendered reconstruction.
While the new emotion consistency loss results in better reconstructed emotions, this alone is insufficient. Large im-age datasets used by previous 3D reconstruction methods, while containing a large number of subjects of diverse eth-nicities, lack emotional expressivity [14, 17, 44, 92]. Large datasets with facial expressions, valence, and arousal in-the-wild, on the other hand, while rich in emotions, do not provide multiple images per subject in diverse condi-tions [7,13,46–48,57,91] and smaller datasets in controlled settings are not suitable for deep learning [54–56, 60, 78].
Multiple images of the same person, however, are required to train current state-of-the-art 3D face reconstruction meth-ods [19, 27, 73]. To overcome this, EMOCA builds on top of DECA [27], a publicly available 3D face reconstruction framework that achieves state-of-the-art identity shape re-construction accuracy [29, 73]. Specifically, we augment
DECA’s architecture with an additional trainable predic-tion branch for facial expression, while keeping other parts fixed. This enables us to only train the expression part of
EMOCA on emotion-rich image data [57], which results in improved emotion reconstruction performance, while re-taining DECA’s identity face shape quality.
Once trained, EMOCA reconstructs a 3D face from a single image (Fig. 1), it significantly outperforms previ-ous state-of-the-art methods in terms of the reconstructed expression quality, it preserves the state-of-the-art identity shape reconstruction accuracy, and the reconstructed face can be readily animated. Further, the expression parame-ters regressed by EMOCA convey sufficient information for in-the-wild emotion recognition, with on-par performance with the best image-based methods [86].
In summary, our main contributions are: 1) The first ap-proach to reconstruct an animatable 3D face model from an in-the-wild image, that is capable of recovering facial expressions that convey the correct emotional state. 2) A novel perceptual emotion-consistency loss that rewards the accuracy of the reconstructed emotion. 3) The first 3D geometry-based framework for in-the-wild emotion recog-nition, with comparable performance to current state-of-the-art image-based methods. 4) The code and model are publicly available for research purposes at https:
//emoca.is.tue.mpg.de. 2.