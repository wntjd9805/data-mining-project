Abstract
Monocular depth estimation is vital for scene under-standing and downstream tasks. We focus on the super-vised setup, in which ground-truth depth is available only at training time. Based on knowledge about the high reg-ularity of real 3D scenes, we propose a method that learns to selectively leverage information from coplanar pixels to improve the predicted depth. In particular, we introduce a piecewise planarity prior which states that for each pixel, there is a seed pixel which shares the same planar 3D sur-face with the former. Motivated by this prior, we design a network with two heads. The ﬁrst head outputs pixel-level plane coefﬁcients, while the second one outputs a dense off-set vector ﬁeld that identiﬁes the positions of seed pixels.
The plane coefﬁcients of seed pixels are then used to predict depth at each position. The resulting prediction is adap-tively fused with the initial prediction from the ﬁrst head via a learned conﬁdence to account for potential devia-tions from precise local planarity. The entire architecture is trained end-to-end thanks to the differentiability of the pro-posed modules and it learns to predict regular depth maps, with sharp edges at occlusion boundaries. An extensive evaluation of our method shows that we set the new state of the art in supervised monocular depth estimation, surpass-ing prior methods on NYU Depth-v2 and on the Garg split of KITTI. Our method delivers depth maps that yield plau-sible 3D reconstructions of the input scenes. Code is avail-able at: https://github.com/SysCV/P3Depth 1.

Introduction
Depth estimation is a fundamental problem in computer vision. It consists in predicting the perpendicular coordinate of the 3D point depicted at each pixel. Applications range from robotics to autonomous cars. There is experimental evidence [84] that depth is the most vital vision-level cue for executing actions, together with semantic segmentation. In this work, we focus on monocular depth estimation, which involves the challenge of scale ambiguity, as the same input image can be generated by inﬁnitely many 3D scenes.
The current trend in solving this task involves fully con-volutional neural networks that output a dense depth predic-Figure 1. Real-world 3D scenes have a high degree of regularity.
We propose a method which can exploit this regularity, by im-plicitly learning intermediate representations that contain useful information about local planes in the scene. The proposed end-to-end model predicts high-quality depth maps with sharp edges at occlusion boundaries, which yield consistent 3D reconstructions. tion either with standard supervision on depth [9, 12, 32, 71] or with self-supervision by using the predicted depth to re-construct neighboring views of the scene [16, 18, 19, 85].
Most supervised approaches use a pixel-level loss which treats predictions at different pixels separately. This regime ignores the high degree of regularity of real-world 3D scenes, which generally yield piecewise smooth depth maps.
A common choice for modeling this prior knowledge of the geometry of real 3D scenes are planes [2, 6, 41, 42].
Planes are the local ﬁrst-order Taylor approximation for lo-cally differentiable depth maps and they are easy to param-eterize using three independent coefﬁcients. Once a pixel is associated with a plane, its depth can be recovered from the position of the pixel and the coefﬁcients of the associ-ated plane. In [83], such a plane coefﬁcient representation is used to learn to predict planes explicitly.
We adopt the plane representation from [83], but we de-part from the explicit prediction of planes and rather use this representation as an appropriate output space for deﬁn-ing interactions between pixels based on planarity priors.
In particular, the ﬁrst head of our network outputs dense 1
plane coefﬁcient maps which are afterwards converted to depth maps, as shown in Fig. 2. Predicting plane coefﬁ-cients is motivated by the fact that two pixels p and q that belong to the same plane ideally have equal plane coefﬁ-cient representations, whereas they generally have different depth. Thus, using the plane coefﬁcient representation of q for predicting depth at the position of p results in a correct prediction if the pixels belong to the same plane.
We leverage this property by learning to identify seed pixels which share the same plane as the examined pixel, whenever such pixels exist, in order to selectively use the plane coefﬁcients of these pixels for improving the pre-dicted depth. This approach is motivated by a piecewise planarity prior which states that for each pixel p with an associated 3D plane, there is a seed pixel q in the neigh-borhood of p which is associated with the same 3D plane as p. To predict depth with this scheme, we need to iden-tify (i) the regions where the prior is valid and (ii) the seed pixels in these regions, by predicting the offsets q − p. We thus design a second head in the network, which outputs a dense offset vector ﬁeld and a conﬁdence map, as shown in
Fig. 2. The predicted offsets are used to resample the plane coefﬁcients from the ﬁrst head and generate a second depth prediction. The depth predictions from the two heads are then fused adaptively using the conﬁdence map as fusion weights, in order to down-weigh the offset-based prediction and rely primarily on the basic depth prediction in regions where the piecewise planarity prior is not valid, e.g. on parts of the scene with high-frequency structures. Supervision on the offsets and conﬁdence map is applied implicitly, by su-pervising the fused depth prediction. Thanks to using seed pixels for prediction, our model implicitly learns to group pixels based on their membership in smooth regions of the depth map. This helps preserve sharp depth discontinuities, as shown in Fig. 1. Last but not least, we propose a mean plane loss which enforces ﬁrst-order consistency of our pre-dicted 3D surfaces with the ground truth and further im-proves performance.
We evaluate our method extensively on 6 datasets for supervised monocular depth estimation: NYU Depth-v2,
KITTI, ScanNet, SUN-RGBD, DIODE Indoor, and ETH-3D. Comparisons to competing approaches demonstrate that we set a new state of the art on NYU Depth-v2 and
KITTI, surpassing the former best-performing method in all commonly used evaluation metrics on NYU and on the Garg split [16] of KITTI. Moreover, in a challenging zero-shot transfer setup, we outperform the prior state of the art on
ScanNet, SUN-RGBD, DIODE Indoor, and ETH-3D. We conduct a thorough ablation study and show quantitatively the merit of our novel formulation for depth prediction. We also provide qualitative comparisons with the prior state of the art, which evidence the high quality of our predictions, in particular when the latter are used for 3D reconstruction. 2.