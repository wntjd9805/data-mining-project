Abstract
Channel pruning has been broadly recognized as an ef-fective technique to reduce the computation and memory cost of deep convolutional neural networks. However, con-ventional pruning methods have limitations in that: they are restricted to pruning process only, and they require a fully pre-trained large model. Such limitations may lead to sub-optimal model quality as well as excessive memory and training cost. In this paper, we propose a novel Channel
Exploration methodology, dubbed as CHEX, to rectify these problems. As opposed to pruning-only strategy, we propose to repeatedly prune and regrow the channels throughout the training process, which reduces the risk of pruning impor-tant channels prematurely. More exactly: From intra-layer’s aspect, we tackle the channel pruning problem via a well-known column subset selection (CSS) formulation. From inter-layer’s aspect, our regrowing stages open a path for dynamically re-allocating the number of channels across all the layers under a global channel sparsity constraint .
In addition, all the exploration process is done in a single training from scratch without the need of a pre-trained large model. Experimental results demonstrate that CHEX can effectively reduce the FLOPs of diverse CNN architectures on a variety of computer vision tasks, including image clas-siﬁcation, object detection, instance segmentation, and 3D vision. For example, our compressed ResNet-50 model on
ImageNet dataset achieves 76% top-1 accuracy with only 25% FLOPs of the original ResNet-50 model, outperforming previous state-of-the-art channel pruning methods. The checkpoints and code are available at here . 1.

Introduction
Albeit the empirical success of deep convolutional neu-ral networks (CNN) on many computer vision tasks, the excessive computational and memory cost impede their de-ployment on mobile or edge devices. Therefore, it is vital to
*Work done during an internship at DAMO Academy, Alibaba Group.
†Work partially done during an internship at Alibaba Group.
‡Work partially done during working period at Alibaba Group.
Figure 1. Comparison of the accuracy-FLOPs Pareto curve of the compressed ResNet-50 models on ImageNet. CHEX shows the top-performing Pareto frontier compared with previous methods.
And we obtain the sub-models without pre-training a large model. explore model compression, which reduces the redundancy in the model while maximally maintaining the accuracy.
Among various model compression approaches, chan-nel pruning has been recognized as an effective tool to achieve practical memory saving and inference accelera-tion on general-purpose hardware. To derive a sub-model, channel pruning removes the redundant channels along with all the associated ﬁlters connected to those channels.
Most existing channel pruning methods [3, 13, 24, 25, 27, 28, 39, 53, 54, 57, 70, 72, 75, 80, 82] adopt a progressive pruning-training pipeline: pre-training a large model until convergence, pruning a few unimportant channels by the pre-deﬁned criterion, and ﬁnetuning the pruned model to restore accuracy. The last two stages are usually executed in an interleaved manner repeatedly, which suffers from long training time. Various attempts have been made to improve the pruning efﬁciency. Training-based channel pruning meth-ods [14, 40, 41, 45, 48, 81] impose sparse regularization such as LASSO or group LASSO to the model parameters during training. However, these commonly adopted regularization may not penalize the parameters to exactly zero. Removing
Figure 2. An illustration of our CHEX method, which jointly optimizes the weight values and explores the sub-model structure in one training pass from scratch. In CHEX, both retained and regrown channels in the sub-model are active, participating in the training iterations. many small but non-zero parameters inevitably damages the model accuracy. Although this problem can be addressed by applying specialized optimizers [5, 32] or model reparame-terization tricks [7], these methods require a well-pretrained large model, which counters our goal of improving pruning efﬁciency. Sampling-based methods [11, 30, 34, 51, 58, 78] directly train sparse models. However, these methods may suffer from training instability and converge to sub-optimal points [17]. [36, 66, 67, 74] shorten or eliminate the pre-training phase, and extract the sub-model at an early stage or even from random initialization. These methods are in-capable to recover prematurely pruned important channels, which limits the model capacity and leads to unacceptable accuracy degradation.
To rectify the aforementioned limitations, we propose a channel exploration methodology called CHEX to ob-tain high accuracy sub-models without pre-training a large model or ﬁnetuning the pruned model. In contrast to the tra-ditional pruning approaches that permanently remove chan-nels [39, 53, 76, 82], we dynamically adjust the importance of the channels via a periodic pruning and regrowing pro-cess, which allows the prematurely pruned channels to be recovered and prevents the model from losing the represen-tation ability early in the training process. From intra-layer’s perspective, we re-formulate the channel pruning problem into the classic column subset selection (CSS) problem in linear algebra, leading to a closed-form solution. From inter-layer’s perspective, rather than sticking to a ﬁxed or manually designed sub-model structure, our approach re-evaluates the importance of different layers after each regrowing stage.
This leads to a sub-model structure exploration technique to dynamically re-allocate the number of channels across all the layers under a given budget. With only one training pass from scratch, our obtained sub-models yield better accuracy than the previous methods under the same FLOPs reductions.
Moreover, the simplicity of our method allows us to derive a theoretical analysis on the training behaviour of CHEX, to provide further insights and interpretation.
The contributions of this paper are highlighted as follows:
• We propose a channel exploration method CHEX with three novel features: (1) a periodic channel pruning and regrowing process, (2) a pruning criterion (i.e., leverage score) based on column subset selection, and (3) a sub-model structure exploration technique.
• Our method obtains the sub-model in one training pass from scratch, effectively reducing the training cost, because it circumvents the expensive pretrain-prune-ﬁnetune cycles.
• Experimentally, CHEX exhibits superior accuracy un-der different FLOPs constraints (as shown in Figure 1), and is applicable to a plethora of computer vision tasks.
For image classiﬁcation on ImageNet, our compressed
ResNet-50 model yields 4× FLOPs reduction while achieving 76% top-1 accuracy, improving previous best result [63] by 0.7%. For object detection on COCO dataset, our method achieves 2× FLOPs reduction on the SSD model while improving 0.7% mAP over the un-pruned baseline. For instance segmentation on COCO dataset and 3D point cloud segmentation on ShapeNet dataset, our method achieves 2× and 11× FLOPs re-ductions on the Mask R-CNN and PointNet++ models with negligible quality loss compared to the unpruned models, respectively.
• We provide a theoretical convergence guarantee of our
CHEX method from the view of non-convex optimiza-tion setting, which is applicable to deep learning. 2. Methodology
Our method takes an existing CNN model as our channel exploration space, which provides the maximum number of explored channels. As illustrated in Figure 2, we describe
the training ﬂow of CHEX using a 3-layer CNN model as an example, in which we set the target channel sparsity1 to 50%. From top to bottom, the three layers contain 6, 8, and 10 channels, respectively, denoted as Conv − 6 − 8 − 10.
Our method starts with a randomly initialized sub-model.
During training, at periodically spaced training iterations (e.g., pre-determined ∆T iterations), a channel pruning and regrowing process is performed, which is referred to as one step in CHEX. A total of N steps are applied, and each step is composed of the following stages:
• A pruning stage removes the unimportant channels to the target sparsity. The number of channels are re-allocated across all different layers via the sub-model structure exploration technique. For example, in Fig-ure 2, the pruned sub-model in step 1 has an architecture
Conv − 3 − 4 − 5, retaining 12 out of 24 channels. It may be adjusted later to Conv−2−3−7 in step 2. Such adaptations continue in the loop across the N steps.
• Immediately after pruning, a channel regrowing stage regrows back a fraction of the previously pruned chan-nels, whose weight values are restored to their most recently used values before being pruned. Note that the regrown channels may be pruned multiple steps before.
A decay scheduler is adopted to gradually reduce the number of regrown channels across the N steps. For ex-ample, in Figure 2, the channel regrowing stage in step 1 re-activates six channels, while it only re-activates four channels in step 2.
Our method interleaves the model training and the peri-odic pruning-regrowing stages. Since the total number of regrown channels is reduced at each step, the sub-model under training enjoys gradually decreased computation cost and converges to the target channel sparsity in the end. As an algorithm guideline, the pseudo-code of CHEX is provided in Algorithm 1. 2.1. Channel pruning stage
Suppose a CNN contains L layers with parameters
{w1, ..., wL}, with wl ∈ RH lW lCl−1×Cl denoting the re-shaped2 convolution weight matrix. C l−1, C l, H l × W l represent the number of input channels, the number of out-put channels, and the kernel size, respectively. The j-th channel in the l-th layer is denoted as wl
:,j. That is, a column in wl represents one channel in the convolution. For notation simplicity, we use K l = H lW lC l−1 in the following text, i.e., wl ∈ RKl×Cl
. 1We follow the notion of structured sparsity introduced in [68]. Our sub-model is a slimmer dense CNN model. 2For ease of derivation, we convert the convolution weight tensor of size
H l × W l × Cl−1 × Cl to the matrix of size H lW lCl−1 × Cl, where the channels are listed as columns in the reshaped weight matrix.
Algorithm 1: Overview of the CHEX method. 1 Input: An L-layer CNN model with weights
W = {w1, ..., wL}; target channel sparsity S; total training iterations Ttotal; initial regrowing factor δ0; training iterations between two consecutive steps ∆T ; total pruning-regrowing steps Tmax; training set D ; 2 Output: A sub-model satisfying the target sparsity S and its optimal weight values W∗; 3 Randomly initialize the model weights W; 4 for each training iteration t ∈ [Ttotal] do 5
Sample a mini-batch from D and update the model 6 7 8 9 10 weights W ; if Mod(t, ∆T ) = 0 and t ≤ Tmax then
Re-allocate the number of channels for each layer in the sub-model {κl, l ∈ [L]} by Eq.(4) ;
Prune {κlC l, l ∈ [L]} channels by CSS-based pruning in Algorithm 2 ;
Compute the channel regrowing factor by a decay scheduler function ;
Perform importance sampling-based channel regrowing in Algorithm 3 ;
Suppose κl denotes the channel sparsity of the l-th layer.
For each layer, channel pruning identiﬁes a set of important channels with index set T l (|T l| = (cid:100)(1 − κl)C l(cid:101)), which retains the most important information in wl, such that the
:,j, j /∈ T l} may be discarded with min-remaining ones {wl imal impact to model accuracy. In other words, channel pruning selects the most “representative” columns from wl that can reconstruct the original weight matrix with mini-mal error. From this perspective, channel pruning can be naturally represented as the Column Subset Selection (CSS) problem in linear algebra [12]. This provides us a new theo-retical guideline for designing channel pruning criterion in a principled way, rather than depending on heuristics. To rigorously characterize the most “representative" columns of a matrix, we formally deﬁne CSS as follows:
Deﬁnition 2.1 (Column Subset Selection). Given a matrix wl ∈ RK×Cl
, let c ≤ C l be the number of columns to select.
Find c columns of wl, denoted by wl c, that would minimize: (cid:107)wl − wl cwl c
† wl(cid:107)2
F or (cid:107)wl − wl cwl c
† wl(cid:107)2 2, (1) where † stands for the Moore-Penrose pseudo-inverse, (cid:107) · (cid:107)F and (cid:107)·(cid:107)2 represent matrix Frobenius norm and spectral norm, respectively.
Since channel pruning and CSS share the same goal of best recovering the full matrix by a subset of its columns, we can leverage the rich theoretical foundations of CSS to derive a new pruning criterion. Our channel pruning stage is conducted periodically during training, thus we employ a computationally efﬁcient deterministic CSS algorithm, re-ferred to as the Leverage Score Sampling [60]. The core
Algorithm 2: CSS-based channel pruning. 1 Input: Model weights wl; pruning ratios κl ; 2 Output: The pruned layer l ; 3 Compute the number of retained channels
˜C l = (cid:100)(1 − κl)C l(cid:101) ; 4 Compute the top ˜C l right singular vectors Vl 5 Compute the leverage scores for all the channels in layer l 2 for all j ∈ [C l] ;
˜Cl of wl ; j = (cid:107)[Vl
ψl
˜Cl ]j,:(cid:107)2 6 Retain the important channels identiﬁed as
T l = ArgTopK({ψl j}; ˜C l) ; 7 Prune channels {wl
:,j, j /∈ T l} from layer l ;
Algorithm 3: Sampling-based channel regrowing. 1 Input: Indices of active channels T l in the sub-model; regrowing factor δt; 2 Output: The regrown layer l ; 3 Compute the importance sampling probabilities by Eq.(2) j = exp((cid:15)l pl j)/ (cid:80) j(cid:48) exp((cid:15)l j(cid:48) ) for all j ∈ [C l]\T l ; 4 Compute the number of regrown channels kl = (cid:100)δtC l(cid:101) ; 5 Perform importance sampling Gl=Multinomial({pl j}; kl) ; 6 Restore the MRU weights of the chosen channels
{ ˆwl j, j ∈ Gl} ; 7 Regrow channels { ˆwl j, j ∈ Gl} to layer l ; of this algorithm involves the leverage scores of matrix wl, which are deﬁned as follows:
Deﬁnition 2.2 (Leverage Scores). Let Vc ∈ RCl×c be the top-c right singular vectors of wl (c represents the number of selected columns from wl). Then, the leverage score of the j-th column of wl is given as: ψj = (cid:107)[Vc]j,:(cid:107)2 2, where
[Vc]j,: denotes the jth row of Vc.
The leverage score sampling algorithm samples c columns of wl that corresponds to the largest c leverage scores of wl. Despite its simplicity, theoretical analysis in [60] has shown that this deterministic solution provably ob-tains near optimal low-rank approximation error for Eq.(1).
Based on the above analysis, we propose a CSS-based channel pruning method with leverage score sampling, as shown in Algorithm 2. When given a pruning ratio κl for layer l, we need to select and retain (cid:100)(1 − κl)C l(cid:101) impor-tant channels. We ﬁrst compute the top (cid:100)(1 − κl)C l(cid:101) right singular vectors of the weight matrix wl. Then, we cal-culate the leverage scores of all the channels in this layer as Deﬁnition 2.2, and rank them in descending order. Fi-nally, we identify the set of important channels to retain as j}; (cid:100)(1 − κl)C l(cid:101)), which gives the in-T l = ArgTopK({ψl dices of channels with the top (cid:100)(1 − κl)C l(cid:101) leverage scores of wl. The remaining bottom-ranking channels are pruned. 2.2. Channel regrowing stage
Since the method trains from a randomly initialized model and the pruning stage may be based on the weights that are not sufﬁciently trained. In the early stage of training, the pruning decisions may not be optimal and some important channels are prematurely pruned. Therefore, after each prun-ing stage, our method regrows a fraction of the previously pruned channels back to the model. The regrown channels are updated in the subsequent training. If they are important to the model, they may survive the future pruning stages after a number of iterations of training. Moreover, the channel regrowing stage enables the model to have better representa-tion ability during training, since the model capacity is not permanently restricted as the one-shot pruning methods.
To complete the regrowing stage, we need to assign proper weight values to the newly activated channels. One straightforward choice is to assign zero values for stable training, since the regrown channels do not affect the output of the model. However, we ﬁnd that regrowing channels with zeros would receive zero gradients in the subsequent training iterations. This is undesirable because the regrown channels would remain deactivated and the method degenerates to the one-shot early-bird pruning [74]. Based on our ablations, we ﬁnd the best scheme is that the newly activated channels restore their most recently used (MRU) parameters, which are the last values before they are pruned. We constantly maintain a copy of the weight values of the pruned chan-nels, in case that they may get regrown back in the future regrowing stages. Note that the regrowing stage may re-grow channels that are pruned multiple steps before, instead of just re-activating what are pruned at the pruning stage immediately before the current regrowing stage.
To determine the channels to regrow, a naive way is to perform uniform sampling from the candidate set {j|j ∈
[C l] \ T l}. However, uniform sampling does not consider the possible inter-channel dependency between the active channels survived in the sub-model and the candidate chan-nels to regrow. Instead, we propose an importance sampling strategy based on channel orthogonality for regrowing, as shown in Algorithm 3. Channel orthogonality automatically implies linear independency, which helps avoid trivial re-growing where the newly regrown channels lie in the span of the active channels. Channel orthogonality also encour-ages the channel diversity and improves model accuracy [1].
Formally, we denote the active channels in layer l by matrix wl j in the candidate set with respect to the active channels can be computed by the classic orthogonal projection formula [26]:
T l. The orthogonality (cid:15)l j of a channel wl j = (cid:107)wl (cid:15)l j − wl
T l (wlT
T l wl
T l )†wlT
T l wl j(cid:107)2 2. (2)
A higher orthogonality value indicates that the channel is harder to approximate by others, and may have a better chance to be retained in the CSS pruning stage of the future steps. Thus, the corresponding channel may be sampled with a relatively higher probability. We use the orthogonality values to design our importance sampling distribution, and
the probability pl j to regrow a channel wl j is given as: j = exp((cid:15)l pl j)/ (cid:88) j(cid:48) exp((cid:15)l j(cid:48)). (3)
Then, the channels to regrow are sampled according to the distribution Multinomial({pl j|j ∈ [C l] \ T l}; (cid:100)δtC l(cid:101)) with-out replacement, where δt is the regrowing factor introduced as follows.
In the regrowing stage, we employ a cosine decay sched-uler to gradually reduce the number of regrown channels so that the sub-model converges to the target channel sparsity at the end of training. Speciﬁcally, the regrowing factor at t-th 1 2 where δ0 is the initial regrowing factor, Tmax denotes the total exploration steps, and ∆T represents the frequency to invoke the pruning-regrowing steps. step is computed as: δt = (cid:18) t · π
Tmax/∆T 1 + cos (cid:19)(cid:19)
δ0, (cid:18) 2.3. Sub-model structure exploration
The starting model architecture may not have balanced layer distributions. Some layers are more important to the model accuracy and more channels need to be preserved, while some other layers may contain excessive number of channels. To better preserve model accuracy, our method dynamically re-distributes the surviving channels across dif-ferent layers in each pruning stage. Such re-distribution is called sub-model structure exploration.
Inspired by [48], we use the learnable scaling factors in batch normalization (BN) 3 [33] to reﬂect the layer impor-tance. Denote the BN scaling factors of all channels across all layers by Γ = {γ1, ..., γL}, γl ∈ RCl
, and the overall target channel sparsity by S. We calculate the layer-wise pruning ratios by ranking all scaling factors in descending order and preserving the top 1 − S percent of the channels.
Then, the sparsity κl for layer l is given as:
κl = (cid:0)(cid:88) j∈[Cl] 1
{γl j ≤q(Γ,S)} (cid:1)/C l, l ∈ [L], (4)
{γl j ≤q(Γ,S)} is 0 if γl where 1 j ≤ q(Γ, S). q(Γ, S) represents the S-th percentile of all the scaling factors Γ. Accordingly, the number of channels in each layer of the sub-model is obtained as (cid:100)(1 − κl)C l(cid:101). j > q(Γ, S) and 1 if γl
[48] relies on LASSO regularization to identify the in-signiﬁcant channels, it extracts the sub-model from a fully pre-trained model in one-shot manner, and the subsequent
ﬁnetuning procedure ﬁxes the architecture without adapta-tion. In contrast, our method proposes a CSS-based pruning criterion without requiring any sparse regularization. And we advocate a repeated pruning and regrowing paradigm as opposed to the pruning-only strategy. We use BN scaling factors only for re-allocating the number of channels. We 3BN applies afﬁne transformations to standardized input feature-maps:
Xout = γ ˜Xin + β, where γ / β are learnable scaling / shifting factors. perform such re-allocation repeatedly at each step to take into account the changing layer importance during training.
Thanks to our regrowing stages which help maintain the exploration space, our method can dynamically re-distribute channels from the less crucial layers to the more important ones, leading to a better sub-model structure. 3. Theoretical Justiﬁcation
We now provide the convergence guarantee for the CHEX method. Let F (W) = Ex∼D[f (W; x)] be the loss function of the deep learning task where x is the data following a distribution D and E[·] is the expectation. In addition, let
Wt ∈ Rd be the model parameter at the t-th training itera-tion, and mt ∈ {0, 1}d be a binary channel mask vector for t = 1, · · · , T . Apparently, quantity Wt (cid:12) mt ∈ Rd is a sub-model pruned from Wt where (cid:12) denotes the element-wise product. The following proposition shows that the CHEX will converge to a neighborhood around the stationary so-lution at rate O(1/
T ) when learning rate is set properly.
Due to the space limitation, we put its proof in the Appendix.
√
Proposition 1 (Convergence Guarantee). Suppose the loss function F (W) is L-smooth, the sampled stochastic gradi-ent is unbiased and has bounded variance, and the relative error introduced by each mask is bounded, i.e., (cid:107)W − W (cid:12) mt(cid:107)2 ≤ δ2(cid:107)W(cid:107)2 and (cid:107)∇F (W) − ∇F (W) (cid:12) mt(cid:107)2 ≤
ζ 2(cid:107)∇F (W)(cid:107)2 for constants δ ∈ [0, 1] and ζ ∈ [0, 1]. If in which C0 = E[F (W0)], learning rate η = the sub-models obtained by CHEX will converge as follows: 2C0
L(T +1)
√
√
σ
T (cid:88)
E[(cid:107)∇F (Wt (cid:12) mt)(cid:107)2] 1
T + 1 t=0
√
≤ 4σ (1−ζ)2
LC0
√
T + 1
+ 2L2δ2 (T + 1)(1 − ζ)2
T (cid:88) t=0
E[(cid:107)Wt(cid:107)2]. (5)
Remark. If there is no pruning (i.e., mt = 1 ∈ Rd) in the training process, it holds that δ = 0 and ζ = 0. Substituting it into (5), we ﬁnd that the CHEX method can converge exactly to the stationary solution, i.e., E[(cid:107)∇F (WT )(cid:107)2] → 0 as T increases to inﬁnity. When a sparse channel mask is utilized, it holds that δ (cid:54)= 0 and ζ (cid:54)= 0. In this scenario, the mask-induced error will inevitably inﬂuence the accuracy of the trained sub-model, i.e., constants δ and ζ will inﬂuence the magnitude of the second term in the upper bound (5). 4. Experiments
To evaluate the efﬁcacy and generality of CHEX, we ex-periment on a variety of computer vision tasks with diverse
CNN architectures. All experiments run on PyTorch frame-work based on DeepLearningExamples [59] with NVIDIA
Method
PT FLOPs Top-1 Epochs Method
PT FLOPs Top-1 Epochs
ResNet-18
PFP [42]
Y 1.27G 67.4% 270
SCOP [65] Y 1.10G 69.2% 230
SFP [22]
Y 1.04G 67.1% 200
FPGM [24] Y 1.04G 68.4% 200
DMCP [15] N 1.04G 69.0% 150
N 1.03G 69.6% 250
CHEX
ResNet-50
GBN [75]
LeGR [3]
SSS [32]
TAS [8]
GAL [45]
Hrank [43]
Y 2.4G 76.2% 350
Y 2.4G 75.7% 150
N 2.3G 71.8% 100
N 2.3G 76.2% 240
Y 2.3G 72.0% 150
Y 2.3G 75.0% 570
--Y 2.2G 74.5%
Y 2.2G 74.9%
Y 2.2G 76.0% 230
N 2.0G 74.7% 120
N 2.0G 76.9% 300
N 2.0G 77.4% 250
Taylor [57]
C-SGD [5]
SCOP [65]
DSA [58]
CafeNet [63]
CHEX-1
N 1.9G 75.3% 200
Y 1.9G 74.7%
SCP [34]
Hinge [41]
AdaptDCP [82] Y 1.9G 75.2% 210
Y 1.6G 74.5% 235
LFPC [21]
-Y 1.5G 75.3% 270
ResRep [7]
Y 1.2G 74.2% 248
Polarize [81]
Y 1.2G 74.6% 150
DSNet [38]
Y 1.1G 73.4% 190
CURL [52]
DMCP [15]
N 1.1G 74.1% 150
MetaPrune [49] N 1.0G 73.4% 160
EagleEye [37] Y 1.0G 74.2% 240
N 1.0G 75.3% 300
CafeNet [63]
N 1.0G 76.0% 250
CHEX-2
Method
FLOPs reduction
SSD object detection
AP AP50 AP75 APS APM APL
Baseline [47]
DMCP [15]
CHEX-1
CHEX-2 0% 25.2 42.7 25.8 7.3 27.1 40.8 50% 24.1 41.2 24.7 6.7 25.6 39.2 50% 25.9 43.0 26.8 7.8 27.8 41.7 75% 24.3 41.0 24.9 7.1 25.6 40.1
Mask R-CNN object detection
Baseline [18]
CHEX 0% 37.3 59.0 40.2 21.9 40.9 48.1 50% 37.3 58.5 40.4 21.7 39.0 49.5
Mask R-CNN instance segmentation
Baseline [18]
CHEX 0% 34.2 55.9 36.2 15.8 36.9 50.1 50% 34.5 55.7 36.7 15.9 36.1 51.2 (b)
Method
#Points
FLOPs reduction
Quality 3D shape classiﬁcation (Quality is accuracy)
Baseline [61]
CHEX 1k 1k 0% 87% 92.8% 92.9% 3D part segmentation (Quality is class/instance mIoU) 82.5%/85.4% 77.1%/84.0% 82.3%/85.2%
Baseline [61]
ADMM [62]
CHEX 0% 90% 91% 2k 2k 2k
ResNet-34
Taylor [57] Y 2.8G 72.8%
Y 2.2G 71.8% 200
SFP [22]
FPGM [24] Y 2.2G 72.5% 200
Y 2.1G 72.9% 240
GFS [72]
Y 2.1G 72.6% 490
DMC [11]
-NPPM [10] Y 2.1G 73.0% 390
SCOP [65] Y 2.0G 72.6% 230
CafeNet [63] N 1.8G 73.1% 300
N 2.0G 73.5% 250
CHEX
ResNet-101
Y 4.4G 77.5% 200
SFP [22]
FPGM [24] Y 4.4G 77.3% 200
Y 4.2G 76.4% 270
PFP [42]
AOFP [6]
Y 3.8G 76.4%
NPPM [10] Y 3.5G 77.8% 390
Y 3.3G 77.4% 490
DMC [11]
N 3.4G 78.8% 250
CHEX-1
N 1.9G 77.6% 250
CHEX-2
-(a) (c)
Table 1. (a): Results of ResNet on ImageNet dataset. “PT”: require pre-training. “Y”: Yes, “N”: No. (b): Results of SSD on COCO2017 and
Mask R-CNN on COCO2014. For objection detection, we evaluate the bounding box AP. For instance segmentation, we evaluate the mask
AP. (c): Results of PointNet++ for 3D point clouds classiﬁcation on ModelNet40 dataset and segmentation on ShapeNet dataset.
Tesla V100 GPUs. We set δ0 = 0.3 and ∆T = 2 epoch, where δ0 means the initial regrowing factor, and ∆T is the number of training iterations between two consecutive pruning-regrowing steps. To keep our method simple and generic, the above hyper-parameters are kept constant for our experiments. We set the rest of the hyper-parameters in the default settings and specify them in the Appendix. In this paper, FLOPs is calculated by counting multiplication and addition as one operation by following [20]. 4.1. Image recognition
For image recognition, we apply CHEX to ResNet [20] with different depths on ImageNet [4]. The baseline ResNet-18/34/50/101 models have 1.8/3.7/4.1/7.6 GFLOPs with 70.3%/73.9%/77.8%/78.9% top-1 accuracy, respectively.
As shown in Table 1(a), CHEX achieves noticeably higher accuracy than the state-of-the-art channel pruning meth-ods under the same FLOPs. For example, compared with
MetaPruning [49], DCMP [15] and CafeNet [63], our pruned
ResNet-50 model with 4× FLOPs reduction achieves 2.6%, 1.6%, and 0.7% higher top-1 accuracy, respectively. On the other hand, at the same target accuracy, CHEX achieves higher FLOPs reduction. For example, CHEX achieves 4×
FLOPs reduction on ResNet-101 model with 77.6% top-1 accuracy, compared to the latest work NPPM [10] which yields 2.2× FLOPs reduction.
The results in Table 1(a) also show an interesting property of our CHEX method. When we search a sub-model with a small FLOPs target, it is better to start our method on a larger model than on a smaller one. For instance, pruning a ResNet-50 model to 1 GFLOPs yields an accuracy 6.6% higher than pruning a ResNet-18 model to 1 GFLOPs. This indicates that CHEX performs training-time structural ex-ploration more effectively when given a larger parametric space. To illustrate this point further, we conduct an addi-tional experiment by applying CHEX to a model with twice the number of channels as the ResNet-50 model, with the goal of reducing its FLOPs to the same level as the original
ResNet-50 model. Notably, this sub-model achieves 78.9% top-1 accuracy at 4.1 GFLOPs. This suggests that CHEX has the potential to optimize existing CNNs. 4.2. Object detection
For object detection, we apply CHEX to the SSD model [47] on COCO2017 dataset [46]. Table 1(b) sum-marizes the performance with different FLOPs reductions.
Our pruned model outperforms the baseline model by 0.7%
AP (25.9 vs. 25.2) while achieving 2× FLOPs reduction.
Method
Baseline
+ Periodic pruning and regrowing
+ Dynamic sub-model structure exploration
+ Importance sampling-based regrowing
Top-1 73.9% 74.8% 75.6% 76.0%
Table 2. Ablation study of different components in CHEX.
Compared with previous SOTA channel pruning method
[15], our method achieves 1.8% higher AP (25.9 vs. 24.1) with 2× FLOPs reduction. Moreover, our method achieves 4× FLOPs reduction with less than 1% AP loss. 4.3. Instance segmentation
For instance segmentation, we apply CHEX to the Mask
R-CNN model [18] on COCO2014 dataset. Since Mask
R-CNN is a multi-task framework, we evaluate both the bounding box AP for object detection and the mask AP for instance segmentation. As shown in Table 1(b), the models pruned using our method achieve 2× FLOPs reduction with-out AP loss, even for the challenging instance segmentation task where the model needs to detect all objects correctly in an image while precisely segmenting each instance. 4.4. 3D point cloud
Apart from 2D computer vision problems, we also apply
CHEX to compress PointNet++ [61] for 3D shape classiﬁca-tion on ModelNet40 [69] dataset and 3D part segmentation on ShapeNet [73] dataset. The results are shown in Table 1(c). On shape classiﬁcation, the model pruned using our method achieves around 7.5× FLOPs reduction while im-proving the accuracy slightly compared to the unpruned baseline. On part segmentation, the model pruned using our method achieves 11× FLOPs reduction while maintaining similar mIoU compared with the unpruned baseline. 5. Ablation Analysis
We investigate the effectiveness of different components in the CHEX method through ablation studies. All the fol-lowing results are based on pruning the ResNet-50 model to 1 GFLOPs (4× reduction) on ImageNet dataset.
Ablation study. In Table 2, we study the effectiveness of different components in CHEX, namely the periodic chan-nel pruning and regrowing process, the sub-model structure exploration technique, and the importance sampling-based regrowing. The baseline is one-shot early-stage pruning [74], where the model is pruned by CSS very early after 6% of the total training epochs, and there is no regrowing stage.
The sub-model architecture is based on the BN scaling fac-tors and kept ﬁxed in the subsequent training. The peri-odic pruning and regrowing process repeatedly samples the important channels and prevents the channels from being pruned prematurely. This brings in 0.9% accuracy improve-ment. When the number of channels in each layer is also
Prune criterion
BN [48] Magnitude [39]
CSS (Ours)
Pretrain-prune-ﬁnetune 73.1%
CHEX 75.3% 73.8% 75.7% 74.6% 76.0%
Table 3. Inﬂuence of the pruning criterion to the top-1 accuracy in pretrain-prune-ﬁnetune framework and our CHEX method.
Initialization of regrown channels
Design choices
Top-1
Zero 74.1%
Random 74.5%
EMA 75.5%
MRU 76.0%
Regrowing factor
Design choices δ0 = 0.1 δ0 = 0.2 δ0 = 0.3 δ0 = 0.4
Top-1 75.2% 75.6% 76.0% 75.9%
Full 76.0%
Scheduler for channel regrowing
Design choices Constant 75.3%
Top-1
Linear decay 75.6%
Cosine decay 76.0
Pruning-regrowing frequency
Design choices ∆T = 1 ∆T = 2 ∆T = 5 ∆T = 10 ∆T = 20
Top-1 75.2% 76.0% 75.8% 75.3% 74.9%
Table 4. Compare different design choices in the regrowing stages of the CHEX method. dynamically adjusted instead of sticking to the ﬁxed struc-ture determined very early in training, we can obtain a more optimal sub-model structure, which further improves the accuracy by 0.8%. Finally, using the importance sampling strategy described in Eq.(3) instead of uniform sampling in the regrowing stages improves the top-1 accuracy to 76%.
Inﬂuence of pruning criterion. We study the inﬂuence of pruning criterion to the ﬁnal accuracy by comparing CSS versus ﬁlter magnitude [39] and BN [48] pruning in Table 3. As suggested by [28, 39, 53, 57], pruning criterion is an important factor in the traditional pretrain-prune-ﬁnetune (PPF) approach. Indeed, when the PPF approach is used, our proposed CSS shows the best accuracy, outperforming magnitude and BN pruning by 0.8% and 1.5%, respectively.
On the other hand, we observe that CHEX is more robust to the pruning criterion, as it improves the accuracy for all three criteria and the accuracy gap among them becomes smaller: CSS outperforms magnitude and BN pruning by 0.3% and 0.7%, respectively. We suspect that this is because
CHEX can dynamically adjust the channel importance and can recover from sub-optimal pruning decisions.
Design choices for the regrowing stage. In Table 4, we investigate the impact of different schemes in the channel regrowing stage of CHEX: (1) Different initialization schemes for the regrown chan-nels affect model quality critically. We have experimented several methods by initializing the regrown channels with random normal distribution [19], zero weights, the exponen-tial moving average (EMA) weights, and most recently used (MRU) weights. The results show that MRU yields the best top-1 accuracy, outperforming the other three initialization schemes by 0.5% ∼ 1.9%. (2) The initial regrowing factor δ0 also affects the model quality. Intuitively, a large regrowing factor can maintain
relatively larger model capacity in the early stage of training, and involve more channels into the exploration steps. As shown, the accuracy is improved by 0.8% as the δ0 increases from 0.1 to 0.3, at the price of more training cost. However, we did not observe further improvement when the regrow-ing factor increases from δ0 = 0.3 to full model size, in which case one step in CHEX consists of pruning to target channel sparsity and regrowing all pruned channels. This suggests that regrowing a fraction of the previously pruned channels may be enough for a comprehensive exploration of the channels if we also aim more training cost savings. (3) We decay the number of regrown channels in each regrowing stage to gradually restrict the scope of exploration.
We compare several decay schedulers, including constant, linear, and cosine. The cosine decay scheduler performs bet-ter than the other two schemes by 0.7% and 0.4% accuracy, respectively. With a decay scheduler, the sub-model under training will enjoy gradually decreased computation cost. (4) The training iterations between two consecutive pruning-regrowing steps, ∆T , also affects the model quality.
A smaller ∆T incurs more frequent pruning-regrowing steps in CHEX, leading to better accuracy in general. We use
∆T = 2 epoch, as it gives the best accuracy. 6.