Abstract
Driving 3D characters to dance following a piece of mu-sic is highly challenging due to the spatial constraints ap-plied to poses by choreography norms.
In addition, the generated dance sequence also needs to maintain tempo-ral coherency with different music genres. To tackle these challenges, we propose a novel music-to-dance framework,
Bailando, with two powerful components: 1) a choreo-graphic memory that learns to summarize meaningful danc-ing units from 3D pose sequence to a quantized code-book, 2) an actor-critic Generative Pre-trained Transformer (GPT) that composes these units to a ﬂuent dance coher-ent to the music. With the learned choreographic mem-ory, dance generation is realized on the quantized units that meet high choreography standards, such that the gen-erated dancing sequences are conﬁned within the spatial constraints. To achieve synchronized alignment between diverse motion tempos and music beats, we introduce an actor-critic-based reinforcement learning scheme to the
GPT with a newly-designed beat-align reward function.
Extensive experiments on the standard benchmark demon-strate that our proposed framework achieves state-of-the-art performance both qualitatively and quantitatively. No-tably, the learned choreographic memory is shown to dis-cover human-interpretable dancing-style poses in an unsu-pervised manner. Code and video demo are available at https://github.com/lisiyao21/Bailando/. (cid:66) Corresponding author 1.

Introduction
Music-conditioned 3D dance generation is an important task for its huge potential to facilitate a variety of real-world applications, e.g., assisting human artists choreograph and driving virtual characters performance. However, to produce satisfactory dancing sequence on given music is still very difﬁcult due to two main challenges: 1) Spatial constraint:
Not all the physically feasible 3D human poses are appli-cable for dance. The subspace of dancing-style poses has stricter positional standards on body, and is selective to be visually expressive and emotionally infectious based on the choreography norms. 2) Temporal coherency with music:
The generated dancing sequence should be consistent with the music rhythm on various genres of beats, while keeping the whole movements ﬂuent.
Most existing dance generation studies intend to solve the two challenges both in a single ingeniously designed network that directly maps music to 3D joint sequence in high-dimensional continuous space [3, 19, 37, 11, 2, 30].
However, such methods are usually unstable in practice and are prone to regress to nonstandard poses beyond the dancing subspace, e.g., freezing or meaningless swaying. Because there is no explicit constraints on target domain to restrict the synthesized dance to be spatially qualiﬁed. To deal with the spatial constraint, some works collect real dancing clips as dance unit and choreograph by splicing these units
[43, 18]. While these methods guarantee the spatial quality of generated dance by directly manipulating on real data, the collection of dance units costs tremendous manual efforts, and they are not compatible with different rhythms. In addi-Figure 2: Dance generation pipeline of Bailando. Given a piece of music, an actor-critic motion GPT autoregressively predicts the future upper-lower pose code pairs according to the music features and starting pose codes. The pose code sequence is then embedded to quantized features via a learned choreographic memory and ﬁnally decoded into a dance sequence by a CNN-based decoder. tion, the units cannot be reused for different kinds of music beats due to their ﬁxed length and speed.
In view of the shortcomings of existing methods, we pro-pose a novel dance generation framework, Bailando, that possesses two main components aiming at the spatial and temporal challenges, respectively. First, to address the spa-tial challenge, a ﬁnite dictionary of quantized dancing units, namely choreographic memory, is made by summarizing fundamental and reusable constituents from movements in the dancing-style subspace. Instead of manually indicating the dance units, we leverage the recent advances of VQ-VAE [38] to encode and quantize 3D joint sequence to a codebook in an unsupervised manner, where each learned code is shown to represent a unique dancing pose. To further enlarge the range that choreography memory can represent, we divide 3D poses into compositional upper and lower half bodies and learn VQ-VAEs for the half bodies separately, such that any piece of dance can be represented into a se-quence of paired pose codes.
Second, to generate temporally harmonic dance sequence, a GPT-like [34] network, named motion GPT, is introduced to translate music and source pose codes to targeted future pose codes. Since the 3D poses are divided into composi-tional half bodies in the choreographic memory, we enhance our motion GPT with proposed cross-conditional causal at-tention layer to keep the coherence of the generated body.
Moreover, to achieve accurate temporal synchronization be-tween diverse motion tempos and music beats, we apply an on-policy reinforcement learning scheme to further im-prove the motion GPT via actor-critic [22] ﬁnetuning with a newly-designed beat-align reward function.
The inference procedure of Bailando is shown in Figure 2.
Given a piece of music and a starting pose code pair, the actor-critic GPT autoregressively predicts the future pose code sequence, which are then embedded to correspond-ing quantized features in choreographic memory, and are
ﬁnally decoded and composed to 3D dance sequence by the dedicated CNN-based decoders of learned pose VQ-VAE.
The contributions of our work can be summarized in three folds: 1) A choreographic memory is created to encode and quantize dancing-style 3D poses, which is achieved by
VQ-VAE in an unsupervised manner. 2) To align diverse motion tempos with different genres of music beats, an actor-critic GPT incorporated with the choreographic memory and cross-conditional causal attention is introduced. 3) Extensive experiments show that our proposed Bailando signiﬁcantly outperforms the existing state of the art on both automatic metrics and visualization judgements. Code and models will be released upon acceptance. 2.