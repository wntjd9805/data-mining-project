Abstract 
In  this  paper,  we  focus  on  the  Audio-Visual  Question 
Answering  (AVQA)  task,  which  aims  to  answer  questions  regarding  different  visual  objects,  sounds,  and  their  as-  sociations  in  videos.  The  problem  requires  comprehen-  sive  multimodal  understanding  and  spatio-temporal  rea-  soning  over  audio-visual  scenes.  To  benchmark  this  task  and  facilitate  our  study,  we  introduce  a  large-scale  MUSIC- 
AVQA  dataset,  which  contains  more  than  45K  question-  answer  pairs  covering  33  different  question  templates  span-  ning  over  different  modalities  and  question  types.  We  de-  velop  several  baselines  and  introduce  a  spatio-temporal  grounded  audio-visual  network  for  the  AVQA  problem.  Our  results  demonstrate  that  AVQA  benefits  from  multisensory  perception  and  our  model  outperforms  recent  A-,  V-,  and 
AVQA  approaches.  We  believe  that  our  built  dataset  has  the  potential  to  serve  as  testbed  for  evaluating  and  pro-  moting  progress  in  audio-visual  scene  understanding  and  spatio-temporal  reasoning.  Code  and  dataset:  http://gewu-  lab.github.io/MUSIC-AVQA/  1.

Introduction 
We  are  surrounded  by  audio  and  visual  messages  in  daily  life,  and  both  modalities  jointly  improve  our  ability  in  scene  perception  and  understanding  [19].  For  instance,  imagine  that  we  are  in  a  concert,  watching  the  performance  and  lis-  tening  to  the  music  at  the  same  time  contribute  to  better  enjoyment  of  the  show.  Inspired  by  this,  how  to  make  ma-  chines  integrate  multimodal  information,  especially  the  nat-  ural  modality  such  as  the  audio  and  visual  ones,  to  achieve  considerable  scene  perception  and  understanding  ability  as  humans  is  an  interesting  and  valuable  topic. 
In  recent  years,  we  have  seen  significant  progress  in  sounding  object  perception  [6, 22, 37, 52],  audio  scene  anal-  ysis  [7,  10,  13,  20,  21,  51,  59],  audio-visual  scene  pars-  ing  [42,  47],  and  content  description  [24,  40,  50]  towards  audio-visual  scene  understanding.  Although  these  methods
†Equal  contribution. 
*Corresponding  author.
Figure  1.  Audio-visual  question  answering  requires  auditory  and  visual  modalities  for  multimodal  scene  understanding  and  spatio-  temporal  reasoning.  For  example,  when  we  encounter  a  complex  musical  performance  scene  involving  multiple  sounding  and  non-  sounding  instruments  above,  it  is  difficult  to  analyze  the  sound  first  term  in  the  question  by  VQA  model  that  only  considers  visual  modality.  While  if  we  only  consider  the  AQA  model  with  mono  sound,  the  left  or  right  position  is  also  hard  to  be  recognized.  How-  ever,  we  can  see  that  using  both  auditory  and  visual  modalities  can  answer  this  question  effortlessly.  associate  objects  or  sound  events  across  audio  and  visual  views,  most  of  them  remain  limited  ability  for  cross-modal  reasoning,  under  complex  audio-visual  scenarios.  In  con-  trast,  humans  are  capable  of  performing  multi-step  spatial  and  temporal  reasoning  over  multimodal  contexts  to  solve  complex  tasks,  such  as  answering  an  audio-visual  question,  but  it  is  quite  challenging  for  machines.  Existing  methods  such  as  Visual  Question  Answering  (VQA)  [3]  and  Audio 
Question  Answering  (AQA)  [9]  only  focus  on  single  modal-  ity,  which  cannot  reason  well  in  a  more  natural  scenario  with  both  audio  and  visual  modalities.  For  instance,  as  shown  in  Fig.  1,  when  answering  the  audio-visual  question 
“ Which  clarinet  makes  the  sound  first "  for  this  instrumental  ensemble,  it  requires  to  locate  sounding  objects  “ clarinet "  in  the  audio-visual  scenario  and  focus  on  the  “ first "  sound-  ing  “ clarinet "  in  the  timeline.  To  answer  the  question  cor-   
rectly,  both  effective  audio-visual  scene  understanding  and  spatio-temporal  reasoning  are  essentially  desired. 
In  this  work,  we  focus  on  the  Audio-Visual  Question  An-  swering  (AVQA)  task,  which  aims  to  answer  questions  re-  garding  visual  objects,  sounds  and  their  association.  To  this  end,  a  computational  model  is  essentially  required  to  equip  with  effective  multimodal  understanding  and  reasoning  ability  on  rich  dynamic  audio-visual  scenes.  To  facilitate  the  aforementioned  research,  we  built  a  large-scale  Spatio- 
Temporal  Music  AVQA  (MUSIC-AVQA)  dataset.  Con-  sidering  that  musical  performance  is  a  typical  multimodal  scene  consisting  of  abundant  audio  and  visual  components  as  well  as  their  interaction,  it  is  appropriate  to  be  utilized  for  the  exploration  of  effective  audio-visual  scene  under-  standing  and  reasoning.  So  we  collected  amounts  of  user-  uploaded  videos  of  musical  performance  from  YouTube,  and  videos  in  the  built  dataset  consist  of  solo,  ensemble  of  the  same  instruments  and  ensemble  of  different  instru-  ments. 
It  contains  9,288  videos  covering  22  instruments,  with  a  total  duration  of  over  150  hours.  45,867  question-  answer  pairs  are  generated  by  human  crowd-sourcing,  with  an  average  of  about  5  QA  pairs  per  video.  The  questions  are  derived  from  33  templates  and  asked  regarding  content  from  different  modalities  at  space  and  time,  which  are  suit-  able  to  explore  fine-grained  scene  understanding  and  spatio-  temporal  reasoning  in  the  audio-visual  context. 
To  solve  the  above  AVQA  task,  we  consider  this  prob-  lem  from  the  spatial  and  temporal  grounding  perspective,  respectively.  Firstly,  the  sound  and  the  location  of  its  vi-  sual  source  is  deemed  to  reflect  the  spatial  association  be-  tween  audio  and  visual  modality,  which  could  help  to  de-  compose  the  complex  scenario  into  concrete  audio-visual  association.  Hence,  we  propose  a  spatial  grounding  module  to  model  such  cross-modal  association  through  attention-  based  sound  source  localization.  Secondly,  since  the  audio-  visual  scene  changes  over  time  dynamically,  it  is  critical  to  capture  and  highlight  the  key  timestamps  that  are  closely  re-  lated  to  the  question.  Accordingly,  the  temporal  grounding  module  that  uses  question  features  as  queries  is  proposed  to  attend  crucial  temporal  segments  for  encoding  question-  aware  audio  and  visual  embeddings  effectively.  Finally,  the  above  spatial-aware  and  temporal-aware  audio-visual  fea-  tures  are  fused  to  obtain  a  joint  representation  for  Question 
Answering.  As  an  open-ended  problem,  the  correct  answers  to  questions  can  be  predicted  by  choosing  words  from  a  pre-  defined  answer  vocabulary.  Our  results  indicate  that  audio-  visual  QA  benefits  from  effective  audio-visual  scene  under-  standing  and  spatio-temporal  reasoning,  and  our  model  out-  performs  recent  A-,  V-,  and  AVQA  approaches. 
To  summarize,  our  contributions  are  threefold: 
•  We  build  the  large-scale  MUSIC-AVQA  dataset  of  musical  performance,  which  contains  more  than  9K  videos  annotated  by  over  45K  QA  pairs,  spanning  over  different  modal  scenes. 
•  A  spatio-temporal  grounding  model  is  proposed  to  solve  the  fine-grained  scene  understanding  and  reason-  ing  over  audio  and  visual  modalities. 
•  Extensive  experiments  show  that  AVQA  benefits  from  multisensory  perception  and  our  model  is  superior  to  recent  QA  approaches  especially  on  the  questions  that  measures  spatio-temporal  reasoning  ability  of  models.  2.  Related  Work  2.1.  Audio-Visual  Learning 
By  integrating  the  audio  and  visual  information  in  mul-  timodal  scenes,  it  is  expected  to  explore  more  sufficient  scene  information  and  overcome  the  limited  perception  in  single  modality.  Recently,  there  have  been  several  works  utilizing  audio  and  visual  modality  to  facilitate  mul-  timodal  scene  understanding  in  different  perspectives,  such  as  sound  source  localization  [23,  31,  34,  37,  48]  and  sepa-  ration  [10,  13,  41,  59,  61,  63],  audio  inpainting  [62],  event  localization  [4, 43, 64],  action  recognition  [14],  video  pars-  ing  [42, 47],  captioning  [24, 40, 50],  and  dialog  [1, 66]. 
Regarding  previous  works  on  sound  source  localiza-  tion  and  separation,  the  former  mainly  focuses  on  locat-  ing  sounds  in  a  visual  context  [34,  37],  while  the  latter  mainly  centers  around  separating  different  sounds  from  cor-  responding  visual  objects  [12, 59].  These  works  have  made  great  progress  for  the  interaction  of  audio  and  visual  fea-  tures,  but  they  essentially  focus  on  the  perception  of  audio-  visual  objects.  Further,  some  researchers  propose  to  inte-  grate  audio  and  visual  messages  to  explore  semantic  events  and  behaviors  in  multimodal  scenes  [14, 43].  As  expected,  these  works  have  shown  considerable  performance  by  uti-  lizing  more  sufficient  information  from  audio  and  visual  cues.  Based  on  which,  others  took  a  step  forward  to  parse  the  audio-visual  scenes  [42],  describe  content  [24],  and  leverage  contextual  cues  for  dialog  [1, 66]. 
Apart  from  the  above  methods  that  facilitate  scene  un-  derstanding  by  excavating  and  analyzing  different  modal-  ities,  a  unified  multimodal  model  should  also  be  able  to  reason  their  spatio-temporal  correlation.  In  this  work,  dif-  ferent  from  the  previous  methods,  besides  the  fine-grained  scene  understanding,  we  further  propose  to  explore  spatio-  temporal  reasoning  in  the  audio-visual  context.  2.2.  Question  Answering 
In  the  past  years,  several  question  answering  tasks  have  been  proposed  but  in  different  modalities,  including  text  question  answering  [35, 44],  visual  question  answering  [3,  25, 53, 57],  audio  question  answering  [9, 58],  etc. 
VQA  [3, 17, 32]  aims  to  generate  natural  language  an-  swers  about  specific  visual  content.  The  early  research  in 
VQA  focused  on  simple  visual  understanding  in  static  im-  ages  but  ignored  the  spatial  and  semantic  relationships  be- 
Dataset
Origin
Main  sound  type
#  Videos
ActivityNet-QA  [54]
TVQA  [29]
AVSD  [1]
Pano-AVQA  [56]
MUSIC-AVQA
ActivityNet
TV  Show
Charades
Online
YouTube