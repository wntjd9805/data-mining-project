Abstract
Recent studies in talking face generation have focused on building a model that can generalize from any source speech to any target identity. A number of works have al-ready claimed this functionality and have added that their models will also generalize to any language. However, we show, using languages from different language families, that these models do not translate well when the training lan-guage and the testing language are sufficiently different.
We reduce the scope of the problem to building a language-robust talking face generation system on seen identities, i.e., the target identity is the same as the training identity. In this work, we introduce a talking face generation system that generalizes to different languages. We evaluate the ef-ficacy of our system using a multilingual text-to-speech sys-tem. We present the joint text-to-speech system and the talk-ing face generation system as a neural dubber system. Our demo is available at https://bit.ly/ml- face-generation-cvpr22-demo. Also, our screencast is uploaded at https://youtu.be/F6h0s0M4vBI. 1.

Introduction
Talking face generation, a task of synthesizing a face video where the lip is synchronized with the input speech, is one of the most popular research topics in neural video gen-eration. When combined with a text-to-speech (TTS) sys-tem, the joint system allows users to create a talking video with only a text input and has potential applications in news broadcasting, virtual lectures, and digital concierge. Ex-panding the task to support multiple languages would sig-nificantly reduce the amount of effort required to widen the target audience to the global population.
Recent works in talking face generation claim that their models support input speeches in any language [8, 11].
* indicates equal contribution.
Figure 1. An overview of the demonstration. With the text, the lan-guage, and the source video, users can make face videos which in-clude the voice corresponding to text. For training data, we record two hours of footage of the Korean speaker.
However, we observe that such models fail to generalize to certain input speech languages, e.g., Korean. We hypothe-size that the robustness of these models depends on the de-gree of similarity between the training speech language and the input speech language. Thus, we will validate the gener-alization capabilities of multilingual face generation models using speeches of languages from different language fami-lies.
For practical applications of multilingual talking face generation, the speaker’s vocal identity should be preserved
Since multilingual speech across different datasets for desired speakers are often unavailable, the mul-tilingual talking face generation system requires a multilin-languages.
gual TTS model capable of cross-lingual speech synthesis.
While a number of prior works in multilingual TTS discuss cross-lingual speech synthesis, the selection of languages has been underexplored. Thus, existing works’ ability to perform cross-lingual synthesis across languages from dif-ferent language families is questionable.
In this work, we propose a multilingual talking face gen-eration system shown in Fig. 1. We also describe the two models used in the multilingual TTS module and the talk-ing face generation module: a multilingual adaptation of
VITS [6] capable of performing cross-lingual speech syn-thesis while preserving the speaker’s vocal identity, and a talking face generation model capable of generating face videos from synthesized speeches, regardless of the lan-guage.
Our contributions in this work are the following:
• We introduce a system that can synthesize talking face videos in four languages (Korean, English, Japanese, and Chinese) for a monolingual speaker.
• We build a talking face generation model that is robust to different input speech languages.
• Our demonstration can generate 512×512 facial image sequences faster than 25 fps. 2.