Abstract
Grouping and recognition are important components of visual scene understanding, e.g., for object detection and semantic segmentation. With end-to-end deep learning systems, grouping of image regions usually happens im-plicitly via top-down supervision from pixel-level recogni-tion labels.
Instead, in this paper, we propose to bring back the grouping mechanism into deep networks, which allows semantic segments to emerge automatically with only text supervision. We propose a hierarchical Group-ing Vision Transformer (GroupViT), which goes beyond the regular grid structure representation and learns to group image regions into progressively larger arbitrary-shaped segments. We train GroupViT jointly with a text en-coder on a large-scale image-text dataset via contrastive losses. With only text supervision and without any pixel-level annotations, GroupViT learns to group together se-mantic regions and successfully transfers to the task of se-mantic segmentation in a zero-shot manner, i.e., without any further ﬁne-tuning.
It achieves a zero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on PASCAL Context datasets, and performs compet-itively to state-of-the-art transfer-learning methods requir-ing greater levels of supervision. We open-source our code at https://github.com/NVlabs/GroupViT. 1.

Introduction
Visual scenes are naturally composed of semantically-related groups of pixels. The relationship between group-ing and recognition has been studied extensively in visual understanding even before the deep learning era [52, 53]. In bottom-up grouping, the idea is to ﬁrst re-organize pixels into candidate groups and then to process each group with a recognition module. This pipeline has been successfully applied in image segmentation from superpixels [60], con-structing region proposals for object detection [75, 94] and semantic segmentation [3]. Beyond bottom-up inference, top-down feedback from recognition can also provide sig-nals to perform better visual grouping [74, 93].
*Jiarui Xu was an intern at NVIDIA during the project.
Figure 1. Problem Overview. First, we jointly train GroupViT and a text encoder using paired image-text data. With GroupViT, meaningful semantic grouping automatically emerges without any mask annotations. Then, we transfer the trained GroupViT model to the task of zero-shot semantic segmentation.
However, on moving to the deep learning era, the ideas of explicit grouping and recognition have been much less separated and more tightly coupled in end-to-end train-ing systems. Semantic segmentation, e.g., is commonly achieved via a Fully Convolutional Network [47], where pixel grouping is only revealed at the output by recognizing each pixel’s label. This approach eliminates the need to per-form explicit grouping. While this method is very powerful and still delivers state-of-the-art performance, there are two major limitations that come with it: (i) learning is limited by the high cost of per-pixel human labels; and (ii) the learned model is restricted only to a few labeled categories and can-not generalize to unseen ones.
Recent developments in learning visual representations from text supervision have shown tremendous success on
transferring to downstream tasks [59]. The learned model can not only be transferred to ImageNet classiﬁcation in a zero-shot manner and achieve state-of-the-art performance, but can also perform recognition on object categories be-yond ImageNet. Inspired by this line of research, we ask the question: Can we also learn a semantic segmentation model purely with text supervision, and without any per-pixel annotations, capable of generalizing to different sets of objects categories, or vocabularies, in a zero-shot man-ner?
To accomplish this, we propose to bring back the group-ing mechanism into deep networks, which allows semantic segments to emerge automatically with only text supervi-sion. An overview of our approach is illustrated in Fig. 1.
By training on large-scale paired image-text data with con-trastive losses, we enable the model to be zero-shot trans-ferred to several semantic segmentation vocabularies, with-out requiring any further annotation or ﬁne-tuning. Our key idea is to leverage the Vision Transformer (ViT) [22] and incorporate a new visual grouping module into it.
We call our model GroupViT (Grouping Vision Trans-former). Compared to convolutional neural networks (Con-vNets), which operate on regular grids, the global self-attention mechanism of Transformers naturally provides the
ﬂexibility to combine visual tokens into non-grid-like seg-ments. Thus, instead of organizing visual tokens into grids, as recent ViT-based applications [16, 23, 44, 81] do, we propose to perform hierarchical grouping of visual tokens into irregular-shaped segments. Speciﬁcally, our GroupViT model is organized in different stages through a hierarchy of Transformer layers, where each stage contains multiple
Transformers to perform information propagation among the group segments, and a grouping module that merges smaller segments into larger ones. With different input images, our model dynamically forms different visual seg-ments, each intuitively representing a semantic concept.
We train GroupViT with text supervision only. To per-form learning, we merge visual segment outputs in the ﬁnal stage of GroupViT using average pooling. We then com-pare this image-level embedding to those derived from tex-tual sentences via contrastive learning. We construct pos-itive training pairs by using corresponding image and text pairs, and negative ones by using text from other images.
We extract the text embedding with a Transformer model, trained jointly along with GroupViT from scratch. Interest-ingly, even though we only provide textual training super-vision at the image level, we ﬁnd that semantically mean-ingful segments automatically emerge using our grouping architecture.
During inference, for the task of semantic segmentation, given an input image, we extract its visual groups using
GroupViT (Fig. 1). Each ﬁnal group’s output represents a segment of the image. Given a vocabulary of label names for segmentation, we use the text Transformer to extract each label’s textual embedding. To perform semantic seg-mentation, we then assign the category labels to image seg-ments according to their mutual similarity in the embedding space. In our experiments, we show that GrouViT trained on the Conceptual Caption [10, 63] and Yahoo Flickr Cre-ative Commons [69] datasets with text supervision alone, can transfer to semantic segmentation tasks on the PASCAL
VOC [24] and PASCAL Context [54] datasets in a zero-shot manner. Without any ﬁne-tuning, we achieve a mean in-tersection over union (mIoU) of 52.3% on PASCAL VOC 2012 and an mIoU of 22.4% on PASCAL Context, perform-ing competitively to state-of-the-art transfer-learning meth-ods requiring greater levels of supervision. To our knowl-edge, our work is the ﬁrst to perform semantic segmentation on different label vocabularies in a zero-shot manner with text supervision alone, without requiring any pixel-wise la-bels.
Our contributions are the following:
• Moving beyond regular-shaped image grids in deep networks, we introduce a novel GroupViT architecture to perform hierarchical bottom-up grouping of visual concepts into irregular-shaped groups.
• Without any pixel-level labels and training and with only image-level text supervision using contrastive losses, GroupViT successfully learns to group image regions together and transfers to several semantic seg-mentation vocabularies in a zero-shot manner.
• To our knowledge, ours is the ﬁrst work to explore zero-shot transfer from text supervision alone to sev-eral semantic segmentation tasks without using any pixel-wise labels and establishes a strong baseline for this new task. 2.