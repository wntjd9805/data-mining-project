Abstract
Text-based video segmentation aims to segment the tar-get object in a video based on a describing sentence. Incor-porating motion information from optical ﬂow maps with appearance and linguistic modalities is crucial yet has been largely ignored by previous work. In this paper, we design a method to fuse and align appearance, motion, and linguistic features to achieve accurate segmentation. Speciﬁcally, we propose a multi-modal video transformer, which can fuse and aggregate multi-modal and temporal features between frames. Furthermore, we design a language-guided feature fusion module to progressively fuse appearance and motion features in each feature level with guidance from linguistic features. Finally, a multi-modal alignment loss is proposed to alleviate the semantic gap between features from different modalities. Extensive experiments on A2D Sentences and
J-HMDB Sentences verify the performance and the gener-alization ability of our method compared to the state-of-the-art methods. 1.

Introduction
Text-based video segmentation aims at locating and seg-menting the object described by a language sentence in a video sequence. Unlike traditional tasks, which do predic-tion on video- or frame- level, e.g. text-to-video retrieval
[27, 40, 52], video caption [32, 62], video question an-swering [22, 51], and language-queried video localization
[1, 60], this task requires relatively more ﬁne-grained multi-modal and temporal understanding for pixel-level segmen-tation. The challenge of this task can be thus summarized as: (1) how to reason between visual and linguistic modali-ties to locate the target object, and (2) how to leverage tem-poral information to enhance segmentation.
To solve the former problem, previous works adopt sim-ple concatenation [18], generating dynamic ﬁlters [16, 48] and cross-modal attention modules [21, 49] to achieve in-teractions between two modalities. When it comes to the
*Corresponding author.
Our code is publicly available at: https : / / github . com / wangbo-zhao/2022CVPR-MMMMTBVS.
Figure 1. Comparison between baseline and our model. We adopt
”B” in 4.4 as the baseline model. Compared with the baseline model, our model can incorporate motion information from optical
ﬂow maps with appearance and linguistic features and generate better segmentation masks. latter problem, they usually adopt 3D convolution neural networks (3D CNNs) e.g. I3D [8] to extract features from a video clip. However, all these methods ignore exploring the explicit motion information between frames for text-based video segmentation. In this task, the target object usually has action, and the corresponding text contains some words to describe its motion e.g. driving and jumping in Figure 1.
This means that the motion information may help the model to ﬁnd the target object. Despite the fact that some motion information between frames can be implicitly learned in 3D
CNNs, it can not well interact with other modalities. In-troducing motion information has been tried in some video tasks [9, 15, 24, 30, 53, 61, 63], but how to incorporate the motion information with appearance and linguistic features in text-based video segmentation is still challenging.
A common way to introduce explicit motion informa-tion is to extract features from ﬂow maps generated from an optical ﬂow estimation model. From ﬂow maps in Fig-ure 1, we can ﬁnd that the target object with motion usu-ally is distinctive and can be easily identiﬁed. This may promote the ﬁnal performance. To leverage the motion in-formation from optical ﬂow, Gavrilyuk et al. [16] adopt two 3D CNNS with different parameters to generate masks from
RGB frames and optical ﬂow maps, respectively, then com-pute weighted averaged masks from them. However, such a simple fusion strategy ignores the interaction between mo-tion modalities and appearance and linguistic features, lead-ing to unsatisfactory improvement and huge computational overhead. Hence, designing a model to effectively incorpo-rate the motion information from the optical ﬂow with ap-pearance features from RGB frames and linguistic features is necessary.
Motivated by observations above, we propose our multi-modal fusion and alignment network. First, since many previous works [3, 6, 9, 19] have demonstrated the supe-riority of transformers in reasoning and fusing multi-modal and temporal features, we build a multi-modal video trans-former (MMVT) to model the interaction between appear-ance, motion, and linguistic features in different frames.
Our transformer contains two attention modules in each layer: cross-modal attention and temporal attention. The former aims at fusing three modalities features, while the latter is adopted to aggregate fused features in the temporal dimension. By stacking them several layers, multi-modal information can ﬂow and interact with each other between different frames. Beneﬁting from the multi-modal interac-tion between frames in MMVT, we do not rely on 3D CNNs to extract temporal information, which largely reduce the computational overhead.
Then, to fuse multi-modal features progressively, we propose the language-guided feature fusion (LGFF) module and insert it into each level to decode features. In each mod-ule, useful appearance and motion features will be selected by the linguistic feature, with the help of features from the higher level. By doing this, useful features can be gradually selected and fused. Moreover, since both appearance, mo-tion, and language features are distinctive modalities fea-tures, which are generated from backbones separately pre-trained on different datasets, the semantic gap between them would be large [20]. To alleviate this problem, we design a multi-modal alignment loss, which explicitly encourages the network to learn to align three modalities features in an embedding space, which further improves the performance of our model.
In Figure 1, compared with the baseline model without motion information, our model can accurately locate the tar-get object, obtain a more complete mask, and distinguish the target object from others. Our main contributions can be summarised as:
• To the best of our knowledge, we are the ﬁrst to incor-porate the motion information from optical ﬂow maps with appearance and linguistic features for text-based video segmentation.
• We propose a transformer-based model to fuse multi-modal and temporal features and design a language-guided feature fusion module to progressively fuse multi-modal features from different feature levels.
• Noticing the semantic gap between different modal features, we propose a multi-modal alignment loss to explicitly align features from three different modali-ties, which further improve the performance of our method.
• Extensive experiments are conducted to verify the ef-fectiveness of proposed methods. Our approach sig-niﬁcantly surpasses existing state-of-the-art methods on most metrics on A2D Sentences and J-HMDB Sen-tences dataset with less computational overhead. 2.