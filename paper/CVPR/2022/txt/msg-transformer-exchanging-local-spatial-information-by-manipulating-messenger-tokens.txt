Abstract
Transformers have offered a new methodology of de-signing neural networks for visual recognition. Compared to convolutional networks, Transformers enjoy the ability of referring to global features at each stage, yet the at-tention module brings higher computational overhead that obstructs the application of Transformers to process high-resolution visual data. This paper aims to alleviate the conflict between efficiency and flexibility, for which we pro-pose a specialized token for each region that serves as a messenger (MSG). Hence, by manipulating these MSG to-kens, one can flexibly exchange visual information across regions and the computational complexity is reduced. We then integrate the MSG token into a multi-scale architec-ture named MSG-Transformer. In standard image classi-fication and object detection, MSG-Transformer achieves competitive performance and the inference on both GPU and CPU is accelerated. Code is available at https:
//github.com/hustvl/MSG-Transformer. 1.

Introduction
The past decade has witnessed the convolutional neural networks (CNNs) dominating the computer vision commu-nity. As one of the most popular models in deep learning,
CNNs construct a hierarchical structure to learn visual fea-tures, and in each layer, local features are aggregated using convolutions to produce features of the next layer. Though simple and efficient, this mechanism obstructs the commu-nication between features that are relatively distant from each other. To offer such an ability, researchers propose to replace convolutions by the Transformer, a module which is first introduced in the field of natural language process-ing [50]. It is shown that Transformers have the potential to
†Corresponding author.
The work was done during Jiemin Fang’s internship at Huawei Inc. learn visual representations and achieve remarkable success in a wide range of visual recognition problems including image classification [13, 39], object detection [4], semantic segmentation [61], etc.
The Transformer module works by using a token to for-mulate the feature at each spatial position. The features are then fed into self-attention computation and each to-ken, according to the vanilla design, can exchange infor-mation with all the others at every single layer. This de-sign facilitates the visual information to exchange faster but also increases the computational complexity, as the compu-tational complexity grows quadratically with the number of tokens – in comparison, the complexity of a regular convo-lution grows linearly. To reduce the computational costs, researchers propose to compute attention in local windows of the 2D visual features. However constructing local at-tention within overlapped regions enables communications between different locations but causes inevitable memory waste and computation cost; computing attention within non-overlapped regions impedes information communica-tions. As two typical local-attention vision Transformer methods, HaloNet [49] partitions query features without overlapping but overlaps key and value features by slightly increasing the window boundary; Swin Transformer [31] builds implicit connections between windows by alterna-tively changing the partition style in different layers, i.e., shifting the split windows. These methods achieve com-petitive performance compared to vanilla Transformers, but
HaloNet still wastes memories and introduces additional cost in the key and value; Swin Transformer relies on fre-quent 1D-2D feature transitions, which increase the imple-mentation difficulty and additional latency.
To alleviate the burden, this paper presents a new methodology towards more efficient exchange of informa-tion. This is done by constructing a messenger (MSG) to-ken in each local window. Each MSG token takes charge of summarizing information in the corresponding window
and exchange it with other MSG tokens. In other words, all regular tokens are not explicitly connected to other regions, and MSG tokens serve as the hub of information exchange.
This brings two-fold benefits. First, our design is friendly to implementation since it does not create redundant copies of data like [39, 49, 60]. Second and more importantly, the flexibility of design is largely improved. By simply ma-nipulating the MSG tokens (e.g., adjusting the coverage of each messenger token or programming how they exchange information), one can easily construct many different archi-tectures for various purposes. Integrating the Transformer with MSG tokens into a multi-scale design, we derive a pow-erful architecture named MSG-Transformer that takes ad-vantages of both multi-level feature extraction and compu-tational efficiency.
We instantiate MSG-Transformer as a straightforward case that the features of MSG tokens are shuffled and recon-structed with splits from different locations. This can effec-tively exchange information from local regions and deliv-ered to each other in the next attention computation, while the implementation is easy yet efficient. We evaluate the models on both image classification and object detection, which achieve promising performance. We expect our ef-forts can further ease the research and application of multi-scale/local-attention Transformers for visual recognition.
We summarize our contributions as follows.
• We propose a new local-attention based vision Trans-former with hierarchical resolutions, which computes attention in non-overlapped windows. Communica-tions between windows are achieved via the proposed
MSG tokens, which avoid frequent feature dimension transitions and maintain high concision and efficiency.
The proposed shuffle operation effectively exchanges information from different MSG tokens with negligible cost.
• In experiments, MSG-Transformers show promising i.e., results on both ImageNet [10] classification, 84.0% Top-1 accuracy, and MS-COCO [28] object de-tection, i.e., 52.8 mAP, which consistently outperforms recent state-of-the-art Swin Transformer [31]. Mean-while, due to the concision for feature process, MSG-Transformer shows speed advantages over Swin Trans-former, especially on the CPU device.
• Not directly operating on the enormous patch tokens, we propose to use the lightweight MSG tokens to ex-change information. The proposed MSG tokens effec-tively extract features from local regions and may have potential to take effects for other scenarios. We be-lieve our work will be heuristic for future explorations on vision Transformers. 2.