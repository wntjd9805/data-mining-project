Abstract
Most indoor 3D scene reconstruction methods focus on recovering 3D geometry and scene layout.
In this work, we go beyond this to propose PhotoScene1, a framework that takes input image(s) of a scene along with approx-imately aligned CAD geometry (either reconstructed au-tomatically or manually specified) and builds a photore-alistic digital twin with high-quality materials and simi-lar lighting. We model scene materials using procedural material graphs; such graphs represent photorealistic and resolution-independent materials. We optimize the param-eters of these graphs and their texture scale and rotation, as well as the scene lighting to best match the input image via a differentiable rendering layer. We evaluate our tech-nique on objects and layout reconstructions from ScanNet,
SUN RGB-D and stock photographs, and demonstrate that our method reconstructs high-quality, fully relightable 3D scenes that can be re-rendered under arbitrary viewpoints, zooms and lighting. 1.

Introduction
A core need in 3D content creation is to recreate indoor scenes from photographs with a high degree of photoreal-ism. Such photorealistic “digital twins” can be used in a variety of applications including augmented reality, photo-graphic editing and simulations for training in synthetic yet realistic environments. In recent years, commodity RGBD sensors have become common and remarkable progress has been made in reconstructing 3D scene geometry from both single [14, 47] and multiple photographs [51], as well as in aligning 3D models to images to build CAD-like scene reconstructions [5, 22, 23, 36]. But photorealistic applica-tions require going beyond the above geometry acquisition to capture material and lighting too — to not only recreate appearances accurately but also visualize and edit them at arbitrary resolutions, under novel views and illumination.
Prior works assign material to geometry under the sim-plifying assumptions of homogeneous material [23] or sin-1Code: https://github.com/ViLab-UCSD/photoscene
Figure 1.
Given an input photo and a coarsely aligned 3D scene model, PhotoScene automatically infers high-quality spatially-varying procedural materials and scene illumination to closely match scene appearance. The reconstructed materials are resolution-independent (see zoom insets) and ascribed to the full 3D geometry, to create a high-quality photorealistic digital twin that can be rendered under novel views and lighting. gle objects [38].
In contrast, we deal with the challenge of ascribing spatially-varying material to an indoor scene while reasoning about its complex and global interactions with arbitrary unknown illumination. One approach to our problem would be to rely on state-of-the-art inverse render-ing methods [29, 32] to reconstruct per-pixel material prop-erties and lighting. However, these methods are limited to the viewpoint and resolution of the input photograph, and do not assign materials to regions that are not visible (ei-ther outside the field of view or occluded by other objects).
Instead, we posit that learned scene priors from inverse ren-dering are a good initialization, whereafter a judicious com-bination of expressive material priors and physically-based differentiable rendering can solve the extremely ill-posed optimization of spatially-varying material and lighting.
In this paper, we use procedural node graphs as compact yet expressive priors for scene material properties. Such graphs are heavily used in the content and design industry to represent high-quality, resolution-independent materials with a compact set of optimizable parameters [1, 3]. This offers a significant advantage: if the parameters of a proce-dural graph can be estimated from just the observed parts of the scene in an image, we can use the full graph to ascribe
materials to the entire scene. Prior work of Shi et al. [43] estimates procedural materials, but is restricted to fully ob-served flat material samples imaged under known flash illu-mination. In contrast, we demonstrate that such procedural materials can be estimated from partial observations of in-door scenes under arbitrary, unknown illumination.
We assume as input a coarse 3D model of the scene with possibly imperfect alignment to the image, obtained through 3D reconstruction methods [5, 22, 36], or manu-ally assembled by an artist. We segment the image into distinct material regions, identify an appropriate procedu-ral graph (from a library) for each region, then use the 3D scene geometries and their corresponding texture UV pa-rameterizations to “unwarp” these pixels into (usually in-complete) 2D textures. This establishes a fully differen-tiable pipeline from the parameters of the procedural ma-terial via a physically-based rendering layer to an image of the scene, allowing us to backpropagate the rendering error to optimize the material parameters. In addition, we also estimate rotation and scale of the UV parameterization and optimize the parameters of the globally-consistent scene il-lumination to best match the input photograph.
As shown in Fig. 1 our method can infer spatially-varying materials and lighting even from a single image.
Transferring these materials to the input geometry produces a fully relightable 3D scene that can then be rendered un-der novel viewpoint or lighting. Since procedural materials are resolution-invariant and tileable, we can render closeup views that reveal fine material details, without having ob-served these in the input photograph. This goes significantly beyond the capabilities of current scene-level inverse ren-dering methods and allows for the creation of high-quality, photorealistic replicas of complex indoor scenes. 2.