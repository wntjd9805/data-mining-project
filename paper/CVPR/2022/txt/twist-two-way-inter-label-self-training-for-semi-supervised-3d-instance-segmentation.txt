Abstract
We explore the way to alleviate the label-hungry prob-lem in a semi-supervised setting for 3D instance segmenta-tion. To leverage the unlabeled data to boost model perfor-mance, we present a novel Two-Way Inter-label Self-Training framework named TWIST. It exploits inherent correlations between semantic understanding and instance information of a scene. Specifically, we consider two kinds of pseudo labels for semantic- and instance-level supervision. Our key design is to provide object-level information for denois-ing pseudo labels and make use of their correlation for two-way mutual enhancement, thereby iteratively promoting the pseudo-label qualities. TWIST attains leading perfor-mance on both ScanNet and S3DIS, compared to recent 3D pre-training approaches, and can cooperate with them to further enhance performance, e.g., +4.4% AP50 on 1%-label ScanNet data-efficient benchmark. Code is available at https:// github.com/ dvlab-research/ TWIST. 1.

Introduction
Deep learning methods have achieved great success on 3D point cloud learning. They demand large-scale anno-tated data. Compared to work on scanning, methods for annotations consume substantially more manual effort. For
ScanNet [9], 20 people were hired for collecting the RGB-D scans. However, it took 500 crowd-based workers, each using around 22.3 minutes, to label one scan on average.
To alleviate this label-hungry problem, a direction is to exploit semi-supervised learning (SSL). This setting needs ground-truth labels only for a small fraction of the training set. The target is to leverage a large volume of completely unlabeled data to boost model performance. Contrary to intensive research on image understanding [1, 14, 26, 27, 38, 39], less [20, 49] was carried out on this setting for 3D instance segmentation, which is an important task for 3D perception. Methods of [20, 49] utilize unlabeled data for
*Corresponding Author
Figure 1. Top: Pseudo semantic labels (green means correct and red means incorrect results) produced on unlabeled point cloud by (a) confidence thresholding, (b) our method without the re-correction module, and (c) our full method. Bottom: Pseudo centroids (blue dots) found by various methods. Orange dots mark the ground truth (GT) and red boxes mark the blue dots far from GTs. Our TWIST framework (c) effectively promotes the quality of both semantic and offset-to-centroid pseudo labels. model pre-training through a contrastive loss. They, however, only explore SSL by means of consistency regularization.
In this paper, we address semi-supervised 3D instance segmentation by designing a new self-training framework, which is the first of this kind. We aim to generate high-quality pseudo labels from unlabeled data to improve model training. This goal is challenging to achieve. First, the task requires both semantic- and instance-level understanding of a 3D scene. These two goals may conflict with each other.
For example, we may want different instances of the same class to have different instance IDs but the same semantic
ID. Therefore, it is non-trivial to generate high-quality (joint) pseudo labels for supporting both prediction tasks.
Second, the way to promote consistency among pseudo labels in the instance-level task has much room to explore.
For instance, pseudo semantic labels within the same in-stance should be consistent. Otherwise, it could confuse how points are separated into object instances. Third, an effective pseudo-label evaluation and selection mechanism is in high
demand. It cannot be achieved easily by common strategies, such as simple confidence thresholding [3, 48].
To address these issues, we design the Two-Way Inter-label Self-Training (TWIST) framework that collectively considers two kinds of pseudo labels, i.e., pseudo seman-tic labels for semantic-level supervision and pseudo offset vectors for instance-level supervision. Importantly, TWIST iteratively updates the two pseudo-label sets, while promot-ing their consistency and quality. The key designs include a novel proposal re-correction module to leverage object-level predictions to denoise the pseudo labels and strategies to enable inter-label mutual enhancement.
Specifically, TWIST does not generate pseudo labels in the point level like point-wise confidence thresholding due to its vulnerability to noise, as observed in Fig. 1(a). In-stead, we utilize the model to predict instance proposals and leverage this prior to update the pseudo labels in proposal level, naturally preserving the intra-proposal consistency. To further improve the pseudo-label quality, we develop the proposal re-correction module to provide object-wise evalu-ation along with pseudo-label denoising. This module can be trained in a learnable fashion and takes input of diverse proposal-level samples to mitigate the label-hungry issue.
Another notable characteristic of TWIST is to explic-itly encourage mutual enhancement between two pseudo-label sets. Here, we design two-way bidirectional inter-label interactions, implemented by the semantic-guided in-stance proposal generation module and the proposal-based pseudo-label update module. Also, we design the proposal re-correction module between them as a safeguard to assess proposal quality and correct the labels of low-quality ones.
It encourages better model convergence. By these means, we jointly enhance the two pseudo-label sets significantly, as shown in Fig. 1(c).
We evaluate TWIST on two large-scale 3D datasets of
ScanNet v2 [9] and S3DIS [2]. TWIST outperforms both the supervised-only baseline and state-of-the-art unsupervised pre-training approaches [20, 49] by a large margin. Also, it can cooperate with other 3D pre-training approaches [20, 49] for further performance gain of 0.8 to 4.4 points. This property indicates that TWIST has complementary strength for semi-supervised 3D instance segmentation. Our overall contributions are as follows.
• We demonstrate the effectiveness of self-training for semi-supervised 3D instance segmentation, using two kinds of pseudo labels for effective model training.
• We present TWIST that enables generation of more accurate pseudo labels with object-level denoise and two-way inter-label enhancement.
• A new SOTA semi-supervised learning framework is proposed for 3D instance segmentation. It is verified on two large-scale datasets and shows complementary strength with existing 3D pre-training approaches. 2.