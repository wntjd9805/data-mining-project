Abstract
Few-shot learning (FSL) has received a lot of attention due to its remarkable ability to adapt to novel classes. Al-though many techniques have been proposed for FSL, they mostly focus on improving FSL backbones. Some works also focus on learning on top of the features generated by these backbones to adapt them to novel classes. We present an unsupErvised discriminAnt Subspace lEarning (EASE) that improves transductive few-shot learning per-formance by learning a linear projection onto a subspace built from features of the support set and the unlabeled query set in the test time. Speciﬁcally, based on the support set and the unlabeled query set, we generate the similar-ity matrix and the dissimilarity matrix based on the struc-ture prior for the proposed EASE method, which is efﬁ-ciently solved with SVD. We also introduce conStraIned wAsserstein MEan Shift clustEring (SIAMESE) which ex-tends Sinkhorn K-means by incorporating labeled support samples. SIAMESE works on the features obtained from
EASE to estimate class centers and query predictions. On the mini-ImageNet, tiered-ImageNet, CIFAR-FS, CUB and
OpenMIC benchmarks, both steps signiﬁcantly boost the performance in transductive FSL and semi-supervised FSL. 1.

Introduction
Supervised end-to-end learning has been extremely suc-cessful in computer vision, speech, and machine translation tasks due to larger datasets and the rapid development of deep convolutional architectures. However, with the cur-rent learning paradigm, limited data is an obstacle in train-ing a sufﬁciently good model for inference. In many tasks, annotation is likely to be scarce or costly to obtain, as the annotation process may require expert knowledge (e.g., as-sociation of medical images with diseases).
*The corresponding author. Code: https : / / github . com / allenhaozhu/EASE
In contrast to the requirement of big data in deep learn-ing, humans learn new objects from a few examples. In-spired by the ability of biological vision, researchers pro-posed few-shot learning (FSL) [5]. FSL algorithms can be broadly classiﬁed into three categories: metric learning, meta-learning, and transfer learning. The goal of metric-based learning approaches is to learn a mapping from im-ages to an embedding space in which images of the same class are closer to each other, whereas images of differ-ent classes are separated apart. We expect this property to apply to classes that have not been seen before. Meta-learning is tasked with resolving an individual task-speciﬁc optimization that can be easily adapted to new tasks without overﬁtting. Transfer learning includes pre-training a feature extractor in the ﬁrst stage and then learning to reuse this knowledge to obtain a classiﬁer on new samples.
Several recent studies [4,9,10,15,26,31] explored trans-ductive inference for few-shot tasks. At the test time, trans-ductive few-shot methods perform the class label inference jointly for all the unlabeled query samples of the task, rather than one sample at a time, as in the case of inductive infer-ence [17, 25, 38, 40, 52–55]. Therefore, transductive few-shot methods typically perform better than their inductive counterparts. The core idea of transductive FSL meth-ods is to connect the query set and the support set with a pseudo-label or via a similarity measure e.g., the Gaussian kernel. However, such methods ignore the potential struc-ture among the data points in the support set and the query set. In this paper, we argue that features in the inference step can be approximately drawn from a union of multiple subspaces, and thus the sample afﬁnity matrix follows the block-diagonal prior. Based on this prior, we generate the similarity matrix for all samples in each task based on sub-space clustering with a low-rank representation, instead of measuring the Euclidean distance between samples in the support or query set. By constructing a dissimilarity matrix with a simple assumption, we learn a linear projection for maximizing the similarity and minimizing the dissimilarity via a closed-form solution. Furthermore, we improve the ﬁ-nal performance by reﬁning each class mean with unlabeled
data. In experiments, our model outperforms state-of-the-art methods by signiﬁcant margins, consistently providing improvements across different settings, datasets, and train-ing models. Furthermore, our transductive inference is very fast, with runtimes that are close to the runtimes of inductive inference, and can be used for the large-scale task.
Our contributions are as follows: i. We propose an assumption that the features in the infer-ence step can be approximately drawn from a union of multiple subspaces, and thus its similarity matrix fol-lows the block-diagonal prior. ii. We propose an unsupErvised discriminAnt Subspace lEarning (EASE), which is able to learn a discriminant subspace by maximizing the inter-class distance and minimizing the intra-class distance, akin to positive and negative sampling in graph node embedding. iii. As a minor contribution, we propose conStraIned wAsserstein MEan Shift clustEring (SIAMESE) which extends Sinkhorn K-means by incorporating labels of the support set. SIAMESE works on the features from
EASE to estimate class centers and query predictions. 2.