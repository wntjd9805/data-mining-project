Abstract
Layout design is ubiquitous in many applications, e.g. architecture/urban planning, etc, which involves a lengthy iterative design process. Recently, deep learning has been leveraged to automatically generate layouts via image gen-eration, showing a huge potential to free designers from la-borious routines. While automatic generation can greatly boost productivity, designer input is undoubtedly crucial.
An ideal AI-aided design tool should automate repetitive routines, and meanwhile accept human guidance and pro-vide smart/proactive suggestions. However, the capabil-ity of involving humans into the loop has been largely ig-nored in existing methods which are mostly end-to-end ap-proaches. To this end, we propose a new human-in-the-loop generative model, iPLAN, which is capable of automati-cally generating layouts, but also interacting with designers throughout the whole procedure, enabling humans and AI to co-evolve a sketchy idea gradually into the final design. iPLAN is evaluated on diverse datasets and compared with existing methods. The results show that iPLAN has high fidelity in producing similar layouts to those from human designers, great flexibility in accepting designer inputs and providing design suggestions accordingly, and strong gen-eralizability when facing unseen design tasks and limited training data. 1.

Introduction
Layout generation has recently spiked research interests in computer vision/graphics, aiming to automate the design process and boost the productivity. The traditional design process follows a diagram of iteratively adjusting/finalizing details from coarse to fine and global to local, which im-poses repetitive and laborious routines on designers. Very recently, it has been shown that automatic image genera-tion of such designs (with minimal human input) is pos-sible through learning from data [12, 26, 27, 36]. This new line of research combines deep learning with design and has
*Corresponding author demonstrated a new avenue for AI-aided design.
To achieve full automation, current research tends to learn from existing designs in an end-to-end fashion, and then to generate new ones with qualitative similarity and sufficient diversity. Taking floorplan as an example, au-tomated generation can be based on simple human input, such as the boundary of the floor space [36], the relations among rooms [26, 27], or both [12]. While fully auto-mated generation is important, design is in nature a pro-cedural process, which involves alternations between repet-itive routines and creative thinking at multiple intermediate stages [29]. Therefore, an ideal AI-aided system should au-tomate the routine part while allowing the designer to im-part creativity. This requires the system to be able to in-teract with the designer, in the sense that it should accept designerâ€™s guidance, then actively suggest possible solu-tions accordingly, completing a feed-back loop. So far, the human-in-the-loop element is largely missing, which pre-vents a closer integration of AI and existing design practice.
Designing such an AI model faces several intrinsic chal-lenges. In practice, learning how to interact with the de-signer requires a full observation of the decisions made at every intermediate stage. However, existing datasets, such as RPLAN [36] and LIFULL [22], usually only include the final designs, without the stage-to-stage design process.
One potential solution to overcome this issue is to reverse-engineer intermediate stages from final designs, which how-ever leads to another difficulty: the order of stages depends on the specific task/goal and could vary dramatically even for the same final design. Further, the order uncertainty is exacerbated by the strong personal styles and preferences of designers. Thus, how to design an AI system that can ac-count for the above factors is a key research question, which is under-explored to date.
In this paper, we propose a new human-in-the-loop gen-erative model for layout generation, which is referred to as interactive planning (iPLAN). Unlike previous work, iPLAN is equipped with a user-friendly interaction mech-anism, which is achieved by letting the AI model learn the multi-stage design process, aiming to accommodate free-form user interactions and propose design suggestions at
every stage. This allows designer inputs at different stages across a wide range of levels of detail, while offering the capability of fully automated generation. To address the challenge of missing procedural design data, we reverse-engineer the final design to obtain the stage-to-stage pro-cess, based on principles that are widely adopted by profes-sional designers [29]. This enables us to design a Markov chain model to capture the full design procedure. Since there is more than one way to reverse-engineer the final de-signs (i.e., the stage order can vary), our model is designed with the capacity of accepting inputs with an arbitrary or-der, and consequently can learn the style variations implic-itly from the data.
While iPLAN is general, we focus on floorplan design in this paper. iPLAN has been validated on two large-scale benchmark datasets, i.e., RPLAN [36] and LIFULL [22], under diverse scenarios. The experiments show that our model is versatile in accepting designer inputs at various levels of detail, from minimal input and automatic gen-eration, to stage-to-stage human guidance and interactive design. By learning from designs augmented by reverse-engineered processes, our model exhibits high fidelity in generating new designs with close style similarity and suf-ficient diversity. Finally, our model is highly flexible and generalizable when trained on varying amounts of data and facing unseen spaces and design requirements that are cate-gorically different from the training data.
Contributions: (i) We propose a novel human-in-the-loop generative model iPLAN which respects design prin-ciples and mimics the design styles of professional design-ers implicitly. (ii) We demonstrate a successful fine-grained stage-to-stage generative model for floorplan, as opposed to existing end-to-end approaches. (iii) We show a variety of design scenarios, including fully automated generation, interactive planning with user instructions, and generaliza-tion for unseen tasks; (iv) We conduct extensive evaluations on diverse benchmark datasets and demonstrate that iPLAN outperforms the state of the art under multiple metrics. 2.