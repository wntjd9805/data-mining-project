Abstract
Recent works have shown that the computational efﬁ-ciency of video recognition can be signiﬁcantly improved by reducing the spatial redundancy. As a representative work, the adaptive focus method (AdaFocus) has achieved a favorable trade-off between accuracy and inference speed by dynamically identifying and attending to the informa-tive regions in each video frame. However, AdaFocus re-quires a complicated three-stage training pipeline (involv-ing reinforcement learning), leading to slow convergence and is unfriendly to practitioners. This work reformulates the training of AdaFocus as a simple one-stage algorithm by introducing a differentiable interpolation-based patch se-lection operation, enabling efﬁcient end-to-end optimiza-tion. We further present an improved training scheme to address the issues introduced by the one-stage formula-tion, including the lack of supervision, input diversity and training stability. Moreover, a conditional-exit technique is proposed to perform temporal adaptive computation on top of AdaFocus without additional training. Extensive experiments on six benchmark datasets (i.e., ActivityNet,
FCVID, Mini-Kinetics, Something-Something V1&V2, and
Jester) demonstrate that our model signiﬁcantly outper-forms the original AdaFocus and other competitive base-lines, while being considerably more simple and efﬁcient to train. Code is available at https://github.com/
LeapLabTHU/AdaFocusV2.
Table 1. A comparison of training the original AdaFocus model (AdaFocusV1, from (cid:182) to (cid:186)) and AdaFocusV2 (end-to-end) on
Something-Something (Sth-Sth) V1 dataset. Both procedures start from the same initial backbone networks. Herein, fG, fL, fC and π are the model components (see Section 3.1 for details).
AdaFocusV1
AdaFocusV2
Pre-training
Stage-1
Stage-2
Stage-3 (cid:182) Pre-train fG on Sth-Sth V1. (cid:183) Pre-train fL on Sth-Sth V1. (cid:184) Train fL and fC using random patches. (cid:185) Train π using reinforcement learning. (cid:186) Fine-tune fL and fC.
End-to-End
Training (fG, fL, fC, π)
Figure 1. Comparisons of AdaFocusV1 and AdaFocusV2 on
Sth-Sth V1 in terms of accuracy v.s. training cost. The training time is measured based on 4 NVIDIA 3090 GPUs. The two sides of grey arrows correspond to the same network architecture (i.e., the same inference cost). Our AdaFocusV2 accelerates the training by 2.2-2.4×, while boosting the accuracy by 1.0-1.5%. 1.

Introduction
Deep networks have achieved remarkable success in large-scale video recognition tasks [3, 14, 16, 25, 58]. Their high accuracy has fueled the desire to deploy them for auto-matically recognizing the actions, events, or other contents within the explosively growing online videos in recent years (e.g., on YouTube). However, the models with state-of-the-∗Equal contribution. (cid:12)Corresponding authors. art performance [1,13,20,42,51,59] tend to be computation-ally intensive during inference. In real-world applications such as recommendation [8, 9, 18], surveillance [4, 7] and content-based searching [30], computation translates into power consumption and latency, both of which should be minimized for environmental, safety or economic reasons.
Several algorithms have been proposed to reduce the temporal redundancy of videos [19, 21, 35, 36, 47, 57, 67, 68, 73] by allocating the majority of computation to the
most task-relevant video frames rather than all. Orthogo-nal to these approaches, the recently proposed adaptive fo-cus network (AdaFocus) [64] reveals that reducing the spa-tial redundancy in video analysis yields promising results for efﬁcient video recognition. The AdaFocus framework is also compatible with the aforementioned temporal-adaptive methods to realize highly efﬁcient spatial-temporal compu-tation. Speciﬁcally, AdaFocus reduces the computational cost by applying the expensive high-capacity network only on some relatively small patches. These patches are strate-gically selected to capture the most informative regions of each video frame. In particular, the patch localization task is formulated as a non-differentiable discrete decision task, which is further solved with reinforcement learning. As a consequence, AdaFocus needs to be trained with a compli-cated three-stage training pipeline (see Table 1), resulting in long training time and being unfriendly to users.
This paper seeks to simplify the training process of
AdaFocus. We ﬁrst introduce a differentiable interpolation-based formulation for patch selecting, allowing gradient back-propagation throughout the whole model. We note that a straightforward implementation of end-to-end train-ing leads to optimization issues, including the lack of super-vision, input diversity and training stability, which severely degrade the performance. Therefore, we further propose three tailored training techniques: auxiliary supervision, diversity augmentation and stopping gradient, to address the aforementioned issues. These simple but effective techniques enable the simple one-stage formulation of our
AdaFocus algorithm to be trained effectively, and eventu-ally outperform the three-stage counterparts in terms of both test accuracy and training cost. The experimental compar-isons are presented in Figure 1. Our proposed method is referred to as AdaFocusV2.
An additional advantage of AdaFocus is that it can be easily improved by further considering temporal redun-dancy. The original paper implements this idea by dy-namically skipping less valuable frames with reinforcement learning. In contrast, this work proposes a simpliﬁed early-exit algorithm that removes the requirement of introducing additional training, but achieves competitive performance.
The effectiveness of AdaFocusV2 is extensively evalu-ated on six video recognition benchmarks (i.e., ActivityNet,
FCVID, Mini-Kinetics, Sth-Sth V1&V2, and Jester). Ex-perimental results show that the training of AdaFocusV2 is 2 faster (measured in wall-time) than the original counter-× part, while achieving consistently higher accuracy. 2.