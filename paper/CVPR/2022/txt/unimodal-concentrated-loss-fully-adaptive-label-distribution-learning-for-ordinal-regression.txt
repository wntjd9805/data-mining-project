Abstract
Learning from a label distribution has achieved promis-ing results on ordinal regression tasks such as facial age and head pose estimation wherein, the concept of adaptive label distribution learning (ALDL) has drawn lots of atten-tion recently for its superiority in theory. However, com-pared with the methods assuming fixed form label distribu-tion, ALDL methods have not achieved better performance.
We argue that existing ALDL algorithms do not fully ex-ploit the intrinsic properties of ordinal regression. In this paper, we emphatically summarize that learning an adap-tive label distribution on ordinal regression tasks should follow three principles. First, the probability correspond-ing to the ground-truth should be the highest in label dis-tribution. Second, the probabilities of neighboring labels should decrease with the increase of distance away from the ground-truth, i.e., the distribution is unimodal. Third, the label distribution should vary with samples changing, and even be distinct for different instances with the same label, due to the different levels of difficulty and ambiguity.
Under the premise of these principles, we propose a novel loss function for fully adaptive label distribution learning, namely unimodal-concentrated loss. Specifically, the uni-modal loss derived from the learning to rank strategy con-strains the distribution to be unimodal. Furthermore, the estimation error and the variance of the predicted distribu-tion for a specific sample are integrated into the proposed concentrated loss to make the predicted distribution maxi-mize at the ground-truth and vary according to the predict-ing uncertainty. Extensive experimental results on typical ordinal regression tasks including age and head pose es-timation, show the superiority of our proposed unimodal-concentrated loss compared with existing loss functions.
∗Authors contribute equally to this work.
†Shiliang Pu is the corresponding author.
Figure 1. Distributions predicted by Mean-Variance method [22] and ours. Our predictions are optimized to be unimodal and learned according to specific instances adaptively. On the contrary, predictions of Mean-Variance are optimized to be concentrated for all instances and do not ensure unimodal distributions explicitly. 1.

Introduction
Ordinal regression solves the challenging problems that labels are related in a natural or implied order. Many critical tasks are involved in the ordinal regression problem, e.g., facial age estimation, head pose estimation, facial attrac-tiveness computation and movie ratings, which play an im-portant role in many practical applications such as human-computer interaction, driver monitoring, precise advertising and video surveillance [10, 32].
Early classic works [11, 17, 20, 24, 37, 38] are based on ordinary classification or regression, which do not perform well due to ignoring the ordinal relationship among labels, and suffering from the ambiguous labeling. In recent years, ranking based methods [3, 21] are proposed which use mul-tiple binary classifiers to determine the rank order. They explicitly make use of the ordinal information but they do
not consider the label ambiguity.
To address the ordinal relationship and label ambiguity, label distribution learning (LDL) [7] converts a single label to a label distribution. The label distribution covers a cer-tain number of class labels, representing the degree to which each label describes the instance. Since the real distribution for each instance is not available and must be artificially generated with proper assumption, it can be called fixed form label distribution learning (FLDL). The typical form is the Gaussian distribution centered at the ground-truth with assumed standard deviation [1, 7, 8]. Although FLDL ap-proaches achieve improved performance, however, they use a fixed form distribution to describe various instances which limits their expression ability.
To overcome this limitation, the concept adaptive la-bel distribution learning (ALDL) [9] has been proposed.
Among the ALDL based methods, Mean-Variance [22] is a typical work achieving the promising result, which esti-mates a distribution with learned mean and variance. How-ever, it pursues a highly concentrated distribution for all in-stances by making the mean as close to the ground-truth as possible, and the variance as small as possible. Moreover, it can not guarantee the learned distribution is unimodal by a joint use of softmax and mean-variance loss without uni-modal constraint. Therefore, we observe that the distribu-tions learned by Mean-Variance are not fully adaptive and are multimodal for some instances, as shown in Fig. 1. We can see the learned distribution for the older man is mul-timodal, and the learned distributions for the two persons are similar. The learned distributions do not accord with the tendency of facial aging, which might be significantly different at different ages [9].
Obviously, current ALDL methods have not fully ex-In ploited the intrinsic properties of ordinal regression. this paper, the following three principles are summarized for ordinal regression. First, following the empirical risk minimization, the probability corresponding to the ground-truth should be the highest in a label distribution. Second, the labels in ordinal regression tasks change gradually, and the similarity between the test instance and the class pro-totype decreases gradually when the label move away from the ground-truth. Therefore, the probabilities of neighbor-ing labels accounting for the instance should decrease with the increase of distance away from the ground-truth, i.e., the distribution is unimodal. Third, the label distribution should vary with the samples changing, and even be distinct for different instances with the same label, due to the differ-ent levels of difficulty and ambiguity. In other words, the learned label distribution should be adaptive for a particular instance. To satisfy the principles above, we propose a new adaptive label distribution learning approach equipped with a unimodal-concentrated loss. Based on principle I, we di-rectly maximize the probability at the ground-truth via con-centrated loss as our primary learning objective. Based on principle II, the unimodal loss derived from learning to rank strategy (LTR) [6] is introduced to constrain the distribution to be unimodal. If two neighboring labels are ranked incor-rectly, a positive loss would be output to update the train-able parameters to correct the ordinal relationship. Based on principle III, the variance of the distribution correspond-ing to the concentration degree is integrated and optimized jointly in the concentrated loss, which can be regarded as an indicator of data uncertainty and label ambiguity. The main contributions of this work are three-fold:
• We are the first to comprehensively summarize the in-trinsic principles for learning an adaptive label distri-bution on ordinal regression tasks. First, the probabil-ity at the ground-truth should be the highest in the dis-tribution. Second, the distribution should be unimodal.
Third, the distribution should be adaptive to individual instances. These three principles would shed light on the design of loss functions for future works in the field of ordinal regression.
• Different from previous methods which do not fully comply the above principles, we propose a new unimodal-concentrated loss, with the unimodal part constraining the distribution to be unimodal, and with the concentrated part making the distribution concen-trated at the ground-truth and fully adaptive to individ-ual instances.
• The proposed loss can be easily embedded into exist-ing CNNs without modifying the structure, and exten-sive experimental results demonstrate its superiority. 2.