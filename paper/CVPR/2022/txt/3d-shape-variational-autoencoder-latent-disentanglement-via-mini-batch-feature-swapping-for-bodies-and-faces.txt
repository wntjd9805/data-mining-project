Abstract
Learning a disentangled, interpretable, and structured latent representation in 3D generative models of faces and bodies is still an open problem. The problem is particularly acute when control over identity features is required. In this paper, we propose an intuitive yet effective self-supervised approach to train a 3D shape variational autoencoder (VAE) which encourages a disentangled latent representa-tion of identity features. Curating the mini-batch generation by swapping arbitrary features across different shapes al-lows to define a loss function leveraging known differences and similarities in the latent representations. Experimental results conducted on 3D meshes show that state-of-the-art methods for latent disentanglement are not able to disen-tangle identity features of faces and bodies. Our proposed method properly decouples the generation of such features while maintaining good representation and reconstruction capabilities. Our code and pre-trained models are avail-able at github.com/simofoti/3DVAE-SwapDisentangled. 1.

Introduction
The generation of 3D human faces and bodies is a com-plex task with multiple potential applications ranging from movie and game productions, to augmented and virtual re-ality, as well as healthcare applications. Currently, the gen-eration procedure is either manually performed by highly skilled artists or it involves semi-automated avatar design tools. Even though these tools greatly simplify the design process, they are usually limited in flexibility because of the intrinsic constraints of the underlying generative mod-els [17]. Blendshapes [28, 31, 38], 3D morphable mod-els [5,25,33], autoencoders [3, 8,16,35], and generative ad-versarial networks [1, 7, 15, 24] are currently the most used generative models, but they all share one particular issue: the creation of local features is difficult or even impossi-ble. In fact, not only do generative coefficients (or latent variables) lack any semantic meaning, but they also create global changes in the output shape. For this reason, we fo-cus on the problem of 3D shape creation by enforcing dis-entanglement among sets of generative coefficients control-ling the identity of a character.
Following [4, 18, 19] we define a disentangled latent rep-resentation as one where changes in one latent unit affects only one factor of variation while being invariant to changes in other factors. More interpretable and structured latent representations of data that expose their semantic mean-ing have been widely researched in the artificial intelli-gence community [10,13,18,19,22], but this is still an open problem especially for generative models of 3D shapes [3].
Given the superior representation capabilities, the reduced number of parameters, and the stable training procedures, we decide to focus our study on deep-learning-based gen-erative models and in particular on variational autoencoders (VAEs). In this field, recent work has tried to address the latent disentanglement problem for 3D shapes and man-aged to decouple the control over identity and expression (or pose) [1,3,8], but they are still unable to properly disen-tangle identity features. Some success has been achieved in the generation of 3D shapes of furniture [29, 43], but the structural variability of the data requires complex archi-tectures with multiple encoders and decoders for different furniture parts. In contrast, our method relies on a single
VAE which is trained by curating the mini-batch genera-tion procedure and with an additional loss. The intuition behind our method is that if we swap features (e.g. nose, ears, legs, arms, etc.) across the input data in a controlled manner (Fig. 1, Left), we not only know a priori which shapes within a mini-batch have (do not have) the same fea-ture, but we also know which are (are not) created from the same face (body). These differences and similarities across shapes should be captured in the latent representa-tion. Therefore, assuming that different subsets of latent variables correspond to different features, we can partition the latent space and leverage the structure of the input batch to encourage a more disentangled, interpretable, and struc-tured representation.
With the objective of building a model capable of gen-erating 3D meshes, we define our VAE architecture extend-ing [16]. This state-of-the-art model proved to be fast and capable of better capturing non-linear representations of 3D meshes, while leveraging very intuitive convolutional op-erators characterised by a reduced number of parameters.
Nonetheless, the network choice is arbitrary and we expect our method to be working also with other network config-urations and operators. Even though we consider meshes as our primary data structures, it is also worth noting that, by providing semantic segmentations of the different fea-tures, our method is applicable to voxel- or point-cloud-based generative models. We believe that the generality of the proposed method is particularly important in the current geometric deep learning field, where definitions of 3D con-volutions and pooling operators are still an open problem.
To summarise, the key contributions of our approach are: (i) the definition of a new mini-batching procedure based on feature swapping, (ii) the introduction of a novel loss func-tion capable of leveraging shape differences and similarities within each mini-batch, and (iii) the consequent creation of a 3D-VAE capable of generating 3D meshes from a more interpretable and structured latent representation. 2.