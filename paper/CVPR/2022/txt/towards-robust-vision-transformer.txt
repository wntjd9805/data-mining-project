Abstract
Recent advances on Vision Transformer (ViT) and its im-proved variants have shown that self-attention-based net-works surpass traditional Convolutional Neural Networks (CNNs) in most vision tasks. However, existing ViTs fo-cus on the standard accuracy and computation cost, lack-ing the investigation of the intrinsic influence on model ro-bustness and generalization. In this work, we conduct sys-tematic evaluation on components of ViTs in terms of their impact on robustness to adversarial examples, common cor-ruptions and distribution shifts. We find some components can be harmful to robustness. By leveraging robust compo-nents as building blocks of ViTs, we propose Robust Vision
Transformer (RVT), which is a new vision transformer and has superior performance with strong robustness. Inspired by the findings during the evaluation, we further propose two new plug-and-play techniques called position-aware at-tention scaling and patch-wise augmentation to augment our RVT, which we abbreviate as RVT∗. The experimen-tal results of RVT on ImageNet and six robustness bench-marks demonstrate its advanced robustness and general-ization ability compared with previous ViTs and state-of-the-art CNNs. Furthermore, RVT-S∗ achieves Top-1 rank on multiple robustness leaderboards including ImageNet-C,
ImageNet-Sketch and ImageNet-R. 1.

Introduction
Following the popularity of transformers in Natural Lan-guage Processing (NLP) applications, e.g., BERT [8] and
GPT [30], there has sparked particular interest in inves-tigating whether transformer can be a primary backbone for computer vision applications previously dominated by
Convolutional Neural Networks (CNNs). Recently, Vi-sion Transformer (ViT) [10] successfully applies a pure transformer for classification which achieves an impres-sive speed-accuracy trade-off by capturing long-range de-pendencies via self-attention. Base on this seminal work, numerous variants have been proposed to improve ViTs
Figure 1. Comparison between RVT and the baseline transform-ers. The robust accuracy in figure is recorded under FGSM [11] adversary. from different perspectives containing training data effi-introducing ciency [40], self-attention mechanism [25], convolution [23,45,50] or pooling layers [20,43], etc. How-ever, these works only focus on the standard accuracy and computation cost, lacking the investigation of the intrinsic influence on model robustness and generalization.
In this work, we take initiatives to explore a ViT model with strong robustness. To this end, we first give an em-pirical assessment of existing ViT models in Figure 1. Sur-prisingly, although all ViT variants reproduce the standard accuracy claimed in the paper, some of their modifications may bring devastating damages on the model robustness. A vivid example is PVT [43], which achieves a high standard accuracy but suffered with large drop of robust accuracy.
We show that PVT-Small obtains only 26.6% robust accu-racy, which is 14.1% lower than original DeiT-S in Figure 1.
To demystify the trade-offs between accuracy and ro-bustness, we analyze ViT models with different patch em-bedding, position embedding, transformer blocks and clas-sification head whose impact on the robustness that has never been thoroughly studied. Based on the valuable find-ings revealed by exploratory experiments, we propose a
Robust Vision Transformer (RVT), which has significant improvement on robustness, but also exceeds most other transformers in accuracy. In addition, we propose two new plug-and-play techniques to further boost the RVT. The first is Position-Aware Attention Scaling (PAAS), which plays the role of position encoding in RVT. PAAS im-proves the self-attention mechanism by filtering out redun-dant and noisy position correlation and activating only ma-jor attention with strong correlation, which leads to the en-hancement of model robustness. The second is a simple and general patch-wise augmentation method for patch se-quences which adds rich affinity and diversity to training data. Patch-wise augmentation also contributes to the model generalization by reducing the risk of over-fitting. With the above proposed methods, we can build an augmented Ro-bust Vision Transformer∗ (RVT∗). Contributions of this pa-per are three-fold:
• We give a systematic robustness analysis of ViTs and reveal harmful components. Inspired by it, we reform robust components as building blocks as a new trans-former, named Robust Vision Transformer (RVT).
• To further improve the RVT, we propose two new plug-and-play techniques called position-aware atten-tion scaling and patch-wise augmentation. Both of them can be applied to other ViT models and yield sig-nificant enhancement on robustness and standard accu-racy.
• Experimental results on ImageNet and six robustness benchmarks show that RVT exhibits best trade-offs between standard accuracy and robustness compared with previous ViTs and CNNs. Specifically, RVT-S∗ achieves Top-1 rank on ImageNet-C, ImageNet-Sketch and ImageNet-R. 2.