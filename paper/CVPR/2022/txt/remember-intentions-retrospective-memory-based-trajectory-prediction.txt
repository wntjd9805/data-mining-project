Abstract
To realize trajectory prediction, most previous methods adopt the parameter-based approach, which encodes all the seen past-future instance pairs into model parameters. How-ever, in this way, the model parameters come from all seen instances, which means a huge amount of irrelevant seen instances might also involve in predicting the current situa-tion, disturbing the performance. To provide a more explicit link between the current situation and the seen instances, we imitate the mechanism of retrospective memory in neuropsy-chology and propose MemoNet, an instance-based approach that predicts the movement intentions of agents by looking for similar scenarios in the training data.
In MemoNet, we design a pair of memory banks to explicitly store repre-sentative instances in the training set, acting as prefrontal cortex in the neural system, and a trainable memory ad-dresser to adaptively search a current situation with similar instances in the memory bank, acting like basal ganglia.
During prediction, MemoNet recalls previous memory by using the memory addresser to index related instances in the memory bank. We further propose a two-step trajectory prediction system, where the first step is to leverage Memo-Net to predict the destination and the second step is to fulfill the whole trajectory according to the predicted destinations.
Experiments show that the proposed MemoNet improves the
FDE by 20.3%/10.2%/28.3% from the previous best method on SDD/ETH-UCY/NBA datasets. Experiments also show that our MemoNet has the ability to trace back to specific instances during prediction, promoting more interpretability. 1.

Introduction
Trajectory prediction aims to predict the future move-ments for one or multiple interacting agents given the past trajectories. On the one hand, this task has broad practical applications to autonomous driving [24], drones [6], surveil-lance systems [42] and interactive robotics [18]; on the other hand, this is a fundamental scientific question about linking
*Equal contribution.
Code is available at: https://github.com/MediaBrain-†Corresponding author.
SJTU/MemoNet
Figure 1. MemoNet mimics retrospective memory process. We use the memory bank to explicitly store representative instances, acting like prefrontal cortex; and the memory addresser to search similar memory instances with current situation, acting like basal ganglia. the past to the future. The overall strategy is to summarize useful experiences from a large amount of seen past-future pairs and then leverage those experiences to predict possible future intentions for the current situation.
To obtain useful experiences, previous works consider a parameter-based approach, which uses training data to op-timize model parameters. In this way, all the experiences are implicitly summarized and stored in a model as a whole during the optimization process. For example, [16, 35, 46] use encoder-decoder architectures and [13, 15] consider generator-discriminator architectures to regress future trajec-tory predictions. [17,23,32,38,47] use conditional variational autoencoders to sample multiple future trajectory embedding from latent distributions. [10,26] rely on a bivariate Gaussian
Mixture Model to output position distributions. However, the parameter-based approach is not optimal for two reasons.
First, it lacks interpretability because all model parameters do not have clear semantic meaning in the physical world.
This is critical in safety-sensitive applications, such as au-tonomous driving. Second, since the model parameters are trained from all seen instances, a huge amount of irrelevant seen past-future pairs might also involve in predicting the current situation, disturbing the performance.
To promote more interpretability and provide a more explicit link between the current situation and the seen in-stances, we propose MemoNet whose working mechanism is
inspired by human’s retrospective memory in neuropsychol-ogy [3, 9], the process that human learns intended future ac-tions by recalling information learned before. The proposed
MemoNet achieves the intention prediction by searching for similar instances stored during training. In MemoNet, we use a pair of past and intention memory banks to store the features of past-future instance pairs and a memory addresser to search relevant instances with the new prediction case in the memory bank. The memory bank simulates the pre-frontal cortex in the neural system, which records the human reaction when performing a task. The memory addresser simulates the basal ganglia in the neural system, which ac-tivates the related memory records in the prefrontal cortex.
Fig.1 shows an analogy between the retrospective memory process and our MemoNet process.
The proposed MemoNet includes four key designs. First, we propose a joint-reconstruction-based feature-learning ar-chitecture to initialize the pair of past and intention memory banks. The architecture contains two encoders and follows a joint-reconstruction structure to obtain compatible past trajectory and future intention features. Second, we propose a memory filter algorithm to erase the redundant memory in-stances in the memory banks. The filter algorithm is training-free and invariant to the permutation of training samples, providing high efficiency and robustness for the memory banks. Third, we propose a trainable memory addresser to search similar memory instances. The addresser contains a learnable attention network to compute similarity scores. To train such an addresser, we propose a pseudo-label genera-tion to guide the addresser to correctly search most similar memory instances. Fourth, we propose an intention cluster-ing to produce diverse intention predictions. Through the clustering algorithm, intentions with low-frequency occur-rences are captured to promote the prediction diversity and intentions with high-frequency occurrences are merged to improve the prediction robustness.
We build a two-step trajectory prediction system, where the first step is to leverage MemoNet to predict the intentions and the second step is to fulfill the whole trajectory accord-ing to the predicted intentions. Note that MemoNet only predicts the destination to represent the intention because the destination carries most of the modality information in a trajectory. This two-step prediction disassembles a com-plex problem into two relatively simple problems, promoting a more accurate prediction. To evaluate the effectiveness of our method, we conduct experiments on three datasets:
Stanford Drones (SDD), ETH-UCY and NBA. The quanti-tative result shows we outperform the previous state-of-the-art method 20.3%/10.2%/28.3% on FDE representing we achieve an accurate intention prediction with the MemoNet.
The qualitative results also reflect that our MemoNet has the ability to trace back to specific memorized samples during the prediction, promoting more interpretability.
The main contributions of this paper are:
• We propose MemoNet, a novel instance-based frame-work to achieve future intention prediction. The working mechanism of MemoNet is based on a more explicit link between the current situation and seen instances, imitating retrospective memory studied in neuropsychology.
• We propose four novel designs in MemoNet, including 1) reconstruction-based feature-learning architecture, which initializes the memory banks, 2) memory filtering, which reduces the redundancy in memory banks, 3) memory ad-dresser, which searches similar memory instances with the incoming prediction case in memory banks, and 4) intention clustering, which promotes prediction diversity.
• We conduct experiments to evaluate our method on several real-world datasets. Our approach achieves the state-of-the-art on well-established pedestrian trajectory predic-tion datasets by reducing the FDE 20.3%/10.2%/28.3% on
SDD/ETH-UCY/NBA datasets. Our approach also equips with the ability to trace back to specific memorized instances during the prediction, promoting more interpretability. 2.