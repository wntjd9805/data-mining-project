Abstract
Spatial-Temporal Video Super-Resolution (ST-VSR) aims to generate super-resolved videos with higher resolution (HR) and higher frame rate (HFR). Quite intuitively, pi-oneering two-stage based methods complete ST-VSR by directly combining two sub-tasks: Spatial Video Super-Resolution (S-VSR) and Temporal Video Super-Resolution (T-VSR) but ignore the reciprocal relations among them.
Specifically, 1) T-VSR to S-VSR: temporal correlations help accurate spatial detail representation with more clues; 2)
S-VSR to T-VSR: abundant spatial information contributes to the refinement of temporal prediction. To this end, we propose a one-stage based Cycle-projected Mutual learn-ing network (CycMu-Net) for ST-VSR, which makes full use of spatial-temporal correlations via the mutual learn-ing between S-VSR and T-VSR. Specifically, we propose to exploit the mutual information among them via itera-tive up-and-down projections, where the spatial and tem-poral features are fully fused and distilled, helping the high-quality video reconstruction. Besides extensive exper-iments on benchmark datasets, we also compare our pro-posed CycMu-Net with S-VSR and T-VSR tasks, demonstrat-ing that our method significantly outperforms state-of-the-art methods. Codes are publicly available at: https:
//github.com/hhhhhumengshun/CycMuNet. 1.

Introduction
Spatial-temporal video super-resolution (ST-VSR) aims to produce the high-resolution (HR) and high-frame-rate (HFR) video sequences from the given low-resolution (LR) and low-frame-rate (LFR) input. This task has drawn great attention due to its popular applications [29, 30, 53], includ-ing HR slow-motion generation, movie production, high-definition television upgrades, etc. Great success has been
†Equal Contribution
‡Corresponding Author recently achieved in ST-VSR tasks, as illustrated in Fig-ure 1(a), which can be roughly divided into two cate-gories: two-stage and one-stage based methods. The for-mer decomposes it into two sequential sub-tasks: spatial video super-resolution (S-VSR) and temporal video super-resolution (T-VSR), which are individually completed with image/video super-resolution technologies [19, 51, 58] and video frame interpolation technologies [28, 40]. However, more spatial information generated by the S-VSR task can be used for the refinement of temporal prediction, while more temporal information predicted by the T-VSR task can be used to facilitate the reconstruction of spatial details. As a result, the two-stage based approaches are far from pro-ducing satisfied predictions due to lacking the ability to mu-tually explore the coupled correlations between S-VSR and
T-VSR.
Recently, integrating these two sub-tasks into a unified framework with a one-stage process becomes more pop-ular. Naturally, based on the parallel or serially process-ing modes (Figure 1(b) (i) for parallel process and (ii)(iii) for serial process), diverse and effective schemes have been developed [7, 8, 29, 30, 53, 55]. Unfortunately, the parallel methods [29, 30] barely consider the coupled correlations between the two sub-tasks, while the serial methods [53,55] fail to fully exploit mutual relations since they only focus on the unilateral relationship, such as “T-to-S” or “S-to-T”.
In particular, the unilateral learning will accumulate recon-struction errors, which we define as cross-space (spatial and temporal spaces) errors, consequently leading to obvious aliasing effect in super-resolved results.
For thorough utilization of spatial and temporal infor-mation, we propose to promote the one-stage method with mutual learning, and devise a novel cycle-projected mutual learning network (CycMu-Net) for ST-VSR. As shown in
Figure 1(c), the philosophy of CycMu-Net is to explore the mutual relations and achieve the spatial-temporal fusion to eliminate the cross-space errors. Specifically, the key part of CycMu-Net is the iterative up-and-down projection units between the spatial and temporal embedding spaces, involv-Figure 1. Different schemes for ST-VSR. (a) Two-stage based methods: (i) they perform ST-VSR task by independently using the advanced
S-VSR methods and then T-VSR methods or vice versa (ii). (b) One-stage based method: they unify S-VSR and T-VSR tasks into one model with parallel or cascaded manners without considering the mutual relations between S-VSR and T-VSR. (c) Mutual method: Our method makes full use of the mutual relations via mutual learning between S-VSR and T-VSR. ing a process of aggregating temporal relations to achieve an accurate representation of spatial details, and a feedback refinement of temporal information via the updated spatial prediction. We validate the proposed CycMu-Net on the
ST-VSR task and its two sub-tasks, involving S-VSR and
T-VSR. Experimental results demonstrate that CycMu-Net achieves appealing improvements over the SOTA methods on all tasks. Our contributions are summarized as follows: 1) We propose a novel one-stage based cycle-projected mutual learning network (CycMu-Net) for spatial-temporal video super-resolution, which can make full use of the cou-pled spatial-temporal correlations via mutual learning be-tween S-VSR and T-VSR. 2) To eliminate the cross-space errors and promote the inference accuracy, we devise iterative up-and-down pro-jection units to exploit the mutual information between S-In
VSR and T-VSR for a better spatial-temporal fusion. these units, more spatial information are provided for the refinement of temporal prediction while temporal correla-tions are used to promote texture and detail reconstruction. 3) We conduct extensive experiments on ST-VSR, S-VSR and T-VSR tasks for a comprehensive evaluation, showing that our method performs well against the state-of-the-art methods. 2.