Abstract
Detecting objects from LiDAR point clouds is of tremen-dous signiﬁcance in autonomous driving. In spite of good progress, accurate and reliable 3D detection is yet to be achieved due to the sparsity and irregularity of LiDAR point clouds. Among existing strategies, multi-view methods have shown great promise by leveraging the more comprehen-sive information from both bird’s eye view (BEV) and range view (RV). These multi-view methods either reﬁne the pro-posals predicted from single view via fused features, or fuse the features without considering the global spatial con-text; their performance is limited consequently. In this pa-per, we propose to adaptively fuse multi-view features in a global spatial context via Dual Cross-VIew SpaTial Atten-tion (VISTA). The proposed VISTA is a novel plug-and-play fusion module, wherein the multi-layer perceptron widely adopted in standard attention modules is replaced with a convolutional one. Thanks to the learned attention mecha-nism, VISTA can produce fused features of high quality for prediction of proposals. We decouple the classiﬁcation and regression tasks in VISTA, and an additional constraint of attention variance is applied that enables the attention mod-ule to focus on speciﬁc targets instead of generic points.
We conduct thorough experiments on the benchmarks of nuScenes and Waymo; results conﬁrm the efﬁcacy of our designs. At the time of submission, our method achieves 63.0% in overall mAP and 69.8% in NDS on the nuScenes benchmark, outperforming all published methods by up to 24% in safety-crucial categories such as cyclist. 1.

Introduction
LiDAR is one of the prominent sensors which is widely used in autonomous driving to provide precise 3D informa-* indicates equal contribution.
†Correspondence to Kui Jia <kuijia@scut.edu.cn>.
Figure 1. Comparison between the single-view detection and the proposed VISTA-based multi-view fusion. (a) shows the single-view detection pipeline. (b) illustrates the proposed VISTA-based multi-view fusion. The BEV and RV features which are extracted by a shared 3D backbone are passed into individual necks, and pass through the VISTA to output high quality fused features. tion of the objects. Therefore, LiDAR based 3D object de-tection has attracted a lot of attention. Many 3D object de-tection algorithms [19, 29, 32] apply the convolutional neu-ral networks to 3D point clouds by voxelizing the unordered and irregular point clouds into volumetric grids. Neverthe-less, the 3D convolutional operator is computationally inef-ﬁcient and memory-consuming. To mitigate these issues, a line of works [6, 28] utilize sparse 3D convolutions [9–11] in the network backbones to extract features. As illustrated in the Figure 1 (a), these works project the 3D feature maps into the bird’s eye view (BEV) or range view (RV), and the object proposals are produced from these 2D feature maps using proposal methods.
Different views have their own advantages and draw-backs to consider.
In BEV, objects do not overlap with each other and the size of each object is invariant to the distance from the ego-vehicle. RV is the native represen-tation of LiDAR point clouds, therefore, it can produce compact and dense features. However, projection would inevitably impair the integrity of spatial information con-veyed in the 3D space no matter which of BEV or RV is chosen. For example, due to the self-occlusion and the
characteristic of LiDAR data generation, the BEV repre-sentation is extremely sparse and it consolidates the height information of 3D point clouds, occlusion and variation in object size would be severer in RV since it loses depth in-formation. Obviously, learning jointly from multiple views, a.k.a multi-view fusion provides us a solution to accurate 3D object detection. Some of the previous multi-view fu-sion algorithms [7, 17] produce the proposals from a single view and utilize the multi-view features to reﬁne propos-als. Performances of such algorithms highly depend on the quality of the produced proposals; however, proposals gen-erated from a single view make no use of all the available in-formation, possibly leading to suboptimal solutions. Other works [5, 31] fuse the multi-view features according to the coordinate projection between different views. Accuracy of such fusion methods relies on the complementary informa-tion provided in the corresponding region of the other view; yet the occlusion effect is inevitable, inducing low-quality multi-view feature fusion.
To boost the performances of 3D object detection, in this paper, given learned 3D feature maps from both BEV and
RV, we propose to produce high quality fused multi-view features from the global spatial context via Dual Cross-VIew SpaTial Attention (VISTA) for proposal prediction, as demonstrated in Figure 1 (b). The proposed VISTA uti-lizes the attention mechanism originated in the transformer which is successfully applied to various research context (e.g. natural language processing, 2D computer vision).
Compared with direct fusion via coordinate projections, the inbuilt attention mechanism in VISTA exploits the global information and adaptively models all the pairwise correla-tions across views by treating features of individual views as sequences of feature elements. To model the cross-view correlations comprehensively, the local context in both views must be taken into account, thus we replace the MLPs in the conventional attention module with the convolutional operators, of which we show the effectiveness in the Sec-tion 6. Nevertheless, learning the correlations across views is still challenging, as shown in Section 6. Directly adopt-ing the attention mechanism for multi-view fusion brings little gains and thus, we argue that it is mainly due to the characteristic of the task 3D object detection itself.
Generally, the 3D object detection task could be divided into two sub-tasks: classiﬁcation and regression. As elab-orated in [5, 22], the 3D object detector faces many chal-lenges when detecting objects in the whole 3D scenes, such as occlusion, background noise and the scarce texture in-formation of point cloud.
In consequence, the attention correlations are difﬁcult to learn and the attention module tends to learn the mean of the whole scene, which is un-expected as the attention module is designed for paying at-tention to regions of interest. Therefore, we explicitly con-strain the variance of the attention maps learned by the at-tention mechanism, which guides the attention module to be aware of the meaningful regions in the complex 3D outdoor scenes. Moreover, different learning targets for classiﬁca-tion and regression determine the different expectations of the learned queries and keys in the attention module. The various regression targets (e.g. scale, translate) across dif-ferent objects expect the queries and keys to be aware of the characteristic of the objects. The classiﬁcation task instead, pushes the network to understand the common properties of the object classes. Inevitably, sharing the same attention modeling will bring conﬂicts into the training of these two tasks. Furthermore, on one hand, due to the loss of tex-ture information, it is difﬁcult for neural networks to extract semantic features from point clouds. On the other hand, the neural networks can easily learn the geometric property of objects from point clouds. As a result, during training, a dilemma that the classiﬁcation being dominant by the re-gression is aroused. To tackle these challenges, we decouple these two tasks in the proposed VISTA to learn to aggregate different cues in terms of different tasks.
Our proposed VISTA is a plug-and-play module and can be adopted to the recent advanced target assign strategies.
We test our proposed VISTA-based multi-view fusion on different target assign algorithms on the benchmark datasets of nuScenes [2] and Waymo [25]. Ablation studies on their validation sets conﬁrm our conjecture. Thanks to the high quality fused features produced by the proposed VISTA, our proposed method outperforms all the published algorithms.
At the time of submission, our ﬁnal results achieve 63.0% in overall mAP and 69.8% in NDS on nuScenes leaderboard.
On Waymo Open Dataset, we achieve 74.0%, 72.5%, and 71.6% level 2 mAPH on vehicle, pedestrian and cyclist. We summarize our main contributions as follows.
• We propose a novel plug-and-play fusion module
Dual Cross-VIew SpaTial Attention (VISTA) to pro-duce well-fused multi-view features to boost the per-formances of 3D object detector. Our proposed
VISTA replaces the MLPs with convolutional opera-tors, which is capable of better handling the local cues for attention modeling.
• We decouple the regression and classiﬁcation tasks in the VISTA to leverage individual attention modeling to balance the learning of these two tasks. We apply the attention variance constraint to VISTA during training phase, which facilitate the attention learning and em-power the network to attend to the regions of interest.
• We conduct thorough experiments on the benchmark datasets of nuScenes and Waymo. Our proposed
VISTA-based multi-view fusion can be adopted in var-ious advanced target assign strategies, easily boost the original algorithms and achieve state-of-the-art perfor-mances on the benchmark datasets. Speciﬁcally, our
proposed method outperforms the second best meth-ods by 4.5% in overall performance, and up to 24% on the safety-crucial object categories like cyclist. 2.