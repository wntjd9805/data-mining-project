Abstract
We introduce Neural Point Light Fields that represent scenes implicitly with a light ﬁeld living on a sparse point cloud. Combining differentiable volume rendering with learned implicit density representations has made it pos-sible to synthesize photo-realistic images for novel views of small scenes. As neural volumetric rendering methods require dense sampling of the underlying functional scene representation, at hundreds of samples along a ray cast through the volume, they are fundamentally limited to small scenes with the same objects projected to hundreds of train-ing views. Promoting sparse point clouds to neural implicit light ﬁelds allows us to represent large scenes effectively with only a single radiance evaluation per ray. These point light ﬁelds are as a function of the ray direction, and local point feature neighborhood, allowing us to interpolate the light ﬁeld conditioned training images without dense object coverage and parallax. We assess the proposed method for novel view synthesis on large driving scenarios, where we synthesize realistic unseen views that existing implicit ap-proaches fail to represent. We validate that Neural Point
Light Fields make it possible to predict videos along unseen trajectories previously only feasible to generate by explic-itly modeling the scene. 1.

Introduction
Learning implicit volumetric scene representations has made it possible to synthesize photo-realistic images of sin-gle scenes [20, 24, 27, 39]. The most successful meth-ods combine a conventional volumetric rendering approach with a coordinate-based neural network that predicts den-sity and radiance [24]. As such, instead of explicitly stor-ing density and radiance in a high-dimensional 5D volume, these methods represent this volume as a learned function, that can be further decomposed into radiance and illumina-tion [53, 40, 5]. Although the implicit volumetric represen-tation is highly memory-efﬁcient and differentiable, it also fundamentally requires sampling the volume, that is evalu-ating the coordinate-based network, hundreds of times for each ray for a given pixel. This mandates long training and small volumetric support inside the volume.
Figure 1: Neural Point Light Fields encode the information of a
Light Field representation of a scene on a point cloud capture. An image is rendered for each camera ray based on the local encoding of the Light Field on relevant points.
To tackle these challenges, hybrid representations [13, 19, 15] are used to embed or “bake” local radiance func-tions on explicit sparse proxy representations such as coarse voxel grids, point clouds or meshes to enable faster render-ing by ignoring empty space. While this approach drasti-cally improves rendering speed at test time, it still requires volumetric sampling during training. This is because the scene geometry must be learned during the training pro-cess. These methods share the limitations of volumetric approaches during training and, as such, have also been lim-ited to small scenes that are costly to train. Learning repre-sentations for large outdoor scenes is an open challenge.
Unfortunately, approaches that are free of implicit rep-resentations do not yet offer an alternative. Speciﬁcally, explicitly storing features on proxy geometry [34, 33, 17] has not been able to achieve the same quality as volumetric methods when interpolating a view without a nearby train-ing sample. Existing formulations utilize geometry as a pro-jection canvas combined with features extracted from target views, and therefore require a large number of input images near the target view.
In this work, we depart from volumetric models and in-troduce Neural Point Light Fields, a local implicit repre-sentation that encodes a light ﬁeld on a point cloud. The proposed representation supports novel view synthesis in
large outdoor scenes without strong parallax needed as in volumetric methods. Although recent automotive depth es-timation networks make it possible to estimate dense depth point clouds from video data, we assume measured lidar point clouds as input to our method, especially as lidar data is readily available in most outdoor vehicle datasets [42, 10] and recently released smartphones. Although sparse, the li-dar geometry provides enough cues to encode a local light
ﬁeld on the point cloud. Instead of a 5D volumetric radiance function, or a conventional 4D light ﬁeld [18], we propose to formulate a light ﬁeld only depending on the two dimen-sional ray direction and a one dimensional index pointing to a point cloud featureThis formulation makes it possible to evaluate a single radiance prediction per ray.
We extract features for each point with a learned feature extractor on point cloud projections [11]. For a given cam-era pose, we shoot rays for each pixel and select a set of close points inside the point cloud. The features from these selected points are then weighted by passing the points rel-ative position to the ray and features through an attention module, resulting in a single ray feature code. The color for each ray is then reconstructed by an implicit light ﬁeld representation conditioned by this feature code. We assess the proposed method on a large-scale automotive driving dataset [42] and demonstrate novel view synthesis along un-seen trajectories with quality unseen before.
Speciﬁcally, we make the following contributions
• We introduce Neural Point Light Fields, a representa-tion that implicitly encodes features in a point cloud, requiring only a single radiance evaluation per ray.
• The proposed method lifts the restrictions of volumet-ric scene representations by exploiting sparse geome-try available in estimated or captured point clouds.
• We validate the proposed method on novel video syn-thesis tasks for large-scale driving scenes, demonstrat-ing the proposed method’s capability of generating re-alistic novel views along trajectories which cannot be handled by existing implicit representation methods.
Our code and trained models are available on our website: https : / / light . princeton . edu / neural - point -light-fields
Scope Even though existing automotive datasets include data from multiple cameras, lidar and radar sensors, we focus on learning from a single camera with a single tra-jectory per scene, and without highly dynamic scene mo-tion. In contrast to densely observing the scene across a full hemisphere [24], captured images in our case are sparsely distributed along the driving trajectory We note that ex-tending training to multiple camera views is not straightfor-ward, as camera poses, exposure and tone-mapping differ-ences have to be accounted for. Exploiting multiple cameras and adding dynamic object support to the proposed method could constitute exciting future directions. 2.