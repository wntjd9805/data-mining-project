Abstract
Energy-based latent variable models (EBLVMs) are more expressive than conventional energy-based models.
However, its potential on visual tasks are limited by its training process based on maximum likelihood estimate that requires sampling from two intractable distributions.
In this paper, we propose Bi-level doubly variational learn-ing (BiDVL), which is based on a new bi-level optimiza-tion framework and two tractable variational distributions to facilitate learning EBLVMs. Particularly, we lead a de-coupled EBLVM consisting of a marginal energy-based dis-tribution and a structural posterior to handle the difﬁculties when learning deep EBLVMs on images. By choosing a symmetric KL divergence in the lower level of our frame-work, a compact BiDVL for visual tasks can be obtained.
Our model achieves impressive image generation perfor-mance over related works. It also demonstrates the signif-icant capacity of testing image reconstruction and out-of-distribution detection. 1.

Introduction
Energy-based models (EBMs) [24] are known as pow-erful generative models widely studied in the ﬁeld of ma-chine learning, which deﬁne a general distribution explic-itly by normalizing exponential negative energy. EBMs have also been successfully applied to solve visual tasks, such as image synthesis [6], classiﬁcation [12], out-of-distribution (OOD) detection [30] or semi-supervised learn-ing [7]. Further, latent variables are incorporated to deﬁne
*Corresponding Author energy-based latent variable models (EBLVMs), which are more expressive and capable of representation learning [4].
However, the effectiveness of EBLVM deteriorates when applying it to solve computer vision problems, because learning by maximum likelihood estimation (MLE) usually suffers from the doubly intractable model problem [42], i.e., sampling from the posterior and the joint distribu-tion of model is nontrivial due to the intractable integrals in normalizing denominator. Recent works [6, 35] lever-age gradient-based Markov Chain Monte Carlo (MCMC), such as Langevin dynamics, to approximately sample from
EBMs but require lots of steps, since fewer steps may lead to arbitrarily far sampling distribution from the tar-get. Moreover, to handle the doubly intractable problem of EBLVMs, double MCMC sampling are needed and as a result making it infeasible on high-dimensional images.
To accomplish EBLVMs efﬁciently, this paper intro-duces a Bi-level Doubly Variational Learning (BiDVL) framework, which is based on tractable variational poste-rior and variational joint distribution to estimate the gradi-ents in MLE. We formulate BiDVL as a bi-level optimiza-tion (BLO) problem as illustrated in Fig. 1. In speciﬁc, the gradient estimate is compelled to ﬁt the real one by explor-ing variational distributions to achieve the model distribu-tions in the lower level, and the resulting gradient estimate is then used for optimizing the objective in the upper level.
Theoretically, BiDVL is equivalent to the original MLE ob-jective under the nonparametric assumption [10], while to optimize it practically, we propose an efﬁcient alternative optimization scheme.
Moreover, the drawbacks of doubly variational learning come from its unstableness when learning a deep EBLVM on images. For example, AdVIL [25] that can be approx-on images. To overcome these, we deﬁne a decoupled
EBLVM and choose a symmetric KL divergence in the lower level. The resulting compact objective implies the consistency for learning EBLVMs. 3) We demonstrate the impressive image generation quality among baseline models. Besides, BiDVL also shows the remarkable performance of testing image recon-struction and OOD detection. 2.