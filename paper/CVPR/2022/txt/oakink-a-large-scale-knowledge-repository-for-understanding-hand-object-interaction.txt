Abstract
Learning how humans manipulate objects requires ma-chines to acquire knowledge from two perspectives: one for understanding object affordances and the other for learn-ing human’s interactions based on the affordances. Even though these two knowledge bases are crucial, we find that current databases lack a comprehensive awareness of them.
In this work, we propose a multi-modal and rich-annotated knowledge repository, OakInk, for visual and cognitive un-derstanding of hand-object interactions. We start to collect 1,800 common household objects and annotate their affor-dances to construct the first knowledge base: Oak. Given the affordance, we record rich human interactions with 100 selected objects in Oak. Finally, we transfer the interac-tions on the 100 recorded objects to their virtual counter-parts through a novel method: Tink. The recorded and transferred hand-object interactions constitute the second knowledge base: Ink. As a result, OakInk contains 50,000 distinct affordance-aware and intent-oriented hand-object interactions. We benchmark OakInk on pose estimation and grasp generation tasks. Moreover, we propose two practical applications of OakInk: intent-based interaction generation and handover generation. Our dataset and source code are publicly available at www.oakink.net. 1.

Introduction
Enabling a machine to understand and imitate the behav-ior of humans has been a long-term vision in the history of science. Among the tasks derived from it, learning how hu-mans manipulate objects is a fundamental challenging one.
As most tools are designed for achieving function, human can easily learn to manipulate them through instruction or experiences. However, these experiences are hard for a ma-chine to acquire. It was not until recently that data-driven approaches have begun to promote research on learning hu-⋆Equal contribution.
†Cewu Lu is the corresponding author. He is the member of Qing Yuan
Research Institute and MoE Key Lab of Artificial Intelligence, AI Institute,
Shanghai Jiao Tong University, and Shanghai Qi Zhi institute, China.
Figure 1. Illustration of different data modalities in OakInk reposi-tory. The left column shows human manipulating 3 source objects (mug, camera, and headphones). The right 5 columns show the transferred interactions on 15 virtual counterpart objects. man manipulation [2, 15, 20, 35, 43, 59]. Prior work has tried to empower a machine complex skills such as hand-object localization [45], pose estimation [29], grasp gener-ation [11], and action imitation [41].
Two fundamental components for learning human ma-nipulation are 1) the affordance of the objects and 2) how human hand would interact with the objects based on those affordances. While the word “affordance” has different for-mulations in different tasks, in this paper, we denote “af-fordance” as the functionality of object. Since 2019, there have been at least 9 datasets of hand-object interaction re-leased: ObMan [23], YCBAfford [11], HO3D [19], Con-tactPose [5], GRAB [51], DexYCB [10], two H2O [28, 57] and DexMV [41]. However, these datasets lack compre-hensive awareness of the object’s affordance and the hand’s interactions with it. First, existing real-world datasets only contain a small number of objects and hand interactions.
As two illustrative examples, only 20 objects were captured in DexYCB, and only 2.3K distinct interactions were cap-tured among 2.9M images in ContactPose (0.08%). Sec-ond, even if synthetic dataset [23] can extend to large num-bers of interactions in grasp simulator: GraspIt [33], the generated grasps neither reflect the distribution of human interactions nor consider the object’s affordance itself. To
understand how humans manipulate objects, we propose to build the machine’s knowledge from two perspectives: object-centric and human-centric perspective. To this end, we construct two interrelated knowledge bases. One is an
Object Affordance Knowledge base (Oak base, Sec. 3.1) in which we provide comprehensive descriptions of objects’ affordances within a knowledge graph, and the other is an
Interaction Knowledge base (Ink base, Sec. 3.2) in which we collect diverse human hand interactions that provide demonstrations of manipulating the object according to its affordances.
To construct the Oak base, we firstly collect 1,800 house-hold objects that are designed for single-hand manipulation.
The sources of objects in Oak base are four-fold: 1) self-collected from online vendors, 2) ShapeNet [9] models, 3)
YCB [6] and 4) ContactDB [3] objects. Second, through exhaustively reviewing the objects in the above sources, we build an object knowledge graph that arranges objects with two types of abstractions, namely taxonomy and attribute (Fig. 2). This object knowledge graph enables us to make a quick extension for new objects and conduct convenient clustering for objects of similar affordance.
To construct the Ink base, we start to collect human ex-periences on performing hand-object interactions based on the object’s affordance. We select 100 representative ob-jects from Oak base, invite 12 human subjects to perform demonstrations, and set up a multi-sensor MoCap platform for recording (Fig. 3). The recorded sequences constitute a real-world image dataset that contains 230,064 RGB-D frames capturing 12 subjects performing up to 5 intent-oriented hand interactions with objects in a pool of total 100 instances from 32 categories. The objects that appeared in the recorded sequences are denoted as the “source” objects.
Next, given the real-world human demonstration, we aim to transfer their experience on the source object to its vir-tual counterparts with similar affordances (target objects).
The transferred hand interaction should not only ensure its physical plausibility, but also keep the consistent intent and match the size, shape, and affordance of the target object (Fig. 1). To this end, we propose a learning-fitting hybrid method: Tink for Transferring the Interaction Knowledge among objects (Sec. 3.3). Tink consists of three modules: namely an implicit shape interpolation, an explicit contact mapping, and an iterative pose refinement. With Tink, we extend the total number of distinct hand-object interactions in Ink base to 50,000.
Through combining the above two knowledge bases:
Oak and Ink, we construct a large-scale knowledge reposi-tory: OakInk. The advantages of our OakInk are three-fold: 1) It provides comprehensive knowledge for understand-ing hand-object interactions from two perspectives: ob-ject affordances and human experiences; 2) It contains two large-scale datasets of image-based and geometry-based hand-object interaction; 3) It provides rich annotations in-cluding hand and object poses, scanned object models, af-fordances, fine-grained contact and stress patterns, and in-tents labels. OakInk can benefit researches in two com-munities: 1) pose estimation [13, 21, 30], shape reconstruc-tion [23,25], and action recognition [15,28,52] in computer vision, CV; 2) grasp generation [24,51,59] and motion syn-thesis [7, 38] in computer graphics, CG; Among all the top-ics above, we find pose estimation and pose generation are most relevant to our interests. In this paper, we benchmark
OakInk on three existing tasks and propose two new tasks:
One is an intent-based hand pose generation and the other is a human-to-human handover generation.
Our contributions are concluded in three-fold. First, we construct OakInk, a large-scale knowledge repository for understanding hand-object interactions. Second, inside
OakInk, we propose a novel method Tink that transfers the interaction knowledge among objects with similar affor-dance. Finally, we provide extensive evaluations for bench-marking OakInk on three existing tasks and propose two novel tasks: generating plausible hand poses for more cus-tomized purposes. 2.