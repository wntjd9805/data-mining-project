Abstract
Face models are widely used in image processing and other domains. The input data to create a 3D face model ranges from accurate laser scans to simple 2D RGB pho-tographs. These input data types are typically deficient either due to missing regions, or because they are under-constrained. As a result, reconstruction methods include embedded priors encoding the valid domain of faces. Sys-tem designers must choose a source of input data and then choose a reconstruction method to obtain a usable 3D face.
If a particular application domain requires accuracy X, which kinds of input data are suitable? Does the input data need to be 3D, or will 2D data suffice? This paper takes a step toward answering these questions using synthetic data.
A ground truth dataset is used to analyze accuracy obtain-able from 2D landmarks, 3D landmarks, low quality 3D, high quality 3D, texture color, normals, dense 2D image data, and when regions of the face are missing. Since the data is synthetic it can be analyzed both with and with-out measurement error. This idealized synthetic analysis is then compared to real results from several methods for con-structing 3D faces from 2D photographs. The experimental results suggest that accuracy is severely limited when only 2D raw input data exists. 1.

Introduction
Three dimensional face models are used in a diverse set of application domains, including biometric identifica-tion [10], 3D avatars [54], social media filters, and photo editing [46]. All of these applications benefit from high ac-curacy 3D models, however the degree of accuracy required varies between a realistically rendered movie and a fun mo-bile app with cartoon ears.
The input data to create a 3D face model has a wide vari-ation, ranging from 0.1 mm accurate laser scans to simple 2D RGB photographs. Raw data is usually not directly us-able, whether due to holes in scanned meshes or because 2D data is insufficiently constrained to determine 3D. A com-mon solution is to apply a model to constrain or clean up the raw data. The model can take many forms, for example it could be an explicit prior applied to the distribution of a parameter space, or implied by a specific neural network ar-chitecture and the trained node weights. In either case the goal is to best use the limited input data to predict a valid and complete 3D face.
System designers must choose a source of input data and then choose a model and a method of fitting to obtain a us-able 3D face. The vast majority of research has focused on novel sensor designs to improve the raw data, or novel re-construction methods and prior models which do a better job producing faces from under-constrained data. In con-trast, this paper seeks to answer the system designers ques-tion - If my application domain requires accuracy X, how good does my input data need to be? Must I start from 3D data, or can I start from a photograph and predict a 3D model which is good enough? If my user is wearing glasses and my data is missing eyes, can I predict something based on observation of the rest of the face? What about if there is a mask blocking the mouth region? How much accuracy will that cost me? In order to provide insight into questions of this kind, we start with an existing ground truth dataset, use an existing prior model, and then evaluate the accuracy obtainable from different raw data types.
Our input data is derived from a publicly available dataset which has 100s of 3D face scans in correspondence, as well as multi-view 2D images of the same subjects [47].
This allows us to compare final 3D accuracy when raw in-put data is 2D feature points, 2D photographs, low quality 3D, 3D feature points, high quality 3D with partial missing data, and complete high quality 3D.
Our primary analysis makes use of “synthetic” experi-ments. This allows extensive analysis both with and with-out noise. Input data is constructed directly from ground truth by removing data to mimic measurement of only land-marks, or only low resolution 3D, or missing eyes. These experiments test whether the prior model can predict accu-rate results for the unobserved portion of the face. We check that the synthetic results generalize by conducting “real” ex-periments using only RGB photographs as input and testing
the behavior of several published methods on this very lim-ited input data type.
Hundreds of papers exist proposing prior models rang-ing from simple interpolation to modern deep learning. Re-construction accuracy is necessarily tied to the prior model chosen, and this year’s state of the art will not be as good as next year’s. We do not attempt to provide an indication of the absolute accuracy obtainable, but rather provide a com-parison of error magnitudes when starting from different raw inputs. For our experiments, we choose a Morphable
Face Model because it is one of the most widely used mod-els, it has existed for 20 years [9], surveys exist [18], and it has some mathematical similarities to blendshape mod-els that are industry standard in animation [27]. This model is a simple linear system constructed by finding the princi-pal components of a training set, and fitting data to a linear sum of these components. Since this is perhaps the sim-plest possible model (linear), it is well understood by many researchers, and thus we hope allows easy interpretation of our results.
The contribution of this paper is a careful analysis of 3D facial reconstruction accuracy when starting from input data with various levels of completeness. 2.