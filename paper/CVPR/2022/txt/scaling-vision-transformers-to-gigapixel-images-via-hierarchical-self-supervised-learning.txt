Abstract
Vision Transformers (ViTs) and their multi-scale and hi-erarchical variations have been successful at capturing im-age representations but their use has been generally stud-ied for low-resolution images (e.g. 256 × 256, 384 × 384).
For gigapixel whole-slide imaging (WSI) in computational pathology, WSIs can be as large as 150000 × 150000 pixels at 20× magnification and exhibit a hierarchical structure of visual tokens across varying resolutions: from 16 × 16 im-ages capturing individual cells, to 4096×4096 images char-acterizing interactions within the tissue microenvironment.
We introduce a new ViT architecture called the Hierarchical
Image Pyramid Transformer (HIPT), which leverages the natural hierarchical structure inherent in WSIs using two levels of self-supervised learning to learn high-resolution image representations. HIPT is pretrained across 33 can-cer types using 10,678 gigapixel WSIs, 408,218 4096×4096 images, and 104M 256 × 256 images. We benchmark HIPT representations on 9 slide-level tasks, and demonstrate that: 1) HIPT with hierarchical pretraining outperforms current state-of-the-art methods for cancer subtyping and survival prediction, 2) self-supervised ViTs are able to model im-portant inductive biases about the hierarchical structure of phenotypes in the tumor microenvironment. 1.

Introduction
Tissue phenotyping is a fundamental problem in com-putational pathology (CPATH) that aims at characterizing objective, histopathologic features within gigapixel whole-slide images (WSIs) for cancer diagnosis, prognosis, and the estimation of response-to-treatment in patients [39, 41, 54]. Unlike natural images, whole-slide imaging is a chal-lenging computer vision domain in which image resolu-tions can be as large as 150000 × 150000 pixels, with many methods using the following three-stage, weakly-supervised framework based on multiple instance learning
∗ Contributed Equally.
Figure 1. Hierarchical Structure of Whole-Slide Images (WSIs). Left. Unlike natural images, since WSIs have a fixed scale, there exists a hierarchical structure of visual tokens at vary-ing image resolutions. Right. In addition to formulating a single 256 × 256 image as as sequence of 256 [16 × 16] tokens, we can also view these 256 × 256 image as being part of a larger, disjoint sequence of [256 × 256] tokens in a 4096 × 4096 region. (MIL): 1) tissue patching at a single magnification objec-tive (“zoom”), 2) patch-level feature extraction to construct a sequence of embedding instances, and 3) global pooling of instances to construct a slide-level representation for weak-supervision using slide-level labels (e.g. - subtype, grade, stage, survival, origin) [12, 19, 37, 38, 52, 53, 68, 70, 85].
Though achieving “clinical-grade” performance on many cancer subtyping and grading tasks, this three-stage process has a few important design limitations. First, patch-ing and feature extraction are generally fixed to [256 × 256] context regions. Though able to discern fine-grained mor-phological features such as nuclear atypia or tumor pres-[256 × 256] win-ence, depending on the cancer type, dows have limited context in capturing coarser-grained fea-tures such as tumor invasion, tumor size, lymphocytic infil-trates, and the broader spatial organization of these pheno-types in the tissue microenvironment, as depicted in Figure 1 [6,15,22]. Second, in contrast with other image-based se-quence modeling approaches such as Vision Transformers (ViTs), MIL uses only global pooling operators due to the large sequence lengths of WSIs [38]. As a result, this lim-itation precludes the application of Transformer attention for learning long-range dependencies between phenotypes such as tumor-immune localization, an important prognos-tic feature in survival prediction [1, 44, 63]. Lastly, though recent MIL approaches have adopted self-supervised learn-ing as a strategy for patch-level feature extraction (called tokenization in ViT literature), parameters in the aggrega-tion layers still require training [8, 16, 18, 20, 43, 45, 62]. In viewing patch-based sequence modeling of WSIs in rela-tion to ViTs, we note that the architectural design choice of using Transformer attention enables pretraining of both the tokenization and aggregation layers in ViT models, which is important in preventing MIL models from over- or under-fitting in low-data regimes [5, 13, 23, 33, 46].
To address these issues, we explore the challenge of de-veloping a Vision Transformer for slide-level representation learning in WSIs. In comparison to natural images which are actively explored by ViTs, we note a key difference in modeling WSIs is that visual tokens would always be at a fixed scale for a given magnification objective. For instance, scanning WSIs at a 20× objective results in a fixed scale of approximately 0.5µm per pixel, allowing for consistent comparison of visual elements that may elucidate important histomorphological features beyond their normal reference ranges. Moreover, WSIs also exhibit a hierarchical struc-ture of visual tokens at varying image resolutions at 20× magnification: the 16 × 16 images encompass the bound-ing box of cells and other fine-grained features (stroma, tu-mor cells, lymphocytes) [22, 36], 256 × 256 images cap-ture local clusters of cell-to-cell interactions (tumor cel-lularity) [2, 7, 30, 59], 1024 × 1024-4096 × 4096 images further characterize macro-scale interactions between clus-ters of cells and their organization in tissue (the extent of tumor-immune localization in describing tumor-infiltrating versus tumor-distal lymphocytes) [1,9], and finally the over-all intra-tumoral heterogeneity of the tissue microenviron-ment depicted at the slide-level of the WSI [4,35,39,57,63].
The hypothesis that this work tests is that the judicious use of this hierarchy in self-supervised learning results in better slide-level representations.
We introduce a Transformer-based architecture for hier-archical aggregation of visual tokens and pretraining in gi-gapixel pathology images, called Hierarchical Image Pyra-mid Transformer (HIPT). We approach the task of slide-level representation learning in a manner similar to learn-ing long document representations in language modeling, in which we develop a three-stage hierarchical architecture that performs bottom-up aggregation from [16 × 16] visual tokens in their respective 256 × 256 and 4096 × 4096 win-dows to eventually form the slide-level representation, as demonstrated in Figure 2 [76, 82]. Our work pushes the boundaries of both Vision Transformers and self-supervised learning in two important ways. By modeling WSIs as a disjoint set of nested sequences, within HIPT: 1) we de-compose the problem of learning a good representation of a WSI into hierarchically-related representations each of which can be learned via self-supervised learning, and 2) we use student-teacher knowledge distillation (DINO [13]) to pretrain each aggregation layers with self-supervised learn-ing on regions as large as 4096 × 4096.
We apply HIPT to the task of learning representations of gigapixel histopathological images extracted at 20× res-olution. We show that our method achieves superior per-formance to conventional MIL approaches. The difference is pronounced in context-aware tasks such as survival pre-diction in which larger context is appreciated in charac-terizing broader prognostic features in the tissue microen-vironment [1, 17, 60, 63]. Using K-Nearest Neighbors on the 4096 × 4096 representations of our model, we outper-form several weakly-supervised architectures in slide-level classification – an important step forward in achieving self-supervised slide-level representations. Finally, akin to self-supervised ViTs on natural images that can perform seman-tic segmentation of the scene layout, we find that the multi-head self-attention in self-supervised ViTs learn visual con-cepts in histopathology tissue (from fine-grained visual con-cepts such as cell locations in the ViT256-16 to coarse-grained visual concepts such as broader tumor cellularity in the ViT4096-256), as demonstrated in Figure 3, 4. We make code available at https://github.com/mahmoodlab/HIPT. 2.