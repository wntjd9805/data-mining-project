Abstract
A range of video modeling tasks, from optical ﬂow to multiple object tracking, share the same fundamental chal-lenge: establishing space-time correspondence. Yet, ap-proaches that dominate each space differ. We take a step to-wards bridging this gap by extending the recent contrastive random walk formulation to much denser, pixel-level space-time graphs. The main contribution is introducing hier-archy into the search problem by computing the transi-tion matrix between two frames in a coarse-to-ﬁne man-ner, forming a multiscale contrastive random walk when ex-tended in time. This establishes a uniﬁed technique for self-supervised learning of optical ﬂow, keypoint tracking, and video object segmentation. Experiments demonstrate that, for each of these tasks, the uniﬁed model achieves perfor-mance competitive with strong self-supervised approaches speciﬁc to that task.1 1.

Introduction
Temporal correspondence underlies a range of video understanding tasks, from optical ﬂow to object tracking.
At the core, the challenge is to estimate the motion of some entity as it persists in the world, by searching in space and time. For historical reasons, the practicalities differ substantially across tasks: optical ﬂow aims for dense correspondences but only between neighboring pairs of frames, whereas tracking cares about longer-range correspondences but is spatially sparse. We argue that the time might be right to try and re-unify these different takes on temporal correspondence.
An emerging line of work in self-supervised learning has shown that generic representations pretrained on unlabeled images and video can lead to strong performance across a range of tracking tasks [20, 26, 35, 75, 77]. The key idea is that if tracking can be formulated as label propagation [86] on a space-time graph, all that is needed is a good mea-sure of similarity between nodes. Indeed, the recent con-trastive random walk (CRW) formulation [26] shows how such a similarity measure can be learned for temporal cor-1Project page at https : / / jasonbian97 . github . io / flowwalk respondence problems, suggesting a path towards a uniﬁed solution. However, scaling this perspective to pixel-level space-time graphs holds challenges. Since computing sim-ilarity between frames is quadratic in the number of nodes, estimating dense motion is prohibitively expensive. More-over, there is no way of explicitly estimating the motion in ambiguous cases, like occlusion. In parallel, the unsuper-vised optical ﬂow community has adopted highly effective methods for dense matching [83], which use multiscale rep-resentations [9, 45, 61, 84] to reduce the large search space, and smoothness priors to deal with ambiguity and occlu-sion. But, in contrast to the self-supervised tracking meth-ods, they rely on hand-crafted distance functions, such as the Census Transform [32, 47]. Furthermore, because they focus on producing point estimates of motion, they may be less robust under long-term dynamics.
In this work, we take a step toward bridging the gap be-tween tracking and optical ﬂow by extending the contrastive random walk formulation [26] to much denser, pixel-level space-time graphs. The main contribution is introducing hi-the multiscale con-erarchy into the search problem, i.e. trastive random walk. By integrating local attention in a coarse-to-ﬁne manner, the model can efﬁciently consider a distribution over pixel-level trajectories. Through ex-periments across optical ﬂow and video label propagation benchmarks, we show:
• This provides a uniﬁed technique for self-supervised op-tical ﬂow, pose tracking, and video object segmentation.
• For optical ﬂow, the model is competitive with many re-cent unsupervised optical ﬂow methods, despite using a novel loss function (without hand-crafted features).
• For tracking, the model outperforms existing self-supervised approaches on pose tracking, and is compet-itive on video object segmentation.
• Cycle-consistency provides a complementary learning signal to photo-consistency.
• Multi-frame training improves two-frame performance. 2.