Abstract
Recently, self-attention mechanisms have shown impres-sive performance in various NLP and CV tasks, which can help capture sequential characteristics and derive global information. In this work, we explore how to extend self-attention modules to better learn subtle feature embed-dings for recognizing fine-grained objects, e.g., different bird species or person identities. To this end, we propose a dual cross-attention learning (DCAL) algorithm to co-ordinate with self-attention learning. First, we propose global-local cross-attention (GLCA) to enhance the inter-actions between global images and local high-response re-gions, which can help reinforce the spatial-wise discrim-inative clues for recognition. Second, we propose pair-wise cross-attention (PWCA) to establish the interactions between image pairs. PWCA can regularize the attention learning of an image by treating another image as dis-tractor and will be removed during inference. We observe that DCAL can reduce misleading attentions and diffuse the attention response to discover more complementary parts for recognition. We conduct extensive evaluations on fine-grained visual categorization and object re-identification.
Experiments demonstrate that DCAL performs on par with state-of-the-art methods and consistently improves multiple self-attention baselines, e.g., surpassing DeiT-Tiny and ViT-Base by 2.8% and 2.4% mAP on MSMT17, respectively. 1.

Introduction
Self-attention is an attention mechanism that can relate different positions of a single sequence and draw global dependencies. It is originally applied in natural language processing (NLP) tasks [10, 46] and exhibits the outstand-ing performance. Recently, Transformer with self-attention learning has also been explored for various vision tasks (e.g., image classification [5, 12, 19, 37, 45, 51] and object detection [2, 68]) as an alternative of convolutional neu-*Equal contribution. ral network (CNN). For general image classification, self-attention has been proved to work well for recognizing 2D images by viewing image patches as words and flattening them as sequences [12, 45].
In this work, we investigate how to extend self-attention modules to better learn subtle feature embeddings for rec-ognizing fine-grained objects, e.g., different bird species or person identities. Fine-grained recognition is more chal-lenging than general image classification owing to the sub-tle visual variations among different sub-classes. Most of existing approaches build upon CNN to predict class prob-abilities or measure feature distances. To address the subtle appearance variations, local characteristics are often cap-tured by learning spatial attention [15, 34, 40, 60] or explic-itly localizing semantic objects / parts [11, 56, 58, 61]. We adopt a different way to incorporate local information based on vision Transformer. To this end, we propose global-local cross-attention (GLCA) to enhance the interactions between global images and local high-response regions. Specifically, we compute the cross-attention between a selected subset of query vectors and the entire set of key-value vectors. By coordinating with self-attention learning, GLCA can help reinforce the spatial-wise discriminative clues to recognize fine-grained objects.
Apart from incorporating local information, another so-lution to distinguish the sutble visual differences is pair-wise learning. The intuition is that one can identify the subtle variations by comparing image pairs. Exiting CNN-based methods design dedicated network architectures to enable pair-wise feature interaction [16, 69]. A contrastive loss [16] or score ranking loss [69] is used for feature learn-ing. Motivated by this, we also employ a pair-wise learning scheme to establish the interactions between image pairs.
Different from optimizing the feature distance, we propose pair-wise cross-attention (PWCA) to regularize the atten-tion learning of an image by treating another image as dis-tractor. Specifically, we compute the cross-attention be-tween query of an image and combined key-value from both images. By introducing confusion in key and value vectors, the attention scores are diffused to another image so that
the difficulty of the attention learning of the current image increases. Such regularization allows the network to dis-cover more discriminative regions and alleviate overfitting to sample-specific features. It is noted that PWCA is only used for training and thus does not introduce extra compu-tation cost during inference.
The proposed two types of cross-attention are easy-to-implement and compatible with self-attention learning. We conduct extensive evaluations on both fine-grained visual categorization (FGVC) and object re-identification (Re-ID).
Experiments demonstrate that DCAL performs on par with state-of-the-art methods and consistently improves multiple self-attention baselines. Particularly, for FGVC, DCAL im-proves DeiT-Tiny by 2.5% and reaches 92.0% top-1 accu-racy with the larger R50-ViT-Base backbone on CUB-200-2011. For Re-ID, DCAL improves DeiT-Tiny and ViT-Base by 2.8% and 2.4% mAP on MSMT17, respectively.
Our main contributions can be summarized as follows. (1) We propose global-local cross-attention to enhance the interactions between global images and local high-response regions for reinforcing the spatial-wise discriminative clues. (2) We propose pair-wise cross-attention to establish the in-teractions between image pairs by regularizing the attention learning. (3) The proposed dual cross-attention learning can complement the self-attention learning and achieves consis-tent performance improvements over multiple vision Trans-former baselines on various FGVC and Re-ID benchmarks. 2.