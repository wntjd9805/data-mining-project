Abstract
The advances in monocular 3D human pose estimation are dominated by supervised techniques that require large-scale 2D/3D pose annotations. Such methods often behave erratically in the absence of any provision to discard un-familiar out-of-distribution data. To this end, we cast the 3D human pose learning as an unsupervised domain adap-tation problem. We introduce MRP-Net1 that constitutes a common deep network backbone with two output heads subscribing to two diverse configurations; a) model-free joint localization and b) model-based parametric regres-sion. Such a design allows us to derive suitable measures to quantify prediction uncertainty at both pose and joint level granularity. While supervising only on labeled synthetic samples, the adaptation process aims to minimize the un-certainty for the unlabeled target images while maximizing the same for an extreme out-of-distribution dataset (back-grounds). Alongside synthetic-to-real 3D pose adaptation, the joint-uncertainties allow expanding the adaptation to work on in-the-wild images even in the presence of occlu-sion and truncation scenarios. We present a comprehensive evaluation of the proposed approach and demonstrate state-of-the-art performance on benchmark datasets. 1.

Introduction 3D human pose estimation forms a core component of several human-centric technologies such as augmented real-ity [24], gesture recognition [6], etc. Most of the 3D human pose estimation approaches heavily rely on fully supervised training objectives [15, 59, 80], demanding access to large-scale datasets with paired 3D pose annotation. However, the inconvenience of 3D pose acquisition stands as a signif-icant bottleneck. Unlike a 2D pose, it is difficult to manu-ally annotate an anthropomorphically constrained 3D pose for an in-the-wild RGB image. Thus, most of the paired 3D pose datasets are collected in lab environments via body-worn sensors or multi-camera studio setups [28, 78] that are
*equal contribution. 1Project page: https://sites.google.com/view/mrp-net
Figure 1. The proposed unsupervised adaptation framework uti-lizes a multi-representation consistency based uncertainty estima-tion for simultaneous OOD detection and adaptation. difficult to install outdoors. This often limits the dataset di-versity in terms of the variety in poses, appearances (back-ground and lighting conditions), and outfits.
Some works [69, 71] propose weakly supervised tech-niques to bypass the requirement of 3D pose annotations.
Several of these works leverage available paired 2D pose datasets or off-the-shelf image-to-2D pose estimation net-works [34, 63, 83]. To address the inherent 2D-to-3D ambi-guity, some works either rely on multi-view image pairs [11, 32,38] or utilize unpaired 3D pose samples [10,88]. Though such methods perform well when evaluated on the same dataset, they lack cross-dataset generalization.
Consider a scenario where we want to deploy a 3D hu-man pose estimation system in a new application environ-ment (i.e. target domain). From the vendor’s perspective, a general approach would be to improve the system’s general-ization via supervised training on a wide variety of labeled source domains [48]. However, target-specific training usu-ally achieves the best performance beyond the generic sys-tem. Though it is not convenient to collect annotations for every novel deployment scenario, an effective unsupervised adaptation framework stands as the most practical way for-ward. Unsupervised adaptation [19, 37, 84] seeks a learn-ing technique that can minimize the domain discrepancy be-Table 1. Comparison of positive (in green) and negative (in red) attributes of ours against prior 3D human pose estimation methods.
Methods
Zhou et al. [102]
Rhodin et al. [70]
Iqbal et al. [29]
Doersch et al. [17]
LCR-Net++ [74]
PoseNet3D [81]
Ours
Real Sup. 2D pose
✓
✗
✓
✗
✓
✓
✗
Multi view
✗
✓
✓
✗
✗
✗
✗ 3D pose
✗
✗
✗
✗
✓
✗
✗
Synthetic 3D pose
Sup.
✗
✗
✗
✓
✗
✗
✓
Generalization capability
Occlusion Uncertainty
✗
✗
✗
✗
✓
✗
✓
✓
✗
✗
✗
✗
✗
✓ tween a labeled source and an unlabeled target. Thus, the vendor has to collect unlabeled RGB inputs from the new environment to enable the adaptation process. Let us con-sider a different scenario where the target environment is identical to one of the source domains implying no domain-shift. Here, the vendor can choose to directly deploy the generic system without adaptation training. However, the system must have a provision to detect whether it is required to run the adaptation process.
In other words, it should have the ability to discern out-of-distribution (OOD) scenar-ios [26,47,51]. Such an ability is more crucial while deploy-ing in a continually changing environment [89], e.g. a model adapted for sunny weather conditions would fail while en-countering rainy weather, thus requiring re-adaptation.
We propose a novel domain adaptation (DA) frame-work, MRP-Net (Fig. 1), equipped with uncertainty esti-mation [35] for the monocular 3D human pose estimation task. To this end, we use a multi-representation pose net-work with a common backbone followed by two pose esti-mation heads subscribing to two diverse output configura-tions; a) Heat-map based joint localization and b) Model-based parametric regression. This not only encourages ensemble-diversity required for uncertainty estimation [18] but also allows us to encompass the merits of both schools of thought [58, 80]. The former configuration advocates maintaining the spatial structure via a fully-convolutional design [56, 61, 79] while lacking provisions to inculcate structural articulation and bone-length priors. The latter ad-vocates regressing a parametric form of the pose as a whole (via fully-connected layers) [30, 33, 49, 82] while allowing model-based structural prior infusion [39, 67]. We use the 3D graphics-based synthetic SURREAL dataset [86] as the labeled source domain to supervise our backbone network.
In addition, we derive useful measures to quantify the prediction uncertainty at two granularity levels; viz a) pose-uncertainty, b) joint-uncertainty. During training, we uti-lize both a labeled source and a dataset of backgrounds (BG) to elicit the desired behavior of the uncertainties.
Here, the backgrounds approximate an extreme out-of-distribution scenario. Upon encountering the unlabeled tar-get, the adaptation process seeks to reduce the target uncer-tainties alongside a progressive self-training on a set of re-liable pseudo-labels. Alongside the adaptation for datasets with full-body visibility, the joint-uncertainty lays a suitable ground to expand our adaptation to work on in-the-wild tar-get domain (unlabeled) with partial body visibility (i.e. un-der external occlusion or truncated frame scenarios). We present an extensive evaluation of the proposed framework under a variety of source-to-target settings. In summary:
• We propose a novel domain adaptation framework,
MRP-Net, that uses a multi-representation pose net-work. Here, pose-uncertainty is quantified as the dis-agreement between pose predictions through the two output heads subscribing towards two diverse design configurations (model-free versus model-based).
• We propose to utilize negative samples (backgrounds and simulated synthetic joint-level occlusions) to im-prove the effectiveness of the proposed pose and joint uncertainties. The presence of negatives also helps to retain the uncertainty estimation ability even while adapting to a novel target scenario. synthetic (SURREAL) to in-studio adapta-tion outperforms the comparable prior-arts on Hu-man3.6M [28]. Our in-studio (Human3.6M) to in-the-wild adaptation achieves state-of-the-art performance across four datasets. We show uncertainty-aware 3D pose estimation results for unsupervised adaptation to in-the-wild samples with partial body visibility.
• Our 2.