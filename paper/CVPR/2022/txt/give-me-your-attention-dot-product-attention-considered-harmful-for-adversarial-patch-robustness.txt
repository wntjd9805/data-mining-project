Abstract 1.

Introduction
Neural architectures based on attention such as vision transformers are revolutionizing image recognition. Their main beneﬁt is that attention allows reasoning about all parts of a scene jointly.
In this paper, we show how the global reasoning of (scaled) dot-product attention can be the source of a major vulnerability when confronted with adversarial patch attacks. We provide a theoretical under-standing of this vulnerability and relate it to an adversary’s ability to misdirect the attention of all queries to a single key token under the control of the adversarial patch. We pro-pose novel adversarial objectives for crafting adversarial patches which target this vulnerability explicitly. We show the effectiveness of the proposed patch attacks on popular image classiﬁcation (ViTs and DeiTs) and object detection models (DETR). We ﬁnd that adversarial patches occupy-ing 0.5% of the input can lead to robust accuracies as low as 0% for ViT on ImageNet, and reduce the mAP of DETR on MS COCO to less than 3%.
The attention mechanism plays a prominent role in re-cent success of transformers for different language and image processing tasks. Recent breakthroughs in image recognition using vision transformers [12, 23, 37] have in-spired different architectures’ design for tackling tasks such as object detection [4, 8], semantic segmentation [35, 41, 46, 48], image synthesis [13, 17, 40], video understand-ing [3, 6, 15, 15, 19, 24, 30, 36, 49], and low-level vision tasks [9, 21, 42, 47]. These different transformers use the dot-product attention mechanism as an integral component of their architectural design to model global interactions among different input or feature patches. Understanding robustness of these dot-product attention-based networks against adversarial attacks targeting security-critical vulner-abilities is important for their deployment into real-world applications.
The increasing interest of research in transformers across vision tasks has motivated several recent works [2, 5, 7, 14,
16,27–29,34,43] to study their robustness against adversar-ial attacks. Some prior works [2, 5, 28, 34] have hypothe-sized that transformers are more robust than convolutional neural networks (CNNs) against these attacks. On the other hand, [14, 16, 27, 43] have shown that vision transformers are not an exception and are also prone to adversarial at-tacks. In particular, [43] shows that an adversarial attack can be tailored to transformers to achieve high adversar-ial transferability. These ﬁndings indicate that robustness evaluation protocols (attacks) designed for CNNs might be suboptimal for transformers. In the same line of work, we identify a principled vulnerability in the widely-used dot-product attention in transformers that can often be exploited by image-based adversarial patch attacks.
Dot-product attention computes the dot-product similar-ity of a query token with all key tokens, which is later nor-malized using the softmax operator to obtain per token at-tention weights. These attention weights are then multiplied with value tokens to control the value token’s contribution in the attention block. Gradient-based adversarial attacks backpropagate gradients through all the components in the architecture including the attention weights. We observe that on pretrained vision transformers, because of the soft-max, the gradient ﬂow through the attention weights is often much smaller than the ﬂow through the value token (refer to Sec. 4.1). Consequently, gradient-based attacks with a standard adversarial objective are biased towards focusing on adversarial effects propagated through the value token and introduce little or no adversarial effect on the attention weights, thus limiting the attack’s potential.
Our work aims to adversarially affect the attention weights, even if those operate in a saturated regime of soft-max where gradient-based adversarial attacks are impaired.
Speciﬁcally, we propose losses that support the adversary in misdirecting the attention of most queries to a key token that corresponds to the adversarial patch, i.e., to increase the attention weights of the queries to the targeted key token.
We further study necessary conditions (refer to Sec. 4.2) for a successful attack that misdirects attention weights: both (a) having projection matrices with large singular values and (b) having higher embedding dimensions, allows am-plifying the effect of perturbations in a single token, and thus changing the embedding of a single key considerably.
Moreover, (c) having less centered inputs (larger absolute value of input mean) to the dot-product attention results in distinct clusters of queries and keys that are distant from each other. Under this condition, moving a single key closer to the query cluster center can make the key most similar to the majority of queries simultaneously (see Figure 1).
We propose a family of adversarial losses and attacks called Attention-Fool that directly acts on the dot-product outputs (pre-softmax dot-product similarities). These losses optimize the adversarial patch to maximize the dot-product similarity of all the queries to a desired key (typically the key whose token corresponds to the input region with ad-versarial patch). This approach maximizes the number of queries that attend to the targeted key in an attention head, please refer to Figure 2 for an illustration. We apply these losses at multiple attention heads and layers in transform-ers to misdirect the model’s attention from the image con-tent to the adversarial patch in order to encourage wrong predictions. We show that our Attention-Fool adversarial losses improve gradient-based attacks against different vi-sion transformers ViTs [12] and DeiTs [37] for image clas-siﬁcation and also signiﬁcantly improve the attack’s effec-tive on the object detection model DETR [8].
Our contributions can be summarized as follows: We
• Identify the necessary conditions for the existence of a vulnerability in dot-product attention layers weights, see Section 4.2.
• Provide reasons why this vulnerability may not be fully exploited by vanilla gradient-based adversarial attacks, see Section 4.1.
• Introduce Attention-Fool, a new family of losses which deﬁnes losses directly on the attention layers dot-product similarities, see Section 5.
• Show that transformers for image classiﬁcation and object detection are highly sensitive to small adversar-ial patches, see Section 6. 2.