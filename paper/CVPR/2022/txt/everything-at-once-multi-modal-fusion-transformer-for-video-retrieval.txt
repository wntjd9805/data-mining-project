Abstract
Multi-modal learning from video data has seen increased attention recently as it allows training of semantically mean-ingful embeddings without human annotation, enabling tasks like zero-shot retrieval and action localization. In this work, we present a multi-modal, modality agnostic fusion trans-former that learns to exchange information between multiple modalities, such as video, audio, and text, and integrate them into a fused representation in a joined multi-modal embedding space. We propose to train the system with a combinatorial loss on everything at once – any combina-tion of input modalities, such as single modalities as well as pairs of modalities, explicitly leaving out any add-ons such as position or modality encoding. At test time, the resulting model can process and fuse any number of input modalities.
Moreover, the implicit properties of the transformer allow to process inputs of different lengths. To evaluate the proposed approach, we train the model on the large scale HowTo100M dataset and evaluate the resulting embedding space on four challenging benchmark datasets obtaining state-of-the-art results in zero-shot video retrieval and zero-shot video action localization. Our code for this work is also available.1 1.

Introduction
Humans capture their world in various ways, combin-ing different sensory input modalities such as vision, sound, touch, and more, to make sense of their environment. Video data approximates this type of input by combining visual and audio information as two coherent and complementary signals that can be further enhanced with a text descrip-tion. Recent research has therefore started to explore how the information of those different modalities can be lever-aged to learn meaningful representations from this kind of content. Such systems can be used for representation learn-ing, for example, to learn multi-modal embedding spaces on video data [1, 2], where the input of one modality such 1https://github.com/ninatu/everything at once
Figure 1. Overview of the proposed approach for self-supervised learning of multi-modal embedding space. The fusion transformer is able to process any combination of input modalities. Internally, the transformer allows each modality to attend to each other. The proposed architecture is trained with a combinatorial contrastive loss considering each possible combination of input modalities. as text, can be matched to one or more other modalities such as video and audio, enabling tasks such as nearest-neighbor based zero-shot classification or video retrieval
[20, 37, 44]. Our work in this paper focuses on the later problem, namely the learning of meaningful multi-modal embedding spaces. Current approaches in this area usually learn encodings for different modalities by projecting inputs to a common space and applying contrastive loss to bring embeddings from co-occurred modalities together. Such ap-proaches can be based on classical neural network elements to learn those encodings [4, 12, 35, 37, 44], i.e. convolutional neural networks backbones and non-linear projections [37], multiple instance learning [35], or clustering [12]. More recently transformer based methods have also been pro-posed [1, 10, 20, 32]. To generate the final embedding space, they use multiple independent single-modality self-attention transformer blocks [10,21,32], or a single transformer model for all modalities [20], or a single modality-agnostic trans-former [1]. In the last approach, modalities are still pro-cessed independently and one-by-one forwarded to achieve a single-modality embedding. But so far, none of these trans-formers allow for adaption to any given number of input
modalities. Although modality-agnostic transformers that handle multiple input modalities such as PerceiverIO [26] have been proposed, they have been constructed for learning a latent space that can cover multiple tasks in different do-mains. Compared to our work, the latent space in such cases mainly serves the purpose of compressing multiple inputs and tasks in one model.
In this work, we propose an approach that leverages self-attention for multi-modal learning which jointly processes any number of modalities and allows modalities to attend to each other. A high level overview our architecture is shown in Figure 1. Input tokens from one or more modalities are passed through a fusion transformer that attends features relevant for a combined input, followed by a projection to a joint multi-modal embedding space. We design and train the fusion transformer to cover three aspects of multi-modal video learning: first, it should allow modalities to attend to each other and learn multi-modal correlations; second, it should be modality-agnostic and handle any possible modal-ity input combination; and third, as different modalities and samples can vary in length, it should be able to process input of any length. To enable the fusion transformer to address all those tasks, we follow the idea of a universal self-attention in the transformer block and share key, query, and value weights to all tokens, agnostic of their input modality. In this way, self-attentions learns which input tokens to attend from single modalities as well as from any combination of modalities in a general way.
To train the model, we propose a combinatorial loss func-tion which considers contrastive loss between all possible and available input combinations. For example, in the case of vision, text, and audio, the loss is based on each modality embedding alone as well as based on pairwise vision-text, audio-text, and text-audio combinations as shown in Fig-ure 1. The resulting model is thus able to fuse any number of input modalities at test time. Compared to other universal self-attention methods, we omit any meta information en-coding such as position or modality embedding. This further allows us to process any input of different lengths, as we are no longer bound to a maximum input size defined at training time. Note that while we refer to this transformer as a fusion transformer, we are not proposing a new transformer archi-tecture, but rather refer to it as a transformer that is trained in a way that enables fusion without any need for changes to the self-attention mechanism. As a result, the final modal can be used for any type of input, single modalities or combinations of multiple ones, as well as for any input length.
We evaluate the proposed approach by training the model on the HowTo100M dataset [37] and testing its zero-shot text-to-video retrieval and step action localization on four down-stream datasets, namely YouCook2 [55], MSR-VTT [52],
CrossTask [58] and Mining YouTube [29]. Our results show that the proposed combination of a fusion transformer to-gether with a combinatorial loss function improves perfor-mance and leads to new state-of-the-art results. We summa-rize the contributions of the paper as follows:
• We propose a multi-modal fusion transformer that pro-cesses input of any combination of modalities and any length and attends relevant features with respect to cross-modal information.
• We propose a combinatorial contrastive loss that con-siders all possible combinations of input modalities at training time.
• We show that using such a multi-modal fusion trans-former as an intermediate processing step can signif-icantly improve performance for multi-modal embed-ding space learning. 2.