Abstract
Misinformation is now a major problem due to its poten-tial high risks to our core democratic and societal values and orders. Out-of-context misinformation is one of the easiest and effective ways used by adversaries to spread vi-ral false stories. In this threat, a real image is re-purposed to support other narratives by misrepresenting its context and/or elements. The internet is being used as the go-to way to verify information using different sources and modali-ties. Our goal is an inspectable method that automates this time-consuming and reasoning-intensive process by fact-checking the image-caption pairing using Web evidence. To integrate evidence and cues from both modalities, we intro-duce the concept of ‘multi-modal cycle-consistency check’
; starting from the image/caption, we gather tex-tual/visual evidence, which will be compared against the other paired caption/image, respectively. Moreover, we propose a novel architecture, Consistency-Checking Net-/ work (CCN), that mimics the layered human reasoning across the same and different modalities: the caption vs. textual evidence, the image vs. visual evidence, and the image vs. caption. Our work offers the ﬁrst step and bench-mark for open-domain, content-based, multi-modal fact-checking, and signiﬁcantly outperforms previous baselines that did not leverage external evidence1. 1.

Introduction
Recently, there has been a growing and widespread con-cern about ‘fake news’ and its harmful societal, personal, and political consequences [1, 18, 24], including people’s own health during the pandemic [6, 7, 30]. Misusing gen-erative AI technologies to create deepfakes [13, 21, 36] further fuelled these concerns [5, 14]. However, image-repurposing— where a real image is misrepresented and used out-of-context with another false or unrelated narrative 1For code, checkpoints, and dataset, check: abdelnabi.github.io/OoC-multi-modal-fc/ https : / / s -to create more credible stories and mislead the audience— is still one of the easiest and most effective ways to create realistically-looking misinformation.
Image-repurposing does not require profound technical knowledge or experi-ence [2, 27], which potentially ampliﬁes its risks. Images usually accompany real news [41]; thus, adversaries may augment their stories with images as ‘supporting evidence’ to capture readers’ attention [15, 27, 47].
Image re-purposing datasets and threats. Gathering large-scale labelled out-of-context datasets is hard due to the scarcity and substantial manual efforts. Thus, previ-ous work attempted to construct synthetic out-of-context datasets [20, 39]. A recent work [27] proposed to auto-matically, yet non-trivially, match images accompanying real news with other real news captions. The authors used trained language and vision models to retrieve a close and convincing image given a caption. While this work con-tributes to misinformation detection research by automat-ically creating datasets, it also highlights the threat that machine-assisted procedures may ease creating misinfor-mation at scale. Furthermore, the authors reported that both defense models and humans struggled to detect the out-of-context images. In this paper, we use this dataset as a chal-lenging benchmark; we leverage external evidence to push forward the automatic detection.
Fact-checking. To ﬁght misinformation, huge fact-checking efforts are done by different organizations [33,34].
However, they require substantial manual efforts [43]. Re-searchers have proposed several automated methods and benchmarks to automate fact-checking and veriﬁcation [32, 42]. However, most of these works focus on textual claims.
Fact-checking multi-modal claims has been under-explored.
Our approach. People frequently use the Internet to verify information. We aggregate evidence from images, articles, different sources, and we measure their consensus and consistency. Our goal is to design an inspectable frame-work that automates this multi-modal fact-checking process and assists users, fact-checkers, and content moderators.
More speciﬁcally, we propose to gather and reason over evidence to judge the veracity of the image-caption pair.
, we use the image to ﬁnd its other occurrences on
First the internet, from which, we crawl textual evidence (e.g., captions), which we compare against the paired caption.
, we use the caption to ﬁnd other images as
Similarly visual evidence to compare against the paired image. We call this process: ‘multi-modal cycle-consistency check’.
Importantly, we retrieve evidence in a fully automated and
ﬂexible open-domain manner [8]; no ‘golden evidence’ is pre-identiﬁed or curated and given to the model.
To evaluate the claim’s veracity, we propose a novel architecture, the Consistency-Checking Network (CCN), that consists of 1) memory networks components to eval-uate the consistency of the claim against the evidence (de-scribed above), 2) a CLIP [35] component to evaluate the consistency of the image and caption pair themselves. As the task requires machine comprehension and visual un-derstanding, we perform different evaluations to design the memory components and the evidence representations.
Moreover, we conduct two user studies to 1) measure the human performance on the detection task and, 2) under-stand if the collected evidence and the model’s attention over the evidence help people distinguish true from falsiﬁed pairs. Figure 1 depicts our framework, showing a falsiﬁed example from the dataset along with the retrieved evidence.
Contributions. We summarize our contributions as follows: 1) we formalize a new task of multi-modal 2) We propose the ‘multi-modal cycle-fact-checking. consistency check’ to gather evidence about the multi-modal claim from both modalities. 3) We propose a new inspectable framework, CCN, to mimic the aggregation of observations from the claims and world knowledge. 4) We perform numerous evaluations, ablations, and user studies and show that our evidence-augmented method signiﬁcantly improves the detection over baselines. 2.