Abstract
This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP uni-fies object detection and phrase grounding for pre-training.
The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to im-prove both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generat-ing grounding boxes in a self-training fashion, making the learned representations semantic-rich.
In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs.
The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recogni-tion tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training),
GLIP achieves 49.8 AP and 26.9 AP, respectively, surpass-ing many supervised baselines.1 2) After fine-tuned on
COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code will be re-leased at https://github.com/microsoft/GLIP. 1.

Introduction
Visual recognition models are typically trained to predict a fixed set of pre-determined object categories, which limits their usability in real-world applications since additional la-beled data are needed to generalize to new visual concepts and domains. CLIP [40] shows that image-level visual rep-resentations can be learned effectively on large amounts of raw image-text pairs. Because the paired texts contain a boarder set of visual concepts than any pre-defined concept
∗The three authors contributed equally. ♠ Corresponding author.
†Work done when interning at Microsoft Research. 1Supervised baselines on COCO object detection: Faster-RCNN w/
ResNet50 (40.2) or ResNet101 (42.0), and DyHead w/ Swin-Tiny (49.7). pool, the pre-trained CLIP model is so semantically rich that it can be easily transferred to downstream image classifi-cation and text-image retrieval tasks in zero-shot settings.
However, to gain fine-grained understanding of images, as required by many tasks, such as object detection [31, 44], segmentation [6, 35], human pose estimation [49, 56], scene understanding [14, 25, 57], action recognition [17], vision-language understanding [7,27–30,36,48,50,63,65], object-level visual representations are highly desired.
In this paper, we show that phrase grounding, which is a task of identifying the fine-grained correspondence between phrases in a sentence and objects (or regions) in an image, is an effective and scalable pre-training task to learn an object-level, language-aware, and semantic-rich visual representa-tion, and propose Grounded Language-Image Pre-training (GLIP). Our approach unifies the phrase grounding and ob-ject detection tasks in that object detection can be cast as context-free phrase grounding while phrase grounding can be viewed as a contextualized object detection task. We highlight our key contributions as follows.
Unifying detection and grounding by reformulating object detection as phrase grounding. The reformulation changes the input of a detection model: it takes as input not only an image but also a text prompt that describes all the candidate categories in the detection task2. For example, the text prompt for COCO object detection [32] is a text string that consists of 80 phrases, i.e., the 80 COCO object class names, joined by “. ”, as shown in Figure 1 (Left).
Any object detection model can be converted to a ground-ing model by replacing the object classification logits in its box classifier with the word-region alignment scores, i.e., dot product of the region (or box) visual features and the token (or phrase) language features, as shown in Figure 1 (Right). The language features are computed using a lan-guage model, which gives the new detection (or ground-ing) model a dual-encoder structure. Different from CLIP that fuses vision and language only at the last dot product layer [40], we show that deep cross-modality fusion applied 2Different from typical phrase grounding tasks, phrases in the text prompt for an object detection task may not be present in the image.
Figure 1. A unified framework for detection and grounding. Unlike a classical object detection model which predicts a categorical class for each detected object, we reformulate detection as a grounding task by aligning each region/box to phrases in a text prompt. GLIP jointly trains an image encoder and a language encoder to predict the correct pairings of regions and words. We further add the cross-modality deep fusion to early fuse information from two modalities and to learn a language-aware visual representation.
Figure 2. Grounding pre-dictions from GLIP. GLIP entities, can locate phrases with attributes, and even abstract words. rare by GLIP, as shown in Figure 1 (Middle), is crucial to learn high-quality language-aware visual representations and to achieve superior transfer learning performance. The unifi-cation of detection and grounding also allows us to pre-train using both types of data and benefits both tasks. On the detection side, the pool of visual concepts is significantly enriched thanks to the grounding data. On the grounding side, detection data introduce more bounding box annota-tions and help train a new SoTA phrase grounding model.
Scaling up visual concepts with massive image-text data. Given a good grounding model (teacher), we can augment GLIP pre-training data by automatically generat-ing grounding boxes for massive image-text-paired data, in which noun phrases are detected by an NLP parser [2].
Thus, we can pre-train our (student) GLIP-Large model (GLIP-L) on 27M grounding data, including 3M human-annotated fine-grained data and 24M web-crawled image-text pairs. For the 24M image-text pairs, there are 78.1M high-confidence (> 0.5) phrase-box pseudo annotations, with 58.4M unique noun phrases. We showcase two real examples of the generated boxes in Figure 2. The teacher model can accurately localize some arguably hard con-cepts, such as syringes, vaccine, beautiful caribbean sea turquoise, and even abstract words (the view). Training on such semantic-rich data delivers a semantic-rich student model.
In contrast, prior work on scaling detection data simply cannot predict concepts out of the teacher models’ pre-defined vocabulary [68].
In this study, we show that this simple strategy of scaling up grounding data is em-pirically effective, bringing large improvements to LVIS and 13 downstream detection tasks, especially on rare cat-egories (Sections 4.2 and 5). When the pre-trained GLIP-L model is fine-tuned on COCO, it achieves 60.8 AP on
COCO 2017val and 61.5 on test-dev, surpassing the current public SoTA models [9, 58] that scale up object detection data in various approaches.
Transfer learning with GLIP: one model for all. The grounding reformulation and semantic-rich pre-training fa-cilitate domain transfer. GLIP can be transferred to various tasks with few or even no additional human annotations.
When the GLIP-L model is directly evaluated on the COCO and LVIS datasets (without seeing any images in COCO during pre-training), it achieves 49.8 and 26.9 AP on COCO val2017 and LVIS val, respectively, surpassing many super-vised baselines. When evaluated on 13 existing object de-tection datasets, spanning scenarios including fine-grained species detection, drone-view detection, and ego-centric de-tection, the setting which we term “Object Detection in the
Wild” (ODinW) (Section 5.1), GLIP exhibits excellent data efficiency. For example, a zero-shot GLIP-L outperforms a 10-shot supervised baseline (Dynamic Head) pre-trained on
Objects365 while a 1-shot GLIP-L rivals with a fully super-vised Dynamic Head. Moreover, when task-specific anno-tations are available, instead of tuning the whole model, one could tune only the task-specific prompt embedding, while keeping the model parameters unchanged. Under such a prompt tuning setting (Section 5.2), one GLIP model can simultaneously perform well on all downstream tasks , re-ducing the fine-tuning and deployment cost. 2.