Abstract
The human visual system is remarkable in learning new visual concepts from just a few examples. This is precisely the goal behind few-shot class incremental learning (FS-CIL), where the emphasis is additionally placed on ensuring the model does not suffer from “forgetting”. In this paper, we push the boundary further for FSCIL by addressing two key questions that bottleneck its ubiquitous application (i) can the model learn from diverse modalities other than just photo (as humans do), and (ii) what if photos are not read-ily accessible (due to ethical and privacy constraints). Our key innovation lies in advocating the use of sketches as a new modality for class support. The product is a “Doodle
It Yourself” (DIY) FSCIL framework where the users can freely sketch a few examples of a novel class for the model to learn to recognise photos of that class. For that, we present a framework that infuses (i) gradient consensus for domain invariant learning, (ii) knowledge distillation for preserving old class information, and (iii) graph attention networks for message passing between old and novel classes. We exper-imentally show that sketches are better class support than text in the context of FSCIL, echoing ﬁndings elsewhere in the sketching literature. 1.

Introduction
Fully supervised learning has served us great with per-formances on ImageNet already surpassing human-level
In reality, however, such progress is primarily lim-[18]. ited to a small number of object classes where labels were explicitly curated (1000 in ImageNet vs. possibly millions out there). Class Incremental Learning [29, 21, 23] is one of the popular fronts that attempt to extend model perception to novel classes while not “forgetting” about classes learned already. Amongst its many variants, the recent Few-Shot
Class Incremental Learning (FSCIL) [54] is the most realis-tic where it also dictates the model to learn new classes with very few examples, the same as humans do.
*Interned with SketchX
Figure 1. Illustration of our DIY-FSCIL framework. For instance, given sketch exemplars (1-shot here) from 3 novel classes as support-set, a 10-class classiﬁer gets updated to (10 + 3)-class classiﬁer that can classify photos from both base and novel classes.
As easy as providing a few samples might sound, ques-tions start to emerge in practice as to (i) what data modality should the samples take? and (ii) how could these samples be obtained in practice. These questions, we argue, are key to the potentially ubiquitous application of FSCIL as (i) hu-mans also learn from a broad range of data modalities that are not limited to just photo, and (ii) there are scenarios where photos are not necessarily always readily available due to privacy and ethical constraints (e.g., copyright).
In this paper, we set out to study the role of human sketches as a support modality for FSCIL. This results in a ﬂexible FSCIL system that learns new classes just by ob-serving a few sketches doodled by users themselves. Fig. 1 illustrates schematically our “Doodle It Yourself (DIY)”
FSCIL scenario – “DIY-FSCIL”. This importantly ad-dresses the aforementioned problems in that (i) learning is no longer ﬁxed to just photos but ﬂexibly cross-modal with other data forms (just as humans do), and (ii) it works without asking the users to source photos which might have practical constraints attached (e.g., copyright, hazardous en-vironments). There is of course also the added beneﬁt of injecting creativity to the classiﬁer by sketching something off the user’s imagination [16], e.g., a “ﬂying cow”?
The advocate of sketches is largely motivated by the line of work examining human-centric characteristics of sketches in many parallel applications – notably image re-trieval [12], where the ﬁne-grained nature of sketches is used to successfully conduct instance-level retrieval [2, 6, 46, 14, 11]. Sketches in context of FSCIL is closely rem-iniscent of its usage in ﬁne-grained retrieval. While in re-trieval they utilise the detailed nature of sketches to conduct sketch-photo matching, we use a few sketches collectively as faithful visual representatives (support) of novel classes for incremental learning. We show that sketches are better suited as class support in comparison to text, thanks to its inherent ﬁne-grained nature, validated by ﬁndings in con-temporary sketch literature [52, 11, 6].
Nonetheless, using sketches as class support in the FS-CIL setting is non-trivial. Sketch, despite being visually representative, is just a coarse contour-like depiction of the visual world, that sit in an entirely different domain from photo [27]. Thus, off-the-shelf models naively pre-trained on photos commonly fail to generalise well on sketches [8].
Moreover, due to its highly abstract nature, the same object may be sketched in various ways under unique user-styles
[51, 46], and with varied levels of detail [45]. We are also distinctly different to the parallel problem of SBIR – SBIR typically get exposed to paired sketch-photo data at train-ing to learn a cross-modal embedding; we on the other hand need to work with sketches only at training (i.e., no photo information whatsoever), yet still aim to generate classiﬁ-cation layer weights to classify photos from novel classes.
Three key design considerations for this cross-domain sketch-based FSCIL are: (i) how to make the model work cross-modal, (ii) how to preserve old class information, and (iii) how to leverage information from old classes to learn new ones. For the ﬁrst issue, we design a gradient consen-sus based strategy that updates the model towards mutual agreement in the gradient space between sketch and photo domain, thus achieving a domain invariant feature extractor.
For the second, we model an additional knowledge distilla-tion loss to retain the acquired knowledge from old classes while incrementing the classiﬁer to novel classes. Lastly, we devise a graph neural network to generate more discrim-inative decision boundaries for the incremented classiﬁer via message passing between old and novel classes.
To summarise, our contributions are: (a) We extend in-cremental learning research even further towards practical-ity and human-likeness. (b) We achieve that by introducing sketches as class support for FSCIL, allowing the system to learn from modalities other than just photos and addressing issues around ethics and privacy while allowing user cre-ativity. (c) We introduce the ﬁrst cross-modal framework to tackle this novel DIY-FSCIL problem. 2.