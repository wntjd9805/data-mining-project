Abstract
This paper presents a Generative prior ReciprocAted
Invertible rescaling Network (GRAIN) for generating faith-ful high-resolution (HR) images from low-resolution (LR) invertible images with an extreme upscaling factor (64×).
Previous researches have leveraged the prior knowledge of a pretrained GAN model to generate high-quality upscal-ing results. However, they fail to produce pixel-accurate results due to the highly ambiguous extreme mapping pro-cess. We remedy this problem by introducing a reciprocated invertible image rescaling process, in which high-resolution information can be delicately embedded into an invertible low-resolution image and generative prior for a faithful HR reconstruction. In particular, the invertible LR features not only carry signiﬁcant HR semantics, but also are trained to predict scale-speciﬁc latent codes, yielding a preferable utilization of generative features. On the other hand, the enhanced generative prior is re-injected to the rescaling pro-*Corresponding author (hesfe@scut.edu.cn). cess, compensating the lost details of the invertible rescal-ing. Our reciprocal mechanism perfectly integrates the ad-vantages of invertible encoding and generative prior, lead-ing to the ﬁrst feasible extreme rescaling solution. Exten-sive experiments demonstrate superior performance against state-of-the-art upscaling methods. Code is available at https://github.com/cszzx/GRAIN . 1.

Introduction
Due to the explosive growth of image data, image down-scaling is a typical way for fast data processing and efﬁ-cient storage. Therefore, the capability of rescaling a low-resolution image to a high-resolution one is of great impor-tance for many multimedia applications. However, due to the highly underspeciﬁed mapping process from low-resolution (LR) to high-resolution (HR), different image priors have to be introduced to reduce the learning ambiguity, especially for an extreme upscaling setting (e.g., 64×).
Large-scale data prior is typically used in deep learning based super-resolution to mimic the transformation from LR to HR [6,8,13]. Due to the dominated pixel-wise constraints, these methods can smoothly recover the overall structure.
For the same reason, small scale details cannot be “synthe-sized” from scratch and therefore these methods are typically limited to 8× upscaling.
Recent advances in generative adversarial networks (GANs) demonstrate that the generative prior [3, 22, 30, 35] derived from a pretrained GAN model provides rich and diverse supplementary representations for the extreme up-scaling process. The basic principle is that a small LR image can be mapped to the input latent code of a pretrained GAN model, such that a perceptually similar output can be pro-duced by the generator. Although the inherent knowledge of a pretrained GAN enables a plausible extreme upscaling (64×), the vague LR input prevents them from locating a perfect latent code and therefore cannot preserve the original characteristics (see Fig. 1b and Fig. 1c).
The above issue raises a question: can we enrich the in-formativeness of an extremely downscaled image to better incorporate with the generative prior? To answer this ques-tion, we turn to the alternative invertible prior. Previous invertible rescaling [17,28] embeds the high-resolution input image into inconspicuous and reconstructible patterns ap-pended on the LR image, such that it can be easily rescaled to the original resolution. In the scenario of extreme upscal-ing, however, the LR image (16×16) is too small to embed sufﬁcient hints for recovery (see Fig. 1d). To activate differ-ent upscaling priors under extreme scenarios, we propose in this paper a Generative prior ReciprocAted Invertible rescal-ing Network (GRAIN), which maximizes the potential of invertible and generative priors. In particular, it consists of three reciprocal components, i.e., an invertible extreme rescaling module, a scale-speciﬁc generative prior module, and an upscaling priors decoding module. The rescaling module is trained for two purposes. On one hand, it embeds the HR information to an extreme LR form that can be re-verted back to the original. On the other hand, the invertible features are optimized to produce scale-speciﬁc latent codes of the pretrained GAN model. In this way, the HR infor-mation is maximally entangled with the rescaling process in image-level as well as the generative latent space. The enhanced generative features are then reciprocated back to the invertible representations for decoding the ﬁnal upscaling result. Extensive experiments demonstrate faithful and supe-rior upscaling performance against state-of-the-art upscaling methods using different types of image priors. Our proposed method is the ﬁrst feasible extreme rescaling solution that can be beneﬁcial for storing and transferring data due to our perceivable yet invertible extremely downscaled LR re-sults (Fig. 1e). We show that our method can break through the limitation of the pretrained GAN data distribution and recover outlier inputs, while being also applicable to other domains by switching to different pretrained models.
Our contributions can be summarized as follows:
• We propose the ﬁrst invertible extreme rescaling frame-work that allows perceivable downscaling and faithful upscaling with an extreme scaling factor (64×).
• We design a reciprocal strategy that elegantly connects the invertible prior with the generative prior. It maximizes the advantages of these two priors, largely mitigating the ill-posed nature of the image upscaling process.
• The proposed model sets new state-of-the-art in extreme image rescaling. We also explore and analyze the image invertibility with respect to different inﬂuencing factors like network structure, scaling factor, and data domain. 2.