Abstract
Guided depth super-resolution (GDSR) is an essential topic in multi-modal image processing, which reconstructs high-resolution (HR) depth maps from low-resolution ones collected with suboptimal conditions with the help of HR
RGB images of the same scene. To solve the challenges in interpreting the working mechanism, extracting cross-modal features and RGB texture over-transferred, we propose a novel Discrete Cosine Transform Network (DCTNet) to alle-viate the problems from three aspects. First, the Discrete Co-sine Transform (DCT) module reconstructs the multi-channel
HR depth features by using DCT to solve the channel-wise optimization problem derived from the image domain. Sec-ond, we introduce a semi-coupled feature extraction module that uses shared convolutional kernels to extract common information and private kernels to extract modality-specific information. Third, we employ an edge attention mechanism to highlight the contours informative for guided upsampling.
Extensive quantitative and qualitative evaluations demon-strate the effectiveness of our DCTNet, which outperforms previous state-of-the-art methods with a relatively small number of parameters. The code is available at https:// github.com/Zhaozixiang1228/GDSR-DCTNet. 1.

Introduction
With the popularity of consumer-oriented depth estima-tion sensors, e.g., Time-of-Flight (ToF) and Kinect cameras, depth maps have promoted advancements in autonomous driving [24, 37], pose estimation [42, 56], virtual reality
[20, 28], and scene understanding [10, 64]. Unfortunately, due to the technical limitations and suboptimal imaging con-ditions, depth images are often low-resolution (LR) and noisy. However, high-resolution (HR) RGB images (or in-tensity images) are relatively easy to obtain in the same scene
*Corresponding authors.
Figure 1. Overview of DCTNet. First, the SCFE module extracts shared and private features from the depth (LR) and RGB (HR) images. The GESA module employs the RGB feature to obtain edge attention weights useful for SR. The multi-modal features and attention weights are then processed by the DCT module, where
DCT is utilized in each channel to get HR depth features. Finally, the reconstruction module outputs the SR depth map. when acquiring depth maps. Therefore, guided depth map super-resolution (GDSR) with RGB images has become an essential topic in multi-modal image processing and multi-modal super-resolution (SR). Our research is based on the assumption that there are statistical co-occurrences between the texture edges of RGB images and the discontinuities of depth maps [40]. In this way, information in RGB images can be utilized to restore HR depth maps when the LR depth maps are unsatisfactory for downstream applications.
For image SR, deep neural networks have become the de facto methodology due to their ability in modeling the mapping from LR to HR images [5, 25, 62]. However, image
SR mainly focuses on reconstructing fine details and tex-tures, while depth SR models need to infer textureless and piecewise affine regions that have sharp depth discontinu-ities [40]. Besides, depth maps can be noisy and suffer from a lower tolerance for artifacts in real-world applications [54].
Therefore, we can hardly adopt the methods for image SR without appraising the unique characteristics of depth SR.
Conventional methods for GDSR can be divided into three categories, i.e., filter- [27, 29, 30, 33], optimization-[4,6,23,35,59] and learning-based [8,54,55] methods. Filter-based (or local) methods focus on preserving sharp depth edges under the guidance of the intensity image. However, for texture-rich RGB images, irrelevant edges may be trans-ferred to depth images (known as texture over-transferred).
In addition, the explicitly defined filters can only model a specific visual task and lack flexibility. Optimization-based (or global) methods design energy functions based on diverse data prior, with data-fidelity regularization terms constraint the solution space [38, 63]. However, natural priors are often challenging to be explicitly represented and learned. The third category contains learning-based methods, which em-ploy data-driven pipelines to learn the dependency between multi-modal inputs. Representative works in this category use sparse dictionary learning [15, 19, 51], which learn dic-tionaries in a group learning manner and set constraints on the sparse representations of different modalities [2, 63].
Deep learning (DL) models are introduced to learn the mapping from LR to HR images [44, 47, 49, 50, 52, 61], but they still often cooperate classic methods for depth upsam-pling. For example, learnable filter [16, 53] (combination of
DL and filter-based methods) and algorithm unrolling [3, 58] (DL with optimization-based methods) have shown promis-ing results. However, there are still challenges in conven-tional methods, including edge mismatch and texture over-transferred between the RGB/depth images, difficulty to learn of natural priors effectively, and limited interpretability for the internal mechanism of DL architectures.
To this end, we propose a Discrete Cosine Transform
Network (DCTNet) for the GDSR task, inspired by cou-pled dictionary learning and physics-based modeling.
It consists of four components: semi-coupled feature extrac-tion (SCFE), guided edge spatial attention (GESA), discrete cosine transform (DCT) module, and a depth reconstruc-tion (DR) module. The workflow is illustrated in Figure 1.
Our contributions can be summarized as follows:
First, we propose the semi-coupled residual blocks to leverage the correlation between the intensity edge in RGB images and the depth discontinuities in depth images, but still preserve the unique properties like detailed texture and seg-ment smoothness in two modalities. In each convolutional layer of this block, half of the kernels are responsible for extracting shared information in depth/RGB images, which is applied to both modalities. The rest half of the convo-lution kernels are designed to extract unique information in the depth and RGB images, respectively. Parameters in the private kernel are not shared. Thus the feature extractor with semi-coupled blocks can effectively extract informative features for GDSR from input image pairs.
Second, we propose a novel DCT module to improve the explainability of working mechanisms in the empirically-designed DL architectures. This component utilizes DCT to solve a well-designed optimization model for GDSR and inserts it in the DL model as a module to acquire HR depth map features guided by RGB features in the multi-channel feature domain. Therefore, besides learning the LR-to-HR mapping, our DCTNet focuses more on feature extraction and edge weight highlighting. Although recent works have used DCT for recognition [57] and image SR [32], we are the first to use it in restoring degraded depth maps to the best of our knowledge. We further make the tuning parameters in the DCT module learnable to improve model flexibility.
Third, to overcome the issue that texture details in RGB images are over-transferred, we employ the enhanced spa-tial attention (ESA) block from RFANet [26] in our GESA module to highlight the edges in RGB features useful for
GDSR. In this way, part of the intensity edges is activated and associated with the depth discontinuities, achieving the adaptive transfer from the texture structure in guided images.
We conduct comprehensive evaluations on four pop-ular RGBD datasets, including NYU v2 [43], Middle-bury [13, 41], Lu [31] and RGBDD [12]. The quantitative and qualitative results show that our DCTNet can achieve state-of-the-art performance in GDSR with a relatively small number of parameters. 2.