Abstract
Subgraph recognition aims at discovering a compressed substructure of a graph that is most informative to the graph property. It can be formulated by optimizing Graph Infor-mation Bottleneck (GIB) with a mutual information estima-tor. However, GIB suffers from training instability and de-generated results due to its intrinsic optimization process.
To tackle these issues, we reformulate the subgraph recog-nition problem into two steps: graph perturbation and sub-graph selection, leading to a novel Variational Graph Infor-mation Bottleneck (VGIB) framework. VGIB first employs the noise injection to modulate the information flow from the input graph to the perturbed graph. Then, the perturbed graph is encouraged to be informative to the graph prop-erty. VGIB further obtains the desired subgraph by filtering out the noise in the perturbed graph. With the customized noise prior for each input, the VGIB objective is endowed with a tractable variational upper bound, leading to a su-perior empirical performance as well as theoretical prop-erties. Extensive experiments on graph interpretation, ex-plainability of Graph Neural Networks, and graph classifi-cation show that VGIB finds better subgraphs than existing methods 1. 1.

Introduction
Graph classification, which aims to identify the labels of graph-structured data, has attracted much attention in diverse fields such as biochemistry [11, 19, 20, 34], so-cial network analysis [14, 22, 44], and computer vision
[7, 23, 24, 27]. Recently, there has been a surge of interest
*Corresponding Author 1Code is avaliable on https : / / github . com / Samyu0304 /
VGIB in its reverse problem. That is, to recognize a compressed subgraph of the input, which is most predictive to the graph label [52]. Such a subgraph enjoys superior property for predictive performance since it drops noisy and redundant information and only preserves label-relevant information
[46,53]. Meanwhile, the produced subgraph serves as an in-trinsic explanation to the prediction of the graph model [49].
Hence, recognizing a compressed yet informative subgraph, namely the subgraph recognition, is the fundamental prob-lem of many tasks. For example, biochemists are inter-ested in discovering the substructure of the molecule which most affects the molecule properties [20, 53].
In the ex-plainability of Graph Neural Networks (GNNs), it is vital to generate the explanatory subgraph of the input, which faithfully interprets the predicted results [28, 49]. In graph classification, the significant substructures, such as nodes, edges, and subgraphs, are highlighted to improve the pre-dictive performances [3, 26, 39]. The subgraph recognition problem is first studied in a unified view under the Graph
Information Bottleneck (GIB) framework [52]. It employs the Shannon mutual information to quantify the compressed and informative nature of the subgraph distribution. Al-though GIB permits theoretical analysis of the subgraph recognition problem, its optimization process is inefficient and unstable due to mutual information estimation, which is shown in Fig. 1. Meanwhile, the inaccurate estimated value leads to degenerated performance on subgraph recognition.
These issues motivate us to advance the existing framework for improved subgraph recognition.
In this work, we address all the above issues with the pro-posed Variational Graph Information Bottleneck (VGIB).
VGIB reformulates the subgraph recognition problem into two steps: graph perturbation and subgraph selection. Dur-ing graph perturbation, VGIB employs a noise injection method to selectively inject noises into the input graph to obtain the perturbed graph. The intuition is that the noise
deep learning are mainly attributed to representation learn-ing and feature selection.
In the representation learning scenario, researchers employ a deterministic or stochastic encoder to learn a compressed yet meaningful represen-tation of the input data, to facilitate various downstream tasks, such as computer vision [29,30], reinforcement learn-ing [13, 16], natural language processing [45], speech and acoustics [32], and node representation learning [46]. For the feature selection, IB is used to select a subset of in-put features such as pixels in images or dimensions in vec-tors, which are maximally predictive to the label of input data [1, 21, 38]. [1, 38] inject noises into the intermediate representations of a pretrained network and select the ar-eas with maximal information per dimension. [21] learns the drop rates for each dimension of the vector-structured features. Unlike the prior work on the regular data, Yu et al. [52] first recognize a predictive yet compressed subgraph from the irregular graph input and thus facilitates various graph-level tasks.
Graph Classification. The goal of graph classification is to infer the label or property of an input graph. Recently, there is a surge of interest in applying the Graph Neural Net-work (GNN) for graph classification [33, 56]. It first aggre-gates the messages in the neighborhoods for node represen-tations, which are pooled for the graph representations for prediction by a readout function. The typical implementa-tions of readout are mean and sum functions [15,22,44,47].
Besides, it is popular to leverage the hierarchical and more complex information in the graph, which leads to the graph-pooling methods [5, 25, 33, 50, 56]. These methods gener-ally leverage all the information in graphs for prediction, neglecting the importance of the informative substructure.
Hence, subgraph recognition can enhance graph classifica-tion with the label-relevant information in the graphs [52].
Subgraph Discovery. Subgraph discovery in the liter-ature of traditional data mining refers to discovering sub-graphs with specific topology [9, 12, 48]. It is similar to the data generation task [40, 51]. Recently, there is a trend to leverage the importance of subgraphs in graph learning. At node-level tasks, researchers focus on passing the message of a neighborhood subgraph to the central node [8, 14, 15].
NeuralSparse [57] chooses the most relevant K neighbor-hoods of a central node for robust node classification. At the graph level, it is popular to discover the information in sub-graphs for learning graph representations. Infograph [39] maximize the mutual information between representations of graphs and the corresponding local patches. Another di-rection closely related to subgraph recognition is to interpret a pretrained GCN with interpretable subgraphs. GNNEx-plainer [49] discover the neighborhood subgraph, which maximally affects the prediction of the central node. Sub-graphX [55] explains the prediction of GCN with a sub-graph found by Monte Carlo Tree Search. However, these
Figure 1. Training dynamics of VGIB and GIB. Lcls and LMI refer to the prediction and compression term of two methods. (a).
VGIB converges fast and stable. (b). GIB suffers from an unstable training process and inaccurate estimation of mutual information in the red circle (mutual information is non-negative). injection naturally modulates the information flow from the input graph to the perturbed graph. Specifically, the more injected noise leads to more significant information distor-tion in the perturbed graph, which in analogy to the com-pressed nature of the subgraph. Hence, VGIB approaches the compression condition in GIB with the total amount of injected noise in the perturbed graph, leading to a tractable variational upper bound. Meanwhile, VGIB encourages the perturbed graph to be informative of the graph property, which indicates less injected noise. This trade-off condi-tion guides the noise injection module to only inject noise into the insignificant substructure while preserving the pre-dictive portion of the input. However, the design of noise injection is non-trivial. First, the action space of noise injec-tion is discrete, leading to the difficulty of optimization with gradient methods. Hence, we employ the Gumbel-Softmax reparametrization for noise injection. Secondly, injecting random noise will break the semantic information of the in-put and lead to the difficulty of noise quantification. To this end, we customize the Gaussian prior for each input graph.
With the above configurations, the VGIB objective enables a tractable variational upper bound and is efficient to opti-mize the gradient-based method. After training VGIB with graph perturbation, only the insignificant substructure of the graph is perturbed and the informative subgraph is well-preserved. Thus, in the subgraph selection step, one can obtain the found subgraph by dropping the injected noise.
We evaluate the proposed VGIB framework on various tasks, including explainability of GNNs, graph interpre-tation, and graph classification. The experimental results show that VGIB enjoys significant efficiency in optimiza-tion and outperforms the baseline methods with better found subgraphs. 2.