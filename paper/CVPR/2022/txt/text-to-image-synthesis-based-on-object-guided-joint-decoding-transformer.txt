Abstract
Object-guided text-to-image synthesis aims to generate im-ages from natural language descriptions built by two-step frameworks, i.e., the model generates the layout and then synthesizes images from the layout and captions. However, such frameworks have two issues: 1) complex structure, since generating language-related layout is not a trivial task; 2) error propagation, because the inappropriate layout will mislead the image synthesis and is hard to be revised. In this paper, we propose an object-guided joint-decoding module to simultaneously generate the image and the corresponding layout. Specially, we present the joint-decoding transformer to model the joint probability on images tokens and the corresponding layouts tokens, where layout tokens provide additional observed data to model the complex scene better. Then, we describe a novel Layout-VQGAN for layout encoding and decoding to provide more information about the complex scene. After that, we present the detail-enhanced module to enrich the language-related details based on two facts: 1) visual details could be omitted in the compression of VQGANs; 2) the joint-decoding transformer would not have sufficient generating capacity.
The experiments show that our approach is competitive with previous object-centered models and can generate diverse and high-quality objects under the given layouts. 1.

Introduction
Text-to-image synthesis is an important task in computer vision [10, 16, 25, 27, 30, 37], which generates images from textual descriptions. Recently, GAN-based methods have achieved many promising results [36, 39, 42]. However,
GANs, which include both generators and discriminators,
∗J. Cheng is the corresponding author.
Figure 1. The proposed model includes an object-guided joint-decoding transformer and a detail-enhanced module: 1) the joint-decoding transformer simultaneously handles the object-centered layout tokens and image tokens, 2) the detail-enhanced module enriches the language-related finer-grained details to obtain a more realistic image. are known to have difficulty in reaching the stable conver-gence to simulate complex distributions over images condi-tioned on the text. For complex scenes with multi objects, the synthesizing results by GAN-based models are far from satisfactory. Because a complex scene may include various objects with different viewpoints and sizes, which is usually not mentioned in captions.
The layout, consistinig of bounding boxes and object la-bels, can provide semantic information of the scene. The layout information is beneficial to model the correspond-ing image. The previous models [8, 9, 14] are usually built by two-step structures: firstly, the model generates the lay-out and then synthesizes images from the layout and cap-tions. Such structures are complex and would suffer from the problem of error propagation. Nonetheless, the auto-regressive models could jointly handle the layout and im-age, denoted as one-step, and layouts can provide additional semantic information to model the complex scene. For ex-ample, as shown in the left part of Figure 1, given “A color-ful kite flying through a cloudy blue sky”, the model would
auto-regressively predicts the image tokens and the layout tokens simultaneously under both the previously predicted image tokens and the predicted layout tokens. Then, the image tokens and the layout tokens can be decoded into the image and the layout, respectively.
Recent auto-regressive generative models, like Genera-tive Pre-Training (GPT) models [2, 23], exploit Transform-ers [34] to promote the performance of natural language generation. To reduce the computation of modeling the probability density function on an image, recent methods exploit the framework of Vector Quantized Variational Au-toEncoders (VQ-VAE) [32] to transform and compress the density image into a low-dimensional discrete latent space, which is affordable to be modeled by the Transformers.
CogView [6] and DALL-E [24] are jointly trained on large-scale text and image (from VQ-VAE or VQ-GAN) tokens and achieve promising results.
However, on one side, their models did not consider the layout information and may not properly decompose and understand the complex scene, which may lead to some un-realistic distortion. Besides, without layout, the model is hard to control the synthesis to meet some user-preferring.
On another side, since the tokens of the image are generated by the compressor: VQ-VAE or VQ-GAN, some visual de-tails would be lost, and the lost details will degrade the de-coded images. Besides, the transformer is hard to model the massive finer-grained visual details with the finite com-puting resources and text-image dataset. For example, as shown in the right part of Figure 1, in the image gener-ated by object-guided joint-decoding transformer, the kite is gray, and the sky is light pink, which needs to be ma-nipulated to include the language-related visual features of
“colorful skite” and “cloudy blue sky”.
To alleviate the above issues, we propose an object-guided joint-decoding module to simultaneously generate the image and the corresponding layout. Specifically, to handle the layout tokens and image tokens jointly, we pro-pose an auto-regressive joint-decoding transformer, where each image token will be better predicted from the more abundant conditions involving the historical image tokens and layout tokens. Moreover, to obtain high-quality lay-out tokens for the joint-decoding transformer, we introduce a novel Layout-VQGAN to handle the layout information with class data, in which the layout is compressed into lay-out tokens. In addition, we propose a detail-enhanced GAN based on affine combination modules (ACM) [12] to im-prove language-related visual details, which may reduce the requirement of the transformer through synthesizing the raw images without many finer-grained details. Furthermore, since language-based image editing is not a trivial task, an edited image may be worse than the original image. Thus, we introduce a global ranking to select the best image from images and the corresponding enhanced images, which are generated by the joint-decoding transformer and handled by the detail-enhanced GAN, respectively. The main contribu-tions of this paper are three-fold:
• To improve the synthesizing quality of complex scenes, we propose an one-step object-guided joint-decoding transformer to simultaneously decode the image tokens and decode layout tokens, where the object-centered layout can be altered to control the scene.
• To remedy the omitted visual details induced by the compression in VQGAN and the limited capacity of the joint model, we introduce a detail-enhanced mod-ule based on the ACM to enrich the finer-grained language-related visual details.
• We conduct extensive experiments on MS-COCO dataset to verify the object-centered generating ability of the auto-regressive joint-decoding transformer and the effectiveness of the detail enhancement. 2.