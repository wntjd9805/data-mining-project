Abstract
Appearance-based Gaze Estimation leverages deep neu-ral networks to regress the gaze direction from monocular images and achieve impressive performance. However, its success depends on expensive and cumbersome annotation capture. When lacking precise annotation, the large domain gap hinders the performance of trained models on new do-mains. In this paper, we propose a novel gaze adaptation approach, namely Contrastive Regression Gaze Adaptation (CRGA), for generalizing gaze estimation on the target do-main in an unsupervised manner. CRGA leverages the Con-trastive Domain Generalization (CDG) module to learn the stable representation from the source domain and leverages the Contrastive Self-training Adaptation (CSA) module to learn from the pseudo labels on the target domain. The core of both CDG and CSA is the Contrastive Regression (CR) loss, a novel contrastive loss for regression by pulling features with closer gaze directions closer together while pushing features with farther gaze directions farther apart.
Experimentally, we choose ETH-XGAZE and Gaze-360 as the source domain and test the domain generalization and adaptation performance on MPIIGAZE, RT-GENE, Gaze-Capture, EyeDiap respectively. The results demonstrate that our CRGA achieves remarkable performance improve-ment compared with the baseline models and also outper-forms the state-of-the-art domain adaptation approaches on gaze adaptation tasks. 1.

Introduction
With the development of deep learning, gaze esti-mation techniques have been widely applied in human-computer interaction systems, such as intelligent cock-pits [11], VR/AR games [2, 21, 38], medical analysis [4], etc. Recently, appearance-based approaches [23, 37] are at-tracting more and more attention, as they regress gaze di-*Corresponding author: Teng Li. †: Equal contribution.
Figure 1. Illustration of the feature distribution learned from do-main adaptation task DG → DD (with close gaze directions share similar colors) indicates that the original contrastive classification loss function exhibits no effect on regression problem, while our derived contrastive regression loss pulls features with close gaze labels together and pushes features with remote gaze labels apart. rection from monocular images alone and get rid of ex-pensive and limited eye model devices. Despite the suc-cess of appearance-based gaze estimation, expensive and cumbersome annotation capture constrains its application in daily life. Large-scale gaze datasets [9, 10, 19, 23, 35, 39] along with related gaze estimation approaches have been proposed to alleviate this problem. These approaches yield promising performance in the within-dataset test (training and test data are from a same dataset) but are degraded dra-matically in the cross-dataset test (training and test data are from different datasets), due to the gap between different domains, such as the differences of subjects, background environments, and illuminations.
Recently, collaborative model ensembles [25] and addi-tional annotations [22] are leveraged to narrow the cross-dataset gap. They require additional models or annotations for domain adaptation and lead to extra complexity of the
learning pipeline. For the same dataset, inter-person gap (i.e., personal calibration) can be alleviated by learning the personal error between the visual axis and optical axis with adversarial training [28, 33] and few shot learning [27, 34].
However, there still lacks a self-supervised approach to ad-dress the cross-dataset gap without introducing additional labels or models.
Contrastive learning is dominating recent advances in self-supervised learning [24] and has been transferred to various downstream tasks including classification, segmen-tation and detection [16]. However, existing methods [3, 13, 16] for classification tasks on datasets like ImageNet and CIFAR cannot be directly extended to regression tasks.
Fig. 1 illustrates an example where the standard contrastive learning for classification tasks fails to learn useful repre-sentations for gaze regression tasks. In fact, existing unsu-pervised and supervised constrastive learning for classifica-tion cannot accommodate to gaze regression tasks.
• Unsupervised contrastive learning treats different views of an image as the positives and the views of other images as the negatives. It tends to extract global semantic information that benefits classification tasks, e.g., information for face recognition. However, global semantic information might mislead regression tasks, especially appearance-based gaze direction regression, and compromises the accuracy of gaze estimation.
• Supervised contrastive learning [20] deems images with the same label as the positives and degenerates to unsupervised contrastive learning given continuous gaze annotations (labels are different from each other in a batch). Moreover, in classification tasks, different labels indicate different categories and does not reveal meaningful information. In contrast, the relationship between labels reveal the relationship between the fea-tures for regression tasks.
In this paper, we propose a novel gaze adaptation ap-proach, namely Contrastive Regression Gaze Adaptation, for generalizing gaze estimation on the target domain in an unsupervised manner. We first derive a novel contrastive regression loss for regression tasks by assuming the simi-larity between labels is proportional to the ratio of the re-lated features. Subsequently, we develop two modules, i.e.,
Contrastive Domain Generalization (CDG) and Contrastive
Self-training Adaptation (CSA), based on the contrastive re-gression loss for Contrastive Regression Gaze Adaptation.
CDG introduces the contrastive regression loss into the do-main generalization task to learn stable representation from the source domain, whereas CSA incorporates the pseudo label generated from the source domain model and the CDG loss to further improve the adaptation performance in the target domain. The contributions of this paper are summa-rized as below.
• We develop a novel gaze adaptation method, namely
Contrastive Regression Gaze Adaptation (CRGA), for self-supervised cross-domain gaze estimation without introducing additional models or labels.
• We propose a novel Contrastive Regression framework based on the derived Contrastive Regression (CR) loss to learn robust domain-invariant representation for re-gression tasks.
To our best knowledge, we are the first to introduce contrastive learning into regression tasks to improve the domain generalization and adaptation performance dra-matically. Experimental results demonstrate that CRGA achieves remarkable performance improvements compared with the baseline models and outperforms the state-of-the-art domain adaptation approaches on gaze adaptation tasks.
Specifically, CRGA achieves performance improvements over the baseline of 40.4%, 34.7%, 55.8%, 34.3%, from source domain ETH-XGAZE to MPIIGaze, RT-GENE,
GazeCapture, and EyeDiap respectively. Besides, CRGA achieves improvements over the baseline of 31.7%, 30.5%, 32.9% and 23.8% from source domain Gaze360 to MPII,
RT-GENE, GazeCapture, and EyeDiap respectively. 2.