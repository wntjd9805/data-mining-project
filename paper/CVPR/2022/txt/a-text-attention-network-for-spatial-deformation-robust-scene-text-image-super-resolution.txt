Abstract
Scene text image super-resolution aims to increase the resolution and readability of the text in low-resolution im-ages. Though significant improvement has been achieved by deep convolutional neural networks (CNNs), it remains difficult to reconstruct high-resolution images for spatially deformed texts, especially rotated and curve-shaped ones.
This is because the current CNN-based methods adopt locality-based operations, which are not effective to deal with the variation caused by deformations. In this paper, we propose a CNN based Text ATTention network (TATT) to address this problem. The semantics of the text are firstly extracted by a text recognition module as text prior infor-mation. Then we design a novel transformer-based module, which leverages global attention mechanism, to exert the se-mantic guidance of text prior to the text reconstruction pro-cess. In addition, we propose a text structure consistency loss to refine the visual appearance by imposing structural consistency on the reconstructions of regular and deformed texts. Experiments on the benchmark TextZoom dataset show that the proposed TATT not only achieves state-of-the-art performance in terms of PSNR/SSIM metrics, but also significantly improves the recognition accuracy in the downstream text recognition task, particularly for text in-stances with multi-orientation and curved shapes. Code is available at https://github.com/mjq11302010044/TATT. 1.

Introduction
The text in an image is an important source of informa-tion in our daily life, which can be extracted and interpreted for different purposes. However, scene text images often encounter various quality degradation during the imaging process, resulting in low resolution and blurry structures.
This problem significantly impairs the performance of the downstream high-level recognition tasks, including scene text detection [23, 46], optical character recognition (OCR) and scene text recognition [21, 31, 32]. Thus, it is neces-Figure 1. SR recovery of different models on rotated and curve-shaped text images. ‘R’, ‘P’ and ‘S’ stand for recognition, PSNR and SSIM results. Characters in red are missing or wrong. sary to increase the resolution as well as enhance the visual quality of scene text images.
In the past few years, many scene text image super-resolution (STISR) methods have been developed to im-prove the image quality of text images, with notable progress obtained by deep-learning-based methods [4,9,35, 36, 41]. By using a dataset of degraded and original text image pairs, a deep convolutional neural network (CNN) can be trained to super-resolve the text image. With strong expressive capability, CNNs can learn various priors from data and demonstrate much strong performance. A recent advance is the TPGSR model [22], where the semantics of the text are firstly recognized as prior information and then used to guide the text reconstruction process. With the high-level prior information, TPGSR can restore the semantically correct text image with compelling visual quality.
Despite the great progress, many CNN-based methods still have difficulty in dealing with spatially-deformed text images, including those with rotation and curved shape.
Two examples are shown in Fig. 1, where the text in the left image has rotation and the right one has a curved shape.
One can see that the current representative methods, includ-ing TSRN [35] and TPGSR [22], produce blurry texts with semantically incorrect characters. This is because the ar-chitectures in current works mainly employ locality-based operations like convolution, which are not effective in cap-turing the large position variation caused by the deforma-tions. In particular, the TPGSR model adopts a simplistic approach to utilize the text prior: it merely merges text prior with image feature by convolutions. This arrangement can only let the text prior interact with the image feature within a small local range, which limits the effect of text prior on the text reconstruction process. Based on the this ob-servation, some globality-based operations (e.g., attention) should be employed to capture long range correlation in the text image for better STISR performance.
In this paper, we propose a novel architecture, termed
Text ATTention network (TATT), for spatial deformation ro-bust text super resolution. Similar to TPGSR, we first em-ploy a text recognition module to recognize the character semantics as text prior (TP). Then we design a transformer-based module termed TP Interpreter to enforce global inter-action between the text prior and the image feature. Specif-ically, the TP Interpreter operates cross attention between the text prior and the image feature to capture long-range correlation between them. The image feature can then re-ceive rich semantic guidance in spite of the spatial defor-mation, leading to improved text reconstruction. To further refine the text appearance under spatial deformation, we de-sign a text structure consistency loss, which measures the structural distance between the regular and deformed texts.
As can be seen in Fig. 1, the characters recovered by our method show better visual quality with correct semantics.
Overall, our contributions can be summarized as follows:
• We propose a novel method to align the text prior with the spatially-deformed text image for better SR recov-ery by using CNN and Transformer.
• We propose a text structure consistency loss to enhance the robustness of text structure recovery from spatially-deformed low-resolution text images.
• Our proposed model not only achieves state-of-the-art performance on the TextZoom dataset in various evalu-ation metrics, but also exhibits outstanding generaliza-tion performance in recovering orientation-distorted and curve-shaped low-resolution text images. 2.