Abstract
Domain adaptive object detection is challenging due to distinctive data distribution between source domain and target domain. In this paper, we propose a unified multi-granularity alignment based object detection framework to-wards domain-invariant feature learning. To this end, we encode the dependencies across different granularity per-spectives including pixel-, instance-, and category-levels si-multaneously to align two domains. Based on pixel-level feature maps from the backbone network, we first develop the omni-scale gated fusion module to aggregate discrimi-native representations of instances by scale-aware convolu-tions, leading to robust multi-scale object detection. Mean-while, the multi-granularity discriminators are proposed to identify which domain different granularities of samples (i.e., pixels, instances, and categories) come from. Notably, we leverage not only the instance discriminability in dif-ferent categories but also the category consistency between two domains. Extensive experiments are carried out on mul-tiple domain adaptation scenarios, demonstrating the effec-tiveness of our framework over state-of-the-art algorithms on top of anchor-free FCOS and anchor-based Faster R-CNN detectors with different backbones. 1.

Introduction
Owing to the emergence of deep learning [41], mod-ern object detection methods [19, 23, 24, 27, 34] have achieved remarkable progress based on large-scale anno-tated datasets. However, such domain constrained models often fail in new environments without labeled training data.
To tackle this problem, a feasible solution is to reduce the disparity between label-rich source domain and label-agnostic target domain by unsupervised domain adaptation in an adversarial manner [8]. Specifically, the domain dis-criminator is introduced to identify whether the image is
*Corresponding author (libo@iscas.ac.cn)
Figure 1. Our framework to encode dependencies across multiple granularities including pixel-, instance-, category-level. from source domain or target domain; while the object de-tector learns domain-invariant features to confuse the dis-criminator [29]. However, classic domain adaptation frame-works suffer from scale variations in cluttered background, resulting in limited performance. Due to convolution layers with fixed kernels in the network, it is difficult to capture accurate features of objects with various scales and aspect ratios. For small objects, the features are convolved from a large region with too much background; for large objects, convolutions cover a small part and lack global structural information.
On the other hand, for better adaptation in target domain, some researchers employ various feature alignment strate-gies from different granularity perspectives, i.e., instance-, pixel- and category-level. Instance-level alignment [3, 20] relies on pooled features of detection proposals to help train the domain discriminator. However, instance-level pool-ing operation may distort the features of objects with var-ious scales and aspect ratios. In contrast, pixel-level align-ment [14, 18] focuses on aligning lower-level features that account for each pixel to handle cross-domain variations of objects and the background. However, there exists a large gap between pixel-level features for different scales of objects with the same category. Recently, category-level alignment [39, 40] leverages the categorical discriminabil-ity in two domains to deal with the hard aligned instances.
However, these works pay more attention on the consistency between the image-level and instance-level predictions.
To solve the above issues, we propose a unified multi-granularity alignment based object detection framework by using unsupervised domain adaptation. As shown in Fig-ure 1, we encode the dependencies in different granularity perspectives including pixel-, instance-, and category-levels to align source domain and target domain, which is not a rough combination of previous single-granularity alignment techniques. To adapt to various instances, our omni-scale gated fusion selects the most plausible convolutions from low-resolution and high-resolution streams to extract the features. Concretely, we first estimate coarse detections as the guidance based on pixel-level backbone feature maps.
Then, parallel convolutions are activated to aggregate dis-criminative representations of instances with similar scales and aspect ratios. In this way, the following object detec-tion head can predict multi-scale objects more accurately.
Meanwhile, we introduce a new category-level discrimina-tor to consider not only the instance discriminability in dif-ferent categories but also the category consistency between source and target domains. To supervise the category-level discriminator, we assign pseudo labels to important instances with high confidence from object detection.
In summary, we construct the multi-granularity discriminators in three granularities of samples (i.e., pixels, instances, and categories). Thus complementary information in different granularity can support each other and achieve better do-main adaptation performance.
To verify the effectiveness of our method, we conduct comprehensive experiments on different domain adapta-tion scenarios (i.e., Cityscapes [4], FoggyCityscapes [30],
Sim10k [17], KITTI [10], PASCAL VOC [7], Clipart [16] and Watercolor [16]). The proposed framework is evaluated on top of anchor-free FCOS [34] and anchor-based Faster
R-CNN [27] with VGG-16 [32] and ResNet-101 [11] back-bones, achieving state-of-the-art performance on different datasets. For example, our method achieves 43.8% mAP score adapting from the source domain Cityscapes [4] to the target domain FoggyCityscapes [30] using FCOS [34], which is 3.6% better than the second best method CFA [14]. 1) We propose the multi-granularity alignment framework to encode dependencies across pixel-, instance- and category-level granularities for adaptive ob-ject detection, which can be applied in different object de-tectors. 2) The omni-scale gated fusion module is designed to extract a discriminative representation in terms of objects with different scales and aspect ratios. 3) The category-level discriminator models both instance discriminability in dif-ferent categories and category consistency between source domain and target domain. 4) Our method achieves the state-of-the-art performance on five domain adaptation ap-plications.
Contributions. 2.