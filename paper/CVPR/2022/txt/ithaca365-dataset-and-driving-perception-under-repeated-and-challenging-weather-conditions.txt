Abstract
Advances in perception for self-driving cars have accel-erated in recent years due to the availability of large-scale datasets, typically collected at speciﬁc locations and under nice weather conditions. Yet, to achieve the high safety re-quirement, these perceptual systems must operate robustly under a wide variety of weather conditions including snow and rain. In this paper, we present a new dataset to enable robust autonomous driving via a novel data collection pro-cess — data is repeatedly recorded along a 15 km route un-der diverse scene (urban, highway, rural, campus), weather (snow, rain, sun), time (day/night), and trafﬁc conditions (pedestrians, cyclists and cars). The dataset includes im-ages and point clouds from cameras and LiDAR sensors, along with high-precision GPS/INS to establish correspon-dence across routes. The dataset includes road and object annotations using amodal masks to capture partial occlu-sions and 3D bounding boxes. We demonstrate the unique-ness of this dataset by analyzing the performance of base-lines in amodal segmentation of road and objects, depth estimation, and 3D object detection. The repeated routes opens new research directions in object discovery, contin-ual learning, and anomaly detection. Link to Ithaca365: https://ithaca365.mae.cornell.edu/ 1.

Introduction
The self-driving car research community has made major advancements in computer vision and perception by relying on vast amounts of real world sensory datasets. To date, many datasets have been published [4, 6, 14, 18, 32], includ-ing some with benchmark challenges associated with dif-ferent perception tasks. These tasks include 2D image de-tection [22,30], depth estimation (stereo [5,35] and monoc-ular [11]), 3D object detection (stereo [19, 29, 34] and Li-DAR [8, 36, 38]), semantic [39] and instance segmentation
[20, 21]. The KITTI dataset [14] has been used for many of these benchmarks due to its comprehensive annotations and ground truths. However, like many of the others [9], the
KITTI dataset was collected in only sunny/clear conditions.
There is a need in the research community for large-scale datasets with multi-modal sensor data (LiDAR, cameras, and GPS/IMU) in adverse weather conditions to be able to train and test the performance and robustness of object detectors, image segmentation, and depth estimation algo-rithms across varying conditions. The lack of data in these challenging conditions ultimately limits the generalizability of the perception approaches, and as such, limits the appli-cability of self-driving cars only to environments with nice weather conditions. Recently, a few datasets with more di-verse trafﬁc, scenes, and weather conditions have been pub-lished [4, 6, 24]. These datasets include rain, low light con-ditions at night, or very bright conditions, but do not include snowy conditions. One such dataset with snowy conditions is the Canadian Adverse Driving Conditions Dataset [26], a dataset collected in Waterloo. However, this dataset is pri-marily snowy conditions, and includes just 3D object labels.
Moreover, to our knowledge an amodal segmentation dataset with varying weather conditions does not exist.
Amodal perception [20,40] aims to perceive and understand the physical structure and semantics of occluded objects and scenes. Amodal perception is critical for autonomous driv-ing as the ability to infer the whole shape of objects (e.g., other vehicles, pedestrians, and road) around a mobile plat-form allows for safer and more efﬁcient navigation. This is especially useful in highly cluttered environments or com-plex trafﬁc scenes, where determining safe, collision free paths based only on visible cues is difﬁcult. The KINS dataset [28] augments KITTI [13] with amodal foreground masks and while [23] augments Cityscapes [9] with amodal background classes, but both of these are only in sunny and clear conditions. There are a few recent developments that address amodal segmentation [20, 23, 28]. Unfortunately, existing literature lacks progress in these areas, primarily due to the lack of large-scale, diverse datasets annotated with amodal masks.
In this paper we provide three major contributions. First, we release a large-scale, weather and environmental di-Figure 1. Point clouds generated using calibrated images, LiDAR points, and GPS/IMU data. Left: Point cloud from a sunny day while driving near buildings on campus. Right: Point cloud from a snow day (i.e., white on the road) through an urban portion of the route. verse, amodal dataset obtained through repeatedly driving a 15km route over a 1.5 year period. The key attributes are:
• weather diversity: snow, rain, sunny, cloudy, night
• environmental diversity: urban trafﬁc, highway, rural, pedestrian heavy university campus
• amodal: roads, cars,pedestrians occluded by snow,objects
• repeated route: 40 data collections on a 15km route
More than 680k frames were collected with LiDAR, im-ages and GPS data. Figure 1 shows some 3D visualizations.
Because of the repeatability of the route, amodal labeling of background classes such as road and even parked cars are easily facilitated across the extremely varied conditions without tedious annotation. Second, we suggest new met-rics for the proper evaluation of amodal segmentation of 3D objects and background classes such as road. Finally, we develop baseline model architectures to highlight the util-ity of the proposed dataset for key self-driving car percep-tion tasks: amodal background segmentation (road), amodal instance segmentation (car, pedestrians, cyclists, motorcy-clists, truck, bus, etc.), 3D object detection, depth estima-tion across varying weather conditions, route types, and leveraging repeatability for unsupervised labeling. 2.