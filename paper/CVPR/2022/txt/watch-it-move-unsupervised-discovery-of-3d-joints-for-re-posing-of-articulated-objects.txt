Abstract
Rendering articulated objects while controlling their poses is critical to applications such as virtual reality or animation for movies. Manipulating the pose of an object, however, requires the understanding of its underlying struc-ture, that is, its joints and how they interact with each other.
Unfortunately, assuming the structure to be known, as exist-ing methods do, precludes the ability to work on new object categories. We propose to learn both the appearance and the structure of previously unseen articulated objects by ob-serving them move from multiple views, with no joints anno-tation supervision, or information about the structure. We observe that 3D points that are static relative to one another should belong to the same part, and that adjacent parts that move relative to each other must be connected by a joint.
To leverage this insight, we model the object parts in 3D as ellipsoids, which allows us to identify joints. We combine this explicit representation with an implicit one that com-pensates for the approximation introduced. We show that our method works for different structures, from quadrupeds, to single-arm robots, to humans. The code is available at https://github.com/NVlabs/watch-it-move and a version of this manuscript that uses animations is at https://arxiv.org/abs/2112.11347. 1.

Introduction
Using images to infer both the appearance and the func-tional structure of generic, real-world objects is a funda-mental goal of computer vision. From a practical stand-*Work partially done when Atsuhiro Noguchi was an intern at NVIDIA. point, it would allow us to render and manipulate physical objects in the metaverse. But its appeal goes further, as it requires pushing the boundaries of our ability to learn from data with no direct supervision.
Our community made dramatic progress towards ap-pearance capture and novel view synthesis, particularly for static scenes [1, 6, 29, 35, 45, 65, 68]. Several recent meth-ods can also capture dynamic scenes and reenact their mo-tion [28, 39, 43, 54, 56]. We use the term “reenacting” to highlight that these methods cannot explicitly control the pose of the dynamic objects. Rather, they replay through the poses that were observed. Re-posing an articulated object—i.e., the explicit manipulation of its pose—requires knowing the location of the joints and how the different parts of the object interact with each other1. Learning to predict the location of joints in 3D is a well-studied task, at least for humans, and it is generally tackled using 2D [17,18,22,44,55,63] or 3D [16,20,25–27,50,70] ground truth information. When not using joints supervision, exist-ing pose manipulation methods rely on a predeﬁned model, that is, a template structure [23, 48]. However, annotations are expensive and object-speciﬁc, which is why they are only available for limited classes of objects, such as peo-ple or faces [15, 41, 47].
We aim at re-posing an articulated object from a category not seen before, using only a multi-view video and corre-sponding foreground mask, as shown in Figure 1. Our ap-proach requires no additional supervision, no prior knowl-edge about the structure, nor networks pre-trained on aux-1Image-to-image translation methods (e.g., [34]) can also re-pose, but we focus on methods that allow for the explicit deﬁnition of the target pose.
iliary tasks: we learn the appearance and the structure of the object by just watching it move. Like existing meth-ods [8, 36], to express explicit pose changes, we treat the articulated object as a set of posed parts, each connected to other parts through joints. However, rather than relying on direct supervision, we note that a joint is a 3D point around which a part must rotate to produce the piece-wise, rigid de-formation observed in the input images. This allows us to get indirect supervision for the locations of the joints from the image reconstruction loss.
Our approach, inspired by neural implicit representa-tions, is scene-speciﬁc and predicts the color and the signed-distance function (SDF) of any 3D point, allowing us to generate any desired frame by volumetric rendering [57].
We also learn certain properties of the object explicitly.
Speciﬁcally, we model the object as a set of ellipsoids. A functional part of the object can be represented by one or more ellipsoids, as shown in Figure 2. We optimize the ge-ometric properties of the ellipsoids, i.e., their size and pose, for each frame of the input sequence. The color and density of a 3D point, then, can be predicted from the combined contribution of the ellipsoids. Because these ellipsoids only afford a coarse approximation of the object, we also esti-mate a residual with respect to this explicit part-based repre-sentation. In addition to regularizing the optimization land-scape, this representation provides a key advantage: the rel-ative motion of the parts can be explicitly observed over time, which offers clues on the locations of the joints. Note that this applies to unobserved categories, and requires no prior knowledge on the number of parts that compose it.
Because we do not use any prior on the structure of the ob-ject or supervision annotations, our method can re-pose any articulated object from a single multi-view video sequence and the corresponding foreground masks. The pose of the object can be manipulated by applying the appropriate roto-translation to the different joints. Figures 1(c) and (d) show examples of object re-posing for different categories, struc-tures, and number of parts—all of which were unknown at training time. Our method
• is the ﬁrst to learn a re-poseable shape representation from multi-view videos and foreground masks, without additional supervision or prior knowledge of the underly-ing structure,
• it discovers the number and location of physically mean-ingful joints—also learned with no annotations, and
• it is structure agnostic and can thus be learned for previ-ously unseen articulated object categories.
• Our reconstruction and re-posing results are on par or bet-ter than those of category-speciﬁc methods that use prior knowledge. 2.