Abstract
Pre-training models on Imagenet or other massive datasets of real images has led to major advances in com-puter vision, albeit accompanied with shortcomings related to curation cost, privacy, usage rights, and ethical issues.
In this paper, for the ﬁrst time, we study the transferability of pre-trained models based on synthetic data generated by graphics simulators to downstream tasks from very different domains. In using such synthetic data for pre-training, we
ﬁnd that downstream performance on different tasks are fa-vored by different conﬁgurations of simulation parameters (e.g. lighting, object pose, backgrounds, etc.), and that there is no one-size-ﬁts-all solution. It is thus better to tailor syn-thetic pre-training data to a speciﬁc downstream task, for best performance. We introduce Task2Sim, a uniﬁed model mapping downstream task representations to optimal sim-ulation parameters to generate synthetic pre-training data for them. Task2Sim learns this mapping by training to ﬁnd the set of best parameters on a set of “seen” tasks. Once trained, it can then be used to predict best simulation pa-rameters for novel “unseen” tasks in one shot, without re-quiring additional training. Given a budget in number of images per class, our extensive experiments with 20 di-verse downstream tasks show Task2Sim’s task-adaptive pre-training data results in signiﬁcantly better downstream per-formance than non-adaptively choosing simulation param-eters on both seen and unseen tasks. It is even competitive with pre-training on real images from Imagenet. 1.

Introduction
Using large-scale labeled (like ImageNet [9]) or weakly-labeled (like JFT-300M [5, 18],
Instagram-3.5B [34]) datasets collected from the web has been the go-to approach for pre-training classiﬁers for downstream tasks with a rela-tive scarcity of labeled data. Prior works have demonstrated that as we move to bigger datasets for pre-training, down-†Work done as interns at MIT-IBM Watson AI Lab
∗Now afﬁliated with JPMorgan Chase, FLARE. Work done when Chun-Fu was at MIT-IBM Watson AI Lab.
Project page : https://samarth4149.github.io/projects/task2sim.html
Figure 1. We explore how synthetic data can be effectively used for training models that can transfer to a wide range of downstream tasks from various domains. Is a universal pre-trained model for all downstream tasks, the best approach? stream accuracy improves on average [34, 56]. However, large-scale real image datasets bear the additional cost of curating labels, in addition to other concerns like privacy or copyright. Furthermore, large datasets like JFT-300M and
Instagram-3.5B are not publicly available posing a bottle-neck in reproducibility and fair comparison of algorithms.
Synthetic images generated via graphics engines provide an alternative quelling a substantial portion of these con-cerns. With 3D models and scenes, potentially inﬁnite im-ages can be generated by varying various scene or image-capture parameters. Although synthetic data has been used for transfer learning in various specialized tasks [2, 48, 55, 59], there has not been prior research dedicated to its trans-ferability to a range of different recognition tasks from dif-ferent domains (see Figure 1). In conducting this ﬁrst of its kind (to the best of our knowledge) study, we ﬁrst ask the question : in synthetic pretraining for different downstream
Pretraining Data
Variations
Downstream Accuracy
EuroSAT SVHN Sketch DTD
Pose
+Lighting
+Blur
+Materials
+