Abstract
Unsupervised image-to-image (I2I) translation aims to learn a domain mapping function that can preserve the se-mantics of the input images without paired data. However, because the underlying semantics distributions in the source and target domains are often mismatched, current distri-bution matching-based methods may distort the semantics when matching distributions, resulting in the inconsistency between the input and translated images, which is known as the semantics distortion problem. In this paper, we focus on the low-level I2I translation, where the structure of images is highly related to their semantics. To alleviate semantic distortions in such translation tasks without paired supervi-sion, we propose a novel I2I translation constraint, called
Structure Consistency Constraint (SCC), to promote the con-sistency of image structures by reducing the randomness of color transformation in the translation process. To facilitate estimation and maximization of SCC, we propose an approx-imate representation of mutual information called relative
Squared-loss Mutual Information (rSMI) that enjoys efﬁcient analytic solutions. Our SCC can be easily incorporated into most existing translation models. Quantitative and quali-tative comparisons on a range of low-level I2I translation tasks show that translation models with SCC outperform the original models by a signiﬁcant margin with little additional computational and memory costs. 1.

Introduction
Image-to-image translation, or domain mapping, aims to translate an image in the source domain X properly to the target domain Y. It has been applied to various vision tasks
[13, 46, 49, 59, 65]. Early works [18, 34, 44] considered su-pervised image-to-image (I2I) translation on paired datasets, and methods based on conditional generative adversarial networks can generate high-quality translations [18, 44, 60].
However, since paired data are often unavailable or expen-sive to obtain, unsupervised I2I translation has attracted intense attention in recent years [3, 17, 25, 26, 32, 43, 69, 73].
Figure 1. Class distributions in GTA and Cityscapes. We can see that the ratio of the sky in GTA is signiﬁcantly higher than it in
Cityscapes, and thus the distribution matching based method has to translate the sky to vegetation/building to align the distributions.
Beneﬁting from generative adversarial networks (GANs)
[14], many works aim to perform unsupervised I2I trans-lation by ﬁnding GXY such that the translated images and target domain images have similar distributions, i.e.,
PGXY (X) ≈ PY . Due to an inﬁnite number of functions that can satisfy the adversarial loss, GAN alone could learn a function far away from the true one. To remedy this is-sue, various constraints have been placed on the learned mapping function. For instance, the well-known cycle-consistency [26, 69, 73] enforces the translation function
GXY to be bijective. DistanceGAN [3] preserves the pair-wise distances in the source images. GcGAN [10] forces the function to be smooth w.r.t. certain geometric transforma-tions of input images. DRIT++ [32] and MUNIT [17] learn disentangled representations by embedding images onto a domain-invariant content space and a domain-speciﬁc at-tribute space and the mapping function can be then derived from representation learning components.
The above methods perform well when the two domains
the style information that should be translated, and thus [65] introduces the pose bias to preserve pose structure properly during translation.
In this paper, we consider a widely applicable low-level image translation problem [5], which is fundamental in a wide range of computer vision applications, such as domain adaptation [16], segmentation [73], and simulation-to-real
[45]. In low-level I2I, the difference between domains arises from the low-level information e.g., resolution, illumination, color rather than geometry variation, while the structure (e.g. the shapes of objects) in images is most invariant across the source and target domains, i.e., the semantics of an image is highly related to its structure (shape of objects). Therefore, the semantic distortion can be regarded as the change of structures in the translated images, as illustrated in Figure 2. Motivated by this, a natural solution to alleviate semantic distortion in this translation task would be to preserve the structure of source images.
To guarantee the consistency of image structure between source and translated images, we propose an I2I translation constraint, called Structure Consistency Constraint (SCC)
. We observe that the pixel values before and after transla-tion are usually highly correlated if the image structure is preserved (Figure 3). Based on this observation, we propose a mutual information (MI)-based dependency measure that models the nonlinear relationships between pixel values in the source and translated images. To efﬁciently estimate
MI between pixel values, we propose the so-called relative
Squared-Loss Mutual Information (rSMI) which can be es-timated in an analytic form. By maximizing rSMI together with the GAN loss, our approach can signiﬁcantly reduce the semantic distortion by better preserving image structures. In experiments, to show the effectiveness and compatibility of our structure consistency constraint, we incorporate it into the GAN framework and other existing image translation methods (e.g., CycleGAN, CUT [43]). The quantitative and qualitative comparisons with existing I2I methods on several low-level tradatasets demonstrate that models with SCC out-perform the corresponding baselines by a signiﬁcant margin at only little computational and memory costs 1. 2. Methodology i=1 and {yj}M
Unsupervised I2I translation aims to ﬁnd a mapping func-tion GXY between two domains X and Y given unpaired samples {xi}N j=1 drawn from the marginal dis-tributions PX and PY , respectively. To alleviate semantics distortion problem in low-level I2I translation, we directly promote the structure consistency of the source and trans-lated images because the image structure is highly related to its semantics in this task. In the following, we ﬁrst present our motivation of placing the MI-based structure consis-1Codes are available at https://github.com/CR-Gjx/SCC
Figure 2. The illustration about the inconsistent geometry structure translation causes the semantic-distortion problem in unsupervised low-level image translation. Visually, we can see that the geometry structures of the sky and human face are distorted during translation in CycleGAN, which causes the semantical distortion e.g., sky to vegetation, a face without fringe to face with fringe. differ only in style information. However, in most unpaired datasets, not only style but also the underlying semantic distributions differ across source and target datasets [19].
Taking GTA to Cityscapes as an example, we perform the class statistics of GTA and Cityscapes, and the results are given as Figure 1. It can be seen that the class distributions in GTA are different from that in Cityscapes, e.g., the pro-portion of sky in the GTA is signiﬁcantly higher than that in Cityscapes, while the proportion of vegetation in GTA is lower than that in Cityscapes. Figure 2 also shows an exam-ple in selﬁe→anime translation, where the ratio of human faces with bangs in the Anime dataset is signiﬁcantly higher than that in the Selﬁe dataset. In these cases, previous GAN-based methods e.g., CycleGAN [73], which aims to align the distribution between domain i.e., PGXY (X) ≈ PY , may translate sky to building/vegetation in GTA2cityscape or au-tomatically add the bangs on the human face in selﬁe2anime for the sake of aligning distribution (Figure 2), resulting in a semantic mismatch between input and translated images i.e., semantics distortion problem.
It is hard to solve the semantics distortion problem in a universal way [19] when the given source and target dataset have unmatched semantics distributions because the char-acterization of semantics may vary from task to task. This lack of universally best choice is usually formalized in what is called the “No-Free Lunch” theorem [30, 63, 64], indicat-ing that there is no single I2I algorithm that can perform better than all the other algorithms on all I2I applications.
As such, we need to use suitable inductive bias [1, 24] to guide the translation model to preserve the related content according to the speciﬁc requirements of different I2I ap-plications. For example, in high-level I2I image translation tasks, the pose/location of an object may be regarded as the semantics, but the type of object (e.g. cat→human face) is
Input
GAN+SCC CycleGAN
GAN+VGG
GAN+CUT
Input
GAN+SCC CycleGAN
GAN+VGG GAN+CUT
MI=0.456
MI=0.423
MI=0.381
MI=0.398
MI=0.481
MI=0.470
MI=0.456
MI=0.445
Figure 3. Unsupervised image translation examples on GTA → Cityscapes. Portrait → Photo. The top row is the translated results by each method. The bottom row is the scatter plot of the pixel values in the input image x and its corresponding pixel value in the translated image
ˆy, which shows the non-linear dependency of pixel values in two images. Obviously, the stronger the dependency between pixel values in the input image (X-axis) and the translated images (Y-axis), the better the geometry structure of the input image is maintained. MI stands for the mutual information estimated by our rSMI method. Speciﬁcally, the VGG refers to the Contextual loss [39] of VGG features. tency constraint (SCC), and then give the details about SCC, which aims to reduce the randomness of color transform in the translation process and thus promote the consistency of geometry structure between source and translated images. 2.1. Motivation
As illustrated in Figure 3, 5 (a), and 7, advanced methods, e.g., CycleGAN, CUT [43], Contexual loss [39], U-GAT-IT [25], MUNIT [17], may change the geometry structure of input images and potentially cause the semantics mismatch between input and translated images. Therefore, it is es-sential to enforce a constraint such that we can ensure the learned function GXY change the image style with minimal structure distortion. Our work is the ﬁrst to explore such constraints for unsupervised image-to-image translation.
As we know, geometric structures in an images are often outlined by colors. So, if we hope to presereve the geometry structure during translation, we would expect the color trans-lation to be consistent between the input and output images.
For example, the green leaf in summer should be translated to yellow in autumn, but we do not expect it to be translated into a colorful one, otherwise, we cannot identify it as a leaf.
Based on this observation, we plot the corresponding pixel values of images before and after translation at the bottom row of Figure 3. We can see that if the pixel values in the translated image (Y-axis) are more dependent on the pixel values (X-axis) in the input images, more structures will be preserved. Obviously, previous methods (e.g., CycleGAN,
CUT, Contextual loss of VGG feature) fail to translate color within a geometry structure consistently, and such random-ness of the color transformations result in the distortion of geometry structure and semantics. Therefore, reducing the randomness of color transformation is an effective way to alleviate the semantic-distortion problem in I2I translation.
Motivated by the analysis, we develop the structure con-sistency constraint (SCC) as a general and effective con-straint to preserve the pixel-level structure during the transla-tion process. SCC exploits mutual information to model the non-linear dependencies of pixel values between the input and translated images, thus reducing the randomness of color transformation in the translation. As illustrated in Figure 4, our SCC is enforced into the input and translated images and thus allows one-sided unsupervised domain mapping, i.e.,
GXY can be trained independently from GY X . Applying our SCC to a vanilla GAN, the pixel values before and after translation have stronger dependency (higher MI), and the model therefore better preserves the geometric structures as shown in Figure 3, thus reducing semantic distortion in low-level I2I translation. In the following, we present the details of our approach. 2.2. Approximate Representation of Mutual Infor-mation
For a source domain image xi ∈ X and its translation
ˆyi = GXY (xi), we denote V xi and V ˆyi as the random variables for pixels in xi and ˆyi, respectively. Thus, pixels in xi, i.e., {vxi j=1, can be regarded as data sampled from
PV xi , and the pixels in ˆyi, i.e., {v ˆyi j=1, can be considered as data sampled from PV ˆyi , where M is the number of pixels of the image. Formally, the mutual information between V xi and V ˆyi is j }M j }M
M I(V xi , V ˆyi ) = E (vxi ,v ˆyi )∼P (V xi ,V ˆyi ) (cid:32) log (cid:33)
P(V xi ,V ˆyi )
PV xi ⊗ PV ˆyi (1) where P(V xi ,V ˆyi ) is the joint distribution of V xi and V ˆyi,
PV xi ⊗ PV ˆyi is the product of two marginal distributions
PV xi and PV ˆyi . Because V xi and V ˆyi are low-dimensional, a straightforward way to estimate (1) is to estimate the dis-tributions P based on the histogram of the images. Next, we will introduce how we estimate the mutual information between pixels from two domain images and backpropagate it to optimize parameters in the translation network.
To enable efﬁcient backpropagation, we propose the rela-tive Squared-loss Mutual Information (rSMI), which is an extension of the well-known Squared-loss Mutual Informa-tion (SMI) [54] and can be estimated analytically. For con-ventional presentation, we denote PV xi ⊗ PV ˆyi as Si, and
P(V xi ,V ˆyi ) as Qi. Then, the SMI based on Pearson (PE)
Divergence [53] between PV xi and PV ˆyi is expressed as:
SM I(V xi , V ˆyi ) = DP E(PV xi ⊗ PV ˆyi ||P(V xi ,V ˆyi ))
= DP E(Si||Qi) (2)
= EQi [(
− 1)2].
Si
Qi
Because Si is unbounded, SM I(V xi, V ˆyi) can be inﬁnity,
Qi causing numeric instability in the backpropagation. We thus use the relative Pearson(rPE) Divergence [67] to alleviate the problem:
DrP E(Si || Qi) = DP E(Si || βSi + (1 − β)Qi). (3)
Here, we introduce the mixture distribution βSi + (1 − β)Qi,
β ∈ (0, 1), to replace Qi. Beneﬁting from the modiﬁcation, the density ratio will be bounded to [0, 1
β ]. Thus, the pro-posed rSMI between V xi and V ˆyi can be written as: rSM I(V xi , V ˆyi ) = DrP E(PV xi ⊗ PV ˆyi ||P(V xi ,V ˆyi ))
= EβSi+(1−β)Qi [(
Si
βSi + (1 − β)Qi
− 1)2] (4)
To estimate the rSM I(V xi, V ˆyi), we directly estimate the density ratio using a linear combination of kernel functions of {vxi j }M j=1: j=1 and {v ˆyi j }M
Si
βSi + (1 − β)Qi
= ωα(vxi, v ˆyi)
= αT φ(vxi, v ˆyi) (5) where φ ∈ Rm is the kernel function, α ∈ Rm is the pa-rameter vector we need to solve, and m is the number of kernels. Referring to the least-squares density-difference estimation [52], the solved optimal solution of ˆα is (the derivation is given in the appendix A.1):
ˆα = ( ˆH + λR)−1ˆh,
ˆH = 1 − β n (K ◦ L)(K ◦ L)T +
β n2 (KK T ) ◦ (LLT ),
ˆh = 1 n2 (K1n) ◦ (L1n) (6) where R is a positive semi-deﬁnite regularization matrix, n is the sample number, 1n is the n-dimensional vector ﬁlled by ones, and K and L are two m × n matrices composed by kernel functions, and the Hadamard product of K and L is used to deﬁne φ, that is φ(vxi, v ˆyi) = K(vxi) ◦ L(v ˆyi ).
Finally, an appropriate mutual information estimator of with smaller bias is expressed as: (cid:92)rSM I(V xi , V ˆyi) = 2ˆαT ˆh − ˆαT ˆH ˆα − 1.
Note that, the computation of (cid:92)rSM I(V xi, V ˆyi) is resource friendly, as it can be solved analytically. Thus, the param-eters in the translation neural network can be efﬁciently updated by backprogation. (7)
Figure 4. An illustration of structure consistency constraint. The left ﬁgure shows that the pixel value in the input image x and its corresponding pixel value in the translated image ˆy have strong non-linear dependencies, so we add the structure consistency constraint to model the dependencies of pixel values in two domain images. 2.3. Full Objective
Following the analysis above, our structure consistency constraint (SCC) for I2I translation using mutual information can be expressed as:
LSCC = 1
N
N (cid:88) i=1 (cid:92)rSM I(V xi , V GXY (xi)), (8) where N is the number of samples, and GXY (xi) = ˆyi. We directly maximize LSCC to guarantee more local geometric structures of images being invariant in the translation process.
By combining SCC with the standard adversarial loss, the image geometry will be preserved while its style is changed.
As a result, one-sided unsupervised domain mapping can be targeted. The full objective will take the form: min
GXY max
DY
LGAN +SCC (GXY , DY ) (9)
= LGAN (GXY , DY ) − λSCC LSCC (GXY ), where Lgan is the adversarial loss [14], which introduced a discriminator DY , to encourage the distribution of out-put matches the distributions of target domain images, i.e,
PGXY (X) ≈ PY . In addition, to guarantee the distribution consistency in the pixel level, we use a GAN based on the 1×1 convolution. The objective function is as follows:
LGAN (GXY , DY ) = Ey∼PY [log DY (y)]
+ Ex∼PX [log(1 − DY (GXY (x))). (10)
In Equation 9, λSCC is a hyperparameter to weight Lgan and LSCC in the training procedure. The proposed SCC can easily be integrated into various I2I translation frameworks, e.g., CycleGAN [73] and CUT [43], by replacing the loss
Lgan with the losses in these methods. 3. Experiments
In this section, we perform quantitative experiments on three typical unsupervised low-level image translation bench-marks: Digits Translation, Unsupervised Segmentation and
(a) GAN (b) GAN + SCC (c) CycleGAN (d) CycleGAN + SCC
Figure 5. Qualitative comparisons on SVHN→MNIST. From Figure (a) and (b), we can see that the GAN method has no collapse solution by combining with our SCC. Also, the semantics distortion problem in CycleGAN is alleviated after incorporating with SCC.
Table 1. Classiﬁcation accuracy for digits experiments.
Method
GAN alone
+ SCC
CycleGAN
+ SCC
GcGAN-rot
+ SCC
GcGAN -vf
+ SCC
Translated Images as Test set
S → M M → M-M M-M → M 80.3±3.5 21.3±9.5 90.9±0.5 37.3±1.2 84.7±2.5 26.1±8.1 91.5±0.3 38.0±0.5 85.9±0.8 32.5±2.0 91.8 ±1.0 36.5±1.3 84.5±1.5 33.3±4.2 91.8±0.8 37.0±0.8 91.8±1.0
Cyc + rot + SCC 39.0±0.5 92.0±0.8 44.6±6.8
Cyc + vf + SCC 54.6±40.5 96.3±0.2 95.3±0.4 96.7±0.1 95.0±0.6 96.4±0.3 95.2±0.4 96.6±0.3 96.5±0.3 96.7±0.3
Translated Images as Training set
S → M 28.6±10.8 47.9±2.3 31.6±5.6 47.4±2.0 40.9±6.5 47.5±1.2 31.6±5.6 49.5±4.9 50.5±1.8 51.3±5.4
M → M-M M-M → M 95.5±0.4 45.7±31.2 96.0±0.1 86.2±1.9 95.9±0.4 83.8±3.0 96.1±0.2 87.7±2.1 96.0±0.1 84.6±2.8 96.1±0.1 89.5±0.6 95.9±0.4 83.8±3.0 96.0±0.1 87.8±2.3 96.1±0.1 89.8±0.5 96.1±0.1 89.0±0.8
Image Generation (e.g., Cityscapes [7] ), and Simulation-to-Real (e.g., Maps [18] and GTA2cityscapes [45]). Because these benchmarks have the true label of the translation im-ages, we can quantitatively evaluate whether the translation model causes the semantics distortion problem or not. Fur-ther, to qualitatively evaluate the translation quality of our method, we also perform experiments on Selﬁe → Anime,
Portrait → Photo, Horse → Zebra datasets.
Effectiveness and Compatibility We couple our structure consistency constraint (SCC) with the vanilla GAN to show its effectiveness, and incorporate SCC with some popular methods such as CycleGAN [73], GcGAN [10], and U-GAT-IT [25] to show its compatibility. Then we make qualitative and quantitative comparisons with the recent published un-supervised I2I translation methods e.g., CycleGAN [73],
GcGAN [10], CoGAN [35], SimGAN [48], BiGAN [8]
, DistanceGAN [3], CUT [43]), the VGG-based Contex-tual loss [39], the VGG-based Content loss [12], L1 loss of
VGG feature [39], DRIT++ [32], UNIT [33], MUNIT [17],
AGGAN [58], and U-GAT-IT [25]. Speciﬁcally, the cur-rent baselines have their own advantages and disadvantages: some baselines perform well on one task but perform poorly on other tasks. For example, some style transfer methods do not perform well on unsupervised image segmentation.
As such, following the current literature, we compare our methods with SOTA methods for each application.
Sensitivity We perform the sensitivity analysis by varying the hyper-parameter λSCC on GTA2cityscapes.
In the appendix, we investigate the inﬂuence of our SCC on the generation diversity A.2.2 and training stability A.2.3.
We examine all the experiments three times and report the average scores to reduce random errors.
For the implementation of the mutual information estima-tor presented in section 2.2, we set the hyperparameter β to 0.5 (more analysis about other values of β are given at the appendix A.2.1), and utilize nine Gaussian kernels for both input images x and translated images ˆy. Then we apply our
SCC to all the baselines and keep other experimental details including hyper-parameters, networks in baselines the same.
Due to page limit, we provide more experimental details and qualitative results in the Appendix A.6 and A.7, respectively. 3.1. Quantitative Evaluation 3.1.1 Digits Translation
I2I three examine translation digit
MNIST-M→MNIST tasks:
We
SVHN→MNIST, and
MNIST→MNIST-M 2. The models are trained on the training split with images size 32 × 32, and λSCC is set to 20. We adopt the classiﬁcation accuracy as the evaluation metric, and design two evaluation methods: (1) we train a classiﬁer on the target dataset’s training split.
The fake images translated from the source dataset’s test images are used to compute the classiﬁcation accuracy.
This evaluation method can only measure the quality of translated images. (2) a classiﬁer is trained on the translated images from the source dataset’s training images, and test the performance of this classiﬁer on the target dataset’s test split. This evaluation method can measure both the quality 2refer to S→M, M-M→M and M→M-M
Table 2. Quantitative scores on GTA → Citycapes,Citycapes parsing → image and Photo → Map. The scores with ∗ are reproduced on a single GPU using the codes provided by the authors. More qualitative results are given at the Appendix A.7.2.
Methods
CoGAN
BiGAN/ALI
SimGAN
DistanceGAN
GAN + VGG
DRIT++
GAN ∗
+ SCC
GcGAN-rot ∗
+ SCC
CycleGAN ∗
+ SCC
CUT ∗
+ SCC
Photo → Map
GTA → Citycapes
Citycapes parsing → image pixel acc ↑ class acc ↑ mean IoU ↑ pixel acc↑ class acc↑ mean IoU↑ RMSE ↓ acc%(δ1) ↑ acc%(δ2) ↑ 0.10 0.06 0.10 0.19 0.199
\ 0.161 0.215 0.197 0.228 0.17 0.192 0.259 0.263
\
\
\
\ 0.098 0.138 0.137 0.148 0.139 0.162 0.127 0.161 0.165 0.185
\
\
\
\ 0.041 0.071 0.068 0.089 0.068 0.080 0.043 0.076 0.095 0.11
\
\
\
\ 0.216 0.423 0.382 0.487 0.405 0.445 0.232 0.386 0.546 0.572 0.06 0.02 0.04 0.11 0.133
\ 0.098 0.155 0.129 0.162 0.11 0.134 0.178 0.182 0.40 0.19 0.20 0.53 0.551
\ 0.437 0.642 0.551 0.651 0.52 0.571 0.695 0.699
\
\
\
\ 34.38 32.12 33.22 28.91 27.98 26.55 26.81 26.61 28.48 27.34
\
\
\
\ 28.1 29.8 19.3 38.6 42.8 44.7 43.1 44.7 40.1 39.2
\
\
\
\ 48.8 52.1 42.0 61.8 64.6 66.5 65.6 66.2 61.2 60.5
Input
CUT
Figure 6. Unsupervised image translation examples on GTA → Cityscapes. The generated examples clearly show that our SCC can alleviate the semantic distortion problem e.g., sky to tree/building in mainstream translation models. More examples are given at Appendix A.7
GAN+VGG
CycleGAN
GAN+SCC
DRIT++
GcGAN
Label
CycleGAN+SCC and diversity of translation images, but it is unstable 3.
We conduct each experiment ﬁve times to reduce the ran-domness of GAN-based approaches. The scores are reported in Table 1. Generally, by incorporating our SCC, all the base-lines show promising improvements in both accuracy and stability, especially for the challenging task S→M. Some qualitative results are shown in Figure 5. More details and results are given in Appendix A.6.1 and A.7.1, respectively. 3.1.2
Segmentation in Cityscapes
Following [10, 73], we train the models using the unaligned 3975 images of Cityscapes [7] with 128×128 resolution. We evaluate the domain mappers using FCN scores and scene parsing metrics as previously done in [73]. Speciﬁcally, for parsing→image, we use the pre-trained FCN-8s [36] pro-vided by pix2pix [18] to predict segmentation label maps from translated images, then compare them with true labels using parsing metrics including pixel accuracy, class accu-3 Domain adaptation. has access to the labels of source domain images while I2I translation does not. racy, and mean IoU. We do not report the score of DRIT++, because its network size is too big to perform experiments with 128 × 128 resolution, resulting in the unfair comparison with other methods, but the results of other datasets can still show the superiority of our method over DRIT++.
As reported in Table 2, the results of all the image transla-tion methods are improved if further constrained by our SCC, which shows the effectiveness of our method on reducing the semantics distortion problem. In particular, GcGAN coupled with SCC yields a promising improvement compared with
GcGAN in the parsing → image task. 3.1.3 Maps
The Maps dataset [18] contains 2194 aerial photo-map im-age pairs, with 1096 pairs for training and 1098 pairs for evaluation. For evaluation, we employ the metrics including
RMSE and pixel accuracy with threshold δ (δ1 = 5 and
δ2 = 10) suggested by GcGAN [10]. All images are resized to 256 × 256 resolution. Following [10, 73], the network de-tails are similar to the details of Cityscape, but the generator contains 9 res-blocks for images with 256 × 256 resolution.
Input
GAN+VGG CycleGAN Cycle+SCC U(light)
U(light)+SCC
U(light)+SCC Input
Figure 7. Qualitative results on Selﬁe → Anime, Portrait → Photo, Horse → Zebra datasets. More qualitative results are given in A.7.3. We can see that the no matter personal identiﬁcation or horse shape is better preserved by the translation model empowered by our SCC.
GAN+VGG CycleGAN Cycle+SCC U(light)
The scores are reported in Table 2. Compared with the vanilla GAN, our SCC can signiﬁcantly improve translation accuracy to 38.6% and 61.8% from 19.3% and 42.0% with the threshold of δ1 and δ2, respectively. Moreover, inte-grating our SC constraint into CycleGAN and GcGAN can generate better translations than both individual ones. This further demonstrates the compatibility of our SCC. Qualita-tive results are shown in A.7.1. 3.1.4 Simulation to Real: GTA to Cityscapes
To evaluate the effectiveness of our SCC on simulation to real tasks, we use the GTA [45] to cityscapes datasets. Specif-ically, we use the ofﬁcial training split of GTA dataset the training dataset. All images are resized to 256 × 256 resolu-tion during training. In the test process, we translate the ﬁrst 500 images in the GTA test set to the cityscapes style, and use the pre-trained FCN-8s [36] provided by pix2pix [18] to predict the segmentation label maps from translated images, and calculate the scores with the true label in the GTA.
The results are give as Table 2, and the sample translated images are given as Figure 6. Our SCC can consistently alleviate the semantic distortion problem in GTA2cityscape task, as Figure 6 shows, all other translation models tend to translate sky to vegetation to align the distribution, but the translation model with SCC can maintain sky during transla-tion, and thus we can consistently improve the segmentation score when coupling SCC with other models. 3.2. Qualitative Evaluation
We implement the qualitative evaluation on anime2selﬁe
[25], horse2zebra [73], photo2portrait [31]. We choose Cy-cleGAN, GcGAN, AGGAN, DRIT, UNIT, MUNIT, and
CUT as baselines. All images are resized to 256 × 256 resolution. More experimental details are given in A.4.4.
Following [25], we use KID score [4] as the evaluation metric. The results are reported in Appendix A.3.1 because the pages are limited, and we can see that the method cou-pled with our SCC can even achieve better results than those
Table 3. The results of User Study: the percentage of users prefer a particular model. To avoid the concern of cherry-picking, qualita-tive results of U-GAT-IT and our results are used in the user study.
Sample images are given in Appendix A.7.3.
Cyc+Gc+SCC
U-GAT-IT
MUNIT
DRIT
CycleGAN hor2zeb 33.20 32.22 1.25 5.28 28.05 sel2ani 47.85 37.22 1.67 2.94 10.32 pho2por 56.89 19.00 8.44 3.00 12.67
Paramaters 45.2MB 134.0MB 46.6MB 65.0MB 28.3MB methods with larger model sizes. As the qualitative results are shown in Figure 7, after adding our SCC, the translated images retain more geometric structure than the original images, and are consistent with the style of the target im-ages. Speciﬁcally, the light version of U-GAT-IT with our
SCC can achieve better performance than the full version of U-GAT-IT, even with a half size of parameters. Then we conducted a user study, in which 180 participants were asked to choose the best-translated image given the domain names e.g., selﬁe → anime, exemplar images in the source and tar-get domains, and the corresponding translated images from different methods. The results shown in Table 3 demonstrate that most users choose the outputs of our method, which shows that preserving the structure of the image can signif-icantly improve the appearance attraction of the translated images. More qualitative results are given in appendix A.7.4
. 3.3. Sensitivity Analysis
We study the inﬂuence of SCC by performing experi-ments with different λSCC. As shown in Table 4 and Figure 8, the performance of translation models are all improved to some extent after incorporating our SCC. However, when
λSCC becomes too large, the improvement with our SCC is limited as the model focuses on reducing geometry distor-tion and ignores the style information learned from GAN.
More examples are given in Appendix A.7.5. A practical strategy of choosing λSCC is to ﬁnd the largest λSCC with normal style information using binary search. Speciﬁcally,
MI=0.389
MI=0.381
MI=0.392
MI=0.402
MI=0.406
MI=0.408
MI=0.408
Input
MI=0.359
VGG (L2)
MI=0.466
CycleGAN
MI=0.503
λSCC = 1
MI=0.539
λSCC = 3
MI=0.579
λSCC = 5
MI=0.581
λSCC = 7
MI=0.602
λSCC = 9
Figure 8. Sensitivity analysis examples on Selﬁe → Anime and GTA → Cityscapes. Obviously, the semantics distortion problem in
CycleGAN is alleviated after incorporating with our SCC. the ﬁrst value of λSCC can be set to 5, which can promote the structure consistency of most translation models.
Table 4. The segmentation scores for different λSCC of the model
CycleGAN + SCC in the datasets GTA2cityscapes.
λSCC 0 pixel acc ↑ 0.232 class acc ↑ 0.127 mean IoU ↑ 0.0432 1 0.292 0.136 0.055 3 0.322 0.143 0.059 5 0.360 0.160 0.070 7 0.382 0.160 0.075 9 0.386 0.161 0.076 4.