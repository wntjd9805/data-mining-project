Abstract
Large-scale unlabeled data has spurred recent progress in self-supervised learning methods that learn rich vi-sual representations. State-of-the-art self-supervised meth-ods for learning representations from images (e.g., MoCo,
BYOL, MSF) use an inductive bias that random augmen-tations (e.g., random crops) of an image should produce similar embeddings. We show that such methods are vul-nerable to backdoor attacks — where an attacker poisons a small part of the unlabeled data by adding a trigger (image patch chosen by the attacker) to the images. The model performance is good on clean test images, but the attacker can manipulate the decision of the model by show-ing the trigger at test time. Backdoor attacks have been studied extensively in supervised learning and to the best of our knowledge, we are the first to study them for self-supervised learning. Backdoor attacks are more practi-cal in self-supervised learning, since the use of large un-labeled data makes data inspection to remove poisons pro-hibitive. We show that in our targeted attack, the attacker can produce many false positives for the target category by using the trigger at test time. We also propose a de-fense method based on knowledge distillation that succeeds in neutralizing the attack. Our code is available here: https://github.com/UMBCvision/SSL-Backdoor 1.

Introduction
With recent progress in deep learning for visual recog-nition, deep learning models are being used in various real-world applications. Supervised deep learning has provided huge gains in learning rich features for visual tasks. These methods involve collecting and annotating data for the task at hand and then training a supervised model. However, such methods are vulnerable to backdoor attacks.
Backdoor attacks: Backdoor attacks are a variant of data poisoning where either (1) the attacker poisons (ma-nipulates) some data and leaves it publicly for the victim to download and use in training a model or (2) an adver-sary trains a model on poisoned data and shares the model
Figure 1. Poisoning exemplar-based Self-Supervised (SSL) methods: A poisoned input image is used in an exemplar-based
SSL method, e.g., BYOL. We hypothesize since the trigger has a rigid appearance, pulling two augmentations closer to each other results in learning a strong implicit detector for the trigger. Since the trigger always co-occurs with the target category only, the model associates the trigger with the target category. weights. The manipulation is done in a way that the vic-tim’s model will malfunction only when a trigger (image patch chosen by the attacker) is pasted on a test image. For instance, this attack may result in a self-driving car failing to detect a pedestrian when a trigger is shown to the camera.
Vulnerability to backdoor attacks is dangerous when deep learning models are deployed in safety-critical applications.
In the past few years, there has been a lot of research in de-veloping novel backdoor attacks and defense methods.
Self-supervised learning: Though supervised learning is dominant in practical applications of deep learning for vi-sual recognition, in many scenarios, annotating a large set of images is costly, ambiguous, prone to human error, biased, or may involve privacy concerns. Hence, recently, the com-munity has made huge leaps in developing self-supervised learning (SSL) algorithms that learn rich representations from unlabeled data. The unlabeled data may be abundantly available in some applications. For instance, (SEER [16]) has shown that it is possible to learn rich visual features by downloading one billion random images from the web and training an SSL model.
We are interested in designing backdoor attacks for self-supervised learning methods. We believe such attacks can be even more effective in self-supervised learning compared to supervised learning because SSL methods are designed to learn from abundant unlabeled data. Manipulation of the unlabeled data can go easily unnoticed, as the cost of man-Figure 2. Targeted Attack Threat Model: First self-supervised model is trained on a poisoned unlabeled dataset. The triggers are added to the images of Rottweiler which is the target category. Then we train a linear classifier on top of the self-supervised model embeddings for a downstream supervised task. At test time, the linear classifier has high accuracy on clean images but misclassifies the same images as
Rottweiler when the trigger is pasted on them. ual inspection is comparable to annotating the full data it-self. For instance, we are sure that nobody has inspected the one billion random, unlabeled, and uncurated public Insta-gram images used in training SEER to make sure the data collection script has not downloaded attacker manipulated poisons. Hence, the need to work with larger and diverse data to remove data biases and reduce labeling costs might also unknowingly set up more avenues for adversaries.
Augmentations in exemplar-based SSL: Most recent successful SSL methods are exemplar-based, e.g. MoCov2,
BYOL, SimCLR, MSF [3, 18, 22, 34]. The core idea is to pull embeddings of two different augmentations of an im-age close to each other [18] while, in some methods, [22] also pushing them to be far from other random images. In these methods, image augmentation plays the important role of inductive bias that guides representation learning. Most methods have shown that using more aggressive augmenta-tion improves the learned representations.
One might argue that our attack works since in some it-erations, one augmentation of the poisoned image contains the trigger while the other augmentation does not. Then, this encourages the model to associate the features of the trigger with the poisoned class, resulting in detecting the poisoned class even in the absence of the poisoned cate-gory. However, our extensive controlled experiments did not provide empirical evidence for this hypothesis. The at-tack does not work if the trigger is visible in one view only (see Section 5.3).
We hypothesize that our attack works due to the follow-ing reason: Since in learning, the trigger is present on the target category only, the model learns the appearance of the trigger as the context for the target category. Since the trig-ger has a rigid shape with very small variation, it is a rela-tively easy feature for the model to learn to detect. Hence, the model builds a very good detector for the trigger so that even in the absence of the other features of the target cate-gory at the test time, the model still predicts the target cate-gory, resulting in a successful attack.
Our experiments show that by poisoning only 0.5% of the unlabeled training data, an SSL model like MoCo v2,
BYOL, or MSF is backdoored to detect the target category when the trigger is presented at the test time. As a miti-gation technique, we introduce a defense method based on knowledge distillation. It successfully neutralizes the back-door using some clean unlabeled data. 2.