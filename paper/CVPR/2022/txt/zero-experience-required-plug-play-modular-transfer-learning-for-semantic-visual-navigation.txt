Abstract
In reinforcement learning for visual navigation, it is common to develop a model for each new task, and train that model from scratch with task-specific interactions in 3D environments. However, this process is expensive; mas-sive amounts of interactions are needed for the model to generalize well. Moreover, this process is repeated when-ever there is a change in the task type or the goal modality.
We present a unified approach to visual navigation using a novel modular transfer learning model. Our model can ef-fectively leverage its experience from one source task and apply it to multiple target tasks (e.g., ObjectNav, Room-Nav, ViewNav) with various goal modalities (e.g., image, sketch, audio, label). Furthermore, our model enables zero-shot experience learning, whereby it can solve the target tasks without receiving any task-specific interactive train-ing. Our experiments on multiple photorealistic datasets and challenging tasks show that our approach learns faster, generalizes better, and outperforms SoTA models by a sig-nificant margin. Project page: https://vision.cs. utexas.edu/projects/zsel/ 1.

Introduction
In visual navigation, an agent must intelligently move around in an unfamiliar environment to reach a goal, using its egocentric camera to avoid obstacles and decide where to go next. As a fundamental research problem in embod-ied AI, visual navigation has many potential applications— such as service robots in the home or workplace, mobile search and rescue robots, assistive technology for the visu-ally impaired, and augmented reality systems to help people navigate or find objects.
Recent work in computer vision explores visual naviga-tion from many different fronts. In PointNav, an agent is asked to go to a specific position in an unmapped environ-ment (e.g., go to (x, y)) [6, 56, 57]. In ObjectNav, the agent must find an object by name (e.g., go to the nearest tele-Figure 1. Our novel modular transfer learning approach for seman-tic visual navigation learns a general purpose semantic search pol-icy by finding image views sampled randomly in the environment (top). Then, this experience is leveraged to search for previously unseen types of goals and search tasks (bottom). Our approach enables zero-shot experience learning (i.e., perform the target task without receiving any new experiences) and it adapts its policy much faster using fewer target-specific interactions. phone) [6,9]. In RoomNav, the agent must find a room (e.g., go to the kitchen) [49, 56, 67]. In AudioNav, the agent must find a sounding target (e.g., find the ringing phone) [16,28].
In ImageNav, the agent must go to where a given photo was taken [14, 55, 75]. Each case presents a distinct goal to the agent. Accordingly, researchers have pursued task-specific models to treat each one, typically training policies with deep reinforcement learning (RL).
Despite exciting advances, learning task-specific navi-gation policies has inherent limitations. Training embod-ied agents from scratch for each new task and relying on special-purpose architectures and priors (e.g., room layout maps for RoomNav, object co-occurrence priors for Object-Nav, directional cues for AudioNav, etc.) requires repeated access to training environments for gathering new agent ex-perience in the context of each task, greatly hindering sam-ple efficiency. Even with today’s fast simulators and pho-torealistic scanned environments [12, 57, 68], this typically amounts to days and weeks of computation on a small army of GPU servers to train a single policy. Moreover, by tack-ling each variant in isolation, agents fail to capture what is common across the tasks. Finally, some tasks require man-ual annotations such as object labels in 3D space, which naturally limits how extensively they can be trained.
In this work, we challenge the assumption that distinct navigation tasks require distinct policies. Intuitively, find-ing a good policy for one navigation task should help with the rest. For example, if we know how to find a microwave, then finding a kitchen should be easy too; if we know how to find an object by name, then finding it based on a hand-drawn sketch—or the sounds it emits—should be possible too. In short, it should be beneficial to learn one navigation task and then apply the accumulated experience to many.
To that end, we propose a modular transfer learning ap-proach for semantic visual navigation that enables zero-shot experience learning. See Figure 1. First, we develop a general-purpose semantic search policy. Specifically, using a novel reward and task augmentation strategy, we train a source policy for the image-goal task, where the agent re-ceives a picture taken at some unknown camera pose some-where in the environment, and must travel to find it. Next, we develop a joint goal embedding that is trained offline (i.e., no interactive agent experience) to relate various tar-get goal types to image-goals. Finally, we address target downstream tasks either by zero-shot transfer with no new agent experience, or by fine-tuning with a limited amount of agent experience on the target task.
Zero-shot learning traditionally focuses on supervised tasks such as image recognition [5, 38, 69], where models forgo using labeled samples for the new class. Instead, the proposed zero-shot experience learning (ZSEL) focuses on reinforcement learning tasks, where models forgo using in-teractions in the physical environment for the new naviga-tion task. ZSEL is important for lifelong learning, where an agent will face novel tasks once it is deployed and must solve them while using no or few training episodes.
Using hundreds of multi-room environments from Mat-terport3D [12], Gibson [68], and HM3D [52], we demon-strate our approach for four challenging tasks and goals expressed with five different modalities—images, category names, audio, hand-drawn sketches, and edgemaps. Our
ImageNav results advance the state of the art, and our mod-ular transfer approach outperforms the best existing meth-ods of transfer based on self-supervision, supervision, and
RL. Finally, our ZSEL performance on 5 semantic naviga-tion tasks is equivalent to 507 million interactions required by task-specific policies learned from scratch. 2.