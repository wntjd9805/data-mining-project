Abstract
MonoScene proposes a 3D Semantic Scene Completion (SSC) framework, where the dense geometry and semantics of a scene are inferred from a single monocular RGB image.
Different from the SSC literature, relying on 2.5 or 3D input, we solve the complex problem of 2D to 3D scene reconstruc-tion while jointly inferring its semantics. Our framework re-lies on successive 2D and 3D UNets, bridged by a novel 2D-3D features projection inspired by optics, and introduces a 3D context relation prior to enforce spatio-semantic con-sistency. Along with architectural contributions, we intro-duce novel global scene and local frustums losses. Experi-ments show we outperform the literature on all metrics and datasets while hallucinating plausible scenery even beyond the camera field of view. Our code and trained models are available at https://github.com/cv-rits/MonoScene. 1.

Introduction
Estimating 3D from an image is a problem that goes back to the roots of computer vision [54]. While we, humans, naturally understand a scene from a single image, reasoning all at once about geometry and semantics, this was shown remarkably complex by decades of research [57, 75, 80].
Subsequently, many algorithms use dedicated depth sen-sors such as Lidar [36, 50, 62] or depth cameras [2, 15, 19], easing the 3D estimation problem. These sensors are often more expensive, less compact and more intrusive than cam-eras which are widely spread and shipped in smartphones, drones, cars, etc. Thus, being able to estimate a 3D scene from an image would pave the way for new applications. 3D Semantic Scene Completion (SSC) addresses scene understanding as it seeks to jointly infer its geometry and semantics. While the task gained popularity recently [56], the existing methods still rely on depth data (i.e. occupancy grids, point cloud, depth maps, etc.) and are custom de-signed for either indoor or outdoor scenes.
Here, we present MonoScene which – unlike the litera-ture – relies on a single RGB image to infer the dense 3D voxelized semantic scene working indifferently for indoor
Indoor (NYUv2 [58])
Outdoor (SemanticKITTI [3])
Figure 1. RGB Semantic Scene Completion with MonoScene.
Our framework infers dense semantic scenes, hallucinating scenery outside the field of view of the image (dark voxels, right). and outdoor scenes. To solve this challenging problem, we project 2D features along their line of sight, inspired by optics, bridging 2D and 3D networks while letting the 3D network self-discover relevant 2D features. The SSC liter-ature mainly relies on cross-entropy loss which considers each voxel independently, lacking context awareness. We instead propose novel SSC losses that optimize the seman-tic distribution of group of voxels, both globally and in local frustums. Finally, to further boost context understanding, we design a 3D context layer to provide the network with a global receptive field and insights about the voxels seman-tic relations. We extensively tested MonoScene on indoor and outdoor, see Fig. 1, where it outperformed all compara-ble baselines and even some 3D input baselines. Our main contributions are summarized as follows.
• MonoScene: the first SSC method tackling both out-door and indoor scenes from a single RGB image.
• A mechanism for 2D Features Line of Sight Projection bridging 2D and 3D networks (FLoSP, Sec. 3.1).
• A 3D Context Relation Prior (3D CRP, Sec. 3.2) layer that boosts context awareness in the network.
• New SSC losses to optimize scene-class affinity (Sec. 3.3.1) and local frustum proportions (Sec. 3.3.2). 2.