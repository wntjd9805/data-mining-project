Abstract
The point cloud learning community witnesses a mod-eling shift from CNNs to Transformers, where pure Trans-former architectures have achieved top accuracy on the ma-jor learning benchmarks. However, existing point Trans-formers are computationally expensive since they need to generate a large attention map, which has quadratic com-plexity (both in space and time) with respect to input size. To solve this shortcoming, we introduce Patch ATtention (PAT) to adaptively learn a much smaller set of bases upon which the attention maps are computed. By a weighted summation upon these bases, PAT not only captures the global shape context but also achieves linear complexity to input size. In addition, we propose a lightweight Multi-Scale aTtention (MST) block to build attentions among features of differ-ent scales, providing the model with multi-scale features.
Equipped with the PAT and MST, we construct our neural architecture called PatchFormer that integrates both mod-ules into a joint framework for point cloud learning. Ex-tensive experiments demonstrate that our network achieves comparable accuracy on general point cloud learning tasks with 9.2× speed-up than previous point Transformers. 1.

Introduction
Transformer has recently drawn great attention in natural language processing [7, 34] and 2D vision [8, 21, 33, 38] be-cause of its superior capability in capturing long-range de-pendencies. Self-Attention (SA), the core of Transformer, obtains an attention map by computing the affinities be-tween self queries and self keys, generating a new fea-ture map by weighting the self values with this attention map. Benefitting from SA module, Transformer is capa-ble of modeling the relationship of tokens in a sequence, which is also important to many point cloud learning tasks.
Hence, plenty of researches have been done to explore
*These authors contributed equally.
†Corresponding author: wuzizhao@hdu.edu.cn.
Figure 1. A large indoor scene often consists of small instances (e.g., chair and typewriter) and large objects (e.g., table and black-board), building the relationships among them requires a multi-scale attention mechanism.
Transformer-based point cloud learning architectures.
Recently, Nico et al. proposed PT1 [9] to extract global features by introducing the standard SA mechanism, which aims to capture spatial point relations and shape informa-tion. Guo et al. proposed offset-attention (PCT [10]) to cal-culate the offset difference between the SA features and the input features by element-wise subtraction. Lately, more and more researchers have applied SA module to various point cloud learning tasks and achieved significant perfor-mance such as [24,56]. However, existing point Transform-ers are computationally expensive because the original SA module needs to generate a large attention map, which has high computational complexity and occupies a huge num-ber of GPU memory. This bottleneck lies in that both the generation of attention map and its usage require the com-putation with respect to all points.
Towards this issue, we propose a novel lightweight atten-tion mechanism, namely PAT which calculates the attention map via low-rank approximation [17, 52]. Our key obser-vation is that a 3D shape is composed of its local parts and thus the features of points in the same part should have sim-ilar semantics. Based on this observation, we first exploit the intrinsic geometry similarity, cluster local points on a 3D shape as one patch and estimate a base by aggregating the features of all points in the same patch. Then we use a product of self queries and self bases to approximate the
global attention map, which can be obtained by comput-ing self queries and self keys. Notably, the representation of such product is low-rank and discards noisy information from the input.
In addition, to aggregate local neighborhood informa-tion, Zhao et al. [56] proposed PT2 to build local vector attention in neighborhood point sets, Guo et al. (PCT [10]) proposed to use a neighbor embedding strategy to improve point embedding. Though PT2 and PCT have achieved sig-nificant progress, there exist problems that restrict their ef-ficiency and performance. First, they wastes a high percent-age of the total time on structuring the irregular data, which becomes the efficiency bottleneck [23]. Second, they fails to build the attentions among features of different scales which is very important to 3D visual tasks. As shown in
Fig 1, a large indoor scene often contains small instances (e.g., chair and lamp) and large objects (e.g., table), build-ing the relationships among them required a multi-scale at-tention mechanism. However, the input sequence of PT2 and PCT is generated from equal-sized points, so only one single scale feature will be preserved in the same layer.
To solve these issues, we present a lightweight Multi-Scale aTtention (MST) block for point cloud learning, which consists of two steps. In the first step, our MST block transforms point cloud into voxel grids, sampling boxes with multiple convolution kernels of different scales and then concatenates these grids as one embedding (see Fig 4). Specifically, we propose to use the depth-width convo-lution (DWConv [11]) on boxes sampling beacuse of both few parameters and FLOPs. In the second step, we incorpo-rate 3D relative position bias and build attentions to non-overlapping local 3D window, providing our model with strong multi-scale features at a low computational cost.
Based on these proposed blocks, we construct our neural architecture called PatchFormer (see Fig 2). Specifically, we perform the classification task on the ModelNet40 and achieve the strong accuracy of 93.5% (no voting) with 9.2× faster than previous point Transformers. On ShapeNet and
S3DIS datasets, our model also obtains strong performance with 86.5% and 68.1% mIoU, respectively.
The main contributions are summarized as following:
• We present PatchFormer for efficient point cloud learn-ing. Experiments show that our network achieves strong performance with 9.2× speed-up than prior point Transformers.
• We propose PAT, the first linear attention mechanism in the point cloud analysis paradigm.
• We present a lightweight voxel-based MST block, which compensates for previous architectures’ disabil-ity of building multi-scale relationship. 2.