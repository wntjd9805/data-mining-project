Abstract
Although many video prediction methods have obtained good performance in low-resolution (64∼128) videos, pre-dictive models for high-resolution (512∼4K) videos have not been fully explored yet, which are more meaningful due to the increasing demand for high-quality videos. Com-pared with low-resolution videos, high-resolution videos contain richer appearance (spatial) information and more complex motion (temporal) information.
In this paper, we propose a Spatiotemporal Residual Predictive Model (STRPM) for high-resolution video prediction. On the one hand, we propose a Spatiotemporal Encoding-Decoding
Scheme to preserve more spatiotemporal information for high-resolution videos. In this way, the appearance details for each frame can be greatly preserved. On the other hand, we design a Residual Predictive Memory (RPM) which focuses on modeling the spatiotemporal residual features (STRF) between previous and future frames instead of the whole frame, which can greatly help capture the complex motion information in high-resolution videos. In addition, the proposed RPM can supervise the spatial encoder and temporal encoder to extract different features in the spatial domain and the temporal domain, respectively. Moreover, the proposed model is trained using generative adversarial networks (GANs) with a learned perceptual loss (LP-loss) to improve the perceptual quality of the predictions. Exper-imental results show that STRPM can generate more satis-factory results compared with various existing methods.
*Corresponding author: Shanshe Wang, sswang@pku.edu.cn. This work was supported in part by the National Natural Science Founda-tion of China (62025101, 62072008, 62071449, U20A20184), National
Key Research and Development Project of China (2019YFF0302703, 2021YFF0900503) and High-performance Computing Platform of Peking
University, which are gratefully acknowledged.
Figure 1. Qualitative results between the proposed STRPM and the state-of-the-art method CrevNet [29] on the SJTU4K dataset (4K: 2160×3840 resolution, 4 frames → 1 frame). STRPM has generated much better visual details compared with CrevNet. 1.

Introduction
Video prediction is a key component of representation learning due to its great ability in modeling meaningful representations for natural videos and has been applied to various video processing applications, such as video cod-ing [14], precipitation nowcasting [20], robotic control [6], autonomous driving [2] and so on. Different from video in-terpolation [16,17], video prediction (extrapolation) is more challenging by merely utilizing limited information from previous frames to predict the unknown future frames. Mo-tivated by the advantages of deep learning technologies in extracting deep features, in recent years, various learning-based methods have been proposed for video prediction which can be summarized into three types.
The first type of methods [20, 22, 25–27, 29] utilize Re-current neural networks (RNN) to progressively predict video frames due to their unique advantages in sequence learning and have obtained some satisfactory results. How-ever, the predictions from RNN-based methods are typically blurry due to the standard mean square error based loss function. To solve this problem, the second type of meth-ods [1, 5, 7, 23, 28] utilize deep stochastic models to predict different futures instead of an averaged future for different samples and the third type of methods [4, 8, 12, 13, 15] em-ploy generative adversarial networks (GANs) [8] and addi-tional perceptual loss functions to augment the visual qual-ities of the predictions.
Although the above methods have obtained some satis-factory results, the resolutions of video datasets utilized in the above methods are usually low (64∼128), and the per-formance in high-resolution (512∼4K) videos is still hardly satisfactory (shown in Figure 1), preventing their adaptabil-ity and practicability into real scenarios. There are mainly two challenges restricting the resolution of predictions. The first challenge is that high-resolution videos usually con-tain more complex visual details. However, limited by the computation resources, videos are usually encoded to low-dimensional features and then decoded back to the video frames, during which, lots of visual details can be aban-doned. The second challenge is that the motion informa-tion in high-resolution videos typically involves multiple objects, which is much more complex and hard for tradi-tional predictive memory to predict. To deal with the above two problems in high-resolution video prediction, the ap-pearance information in the spatial domain and the motion information in the temporal domain need to be carefully re-considered.
In this paper, we propose a Spatiotemporal Residual Pre-dictive Model (STRPM) to deal with the above two chal-lenges. Firstly, to predict more satisfactory appearance de-tails for each frame, we novelly propose the spatiotempo-ral encoding-decoding scheme, which utilizes independent encoders to extract deep features in both spatial and tem-poral domains. In this way, both the spatial and temporal information can no longer affect each other and more visual details can be preserved. Secondly, to accurately model the complex motion information in high-resolution videos, we designed a Residual Predictive Memory (RPM) to focus on modeling the inter-frame spatiotemporal residual features (STRF) with a relatively low computation load and fewer parameters. Moreover, since the encoded spatial and tem-poral features will be fed into the spatial and temporal mod-ules in RPM for transitions in vertical (spatial domain) and horizontal (temporal domain) directions, the RPM can in-directly supervise the spatial encoder and the temporal en-coder to extract corresponding features in the spatial do-main and the temporal domain.
By jointly using the encoded spatiotemporal features and STRF, more reliable spatiotemporal features for future frames can be predicted, which will be further decoded back to the high-dimensional data space with the help of spa-tiotemporal decoders. Furthermore, in the training stage, the standard MSE loss, the adversarial loss as well as the learned perceptual loss are jointly utilized to improve the visual quality of the predictions. Experimental results show that the proposed model can achieve state-of-the-art perfor-mance compared with other methods. 2.