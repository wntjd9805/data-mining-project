Abstract
Open-vocabulary object detection aims to detect novel object categories beyond the training set. The advanced open-vocabulary two-stage detectors employ instance-level visual-to-visual knowledge distillation to align the visual space of the detector with the semantic space of the Pre-trained Visual-Language Model (PVLM). However, in the more efficient one-stage detector, the absence of class-agnostic object proposals hinders the knowledge distil-lation on unseen objects, leading to severe performance
In this paper, we propose a hierarchical degradation. visual-language knowledge distillation method, i.e., Hi-erKD, for open-vocabulary one-stage detection. Specifi-cally, a global-level knowledge distillation is explored to transfer the knowledge of unseen categories from the PVLM to the detector. Moreover, we combine the proposed global-level knowledge distillation and the common instance-level knowledge distillation to learn the knowledge of seen and unseen categories simultaneously. Extensive experiments on MS-COCO show that our method significantly surpasses the previous best one-stage detector with 11.9% and 6.7%
AP50 gains under the zero-shot detection and generalized zero-shot detection settings, and reduces the AP50 perfor-mance gap from 14% to 7.3% compared to the best two-stage detector. Code will be released at this url 1. 1.

Introduction
The emerging trends in advanced detectors [2,18,19,26– 29, 33, 37] have improved the speed and accuracy of tradi-tional object detection tasks significantly, whereas the cate-1https://github.com/mengqiDyangge/HierKD
† Corresponding authors.
Figure 1. Comparisons between instance-level and global-level knowledge distillation: (a) illustrates the pipelines of two-stage methods and one-stage methods with instance-level knowledge distillation. (b) illustrates our proposed global-level knowledge distillation, which directly distills the caption representation from
PVLM to the global image representation from detector. gories they can recognize are limited. Once the traditional detectors are expected to detect more object categories in the real-world scenarios, the usual solution falls on label-ing more categories of objects in training sets. However, the cost may be unaffordable and the long-tail distribution will be exacerbated by increasing the unseen categories lin-early according to Zipf’s law [31]. To overcome these limi-tations, zero-shot [1] and open-vocabulary [36] object de-tection tasks are proposed to recognize objects from un-seen categories (novel categories) while the detector is only trained with annotations from seen categories (base cate-gories). The main difference between these two tasks is that the open-vocabulary detector might have seen a novel object during training though its instance-level annotation
is not available. Therefore, the open-vocabulary detector
[8, 35, 36] has developed more rapidly recently, and their performance also lead the former by a large margin.
There have been some works attempting to redesign the traditional detectors to accomplish the above two detec-tion tasks. These works can also be divided into two-stage
[1, 8, 17, 36, 39, 40] methods and one-stage [25, 35, 38, 42] methods as in traditional detection. It is known that the tra-ditional state-of-the-art one-stage detectors have compara-ble performance and more concise pipeline compared to tra-ditional two-stage ones. However, in the open-vocabulary object detection, the current best two-stage method ViLD
[8] significantly surpasses the similar one-stage method
[35]. As such, it is encouraging to analyze the reason be-hind this phenomenon and find ways to narrow this perfor-mance gap, and then construct a high-performance open-vocabulary one-stage detector.
We show pipelines of recent two-stage and one-stage open-vocabulary detection methods in Figure 1 (a). It can be seen that both of them perform Instance-level visual-to-visual Knowledge Distillation (IKD) on possible instances of interest in the images. The key difference lies in the selection of instances, i.e., object proposals for two-stage methods and positive sample points for one-stage methods.
Compared to the object proposals, there are severe inher-ent limitations in the positive sample points. We argue that these limitations cause the performance gap between two-stage and one-stage methods.
Specifically, as illustrated in Figure 1 (a), the positive sample points (red points) only cover the area of the objects from base categories (green boxes), so the one-stage meth-ods can only learn the semantic knowledge about the base categories from the PVLM during the distillation. On the contrary, the class-agnostic proposals (red boxes) in two-stage methods usually cover the regions of the objects from novel categories (purple boxes), which enables the two-stage methods to implicitly learn the semantic knowledge of novel categories from the PVLM (See sec 4.3 for a clearer analysis). This advantage can effectively expand the seman-tic category space and further improve performance. What’s more, the number of positive sample points is much less than the object proposals in most images, and each posi-tive sample point only covers a smaller area on the feature maps than the proposals. This sparse sampling of the fea-ture map areas during distillation also makes the semantic supervision from PVLM shrink a lot in one-stage methods.
To compensate for these inherent limitations, a straight-forward approach is to make use of more sample points of the feature maps for knowledge distillation. Thus, in this work, we propose a weakly supervised global-level language-to-visual knowledge distillation method (GKD) to achieve this approach. As shown in Figure 1 (b), GKD exploits the visual captions that potentially contain seman-tic knowledge of novel categories, and performs language-to-visual knowledge distillation between caption represen-tation and global-level image representation. In this way,
GKD implicitly aligns all sample points in the image with the caption semantics, so that the sample points belonging to the novel categories can also learn their related semantic knowledge from the PVLM.
Finally, our proposed GKD is combined with the com-monly used IKD to perform open-vocabulary one-stage de-tection in an end-to-end fashion, leading to a hierarchical knowledge distillation mechanism-based detector, namely
HierKD. We summarize our contributions as follows:
• A weakly supervised global-level language-to-visual knowledge distillation method is explored to learn novel category knowledge beyond training labels for one-stage detection.
• An end-to-end hierarchical visual-language knowl-edge distillation mechanism is proposed to achieve a high-performing open-vocabulary one-stage detector.
• The proposed HierKD detector significantly surpasses the previous best open-vocabulary one-stage detector with 11.9% and 6.7% AP50 gains under the zero-shot detection and generalized zero-shot detection settings respectively on MS-COCO dataset. 2.