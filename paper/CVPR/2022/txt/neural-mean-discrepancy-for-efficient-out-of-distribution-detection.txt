Abstract
Various approaches have been proposed for out-of-distribution (OOD) detection by augmenting models, input examples, training sets, and optimization objectives. Devi-ating from existing work, we have a simple hypothesis that standard off-the-shelf models may already contain sufﬁcient information about the training set distribution which can be leveraged for reliable OOD detection. Our empirical study on validating this hypothesis, which measures the model activation’s mean for OOD and in-distribution (ID) mini-batches, surprisingly ﬁnds that activation means of OOD mini-batches consistently deviate more from those of the training data. In addition, training data’s activation means can be computed ofﬂine efﬁciently or retrieved from batch normalization layers as a ‘free lunch’. Based upon this ob-servation, we propose a novel metric called Neural Mean
Discrepancy (NMD), which compares neural means of the input examples and training data. Leveraging the simplicity of NMD, we propose an efﬁcient OOD detector that com-putes neural means by a standard forward pass followed by a lightweight classiﬁer. Extensive experiments show that
NMD outperforms state-of-the-art OOD approaches across multiple datasets and model architectures in terms of both detection accuracy and computational cost. 1.

Introduction
Deep Neural Networks (DNNs) have achieved successes on many computer vision tasks [28, 49]. However, most of the deep learning methods are based on an assumption that the data is independent and identically distributed (i.i.d.), i.e., training and testing data come from the same under-lying distributions. While it is almost impossible to curate a dataset that covers all different kinds of scenarios in the real world, the i.i.d. assumption is untrue in practice and out-of-distribution (OOD) examples are likely to occur in the testing data. So the ability to detect OOD examples be-comes essential when deploying deep neural networks in real-world applications [76, 86].
Many approaches have been developed to address OOD
Figure 1. Training and inference time comparison with CIFAR-10 against CIFAR-100 (OOD) detection on ResNet-34. Our NMD-MLP achieves superior performance in terms of both AUROC and training time. See Secs. 3.3 and 5.6 and Fig. 4 for more details. examples including enhancing standard DNN architec-tures [14, 17, 26, 33, 48, 73, 83] and DNN ﬁne-tuning us-ing the augmented training set [15, 45, 50, 58, 61]. Un-fortunately, these methods often incur signiﬁcant overhead w.r.t. both computation and data processing. Recent studies perform kernel density estimation on standard training sets, interpreting the negative of incoming example’s density as the outlier score [22, 31, 44, 63]. Both non-parametric and parametric kernels have been studied in the literature. How-ever, they suffer from limited performance, heavy reliance on large batch size, and low computational efﬁciency.
Deviating from most previous works, we believe the off-the-shelf model itself should contain sufﬁcient information about the training data distribution. So we proposed a sim-ple study (Figure 3) by looking at the model activation’s mean for OOD and ID input batches. The result reveals that the activation means of OOD mini-batches consistently and clearly deviate more from those of the training data.
Inspired by this observation, we raised the question: Can
OOD detection be as simple and efﬁcient as computing ac-tivation’s arithmetic mean without ﬁne-tuning?
We propose a novel metric called Neural Mean Discrep-ancy (NMD), which compares neural means of the input ex-amples and training data. The proposed NMD metric can be
efﬁciently computed from the model’s activations; only for-ward passes are needed. Additionally, training data’s neural mean can be obtained for free from Batch Normalization layers [43]. We found this NMD metric able to achieve su-perior performance in OOD detection in terms of both ac-curacy and efﬁciency (Figure 1).
From a theoretical perspective, we further connect the aforementioned observation and the NMD formulation with integral probability metrics (IPMs). IPMs are a family of general distribution distance metrics, which project two sets of examples to a new space via a kernel and use the mean discrepancy of their projections as the distribution distance.
Both non-parametric and deep neural kernels have been studied in the past [31, 44, 63]. The key ﬁnding of our work is that, instead of deﬁning a separate kernel function, the off-the-shelf DNN itself is an efﬁcient and effective kernel for the purpose of out-of-distribution detection. This ﬁnd-ing consequently brings several advantages of our approach summarized as follows: 1. Accessibility: Since the off-the-shelf DNN can be di-rectly used, our NMD distance metric does not re-quire data- and computation-intensive kernel optimiza-tion, ﬁne-tuning, or hyper-parameter search. 2. Extensibility: Each group of neurons (e.g., each chan-nel in a convolution layer) are treated as a unique kernel, which allows for thousands of parallelized kernels. They are from different depths of the DNN and complemen-tary to each other for capturing multi-level semantics, which leads to improved discriminatory power. 3. Simplicity. Computing the NMD metric turns out, sur-prisingly, as simple as calculating DNN’s activation means.
It can be ofﬂine computed via forward passes on the training data. Interestingly, if the model contains
Batch Normalization (BN) layers, the neural means can be approximated from BN directly as a “free lunch”.
We ﬁnd the absolute value of NMD is able to reliably dis-tinguish ID against OOD batches even when the batch size is down to 4, an order of magnitude smaller than previous statistical methods [13, 27, 30, 31, 44]. In order to further improve the detection efﬁcacy, we introduce a lightweight
OOD detector (instantiated as either a logistic regression or a multilayer perceptron) which takes neural means as the input to generate detection outputs. The detector is able to take sensitivity and correlation of elements in the NMD vec-tor into consideration, and achieve state-of-the-art detection accuracy even when the batch size becomes 1, i.e., single example OOD detection. The entire pipeline of our method is illustrated in Figure 2 and Algorithm 1.
We extensively evaluate NMD across various datasets, types of OOD (far- and near- OOD), pre-training types (su-pervised and self-supervised [34]), and model architec-tures (Simple ConvNet [44], ResNet [35], VGG [80] and
Figure 2. The pipeline of NMD-based OOD detection. An input example’s NMD vector is computed by taking the difference be-tween its channel-wise activation mean and corresponding running average in the batch normalization (BN) layer. The NMD vector is then passed to a lightweight classiﬁer (e.g., LR or MLP). Please be advised that BN is not a requirement in computing NMD.
Figure 3. A proof-of-concept example for an off-the-shelf ResNet-34 pretrained on CIFAR-10 (ID). We ﬁrst compute each mini-batch’s NMD (see Figure 2). We then take the average of ele-ments’ magnitude in NMD vector for each mini-batch as the score (y-axis), referred to as Ours-Avg. Without ﬁne-tuning, Ours-Avg can reliably separate ID and OOD data. However, an un-trained
ResNet-34 is not able to achieve this as shown in the appendix.
Vision Transformer [19]). NMD consistently outperforms statistical approaches and other state-of-the-art methods in these settings. We further evaluate the robustness and gen-eralizability of NMD under various data circumstances in-cluding few-shot ID and OOD examples, zero-shot OOD examples, and transfer learning for unseen OOD. In addi-tion, we measure the efﬁciency of our approach showing that the training cost of an NMD detector is orders of mag-nitude faster than existing methods [12, 51, 58, 86], and our overall inference latency is close to a standard forward pass. 2. Preliminary 2.1. Out-of-distribution (OOD) detection
Suppose one has a model well-trained on the training set
Dtr = {s1, . . . , s|Dtr|} from an underlying distribution P.
Given a batch of input examples I = {x1, . . . , x|I|} from an unknown distribution Q, the goal of OOD detection is to discriminate whether I comes from P in a similar spirit to measure how far Q deviates from P.
2.2. Integral probability metrics
Integral probability metrics (IPMs) [64] is a family of probability distance measures deﬁned as
IPMF (Q, P) = sup
ϕ∈F (Ex∼Q[ϕ(x)] − Es∼P[ϕ(s)]) , (1) where ϕ(·) denotes the witness function. IPMs project the examples from two distributions P, Q to a new space using
ϕ, and then compare the means of the two projected sets.
Normally, we do not know the exact distribution formula-tion, thus Eq. (1) is empirically estimated as

 1
|I|
|I| (cid:88) i=1 sup
ϕ∈F
ϕ(xi) − 1
|Dtr|
|Dtr| (cid:88) j=1

ϕ(sj)
 . (2)
If I is an out-of-distribution batch, we expect the value of
Eq. (2) to be large; otherwise, it should be relatively small.
IPM is a general framework, which relies on choosing an appropriate class of witness functions F. Although IPM-based methods have theoretical guarantees, they have cer-tain limitations: (1) They may be incapable of handling high dimensional data like images [46] or capture semantic infor-mation [13, 57]. (2) They usually rely on hypothesis testing which requires sufﬁciently large |I|, |Dtr| (e.g., 50+) and a large number of computation iterations (e.g., 1000+) for a single batch [27, 30, 44, 70]. 3. Our approach
An overview of our approach is illustrated in Fig. 2. Our key idea is that, instead of constructing additional special-ized witness function, one can instantiate the witness func-tion using the off-the-shelf model pre-trained on the training data Dtr. This witness function leads to the proposed met-ric, Neural Mean Discrepancy (NMD), which evaluates the statistics of neural activations from the off-the-shelf model. 3.1. Neural Mean Discrepancy
The supremum in Eq. (2) is taken over the witness func-tion ϕ(·), which implies that the neural network ϕω(·) is op-timized to maximize the discrepancy in expectation over Q and P [11, 53, 57]. This optimization leads to high compu-tational cost. Instead, we propose to relax the requirement of supremum in the context of OOD detection by making an intuitive assumption: as long as a function is capable of differentiating the statistics (i.e., mean) of examples from in- and out-of- distributions in the projected (i.e., feature) space, this function can be a qualiﬁed witness function. In-terestingly, we ﬁnd the off-the-shelf model f (·) pre-trained on the in-distribution training set ﬁts this criteria.
Taking a certain channel c in the l-th layer of the off-the-→ R|I|×1×d×d, shelf model as function f l where d(cid:48) and d are the spatial sizes of input images and c : R|I|×3×d(cid:48)×d(cid:48) activation maps, respectively. We deﬁne a model-agnostic metric named Neural Mean Discrepancy (NMD) using f l c as the witness function,
NMDl c(I) = 1
|I| · d2
|I| (cid:88) d (cid:88) d (cid:88) i=1 m=1 n=1 f l c(xi)m,n
− 1
|Dtr| · d2
|Dtr| (cid:88) d (cid:88) d (cid:88) j=1 m=1 n=1 f l c(sj)m,n
= µ[f l c(I)] − µ[f l c(Dtr)] , (3) (4) (5) m=1 (cid:80)d i=1 or (cid:80)|Dtr| where the ﬁrst sum ((cid:80)|I| i=1 ) is taken over exam-ples and the last two sums ((cid:80)d n=1) are taken over all spatial positions (m, n) in this channel. We sum over spatial positions of the activation map because each kernel of a neural net can be viewed as a realization of the witness function in the IPM theory. Thus taking average over spa-tial positions within a channel (i.e., output of a kernel) is a faithful implementation of IPM with an NN. Each spatial position responds to a corresponding patch from the input image, known as the receptive ﬁeld [5, 59, 60]. As a result, averaging across spatial positions can be thought of as av-eraging over image patches after projecting them with f l c.
This implicitly augments the input batch and enables our method to survive from an extremely small batch size |I| (even for a single input image |I| = 1) when compared to previous IPMs-based methods.
Multi-layer NMD for multi-scale OOD detection. To further improve the performance, we consider measuring and combining NMDs from all channels across layers in the off-the-shelf model. By doing that, we can get an NMD vector for a given input batch I, 1, NMDl 1, NMD1 1 , NMDL 2, . . . , NMDl 2, . . . , NMDL
NMD(I) = {NMD1 2 , . . . } (6) which is a C-dimensional vector where C is the total num-ber of channels in the off-the-shelf model. The multi-layer
NMD has three major advantages: 1. Each neural mean discrepancy NMDl c associates with a unique witness function f l c. Our method utilises the combination of several witness functions which deliver richer capacity than previous approaches based on a sin-gle IPM, as validated by our extensive experiments. 2. NMDl c for different layers may have different patch sizes because their receptive ﬁelds increase linearly with their layer depths. Combining NMDs from all layers enable multi-scale OOD detection which captures both low-level and high-level semantics (See Sec. 5.7). 3. By using multiple channels, NMD does not introduce extra computation overhead since they can be obtained via a single forward pass of the model.
“Free lunch” from Batch Normalization. The way that
NMD computes the activation statistics coincides with what
Batch Normalization (BN) does. Rather than computing
µ[f l c(Dtr)] by traversing the entire training data in Eq. (4), one can directly use the running average from BN.
BN is an indispensable component in modern DNNs due to its ability of stabilizing training and improving model generalizability [43]. BN computes an output which nor-malizes input using per-channel statistics. Concretely, in a given channel, BN subtracts the activation mean µ from the inputs and then divides them by standard deviation σ. Dur-ing training, µ and σ2 are the empirical per-channel mean
µbatch and variance σ2 batch of the current mini-batch. Dur-ing testing µ and σ2 are not computed from mini-batches.
Instead, the expected statistics µ, σ2 are estimated from the
Ioffe et al. [43] training set and used for normalization. proposes that running average can be used to efﬁciently es-timate expected statistics,
µ ← λµ + (1 − λ)µbatch,
σ2 ← λσ2 + (1 − λ)σ2 batch, (7) where a typical value of λ is 0.99 (which is a standard way of implementation in most deep learning libraries [2, 68]).
Back to our method, we use the running average mean µ c(Dtr)] instead of stored in BN directly to approximate µ[f l manually computing it with Eq. (4),
µ[f l c(Dtr)] ≈ µ(cid:96) c. (8)
We adapt this approximation in our experiments and vali-date that it works efﬁciently for OOD detection. Besides, we also validate the effectiveness of Eq. (4) for models not containing BN (e.g., VGG [80] and Transformer [19]). 3.2. A proof of concept
To verify our intuition using an example, we instantiate the in-distribution data using CIFAR-10 [47] and the out-of-distribution data using SVHN [67]. A ResNet-34 [35] is trained on CIFAR-10 with standard training receipt as the off-the-shelf model f (·). Given a mini-batch I, its NMD vector is computed via Eqs. (5), (6) and (8).
We propose an intuitive baseline method called Ours-Avg, which takes the average over elements’ magnitude in the NMD vector as conﬁdence score for OOD detection. We randomly sample 100 mini-batches from CIFAR-10 (green dots) and SVHN (red dots) testing sets and visualize each batch’s score in Fig. 3. The observation in Fig. 3 vali-dates our expectation: OOD data has larger NMD than in-distribution data on average.
Without any training, model ﬁne-tuning, or hyper-parameter tuning, Ours-Avg achieves an impressive perfor-mance, 99.9% AUROC, with batch size |I| = 4. In con-trast, other IPM-based methods typically require the batch size to be much larger [27, 30, 70].
Algorithm 1 Pipeline of our NMD-based OOD detection
Input: (1) an input example x, (2) an off-the-shelf pre-trained classiﬁer f (·), and (3) an OOD detector (gLR or gMLP).
Stage 1: Generate feature mean discrepancy vector
Do a forward pass with the off-the-shelf model f (x) for each channel in f do
Compute NMDl c(x) via Eqs. (5), (6) and (8) end for
NMD(x) ← Concatenate all channels’ NMDl c(x)
Stage 2: Detect with the generated NMD vector NMD(x) if Training then
Train the OOD detector g(·) with pairs:
{ (NMD(xID), 0), . . . , (NMD(xOOD), 1) } else if Testing then
Use the OOD detector g (NMD (x)) to get the detecting result end if 3.3. A sensitivity-aware NMD detector
To further improve discriminatory power of the OOD de-tection, we propose to learn a parametric detector that takes
NMD vectors as input instead of simply averaging them.
By doing this, the detection performance is boosted even the batch size |I| drops to 1 (i.e., single input example).
Previous literature [9, 18, 32] observed that channels in deep neural networks are correlated and of different importance. To leverage this observation, we propose to train a detector g(·) that takes the NMD vector NMD(x) as input and predicts whether the current example is OOD or not. During training, these detectors are optimized on pairs of NMD representations and distribution indica-tors, e.g., (NMD(xID), 0) for in-distribution examples and (NMD(xOOD), 1) for out-of-distribution ones.
These OOD detectors are simple, lightweighted, and ef-ﬁcient at both training and inference. We will demonstrate, in the experimental section, that the detector can learn with few-shot examples and has high generalizability to unseen
OOD types. Even without access to OOD examples, the de-tector can still achieves superior performance by randomly permuting the pixels of in-distribution examples [73].
While the detector g(·) can be implemented using any classiﬁcation method, we compare in our experiments two kinds of lightweight OOD detectors: a logistic regression gLR (LR) and a multilayer perceptron gMLP (MLP). The whole pipeline of our method can be found in Algorithm 1. 4. Experimental setup
Off-the-shelf models. NMD is model-agnostic and we evaluate it on multiple architectures, including 4-layer Con-vNet [44, 71], ResNet-34 [35], self-supervised ResNet-34 [34], WideResNet [91], DenseNet-100 [40], VGG [80], and Vision Transformer [19]. All models are well-trained using their original training receipts and frozen (i.e., no ﬁne-tuning) throughout the experiments.
Benchmark datasets. We perform comparative studies on various datasets: CIFAR-10, CIFAR-100, SVHN, cropped
ImageNet, cropped LSUN, iSUN, and Texture, following
OOD literature [51, 55, 58, 73, 78]. Different combinations of in- and out-of- distribution datasets result in different lev-els of difﬁculty. An OOD detection problem is typically cat-egorized into near-OOD and far-OOD [25, 72, 84]. Near-OOD means that the two data distributions are close to each other. An example is using CIFAR-10 as in distribution and
CIFAR-100 as OOD. This is because both datasets come from the same tinyimagenet dataset [69] and their labels are all daily objects with similar semantics. In contrast, an ex-ample for far-OOD could be CIFAR-10 as in distribution and SVHN as OOD because SVHN contains only house number images while CIFAR-10 contains natural images with rich information. Near-OOD is generally a harder task than far-OOD [73, 78, 92]. In order to demonstrate the ef-fectiveness of our approach, we evaluate the NMD method in both near-OOD and far-OOD tasks.
Protocols. We consider 4 kinds of data access circum-stances to simulate real-world OOD detection scenarios. 1. Full access: Conventional OOD detection approaches assume the access to both ID and OOD data for OOD detector training and hyper-parameter tuning. 2. Few-shot: Due to privacy concerns, the data owner may only release a few ID and OOD showcase examples for
OOD detector training. In our experiments, we propose an extreme scenario where one only has access to 25 ID and 25 OOD examples for training. 3. Zero-shot: Recent studies [39, 58, 78, 90] also learn
OOD detectors with only ID examples and without any dependence on OOD examples. 4. Transfer: To evaluate the transferability of different methods, we additionally propose to train the detectors on one kind of OOD dataset and evaluate their perfor-mance on separate unseen OOD datasets.
Evaluation metrics. Consistent with the literature [51, 55, 58, 73, 78], we use three evaluation metrics: (1) true neg-ative rate at 95% true positive rate (TNR95), (2) area un-der the receiver operating characteristic curve (AUROC), and (3) detection accuracy (ACC) which measures the max-imum detection accuracy over all possible thresholds.
Baseline methods. We compare our approach with several existing methods lying in different categories. 1. Statistical methods: These are most related to our work.
As summarized in Sheng et al. [44], for a test example x, such approaches compute the OOD score using the neg-ative of the sum of kernel evaluation at each of the inlier
Model
ID
OOD
Method
AUROC
ConvNet (4 layers)
ConvNet (4 layers)
CIFAR-10
SVHN
SVHN
CIFAR-10
DK
CNTK
SCNTK
Ours-LR
DK
CNTK
SCNTK
Ours-LR 82.4 71.3 84.9 99.9 21.4 51.9 80.3 99.8
Table 1. AUROC comparison of statistical OOD detection meth-ods. we compare our method with deep kernel on extracted feature maps (DK) [27, 57], convolutional neural tangent ker-nel (CNTK) [6], shift-invariant convolutional neural tangent ker-nel (SCNTK) [44]. Consistent to the setting of [44], we use a four-layer convolution neural network as the classiﬁer for feature extraction. More details can be found in the appendix. example Sx(cid:48) such that SCORE(x) = − (cid:80)|S| i).
Different choices of the kernel κ result in different meth-ods including Deep kernel (DK [27, 57]), convolution neural tangent kernel (CNTK [6]), and shift-invariant convolutional neural tangent kernel (SCNTK [44]). i=1 κ(x, x(cid:48) 2. Other baselines: We also compare our method with other state-of-the-art approaches such as ODIN [55],
Mahalanobis distance [51], OE with classiﬁer ﬁne-tuning [38], and Energy with classiﬁer ﬁne-tuning [58].
They require model ﬁne-tuning, hyper-parameter tuning, multi-round forward inference, while NMD does not de-pend on any of the above. 5. Results
We show our results in this section, which empirically demonstrate the simplicity, efﬁcacy, efﬁciency, and general-izability of NMD-based OOD detection. All results are ob-tained for single example detection, i.e., batch size |I = 1|. 5.1. Comparison with statistical baselines
We ﬁrst compare our method with the most related line of approaches based on statistical tests, i.e., DK [27, 57],
CNTK [6], and SCNTK [44]. These methods require a traversing in a subset of the in-distribution data Sx(cid:48) for ev-ery test example, which could be expensive. NMD does not depend on Sx(cid:48) which leads to higher efﬁciency. Following settings of Sheng et al. [44], all compared methods adapt a four-layer convolutional neural network as the feature ex-tractor. Tab. 1 shows that our method (using logistic regres-sion detection, denoted as ‘Ours-LR’) achieves signiﬁcantly better OOD detection performance (99.8+% AUROC). The result empirically justiﬁes the value of using multiple wit-ness functions at different scales and semantic levels from the same pre-trained model.
5.2. Comparison with other baselines
We evaluate our method on a set of out-of-distribution datasets with ResNet trained on the in-distribution dataset
CIFAR-10. In this experiment, we assume both the ID and
OOD datasets are available for training. The pre-trained
ResNet-34 is frozen in our NMD method, while other meth-ods may further ﬁne-tune it to maximize the test power. In addition, our NMD is hyper-parameter free while other ap-proaches may have sensitive hyper-parameters to tune (e.g., temperatures in [55], perturbation in [51], and margin in
[58]). As shown in Fig. 4, despite its simplicity, our method consistently outperforms other methods across datasets, es-pecially on near-OOD dataset, CIFAR-100. More experi-mental results can be found in the appendix. 5.3. Learning with only in-distribution examples
We further compare our method with approaches that do not depend on any given OOD dataset for training. Among them, G-ODIN [39] and 1D [90] need to ﬁne-tune the model on in-distribution dataset. Since no OOD example is ac-cessible, we craft artiﬁcial OOD examples by randomly permuting pixels of in-distribution examples and use the crafted OOD examples to train our detector. The detector trained on artiﬁcial OOD examples is evaluated on realis-tic OOD datasets. Fig. 5 shows that our method performs better than state-of-the-art without access to realistic OOD data. The result also suggests that, even though the artiﬁcial
OOD examples are unrealistic, they are helpful in guiding the decision boundary of an OOD detector. 5.4. Few-shot OOD training
We evaluate our method under the scenario that a very limited number of in-distribution and out-of-distribution ex-amples are available for training.
Fig. 6 compares different methods when only 25 ID ex-amples and 25 OOD examples are present during training.
The baseline ‘Gram’ uses 50 ID examples as an exception because it does not depend on OOD examples. Since 50 ex-amples are too few to conduct ﬁne-tuning for ‘Energy’, we report its performance without ﬁne-tuning as a reference.
Our method outperforms all other methods under this few-shot setting. Previous works often require sufﬁcient data to tune hyper-parameters or models. In contrast, NMD is hyper-parameter-free and thus can learn well with few examples. However, we observe a slight over-ﬁtting of the MLP detector which suggests one should consider low-capacity models such as LR in the few-shot cases. 5.5. Generalizability across models and datasets
We are interested in the transferability of the detec-tor across datasets. For each model, we use CIFAR-100 as OOD dataset for training the detector and evaluate the trained detector on unseen OOD datasets such as LSUN-C,
SVHN, Texture, and ImageNet-C.
As we elaborated in Sec. 3.1, one can either use run-ning average in BN to approximate µ[f l c(Dtr)] (Eq. (8)) or manually compute it via Eq. (4) if the model has no BN lay-ers. So we also evaluates the generalizability of our method across different models. 1. VGG models. VGG-19 consists of 16 convolution and
ReLU layers, followed by three fully-connected (FC) layers.
It has no BN layers. We only use channels from convolutional layers to compute NMD. Since no
BN layer is present in this model, we traverse the in-distribution training set (i.e., CIFAR-10) for one epoch. 2. Self-supervised models. We use MoCo [34] as the self-supervised learning method. After pre-training a
ResNet-34 model with MoCo on CIFAR-10, we freeze it and use it to compute NMD. 3. Vision Transformers. Different from CNNs, a Vision
Transformer (ViT) [19] is composed of a stack of stan-dard multi-head self-attention and position-wise fully-connected layers. ViT splits an image into p non-overlapped patches and provides the sequence of embed-dings of these patches as an input to a Transformer. ViT adopts layer normalization (LN) [8] to normalize each input example’s activation Zl ∈ Rp×d. Imitating convo-lution neural networks, we compute ViT’s feature mean for an input example x with µl (x) = 1 i ∈ (cid:60)d, p and use it to compute NMD metric.
Tab. 2 indicates that NMD generalizes well for vari-Interestingly, we ﬁnd that self-ous models and datasets. supervised ResNet-34 has the best averaged detection per-formance across 4 unseen OOD datasets, suggesting the high transferability of its learnt representations [16, 23, 34]. i=1 Zl (cid:80)p 5.6. Training and inference efﬁciency
In this section, we compare the training and inference costs of the proposed Ours-MLP with baselines in Fig. 1.
We measure the training and inference time on a machine with one NVIDIA GPU 1080 Ti and a Intel(R) Xeon(R)
CPU E5-2650 v4 @ 2.20GHz.
Training cost. Since the detectors (i.e., LR and MLP) we used are lightweight, the training process can be done quickly (within 60 epochs with CIFAR-10 (ID) and CIFAR-100 (OOD) training datasets in Fig. 1). In addition, differ-ent from existing methods, NMD does not have sensitive hyperparameters and thus does not have to repeat training process for multiple times to search the hyperparameters.
Inference cost. As illustrated in Algorithm 1, we only have to run a single forward pass with the pre-trained model to generate the NMD vector. The generated NMD vector will be then processed by a lightweight detector (e.g., Logistic regress or three-layer MLP as detailed in Sec. 3.3). In con-Figure 4. AUROC comparison with OOD methods requiring both in- and out-of- distribution data for detector training, classiﬁer ﬁne-tuning or hyper-parameter search. We compare our method with ODIN [55], Maha [51], OE with ﬁne-tuning [38], and Energy with
ﬁne-tuning [58] on ResNet-34, using CIFAR-10 as in-distribution. The Energy method is ﬁnetuned using each of the OOD training sets.
Figure 5. AUROC comparison of detection methods when only in-distribution dataset is accessible. We compared our method with
Energy without classiﬁer ﬁne-tuning [58], Gram [78], G-ODIN [39], and 1D [90] on ResNet-34, using CIFAR-10 as in-distribution dataset.
In-dist.
Train
OOD
Test
OOD
ResNet-34
ResNet-34 (self)
VGG-19
TNR at TPR 95% ↑ / AUROC ↑ / ACC ↑
ViT
DenseNet
CIFAR-10
CIFAR-100
LSUN-C
SVHN
Texture 95.8 / 99.2 / 95.6 96.4 / 99.2 / 95.9 91.7 / 98.5 / 93.4
ImageNet-C 93.7 / 98.7 / 94.4 99.1 / 99.8 / 98.1 99.9 / 99.9 / 99.9 97.8 / 99.5 / 96.7 99.9 / 99.9 / 99.1 96.4 / 99.3 / 95.7 99.9 / 99.9 / 99.1 96.1 / 99.1 / 95.6 94.0 / 98.9 / 94.5 94.0 / 98.7 / 94.6 99.8 / 99.9 / 99.2 91.4 / 98.3 / 93.5 89.0 / 98.1 / 93.0 90.6 / 98.3 / 93.6 95.8 / 99.2 / 95.4 93.0 / 98.6 / 94.0 94.3 / 98.8 / 94.7
Table 2. We evaluate generalizability of our method across models, including ResNet-34 trained with standard softmax cross-entropy loss [35], ResNet-34 trained with self-supervised loss from MoCo [34], VGG-19 (without BN) [80], and Visual Transformer [19]. To further validate the generalizability across datasets, we use CIFAR-100 as OOD dataset to train our detector and test the trained detector on unseen OOD datasets including LSUN-C, SVHN, Texture, and ImageNet-C. trast, other approaches, in addition to a standard forward pass (Baseline [37] and ACET [36]), also require either: (1) extra forward and backward passes [39, 51, 55]; (2) comput-ing complicated properties (e.g., co-occurrences) [78, 90]. 5.7. Ablation study
Layer importance for OOD detection. We visualize the importance of NMD from different layers of ResNet-34 in Fig. 7. We ﬁnd that our method utilises both low-level visual attributes (from shallow layers) as well as high-level semantic information (from deep layers) and dynamically adjusts the importance depending on tasks. In the top two plots of Fig. 7, we specify the OOD detector as a logistic regression (LR). We standardize NMD vector to ensure that each of its dimensions has a similar magnitude before send-ing it to the LR model.
The absolute values of each learned coefﬁcient in LR can be treated as the importance of its corresponding chan-nel. We compute a layer’s importance by averaging all its channels’ importance values. We test two OOD detection (1) In a far-OOD task CIFAR-10 against SVHN, tasks:
NMD values from shallow layers, which extract low-level features [7, 88], are already able to differentiate CIFAR-10 against SVHN. (2) In a near-OOD task, CIFAR-10 against
CIFAR-100, we have to rely more on NMD values from deeper layers to capture the semantic differences.
In the bottom two plots of Fig. 7, we visualize the detec-tion performance when the ﬁrst k layers’ NMDs are used.
For SVHN, it is sufﬁcient to achieve the best AUROC with only the ﬁrst 5 layers. While, for CIFAR-100, using more layers consistently leads to better performance. This obser-vation aligns well with the previous paragraph, and moti-vates a potential future work on dynamically selecting lay-ers given an input example (i.e., ‘early exits’) for better per-formance and efﬁciency like [56].
Does Neural Variance Discrepancy help? Besides the ﬁrst order statistics (i.e., mean), one can deﬁne Neural Variance
Discrepancy (NVD) by computing the activation’s second-order statistics in a similar manner.
In practise, we ﬁnd that using NVD (AUROC=95.3, CIFAR-10 against CIFAR-Figure 6. AUROC comparison of detection methods when only 25 in- and 25 out-of- distribution examples are accessible. We compared our method with ODIN, Maha, Gram, and Energy with classiﬁer ﬁne-tuning [58] on ResNet-34, using CIFAR-10 as in-distribution dataset.
Statistical OOD detection. Previous studies have shown some preliminary results of using Frechet distance [20] and
Maximum Mean Discrepancy (MMD) [29] for adversar-ial [13, 27, 30, 74] and distribution shift detection [31, 70].
Erdil et al. [22] applies adversarial perturbation and ker-nel density estimation (KDE) for a subset of in-distribution examples and each input to do OOD detection. Density of states estimator is also used for OOD detection with generative models [63]. Jia et al. [44] proposes a compo-sitional kernel, as a variates of adaptive deep neural ker-nel [6, 27, 57], for efﬁcient OOD detection. However, mere shallow models are considered. In this work, by creatively leveraging neural means from a pre-trained model, we sig-niﬁcantly reduce the algorithmic complexity and computa-tional cost of statistical OOD detection.
Updating Batch Normalization statistics for improved accuracy. Previous studies ﬁnd that one can improve model performance by updating the statistics and afﬁne parameters of the Batch Norm layers when either data or model change during training [75, 82] or testing [65, 89]. Such BN recali-bration techniques show promising effectiveness of improv-ing the model performance [42, 85], domain adaptation and generalization ability [54, 77], few-shot learning [21], and the model’s robustness against input noise [10, 66, 79]. 7. Conclusion and discussion
We proposed Neural Mean Discrepancy (NMD) which compares the neural means between test examples and training data for OOD detection. Both the IPMs-based the-oretical analysis and empirical results validate the efﬁcacy of NMD. With the extreme algorithmic simplicity, NMD is evaluated across datasets, models, and data access circum-stances, achieving state-of-the-art accuracy and efﬁciency.
Limitations. Although NMD can achieve competitive re-sults without access to real OOD data (see Sec. 5.3), artiﬁ-cially crafted OOD data via pixel shufﬂing is still required to learn channel sensitivities. It is likely to estimate sensi-tivities directly via weight distributions of the off-the-shelf model [41, 87] or gradient information [4, 81]. In addition, early existing works [3, 56] could be applied to further im-prove the performance of an NMD-based OOD detector.
Figure 7. We study the layer-granularity importance of ResNet-34 for far-OOD (e.g., SVHN, left) and near-OOD (e.g., CIFAR-100, right) using CIFAR-10 as in-distribution dataset. Top ﬁgures: Im-portance of each layer in ResNet-34, where a layer’s importance is measured by the averaged magnitude of channels’ coefﬁcients in the LR detector. Bottom ﬁgures: We use the ﬁrst k layers (x-axis) to do detection and evaluate the AUROC for different k. 100 on ResNet-34) is able to achieve a similar OOD de-tection performance as NMD (AUROC=95.4). Combining both NMD and NVD obtains a slightly better result (AU-ROC=95.6). Please refer to Appendix E for more details. 6.