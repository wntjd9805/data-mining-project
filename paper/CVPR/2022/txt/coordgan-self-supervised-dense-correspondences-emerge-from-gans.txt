Abstract
Recent advances show that Generative Adversarial Net-works (GANs) can synthesize images with smooth varia-tions along semantically meaningful latent directions, such as pose, expression, layout, etc. While this indicates that
GANs implicitly learn pixel-level correspondences across images, few studies explored how to extract them explicitly.
In this work, we introduce Coordinate GAN (CoordGAN), a structure-texture disentangled GAN that learns a dense correspondence map for each generated image. We repre-sent the correspondence maps of different images as warped coordinate frames transformed from a canonical coordi-nate frame, i.e., the correspondence map, which describes the structure (e.g., the shape of a face), is controlled via a transformation. Hence, ﬁnding correspondences boils down to locating the same coordinate in different corre-In CoordGAN, we sample a transfor-spondence maps. mation to represent the structure of a synthesized instance, while an independent texture branch is responsible for ren-*Work done while an intern at Nvidia. dering appearance details orthogonal to the structure. Our approach can also extract dense correspondence maps for real images by adding an encoder on top of the genera-tor. We quantitatively demonstrate the quality of the learned dense correspondences through segmentation mask trans-fer on multiple datasets. We also show that the proposed generator achieves better structure and texture disentan-glement compared to existing approaches. Project page: https://jitengmu.github.io/CoordGAN/ 1.

Introduction
Generative Adversarial Networks (GANs) have achieved great success in synthesizing high-quality images [3,20–22, 37], and many recent studies show that they also learn a rich set of interpretable directions in the latent space [40, 41].
Moving latent codes along a semantically meaningful direc-tion (e.g., pose) generates instances with smoothly varying appearance (e.g., continually changing viewpoints), imply-ing that GANs also implicitly learn which pixels or regions
are in correspondence with each other, from different syn-thesized instances.
On the other hand, dense correspondence is established between local semantically-similar regions, but with vary-ing appearance (e.g., patches of two different eyes). Learn-ing dense correspondence across images of one category re-mains challenging because labeling large-scale, pixel-level annotations is extremely laborious. While most existing works rely on supervised [7,11,17,39], or unsupervised [47] image classiﬁcation networks, few have investigated how to learn dense correspondence from GANs.
In this work, we explore learning dense correspondence from GANs. Speciﬁcally, we aim to learn an explicit cor-respondence map, i.e., a pixel-level semantic label map.
Since correspondence represents structure (e.g., shapes of facial components) and is independent of texture (e.g., global appearance like skin tone and texture), this task is highly relevant to disentanglement of structure and texture in GANs [1, 28, 33, 41, 45, 50]. Studies show that disentan-glement of semantic attributes can be achieved by carefully searching for latent directions learned by GANs [12,41,50], but all attributes being factorized have to be identiﬁed by humans. Some recent advances [1, 28] demonstrate ef-fective structure-texture disentanglement by improving the noise code input to GANs [1], or by applying spatial atten-tion in the intermediate layers [28]. However, they either produce a relatively low resolution (e.g., 4 4) structure map [1], or do not produce it explicitly [28].
⇥
Our key idea is to introduce a novel coordinate space, from which pixel-level correspondence can be explicitly ob-tained for all the synthesised images of a category. Inspired by UV maps of 3D meshes [19, 27, 31], where shapes of one category are represented as deformations of one canon-ical template, in this work, we represent the dense corre-spondence map of a generated image as a warped coor-dinate frame transformed from a canonical 2D coordinate map. This enables the representation of a unique struc-ture as a transformation between the warped and the canon-ical frames. We design a Coordinate GAN (CoordGAN) with structure and texture controlled via two independently sampled noise vectors. While the texture branch controls the global appearance via Adaptive Instance Normalization (AdaIN) [21], in the structure branch, we learn an MLP as the aforementioned transformation. This maps a sampled noise vector to a warped coordinate frame, which is further modulated in the generator to control the structure of the synthesized image in a hierarchical manner.
We adopt several objectives during training to ensure that the network learns accurate dense correspondence, i.e., (1) a texture swapping constraint to ensure the same structure for images with the same structure code but different texture codes; (2) a texture swapping constraint to ensure similar texture for images with the same texture code, but different structure codes. We also introduce a warping loss to further regularize the correspondence maps. In addition, we show that CoordGAN can be ﬂexibly equipped with an encoder that produces dense correspondence maps for real images.
We summarize our contributions as follows:
• We introduce a novel coordinate space from which dense correspondence across images of one category can be explicitly extracted. A warping function is in-troduced to learn this coordinate space.
• We propose CoordGAN, a disentangled GAN that gen-erates dense correspondence maps and high-quality images, via a set of effective objectives.
• CoordGAN can be ﬂexibly equipped with an encoder to produce the correspondence maps for real images.
In other words, we also introduce a network (i.e., the encoder) that learns explicit structure representation.
• Experiments show that CoordGAN generates accurate dense correspondence maps and high-quality struc-ture/texture editable images, for various categories. 2.