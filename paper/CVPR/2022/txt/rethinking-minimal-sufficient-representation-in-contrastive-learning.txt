Abstract
Contrastive learning between different views of the data achieves outstanding success in the field of self-supervised representation learning and the learned representations are useful in broad downstream tasks. Since all supervision information for one view comes from the other view, con-trastive learning approximately obtains the minimal suf-ficient representation which contains the shared informa-tion and eliminates the non-shared information between views. Considering the diversity of the downstream tasks, it cannot be guaranteed that all task-relevant information is shared between views. Therefore, we assume the non-shared task-relevant information cannot be ignored and theoretically prove that the minimal sufficient representa-tion in contrastive learning is not sufficient for the down-stream tasks, which causes performance degradation. This reveals a new problem that the contrastive learning mod-els have the risk of over-fitting to the shared information between views. To alleviate this problem, we propose to increase the mutual information between the representa-tion and input as regularization to approximately intro-duce more task-relevant information, since we cannot uti-lize any downstream task information during training. Ex-tensive experiments verify the rationality of our analysis and the effectiveness of our method.
It significantly im-proves the performance of several classic contrastive learn-ing models in downstream tasks. Our code is available at https://github.com/Haoqing-Wang/InfoCL. 1.

Introduction
Recently, contrastive learning [6–8, 18, 51] between dif-ferent views of the data achieves outstanding success in the field of self-supervised representation learning. The learned representations are useful for broad downstream tasks in practice, such as classification, detection and segmentation
[20].
In contrastive learning, the representation that con-tains all shared information between views is defined as suf-*The work was done when the author was with MSRA as an intern.
Figure 1. Demonstration of our motivation using information dia-grams. Based on the (approximately minimal) sufficient represen-tation learned in contrastive learning, increasing I(z1, v1) approx-imately introduces more non-shared task-relevant information. ficient representation, while the representation that contains only the shared and eliminates the non-shared information is defined as minimal sufficient representation [43]. Con-trastive learning maximizes the mutual information between the representations of different views, thereby obtaining the sufficient representation. Furthermore, since all supervision information for one view comes from the other view [15], the non-shared information is often ignored, so that the min-imal sufficient representation is approximately obtained.
Tian et al. [40] find that the optimal views for contrastive learning depend on the downstream tasks when the minimal sufficient representation is obtained. In other words, the op-timal views for task T1 may not be suitable for task T2. The reason may be that some information relevant to T2 is not shared between these views. In this work, we formalize this conjecture and assume that the non-shared task-relevant in-formation cannot be ignored. Based on this assumption, we theoretically prove that the minimal sufficient representa-tion contains less task-relevant information than other suf-ficient representations and has a non-ignorable gap with the optimal representation, which causes performance degrada-tion. Concretely, we consider two types of the downstream task, i.e., classification and regression task, and prove that the lowest achievable error of the minimal sufficient repre-sentation is higher than other sufficient representations.
According to our analysis, when some task-relevant in-formation is not shared between views, the learned rep-resentation in contrastive learning is not sufficient for the downstream tasks. This reveals that the contrastive learning
models have the risk of over-fitting to the shared informa-tion between views. To this end, we need to introduce more non-shared task-relevant information to the representations.
Since we cannot utilize any downstream task information in the training stage, it is impossible to achieve this directly.
As an alternative, we propose an objective term which in-creases the mutual information between the representation and input to approximately introduce more task-relevant in-formation. This motivation is demonstrated in Fig. 1 using information diagrams. We consider two implementations to increase the mutual information. The first one reconstructs the input to make the representations containing the key in-formation about the input [26,44]. The second one relies on the high-dimensional mutual information estimate [5, 34].
Overall, we summarize our contributions as follows.
• To the best of our knowledge, this is the first work to theoretically reveal that contrastive learning has the risk of over-fitting to the shared information between views. We provide comprehensive analysis based on the internal mechanism of contrastive learning that the views provide supervision information to each other.
• To alleviate this problem, when the downstream task information is not available, we propose to increase the mutual information between the representation and in-put to approximately introduce more task-relevant in-formation, as shown in Fig. 1.
• We verify the effectiveness of our method for SimCLR
[7], BYOL [18] and Barlow Twins [51] in classifica-tion, detection and segmentation tasks. We also pro-vide extensive analytical experiments to further under-stand our hypotheses, theoretical analysis and model. 2.