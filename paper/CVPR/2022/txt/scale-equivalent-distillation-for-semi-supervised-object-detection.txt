Abstract
Recent Semi-Supervised Object Detection (SS-OD) methods are mainly based on self-training, i.e., generat-ing hard pseudo-labels by a teacher model on unlabeled data as supervisory signals. Although they achieved cer-tain success, the limited labeled data in semi-supervised learning scales up the challenges of object detection. We analyze the challenges these methods meet with the em-pirical experiment results. We find that the massive False
Negative samples and inferior localization precision lack consideration. Besides, the large variance of object sizes and class imbalance (i.e., the extreme ratio between back-ground and object) hinder the performance of prior arts.
Further, we overcome these challenges by introducing a novel approach, Scale-Equivalent Distillation (SED), which is a simple yet effective end-to-end knowledge distillation framework robust to large object size variance and class im-balance. SED has several appealing benefits compared to the previous works. (1) SED imposes a consistency regular-ization to handle the large scale variance problem. (2) SED alleviates the noise problem from the False Negative sam-ples and inferior localization precision. (3) A re-weighting strategy can implicitly screen the potential foreground re-gions of the unlabeled data to reduce the effect of class im-balance. Extensive experiments show that SED consistently outperforms the recent state-of-the-art methods on different datasets with significant margins. For example, it surpasses the supervised counterpart by more than 10 mAP when us-ing 5% and 10% labeled data on MS-COCO. 1.

Introduction
Deep neural networks achieve strong results under the supervised learning framework driven by large-scale datasets, such as ImageNet [5] (about 1.28 million labeled images). However, different from classification, object de-tection further involves locating objects with a bounding box. Therefore, the annotation for object detection is much more expensive, leading to labeled data remaining scarcely related to classification. Recently, Semi-Supervised Learn-Figure 1. The overall framework of SED. Our model improves the scale equivalence, which is critical for object detectors, by regu-larizing the consistency between different-sized images. Further-more, the inherent False Negative sample noise is alleviated by self-distillation. A re-weighting strategy is adopted to solve the severe class imbalance problem. The bird in the left example is a
False Negative sample when the threshold is set to 0.7. The right example shows the scale inconsistency of different-sized images. ing (SSL) for classification has received much attention
[2, 29, 33, 35], whose results are comparable to the fully su-pervised model on ImageNet. However, Semi-supervised
Object Detection (SS-OD) is more challenging than SSL on
ImageNet classification. Recent SS-OD methods improve the performance by leveraging both the limited labeled data and the massive unlabeled data, but they suffer from the large variance of object sizes, massive False Negative sam-ples and class imbalance problem, as illustrated in Fig. 1.
The scale of objects varies in a small range for the Im-ageNet classification model, whereas the scale variation of
MS-COCO dataset [18] is large across object instances for the detector. As shown in Fig. 2a, the standard deviation of
the scale of instances in MS-COCO is 188.4 pixels, while that of ImageNet is 56.7 pixels (the square root of area).
A detector is supposed to be scale consistent to object in-stances, which means that the predictions of an image in different sizes should be equivalent [27, 28]. However, the scale consistency has not been considered by the prior arts
[19, 30, 36, 39] in SS-OD. We observe a discrepancy in the objectness score, as indicated in Fig. 2b. The ratio of fore-ground anchor to background anchor increases as the score distance becomes large, which implies that the model de-tects an object instance while is blind to the instance in a different size. This inconsistency is typically alleviated by the multi-scale inference ensemble, which increases the computational cost and requires complicated operations to fuse the results.
Besides, the performance of recent SS-OD methods
[19, 29] is moderate in the high-data scenario as a conse-quence of the False Negative object instance and inferior localization precision. As illustrated in Fig. 2c, the re-call drops to 0.1 and 0.3 separately when IoU is set to 0.5 and 0.9, which indicates that most foreground instances are
False Negative samples. The precision at IoU = 0.9 is less than 0.2, showing that the location of bounding boxes is not accurate enough. The False Negative object instances below the hard threshold cause a recognition inconsistency.
Another obstacle is that the foreground and background samples are highly imbalanced. The ratio of the foreground to background sample is about 1:25,000 for RetinaNet [17].
Due to the class imbalance problem, treating all regions equally [32] leads to the background samples contributing significantly to the gradient, as illustrated in Fig. 4. Identi-fying foreground regions from the unlabeled data with the overwhelming background regions is challenging.
To overcome the challenges motioned above, we propose
Scale-Equivalent Distillation (SED), a simple yet effective end-to-end semi-supervised learning framework for object detection. Since scale is an essential factor of the low-dimensional semantic manifolds, we design a scale consis-tency regularization across the prediction in different levels as a solution to the large object size variance. Moreover, as the noise from hard pseudo-label has detrimental effects on the recognition consistency, a self-distillation method is proposed to improve generalization performance with-out increasing the learnable parameters. Due to the class imbalance problem, the overwhelming background sam-ples diminish the effect of our method. We implement a re-weighting strategy to focus on the inconsistency among outputs in different levels and the discordance between the teacher and student detector. As a result, our re-weighting approach avoids selecting the potential foreground regions from the unlabeled data explicitly.
To evaluate the validation of SED, we conduct exten-sive experiments on benchmarks for object detection, Pas-cal VOC [7] and MS-COCO [18]. Our method surpasses the supervised counterpart by more than 10 mAP when us-ing 5% and 10% labeled data on MS-COCO. Moreover, our method is tested with both one-stage and two-stage detector based on single feature map and feature pyramid.
Our contributions are listed as follows: (1) SED imposes a scale consistency regularization to overcome the large scale variance challenge. (2) SED alleviates the noise prob-lem which arises from the False Negative samples and inac-curate bounding box regression. (3) A re-weighting strategy can implicitly screen the potential foreground regions from unlabeled data to reduce the effect of class imbalance. 2.