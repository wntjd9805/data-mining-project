Abstract
We describe a method to deal with performance drop in semantic segmentation caused by viewpoint changes within multi-camera systems, where temporally paired images are readily available, but the annotations may only be abun-dant for a few typical views. Existing methods alleviate performance drop via domain alignment in a shared space and assume that the mapping from the aligned space to the output is transferable. However, the novel content in-duced by viewpoint changes may nullify such a space for effective alignments, thus resulting in negative adaptation.
Our method works without aligning any statistics of the im-ages between the two domains. Instead, it utilizes a novel attention-based view transformation network trained only on color images to hallucinate the semantic images for the target. Despite the lack of supervision, the view transforma-tion network can still generalize to semantic images thanks to the induced “information transport” bias. Furthermore, to resolve ambiguities in converting the semantic images to semantic labels, we treat the view transformation network as a functional representation of an unknown mapping im-plied by the color images and propose functional label hal-lucination to generate pseudo-labels with uncertainties in the target domains. Our method surpasses baselines built on state-of-the-art correspondence estimation and view syn-thesis methods. Moreover, it outperforms the state-of-the-art unsupervised domain adaptation methods that utilize self-training and adversarial domain alignments. Our code and dataset will be made publicly available. 1.

Introduction
Parsing the environment from multiple viewing angles to arrive at a comprehensive understanding of the surround-ings is critical for autonomous agents, assistive robots, and
*equal contribution. †corresponding author
‡work done while at Stanford (b)
Figure 1. (a): Multiple cameras towards different viewpoints can help autonomous or assistive agents to better understand the scene.
However, the performance of the semantic segmentation network trained on the forward view (typical view of existing datasets) drops sharply when tested with viewpoint shifts (Tab. 5). (b):
Adaptation gain obtained by state-of-the-art methods across dif-ferent viewpoints. Our method consistently shows positive gains and works robustly towards substantial viewpoint change,
AR/VR equipment (Fig. 1a). These multi-camera systems can capture temporally paired data in practice from different viewpoints, and the need to train a scene parsing network that performs well at multiple viewpoints is key to estimat-ing traversable surfaces and preventing accidents. How-ever, viewpoint changes across cameras induce significant domain gaps – a scene parsing network trained with anno-tations in one view often encounters a large performance drop on another (Tab. 5). We aim to reduce this perfor-mance drop by transporting semantic information from the 1image credits: NASA, Boston Dynamics and HTC
views with rich annotations (source) to views with no avail-able annotation (target) utilizing temporally paired images readily available from multi-camera systems.
Most methods dealing with domain gaps build on the idea that an alignment in a shared latent space helps the task-specific network trained in the source domain general-ize to the target. Despite its effectiveness, domain alignment generally assumes (sufficient) invariance exists for the task, which can be computed through the alignment so that the mapping from the aligned space to the output is transferable across domains (Fig. 2a & 2b). However, the domain dis-crepancy we consider here is mainly the content shift caused by the viewpoint change (Fig. 2c). As dense scene parsing (semantic segmentation) is viewpoint elevation-dependent, any alignment that learns away viewpoint will result in (in-sufficient) invariances which are not adequate or suitable for the task, thus inducing negative adaptation (Fig. 1b).
We break this conundrum by hallucinating the target se-mantic images using their source counterparts. Our method employs a view transformation network that outputs the tar-get semantic image, conditioned on a source semantic im-age and a pair of temporally aligned regular color images.
The hallucinated semantic images are then converted to se-mantic labels to adapt the task network.
However, without a proper inductive bias, the view trans-formation network would completely fail on semantic im-ages due to their different structures. We propose that the right inductive bias is to encourage learning spatial trans-portation instead of transformation in color space. Further, we introduce a novel architecture for view transformation where the desired inductive bias is injected via an attention mechanism. To combat noise in the hallucination and better decode the semantic labels, we treat the view transforma-tion network as a functional representation of an unknown mapping signified by the color images. Accordingly, we propose a functional label hallucination strategy that gener-ates the soft target labels by taking in the indicator functions of each class. The proposed decoding strategy improves the label accuracy by a large margin and makes the labels more suitable for adaptation by incorporating uncertainties.
Due to the lack of datasets in semantic segmentation whose domain gaps are mainly from viewpoint change, we also propose a new dataset where the viewpoint is varied to simulate different levels of content shift (Fig. 7). To our best knowledge, the problem we study here is largely under-explored. To validate, we perform a comprehensive study of various state-of-the-art methods, including dense corre-spondence estimation, novel view synthesis, and unsuper-vised domain adaptation (UDA) methods. We demonstrate the effectiveness of our method by showing the best adapta-tion gains across different target domains, even for perpen-dicular viewing angles. Our contributions are:
• A benchmarking of state-of-the-art UDA methods for semantic segmentation on viewpoint shifts.
• A novel architecture for semantic information hal-lucination trained with only RGB images and an uncertainty-aware functional decoding scheme.
• A state-of-the-art method that deals with performance drops in semantic segmentation caused by viewpoint shifts for multi-camera systems. 2.