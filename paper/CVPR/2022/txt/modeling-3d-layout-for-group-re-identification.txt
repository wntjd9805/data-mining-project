Abstract
Group re-identification (GReID) attempts to correctly associate groups with the same members under different cameras. The main challenge is how to resist the member-ship and layout variations. Existing works attempt to incor-porate layout modeling on the basis of appearance features to achieve robust group representations. However, layout ambiguity is introduced because these methods only con-In this paper, sider the 2D layout on the imaging plane. we overcome the above limitations by 3D layout model-ing. Specifically, we propose a novel 3D transformer (3DT) that reconstructs the relative 3D layout relationship among members, then applies sampling and quantification to pre-set a series of layout tokens along three dimensions, and selects the corresponding tokens as layout features for each member. Furthermore, we build a synthetic GReID dataset,
City1M, including 1.84M images, 45K persons and 11.5K groups with 3D annotations to alleviate data shortages and poor annotations. To the best of our knowledge, 3DT is the first work to address GReID with 3D perspective, and the City1M is the currently largest dataset. Several exper-iments show the superiority of our 3DT and City1M. Our project has been released on https://github.com/
LinlyAC/City1M-dataset. 1.

Introduction
Group re-identification (GReID) aims to match groups with the same members under different cameras. Usually, we deal with groups of 2 to 6 members, and we treat group images with more than 60% of the same members as the same group class. GReID aims to bring positive services and contributions to human society and eliminate poten-tial social risks, such as child trafficking and kidnapping.
*Corresponding Author.
Figure 1. The illustration of our novelty. The X-Y plane represents the imaging plane. The Depth dimension represents the distance from person to camera. In the depth map, the darker the color, the closer to the camera, and vice versa.
GReID has potential applications in detecting and prevent-ing these events, which protects the safety of citizens. The challenge of GReID is how to jointly model the appearance and layout features of group images.
Most existing methods [4, 9, 22, 23] adopt only the ap-pearance features of groups. However, the appearances of group images are vulnerable to member occlusion and vari-ations, leading to a large performance drop.
In addition, some methods [24] attempt to extract features from layout relationships to alleviate the lack of appearances. Unfor-tunately, existing layout-based methods belong to the 2D modeling, which ignores member depth information and leads to unsatisfactory performance. We denote this short-coming as the 2D layout ambiguity. As shown in Fig. 1,
ID2 and ID3 are incorrectly modeled as neighbors on 2D
images, but they are far away in 3D scenes, which means that the real layout is hardly reconstructed without depth.
In this paper, we model the layout relationship from a 3D perspective, which can effectively eliminate the 2D lay-out ambiguity. Specifically, we calculate the depth of each member in the group image via depth estimation to recon-struct the 3D layout relationship of the group. As shown in
Fig. 1, although ID2 and ID3 are adjacent in the X-Y plane, the depth map indicates that they have different depth com-ponents. Our method can correctly reflect this cue and re-construct a relatively accurate group layout. Based on this, we propose a 3D Transformer (3DT), which performs sam-pling and quantization in the X-Y-D space, and presets a se-ries of layout tokens along each dimension. 3DT calculates the average center position of each member and concate-nates the corresponding layoutâ€™s tokens in the three axes as the layout feature. Finally, 3DT extracts the group feature by joint modeling the appearance and layout features.
Furthermore, we find that existing datasets do not pro-vide 3D labels, and constructing a dataset with rich labeling is highly expensive. Therefore, we contribute a synthetic
GReID dataset, named City1M, which has the following three advantages. 1) Larger data scale. City1M includes 1.84M images, 45K persons and 11.5K groups. Compared with the current largest dataset CSG [19], the number of images and group identities are 600 times and 7 times that of CSG respectively. 2) More diversified samples. To simu-late the real-world monitoring scene, City1M considers illu-mination variations, occlusions, resolution variations, intra-group member and layout variations. 3) More detailed an-notations. Not only do we provide 3D position label of each member, but we also provide other annotations such as the shooting time, camera coordinates, and orientation. These advantages greatly facilitate the research of GReID.
Our contributions can be summarized as follows: 1. We propose the 3D Transformer (3DT) to perform 3D layout modeling, which eliminates the layout ambigu-ity in existing methods. To the best of our knowledge, we are the first 3D-based method. Compared with the 2D-based methods, our method can obtain more accu-rate layout features. 2. We propose a large-scale synthetic GReID dataset to alleviate data shortages and poor annotations, which contains 1.84M images with 11.5K groups and is three orders of magnitude larger than the existing dataset. 3. Lots of experiments demonstrate the superiority of the proposed 3DT and City1M. 3DT exceeds the exist-ing methods by 29.7%, 25.6% and 6.9% on Rank1 on
CSG, DukeGroup and RoadGroup. The 3DT+, pre-trained on City1M, will further improve 2.2%, 7.9% and 2.4% on Rank1. Surprisingly, a strong perfor-mance has been achieved by testing the pretrained model directly on real datasets. 2.