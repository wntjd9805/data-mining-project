Abstract
Recently, Vision Transformers (ViT), with the self-attention (SA) as the de facto ingredients, have demon-strated great potential in the computer vision community.
For the sake of trade-off between efficiency and perfor-mance, a group of works merely perform SA operation within local patches, whereas the global contextual infor-mation is abandoned, which would be indispensable for visual recognition tasks. To solve the issue, the subse-quent global-local ViTs take a stab at marrying local SA with global one in parallel or alternative way in the model.
Nevertheless, the exhaustively combined local and global context may exist redundancy for various visual data, and the receptive field within each layer is fixed. Alternatively, a more graceful way is that global and local context can adaptively contribute per se to accommodate different vi-sual data. To achieve this goal, we in this paper propose a novel ViT architecture, termed NomMer, which can dy-namically Nominate the synergistic global-local context in vision transforMer. By investigating the working pattern of
NomMer, we further explore what context information is fo-cused. Beneficial from this “dynamic nomination” mech-anism, without bells and whistles, the NomMer can not only achieve 84.5% Top-1 classification accuracy on Im-ageNet with only 73M parameters, but also show promis-ing performance on dense prediction tasks, i.e., object de-tection and semantic segmentation. The code and mod-els are publicly available at https://github.com/
TencentYoutuResearch/VisualRecognition-NomMer. 1.

Introduction
In computer vision, Convolutional Neural Net-works (CNNs) [6, 9, 11, 21, 23, 35] have served as the de facto gold standard for many years. Recently, the Vision
Transformer (ViT) [8] and its variants [25, 31] have been
*Equal contribution. †Contact person.
Illustration of motivation of the proposed NomMer.
Figure 1. (a) Global-local ViTs in parallel structure. (b) Global-local ViTs in successive structure. Previous global-local ViTs only focus on fus-ing global and local contexts while the modulation on them lacks, where the redundant information may have negative impact when recognizing various cases. (c) Our NomMer. When recognizing an object, our method can dynamically yield synergistic context from global-local context through nominator. proposed challenging the status quo. Thanks to the global information communication and content-dependent learn-ing nature of Self-Attention (SA), which is substantively different from local behavior in CNN, ViTs have shown superior performance on many visual recognition tasks.
However, the ViTs reasoning global dependency of split feature patch embeddings (a.k.a, tokens) is computationally expensive. To attack the issues, many recent works, such as SwinT [15], TNT-ViT [38] and HaloNet [27], introduce
CNN-like inductive bias (e.g., locality, translation equivari-ance) by building the token relations via self-attentions only within local windows and hierarchically aggregating them in a bottom-up manner. These local SA-based ViTs signifi-cantly improve the data efficiency, whereas the global con-textual relation, especially in the early stage, is abandoned.
To remedy this shortage, as illustrated in Fig. 1 (a) and (b), two kinds of “global-local ViTs” take a compromise, where both local and global visual dependencies are incorporated for each token. The first kind [12, 18, 37] marries the con-text aggregated by local SA or CNN with the one captured by global SA in parallel structure, such as multi-granular connection [24], in the basic block of hierarchical ViT. On the other hand, the local and global context are aggregated alternatively in the second kind [32,39]. However, no matter in the parallel or successive manner, these global-local ViTs only focus on fusing global-local context while the modula-tion on it lacks, where the redundant information may have negative impact when recognizing various cases. For exam-ple, the “lampshade” is mis-recognized as “confectionery” by previous method, probably due to redundant contextual clues, such as colored lights, misleading the model.
According to a pioneering research [22], the human vi-sual system can simultaneously process both peripheral and foveal vision when perceiving the real world scene, which also exhibits the intriguing property of modulation under various scenes. As a result, the redundant information can be naturally ignored. Specifically, foveal vision focuses on a local region of interest with more visual details, such as fine-grained details, colors or textures of objects in the scene, while peripheral vision refers to the one viewed at large angles but containing rough global scene information.
Inspired by the above preliminary and observations, in this paper, we regard the locally aggregated context in global-local ViT model as the foveal vision, while the glob-ally aggregated one is treated as the peripheral-like visual information. Moreover, we propose a more effective con-text leverage strategy that the useful dependency informa-tion is nominated from local and global context dynami-cally. To achieve this nomination process, we need to solve the following two non-trivial problems: (i) How to make nominated global and local context work in harmony when processing different visual cases? (ii) How to preserve the information at most without increasing computational cost evidently when reasoning global dependency?
For the first problem, a straightforward way of nomi-nation is to directly hard-sample from global or local con-text. Unfortunately, this sample process is indifferentiable, which makes the model suffer from the gradient lost prob-lem. To overcome this issue, we coin a novel learnable Syn-ergistic Context Nominator (SCN) to dynamically yield the global-local context with synergy for each spatial location, which is vividly shown by Fig. 1 (c). Additionally, as for the second global context reasoning problem, most of pre-vious methods [12, 18] adopt the pooling or bilinear down-sampling before conducting global SA-based context ag-gregation to strike favorable efficiency/performance trade-offs. Nevertheless, this naive computational simplification may remove both redundant and salient information, lead-ing to the detriment on performance. Considering there ex-ists tremendous redundancy in natural images containing most smooth signals with high frequency noise, we build a Compressed Global Context Aggregator (CGCA) upon
Discrete Cosine Transform (DCT) [1] to reason the global dependency with redundancy reduced from the frequency domain but without increasing computational complexity.
This global context reasoning mechanism is also surpris-ingly consistent with the working behavior of human visual system [22] when processing peripheral vision.
Based on the SCN and CGCA submodules, we carve out the basic Transformer blocks and stack them into our
NomMer framework, which can dynamically Nominate the synergistic context in vision transforMer for various visual data. We have experimentally verified the effectiveness of our proposed method on image classification task as well as dense prediction tasks, i.e., object detection and seman-tic segmentation. Our contributions can be summarized as follows: 1) We propose a novel learnable Synergistic Context Nom-inator (SCN) in terms of aggregated context, which is in stark contrast to previous ViTs with global-local context greedly aggregated. 2) We coin a novel Compressed Global Context Aggrega-tor (CGCA) more effective to reduce global redundancy and to capture global correlations. 3) We propose a novel ViT framework, termed NomMer, which enables the nominated global-local context comple-mentary with each other for various cases and tasks. We also investigate the working behavior of SCN in NomMer. 4) Thanks to the “nomination” mechanism, the NomMer can achieve 84.5% Top-1 classification accuracy on Im-ageNet with only 73M parameters. With fewer parame-ters, our small and tiny models can still achieve 83.7% and 82.6% accuracy respectively. We also witness the promis-ing performance of NomMer on dense prediction tasks, i.e., object detection and semantic segmentation. 2.