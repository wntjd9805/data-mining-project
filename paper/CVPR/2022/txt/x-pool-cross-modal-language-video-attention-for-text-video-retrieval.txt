Abstract
In text-video retrieval, the objective is to learn a cross-modal similarity function between a text and a video that ranks relevant text-video pairs higher than irrelevant pairs.
However, videos inherently express a much wider gamut of information than texts.
Instead, texts often capture sub-regions of entire videos and are most semantically similar to certain frames within videos. Therefore, for a given text, a retrieval model should focus on the text’s most semantically similar video sub-regions to make a more relevant compar-ison. Yet, most existing works aggregate entire videos with-out directly considering text. Common text-agnostic ag-gregations schemes include mean-pooling or self-attention over the frames, but these are likely to encode misleading vi-sual information not described in the given text. To address this, we propose a cross-modal attention model called X-Pool that reasons between a text and the frames of a video.
Our core mechanism is a scaled dot product attention for a text to attend to its most semantically similar frames. We then generate an aggregated video representation condi-tioned on the text’s attention weights over the frames. We evaluate our method on three benchmark datasets of MSR-VTT, MSVD and LSMDC, achieving new state-of-the-art re-sults by up to 12% in relative improvement in Recall@1.
Our ﬁndings thereby highlight the importance of joint text-video reasoning to extract important visual cues according to text. Full code and demo can be found at: layer6ai-labs.github.io/xpool/. 1.

Introduction
The advent of video content platforms like TikTok,
YouTube and Netﬂix have enabled the mass outreach of videos around the world. The ability to retrieve videos that are most semantically similar to a provided text-based search query allows us to quickly ﬁnd relevant information and to make sense of massive amounts of video data.
*Authors contributed equally to this work.
Figure 1. Illustration of the joint text and visual representations for a single video and its captions taken verbatim from the MSR-VTT dataset. Since the video is capturing more content than each individual text, aggregating the entire video regardless of the input text can be misleading.
However,
The task of text-video retrieval is an approach to solve this problem wherein the objective is for a model to learn a similarity function between texts and videos. To compute the similarity between both modalities, a common tech-nique is to ﬁrst embed a text and a video into a joint latent space and then apply a distance metric such as the cosine similarity between the text and video embeddings [5,12,22]. there is an important discrepancy between both modalities that makes such a direct comparison chal-lenging. Videos inherently express a much wider gamut of information than texts, so a text generally cannot fully cap-ture the entire contents of a video. Instead, texts are most semantically similar to sub-regions of videos, represented as a subset of frames. Depending on the given text, the frames that are the most semantically similar would differ, so multiple equally valid texts can match a particular video.
For example, in Figure 1, we show frames of a sample video from the MSR-VTT dataset [40]. The frames depict various scenes from international news and express different visual content. Moreover, we show multiple captions associated with this video, and observe that each caption best matches a different video frame but can seem irrelevant to others.
In this example, we would expect the same video to be re-trieved for any of these queries, even though the relevant content is limited to sub-regions of the video.
Based on this observation, we want a retrieval model to focus on the video sub-regions that are most relevant to the given text during retrieval. A model should there-fore directly reason between texts and the frames of videos to extract the most relevant information as described in each text. However, most existing works do not apply di-rect cross-modal reasoning, and instead utilize the entire contents of a video such as through mean-pooling or self-attention [5, 12, 26, 30]. By encoding a video independently from a given text, a model is likely to encode superﬂuous or even distracting visual information that is not described in the text, which can reduce retrieval performance.
To address this gap, we design a cross-modal attention model that we call X-Pool to allow for joint reasoning be-tween a text and a video’s frames. Unlike previous works that pool the entire frames of a video, our model provides
ﬂexibility for a text to attend to its most semantically similar frames and then generates an aggregated video representa-tion conditioned on those frames.
Our main contributions can be summarized as follows: (i) We show empirically through a proof of concept that tex-t-conditioned video pooling allows a model to reason about the most relevant video frames to a given text, which outper-forms baselines that use text-agnostic video pooling; (ii) We propose a cross-modal attention model that extends our proof of concept with parametric capacity for a text to attend to its most semantically similar video frames for aggrega-tion which we call X-Pool. X-Pool obtains state-of-the-art results across the popular benchmark datasets of MSR-VTT
[40], MSVD [8] and LSMDC [34]; (iii) We demonstrate the robustness of X-Pool to videos with increasing amounts of content diversity, such as videos with many scene tran-sitions. We show how text-agnostic pooling methods are much more sensitive to such videos compared to our text– conditioned X-Pool model. 2.