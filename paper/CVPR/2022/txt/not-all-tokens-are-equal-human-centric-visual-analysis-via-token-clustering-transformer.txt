Abstract
Vision transformers have achieved great successes in many computer vision tasks. Most methods generate vi-sion tokens by splitting an image into a regular and fixed grid and treating each cell as a token. However, not all re-gions are equally important in human-centric vision tasks, e.g., the human body needs a fine representation with many tokens, while the image background can be modeled by a few tokens. To address this problem, we propose a novel
Vision Transformer, called Token Clustering Transformer (TCFormer), which merges tokens by progressive cluster-ing, where the tokens can be merged from different locations with flexible shapes and sizes. The tokens in TCFormer can not only focus on important areas but also adjust the token shapes to fit the semantic concept and adopt a fine resolution for regions containing critical details, which is beneficial to capturing detailed information. Extensive ex-periments show that TCFormer consistently outperforms its counterparts on different challenging human-centric tasks and datasets, including whole-body pose estimation on
COCO-WholeBody and 3D human mesh reconstruction on 3DPW. Code is available at https://github.com/ zengwang430521/TCFormer.git.
Figure 1. Comparisons between vision tokens generated by (a) standard grids and (b) TCFormer. The token regions of different tokens, or the image regions represented by vision tokens, are visu-alized by different colors. From left to right, different images rep-resent different stages. Grid-based tokens treat all regions equally as shown in (a). While the tokens in (b) treat image regions dy-namically. Tokens distribute more densely on the human body. For background regions, a large area is represented by a single token (in blue), while for the regions containing important details, such as the face area, tokens with fine spatial sizes are used (in red). 1.

Introduction
Human-centric tasks [12, 18, 21, 60, 69] of computer vi-sion such as face alignment [2, 61, 76], human pose esti-mation [8, 22, 31, 37, 44, 45, 50, 54, 65, 67], and 3D human mesh reconstruction [27, 29, 52, 74] have drawn increasing research attention owing to their broad applications such as action recognition, virtual reality, and augmented reality.
Inspired by the success of transformers in natural lan-guage processing, vision transformers are recently devel-oped to solve human-centric computer vision tasks and achieve state-of-the-art performance [33,35,40,66,71]. The properties of transformers such as the long-range attention between image patches are beneficial to model the relation-ship between different body parts and thus are critical in human-centric visual analysis.
Since the traditional transformers employed a sequence of tokens as input, most existing vision transformers fol-low this paradigm by dividing an input image into a regular and fixed grid, where each cell (image patch) is treated as a token as shown in Figure 1 (a). The grid-based token gener-ation is simple and achieves great successes in many com-puter vision tasks [10, 38, 57] such as image recognition, object detection, and segmentation.
However, fixed grid based vision tokens are sub-optimal for human-centric visual analysis. In human-centric visual analysis, the image regions of the human body are more crucial than the image background, motivating us to repre-sent different image regions by vision tokens with dynamic shape and size 1. But the token regions of the grid-based vi-sion tokens are rectangular areas with fixed location, shape and size. Uniform vision token distribution is not able to allocate more tokens to important areas.
To solve this problem, we propose a novel vision trans-former, named Token Clustering Transformer (TCFormer), which generates tokens by progressive token clustering.
TCFormer generates tokens dynamically at every stage. As shown in Figure 1 (b), it is able to generate tokens with various locations, sizes, and shapes. Firstly, unlike the grid-based tokens, tokens after clustering are not limited to the regular shape and can focus on important areas e.g., the hu-man body. Secondly, TCFormer dynamically generates to-kens with appropriate sizes to represent different regions.
For the regions full of important details such as the human face, tokens with finer size are allocated. In contrast, a sin-gle token (e.g., the token in blue in Figure 1 (b)) is used to represent a large area of the background.
In TCFormer, every pixel in the feature map is initialized as a vision token at the first stage, whose token region is the region covered by the pixel. We progressively merge tokens with similar semantic meanings and obtain different num-bers of tokens in different stages. To this end, we carefully design a Clustering Token Merge (CTM) block. Firstly, given tokens from the previous stage, CTM groups them by applying the k-nearest-neighbor based density peaks clus-tering algorithm [11] on the token features. Secondly, the tokens assigned to the same cluster are merged to a single token by averaging the token features. Finally, the tokens are fed into a transformer block for feature aggregation. The token region of the merged token is the union of the input token regions.
Aggregation of multi-stage features is proved to be ben-eficial for human-centric analysis [49, 71]. Most prior works [38, 57, 71] transform vision tokens to feature maps and aggregate features in the form of feature maps. How-ever, when transforming our dynamic vision tokens to fea-ture maps, multiple tokens may locate in the same pixel grid, causing the loss of details. To solve this problem, we propose a Multi-stage Token Aggregation (MTA) head, which is able to preserve image details in all stages in an efficient way. Specifically, the MTA head starts from the to-kens in the last stage, and then progressively upsamples to-kens and aggregates token features from the previous stage, until features in all the stages are aggregated. The aggre-gated tokens are in one-to-one correspondence with pixels in the feature maps and are reshaped to the feature maps for subsequent processing. 1We call the image region represented by a token as the token region and use token location, shape, and size to denote that of its token region.
We summarize our contributions as follows.
• We propose a Token Clustering Transformer (TC-Former), which generates vision tokens of various lo-cations, sizes, and shapes for each image by progres-sive clustering and merging tokens. To the best of our knowledge, it is the first time that clustering is used for dynamic token generation.
• We propose a Multi-stage Token Aggregation (MTA) head to aggregate token features in multiple stages, re-serving detailed information in all stages efficiently.
• Extensive experiments show that TCFormer consis-tently outperforms its counterparts on different chal-lenging human-centric tasks and datasets, including whole-body pose estimation on COCO-WholeBody and 3D human mesh reconstruction on 3DPW. 2.