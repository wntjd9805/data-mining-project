Abstract
Vision-and-Language (V+L) pre-training models have achieved tremendous success in recent years on various multi-modal benchmarks. However, the majority of exist-ing models require pre-training on a large set of paral-lel image-text data, which is costly to collect, compared to image-only or text-only data. In this paper, we explore unsupervised Vision-and-Language pre-training (UVLP) to learn the cross-modal representation from non-parallel im-age and text datasets. We found two key factors that lead to good unsupervised V+L pre-training without parallel data: (i) joint image-and-text input (ii) overall image-text align-ment (even for non-parallel data). Accordingly, we pro-pose a novel unsupervised V+L pre-training curriculum for non-parallel texts and images. We first construct a weakly aligned image-text corpus via a retrieval-based approach, then apply a set of multi-granular alignment pre-training tasks, including region-to-tag, region-to-phrase, and image-to-sentence alignment, to bridge the gap between the two modalities. A comprehensive ablation study shows each granularity is helpful to learn a stronger pre-trained model.
We adapt our pre-trained model to a set of V+L downstream tasks, including VQA, NLVR2, Visual Entailment, and Ref-COCO+. Our model achieves the state-of-art performance in all these tasks under the unsupervised setting. 1.

Introduction
Vision-and-Language pre-trained (VLP) models [8, 15, 16, 18, 23, 24, 27, 31, 32, 37, 39, 41, 43] that learn the joint cross-modal representation have revolutionized the research on various vision-and-language tasks in recent years. How-ever, the success of VLP models relies on the availability of a large-scale aligned image-text corpora. The widely used crowd-sourced pre-training datasets such as MS COCO
[7, 29] and Visual Genome [20] require expensive human
*The two authors contribute equally.
Figure 1. Meta average scores of VQA, NLVR2, VE, and Re-fCOCO+ fine-tuned from different pre-trained models. All pre-training are conducted on Conceptual Captions (CC) with differ-ent ratio of parallel data, i.e., a fixed amount of data is originally aligned while the rest is randomly shuffled. 0% refers to the case of unsupervised V+L pre-training. We also plot the performance of our proposed approach against U-VisualBERT [26]. Breakdown of the accuracy of each task is listed in the supplementary file. annotations which are hard to scale up. Recently, the web crawled image-text datasets like Conceptual Captions 3M [35] and CC12M [5], and SBU Captions [33], etc. have dramatically reduced the need for massive human anno-tation but still require heavy post-cleaning procedures to get aligned image-text pairs. In comparison, the language corpora and image collection are readily available from the web. The convenience of getting a large-scale single-modality data has benefited the self-supervised learning of vision [3, 6, 12] and language [11, 30] domains respectively.
This raises a question: Can we take advantage of easily-accessible large-scale single-modality data to perform unsu-pervised V+L pre-training without parallel text and images (UVLP)?
We define UVLP as follows: given the separately crawled image collection I = {i1, i2, . . . , inI } and text cor-pus T = {t1, t2, . . . , tnT }, we aim to pre-train a multi-1
modal model from such data. U-VisualBERT [26] is the first UVLP work, where the authors have trained their model on un-aligned text and image data in a round-robin fashion and simply use object tags as an anchor point to bridge the gap between the two modalities. Their research demonstrates that a shared multi-modal embedding can be learned by just presenting a single modality at a time.
This however introduces an input discrepancy between pre-training and fine-tuning stages as each downstream V+L task requires both modalities (image, text) as the input.
In this work, we investigate (i) whether presenting a joint image-text data from non-parallel data would improve the learned joint embedding space. Furthermore, (ii) if joint image-text data is fed into the model, how does its latent alignment affect the cross-modal representation learning?
To explore these two questions, we simply use the im-ages and captions from Conceptual Captions (CC) dataset
[35] as independently collected uni-modal corpus and per-form the following analysis. First, we compare the pre-trained model’s performance between the two data input strategies: one is presenting one image or text at a time (round-robin) and the other is presenting a concatenation of a pair of randomly sampled image and text (0% alignment ratio). Second, we prepare five sets of image-text pairs from
Conceptual Captions with different levels of pairwise align-ment by controlling the ratio of original aligned image-text data from 20% to 100% (while the remaining is randomly sampled from each modality). A single-stream transformer is used for all experiments with the standard pre-training objectives: masked language modeling (MLM) on language input and masked region modeling (MRM) on vision input.
After pre-training, we adapt the model to a series of four downstream V+L tasks, including VQA [2], NLVR2 [38],
Visual Entailment (VE) [42], and RefCOCO+ [44]. The performance is measured as the meta average of all tasks after fine-tuning. The results are summarized in Fig. 1.
From Fig. 1, it is clear that joint MLM+MRM learning out-performs round-robin MLM/MRM. Such gains show that joint image-and-text input is necessary for UVLP even when the input is un-aligned. We also observe a strong pos-itive correlation between the alignment of image-text pairs and the meta average of the fine-tuned downstream tasks of the resulting model. This conveys a seemingly intuitive but quite important message that the more aligned the image-text data is the better the pre-trained model performs.
Inspired by these analyses, we propose Unsuper-vised Vision-and-Language Pre-training via Retrieval-based Multi-Granular Alignment (µ-VLA), which uses our novel unsupervised V+L pre-training curriculum for non-parallel data. We first construct a weakly-aligned image-text dataset via retrieval. Given an image, we take its de-tected object tags as the reference sentence and retrieve the closest sentences from the text corpus via sentence BERT embedding [34] similarity. Though the constructed pairs are noisy, the mere weak alignment of concepts is key to learning the latent alignment. We propose to let the model gradually learn a multi-granular alignment, i.e., region-to-object tag level, region-to-noun phrase level, and image-to-sentence level to more effectively bridge the gap between the two modalities. We show how each granularity learned from the weakly-aligned pairs contributes to the final pre-trained model’s performance. Experiments show our ap-proach achieves the state-of-art performance (in Fig. 1), with a clear gain over [26] on the 4 downstream tasks.
Towards practical applications, we also validate the ef-fectiveness of our approach under a more realistic setting, where the images are from CC and the captions are from
BookCorpus (BC) [49]. Similar performance gains are achieved in this harder setting, showing the robustness of our approach.
To summarize, our contributions are three-fold: (i) We analyze what leads to a good unsupervised V+L pre-training joint image-and-text input, and found two key factors: (ii) Ac-and overall alignment between image-text pairs. cordingly, we propose a novel retrieval-based pre-training curriculum, which applies multi-granular alignment pre-training tasks between weakly aligned image-text pairs to bridge the gap between the two modalities. (iii) We pro-vide comprehensive experiments and analyses showing the robustness of our approach when compared to SOTA super-vised and unsupervised V+L pre-training methods. 2.