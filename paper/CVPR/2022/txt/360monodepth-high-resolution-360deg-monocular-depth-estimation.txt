Abstract 360° cameras can capture complete environments in a single shot, which makes 360° imagery alluring in many computer vision tasks. However, monocular depth estimation remains a challenge for 360° data, particularly for high resolutions like 2K (2048×1024) and beyond that are important for novel-view synthesis and virtual reality applications. Current
CNN-based methods do not support such high resolutions due to limited GPU memory. In this work, we propose a flex-ible framework for monocular depth estimation from high-resolution 360° images using tangent images. We project the 360° input image onto a set of tangent planes that produce perspective views, which are suitable for the latest, most ac-curate state-of-the-art perspective monocular depth estima-tors. To achieve globally consistent disparity estimates, we recombine the individual depth estimates using deformable multi-scale alignment followed by gradient-domain blending.
The result is a dense, high-resolution 360° depth map with a high level of detail, also for outdoor scenes which are not supported by existing methods. Our source code and data are available at https://manurare.github.io/360monodepth/. 1.

Introduction
Monocular depth estimation has recently seen a significant boost thanks to convolutional neural networks. CNNs have demonstrated an unprecedented expressive power to learn intricate geometric relationships from data, resembling the capability of humans to exploit visual cues to perceive depth.
Monocular depth estimates have enabled impressive new approaches for 3D photography [33, 61] and novel-view syn-thesis of dynamic scenes [20, 43]. However, most approaches for monocular depth estimation are limited to low-resolution1 perspective images, with a limited field-of-view.
Nevertheless, 360° cameras are becoming increasingly popular and widespread in the computer vision community.
The omnidirectional 360° field-of-view captured by these devices is appealing for tasks such as robust, omnidirectional
SLAM [66, 77], scene understanding and layout estimation
[31, 67, 75, 81], or VR photography and video [5, 59]. State-*These authors contributed equally to this work. 1For example, 384×384≈0.15 megapixels for MiDaS [55, 56].
of-the-art monocular depth estimation approaches for 360° images [30, 40, 52, 67, 74] are currently limited to resolu-tions of 1024×512≈0.5 megapixels. While this is sufficient for tasks like layout estimation, it is insufficient for VR ap-plications as they require resolutions of at least 2 megapixels to match the resolution of VR headsets [34] and achieve full immersion [12, 45]. Our work aims to fill this gap.
Existing monocular 360° depth estimation approaches build on CNNs whose spatial resolution is fundamentally limited by the GPU memory available during training. These methods are therefore restricted to small batch sizes of 4 to 8 for 0.5 megapixel images on an NVIDIA 2080 Ti with 11 GB memory [30, 52, 67]. For this reason, single-CNN ap-proaches become impractical for predicting high-resolution depth maps with multiple megapixels.
In this work, we introduce a general and flexible frame-work for monocular depth estimation from high-resolution 360° images inspired by Eder et al.’s tangent images [16].
Our approach projects the input 360° image to a collection of perspective tangent images, e.g. using the faces of an icosahedron. We then use state-of-the-art perspective monoc-ular depth estimators endowed with powerful generalisation capability for obtaining dense, detailed depth maps for each tangent image. Subsequently, we optimally align individual depth maps using multi-scale spatially-varying deformation fields to bring them into global agreement. Finally, we merge the aligned depth maps using gradient-based blending for a seamless high-resolution 360° depth map. Our technical contributions are as follow: 1. A simple, yet powerful and practical framework for high-quality multi-megapixel 360° monocular depth estimation based on aligning and blending depth maps predicted from perspective tangent images. 2. Support for increased resolutions using tangent images, and improved quality by forward compatibility for fu-ture monocular depth estimation approaches. 3. We provide 2048×1024 ground-truth depth maps for
Matterport3D’s stitched skyboxes to advance future high-resolution depth estimation approaches. 2.