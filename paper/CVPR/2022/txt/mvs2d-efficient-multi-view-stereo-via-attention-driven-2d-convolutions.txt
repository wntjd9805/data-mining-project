Abstract
Deep learning has made signiﬁcant impacts on multi-view stereo systems. State-of-the-art approaches typically involve building a cost volume, followed by multiple 3D convolution operations to recover the input image’s pixel-wise depth. While such end-to-end learning of plane-sweeping stereo advances public benchmarks’ accuracy, they are typically very slow to compute. We present MVS2D, a highly efﬁcient multi-view stereo algorithm that seam-lessly integrates multi-view constraints into single-view net-works via an attention mechanism. Since MVS2D only builds on 2D convolutions, it is at least 2 faster than all the notable counterparts. Moreover, our algorithm pro-duces precise depth estimations and 3D reconstructions, achieving state-of-the-art results on challenging bench-marks ScanNet, SUN3D, RGBD, and the classical DTU dataset. our algorithm also out-performs all other algo-rithms in the setting of inexact camera poses. Our code is released at https://github.com/zhenpeiyang/
MVS2D
× 1.

Introduction
Multi-view Stereo (MVS) aims to reconstruct the under-lying 3D scene or estimate the dense depth map using mul-tiple neighboring views. It plays a key role in a variety of 3D vision tasks. With high-quality cameras becoming more and more accessible, there are growing interests in develop-ing reliable and efﬁcient stereo algorithms in various appli-cations, such as 3D reconstruction, augmented reality, and autonomous driving. As a fundamental problem in com-puter vision, MVS has been extensively studied [9]. Recent research shows that deep neural networks, especially con-volutional neural networks (CNNs), lead to more accurate and robust systems than traditional solutions. Several ap-proaches [20, 57] report exceptional accuracy on challeng-ing benchmarks like ScanNet [7] and SUN3D [47].
State-of-the-art CNN-based multi-view approaches typi-cally fall into three categories: 1) Variants of a standard 2D
UNet architecture with feature correlation [22, 28]. How-ever, these approaches work best for rectiﬁed stereo pairs,
∗ Experiments are conducted by Z. Yang at The University of Texas at Austin. Email: yzp@utexas.edu
Figure 1. Inference frame per second (FPS) vs. depth error (Ab-sRel) on ScanNet [7]. Our model achieve signiﬁcant reduction in inference time, while maintaining state-of-the-art accuracy. and extending them to multi-view is nontrivial. 2) Con-structing a differential 3D cost volume [12,14,15,30,53,54].
These algorithms signiﬁcantly improve the accuracy of
MVS, but at the cost of heavy computational burdens. Fur-thermore, the predicted depth map by 3D convolution usu-ally contains salient artifacts which have to be rectiﬁed by a 2D reﬁnement network [15]. 3) Maintain a global scene rep-resentation and fuse multi-view information through ray-casting features from 2D images [29]. This paradigm can-not handle large-scale scenes because of the vast memory consumption on maintaining a global representation.
Aside from multi-view depth estimation, we have also witnessed the tremendous growth of single-view depth pre-diction networks [21, 32, 48, 52, 56]. As shown in Table 3, Bts [21] has achieved impressive result on ScanNet [7].
Single-view depth prediction roots in learning feature repre-sentations to capture image semantics, which is orthogonal to correspondence-computation in multi-view techniques.
A natural question is how to combine single-view depth cues and multi-view depth cues.
We introduce MVS2D that combines the strength of single-view and multi-view depth estimations. The core contribution is an attention mechanism that aggregates fea-tures along epipolar lines of each query pixel on the refer-ence images. This module captures rich signals from the reference images. Most importantly, it can be easily inte-grated into standard CNN architectures deﬁned on the input
image, introducing relatively low computational cost.
Our attention mechanism possesses two appealing char-acteristics: 1) Our network only contains 2D convolutions. 2) Besides relying on the expressive power of 2D CNNs, the network seamlessly integrates single-view feature rep-resentations and multi-view feature representations. Conse-quently, MVS2D is the most efﬁcient approach compared to state-of-the-art algorithms (See Figure 1). It is 48 faster than NAS [20], 39 faster faster than DPSNet [15], 10 than MVSNet [53], 4.7 faster than FastMVSNet [57], and almost 2 speed-up over the most recent fastest approach
PatchmatchNet [44]. In the mean-time, MVS2D achieves state-of-the-art accuracy.
×
×
×
×
×
Intuitively, the beneﬁt of MVS2D comes from the early fusion of the intermediate feature representations. The out-come is that the intermediate feature representations con-tain rich 3D signals. Furthermore, MVS2D offers ample space where we can design locations of the attention mod-ules to address different inputs. One example is when the input camera poses are inaccurate, and corresponding pix-els deviate from the epipolar lines on the input reference images. We demonstrate a simple solution, which installs multi-scale attention modules on an encoder-decoder net-work. In this conﬁguration, corresponding pixels in down-sampled reference images lie closer to the epipolar lines, and MVS2D detect and rectify correspondences automati-cally.
We conduct extensive experiments on challenging benchmarks ScanNet [7], SUN3D [47], RGBD [36] and
Scenes11 [36]. MVS2D achieves the state-of-the-art per-formance on nearly all the metrics. Qualitatively, compared to recent approaches [15,20,53,57], MVS2D helps generate higher quality 3D reconstruction outputs. 2.