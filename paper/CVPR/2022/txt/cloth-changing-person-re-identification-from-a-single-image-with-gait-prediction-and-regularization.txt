Abstract
Cloth-Changing person re-identification (CC-ReID) aims at matching the same person across different loca-tions over a long-duration, e.g., over days, and therefore inevitably has cases of changing clothing. In this paper, we focus on handling well the CC-ReID problem under a more challenging setting, i.e., just from a single image, which en-ables an efficient and latency-free person identity matching for surveillance. Specifically, we introduce Gait recogni-tion as an auxiliary task to drive the Image ReID model to learn cloth-agnostic representations by leveraging per-sonal unique and cloth-independent gait information, we name this framework as GI-ReID. GI-ReID adopts a two-stream architecture that consists of an image ReID-Stream and an auxiliary gait recognition stream (Gait-Stream). The
Gait-Stream, that is discarded in the inference for high ef-ficiency, acts as a regulator to encourage the ReID-Stream to capture cloth-invariant biometric motion features during the training. To get temporal continuous motion cues from a single image, we design a Gait Sequence Prediction (GSP) module for Gait-Stream to enrich gait information. Finally, a semantics consistency constraint over two streams is en-forced for effective knowledge regularization. Extensive ex-periments on multiple image-based Cloth-Changing ReID benchmarks, e.g., LTCC, PRCC, Real28, and VC-Clothes, demonstrate that GI-ReID performs favorably against the state-of-the-art methods. 1.

Introduction
Person re-identification (ReID) aims at identifying a spe-cific person across cameras, times, and locations. Abundant approaches have been proposed to address the challeng-ing geometric misalignment among person images caused
Figure 1. (a) shows a realistic wanted case that a suspect changed her coat from black to white for hiding. (b) reveals that the gait of person could help ReID, especially when the identity matching meets the cloth-changing challenge (All faces in the images are masked for anonymization). by diversities of human poses [43, 48, 66], camera view-points [24, 50, 62], and style/scales [25, 26]. These methods usually inadvertently assume that both query and gallery images of the same person have the same clothing. In gen-eral, they perform well on the trained short-term datasets but suffer from significant performance degradations when testing on a long-term collected ReID dataset [45,52,57,59].
Because large clothing variations occur over long-duration among these datasets, which seriously hinders the accuracy of ReID. For example, Figure 1(a) shows a realistic wanted case 1 where a suspect that captured by surveillance devices at different times/locations changed her coat from black to white, which makes ReID difficult, especially when she wears a mask and the captured images are of low quality.
In recent years, to handle the cloth-changing ReID (CC-ReID) problem, some studies have contributed some new datasets where clothing changes are commonplace (e.g.,
*This work was done when he was visiting Alibaba as a research intern.
†Corresponding author. 1Information comes from https://www.wjr.com/2016/01/06/woman-wanted-in-southwest-detroit-bank-robbery/
Celebrities-reID [19, 21], PRCC [57], LTCC [45], Real28 and VC-Clothes [52]). They also propose some new al-gorithms that could learn cloth-agnostic representations for
CC-ReID. For instance, Yang et al. [57] propose a contour-sketch-based network to overcome the moderate cloth-changing problem. Similarly, Qian et al. [45], Li et al. [32], and Hong et al. [17] all use body shape to tackle the CC-ReID problem. However, no matter of using a contour sketch or body shape, all these methods are prone to suffer from the estimation error problem. Because the single-view contour/shape inference (from 2D image) is extremely dif-ficult due to the vast range of possible situations, especially when people wear thick clothes in winter. Besides, these contour-sketch-based or shape-based methods only focus on extracting static spatial cues from persons as extra cloth-agnostic representations, the rich dynamic motion informa-tion (e.g., gait, implied motion [28]) are often ignored.
In this paper, we explore to leverage the unique gait features that imply dynamic motion cues of a pedestrian to drive a model to learn cloth-agnostic and discriminative
ReID representations. As shown in Figure 1(b), although it is hard to identify the same person when he/she wears different clothes, or to distinguish the different persons when they wear similar/same clothes, we can still leverage their unique/discriminative gaits to achieve correct identity matching. It is because that gait, as a unique biometric fea-ture, has the superior invariance compared with other easy-changing appearance characteristics, e.g., face, body shape, contour [36,63]. Besides, gait can be authenticated at a long distance even with low-quality camera imaging.
Unfortunately, existing gait-related studies mainly rely on large video sequences [3, 8]. Capturing videos requires time latency and saving videos needs a large hardware stor-age cost, which are both undesirable for the real-time ReID applications. Even the recent work [55] first attempts to achieve gait recognition from a single image, how to lever-age gait feature to handle CC-ReID problem from a single image is still under-studied and this task is more challenging due to the potential viewpoint-variations and occlusions.
In this paper, we propose a Gait-assisted Image-based
ReID framework, termed as GI-ReID, which could learn cloth-agnostic ReID representations from a single image with the gait feature assistance. GI-ReID consists of a main image-based ReID-Stream and an auxiliary gait recognition stream (Gait-Stream). Figure 2 shows the entire framework.
The Gait-Stream aims to regularize the ReID-Stream to learn cloth-agnostic features from a single RGB image for effective CC-ReID. It is discarded in the inference for the high efficiency. Since the comprehensive gait features ex-traction typically needs a gait video sequence as input [3,8], we introduce a new Gait Sequence Prediction (GSP) mod-ule for Gait-Stream to approximately forecast continuous gait frames from a single input query image, which enriches the learned gait information. Finally, to encourage the main
ReID-Stream’s efficient learning from Gait-Stream, we fur-ther enforce a high-level Semantics Consistency (SC) con-straint for the same person over two streams’ features. We summarize our main contributions as follows:
• We specially aimed at handling the challenging cloth-changing issue for image ReID to promote practical ap-plications. A Gait-assisted Image-based cloth-changing
ReID (GI-ReID) framework is proposed. As a regulator, the Gait-Stream in GI-ReID can be removed in the infer-ence without sacrificing ReID performance. This reduces the dependency on the accuracy of gait recognition, mak-ing our method computationally efficient and robust.
• A well-designed Gait Sequence Prediction (GSP) mod-ule makes our method effective in the challenging image-based ReID scenarios. And, a high-level semantics con-sistency (SC) constraint enables an effective regular-ization over two streams, enhancing the distinguishing power of ReID-Stream under the cloth-changing setting.
With the gait prediction and regularization, GI-ReID achieves a state-of-the-art performance on the image-based cloth-changing ReID. It is also general enough to be com-patible with the existing ReID-specific networks, except
ResNet-50 [13], we also use OSNet [69], LTCC-shape [45], and PRCC-contour [57] as our baselines for evaluation. 2.