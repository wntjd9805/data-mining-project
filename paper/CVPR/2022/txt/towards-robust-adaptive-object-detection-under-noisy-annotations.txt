Abstract
Domain Adaptive Object Detection (DAOD) models a joint distribution of images and labels from an annotated source domain and learns a domain-invariant transforma-tion to estimate the target labels with the given target do-main images. Existing methods assume that the source do-main labels are completely clean, yet large-scale datasets often contain error-prone annotations due to instance am-biguity, which may lead to a biased source distribution and severely degrade the performance of the domain adaptive detector de facto. In this paper, we represent the first ef-fort to formulate noisy DAOD and propose a Noise Latent
Transferability Exploration (NLTE) framework to address this issue.
It is featured with 1) Potential Instance Min-ing (PIM), which leverages eligible proposals to recapture the miss-annotated instances from the background; 2) Mor-phable Graph Relation Module (MGRM), which models the adaptation feasibility and transition probability of noisy samples with relation matrices; 3) Entropy-Aware Gradi-ent Reconcilement (EAGR), which incorporates the seman-tic information into the discrimination process and enforces the gradients provided by noisy and clean samples to be consistent towards learning domain-invariant representa-tions. A thorough evaluation on benchmark DAOD datasets with noisy source annotations validates the effectiveness of
NLTE. In particular, NLTE improves the mAP by 8.4% un-der 60% corrupted annotations and even approaches the ideal upper bound of training on a clean source dataset. 1 1.

Introduction
Recent years have witnessed great progress in domain adaptive object detection (DAOD) [6, 17, 19, 23, 38, 47, 49, 57, 58]. It alleviates the performance drop of the detectors when applied to unseen domains due to the domain shift.
*Corresponding author. This work was supported by Hong Kong Re-search Grants Council (RGC) General Research Fund 11211221 (CityU 9043152). 1Code is available at https://github.com/CityU-AIM-Group/NLTE. (a) (b)
Figure 1. Examples of noisy annotations in Cityscapes dataset.
Miss-annotated samples: The bicycle in (a); the rider and car in (c). Class-corrupted samples: The rider and bicycle are labeled as person in (a); the motorcycle is labeled as bicycle in (b). (c)
Most DAOD methods are constructed with domain adver-sarial training [10], in which a domain classifier is proposed to train the feature extractor to perform a domain-invariant transformation of images from different domains. However, existing methods are all built with an ideal condition that a clean source domain is accessible, which is impractical in many real-world applications [22, 26]. The annotations can be noisy due to various reasons, including ambiguous objects caused by occlusion or obscureness, limited crowd-sourcing or labeling time, low quality labeled web-crawled images, etc. [8,30] Frustratingly, the noisy class annotations occur frequently, even in benchmark DAOD source datasets such as Cityscapes, as shown in Fig. 1. The noisy annota-tions can be categorized into two groups: miss-annotated instances (Fig. 1 (a), (c)) and class-corrupted instances (Fig. 1 (a), (b)). More specifically, it has been studied that addressing the classification error is critical to the detec-tor [2, 53], thus the noisy class labels in the source dataset could severely damage the domain adaptive detectors.
The intuitive solution for solving the noisy DAOD prob-lem is to combine approaches in learning with noisy la-bels for classification and domain adaptive object detec-tion. However, this direct combination may encounter sev-eral challenges. Firstly, existing methods in learning with noisy labels for image classification [13, 35, 45] minimize or totally filter out the impact of noisy annotated samples during training the network. While in DAOD, the source images with noisy labels are still useful for aligning with target domain as the domain discriminator is class-agnostic, and the target images could benefit source dataset denois-ing in reverse. If source samples with rich domain-specific
sition probability across domains. Afterwards, local rela-tion matrices are built to explore the alignment feasibility of noisy samples, and the transition probabilities of noisy sam-ples are regularized by the global relation matrix. Finally, as both miss-annotated and class-corrupted noises are con-tributive to learn domain-invariant representations, we pro-pose an Entropy-Aware Gradient Reconcilement (EAGR) strategy for harmonizing the adaptation procedure of noisy and clean samples. It affiliates class confidence into the dis-criminator, then enforces the gradients of clean and noisy samples to be consistent towards a domain-invariant direc-tion. Experiments are conducted on both synthetic noisy datasets and real-world scenarios. NLTE outperforms var-ious possible baselines, which validates its effectiveness.
With 60% noisy rate, NLTE can significantly improve the mAP of the baseline domain adaptive detector by 8.4%, and only drops by 2% when compared with the clean scenario. 2.