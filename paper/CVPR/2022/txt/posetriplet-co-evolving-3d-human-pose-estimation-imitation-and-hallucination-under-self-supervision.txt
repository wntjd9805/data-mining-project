Abstract
Existing self-supervised 3D human pose estimation schemes have largely relied on weak supervisions like con-sistency loss to guide the learning, which, inevitably, leads to inferior results in real-world scenarios with unseen poses.
In this paper, we propose a novel self-supervised approach that allows us to explicitly generate 2D-3D pose pairs for augmenting supervision, through a self-enhancing dual-loop learning framework. This is made possible via in-troducing a reinforcement-learning-based imitator, which is learned jointly with a pose estimator alongside a pose hal-lucinator; the three components form two loops during the training process, complementing and strengthening one an-other. Specifically, the pose estimator transforms an input 2D pose sequence to a low-fidelity 3D output, which is then enhanced by the imitator that enforces physical constraints.
The refined 3D poses are subsequently fed to the hallucina-tor for producing even more diverse data, which are, in turn, strengthened by the imitator and further utilized to train the pose estimator. Such a co-evolution scheme, in practice, enables training a pose estimator on self-generated motion data without relying on any given 3D data. Extensive exper-iments across various benchmarks demonstrate that our ap-proach yields encouraging results significantly outperform-ing the state of the art and, in some cases, even on par with results of fully-supervised methods. Notably, it achieves 89.1% 3D PCK on MPI-INF-3DHP under self-supervised cross-dataset evaluation setup, improving upon the previ-ous best self-supervised method [16, 26] by 8.6%. 1.

Introduction
Video-based 3D human pose estimation aims to infer 3D pose sequences from videos, and therefore plays a crucial role in many applications such as action recognition [47,
*Equal contribution. Email: {gongkehong, zhangjianfeng, taowang}@u.nus.edu, l.libingbing@gmail.com
â€ Corresponding author. Email: xinchao@nus.edu.sg
Figure 1. Overview of our PoseTriplet framework. The pose estimator, imitator, hallucinator are trained jointly in a dual-loop strategy. In the first loop, the estimator provides physically im-plausible motion information, which is then enhanced by the im-itator via enforcing physical constraints to generate physically plausible motion. In the second loop, the hallucinator generates more diverse motion patterns given motion sequence from previ-ous loop, and sends them to the imitator again for further refine-ment. This dual-loop paradigm facilitates tight co-evolution of the three components and enables iterative self-improving training of the estimator with the generated diverse and plausible motion data. 58], virtual try-on [31], and mixed reality [5, 20, 34]. Ex-isting methods [22, 33, 37, 38, 48] mainly rely on the fully-supervised paradigms, in which the ground truth 3D data are given as input. However, capturing 3D pose data is cost-intensive and time-consuming, as it typically requires a multi-view setup or a motion capturing system [17, 34], making it infeasible under in-the-wild scenarios.
To this end, two categories of methods have been intro-duced to alleviate the 3D data availability issue. The first category explores the semi-supervised settings, in which only a small amount of the 3D annotations are given [30,36, 68]. The second category, on the other hand, assumes no 3D data are available at all and only 2D poses are provided. Un-der this setup, state-of-the-art methods have mainly focused on imposing weak supervision signals to guide the training, such as aligning the projection of an inferred 3D pose with a 2D pose [4, 16, 61]. Due to the lack of 3D data and hence the missing of 2D-3D pairs, these methods are, by nature, brittle to the challenging scenarios such as unseen poses in-herent to the in-the-wild tasks.
In this paper, we propose a novel self-supervised ap-proach termed as PoseTriplet, which allows for explicitly generating physically- and semantically-plausible 2D-3D pose pairs, so that full supervisions can be imposed and further significantly strengthen the self-learning process.
This is made possible through introducing a reinforcement-learning-based imitator, which is jointly optimized with the pose estimator alongside a pose hallucinator. Specifi-cally, the imitator takes the form a of physics simulator with non-differentiable dynamics to ensure physically plausibil-ity. The hallucinator helps generate more diverse motion with generative motion completion. These three key com-ponents are integrated into a self-contained framework and co-evolve via a dual-loop strategy as the training proceeds.
With only 2D pose data as input, PoseTriplet progressively generates, refines and hallucinates 3D data, which in turn reinforces all components in the loop. Once trained, each component of PoseTriplet can be readily taken out and serves as an off-the-shelf tool for its dedicated task, such as pose estimation or imitation.
The key motivation behind co-evolving the pose estima-tor, imitator and hallucinator, lies in their complementary natures. In particular, pose estimator takes 2D poses as in-put and generates 3D poses with reasonable semantics (e.g., nature behaviors) but implausible dynamics; such derived 3D poses are then refined through the physics-based im-itator that enforces physical constraints. Conversely, the reinforcement-learning-based imitator is possible to gener-ate unnatural behaviors (e.g., overly energetic movements), which can be rectified through the pose estimator to ensure the semantic plausibility. Pose hallucinator, on the other hand, enhances the data diversity by producing realistic 3D pose sequences under both the semantic and physical guid-ance, which further strengthens data synthesizing and hence improves generalization performance.
We show the overall workflow of PoseTriplet in Fig. 1, which effectively aligns with aforementioned motivation.
Unlike prior endeavors that rely on self-consistency-based supervisions or 3D sequences as input, PoseTriplet, through the dual-loop scheme, turns the input 2D poses into de-pendable 3D poses of realistic semantics and dynamics, thereby lending itself to much stronger supervisions and consequently the co-evolution of the pose estimator, imi-tator and hallucinator. Experimental results across H36M, 3DHP, and 3DPW datasets demonstrate that, PoseTriplet gives rises to pose estimation results significantly superior to the state-of-the-art self-supervised methods, and some-times even on par with results from fully-supervised ones.
Notably, it achieves 89.1% 3D PCK on MPI-INF-3DHP un-der self-supervised cross-dataset evaluation setup, improv-ing upon the previous best self-supervised method [16, 26] by 8.6%.
Our contribution is therefore a novel scheme dedicated for self-supervised 3D pose estimation, achieved by the co-evolution of a pose estimator, imitator, and hallucina-tor. The three components complement and benefit one an-other, together leading to a self-contained system that en-ables realist 3D pose sequences and further the 2D-3D aug-mented supervisions. By taking only 2D poses as input,
PoseTriplet delivers truly encouraging results across vari-ous benchmarks, largely outperforming the state of the art and even approaching full-supervised results. 2.