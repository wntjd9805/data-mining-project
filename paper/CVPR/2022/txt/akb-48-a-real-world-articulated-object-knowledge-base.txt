Abstract
Human life is populated with articulated objects. A com-prehensive understanding of articulated objects, namely appearance, structure, physical property, and semantics, will benefit many research communities. As current artic-ulated object understanding solutions are usually based on synthetic object dataset with CAD models without physics properties, which prevent satisfied generalization from sim-ulation to real-world applications in visual and robotics tasks. To bridge the gap, we present AKB-48: a large-scale Articulated object Knowledge Base which consists of 2,037 real-world 3D articulated object models of 48 cat-egories. Each object is described by a knowledge graph
ArtiKG. To build the AKB-48, we present a fast articula-tion knowledge modeling (FArM) pipeline, which can fulfill the ArtiKG for an articulated object within 10-15 minutes, and largely reduce the cost for object modeling in the real
†Cewu Lu is the corresponding author. He is the member of Qing Yuan
Research Institute and MoE Key Lab of Artificial Intelligence, AI Institute,
Shanghai Jiao Tong University, and Shanghai Qi Zhi Institute, China. world. Using our dataset, we propose AKBNet, an integral pipeline for Category-level Visual Articulation Manipula-tion (C-VAM) task, in which we benchmark three sub-tasks, namely pose estimation, object reconstruction and manipu-lation. Dataset, codes, and models are publicly available at https://liuliu66.github.io/AKB-48. 1.

Introduction
Articulated objects, composed of more than one rigid part connected by joints allowing rotational or translational movements in 3D space, are pervasive in our daily life.
Knowledge about the articulated objects can be beneficial to many research communities, such as computer vision, robotics and embodied AI. Thus, many articulated object datasets have been proposed to facilitate the research, such as PartNet-Mobility [31], ReArt-48 [17], RBO [20]. How-ever, these datasets generally focus more on the structural information (e.g. part segmentation, kinematic structure), but pay less attention to the appearance (e.g. texture, fine geometry), the physics properties (e.g. per-part mass, iner-tial, material and friction) and semantics (e.g. category, af-fordance). While some important tasks heavily rely on these information such as object detection (texture) [2], 3D recon-struction (fine geometry) [19], object manipulation (phys-ical property) [5], and so on, the lacking of such object knowledge in these datasets can prevent satisfied general-ization for the learning models.
To boost the research on articulated objects, in this paper, we present AKB-48: a large-scale real-world Articulated
Knowledge Base which includes 48 categories, 2,037 in-stances. For each instance, the object model is scanned from the real counterpart and refined manually (Sec. 3.2), and the object knowledge is organized to a graph, named
Articulation Knowledge Graph (ArtiKG), which contains the detailed annotations of different kinds of object at-tributes and properties (Sec. 3.1). To make the scanning and annotation process feasible for large datasets, we present a
Fast Articulation Knowledge Modeling (FArM) pipeline (Sec. 3.3). In detail, we develop an object recording system with 3D sensors and turntables, a GUI that integrates struc-tural and semantic annotations, and standard real-world ex-periments for physical property annotation (Fig. 3). In this way, we can save a large amount of money and time bud-get for modeling real-world articulated objects (∼$3 to buy, 10-15min to annotate per object). A thorough comparison between the CAD modeling and reverse scanning can be re-ferred to Sec. 3.2. To summarize, our pipeline can save 33 folds on the money budget and 5 folds on the time budget. research, we propose
AKBNet, an integral pipeline for Category-level Visual
Articulation Manipulation (C-VAM) task. To address C-VAM problem, the vision system AKBNet should be able to estimate the object pose, reconstruct the object geome-try and learn the policy for manipulation at category level.
Thus, it consists of three perception sub-modules:
To utilize the AKB-48 for
• Pose Module for Category-level Articulated Object
Pose Estimation. This module aims to estimate the per-part 6D pose of an unseen articulated object in one category. However, prior researches generally study on kinematic category, that is objects of a category are defined to have the same kinematic structure. Our pose module extends the concept of “category” to semantic category, in which the category is defined by the se-mantics and different kinematic structures are allowed. (Sec. 4.1)
• Shape Module for Articulated Object Reconstruction.
After the pose is obtained, along with the shape code encoding from input images, we can reconstruct the shape for each part [25]. Full geometry is critical for manipulation to determine where to interact with. (Sec. 4.2)
• Manipulation Module for Articulated Object Manip-ulation. Once we obtain the articulation information (e.g. part segments, per-part pose, joint properties, full mesh, etc.) through perception, we can learn the in-teraction policy over the observations. We benchmark manipulation tasks with opening and pulling that are corresponding to revolute and prismatic joint respec-tively. (Sec. 4.3)
To evaluate the AKBNet, we report the results individ-ually and systematically. For individual evaluation of each module, we assume the input to the module is the ground truth of the last module, while for systematical evaluation, the input is the output of the last module. Apparently, we cannot benchmark all the tasks which can be supported by the proposed AKB-48. We hope it could serve as a good platform for future articulation research in computer vision and robotics community.
Our contributions can be summarized in three folds:
• We introduce AKB-48, containing 2,037 articulated models across 48 categories, in which we adopt a multi-modal knowledge graph ArtiKG to organize the rich annotations. It can contribute to close the gap be-tween the current vision and embodied AI researches.
To the best of our knowledge, it is the first large-scale articulation dataset with rich annotations col-lected from the real world.
• We propose a fast articulation knowledge object mod-eling pipeline, FArM, which makes it much easier to collect articulated objects from the real world. Our pipeline greatly eases the cost on time and money when building real-world 3D model datasets.
• We propose an integral pipeline AKBNet for the in-tegral category-level visual articulation manipulation (C-VAM) task. Experiments show our approach is ef-fective both individually and systematically in the real world. 2.