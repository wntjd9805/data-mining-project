Abstract
Scene Graph Generation (SGG) has attracted more and more attention from visual researchers in recent years, since
Scene Graph (SG) is valuable in many downstream tasks due to its rich structural-semantic details. However, the ap-plication value of SG on downstream tasks is severely lim-ited by the predicate classification bias, which is caused by long-tailed data and presented as semantic bias of predicted relation predicates. Existing methods mainly reduce the prediction bias by better aggregating contexts and integrat-ing external priori knowledge, but rarely take the semantic similarities between predicates into account. In this paper, we propose a Predicate Probability Distribution based Loss (PPDL) to train the biased SGG models and obtain unbi-ased Scene Graphs ultimately. Firstly, we propose a predi-cate probability distribution as the semantic representation of a particular predicate class. Afterwards, we re-balance the biased training loss according to the similarity between the predicted probability distribution and the estimated one, and eventually eliminate the long-tailed bias on predicate classification. Notably, the PPDL training method is model-agnostic, and extensive experiments and qualitative anal-yses on the Visual Genome dataset reveal significant per-formance improvements of our method on tail classes com-pared to the state-of-the-art methods. 1.

Introduction
Scene graph generation (SGG) [12] is concerned with producing comprehensive, structured representations about
* Corresponding Author.
This work is supported by the Chinese Scientific and Technical Inno-vation Project 2030 (2018AAA0102100), National Natural Science Foun-dation of China (U1936206, U1903128).
Figure 1. An illustration of long-tailed bias and unbiased scene graph generation (SGG). (a) The long-tailed distribution of dif-ferent predicate categories in Visual Genome [14]. (b) The input image with bounding boxes. (c) The ground-truth scene graph. (d)
The biased SG from VCTree [29] model. (e) The unbiasd SG from the same model with our proposed unbiased training method. images. A scene graph is a directed graph composed of ob-jects entity pairs with their relations in an image, in which the objects and relations are represented as nodes and edges respectively. Due to the rich structural-semantic informa-tion, scene graph has been widely used and achieved great improvements among these downstream tasks such as im-age generation [10, 11], visual question answering [6, 25], image captioning [2, 7, 15, 33, 37, 43], semantic image re-trieval [12,23,24] and thus has been drawing more and more attention.
Although great progress has been made in capturing the object-to-object relationship and visual reasoning, the ex-isting SGG methods still cannot meet the requirements of
above rarely care about the semantic similarity of different predicates.
Inspired by the Focal Loss [18] for dense object detec-tion, we propose a novel loss function, Predicate Prob-ability Distribution based Loss (PPDL), to weaken the model’s suppression for the tail classes. We first build a Predicate Probability Distribution Matrix (PPDM) to represent the estimated probability distribution of each predicate class. As shown in Fig.2, the predicate “walking on” is most likely to be misclassified as “standing on” and
“on”, because these predicates have the higher similarity to
“walking on” in the probability distribution space. Thus, we can determine whether there is a bias in predicate classifi-cation of each training example by examining the similar-ity between the predicted predicate probability distribution and the corresponding estimated one. Thereafter, we can reweight the loss of predicted predicate if existing predic-tion bias. As illustrated in Fig.2 (b), the less semantic triplet
⟨man, on, beach⟩ is generated by biased model (e.g., VC-Tree model). However, after training with PPDL, we can down-weight the loss of head classes and focus on train-ing on hard but meaningful tail classes, eventually obtain-ing much more meaningful predicates (e.g., “walking on” as shown in Fig.2 (d)).
In order to better estimate the probability distribution of each predicate class, we propose a dynamic updating method for PPDM during training time. Instead of relying on simple co-occurrence statistics, the PPDM can be adap-tively updated by summing the probability distributions of the unbiased predicted relationships in each mini-batch and gradually approaches the real average probability distribu-tion of the training data.
In summary, our main contributions are three-fold:
• We analyze the ignorance of semantic relevance among some predicates in existing SGG models and integrate the predicate probability distribution into un-biased training loss, PPDL, which is proposed to miti-gate the impact of long-tailed data on SGG. We high-light that PPDL is a model-agnostic training strategy and thus applicable for a variety of existing SGG mod-els.
• Furthermore, we propose an adaptively updating method for PPDM to estimate realistic probability dis-tribution of each predicate during biased model train-ing, which will be described in more detail below.
• Extensive experiments and qualitative analyses on the widely used SGG benchmark dataset of Visual
Genome demonstrate the effectiveness of our proposed unbiased training loss, PPDL. Impressively, the pro-posed PPDL significantly improves most of the predi-cates, and the performance of tail classes is enhanced apparently.
Figure 2. A toy example of Predicate Probability Distribution (PPD) based loss reweighting method. (a) The input image with (b) The biased loss and SG from the biased bounding boxes. model. (c) An illustration of our proposed PPD based debiasing strategy, which calculates the similarities between predicted PPDs and estimated ground-truth PPDs for subsequent loss reweighting.
In this part, we use different shades of color to indicate the magni-tude of the different values, and the highest value of each distribu-tion is identified by a red dotted box. (d) The unbiased loss and SG from the same biased model with our unbiased training method. downstream tasks for actual application scenarios. As il-lustrated in Fig.1 (a), the number of the head classes (e.g.,
“on”, “has”, “of ”, “in”) far exceeds the tail classes (e.g.,
“in front of ”, “walking on”), which shows the long-tailed distribution of predicates in Visual Genome dataset. On the one hand, driven by long-tail data, most existing bi-ased SGG methods are trained to “prefer” high-frequency predicates. Therefore, the tail classes tend to be neglected and misclassified in predicate classification. For instance, in
Fig.1 (d), the relation between “bike”, and “street” are pre-dicted as high-frequency predicate “on” by the biased SGG model, VCTree [29]. On the other hand, the head classes tend to be less semantic while the tail classes contain much richer semantic information, hence even sometimes both head and tail classes can be regarded as correct in the same object pair, the semantic less result of high-frequency pred-icates can significantly degrade the performance of SGG in downstream tasks that require richer semantics, e.g., story-telling [30]. For these reasons, the fact that the long-tailed data distribution severely degrades the performance of SGG has attracted more attention.
To address the long-tailed distribution among different predicate classes, the Counterfactual Causal Inference [28] has been developed, which distinguishes the good and the bad bias in training, and then keeps the good one. Chen et al. [3] proposed to embed the statistical prior knowledge of predicates and object pairs in datasets into message passing.
Furthermore, instead of relying on prior knowledge or bet-ter inference methods, Yu et al. [38] and Suhail et al. [26] improved the existing biased training method with an un-biased training loss. However, the approaches described
2.