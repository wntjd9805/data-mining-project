Abstract
Most of the existing Out-Of-Distribution (OOD) detec-tion algorithms depend on single input source: the feature, the logit, or the softmax probability. However, the immense diversity of the OOD examples makes such methods frag-ile. There are OOD samples that are easy to identify in the feature space while hard to distinguish in the logit space and vice versa. Motivated by this observation, we propose a novel OOD scoring method named Virtual-logit Match-ing (ViM), which combines the class-agnostic score from feature space and the In-Distribution (ID) class-dependent logits. Speciﬁcally, an additional logit representing the vir-tual OOD class is generated from the residual of the fea-ture against the principal space, and then matched with the original logits by a constant scaling. The probability of this virtual logit after softmax is the indicator of OOD-ness.
To facilitate the evaluation of large-scale OOD detection in academia, we create a new OOD dataset for ImageNet-1K, which is human-annotated and is 8.8× the size of ex-isting datasets. We conducted extensive experiments, in-cluding CNNs and vision transformers, to demonstrate the effectiveness of the proposed ViM score. In particular, us-ing the BiT-S model, our method gets an average AUROC 90.91% on four difﬁcult OOD benchmarks, which is 4% ahead of the best baseline. Code and dataset are available at https://github.com/haoqiwang/vim. 1.

Introduction
Considering most deep image classiﬁcation models are trained in the closed-world setting, the out-of-distribution (OOD) issue arises and deteriorates customer experience when the models are deployed in production, facing inputs coming from the open world [9]. For instance, a model may wrongly but conﬁdently classify an image of crab into the clapping class, even though no crab-related concepts appear in the training set. OOD detection is to decide whether an input belongs to the training distribution. OOD detection
* These two authors contribute equally to the work.
† Corresponding author: Wayne Zhang.
Figure 1. The AUROC (in percentage) of nine OOD detection al-gorithms applied to a BiT model trained on ImageNet-1K. The
OOD datasets are ImageNet-O (x-axis) and OpenImage-O (y-axis). Methods marked with box (cid:3) use the feature space; methods with triangle (cid:52) use the logit; and methods with diamond ♦ use the softmax probability. The proposed method ViM (marked with *) uses information from both features and logits. complements classiﬁcation and ﬁnds its application in ﬁelds such as autonomous driving [19], medical analysis [30] and industrial inspection [1]. A comprehensive review of OOD and related topics including open set recognition, novelty detection and anomaly detection can be found in [38].
The core of an OOD detector is a scoring function φ that maps an input feature x to a scalar in R, indicating to what extent the sample is likely to be OOD. In testing, a threshold
τ is decided, ensuring that the validation set retains at least a given true-positive rate (TPR), e.g. the typical value of 0.95.
The input example is regarded as OOD if φ(x) > τ and as
ID (i.e., in-distribution) otherwise. In cases where a score indicating the ID-ness is convenient, we can mentally use the negative of OOD score as the ID score.
Researchers have designed quite a few scoring functions by seeking properties that are naturally held by ID examples and easily violated by OOD examples, or vice versa. Scores are mainly derived from three sources: (1) the probability, such as the maximum softmax probabilities [13], the min-imum KL-divergence between the softmax and the mean class-conditional distributions [12]; (2) the logit, such as the maximum logits [12], the logsumexp function over log-its [25]; and (3) the feature, such as the norm of the residual between feature and the pre-image of its low-dimensional embedding [27], the minimum Mahalanobis distance be-tween the feature and the class centroids [23], etc. In these methods, OOD scores can be directly computed from ex-isting models without re-training, making the deployment effortless. However, as illustrated in Fig. 1, their perfor-mances are limited by the singleness of their information source: using features exclusively disregards the classiﬁ-cation weights with class-dependent information; using the logit or the softmax solely misses feature variations in the null space [3], which carries class-agnostic information; and the softmax further discards the norm of logits. To cope with the immense diversity that manifests in OOD samples, we ask the question, is it helpful to design an OOD score that utilizes multiple sources?
Built upon the success of prior arts, we design a novel scoring function termed the Virtual-logit Matching (ViM) score, which is the softmax score of a constructed virtual
OOD class whose logit is jointly determined by the feature and the existing logits. To be speciﬁc, the scoring function
ﬁrst extracts the residual of the feature against a principal subspace, and then converts it to a valid logit by match-ing its mean over training samples to the average maximum logits. Finally, the softmax probability of the devised OOD class is the OOD score. From the construction of ViM, we can see intuitively that the smaller the original logits and the greater the residual, the more likely it is to be OOD.
Different from the aforementioned methods, another line of works tailors the features learned by the network to bet-ter identify ID and OOD by imposing dedicated regulariza-tion losses [5, 16, 18, 40] or by exposing generated or real collected OOD samples [22, 37]. As they all require the re-training of the network, we brieﬂy mention them here and will not delve into the details.
Recently, OOD detection in large-scale semantic space has attracted increasing attention [12, 15, 18, 29], advanc-ing OOD detection methods toward real-world applications.
However, the current shortage of clean and realistic OOD datasets for large-scale ID datasets becomes an impediment to the ﬁeld. Previous OOD datasets were curated from pub-lic datasets which were collected with a predeﬁned tag list, such as iNaturalist, Texture, and ImageNet-21k (Tab. 1).
This may lead to a biased performance comparison, specif-ically, the hackability of small coverage as described in
Sec. 5. To avoid this risk, we build a new OOD benchmark for ImageNet-1K [4] models, OpenImage-O, from OpenIm-age dataset [21] with natural class distribution. It contains 17,632 manually ﬁltered images, and is 7.8× larger than the recent ImageNet-O [15] dataset.
We extensively evaluate our method on various models using ImageNet-1K as the ID dataset. The model archi-tectures range from the classical ResNet-50 [11], to the re-Dataset
Image Distribution
#Image Labeling Method natural class statistics 17, 632 image-level manual
OpenImage-O 5, 160 tag-level manual
Texture [2] predeﬁned tag list 10, 000 iNaturalist [18, 34] predeﬁned tag list tag-level manual hard adversarial OOD 2, 000 image-level manual
ImageNet-O [18]
Table 1. OpenImage-O follows natural class statistics, while
ImageNet-O is adversarially built to be hard. Both datasets have image-level OOD annotation. Texture and iNaturalist are selected by tags, and their OOD labels are annotated in tag-level. cent BiT [20], and to the latest ViT-B16 [8], RepVGG [7],
DeiT [33] and Swin Transformer [26]. From the results on four OOD datasets, including OpenImage-O, ImageNet-O,
Texture, and iNaturalist, we found that model selection af-fected the performance of many baseline methods, while our method performs stably well. Specially, our method achieved an average AUROC of 90.91% using the BiT model, which greatly surpasses the best baseline whose av-erage AUROC is 86.62%.
Our contributions are threefold. (1) We proposed a novel
OOD detection method ViM, that works well for a large range of models and datasets, owing to the effective fusion of information from both features and logits. The method is lightweight and fast, requiring neither extra OOD data nor re-training. (2) We conducted comprehensive experiments and ablation studies on the ImageNet-1K dataset, includ-ing CNNs and vision transformers. (3) We curated a new
OOD dataset for ImageNet-1K called OpenImage-O, which is very diverse and contains complex scenes. We believe it will facilitate research on large-scale OOD detection. 2.