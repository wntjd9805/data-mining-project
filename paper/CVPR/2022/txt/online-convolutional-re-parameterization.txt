Abstract
Structural re-parameterization has drawn increasing at-tention in various computer vision tasks. It aims at improv-ing the performance of deep models without introducing any inference-time cost. Though efficient during inference, such models rely heavily on the complicated training-time blocks to achieve high accuracy, leading to large extra training cost.
In this paper, we present online convolutional re-parameterization (OREPA), a two-stage pipeline, aiming to reduce the huge training overhead by squeezing the complex training-time block into a single convolution. To achieve this goal, we introduce a linear scaling layer for better opti-mizing the online blocks. Assisted with the reduced training cost, we also explore some more effective re-param compo-nents. Compared with the state-of-the-art re-param mod-els, OREPA is able to save the training-time memory cost by about 70% and accelerate the training speed by around 2×. Meanwhile, equipped with OREPA, the models out-perform previous methods on ImageNet by up to +0.6%.
We also conduct experiments on object detection and se-mantic segmentation and show consistent improvements on the downstream tasks. Codes are available at https:
//github.com/JUGGHM/OREPA_CVPR2022. 1.

Introduction
Convolutional Neural Networks (CNNs) have seen the success of many computer vision tasks, including classifi-cation [21, 27, 38, 25], object detection [33, 28, 32], seg-mentation [7, 44], etc. The trade-off between accuracy and model efficiency has been widely discussed. In general, a model with higher accuracy usually requires a more compli-cated block [25, 24, 18], a wider or deeper structure [41, 4].
However, such models are always too heavy to be deployed, especially in the scenarios where the hardware performance
*Work done during an internship at Alibaba Cloud Computing Ltd.
†Corresponding Author.
Figure 1. Comparison of (a) a vanilla convolutional layer, (b) a typical re-param block, and (c) our online re-param block in the training phase. All of these structures are converted to the same (d) inference-time structure. is limited, and real-time inference is required. Taking ef-ficiency into consideration, smaller, compacter, and faster models are preferred.
In order to obtain a deploy-friendly model and keep a high accuracy, structural re-parameterization based meth-ods [14, 16, 17, 19] are proposed for a “free” performance improvement.
In such methods, the models have differ-ent structures during the training phase and the inference phase. Specifically, they [16, 1] use complicated training-phase topologies, i.e., re-parameterized blocks, to improve the performance. After training, they squeeze a complicated block into a single linear layer through equivalent transfor-mation. The squeezed models are usually with a neat archi-tecture, e.g., usually a VGG-like [17] or a ResNet-like [16] structure. From this perspective, the re-parameterization strategies can improve model performances without intro-ducing additional inference-time cost.
It is believed that the normalization (norm) layer is the crucial component in re-param models.
In a re-param block (Fig. 1(b)), a norm layer is always added right-after each computational layer. It is observed that the removal of such norm layers would lead to severe performance degra-dation [17, 16]. However, when considering the efficiency,
the utilization of such norm layers unexpectedly brings huge computational overhead in the training phase. The complicated block could be squeezed into a single convolu-tional layer in the inference phase. But, during training, the norm layers are non-linear, i.e., they divide the feature map by its standard deviation, which prevents us from merging the whole block. As a result, there exist plenty of interme-diate computational operations (large FLOPS) and buffered feature maps (high memory usage). Even worse, the high training budget makes it difficult to explore more complex and potentially stronger re-param blocks. Naturally, the fol-lowing question arises,
• Why does normalization matter in re-param?
According to the analysis and experiments, we claim that it is the scaling factors in the norm layers that counts most, since they are able to diversify the optimization direction of different branches.
Based on the observations, we propose Online Re-Parameterization (OREPA) (Fig. 1(c)), a two-stage pipeline which enables us to simplify the complicated training-time re-param blocks. In the first stage, block linearization, we remove all the non-linear norm layers and introduce the lin-ear scaling layers. Such layers are with similar property as norm layers, that they diversify the optimization of dif-ferent branches. Besides, these layers are linear, and can be merged into convolutional layers during training. The second stage, named block squeezing, simplifies the com-plicated linear block into a single convolutional layer. The
OREPA significantly shrinks the training cost by reduc-ing the computational and storage overhead caused by the intermediate computational layers, with only minor com-promising on performance. Moreover, the high-efficiency makes it feasible to explore much more complicated re-parameterized topologies. To validate this, we further pro-pose several re-parameterized components for better perfor-mance.
We evaluate the proposed OREPA on the ImageNet [13] classification task. Compared with the state-of-the-art re-param models [16], OREPA reduces the extra training-time
GPU memory cost by 65% to 75%, and speeds up the train-ing process by 1.5× to 2.3×. Meanwhile, our OREPA-ResNet and OREPA-VGG consistently outperform previous methods [16, 17] by +0.2%∼+0.6%. We evaluate OREPA on the downstream tasks, i.e., object detection and seman-tic segmentation. We find that OREPA could consistently bring performance gain on these tasks.
Our contributions can be summarized as follows:
• We propose the Online Convolutional Reparameteri-zation (OREPA) strategy, which greatly improves the training efficiency of re-parameterization models and makes it possible to explore stronger re-param blocks.
• According to our analysis on the mechanism by which the re-param models work, we replace the norm layers with the introduced linear scaling layers, which still provides diverse optimization directions and preserves the representational capacity.
• Experiments on various vision tasks demonstrate
OREPA outperforms previous re-param models in terms of both accuracy and training efficiency. 2.