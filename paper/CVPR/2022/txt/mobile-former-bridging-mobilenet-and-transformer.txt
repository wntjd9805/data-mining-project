Abstract
We present Mobile-Former, a parallel design of Mo-bileNet and transformer with a two-way bridge in between.
This structure leverages the advantages of MobileNet at lo-cal processing and transformer at global interaction. And the bridge enables bidirectional fusion of local and global features. Different from recent works on vision transformer, the transformer in Mobile-Former contains very few to-kens (e.g. 6 or fewer tokens) that are randomly initial-ized to learn global priors, resulting in low computational cost. Combining with the proposed light-weight cross at-tention to model the bridge, Mobile-Former is not only computationally efficient, but also has more representation power. It outperforms MobileNetV3 at low FLOP regime from 25M to 500M FLOPs on ImageNet classification. For instance, Mobile-Former achieves 77.9% top-1 accuracy at 294M FLOPs, gaining 1.3% over MobileNetV3 but saving 17% of computations. When transferring to object detec-tion, Mobile-Former outperforms MobileNetV3 by 8.6 AP in RetinaNet framework. Furthermore, we build an effi-cient end-to-end detector by replacing backbone, encoder and decoder in DETR with Mobile-Former, which outper-forms DETR by 1.3 AP but saves 52% of computational cost and 36% of parameters. Code will be released at https://github.com/aaboys/mobileformer. 1.

Introduction
Figure 1. Overview of Mobile-Former, which parallelizes Mo-bileNet [28] on the left side and Transformer [38] on the right side.
Different from vision transformer [10] that uses image patches to form tokens, the transformer in Mobile-Former takes very few learnable tokens as input that are randomly initialized. Mobile (refers to MobileNet) and Former (refers to transformer) commu-nicate through a bidirectional bridge, which is modeled by the pro-posed light-weight cross attention. Best viewed in color.
How to design efficient networks to effectively en-code both local processing and global interaction?
Recently, vision transformer (ViT) [10,34] demonstrates the advantage of global processing and achieves significant performance boost over CNNs. However, when constrain-ing the computational budget within 1G FLOPs, the gain of
ViT diminishes. If we further challenge the computational cost, MobileNet [16, 17, 28] and its extensions [12, 19] still dominate their backyard (e.g. fewer than 300M FLOPs for
ImageNet classification) due to their efficiency in local pro-cessing filters via decomposition of depthwise and point-wise convolution. This in turn naturally raises a question:
A straightforward idea is to combine convolution and vision transformer. Recent works [11, 40, 41] show the benefit of combining convolution and vision transformer in series, ei-ther using convolution at the beginning or intertwining con-volution into each transformer block.
In this paper, we shift the design paradigm from series to parallel, and propose a new network that parallelizes Mo-bileNet and transformer with a two-way bridge in between (see Figure 1). We name it Mobile-Former, where Mobile refers to MobileNet and Former stands for transformer. Mo-ficient CNNs and vision transformers from 25M to 500M
FLOPs (see Figure 2), showcasing the usage of transformer at the low FLOP regime where efficient CNNs dominate.
When transferring from image classification to object detection, Mobile-Former significantly outperforms Mo-bileNetV3 as backbone in RetinaNet [21], gaining 8.6 AP (35.8 vs. 27.2 AP) with even less computational cost. In addition, we build an efficient end-to-end detector by using
Mobile-Former to replace backbone and encoder/decoder in
DETR [1]. Using the same number of object queries (100), it gains 1.3 AP over DETR (43.3 vs. 42.0 AP) but has sig-nificantly fewer FLOPs (41G vs. 86G) and smaller model size (26.3M vs. 41.3M). 2.