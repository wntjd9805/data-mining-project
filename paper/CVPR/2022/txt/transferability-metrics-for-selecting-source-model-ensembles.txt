Abstract 1.

Introduction
We address the problem of ensemble selection in trans-fer learning: Given a large pool of source models we want to select an ensemble of models which, after ﬁne-tuning on the target training set, yields the best performance on the target test set. Since ﬁne-tuning all possible ensembles is computationally prohibitive, we aim at predicting perfor-mance on the target dataset using a computationally efﬁ-cient transferability metric. We propose several new trans-ferability metrics designed for this task and evaluate them in a challenging and realistic transfer learning setup for se-mantic segmentation: we create a large and diverse pool of source models by considering 17 source datasets covering a wide variety of image domain, two different architectures, and two pre-training schemes. Given this pool, we then au-tomatically select a subset to form an ensemble performing well on a given target dataset. We compare the ensemble se-lected by our method to two baselines which select a single source model, either (1) from the same pool as our method; or (2) from a pool containing large source models, each with similar capacity as an ensemble. Averaged over 17 target datasets, we outperform these baselines by 6.0% and 2.5% relative mean IoU, respectively.
In transfer learning we want to re-use knowledge previ-ously learned on a source task to help learning a target task.
The most common form of transfer learning in computer vision is to pre-train a single source model on the generic
ILSVRC dataset [3, 14, 26, 31, 33, 38, 59, 80] and then ﬁne-tune it on the target dataset. However, often a more domain-speciﬁc approach can lead to better results [49, 54, 75].
Hence, it is beneﬁcial to have a large pool of diverse source models such that it contains models suited for many differ-ent target tasks. The problem then becomes: how can we automatically and efﬁciently select good source models for a given target task?
Recently, transferability metrics were introduced to ad-dress this problem [4, 44, 55, 65, 66, 76]. The general goal is to select a single source model which, after ﬁne-tuning on the target training set, yields the best performance on the target test set. Transferability metrics enable to select this model efﬁciently without carrying out expensive ﬁne-tuning on the target training set.
While previous transferability metrics consider select-ing a single source model, in this paper we aim at select-ing a subset containing multiple source models to form an ensemble. Ensembles are a general technique used to im-prove model accuracy, out-of-distribution robustness, and to estimate uncertainty [7, 17, 23, 24, 39, 40, 56, 70]. Fur-thermore, by aggregating multiple source models, we can combine knowledge coming from multiple source datasets and image domains, which may be beneﬁcial for a partic-ular target task. Hence, in this paper we extend previous work on transferability by proposing several transferability metrics designed for ensemble selection.
To evaluate ensemble selection we introduce a challeng-ing experimental setup. We consider semantic segmenta-tion as a task, with a truly diverse pool of source mod-els, as we train them on 17 complete datasets spanning a wide variety of images domains, while also varying their model architectures and pre-training schemes (Fig. 1). In contrast, previous works typically focus on image classiﬁ-cation [4, 44, 55, 65, 66, 76], consider a narrower range of at most 4 source datasets [44, 55, 65, 76], and often generate multiple datasets artiﬁcially by sampling different subsets of classes out of a single actual dataset [4, 55, 65, 66].
To summarize, we make the following contributions: (1)
We design transferability metrics for ensemble selection. (2) We consider a challenging application scenario on se-mantic segmentation featuring a large and truly diverse pool of source models. (3) We compare the ensemble selected by our method to two baselines which select a single source model, either from the same pool as our method; or from a pool containing large source models, each with similar ca-pacity as an ensemble. Averaged over 17 target datasets, we outperform these baselines by 6.0% and 2.5% relative mean
IoU, respectively (Sec. 5.2). 2.