Abstract
We propose an embarrassingly simple point annotation scheme to collect weak supervision for instance segmenta-tion. In addition to bounding boxes, we collect binary labels for a set of points uniformly sampled inside each bound-ing box. We show that the existing instance segmentation models developed for full mask supervision can be seam-lessly trained with point-based supervision collected via our scheme. Remarkably, Mask R-CNN trained on COCO,
PASCAL VOC, Cityscapes, and LVIS with only 10 anno-tated random points per object achieves 94%–98% of its fully-supervised performance, setting a strong baseline for weakly-supervised instance segmentation. The new point annotation scheme is approximately 5 times faster than an-notating full object masks, making high-quality instance segmentation more accessible in practice.
Inspired by the point-based annotation form, we propose a modification to PointRend instance segmentation mod-ule. For each object, the new architecture, called Implicit
PointRend, generates parameters for a function that makes the final point-level mask prediction.
Implicit PointRend is more straightforward and uses a single point-level mask loss. Our experiments show that the new module is more suitable for the point-based supervision. 1 1.

Introduction
The task of instance segmentation requires an algo-rithm to locate objects and delineate them with pixel-level binary masks. Manual annotation of object masks for training is significantly more complex and time-consuming than other forms of image annotation like image-level cat-egories [1, 9, 13, 15, 34, 60, 61] or per-object bounding boxes [2, 21, 23, 52]. For example, it takes on average 79.2 seconds per instance to create a polygon-based object mask in COCO [33], whereas a bounding box for an object can be annotated 11 times faster in only 7 seconds [41].
Weakly-supervised methods, that use easier to acquire ground truth annotation forms, make instance segmentation more accessible for new categories or scene types, as the
∼
*Work done during an internship at Facebook AI Research. 1Project page: https://bowenc0221.github.io/point-sup
Figure 1. Our point-based instance annotation scheme. We col-lect object bounding boxes with points randomly sampled inside each box and annotated as the object (red) or background (blue).
Our experiments show that a bounding box and 10 annotated points per instance are approximately 5 times faster to collect than the standard object mask annotation and such ground truth is suf-ficient to train a standard model like Mask R-CNN [19] to achieve 94%–98% of its fully-supervised performance on various datasets. efforts required to collect the data are lower. Such models showed a great progress on smaller datasets under a fixed annotation time budget [4,28], however, they are still far be-hind fully-supervised methods for large-scale datasets like
COCO. The recent BoxInst model [52] outperforms pre-vious weakly-supervised approaches with box supervision but achieves only 85% of its fully-supervised counterpart performance on COCO. A natural question emerges: Is ob-ject mask training data necessary to get closer to the fully-supervised performance? And is there an easier to collect annotation form for the instance segmentation task?
Beyond bounding boxes and image-level categories, point clicks and squiggles are the other time-efficient an-notation forms most commonly used in interactive segmen-tation scenarios [5, 29, 30, 35, 58]. Several semantic and in-stance segmentation methods directly use them for train-ing [4,31]. In our paper, we present a new instance segmen-tation annotation scheme that collects both bounding boxes and point-based annotation (see Figure 1). Unlike previous works where points are clicked by annotators, we follow a different process where points are sampled randomly inside an object bounding box and an annotator is asked to classify each point as the object or background. This point-based annotation scheme is simple and we empirically find such annotation to perform well on large-scale datasets.
With randomly sampled points, the annotation process can be easily simulated with existing instance segmentation ground truth. This property allows us to test the new anno-tation scheme and show its efficacy on multiple large-scale datasets without a major annotation effort that is usually a prerequisite for other annotation schemes [4, 31].
The key advantage of the point-based annotation pro-duced by our scheme is in its ability to seamlessly su-pervise instance segmentation models that directly predict object masks with no changes to their architectures or training pipelines. In our experiments, we train Mask R-CNN [19], PointRend [25], and CondInst [51] with this point-based supervision. For each object, the standard mask loss is computed for ground truth points by interpo-lating mask predictions at these points. We show that the point-based annotation obtained via our scheme is appli-cable across different categories and scene types by sim-ulating it on COCO [32], PASCAL VOC [12], LVIS [17], and Cityscapes [11] datasets. Mask R-CNN trained with only 10 annotated points per object achieve 94%–98% of its fully-supervised performance on these datasets. In ad-dition, we propose a simple point-based data augmentation strategy and explore self-training paradigm for point-based supervision to show that the gap to the full supervision can be further reduced. Finally, we show that point-based pre-training is matching mask-based in a transfer learning setup.
To analyze the performance/annotation time trade-off of the new annotation scheme, we created a simple labeling tool and measured that a trained human annotator classifies a point in 0.9 seconds on average. This means, together with a bounding box annotation that can be done in 7 seconds via extreme point clicking [41], the total annotation time for 10 0.9). This is 5 times points per object is 16 seconds (7 + 10 faster than polygon-based mask annotation in COCO. For a new dataset, the point-based annotation from our scheme allows the standard models to achieve performance close to full supervision at a fraction of the full data collection time.
In our experiments, we observe that PointRend [25] su-pervised with point-based annotations performs on par with
Mask R-CNN [19] supervised in the same way, whereas with the standard full mask supervision it performs better.
Inspired by this finding, we propose Implicit PointRend.
Instead of a coarse mask prediction used in PointRend to provide region-level context to distinguish objects, for each object Implicit PointRend generates different parameters for a function that makes the final point-wise mask predic-tion. The new model is simpler than PointRend: (1) it does not require an importance point sampling during training and (2) it uses a single point-level mask loss instead of two mask losses. Implicit PointRend can be trained directly with point supervision without any intermediate prediction inter-polation steps. Our experiments demonstrate that the new module outperforms PointRend with point supervision.
· 2.