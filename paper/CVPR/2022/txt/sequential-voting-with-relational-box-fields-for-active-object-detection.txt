Abstract
A key component of understanding hand-object interac-tions is the ability to identify the active object – the object that is being manipulated by the human hand. In order to accurately localize the active object, any method must rea-son using information encoded by each image pixel, such as whether it belongs to the hand, the object, or the back-ground. To leverage each pixel as evidence to determine the bounding box of the active object, we propose a pixel-wise voting function. Our pixel-wise voting function takes an initial bounding box as input and produces an improved bounding box of the active object as output. The voting function is designed so that each pixel inside of the input bounding box votes for an improved bounding box, and the box with the majority vote is selected as the output. We call the collection of bounding boxes generated inside of the voting function, the Relational Box Field, as it char-acterizes a field of bounding boxes defined in relationship to the current bounding box. While our voting function is able to improve the bounding box of the active object, one round of voting is typically not enough to accurately local-ize the active object. Therefore, we repeatedly apply the voting function to sequentially improve the location of the bounding box. However, since it is known that repeatedly applying a one-step predictor (i.e., auto-regressive process-ing with our voting function) can cause a data distribution shift, we mitigate this issue using reinforcement learning (RL). We adopt standard RL to learn the voting function pa-rameters and show that it provides a meaningful improve-ment over a standard supervised learning approach. We perform experiments on two large-scale datasets: 100DOH improving AP50 performance by 8% and MECCANO, and 30%, respectively, over the state of the art.
The project page with code and visualizations can be found at https://fuqichen1998.github.io/SequentialVotingDet/. 1.

Introduction
Finding the active object – the object that is being manip-ulated by the human hand – is a crucial task towards under-Figure 1. Relational Box Field and Pixel-wise Voting visual-ization. Each green bounding box is an estimated active object bounding box for a pixel inside the blue input bounding box (ini-tialized with a detected hand box). The voting function selects the majority vote prediction (red) as the improved active object bound-ing box estimate. To ensure visibility, we only show 200 sampled predictions. standing human-object interactions, especially in egocentric videos where hands are the only visible human parts. It is also an essential step to a variety of downstream tasks in-cluding joint hand-object pose estimation [6, 7, 19, 25, 33], reconstruction [5, 14], activity recognition [10, 20, 22], and imitation learning [34]. However, an accurate localization of the active object can be challenging due to natural oc-clusions caused by the hands during interactions. During a hand-object interaction, it is common for the hand to oc-clude most of the object in order to grasp the object. On one hand, this makes it hard to detect active objects. On the other hand, the appearance of the hand actually contains important information about the location, shape, size, and pose of the active object. It is important to develop com-puter vision algorithms that can leverage each pixel of the image, especially the hands and objects, to accurately esti-mate a bounding box around the active object.
To advance the state of art in active object detection, we introduce a pixel-wise voting function to improve the active object bounding box estimate while being robust to occlu-sion. The voting function takes as input an initial bound-ing box estimate of the active object (typically seeded by bounding box around the hand region), and then predicts an improved bounding box, where the improved bounding box is tighter and more centered around the active object. In-side the voting function, we predict a large set of improved active object bounding boxes, by allowing each pixel in-side the input box to regress a new bounding box. We call the collection of active object bounding boxes as the Rela-tional Box Field (RBF), as they represent a field of bound-ing boxes related to pixels inside the input bounding box.
As we can observe in Fig. 1, the pixel-wise predictions can be quite diverse because they depend on features from dif-ferent locations in the input image. Our method overcomes inconsistencies across the RBF by using a technique similar to [9,26,32,36], where our voting function finds the consen-sus from pixel-wise predictions by selecting the bounding box with a majority vote. Similar to the Hough transform, our voting scheme is able to minimize the influence of out-liers and generate stable predictions through the power of aggregation. We also show later in our experiments that our voting scheme is more robust when compared to standard regression methods.
While the voting function can provide a better active ob-ject bounding box estimation, one round of voting is typ-ically not enough to accurately localize the active object.
Similar to the idea of boosting [12] where one uses a se-quence of computational units to iteratively improve pre-diction performance, we apply multiple rounds of the voting function to progressively obtain a more accurate active ob-ject bounding box. However, repeatedly applying a function trained for one-step prediction (i.e., supervised learning or behavior cloning) can result in a data distribution shift, also known as covariate shift [31]. For example, each time a one-step predictor is used in an auto-regressive manner (i.e., the output is passed as input for the next sequence), it can intro-duce small errors which can compound over the prediction sequence, leading to a data distribution shift, which can lead to bad performance towards the end of a sequence. In order to mitigate this issue, we use reinforcement learning (RL) to learn the proposed voting function. RL is designed to ac-count for distribution shifts in auto-regressive processes by evaluating and optimizing over sequences. Specifically, we use a Markov Decision Process (MDP) to model the sequen-tial decision-making process of obtaining an optimal active object bounding. In each step of the sequential decision-making process, the voting function (the MDP policy) takes an initial estimate of the active object bounding box as input (the MDP state) and predicts a new improved active object
Figure 2. Previous methods detect active object by first detect-ing hands and objects independently, followed by an interaction detection to match active objects and hands. Our method directly detects the active object corresponding to each hand a sequential decision-making process dependent on hand. bounding box (the MDP action and next state).
Interest-ingly, we find that the first estimate of the bounding box of the active object can be seeded using a hand bounding box since active objects are usually near the hands. We demon-strate that using reinforcement learning provides a meaning-ful improvement over standard supervised learning models for active object detection.
Our approach is evaluated on two large-scale hand-object interaction datasets: 100DOH [30] and MECCANO
[27] datasets. Experiments show that our method achieves new state-of-the-art performance on both hand-object de-tection and active object detection tasks. We also demon-strate the better generalization ability of our model by eval-uating its performance across the datasets. Last, we provide a comprehensive ablation study for our design choices. 2.