Abstract
Temporal grounding in videos aims to localize one tar-get video segment that semantically corresponds to a given query sentence. Thanks to the semantic diversity of natural language descriptions, temporal grounding allows activ-ity grounding beyond pre-defined classes and has received increasing attention in recent years. The semantic diver-sity is rooted in the principle of compositionality in lin-guistics, where novel semantics can be systematically de-scribed by combining known words in novel ways (composi-tional generalization). However, current temporal ground-ing datasets do not specifically test for the compositional generalizability. To systematically measure the composi-tional generalizability of temporal grounding models, we introduce a new Compositional Temporal Grounding task and construct two new dataset splits, i.e., Charades-CG and ActivityNet-CG. Evaluating the state-of-the-art meth-ods on our new dataset splits, we empirically find that they fail to generalize to queries with novel combinations of seen words. To tackle this challenge, we propose a varia-tional cross-graph reasoning framework that explicitly de-composes video and language into multiple structured hi-erarchies and learns fine-grained semantic correspondence among them. Experiments illustrate the superior compo-sitional generalizability of our approach. The repository of this work is at https://github.com/YYJMJC/
Compositional-Temporal-Grounding. 1.

Introduction
Understanding rich and diverse activities in videos is a prominent and fundamental goal of video understanding.
While there have been significant works in activity recog-nition [3, 8] and localization [28, 37], one major limitation of these works is that they are restricted to pre-defined ac-∗Yueting Zhuang is the corresponding author.
Figure 1. (a) On the top, we show three examples of two queries. (b) On the bottom, we report comparisons on Charades-CG with metric R@1, IoU@0.5. The left blue box represents the original model. The middle yellow box represents the model with shuffled queries as input. The right green box represents the performance on the queries that contain novel compositions. tion classes, thus suffering from scaling to various complex activities. A natural solution to this problem is to utilize the systematic compositionality [4, 9, 31] of human language, which allows us to form novel compositions by combin-ing known words in novel ways to describe unseen activ-ities (i.e. compositional generalization). Therefore, a new task, namely temporal grounding in videos [10, 17], has recently received increasing attention. Formally, give an untrimmed video and a query sentence, it aims to identify the start and end timestamps of one specific moment that semantically corresponds to the given query sentence.
Although the compositional generalization is a key prop-erty of human language that allows temporal grounding beyond pre-defined classes, current temporal grounding datasets do not specifically test for this ability. The train-ing and testing splits of existing datasets contain almost the same compositions (e.g. verb-noun pair, adjective-noun pair, etc). Our statistical results show that only 1.37% and 5.19% of testing sentences contain novel compositions in the Charades-STA [10] and ActivityNet Captions [17]
datasets, respectively. To systematically measure the com-positional generalizability (CG) of existing methods, we in-troduce a new task, Compositional Temporal Grounding.
Our compositional temporal grounding task aims to test whether the model can generalize to the sentences that con-tain novel compositions of seen words. We construct two re-organized datasets Charades-CG and ActivityNet-CG.
Our dataset split protocols enable us to measure whether a model can generalize to novel compositions, of which the individual components have been observed during training but the combination is novel.
Using our newly constructed datasets, we evaluate mod-ern state-of-the-art (SOTA) temporal grounding models, and empirically find that SOTA models fail to achieve compositional generalization, though they have achieved promising progress on the typical temporal grounding task.
We observe that their performance drops dramatically (Fig-ure 1.b, left vs. right). The results indicate that the SOTA models may not well generalize to novel compositions. Fur-thermore, as word order is a crucial factor for the composi-tionality of language, we analyze the word order sensitivity of SOTA models to gain more intuitive insight. Specifically, we randomly shuffled queries in advance and then use the shuffled sentences to train and evaluate the models. Sur-prisingly, we find that they are insensitive to the word order, even though permuting word order destroys the complete semantics of original sentences (Figure 1.b, left vs. middle).
These observations confirm with recent studies [35,41] sug-gesting that current models are heavily driven by superficial correlations. This pushes us to rethink the solution of tem-poral grounding.
When we systematically analyze the SOTA models, we find that previous temporal grounding methods largely ne-glect the structured semantics in video and language, which is crucial for compositional reasoning. These methods [10, 32,45,47] mainly encode both sentence and video segments into unstructured global representations, respectively, and then devise specific cross-modal interaction modules to fuse them for final prediction. These global representations fail to explicitly model video structure and language composi-tions. Take the novel composition of “throws flowers” in
Figure 1.a as example. If the model infers the individual semantics of the two words, as well as establish the cor-respondence of them to specific semantics in video (i.e. the action “throw” and the object “flower” in video ), the model can easily localize the novel composition in video by com-posing the corresponding video semantics of the two words. insight, we propose a novel
VarIational croSs-graph reAsoning (VISA) framework for compositional temporal grounding. By explicitly modeling the semantic structures of video and language, and inferring the fine-grained correspondence between them, our VISA model can achieve joint compositional reasoning. Specifi-Motivated by this cally, we first introduce a hierarchical semantic graph that explicitly decomposes both video and language into three semantic hierarchies (i.e. global events, local actions, and atomic objects). The hierarchical semantic graph serves as unified structured representations for both video and lan-guage, which tightly couple multi-granularity semantics be-tween the two modalities. Second, we propose a variational cross-graph correspondence learning that establishes fine-grained semantic correspondence between the semantic hi-erarchical graphs of video and language.
Our contributions are summarized as follows:
• We introduce a new task, Compositional Temporal
Grounding, as well as new splits of two prevailing tem-poral grounding datasets, which are able to measure the compositional generalizability of existing methods.
• We perform in-depth analyses on several SOTA mod-els and empirically find that they fail to achieve com-positional generalization
• We propose a VarIational croSs-graph reAsoning (VISA) framework that decomposes video and lan-guage into hierarchical graphs and learns fine-grained cross-graph correspondence between them.
• Experimental results validate the significant superior-ity of our approach on compositional generalizability. 2.