Abstract
Video frame interpolation (VFI) works generally predict intermediate frame(s) by first estimating the motion between inputs and then warping the inputs to the target time with the estimated motion. This approach, however, is not opti-mal when the temporal distance between the input sequence increases as existing motion estimation modules cannot ef-fectively handle large motions. Hence, VFI works perform well for small frame gaps and perform poorly as the frame gap increases. In this work, we propose a novel framework to address this problem. We argue that when there is a large gap between inputs, instead of estimating imprecise motion that will eventually lead to inaccurate interpolation, we can safely propagate from one side of the input up to a reliable time frame using the other input as a reference. Then, the rest of the intermediate frames can be interpolated using standard approaches as the temporal gap is now narrowed.
To this end, we propose a propagation network (PNet) by extending the classic feature-level forecasting with a novel motion-to-feature approach. To be thorough, we adopt a simple interpolation model along with PNet as our full model and design a simple procedure to train the full model in an end-to-end manner. Experimental results on several benchmark datasets confirm the effectiveness of our method for long-term VFI compared to state-of-the-art approaches. 1.

Introduction
Video frame interpolation (VFI) aims at predicting one or more intermediate frames from a given frame sequence.
Given inputs ⟨xt, xt+n⟩, where n is the frame gap between the inputs, existing VFI works generally follow two steps.
First, they estimate the motion between xt and xt+n us-ing off-the-shelf motion estimation modules or by imposing motion constraints. Then, they warp the inputs to the target time and synthesize an intermediate frame.
VFI works target temporal super-resolution on a premise the frame rate of the input sequence is often al-that ready sufficiently high. We have experimentally verified this notion by evaluating several state-of-the-art VFI meth-ods [2, 12, 17, 19, 30] on input sequences sampled at dif-ferent frame rates. Even though a reasonable performance decrease is an expected phenomenon, we observed a signif-icant drop in performance when the frame rate of the input sequence decreases (see Table 1), highlighting that inter-polating frames becomes very challenging as the temporal distance between consecutive frames increases. Moreover, far less attention has been given to this problem in past lit-erature as most evaluations have been done on videos with fixed frame rate (mostly 30 fps). We argue that the main reason behind this limitation is partly associated with the working principle of VFI works. If the estimated motion between inputs is inaccurate, then the interpolated frame synthesized by time warping the inputs with the estimated motion will also likely be inaccurate. This is particularly problematic when the temporal gap between input frames is large as existing flow or kernel based motion estimation modules can not effectively handle large motions.
In this work, we tackle the long-term video interpolation problem and propose a general VFI framework robust to rel-atively low frame rates. Specifically, when there is a large gap between input frames, instead of predicting the motion between the inputs which will likely be imprecise and even-tually lead to inaccurate interpolation, we conjecture that we can safely propagate from one side of the input to a re-liable extent of time using the other input as a useful refer-ence, i.e. given ⟨xt, xt+n⟩, we propagate up to xt+∆t from the side of the first input xt and we similarly propagate up to xt+n−∆t from the side of the second input xt+n, where
∆t is the extent of propagation. This is intuitive because the intermediate frames in the neighborhood of xt will most likely depend on xt compared to xt+n, and vice versa. Once we propagate to a reliable time frame from both sides, the rest of the intermediate frames between xt+∆t and xt+n−∆t can be interpolated using existing interpolation approaches as the temporal gap is now reduced to n − 2∆t.
To this end, we propose a propagation network (PNet) that predicts future frames by relying more on one of the inputs while attending the other. We accomplish this by extending the classic feature-to-feature (F2F) forecasting
[5,8,36,37,43,44] with a novel motion-to-feature (M2F) ap-proach, where we introduce optical flow as another modal-ity to guide the propagation of features and to enforce tem-poral consistency between the propagated features. Unlike feature supervision which makes a network more dependent on the semantics of input frames, our motion supervision allows the network to focus on the motion between inputs and ensures features are propagated accordingly irrespec-tive of the contents of the images. Moreover, while most
F2F works focus on predicting task-specific outputs such as segmentation maps, we perform RGB forecasting by de-signing a frame synthesis network that reconstructs frames from the propagated features in a coarse-to-fine manner.
We experimentally show that the proposed PNet can be used as a plug in module to make existing state-of-the-art
VFI approaches [2, 12, 17, 19, 30] robust particularly when there is a considerable temporal gap between inputs. To be thorough, we adopt a light version of SloMo [17] along with PNet as our full model and devise a simple, yet ef-fective, procedure to successfully train the full model in an end-to-end manner. We comprehensively analyze our work and previous methods on several widely used datasets
[11, 27, 40] and confirm the favorability of our approach.
Moreover, we carry out ablation experiments to shed light on the network design and loss function choices. 2.