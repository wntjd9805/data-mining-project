Abstract
Transfer learning has been recently popularized as a data-efﬁcient alternative to training models from scratch, in particular for computer vision tasks where it provides a remarkably solid baseline. The emergence of rich model repositories, such as TensorFlow Hub, enables the prac-titioners and researchers to unleash the potential of these models across a wide range of downstream tasks. As these repositories keep growing exponentially, efﬁciently select-ing a good model for the task at hand becomes paramount.
We provide a formalization of this problem through a famil-iar notion of regret and introduce the predominant strate-gies, namely task-agnostic (e.g. ranking models by their
ImageNet performance) and task-aware search strategies (such as linear or kNN evaluation). We conduct a large-scale empirical study and show that both task-agnostic and task-aware methods can yield high regret. We then propose a simple and computationally efﬁcient hybrid search strat-egy which outperforms the existing approaches. We high-light the practical beneﬁts of the proposed solution on a set of 19 diverse vision tasks. 1.

Introduction
Services such as TensorFlow Hub or PyTorch Hub1 offer a plethora of pre-trained models that often achieve state-of-the-art performance on speciﬁc tasks in the vision domain.
The predominant approach, namely choosing a pre-trained model and ﬁne-tuning it to the downstream task, is an ef-fective and data efﬁcient approach [12, 13, 18, 23, 33, 35].
Perhaps surprisingly, this approach is also effective when the pre-training task is signiﬁcantly different from the target task, such as when applying an ImageNet pre-trained model to diabetic retinopathy classiﬁcation [20]. Fine-tuning of-ten entails adding several more layers to the pre-trained deep network and tuning all the parameters using a limited
*Work done while interning at Google Research. Correspondence to C.
Renggli (cedric.renggli@inf.ethz.ch) and M. Lucic (lucic@google.com). 1https://tfhub.dev and https://pytorch.org/hub amount of downstream data. Due to the fact that all pa-rameters are being updated, this process can be extremely costly in terms of compute [35]. Fine-tuning all models to
ﬁnd the best performing one is becoming computationally infeasible. A more efﬁcient alternative is to simply train a cheap classiﬁer on top of the learned representation (e.g. pre-logits). However, the performance gap with respect to
ﬁne-tuning can be rather large [13, 14].
This raises a very practical question: Given a new task, how to pick the best model to ﬁne-tune? This question was intensively studied in recent years and existing approaches can be divided into two groups: (a) task-agnostic model search strategies, which rank pre-trained models indepen-dently of the downstream task (e.g. ranking the models by
ImageNet accuracy, if available) [14], and (b) task-aware model search strategies, which make use of the downstream dataset in order to rank models (e.g. kNN classiﬁer accu-racy as a proxy for ﬁne-tuning accuracy, or using the meta-learned Task2Vec representations) [1, 16, 22]. Most of the prior work attempts to answer this question by using a ho-mogeneous sets of pre-trained models whereby the models share the same architecture or they were trained on the same dataset. However, this does not reﬂect the current landscape of models available in online repositories.
As a result, several practically relevant questions re-mained open. Firstly, how do existing methods compare in the presence of both “generalist models” (e.g. models trained on a relatively diverse distribution such as Ima-geNet) and “expert models” (e.g. trained on domain-speciﬁc datasets such as plants), across a diverse collection of datasets? Secondly, is there a method which strikes a good balance between computational cost and performance?
Our contributions. In this paper, we provide a large-scale, systematic empirical study of these questions. (i) We deﬁne and motivate the model search problem through a no-tion of regret. We conduct the ﬁrst study of this problem in a realistic setting focusing on heterogeneous model pools. (ii) We consider 19 downstream tasks on a heterogeneous set of 46 models grouped into 5 representative sets. (iii) We highlight the dependence of the performance of each strat-egy on the constraints of the model pool, and show that, per-haps surprisingly, both task-aware and task-agnostic proxies suffer a large regret on a signiﬁcant fraction of downstream tasks. (iv) Finally, we develop a hybrid approach which generalizes across model pools as a practical alternative. 2.