Abstract
Various facial manipulation techniques have drawn seri-ous public concerns in morality, security, and privacy. Al-though existing face forgery classiﬁers achieve promising performance on detecting fake images, these methods are vulnerable to adversarial examples with injected impercep-tible perturbations on the pixels. Meanwhile, many face forgery detectors always utilize the frequency diversity be-tween real and fake faces as a crucial clue. In this paper, in-stead of injecting adversarial perturbations into the spatial domain, we propose a frequency adversarial attack method against face forgery detectors. Concretely, we apply dis-crete cosine transform (DCT) on the input images and in-troduce a fusion module to capture the salient region of ad-versary in the frequency domain. Compared with existing adversarial attacks (e.g. FGSM, PGD) in the spatial do-main, our method is more imperceptible to human observers and does not degrade the visual quality of the original im-ages. Moreover, inspired by the idea of meta-learning, we also propose a hybrid adversarial attack that performs at-tacks in both the spatial and frequency domains. Exten-sive experiments indicate that the proposed method fools not only the spatial-based detectors but also the state-of-the-art frequency-based detectors effectively. In addition, the proposed frequency attack enhances the transferability across face forgery detectors as black-box attacks. 1.

Introduction
With the rapid development of generative adversarial network (GAN), face forgery generation attracts increas-ing attention, such as Deepfake [46], FaceSwap [25],
Face2Face [45], and NeuralTextures [44]. These techniques derive plenty of interesting applications, for instance, trying on makeup virtually and editing faces in the ﬁlm industry.
However, despite the positive aspect, face forgery genera-tion may be maliciously abused, causing serious problems
* Corresponding author. (a) Original image (b) FGSM [51] (c) PGD [31] (d) Ours
Figure 1.
Illustration of adversarial examples generated by
FGSM [15], PGD [31] and our method. The original image is classiﬁed as a fake face by the face forgery detector. After im-plementing these attacks, the adversarial examples are misclassi-ﬁed as real faces. Compared with FGSM [15] and PGD [31], our method associated with the frequency adversarial attack generates more natural perturbations, where the image quality of the adver-sarial example is much closer to the original image. of security and privacy. Therefore, it is essential to design face forgery detection methods to distinguish the manipu-lated face from the real one.
Various face forgery detectors [1,7,8,38] are proposed to learn the decision boundary between real and fake faces and achieve signiﬁcant performance on multiple datasets [9,37].
However, existing methods are vulnerable to the adversar-ial examples, which leaves a serious backdoor for the secu-rity of detectors. For instance, a forged face image that is classiﬁed correctly as fake by adding adversarial perturba-tions can fool the detector to make a wrong decision as real.
Existing works [4, 13, 21, 26, 34] have explored the robust-ness of face forgery detection methods, but these methods add adversarial perturbations or patches on the original im-ages, which are easily recognized by human eyes. In brief, the adversarial examples aim to fool a face forgery detec-tor, while the objective of face forgery generation is to fool humans. An implicit attack that fools humans and detectors at the same time brings out a more serious problem of se-curity. Meanwhile, more and more works [5, 36] consider the frequency diversity between real and fake faces as the essential clues for face forgery detection. It inspires us to conduct the adversarial attack in the frequency domain to boost the transferablility across various detectors.
To address the above issues, we propose a frequency ad-versarial attack method to add adversarial perturbations in the frequency domain. First, we apply discrete cosine trans-form (DCT) to transfer the input images into the frequency domain. Speciﬁcally, we utilize a fusion module to slightly modify the energy in different frequency bands via the ad-versarial loss. The indirect injection of adversary into fre-quency domain avoids the redundant noise of attacks in the spatial domain (e.g., FGSM [51], PGD [31]) and does not degrade the visual quality of original images. After that, we apply inverse DCT back to the spatial domain and obtain the ﬁnal adversarial examples. For face forgery detectors, some existing methods [41, 53] only consider the noise pat-tern in the spatial domain to detect the fake faces, while oth-ers [28, 30, 36] utilize the frequency information as a clue.
Moreover, some methods [5, 27, 32] combine the discrimi-native features from both domains to learn the boundary be-tween real and fake faces. Therefore, in order to enhance the generalization of the proposed attack method, we propose a hybrid adversarial attack to integrate the spatial adversarial attack and frequency adversarial attack into a whole frame-work. Inspired by the idea of meta-learning [35], we alter-nately optimize the perturbations based on the adversarial gradients in different domains. The compatible ensemble of adversarial attacks can reserve the virtues of attacks in both domains. Adversarial examples with different attacks are illustrated in Figure 1.
Our main contributions can be summarized as follows.
• For the task of face forgery detection, we propose a novel adversarial attack method to generate perturba-tions in the frequency domain. Compared with the pre-vious attacks, our method generates more impercepti-ble perturbations for human observers.
• To further boost the transferability of the attack, we propose a hybrid adversarial attack based on the strat-egy of meta-learning to simultaneously perform at-tacks on the spatial and frequency domain.
• We perform the proposed method both on the spatial-based face forgery detectors and the state-of-the-art frequency-based detectors. Extensive experiments on benchmarks demonstrate the effectiveness of our at-tack under both white-box and black-box settings. 2.