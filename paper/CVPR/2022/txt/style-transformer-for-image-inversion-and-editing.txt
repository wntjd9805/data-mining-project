Abstract
Existing GAN inversion methods fail to provide latent codes for reliable reconstruction and ﬂexible editing simul-taneously. This paper presents a transformer-based image inversion and editing model for pretrained StyleGAN which is not only with less distortions, but also of high quality and ﬂexibility for editing. The proposed model employs a
CNN encoder to provide multi-scale image features as keys and values. Meanwhile it regards the style code to be deter-mined for different layers of the generator as queries. It ﬁrst initializes query tokens as learnable parameters and maps them into W + space. Then the multi-stage alternate self-and cross-attention are utilized, updating queries with the purpose of inverting the input by the generator. Moreover, based on the inverted code, we investigate the reference-and label-based attribute editing through a pretrained la-tent classiﬁer, and achieve ﬂexible image-to-image transla-tion with high quality results. Extensive experiments are carried out, showing better performances on both inversion and editing tasks within StyleGAN. Codes are available at https://github.com/sapphire497/style-transformer. 1.

Introduction
Generative Adversarial Network (GAN) [5, 25, 27] has been signiﬁcantly improved during recent years. Particu-larly, with the help of AdaIN [15] or it variation Modulat-edConv, StyleGAN [17, 18] is able to synthesize high res-olution images with moderate quality. Therefore, utilizing
*Corresponding author, email: sunli@ee.ecnu.edu.cn. This work is supported by the Science and Technology Commission of Shanghai Mu-nicipality (No.19511120800) and Natural Science Foundation of China (No.61302125).
the pretrained and ﬁxed StyleGAN for downstream tasks becomes a hot research topic, especially in the editing task of image-to-image (I2I) translation [3, 11, 29, 30, 35, 36].
To edit a given real-world image, we ﬁrst need to ﬁnd out its input noise vector z or intermediate latent code w, which can faithfully reconstruct the speciﬁed real image us-ing the pretrained generator. Then, the code is modiﬁed by an offset corresponding to the target attribute, so that it can be mapped into an edited image, while preserving the original details. Despite of the great efforts, invert-ing [1, 2, 28, 31, 40] or editing [3, 11, 29, 30, 35] images for StyleGAN is still challenging due to following reasons.
First, there are several candidate latent embeddings. Ex-isting methods [31, 35, 42] reveal that different choices on them are critical. Compared to Z or W space with a sin-gle 512-d vector, W + has the enough ability to describe image details, therefore it is suitable for inversion. In W +, each image is represented by 18 different codes, and each of them is 512-d. They are given to the generator to formulate features and ﬁnal synthesis from low to high resolutions in sequence. However, the code in W + can not be well edited unless imposing enough regularization. Second, the distri-bution in W or W + are highly complex. Real images only lie on the manifold in the space [31]. Moreover, different di-mensions are often entangled for a single attribute, making independent editing difﬁcult.
This paper aims to improve the encoder-based image in-version and editing for StyleGAN at the same time.
In-spired by the great success of transformer in image clas-siﬁcation [10, 22] and object detection [6, 43], we utilize it to ﬁnd the appropriate latent code in W + space for image inversion and editing tasks. The basic idea is to regard la-tent codes in different generation stages as query tokens, and image features at different spatial positions as keys and values, then perform the multi-head cross-attention to up-date the queries in an iterative way. Meanwhile, before the cross-attention, the queries are also allowed to access oth-ers through the self-attention, to enhance the regularization on them, so the ﬁnal codes given to the generator become tightly linked.
Particularly, queries ﬁrst interact with image features (keys) by comparing similarities between each query-key pair. Then they are organized into the attention matrix to dynamically weight the features (values) and update queries for the transformer block in next stage. The image features, used as keys and values, are obtained by a CNN encoder. To capture the image details at different resolutions, we employ a two-pyramid encoder proposed in [28] to provide multi-scale features as keys and values. Note that our model has the multiple cross-attentions from low to high resolutions, and the style queries are gradually updated by features at different scales. Therefore, general contents in queries are
ﬁrst formulated, and then reﬁned by details in the higher resolution. After several times self- and cross-attentions, queries absorb enough details from the input image, so they can be utilized to invert it by the pretrained generator.
We are further interested in the way to edit the codes for translating a speciﬁed attribute. Traditional approaches
[11, 29, 35] assume the linear separations in the latent space for a binary attribute, so inverted code from different images are edited by the same direction. We argue that the identical direction is not optimal for the editing quality, and may re-duce the result diversity. Inspired by [7, 14], we divide the image editing in StyleGAN into two different types. One is label-based editing, in which only target label is speciﬁed.
The other is reference-based editing, which requires another image to supply the desired style. For the former, a pre-trained non-linear latent classiﬁer is used to determine the direction. it computes a loss for the inverted code accord-ing to the target label, and its gradient is back-propagated to the code, giving the editing direction. In the latter case, we want to determine the exact editing vector from the ref-erence. Therefore, the inverted code from the source is used as query, and from the reference as key and value. The cross-attention is performed between them. The parame-ters in the attention module is trained under the supervision from the latent classiﬁer, encouraging the edited attribute to be similar with the reference and other attributes without any changes. The proposed editing method is able to give the diverse results while maintaining the quality of image.
The contribution of the paper is summarized into fol-lowing aspects. First, we propose novel multi-stage style transformer in W + space to invert image accurately. The transformer includes the self- and cross-attention modules, in which the style queries gradually get updated from the multi-scale image features. Second, we characterize the image editing in StyleGAN into label-based and reference-based, and use a non-linear classiﬁer to generate the editing vector. Diverse and ﬁdelity editing results are obtained. 2.