Abstract
Real-world autonomous driving datasets comprise of im-ages aggregated from different drives on the road. The ability to relight captured scenes to unseen lighting con-ditions, in a controllable manner, presents an opportunity to augment datasets with a richer variety of lighting con-ditions, similar to what would be encountered in the real-world. This paper presents a novel image-based relight-ing pipeline, SIMBAR, that can work with a single image as input. To the best of our knowledge, there is no prior work on scene relighting leveraging explicit geometric rep-resentations from a single image. We present qualitative comparisons with prior multi-view scene relighting base-lines. To further validate and effectively quantify the ben-efit of leveraging SIMBAR for data augmentation for au-tomated driving vision tasks, object detection and tracking experiments are conducted with a state-of-the-art method, a Multiple Object Tracking Accuracy (MOTA) of 93.3% is achieved with CenterTrack on SIMBAR-augmented KITTI -an impressive 9.0% relative improvement over the baseline
MOTA of 85.6% with CenterTrack on original KITTI, both models trained from scratch and tested on Virtual KITTI.
For more details and sample relit datasets, please visit our project website (https://simbarv1.github.io). 1.

Introduction
A lack of diversity in lighting conditions is a known is-sue with manually collected real-world autonomous driving datasets [1, 3, 14, 18]. For example, KITTI [18] has video sequences captured only during noon, with similar lighting and shadow conditions across different sequences. More re-cent datasets [32, 37, 59], one such as BDD100K [59], are comparatively better in terms of diversity and have images captured during multiple times of the day. Still, between images collected from the same drive, there are minimal changes in lighting conditions. Furthermore, attempting to acquire data for all types of lighting conditions is implausi-ble both in terms of time and money.
This lack of diversity in lighting conditions and by ex-Figure 1.
Input images (Left) are shown against SIMBAR-relit outputs (Middle, Right). SIMBAR synthesized two lighting varia-tions for (a)(b) Div2k, (c) BDD100K and (d) KITTI. tension, the shadows present within a scene, often serves as a crucial roadblock in successful real-world deployment of perception models for safety-critical automated driving ap-plications. Models trained with limited lighting conditions are unable to generalize to the plethora of lighting condi-tions encountered in the real-world [26, 28]. The ability to relight existing datasets in a controllable manner presents an opportunity to develop improved perception models. is an extremely difficult vision task.
However, scene relighting, in the absence of depth sen-implic-sors, itly comprises of three main sub-tasks: shadow detection
[10, 25, 53], removal [23, 24, 53] and insertion [61]. Of these, shadow removal and insertion are most challenging because shadows blend tightly with the source object ge-ometry [2, 15]. This coupling makes it difficult to separate the shadow from its parent object without a strong 3D ge-It
ometric understanding of the scene [4, 8, 20]. To address this, most prior scene relighting methods rely on multiple camera views of the source lighting condition to estimate the 3D scene geometry [42, 49, 62]. The relatively few prior methods that can work with a single image are based on
Generative Adversarial Networks (GANs) [6]. GANs are known to be difficult to train [31, 38], limited in control-lability [52], and often produce results that are physically inconsistent with scene geometry [16]. To the best of our knowledge, there is no prior work on controllable scene re-lighting using a single input image.
This paper presents a novel Single IMage-BAsed scene
Relighting pipeline, SIMBAR. It takes a single image as input and produces relit versions for a wide variety of sun positions and sky zeniths, as shown in Fig. 1. The top two rows show relit results from the Div2k [1]. Div2k is an internet-scraped dataset with images of a wide variety of ob-ject classes, that SIMBAR is able to effectively relight. The first row shows realistic variations in sky colors, shadow ori-entations, and consistent cast shadow locations and light in-tensities for an outdoor scene with complex structures. The second row is a challenging low-light desert scene. SIM-BAR cleanly removes existing hard cast shadows of the rock in the foreground and realistically recasts geometri-cally consistent shadows for the provided sun angle. Addi-tionally, the mountainous landscape in the horizon has also been effectively relit. The third and fourth rows also show geometrically consistent and visually realistic relit versions of a KITTI road driving scene and a tunnel/underpass scene from BDD100K respectively. Most notable is the variation in hard cast shadows of the tunnel in the BDD100K exam-ple and the two cars in the KITTI example.
SIMBAR consists of two main modules: (i) geometry es-timation and (ii) image relighting. The geometry estimation module is responsible for computing scene mesh proxy and illumination buffers. We are inspired by WorldSheet [22] to use external depth networks to obtain a scene mesh. Note that WorldSheet is a novel view synthesis pipeline that does not have relighting purpose. The image relighting module is inspired by prior work on multi-view scene relighting us-ing a geometry aware network [42], referred to as MVR for brevity. Section 3.1 provides a short overview of Sin-gle Image-Based Scene Geometry Estimation and MVR, followed by a detailed description of SIMBARâ€™s pipeline description in Section 3.2. Our work is closest in terms of goals and overall pipeline structure to MVR. Therefore, scene relighting comparisons are provided with both out-of-the-box MVR and its improved version, MVR-I, where we refined MVR for autonomous driving datasets with limited views, in Section 3.4. Across the board, SIMBAR provides significantly more realistic and geometrically consistent re-lit images, even though it takes as input a single image, as compared to MVR/MVR-I that take as input multiple im-ages of the same scene.
Another major limitation of all prior works on scene re-lighting is the lack of a quantitative evaluation of the effec-tiveness of scene relighting in augmenting vision datasets.
In the absence of such a metric, the real-world applicability and usefulness of any scene relighting methodology cannot be established. To address this, in Section 4, we perform im-age relighting-based data augmentation experiments with a state-of-the-art object detection and tracking network, Cen-terTrack [64]. Section 4.1 provides a detailed overview of our experiment setup. We train three different CenterTrack models on: (i) original KITTI tracking dataset with 21 real-world sequences captured at noon; (ii) augmented KITTI with MVR-I relit sequences; and (iii) augmented KITTI with SIMBAR relit sequences. All models are tested on Vir-tual KITTI (vKITTI) [17], which consists of clones of real
KITTI sequences in a variety of lighting conditions. Sec-tion 4.2 shows that CenterTrack models augmented with re-lit KITTI images (from either MVR-I or SIMBAR) consis-tently outperform the baseline CenterTrack model. Specif-ically, the CenterTrack model trained on KITTI augmented with SIMBAR achieves the highest Multiple Object Track-ing Accuracy (MOTA) of 93.3% - a 9.0% relative improve-ment over the baseline MOTA of 85.6%. This model also achieves the highest Multiple Object Detection Accuracy (MODA) of 94.1% - again an impressive 8.9% relative im-provement over the baseline MODA of 86.4%.
To summarize, the main contributions of this paper are: 1. A novel single-view image-based scene relighting pipeline, called SIMBAR, that offers lighting control-lability without the need for multi-perspective images. 2. Single image-based geometry estimation via adapting dense prediction transformer monodepth model and better representation of far-away background objects. 3. An improved version of MVR [42], called MVR-I, with fewer artifacts and smoother surfaces in the gen-erated mesh for road driving scenes with limited views, resulting in more realistic relit images. 4. Qualitative evaluation and comparison of scene re-lighting results using MVR, MVR-I and SIMBAR, on multiple automated driving datasets, such as KITTI
[18] and BDD100K [59]. 5. Quantitative evaluation of the effectiveness of aug-menting the popular KITTI 2D tracking dataset using
SIMBAR and MVR-I for simultaneous object detec-tion and tracking using CenterTrack. 2.