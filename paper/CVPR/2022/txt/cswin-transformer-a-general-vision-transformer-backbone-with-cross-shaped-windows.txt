Abstract 1.

Introduction
We present CSWin Transformer, an efﬁcient and effec-tive Transformer-based backbone for general-purpose vision tasks. A challenging issue in Transformer design is that global self-attention is very expensive to compute whereas local self-attention often limits the ﬁeld of interactions of each token. To address this issue, we develop the Cross-Shaped Window self-attention mechanism for computing self-attention in the horizontal and vertical stripes in parallel that form a cross-shaped window, with each stripe obtained by splitting the input feature into stripes of equal width. We provide a mathematical analysis of the effect of the stripe width and vary the stripe width for different layers of the
Transformer network which achieves strong modeling capa-bility while limiting the computation cost. We also introduce
Locally-enhanced Positional Encoding (LePE), which han-dles the local positional information better than existing encoding schemes. LePE naturally supports arbitrary input resolutions, and is thus especially effective and friendly for downstream tasks. Incorporated with these designs and a hi-erarchical structure, CSWin Transformer demonstrates com-petitive performance on common vision tasks. Speciﬁcally, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra training data or label, 53.9 box AP and 46.4 mask
AP on the COCO detection task, and 52.2 mIOU on the
ADE20K semantic segmentation task, surpassing previous state-of-the-art Swin Transformer backbone by +1.2, +2.0,
+1.4, and +2.0 respectively under the similar FLOPs setting.
By further pretraining on the larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K and high segmentation performance on ADE20K with 55.7 mIoU. 1
*Work done during an internship at Microsoft Research Asia.
†Dongdong Chen is the corresponding author. 1Code and pretrain model is available at https://github.com/ microsoft/CSWin-Transformer
Transformer-based architectures [12, 30, 42, 49] have re-cently achieved competitive performances compared to their
CNN counterparts in various vision tasks. By leveraging the multi-head self-attention mechanism, these vision Trans-formers demonstrate a high capability in modeling the long-range dependencies, which is especially helpful for handling high-resolution inputs in downstream tasks, e.g., object de-tection and segmentation. Despite the success, the Trans-former architecture with full-attention mechanism [12] is computationally inefﬁcient.
To improve the efﬁciency, one typical way is to limit the attention region of each token from full-attention to lo-cal/windowed attention [30, 44]. To bridge the connection between windows, researchers further proposed halo and shift operations to exchange information through nearby win-dows. However, the receptive ﬁeld is enlarged quite slowly and it requires stacking a great number of blocks to achieve global self-attention. A sufﬁciently large receptive ﬁeld is crucial to the performance especially for the downstream tasks(e.g., object detection and segmentation). Therefore it is important to achieve large receptive ﬁled efﬁciently while keeping the computation cost low.
In this paper, we present the Cross-Shaped Window (CSWin) self-attention, which is illustrated in Figure 1 and compared with existing self-attention mechanisms. With
CSWin self-attention, we perform the self-attention calcu-lation in the horizontal and vertical stripes in parallel, with each stripe obtained by splitting the input feature into stripes of equal width. This stripe width is an important parameter of the cross-shaped window because it allows us to achieve strong modelling capability while limiting the computation cost. Speciﬁcally, we adjust the stripe width according to the depth of the network: small widths for shallow layers and larger widths for deep layers. A larger stripe width encour-ages a stronger connection between long-range elements and
Split Head (cid:1871)(cid:1875) (cid:1860)(cid:3012)(cid:512)(cid:2870) (cid:1860)(cid:2869) (cid:1871)(cid:1875) (cid:1860)(cid:3012) (cid:1860)(cid:3012)(cid:512)(cid:2870)(cid:2878)(cid:2869)
Concat
Full Attention (cid:1860)(cid:3012) (cid:1860)(cid:2869) (cid:1860)(cid:3012) (cid:1860)(cid:2869)
Slide Local
Next
Block (cid:1860)(cid:3012) (cid:1860)(cid:2869)
Shifted Local
Next
Block (cid:1860)(cid:3012) (cid:1860)(cid:3012) (cid:1860)(cid:2869) (cid:1860)(cid:3012) (cid:1860)(cid:2869) (cid:1860)(cid:3012) (cid:1860)(cid:2869) (cid:1860)(cid:3012)
Dynaic Stripe Window + Parallel Grouing Heads = CSWin
Criss-Cross
Local + Global (cid:1860)(cid:2869) (cid:1860)(cid:2869)
Sequential Axial
Figure 1. Illustration of different self-attention mechanisms, our CSWin is fundamentally different from two aspects. First, we split multi-heads ({h1, . . . , hK }) into two groups and perform self-attention in horizontal and vertical stripes simultaneously. Second, we adjust the stripe width according to the depth network, which can achieve better trade-off between computation cost and capability achieves better network capacity with a small increase in computation cost. We will provide a mathematical analysis of how the stripe width affects the modeling capability and computation cost.
It is worthwhile to note that with CSWin self-attention mechanism, the self-attention in horizontal and vertical stripes are calculated in parallel. We split the multi-heads into parallel groups and apply different self-attention op-erations onto different groups. This parallel strategy intro-duces no extra computation cost while enlarging the area for computing self-attention within each Transformer block.
This strategy is fundamentally different from existing self-attention mechanisms [18, 30, 45, 56] that apply the same attention operation across multi-heads((Figure 1 b,c,d,e), and perform different attention operations sequentially(Figure 1 c,e). We will show through ablation analysis that this differ-ence makes CSWin self-attention much more effective for general vision tasks.
Based on the CSWin self-attention mechanism, we fol-low the hierarchical design and propose a new vision
Transformer architecture named “CSWin Transformer” for general-purpose vision tasks. This architecture provides signiﬁcantly stronger modeling power while limiting compu-tation cost. To further enhance this vision Transformer, we introduce an effective positional encoding, Locally-enhanced
Positional Encoding (LePE), which is especially effective and friendly for input varying downstream tasks such as ob-ject detection and segmentation. Compared with previous positional encoding methods [9, 35, 45], our LePE imposes the positional information within each Transformer block and directly operates on the attention results instead of the attention calculation. The LePE makes CSWin Transformer more effective and friendly for the downstream tasks.
As a general vision Transformer backbone, the CSWin
Transformer demonstrates strong performance on image clas-siﬁcation, object detection and semantic segmentation tasks.
Under the similar FLOPs and model size, CSWin Trans-former variants signiﬁcantly outperforms previous state-of-the-art (SOTA) vision Transformers. For example, our base variant CSWin-B achieves 85.4% Top-1 accuracy on
ImageNet-1K without any extra training data or label, 53.9 box AP and 46.4 mask AP on the COCO detection task, 51.7 mIOU on the ADE20K semantic segmentation task, surpass-ing previous state-of-the-art Swin Transformer counterpart by +1.2, +2.0, 1.4 and +2.0 respectively. Under a smaller
FLOPs setting, our tiny variant CSWin-T even shows larger performance gains, i.e.,, +1.4 point on ImageNet classiﬁca-tion, +3.0 box AP, +2.0 mask AP on COCO detection and
+4.6 on ADE20K segmentation. Furthermore, when pretrain-ing CSWin Transformer on the larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K and high segmentation performance on ADE20K with 55.7 mIoU. 2.