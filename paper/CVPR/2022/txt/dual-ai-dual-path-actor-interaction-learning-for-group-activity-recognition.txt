Abstract
Learning spatial-temporal relation among multiple ac-tors is crucial for group activity recognition. Different group activities often show the diversiﬁed interactions be-tween actors in the video. Hence, it is often difﬁcult to model complex group activities from a single view of spatial-temporal actor evolution. To tackle this problem, we propose a distinct Dual-path Actor Interaction (Dual-AI) framework, which ﬂexibly arranges spatial and tempo-ral transformers in two complementary orders, enhancing actor relations by integrating merits from different spatio-temporal paths. Moreover, we introduce a novel Multi-scale
Actor Contrastive Loss (MAC-Loss) between two interac-tive paths of Dual-AI. Via self-supervised actor consistency in both frame and video levels, MAC-Loss can effectively distinguish individual actor representations to reduce ac-tion confusion among different actors. Consequently, our
Dual-AI can boost group activity recognition by fusing such discriminative features of different actors. To evaluate the proposed approach, we conduct extensive experiments on the widely used benchmarks, including Volleyball [21], Col-lective Activity [11], and NBA datasets [49]. The proposed
Dual-AI achieves state-of-the-art performance on all these datasets. It is worth noting the proposed Dual-AI with 50% training data outperforms a number of recent approaches with 100% training data. This conﬁrms the generalization power of Dual-AI for group activity recognition, even under the challenging scenarios of limited supervision. 1.

Introduction
Group Activity Recognition (GAR) is an important prob-lem in video understanding. In this task, we should not only
∗ Equal contribution. † Corresponding author.
Figure 1. Accuracy per Category and Example of left spike and right set group activity. Red dashed line and Violet dashed line be-low show spatial and temporal actor interaction respectively. With spatial and temporal modeling applied in different orders, ST path and TS path learn different spatiotemporal patterns and thereby are skilled at different classes, supported by the accuracy plot. recognize individual action of each actor but also under-stand collective activity of multiple involved actors. Hence, it is vital to learn spatio-temporal actor relations for GAR.
Several attempts have been proposed to model actor re-lations by building visual attention among actors [6, 16, 19, 26,46,49,51]. However, it is often difﬁcult for joint spatial-temporal optimization [8, 37]. For this reason, the recent approaches in group activity recognition often decompose spatial-temporal attention separately for modeling actor in-teraction [16, 26, 49]. But single order of space and time is insufﬁcient to describe complex group activities, due to the fact that different group activities often exhibit diversiﬁed spatio-temporal interactions.
For example, Fig. 1 (a) refers to the l-spike activity in the volleyball, where the hitting player (actor 1) and the de-fending player (actor 4) move fast to hit and block the ball, while other accompanying players (e.g., actor 2 and actor 3) stand without much movement. Hence, for this group ac-tivity, it is better to ﬁrst understand temporal dynamics of each actor, and then reason spatial interaction among actors in the scene. On the contrary, Fig. 1 (b) refers to the r-set activity in the volleyball, where most players in the right-side team are moving cooperatively to tackle the ball falling on different positions, e.g., actor 1 jumps and sets the ball, while actor 2 jumps together to make a fake spiking action.
Hence, for this group activity, it is better to reason spatial actor interaction ﬁrst to understand the action scene, and then model temporal evolutions of each actor. In fact, as shown in the accuracy plot of Fig. 1, the order of space and time interaction varies for different activity categories.
Based on these observations, we propose a distinct
Dual-path Actor Interaction (Dual-AI) framework for GAR, which can effectively integrate two complementary spa-tiotemporal views to learn complex actor relations in videos. Speciﬁcally, Dual-AI consists of Spatial-Temporal (ST) and Temporal-Spatial (TS) Interaction Paths, with as-sistance of spatial and temporal transformers. ST path ﬁrst takes spatial transformer to capture spatial relation among actors in each frame, and then utilizes temporal transformer to model temporal evolution of each actor over frames. Al-ternatively, TS path arranges spatial and temporal trans-formers in a reverse order to describe complementary pat-In this case, our Dual-AI can tern of actor interaction. comprehensively leverage both paths to generate robust spa-tiotemporal contexts for boosting GAR.
Furthermore, we introduce a novel Multi-scale Actor
Contrastive Loss (MAC-Loss), which is a concise but ef-fective self-supervised signal to enhance actor consistency between two paths. Via such actor supervision in all the frame-frame, frame-video, video-video levels, we can fur-ther reduce action confusion between any two individual actors to improve the discriminative power of actor repre-sentations in GAR.
Finally, we conduct extensive experiments on the widely-used benchmarks to evaluate our designs. Our Dual-AI simply achieves state-of-the-art performance on all the fully-annotated datasets, such as Volleyball, Collective Ac-tivity. More interestingly, our Dual-AI with 50% training data is competitive to a number of recent approaches with 100% training data in Volleyball as shown in Fig. 2, which clearly demonstrates the generalization power of our Dual-AI. Motivated by this, we further investigate the challeng-ing setting with limited actor supervision [49], where Dual-AI also achieves SOTA results on Weak-Volleyball-M and
NBA datasets. All these results show that our Dual AI is ef-fective for learning spatiotemporal actor relations in GAR.
Figure 2. Accuracy comparison with data in different percent-age on Volleyball dataset. Our method achieves SOTA perfor-mance, and achieves 94.2% with 50% data, which is competitive to a number of recent approaches [16, 30, 46] trained with 100% data. Solid point means result with additional optical ﬂow input. 2.