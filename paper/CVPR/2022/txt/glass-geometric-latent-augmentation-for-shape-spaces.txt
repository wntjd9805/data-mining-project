Abstract 1.

Introduction
We investigate the problem of training generative mod-els on very sparse collections of 3D models. Particularly, instead of using difﬁcult-to-obtain large sets of 3D models, we demonstrate that geometrically-motivated energy func-tions can be used to effectively augment and boost only a sparse collection of example (training) models. Technically, we analyze the Hessian of the as-rigid-as-possible (ARAP) energy to adaptively sample from and project to the under-lying (local) shape space, and use the augmented dataset to train a variational autoencoder (VAE). We iterate the process, of building latent spaces of VAE and augmenting the associated dataset, to progressively reveal a richer and more expressive generative space for creating geometrically and semantically valid samples. We evaluate our method against a set of strong baselines, provide ablation studies, and demonstrate application towards establishing shape correspondences. GLASS produces multiple interesting and meaningful shape variations even when starting from as few as 3-10 training shapes. Our code is available at https:
//sanjeevmk.github.io/glass_webpage/.
This paper is concerned with generating plausible defor-mations of a 3D shape from a very sparse set of examples.
Fig. 1 shows an input of 10 human 3D meshes in differ-ent poses, and the additional deformations generated by our method. 3D deformations have a strong semantic element to them – e.g., a human’s limbs should only bend at the joints, and then, under normal circumstances, not beyond certain angular ranges. Arguably, this can only be deduced in general via learning by example from a dataset.
Unfortunately, in contrast to 2D images, the 3D domain poses several challenges for data-driven frameworks. Prob-ably the most signiﬁcant one is that data acquisition is com-plex and tedious, making datasets both scarcer and sparser.
Given this data paucity, we tackle the challenge of gen-erating additional meaningful deformations from a given (very) sparse set of landmark deformations. Our method meaningfully augments the sparse sets to create larger datasets that, in turn, can be leveraged by other techniques that cannot operate on sparse datasets.
Producing plausible deformations from a few landmarks is difﬁcult. Linearly interpolating the vertices of two land-marks yields highly implausible intermediates. A key in-sight is that while meaningful deformations are semantic, they often have a very strong pure-geometric element, e.g., they are smooth (i.e., preserve local details) and don’t dis-tort the shape too much (i.e., local distances are preserved).
However, simply perturbing vertices while minimizing a ge-ometric energy (e.g., smoothness or metric distortion) gen-erates artifacts such as smooth global bending or surface ripples because by itself, the energy is not a sufﬁcient con-straint. Interpolating landmark pairs, while preserving the energy, fares better but produces limited variations [1, 25].
Our paper, like other recent approaches [12, 20], advocates learning a low-dimensional generative latent space which maps out the underlying manifold jointly deﬁned by the landmarks, while simultaneously minimizing a deformation energy. However, these prior methods still require a large dataset to learn a rich set of variations.
Our core contribution is to address this difﬁculty with a novel data augmentation approach that alternates between latent space training and energy-guided exploration. We employ supervised learning of a generative space from a training dataset, a very sparse one, but augment that set in an unsupervised, geometry-aware way. Speciﬁcally, we train a Variational Autoencoder (VAE) on the given dataset.
After training, we use the eigenmodes of a deformation en-ergy’s Hessian to perturb and project latent codes of train-ing shapes in a way that ensures they yield smooth, low-distortion deformations, which we add back as data aug-mentation to the input set. We then re-train the VAE on the augmented dataset and repeat the process iteratively until the space has been densely sampled. In addition to reduc-ing spurious deformations, the use of a low-dimensional, jointly-trained latent space allows low-energy perturbations of one landmark to be inﬂuenced by other landmarks, yield-ing richer variations. We call our method GLASS.
We evaluate GLASS on several established datasets and compare performance against baselines using multiple met-rics. The experiments show the effectiveness of GLASS to recover meaningful additional deformations from a mere handful of exemplars. We also evaluate the method in the context of shape correspondence, demonstrating that our sampling process can be used as a data augmentation tech-nique to improve existing strongly-supervised correspon-dence algorithms (e.g., 3D-CODED [18]). 2.