Abstract
Adapting pre-trained models with broad capabilities has become standard practice for learning a wide range of downstream tasks. The typical approach of ﬁne-tuning dif-ferent models for each task is performant, but incurs a sub-stantial memory cost. To efﬁciently learn multiple down-stream tasks we introduce Task Adaptive Parameter Shar-ing (TAPS), a simple method for tuning a base model to a new task by adaptively modifying a small, task-speciﬁc sub-set of layers. This enables multi-task learning while min-imizing the resources used and avoids catastrophic forget-ting and competition between tasks. TAPS solves a joint optimization problem which determines both the layers that are shared with the base model and the value of the task-speciﬁc weights. Further, a sparsity penalty on the num-ber of active layers promotes weight sharing with the base model. Compared to other methods, TAPS retains a high accuracy on the target tasks while still introducing only a small number of task-speciﬁc parameters. Moreover, TAPS is agnostic to the particular architecture used and requires only minor changes to the training scheme. We evaluate our method on a suite of ﬁne-tuning tasks and architectures (ResNet,DenseNet,ViT) and show that it achieves state-of-the-art performance while being simple to implement. 1.

Introduction
Real-world applications of deep learning frequently require performing multiple tasks (Multi-Task Learn-ing/MTL). To avoid competition between tasks, a simple solution is to train separate models starting from a common pre-trained model. Although this approach results in capa-ble task-speciﬁc models, the training, inference, and mem-ory cost associated grows quickly with the number of tasks.
Further, tasks are learned independently, missing the oppor-tunity to share when tasks are related.
*Work done during an internship at AWS AI Labs.
†Corresponding Author.
Ideally, one would train a single model to solve all tasks simultaneously. A common approach is to ﬁx a base model and add task-speciﬁc parameters (e.g., adding branches, classiﬁers) which are trained separately for each task. How-ever, deciding where to branch or add parameters is non-trivial since the optimal choice depends on both the ini-tial model and the downstream task, to the point that some methods train a secondary network to make these decisions.
Moreover, adding weights (layers, parameters, etc.) to a network independent of the task is also not ideal: some methods [18, 23, 33] add a small ﬁxed number of learn-able task-speciﬁc parameters, however, they sacriﬁce per-formance when the downstream task is dissimilar from the pre-training task. Other methods perform well on more dif-ﬁcult tasks but add an unnecessary number of parameters for simpler tasks [9,39,48], hindering the learning of a large numbers of tasks.
In this work, we overcome these issues by introduc-ing Task Adaptive Parameter Sharing (TAPS). Rather than modifying the architecture of the network or adding a ﬁxed set of parameters, TAPS adaptively selects a minimal subset of the existing layers and retrains them. At ﬁrst sight, select-ing the best subset of layers to adapt is a complex combi-natorial problem which requires an extensive search among 2L different conﬁgurations, where L is the number of lay-ers. The key idea of TAPS is to relax the layer selection to a continuous problem, so that deciding which layers of the base model to specialize into task-speciﬁc layers can be done during training by solving a joint optimization using stochastic gradient descent.
The ﬁnal result is a smaller subset of task-speciﬁc pa-rameters (the selected layers) which replace the base layers.
Our approach has several advantages: (i) It can be applied to any architecture and does not need to modify it by in-troducing task-speciﬁc branches; (ii) TAPS does not reduce the accuracy of the target task (compared to the paragon of full ﬁne-tuning) while introducing fewer task-speciﬁc pa-rameters; (iii) The decision of which layers to specialize is interpretable, done with a simple optimization procedure, and does not require learning a policy network; (iv) It can
T1 Classiﬁer
T1 Classiﬁer
T1 Classiﬁer
T2 Classiﬁer
T2 Classiﬁer
T2 Classiﬁer
T1 Classiﬁer
T1 Classiﬁer
T1 Classiﬁer
T2 Classiﬁer
T2 Classiﬁer
T2 Classiﬁer
T1 Classiﬁer
T1 Classiﬁer
T1 Classiﬁer
T2 Classiﬁer
T2 Classiﬁer
T2 Classiﬁer
Layer 3
Layer 3
Layer 3
Layer 3
Layer 3
Layer 3
Layer 2
Layer 2
Layer 2 (T1) Layer 2 (T1) Layer 2 (T1) Layer 2
Layer 2
Layer 2
Layer 2
Layer 3
Layer 3
Layer 3
Layer 3
Layer 3
Layer 3
Layer 2
Layer 2
Layer 2
Layer 2
Layer 2
Layer 2
Layer 1
Layer 1
Layer 1
Layer 1
Layer 1
Layer 1 (T2) Layer 1 (T2) Layer 1 (T2) Layer 1
Layer 1
Layer 1
Layer 1
Layer 1
Layer 1
Layer 1 (a) Feature Extractor (b) Our Method (c) Fine-Tuning
Figure 1. Overview of our approach. Difference between (b) our approach, (a) feature extractor as well as (c) ﬁnetuning. Here we have
Our Method
Our Method two tasks T1 and T2. T1 shown by turquoise and T2 by green. Yellow boxes denote the base network layer. Our approach adds task speciﬁc parameters to different layers based on the target tasks. Notice that this is in contrast to (a) no task speciﬁc layers are used and (c) where every layer is tasks speciﬁc. Also (c) suffers from catastrophic forgetting, and lose the base network’s parameters unlike (b) and (a).
Fixed Feature Extractor
Fixed Feature Extractor
Fixed Feature Extractor
Our Method
Fine-Tuning
Fine-Tuning
Fine-Tuning be implemented in a few lines, and requires minimal change to the training scheme.
Our method ﬁnds both intuitive sharing strategies, and other less intuitive but effective ones. For example, on
ResNet models TAPS tends to modify only the last few layers while for ViT models TAPS discovers a signiﬁcantly different sharing pattern, learning to share the feed-forward layers and only adapting the self-attention layers.
We test our method on standard benchmarks and show that it outperforms several alternative methods. Moreover, we show that the results of our method are in-line with the standard ﬁne-tuning practices used in the community. The contributions of the paper can be summarized as follows: 1. We propose TAPS, a method for differentiably learn-ing which layers to tune when adapting a pre-trained network to a target task. This could range from adapt-ing or specializing an entire model, to only changing 0.1% of the pre-trained model, depending on the com-plexity/similarity of the new task. 2. We show that TAPS can optimize for accuracy or efﬁ-ciency and performs on par with other methods. More-over, it automatically discovers effective architecture-speciﬁc sharing patterns as opposed to hand-crafted weight or layer sharing schemes. 3. TAPS enables efﬁcient incremental and joint multi-task learning without competition or forgetting. 2.