Abstract
Traditional object detectors are ill-equipped for incre-mental learning. However, fine-tuning directly on a well-trained detection model with only new data will lead to catastrophic forgetting. Knowledge distillation is a flexi-ble way to mitigate catastrophic forgetting. In Incremen-tal Object Detection (IOD), previous work mainly focuses on distilling for the combination of features and responses.
However, they under-explore the information that contains in responses. In this paper, we propose a response-based incremental distillation method, dubbed Elastic Response
Distillation (ERD), which focuses on elastically learning responses from the classification head and the regression head. Firstly, our method transfers category knowledge while equipping student detector with the ability to retain
In localization information during incremental learning. addition, we further evaluate the quality of all locations and provide valuable responses by the Elastic Response Selec-tion (ERS) strategy. Finally, we elucidate that the knowl-edge from different responses should be assigned with dif-ferent importance during incremental distillation. Exten-sive experiments conducted on MS COCO demonstrate our method achieves state-of-the-art result, which substantially narrows the performance gap towards full training. Code is available at https://github.com/Hi-FT/ERD. 1.

Introduction
In the natural world, the visual system of creatures could constantly acquire, integrate and optimize knowl-edge. Learning mode is inherently incremental for them.
In contrast, currently, the classic training paradigm of ob-ject detection models [19,33] does not have such capability.
Supervised object detection paradigm relies on accessing pre-defined labeled data. This learning paradigm implic-itly assumes data distribution is fixed or stationary [9, 37], while data from real world is represented by continuous
*Corresponding author.
Figure 1. The effect of various responses for IOD. and dynamic data flow, whose distribution is non-stationary.
When the model continuously obtains knowledge from non-stationary distribution, new knowledge would interfere with the old one, triggering catastrophic forgetting [11, 26].
Based on whether the task identity is provided or must be inferred [34], researchers divide Incremental Learning (IL) into three types: task/domain/class IL. In this paper, we fo-cus on the most intractable scenario for object detection: class incremental object detection.
A flexible way to solve IOD is knowledge distillation
[14]. [28] stressed that the Tower layers could reduce catas-trophic forgetting significantly. They implemented incre-mental learning on an anchor-free detector and selectively performed distillation on non-regression outputs. Mean-while, in knowledge distillation for object detection where incremental learning was not introduced, previous work ex-tracted knowledge from the combined distillation of differ-ent components. For example, [5] and [32] distilled all com-ponents of the detector. Nevertheless, the nature of these methods are designed using feature-based knowledge dis-tillation [6], response-based method [12] has not been ex-plored in IOD [25] yet. Besides, the advantage of response-based method is that it provides the reasoning information
[14, 27] of the teacher detector. Therefore, an elaborate de-sign for different responses is essential [23].
This paper focuses on a practical and challenging prob-lem concerning IOD: how to learn response from classifica-tion predictions and bounding boxes. Responses in object
detection contain logits together with the offset of bound-ing box [12]. Firstly, since the number of ground truth on each new image is uncertain, one of the foremost considera-tions is to validate the response of all samples, determining which response is positive or negative and which response each object should regress towards. Furthermore, as shown in Figure 1, we find that not all responses are important to prevent catastrophic forgetting, thus an appropriate number of response nodes is ideal. [16] also proposed that synap-tic consolidation achieves continuous learning by reducing synaptic plasticity critical to previous learning tasks. To sum up, we guide the student detector following the behav-ior of teacher on the old objects by constraining important responses to stay close to their old values.
To tackle the above problems, this paper rethinks response-based knowledge distillation method, finding that distillation at proper locations is crucial for facilitating IOD.
Driven by this inspiration, we proposed an Elastic Response
Distillation (ERD) scheme that elastically learns responses from classification head and regression head respectively.
Unlike previous work, we introduce incremental localiza-tion distillation [38] in regression response to equip student detector with the ability to learn location ambiguity [20] during incremental learning. Besides, we propose Elastic
Response Selection (ERS) strategy to automatically select distillation nodes based on statistical characteristics from different responses, which evaluates the qualities of all lo-cations and provides valuable responses. In this paper, we explain how we implement the constraint, and finally how we determine which responses are important. We greatly alleviate catastrophic forgetting problem and significantly narrow the gap with full training. Extensive experiments on the MS COCO dataset support our analysis and conclusion.
The our contributions can be summarized as follows, (i) To the best of our knowledge, this paper is the first work to explore the response-based distillation method in
IOD and dissect the essential differences between feature-based and response-based solutions for IOD. (ii) We pro-pose ERD based on statistical analysis, which separately distills selective classification and regression responses us-(iii) Extensive experi-ing the proposed ERS strategy. ments on MS COCO demonstrate that the proposed method achieves state-of-the-art performance and can be easily ex-tended to different detectors. 2.