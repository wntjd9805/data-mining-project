Abstract
In autonomous driving, LiDAR point-clouds and RGB images are two major data modalities with complementary cues for 3D object detection. However, it is quite difﬁcult to sufﬁciently use them, due to large inter-modal discrepan-cies. To address this issue, we propose a novel framework, namely Contrastively Augmented Transformer for multi-modal 3D object Detection (CAT-Det). Speciﬁcally, CAT-Det adopts a two-stream structure consisting of a Point-former (PT) branch, an Imageformer (IT) branch along with a Cross-Modal Transformer (CMT) module. PT, IT and CMT jointly encode intra-modal and inter-modal long-range contexts for representing an object, thus fully explor-ing multi-modal information for detection. Furthermore, we propose an effective One-way Multi-modal Data Augmenta-tion (OMDA) approach via hierarchical contrastive learn-ing at both the point and object levels, signiﬁcantly improv-ing the accuracy only by augmenting point-clouds, which is free from complex generation of paired samples of the two modalities. Extensive experiments on the KITTI benchmark show that CAT-Det achieves a new state-of-the-art, high-lighting its effectiveness. 1.

Introduction 3D object detection is a fundamental step in autonomous
It mainly operates 3D point-driving perception systems. clouds acquired by LiDAR sensors and provides important spatial clues including location, direction, and object size.
Despite true and accurate geometry information recorded, the distribution of point-clouds is disordered, irregular, and sparse, making 3D object detection a challenging task.
The past few years have witnessed the fast development in 3D object detection. A large number of methods are in-troduced in the literature, and according to the input form in detection feature learning, the methods are roughly catego-*indicates the corresponding author.
Figure 1. Illustration of the fusion process in CAT-Det. (A): A failure case due to few points at a long distance in the point-cloud modality. (A’): Its corresponding case in the image modality. Al-though the feature of (A) is enhanced by those of (B) and (C) in the PT branch, this is often insufﬁcient. With the CMT module, the feature of (A) is further enhanced by that of (A’) which also integrates the contributions from (D’) and (F’) in the IT branch, and accurate detection is ﬁnally achieved. rized into grid-based and point-based. The former initially converts point-clouds into regular grids by projecting them to images of speciﬁc views [11, 22] or subdividing them to voxels in the space [19, 43, 50, 60] and further conducts 2D or 3D Convolutional Neural Networks (CNN) to en-code geometric cues. The latter directly takes raw points and applies point-cloud deep learning networks e.g. Point-Net [34]/PointNet++ [35] or graph neural networks e.g.
DGCNN [45] to capture shape structures [38, 40, 51, 56].
More recently, several attempts [15, 37, 52] deliver stronger models by integrating point-based and grid-based networks as hybrid representations, reporting better results.
To boost the performance of 3D object detection, another
strategy targets on multi-modal solutions, which makes use of 3D point-clouds along with 2D images. Although images have not yet proved so competent as an independent modal-ity for this issue evidenced by inferior baselines [2,5,21,30], the combination of geometric and textural clues conveyed in point-clouds and images does lead to accuracy gains for their natural complementarity [7, 18, 33, 41, 53]. F-PointNet [33] and F-ConvNet [46] perform the fusion in se-ries, where 3D frustum proposals are ﬁrstly cropped based on prepared 2D regions through a standard 2D CNN detec-tor and each point within the proposal is then segmented and screened using a PointNet-like block for regression.
By contrast, more studies fulﬁll this task in parallel. For example, [41, 48] conduct data-level fusion by enhancing 3D coordinates with point-wise 2D segmentation features;
[7, 17, 18, 22, 23] achieve feature-level fusion of 2D and 3D representations from individual networks by simple con-catenation or speciﬁc modules; and [32] implements box-level fusion which merges the individual candidate sets of a couple of 2D and 3D detectors in a learning manner. Differ-ent from the LiDAR only methods that continuously update for more sophisticatedly designed models and more suit-able training schemes in the single point-cloud modality, the multi-modal alternatives endeavour to leverage more di-verse information and suggest great potential. However, as the KITTI [12] leaderboard displays, there still exists a cer-tain gap between the multi-modal methods and the top Li-DAR only ones [59].
Such a gap is due to three aspects. (1) In multi-modal 3D object detection, PointNet++ [35]/3D sparse convolu-tions [50] and 2D CNNs are principal building blocks to extract point-cloud and image features respectively. Lim-ited by their local receptive ﬁelds, contexts cannot be com-prehensively acquired from both the modalities, triggering information loss. (2) The widely adopted fusion schemes, particularly the ones at the feature-level, such as direct con-catenation [7, 18], additional convolution [22, 23], and sim-ple attention [17, 53], assign no weights or coarse weights learned within limited receptive ﬁelds to different features, where crucial clues are not well highlighted. (3) Ground-truth data augmentation [50] is a common practice to fa-cilitate LiDAR only methods; unfortunately, it is not so straightforward to apply this mechanism to multi-modal methods as augmentation in the single modality tends to cause semantic misalignment. [42] indeed presents a cross-modal augmentation technique for paired data, but the pro-cedure on images is cumbersome and easy to incur noise.
To address the issues mentioned above, this paper pro-poses a novel framework for multi-modal 3D object de-tection, namely Contrastively Augmented Transformer De-It adopts a two-stream structure, con-tector (CAT-Det). sisting of a Pointformer (PT) branch, an Imageformer (IT) branch together with a Cross-Modal Transformer (CMT) module. Unlike PointNet++ and CNNs, both the PT and
IT branches possess large receptive ﬁelds, which are able to respectively capture rich global context information in point-clouds and images to strengthen features of hard sam-ples. Subsequently, the CMT module conducts cross-modal feature interaction and multi-modal feature combination, where essential cues extracted in the two modalities are suf-ﬁciently emphasized with holistically learned ﬁne-grained weights. The integration of PT, IT, and CMT fully encodes intra-modal and inter-modal long-range dependencies as a powerful representation, thus beneﬁting detection perfor-mance. In addition, we propose a one-way multi-modal data augmentation (OMDA) approach through hierarchical con-trastive learning, which accomplishes effective augmenta-tion by solely performing on the point-cloud modality.
In summary, the major contributions of this paper are: (1) We propose a novel CAT-Det framework for multi-modal 3D object detection, with a pointformer branch, an imageformer branch and a cross-modal transformer mod-ule. To the best of our knowledge, it is the ﬁrst attempt that applies the transformer structure to the given task. (2)
We propose a one-way data augmentation approach for multi-modal 3D object detection via hierarchical contrastive learning, signiﬁcantly improving the accuracy only by aug-menting point-clouds, thus free from complex generation of paired samples of the two modalities. (3) We achieve a newly state-of-the-art mAP of all the three classes on the
KITTI test set in comparison to the published counterparts, and demonstrate its advantage in detecting hard objects. 2.