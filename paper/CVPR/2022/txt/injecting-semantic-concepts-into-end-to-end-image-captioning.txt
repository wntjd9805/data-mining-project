Abstract
Tremendous progresses have been made in recent years in developing better image captioning models, yet most of them rely on a separate object detector to extract regional features. Recent vision-language studies are shifting towards the detector-free trend by leveraging grid representations for more ﬂexible model training and faster inference speed.
However, such development is primarily focused on image understanding tasks, and remains less investigated for the caption generation task. In this paper, we are concerned with a better-performing detector-free image captioning model, and propose a pure vision transformer-based image caption-ing model, dubbed as ViTCAP, in which grid representations are used without extracting the regional features. For im-proved performance, we introduce a novel Concept Token
Network (CTN) to predict the semantic concepts and then incorporate them into the end-to-end captioning. In particu-lar, the CTN is built on the basis of a vision transformer, and is designed to predict the concept tokens through a classiﬁ-cation task, from which the rich semantic information con-tained greatly beneﬁts the captioning task. Compared with the previous detector-based models, ViTCAP drastically sim-pliﬁes the architectures and at the same time achieves com-petitive performance on various challenging image caption-ing datasets. In particular, ViTCAP reaches 138.1 CIDEr scores on COCO-caption Karpathy-split, 93.8 and 108.6
CIDEr scores on nocaps and Google-CC captioning datasets, respectively. 1.

Introduction
The task of image captioning aims to generate human-readable descriptive text from an image. Recent studies have witnessed its great development which are primarily reﬂected in the aspects of more advanced cross-modal fusion architectures [11, 45, 50, 54, 57, 63, 65, 67, 70]; more expres-sive object-centric features [4, 69] & tags [16, 23, 35, 58] obtained from a pre-trained object detection model; or learn-Figure 1. Comparisons of different image captioning models.
Top: A general image captioning pipeline. Bottom: (a). Prevailing conventional models [23, 36, 69] which are based on an object detector to extract regional features. Object tags [35, 69] can be optionally used to assist the text generation through a multi-modal decoder network. This usually requires regional operations (REG.
OPE.) that are time consuming. (b). To eliminate the detection module, a ResNet variant [20] or Vision Transformer [30] can be applied as substitution to output the grid feature [61, 62]. This replacement has been studied on the image understanding task recently but very few works focus on the generation task. (c). Our proposed ViTCAP, which is detector-free and incorporates a novel
Concept Token Network to predict semantic concepts as tokens for the image captioning task. ing general Vision and Language (VL) representations from large image-text corpus [16, 35, 58, 61, 62, 71].
Despite these signiﬁcant advances, most of the main-stream captioning models [11, 45, 54, 70] rely heavily on a bulky object detector to provide regional visual representa-tions for the multimodal interaction, as shown in Figure 1-a.
In spite of the superior performance brought by the object features, the ensuing difﬁculties occur as they: 1) lead to heavy computational load due to the regional operations (i.e., RPN, RoI Pooling, and NMS). These intermediate op-erations unavoidably cause training inefﬁciency and high inference latency at prediction stage [30, 58]; 2) require
box annotations and largely limit the ﬂexibility in training and application. To address these challenges, there is an emerging trend that more recent works propose to elimi-nate the detector for the VL pre-training in an end-to-end fashion [25, 26, 30, 61, 64]. In such detector-free design, a general visual encoder serves as a substitute for the detec-tor and from which the grid features are produced for later cross-modal fusion, as in Figure 1-b. Heretofore, the major-ity of these works mainly focus on the image understand-ing task, which is typically cast as a classiﬁcation problem, and only a few of them shed light on the generation task.
In [62], the image is encoded with ResNet [20] and the per-formance (117.3 CIDEr on COCO [62]) is still far from the state-of-the-art detector-based approach (129.3 CIDEr with
VinVL-base [69]). The challenge remains uncharted and insufﬁciently investigated regarding how to build a stronger detector-free image captioning model.
Previous efforts [16, 23, 35, 58, 69] have demonstrated that the object tags play an important role in improving the captioning performance. Instead of gleaning the object tags from the detector, we introduce a novel fully VIsion
Transformer based image CAPtioning model, dubbed ViT-CAP, with a lightweight Concept Token Network (CTN) that produces concept tokens (see Figure 1-c). ViTCAP is con-structed on the basis of a vision transformer [13] as the stem image encoder. Our vision transformer backbone starts with encoding the image and produces grid features, on top of which the CTN branch is then applied to predict semantic concepts of images. We represent the semantic concepts at the token level instead of the tag level to avoid the tokeniza-tion. The multi-modal module then takes the input of both grid representations and Top-K concept tokens for decoding.
During training, the CTN is optimized to predict the pseudo ground-truth concepts extracted from image captions via a simple classiﬁcation task. We also investigate to adopt the object tags from the detector as the pseudo ground-truth, and empirically observe no further improvement. Overall, this straight-forward design allows the injection of semantic concepts into the multi-modal fusion module with abundant semantics, and is critical for the improved captioning perfor-mance.
Our ablative analysis suggests that, with no bells and whistles, simple vanilla transformer architecture based ViT-CAP 1) signiﬁcantly outperforms existing detector-free cap-tioning models; 2) surpasses most detector-based models and 3) approaches the state-of-the-art detector-based models. In particular, ViTCAP achieves 138.1 CIDEr scores on COCO-caption Karpathy split [38], 108.6 on Google-CC [52], and 95.4 on nocaps [1] datasets.
To summarize our contributions:
• We present a detector-free image captioning model ViT-CAP with fully transformer architecture, where it leverages grid representations without regional operations.
• We propose to inject semantic concepts into end-to-end captioning by learning from open-form captions. We ﬁnd that our proposed concept classiﬁcation training and con-cept tokens signiﬁcantly beneﬁt the captioning task.
• Extensive evaluations on multiple captioning datasets con-ﬁrm the validity of our method. ViTCAP achieves compet-itive or even leading results amongst detector-based prior arts with clear inference-time advantages. 2.