Abstract
Motion style transfer is a common method for enriching character animation. Motion style transfer algorithms are often designed for offline settings where motions are pro-cessed in segments. However, for online animation applica-tions, such as real-time avatar animation from motion cap-ture, motions need to be processed as a stream with mini-mal latency. In this work, we realize a flexible, high-quality motion style transfer method for this setting. We propose a novel style transfer model, Style-ERD, to stylize motions in an online manner with an Encoder-Recurrent-Decoder structure, along with a novel discriminator that combines feature attention and temporal attention. Our method styl-izes motions into multiple target styles with a unified model.
Although our method targets online settings, it outperforms previous offline methods in motion realism and style expres-siveness and provides significant gains in runtime efficiency. 1.

Introduction
Animators commonly seek to create stylized motions to express the characters’ personalities or emotions, thus mak-ing characters more lifelike. Since many computer anima-tion techniques are based on motion capture data, the vari-ety and diversity of the motion data play an essential role in the quality of the resulting animation. However, a capture-everything approach scales poorly if there is a need to cap-ture every style, e.g., childlike or depressed, for every mo-tion type. Hence, animators usually capture motion in a neutral style and then stylize them by hand, which is again laborious. This motivates automated methods for stylizing existing motions according to desired target-style labels.
In this work, we develop a novel motion style transfer framework capable of stylizing streaming input motion data for online applications, which we define as Online Motion
Style Transfer. As shown in Fig. 1, current motion style transfer methods [1, 5, 17, 19, 38, 54] with deep learning models require a motion segment as input, and produce a transferred motion segment as output, and where the input (a) Offline motion style transfer. (b) Online motion style transfer.
Figure 1. a) Offline motion style transfer processes motion seg-ments, while b) online motion style transfer processes motions in a stream. segment has a minimum duration of 1 s. While these meth-ods make significant progress on a difficult problem, with a subset being described as being real-time [47, 57], they still suffer from startup latency caused by waiting for the multiple frames required as input. For online motion style transfer, only the current frame is processed by the model, which enables the direct processing of the stream of motion data. We believe such transfer methods are more suitable for many novel applications requiring streaming motion data.
For example, in animating a human avatar, motion is cap-tured online to animate the virtual avatar in real-time, and the streaming motion data needs to be processed with min-imal latency. Online motion style transfer can also be eas-ily incorporated into the workflows of real-time motion sys-tems, such as games, interactive exhibitions, and augmented reality with minimal additional latency.
Motion style transfer exists as a long-standing research problem due to several difficulties, among many: (1) lack of a standardized qualitative style representation for mo-tions, (2) difficulty in handling and generating temporally correlated data, (3) a lack of temporally registered motion data in different styles. Several approaches [20, 46, 57, 60] aim to solve this problem with manually designed mod-els. However, they often fail to generalize well to large motion datasets with various styles. Researchers have de-veloped more scalable methods with the rapid progress of deep learning machinery [1, 6, 17, 36]. However, only a few of them can transfer the motion to multiple target styles with
a unified model [1, 38]. On top of the aforementioned chal-lenges, online motion style transfer poses more difficulties because style and content are ill-defined and unrecogniz-able within one frame, yielding low-quality transfer results.
Current offline motion style transfer methods are commonly conditioned on multiple input frames in order to understand the motion semantics, thus realizing better transfer at the cost of introducing non-trivial latency. Although offline methods can be adapted to online settings by padding the current frame with past frames, there is no mechanism to guarantee the continuity among output frames.
To accomplish high-quality, efficient motion style trans-fer with minimal latency, we embed knowledge regarding the previous frames in the memory of the motion transfer module in order to infer and track the style and content.
The transfer module is thus aware of the context of con-tent and style even when only presented with the current frame. We adapt the Encoder-Recurrent-Decoder (ERD) framework to the online motion style transfer task by de-signing novel recurrent residual connections to capture fea-tures for each style. We name this novel architecture as
Style-ERD. In Style-ERD, we enable each residual connec-tion to learn its own initial hidden state h0 conditioned on the style and content label. The learned hidden states are vi-tal to the responsiveness of the style transfer results. In ad-dition, to produce temporally coherent motions, we design a new discriminator with feature and temporal attention, FT-Att Discriminator, to supervise the post-transfer style. As a result, our deep learning model demonstrates a strong capa-bility to perform the desired motion style transfer efficiently and with minimal latency.
The contributions of this work are as follows: (1) We in-troduce the online motion style transfer problem and aim to stimulate future research into this area to facilitate real-time animation applications. (2) We present a novel framework,
Style-ERD, as well as a new supervision module, FT-Att
Discriminator, achieving the goal of style transferring mo-tion with minimum latency. Our style transfer framework provides a 5× reduction in compute time, as compared with the current state-of-the-art approach. (3) Our method can transfer the motion into its stylistic counterpart with high fidelity, showing better style transfer as compared to offline methods. 2.