Abstract
We present a method for learning a generative 3D model based on neural radiance ﬁelds, trained solely from data with only single views of each object. While generating re-alistic images is no longer a difﬁcult task, producing the corresponding 3D structure such that they can be rendered from different views is non-trivial. We show that, unlike ex-isting methods, one does not need multi-view data to achieve this goal. Speciﬁcally, we show that by reconstructing many images aligned to an approximate canonical pose with a single network conditioned on a shared latent space, you can learn a space of radiance ﬁelds that models shape and appearance for a class of objects. We demonstrate this by training models to reconstruct object categories using datasets that contain only one view of each subject with-out depth or geometry information. Our experiments show that we achieve state-of-the-art results in novel view synthe-sis and high-quality results for monocular depth prediction. https://lolnerf.github.io 1.

Introduction
A long-standing challenge in computer vision is the ex-traction of 3D geometric information from images of the real world [37]. Understanding 3D geometry is critical to under-standing the physical and semantic structure of objects and scenes, but achieving it remains a very challenging problem.
Work in this area has mainly focused either on deriving geo-metric understanding from more than one view [1, 25, 62], or by using known geometry to supervise the learning of geometry from single views [10, 14, 18, 43]. Here, we take a more ambitious approach and aim to derive equivalent 3D understanding in a generative model from only single views of objects, and without relying on explicit geometric information like depth or point clouds. Deriving such 3D understanding is, however, non trivial. While Neural Radi-ance Field (NeRF)-based methods [13, 44] have shown great promise in geometry-based rendering, they focus on learning a single scene from multiple views.
Existing NeRF works [16, 44, 52] all require supervision from more than one viewpoint, as without it, NeRF methods are prone to collapse to a ﬂat representation of the scene, be-cause they have no incentive to create a volumetric represen-tation; see Figure 2 (left). This serves as a major bottleneck, as multiple-view data is hard to acquire. Thus, architectures have been devised to workaround this that combine NeRF and Generative Adversarial Networks (GANs) [9, 47, 57], where the multi-view consistency is enforced through a dis-criminator to avoid the need for multi-view training data.
In this work we show that – surprisingly – having only
single views of a class of objects is enough to train NeRF models without adversarial supervision, as long as a shared generative model is trained, and approximate camera poses are provided. In a nutshell, the multi-view constraint of ex-isting works no longer necessarily needs to be enforced, and cameras do not have to be accurate to achieve compelling results; see Figure 2 (right). Speciﬁcally, we roughly align all images in the dataset to a canonical pose using predicted 2D landmarks, which is then used to determine from which view the radiance ﬁeld should be rendered to reproduce the original image. For the generative model we employ an auto-decoder framework [51]. To improve generalization, we further train two models, one for the foreground – the com-mon object class of the dataset – and one for the background, since the background is often inconsistent throughout the data, hence unlikely to be subject to the 3D-consistency bias.
We also encourage our model to model shapes as solid sur-faces (i.e. sharp outside-to-inside transitions), which further improves the quality of predicted shapes; see the improve-ments from Figure 2 (middle) to Figure 2 (right). t s e t n i a r t
A noteworthy aspect of our method is that we do not require rendering of entire images, or even patches, while training. In frame-the auto-decoder work, we train our models to reconstruct images from datasets, and at the same time ﬁnd the optimal latent repre-sentations for each image – an objective that can be enforced on individual pixels. Hence, our method can be trained with arbitrary image sizes without any increase in memory re-quirement during training. In contrast, existing methods that utilize GANs [9,47,57] supervise on inter-pixel relationships through their discriminators, greatly limiting or outright pre-venting them from being able to scale with respect to training image resolution. In summary, we: (middle)
Figure 2 (right) (left)
•
•
• propose a method for learning 3D reconstruction of ob-ject categories from single-view images which decouples training complexity from image resolution; show that single views are enough to learn high-quality prediction of geometry (e.g. depth) without any geometric supervision (Figure 3); show that our method exceeds adversarial methods in rep-resenting appearance of objects from the learned category by reconstructing held-out images and novel views. 2.