Abstract
We present 3MASSIV, a multilingual, multimodal and multi-aspect, expertly-annotated dataset of diverse short videos extracted from short-video social media platform -Moj. 3MASSIV comprises of 50k short videos (20 sec-onds average duration) and 100K unlabeled videos in 11 different languages and captures popular short video trends like pranks, fails, romance, comedy expressed via unique audio-visual formats like self-shot videos, reaction videos, lip-synching, self-sung songs, etc. 3MASSIV presents an opportunity for multimodal and multilingual semantic un-derstanding on these unique videos by annotating them for concepts, affective states, media types, and audio language.
We present a thorough analysis of 3MASSIV and highlight the variety and unique aspects of our dataset compared to other contemporary popular datasets with strong baselines.
We also show how the social media content in 3MASSIV is dynamic and temporal in nature, which can be used for se-mantic understanding tasks and cross-lingual analysis. 1.

Introduction
Semantic understanding of videos has been a well-researched problem but still continues to garner a lot of attention from the computer vision and multimedia re-search communities because videos encode rich informa-tion which can be understood across different dimensions using various tasks. Notable progress has been made in terms of analyzing these video for tasks like action clas-siﬁcation [34, 40, 64], action localization [16, 85], video de-scription [11, 75], video question answering [41, 66, 81], object and scene understanding [85], etc. The majority of these tasks are focused on recognizing visual aspects present/happening in the video, e.g., action, scene, object detection, and classiﬁcation.
Figure 1. 3MASSIV: We highlight three videos uploaded by a particular user. The concept labels for these are festival, couple ro-mance, comedy respectively. We also see the diversity in the video types, self-shot, split-screen and special effects. We also observe how the content is temporally aligned to real-word events; for in-stance, the festivals. 3MASSIV has 50K such annotated videos across 11 languages with masked user identiﬁers and timestamps for deeper semantic analysis of social media content. Faces have been blurred for preserving privacy.
Detecting these visual aspects helps in answering what occurs in a video? But, it does not capture how viewers in-terpret the video? and which concept(s) the creator of the video wishes to convey? In this work, we investigate the semantic understanding of videos uploaded on short-video social media platform - Moj 1 from the perspective of cre-ators and viewers of these videos, which has not been ex-plored before, primarily due to the lack of large-scale an-notated video datasets. Considering the rapid adoption of social media, a holistic understanding of the creation, con-*The ﬁrst two authors contributed equally to this work. 1https://mojapp.in
sumption, and popularity dynamics of these videos forms an important and timely research direction.
To facilitate this under-explored research direction, we present a novel dataset, 3MASSIV, built from short videos posted on the short-video platform - Moj. Even though existing datasets for semantic understanding source videos from social media (e.g. YouTube [1], Vine [48], Face-book [52]), they are not suitable for our task. We high-light the key challenges and elaborate on how 3MASSIV addresses them:
• Taxonomy: Prior datasets
[1, 19, 34] adopt a top-down approach of constructing a vocabulary of visual concepts from domain-independent taxonomies (e.g. freebase) and mining videos from social media using this vocabulary. However, this vocabulary is not ex-haustive and ﬁne-grained for capturing popular con-cepts in social media discourse. Moreover, this method generates ”easy videos” as search engines prioritize them ﬁrst [52]. [52] adopt uniform sampling to ad-dress this problem while we construct a comprehensive bottom-up taxonomy using popularity-based sampling of videos for bridging this gap.
• Novel video types: Existing datasets do not cap-ture novel and challenging video formats like split-screen videos, special effects (masks/graphics overlaid on faces), portrait videos, lip-syncing to pre-recorded audio, etc. (Figure 1) which are dominant on social media platforms. 3MASSIV curates the videos from a short video platform - Moj and annotates them for these media types for ﬁlling this gap.
• Video Narrative: Broadly speaking, there are three distinct kinds of videos on social media: a) Micro Nar-rative: Videos which are short in duration [48] (5-6 secs) or are clipped out from longer videos [15,46,85], b) Long Narrative: Longer videos [1, 10, 78], usually more than 1-2 minutes, which tell a more detailed nar-rative or story c) Short Narrative: These are longer than micro-videos (typically 10-20 secs) and provide authors and content creators more ﬂexibility in terms of time limits. Despite the explosive growth of short video platforms like Tiktok, Reels, Youtube Shorts, and Moj, short videos have not been explored in detail in the Computer Vision and AI communities, primar-ily because of the lack of a large-scale labeled dataset. 3MASSIV contains complete videos created with a short and concise narrative presenting an opportunity to understand this new avenue of video understanding.
• Sparse/Noisy Hashtags: Since expert annotation is expensive, large datasets often use hashtags added by the creators [48]. However, hashtags are usually sparse
- 56% of videos did not have hashtags in MV-58 [48].
Also, they can be noisy, as shown in (A.6). Our dataset, 3MASSIV, addresses this by manually anno-tating the videos using expert annotators.
• Linguistic Diversity: Existing datasets for seman-tic understanding of videos are not motivated towards exploring linguistic diversity while 3MASSIV com-prises of videos from 11 languages, annotated with the language of the audio for facilitating multilingual se-mantic understanding of videos. 3MASSIV contains concept, affective states, audio type, video type and language annotations for understand-ing the creator’s and viewer’s perspectives. We label the videos with the following annotations for modeling the viewer’s perspective:
• Concept:
Each video is annotated for a con-cept (across 34 labels) by expert annotators. Our dataset contains widely popular and unique social me-dia concepts like pranks, fails, romance, philanthropy, comedy, etc. Figure 2 shows some examples which demonstrate that understanding these videos, which are very human-centric, self-shot with a short story goes beyond detecting and classifying the audio-visual aspects and makes 3MASSIV challenging.
• Affective States: We provide annotations for 11 emo-tion categories present in these videos.
Similarly, to understand the creator’s perspective, we pro-vide annotations for media types that content creators use to convey their point. Figure 1 shows some of the examples.
• Audio Types: The audio types are unique and di-verse with recorded/self-sung songs, dialogues, mono-logues, instrumentals, etc.
Video formatting comprises of
• Video Types: slideshows, self-shot, split-screens, movie/TV-serial clips, etc. which are very popular on short video platforms. animations,
Additionally, our dataset 3MASSIV can be used for various tasks and applications, such as:
• Multilingual Modeling: We provide annotations for the 11 different languages, opening opportunities for multilingual semantic understanding.
• Creator Modeling: We also provide masked creator identiﬁers and recent videos uploaded by these cre-ators (100k videos), opening up exciting user model-ing ideas inspired by semantic video understanding.
• Temporal Analysis: Social media content has a very short life span and is very dynamic. To enhance under-standing here, we provide timestamps of these videos, which can help model temporal dynamics of the nature of popular content on such platforms. Moreover, we provide masked user proﬁles to identify videos from the same creators to analyze the shift in their perspec-tives over time.
To the best of our knowledge, 3MASSIV is the ﬁrst human-annotated large-scale dataset of short videos that can
be used for modeling concepts, affective states, and media types across 11 languages, presenting a unique opportunity for understanding social media content. Overall, 3MAS-SIV contains 900 hours of video data uploaded by 23121 creators with 50K expertly annotated videos and 100K un-labeled videos with an average duration of around 20 sec-onds. We also present baseline results to empirically estab-lish that 3MASSIV is challenging and unique in Section 4.
In Section 5, we discuss the application of 3MASSIV over various research problems. 2.