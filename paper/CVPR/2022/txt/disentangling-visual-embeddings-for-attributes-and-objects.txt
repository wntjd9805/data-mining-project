Abstract
We study the problem of compositional zero-shot learn-ing for object-attribute recognition. Prior works use visual features extracted with a backbone network, pre-trained for object classification and thus do not capture the subtly dis-tinct features associated with attributes. To overcome this challenge, these studies employ supervision from the lin-guistic space, and use pre-trained word embeddings to bet-ter separate and compose attribute-object pairs for recog-nition. Analogous to linguistic embedding space, which al-ready has unique and agnostic embeddings for object and attribute, we shift the focus back to the visual space and pro-pose a novel architecture that can disentangle attribute and object features in the visual space. We use visual decom-posed features to hallucinate embeddings that are represen-tative for the seen and novel compositions to better regular-ize the learning of our model. Extensive experiments show that our method outperforms existing work with significant margin on three datasets: MIT-States, UT-Zappos, and a new benchmark created based on VAW. The code, mod-els, and dataset splits are publicly available at https:
//github.com/nirat1606/OADis. 1.

Introduction
Objects in the real world can appear with different prop-erties, i.e., different color, shape, material, etc. For instance, an apple can be red or green, cut or peeled, raw or ripe, and even dirty or clean. Understanding object properties can greatly benefit various applications, e.g., robust object de-tection [5, 14, 15, 26], human object interaction [7, 49, 51], and activity recognition [1, 3, 4, 16, 18, 34]. Since the total number of possible attribute-object pairs in the real world is prohibitively large, it is impractical to collect image ex-amples and train multiple classifiers. Prior works proposed compositional learning, i.e., learning to compose knowl-edge of known attributes and object concepts to recognize a new attribute-object composition. Datasets such as MIT-States [24] and UT-Zappos [56] are commonly used to study this task, with joint attribute-object recognition for a di-verse, yet limited set of objects and attributes.
Figure 1. Method illustration: Given an input image I of peeled apple, we use two other images: (1) one with same ob-ject, different attribute Iobj - sliced apple, (2) one with same attribute, different object Iattr - peeled orange. We propose a novel architecture that takes I and Iattr, and extracts their visual similarity features for peeled and visual dissimilarity features for orange. Similarly, using I and Iobj, the visual similarity features for apple, and the dissimilarity features for sliced can be extracted. We compose these primitive visual features to hallucinate a seen pair peeled apple, and a novel unseen pair sliced orange to be used for regularizing our embedding space. Note that this is a visualization of embedding space com-position, we do not generate images.
Compositional learning refers to combining simple primitive concepts to understand a complex concept. This idea dates back to Recognition and Composition theory by
Biederman [6], and early work in the visual domain by
Hoffman [22], which proposed recognition by parts for pose estimation. Prior works explore compositionality to a cer-tain degree, e.g., via feature sharing and shared embeddings space. Among them, most works use linguistically inspired losses to separate attributes and objects in the shared em-bedding space, then use that primitive knowledge to com-pose new complex pairs. Using linguistic embeddings is helpful since: (1) there is a clear distinction between at-tribute and object in the embedding space, and (2) these embeddings already contain semantic knowledge of simi-lar objects and attributes, which is helpful for composition.
However, unlike word embedding, it is difficult to discrimi-nate the object and attribute in the visual embedding space.
This is due to the fact that image feature extractor is usu-ally pre-trained for object classification, often along with image augmentation (e.g., color jitter) that tends to produce attribute-invariant image representation, thus does not learn objects and attributes separately. In this paper, we propose a new direction that focuses on visual cues, instead of using linguistic cues explicitly for novel compositions.
Analogous to linguistic embedding, our work focuses on disentangling attribute and object in the visual space. Our method, Object Attribute Disentanglement (OADis), learns distinct and independent visual embeddings for peeled and apple from the visual feature of peeled apple. As shown in Figure 1, for image I of peeled apple, we use two other images: one with same object and different attribute Iobj (e.g., sliced apple), and one with same attribute and different object Iattr (e.g., peeled orange).
OADis takes I and Iobj and learns the similarity (apple) and dissimilarity (sliced) of the second image with re-spect to the first one. Similarly, using I and Iattr, the com-monality between them (peeled) and the left out dissim-ilarity (orange) can also be extracted. Further, composi-tion of these extracted visual primitives are used to halluci-nate seen and unseen pair, peeled apple and sliced orange respectively.
For compositional learning, it is necessary to decom-pose first before composing new unseen attribute-object pairs. As humans, we have the ability to imagine an unseen complex concept using previous knowledge of its primitive concepts. For example, if someone has seen a clown and a unicycle, they can imagine clown on a unicycle even if they have never seen this combina-tion in real life [20, 43]. This quality of imagination is the basis of various works such as GANs [12], CLIP [47] and DALL-E [48]. However, these works rely on larger datasets and high computation power for training. We study this idea of imagination for a smaller setup by compos-ing newer complex concepts using disentangled attributes and object visual features. Our work focuses on answer-ing the question, can there be visual embedding of peeled and apple, disentangled separately from visual feature of peeled apple? Our contributions are as follows:
• We propose a novel approach, OADis, to disentangle at-tribute and object visual features, where visual embed-ding for peeled is distinct and independent of embed-ding for apple.
• We compose unseen pairs in the visual space using the disentangled features. Following Compositional Zero-shot Learning (CZSL) setup, we show competitive im-provement over prior works on standard datasets [24, 56].
• We propose a new large-scale benchmark for CZSL us-ing an existing attribute dataset VAW [45], and show that
OADis outperforms existing baselines. 2.