Abstract
CounterFactual (CF) visual explanations try to find im-ages similar to the query image that change the decision of a vision system to a specified outcome. Existing meth-ods either require inference-time optimization or joint train-ing with a generative adversarial model which makes them time-consuming and difficult to use in practice. We propose a novel approach, Cycle-Consistent Counterfactuals by La-tent Transformations (C3LT), which learns a latent trans-formation that automatically generates visual CFs by steer-ing in the latent space of generative models. Our method uses cycle consistency between the query and CF latent rep-resentations which helps our training to find better solu-tions. C3LT can be easily plugged into any state-of-the-art pretrained generative network. This enables our method to generate high-quality and interpretable CF images at high resolution such as those in ImageNet. In addition to several established metrics for evaluating CF explanations, we in-troduce a novel metric tailored to assess the quality of the generated CF examples and validate the effectiveness of our method on an extensive set of experiments. 1.

Introduction
With convolutional neural networks (CNNs) revolution-izing the space of automatic visual recognition, there have been many approaches attempting to better explain the in-ner workings of CNNs, including attribution maps [21, 41], concept-based explanations [9, 36], rule-based explanations
[8], prototype-based explanations [5], etc. However, when presented to humans, those kinds of explanations were not necessarily easy to process. Recently, a substantial user study [16] shows that Grad-CAM [41], LIME superpix-els [36], etc., were not as informative to humans as simple nearest neighbors from the training set.
Those findings suggest that humans prefer to see exam-ples that are just similar to the natural images rather than heatmaps, superpixels, etc. and counterfactual (CF) expla-nations [7, 11, 28, 30, 45, 46] might be more useful in help-ing humans to understand deep networks. CF explanations show humans examples that are similar to the explanation subject but deep networks predict them as a different cate-gory. Such explanations have also been advocated by social scientists [27, 45] as a preferred mode of explanation.
This paper mainly deals with CF explanations in the vi-sual domain. CF explanation in the visual domain is more difficult to generate than categorical inputs where one can simply search for adversarial examples [28, 30, 45]. Meth-ods that directly optimize for perturbations in the input space [7] often lead to adversarial solutions [43], which ma-nipulate CNN predictions with imperceptible changes. Ad-versarial examples are usually off the data manifold, where
CNNs are fooled because they do not generalize to the kinds of data that have never been seen in training. In addition, finding and replacing patches of features from images in the CF class to a query one [11] also moves the images off the natural image manifold by creating irregular edges.
Successful CF explanations usually avoid being adver-sarial by staying on the same data manifold the network has been trained on. Hence, prior work usually utilizes a gener-ative model such as a generative adversarial network (GAN) or variational autoencoder (VAE) that ensures the generated
CF example lies on the data manifold. For example, Ex-plainGAN [39] jointly trains a GAN for each category along with a mask generator that generates a masked region from the latent code of the image, so that after the masked region is transformed the image would be classified as another cat-egory by the CNN. Some other algorithms [37] optimize for latent codes of a VAE model that will generate an image similar to the original one yet classified as another category.
Despite these prior work, it remains difficult to apply
CF explanations in practice. One can consider two realis-tic use cases for explanation algorithms. The first is de-bugging, where users attempt to check why CNN is mak-ing a certain wrong classification. The second is knowl-edge gathering, where users may try to utilize explanations
In to understand subtle differences between two classes. both cases, it would be beneficial for the user to quickly churn through many examples to help building their mental
model. Even better, the user may want to make some re-alistic edits (e.g. based on GANs)) to the image and then obtain a new CF out of the edited image. In those cases, it would be ideal if CF images can be generated on-the-fly.
However, most previous approaches solve an optimization for each image [7,11,23,26,30,38], which often makes gen-erating CF examples time-consuming.
In this paper, we propose a novel approach that optimizes for a nonlinear transformation in the latent space. The trans-formation morphs the latent code of an input image into a
CF latent vector that can be decoded into an image which looks similar to the original one, but has semantically mean-ingful, perceptible differences so that CNN classifies it as another category. Different from [39], our approach does not require joint training with GANs. It utilizes a pretrained generative model (GAN/VAE) hence can easily adapt to current and future generative algorithms that are being pro-posed every day due to significant ongoing research. As an example, this enables our framework to go beyond sim-ple datasets and generate high-resolution images, e.g. Ima-geNet, with the current GAN algorithms available now. We further adopt a cycle-consistency loss function [50] that im-proves the consistency and performance of our approach.
Furthermore, we evaluate our approach comprehensively in a quantitative manner. For CF explanations, literature has suggested certain properties to be desirable [29, 44]:
I VALIDITY. The model should assign the CF examples x′ to the CF class c′ in order to be valid.
II PROXIMITY. The CF examples x′ should stay as close as possible (in terms of some distance function) to the original query instance x.
III SPARSITY. Minimal number of query features should be perturbed in order to generate CF examples.
IV REALISM. The CF examples should lie close to the data manifold so that it appears realistic.
V SPEED. The CF explanations should be generated in interactive speed in order to be deployed in real-world applications.
For example, adversarial examples may be valid CFs, but they fail in terms of realism. We propose to use a set of metrics that comprehensively measure all these aspects, in-cluding a novel metric that inspects the quality of the CF examples across a series of changes.
Below we list our contributions in this paper:
• We introduce a novel framework to generate realistic
CFs at high resolution by learning a transformation in the latent space of a pretrained generative model.
• We propose a set of novel quantitative evaluation met-rics tailored for counterfactual explanations.
• Extensive qualitative and quantitative evaluations show the effectiveness of our method and its capabil-ity to generate high-resolution CF images by plugging into existing generative algorithms. 2.