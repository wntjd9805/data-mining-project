Abstract
The transferability of adversarial examples allows the deception on black-box models, and transfer-based targeted attacks have attracted a lot of interest due to their prac-tical applicability. To maximize the transfer success rate, adversarial examples should avoid overfitting to the source model, and image augmentation is one of the primary ap-proaches for this. However, prior works utilize simple im-age transformations such as resizing, which limits input di-versity. To tackle this limitation, we propose the object-based diverse input (ODI) method that draws an adver-sarial image on a 3D object and induces the rendered im-age to be classified as the target class. Our motivation comes from the humans’ superior perception of an image printed on a 3D object. If the image is clear enough, hu-mans can recognize the image content in a variety of view-ing conditions. Likewise, if an adversarial example looks like the target class to the model, the model should also classify the rendered image of the 3D object as the tar-get class. The ODI method effectively diversifies the in-put by leveraging an ensemble of multiple source objects and randomizing viewing conditions. In our experimental results on the ImageNet-Compatible dataset, this method boosts the average targeted attack success rate from 28.3% to 47.0% compared to the state-of-the-art methods. We also demonstrate the applicability of the ODI method to adver-sarial examples on the face verification task and its supe-rior performance improvement. Our code is available at https://github.com/dreamflake/ODI. 1.

Introduction
Deep learning models have demonstrated outstanding performance in a variety of fields and have permeated our daily lives [6, 10, 13]. However, adversarial examples show that these models are vulnerable to maliciously crafted small input perturbations [7, 35]. Interestingly, an adversar-ial example that is generated to attack a network is likely to disturb other networks as well. This intriguing property of
Figure 1. Illustrations of our motivation. If a targeted adver-sarial example really looks like the target class to the model, the model should also classify the adversarial examples projected on the 3D objects as the target class.
Figure 2. The framework of targeted adversarial attacks with the proposed object-based diverse input (ODI) method. Please note that the ODI method exploits 3D adversarial objects in the generating phase only. It finally improves the transferability of 2D adversarial examples. adversarial examples is known as transferability [21,26,37].
This property allows an adversary to attack a black-box tar-get model without knowing its interior.
On black-box models, targeted attacks are significantly more challenging compared to non-targeted attacks which simply induce the victim models to malfunction without specifying a target class [21, 43]. Targeted attacks de-mand further exploration since they can cause more serious problems by deceiving models into predicting a designated harmful target class. Research on these transfer-based tar-geted attacks is important since it can help service providers prepare their models against these potential threats and as-sess the robustness of their models.
The transfer success rates greatly vary depending on the difference between the source and target models. Various approaches have been presented to improve the transferabil-ity, such as introducing momentum [7,22] and different loss functions [21, 43] for better optimization, input data aug-mentation [41, 44], and utilizing an ensemble of multiple source models [23].
Among focus their strategies, we and on input these limitations. transformation-based methods
These methods create adversarial examples that are robust against image transforms such as random resizing [41, 44] and translation [8] to prevent overfitting to the source model. However, since these methods use simple image transformations, they limit the diversity of input.
Our motivation for tackling this limitation comes from the humans’ superior perception of an image printed on a 3D object (e.g., promotional merchandise commonly dis-tributed at event booths). As a 2D image is projected on a 3D object, the original image is bent, the color looks differ-ent due to illumination, and some parts of the image are in-visible depending on the viewpoint. Nevertheless, if the im-age is clear enough, humans can recognize the image con-tent on the 3D object in a variety of viewing conditions.
Likewise, if an adversarial example really looks like the tar-get class to the source model, the model should also recog-nize the target class in the image printed on 3D objects. Our motivation is illustrated in Fig. 1.
From this motivation, we propose the object-based di-verse input (ODI) method for boosting the transferability of targeted adversarial examples. Specifically, we introduce 3D objects and project an adversarial example on the ob-jects’ surfaces. Then, we induce the rendered objects to be classified as the target class in a variety of rendering envi-ronments, including different lighting and viewpoints. This realistic input diversification can generalize the attack abil-ity and improve the transferability of the adversarial exam-ple. The overall scheme is illustrated in Fig. 2.
Our contributions can be listed as follows.
• We propose the object-based diverse input (ODI) method to enhance the transferability of targeted ad-versarial examples. To our knowledge, this is the first time that 3D objects are used as canvases for 2D ad-versarial examples during their optimizations.
• We discovered that the attack success rate varies de-pending on the 3D object (e.g., a pillow and a cup).
Our experimental results also indicate that an ensemble of carefully chosen source objects can further improve transferability.
• In the experimental results with four source models and ten target models on the ImageNet dataset, the proposed ODI method boosts the average targeted at-tack success rate from 28.3% to 47.0% compared to the combination of state-of-the-art methods.
• We also demonstrate the applicability of the ODI method to adversarial examples on the face verification task and its overwhelming performance improvement. 2.