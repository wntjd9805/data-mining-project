Abstract 1.

Introduction
The CLIP network measures the similarity between nat-ural text and images; in this work, we investigate the entan-glement of the representation of word images and natural images in its image encoder. First, we find that the image encoder has an ability to match word images with natural images of scenes described by those words. This is consis-tent with previous research that suggests that the meaning and the spelling of a word might be entangled deep within the network. On the other hand, we also find that CLIP has a strong ability to match nonsense words, suggesting that processing of letters is separated from processing of their meaning. To explicitly determine whether the spelling ca-pability of CLIP is separable, we devise a procedure for identifying representation subspaces that selectively isolate or eliminate spelling capabilities. We benchmark our meth-ods against a range of retrieval tasks, and we also test them by measuring the appearance of text in CLIP-guided gener-ated images. We find that our methods are able to cleanly separate spelling capabilities of CLIP from the visual pro-cessing of natural images.
The distinction between written words and visual objects is crystal clear for us: we would never confuse an object with a written word describing that object. However, it has been shown [9] that attaching a white sheet of paper with
ªiPadº written on it to an apple, will cause a neural net-work to shift its prediction to lean towards what is written instead of recognizing the fruit. We hypothesize that the network learns to confuse text with objects because of the prevalence of text in real-world training data: text on prod-ucts, signs, and labels is often visible next to the thing it represents (Figure 2), which is perhaps why a neural net-work would struggle to distinguish an object from its writ-ten name. Beginning with a pretrained network that exhibits this text/object confusion, we ask if the perception of text by a network can be separated from the perception of objects.
We study the representations of the CLIP [20] network, which is trained to measure the similarity between natu-ral text and images, and which has been shown to be vul-nerable to confusion between written text and visual con-cepts [9,16]. In [9], feature visualizations of neurons within
CLIP revealed the presence of ªmulti-modal neuronsº that
a contrastive loss to identify a large representation subspace for information about visual words. Rather than measuring classification accuracy, we verify our findings by applying the probed model to generate images. Concurrent work [16] applies cognitive science tools and finds evidence that the vision and language do not share semantic representation in
CLIP network, consistent with our findings.
Controllable GAN Generation Increasingly powerful image GAN models have sparked interest in steerable im-age generation methods that synthesize an image by guid-ing the generator towards some objective: GAN output can be steered by directly guiding generation towards target im-ages [12]; or by optimizing loss of a classifier [8, 23]; or
PCA, clustering or other methods can also be used to di-rectly identify meaningful representation subspaces for ma-nipulating a GAN [3, 11, 24]. The release of CLIP [20], a large-scale model to score text-and-image similarity has unleashed a wave of creativity, because it enables any gen-erative model to be guided by open text. The state-of-the-art DALL-E [21] uses CLIP; and CLIP has also been com-bined with StyleGAN [2, 14, 19], BigGAN [18], and VQ-GAN [4±6]. Like these methods, we investigate the ability of CLIP to steer VQGAN, however instead of generating in-dividual images, we ask whether the broad ability of CLIP to read and draw visual words can be controlled. 3. Terminology
To avoid confusion while discussing words within im-ages, we begin by defining some terminology.
Kinds of images:
• image text:
± synthetic image text : an image of text rendered on a white background
± image text in the wild: text on a signboard found in a photograph of a real scene
• natural images: images depicting the real world
• natural image with text: natural image is modified by adding rendered text
• natural image with word class label: natural image with text, where the text is a class name
Kinds of text:
• text class label: the text name of a class category, composed by prepending a string ªan image of aº to the name
• text string: a word as processed by a text encoder; this could be either a real English word or a fake nonsense string, composed of random letters 4. Visual comprehension
Does the image encoder of CLIP encode image text dif-ferently from the way it encodes the visual concept de-scribed by that same text?
Figure 2. Top row: examples of written text in natural images, bot-tom row: generated images conditioned on words ("peas", "stop sign", "hall", "bar", "snickers"). activate when presented with different forms of the same concept; for example, the same neuron will activate on an image of a written word and an image of the object de-scribed by that word. In addition to this, we have found that text-to-image generation methods that use CLIP will spell out the word they have been conditioned on (Figure 1). To-gether, these findings indicate a deeply rooted correlation between written words and their visual concepts in the im-age encoder of CLIP.
In this paper, we investigate how CLIP makes sense of written words, and whether CLIP distinguishes its un-derstanding of written words from their visual meaning.
Specifically, we investigate whether the image encoding permits separation of information about written words from the visual concepts described by those words. We find that a simple setup and an orthogonal projection can in fact sep-arate the two capabilities. We demonstrate applications of this disentanglement by removing text artifacts in text-to-image generation, and by defending against typographic at-tacks. We collect a dataset of 180 images of 20 objects and 8 attacks and measure the confusion between the true object labels and typographic attacks between the CLIP model and our disentangled representation. We find that in both dis-tinct applications, the effect of text is greatly reduced. 2.