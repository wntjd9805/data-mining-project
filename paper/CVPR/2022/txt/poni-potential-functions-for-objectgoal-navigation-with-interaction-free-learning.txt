Abstract
State-of-the-art approaches to ObjectGoal navigation (ObjectNav) rely on reinforcement learning and typically require significant computational resources and time for learning. We propose Potential functions for ObjectGoal
Navigation with Interaction-free learning (PONI), a modu-lar approach that disentangles the skills of ‘where to look?’ for an object and ‘how to navigate to (x, y)?’. Our key in-sight is that ‘where to look?’ can be treated purely as a perception problem, and learned without environment in-teractions. To address this, we propose a network that pre-dicts two complementary potential functions conditioned on a semantic map and uses them to decide where to look for an unseen object. We train the potential function network using supervised learning on a passive dataset of top-down semantic maps, and integrate it into a modular framework to perform ObjectNav. Experiments on Gibson and Mat-terport3D demonstrate that our method achieves the state-of-the-art for ObjectNav while incurring up to 1,600× less computational cost for training. Code and pre-trained mod-els are available.1 1.

Introduction
Embodied visual navigation is the computer vision prob-lem where an agent uses visual sensing to actively interact with the world and perform navigation tasks [2, 3, 6, 51].
We have witnessed substantial progress in embodied visual navigation over the past decade, fueled by the availability of large-scale photorealistic 3D scene datasets [10, 45, 60] and fast simulators for embodied navigation [33, 51, 60].
ObjectNav has gained popularity in recent years [2, 6, 50]. Here, an agent enters a novel and unmapped 3D scene, and is given an object category to navigate to (e.g., a chair).
To successfully solve the task, the agent needs to efficiently navigate to the object and stop near it within a given time budget. This is a fundamental problem for embodied agents which requires semantic reasoning about the world (e.g.,
TV is in the living room, oven is in the kitchen, and chairs are near tables), and it serves as a precursor to more com-plex object manipulation tasks [33, 55].
Prior work has made good progress on this task by formulating it as a reinforcement learning (RL) problem and developing useful representations [21, 62], auxiliary tasks [63], data augmentation techniques [37], and im-proved reward functions [37]. Despite this progress, end-to-end RL incurs high computational cost, has poor sample ef-ficiency, and tends to generalize poorly to new scenes [8,13, 37] since skills like moving without collisions, exploration, and stopping near the object are all learned from scratch purely using RL. Modular navigation methods aim to ad-dress these issues by disentangling ‘where to look for an ob-ject?’ and ‘how to navigate to (x, y)?’ [13,36]. These meth-ods have emerged as strong competitors to end-to-end RL with good sample efficiency, better generalization to new scenes, and simulation to real-world transfer [12,13]. How-ever, since ‘where to look?’ is formulated as an RL prob-lem with interactive reward-based learning, these methods require extensive computational resources (∼8 GPUs) over multiple days for training.
We hypothesize that the ‘where to look?’ question for an unseen object is fundamentally a perception problem, and can be learned without any interactions. Based on this in-sight, we introduce a simple-yet-effective method for Ob-jectNav – Potential functions for ObjectGoal Navigation with Interaction-free learning (PONI). A potential function is a 0-1 valued function defined at the frontiers of a 2D top-down semantic map2, i.e., the map locations that lie on the edges of the explored and unexplored regions (see Fig. 1, right). It represents the value of visiting each location in order to find the object (higher the value, the better). Given the potential function, we can decide ‘where to look?’ by simply selecting the maximum potential location.
We propose the potential function network, a convolu-tional encoder-decoder model that estimates the potential function from a partially filled semantic map. Critically, we propose to train it interaction-free using a dataset of 1Website: https://vision.cs.utexas.edu/projects/poni/ 2The 2D semantic map contains the object category per map location.
Figure 1. Potential functions for ObjectGoal Navigation with Interaction-free learning (PONI). We introduce a method to decide
‘where to look?’ for an unseen object in indoor 3D environments. Our key insight is that this is fundamentally a perception problem that can be solved without any interactive learning. We address this problem by defining a potential function, which is [0, 1]-valued function that represents the value of visiting each location in order to find the object. Given the potential function, we can simply select its argmax location to decide where to look for the object. We propose a potential function network that uses geometric and semantic cues from a partially filled semantic map to predict the potential function for the object of interest (e.g., a shower). We train this network in a non-interactive fashion on a dataset of semantic maps, and integrate it into a modular framework for performing ObjectNav. top-down semantic maps obtained from 3D semantic an-notations [10, 60] (see Fig. 1, center). This is unlike prior work on RL which interactively learns ObjectNav policies by designing reward functions using the same semantic an-notations [13,37,63]. Specifically, our network predicts two complementary potential functions by leveraging geometric and semantic cues in the semantic map (e.g., the environ-ment layout, room-object, and object-object relationships).
The area potential function captures the unexplored areas in the map for efficient exploration, while the object potential function is a geodesic-distance based function that helps de-cide how to reach the object. Once trained, we deploy the potential function network in a modular framework for Ob-jectNav, where we combine the area and object potential predictions to decide where to look for a goal object.
We perform experiments on the photorealistic 3D envi-ronments of Gibson [60] and Matterport3D [10]. Our pro-posed method outperforms a state-of-the-art modular RL method [13] on Gibson with 7× lower training cost, and a state-of-the-art end-to-end RL method [37] on MP3D with 1,600× lower training cost. Our method sets the state-of-the-art on the Habitat ObjectNav challenge leaderboard [6] when compared to previously published methods. 2.