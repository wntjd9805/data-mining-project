Abstract
Blind face restoration is to recover a high-quality face image from unknown degradations. As face image contains abundant contextual information, we propose a method,
RestoreFormer, which explores fully-spatial attentions to model contextual information and surpasses existing works that use local operators. RestoreFormer has several ben-efits compared to prior arts. First, unlike the conven-tional multi-head self-attention in previous Vision Trans-formers (ViTs), RestoreFormer incorporates a multi-head cross-attention layer to learn fully-spatial interactions be-tween corrupted queries and high-quality key-value pairs.
Second, the key-value pairs in ResotreFormer are sam-pled from a reconstruction-oriented high-quality dictionary, whose elements are rich in high-quality facial features specifically aimed for face reconstruction, leading to su-*This work is supported by the General Research Fund of HK
No.27208720 and 17212120. perior restoration results. Third, RestoreFormer outper-forms advanced state-of-the-art methods on one synthetic dataset and three real-world datasets, as well as produces images with better visual quality. Code is available at https://github.com/wzhouxiff/RestoreFormer.git. 1.

Introduction
Blind face restoration aims at restoring a high-quality face from a degraded one that has suffered from com-plex and diverse degradations, such as down-sampling, blur, noise, compression artifact, etc. Since the degradations are unknown in the real world, restoration is a challenging task.
Although there are some works [3, 18, 39] tending to re-store high-quality face only based on the information in the degraded one, most of the existing works have demonstrated that priors play a critical role in blind face restoration.
These priors include geometric priors [5,7,21,32,41,42,46], references [10, 24, 26, 27], and generative priors [14, 29, 35, 37]. Geometric priors can be landmarks [7, 21], facial pars-ing maps [5, 32], or facial component heatmaps [41]. They are considered to be helpful to reconstruct the facial struc-ture. However, since most of them are estimated from the corrupted faces, their performance is restricted by the qual-ity of the corrupted inputs. Reference priors are from high-quality exemplars [10,26,27] or facial component dictionar-ies [24]. Whereas, the high-resolution exemplars with the same identity of the degraded image are not always acces-sible and the existing dictionaries-based methods only con-sider facial components, e.g. eyes, mouth, and nose. Gener-ative priors encapsulated in a well-trained high-quality face generator are also adopted in blind face restoration. By ex-ploring an appropriate latent vector from the latent space of a generator [14, 29] or straightly projecting the degraded face into the latent space [35, 37], their generators are pos-sible to generate a high-quality face with realness.
In these prior-based works, there are two sources of in-formation: the degraded face with identity information and the priors with high-quality facial details. For restoring faces with realness and fidelity, it is important to fuse these two kinds of information. Most of the existing arts simply combine them by concatenation [10, 26,27]. Also, there ex-ist works [5, 24, 37] proposing to fuse these two kinds of in-formation by Spatial Feature Transformer (SFT) [38].How-ever, SFT fuses the information pixel-wisely which neglects the abundant facial context and ends up with sub-optimal restored results. Therefore, we propose a RestoreFormer, which aims for exploring fully-spatial attentions to glob-ally model contextual information and finally transforms the feature from the degraded face into another one close to the ground-truth face feature according to its correspond-ing high-quality facial priors. Different from existing ViTs works [4, 6, 11, 47] that tend to implement fully-spatial at-tentions with multi-head self-attention, our RestoreFormer proposes a multi-head cross-attention layer. Specifically, it takes the features of a corrupted face as queries while their key-value pairs are from high-quality facial priors. By glob-ally and spatially incorporating the corrupted facial features with their corresponding high-quality priors, the proposed method can simultaneously restore a face with realness and fidelity.
Besides, the high-quality dictionary (denoted as HQ Dic-tionary) proposed in this paper is a reconstruction-oriented one. It is learned from plenty of undegraded faces by a high-quality face generation network motivated by the idea of vector quantization [30]. Therefore, it is rich in high-quality facial details that are learned for face restoration. Compared to the previous Component Dictionaries proposed by Li et al. [24], whose elements are features of face components generated from amounts of high-quality faces with an off-line approach, our HQ Dictionary has two advantages: (1)
HQ Dictionary owns rich and diverse details specifically aimed for high-quality face reconstruction, while the pri-ors generated with an off-line recognition-oriented model, (2) HQ such as VGG [33], may not have such abilities.
Dictionary involves all the areas of a face while the Com-ponent Dictionaries [24] only provide priors for eyes, nose, and mouth which restrict the ability for face restoration.
In conclusion, our main contributions are as follows:
• We propose a RestoreFormer to learn fully-spatial in-teractions between corrupted queries and high-quality key-value pairs which can attain a high-quality face with realness and fidelity from a degraded face.
• We learn a new HQ Dictionary as priors in Restore-Its reconstruction-oriented property plays a
Former. critical role in face restoration.
• Extensive experiments show that our RestoreFormer outperforms advanced state-of-the-art methods on both synthetic and real-world datasets, as well as restores faces with better visual quality. 2.