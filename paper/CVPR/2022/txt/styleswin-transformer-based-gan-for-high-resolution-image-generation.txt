Abstract
Despite the tantalizing success in a broad of vision tasks, transformers have not yet demonstrated on-par ability as
ConvNets in high-resolution image generative modeling. In this paper, we seek to explore using pure transformers to build a generative adversarial network for high-resolution image synthesis. To this end, we believe that local atten-tion is crucial to strike the balance between computational efficiency and modeling capacity. Hence, the proposed gen-erator adopts Swin transformer in a style-based architec-ture. To achieve a larger receptive field, we propose double attention which simultaneously leverages the context of the local and the shifted windows, leading to improved gener-ation quality. Moreover, we show that offering the knowl-edge of the absolute position that has been lost in window-based transformers greatly benefits the generation quality.
The proposed StyleSwin is scalable to high resolutions, with both the coarse geometry and fine structures benefit from the strong expressivity of transformers. However, block-ing artifacts occur during high-resolution synthesis because performing the local attention in a block-wise manner may break the spatial coherency. To solve this, we empirically investigate various solutions, among which we find that em-ploying a wavelet discriminator to examine the spectral discrepancy effectively suppresses the artifacts. Extensive experiments show the superiority over prior transformer-based GANs, especially on high resolutions, e.g., 1024 ×
*Author did this work during his internship at Microsoft Research Asia.
†Corresponding author. 1024. The StyleSwin, without complex training strategies, excels over StyleGAN on CelebA-HQ 1024, and achieves on-par performance on FFHQ-1024, proving the promise of using transformers for high-resolution image genera-tion. The code and pretrained models are available at https://github.com/microsoft/StyleSwin. 1.

Introduction
The state of image generative modeling has seen dra-matic advancement in recent years, among which genera-tive adversarial networks [14, 41] (GANs) offer arguably the most compelling quality on synthesizing high-resolution images. While early attempts focus on stabilizing the train-ing dynamics via proper regularization [15,16,36,46,47] or adversarial loss designs [2, 25, 39, 45], remarkable perfor-mance leaps in recent prominent works mainly attribute to the architectural modifications that aim for stronger mod-eling capacity, such as adopting self-attention [66], aggres-sive model scaling [4], or style-based generators [29, 30].
Recently, drawn by the broad success of transformers in dis-criminative models [11, 32, 43], a few works [24, 37, 62, 67] attempt to use pure transformers to build generative net-works in the hope that the increased expressivity and the ability to model long-range dependencies can benefit the generation of complex images, yet high-quality image gen-eration, especially on high resolutions, remains challenging.
This paper aims to explore key ingredients when us-ing transformers to constitute a competitive GAN for high-resolution image generation. The first obstacle is to tame the quadratic computational cost so that the network is scal-able to high resolutions, e.g., 1024 × 1024. We propose to leverage Swin transformers [43] as the basic building block since the window-based local attention strikes a balance be-tween computational efficiency and modeling capacity. As such, we could take advantage of the increased expressivity to characterize all the image scales, as opposed to reduc-ing to point-wise multi-layer perceptrons (MLP) for higher scales [67], and the synthesis is scalable to high resolution, e.g., 1024×1024, with delicate details. Besides, the local at-tention introduces locality inductive bias so there is no need for the generator to relearn the regularity of images from scratch. These merits make a simple transformer network substantially outperform the convolutional baseline.
In order to compete with the state of the arts, we further propose three instrumental architectural adaptations. First, we strengthen the generative model capacity by employing the local attention in a style-based architecture [29], dur-ing which we empirically compare various style injection approaches for our transformer GAN. Second, we propose double attention in order to enlarge the limited receptive field brought by the local attention, where each layer attends to both the local and the shifted windows, effectively im-proving the generator capacity without much computational overhead. Moreover, we notice that Conv-based GANs im-plicitly utilize zero padding to infer the absolute positions, a crucial clue for generation, yet such feature is missing in the window-based transformers. We propose to fix this by in-troducing sinusoidal positional encoding [52] to each layer such that absolute positions can be leveraged for image syn-thesis. Equipped with the above techniques, the proposed network, dubbed as StyleSwin, starts to show advantageous generation quality on 256 × 256 resolution.
Nonetheless, we observe blocking artifacts when syn-thesizing high-resolution images. We conjecture that these disturbing artifacts arise because computing the attention independently in a block-wise manner breaks the spatial coherency. That is, while proven successful in discrimina-tive tasks [43, 56], the block-wise attention requires special treatment when applied in synthesis networks. To tackle these blocking artifacts, we empirically investigate various solutions, among which we find that a wavelet discrimina-tor [13] examining the artifacts in spectral domain could effectively suppress the artifacts, making our transformer-based GAN yield visually pleasing outputs.
The proposed StyleSwin, achieves state-of-the-art quality on multiple established benchmarks, e.g., FFHQ, CelebA-HQ, and LSUN Church. In particular, our approach shows compelling quality on high-resolution image synthesis (Fig-ure 1), achieving competitive quantitative performance rel-ative to the leading ConvNet-based methods without com-plex training strategies. On CelebA-HQ 1024, our approach achieves an FID of 4.43, outperforming all the prior works including StyleGAN [29]; whereas on FFHQ-1024, we ob-tain an FID of 5.07, approaching the performance of Style-GAN2 [30]. 2.