Abstract 1.

Introduction
Recognition of materials from their visual appearance is essential for computer vision tasks, especially those that involve interaction with the real world. Material segmen-tation, i.e., dense per-pixel recognition of materials, re-mains challenging as, unlike objects, materials do not ex-hibit clearly discernible visual signatures in their regular
RGB appearances. Different materials, however, do lead to different radiometric behaviors, which can often be cap-tured with non-RGB imaging modalities. We realize multi-modal material segmentation from RGB, polarization, and near-infrared images. We introduce the MCubeS dataset (from MultiModal Material Segmentation) which contains 500 sets of multimodal images capturing 42 street scenes.
Ground truth material segmentation as well as seman-tic segmentation are annotated for every image and pixel.
We also derive a novel deep neural network, MCubeSNet, which learns to focus on the most informative combinations of imaging modalities for each material class with a newly derived region-guided filter selection (RGFS) layer. We use semantic segmentation as a prior to “guide” this filter se-lection. To the best of our knowledge, our work is the first comprehensive study on truly multimodal material segmen-tation. We believe our work opens new avenues of practical use of material information in safety critical applications.
Thanks to the large strides made in object recognition research, computers can now tell what the object in an im-age is with sufficient accuracy. Telling what an object is, however, often insufficient to act in the real world. Our own visual system can not only tell a cup from a table, but also a paper cup from a ceramic one so that we can plan our grasp before touching it. If a computer could similarly tell what an object is made of, critical decisions can be made faster and more accurately. In particular, dense pixel-wise recognition of materials in an image becomes an essential task. We refer to this as material segmentation and distin-guish it from classic “material recognition” which focuses on recognizing materials image-wise or for isolated objects.
Successful material segmentation would be particularly beneficial for road scene analysis. If an autonomous vehicle or an advanced driver assistance system (ADAS) can tell an asphalt road from a concrete one or a leaf on the road from dirt, it can execute safer control. Outdoor material segmen-tation, however, remains elusive mainly due to the rich va-riety of materials encountered in the real world and the lack of annotated data. Closest works only realize image-wise recognition of materials or are primarily of indoor architec-tural, professional photographs [1, 27]. It is also worth clar-ifying the distinction of material segmentation from stuff segmentation. “Stuff” is not a material but rather refers to
objects without discernible boundaries (e.g., a road, a rep-resentative “stuff,” is composed of different materials such as asphalt, paint for markings, and metal for manholes).
The difficulty of material segmentation is exacerbated by the fact that materials lack well-defined visual features in regular RGB images. Unlike objects which largely exhibit different looks including shape contours and surface tex-tures, different materials often result in similar appearance in regular color images. For instance, a ceramic cup and a plastic cup would have similar global shapes and local sur-face textures in an image. Some materials don’t even have their own defined appearances. For instance, water does not have its own color and metal mostly mirror-reflects, both of which take on the appearance of their surroundings.
Where should we look for reliable visual cues to recog-nize materials? The surface composition of different mate-rials not just in their spatial distribution but also in their sub-surface structure give rise to characteristic radiometric be-haviors. For instance, subtle differences in the mesoscopic surface structure change polarization of incident light and variation in subsurface composition result in different ab-sorption of near-infrared (NIR) light. These radiometric features can potentially let us discern different materials robustly. Few works in the past have exploited different imaging modalities in isolation for only material recogni-tion. Recent advances in imaging sensors, most notably the introduction of quad-Bayer CMOS, have brought the op-portunity to leverage a variety of imaging modalities in a compact passive setup at low cost, making it particularly suitable for autonomous vehicles and mobile robots. We believe the time is ripe to systematically study what multi-modal imaging can offer to material segmentation.
In this paper, we realize multimodal material segmenta-tion, the recognition of per-pixel material categories from a set of images from the same vantage point but of different imaging modalities. In particular, we consider the combi-nation of regular RGB, polarization, and near-infrared im-ages at each instance. We build an imaging system con-sisting of a binocular stereo of quad-Bayer RGB polariza-tion cameras, a monocular near-infrared camera, and a Li-DAR to capture outdoor road scenes. We introduce a new dataset of multimodal material images which we refer to as the MCubeS dataset (from MultiModal Material Segmenta-tion). The MCubeS dataset contains 500 sets of images of these imaging modalities taken at walking speed that em-ulates the vantage of an autonomous vehicle in 42 scenes and is fully annotated for each pixel. In addition to materi-als, we also annotate semantic segmentation labels. To our knowledge, MCubeS is the first of its kind and scale and opens new avenues of research on material segmentation.
We derive a novel deep architecture, which we refer to as
MCubeSNet, for learning to accurately achieve multimodal material segmentation. We introduce region-guided filter selection (RGFS) to let MCubeSNet learn to focus on the most informative combinations of imaging modalities for each material class. We use object categories obtained with vanilla semantic segmentation as a prior to “guide” the fil-ter selection. The network learns to select different convo-lution filters for each semantic class from a learned image-wise set of filters. This region-guided filter selection layer enables “dynamic” selection of filters, and thus combina-tions of imaging modalities, tailored to different potential materials underlying different semantic regions (i.e., object categories) without significant computational overhead.
To the best of our knowledge, our work is the first for multimodal material segmentation. In the absence of past methods, we experimentally validate the effectiveness of
MCubeSNet by comparing its accuracy to state-of-the-art semantic segmentation methods. The experimental results, including ablation studies, clearly show that MCubeSNet can accurately and robustly recognize materials from mul-timodal data. The selected filters also reveal that character-istic radiometric properties of different materials are cap-tured with unique combinations of imaging modalities. We believe our work makes an important step forward in mate-rial segmentation and opens new avenues of practical use of material information in safety critical applications. All data and code can be found on our project web page. 2.