Abstract
With the rise of powerful pre-trained vision-language models like CLIP, it becomes essential to investigate ways to adapt these models to downstream datasets. A recently proposed method named Context Optimization (CoOp) in-troduces the concept of prompt learning—a recent trend in
NLP—to the vision domain for adapting pre-trained vision-language models. Specifically, CoOp turns context words in a prompt into a set of learnable vectors and, with only a few labeled images for learning, can achieve huge improve-ments over intensively-tuned manual prompts. In our study we identify a critical problem of CoOp: the learned con-text is not generalizable to wider unseen classes within the same dataset, suggesting that CoOp overfits base classes observed during training. To address the problem, we pro-pose Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural net-work to generate for each image an input-conditional token (vector). Compared to CoOp’s static prompts, our dynamic prompts adapt to each instance and are thus less sensitive to class shift. Extensive experiments show that CoCoOp generalizes much better than CoOp to unseen classes, even showing promising transferability beyond a single dataset; and yields stronger domain generalization performance as well. Code is available at https://github.com/
KaiyangZhou/CoOp. 1.

Introduction
Recent research in large-scale vision-language pre-training has achieved striking performance in zero-shot im-age recognition [13,24,33,40], demonstrating a potential in learning open-world visual concepts for such a paradigm.
The key design lies in how visual concepts are modeled.
In traditional supervised learning where labels are dis-cretized, each category is associated with a randomly initial-ized weight vector that is learned to minimize the distance with images containing the same category. Such a learning (cid:0)
Corresponding author method focuses on closed-set visual concepts, limiting the model to a pre-defined list of categories, and is unscalable when it comes to new categories unseen during training.
In contrast, for vision-language models1 like CLIP [40] and ALIGN [24], the classification weights are diametri-cally generated by a parameterized text encoder (e.g., a
Transformer [48]) through prompting [34]. For instance, to differentiate pet images containing different breeds of dogs and cats, one can adopt a prompt template like “a photo of a {class}, a type of pet” as input to the text encoder, and as a result, class-specific weights for classification can be synthesized by filling in the “{class}” token with real class names. Compared to discrete labels, vision-language mod-els’ source of supervision comes from natural language, which allows open-set visual concepts to be broadly ex-plored and has been proven effective in learning transferable representations [24, 40].
With the rise of such powerful vision-language models, the community has recently started to investigate potential solutions to efficiently adapt these models to downstream datasets [14, 53, 56, 62]. To fit web-scale data, such as the 400 million pairs of images and texts used by CLIP, vision-language models are purposefully designed to have high ca-pacity, entailing that the model size would be enormous, typically with hundreds of millions of parameters or even billions. Therefore, fine-tuning the entire model, as often adopted in deep learning research [18], is impractical and might even damage the well-learned representation space.
A safer approach is to tune a prompt by adding some context that is meaningful to a task, like “a type of pet” for the pet dataset mentioned above, which has been found ef-fective in improving performance [40]. However, prompt engineering is extremely time-consuming and inefficient as it has to be based on trial and error, and does not guaran-tee an optimal prompt either. To automate prompt engi-neering, Zhou et al. [62] have recently explored the concept of prompt learning—a recent trend in NLP [15, 25, 30, 32, 44, 60]—for adapting pre-trained vision-language models.
Their approach, Context Optimization (CoOp), turns con-1We follow existing studies [13,24,33,40] to refer to CLIP-like models as vision-language models.
(a) Both CoOp and CoCoOp work well on the base classes observed during training and beat manual prompts by a significant margin. (b) The instance-conditional prompts learned by CoCoOp are much more generalizable than CoOp to the unseen classes.
Figure 1. Motivation of our research: to learn generalizable prompts. The images are randomly selected from SUN397 [55], which is a widely-used scene recognition dataset. text words in a prompt into a set of learnable vectors, taking advantage of the differentiable nature of neural networks.
With only a few labeled images for learning, CoOp achieves huge improvements over intensively-tuned manual prompts across a wide range of image recognition datasets.
In our study, we identify a critical problem of CoOp: the learned context is not generalizable to wider unseen classes within the same task. Figure 1 illustrates the problem: the context learned by CoOp works well in distinguishing the base classes like “arrival gate” and “cathedral” but suffers a significant drop in accuracy when it is transferred to the new (unseen) classes, such as “wind farm” and “train railway”— even though the task’s nature remains the same, i.e., recog-nizing scenes. The results suggest that the learned context overfits the base classes, thus failing to capture more gen-eralizable elements that are vital for broader scene recog-nition. We argue that such a problem is caused by CoOp’s static design: the context, which is fixed once learned, is optimized only for a specific set of (training) classes. On the contrary, the manually-designed prompts adopted by the zero-shot method are relatively generalizable.
To address the weak generalizability problem, we intro-duce a novel concept: conditional prompt learning. The key idea is to make a prompt conditioned on each input instance (image) rather than fixed once learned. To make the model parameter-efficient, we introduce a simple yet effective im-plementation of conditional prompt learning. Specifically, we extend CoOp by further learning a lightweight neural network to generate for each image an input-conditional to-ken (vector), which is combined with the learnable con-text vectors. We call our approach Conditional Context
Optimization (CoCoOp).2 An overview is shown in Fig-ure 2. Interestingly, the paradigm of CoCoOp is analogous to image captioning [49], which explains why instance-2Pronounced as /k@U­ku:p/. conditional prompts are more generalizable: they are op-timized to characterize each instance (more robust to class shift) rather than to serve only for some specific classes.
We present comprehensive experiments on 11 datasets covering a diverse set of visual recognition tasks. Specifi-cally, we design a base-to-new generalization setting where a model is first learned using base classes and then tested on completely new classes. Compared with the zero-shot method [40] and CoOp [62], our approach achieves the best overall performance (Table 1). Importantly, CoCoOp gains significant improvements over CoOp in unseen classes (Fig-ure 3(a)), allowing the gap between manual and learning-based prompts to be substantially reduced.
In a more challenging scenario where the context learned for one task is directly transferred to other tasks with dras-tically different classes, CoCoOp still beats CoOp with a clear margin (Table 2), suggesting that instance-conditional prompts are more transferable and have the potential to suc-ceed at larger scale. CoCoOp also obtains stronger domain generalization performance than CoOp (Table 3), further justifying the strengths of dynamic prompts.
In summary, our research provides timely insights into the generalizability problem in prompt learning, and cru-cially, demonstrates the effectiveness of a simple idea in various problem scenarios. We hope our approach and the findings presented in this work can pave the way for future research in generalizable—and transferable—prompt learn-ing. 2.