Abstract
Structural re-parameterization (Rep) methods achieve noticeable improvements on simple VGG-style networks.
Despite the prevalence, current Rep methods simply re-parameterize all operations into an augmented network, in-cluding those that rarely contribute to the model’s perfor-mance. As such, the price to pay is an expensive com-putational overhead to manipulate these unnecessary be-haviors. To eliminate the above caveats, we aim to boot-strap the training with minimal cost by devising a dy-namic re-parameterization (DyRep) method, which encodes
Rep technique into the training process that dynamically evolves the network structures. Concretely, our proposal adaptively finds the operations which contribute most to the loss in the network, and applies Rep to enhance their representational capacity. Besides, to suppress the noisy and redundant operations introduced by Rep, we devise a de-parameterization technique for a more compact re-parameterization. With this regard, DyRep is more efficient than Rep since it smoothly evolves the given network instead of constructing an over-parameterized network. Experi-mental results demonstrate our effectiveness, e.g., DyRep improves the accuracy of ResNet-18 by 2.04% on ImageNet and reduces 22% runtime over the baseline. Code is avail-able at: https://github.com/hunto/DyRep. 1.

Introduction
The advent of automatic feature engineering fuels deep convolution neural networks (CNNs) to reach the remark-able success in a plethora of computer vision tasks, such as image classification [8,9,29,34], object detection [5,16,19], and semantic segmentation [7, 33].
In the path of pursu-ing better performance than that of early prototypes such as VGG [20] and ResNet [8], current deep learning mod-[10, 15, 29] generally are embodied with billions of els
*Correspondence to: Shan You <youshan@sensetime.com>.
Figure 1. Accuracy and training cost of ResNet on ImageNet dataset using origin, DBB, and our DyRep models. Our DyRep obtains the highest accuracies yet has much smaller training cost compared to DBB. parameters and paramount well-tailored architectures and operations (e.g., channel-wise attention in SENet [10] and branch-concatenation in Inception [23]). From this per-spective, we may encounter a dilemma in the sense that a learning model with good performance should be heavy and computationally intensive, which is extremely hard to deploy and has a high inference time. To this end, a crit-ical question is: how to enhance the ability of neural net-works without incurring expensive computational overhead and high inference complexity?
Structural re-parameterization technique (Rep) and its variants [2, 3, 32], which construct an augmented model in training and transform it back to the original model in in-ference, have emerged as a leading strategy to address the above issue. Concretely, these methods enhance the repre-sentational capacities of models by expanding the original convolution operations with multiple branches in training, then fusing them into one convolution for efficient infer-ence without accuracy drop. Representative examples in-clude RepVGG [3] and DBB [2]. The former enhances
VGG-style networks by expanding the 3 × 3 Conv to an accumulation of three branches (i.e., 3 × 3 Conv, 1 × 1
Conv, and residual connection) in the training process and re-parameterizing it back to the original 3 × 3 Conv in the inference time. The latter improves CNNs by enriching
Figure 2. Overview of Dynamic Re-parameterization (DyRep). Train (left panel): Starting from a simple model, DyRep dynamically adjusts the network structures in training by expanding operations to multi-branch blocks or cutting redundant branches. Inference (right panel): The trained model is transformed to the original model for inference. the types of expanding branches (i.e., introducing 6 equiv-alent transformations of re-parameterization) and unifying them into a universal building block which applies to vari-ous CNNs such as ResNet [8] and MobileNet [9]. Never-theless, a common caveat of current Rep and its variants is coarsely re-parameterizing all branches into an augmented network, where a large portion of them may seldom en-hance the model’s performance. In other words, directly uti-lizing the same branches in all layers would lead to subop-timal structures. Furthermore, these redundant operations would result in an expensive or even unaffordable memory and computation cost, since the memory consumption in-creases linearly with the number of branches.
To conquer the aforementioned issues, we propose a novel re-parameterization method, dubbed as DyRep, to dy-namically evolve network structures during training and re-cover to the original network in inference, as illustrated in
Figure 2. In particular, the key concept behind our proposal is adaptively seeking the operations with the biggest con-tributions to the performance (or loss as its surrogate) in-stead of pursuing a universal re-parameterization to all of them, which ensures both the efficacy and accuracy to aug-ment the network. In DyRep, the operation with the biggest contribution amounts to the operation with the most signifi-cant saliency score. As our first technical contribution, this measure is partially inspired by the gradient-based pruning methods, which utilize the gradients w.r.t. the loss to calcu-late the saliency scores of filters.
Since the existing Rep methods are designed for trans-forming the model to a narrow one at the end of the training, there is no plug-and-play technique to expand one convolu-tion to multiple branches while keeping the training stable.
To achieve a training-aware Rep, we first extend the Rep technique in such a case, then propose to stabilize the train-ing by initializing the additional branches with small scale factors in batch normalization (BN) layers. By doing so, the additional branches would start with minor importance, making trivial changes on the original weights, and thus ob-taining a smooth structure evolution.
Our second key technical contribution is devising a de-parameterization method to unearth and discard the redun-dant operations that appeared in Rep. Since we initialize the BN layer in the newly-added branch with small scale factor, which can be treated as a relaxed gate to turn on or cut off one branch. That is, if a branch has a significant small scale value compared to other branches, it will make a minor contribution to the outputs. Therefore, we could discard it and absorb its weights to other branches for better efficiency. Specifically, if a branch has a zero scale factor, its operations would not affect the output.
Our main contributions are summarized as follows.
• We propose DyRep, a dynamic re-parameterization method that applies to training, aiming to enhance the
Rep performance with minimal overhead. By identify-ing the important operations dynamically during train-ing, our proposal achieves significant efficiency and performance improvement.
• Our DyRep is more friendly to downstream tasks such as object detection. Different from previous Rep and
NAS methods that need to first train a network on im-age classification task, followed by transferring it to downstream tasks, DyRep can directly adapt the struc-tures in downstream tasks. This property dramatically reduces the computational cost.
• Extensive experiments on image classification and its downstream tasks demonstrate that DyRep outper-forms other Rep methods in the measure of both the accuracy and the runtime cost.
2.