Abstract
Recently, memory-based approaches show promising re-sults on semi-supervised video object segmentation. These methods predict object masks frame-by-frame with the help of frequently updated memory of the previous mask. Dif-ferent from this per-frame inference, we investigate an al-ternative perspective by treating video object segmentation as clip-wise mask propagation.
In this per-clip inference scheme, we update the memory with an interval and simul-taneously process a set of consecutive frames (i.e. clip) be-tween the memory updates. The scheme provides two poten-tial benefits: accuracy gain by clip-level optimization and efficiency gain by parallel computation of multiple frames.
To this end, we propose a new method tailored for the per-clip inference. Specifically, we first introduce a clip-wise operation to refine the features based on intra-clip cor-relation.
In addition, we employ a progressive matching mechanism for efficient information-passing within a clip.
With the synergy of two modules and a newly proposed per-clip based training, our network achieves state-of-the-art performance on Youtube-VOS 2018/2019 val (84.6% and 84.6%) and DAVIS 2016/2017 val (91.9% and 86.1%). Fur-thermore, our model shows a great speed-accuracy trade-off with varying memory update intervals, which leads to huge flexibility.
Figure 1. (top) An illustrative example of per-clip inference where the memory update interval L is 5. We mark memory frames using (bottom) Accuracy vs. FPS - We compare red image borders. our model under the different inference setting of L with SOTA methods [9, 29, 35, 50, 52]. We report the overall score and FPS on Youtube-VOS 2019 [47] validation set. For a fair comparison, we compute the FPS of all the reported methods using the same machine. We additionally report STCN variants, extended in the same way as in ‘Ours’. Note that the FPS axis is in the log scale. 1.

Introduction
The goal of semi-supervised video object segmentation (VOS) is to segment foreground objects in every frame of a video given a ground truth object mask in the first frame. One of the latest breakthroughs in this task is a memory-based approach that the Space-Time Memory net-work (STM) [29] proposed. STM encodes and stores the past frames with the corresponding masks as memory (i.e. memory update step) then estimates the mask of the cur-rent (query) frame through learned spatio-temporal memory matching (i.e. mask prediction step). It iterates the mem-† This work was done during an internship at Adobe Research. ory update and the mask prediction steps frame-by-frame.
Since the success of STM, the memory-based approach has dominated the field of semi-supervised VOS. Many variants improve STM with advanced memory read pro-cess [8, 14, 22, 34, 35] or efficient memory storage [20, 46].
One notable improvement of the memory-based ap-proach has been made in STCN [9]. It formulates memory matching as direct image-to-image correspondence learning and proposes siamese key encoders for memory and query frames. STCN also shows that L2 similarity is more ro-bust than inner-product for memory matching. With the advanced memory matching, STCN showcases that mem-ory updates may not be needed at every frame. Instead, it updated the memory only at every fifth frame, resulting in considerable speedup while achieving SOTA accuracy.
Inspired by the progress, we further delve into a per-clip inference scheme in the memory-based approach.
If we conduct the memory update periodically with an interval, we can group the input video frames into a set of consec-utive frames (i.e. clip) according to the update interval and perform the mask prediction clip-by-clip instead of frame-by-frame. We call it per-clip inference (Fig. 1). This new in-ference scheme provides two opportunities. First, it enables us to access nearby frames before making predictions (i.e. non-causal), while the frame-by-frame prediction provides no access for the networks to the future frames (i.e. causal).
With this non-causal system, we can exchange information among the frames in a clip and may make optimized predic-tions for the clip. To the best of our knowledge, there is no previous work in the memory-based approach that leverages clip-wise optimization. Another opportunity is the flexibil-ity between accuracy and speed tradeoff.
Increasing the memory update interval may provide near-linear speedup, since there are lower computations for memory update and, more importantly, the majority of the computations within a clip may be processed in parallel.
Based on the motivation, we present a new semi-supervised video object segmentation method, PCVOS, that is tailored for the per-clip inference scheme. Given the per-clip inference scenario, we propose the following changes from the standard memory-based methods. To optimize the features using intra-clip correlation, we propose intra-clip refinement module that performs a clip-wise operation.
Specifically, we employ a transformer [39] to aggregate in-formation in a spatial-temporal neighborhood. Since the features from the memory readout are a critical source of in-formation for mask prediction, we place the refinement af-ter the memory readout. The module aggregates and refines the features resulting in consistent and robust mask predic-tions. To enhance accuracy and speed tradeoff, we pro-pose a progressive memory matching mechanism. While increasing the memory update interval provides a great op-portunity for improving efficiency, we observed that mem-ory readout accuracy is gradually degraded as the interval increases. Our progressive matching module provides a lightweight solution to augment the memory and boosts the memory readout accuracy when the memory update inter-val is long. In addition, we provide a new training scheme.
We form each training sample with multiple clips and train our model with clip-level supervision. Compared to previ-ous per-frame training [28, 29], we found that our per-clip training is much effective for our method.
With our new perspective and proposals, our method (e.g. 84.6 on achieves
Youtube-VOS 2018 val, 86.1 on DAVIS 2017 val). Fur-state-of-the-art performance thermore, by varying memory update intervals, we offer multiple variant models with great accuracy and efficiency trade-off. For example, our efficient model, Ours-L15, achieves better accuracy than STCN1 while running almost three times faster as depicted in Fig. 1. More importantly, it could be possible to enjoy the flexibility with a single trained model via adaptive modulation of memory update interval at the test time.
Our contributions are summarized as follows: 1. We reformulate semi-supervised video object segmen-tation from a per-clip inference perspective, offering an alternative to the dominating per-frame inference. 2. We propose the Per-Clip VOS model (PCVOS) that is tailored for per-clip inference. 3. Our method achieves state-of-the-art performance on multiple benchmarks along with efficient variants pro-viding great accuracy-speed balance. 2.