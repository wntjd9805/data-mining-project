Abstract
While supervised deep learning has been a prominent tool for solving many image restoration problems, there is an increasing interest on studying self-supervised or un-supervised methods to address the challenges and costs of collecting truth images. Based on the neuralization of a
Bayesian estimator of the problem, this paper presents a self-supervised deep learning approach to general image restoration problems. The key ingredient of the neuralized estimator is an adaptive stochastic gradient Langevin dy-namics algorithm for efficiently sampling the posterior distri-bution of network weights. The proposed method is applied on two image restoration problems: compressed sensing and phase retrieval. The experiments on these applications showed that the proposed method not only outperformed existing non-learning and unsupervised solutions in terms of image restoration quality, but also is more computationally efficient. 1.

Introduction
Image restoration is about calculating an image x from a collection of its measurements, denoted by y, whose rela-tionship can be described as y = Ψ(x) + n, (1) where Ψ denotes the image formation process and n denotes measurement noise.
Image restoration is one fundamen-tal problem encountered in a wide range of image-related applications. For example, image restoration in digital pho-tography, compressed sensing, computed tomography (CT), and magnetic resonance imaging (MRI) in medical imag-ing, phase retrieval for scientific imaging, and many others.
In general, the problem (1) is an ill-posed inverse problem, whose direct inversion is either not unique or sensitive to measurement noise.
Over last decades, regularization method, or equivalently
Bayesian estimator, has been the dominant tool for image restoration. These non-learning methods either impose cer-tain prior or assume certain prior distribution on images for addressing solution ambiguity and noise sensitivity. How-ever, it remains challenging to define an accurate image prior. In recent years, deep neural network (DNN) emerges as a prominent tool for solving inverse problems; see e.g.
[11, 12, 16, 17, 20, 33, 49, 52, 58, 61, 62]. The majority of existing DNN-based solutions are supervised on an external training dataset with truth images. Such a prerequisite on truth images limits their wider applications in practice. For example, collecting truth images can be very challenging and costly in medical imaging and scientific imaging. Also, the generalization performance of a supervised learning method can be a concern in practice, if the network is trained over a biased dataset where the structures of test images are not present in training samples.
In recent years, it is receiving an increasing interest on developing deep learning methods for imaging, which do not require truth images for training DNNs. The so-called plug-and-play prior attempts to address such an issue by adopting some pre-trained denoising network in an iterative image restoration scheme; See e.g. [35, 48, 56, 63]. While these methods do not explicitly call truth images, the pre-trained networks are still supervised over the dataset with truth images related to the image for restoration. Another approach is using generative adversarial network (GAN) to synthesize training samples for training the network; see e.g. [43]. Similarly, the performance of GAN-based methods highly relies on the effectiveness of the pre-trained GAN model on simulating truth images. While GAN has been very effective on simulating the images in specific domain such as face and text, it is not so for other types of images, e.g., medical images and scientific images.
The methods above are not completely free from the prerequisite of accessing related truth images. Recently, there has been a rapid progress on unsupervised or self-supervised learning for image denoising using un-trained
DNNs; See e.g. [3, 14, 28, 40, 41, 50, 55]. However, the generalization of these denoising networks to ill-posed image restoration problems is not trivial. The existence of the non-zero null space to calculate the integral (3) via the MC method:
Null(Ψ) = {x : Ψ(x) = 0} ̸= {0} in image restoration make it quite different from image denoising, as the error induced by the existence of non-trivial space Null(Ψ) ̸= {0} cannot be treated as random noise. One pioneering work is the so-called deep image prior (DIP) [55] which shows that there exists certain implicit prior induced by a convolutional neural network (CNN). The DIP states that regular image structures appear before random noise when training a CNN. Such a prior has been exploited in many image restoration tasks, e.g. super-resolution [55],
CT reconstruction [23], image separation [22] and blind de-blurring [32, 44]. In addition to DIP, there are some other approaches to address the overfitting caused by the absence of truth images during training. For CS image reconstruc-tion, Pang et al. [39] proposed to train a Bayesian DNN with Gaussian random weights. Heckle [24] used an under-parametrized network. Metzler et al. [38] and Zhussip et al. [67] proposed to regularize the denoising network in an iterative scheme by Stein’s unbiased risk estimator (SURE). 1.1. Motivation and main idea
A self-supervised deep learning method for image restora-tion is very attractive to the applications where collecting truth images is challenging, e.g. medical and scientific imag-ing. This paper is about studying an efficient and effective method for training a NN to process testing data, where nei-ther pre-trained model nor training sample with truth image is used during training.
The proposed method is derived from the so-called mini-mum mean squared error (MMSE) estimator of the problem (1) defined by (cid:90) (cid:98)x = xp(x|y)dx, (2) where p(x|y) denotes the posterior distribution. One way to calculate (2) is using the Monte Carlo (MC) method. In-stead of directly sampling p(x|y), we use a generative CNN f (ϵ0; θ), parametrized by θ, to re-parametrize x: x = f (ϵ0; θ). (cid:98)x ≈ (cid:88) k f (ϵ0; θk), where θk ∼ p(θ|y, ϵ0).
Efficient sampling restricted in feasible set. How to efficiently sample θ is critical for an accurate calculation of the integral (3). A natural treatment is, instead of sampling
θ in the whole space, we only sample those parameters in a feasible set Ω, where the density function π(θ) concentrates.
Suppose measurement noise n is i.i.d. Gaussian white noise with variance σ2. By large number theory, we have, for image size N → ∞, 1
N
L(θ) =
∥Ψ(f (ϵ0; θ)) − y∥2 2 → σ2. (4)
In other words, with sufficiently large image size, the density of θ whose f (ϵ0; θ) is close to x, concentrates within the set:
∥Ψ(x) − y∥2 2 = 1
N
Ωϵ := {θ : σ2 − ϵ ≤ L(θ) ≤ σ2 + ϵ}, (5) where ϵ is a small threshold. A detailed analysis of (5) is provided in the supplementary file. To conclude, the samples from π(θ) within the feasible set Ωϵ defined by (5) are sufficient for accurately calculating the integral (3).
Adaptive SGLD for restricted MC sampling.
SGLD is an Markov chain Monte Carlo (MCMC) sampling algo-rithm, which is proposed in [57] for efficiently sampling network weights. SGLD simulates dynamics of molecu-lar systems with stochastic differential equation given by dθt = −∇L(θt)dt + 2dWt, where Wt is stationary Gaus-sian process with zero-mean and L is a loss function.
√
In this paper, we proposed a new type of SGLD for effec-tively sampling from π(θ) = p(θ|y, ϵ0) within the feasible set Ωϵ defined by (5). The corresponding stochastic differen-tial equation is defined by dθt = −∇L(θt)dt + β exp(c0(
σ2
L(θt)
− 1))dWt, where L(θ) is defined by (4), in the case where n ∼
N (0, σ2I) and θ follows an uniform distribution. Then the discretization of the equation above leads to an adaptive stochastic gradient Langevin dynamics (ASGLD):
Then, (cid:98)x in (2) can be re-expressed as (cid:90) (cid:98)x = f (ϵ0; θ)p(θ|y, ϵ0)dθ. (3)
Such a re-parametrization allows us to utilize implicit image prior induced by CNN. Then, the remaining task is how to efficiently sample the posterior distribution
π(θ) = p(θ|y, ϵ0),
θk+1 = θk − γk · ∇L(θk) + β exp(c0(
σ2
L(θk)
√
− 1))
γk · ϵ, (6) where ϵ ∼ N (0, I). It can be seen that in comparison to classic SGLD, ASGLD adaptively adjust the magnitude of noise perturbation based on the loss function L(θ). The samples from ASGLD will concentrate within the feasible set Ωϵ. See Section 3 for more details.
1.2. Main contribution
This paper proposed a self-supervised method for solv-ing ill-posed image restoration problems, without requiring any external truth image. The method trains a CNN which approximates the MMSE estimator of the problem via MC-based integration, where the key is how to efficiently sample the posterior distribution. The answer from this paper is an adaptive SGLD method, an efficient MCMC scheme that focuses on concentrated regions of the posterior distribution.
The proposed method is applied to solve image restora-tion problems in two imaging modalities: CS and phase retrieval. The experiments show that the proposed method not only provides state-of-the-art performance among all existing dataset-free solutions, but also is more computation-ally efficient. See below for the summary of the differences between ASGLD and most related unsupervised methods.
• ASGLD vs. DIP [55] and its extensions: DIP utilizes the implicit prior induced by a CNN and trains the net-work with early-stopping. ASGLD also training a network and its algorithm is motivated by MCMC sampling based approximation to the MMSE estimator of the problem.
• ASGLD vs. BNN [39]: Both train a network to approxi-mate the MMSE estimator of the problem. BNN approxi-mates it via variational approximation using a Bayesian neural network (BNN) with random weights. ASGLD approximates it using MC-sampling-based integration im-plemented using an efficient MCMC sampler.
• ASGLD vs. plain SGLD : In comparison to classic SGLD with constant noise variance for general MCMC sampling,
ASGLD proposes a new SGLD scheme with adaptive noise variance, which enables one to efficiently calculate the MC-based integration.
See below for the summary of main contributions:
• An MC-sampling-based MMSE estimator of image restoration with untrained deep network.
• A new adaptive SGLD scheme of MCMC sampling for efficient calculation of MC-based integration.
• Noticeable performance improvement over existing unsu-pervised methods in two image restoration tasks.
• A general self-supervised method with potential applica-tions to other ill-posed inverse problems in imaging. 2.