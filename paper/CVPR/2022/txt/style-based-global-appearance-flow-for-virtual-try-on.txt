Abstract 1.

Introduction
Image-based virtual try-on aims to fit an in-shop gar-ment into a clothed person image. To achieve this, a key step is garment warping which spatially aligns the target gar-ment with the corresponding body parts in the person im-age. Prior methods typically adopt a local appearance flow estimation model. They are thus intrinsically susceptible to difficult body poses/occlusions and large mis-alignments between person and garment images (see Fig. 1). To over-come this limitation, a novel global appearance flow esti-mation model is proposed in this work. For the first time, a StyleGAN based architecture is adopted for appearance flow estimation. This enables us to take advantage of a global style vector to encode a whole-image context to cope with the aforementioned challenges. To guide the StyleGAN flow generator to pay more attention to local garment de-formation, a flow refinement module is introduced to add local context. Experiment results on a popular virtual try-on benchmark show that our method achieves new state-of-the-art performance. It is particularly effective in a ‘in-the-wild’ application scenario where the reference image is full-body resulting in a large mis-alignment with the gar-ment image (Fig. 1 Top). Code is available at: https:
//github.com/SenHe/Flow-Style-VTON .
In 2020,
The transition from offline in-shop retail to e-commerce has been accelerated by the recent pandemic caused lock downs. retail e-commerce sales worldwide amounted to 4.28 trillion US dollars and e-retail revenues are projected to grow to 5.4 trillion US dollars in 2022.
However, when it comes to fashion, one of key offline experiences missed by the on-line shoppers is the chang-ing room where a garment item can be tried-on. To re-duce the return cost for the online retailers and give shop-pers the same offline experience online, image-based vir-tual try-on (VTON) has been studied intensively recently
[9, 10, 13, 14, 19, 24, 38, 39, 42, 43].
A VTON model aims to fit an in-shop garment into a per-son image. A key objective of a VTON model is to align the in-shop garment with the corresponding body parts in the person image. This is due to the fact that the in-shop gar-ment is usually not spatially aligned with the person image (see Fig. 1). Without the spatial alignment, directly apply-ing advanced detail-preserving image to image translation models [18, 30] to fuse the texture in person image and gar-ment image will result in unrealistic effect in the generated try-on image, especially in the occluded and misaligned re-gions.
Previous methods address this alignment problem through garment warping, i.e., they first warp the in-shop garment, which is then concatenated with the person image and fed into an image to image translation model for the fi-nal try-on image generation. Many of them [9,14,19,38,42, 43] adopt a Thin Plate Spline (TPS) [7] based on the warp-ing method, exploiting the correlation between features ex-tracted from the person and garment images. However, as analyzed in previous works [5, 13, 42], TPS has limitations in handling complex warping, e.g., when different regions in the garment require different deformations. As a result, recent SOTA methods [10, 13] estimate dense appearance flow [45] to warp the garment. This involves training a net-work to predict the dense appearance flow field representing the deformation required to align the garment with the cor-responding body parts.
However, existing appearance flow estimation methods are limited in accurate garment warping due to the lack of global context. More specifically, all existing methods are based on local feature’s correspondence, e.g., local feature concatenation or correlation1, developed for optical flow estimation [6, 17]. To estimate the appearance flow, they make the unrealistic assumption that the corresponding re-gions from the person image and the in-shop garment are located in the same local receptive filed of the feature ex-tractor. When there is a large mis-alignment between the garment and corresponding body parts (Fig. 1 Top), cur-rent appearance flow based methods will deteriorate drasti-cally and generate unsatisfactory results. Lacking a global context also make existing flow-based VTON methods vul-nerable to difficult poses/occlusions (Fig. 1 Bottom) when correspondences have to be searched beyond a local neigh-borhood. This severely limits the use of these methods ‘in-the-wild’, whereby a user may have a full-body picture of herself/himself as the person image to try-on multiple gar-ment items (e.g., top, bottom, and shoes).
To overcome this limitation, a novel global appearance flow estimation model is proposed in this work. Specifi-cally, for the first time, a StyleGAN [21, 22] architecture for dense appearance flow estimation. This differs funda-mentally from existing methods [6, 10, 13, 17] which em-ploy a U-Net [30] architecture to preserve local spatial con-text. Using a global style vector extracted from the whole reference and garment images makes it easy for our model to capture global context. However, it also raises an im-portant question: can it capture local spatial context cru-cial for local alignments? After all, a single style vector seemingly has lost local spatial context. To answer this question, we first note that StyleGAN has been successfully 1It is worth noting that the tensor correlation methods [6, 10, 17] have the potential to reach global receptive field. However, its computation grows quadratically with respect to the input size. To make it tractable, its actual implementation is still based on limited local neighborhoods. applied to local face image manipulation tasks, where dif-ferent style vectors can generate the same face at different viewpoints [34] and different shapes [15, 28]. This suggests that a global style vector does have local spatial context en-coded. However, we also note that the vanilla StyleGAN ar-chitecture [21, 22], though much more robust against large mis-alignment and difficult poses/occlusions compared to
U-Net, is weaker when it comes to local deformation mod-eling. We therefore introduce a local flow refinement mod-ule in the existing StyleGAN generator to have the better of both worlds.
Concretely, our StyleGAN-based warping module (W in
Fig. 2) consists of stacked warping blocks that takes as in-puts a global style vector, garment features and person fea-tures. The global style vector is computed from the lowest resolution feature maps of the person image and the in-shop garment for global context modeling. In each warping block in the generator, the global style vector is used to modu-late the feature channels which takes in the corresponding garment feature map to estimate the appearance flow. To enable our flow-estimator to model the fine-grained local appearance flow, e.g., the arm and hand regions in Fig. 5, in each warping block on top of the style based appearance flow estimation part, we introduce a refinement layer. This refinement layer first warps the garment feature map, which is subsequently concatenated with the person feature map at the same resolution and then used to predict the local de-tailed appearance flow.
The contributions of this work are as follow: (1) We propose a novel style-based appearance flow method to warp the garment in virtual try-on. This global flow estima-tion approach makes our VTON model much robust against large mis-alignments between person and garment images.
This makes our method more applicable to ‘in-the-wild’ ap-plication where a full-body person image with natural poses is used (see in Fig. 1). (2) We conduct extensive experi-ments to validate our method, demonstrating clearly that it is superior to existing state-of-the-art alternatives. 2.