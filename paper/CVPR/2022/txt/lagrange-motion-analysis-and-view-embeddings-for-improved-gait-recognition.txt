Abstract
Gait is considered the walking pattern of human body, which includes both shape and motion cues. However, the main-stream appearance-based methods for gait recogni-tion rely on the shape of silhouette. It is unclear whether motion can be explicitly represented in the gait sequence modeling. In this paper, we analyzed human walking us-ing the Lagrange’s equation and come to the conclusion that second-order information in the temporal dimension is necessary for identification. We designed a second-order motion extraction module based on the conclusions drawn.
Also, a light weight view-embedding module is designed by analyzing the problem that current methods to cross-view task do not take view itself into consideration explicitly. Ex-periments on CASIA-B and OU-MVLP datasets show the effectiveness of our method and some visualization for ex-tracted motion are done to show the interpretability of our motion extraction module. 1.

Introduction
Gait is a biometric presenting the walking pattern of pedestrian for identity recognition and has an edge over other biometrics such as face, iris or fingerprint since it can be recognized without touch and at a distance. Although it has been studied for years, there are still some challenges in gait recognition. For example, variations like carrying conditions [4, 15, 16, 42, 46], coat-wearing and viewpoint differences [41, 45] may cause changes in gait appearance and make it hard to distinguish pedestrian.
Existing appearance-based approaches for gait recogni-tion rely heavily on the visual appearance of silhouettes.
However, when the view angle is close, the appearance dif-ference between two different person can be smaller than that of the same people but viewed from two different an-gles.
*Corresponding author.
Figure 1. Three samples from CASIA-B dataset, where A and B denote ID 39 and ID 77 respectively. A-1 and A-2 are two samples of A selected from different sequences. It can be found that it is difficult to find the difference between A and B visually. Even on some frames, A-2 are more similar to B-1 than A-1.
A common approach to address the aforementioned is-sue is learning viewpoint-invariant or robust features [6, 10, 20, 21, 35, 37, 38, 43]. However, these works focus on how to extract apparent information and the fusion of spatial or temporal features. The detection or estimation of viewpoint is overlook and there is few model explicitly making use of viewpoint. In other words, the viewpoint-robustness of these methods is solely based on the coverage of data, which is a well-known ill-posed problem.
Even when the viewpoint is close, the apparent infor-mation is still not very reliable. As shown in Figure 1, it is difficult to distinguish the identity of the three sam-ples only from the body shape.This phenomenon explains why pure appearance-based method such as Gait Energy
Images (GEIs) [34] cannot achieve ideal performance. The similar situation will also occur in state-of-the-art Gaitset method [6] which does not use temporal information either.
We argue that the ultimate solution to the problem shown in Figure 1 is the gait motion. Recently, some methods mak-ing use of temporal features are proposed [7, 20, 20, 28, 39].
Although these models show a stronger edge in recognition accuracy, They do not discuss the motion information in gait to the extent that some discriminatory biological infor-mation may be missing.
In this paper, by mathematical modeling analysis, we ar-gue that it is difficult to distinguish people using only first-ods are greatly affected by the accuracy of pose estimation.
Pedestrian pose estimation itself still remains a challenging problem [14, 18], especially for cross-domain pose estima-tion [44], which is a closer scenario to gait recognition.
Nowadays, with the development of deep learning, the performance of appearance-based methods have made a greater breakthrough. Wu et al. [37] and Chao et al. [6] pro-posed networks suitable for gait recognition firstly. Wolf et al. [36], Lin et al. [20] and Huang et al. [12] used three dimensional convolution on gait recognition. Fan et al. [7] and Huang et al. [11] take temporal models into considera-tion.
View-invariant Modeling Viewpoint change is a chal-lenging problem in biometrics including face recognition and gait recognition. Compared with face, there are fewer methods take view into consideration in gait recognition.
He et al. [9] proposed a multi-task GAN and use view la-bels as the supervision to train the GAN. Chai et al. [5] take different projection matrices as view embedding methods and approach high growth on several backbones. However, these models are complex and have too many parameters.
Optical Flow and Motion Optical flow is one repre-sentation of motion and optical flow estimation is a task which predicts the pixel-to-pixel correspondence between two adjacent frames. Recently, many deep learning meth-ods [29, 40] are used for optical flow estimation. Among these methods, RAFT [32] is the one with perfect perfor-mance and the fastest speed now. Optical flow has been used in many areas including action recognition [3, 26] and video generation [1]. 3. Why second-order motion?
Gait is recognized as the walking pattern that can dis-In the early years However, tinguish pedestrian [23]. appearance-based methods with convolutional neural net-works now mainly focus on the two dimensional feature of silhouettes. Even Gaitset [6], which is one of the-state-of-the-arts, does not rely on any temporal feature. It is hard to prove whether current state-of-the-art methods depends on human body shape or traditional “gait”. In the early years, some methods [8, 27, 30] have explored the effects of mo-tion as well as acceleration (second-order motion) on gait recognition, but they have not looked deeper into the theory and the physics behind it.
Therefore, in order to explore the essential information, we propose to use the Lagrange’s Equation [13] to analyze the walking of human. As shown in Figure 3, we assume that the human thighs and legs are rigid and model them mechanically. The length and mass of the two thighs and two legs are denoted as l1, l2, m1, m2 and l3, l4, m3, m4 re-spectively. θi represents the angle between them and verti-cal lines. Also, the human body is assumed to move forward with a small distance x.
Figure 2. Overview of our multi-branch framework. order temporal information 1. To effectively modeling the walking pattern of a pedestrian, second-order motion is nec-essary. To verify this idea, a novel motion-assisted gait recognition method is proposed. To further reduce the neg-ative impact of viewpoint difference, a view-aware embed-ding method is also introduced. It results in a multi-branch framework which combined the view, appearance and in-trinsic motion of silhouette sequences. Experimental results show that the proposed model can effectively narrow the intra-class distance caused by view variance.
The major contribution of this paper can be summarized as the following four aspects:
• We model human walking by Lagrange’s Equation and come to the conclusion that we need to use second-order motion features to represent the gait in addition to the first-order motion features.
• Based on the conclusion of Lagrange motion analysis, we propose a second-order Motion Extraction Module to extract features on the high-level feature maps.
• We proposed a novel and light-weighted view embed-ding to narrow the difference caused by changes of view.
• We apply our proposed method to the widely used
CASIA-B and OU-MVLP datasets and the effective-ness of our method is verified. Some visualizations are conducted to further prove the validity of our idea. 2.