Abstract
Effective semi-supervised learning (SSL) in medical im-age analysis (MIA) must address two challenges: 1) work effectively on both multi-class (e.g., lesion classification) and multi-label (e.g., multiple-disease diagnosis) problems, and 2) handle imbalanced learning (because of the high variance in disease prevalence). One strategy to explore in SSL MIA is based on the pseudo labelling strategy, but it has a few shortcomings. Pseudo-labelling has in general lower accuracy than consistency learning, it is not specifi-cally design for both multi-class and multi-label problems, and it can be challenged by imbalanced learning. In this pa-per, unlike traditional methods that select confident pseudo label by threshold, we propose a new SSL algorithm, called anti-curriculum pseudo-labelling (ACPL), which introduces novel techniques to select informative unlabelled samples, improving training balance and allowing the model to work for both multi-label and multi-class problems, and to esti-mate pseudo labels by an accurate ensemble of classifiers (improving pseudo label accuracy). We run extensive ex-periments to evaluate ACPL on two public medical image classification benchmarks: Chest X-Ray14 for thorax dis-ease multi-label classification and ISIC2018 for skin lesion multi-class classification. Our method outperforms previ-ous SOTA SSL methods on both datasets12. 1.

Introduction
Deep learning has shown outstanding results in medical image analysis (MIA) [24, 34, 35]. Compared to computer vision, the labelling of MIA training sets by medical experts is significantly more expensive, resulting in low availability of labelled images, but the high availability of unlabelled
*First two authors contributed equally to this work. 1Supported by Australian Research Council through grants
DP180103232 and FT190100525. 2Code is available at https://github.com/FBLADL/ACPL (a) Diagram of our ACPL (top) and traditional pseudo-label SSL (bottom) (b) Imbalanced distribution on multi-label Chest X-ray14 [39] (left) and multi-class ISIC2018 [36] (right)
Figure 1. In (a), we show diagrams of the proposed ACPL (top) and the traditional pseudo-label SSL (bottom) methods, and (b) displays histograms of images per label for the multi-label Chest
X-ray14 [39] (left) and multi-class ISIC2018 [36] (right). images from clinics and hospitals databases can be explored in the modelling of deep learning classifiers. Furthermore, differently from computer vision problems that tend to be mostly multi-class and balanced, MIA has a number of multi-class (e.g., a lesion image of a single class) and multi-label (e.g., an image from a patient can contain multiple diseases) problems, where both problems usually contain severe class imbalances because of the variable prevalence of diseases (see Fig. 1-(b)). Hence, MIA semi-supervised learning (SSL) methods need to be flexible enough to work with multi-label and multi-class problems, in addition to handle imbalanced learning.
State-of-the-art (SOTA) SSL approaches are usually based on the consistency learning of unlabelled data [5, 6, 32] and self-supervised pre-training [25]. Even though consistency-based methods show SOTA results on multi-class SSL problems, pseudo-labelling methods have shown
better results for multi-label SSL problems [29]. Pseudo-labelling methods provide labels to confidently classified unlabelled samples that are used to re-train the model [22].
One issue with pseudo-labelling SSL methods is that the confidently classified unlabelled samples represent the least informative ones [30] that, for imbalanced problems, are likely to belong to the majority classes. Hence, this will bias the classification toward the majority classes and most likely deteriorate the classification accuracy of the minority classes. Also, selecting confident pseudo-labelled samples is challenging in multi-class, but even more so in multi-label problems. Previous papers [2, 29] use a fixed threshold for all classes, but a class-wise threshold that addresses imbal-anced learning and correlations between classes in multi-label problems would enable more accurate pseudo-label predictions. However, such class-wise threshold is hard to estimate without knowing the class distributions or if we are dealing with a multi-class or multi-label problem. Further-more, using the model output for the pseudo-labelling pro-cess can also cause confirmation bias [1], whereby the as-signment of incorrect pseudo-labels will increase the model confidence in those incorrect predictions, and consequently decrease the model accuracy.
In this paper, we propose the anti-curriculum pseudo-labelling (ACPL), which addresses multi-class and multi-label imbalanced learning SSL MIA problems. First, we introduce a new approach to select the most informative un-labelled images to be pseudo-labelled. This is motivated by our argument that there exists a distribution shift be-tween unlabelled and labelled samples for SSL. An effec-tive learning curriculum must focus on informative unla-belled samples that are located as far as possible from the distribution of labelled samples. As a result, these infor-mative samples are likely to belong to the minority classes in MIA imbalanced learning problems. Selecting these in-formative samples will naturally balance the training pro-cess and, given that they are selected before the pseudo-labelling process, we eliminate the need for estimating a class-wise classification threshold, facilitating our model to work well on multi-class and multi-label problems. The information content measure of an unlabelled sample is computed with our proposed cross-distribution sample in-formativeness that outputs how close an unlabelled sample is from the set of labelled anchor samples (anchor samples are highly informative labelled samples). Second, we intro-duce a new pseudo-labelling mechanism, called informative mixup, which combines the model classification with a K-nearest neighbor (KNN) classification guided by sample in-formativeness to improve prediction accuracy and mitigate confirmation bias. Third, we propose the anchor set pu-rification method that selects the most informative pseudo-labelled samples to be included in the labelled anchor set to improve the pseudo-labelling accuracy of the KNN classi-fier in later training stages.
To summarise, our ACPL approach selects highly infor-mative samples for pseudo-labelling (addressing MIA im-balanced classification problems and allowing multi-label multi-class modelling) and uses an ensemble of classifiers to produce accurate pseudo labels (tackling confirmation bias to improve classification accuracy), where the main technical contributions are:
• A novel information content measure to select infor-mative unlabelled samples named cross-distribution sample informativeness;
• A new pseudo-labelling mechanism, called informa-tive mixup, which generates pseudo labels from an ensemble of deep learning and KNN classifiers; and
• A novel method, called anchor set purification (ASP), to select informative pseudo-labelled samples to be included in the labelled anchor set to improve the pseudo-labelling accuracy of the KNN classifier.
We evaluate ACPL on two publicly available medical image classification datasets, namely the Chest X-Ray14 for tho-rax disease multi-label classification [39] and the ISIC2018 for skin lesion multi-class classification [8,36]. Our method outperforms the current SOTA methods in both datasets. 2.