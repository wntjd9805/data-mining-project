Abstract
The huge burden of computation and memory are two obstacles in ultra-high resolution image segmentation. To tackle these issues, most of the previous works follow the global-local refinement pipeline, which pays more atten-tion to the memory consumption but neglects the inference speed.
In comparison to the pipeline that partitions the large image into small local regions, we focus on infer-ring the whole image directly.
In this paper, we propose
ISDNet, a novel ultra-high resolution segmentation frame-work that integrates the shallow and deep networks in a new manner, which significantly accelerates the inference speed while achieving accurate segmentation. To further exploit the relationship between the shallow and deep fea-tures, we propose a novel Relational-Aware feature Fu-sion module, which ensures high performance and robust-ness of our framework. Extensive experiments on Deep-globe, Inria Aerial, and Cityscapes datasets demonstrate our performance is consistently superior to state-of-the-arts. Specifically, it achieves 73.30 mIoU with a speed of 27.70 FPS on Deepglobe, which is more accurate and 172 × faster than the recent competitor. Code available at https://github.com/cedricgsh/ISDNet. 1.

Introduction
Semantic segmentation is a basic task that has been stud-ied for decades. Unlike other vision tasks, such as image classification, segmentation needs to deal with small ob-jects and fine boundaries that heavily rely on large-scale in-put images [1, 9, 16, 17, 24, 30, 40]. Especially, ultra-high resolution image with millions or even billions of pixels plays a vital role in the fields of remote sensing [34, 41, 42],
*This work was done when S. Guo was an intern in Tencent Youtu Lab.
S. Guo and L. Liu have equal contribution. † Corresponding Authors. (a) Image (b) Ground Truth (c) Ours (d) Deep Network [1] (e) Shallow Network [11] (f) FCtL [22]
Figure 1. Comparison of different model predictions on the ultra-high resolution image. (a) Input image. (b) The corresponding ground truth. (c) Prediction of our efficient segmentation method. (d) Prediction of a deep network input with the downsampled im-age. (e) Prediction of a shallow network input with the full-scale input. (f)Prediction of the latest ultra-high resolution segmenta-tion method. Our method outperforms them in both speed and accuracy. autonomous driving [13, 25, 27], medical imagery applica-tions [15, 19, 28].
However, due to memory and computational limitations, general segmentation methods cannot well handle ultra-high resolution images input. Existing segmentation meth-ods mainly focus on designing a neural network architec-ture for regular resolution images, but overlook the feasi-bility of larger scale input. As shown in Figure 1 (d), a deep and complicated model [1] needs to downsample the input image to meet the memory and speed requirements, but some detailed information is discarded during down-sampling, resulting in poor performance. Although a shal-low and lightweight model [11] can be adapted to process
Figure 2. Comparison for the schemes for ultra-high resolution image segmentation. (a) Design a lightweight model architecture to fit large scale images. (b) Global inference with multiple local patches refinement. (c) Our method by integrating the shallow and deep networks for the input of entire and downsampled image. larger scale input as shown in Figure 1 (e), the performance is poor since it is hard to capture long-range and high-level semantic cues using a simple architecture.
Recently, some methods specially designed for ultra-high resolution segmentation tasks have been proposed [3, 4, 18, 22, 33]. These methods mainly follow the principle of global and local refinement. First, the entire image is in-put into the global network, and then the uncertain regions are refined through the local network multiple times. Al-though these methods require a lower memory consumption and reach higher accuracy in general, their inference speed is very slow. For example, Figure 1 (f) shows a recent method FCtL [22] that requires ∼8s to infer a image with 2448 × 2448 resolution and ∼26s for 5000 × 5000 resolu-tion, which is intolerable in most applications.
To address the above limitations, we aim at achieving a better balance among accuracy, memory, and inference speed for ultra-high resolution segmentation. Instead of the scheme of global and local refinement, we propose ISDNet, a novel framework that infers the segmentation for ultra-high resolution inputs end-to-end. Inspired by the bilateral architecture [36, 37] widely used in lightweight segmenta-tion model design, we proposed a framework to integrate shallow and deep networks for efficient segmentation. Dif-ferent from the typical bilateral models which combine a shallow and a deep branch for the same input to model the spatial and context features respectively, we propose to in-put a different scale of input for shallow and deep branches.
Besides, we empirically find that inputting heterogeneous information for shallow and deep branches and constructing an auxiliary learning task for another domain (e.g., super-resolution) can further help the training of our method.
For intuitive comparison, the prototype of three schemes for ultra-high resolution segmentation are shown in Fig-ure 2. In summary, the contributions of this paper include:
• We propose a novel framework to integrate shallow and deep networks for efficient ultra-high resolution image segmentation. Besides, we empirically observe that heterogeneous inputs can improve accuracy.
• We present a Relation-Aware feature Fusion (RAF) module , which fuses features from shallow and deep branches based on their relationship along with auxil-iary super-resolution and structure distillation losses to enhance the features learned from the deep branch.
• Extensive experiments show that our method achieves remarkable results on Deepglobe [7], Inria Aerial [26] and Cityscapes [6] datasets, while attaining both fast speed and low memory consumption in inference.
It is worth noting that our shallow and deep integration is a general framework, focusing particularly on efficient large scale segmentation, which can be leveraged to combine lots of general semantic segmentation networks including recent transformer based methods e.g. SegFormer [35]. 2.