Abstract
We present a novel self-taught framework for unsuper-vised metric learning, which alternates between predicting class-equivalence relations between data through a mov-ing average of an embedding model and learning the model with the predicted relations as pseudo labels. At the heart of our framework lies an algorithm that investigates con-texts of data on the embedding space to predict their class-equivalence relations as pseudo labels. The algorithm enables efficient end-to-end training since it demands no off-the-shelf module for pseudo labeling. Also, the class-equivalence relations provide rich supervisory signals for learning an embedding space. On standard benchmarks for metric learning, it clearly outperforms existing unsu-pervised learning methods and sometimes even beats super-vised learning models using the same backbone network.
It is also applied to semi-supervised metric learning as a way of exploiting additional unlabeled data, and achieves the state of the art by boosting performance of supervised learning substantially. 1.

Introduction the
Understanding similarities between data is at heart of many machine learning tasks such as data re-trieval [29, 37, 47, 48], face verification [35, 45], person re-identification [7, 56], few-shot learning [41, 46, 49], and representation learning [29, 52, 61]. Metric learning em-bodies the perception of similarity by learning an embed-ding space where the distance between a pair of data rep-resents their inverse semantic similarity. Also, the mapping from data to such a space is typically modeled by deep neu-ral networks.
Recent advances in metric learning rely heavily on super-vised learning using large-scale datasets. Manual annota-tion of such datasets is however costly, and thus could limit the class diversity of training data and in consequence the generalization capability of learned models. Unsupervised metric learning has been studied to resolve this issue. Exist-Figure 1. Accuracy in Recall@1 versus embedding dimension on the CUB-200-2011 [54] dataset using GoogleNet [50] backbone.
Superscripts denote embedding dimensions and â€  indicates super-vised learning methods. Our model with 128 embedding dimen-sions outperforms all previous arts using higher embedding dimen-sions and sometimes surpasses supervised learning methods. ing methods in this line of research mainly synthesize class information of training data by assigning a surrogate class per training instance [13, 55, 59, 60] or discovering pseudo classes through k-means clustering [3, 4, 26, 33, 34, 57], hi-erarchical clustering [57], or random walk [25]. Although these methods have demonstrated impressive results with-out using groundtruth labels in training, they often fail to capture intra-class variation [13, 55, 59, 60] or impose sub-stantial computational burden due to the off-the-shelf tech-niques [3, 4, 25, 26, 33, 34, 57].
In this paper, we propose a new method for unsupervised metric learning that addresses the aforementioned limita-tions of previous work and achieves the state of the art as shown in Fig. 1. The major contribution of this paper is two-fold. First, we introduce a novel, end-to-end self-taught metric learning framework (STML), whose overall pipeline is illustrated in Fig. 2. Unlike the existing works, it predicts class-equivalence relations between data within each mini-batch by self-exploration and leverages the pre-dicted relations as synthetic supervision for metric learning.
Our framework manages two embedding networks, namely
Figure 2. An overview of our STML framework. First, contextualized semantic similarity between a pair of data is estimated on the embedding space of the teacher network. The semantic similarity is then used as a pseudo label, and the student network is optimized by relaxed contrastive loss with KL divergence. Pink arrows represent backward gradient flows. Finally, the teacher network is updated by an exponential moving average of the student. The student network learns by iterating these steps a number of times, and its backbone and embedding layer in light green are considered as our final model. teacher and student whose backbone are initialized identi-cally. Class-equivalence between a pair of data is approxi-mately estimated as their semantic similarity on the embed-ding space of the teacher network. The predicted similarity is used as a soft pseudo label for learning the student model, and the teacher model is in turn updated by a momentum-based moving average of the student. Iterating this process evolves the student model progressively.
The success of STML depends heavily on the quality of predicted semantic similarities, and our second contribu-tion lies in the way of estimating semantic similarities us-ing contexts. Specifically, given a pair of data, we compute their semantic similarity considering the overlap of their contexts (i.e., neighborhoods in the embedding space) as well as their pairwise distance. We found that the contextu-alized semantic similarity approximates class-equivalence precisely. Further, since it has a real value indicating the degree of semantic similarity, it provides rich information beyond binary class-equivalence. Also, since it requires no external module nor memory banks [20, 31], it makes
STML efficient and concise. To further enhance the quality of predicted semantic similarity, we design the teacher net-work to learn a higher dimensional embedding space than the student counterpart. This asymmetric design of the two networks allows the teacher to provide more effective su-pervision thanks to its improved expression power while the student, i.e., our final model, remains compact.
To the best of our knowledge, STML is the only un-supervised metric learning method that can consider se-mantic relations between data in end-to-end training with-out introducing off-the-shelf techniques. Compared to the instance-level surrogate classes [13, 55, 59, 60], pseudo la-bels generated and exploited by STML are more appropriate to capture semantic relations between data since they indi-cate class-equivalence relations. Also, unlike previous work based on pseudo labeling [3, 4, 25, 26, 33, 57], STML em-ploys no external algorithm and thus allows training to be efficient, end-to-end, and less sensitive to hyper-parameters.
Further, it is naturally applied to semi-supervised metric learning [14] as well as unsupervised metric learning.
We first evaluate STML on standard benchmarks for metric learning [32, 48, 54], where it largely outper-forms existing unsupervised learning methods. Surpris-ingly, sometimes it even beats some of supervised learn-ing models using the same backbone network as shown in Fig. 1. Beyond that, its efficacy is demonstrated on two benchmarks for semi-supervised metric learning [14], where it substantially outperforms previous work as well. 2.