Abstract
Anomaly detection is commonly pursued as a one-class classiﬁcation problem, where models can only learn from normal training samples, while being evaluated on both normal and abnormal test samples. Among the success-ful approaches for anomaly detection, a distinguished cat-egory of methods relies on predicting masked information (e.g. patches, future frames, etc.) and leveraging the recon-struction error with respect to the masked information as an abnormality score. Different from related methods, we propose to integrate the reconstruction-based functionality into a novel self-supervised predictive architectural build-ing block. The proposed self-supervised block is generic and can easily be incorporated into various state-of-the-art anomaly detection methods. Our block starts with a con-volutional layer with dilated ﬁlters, where the center area of the receptive ﬁeld is masked. The resulting activation maps are passed through a channel attention module. Our block is equipped with a loss that minimizes the reconstruc-tion error with respect to the masked area in the recep-tive ﬁeld. We demonstrate the generality of our block by integrating it into several state-of-the-art frameworks for anomaly detection on image and video, providing empiri-cal evidence that shows considerable performance improve-ments on MVTec AD, Avenue, and ShanghaiTech. We re-lease our code as open source at: https://github.com/ ristea/sspcab. 1.

Introduction
Anomaly detection is an important task with a broad set of applications ranging from industrial inspection (ﬁnd-ing defects of objects or materials on industrial production lines) [5, 7, 10, 15, 36, 56, 62, 76] to public security (de-tecting abnormal events such as trafﬁc accidents, ﬁghts, explosions, etc.) [12, 13, 17–19, 27, 28, 33, 39, 41, 47–50, 52, 67, 72, 73, 77, 78]. The task is typically framed as a one-class classiﬁcation (outlier detection) problem, where methods [2, 8, 12, 21, 25, 27, 29, 33, 35, 37, 40, 43–45, 49–
∗corresponding author: raducu.ionescu@gmail.com 51, 53, 54, 57, 69, 73, 75, 81, 82] learn a familiarity model from normal training samples, labeling unfamiliar exam-ples (outliers) as anomalies, at inference time. Since ab-normal samples are available only at test time, supervised learning methods are not directly applicable to anomaly de-tection. To this end, researchers turned their attention to other directions such as reconstruction-based approaches
[15, 19, 21, 36, 37, 43, 47, 49, 54, 62, 69, 71], dictionary learning methods [7–9, 14, 40, 55], distance-based models
[6, 10, 25, 27, 50, 51, 53, 57, 58, 63, 65, 68, 70], change de-tection frameworks [11, 26, 38, 48], and probabilistic mod-els [1, 2, 16, 23, 29, 44, 45, 56, 61, 74].
A distinguished subcategory of reconstruction methods relies on predicting masked information, leveraging the re-construction error with respect to the masked information as an abnormality score. The masked information can come in different forms, e.g. superpixels [36], future frames [37], middle bounding boxes [17], among others. Methods in this subcategory mask some part of the input and employ a deep neural network to predict the missing input informa-tion. Different from such methods, we propose to integrate the capability of reconstructing the masked information into a neural block. Introducing the reconstruction task at a core architectural level has two important advantages: (i) it al-lows us to mask information at any layer in a neural network (not only at the input), and (ii) it can be integrated into a wide range of neural architectures, thus being very general.
We design our reconstruction block as a self-supervised predictive block formed of a dilated convolutional layer and a channel attention mechanism. The dilated ﬁlters are based on a custom receptive ﬁeld, where the center area of the ker-nel is masked. The resulting convolutional activation maps are then passed through a channel attention module [24].
The attention module ensures the block does not simply learn to reconstruct the masked region based on linearly in-terpolating contextual information. Our block is equipped with a loss that minimizes the reconstruction error between the ﬁnal activation maps and the masked information. In other words, our block is trained to predict the masked in-formation in a self-supervised manner. Our self-supervised
Figure 1. Our self-supervised predictive convolutional attentive block (SSPCAB). For each location where the dilated convolutional ﬁlter is applied, the block learns to reconstruct the masked area using contextual information. A channel attention module performs feature recalibration by using global information to selectively emphasize or suppress reconstruction maps. Best viewed in color. predictive convolutional attentive block (SSPCAB) is illus-trated in Figure 1. For each location where the dilated con-volutional ﬁlter is applied, the block learns to reconstruct the masked area using contextual information. Meanwhile, the dilation rate becomes a natural way to control the con-text level (from local to global), as required for the speciﬁc application.
We integrate SSPCAB into various state-of-the-art anomaly detection frameworks [18, 34, 37, 39, 49, 79] and conduct comprehensive experiments on the MVTec AD [5],
Avenue [40] and ShanghaiTech [43] data sets. Our empiri-cal results show that SSPCAB can bring signiﬁcant perfor-mance improvements, e.g. the region-based detection crite-rion (RBDC) of Liu et al. [39] on Avenue increases from 41% to 62% by adding SSPCAB. Moreover, with the help of SSPCAB, we are able to report new state-of-the-art per-formance levels on Avenue and ShanghaiTech. Addition-ally, we show extra results on the Avenue data set, indicat-ing that the masked convolutional layer can also increase performance levels, all by itself.
In summary, our contribution is twofold:
• We introduce a novel self-supervised predictive con-volutional attentive block that is inherently capable of performing anomaly detection.
• We integrate the block into several state-of-the-art neu-ral models [18,34,37,39,49,79] for anomaly detection, showing signiﬁcant performance improvements across multiple models and benchmarks. 2.