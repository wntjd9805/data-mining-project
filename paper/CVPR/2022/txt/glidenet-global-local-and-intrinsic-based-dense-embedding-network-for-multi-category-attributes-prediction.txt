Abstract 1.

Introduction
Attaching attributes (such as color, shape, state, action) to object categories is an important computer vision prob-lem. Attribute prediction has seen exciting recent progress and is often formulated as a multi-label classification prob-lem. Yet significant challenges remain in: 1) predicting a large number of attributes over multiple object categories, 2) modeling category-dependence of attributes, 3) method-ically capturing both global and local scene context, and 4) robustly predicting attributes of objects with low pixel-count. To address these issues, we propose a novel multi-category attribute prediction deep architecture named Gli-deNet, which contains three distinct feature extractors. A global feature extractor recognizes what objects are present in a scene, whereas a local one focuses on the area sur-rounding the object of interest. Meanwhile, an intrinsic feature extractor uses an extension of standard convolution dubbed Informed Convolution to retrieve features of objects with low pixel-count utilizing its binary mask. GlideNet then uses gating mechanisms with binary masks and its self-learned category embedding to combine the dense embed-dings. Collectively, the Global-Local-Intrinsic blocks com-prehend the scene’s global context while attending to the characteristics of the local object of interest. The architec-ture adapts the feature composition based on the category via category embedding. Finally, using the combined fea-tures, an interpreter predicts the attributes, and the length of the output is determined by the category, thereby re-moving unnecessary attributes. GlideNet can achieve com-pelling results on two recent and challenging datasets –
VAW and CAR – for large-scale attribute prediction. For in-stance, it obtains more than 5% gain over state of the art in the mean recall (mR) metric. GlideNet’s advantages are es-pecially apparent when predicting attributes of objects with low pixel counts as well as attributes that demand global context understanding. Finally, we show that GlideNet ex-cels in training starved real-world scenarios. more info at http://signal.ee.psu.edu/research/glidenet.html
To fully comprehend a scene, one should not only be able to detect the objects in the scene but also understand the at-tributes (properties) of each object detected. Even if two objects belong to the same category, their behavior might vary depending on their attributes. For example, we can’t predict the route of a driving vehicle based on a still 2D im-age alone, unless we know the vehicle’s heading/direction and if the vehicle is parked or not. Accurate classification of objects and their attributes is critical in numerous applica-tions of computer vision and pattern recognition such as au-tonomous driving where a thorough grasp of the surround-ings is essential for safe driving decisions. In order to drive safely, a driver must be able to predict numerous crucial as-pects. They include, among other things, the activities of other drivers and pedestrians, the slipperiness of the road surface, the weather, traffic signs and their contents, and pedestrian behavior.
Attributes are often defined as semantic (visual) descrip-tions of objects in a scene. An object’s semantic informa-tion includes how it looks (color, size, shape, etc.), inter-acts with surroundings, and behaviors. The category of an object, in general, determines the set of possible attributes that it can have. For instance, a table might have attributes related to shape, color, and material. However, a human will have a more complicated set of attributes related to age, gender, and activity status (sitting, standing, walking, etc.).
Some properties, such as the visible proportion of an ob-ject, may exist across multiple categories. Therefore, to accurately predict an object’s attributes, we must consider the following: 1) some attributes are unique to certain cat-egories, 2) some categories may share the same attribute, 3) some attributes require a global understanding of the en-tire scene and 4) some attributes are inherent to the object of interest.
In this paper, we present a new algorithm –
Global, Local and Intrinsic based Dense Embedding Net-work (GlideNet) – to tackle the attribute prediction prob-lem. GlideNet is capable of addressing the aforementioned listed concerns while also predicting a variety of categories.
Earlier methods for object detection and classification re-lied heavily on tailored or customized features that are ei-ther generated by ORB [58], SIFT [42], HOG [11] or other descriptors. Then, the extracted features pass through a sta-tistical or learning module – such as CRF[28] – to find the relation between the extracted features from the descrip-tor and the desired output. Recently, Convolutional Neu-ral Networks (CNN) have proven their capability in extract-ing better features that ease the following step of classi-fication and detection. This has been empirically proven in various fields, such as in object classification [32, 18], object detection [15, 54] and inverse image problems such as dehazing [44, 73], denoising [39, 56], HDR estimation
[40, 45, 9], etc. Deep learning with CNN typically re-quires a large amount of data for training and regularization
[5, 70, 2, 53, 6]. Classical methods [4, 13] for predicting at-tributes may require less data, however they perform worse than deep learning based techniques.
In this work, we present a new deep learning approach
GlideNet for attributes prediction that is capable of incorpo-rating problem (dataset) specific characteristics. Our main contributions can be summarized as follows:
• We employ three distinct feature-extractors; each has a specific purpose. Global Feature Extractor (GFE) captures global information, which encapsulates infor-mation about different objects in the image (their lo-cations and category type). Local Feature Extractor (LFE) captures local information, which encapsulates information related to attributes of the object as well as its category and binary mask. Lastly, Instance Feature
Extractor (IFE) encapsulates information about the in-trinsic attributes of objects. It ensures that we estimate characteristics solely from the object’s pixels, exclud-ing contributions from other pixels.
• We use a novel convolution layer (named Informed
Convolution) in the IFE to focus on intrinsic informa-tion of the object related to the attributes prediction.
• To learn appropriate weights for each Feature Extrac-tor (FE), we employ a self-attention technique. Uti-lizing binary mask and a self-learned category embed-ding, we generate a “Description” Then we use a gat-ing mechanism to fine-tune each feature layer’s spatial contributions.
• We employ a multi-head technique for the final clas-sification stage for two reasons. First, it ensures that the final classification step’s weights are determined by the category. Second, the length of the final output can vary depending on the category. This is significant since not every category has the same set of attributes.
The term “class” can be confusing because it can refer to the object’s type (vehicle, pedestrian, etc.) or the value of one of the object’s attributes (parked, red, etc). As a result, we avoid using the term “class” throughout the work. We use the word “category” to refer to the object’s type and the word “attribute” for one of the semantic descriptions of that object. In addition, we use uppercase letters X to denote images or 2D spatial features, lowercase bold letters x for 1D features, and lowercase non-bold letters x for scalars, a hat accent over a letter ˆx to denote an estimated value and calligraphic letters X to denote either a mathematical operation or a building block in GlideNet’s architecture. 2.