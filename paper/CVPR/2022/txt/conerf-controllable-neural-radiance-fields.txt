Abstract
We extend neural 3D representations to allow for intu-itive and interpretable user control beyond novel view ren-dering (i.e. camera control). We allow the user to annotate which part of the scene one wishes to control with just a small number of mask annotations in the training images.
Our key idea is to treat the attributes as latent variables that are regressed by the neural network given the scene en-coding. This leads to a few-shot learning framework, where attributes are discovered automatically by the framework, when annotations are not provided. We apply our method to various scenes with different types of controllable attributes (e.g. expression control on human faces, or state control in movement of inanimate objects). Overall, we demonstrate, to the best of our knowledge, for the ﬁrst time novel view and novel attribute re-rendering of scenes from a single video. 1.

Introduction
Neural radiance ﬁeld (NeRF) [30] methods have recently gained popularity thanks to their ability to render photore-alistic novel-view images [28, 35, 36, 50]. In order to widen the scope to other possible applications, such as digital me-dia production, a natural question is whether these meth-ods could be extended to enable direct and intuitive con-trol by a digital artist, or even a casual user. However, cur-rent techniques only allow coarse-grain controls over ma-terials [53], color [18], or object placement [48], or only support changes that they are designed to deal with, such as shape deformations on a learned shape space of chairs [25], or are limited to facial expressions encoded by an explicit face model [12]. By contrast, we are interested in ﬁne-grained control without limiting ourselves to a speciﬁc class of objects or their properties. For example, given a self-portrait video, we would like to be able to control individ-ual attributes (e.g. whether the mouth is open or closed); see Figure 1. We would like to achieve this objective with minimal user intervention, without the need of specialized capture setups [24].
However, it is unclear how ﬁne-grained control can be achieved, as current state-of-the-art models [36] encode the structure of the 3D scene in a single and not interpretable la-tent code. For the example of face manipulation, one could attempt to resolve this problem by providing dense supervi-sion by matching images to the corresponding Facial Action
Coding System (FACS) [11] action units. Unfortunately, this would require either an automatic annotation process or careful and extensive per-frame human annotations, mak-ing the process expensive, generally unwieldy, and, most importantly, domain-speciﬁc. Automated tools for domain-agnostic latent disentanglement are a very active topic of research in machine learning [9, 16, 17], but no effective plug-and-play solution exists yet.
Conversely, we borrow ideas from 3D morphable mod-els (3DMM) [7], and in particular to recent extensions that achieve local control by spatial disentanglement of con-trol attributes [32, 46]. Rather than having a single global code controlling the expression of the entire face, we would like to have a set of local “attributes”, each controlling the corresponding localized appearance; more speciﬁcally, we assume spatial quasi-conditional independence of at-tributes [46]. For our example in Figure 1, we seek an attribute capable to control the appearance of the mouth, another to control the appearance of the eye, etc.
Thus, we introduce a learning framework denoted CoN-eRF (i.e. Controllable NeRF) that is capable of achiev-ing this objective with just few-shot supervision. As il-lustrated in Figure 1, given a single one-minute video, and with as little as two annotations per attribute, CoN-eRF allows ﬁne-grained, direct, and interpretable control over attributes. Our core idea is to provide, on top of the ground truth attribute tuple, sparse 2D mask annotations that specify which region of the image an attribute controls, in spirit of Interactive Digital Photomontage [4] and Video
Sprites [39]. Further, by treating attributes as latent vari-ables within the framework, the mask annotations can be au-tomatically propagated to the whole input video. Thanks to the quasi-conditional independence of attributes, our tech-nique allows us to synthesize expressions that were never seen at training time; e.g. the input video never contained a frame where both eyes were closed and the actor had a smiling expression; see Figure 1 (green box).
Contributions. To summarize, our CoNeRF method1:
• provides direct, intuitive, and ﬁne-grained control over 3D neural representations encoded as NeRF;
• achieves this via few-shot supervision, e.g., just a handful of annotations in the form of attribute values and corre-sponding 2D mask are needed for a one minute video;
• while inspired by domain-speciﬁc facial animation re-search [46], it provides a domain-agnostic technique. 2.