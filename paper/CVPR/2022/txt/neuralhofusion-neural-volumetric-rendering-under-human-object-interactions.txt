Abstract 4D modeling of human-object interactions is critical for numerous applications. However, efficient volumetric cap-ture and rendering of complex interaction scenarios, es-pecially from sparse inputs, remain challenging.
In this paper, we propose NeuralHOFusion, a neural approach for volumetric human-object capture and rendering using sparse consumer RGBD sensors.
It marries traditional non-rigid fusion with recent neural implicit modeling and blending advances, where the captured humans and objects are layer-wise disentangled. For geometry modeling, we propose a neural implicit inference scheme with non-rigid key-volume fusion, as well as a template-aid robust object tracking pipeline. Our scheme enables detailed and com-plete geometry generation under complex interactions and occlusions. Moreover, we introduce a layer-wise human-object texture rendering scheme, which combines volumet-ric and image-based rendering in both spatial and temporal domains to obtain photo-realistic results. Extensive exper-iments demonstrate the effectiveness and efficiency of our approach in synthesizing photo-realistic free-view results under complex human-object interactions. 1.

Introduction
Human-centric 4D content generation enables numerous applications for VR/AR, telepresence and education. How-ever, conveniently reconstructing and rendering human ac-tivities under human-object interactions remain unsolved.
Early high-end solutions [5,7,12,13,20,29] require dense cameras and custom-designed lighting conditions for high-fidelity reconstruction. But such a complicated and expen-sive system setup is undesirable for consumer-level usage.
Light-weight volumetric performance capture is more prac-tical and attractive. Early solutions [14, 21, 22, 51] rely on pre-scanned templates which are unsuitable for on-the-fly human-object interaction modeling. The volumetric fu-Figure 1. Our NeuralHOFusion achieves layer-wise and photo-realistic reconstruction results, using only 6 RGBD cameras. sion approaches Fusion4D [9] and Motion2Fusion [8] fur-ther reconstruct complex human-object interaction scenes with topology changes in real-time. But they heavily rely on high-quality depth sensors and up to 9 high-end
GPUs, which are infeasible for consumer usage. Besides, the low-end fusion approaches [34, 43, 44, 57, 63] adopt the most handy monocular setup with a temporal fusion pipeline [35], but suffer from the inherent self-occlusion constraint. Moreover, the appearance results of the fusion methods are restricted by the limited geometry resolution.
Recent learning-based techniques enable robust human modeling from only light-weight inputs. In particular, vari-ous approaches [40,41,48] utilize implicit function to model human geometry, which is also widely adopted in the vol-umetric capture pipeline [25, 26, 44, 62]. But these meth-ods are restricted to only human without modeling human-object interactions, let alone generating compelling photo-realistic texture. Similarly, despite the progress for realis-tic human rendering [30, 32, 33, 48, 56], few researchers ex-plore the neural rendering strategies for human-object inter-actions, especially under the volumetric capture framework.
On the other hand, various researchers [16,17,38,50,64–67] model the interactions between humans and the surrounding objects or environments. But they only recover the paramet-ric human model rather than reconstructing and rendering the interaction scenes. Only recently, a few methods [45,47] explicitly model human and object simultaneously in the
volumetric capture framework. But they still cannot handle the interaction scenes, which highly limit the practicality.
In this paper, we present NeuralHOFusion – a neural volumetric human-object capture and rendering system us-ing light-weight consumer RGBD sensors (see Fig. 1 for overview). In stark contrast with existing systems, our ap-proach handles various complex human-object interaction scenarios and even multi-person interactions.
It achieves photo-realistic layer-wise geometry and texture rendering in novel views for both the performers and interacted objects.
Generating such a human-object free-viewpoint video with the layer-wise visual effect whilst maintaining light-weight and efficient setting is non-trivial. Our key idea is to organically combine traditional volumetric non-rigid fusion pipeline with recent neural implicit modeling and blend-ing advances, besides embracing a layer-wise scene decou-pling strategy. To this end, we first utilize off-the-shelf instance segmentation approach to distinguish the human and object from the six RGBD streams. For human recon-struction, we propose a fusion-based neural implicit scheme to reason about the human-only geometry details in novel views. Specifically, it combines pixel-aligned features with an occlusion-aware truncated projective SDF (TSDF) fea-ture [62], by utilizing a traditional key-volume non-rigid fusion pipeline [9, 57] in a human-only manner. Such a key-volume fusion-based implicit scheme handles occlu-sions effectively. For object reconstruction, inspired by the recent work [45], we adopt a template-aid robust ob-ject tracking pipeline with a specific initialization process for the following neural blending. Finally, based on the human-object geometry proxy above, we propose a layer-wise neural blending scheme to disentangle human and ob-ject for photo-realistic performance rendering. For the hu-man phase, we combine the image-based rendering with the traditional per-vertex texturing using albedo volume [59], through occlusion-aware blending weight learning. It en-ables accurate human appearance rendering in the target view with the level of texture detail in the spatially adjacent input. For the object rendering, we extend the spatial neural blending into the temporal domain, which learns the blend-ing weight from both the spatial and temporal candidate in-put views for photo-realistic rendering. To summarize, our main contributions include:
• We present the first neural volumetric capture and ren-dering system for human-object interaction scenarios using light-weight consumer RGBD sensors.
• We propose a fusion-based neural implicit inference scheme for detail-preserved human-object reconstruc-tion in an occlusion-aware manner.
• We introduce a layer-wise neural rendering scheme, which combines volumetric and image-based render-ing in both spatial and temporal domains. 2.