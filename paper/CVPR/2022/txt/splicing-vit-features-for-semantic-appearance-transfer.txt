Abstract 1.

Introduction
We present a method for semantically transferring the visual appearance of one natural image to another. Specif-ically, our goal is to generate an image in which objects in a source structure image are “painted” with the visual appearance of their semantically related objects in a tar-get appearance image. Our method works by training a generator given only a single structure/appearance image pair as input. To integrate semantic information into our framework—a pivotal component in tackling this task—our key idea is to leverage a pre-trained and ﬁxed Vision Trans-former (ViT) model which serves as an external semantic prior. Speciﬁcally, we derive novel representations of struc-ture and appearance extracted from deep ViT features, un-twisting them from the learned self-attention modules. We then establish an objective function that splices the de-sired structure and appearance representations, interweav-ing them together in the space of ViT features. Our frame-work, which we term “Splice”, does not involve adversar-ial training, nor does it require any additional input infor-mation such as semantic segmentation or correspondences, and can generate high resolution results, e.g., work in HD.
We demonstrate high quality results on a variety of in-the-wild image pairs, under signiﬁcant variations in the number of objects, their pose and appearance.
“Rope splicing is the forming of a semi-permanent joint between two ropes by partly untwisting and then inter-weaving their strands.” [2]
What is required to transfer the visual appearance between two semantically related images? Consider for example the task of transferring the visual appearance of a spotted cow in a ﬂower ﬁeld to an image of a red cow in a grass ﬁeld (Fig. 1). Conceptually, we have to associate regions in both images that are semantically related, and transfer the visual appearance between these matching regions. Additionally, the target appearance has to be transferred in a realistic man-ner, while preserving the structure of the source image – the red cow should be realistically ”painted” with black and white spots, and the green grass should be covered with yel-lowish colors. To achieve it under noticeable pose, appear-ance and shape differences between the two images, seman-tic information is imperative.
Indeed, with the rise of deep learning and the ability to learn high-level visual representations from data, new vi-sion tasks and methods under the umbrella of “visual ap-pearance transfer” have emerged. For example, the image-to-image translation trend aims at translating a source image from one domain to another target domain. To achieve that, most methods use generative adversarial networks (GANs), 1
given image collections from both domains. Our goal is different – rather than generating some image in a target domain, we generate an image that depicts the visual ap-pearance of a particular target image, while preserving the structure of the source image. Furthermore, our method is trained using only a single image pair as input, which allows us to deal with scenes and objects for which an image col-lection from each domain is not handy (e.g., spotted cows and red cows image collections).
With only a pair of images available as input, how can we source semantic information? We draw inspiration from
Neural Style Transfer (NST) that represents content and an artistic style in the space of deep features encoded by a pre-trained classiﬁcation CNN model (e.g., VGG). While
NST methods have shown a remarkable ability to globally transfer artistic styles, their content/style representations are not suitable for region-based, semantic appearance transfer across objects in two natural images [12]. Here, we pro-pose novel deep representations of appearance and structure that are extracted from DINO-ViT – a Vision Transformer model that has been pre-trained in a self-supervised man-ner [4]. Representing structure and appearance in the space of ViT features allows us to inject powerful semantic in-formation into our method and establish a novel objective function that is used to train a generator using only the sin-gle input image pair.
DINO-ViT has been shown to learn powerful and mean-ingful visual representations, demonstrating impressive re-sults on several downstream tasks including image retrieval, object segmentation, and copy detection [4, 1]. However, the intermediate representations that it learns have not yet been fully explored. We thus ﬁrst strive to gain a better understanding of the information encoded in ViT features across layers. We do so by adopting “feature inversion” visualization techniques previously used in the context of
CNN features. Our study provides a couple of key obser-vations: (i) the global token (a.k.a [CLS] token) provides a powerful representation of visual appearance, which cap-tures not only texture information but more global informa-tion such as object parts, and (ii) the original image can be reconstructed from the deepest features, yet they provide powerful semantic information at high spatial granularity.
Equipped with the above observations, we derive novel representations of structure and visual appearance extracted from deep ViT features – untwisting them from the learned self-attention modules. Speciﬁcally, we represent visual ap-pearance via the global [CLS] token, and represent struc-ture via the self-similarity of keys, all extracted from the last layer. We then train a generator on a single input pair of structure/appearance images, to produce an image that splices the desired visual appearance and structure in the space of ViT features. Our framework does not require any additional information such as semantic segmentation and does not involve adversarial training. Furthermore, our model can be trained on high resolution images, produc-ing high quality results in HD. We demonstrate a variety of semantic appearance transfer results across diverse natural image pairs, containing signiﬁcant variations in the number of objects, pose and appearance. 2.