Abstract
Metric learning aims to learn a highly discriminative model encouraging the embeddings of similar classes to be close in the chosen metrics and pushed apart for dis-similar ones. The common recipe is to use an encoder to extract embeddings and a distance-based loss function to match the representations – usually, the Euclidean dis-tance is utilized. An emerging interest in learning hyper-bolic data embeddings suggests that hyperbolic geometry can be beneficial for natural data. Following this line of work, we propose a new hyperbolic-based model for metric learning. At the core of our method is a vision transformer with output embeddings mapped to hyperbolic space. These embeddings are directly optimized using modified pairwise cross-entropy loss. We evaluate the proposed model with six different formulations on four datasets achieving the new state-of-the-art performance. The source code is available at https://github.com/htdt/hyp_metric. 1.

Introduction
Metric learning task formulation is general and intu-the obtained distances between data embeddings itive: must represent semantic similarity.
It is a typical cogni-tive task to generalize similarity for new objects given some examples of similar and dissimilar pairs. Metric learn-ing algorithms are widely applied in various computer vi-sion tasks: content-based image retrieval [32, 46, 47], near-duplicate detection [65], face recognition [27, 44], person re-identification [5, 63], as a part of zero-shot [47] or few-shot learning [38, 45, 50].
Modern image retrieval methods can be decomposed into roughly two components: the encoder mapping the image to its compact representation and the loss function govern-*Skolkovo Institute of Science and Technology ing the training process. Encoders with backbones based on transformer architecture have been recently proposed as a competitive alternative to previously used convolu-tional neural networks (CNNs). Transformers lack some of
CNN’s inductive biases, e.g., translation equivariance, re-quiring more training data to achieve a fair generalization.
On the other hand, it allows transformers to produce more general features, which presumably can be more beneficial for image retrieval [3,8], as this task requires generalization to unseen classes of images. To alleviate the issue above, several training schemes have been proposed: using a large dataset [7], heavily augmenting training dataset and using distillation [53], using self-supervised learning scenario [3].
The choice of the embedding space directly influences the metrics used for comparing representations. Typically, embeddings are arranged on a hypersphere, i.e. the output of the encoder is L2 normalized, resulting in using cosine similarity as a distance. In this work, we propose to con-sider the hyperbolic spaces. Their distinctive property is the exponential volume growth with respect to the radius, un-like Euclidean spaces with polynomial growth. This feature makes hyperbolic space especially suitable for embedding tree-like data due to increased representation power. The paper [42] shows that a tree can be embedded to Poincar´e disk with an arbitrarily low distortion. Most of the natu-ral data is intrinsically hierarchical, and hyperbolic spaces suit well for its representation. Another desirable property of hyperbolic spaces is the ability to use low-dimensional manifolds for embeddings without sacrificing the model ac-curacy and its representation power [34].
The goal of the loss function is straightforward: we want to group the representations of similar objects in the embed-ding space while pulling away representations of dissimilar objects. Most loss functions can be divided into two cat-egories: proxy-based and pair-based [23]. Additionally to the network parameters, the first type of losses trains prox-ies, which represent subsets of the dataset [32]. This proce-Figure 1. Overview of the proposed method. Two images representing one class (positives) are encoded with the vision transformer, projected into a space of a lower dimension with a fully connected (FC) layer, and then mapped to a hyperbolic space. Blue stars depict the resulting embeddings. Poincar´e disk is shown with uniform triangle tiling on the background to illustrate the manifold curvature. Gray circles represent other samples from the batch (negatives). Finally, arrows in the disk represent distances used in the pairwise cross-entropy loss. Positives are pushed closer to each other, negative are pulled far apart. dure can be seen from a perspective of a simple classifica-tion task: we train matching embeddings, which would clas-sify each subset [33]. At the same time, pair-based losses operate directly on the embeddings. The advantage of pair-based losses is that they can account for the fine-grained in-teractions of individual samples. Such losses do not require data labels: it is sufficient to have pair-based relationships.
This property is crucial for a widely used pairwise cross-entropy loss in self-supervised learning scenario [4, 17, 55].
Instead of labels, the supervision comes from a pretext task, which defines positive and negative pairs. Inspired by these works, we adopt pairwise cross-entropy loss for our experi-ments.
The main contributions of our paper are the following:
• We propose to project embeddings to the Poincar´e ball and to use the pairwise cross-entropy loss with hy-perbolic distances. Through extensive experiments, we demonstrate that the hyperbolic counterpart outper-forms the Euclidean setting.
• We show that the joint usage of vision transformers, hyperbolic embeddings, and pairwise cross-entropy loss provides the best performance for the image re-trieval task. 2. Method
We propose a new metric learning loss that combines representative expressiveness of the hyperbolic space and the simplicity and generality of the cross-entropy loss. The suggested loss operates in the hyperbolic space encourag-ing the representatives of one class (positives) to be closer while pushing the samples from other categories (negatives) away.
The schematic overview of the proposed method is de-picted at Figure 1. The remainder of the section is organized as follows. We start with providing the necessary prelimi-naries on hyperbolic spaces in Section 2.1, then we discuss the loss function in Section 2.2 and, finally, we briefly de-scribe the architecture and discuss pretraining schemes in
Section 2.5. 2.1. Hyperbolic Embeddings cgE, where λc =
Formally, the n-dimensional hyperbolic space Hn is a
Riemannian manifold of constant negative curvature. There exist several isometric models of hyperbolic space, in our c , gD) with work we stick to the Poincar´e ball model (Dn the curvature parameter c (the actual curvature value is then
−c2). This model is realized as a pair of an n-dimensinal ball Dn = {x ∈ Rn : c∥x∥2 < 1, c ≥ 0} equipped with the Riemannian metric gD = λ2 2 1−c∥x∥2 is the conformal factor and gE = In is Euclidean metric tensor. This means, that local distances are scaled by the factor λc approaching infinity near the boundary of the ball.
This gives rise to the ‘space expansion‘ property of hyper-bolic spaces. While, in the Euclidean spaces, the volume of an object of a diameter r scales polynomially in r, in the hyperbolic space, such volumes scale exponentially with r. Intuitively, this is a continuous analogue of trees: for a tree with a branching factor k, we obtain O(kd) nodes on the level d, which in this case serves as a discrete analogue of the radius. This property allows us to efficiently embed hierarchical data even in low dimensions, which is made precise by embedding theorems for trees and complex net-works [42].
Hyperbolic spaces are not vector spaces; to be able to perform operations such as addition, we need to introduce a
so-called gyrovector formalism [54]. For a pair x, y ∈ Dn c , their addition is defined as x ⊕c y = (1 + 2c⟨x, y⟩ + c∥y∥2)x + (1 − c∥x∥2)y 1 + 2c⟨x, y⟩ + c2∥x∥2∥y∥2
. (1)
The hyperbolic distance between x, y ∈ Dn c is defined in the following manner:
Dhyp(x, y) =
√ arctanh( 2
√ c c∥ − x ⊕c y∥). (2)
Note that with c → 0 the distance function (2) reduces to Euclidean: limc→0 Dhyp(x, y) = 2∥x − y∥.
We also need to define a bijection from Euclidean space to the Poincar´e model of hyperbolic geometry. This map-ping is termed exponential while its inverse mapping from hyperbolic space to Euclidean is called logarithmic.
For some fixed base point x ∈ Dn c , the exponential map-ping is a function expc x : Rn → Dn (cid:18) (cid:18)√ tanh c c defined as: (cid:19) v
√
λc x∥v∥ 2 c∥v∥ (cid:19)
. (3) expc x(v) = x ⊕c
The base point x is usually set to 0 which makes formulas less cumbersome and empirically has little impact on the obtained results.
To train our model, we take a sample xi, pass it through the encoder and project the output to hyperbolic space; the resulted representation in hyperbolic space is denoted as zi.
Since our pairwise cross-entropy loss is based on hyperbolic distances, we do not project zi back to Euclidean space and use only the exponential mapping. 2.2. Pairwise Cross-Entropy Loss
At each iteration, we sample N different categories of images and two samples per category. In this case, the total number of samples (batch size) is K = 2N consisting of N positive pairs.
Additionally to hyperbolic distance, we define the dis-implemented with a tance with the cosine similarity, squared Euclidean distance between normalized vectors:
Dcos(zi, zj) = (cid:13) (cid:13) (cid:13) (cid:13) zi
∥zi∥2
− zj
∥zj∥2 (cid:13) 2 (cid:13) (cid:13) (cid:13) 2
= 2 − 2
⟨zi, zj⟩
∥zi∥2 · ∥zj∥2 (4)
The loss function for a positive pair (i, j) is defined as li,j = − log exp (−D(zi, zj)/τ ) k=1,k̸=i exp (−D(zi, zk)/τ ) (cid:80)K
, (5) where D is a distance (Dhyp or Dcos) and τ is a temperature hyperparameter. The total loss is computed for all positive pairs, both (i, j) and (j, i), in a batch.
CUB-200 Cars-196
SOP
In-Shop
ViT-S
DeiT-S
DINO 0.280 0.294 0.315 0.339 0.343 0.327 0.271 0.270 0.301 0.313 0.323 0.318
Table 1. δ-hyperbolicity values calculated for the embeddings ob-tained from different encoders. We can see that the δ values are fairly consistent with respect to different feature extractors. Lower
δ values indicate a higher degree of data hyperbolicity.
If the total number of categories is small and a larger batch size is more suitable from the optimization perspec-tive, it is possible to sample more than two samples per cat-egory. In this case, we sample d images per each category, d ≥ 2. We divide the batch K = dN into d subsets with each subset consisting of N samples from different cate-gories. Next, we obtain the loss value for each pair of sub-sets, as defined Equation (5), summing them up for the final value. 2.3. δ-hyperbolicity
While the curvature value of an underlying manifold for embedding is often neglected, a more efficient way is to es-timate it for each dataset specifically. Following the analysis in [22], we estimate a ‘measure’ of the data hyperbolicity.
This evaluation is made through the computation of the so-called Gromov δ. Its calculation requires first computing
Gromov product for points x, y, z ∈ X : (y, z)x = 1 2 (d(x, y) + d(x, z) − d(y, z)), (6) where (X , d) is an arbitrary metric space. For a set of points, we compute the matrix M of pairwise Gromov prod-ucts (6). The δ value is then defined as the largest entry in the matrix (M ⊗M )−M . Here, ⊗ denotes the min-max ma-trix product defined as (A ⊗ B)ij = maxk min{Aik, Bkj}
[10].
Being rescaled between 0 and 1, the relative δ-hyperbolicity reflects how close to the hyperbolic the hid-den structure is: values tending to 0 show the higher degree of intrinsic data hyperbolicity. The δ value is related to the optimal radius of the Poincar´e ball for embeddings through the following expression c(X) = ( 0.144
)2. We adopt the procedure described in [22] and evaluate δ for image em-beddings extracted using three encoders: ViT-S, DeiT-S and
DINO (described in Section 2.5). Tab. 1 highlights the ob-tained relative δ values for CUB-200, Cars-196, SOP and
In-Shop datasets.
δ 2.4. Feature Clipping
The paper [15] empirically shows that a hyperbolic neu-ral network tends to have vanishing gradients since it pushes
the embeddings close to the boundary of Poincar´e ball, making the gradients of Euclidean parameters vanish. To avoid numerical errors when dealing with hyperbolic neural networks, the common approach is to perform clipping by norm on the points in the Poincar`e ball; the standard norm value is 1√ c (1 − 10−5). Instead, the paper [15] proposes to augment this procedure with an additional technique called feature clipping:
C = min (cid:8)1, xE r
∥xE∥ (cid:9) · xE, (7) where xE lies in the Euclidean space, xE
C is its clipped counterpart and r is a new effective radius of the Poincar´e ball. Intuitively, this allows us to push embeddings further away from the boundary and avoid the vanishing gradients problem; in the experiments of [15] it led to a consistent improvement over baselines. ages and 1K categories). An additional training signal is provided by teacher-student distillation, with a CNN-based teacher [53].
The third solution used in our experiments, DINO [3], is based on self-supervised training. In this case, the model
ViT-S is trained on the ImageNet-1k dataset [41] without labels. The encoder must produce consistent output for dif-ferent parts of an image, obtained using augmentations (ran-dom crop, color jitter, and others). This training scheme is in line with the image retrieval task; in both cases, the en-coder is explicitly trained to produce similar output for se-mantically similar input. However, the goal of these tasks is different: self-supervised learning provides pretrained fea-tures, which are then used for other downstream tasks, while for image retrieval resulting features are directly used for the evaluation. 2.5. Vision Transformers 3. Experiments
In our experiments, we use ViT architecture introduced by [7]. The input image is sliced into patches of size 16×16 pixels. Each patch is flattened and then linearly projected into an embedding. The resulting vectors are concatenated with position embeddings. Also, this set of vectors includes an additional “classification” token. Note that in our case, this token is used to obtain the image embedding, but we do not train a standard classifier as in [7]. For consistency with previous literature, we name this token [class]. The set of resulting vectors is fed into a standard transformer en-coder [56]. It consists of several layers with multiheaded self-attention (MSA) and MLP blocks, with a LayerNorm before and a residual connection after each block. The out-put for the transformer encoder for the [class] token is used as the final image representation. For more details, we refer to [7].
ViT-S [48] is a smaller version of ViT with 6 heads in
MSA (base version uses 12 heads). This architecture is similar to ResNet-50 [18] in terms of number of parame-ters (22M for ViT-S and 23M for ResNet-50) and computa-tional requirements (8.4 FLOPS for ViT-S and 8.3 FLOPS for ResNet-50). This similarity makes it possible to fairly compare with previous works based on ResNet-50 encoder, for this reason, we employ this configuration for our exper-iments. A more thorough description is available in [48].
Vision transformers, compared to CNNs, require more training signal. One solution, as proposed in [7], is to
ImageNet-21k [6] contains approxi-use a large dataset. mately 14M images classified into 21K categories. ViT-S, pretrained on ImageNet-21k, is publicly available [48]; we include it in our experiments. Another solution, DeiT-S [53], is based on the same (ViT-S) architecture and is trained on a smaller ImageNet-1k dataset [41] (a subset of ImageNet-21k consisting of about 1.3M training im-We follow a widely adopted training and evaluation pro-tocol [23] and compare several versions of our method with current state-of-the-art on four benchmark datasets for category-level retrieval. We include technical details of datasets, our implementation and training details, and fi-nally, present empirical results. There are two types of ex-periments, first, we compare with the state-of-the-art, and then we investigate the impact of hyperparameters (encoder patch size, manifold curvature, embedding size and batch size). 3.1. Datasets
CUB-200-2011 (CUB) [61] includes 11,788 images with 200 categories of bird breeds. The training set corresponds to the first 100 classes with 5,864 images, and the remain-ing 100 classes with 5,924 images are used for testing. The images are very similar; some breeds can only be distin-guished by minor details, making this dataset challenging and, at the same time, informative for the image retrieval task. Cars-196 (Cars) [25] consists of 16,185 images repre-senting 196 car models. First 98 classes (8,054 images) are used for training and the other 98 classes (8,131 images) are held out for testing. Stanford Online Product (SOP) [47] consists of 120,053 images of 22,634 products downloaded from eBay.com. We use the standard split: 11,318 classes (59,551 images) for training and remaining 11,316 classes (60,502 images) for testing. In-shop Clothes Retrieval (In-Shop) [28] consists of 7,986 categories of clothing items.
First 3,997 categories (25,882 images) are for training, the remaining 3,985 categories are used for testing partitioned into a query set (14,218 images) and a gallery set (12,612 images).
3.2. Implementation Details
We use ViT-S [48] as an encoder with three types of pre-training (ViT-S, DeiT-S and DINO), details are presented in Section 2.5. The linear projection for patch embeddings as a first basic operation presumably corresponds to low-level feature extraction, so we freeze it during fine-tuning.
The encoder outputs a representation of dimensionality 384, which is further plugged into a head linearly projecting the features to the space of dimension 128. We initialize the bi-ases of the head with constant 0 and weights with a (semi) orthogonal matrix [43]. We include two versions of the head: with a projection to a hyperbolic space (“Hyp-”) and with projection to a unit hypersphere (“Sph-”). In the first case, we use curvature parameter c = 0.1 (in Section 3.4 we investigate how it affects the method’s performance), temperature τ = 0.2 and clipping radius (defined in Sec-tion 2.4) r = 2.3. For spherical embeddings, we use tem-perature τ = 0.1.
To evaluate the model performance, for the encoder, we compute the Recall@K metric for the output with distance
Dcos (Eq. (4)); for the head, we use Dcos for “Sph-” ver-sion and hyperbolic distance Dhyp (Eq. (2)) for “Hyp-” ver-sion. We resize the test images to 224 (256 for CUB) on the smaller side and take one 224 × 224 center crop. Note that some methods use images of higher resolution for train-ing and evaluations, e.g., ProxyNCA++ [52] use 256 × 256 crops indicating that smaller 227 × 227 crops degrade the performance by 4.3% on CUB. However, 224 × 224 is the default size for encoders considered in our work; moreover, some recent methods, such as IRTR [8], use this size for experiments.
We use the AdamW optimizer [29] with a learning rate value 1 × 10−5 for DINO and 3 × 10−5 for ViT-S and DeiT-S. The weight decay value is 0.01, and the batch size equals 900. The number of optimizer steps depends on the dataset: 200 for CUB, 600 for Cars, 25000 for SOP, 2200 for In-Shop. The gradient is clipped by norm 3 for a greater sta-bility. We apply commonly used data augmentations: ran-dom crop resizing the image to 224 × 224 using bicubic interpolation combined with a random horizontal flip. We train with Automatic Mixed Precision in O2 mode 1. All experiments are performed on one NVIDIA A100 GPU. 3.3. Results
Tab. 2 highlights the experimental results for the 128-dimensional head embedding and the results for 384-dimensional encoder embedding are shown in Tab. 3. We include evaluation of the pretrained encoders without train-ing on the target dataset in Tab. 3 for reference. On the
CUB dataset, we can observe the solid performance of methods with ViT encoder; the gap between the second-1https://github.com/NVIDIA/apex best method IRTR and Hyp-ViT is 9%. However, the main improvement comes from the dataset used for pretraining (ImageNet-21k), since Hyp-DINO and Hyp-DeiT demon-strate a smaller improvement, while baseline ViT-S with-out finetuning shows strong performance. We hypothesize that this is due to the presence of several bird classes in the
ImageNet-21k dataset encouraging the encoder to separate them during the pretraining phase.
For the SOP and In-Shop datasets, the difference be-tween Hyp-ViT and Hyp-DINO is minor, while, for Cars-196, Hyp-DINO outperforms Hyp-ViT with a significant margin. These results confirm that both pretraining schemes are suitable for the considered task. The versions with DeiT perform worse compared to ViT- and DINO-based encoders while outperforming CNN-based models. This observation confirms the significance of vision transformers in our ar-chitecture. The experimental results suggest that hyperbolic space embeddings consistently improve the performance compared to spherical versions. Hyperbolic space seems to be beneficial for the embeddings, and the distance in hy-perbolic space suits well for the pairwise cross-entropy loss function. At the same time, our sphere-based versions per-form well compared to other methods with CNN encoders.
Figure 2 illustrates how learned embeddings are ar-ranged on the Poincar´e disk. We use UMAP [31] method with the “hyperboloid” distance metric to reduce the dimen-sionality to 2D for visualization. For the training part, we can see that samples are clustered according to labels, and each cluster is pushed closer to the border of the disk, indi-cating that the encoder separates classes well. However, for the testing part, the structure is more complex. We observe that some of the samples tend to move towards the center and intermix, while others stay in clusters, showing possi-ble hierarchical relationships. We can see that car images are grouped by several properties: pose, color, shape, etc. 3.4. Impact of Hyperparameters
In this section, we investigate the impact of the values of the hyperparameters on the model performance.
Encoder patch size. ViT architecture does not process each pixel independently; for computational feasibility, the input image is sliced into patches projected into the initial embeddings. The default size of the patch is 16 × 16, al-though considering other values is also possible. The exper-iments in [3] have demonstrated a significant performance gain from smaller 8 × 8 patches for self-supervised learn-ing. In this case, the number of parameters of the encoder does not change; however, it requires processing 4× more embeddings, which allows the encoder to learn more com-plex dependencies between patches. We add an experiment with this setup in Tab. 4 demonstrating a substantial perfor-mance improvement (+4.4%) compared to the default con-figuration. In this case, we use the same training procedure,
Method
CUB-200-2011 (K) 8 1 2 4
Cars-196 (K) 1 2 4 8 1
SOP (K) 100 10 1000
--63.9 75.3 84.4 90.6 79.6 86.5 91.9 95.1 72.7 86.2 93.8
Margin [62]
FastAP [2] 73.8 88.0 94.9
-NSoftmax [64] 56.5 69.6 79.9 87.6 81.6 88.7 93.4 96.3 75.2 88.7 95.2 77.2 89.4 94.6
MIC [40] 80.6 91.6 96.2
XBM [59] 83.4 93.0 97.0
IRTR [8] 82.6 89.1 93.2
--66.1 76.8 85.6
-72.6 81.9 88.7 92.8
----------------Sph-DeiT
Sph-DINO
Sph-ViT §
Hyp-DeiT
Hyp-DINO
Hyp-ViT § 73.3 82.4 88.7 93.0 77.3 85.4 91.1 94.4 82.5 93.1 97.3 76.0 84.7 90.3 94.1 81.9 88.7 92.8 95.8 82.0 92.3 96.9 83.2 89.7 93.6 95.8 78.5 86.0 90.9 94.3 82.5 92.9 97.4 74.7 84.5 90.1 94.1 82.1 89.1 93.4 96.3 83.0 93.4 97.5 78.3 86.0 91.2 94.7 86.0 91.9 95.2 97.2 84.6 94.1 97.7 84.0 90.2 94.2 96.4 82.7 89.7 93.9 96.2 85.5 94.9 98.1 98.0 98.3
--98.7 99.0 99.2 99.1 99.3 99.2 99.3 99.4
In-Shop (K) 10 20 30
------1
--86.6 96.8 97.8 98.3 88.2 97.0 98.0 91.3 97.8 98.4 98.7 91.1 98.1 98.6 99.0
-89.3 97.0 97.9 98.4 90.4 97.3 98.1 98.5 90.8 97.8 98.5 98.8 90.9 97.9 98.6 98.9 92.6 98.4 99.0 99.2 92.7 98.4 98.9 99.1
Table 2. Recall@K metric for four datasets for 128-dimensional embeddings. The 6 versions of our method are listed in the bottom section, evaluated for head embeddings. “Sph-” are versions with hypersphere embeddings optimised using Dcos (Eq. (4)), “Hyp-” are versions with hyperbolic embeddings optimised using Dhyp (Eq. (2)). “DeiT”, “DINO” and “ViT” indicate type of pretraining for the vision transformer encoder. Margin, FastAP, MIC, XBM, NSoftmax are based on ResNet-50 [18] encoder, IRTR is based on DeiT [53].
§ pretrained on the larger ImageNet-21k [6].
Method
Dim
CUB-200-2011 (K) 8 1 4 2
Cars-196 (K) 1 2 4 8 1
SOP (K) 10 100 1000
In-Shop (K) 1 10 20 30 512 57.5 68.7 78.3 86.2 82.0 89.0 93.2 96.1 74.2 86.9 94.0 97.8 83.1 95.1 96.9 97.5
A-BIER [36] 512 60.6 71.5 79.8 87.4 85.2 90.5 94.0 96.1 76.3 88.4 94.8 98.2 87.3 96.7 97.9 98.2
ABE [24] 512 56.0 68.3 78.2 86.3 83.4 89.9 93.9 96.5 75.3 87.5 93.7 97.4 90.7 97.8 98.5 98.8
SM [49] 512 65.8 75.9 84.0 89.9 82.0 88.7 93.1 96.1 79.5 90.8 96.1 98.7 89.9 97.6 98.4 98.6
XBM [59] 512 57.1 68.8 78.7 86.5 81.4 88.0 92.7 95.7 74.8 88.3 94.8 98.4 80.9 94.3 95.8 97.2
HTL [13] 512 65.7 77.0 86.3 91.2 84.1 90.4 94.0 96.5 78.2 90.5 96.0 98.7 89.7 97.9 98.5 98.8
MS [58] 512 65.4 76.4 84.5 90.4 84.5 90.7 94.5 96.9 78.6 86.6 91.8 95.4
SoftTriple [37]
HORDE [20] 512 66.8 77.4 85.1 91.0 86.2 91.9 95.1 97.2 80.1 91.3 96.2 98.7 90.4 97.8 98.4 98.7
Proxy-Anchor [23] 512 68.4 79.2 86.8 91.6 86.1 91.7 95.0 97.3 79.1 90.8 96.2 98.7 91.5 98.1 98.8 99.1 86.6 97.5 98.4 98.8
NSoftmax [64] 512 61.3 73.9 83.5 90.0 84.2 90.4 94.4 96.9 78.2 90.6 96.2 512 69.0 79.8 87.3 92.7 86.5 92.5 95.7 97.7 80.7 92.0 96.7 98.9 90.4 98.1 98.8 99.0
ProxyNCA++ [52]
IRTR [8] 84.2 93.7 97.3 99.1 91.9 98.1 98.7 98.9
-384 76.6 85.0 91.1 94.3
ResNet-50 [18] †
DeiT-S [53] †
DINO [3] †
ViT-S [48] † § 2048 41.2 53.8 66.3 77.5 41.4 53.6 66.1 76.6 50.6 66.7 80.7 93.0 25.8 49.1 56.4 60.5 384 70.6 81.3 88.7 93.5 52.8 65.1 76.2 85.3 58.3 73.9 85.9 95.4 37.9 64.7 72.1 75.9 384 70.8 81.1 88.8 93.5 42.9 53.9 64.2 74.4 63.4 78.1 88.3 96.0 46.1 71.1 77.5 81.1 384 83.1 90.4 94.4 96.5 47.8 60.2 72.2 82.6 62.1 77.7 89.0 96.8 43.2 70.2 76.7 80.5
--------Sph-DeiT
Sph-DINO
Sph-ViT §
Hyp-DeiT
Hyp-DINO
Hyp-ViT § 384 76.2 84.5 90.2 94.3 81.7 88.6 93.4 96.2 82.5 92.9 97.2 99.1 89.6 97.2 98.0 98.4 384 78.7 86.7 91.4 94.9 86.6 91.8 95.2 97.4 82.2 92.1 96.8 98.9 90.1 97.1 98.0 98.4 384 85.1 90.7 94.3 96.4 81.7 89.0 93.0 95.8 82.1 92.5 97.1 99.1 90.4 97.4 98.2 98.6 384 77.8 86.6 91.9 95.1 86.4 92.2 95.5 97.5 83.3 93.5 97.4 99.1 90.5 97.8 98.5 98.9 384 80.9 87.6 92.4 95.6 89.2 94.1 96.7 98.1 85.1 94.4 97.8 99.3 92.4 98.4 98.9 99.1 384 85.6 91.4 94.8 96.7 86.5 92.1 95.3 97.3 85.9 94.9 98.1 99.5 92.5 98.3 98.8 99.1
Table 3. Recall@K metric for four datasets, “Dim” column shows the dimensionality of embeddings. The 6 versions of our method are listed in the bottom section, evaluated for encoder embeddings, titles are described in Table 2. Encoders by method: A-BIER, ABE, SM:
GoogleNet [51]; XBM, HTL, MS, SoftTriple, HORDE, Proxy-Anchor: Inception with batch normalization [19]; NSoftmax, ProxyNCA++:
ResNet-50 [18]; IRTR: DeiT [53]. † pretrained encoders without training on the target dataset. § pretrained on the larger ImageNet-21k [6].
Parameter
Encoder(384)
Head
Default c = 0.01 c = 0.05 c = 0.3 c = 0.5 c = 1.0
Head dim. 16
Head dim. 32
Head dim. 64
Batch size 200
Batch size 400
Batch size 1600 92.4 92.3 92.4 92.3 91.8 90.0 88.6 90.2 91.6 92.0 92.5 92.4 92.6 92.6 92.6 92.0 91.0 89.2 83.3 89.6 91.7 91.9 92.5 92.6
Table 5. Recall@1 metric for various hyperparameters for Hyp-ViT configuration on In-Shop dataset. Default configuration is c = 0.1, head dimensionality 128, batch size 900. flat as the Euclidean space; in contrast, larger c values cor-respond to a steeper configuration. Note that according to δ values (Tab. 1), the estimated value of c is close to 0.2, de-pending on the dataset and encoder. However, smaller val-ues tend to provide better stability; we believe this is due to an optimisation process that can be improved for the hyper-bolic space. For this reason, we adjusted the default value towards a smaller 0.1 (Section 3.2).
Embedding size and batch size. As expected, lower output dimensionality leads to lower recall values. How-ever, taking into account a high data variability (3,985 cat-egories in the test set), the experimental results suggest that the method has a reasonable representation power even in the case of lower dimensions.
The batch size directly influences the number of nega-tive examples during the training phase; thus, intuitively, larger values have to be more profitable for the model per-formance. However, as the experiments show (Tab. 5), the method is robust for batch size ≥ 400, having a minor accu-racy degradation for batch size equal to 200. Therefore, for considered datasets, the method does not require distributed training with a large number of GPUs [4] or specific solu-tions with a momentum network [17]. 4.