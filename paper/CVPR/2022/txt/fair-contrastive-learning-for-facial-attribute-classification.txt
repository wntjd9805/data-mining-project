Abstract
Learning visual representation of high quality is essential for image classiﬁcation. Recently, a series of contrastive representation learning methods have achieved preeminent success. Particularly, SupCon [18] outperformed the domi-nant methods based on cross-entropy loss in representation learning. However, we notice that there could be potential ethical risks in supervised contrastive learning. In this paper, we for the ﬁrst time analyze unfairness caused by supervised contrastive learning and propose a new Fair Supervised Con-trastive Loss (FSCL) for fair visual representation learning.
Inheriting the philosophy of supervised contrastive learning, it encourages representation of the same class to be closer to each other than that of different classes, while ensuring fairness by penalizing the inclusion of sensitive attribute information in representation. In addition, we introduce a group-wise normalization to diminish the disparities of intra-group compactness and inter-class separability between de-mographic groups that arouse unfair classiﬁcation. Through extensive experiments on CelebA and UTK Face, we validate that the proposed method signiﬁcantly outperforms SupCon and existing state-of-the-art methods in terms of the trade-off between top-1 accuracy and fairness. Moreover, our method is robust to the intensity of data bias and effectively works in incomplete supervised settings. Our code is available at https://github.com/sungho-CoolG/FSCL. 1.

Introduction
Learning powerful visual representation is important for reliable performance in image classiﬁcation. For a long time, most work has relied on cross-entropy loss to learn the rep-resentation due to its strong performance [4, 11, 34, 41].
Meanwhile, recent studies based on contrastive learning have been bringing a new paradigm for representation learn-ing [2, 10, 12, 36, 40]. They effectively learn visual represen-tation by drawing positive pairs and pushing away negative
*Corresponding author. ones in the high-dimensional space. Despite being origi-nally introduced for unsupervised learning, the contrastive learning strategy proves to be effective in various vision
ﬁelds [14, 22, 23]. Particularly, SupCon [18] achieves better top-1 accuracy than the state-of-the-art methods based on the cross-entropy loss on ImageNet [32] by simply grafting the contrastive loss to the supervised representation learning.
In this paper, we point out that the contrastive loss may pose potential ethical risks. Despite exhibiting strong perfor-mance, it has been underexplored in consideration of fairness which means that the outputs from a model should not be discriminatory in terms of sensitive attributes, such as ethnic-ity, gender, and age. It is a crucial ethical topic and should be diagnosed in order for the model to be leveraged in the real world [5, 26]. To this end, we analyze the representative contrastive learning model (SupCon) from two major per-spectives causing unfairness and propose a new contrastive loss to address both of them.
Learning sensitive attribute information is one of the prin-cipal causes of unfairness [3, 13, 25, 35]. It incurs unfair classiﬁcation by inducing a classiﬁer to determine a deci-sion boundary based on undesirable grounds (i.e., sensitive attributes) [19, 28]. From this point of view, we demonstrate that learning sensitive attribute information leads to the de-crease of SupCon loss on the biased dataset, although the desired behavior is to exclusively learn target class informa-tion. Consequently, a model learns both kinds of information to minimize the loss, which eventually aggravates unfairness.
To solve the problem, we propose a Fair Supervised Con-trastive Loss (FSCL) which prevents encoder networks from learning sensitive attribute information. Basically, it inherits the spirit of supervised contrastive learning that encourages an anchor to be more similar to samples of the same class (i.e., positive samples) than those of other classes (i.e., neg-ative samples). Simultaneously, we limit negative samples to only those having the same sensitive attribute with the anchor among them. In this way, we ensure that learning sen-sitive attribute information no longer helps the contrastive learning. Rather, it hinders optimizing the loss by increasing the similarity between the anchor and negative samples.
On top of that, we analyze SupCon in terms of data imbal-ance between demographic groups, which is another causal factor of unfairness [31]. Concretely, we identify that the im-balanced number of anchors and positive samples between the demographic groups encourages the SupCon loss to put more weight on majority groups. As a result, samples from the majority groups generally have higher similarity to the other samples within the same group and lower similarity to samples having different target classes compared to those from the minority groups. We call the former intra-group compactness and the latter inter-class separability. Since their disparities between the groups result in imbalanced classiﬁcation performances [9, 43, 45], we introduce a group-wise normalization that reduces the gaps by balancing the loss based on the cardinality of anchors and positive samples between the groups. In the experiments, we demonstrate that it further improves fairness with little damage to the classiﬁcation performance.
To validate the effectiveness of our method, we per-form facial attribute classiﬁcation on CelebA [24] and UTK
Face [44] datasets. In various scenarios, the proposed method signiﬁcantly ameliorates fairness over SupCon and outper-forms the state-of-the-art methods in terms of the trade-off between classiﬁcation accuracy and fairness. Besides, our method is robust to the intensity of data bias and effectively improves fairness even in incompletely supervised settings (e.g., without target class labels or with only a few sensitive attribute labels). Furthermore, we show the extensibility of our method to general bias mitigation through experiments on Dogs and Cats dataset [16].
Main contributions. Our main contributions are sum-marized as follows. 1) We analyze the causes of unfairness in contrastive learning and propose a Fair Supervised Con-trastive Loss that improves fairness by penalizing the in-clusion of sensitive attribute information in representation. 2) We introduce a group-wise normalization, which mitigates the group-wise disparities of intra-group compactness and inter-class separability that exacerbate unfairness of repre-sentation. 3) Through extensive experiments, we validate that our method learns fair representation under various environ-ments. It achieves the best trade-off performances between top-1 accuracy and fairness on CelebA and UTK Face. 2.