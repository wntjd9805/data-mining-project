Abstract
In some scenarios, classifier requires detecting out-of-distribution samples far from its training data. With de-sirable characteristics, reconstruction autoencoder-based methods deal with this problem by using input reconstruc-tion error as a metric of novelty vs. normality. We for-mulate the essence of such approach as a quadruplet do-main translation with an intrinsic bias to only query for a proxy of conditional data uncertainty. Accordingly, an im-provement direction is formalized as maximumly compress-ing the autoencoder’s latent space while ensuring its recon-structive power for acting as a described domain transla-tor. From it, strategies are introduced including semantic reconstruction, data certainty decomposition and normal-ized L2 distance to substantially improve original meth-ods, which together establish state-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of CIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Impor-tantly, our method works without any additional data, hard-to-implement structure, time-consuming pipeline, and even harming the classification accuracy of known classes. 1.

Introduction
Supervised discriminative deep classifiers are de facto designed under a static closed-world assumption where the data, which model face in the deployment environment, is supposed to be sampled from the same distribution as train-ing set [12]. However, for applications in the wild, such as the safety-critical autonomous vehicles, test data is of-ten hard to be known a priori. Worse, given that neural networks especially those built on relu or softmax can eas-ily produce not only erroneous, but arbitrarily confident so, class predictions even for totally unrecognizable or irrele-vant samples [9–11,21], it is consequential to empower sys-tem to flag or abstain predictions from unkowns.
Out-of-distribution (OoD) detection is a binary classifi-cation of detecting inputs sampled from distribution differ-ent from training data [11]. Many existing methods rely on training or tuning with data labelled as OoD from other cat-egories [28, 31], adversaries [15, 19] or the leave-out sub-set of training samples [30]. However, it is intractable to cover the full space of OoD particularly for data with large dimensions (e.g., an image) [20], resulting in that a super-vised method capturing limited facets of OoD distribution hardly generalizes without a data selection bias [24]. Mean-while, the classifier’s accuracy on in-distribution (ID) im-ages would be fluctuated by introducing OoD data with ad-ditional training objectives [28,30]. Such factors expose the unsupervised nature of OoD detection.
In a class of unsupervised approaches, input reconstruc-tion residual is considered a novelty metric to avoid the above problems [22]. The essential assumption is that an autoencoder learned to rebuild ID samples could not re-construct OoD comparatively during testing [5]. However, it has been extensively reported that autoencoders can ef-fectively rebuild kinds of OoD samples even better [7, 22], causing an inferior performance of such methods when ap-plied to challenging multi-class OoD detection tasks. In this paper, we investigate in a precise manner the problematic reconstruction of outliers from the perspective of quadru-plet domain translation by formalizing two concrete precon-ditions under which the reconstruction error of an input is a valid data uncertainty measure. First, its latent feature lies within the domain of encoded ID samples. Second, the decoder has adequate reconstructive power to bridge the do-mains of ID images and their latent representations.
Precondition 1 requires system to capture the outliers of latent representations. Without relying on kernel density estimators [1, 7, 23] performing weakly in high dimensions
[17], similar to the scheme considering for compact latent representations [2, 3], we minimize a regularization loss to restrict ID latent features distributed within a certain space.
In conjunction with the training of input reconstruction, it is explicated that, when this space is compressed sufficiently, any latent feature outlier will lie outside it.
There is a coupled problem that an over-restricted latent space might not provide sufficient reconstructive power for large-scale ID images, which violates precondition 2. To mitigate it, we change the reconstruction target from the im-age to its extracted activation vector (AV) feature to reduce the unnecessary requirement of the expressiveness of latent
space. As a further step, from the perspective of domain translation, we deduce a base equation to model the inher-ent connection between input reconstruction error and data certainty. By the probability chain rule, we further factor-ize it to express data certainty as a product of conditional densities defined by the layer-by-layer encoder feature re-construction error. It is proved that, albeit the considerable information loss accumulated from the whole encoding pro-cess making it hard to recover input directly from a compact latent space to satisfy precondition 2, there exists an equiv-alent precondition feasibly to be met as it only requires to recover the information lost after each irreversible encoding layer respectively. Consequently, inspired by above con-cepts, we present a theoretically well-defined framework for
OoD detection, namely layerwise semantic reconstruction.
In this framework, we employ only one fully connected (FC) layer and softmax function as the encoder architec-ture and leverage simple cross entropy loss to restrict the latent space. We provide both experimental evidences and mathematical insights, indicating that under such a setting the maximum value of a latent feature can be utilized as a domain affinity for filtering out OoD data potentially to be reconstructed. Since classifier is inclined to produce smaller neural activations on OoD data [25, 27], to ren-der our method robust against it, reconstruction accuracy is evaluated with the proposed normalized L2 distance.
Our contribution is three-fold: First, we establish a novel perspective for understanding autoencoder-based OoD de-tections, figuring out one direction to improve them by max-imumly condensing autoencoder’s latent space while re-serving sufficient reconstructive power on ID data. Rooting from it, second, a framework of layerwise semantic recon-struction is developed. Third, along with ablation and ro-bustness studies, we provide comprehensive analysis of our proposal using various benchmark datasets to demonstrate its efficacy, indicating that the potentiality of autoencoder-based methods is not as bleak as previously displayed.
• Our method delivers comparable performance to
SOTA methods on various challenging benchmarks.
• As an auxiliary module, our OoD detector is trained in a way orthogonal to the classifier.
• Operated in an unsupervised mode of efficiency and applicability, our method requires no additional data. 2.