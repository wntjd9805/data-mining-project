Abstract
Appearance of dressed humans undergoes a complex ge-ometric transformation induced not only by the static pose but also by its dynamics, i.e., there exists a number of cloth geometric configurations given a pose depending on the way it has moved. Such appearance modeling con-ditioned on motion has been largely neglected in existing human rendering methods, resulting in rendering of phys-ically implausible motion. A key challenge of learning the dynamics of the appearance lies in the requirement of
In this pa-a prohibitively large amount of observations. per, we present a compact motion representation by enforc-ing equivariance—a representation is expected to be trans-formed in the way that the pose is transformed. We model an equivariant encoder that can generate the generalizable representation from the spatial and temporal derivatives of the 3D body surface. This learned representation is de-coded by a compositional multi-task decoder that renders high fidelity time-varying appearance. Our experiments show that our method can generate a temporally coherent video of dynamic humans for unseen body poses and novel views given a single view video. 1.

Introduction
We express ourselves by moving our body that drives a sequence of natural secondary motion, e.g., dynamic move-ment of dress induced by dancing as shown in Figure 1.
This secondary motion is the resultant of complex physi-cal interactions with the body, which is, in general, time-varying. This presents a major challenge for plausible ren-dering of dynamic dressed humans in applications such as video based retargetting or social presence. Many exist-ing approaches such as pose-guided person image gener-ation [7] focus on static poses as a conditional variable. De-spite its promising rendering quality, it fails to generate a physically plausible secondary motion, e.g., generating the same appearance for fast and slow motions.
Figure 1. Given surface normal and velocity of a 3D body model, our method synthesizes subject-specific surface normal and ap-pearance. We specifically focus on synthesis of plausible dynamic appearance by learning an effective 3D motion descriptor.
One can learn the dynamics of the secondary motion from videos. This, however, requires a tremendous amount of data, i.e., videos depicting all possible poses and associ-ated motions. In practice, only a short video clip is avail-able, e.g., the maximum length of videos in social media (e.g., TikTok) are limited to 15-60 seconds. The learned representation is, therefore, prone to overfitting.
In this paper, we address the fundamental question of
“can we learn a representation for dynamics given a lim-ited amount of observations?”. We argue that a meaningful representation can be learned by enforcing an equivariant property—a representation is expected to be transformed in the way that the body pose is transformed. With the equiv-ariance, we model the dynamics of the secondary motion as a function of spatial and time derivative of the 3D body. We construct this representation by re-arranging 3D features in the canonical coordinate system of the body surface, i.e., the UV map, which is invariant to the choice of the 3D co-ordinate system. The UV map also captures the semantic meaning of body parts since each body part is represented by a UV patch. The resulting representation is compact and
discriminative compared to the 2D pose representations that often suffer from geometric ambiguity due to 2D projection.
We observe that two dominant factors significantly im-pact the physicality of the generated appearance. First, the silhouette of dressed humans is transformed according to the body movement and the physical properties (e.g., mate-rial) of the individual garment types (e.g., top and bottom garments might undergo different deformations). Second, the local geometry of the body and clothes is highly cor-related, e.g., surface normals of T-shirt and body surface, which causes appearance and disappearance of folds and wrinkles. To incorporate these factors, we propose a com-positional decoder that breaks down the final appearance rendering into modular subtasks. This decoder predicts the time-varying semantic maps and surface normals as inter-mediate representations. While the semantic maps cap-ture the time-varying silhouette deformations, the surface normals are effective in synthesizing high quality textures, which further enables re-lighting. We combine these inter-mediate representations to produce the final appearance.
Our experiments show that our method can generate a temporally coherent video of an unseen secondary mo-tion from novel views given a single view training video.
We conduct thorough comparisons with various state-of-the-art baseline approaches. Thanks to the discriminative power, our representation demonstrates superior generaliza-tion ability, consistently outperforming previous methods when trained on shorter training videos. Furthermore, our method shows better performance in handling complex mo-tion sequences including 3D rotations as well as rendering consistent views in applications such as free-viewpoint ren-dering. The intermediate representations predicted by our method such as surface normals also enable applications such as relighting which are otherwise not applicable. 2.