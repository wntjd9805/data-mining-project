Abstract
This paper presents Video K-Net, a simple, strong, and unified framework for fully end-to-end video panoptic seg-mentation. The method is built upon K-Net, a method that unifies image segmentation via a group of learnable ker-nels. We observe that these learnable kernels from K-Net, which encode object appearances and contexts, can nat-urally associate identical instances across video frames.
Motivated by this observation, Video K-Net learns to si-multaneously segment and track “things” and “stuff” in a video with simple kernel-based appearance modeling and cross-temporal kernel interaction. Despite the simplicity, it achieves state-of-the-art video panoptic segmentation re-sults on Citscapes-VPS and KITTI-STEP without bells and whistles. In particular on KITTI-STEP, the simple method can boost almost 12% relative improvements over previous methods. We also validate its generalization on video se-mantic segmentation, where we boost various baselines by 2% on the VSPW dataset. Moreover, we extend K-Net into clip-level video framework for video instance segmentation where we obtain 40.5% for ResNet50 backbone and 51.5% mAP for Swin-base on YouTube-2019 validation set. We hope this simple yet effective method can serve as a new flexible baseline in video segmentation. 1 1.

Introduction
Video Panoptic Segmentation (VPS) aims at segmenting and tracking every pixel of input video clips [20,22,57]. As a fundamental technique to scene understanding, it has re-ceived increasing attention in recent years due to its wide applications in many vision systems including autonomous driving and robot navigation [12, 16]. By definition, VPS is
*Equal contribution. 1Both code and models are released at here. an extension of Panoptic Segmentation (PS) [24] into the video domain with the goal of unifying Video Semantic
Segmentation (VSS) [33, 45, 73] and Video Instance Seg-mentation (VIS) [3, 64] into a single task.
Existing studies for video panoptic segmentation can be mainly divided into top-down methods [20, 22] and bottom-up approaches [42,57]. Top-down methods try to solve VPS as a multi-task learning problem via performing semantic segmentation, instance segmentation and multiple object tracking individually . VPSNet [22] is proposed to learn to fuse and warp features. In particular, it applies an optical flow network [14] to align features and attention modules to fuse features [60]. However, these approaches [20, 22] involve many hyper-parameters and ad-hoc designs, lead-ing to a complex system. Bottom-up methods [10, 42, 57] first perform semantic segmentation and then predict near frames’ centers to localize and track each instance where both features are connected by an ASPP module [7] (Fig. 1-(b)). Despite being simpler than top-down approaches, they rely on several post-processing components (i.e., NMS for center grouping, offline mask tracking).
Recently, inspired by the design of object queries in De-tection Transformer (DETR) [5], new efforts [11, 51, 68] emerge to explore unified solutions to image panoptic seg-mentation. In particular, MaskFormer [11] and K-Net [68] unify ‘things’ and ‘stuff’ segmentations by dynamic ker-nels within the mask classification paradigm. The afore-mentioned studies in image segmentation motivate us to simplify cumbersome pipelines in video panoptic segmen-tation. It is noteworthy that several studies [32, 46, 67] in
Multi Object Tracking (MOT) also adopt the query design to achieve end-to-end learning. However, most of them introduce extra tracking queries or boxes to handle each tracked instance.
In this paper, we present Video K-Net, a fully end-to-end framework for video panoptic segmentation. It is observed that the learnable kernels from K-Net [68], which encode
Figure 1. An illustration of previous top-down based VPS method (a), bottom-up based VPS method (b) and the proposed Video K-Net (c). Unlike previous approaches [22, 42] that perform panoptic segmentation and object tracking with independent modules, our method unifies panoptic segmentation and instance level tracking via kernels in a simpler framework. complexity without extra tracking query and RoI features.
The tracking is performed in an online manner. Moreover, it only adds a few extra GFLOPS compared to original K-Net (2.3%GFLOPs). Different from previous works that use
RoI heads [22, 36], our kernel based embedding learning avoids box cropping and results in better association perfor-mance.
Video K-Net obtains consistent and significant improve-ments over the strong baseline K-Net with 3.5% STQ on
KITTI-STEP and 4% VPQon Cityscapes-VPS. In particu-lar, our method boosts almost 12% relative improvements over previous bottom up baseline Motion-Deeplab [57].
Video K-Net achieves new state-of-the-art results on two
VPS datasets including KITTI-STEP [57] and Cityscapes-VPS [22]. Moreover, as shown in Fig. 2, Video K-Net achieves the best trade-off between accuracy and GFLOPS on the two datasets. We further validate the effectiveness of kernel fusion on VSS task with VSPW dataset [33] and we boost previous baselines [8, 70] via considerable mar-gins on two different metrics. Moreover, we also extend the Video K-Net into clip-level processing via extra tempo-ral kernel fusion module for VIS task. We achieve 40.5%
AP using ResNet50 backbone which lead to 4.0% improve-ments over previous VisTR [54] with less GFLOPs. In sum-mary, extensive experiments and analysis demonstrate that
Video K-Net can serve as a new baseline for future research on unified video segmentation tasks. 2.