Abstract
Interpretability is an important property for visual mod-els as it helps researchers and users understand the in-ternal mechanism of a complex model. However, gener-ating semantic explanations about the learned representa-tion is challenging without direct supervision to produce such explanations. We propose a general framework, La-tent Visual Semantic Explainer (LaViSE), to teach any ex-isting convolutional neural network to generate text de-scriptions about its own latent representations at the filter level. Our method constructs a mapping between the vi-sual and semantic spaces using generic image datasets, us-ing images and category names. It then transfers the map-ping to the target domain which does not have semantic la-bels. The proposed framework employs a modular structure and enables to analyze any trained network whether or not its original training data is available. We show that our method can generate novel descriptions for learned filters beyond the set of categories defined in the training dataset and perform an extensive evaluation on multiple datasets.
We also demonstrate a novel application of our method for unsupervised dataset bias analysis which allows us to auto-matically discover hidden biases in datasets or compare dif-ferent subsets without using additional labels. The dataset and code are made public to facilitate further research.1 1.

Introduction
Convolutional neural networks have shown great perfor-mance in visual representation learning, but the learned rep-resentations are usually hard to explain or interpret. The lack of explainability raises the concern that AI systems and models, although very accurate in prediction, may have hidden negative effects on human users and society, such as AI bias. Several studies reported biases in computer vi-sion models for face attribute classification [8, 12], recog-nition [33, 58], and image captioning [25]. It is very chal-1https://github.com/YuYang0901/LaViSE
Figure 1. The proposed framework aims to semantically explain the concepts learned by individual filters in a CNN without super-vision on the concepts used for the semantic explanations. lenging, however, to identify these biases from a black-box model with distributed knowledge.
To date, several methods have been proposed to interpret what are learned and captured in CNNs. These methods vary greatly by the form (visualization, captions, synthe-sized samples), the focus (individual filter vs network level), and the scope (any existing models vs requiring training with supervision) of the generated explanations, and each method has its own strengths and weaknesses.
The main objective of this paper is to generate the tex-tual interpretations of any existing black box model that can overcome the limitations of existing approaches for several reasons. First, it generates words and thus can be more semantically meaningful and objective than visualization based methods [42, 49, 66]. Second, it can apply to any arbitrary network and does not require training or annota-tions, which is much more applicable than methods that re-quire training a model with ground-truth explanation anno-tations [27]. We do train an adapter using general image classification datasets which can apply to any given target model. Third, it can generate explanations using novel con-cepts that are not given in the training set. These properties are critical in understanding black-box models for which we do not have access to the original training data or any
information about the training process. This is a realistic as-sumption in practice where one needs to interpret and scru-tinize a given model.
To this end, we introduce the Latent Visual Semantic Ex-plainer (LaViSE) as a novel framework to teach any ex-isting CNN to generate text descriptions about its own la-tent representations at the filter level. Our framework dif-fers from supervised approaches in that we do not require to annotate the explanation itself along with an input im-age and a category label. Instead, our method constructs a mapping between the visual and the semantic space using generic image datasets (using images and category names), then transfers the mapping to the target domain without semantic labels. We do not attempt to train more “inter-pretable” models [7, 11, 38, 40, 67] but interpret any given network without changing its structure or retraining. Our work is also closely related to the literature of visual at-tribute or concept based learning [6, 17, 45, 47, 50], but we do not require any additional supervision for attribute label-ing, which makes our approach more generally applicable.
It is also important to note that our method does not merely explain each individual filter separately but uses aggregated responses using a novel filter attention method. Experimen-tal results show our method can generate novel descriptions for learned filters beyond the set of categories defined in the training dataset and provide more accurate explanations for filters comparing to the existing method.
While our main contribution is a novel method to gener-ate explanations for any CNNs, our approach can be used in a practical application of comparative analysis where the goal is to discover and explain the differences between given multiple models or multiple sets of images. To demonstrate the utility, we compare a model finetuned from a pretrained model and a model trained from scratch, and we also compare the gender disparities in datasets. Besides public image datasets, we collect and analyze social media photographs posted by U.S. politicians to exemplify the ef-fectiveness of our method in solving more challenging real-world problems. 2.