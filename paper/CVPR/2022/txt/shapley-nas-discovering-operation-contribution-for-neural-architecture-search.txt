Abstract
In this paper, we propose a Shapley value based method to evaluate operation contribution (Shapley-NAS) for neu-ral architecture search. Differentiable architecture search (DARTS) acquires the optimal architectures by optimizing the architecture parameters with gradient descent, which significantly reduces the search cost. However, the mag-nitude of architecture parameters updated by gradient de-scent fails to reveal the actual operation importance to the task performance and therefore harms the effectiveness of obtained architectures. By contrast, we propose to evaluate the direct influence of operations on validation accuracy.
To deal with the complex relationships between supernet components, we leverage Shapley value to quantify their marginal contributions by considering all possible combi-nations. Specifically, we iteratively optimize the supernet weights and update the architecture parameters by eval-uating operation contributions via Shapley value, so that the optimal architectures are derived by selecting the oper-ations that contribute significantly to the tasks. Since the exact computation of Shapley value is NP-hard, the Monte-Carlo sampling based algorithm with early truncation is employed for efficient approximation, and the momentum update mechanism is adopted to alleviate fluctuation of the sampling process. Extensive experiments on various datasets and various search spaces show that our Shapley-NAS outperforms the state-of-the-art methods by a consid-erable margin with light search cost. The code is available at https://github.com/Euphoria16/Shapley-NAS.git. 1.

Introduction
Neural architecture search (NAS) has attracted great in-terest in deep learning since it discovers the optimal ar-chitecture from a large search space of network compo-nents according to task performance and hardware config-âˆ—Corresponding author urations. Pioneering works applied reinforcement learn-ing [48], evolutionary algorithms [30, 38], and Bayesian optimization [22] for the architecture search, but the large computational overhead prohibits practical deployment of
NAS algorithms. Therefore, it is desirable to design highly efficient search strategies without performance degradation.
To reduce the search cost of architecture search, sev-eral efficient search strategies have been presented includ-ing one-shot NAS [29], network transformation [3], and architecture optimization [26]. Among these approaches, one-shot NAS preserves the optimal sub-networks from the over-parameterized supernet with weight sharing, which prevents the time-consuming exhaustive training for model evaluation.
In particular, DARTS [23] converted the dis-crete operation selection into continuous mixing weights learning and iteratively optimized the architecture param-eters and supernet weights by gradient descent with signif-icantly reduced search cost. However, the magnitude of ar-chitecture parameters in DARTS cannot reflect the actual operation importance in general [36, 42, 47]. That is, the operation with the largest parameter magnitude does not necessarily result in the highest validation accuracy, which degrades the performance of derived architectures.
In this paper, we present a Shapley-NAS method to eval-uate the operation contribution via the Shapley value of su-pernet components for neural architecture search. Instead of relying on the magnitude of architecture parameters up-dated by gradient descent, we consider their practical influ-ences on task performance and propose to directly evalu-ate their contributions to the validation accuracy. Moreover, we observe that the operations in the supernet are related to each other: combinations of operations might have different joint influences on performance compared with their sepa-rate ones. In order to deal with such complex relationships, we leverage Shapley value [31,32], an important solution to attribute contributions to players in cooperative game the-ory. Fig. 1 shows the differences between our Shapley-NAS and existing DARTS methods. Shapley value directly mea-sures the contributions of operations according to the vali-Figure 1. The comparison between DARTS and our Shapley-NAS. (a) DARTS constructs a weight-sharing supernet that consists of all candidate operations. The architecture parameters are optimized by gradient descent, which fails to reflect the importance of operations
[36, 42, 47]. (b) The proposed Shapley-NAS method directly evaluates the marginal contribution of operations to the task performance, according to the validation accuracy difference of all possible operation subsets and their counterparts without the given operation. dation accuracy difference. Meanwhile, it considers all pos-sible combinations and quantifies the average marginal con-tribution to handle complex relationships between individ-ual elements. Benefiting from these, Shapley value is effec-tive for obtaining operation importance that is highly cor-related with task performance. Since computing the exact
Shapley value is NP-hard, we employ the Monte-Carlo sam-pling with early truncation for operation permutation set sampling to approximate it efficiently. Finally, we optimize the supernet weights and update the architecture parame-ters iteratively, where the momentum update mechanism is adopted to alleviate the fluctuation caused by the sam-pling process. We empirically demonstrate that the obtained
Shapley value has a higher correlation with task perfor-mance compared with DARTS. We conducted extensive ex-periments on different datasets across various search spaces, where our Shapley-NAS outperforms the state-of-the-art ar-chitecture search methods. We achieve an error rate of 2.43% on CIFAR-10 [19] on the search space of DARTS and obtain the top-1 accuracy of 23.9% on ImageNet [9] un-der the mobile setting. Furthermore, our Shapley-NAS ac-quires the optimal architectures on CIFAR-10 and CIFAR-100 and near-optimal solutions on ImageNet-16-120 of the
NAS-Bench-201 benchmark [11]. 2.