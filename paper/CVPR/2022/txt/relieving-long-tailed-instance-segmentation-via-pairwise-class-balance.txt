Abstract
Long-tailed instance segmentation is a challenging task due to the extreme imbalance of training samples among classes. It causes severe biases of the head classes (with majority samples) against the tailed ones. This renders “how to appropriately define and alleviate the bias” one of the most important issues. Prior works mainly use label distribu-tion or mean score information to indicate a coarse-grained bias. In this paper, we explore to excavate the confusion matrix, which carries the fine-grained misclassification de-tails, to relieve the pairwise biases, generalizing the coarse one. To this end, we propose a novel Pairwise Class Bal-ance (PCB) method, built upon a confusion matrix which is updated during training to accumulate the ongoing pre-diction preferences. PCB generates fightback soft labels for regularization during training. Besides, an iterative learning paradigm is developed to support a progressive and smooth regularization in such debiasing. PCB can be plugged and played to any existing method as a complement. Experimen-tal results on LVIS demonstrate that our method achieves state-of-the-art performance without bells and whistles. Su-perior results across various architectures show the general-ization ability. The code and trained models are available at https://github.com/megvii-research/PCB. 1.

Introduction
The success of modern object detectors and instance seg-mentors has been verified on rich and balanced datasets.
However, their performance drops dramatically when ap-*Equal contribution. This paper is supported by the National Key
R&D Plan of the Ministry of Science and Technology (Project No. 2020AAA0104400), CAAI-Huawei MindSpore Open Fund, and Beijing
Academy of Artificial Intelligence (BAAI). This work is done during Yin-Yin He’s internship at MEGVII Technology.
†Corresponding author.
Figure 1. Visualization of a randomly sampled 10-class categorical confusion matrix of Mask R-CNN with ResNet-50-FPN on LVIS v0.5 after taking logarithm with base 2. In current palette, higher probabilities are shown with lighter colors. It reflects the model biases clearly (e.g., the probability of misclassifying sunglasses to measuring cup is larger than the reverse). plied to datasets like LVIS [12] which is closer to the long-tailed and large-vocabulary category distribution in the real-world scenario. The devil lies in the classification prediction bias caused by extreme imbalanced training sample volumes among foreground classes [35]. Typically, it is common to leverage instructive indications with positive correlation with the prediction bias into modeling for de-biasing and thus achieve better results. Prior works exploit the class-wise sample frequencies in the training set as an intuitive indica-tion [4,12,26,34]. However, the learning quality of a class is not only about the distribution prior but also factors like op-timization hardness [9], relevant to model learning process.
Feng et al. proposed the mean classification score [10] met-ric. Such train-time model statistics can reflect the learning
Table 1. The performance of models before and after post-hoc calibration using different model statistics on the validation set.
Experiments were conducted on LVIS v0.5 using Mask R-CNN with ResNet-50-FPN. MS stands for mean classification score [10] and CM stands for confusion matrix. Calibration using CM could achieve almost fully unbiased performance.
Sampler
Random
RFS [12]
AP
Info. 21.9
/
MS 25.7
CM 26.3 25.6
/
MS 27.0
CM 28.4
APr APc APf 28.9 21.7 4.6 27.3 28.2 14.3 23.7 27.9 26.0 28.5 26.4 16.0 28.0 28.2 20.6 28.8 28.6 27.9
APb 21.9 25.4 25.9 25.6 27.0 28.2 quality of classes beyond mere label distribution. Yet, we no-tice that it considers only the sample classification statistics within each class, ignoring the inter-class similarities.
To involve the inner-and-inter class relationship, the cat-egorical confusion matrix is already a weapon in hand. It carries the dynamic misclassification conditional probability distribution between pairs of classes (Fig. 1). A proof study is conducted on LVIS v0.5 [12] to verify our conjecture. The upper bound performance of post-hoc calibration using con-fusion matrix and mean score (both of the confusion matrix and mean score are collected on the validation set) is testi-fied. Detailed results are summarized in Tab. 1. As shown, calibration using mean score and confusion matrix can both lift the performance, especially for the rare classes. More importantly, the upper bound of confusion matrix calibration is much higher than that of mean score calibration (+ 9.4
APr on the random sampler and + 8.0 APr on RFS [12] sampler with a similar performance on frequent classes).
Confusion matrix calibration achieves almost fully unbiased performance. Please refer to Sec. 3.2 for details.
Accordingly, the fine-grained misclassification probabili-ties between pairwise classes in the confusion matrix, which we conclude as pairwise bias, is powerful as an indicator.
One vital issue which has to be resolved is that the validation set is unavailable in practical training. One straightforward intuition is to utilize instead the confusion matrix over the train set for calibration. Unfortunately, this malfunctions as shown in Tab. 4. This is probably due to the mismatch be-tween the confusion matrix calculated at train and test time, which is originated from diversified patterns of samples.
Inspired by [15] who conducted train-time disentangling to replace test-time post compensation for long-tailed clas-sification, we develop an online pairwise bias-driven cali-bration method named PCB (Pairwise Class Balance). It maintains a confusion matrix during training with fightback targets generated in a matrix-transposed posterior manner to balance the pairwise bias for each ongoing proposal learn-ing. However, naive exploitation might limit the efficacy, as stronger regularization could be detrimental to the discrimi-nation ability while weaker regularization can not relieve the pairwise bias well. To fully absorb the merits of the above
PCB regularization and meanwhile facilitate the debiasing, a light prediction-dependent iterative paradigm is equipped.
It is accomplished by using the discriminative predictions trained by the original one-hot labels to be embedded back to enhance the features recurrently. PCB regularization is gradually applied to the predictions from each step of the enhanced feature. In this way, a more friendly, progressive pairwise class balancing is achieved which is also proven to be effective in later experimental sections.
To summarize, our contributions are as follows:
• We explore the use of confusion matrix to indicate pair-wise model bias in the field of long-tailed instance seg-mentation, which shows a promising upper bound.
• An Pairwise Class Balance method is proposed to tackle the long-tailed instance segmentation.
• Extensive experiments on LVIS v0.5 and LVIS v1.0 show the effectiveness of our method. 2.