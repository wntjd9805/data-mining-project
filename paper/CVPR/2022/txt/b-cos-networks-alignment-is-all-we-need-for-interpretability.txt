Abstract
We present a new direction for increasing the inter-pretability of deep neural networks (DNNs) by promoting weight-input alignment during training. For this, we pro-pose to replace the linear transforms in DNNs by our B-cos transform. As we show, a sequence (network) of such transforms induces a single linear transform that faith-fully summarises the full model computations. Moreover, the B-cos transform introduces alignment pressure on the weights during optimisation. As a result, those induced lin-ear transforms become highly interpretable and align with task-relevant features. Importantly, the B-cos transform is designed to be compatible with existing architectures and we show that it can easily be integrated into common mod-els such as VGGs, ResNets, InceptionNets, and DenseNets, whilst maintaining similar performance on ImageNet. The resulting explanations are of high visual quality and per-form well under quantitative metrics for interpretability.
Code available at github.com/moboehle/B-cos. 1.

Introduction
While deep neural networks (DNNs) are highly suc-cessful in a wide range of tasks, explaining their deci-sions remains an open research problem [29]. The diffi-culty here lies in the fact that such explanations need to faithfully summarise the internal model computations and present them in a human-interpretable manner. E.g., it is well known that piece-wise linear models (e.g., ReLU-based [24]) are accurately summarised by a linear transform for every input [23]. However, despite providing an accu-rate summary, these piece-wise linear transforms are gen-erally not intuitively interpretable for humans and typically perform poorly under quantitative interpretability metrics, cf. [32, 43]. Recent work thus aimed to improve the expla-nations’ interpretability, often focusing on their visual qual-ity [2]. However, gains in the visual quality of the explana-tions often came at the cost of their model-faithfulness [2].
Instead of optimising the explanation method, in this work we aim to optimise the DNNs to inherently provide an explanation that fulfills the aforementioned requirements— the resulting explanations constitute both a faithful sum-mary and have a clear interpretation for humans. For this, we propose the B-cos transform as a drop-in replacement for linear transforms. As such, the B-cos transform can eas-ily be integrated into a wide range of existing DNN archi-tectures and we show that the resulting B-cos DNNs provide high-quality explanations for their decisions, see Fig. 1.
To ensure that these explanations constitute a faithful summary of the models, we design the B-cos transform as an input-dependent linear transform. Importantly, any se-quence of such transforms therefore induces a single linear transform that faithfully summarises the entire sequence. In order to make the induced linear transforms interpretable, the B-cos transform is designed to induce alignment pres-sure on the weights during optimisation, which optimises the model weights to align with task-relevant input pat-terns. The linear transform induced by the model thus has a clear interpretation: it is a direct reflection of the weights the model has learnt during training and specifically reflects those weights that best align with a given input.
In summary, we make the following contributions: (1) We introduce the B-cos transform to improve neural network interpretability. By promoting weight-input align-ment, these transforms are explicitly designed to yield ex-planations that highlight task-relevant patterns in the input. (2) Specifically, the B-cos transform is designed such that any sequence of B-cos transforms can be faithfully sum-marised by a single linear transform. We show that this al-lows to explain not only the models’ output neurons, but also neurons from arbitrary intermediate network layers. (3) We demonstrate that a plain B-cos convolutional neural network without any additional non-linearities, batch-norm layers [15], or regularisation schemes can achieve compet-itive performance on CIFAR10 [18]. In an ablation study we also show that the parameter B allows for fine-grained control over the increase in weight alignment and thus the interpretability of the B-cos networks. (4) To highlight the generality of our approach, we show that the B-cos transform can easily be integrated into var-ious commonly used DNNs such as InceptionNet [39],
ResNet [13], VGG [34], and DenseNet [14] models, whilst maintaining similar performance. More importantly, the re-sulting architectures are highly interpretable under the B-cos explanations and outperform other explanation methods across all tested architectures, both under quantitative met-rics as well as under qualitative inspection. 2.