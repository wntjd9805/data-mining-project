Abstract
Semi-supervised object detection has made significant progress with the development of mean teacher driven self-training. Despite the promising results, the label mismatch problem is not yet fully explored in the previous works, leading to severe confirmation bias during self-training. In this paper, we delve into this problem and propose a sim-ple yet effective LabelMatch framework from two different yet complementary perspectives, i.e., distribution-level and instance-level. For the former one, it is reasonable to ap-proximate the class distribution of the unlabeled data from that of the labeled data according to Monte Carlo Sam-pling. Guided by this weakly supervision cue, we intro-duce a re-distribution mean teacher, which leverages adap-tive label-distribution-aware confidence thresholds to gen-erate unbiased pseudo labels to drive student learning. For the latter one, there exists an overlooked label assignment ambiguity problem across teacher-student models. To rem-edy this issue, we present a novel label assignment mech-anism for self-training framework, namely proposal self-assignment, which injects the proposals from student into teacher and generates accurate pseudo labels to match each proposal in the student model accordingly. Experiments on both MS-COCO and PASCAL-VOC datasets demon-strate the considerable superiority of our proposed frame-work to other state-of-the-arts. Code will be available at https://github.com/HIK-LAB/SSOD. 1.

Introduction
Supervised learning has advanced object detection in the past few years, benefited from tremendous labeled training data [5, 17, 26, 28, 34]. However, it is extremely expen-sive and time-consuming to collect accurate annotations. As an alternative, semi-supervised object detection (SSOD) is proposed to use a small amount of labeled data in conjunc-†Corresponding author
Figure 1. Label mismatch problems on the MS-COCO dataset. 1) Distribution-level mismatch: there exists a bias between the pseudo labels produced by the single confidence threshold and the ground truth labels (GT) during self-training, as shown in the re-lation of the blue bar and the orange dotted line. 2) Instance-level mismatch: there are two kinds of training patterns for the unla-beled data in the previous SSOD frameworks. One is the same as supervised learning, using both classification and box regres-sion for optimization, which will overfit the poor-quality pseudo labels and result in low localization accuracy. To avoid incorrect box regression, another one merely exploits a classification objec-tive [22], which will bring ambiguity due to the similar classifi-cation scores to confuse the post-processing of Non-Maximum-Suppression (NMS). tion with a large amount of unlabeled data to optimize the detectors [14, 22, 31, 37, 40]. Recently, SSOD has achieved growing interest in the object detection community.
Self-training has been proven useful in SSOD, especially mean teacher framework [22, 37], which annotates the un-labeled data by a gradually evolving teacher and guides the learning of a student in a mutually beneficial manner. As the key process of mean teacher, the existing pseudo label-ing methods [22, 37, 40] simply utilize a hand-crafted con-fidence threshold to filter out low-quality pseudo labels and
directly treat the remaining ones as reliable pseudo labels.
However, it is inevitable to encounter the label mismatch problem, leading to severe confirmation bias [1] during self-training. In this paper, we delve into this problem from two perspectives, i.e., distribution-level and instance-level.
From the perspective of the distribution-level label mis-match problem, it is extremely difficult to generate unbiased pseudo labels to match the ground-truth labels with consis-tent class distribution by using a single and fixed confidence threshold due to the class-imbalanced data distribution. As shown in Fig. 1, the number of pseudo labels is much higher than the ground-truth labels in some classes, while far less in some other classes, resulting in abundant false positives and false negatives. From the perspective of the instance-level label mismatch problem, the existing methods directly follow supervised object detection [28] for label assign-ment. However, the situation is totally different in semi-supervised learning since the quality of pseudo label can-not be guaranteed, leading to label assignment ambiguity problem as illustrated in Fig. 1. Especially in mean teacher driven self-training framework, it is crucial to study how to assign the pseudo labels generated by the mean teacher to the proposals generated by the student network rather than a rough IoU based label assignment manner [28]. Based on the aforementioned challenges of label mismatch in two dif-ferent yet complementary granularities, we begin our study and develop a LabelMatch framework.
To address the first challenge, we present a very sim-ple re-distribution mean teacher. Assumed that the labeled data is selected from the entire data gallery via Monte Carlo
Sampling. In this way, the label distribution of the unla-beled data can be approximated from that of the labeled data.
In fact, we have evaluated the label distribution of the labeled and unlabeled data in several popular SSOD datasets, and they all meet this hypothesis which can be exploited as a weakly supervision cue for pseudo label-ing. Under this inspiration, in contrast to a single and fixed confidence threshold, we utilize adaptive label-distribution-aware confidence thresholds (ACT) to generate unbiased pseudo labels for the unlabeled data, supervised by the la-bel distribution in the labeled data. The ACT are category-specific and adaptively up-to-date during self-training.
To address the second challenge, we propose a novel proposal self-assignment method. Before introducing our method, we should highlight that it is infeasible to set all pseudo labels as hard labels due to the poor quality of the pseudo labels, especially at the beginning of self-training.
Under this consideration, we divide the pseudo labels into reliable ones and uncertain ones according to the confidence score. We treat the reliable labels as hard labels for model optimization identically to the supervised manner, while ex-ploiting the uncertain ones via the proposal self-assignment method for soft learning. Detailedly, the proposals from the student are injected into the teacher for proposals correc-tion, which can provide corresponding soft labels to rec-tify each proposal accordingly. Besides, to encourage pos-itive feedback during self-training, we introduce a reliable pseudo label mining (RPLM) strategy to further improve the performance, which aims to convert the high-quality uncer-tain pseudo labels into reliable ones in a curriculum way.
We benchmark LabelMatch with the same experimental settings to Unbiased-Teacher [22] using the MS-COCO [20] and PASCAL-VOC [7] datasets, namely COCO-standard,
COCO-additional, and VOC. LabelMatch achieves new state-of-the-art results across all benchmarks. Especially in the settings with scarce labeled data, i.e., COCO-standard with only 1% labeled data and VOC, our method can surpass the previous state-of-the-arts by a large margin.
The contributions of this paper are listed as follows:
• We contribute to analyzing the label mismatch problem from the perspectives of distribution-level and instance-level, which provides a brand-new direction for SSOD.
• We propose a simple yet effective LabelMatch frame-work to address the label mismatch problems in SSOD.
In this framework, we 1) present a re-distribution mean teacher to address the distribution-level label mismatch problem; 2) design a proposal self-assignment scheme to address the instance-level label mismatch problem; 3) introduce a reliable pseudo label mining strategy for pseudo label re-calibration during self-training.
• The LabelMatch framework achieves new state-of-the-arts on many popular SSOD benchmarks. Also, we build a MMDetection-based semi-supervised object detection codebase for the fair study of SSOD algorithms. 2.