Abstract
This paper aims at recovering the shape of a scene with unknown, non-Lambertian, and possibly spatially-varying surface materials. When the shape of the object is highly complex and that shadows cast on the surface, the task be-comes very challenging. To overcome these challenges, we propose a coordinate-based deep MLP (multilayer percep-tron) to parameterize both the unknown 3D shape and the unknown reﬂectance at every surface point. This network is able to leverage the observed photometric variance and shadows on the surface, and recover both surface shape and general non-Lambertian reﬂectance. We explicitly predict cast shadows, mitigating possible artifacts on these shad-owing regions, leading to higher estimation accuracy. Our framework is entirely self-supervised, in the sense that it re-quires neither ground truth shape nor BRDF. Tests on real-world images demonstrate that our method outperform ex-isting methods by a signiﬁcant margin. Thanks to the small size of the MLP-net, our method is an order of magnitude faster than previous CNN-based methods. 1.

Introduction
Recovering the 3D shape of a non-Lambertian object from its multiple photometric images taken by a ﬁxed cam-era remains a challenging task. The diverse nature of real-world materials manifests a wide range of speculari-ties on the surface, impeding traditional photometric meth-ods [12, 20, 31, 32]. Moreover, shadows commonly appear in non-convex objects occluding part of the object surface, hindering surface normal estimation. Previous attempts to handle shadows often rely on a rather restrictive Lambertian assumption [5]. The problem becomes much complicated if both specularities and shadows appear on the surface.
With the recent advent of deep learning, tremendous pro-gresses have been made in many computer vision problems, and there is no exception for photometric 3D reconstruc-tion [6, 11, 15, 16, 23, 33]. Current existing deep learning methods often tackle the problem in a supervised training manner. The underlying physics principle of image for-mation are not duly utilized.
In addition, the lack of in-terpretability of deep learning methods prevents leveraging the interactions between object appearance and surface nor-mals. Despite various synthetic datasets with augmentation strategies [7,11,16,23], it remains an open challenge to pro-cess real-world images with both specularities and shadows.
In this paper, we propose an unsupervised neural net-work method that overcomes the issues mentioned above.
Our framework takes the image coordinates corresponding to a surface point as the input, and directly outputs the sur-face normal, reﬂectance parameters (i.e. diffuse albedo and specular parameters), and depth at that surface point. We proposed a series of neural specular basis functions to ac-count for the different types of specularities in the real-world. Our neural bases provide the parameterization for the surface reﬂectance and ﬁt the object’s appearance to ob-tain the accurate surface normal. Furthermore, our frame-work explicitly parameterizes the shadowed regions by trac-ing through the estimated depth map. These shadowed re-gions are then excluded from computation in order to avoid possible rendering artifacts. Following the inverse graph-ics rendering idea, we use the estimated surface normal and neural reﬂectance to re-render the pixel intensities of the surface point under different light directions. Our frame-work is optimized by minimizing the difference between the reconstructed and observed images during the inference time. Therefore, there is no need for any ground truth data or pre-training. Our method outperforms both the super-vised and self-supervised state-of-the-art methods on the challenging real-world dataset of DiLiGenT [24]. Com-pared to other self-supervised deep methods [13, 26], our framework is ten times faster. 2.