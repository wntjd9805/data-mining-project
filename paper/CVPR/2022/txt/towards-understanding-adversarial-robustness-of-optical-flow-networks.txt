Abstract
Recent work demonstrated the lack of robustness of op-tical flow networks to physical patch-based adversarial at-tacks. The possibility to physically attack a basic compo-nent of automotive systems is a reason for serious concerns.
In this paper, we analyze the cause of the problem and show that the lack of robustness is rooted in the classical aperture problem of optical flow estimation in combination with bad choices in the details of the network architecture. We show how these mistakes can be rectified in order to make optical flow networks robust to physical patch-based attacks. Ad-ditionally, we take a look at global white-box attacks in the scope of optical flow. We find that targeted white-box at-tacks can be crafted to bias flow estimation models towards any desired output, but this requires access to the input im-ages and model weights. However, in the case of univer-sal attacks, we find that optical flow networks are robust.
Code is available at https://github.com/lmb-freiburg/understanding_flow_robustness. 1.

Introduction
While deep learning has been conquering many new ap-plication domains, it has become increasingly evident that deep networks are vulnerable to distribution shifts. Adver-sarial attacks are a particular way to showcase this vulner-ability, where one finds the minimal input perturbation that is sufficient to corrupt the network output. As the small per-turbation moves the sample out of the training distribution, the network is detached from its learned patterns and fol-lows the suggestive pattern of the attack. Although many methods have been proposed to improve robustness [34], they only alleviate the problem but do not solve it [1].
While most white-box adversarial attacks are mainly of academic relevance as they reveal the weaknesses of deep networks w.r.t. out-of-distribution data, physical adversarial attacks have serious consequences for safe deployment. In physical attacks, the input is not perturbed artificially, but a confounding pattern is placed in the real world to derail the machine learning approach.
@cs.uni-freiburg.de
Figure 1. Overview. Physical patch-based adversarial attacks on optical flow can be avoided by minor architectural changes.
First row: attacked first frame. Second row: ground truth opti-cal flow. Third and fourth rows: the resulting optical flow es-timates of FlowNetC [8] and our proposed Robust FlowNetC.
FlowNetC is strongly affected by the adversarial patch, whereas
Robust FlowNetC is barely affected. For the robust version we make simple design changes based on causes of the attack; see
Section 6.
Most work on adversarial attacks has been concerned with recognition problems, and it looked for a while as if correspondence problems are not a good target for adver-sarial attacks. However, Ranjan et al. [25] showed that they can successfully perform physical adversarial patch at-tacks on optical flow networks. They optimized an adver-sarial local patch that they can paste into both images, such that large errors appear in the estimated optical flow field even far away from the affected image location. They also showed that the same adversarial patch worked on all vul-nerable architectures, and even demonstrated physical at-tacks, where the printed patch is physically added to a scene and derails the optical flow estimation. Ranjan et al. found that different network architectures show different levels of vulnerability, whereas conventional optical flow meth-ods are not vulnerable at all. They hypothesized the cause for the vulnerability to be in the common encoder-decoder architecture of FlowNet [8] and its derivatives but did not provide a conclusive analysis.
In this paper, we continue their work by a deeper analysis of the actual reason behind the vulnerability. In particular, we answer the following questions. (1) What is the true cause of adversarial patch attacks? (2) Knowing the cause, can the patch-based attack also be built without optimizing it for the particular network (zero query black-box attack)? (3) Can the severe vulnerability be avoided by a specific design of the network architecture or by avoiding mistakes in such design? For an overview see Figure 1.
After answering these questions positively, we turn to-wards (global) adversarial perturbation attacks, i.e., attacks that modify the whole image. We demonstrate that any tar-get optical flow field can be generated; see Figure 11. On the other hand, we show that this attack strategy does not ap-ply to universal (input-agnostic) attacks, i.e., global attacks on optical flow networks must exploit the structure of the in-put images. This is different from unprotected recognition networks, which are vulnerable to imperceptible universal attacks [14, 22]. 2.