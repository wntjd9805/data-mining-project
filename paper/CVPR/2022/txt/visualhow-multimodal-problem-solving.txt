Abstract
Recent progress in the interdisciplinary studies of com-puter vision (CV) and natural language processing (NLP) has enabled the development of intelligent systems that can describe what they see and answer questions accordingly.
However, despite showing usefulness in performing these vision-language tasks, existing methods still struggle in un-derstanding real-life problems (i.e., how to do something) and suggesting step-by-step guidance to solve them. With an overarching goal of developing intelligent systems to assist humans in various daily activities, we propose Vi-sualHow, a free-form and open-ended research that fo-cuses on understanding a real-life problem and deriving its solution by incorporating key components across mul-tiple modalities. We develop a new dataset with 20,028 real-life problems and 102,933 steps that constitute their solutions, where each step consists of both a visual illustra-tion and a textual description that guide the problem solv-ing. To establish better understanding of problems and so-lutions, we also provide annotations of multimodal attention that localizes important components across modalities and solution graphs that encapsulate different steps in struc-tured representations. These data and annotations enable a family of new vision-language tasks that solve real-life problems. Through extensive experiments with represen-tative models, we demonstrate their effectiveness on train-ing and testing models for the new tasks, and there is sig-nificant scope for improvement by learning effective atten-tion mechanisms. Our dataset and models are available at https://github.com/formidify/VisualHow. 1.

Introduction
The remarkable progress in vision-language studies has developed visual systems with the ability to understand and generate natural language information. Existing vision-language models mainly focus on the understanding of vi-*Equal contributions.
Figure 1. VisualHow is a vision-language task aiming to infer the solution to a real-life problem. The solution consists of multiple steps each described with an image and a caption. sual input in task-free (i.e., Image Captioning [3, 12, 17] and Visual Storytelling [28]) or question-directed (i.e., Vi-sual Question Answering [2, 24] and Visual Dialog [18]) settings. In other words, their aim is to develop visual sys-tems that can “look and tell”, by describing or answering questions about what is observed in a scene. On large-scale vision-language datasets [1, 2, 12, 14, 18, 24, 28, 37, 39, 62], state-of-the-art models have obtained promising achieve-ments in understanding and predicting visual and textual information. Although achieving significant progress, these methods only perform well on standardized vision-language inference benchmarks and do not generalize to real-life sit-uations to solve problems, which makes their scope of ap-plication relatively limited.
We believe that the next generation of visual intelligence systems will need to develop the ability to help humans
solve real-life problems more directly. Achieving the goal requires them to provide step-by-step solutions with both textual descriptions and visual illustration. Applications of such systems may include: 1. Teaching people everyday and/or domain-specific skills, such as to tie a tie, to make a sandwich, or to change a bicycle tire. 2. Helping people decompose an abstract goal into actionable items, such as to improve social skills, to sleep better, or to become a soccer player. To this end, we introduce a novel research problem –
VisualHow – along with a large-scale dataset and a system-atic evaluation of various modeling approaches. The main objective of VisualHow is to generate a step-by-step vision-language description of how to solve a problem, where a step will be described using an image and a caption. An ex-ample of VisualHow data is shown in Fig. 1. To “involve a pet in Christmas”, one may need to take a series of different actions. While people may still find it difficult to under-stand how to involve a pet in Christmas by only reading the textual descriptions, looking at the visual illustrations will offer great help in the process. Therefore, given the descrip-tion of the problem and the previous steps, the specific goal of VisualHow is to predict a pair of well-matched and com-plementary image and caption to describe what to do next.
Achieving the goal requires the ability to understand three types of relationships: the relationship between the problem and the solution, the relationships between different steps of the solution, and the relationships between the visual and textual information.
Our goal is to enable the development of intelligent sys-tems for tackling various real-life problems. Compared to conventional vision-language tasks, our proposed Visual-How task has the following differentiating factors: 1. Real-life problems and multimodal solutions. Rather than fo-cusing on specific vision-language tasks [2, 13, 18, 24, 28, 37], our dataset contains 18 categories and 317 subcate-gories of real-life problems. Solutions to these problems are described in multiple steps, each with an image-caption pair, enabling the understanding of the decision-making process in problem solving. 2. Fine-grained annotations.
Our VisualHow dataset offers two types of annotations that are absent from existing studies: the solution graphs de-scribing dependencies between different steps, and mul-timodal attention that highlights and associates important keywords and regions of interest. They play an essen-tial role in developing a structured understanding of the problem-solving procedure and closing the semantic gaps between vision and language. 3. New vision-language tasks. Our dataset enables several new vision-language tasks for various aspects of problem solving. Our experi-ments lead to several interesting observations and sugges-tions on improving the model performance.
To summarize, the contributions of this work are: 1. A new VisualHow study aiming to provide the foun-dation for developing novel vision-language methods and pushing the boundaries of multimodal understanding of real-life problems and solutions; 2. A new dataset that consists of diverse categories of problems, multimodal descriptions of solutions, and fine-grained annotations; 3. Experiments on multiple new tasks on different as-pects of the VisualHow problem and extensive analyses of various baseline models. 2.