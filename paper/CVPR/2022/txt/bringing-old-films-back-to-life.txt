Abstract 1.

Introduction
We present a learning-based framework, recurrent trans-former network (RTN), to restore heavily degraded old ﬁlms.
Instead of performing frame-wise restoration, our method is based on the hidden knowledge learned from adjacent frames that contain abundant information about the occlu-sion, which is beneﬁcial to restore challenging artifacts of each frame while ensuring temporal coherency. More-over, contrasting the representation of the current frame and the hidden knowledge makes it possible to infer the scratch position in an unsupervised manner, and such de-fect localization generalizes well to real-world degradations.
To better resolve mixed degradation and compensate for the ﬂow estimation error during frame alignment, we pro-pose to leverage more expressive transformer blocks for spatial restoration. Experiments on both synthetic dataset and real-world old ﬁlms demonstrate the signiﬁcant supe-riority of the proposed RTN over existing solutions. In ad-dition, the same framework can effectively propagate the color from keyframes to the whole video, ultimately yielding compelling restored ﬁlms. The implementation and model will be released at https://github.com/raywzy/Bringing-Old-Films-Back-to-Life.
*Corresponding author.
Old ﬁlm classics have the lasting power to strike the reso-nance and fantasies of audiences today. Unfortunately, many of them have become less popular because people are no longer used to the low resolution and disturbing artifacts caused by the photographic ﬁlm aging. Film restoration techniques have been developed to bring these old ﬁlms back to life, which nonetheless takes painstaking efforts.
The restoration nowadays is conducted digitally, where the artists meticulously examine each frame, manually retouch the blemishes, ﬁx up the ﬂickering and ﬁnally perform the colorization frame by frame, so repairing the entire old ﬁlm brings insurmountable expenses. Hence, people desire an al-gorithm that performs all of these tedious tasks automatically such that old ﬁlms can be revived in modern looking.
Old ﬁlms typically suffer from mixed degradations, which, to the best of our knowledge, only a few works aim to solve. While one can sequentially apply dedicated mod-els for restoration, the models designed for speciﬁc tasks cannot generalize well to real-world degradations. Recently, higher-order degradation models [41, 48] have been pro-posed to characterize the real-world degradations, yet these works mainly consider the photometric degradations, such as blurriness and noises, rather than the structured defects (e.g., scratches, cracks, etc.) that obstruct the most in old
ﬁlms. Closely related to our work, [37] attempts to address complex degradations in vintage photos, yet its frame-wise processing on old ﬁlms does not yield temporally consis-tent results. The DeepRemaster [14], in comparison, targets video restoration as well as colorization, yet this work cannot sufﬁciently leverage the temporal information with explicit frame alignment and the spatial information with long-range correlations, thus unable to ﬁx up large cracked areas.
In this work, we seek to unify the entire ﬁlm restora-tion tasks with a single framework in which we conduct spatio-temporal restoration. The key insight is that most degradations in old ﬁlms, especially structured defects, are temporally variant, i.e., the structured defects occluded in one frame may reveal its content in successive frames. There-fore, we propose to repair the degradations by leveraging the spatio-temporal context rather than relying on the halluci-nation. Speciﬁcally, we propose a bi-directional recurrent network (Figure. 2) which aggregates the knowledge of the scene across adjacent frames, effectively reducing the ﬁlm
ﬂickering. The hidden state of the recurrent module embeds the representation of the scene content. After the alignment, the restoration for a speciﬁc frame fuses such hidden repre-sentation as it offers useful knowledge of the ﬁlm content underlying the defects. Such a recurrent scheme brings three-fold beneﬁts. First, the ﬁlm degradations, no matter how severe they are, can be fully restored as long as the information is well-preserved in other frames. Second, the explicit maintenance of the hidden knowledge ensures that the restoration for frames is temporally consistent in a long period. More importantly, the structured defects can be local-ized in an unsupervised manner because these areas show a larger discrepancy between the representation of the current frame and the hidden state. As opposed to [37] that requires a defect segmentation network, such defect localization is more generalizable to real-world old ﬁlm degradations.
Spatially, we need a module to account for the slight mis-match during the frame alignment. As such, we propose to leverage the Swin Transformer [25] — even if the hidden representation is not accurately aligned, the interaction of the corresponding pixels can still be modeled through self-attention. Indeed, we observe more stabilized training of the recurrent module due to the use of attention. Besides, thanks to the superior expressivity, the transformer blocks offer improved restoration ability for mixed degradations which would be hard to resolve using a specialized Con-vNet [34, 46]. Thus, the proposed network makes the best of the recurrent module and transformers: the memorization nature of recurrent modules beneﬁts the temporal coherency whereas the long-range modeling capability of transformers helps the spatial restoration, which signiﬁcantly outperforms strong baselines on synthetic datasets, and yields unprece-dented quality when restoring real old ﬁlms.
Moreover, we show that the same framework can be easily
Figure 2. Pipeline overview. Our method follows a bi-directional
RNN architecture.
: Feature aggregation.
D: Pixel reconstruction decoder.
: Spatial restoration.
R
F adapted for ﬁlm colorization as well. We follow the coloriza-tion pipeline favored by artists that only a few keyframes undergo manual colorization, whose color is then propagated to the rest frames. Our method performs favorably over the leading colorization methods, effectively propagating the color from keyframes to the whole video. As shown in Fig-ure. 1, our method uniﬁes the restoration and colorization, and demonstrates the capability of reviving the old ﬁlms as if they were captured by yesterday. 2.