Abstract
Human skeleton-based action recognition offers a valu-able means to understand the intricacies of human behav-ior because it can handle the complex relationships be-tween physical constraints and intention. Although several studies have focused on encoding a skeleton, less atten-tion has been paid to embed this information into the la-tent representations of human action.
InfoGCN proposes a learning framework for action recognition combining a novel learning objective and an encoding method. First, we design an information bottleneck-based learning objec-tive to guide the model to learn informative but compact latent representations. To provide discriminative informa-tion for classifying action, we introduce attention-based graph convolution that captures the context-dependent in-trinsic topology of human action. In addition, we present a multi-modal representation of the skeleton using the rel-ative position of joints, designed to provide complemen-InfoGCN1 surpasses tary spatial information for joints. the known state-of-the-art on multiple skeleton-based ac-tion recognition benchmarks with the accuracy of 93.0% on NTU RGB+D 60 cross-subject split, 89.8% on NTU
RGB+D 120 cross-subject split, and 97.0% on NW-UCLA. 1.

Introduction
Human action recognition is a fundamental problem in computer vision with rich applications, including emer-gency detection [36], sign language recognition [35], and gesture recognition for VR / AR [57], to name a few. In particular, human action recognition based on the skele-ton [6, 7, 19, 44, 58] has attracted much interest in computer vision because of its robustness against a cluttered back-ground. One of the key achievements in skeleton-based ac-tion recognition is a graph convolution network (GCN [21]) based approach.
*These authors contributed equally to this work 1Code is available at github.com/stnoah1/infogcn
Figure 1. Conceptual diagram of InfoGCN. We propose an IB objective and a corresponding loss to guide our model to learn maximally informative representations for skeleton-based action recognition. The encoder infers an intrinsic topology of joints, which provides contextual information beyond physical connec-tivity. The colored lines on the bottom indicate inferred intrinsic topology, and the thickness represents the strength of the relation.
This paper introduces a novel skeleton-based prediction framework for action recognition. Our approach advances the state-of-the-art in three critical aspects. The first is the algorithm for representation learning. A large body of works have demonstrated that representation learning con-siderably influences the performance of machine learning tasks [2, 5, 13, 23, 29, 59, 61]. Our approach is inspired by the information bottleneck (IB) theory [49]. We derive novel IB objective and the corresponding loss to learn the latent representation to be maximally informative to the tar-get variable while compressing the input information con-ditionally and marginally, as illustrated at the top of Fig. 1.
The model learned with the proposed objective performs recognition by encoding implicit and general latent repre-sentation, bridging the input-level physical information and action semantics.
The second is the encoding method of the skeleton.
Graph representation of the skeleton using bone connectiv-ity (extrinsic topology) [27, 33, 44, 56, 60] has an inherent limitation: it can ignore possible joint relations, called an intrinsic topology. When we “take a selfie”, for instance, there may be an intrinsic relation between the hand hold-ing a phone and the upper body since we jointly move them to locate the upper body on the screen of the phone (as the inferred intrinsic topology from our model in Fig. 1). The intrinsic topology of joints [40] provides contextual infor-mation to recognize human action. In this context, we de-velop a novel self-attention based graph convolution (SA-GC) module, to extract the intrinsic graph structure when encoding a sequence of the skeleton. As shown at the bot-tom of Fig. 1, for the similar poses that appear in different actions, the inferred topologies can be different based on their behavioral contexts.
Lastly, we propose a multi-modal skeleton representa-tion by utilizing the joints’ relative positions. It provides complementary spatial information of a joint. An ensemble of the models trained with the representations drastically improves recognition performance.
By coupling the aforementioned three proposals, we in-troduce a new learning framework for skeleton-based ac-tion recognition named InfoGCN. To verify the effective-ness of our approach, we perform empirical evaluations in skeleton-based action recognition and compare our re-sults with competitive baselines on three popular bench-mark datasets: NTU RGB+D 60 & 120 [30, 42], and NW-UCLA [55]. Experimental results show that our model achieves state-of-the-art performance on all three datasets in terms of accuracy. Analysis shows that the learned la-tent representation of action adheres to the proposed IB con-straints, and the context-dependent intrinsic topology is in-ferred adaptively depending on the behavioral context.
Our contributions are as follows:
• Information Bottleneck Objectives. We introduce a novel learning objective based on IB that aims to learn an efficiently compressed latent representation of an action.
• Self-Attention based Graph Convolution. We propose an SA-GC module that infers a context-dependent intrin-sic topology in spatial modeling of a skeleton.
• Multi-Modal Representation. We present a multi-modal representation of a skeleton for the model ensem-ble that drastically improves action recognition perfor-mance.
• Empirical Verification. Extensive experiments demon-strate the advantages of our work.
InfoGCN achieves state-of-the-art performance on the three datasets in skeleton-based action recognition. 2.