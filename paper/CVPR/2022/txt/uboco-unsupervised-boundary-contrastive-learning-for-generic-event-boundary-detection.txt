Abstract
Generic Event Boundary Detection (GEBD) is a newly suggested video understanding task that aims to find one level deeper semantic boundaries of events. Bridging the gap between natural human perception and video under-standing, it has various potential applications, including interpretable and semantically valid video parsing. Still at an early development stage, existing GEBD solvers are sim-ple extensions of relevant video understanding tasks, disre-garding GEBD’s distinctive characteristics. In this paper, we propose a novel framework for unsupervised/supervised
GEBD, by using the Temporal Self-similarity Matrix (TSM) as the video representation. The new Recursive TSM Pars-ing (RTP) algorithm exploits local diagonal patterns in
TSM to detect boundaries, and it is combined with the
Boundary Contrastive (BoCo) loss to train our encoder to generate more informative TSMs. Our framework can be applied to both unsupervised and supervised settings, with both achieving state-of-the-art performance by a huge mar-gin in GEBD benchmark. Especially, our unsupervised method outperforms the previous state-of-the-art “super-vised” model, implying its exceptional efficacy. 1.

Introduction
With the proliferation of video platforms, video under-standing tasks are drawing substantial attention in computer vision community. The prevailing convention for video pro-cessing [3,15,16,21,28] is still dividing the whole video into short non-overlapping snippets with a fixed duration, which neglects the semantic continuity of the video. On the other hand, cognitive scientists have observed that human senses the visual stream as a set of events [39], which alludes that there is room for research to find out a video parsing method
*equal contribution, ordered by surname that preserves semantic validity and interpretability of video snippets.
From this perspective, Generic Event Boundary Detec-tion (GEBD) [37] can be seen as a new attempt to intercon-nect human perception mechanisms to video understanding.
The goal of GEBD is to pinpoint the content change mo-ments in which humans perceive them as event boundaries.
To reflect human perception, labels for the task are anno-tated following the instruction of finding boundaries of one level deeper granularity compared to the video-level event, regardless of action classes. This characteristic differenti-ates GEBD from the previous video localization tasks [42] in the sense that the labels are only given by natural human perception, not the predefined action classes.
Recently released Kinetics-GEBD [37] is the first dataset for the GEBD. The distinctive point in the dataset is that the event boundary labels are annotated by 5 different an-notators, making the dataset convey the subjectivity of hu-man perception. Besides, various baseline methods for the
GEBD task were also included in [37]. Based on the sim-ilarity to Temporal Action Localization (TAL), many of the suggested GEBD methods were extensions of previous
TAL works [26, 27], while several unsupervised methods exploited shot detection methods and event segmentation theory [25, 36, 43]. Detailed explanations about baseline
GEBD methods can be found in Section 2.
Nevertheless, straightforward adoptions of existing methods have a clear limitation in that they directly use pre-trained features to predict boundary points. As the features are extracted from the classification-pretrained networks like ResNet-50 [19], they inevitably focus on class-specific or object-centric information. However, many event bound-aries, especially those that are hard to detect, do not entail scene change (e.g. Stand-by frame and Running frame in
Figure 1 share most of the scene information). For captur-ing event boundaries, what really matters is the relation-ship between adjacent frames, implying the need for a new
Figure 1. GEBD is a task of finding boundaries of one level deeper granularity compared to the video level event. (a) The relationship between frames showing that the local similarities between adjacent frames are maintained except near the event boundary. (b) Similarity pattern near a event boundary B where the yellow region S indicates high similarity score and the blue region D indicates low similarity score. (c) Temporal Self-similarity Matrix (TSM) represents the pairwise self-similarity scores between video frames. Similar patterns are observed in event boundaries, and we can detect boundary frames by mining this pattern from the TSM. method to properly exploit it.
In this paper, we introduce a novel method to discover generic event boundaries, which can be put into both unsu-pervised and supervised settings. Our main intuition comes from observing the self-similarity of a video, visualized as
Temporal Self-similarity Matrix (TSM). While TSM has been considered as a useful tool to analyze periodic videos due to its robustness against noise [1, 2, 14, 34], we found that TSM’s potential is not limited to periodic videos, but can also be extended to analyzing non-periodic videos if we focus on its local diagonal patterns. To be specific, we can exploit TSM’s robustness by taking it as an information bottleneck for the GEBD solver, making it perform well on unseen scenes, objects, and even action classes [14].
Figure 1 briefly illustrates our observation. For a se-quence of events in a given video, there is a semantic incon-sistency at the event boundary, resulting in the similarity-break near the boundary point. As TSM depicts self-similarity scores between video frames, this similarity-break brings distinctive patterns (Figure 1 (c)) on TSM, which can be a meaningful cue when it comes to detect-ing event boundaries. Hence, we exploit TSM as the fi-nal representation of the given video and devise a novel method to detect event boundaries, namely Recursive TSM
Parsing (RTP). Paired with our Boundary Contrastive loss (BoCo loss) , RTP can be extended to Unsupervised Bound-ary Contrastive (UBoCo) learning, a fully label-free train-ing framework for event boundary detection.
Going one step further, we also propose Supervised
Boundary Contrastive (SBoCo) learning approach for the
GEBD task, which utilizes TSM as an interpretable inter-mediate representation. Unlike UBoCo that uses an algo-rithmic method to parse TSM, this supervised approach has
TSM decoder, which is a standard neural network. By merging binary cross-entropy (BCE) and BoCo loss, our supervised approach achieved the state-of-the-art perfor-mance in recent official GEBD challenge1.
To summarize, the main contribution of the paper is as follows:
• We discovered that the properties of Temporal Self-Similarity Matrix (TSM) matches very well with the
Generic Event Boundary Detection (GEBD) task, and propose to use TSM as the representation of the video to solve GEBD.
• Taking advantage of TSM’s distinctive boundary pat-terns, we propose Recursive TSM Parsing (RTP) al-gorithm, which is a divide-and-conquer approach for detecting event boundaries.
• Unsupervised framework for GEBD is introduced by combining RTP and the new Boundary Contrastive (BoCo) loss. Using the BoCo loss, the video encoder can be trained without labels and generate more dis-tinctive TSMs. Our unsupervised framework outper-forms not only previous unsupervised methods, but also supervised methods.
• Our framework can be easily extended to the super-vised setting by adding a decoder and achieve the state-of-the-art performance by a large margin (16.2%). 1CVPR’21 LOng-form VidEo Understanding (LOVEU) Kinetics-GEBD challenge
2.