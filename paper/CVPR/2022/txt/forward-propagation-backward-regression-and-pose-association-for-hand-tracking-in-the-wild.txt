Abstract
We propose HandLer, a novel convolutional architec-ture that can jointly detect and track hands online in un-constrained videos. HandLer is based on Cascade-RCNN with additional three novel stages. The ﬁrst stage is For-ward Propagation, where the features from frame t-1 are propagated to frame t based on previously detected hands and their estimated motion. The second stage is the Detec-tion and Backward Regression, which uses outputs from the forward propagation to detect hands for frame t and their relative offset in frame t-1. The third stage uses an off-the-shelf human pose method to link any fragmented hand tracklets. We train the forward propagation and backward regression and detection stages end-to-end together with the other Cascade-RCNN components.
To train and evaluate HandLer, we also contribute
YouTube-Hand, the ﬁrst challenging large-scale dataset of unconstrained videos annotated with hand locations and their trajectories. Experiments on this dataset and other benchmarks show that HandLer outperforms the exist-ing state-of-the-art tracking algorithms by a large margin.
Code and data are available at https://vision.cs. stonybrook.edu/˜mingzhen/handler/. 1.

Introduction
Hand tracking is an important problem in various ap-plication scenarios, from gesture and activity recognition to contact tracing and skill evaluation. One approach for tracking hands is to consider them as parts of a human body and then perform hand tracking based on the tracked human pose. But pose detection and tracking can be unreliable by itself, especially for people that are partially occluded or outside the ﬁeld of view of the camera. Another approach for hand tracking is to use off-the-shelf tracking methods.
Unfortunately, single-object trackers are not appropriate for tracking multiple hands, while existing multiple-object trackers do not work well for hands even though they have shown impressive performance for tracking pedestrians and vehicles [2, 5, 19, 47, 49, 50, 62]. Hand tracking is difﬁcult because hands are not ordinary objects, given the extreme
Figure 1. We develop an advanced detection and association algo-rithm for tracking multiple hands. Different with previous meth-ods that estimate probability of a detected hand on the objectness scores at current frame only, we estimate the probability based on both the objectness scores at frames t and t-1, and associate hands across frame via both pose and tracking offsets. articulation of hands and the frequent interaction of hands with other objects. In a short period of a few frames, the size, shape, location, and visibility of a hand can change dramatically and frequently. Many existing multiple-object trackers use the detection and assoication paradigm. How-ever, hand detection would fail in the presence of motion blur and occlusion, while hand linking across time is dif-ﬁcult as the size, location, pose, and appearance of a hand can change drastically. Simultaneously, two different hand instances might look alike, so distinguishing them would be difﬁcult even for a sophisticated re-identiﬁcation module that has been trained speciﬁcally for hands.
In this work, we develop a novel convolutional architec-ture that can detect and track hands in unconstrained videos.
We name the proposed architecture HandLer, which stands for Hand Linker. HandLer takes as input two consecutive video frames at times t-1 and t, and output the detected hands in frame t as well as their corresponding locations in frame t-1. The processing pipeline consists of three stages. The ﬁrst stage is the Forward Propagation, which propagates features from frame t-1 to frame t based on the locations of previously detected hands and their esti-mated movements. The second stage is the Detection and
2.