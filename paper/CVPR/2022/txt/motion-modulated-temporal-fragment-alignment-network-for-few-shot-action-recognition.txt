Abstract
While the majority of FSL models focus on image classi-ﬁcation, the extension to action recognition is rather chal-lenging due to the additional temporal dimension in videos.
To address this issue, we propose an end-to-end Motion-modulated Temporal Fragment Alignment Network (MT-FAN) by jointly exploring the task-speciﬁc motion modu-lation and the multi-level temporal fragment alignment for
Few-Shot Action Recognition (FSAR). The proposed MT-FAN model enjoys several merits. First, we design a mo-tion modulator conditioned on the learned task-speciﬁc mo-tion embeddings, which can activate the channels related to the task-shared motion patterns for each frame. Second, a segment attention mechanism is proposed to automati-cally discover the higher-level segments for multi-level tem-poral fragment alignment, which encompasses the frame-to-frame, segment-to-segment, and segment-to-frame align-ments. To the best of our knowledge, this is the ﬁrst work to exploit task-speciﬁc motion modulation for FSAR. Extensive experimental results on four standard benchmarks demon-strate that the proposed model performs favorably against the state-of-the-art FSAR methods. 1.

Introduction
Deep learning has achieved tremendous success in the
ﬁeld of action recognition [31,37,38,43]. However, modern deep learning approaches require large amounts of anno-tated data, and collecting these data is laboriously difﬁcult and costly [1]. To reduce the need for human annotation,
Few-Shot Learning (FSL) [9, 32, 41, 44] has been proposed and gained increasing interest, which aims at classifying un-labeled samples (query set) into new unseen classes with only a few labeled examples (support set).
While the majority of FSL models [9, 32, 41, 44] focus on image classiﬁcation, its extension to video classiﬁca-tion is rather challenging. This is because videos have a much more complicated structure than images with an ad-ditional temporal dimension [4]. To utilize the temporal information, some recent methods [3, 4, 25] perform tem-∗Corresponding Author
Figure 1. Different ways of alignment. (a), (b): The previous methods [3, 4, 25] match videos at the single level, i.e., “frame to frame” or “segment to segment”. (c) The temporal fragment alignment is performed at multiple levels, i.e., “frame to frame”,
“segment to segment”, and “frame to segment”, which suits the videos with different speeds in the real-world setting. poral alignment to match the video frames or segments in the temporal dimension (see Figure 1), which helps to dif-fer order-sensitive actions. In [4], Cao et al. use Dynamic
Time Warping [24] to ﬁnd the optimal alignment path be-tween frames. Then the video distance is measured as the alignment cost of frame sequences. In [3] and [25], the at-tention mechanisms are utilized to achieve temporal align-ment. In [3], Bishay et al. ﬁrst uniformly sample segments from videos, with each of them containing the ﬁxed length of frames. Then they feed these segments into 3D CNNs to extract motion features, and conduct segment-level atten-tion for temporal alignment. Similarly, in [25], it randomly samples pairs/triplets of frames from videos to form video segments, and performs attention over these segments.
By studying the previous Few-Shot Action Recogni-tion (FSAR) methods that are based on temporal align-ment [3, 4, 25], we sum up two aspects that are imperative for building a robust FSAR model. (1) Task-speciﬁc Mo-tion Pattern Mining. Motion modeling has been proved essential for action recognition, as videos contain rich tem-poral structures [19]. Some methods [3, 16, 48] adopt 3D
CNNs to extract motion features for FSAR. However, they have high computational costs and lack speciﬁc consider-ation in modeling temporal structure for the few-shot set-ting. Specially in FSAR, the tasks are composed of novel categories, which causes large inter-task variances. Thus, utilizing the same network to extract motion features for all tasks may not be suitable, as the motion patterns have sub-stantial variances in different speciﬁc tasks. Therefore, how to effectively mine task-speciﬁc motion patterns is of vi-tal importance for FSAR. (2) Multi-level Temporal Frag-ment Alignment. Most previous methods perform either the frame-level alignment [4] or the segment-level align-ment [3,25] (see Figure 1 (a) and (b)), which may cause am-biguity and misalignment when matching videos at different speeds. Furthermore, the segments in [3, 25] are obtained by pre-deﬁned sampling strategies, and thus suffer large randomness and may exacerbate the problem of misalign-ment. In fact, the temporal alignment in the real-world set-ting not only includes the “frame-to-frame” and “segment-to-segment” alignment, but also includes the “frame-to-segment” alignment, where segments are composed of sev-eral semantically-related frames (see Figure 1(c)). Here, we use the temporal fragments to uniformly represent both frames and segments. To achieve robust matching for videos at different speeds, the model should have abilities to automatically discover higher-level segments and perform multi-level temporal fragment alignment.
Inspired by the above insights, we propose an end-to-end
Motion-modulated Temporal Fragment Alignment Net-work (MTFAN) by jointly exploring the task-speciﬁc mo-tion modulation module and the multi-level temporal frag-ment alignment module for FSAR. In the task-speciﬁc mo-tion modulation module, we ﬁrst aggregate the temporal differences over the consecutive frames from the support videos to induce the task-speciﬁc motion pattern. Subse-quently, a motion modulator is proposed to excite motion-relevant channels for frames according to the task-level tem-In this way, the networks are forced poral knowledge. to discover and enhance the task-shared informative mo-tion information, which facilitates a better alignment be-tween videos in the same task. In the multi-level temporal fragment alignment module, we propose a Transformer-inspired Segment Attention Layer to adaptively generate segments by aggregating the arbitrary number of related frames. Speciﬁcally, we introduce several learnable seg-ment prototypes conditioned on the prior video context to serve as queries, and take frame features as keys and val-ues. We can obtain higher-level segments by operating at-tention between the segment prototypes and frames. With the frames and the discovered segments, we can exploit more diverse alignments between the temporal fragments, including the “frame to frame”, “segment to segment”, and
“frame to segment”. By considering the multi-level tempo-ral fragment alignment, the model could ﬂexibly discover and align similar temporal patterns with different time dura-tions. Finally, we reformulate the temporal fragment align-ment process as an Optimal Transport problem [40] and use the Sinkhorn algorithm [8] to solve it.
The contributions of our model could be summarized into three-fold: (1) We propose an end-to-end Motion-modulated Temporal Fragment Alignment Network (MT-FAN) by jointly exploiting the task-speciﬁc motion modu-lation and the multi-level temporal fragment alignment. (2)
We design a motion modulator to activate the channels re-lated to task-speciﬁc motion patterns for the frames. Also, a segment attention layer is proposed to discover the higher-level segments for multi-level temporal fragment alignment.
To our best knowledge, this is the ﬁrst work to exploit task-speciﬁc motion modulation for FSAR. (3) Extensive experi-mental results on four challenging benchmarks demonstrate that our method performs favorably against the state-of-the-art FSAR methods. 2.